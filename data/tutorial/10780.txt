systran   s pure id4 systems

josep crego, jungi kim, guillaume klein, anabel rebollo, kathy yang, jean senellart
egor akhanov, patrice brunelle, aur  elien coquard, yongchao deng, satoshi enoue, chiyo geiss

joshua johanson, ardas khalsa, raoum khiari, byeongil ko, catherine kobus, jean lorieux
leidiana martins, dang-chuan nguyen, alexandra priori, thomas riccardi, natalia segal
christophe servan, cyril tiquet, bo wang, jin yang, dakun zhang, jing zhou, peter zoldan

firstname.lastname@systrangroup.com

systran

abstract

1 introduction

6
1
0
2

 
t
c
o
8
1

 

 
 
]
l
c
.
s
c
[
 
 

1
v
0
4
5
5
0

.

0
1
6
1
:
v
i
x
r
a

since the    rst online demonstration of
id4 (id4) by
lisa (bahdanau et al., 2014), id4 de-
velopment has recently moved from lab-
oratory to production systems as demon-
strated by several entities announcing roll-
out of id4 engines to replace their ex-
isting technologies. id4 systems have
a large number of training con   gurations
and the training process of such systems
is usually very long, often a few weeks,
so role of experimentation is critical and
important
in this work, we
present our approach to production-ready
systems simultaneously with release of
online demonstrators covering a large va-
riety of languages (12 languages, for 32
language pairs). we explore different
practical choices: an ef   cient and evolu-
tive open-source framework; data prepara-
tion; network architecture; additional im-
plemented features; tuning for production;
etc. we discuss about evaluation method-
ology, present our    rst    ndings and we    -
nally outline further work.

to share.

our ultimate goal is to share our expertise
to build competitive production systems
for    generic    translation. we aim at con-
tributing to set up a collaborative frame-
work to speed-up adoption of the technol-
ogy, foster further research efforts and en-
able the delivery and adoption to/by in-
dustry of use-case speci   c engines inte-
grated in real production work   ows. mas-
tering of the technology would allow us
to build translation engines suited for par-
ticular needs, outperforming current sim-
plest/uniform systems.

neural mt has recently achieved state-of-the-
art performance in several large-scale translation
tasks. as a result, the deep learning approach to
mt has received exponential attention, not only
by the mt research community but by a growing
number of private entities, that begin to include
id4 engines in their production systems.

in

the

last

decade,
have

several
open-
source mt toolkits
emerged   moses
(koehn et al., 2007) is probably the best-known
out-of-the-box mt system   coexisting with
commercial alternatives,
though lowering the
entry barriers and bringing new opportunities on
both research and business areas. following this
direction, our id4 system is based on the open-
source project id195-attn1 initiated by the
harvard nlp group2 with the main contributor
yoon kim. we are contributing to the project by
sharing several features described in this technical
report, which are available to the mt community.
neural mt systems have the ability to directly
model, in an end-to-end fashion, the association
from an input text (in a source language) to its
translation counterpart (in a target language). a
major strength of neural mt lies in that all the
necessary knowledge, such as syntactic and se-
mantic information, is learned by taking the global
sentence context into consideration when mod-
eling translation. however, neural mt engines
are known to be computationally very expen-
sive, sometimes needing for several weeks to ac-
complish the training phase, even making use of
cutting-edge hardware to accelerate computations.
since our interest is for a large variety of lan-
guages, and that based on our long experience with
machine translation, we do not believe that a one-
   ts-all approach would work optimally for lan-

1https://github.com/harvardnlp/id195-attn
2http://nlp.seas.harvard.edu

guages as different as korean, arabic, spanish or
russian, we did run hundreds of experiments, and
particularily explored language speci   c behaviors.
one of our goal would indeed be to be able to in-
ject existing language knowledge in the training
process.

in this work we share our recipes and experi-
ence to build our    rst generation of production-
ready systems for    generic    translation, setting a
starting point to build specialized systems. we
also report on extending the baseline id4 en-
gine with several features that
in some cases
increase performance accuracy and/or ef   ciency
while for some others are boosting the learning
curve, and/or model speed. as a machine trans-
lation company, in addition to decoding accuracy
for    generic domain   , we also pay special atten-
tion to features such as:

    training time

    customization possibility: user terminology,

id20

    preserving and leveraging internal format

tags and misc placeholders

    practical integration in business applications:
for instance online translation box, but also
translation batch utilities, post-editing envi-
ronment...

    multiple deployment environments: cloud-
based, customer-hosted environment or em-
bedded for mobile applications

    etc

more important than unique and uniform trans-
lation options, or reaching state-of-the-art research
systems, our focus is to reveal language speci   c
settings, and practical tricks to deliver this tech-
nology to the largest number of users.

the remaining of this report is as follows: sec-
tion 2 covers basic details of the id4 system em-
ployed in this work. description of the transla-
tion resources are given in section 3. we report on
the different experiments for trying to improve the
system by guiding the training process in section
4 and section 5, we discuss about performance.
in section 6 and 7, we report on evaluation of the
models and on practical    ndings. and we    nish by
describing work in progress for the next release.

2 system description

we base our id4 system on the encoder-decoder
framework made available by the open-source
project id195-attn. with its root on a num-
ber of established open-source projects such as
andrej karpathy   s char-id56,3 wojciech zaremba   s
standard long short-term memory (lstm)4 and
the id56 library from element-research,5
the
framework provides a solid id4 basis consist-
ing of lstm, as the recurrent module and faith-
ful reimplementations of global-general-attention
model and input-feeding at each time-step of the
id56 decoder as described by luong et al. (2015).
it also comes with a variety of features such as
the ability to train with bidirectional encoders and
pre-trained id27s, the ability to handle
unknown words during decoding by substituting
them either by copying the source word with the
most attention or by looking up the source word
on an external dictionary, and the ability to switch
between cpu and gpu for both training and de-
coding. the project is actively maintained by the
harvard nlp group6.

over the course of the development of our own
id4 system, we have implemented additional
features as described in section 4, and contributed
back to the open-source community by making
many of them available in the id195-attn
repository.

id195-attn is implemented on top of the
popular scienti   c computing library torch.7 torch
uses lua, a powerful and light-weight script lan-
guage, as its front-end and uses the c language
where ef   cient implementations are needed. the
combination results in a fast and ef   cient system
both at the development and the run time. as an
extension, to fully bene   t from multi-threading,
optimize cpu and gpu interactions, and to have
   ner control on the objects for runtime (sparse ma-
trix, quantized tensor, ...), we developed a c-based
decoder using the c apis of torch, called c-torch,
explained in detail in section 5.4.

the number of parameters within an id4
model can grow to hundreds of millions, but there
are also a handful of meta-parameters that need to
be manually determined. for some of the meta-

3https://github.com/karpathy/char-id56
4https://github.com/wojzaremba/lstm
5https://github.com/element-research/id56
6http://nlp.seas.harvard.edu
7http://torch.ch

model

training

text unit

embedding dimension: 400-1000
hidden layer dimension: 300-1000
number of layers: 2-4
uni-/bi-directional encoder

optimization method
learning rate
decay rate
epoch to start decay
number of epochs
dropout: 0.2-0.3
section 4.1
vocabulary selection
word vs. subword (e.g. bpe)

train data section 3

size (quantity vs. quality)
max sentence length
selection and mixture of domains

table 1:
there are a large number of meta-
parameters to be considered during training. the
optimal set of con   gurations differ from language
pair to language pair.

parameters, many previous work presents clear
choices on their effectiveness, such as using the
attention mechanism or feeding the previous pre-
diction as input to the current time step in the
decoder. however,
there are still many more
meta-parameters that have different optimal values
across datasets, language pairs, and the con   gura-
tions of the rest of the meta-parameters. in table 1,
we list the meta-parameter space that we explored
during the training of our id4 systems.

in appendix b, we detail the parameters used for

the online system of this    rst release.

3 training resources

training    generic    engines is a challenge, be-
cause there is no such notion of generic transla-
tion which is what online translation service users
are expecting from these services. indeed online
translation is covering a very large variety of use
cases, genres and domains. also available open-
source corpora are domain speci   c: europarl
(koehn, 2005), jrc (steinberger et al., 2006) or
multiun (chen and eisele, 2012) are legal texts,
ted talk are scienti   c presentations, open subtitles
(tiedemann, 2012) are colloquial, etc. as a result,

the training corpora we used for this release were
built by doing a weighted mix all of the available
sources. for languages with large resources, we
did reduce the ratio of the institutional (europal,
un-type), and colloquial types     giving the pref-
erence to news-type, mix of webpages (like giga-
word).

our strategy, in order to enable more experi-
ments was to de   ne 3 sizes of corpora for each lan-
guage pair: a baseline corpus (1 million sentences)
for quick experiments (day-scale), a medium cor-
pus (2-5m) for real-scale system (week-scale) and
a very large corpora with more than 10m seg-
ments.

the amount of data used to train online systems
are reported in table 2, while most of the individ-
ual experimental results reported in this report are
obtained with baseline corpora.

note that size of the corpus needs to be consid-
ered with the number of training periods since the
neural network is continuously fed by sequences
of sentence batches till the network is considered
trained.
in junczys-dowmunt et al. (2016), au-
thors mention using corpus of 5m sentences and
training of 1.2m batches each having 40 sentences
    meaning basically that each sentence of the full
corpus is presented 10 times to the training.
in
wu et al. (2016), authors mention 2m steps of 128
examples for english   french, for a corpus of 36m
sentences, meaning about 7 iterations on the com-
plete corpus. in our framework, for this release,
we systematically extended the training up to 18
epochs and for some languages up to 22 epochs.

selection of the optimal system is made af-
ter the complete training by calculating scores
on independent test sets. as an outcome, we
have seen different behaviours for different lan-
guage pairs with similar training corpus size ap-
parently connected to the language pair complex-
ity. for instance, english   korean training per-
plexity still decreases signi   cantly between epoch
13 and 19 while italian   english perplexity de-
creases marginally after epoch 10.
for most
languages,
in our set-up, optimal systems are
achieved around epoch 15.

we did also some experiment on the corpus size.
intuitively, since id4 systems do not have the
memorizing capacity of pbmt engines, the fact
that the training use 10 times 10m sentence cor-
pus, or 20 times 5m corpus should not make a
huge difference.
in one of the experiment, we

compared training on a 5m corpus trained over 20
epochs for english to/from french, and the same
5m corpus for only 10 epochs, followed by 10
additional epochs on additional 5m corpus. the
10m being completely homogeneous. in both di-
rections, we observe that the 5    10 + 5    10
training is completing with a score improvement
of 0.8     1.2 compared to the 5    20 showing that
the additional corpus is managing to bring a mean-
ingful improvement. this observation leads to a
more general question about how much corpus is
needed to actually build a high quality id4 en-
gine (learn the language), the role and timing of
diversity in the training and whether the incremen-
tal gain could not be substituted by terminology
feeding (learn the lexicon).

4 technology

in this section we account for several experiments
that improved different aspects of our translation
engines. experiments range from preprocessing
techniques to extend the network with the abil-
ity to handle named entities, to use multiple word
features and to enforce the attention module to be
more like word alignments. we also report on dif-
ferent levels of translation customization.

4.1 id121

all corpora are preprocessed with an in-house
toolkit. we use standard token separators (spaces,
tabs, etc.) as well as a set of language-dependent
linguistic rules. several kinds of entities are recog-
nized (url and number) replacing its content by the
appropriate place-holder. a postprocess is used
to detokenize translation hypotheses, where the
original raw text format is regenerated following
equivalent techniques.

for each language, we have access to lan-
guage speci   c id121 and id172
rules. however, our preliminary experiments
showed that there was no obvious gain of using
these language speci   c id121 patterns, and
that some of the hardcoded rules were actually de-
grading the performance. this would need more
investigation, but for the release of our    rst batch
systems, we used a generic id121 model for
most of the languages except arabic, chinese and
german. in our past experiences with arabic, sep-
arating segmentation of clitics was bene   cial, and
we retained the same procedure. for german and
chinese, we used in-house compound splitter and

id40 models, respectively.

in our current id4 approach, vocabulary size
is an important factor that determines the ef-
   ciency and the quality of the translation sys-
tem; a larger vocabulary size correlates directly
to greater computational cost during decoding,
whereas low coverage of vocabulary leads to se-
vere out-of-vocabulary (oov) problems, hence
lowering translation quality.

in most language pairs, our strategy combines a
vocabulary shortlist and a placeholder mechanism,
as described in sections 4.2 and 4.3. this ap-
proach, in general, is a practical and linguistically-
robust option to addressing the    xed vocabulary
issue, since we can take the full advantage of in-
ternal manually-crafted dictionaries and customi-
sized user dictionaries (uds).

two language pairs:

a number of previous work such as character-
level (chung et al., 2016), hybrid word-character-
based (luong and manning, 2016) and subword-
level (sennrich et al., 2016b) address issues that
arise with morphologically rich languages such as
german, korean and chinese. these approaches
either build accurate open-vacabulary word repre-
sentations on the source side or improve transla-
tion models    generative capacity on the target side.
among those approaches, subword id121
yields competitive results achieving excellent vo-
cabulary coverage and good ef   ciency at the same
time.
for

enko and jaen,
we used source and target sub-word tokeniza-
tion (bpe, see (sennrich et al., 2016b)) to re-
duce the vocabulary size but also to deal with
rich morphology and spacing    exibility that can
be observed in korean.
although this ap-
proach is very seducing by its simplicity and
also used systematically in (wu et al., 2016) and
(junczys-dowmunt et al., 2016), it does not have
signi   cant side effects (for instance generation of
impossible words) and is not optimal to deal with
actual word morphology - since the same suf   x
(josa in korea) depending on the frequency of the
word ending it is integrated with, will be splitted
in multiple representations. also, in korean, these
josa, are is an    agreement    with the previous syl-
labus based on their    nal endings: however such
simple information is not explicitely or implicitely
reachable by the neural network.

the sub-word encoding algorithm byte pair en-
coding (bpe) described by sennrich et al. (2016b)

was re-implemented in c++ for further speed op-
timization.

4.2 word features

input

sennrich and haddow (2016) showed that using
additional
features improves translation
quality. similarly to this work, we introduced in
the framework the support for an arbitrary number
of discrete word features as additional inputs to the
encoder. because we do not constrain the num-
ber of values these features can take at the same
time, we represent them with continuous and nor-
malized vectors. for example, the representation
of a feature f at time-step t is:

x(t)

i =( 1

nf
0

if f takes the ith value
otherwise

(1)

where nf is the number of possible values the

feature f can take and with x(t)     rnf .

these representations are then concatenated
with the id27 to form the new input
to the encoder.

we extended this work by also supporting ad-
ditional features on the target side which will be
predicted by the decoder. we used the same input
representation as on the encoder side but shifted
the features sequence compared to the words se-
quence so that the prediction of the features at
time-step t depend on the word at time-step t they
annotate. practically, we are generating feature at
time t + 1 for the word generated at time t.

to learn these target features, we added a lin-
ear layer to the decoder followed by the softmax
function and used the mean square error criterion
to learn the correct representation.

for this release, we only used case information
as additional feature. it allows us to work with a
lowercased vocabulary and treat the recasing as a
separate problem. we observed that the use of this
simple case feature in source and target does im-
prove the translation quality as illustrated in fig-
ure 1. also, we compared the accuracy of the
induced recasing with other recasing frameworks
(sri disamb, and in-house recasing tool based on
id165 language models) and observed that the
prediction of case by the nn was higher than us-
ing external recaser, which was expected since nn
has access to source in addition to the source sen-
tence context, and target sentence history.

baseline
source case
src&tgt
src&tgt+embed

u
e
l
b

28

26

24

22

20

18

16

6

8

10

12
perplexity

14

16

figure 1: comparison of training progress (per-
plexity/id7) with/without source (src) and tar-
get (tgt) case features, with/without feature em-
bedding (embed) on wmt2013 test corpus for
english-french. score is calculated on lowercase
output. the perplexity increases when the target
features are introduced because of the additional
classi   cation problem. we also notice a notice-
able increase in the score when introducing the
features, in particular the target features. so these
features do not simply help to reduce the vocabu-
lary, but also by themselves help to structure the
nn decoding model.

4.3 named entities (ne)

systran   s rbmt and smt translation engines
utilize number and named entity (ne) module to
recognize, protect, and translate ne entities. simi-
larly, we used the same internal ne module to rec-
ognize numbers and named entities in the source
sentence and temporarily replaced them with their
corresponding placeholders (table 3).

both the source and the target side of the train-
ing dataset need to be processed for ne placehold-
ers. to ensure the correct entity recognition, we
cross-validate the recognized entity across paral-
lel dataset, that is: a valid entity recognition in a
sentence should have the same type of entity in its
parallel pair and the word or phrase covered by the
entities need to be aligned to each other. we used
fast align (dyer et al., 2013) to automatically
align source words to target words.

in our datasets, generally about one-   fth of
the training instances contained one or more ne
placeholders. our training dataset consists of sen-
tences with ne placeholders as well as sentences
without them to be able to handle both instantiated
and recognized entity types.

ne type
number

measurement

money

person (any)

title

first name

initial

last name

middle name

location

organization

product
suf   x

placeholder
ent numeric

ent numex measurement

ent numex money

ent person

ent person title

ent person    rstname
ent person initials
ent person lastname

ent person middlename

ent location

ent organization

ent product
ent suf   x

time expressions

ent timex expression

date

date (day)

date (month)
date (year)

hour

ent date

ent date day
ent date month
ent date year

ent hour

table 3: named entity placeholder types

per source sentence, a list of all entities, along
with their translations in the target language, if
available, are returned by our internal ne recogni-
tion module. the entities in the source sentence is
then replaced with their corresponding ne place-
holders. during id125, we make sure that
an entity placehoder is translated by itself in the
target sentence. when the entire target sentence
is produced along with the attention weights that
provide soft alignments back to the original source
tokens, placeholders in the target sentences are re-
placed with either the original source string or its
translation.

the substitution of the ne placeholders with
their correct values needs language pair-speci   c
considerations.
in figure 4, we show that even
the handling of arabic numbers cannot be straight-
forward as copying the original value in the source
text.

4.4 guided alignments

we re-reimplemented guided alignment strategy
described in chen et al. (2016). guided align-
ment enforces the attention weights to be more
like alignments in the traditional sense (e.g. ibm4
viterbi alignment) where the word alignments ex-
plicitly indicate that source words aligned to a tar-
get word are translation equivalents.

similarly to the previous work, we created
an additional criterion on attention weights, lga,
such that the difference in the attention weights
and the reference alignments is treated as an error
and directly and additionally optimize the output

en     ko

train

train data

entity-replaced

25 billion

ent numeric billion

250n  

ent numeric n  

decode

input data

entity-replaced

translated

naive substition
expected result

1.4 billion

ent numeric billion

-
-
-

ent numeric n  

-
-

1.4n  
14n  

table 4: examples of english and korean num-
ber expressions where naive recognition and sub-
stitution fails. even if the model correctly pro-
duces correct placeholders, simply copying the
original value will result in incorrect translation.
these kind of structural entities need language
pair-speci   c treatment.

of the attention module.

lga(a,   ) =

1
t

  xt xs

(ast       st)2

the    nal id168 for the optimization is then:

ltotal = wga    lga(a,   ) + (1     wga)    ldec(y, x)

chen et al. (2016) also report

where a is the alignment matrix,    attention
weights, s and t indicating the indices in the
source and target sentences, and wga the linear
combination weight for the guided alignment loss.
that decaying
wga,
thereby gradually reducing the in   uence
from guided alignment, over the course of train-
ing found to be helpful on certain datasets. when
guided alignment decay is enabled, wga is gradu-
ally reduced at this rate after each epoch from the
beginning of the training.

without searching for the optimal parameter
values, we simply took following con   gurations
from the literature: mean square error (mse) as
id168, 0.5 as the linear-combination weight
for guided alignment loss and the cross-id178
loss for decoder, and 0.9 for decaying factor for
guided alignment loss.

the

for

again

alignment, we

utilized
fast align tool. we stored alignments in
sparse format8 to save memory usage, and for
each minibatch a dense matrix is created for faster
computation.

applying such a constraint on attention weights
can help locate the original source word more

8compressed column storage (ccs)

u
e
l
b

52

51

50

49

48

47

46

baseline
guided alignment
guided alignment with decay

1

3

5

7

9 11 13 15 17 19
epoch

figure 2: effect of guided alignment. this partic-
ular learning curve is from training an attention-
based model with 4 layers of bidirectional lstms
with 800 dimension on a 5 million french to en-
glish dataset.

accurately, which we hope to bene   ts better ne
placeholder substitutions and especially unknown
word handling.

in figure 2, we see the effects of guided align-
ment and its decay rate on our english-to-french
generic model. unfortunately, this full compara-
tive run was disrupted by a power outage and we
did not have time to relaunch, however we can
still clearly observe that up to the initial 5 epochs,
guided alignment, with or without decay, provides
rather big boosts over the baseline training. af-
ter 5 epochs, the training with decay slow down
compare to the training without, which is rather
intuitive: the guided alignment is indeed in con-
   ict with the attention learning. what would re-
main to be seen, is if at the training the training,
the baseline and the guided alignment with decay
are converging.

4.5 politeness mode

many languages have ways to express politeness
and deference towards people being reffered to in
sentences. in indo-european languages, there are
two pronouns corresponding to the english you; it
is called the t-v distinction between the informal
latin pronoun tu (t) and the polite latin pronoun
vos (v). asian languages, such as japanese and
korean, make an extensive use of honori   cs (re-
spectful words), words that are usually appended
to the ends of names or pronouns to indicate the
relative ages and social positions of the speakers.
expressing politeness can also impact the vocabu-

mode
formal
informal

correct
30
35

incorrect accuracy
68.2%
79.5%

14
9

table 6: accuracy of generating correct polite-
ness mode of an english-to-korean id4 system.
the evaluation was carried out on a set of 50 sen-
tences only; 6 sentences were excluded from eval-
uation because neither the original nor their trans-
lations contained any verbs.

lary of verbs, adjectives, and nouns used, as well
as sentence structures.

following the work of sennrich et al. (2016a),
we implemented a politeness feature in our id4
engine: a special token is added to each source
sentence during training, where the token indicates
the politeness mode observed in the target sen-
tence. having such an ability to specify the po-
liteness mode is very useful especially when trans-
lating from a language where politeness is not ex-
pressed, e.g. english, into where such expressions
are abundant, e.g. korean, because it provides a
way of customizing politeness mode of the trans-
lation output.
table 5 presents our english-to-korean id4
model trained with politeness mode, and it is clear
that the proper verb endings are generated accord-
ing to the user selection. after a preliminary eval-
uation on a small testset from english to korean,
we observed 70 to 80% accuracy of the polite-
ness generation (table 6). we also noticed that
86% of sentences (43 out of 50) have exactly the
same meaning preserved across different polite-
ness modes.

this simple approach, however, comes at a
small price, where sometimes the unknown re-
placement scheme tries to copy the special token
in the target generation. a more appropriate ap-
proach that we plan to switch to in our future train-
ings is to directly feed the politeness mode into the
sentential representation of the decoder.

4.6 customization
id20 is a key feature for our
customers   it generally encompasses terminol-
ogy, domain and style adaptation, but can also be
seen as an extension of translation memory for hu-
man post-editing work   ows.

systran engines integrate multiple tech-
niques for id20, training full new

in-domain engines,
automatically post-editing
an existing translation model using transla-
tion memories, extracting and re-using termi-
nology. with id4, a
new notion of    specialization    comes close to
the concept of incremental
translation as de-
veloped for id151 like
(ortiz-mart    nez et al., 2010).

4.6.1 generic specialization
id20 techniques have successfully
been used in id151. it is
well known that a system optimized on a speci   c
text genre obtains higher accuracy results than a
   generic    system. the adaptation process can be
done before, during or after the training process.
our preliminary experiments follow the latter ap-
proach. we incrementally adapt a neural mt
   generic    system to a speci   c domain by running
additional training epochs over newly available in-
domain data.

adaptation proceeds incrementally when new
in-domain data becomes available, generated by
human translators while post-editing, which is
similar to the computer aided translation frame-
work described in (cettolo et al., 2014).

we experiment on an english-to-french trans-
lation task. the generic model is a subsample of
the corpora made available for the wmt15 trans-
lation task (bojar et al., 2015). source and tar-
get id4 vocabularies are the 60k most frequent
words of source and target training datasets. the
in-domain data is extracted from the european
medical agency (emea) corpus. table 7 shows
some statistics of the corpora used in this experi-
ment.

type
train

test

corpus
generic
emea
emea

# lines # src tok (en) # tgt tok (fr)

1m
4,393
2,787

24m
48k
23k

26m
56k
27k

table 7: data used to train and adapt the generic
model to a speci   c domain. the test corpus also
belongs to the speci   c domain.

our preliminary results show that incremental
adaptation is effective for even limited amounts
of in-domain data (nearly 50k additional words).
constrained to use the original    generic    vocabu-
lary, adaptation of the models can be run in a few
seconds, showing clear quality improvements on
in-domain test sets.

figure 3 compares the accuracy (id7) of

u
e
l
b

36
35
34
33
32
31
30
29

adapt
full

0

1

2

3

4

5

additional epochs

figure 3: adaptation with in-domain data.

full is trained after concatenation
two systems:
of generic and in-domain data; adapt is initially
trained over generic data (showing a id7 score
of 29.01 at epoch 0) and adapted after running sev-
eral training epochs over only the in-domain train-
ing data. both systems share the same    generic   
id4 vocabularies. as it can be seen the adapt
system improves drastically its accuracy after a
single additional training epoch, obtaining a simi-
lar id7 score than the full system (separated by
.91 id7). note also that each additional epoch
using the in-domain training data takes less than
50 seconds to be processed, while training the full
system needs more than 17 hours.

results validate the utility of the adaptation ap-
proach. a human post-editor would take advan-
tage of using new training data as soon as it be-
comes available, without needing to wait for a long
full training process. however, the comparison is
not entirely fair since full training would allow to
include the in-domain vocabulary in the new full
model, what surely would result in an additional
accuracy improvement.

4.6.2 post-editing engine

recent success of pure neural machine transla-
tion has led to the application of this technology
to various related tasks and in particular to the au-
tomatic post-editing (ape). the goal of this task
is to simulate the behavior of a human post-editor,
correcting translation errors made by a mt sys-
tem.

until recently, most of the ape approaches
have been based on phrase-based smt sys-
tems, either monolingual
to hu-
man post-edition) (simard et al., 2007) or source-

(mt target

u
e
l
b

50

40

30

20

10

0

1

id4
npe multi-source
npe
spe
rbmt

3

5

7

9

11

13

epoch

figure 4: accuracy results of rbmt, id4, npe
and npe multi-source.

aware (b  echara et al., 2011). for many years now
systran has been offering a hybrid statisti-
cal post-editing (spe) solution to enhance the
translation provided by its rule-based mt system
(rbmt) (dugast et al., 2007).

the

success

following

of neural post-
editing (npe) in the ape task of wmt   16
(junczys-dowmunt and grundkiewicz, 2016), we
have run a series of experiments applying the
neural approach in order to improve the rbmt
system output. as a    rst experiment, we com-
pared the performance of our english-to-korean
spe system trained on technical (it) domain
data to two npe systems trained on the same
data: monolingual npe and multi-source npe,
where the input language and the mt hypothesis
sequences have been concatenated together into
one input sequence (separated by a special token).
figure 4 illustrates the accuracy (id7) re-
sults of four different systems at different training
epochs. the rbmt system performs poorly, con-
   rming the importance of post-editing. both npe
systems clearly outperform spe. it can also be ob-
served that adding source information even in a
simplest way possible (npe multi-source), with-
out any source-target alignment, considerably im-
proves npe translation results.

the system performing npe multi-source ob-
tains similar accuracy results than pure id4.
what can be seen is that npe multi-source essen-
tially employs the information from the original
sentence to produce translations. however, no-
tice that bene   ts of utilizing multiple inputs from
different sources is clear at earlier epochs while
once the model parameters converge, the differ-

ence in performances of id4 and npe multi-
source models become negligible.

further experiments are currently being con-
ducted aiming at    nding more sophisticated ways
of combining the original source and the mt
translation in the context of npe.

5 performance

as previously outlined, one of the major draw-
backs of id4 engines is the need for cutting-edge
hardware technology to face the enormous compu-
tational requirements at training and runtime.

regarding training, there are two major issues:
the full training time and the required computation
power, i.e. the server investment. for this release,
most of our trainings have been running on single
gtx geforce 1080 gpu (about $2.5k) while in
(wu et al., 2016), authors mention using 96 k80
gpu for a full week for training one single lan-
guage pair (about $250k). on our hardware, full
training on 2x5m sentences (see section 3) took a
bit less than one month.

a reasonnable target is to maintain training time
for any language pair under one week and keep-
ing reasonable investment so that the full research
community can have competitive trainings but also
indirectly so that all of our customers can bene   t
from the training technology. to do so, we need to
better leverage multiple gpus on a single server
which is on-going engineering work. we also need
to continue on exploring how to learn more with
less data. for instance, we are convinced that
injecting terminology as part of the training data
should be competitive with continuing adding full
sentences.

also, shortening training cycle can also be
achieved by better control of the training cycle.
we have shown that multiple features are boosting
the training pace, and if going to bigger network
is clearly improving performance. for instance,
we are using a bidirectional 4 layer id56 in addi-
tion to our regular id56, but in wu et al. (2016),
authors mention using bidirectional id56 only for
the    rst layer. we need to understand more these
phenomena and restrict to the minimum to reduce
the model size.

finally, work on specialization described in sec-
tions 4.6.1 and 5.2 are promising for long term
maintenance: we could reach a point where we do
not need to retrain from scratch but continuously
improve existing model and use teacher models to

model
baseline
40% pruned
50% pruned
60% pruned
70% pruned
60% pruned and retrained

id7
49.24
48.56
47.96
45.90
38.38
49.26

table 8: id7 scores of pruned models on an
internal test set.

boost initial trainings.

regarding runtime performance, we have been
exploring the following areas and are reaching to-
day throughputs compatible with production re-
quirement not only using gpus but also using
cpu and we report our different strategies in the
following sub-sections.

5.1 pruning

pruning the parameters of a neural network is a
common technique to reduce its memory footprint.
this approach has been proven ef   cient for the
id4 tasks in see et al. (2016). inspired by this
work, we introduced similar pruning techniques in
id195-attn. we reproduced that models pa-
rameters can be pruned up to 60% without any per-
formance loss after retraining as shown in table 8.
with a large pruning factor, neural network   s
weights can also be represented with sparse matri-
ces. this implementation can lead to lower com-
putation time but more importantly to a smaller
memory footprint that allows us to target more
environment. figures 5 and 6 show experiments
involving sparse matrices using eigen9. for ex-
ample, when using the float precision, a mul-
tiplication with a sparse matrix already begins to
take less memory when 35% of its parameters are
pruned.

related to this work, we present in section 5.4
our alternative eigen-based decoder that allows us
to support sparse matrices.

5.2 distillation

despite that surprisingly accurate, id4 systems
need for deep networks in order to perform well.
typically, a 4-layer lstm with 1000 hidden units
per layer (4 x 1000) are used to obtain state-of-
the-art results. such models require cutting-edge

9http://eigen.tuxfamily.org

figure 5: processing time to perform a 1000   
1000 id127 on a single thread.

figure 6: memory needed to perform a 1000   
1000 id127.

hardware for training in reasonable time while in-
ference becomes also challenging on standard se-
tups, or on small devices such as mobile phones.
though, compressing deep models into smaller
networks has been an active area of research.

following the work in (kim and rush, 2016),
we experimented sequence-level knowledge dis-
tillation in the context of an english-to-french
id4 task. knowledge distillation relies on train-
ing a smaller student network to perform better by
learning the relevant parts of a larger teacher net-
work. hence,    wasting   tm parameters on trying to
model the entire space of translations. sequence-
level is the knowledge distillation variant where
the student model mimics the teacher   s actions at
the sequence-level.

the experiment is summarized in 3 steps:

    train a teacher model on a source/reference

training set,

    use the teacher model to produce translations

for the source training set,

    train

a

student model

on

the

new

source/translation training set.

for our initial experiments, we produced 35-
best translations for each of the sentences of the
source training set, and used a normalized id165
matching score computed at the sentence level, to
select the closest translation to each reference sen-
tence. the original training source sentences and
their translated hypotheses where used as training
data to learn a 2 x 300 lstm network.

results showed slightly higher accuracy results
for a 70% reduction of the number of parame-
ters and a 30% increase on decoding speed.
in
a second experiment, we learned a student model
with the same structure than the teacher model.
surprisingly, the student clearly outperformed the
teacher model by nearly 1.5 id7.

we hypothesize that the translation performed
over the target side of the training set produces a
sort of language id172 which is by con-
struction very heterogeneous. such id172
eases the translation task, being learned by not so
deep networks with similar accuracy levels.

500

400

300

200

100

s
/
s
n
e
k
o
t

0

0

20

40
60
batch size

80

100

figure 7: tokens throughput when decoding from
a student model (see section 5.2) with a beam of
size 2 and using float precision. the experi-
ment was run using a standard torch + openblas
install and 4 threads on a desktop intel i7 cpu.

k and a batch of size b, we forward k    b se-
quences into the model. then, the decoder output
is split across each batch and the beam path for
each sentence is updated sequentially.

as sentences within a batch can have large vari-
ations in size, extra care is needed to mask accord-
ingly the output of the encoder and the attention
softmax over the source sequences.

figure 7 shows the speedup obtained using

batch decoding in a typical setup.

5.4 c++ decoder
while torch is a powerful and easy to use frame-
work, we chose to develop an alternative c++
implementation for the decoding on cpu. it in-
creases our control over the decoding process and
open the path to further memory and speed im-
provements while making deployment easier.

our implementation is graph-based and use
eigen for ef   cient matrix computation. it can load
and infer from torch models.

for this release, experiments show that

the
decoding speed is on par or faster
than the
torch-based implementation especially in a multi-
threaded context. figure 8 shows the better use of
parallelization of the eigen-based implementation.

6 evaluation

5.3 batch translation

to increase translation speed of large texts, we
support batch translation that works in addition to
the id125. it means that for a beam of size

evaluation of machine translation has always been
a challenge and subject to many papers and ded-
icated workshops (bojar et al., 2016). while au-
tomatic metrics are now used as standard in the

100

80

60

40

20

s
/
s
n
e
k
o
t

0

1

2

4
threads

eigen-based
torch-based

8

figure 8: tokens throughput with a batch of size
1 in the same condition as figure 7.

research world and have shown good correlation
with human evaluation, ad-hoc human evaluation
or productivity analysis metrics are rather used in
the industry (blain et al., 2011).

as a translation solution company, even if au-
tomatic metrics are used through all the training
process (and we give scores in the section 6.1),
we care about human evaluation of the results.
wu et al. (2016) mention human evaluation but si-
multaneously cast a doubt on the referenced hu-
man to translate or evaluate. in this context, the
claim    almost indistinguishable with human trans-
lation    is at the same time strong but also very
vague. on our side, we have observed during
all our experiments and preparation of specialized
models, unprecedented level of quality, and con-
texts where we could claim    super human    trans-
lation quality.

however, we need to be very carefully de   ning
the tasks, the human that are being compared to,
and the nature of the evaluation. for evaluating
technical translation, the nature of the evaluation
is somewhat easy and really depending on the user
expectation: is the meaning properly conveyed, or
is the sentence faster to post-edit than to translate.
also, to avoid doubts about integrity or compe-
tency of the evaluation we sub-contracted the task
to crosslang, a company specialized in machine
translation evaluation. the test protocol was de-
   ned collaboratively and for this    rst release, we
decided to perform ranking of different systems,
and we present in the section 6.2 the results ob-
tained on two very different language pairs: en-
glish to/from french, and english to korean.

finally, in the section 6.3, we also present some

qualitative evaluation results showing speci   cities
of the id4.

6.1 automatic scoring and system

comparison

figure 9 plots automatic accuracy results, id7,
and perplexities for all language pairs.
it is re-
markable the high correlation between perplexity
and id7 scores, showing that language pairs
with lower perplexity yield higher id7 scores.
note also that different amounts of training data
were used for each system (see table 2). id7
scores were calculated over an internal test set.

from the beginning of this report we have used
   internal    validation and test sets, what makes it
dif   cult to compare the performance of our sys-
tems to other research engines. however, we must
keep in mind that our goal is to account for im-
provements in our production systems. we focus
on human evaluations rather than on any automatic
evaluation score.

6.2 human ranking evaluation
to evaluate translation outputs and compare with
human translation, we have de   ned the following
protocol.

1. for each language pair, 100 sentences    in do-

main    (*) are collected,

2. these sentences are sent to human transla-
tion (**), and translated with candidate model
and using available online translation ser-
vices (***).

3. without noti   cation of the mix of human and
machine translation, a team of 3 professional
translators or linguists    uent in both source
and target languages is then asked to rank 3
random outputs for each sentence based on
their preference as translation. preference in-
cludes accuracy as a priority, but also    uency
of the generated translation. they have the
choice to give them 3 different ranks, or can
also decide to give 2 or the 3 of them the same
rank, if they cannot decide.

(*) for generic domain, sentences from recent
news article were selected online, for technical
(it) sentences part of translation memory de   ned
in section 4.6 were kept apart from the training.
(**) for human translation, we did use translation
agency (human-pro), and online collaborative
translation platform (human-casual).

55

enfr

fren

u
e
l
b

50

45

40

35

30

25

nlfr

esfr
itfr

bren

zhen

fres

aren
enbr
enes esen
brfr

enar

frit
enit

ennl

defr

iten
nlen

deen

arfr

frbr

frar

faen

ende

frde

jako

frzh

koja

3

4
perplexity

5

6

figure 9: perplexity and id7 scores obtained by id4 engines for all language pairs. perplexity and
id7 scores were calculated respectively on validation and test sets. note that english-to-korean and
japanese-to-english systems are not shown in this plot achieving respectively (18.94, 16.50) and (8.82,
19.99) scores for perplexity and id7.

(***) we used naver translator10 (naver), google
translate11
and bing translator12
(bing).

(google)

for this    rst release, we experimented on the
evaluation protocol for 2 different extremely dif-
ferent categories of language pairs. on one hand,
english   french which is probably the most stud-
ied language pairs for mt and for which resources
are very large:
(wu et al., 2016) mention about
36m sentence pairs used in their id4 training
and the equivalent pbmt is completed by a web-
scale target side language models13. also, as
english is a low in   ected language, the current
phrase-based technology for target language en-
glish is more competitive due to the relative sim-
plicity of the generation and weight of gigantic
language models.

on the other hand, english   korean is one of
the toughest language pair due to the far distance
between english and korean language, the small
availability of training corpus, and the rich agglu-
tinative morphology of korean. for a real compar-
ison, we ran evaluation against naver translation
service from english into korean, where naver is
the main south korean search engine.

tables 9 and 10 describe the different evalua-

tions and their results.

several interesting outcomes:

    our vanilla english 7    french model outper-
forms existing online engines and our best of
breed technology.

    for french 7    english, if the model slightly
outperforms (human-casual) and our best of
breed technology,
it stays behind google
translate, and more signi   cantly behind mi-
crosoft translator.

    the generic english 7    korean model shows
closer results with human translation and out-
performs clearly existing online engines.

    the    in-domain    specialized model surpris-
ingly outperforms the reference human trans-
lation.

we are aware that far more evaluations are nec-
essary and we will be launching a larger evalu-

10http://translate.naver.com
11http://translate.google.com
12http://translator.bing
13in 2007, google already mentions using 2 trillion
words in their language models for machine translation
(brants et al., 2007).

ation plan for our next release.
informally, we
do observe that the biggest performance jump
are observed on complicated language pairs, like
english-korean or japanese-english showing that
id4 engines are better able to handle major
structure differences, but also on the languages
with lower resources like farsi-english demon-
strating that id4 is able to learn better with less,
and we will explore this even more.

finally, we are also launching in parallel, a real-
life beta-testing program with our customers so
that we can also obtain formal feedback from their
use-case and related to specialized models.

6.3 qualitative evaluation

in the table 11, we report the result of error analy-
sis for id4, smt and rbmt for english-french
language pair. this evaluation con   rms the trans-
lation ranking performed in the previous but also
exhibits some interesting facts:

    the most salient error comes from missing
words or parts of sentence. it is interesting to
see though that half of these    omissions    are
considered okay by the reviewers and were
most of the time not considered as errors -
it indeed shows the ability of the system not
only to translate but to summarize and get to
the point as we would expect from human
translation. of course, we need to    x the
cases, where the    omissions    are not okay.

    another    nding is that the engine is badly
managing quotes, and we will make sure to
speci   cally teach that in our next release.
other low-hanging fruit are the case genera-
tion, which seems sometimes to get off-track,
and the handling of named entity that we
have already introduced in the system but not
connected for the release.

    on the positive side, we observe that id4
is drastically improving    uency,
reduces
slightly meaning selection errors, and handle
better morphology although it does not have
yet any speci   c access to morphology (like
sub-id27). regarding meaning
selection errors, we will focussing on teach-
ing more expressions to the system which is
still a major structural weakness compared to
pbmt engines.

7 practical issues

translation results from an id4 system, at    rst
glance, is incredibly    uent that your are diverted
from its downsides. over the course of the training
and during our internal evaluation, we found out
multiple practical issues worth sharing:

1. translating very long sentences

2. translating user input such as a short word or

the title of a news article

3. cleaning the corpus

4. alignment

id4 is greatly impacted by the train data, on
which id4 learns how to generate accurate and
   uent translations holistically. because the max-
imal length of a training instance was limited to
a certain length during the training of our mod-
els, id4 models are puzzled by sentences that
exceed this length, not having encountered such a
training data. hard splitting of longer sentences
has some side-effects since the model consider
both parts as full sentence. as a consequence,
whatever is the limit we set for sentence length,
we do also need to teach the neural network how
to handle longer sentences. for that, we are ex-
ploring several options including using separate
model based on source/target alignment to    nd op-
timal breaking point, and introduce special <to
be continued> and <continuing> tokens.
likewise, very short phrases and incomplete or
fragmental sentences were not included in our
training data, and consequently id4 systems fail
to correctly translate such input texts (e.g. figure
10). here also, to enable this, we do simply need
to teach the model to handle such input by inject-
ing additional synthetic data.

also, our training corpus includes a number of
resources that are known to contain many noisy
data. while id4 seems more robust than other
technologies for handling noise, we can still per-
ceive noise effect in translation - in particular for
recurring noise. an example is in english-to-
korean, where we see the model trying to sys-
tematically convert amount currency in addition to
the translation. as demonstrated in section 5.2,
preparing the right kind of input to id4 seems to
result in more ef   cient and accurate systems, and
such a procedure should also be directly applied to
the training data more aggressively.

finally,let us note that source/target alignment
is a must for our users, but this information is
missing from id4 output due to soft alignment.
to hide this issue from the end-users, multi-
ple alignment heuristic are showing tradictional
target-source alignment.

8 further work

in this section we outline further experiments cur-
rently being conducted. first we extend id4
decoding with the ability to make use of multi-
ple models. both external models, particularly
an id165 language model, as well as decoding
with multiple networks (ensemble). we also work
on using external id27s, and on mod-
elling unknown words within the network.

8.1 extending decoding
8.1.1 additional lm
as proposed by (g  ulc  ehre et al., 2015), we con-
duct experiments to integrate an id165 language
model estimated over a large dataset on our neural
mt system. we followed a shallow fusion integra-
tion, similar to how language models are used in a
standard phrase-based mt decoder.

in the context of id125 decoding in id4,
at each time step t, candidate words x are hy-
pothesized and assigned a score according to the
neural network, pn m t (x). sorted according to
their respective scores, the k-best candidates, are
reranked using the score assigned by the language
model, plm (x). the resulting id203 of each
candidate is obtained by the weighted sum of
each log-id203 log p(x) = log plm (x) +
   log pn m t (x). where    is a hyper-parameter
that needs to be tuned.

this technique is specially useful for handling
out-of-vocabulary words (oov). deep networks
are technically constrained to work with limited
vocabulary sets (in our experiments we use target
vocabularies of 60k words), hence suffering from
important oov problems.
in contrast, id165
language models can be learned for very large vo-
cabularies.

initial results show the suitability of the shal-
low integration technique to select the appropriate
oov candidate out of a dictionary list (external re-
source). the id203 obtained from a language
model is the unique modeling alternative for those
word candidates for which the neural network pro-
duces no score.

8.1.2 ensemble decoding

8.3 unknown word handling

ensemble decoding has been veri   ed as a prac-
tical
technique to further improve the perfor-
mance compared to a single encoder-decoder
model
(sennrich et al., 2016b; wu et al., 2016;
zhou et al., 2016). the improvement comes from
the diversity of prediction from different neural
network models, which are learned by random ini-
tialization seeds and shuf   ing of examples dur-
ing training, or different optimization methods
towards the development set(cho et al., 2015).
as a consequence, 3-8 isolated models will
be trained and ensembled together, consider-
ing the cost of memory and training speed.
also,
(junczys-dowmunt et al., 2016) provides
some methods to accelerate the training by choos-
ing different checkpoints as the    nal models.

we implement ensemble decoding by averaging
the output probabilities for each estimation of tar-
get word x with the formula:

x = 1
pens

m=1 pm

x

m pm

wherein, pm
and m is the number of neural models.

x represents probabilities of each x,

8.2 extending id27s

although id4 technology has recently accom-
plished a major breakthrough in machine trans-
lation    eld, it still remains constrained due to the
limited vocabulary size and to the use of bilin-
gual training data. in order to reduce the negative
impact of both phenomena, experiments are cur-
rently being held on using external word embed-
ding weights.

those external id27s are not
learned by the id4 network from bilingual data
only, but by an external model (e.g. id97
(mikolov et al., 2013)). they can therefore be es-
timated from larger monolingual corpora, incorpo-
rating data from different domains.

another advantage lies in the fact that, since ex-
ternal id27 weights are not modi   ed
during id4 training, it is possible to use a dif-
ferent vocabulary for this    xed part of the input
during the application or re-training of the model
(provided that the weights for the words in new vo-
cabulary come from the same embedding space as
the original ones). this may allow a more ef   cient
adaptation to the data coming from a different do-
main with a different vocabulary.

when an unknown word is generated in the target
output sentence, a general encoder-decoder with
attentional mechanism utilizes heuristics based on
attention weights such that the source word with
the most attention is either directly copied as-is or
looked up in a dictionary.

in

the

recent

literature

(gu et al., 2016;
gulcehre et al., 2016), researchers have attempted
to directly model the unknown word handling
within the attention and decoder networks.
having the model learn to take control of both
decoding and unknown word handling will result
in the most optimized way to address the single
unknown word replacement problem, and we
are implementing and evaluating the previous
approaches within our framework.

9 conclusion

neural mt has progressed at a very impressive
rate, and it has proven itself to be competitive
against online systems trained on train data whose
size is several orders of magnitude larger. there
is no doubt that neural mt is de   nitely a tech-
nology that will continue to have a great impact
on academia and industry. however, at its current
status, it is not without limitations; on language
pairs that have abundant amount of monolingual
and bilingual train data, phrase-based mt still per-
form better than neural mt, because neural mt
is still limited on the vocabulary size and de   cient
utilization of monolingual data.

neural mt is not an one-size-   ts-all technology
such that one general con   guration of the model
universally works on any language pairs. for ex-
ample, subword id121 such as bpe pro-
vides an easy way out of the limited vocabulary
problem, but we have discovered that it is not al-
ways the best choice for all language pairs. atten-
tion mechanism is still not at the satisfactory status
and it needs to be more accurate for better con-
trolling the translation output and for better user
interactions.

for upcoming releases, we have begun to mak-
ing even more experiments with injection of var-
ious linguistic knowledges, at which systran
possesses the foremost expertise. we will also
apply our engineering know-hows to conquer the
practical issues of id4 one by one.

acknowledgments

we would like to thank yoon kim, prof. alexan-
der rush and the rest of members of the harvard
nlp group for their support with the open-source
code, their pro-active advices and their valuable
insights on the extensions.

we are also thankful to crosslang and homin
kwon for their thorough and meaningful de   ni-
tion of the evaluation protocol, and their evalua-
tion team as well as inah hong, weimin jiang and
sunhyung lee for their work.

references
[bahdanau et al.2014] dzmitry bahdanau, kyunghyun
cho, and yoshua bengio. 2014. neural machine
translation by jointly learning to align and trans-
late. corr, abs/1409.0473. demoed at nips 2014:
http://lisa.iro.umontreal.ca/mt-demo/.

[b  echara et al.2011] hanna b  echara, yanjun ma, and
josef van genabith. 2011. statistical post-editing
for a statistical mt system.
in mt summit, vol-
ume 13, pages 308   315.

[blain et al.2011] fr  ed  eric blain, jean senellart, hol-
ger schwenk, mirko plitt, and johann roturier.
2011. qualitative analysis of post-editing for high
quality machine translation. mt summit xiii: the
thirteenth machine translation summit [organized
by the] asia-paci   c association for machine trans-
lation (aamt), pages 164   171.

[bojar et al.2015] ond  rej bojar, rajen chatterjee,
christian federmann, barry haddow, matthias
huck, chris hokamp, philipp koehn, varvara lo-
gacheva, christof monz, matteo negri, matt post,
carolina scarton, lucia specia, and marco turchi.
2015. findings of the 2015 workshop on statistical
machine translation.
in proceedings of the tenth
workshop on id151, pages
1   46, lisbon, portugal, september.

[bojar et al.2016] ond  rej bojar, yvette graham, amir
kamran, and milo  s stanojevi  c. 2016. results of the
wmt16 metrics shared task.
in proceedings of the
first conference on machine translation, berlin,
germany, august.

[brants et al.2007] thorsten brants, ashok c popat,
peng xu, franz j och, and jeffrey dean. 2007.
large language models in machine translation.
in
in proceedings of the joint conference on empirical
methods in natural language processing and com-
putational natural language learning. citeseer.

[cettolo et al.2014] mauro cettolo, nicola bertoldi,
marcello federico, holger schwenk, loic barrault,
and chrstophe servan. 2014. translation project
adaptation for mt-enhanced computer assisted trans-
lation. machine translation, 28(2):127   150, octo-
ber.

[chen and eisele2012] yu chen and andreas eisele.
2012. multiun v2: un documents with multilingual
alignments. in lrec, pages 2500   2504.

[chen et al.2016] wenhu chen, evgeny matusov,
shahram khadivi, and jan-thorsten peter. 2016.
guided alignment training for topic-aware neural
machine translation. corr, abs/1607.01628v1.

[cho et al.2015] s  ebastien

jean kyunghyun cho,
roland memisevic, and yoshua bengio. 2015. on
using very large target vocabulary for neural ma-
chine translation. arxiv preprint arxiv:1412.2007.

[chung et al.2016] junyoung chung, kyunghyun cho,
and yoshua bengio. 2016. a character-level de-
coder without explicit segmentation for neural ma-
chine translation. corr, abs/1603.06147.

[dugast et al.2007] lo    c dugast, jean senellart, and
philipp koehn. 2007. statistical post-editing on
systran   s rule-based translation system.
in
proceedings of the second workshop on statisti-
cal machine translation, pages 220   223, prague,
czech republic, june. association for computa-
tional linguistics.

[dyer et al.2013] chris dyer, victor chahuneau, and
noah a. smith. 2013. a simple, fast, and effec-
tive reparameterization of ibm model 2. in proceed-
ings of the 2013 conference of the north ameri-
can chapter of the association for computational
linguistics: human language technologies, pages
644   648, atlanta, georgia, june.

[gu et al.2016] jiatao gu, zhengdong lu, hang li, and
victor o.k. li. 2016. incorporating copying mech-
anism in sequence-to-sequence learning.
in pro-
ceedings of the 54th annual meeting of the associa-
tion for computational linguistics (volume 1: long
papers), pages 1631   1640, berlin, germany, au-
gust. association for computational linguistics.

[g  ulc  ehre et al.2015] c   aglar g  ulc  ehre, orhan firat,
kelvin xu, kyunghyun cho, lo    c barrault, huei-
chi lin, fethi bougares, and holger schwenk
andyoshua bengio.
2015. on using monolin-
gual corpora in id4. corr,
abs/1503.03535.

[gulcehre et al.2016] caglar gulcehre, sungjin ahn,
ramesh nallapati, bowen zhou, and yoshua ben-
gio. 2016. pointing the unknown words. in pro-
ceedings of the 54th annual meeting of the associa-
tion for computational linguistics (volume 1: long
papers), pages 140   149, berlin, germany, august.
association for computational linguistics.

[junczys-dowmunt and grundkiewicz2016] marcin

and roman grundkiewicz.
junczys-dowmunt
2016. log-linear combinations of monolingual and
bilingual id4 models for
automatic post-editing. corr, abs/1605.04800.

[junczys-dowmunt et al.2016] m. junczys-dowmunt,
t. dwojak, and h. hoang.
is neural
machine translation ready for deployment? a
case study on 30 translation directions. corr,
abs/1610.01108, october.

2016.

in proceedings of the 15th annual conference of
the north american chapter of the association for
computational linguistics: human language tech-
nologies, pages 35   40, san diego, california, usa,
june. association for computational linguistics.

[sennrich et al.2016b] rico sennrich, barry haddow,
and alexandra birch. 2016b. neural machine trans-
lation of rare words with subword units. in proceed-
ings of the 54th annual meeting of the association
for computational linguistics (volume 1: long pa-
pers), pages 1715   1725, berlin, germany, august.
association for computational linguistics.

[simard et al.2007] michel simard, cyril goutte, and
pierre isabelle. 2007. statistical phrase-based post-
editing. in in proceedings of naacl.

[steinberger et al.2006] ralf

steinberger,

bruno
pouliquen, anna widiger, camelia ignat, tomaz
erjavec, dan tu   s, and d  aniel varga. 2006. the
jrc-acquis: a multilingual aligned parallel corpus
with 20+ languages. arxiv preprint cs/0609058.

[tiedemann2012] j  org tiedemann. 2012. parallel data,
tools and interfaces in opus. in lrec, pages 2214   
2218.

[wu et al.2016] yonghui wu, mike schuster, zhifeng
chen, quoc v. le, mohammad norouzi, wolf-
gang macherey, maxim krikun, yuan cao, qin
gao, klaus macherey, jeff klingner, apurva shah,
melvin johnson, xiaobing liu,   ukasz kaiser,
stephan gouws, yoshikiyo kato, taku kudo,
hideto kazawa, keith stevens, george kurian, nis-
hant patil, wei wang, cliff young, jason smith, ja-
son riesa, alex rudnick, oriol vinyals, greg cor-
rado, macduff hughes, and jeffrey dean. 2016.
google   s id4 system: bridg-
ing the gap between human and machine translation.
technical report, google.

[zhou et al.2016] jie zhou, ying cao, xuguang wang,
peng li, and wei xu. 2016. deep recurrent mod-
els with fast-forward connections for neural ma-
chine translation. transactions of the association
for computational linguistics, 4:371   383.

[kim and rush2016] yoon kim and alexander m.
rush. 2016. sequence-level knowledge distillation.
corr, abs/1606.07947.

[koehn et al.2007] philipp koehn, hieu hoang,
alexandra birch, chris callison-burch, marcello
federico, nicola bertoldi, brooke cowan, wade
shen, christine moran, richard zens, chris dyer,
ondrej bojar, alexandra constantin, and evan
herbst. 2007. moses: open source toolkit for
id151.
in proceedings
of the 45th annual meeting of the association for
computational linguistics companion volume
proceedings of the demo and poster sessions.

[koehn2005] philipp koehn. 2005. europarl: a paral-
lel corpus for id151. in mt
summit, volume 5, pages 79   86.

[luong and manning2016] minh-thang luong and
christopher d. manning. 2016. achieving open
vocabulary id4 with hybrid
word-character models. corr, abs/1604.00788.

[luong et al.2015] thang luong, hieu pham, and
christopher d. manning.
effective ap-
proaches to attention-based neural machine trans-
lation.
in proceedings of the 2015 conference on
empirical methods in natural language process-
ing, pages 1412   1421, lisbon, portugal, september.
association for computational linguistics.

2015.

[mikolov et al.2013] tomas mikolov, kai chen, greg
corrado, and jeffrey dean. 2013. ef   cient estima-
tion of word representations in vector space. corr,
abs/1301.3781.

[ortiz-mart    nez et al.2010] daniel ortiz-mart    nez, is-
mael garc    a-varea, and francisco casacuberta.
2010. online learning for interactive statistical ma-
chine translation.
in human language technolo-
gies: the 2010 annual conference of the north
american chapter of the association for computa-
tional linguistics, pages 546   554. association for
computational linguistics.

[see et al.2016] abigail see, minh-thang luong, and
christopher d manning.
compression
of id4 models via pruning.
in the proceedings of the 20th signll confer-
ence on computational natural language learning
(conll2016), berlin, germany, august.

2016.

[sennrich and haddow2016] rico sennrich and barry
haddow. 2016. linguistic input features improve
id4. corr, abs/1606.02892.

[sennrich et al.2016a] rico sennrich, barry haddow,
and alexandra birch. 2016a. controlling politeness
in id4 via side constraints.

language

pair

en   br
en   it
en   ar
en   es
en   de
en   nl
en   ko
en   fr
fr   br
fr   it
fr   ar
fr   es
fr   de
fr   zh
ja   ko
nl   fr
fa   en
ja   en
zh   en

#sents

#tokens

vocab

training

#tokens

testing

vocab

target

source

source
2.7m 74.0m 76.6m 150k
3.6m 98.3m 100m 225k
5.0m 126m 155m 295k
3.5m 98.8m 105m 375k
2.6m 72.0m 69.1m 150k
2.1m 57.3m 57.4m 145k
3.5m 57.5m 46.4m 98.9k
9.3m 220m 250m 558k
1.6m 53.1m 47.9m 112k
3.1m 108m 96.5m 202k
5.0m 152m 152m 290k
2.8m 99.0m 91.7m 170k
2.4m 73.4m 62.3m 172k
3.0m 98.5m 76.3m 199k
1.4m 14.0m 13.9m 61.9k
3.0m 74.8m 84.7m 446k
795k
21.7m 20.2m 166k
1.3m 28.0m 22.0m
24k
5.8m 145m 154m 246k

target
213k
312k
357k
487k
279k
325k
58.4k
633k
135k
249k
320k
212k
253k
168k
55.6k
260k
147k
87k
225k

#sents

2k
2k
2k
2k
2k
2k
2k
2k
2k
2k
2k
2k
2k
2k
2k
2k
2k
2k
2k

source
51k
52k
51k
53k
53k
52k
30k
48k
62k
69k
60k
69k
57k
67k
19k
49k
54k
41k
48k

target
53k
53k
62k
56k
51k
53k
26k
55k
56k
61k
60k
64k
48k
51k
19k
55k
51k
32k
51k

source
6.7k
7.3k
7.5k
8.5k
7.0k
6.7k
7.1k
8.2k
7.4k
8.2k
8.5k
8.0k
7.5k
8.0k
9.3k
7.9k
7.7k
6.2k
5.5k

target
8.1k
8.8k
8.7k
9.8k
9.6k
7.9k
11k
8.6k
8.1k
8.8k
8.6k
8.6k
9.0k
5.9k
8.5k
7.5k
8.7k
7.3k
6.9k

oov

source

target

47
66
43
110
30
50
0
77
55
47
42
37
59
51
0
150
197
3
34

64
85
47
119
77
141

-
63
59
57
61
55
104

-
0
-
-
-
-

table 2: corpora statistics for each language pair (iso 639-1 2-letter code, expect for portuguese brazil-
ian noted as    br   ). all language pairs are bidirectional except nlfr, frzh, jaen, faen, enko, zhen. columns
2-6 indicate the number of sentences, running words and vocabularies referred to training datasets while
columns 7-11 indicate the number of sentences, running words and vocabularies referred to test datasets.
columns 12 and 13 indicate respectively the vocabulary of oov of the source and target test sets. (m
stand for milions, k for thousands). since jako and enko are trained using bpe id121 (see section
4.1), there is no oov.

en:

ko with formal mode:

a senior u.s. treasury of   cial is urging china to move faster on making its currency more    exible.

       t n    t  t      n     t                nt  tn    n  n       tn    n          n        n  n                               n                      ntn  n       n   n                          .

ko with informal mode:

       t n    t  t      n     t                nt  tn    n  n       t     n       n  n                               n                      ntn  n       n   n                   n  n  nt.

table 5: a translation examples from an en-ko system where the choice of different politeness modes
affects the output.

domain (*)

language pair
english 7    french generic
french 7    english generic
english 7    korean news
english 7    korean technical (it)

human translation (**) online translation (***)
human-pro, human-casual bing, google
human-pro, human-casual bing, google
human-pro
human-pro

naver, google, bing
n/a

table 9: performed evaluations

human-pro human-casual bing google naver systran v8

english 7    french
french 7    english
english 7    korean
english 7    korean (it)

-64.2
-56.8
-15.4
+30.3

-18.5
+5.5

+48.7
-23.1
+35.5

+10.4
-8.4
+33.7

+31.4

+17.3

+5

+13.2

table 10: this table shows relative preference of systran id4 compared to other outputs calcu-
lated this way: for each triplet where output a and b were compared, we note prefa>b the number of
times where a was strictly preferred to b, and ea the total number of triplet including output a. for
each output, e, the number in the table is compar(sid4, e) = (prefsid4>e     prefe>sid4)/esid4.
compar(sid4, e) is a percent value in the range [   1; 1]

figure 10: effect of translation of a single word through a model not trained for that.

major - contextual meaning

14

word ordering and fluency

category
entity

major
format

morphology

minor - local

minor - sentence level
major

meaning selection

minor

major - prep choice

major - expression
major - not translated

minor

major

missing or duplicated

missing minor

missing major

duplicated major

misc. (minor)

quotes, punctuations

case

total

major
minor
minor & major

id4 rb smt example

7
3

3

3
3

9

4

3
5

2

3

7

6

2

2

6

5
1

2

3
4

17

9

7
1

39

28

16

3

1

2

0

0

0
1

3

5
6

7

10

1
4

14

15

15

1

3

1

0

2

galaxy note 7 7    note 7 de galaxy vs. galaxy note 7
(number localization): $2.66 7    2.66 $vs. 2,66 $

(tense choice): accused 7    accusait vs. a accus  e
the president [...], she emphasized

7    la pr  esident [...], elle a soulign  e vs. la [...], elle

he scanned 7    il scann  e vs. il scannait

game 7    jeu vs. match
[... facing war crimes charges] over [its bombardment of ...]

7    contre vs. pour
[two] counts of murder

7    chefs de meutre vs. chefs d   accusation de meutre

he scanned 7    il scanned vs. il a scann  e
33 senior republicans

7    33 r  epublicains sup  erieurs vs. 33 t  enors r  epublicains

(determiner):[without] a [speci   c destination in mind]
7    sans une destination [...] vs. sans destination [...]

(word ordering):in the sept. 26 deaths

7    dans les morts septembre de 26

vs. dans les morts du 26 septembre

a week after the hurricane struck

7    une semaine apr`es l   ouragan

vs. une semaine apr`es que l   ouragan ait frapp  e

as a working oncologist, giordano knew [...]

7    giordano savait

vs. en tant qu   oncologue en fonction, giordano savait

for the republican presidential nominee

7    au candidat r  epublicain r  epublicain

vs. au candidat r  epublicain

(misplaced quotes)
[...] will be affected by brexit

7    [...] sera touch  ee par le brexit

vs. [...] sera touch  ee par le brexit

47
36
83

84
55
139

54
35
89

table 11: human error analysis done for 50 sentences of the corpus de   ned in the section 6.2 for
english-french on id4, smt (google) and rbmt outputs. error categories are: - issue with entity
handling (entity), issue with morphology either local or re   ecting sentence level missing agreements,
issue with meaning selection splitted into different sub-categories, - issue with word ordering or flu-
ency (wrong or missing determiner for instance), - missing or duplicated words. errors are either minor
when reader could still understand the sentence without access to the source, otherwise is considered as
major. erroneous words are counted in only one category even if several problems add-up - for instance
ordering and meaning selection.

a remarkable results

in this section, we highlight a serie of    remarkable    translations (positively remarkable and also few
negatively outstanding) that we found out during evaluation for a variety of languages.

language

pair
en7   fr

fr7   en

en7   ko

en7   it

it7   fr

it7   fr

es7   en

en7   de

source
the report also said that although
three-quarters of chinese say their
country is playing a larger role in
the world than it did a decade ago,
most want their government to fo-
cus on domestic issues rather than
helping other nations.

fen  etre `a rue se fermait mal, tr`es
tr`es bruyant et mal dormi. petit
d  ejeuner banal.
forty republican senators and con-
gressmen have revoked their sup-
port for the republican presidential
nominee     with nearly 30 of them
calling on him to quit the race alto-
gether in recent days.

all this leads to feelings of inse-
curity and con   ict thus reinforcing
negative spirals.

mi sembra che ci`o non abbia nulla
a che vedere con una penuria ali-
mentare: si tratta di una vera cares-
tia, di una vera e propria catas-
trofe che non `e naturale ma strut-
turale, provocata da un regime
folle, criminale, psicopatico, peg-
giore dei peggiori regimi esistiti
nell   unione sovietica e nella ro-
mania di ceaucescu.

ebbene io sono in totale disaccordo:
lo stato non deve passare in sec-
ondo piano; deve invece intervenire
di pi`u e garantire una maggiore vig-
ilanza in ambito economico.

  es posible un bangladesh m  as am-
able con los ni  nos?

on the other hand, businesses are
constantly complaining about
the
shortage of specialist staff, using
this supposed shortage as an excuse
to use cheap labour.

zh7   en

            ,                              
         .

id4

le rapport dit aussi que, bien que
les trois quarts des chinois disent
que leur pays joue un r  ole plus im-
portant dans le monde qu   il ne l   a
fait il y a dix ans, la plupart veulent
que leur gouvernement se concentre
sur les questions nationales plut  ot
que d   aider d   autres nations.

the room was very clean and the
room was very clean.

40     n          t               ntn   nt                
ntn   nt    n          t                       t    
      n                     n    n   n        n    t    
     ,      n    30     n                  n    nt    
    n       n                nt                   t        
     n        ntn                     .

tutto ci`o porta a sentimenti di insi-
curezza e di con   itto, rafforzando
cos`   la spirale negativa.

il me semble que cela n   a rien
`a voir avec une p  enurie alimen-
taire:
il s   agit d   une v  eritable
famine, d   une v  eritable catastrophe
qui n   est pas naturelle mais struc-
turelle, provoqu  ee par un r  egime in-
sens  e, criminel, psychopathe, pire
que les pires r  egimes de l   union
sovi  etique et de la roumanie de
ceaucescu.

l     etat
je suis tout `a fait d   accord:
ne doit pas passer sous silence; il
doit plut  ot agir et assurer une plus
grande vigilance dans le domaine
  economique.

is bangladesh more friendly with
children?

andererseits beschweren sich un-
ternehmen st  andig   uber den man-
gel an fachpersonal,
indem sie
diese angebliche knappheit als
vorwand benutzen, billige arbeit-
skr  afte einzusetzen.

morning news, now it   s our inter-
active time.

google translate (2016/10/15)
le rapport indique   egalement que,
bien que les trois quarts des chi-
nois disent que leur pays joue un
r  ole plus important dans le monde
qu   il y a dix ans, la plupart veulent
que leur gouvernement de se con-
centrer sur les questions int  erieures
plut  ot que d   aider les autres nations.

street window will not close, very
very noisy and slept badly. mun-
dane breakfast.
(from naver id414): 40n      n        
  t        n   nt                nt n   nt    n        
n    30    n          t                       t        
n   n    t         ,           nt      n    30    n   
nt    n             n    t n  n  n                 t
               n   n                 .

tutto ci`o genera elementi di insi-
curezza e di con   itto rinforzando
quindi spirali negative.

il me semble que cela n   a rien `a
voir avec une p  enurie alimentaire: il
est une v  eritable famine, une catas-
trophe qui est pas naturel, mais
une structure, provoqu  ee par un
r  egime criminel fou, psychopathe,
le pire des pires r  egimes existait
l   union sovi  etique et en roumanie
de ceaucescu.

eh bien, je suis en d  esaccord total:
l   etat ne doit pas prendre un si`ege
arri`ere; il doit plut  ot agir plus et as-
surer une plus grande supervision
dans l     economie.

can a kinder bangladesh with chil-
dren?

auf der anderen seite beschw-
eren sich unternehmen st  andig   uber
den mangel an fachkr  aften, wobei
diese angebliche mangelhaftigkeit
als ausrede f  ur billige arbeitskr  afte
verwendet wird.

(from google gid4): north ko-
rea heard the world, and now to our
interaction time friends.

language

pair
br7   en

fr7   br

source
face `as decis  oes de nice, t    nhamos
de ter em conta esta situac    ao e
de adotar uma metodologia que
contemplasse
transformac     oes
necess  arias.

as

une information pr  esent  ee au mo-
ment opportun signi   e la trans-
parence, laquelle cr  ee la con   ance
et   evite `a l   entreprise de subir des
pertes.

b online system parameters

id4

faced with the nice decisions, we
had to take this situation into ac-
count and adopt a methodology
that would address the necessary
changes.

apresentada

no
a informac     ao
momento
signi   ca
transpar  encia, que cria con   anc  a e
evita que a empresa sofra perdas.

oportuno

google translate (2016/10/15)
nice view of
the decisions we
had to take account of this situa-
tion and adopt a methodology that
took into consideration the neces-
sary changes.

informac     ao apresentada em uma
transpar  encia meio oportuna, que
cria con   anc  a e evita a empresa a
sofrer perdas.

all systems were trained with 4 lstm layers, size of id27 vectors was 500, dropout was set
to 0.3 and we used bidirectional id56 (bid56). column guided alignment indicates wether the network
was trained with guided alignments and on which epoch the feature was stopped.

zh7   en
en7   it
it7   en
en7   ar
ar7   en
en7   es
es7   en
en7   de
de7   en
en7   nl
nl7   en
en7   fr
fr7   en
ja7   en
fr7   br
br7   fr
en7   pt
br7   en
fr7   it
it7   fr
fr7   ar
ar7   fr
fr7   es
es7   fr
fr7   de
de7   fr
nl7   fr
fr7   zh
ja7   ko
ko7   ja
en7   ko
fa7   en

id121

word boundary-generic

generic-compound splitting
compound splitting-generic

generic
generic

generic-crf
crf-generic

generic
generic

generic
generic
generic
generic

bpe

generic
generic
generic
generic
generic
generic

generic-crf
crf-generic

generic
generic

generic

bpe
bpe
bpe
basic

generic-compound splitting
compound splitting-generic

generic-id40

id56
size
800
800
800
800
800
800
800
800
800
800
800
800
800
800
800
800
800
800
800
800
800
800
800
800
800
800
800
800
1000
1000
1000
800

optimal
epoch

12
16
16
15
15
18
18
17
18
17
14
18
17
11
18
18
18
18
18
18
10
15
18
18
17
16
16
18
18
18
17
18

guided

alignment
epoch 4
epoch 4
epoch 4
epoch 4
epoch 4
epoch 4
epoch 4
epoch 4
epoch 4
epoch 4
epoch 4
epoch 4
epoch 4

no

epoch 4
epoch 4
epoch 4
epoch 4
epoch 4
epoch 4
epoch 4
epoch 4
epoch 4
epoch 4
epoch 4
epoch 4
epoch 4
epoch 4

no
no
no
yes

ner
aware
yes
yes
yes
no
no
yes
yes
yes
yes
no
no
yes
yes
no
yes
yes
yes
yes
yes
yes
no
no
yes
yes
no
no
no
no
no
no
no
no

special

double corpora (2 x 5m)

politeness

table 12: parameters for online systems.

