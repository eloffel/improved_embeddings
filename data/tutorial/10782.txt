5
1
0
2

 
r
p
a
5
1

 

 
 
]
l
c
.
s
c
[
 
 

3
v
8
6
5
6

.

2
1
4
1
:
v
i
x
r
a

accepted as a workshop contribution at iclr 2015

improving zero-shot learning by mitigating
the hubness problem

georgiana dinu, angeliki lazaridou, marco baroni
center for mind/brain sciences
university of trento (italy)
georgiana.dinu|angeliki.lazaridou|marco.baroni@unitn.it

abstract

the zero-shot paradigm exploits vector-based word representations extracted from
text corpora with unsupervised methods to learn general mapping functions from
other feature spaces onto word space, where the words associated to the nearest
neighbours of the mapped vectors are used as their linguistic labels. we show that
the neighbourhoods of the mapped elements are strongly polluted by hubs, vectors
that tend to be near a high proportion of items, pushing their correct labels down
the neighbour list. after illustrating the problem empirically, we propose a simple
method to correct it by taking the proximity distribution of potential neighbours
across many mapped vectors into account. we show that this correction leads to
consistent improvements in realistic zero-shot experiments in the cross-lingual,
image labeling and id162 domains.

1

introduction

extensive research in computational linguistics and neural id38 has shown that con-
textual co-occurrence patterns of words in corpora can be effectively exploited to learn high-quality
vector-based representations of their meaning in an unsupervised manner (collobert et al., 2011;
clark, 2015; turney & pantel, 2010). this has in turn led to the development of the so-called zero-
shot learning paradigm as a way to address the manual annotation bottleneck in domains where
other vector-based representations (e.g., images or brain signals) must be associated to word labels
(palatucci et al., 2009). the idea is to use the limited training data available to learn a general map-
ping function from vectors in the domain of interest to word vectors, and then apply the induced
function to map vectors representing new entities (that were not seen in training) onto word space,
retrieving the nearest neighbour words as their labels. this approach has originally been tested in
neural decoding (mitchell et al., 2008; palatucci et al., 2009), where the task consists in learning a
regression function from fmri activation vectors to word representations, and then applying it to the
brain signal of a concept outside the training set, in order to    read the mind    of subjects. in computer
vision, zero-shot mapping of image vectors onto word space has been applied to the task of retriev-
ing words to label images of objects outside the training inventory (frome et al., 2013; socher et al.,
2013), as well as using the inverse language-to-vision mapping for id162 (lazaridou et al.,
2014a). finally, the same approach has been applied in a multilingual context, using translation pair
vectors to learn a cross-language mapping, that is then exploited to translate new words (mikolov
et al., 2013b).
zero-shot learning is a very promising and general technique to reduce manual supervision. how-
ever, while all experiments above report very encouraging results, performance is generally quite
low in absolute terms. for example, the system of frome et al. (2013) returns the correct image
label as top hit in less than 1% of cases in all zero-shot experiments (see their table 2). performance
is always above chance, but clearly not of practical utility.
in this paper, we study one speci   c problem affecting the quality of zero-shot labeling, following up
on an observation that we made, qualitatively, in our experiments: the neighbourhoods surrounding
mapped vectors contain many items that are    universal    neighbours, that is, they are neighbours
of a large number of different mapped vectors. the presence of such vectors, known as hubs, is
an intrinsic problem of high-dimensional spaces (radovanovi  c et al., 2010b). hubness has already

1

accepted as a workshop contribution at iclr 2015

been shown to be an issue for word-based vectors (radovanovi  c et al., 2010a).1 however, as we
show in section 2, the problem is much more severe for neighbourhoods of vectors that are mapped
onto a high-dimensional space from elsewhere through a regression algorithm. we leave a theoret-
ical understanding of why hubness affects regression-based mappings to further work. our current
contributions are to demonstrate the hubness problem in the zero-shot setup, to present a simple and
ef   cient method to get rid of it by adjusting the similarity matrix after mapping, and to show how
this brings consistent performance improvements across different tasks. while one could address the
problem by directly designing hubness-repellent mapping functions, we    nd our post-processing so-
lution more attractive as it allows us to use very simple and general least-squares regression methods
to train and perform the mapping.
we use use the term pivots to stand for a set of vectors we retrieve neighbours for (these comprise
at least, in our setting, the zero-shot-mapped vectors) and targets for the subspace of vectors we
retrieve the neighbours from (often, corresponding to the whole space of interest). then, we can
phrase our proposal as follows. standard nearest neighbour queries rank the targets independently
for each pivot. a single target is allowed to be the nearest neighbour, or among the top k nearest
neighbours, of a large proportion of pivots: and this is exactly what happens empirically (the hubness
problem). we can greatly mitigate the problem by taking the global distribution of targets across
pivots into account. in particular, we use the very straightforward and effective strategy of inverting
the query: we convert the similarity scores of a target with all pivots to the corresponding ranks, and
then retrieve the nearest neighbours of a pivot based on such ranks, instead of the original similarity
scores. we will empirically show that with this method high-hubness targets are down-ranked for
many pivots, and will kept as neighbours only when semantically appropriate.

2 hubness in zero-shot mapping

the zero-shot setup in zero-shot learning, training data consist of vector representations in the
source domain (e.g., source language for translation, image vectors for image annotation) paired
with language labels (the target domain): dtr = {(xi, yi)}m
i=1, where xi     ru and yi     ttr, a
vocabulary containing training labels. at test time, the task is to label vectors which have a novel
label: dts = {(xi, yi)}n
i=1, yi     tts, with tts     ttr =    . this is possible because labels y have
vector representations y     rv.2 training is cast as a multivariate regression problem, learning
a function which maps the source domain vectors to their corresponding target (linguistic-space)
vectors. a straightforward and performant choice (lazaridou et al., 2014a; mikolov et al., 2013b)
is to assume the mapping function is a linear map w, and use a l2-regularized least-squares error
objective:

  w = arg min
w   rv  u

||xw     y||f +   ||w||

(1)

where x and y are matrices obtained through the concatenation of train source vectors and the target
vectors of the corresponding labels.
once the linear function has been estimated, any source vector x     ru can be mapped into the target
domain through xt w.
target space label retrieval given a source element x     s and its vector x, the standard way to
retrieve a target space label (t ) is by returning the nearest neighbour (according to some similarity
measure) of mapped x from the set of vector representations of t . following common practice, we
use the cosine as our similarity measure.
we denote by rankx,t (y) the rank of an element y     t w.r.t. its similarity to x and assuming a
query space t . more precisely, this is the position of y in the (decreasingly) sorted list of similarities:
[cos(x, yi)|yi     t ]. this is an integer from 1 to |t| (assuming distinct cosine values). under this
notation, the standard nearest neighbour of x is given by:

nn1(x, t ) = arg min

y   t

rankx,t (y)

(2)

1radovanovi  c et al. (2010a) propose a supervised hubness-reducing method for document vectors that is

not extensible to the zero-shot scenario, as it assumes a binary relevance classi   cation setup.

2we use x and x to stand for a label and its corresponding vector.

2

accepted as a workshop contribution at iclr 2015

(a) original

(b) mapped, with id173

(c) mapped, no id173

figure 1: distribution of n20 values of target space elements (n20,t s(y) for y     t ). test elements
(pivots) in the target space (original) vs. corresponding vectors obtained by mapping from the source
space (mapped). signi   cantly larger n20 values are observed in the mapped spaces (maxima at 57
and 40 vs. 11 in original space.)

we will use nnk(x, t ) to stand for the set of k-nearest neighbours in t , omitting the t argument
for brevity.
hubness we can measure how hubby an item y     t is with respect to a set of pivot vectors p
(where t is the search space) by counting the number of times it occurs in the k-nearest neighbour
lists of elements in p :

nk,p (y) = |{y     nnk(x, t )|x     p}|

(3)
an item with a large nk value (we will omit the set subscript when it is clear from the context)
occurs in the nnk set of many elements and is therefore a hub.
hubness has been shown to be an intrinsic problem of high-dimensional spaces: as we increase
the dimensionality of the space, a number of elements, which are, by all means, not similar to all
other items, become hubs. as a results nearest neighbour queries return the hubs at top 1, harming
it is known that the problem of hubness is related to concentration, the tendency of
accuracy.
pairwise similarities between elements in a set to converge to a constant as the dimensionality of the
space increases (radovanovi  c et al., 2010b). radovanovi  c et al. (2010a) show that this also holds
for cosine similarity (which is used almost exclusively in linguistic applications): the expectation of
pairwise similarities becomes constant and the standard deviation converges to 0. this, in turn, is
known to cause an increase in hubness.

original vs. mapped vectors
in previous work we have (qualitatively) observed a tendency of the
hubness problem to become worse when we query a target space in which some elements have been
mapped from a different source space. in order to investigate this more closely, we compare the
properties of mapped elements versus original ones. we consider word translation as an application
and use 300-dimensional vectors of english words as source and vectors of italian words as target.
we have, in total, vocabularies of 200,000 english and italian words, which we denote s and t . we
use a set of 5,000 translation pairs as training data and learn a linear map.
we then pick a random test set t s of 1,500 english words that have not been seen in training and map
them to italian using the learned training function (full details in section 3.1 below). we compute
the hubness of all elements in t using the test set items as pivots, and considering all 200, 000 items
in the target space as potential neighbours (as any of them could be the right translation of a test
word). in the    rst setting (original), we use target space items: for the test instance car     auto, we
use the true italian auto vector. in the second and third settings (mapped) we use the mapped vectors
(our predicted translation vector of car into italian), mapped through a matrix learned without and
with id173, respectively. figure 1 plots the distribution of the n20,t s(y) scores in these
three settings.

3

accepted as a workshop contribution at iclr 2015

as the plots show, the hubness problem is indeed greatly exacerbated. when using the original t s
elements, target space hubs reach a n20 level of at most 11, meaning they occur in the nn20 sets
of 11 test elements. on the other hand, when using mapped elements the maximum n20 values are
above 40 (note that the x axes are on different scales in the plots!). moreover, id173 does
not signi   cantly mitigate hubness, suggesting that it is not just a matter of over   tting, such that the
mapping function projects everything near vectors it sees during training.

3 a globally corrected neighbour retrieval method

one way to correct for the increase in hubness caused by mapping is to compute hubness scores for
all target space elements. then, given a test set item, we re-rank its nearest neighbours by downplay-
ing the importance of elements that have a high hubness score. methods for this have been proposed
and evaluated, for example, by radovanovi  c et al. (2010a) and tomasev et al. (2011a). we adopt
a much simpler approach (similar in spirit to tomasev et al., 2011b, but greatly simpli   ed), which
takes advantage of the fact that we almost always have access not to just 1 test instance, but more
vectors in the source domain (these do not need to be labeled instances). we map these additional
pivot elements and conjecture that we can use the topology of the subspace where the mapped pivot
set lives to correct nearest neighbour retrieval. we consider    rst the most straightforward way to
achieve this effect. a hub is an element which appears in many nnk lists because it has high sim-
ilarity with many items. a simple way to correct for this is to normalize the vector of similarities
of each target item to the mapped pivots to length 1, prior to performing nn queries. this way, a
vector with very high similarities to many pivots will be penalized. we denote this method nnnrm.
we propose a second corrected measure, which does not re-weight the similarity scores, but ranks
target elements using nn statistics for the entire mapped pivot set. instead of the nearest neighbour
retrieval method in equation (2), we use a following globally-corrected (gc) approach, that could
be straightforwardly implemented as:

gc1(x, t ) = arg min

y   t

ranky,p (x)

(4)

to put it simply, this method reverses the querying: instead of returning the nearest neighbour of
pivot x as a solution, it returns the target element y which has x ranked highest. intuitively, a hub
may still occur in the nn lists of some elements, but only if not better alternatives are present. the
formulation of gc in equation (4) can however lead to many tied ranks: for example, we want to
translate car, but both italian auto and macchina have car as their second nearest neighbour (so
both rank 2) and no italian word has car as    rst neighbour (no rank 1 value). we use the cosine
scores to break ties, therefore car will be translated with auto if the latter has a higher cosine with the
mapped car vector, with macchina otherwise. note that when only one source vector is available,
the gc method becomes equivalent to a standard nn query. as the cosine is smaller than 1 and
ranks larger or equal to 1, the following equation implements gc with cosine-based tie breaking:

gc1(x, t ) = arg min

y   t

(ranky,p (x)     cos(x, y))

(5)

3.1 english to italian word translation

we    rst test our methods on bilingual lexicon induction. as the amount of parallel data is limited,
there has been a lot of work on acquiring translation dictionaries by using vector-space methods on
monolingual corpora, together with a small seed lexicon (haghighi et al., 2008; klementiev et al.,
2012; koehn & knight, 2002; rapp, 1999). one of the most straightforward and effective methods
is to represent words as high-dimensional vectors that encode co-occurrence only with the words
in the seed lexicon and are therefore comparable cross-lingually (klementiev et al., 2012; rapp,
1999). however, this method is limited to vector spaces that use words as context features, and does
not extend to vector-based word representations relying on other kinds of dimensions, such as those
neural language models that have recently been shown to greatly outperform context-word-based
representations (baroni et al., 2014). the zero-shot approach, that induces a function from one space
to the other based on paired seed element vectors, and then applies it to new data, works irrespective
of the choice of vector representation. this method has been shown to be effective for bilingual
lexicon construction by mikolov et al. (2013b), with dinu & baroni (2014) reporting overall better
performance than with the seed-word-dimension method. we set up a similar evaluation on the task
of    nding italian translations of english words.

4

accepted as a workshop contribution at iclr 2015

word representations the cbow method introduced by mikolov et al. (2013a) induces vector-
based word representations by trying to predict a target word from the words surrounding it within a
neural network architecture. we use the id97 toolkit3 to learn 300-dimensional representations
of 200,000 words with cbow. we consider a context window of 5 words to either side of the target,
we set the sub-sampling option to 1e-05 and estimate the id203 of a target word with the
negative sampling method, drawing 10 samples from the noise distribution (mikolov et al., 2013a).
we use 2.8 billion tokens as input (ukwac + wikipedia + bnc) for english and the 1.6 billion
itwac tokens for italian.4

training and testing both train and test translation pairs are extracted from a dictionary built from
europarl, available at http://opus.lingfil.uu.se/ (europarl, en-it) (tiedemann, 2012).
we use 1,500 english words split into 5 frequency bins as test set (300 randomly chosen in each
bin). the bins are de   ned in terms of rank in the (frequency-sorted) lexicon: [1-5k], [5k-20k],
[20k-50k], [50k-100k] and [100k-200k]. the bilingual lexicon acquisition literature generally
tests on very frequent words only. translating medium or low frequency words is however both
more challenging and useful. we also sample the training translation pairs by frequency, using
the top 1k, 5k, 10k and 20k most frequent translation pairs from our dictionary (by english
frequency), while making sure there is no overlap with test elements.
for each test element we query the entire (200,000) target space and report translation accuracies.
an english word may occur with more than one italian translation (1.2 on average in the entire
data):
in evaluation, an instance is considered correct if any of these is predicted. we test the
standard method (regular nn querying) as well as the two corrected methods: nnnrm and gc.
as previously discussed, the latter bene   t from more mapped data, in addition to an individual test
instance, to be used as pivots. in addition to the 1,500 test elements, we report performance when
mapping other 20,000 randomly chosen english words (their italian translations are not needed). we
actually observed improvements also when using solely the 1,500 mapped test elements as pivots,
but increasing the size with arbitrary additional data (that can simply be sampled from the source
space without any need for supervision) helps performance.

results results are given in figure 2. we report results without id173 as well as with
the id173 parameter    estimated by generalized cross-validation (gcv) (hastie et al., 2009,
p. 244). both corrected methods achieve signi   cant improvements over standard nn, ranging from
7% to 14%. for the standard method, the performance decreases as the training data size increases
beyond 5k, probably due to the noise added by lower-frequency words. the corrected measures
are robust against this effect: adding more training data does not help, but it does not harm them
either. id173 does not improve, and actually hampers the standard method, whereas it
bene   ts the corrected measures when using a small amount of training data (1k), and does not
affect performance otherwise. the results by frequency bin show that most of the improvements
are brought about for the all-important medium- and low-frequency words. although not directly
comparable, the absolute numbers we obtain are in the range of those reported by mikolov et al.
(2013b), whose test data correspond, in terms of frequency, to those in our    rst 2 bins. furthermore,
we observe, similarly to them, that the accuracy scores underestimate the actual performance, as
many translations are in fact correct but not present in our gold dictionary.
the elements with the largest hub score are shown in figure 3 (left). as can be seen, they tend
to be    garbage    low-frequency words. however, in any realistic setting such low-frequency terms
should not be    ltered out, as good translations might also have low frequency. as pointed out
by radovanovi  c et al. (2010b), hubness correlates with proximity to the test-set mean vector (the
average of all test vectors). hubness level is plotted against cosine-to-mean in figure 3 (right).
table 1 presents some cases where wrong translation are    corrected    by the gc measure. the latter
consistently pushes high-hubness elements down the neighbour lists. for example, 11/09/2002, that
was originally returned as the translation of backwardness, can be found in the n20 list of 110 en-
glish words. with the corrected method, the right translation, arretratezza, is obtained. 11/09/2002
is returned as the translation, this time, of only two other english pivot words: orthodoxies and ku-

3https://code.google.com/p/id97/
4corpus sources: http://wacky.sslmit.unibo.it, http://en.wikipedia.org, http://

www.natcorp.ox.ac.uk

5

accepted as a workshop contribution at iclr 2015

train size

1k
5k
10k
20k

train size

1k
5k
10k
20k

no id173

nn nnnrm gc
20.9
14.9
37.7
30.3
37.5
30.0
37.9
25.1

20.9
33.1
33.5
32.9

gcv

nn nnnrm gc
28.7
12.4
27.7
37.5
37.3
28.2
23.7
37.8

25.5
32.9
33.1
32.0

figure 2: percentage accuracy scores for en   it translation. left: no id173 and generalized
cross-validation (gcv) regression varying the training size. right: results split by frequency bins,
5k training and no id173.

hub
blockmonthoff
04.02.05
communauts
limassol
and
ampelia
11/09/2002
cgsi
100.0
cingevano

n20
40
26
26
25
23
23
20
19
18
18

figure 3: en   it translation. left: top 10 hubs and their n20,t st scores. right: n20 plotted against
cosine similarities to test-set mean vector. n20 values correlate signi   cantly with cosines (spearman
   = 0.30, p = 1.0e     300).

n20(hub) x|hub = nn1(x)

translation
almighty   onnipotente
nn:dio
gc: onnipotente
hub: dio (god)
killers   killer
nn: violentatori
gc: killer
hub: violentatori (rapists)
backwardness   arretratezza nn: 11/09/2002
hub: 11/09/2002
gc: arretratezza
table 1: en   it translation. examples of gc    correcting    nn. the wrong nn translation is
always a polluting hub. for these nn hubs, we also report their hubness scores before and after
correction (n20, over the whole pivot set) and examples of words they are nearest neighbour of
(x|hub = nn1(x)) for nn and gc.

righteousness,almighty,jehovah,incarnate,god...
god
killers,anders,rapists,abusers,ragnar
rapists
backwardness,progressivism,orthodoxies...
orthodoxies,kumaratunga

38
20
64
22
110
24

6

accepted as a workshop contribution at iclr 2015

train

chance

gcv

vis   lang
lang   vis

0.02
0.02

nn nnnrm gc
1.5
1.0
0.6
2.2

0.8
1.4

table 2: percentage label and id162 accuracy.

maratunga. the hubs we correct for are not only garbage ones, such as 11/09/2002, but also more
standard words such as dio (god) or violentatori (rapists), also shown in table 1.5

3.2 zero-shot image labeling and retrieving

in this section we test our proposed method in a cross-modal setting, mapping images to word labels
and vice-versa.

experimental setting we use the data set of lazaridou et al. (2014b) containing 5,000 word
labels, each associated to 100 id163 pictures (deng et al., 2009). word representations are
extracted from wikipedia with id97 in skip-gram mode.
images are represented by 4096-
dimensional vectors extracted using the caffe toolkit (jia et al., 2014) together with the pre-trained
convolutional neural network of krizhevsky et al. (2012). we use a random 4/1 train/test split.
results we consider both the usual image labeling setting (vision   language) and the image
retrieval setting (language   vision). for the vision   language task, we use as pivot set the 100k
test images (1,000 labels x 100 images/label) and an additional randomly chosen 100k images.
the search space is the entire label set of 5,000 words. for language   vision, we use as pivot
set the entire word list (5,000) and the target set is the entire set of images (500,000). the objects
depicted in the images form a set of 5,000 distinct elements, therefore, for the word cat, for example,
returning any of the 100 cat images is correct. chance accuracy in both settings is thus at 1/5,000.
table 2 reports accuracy scores.6 we observe that, differently from the translation case, correcting
by normalizing the cosine scores of the elements in the target domain (nnnrm) leads to poorer
results than no correction. on the other hand, the gc method is consistent across domains, and
it improves signi   cantly on the standard nn method in both settings. note that, while there are
differences between the setups, frome et al. (2013) report accuracy results below 1% in all their
zero-shot experiments, including those with chance levels comparable to ours.
in order to investigate the hubness of the corrected solution, we plot similar    gures as in section
2, computing the n20 distribution of the target space elements w.r.t the pivots in the test set.7 fig-
ure 4 shows this distribution 1) for the vectors of the gold word labels in language space, 2) the
corresponding vision   language mapped test vectors, as well as 3) n20 values computed using
gc correction.8 similarly to the translation case, the maximum hubness values increase signi   -
cantly from the original target space vectors to the mapped items. when adjusting the rank with the
gc method, hubness decreases to a level that is now below that of the original items. we observe

corresponding prediction   yi = wxi, the error is given by:(cid:80)k

5prompted by a reviewer, we also performed preliminary experiments with a margin-based ranking objective
similar to the one in wsabie weston et al. (2011) and devise frome et al. (2013) which is typically reported
to outperform the l2 objective in equation 1 (socher et al. (2014)). given a pair of training items (xi, yi) and the
j=1,j(cid:54)=i max{0,   +dist(  yi, yi)   dist(  yi, yj)},
where dist is a distance measure, which we take to be inverse cosine, and    and k are the margin and the number
of negative examples, respectively. we tune   -the margin and k-the number of negative samples on a held-out
set containing 25% of the training data. we estimate w using stochastic id119 where per-parameter
learning rates are tuned with adagrad duchi et al. (2011). results on the en   it task are at 38.4 (nn) and
further improved to 40.6 (gc retrieval), con   rming that gc is not limited to least-squares error estimation
settings.

6the non-regularized objective led to very low results in both directions and for all methods, and we omit

label (e.g., we obtain a single cat vector in image space by averaging the vectors of 100 cat pictures).

7in order to facilitate these computations, we use    aggregated    visual vectors corresponding to each word
8we abuse notation here, as n20 is de   ned as in equation 3 for 1) and 2) and as |{y     gck(x, t )|x     p (cid:48)}|

these results.

for 3).

7

accepted as a workshop contribution at iclr 2015

(a) original

(b) mapped

(c) corrected

figure 4: vision   language. distribution of n20 values of target space elements (n20,t s(y) for
y     t ). pivot vectors of the gold word labels of test elements in target (word) space (original)
vs. corresponding mapped vectors (mapped) vs. corrected mapped vectors (gc) (corrected). n20
values increase from original to mapped sets (max 35 vs. 190), but they drop when using gc cor-
rection (max 20).

the same trend in the language   vision direction (as well as in the translation experiments in the
previous section), the speci   cs of which we however leave out for brevity.

4 conclusion

in this paper we have shown that the basic setup in zero-shot experiments (use multivariate linear
regression with a regularized least-squares error objective to learn a mapping across representational
vectors spaces) is negatively affected by strong hubness effects. we proposed a simple way to cor-
rect for this by replacing the traditional nearest neighbour queries with globally adjusted ones. the
method only requires the availability of more, unlabeled source space data, in addition to the test
instances. while more advanced ways for learning the mapping could be employed (e.g., incorpo-
rating hubness avoidance strategies into non-linear functions or different learning objectives), we
have shown that consistent improvements can be obtained, in very different domains, already with
our query-time correction of the basic learning setup, which is a popular and attractive one, given
its simplicity, generality and high performance. in future work we plan to investigate whether the
hubness effect carries through to other setups: for example, to what extent different kinds of word
representations and other learning objectives are affected by it. this empirical work should pose a
solid basis for a better theoretical understanding of the causes of hubness increase in cross-space
mapping.

5 acknowledgments

this work was supported by the erc 2011 starting independent research grant n. 283554 (com-
poses).

references

baroni, marco, dinu, georgiana, and kruszewski, germ  an. don   t count, predict! a systematic
comparison of context-counting vs. context-predicting semantic vectors. in proceedings of acl,
pp. 238   247, baltimore, md, 2014.

clark, stephen. vector space models of lexical meaning. in lappin, shalom and fox, chris (eds.),
handbook of contemporary semantics, 2nd ed. blackwell, malden, ma, 2015. in press; http:
//www.cl.cam.ac.uk/  sc609/pubs/sem_handbook.pdf.

8

accepted as a workshop contribution at iclr 2015

collobert, ronan, weston, jason, bottou, l  eon, karlen, michael, kavukcuoglu, koray, and kuksa,
pavel. natural language processing (almost) from scratch. journal of machine learning re-
search, 12:2493   2537, 2011.

deng, jia, dong, wei, socher, richard, li, lia-ji, and fei-fei, li. id163: a large-scale hierar-

chical image database. in proceedings of cvpr, pp. 248   255, miami beach, fl, 2009.

dinu, georgiana and baroni, marco. how to make words with vectors: phrase generation in distri-

butional semantics. in proceedings of acl, pp. 624   633, baltimore, md, 2014.

duchi, john, hazan, elad, and singer, yoram. adaptive subgradient methods for online learning

and stochastic optimization. the journal of machine learning research, 12:2121   2159, 2011.

frome, andrea, corrado, greg, shlens, jon, bengio, samy, dean, jeff, ranzato, marc   aurelio, and
mikolov, tomas. devise: a deep visual-semantic embedding model. in proceedings of nips,
pp. 2121   2129, lake tahoe, nv, 2013.

haghighi, aria, liang, percy, berg-kirkpatrick, taylor, and klein, dan. learning bilingual lexicons
in proceedings of acl, pp. 771   779, columbus, oh, usa, june

from monolingual corpora.
2008. url http://www.aclweb.org/anthology/p/p08/p08-1088.

hastie, trevor, tibshirani, robert, and friedman, jerome. the elements of statistical learning, 2nd

edition. springer, new york, 2009.

jia, yangqing, shelhamer, evan, donahue, jeff, karayev, sergey, long, jonathan, girshick, ross,
guadarrama, sergio, and darrell, trevor. caffe: convolutional architecture for fast feature em-
in proceedings of the acm international conference on multimedia, mm    14, pp.
bedding.
675   678, new york, ny, usa, 2014. acm. isbn 978-1-4503-3063-3. doi: 10.1145/2647868.
2654889. url http://doi.acm.org/10.1145/2647868.2654889.

klementiev, alexandre, irvine, ann, callison-burch, chris, and yarowsky, david. toward statistical
machine translation without parallel corpora. in proceedings of eacl, pp. 130   140, avignon,
france, 2012.
isbn 978-1-937284-19-0. url http://dl.acm.org/citation.cfm?
id=2380816.2380835.

koehn, philipp and knight, kevin. learning a translation lexicon from monolingual corpora. in in
proceedings of acl workshop on unsupervised lexical acquisition, pp. 9   16, philadelphia, pa,
usa, 2002.

krizhevsky, alex, sutskever, ilya, and hinton, geoffrey. id163 classi   cation with deep convo-

lutional neural networks. in proceedings of nips, pp. 1097   1105, lake tahoe, nevada, 2012.

lazaridou, angeliki, bruni, elia, and baroni, marco. is this a wampimuk? cross-modal mapping
between id65 and the visual world. in proceedings of acl, pp. 1403   1414,
baltimore, md, 2014a.

lazaridou, angeliki, pham, the nghia, and baroni, marco. combining language and vision with a

multimodal skip-gram model. in nips workshop on learning semantics, 2014b.

mikolov, tomas, chen, kai, corrado, greg, and dean, jeffrey. ef   cient estimation of word repre-

sentations in vector space. http://arxiv.org/abs/1301.3781/, 2013a.

mikolov, tomas, le, quoc, and sutskever, ilya. exploiting similarities among languages for ma-

chine translation. http://arxiv.org/abs/1309.4168, 2013b.

mitchell, tom, shinkareva, svetlana, carlson, andrew, chang, kai-min, malave, vincente, mason,
robert, and just, marcel. predicting human brain activity associated with the meanings of nouns.
science, 320:1191   1195, 2008.

palatucci, mark, pomerleau, dean, hinton, geoffrey e, and mitchell, tom m. zero-shot learning

with semantic output codes. in proceedings of nips, pp. 1410   1418, 2009.

radovanovi  c, milos, nanopoulos, alexandros, and ivanovi  c, mirjana. on the existence of obstinate

results in vector space models. in proceedings of sigir, pp. 186   193, 2010a.

9

accepted as a workshop contribution at iclr 2015

radovanovi  c, milo  s, nanopoulos, alexandros, and ivanovi  c, mirjana. hubs in space: popular
nearest neighbors in high-dimensional data. journal of machine learning research, 11:2487   
2531, 2010b.

rapp, reinhard. automatic identi   cation of word translations from unrelated english and german
corpora. in proceedings of the 37th annual meeting of the association for computational lin-
guistics on computational linguistics, acl    99, pp. 519   526. association for computational
linguistics, 1999.

socher, richard, ganjoo, milind, manning, christopher, and ng, andrew. zero-shot learning

through cross-modal transfer. in proceedings of nips, pp. 935   943, lake tahoe, nv, 2013.

socher, richard, le, quoc, manning, christopher, and ng, andrew. grounded compositional se-
mantics for    nding and describing images with sentences. transactions of the association for
computational linguistics, 2:207   218, 2014.

tiedemann, j  org. parallel data, tools and interfaces in opus. in proceedings of the eight interna-

tional conference on language resources and evaluation (lrec   12), istanbul, turkey, 2012.

tomasev, nenad, brehar, raluca, mladenic, dunja, and nedevschi, sergiu. the in   uence of hubness
intelligent computer communication and

on nearest-neighbor methods in object recognition.
processing (iccp), 2011 ieee international conference, 2011a.

tomasev, nenad, radovanovic, milos, mladenic, dunja, and ivanovic, mirjana. a probabilistic

approach to nearest-neighbor classi   cation: naive hubness bayesian knn. in cikm, 2011b.

turney, peter and pantel, patrick. from frequency to meaning: vector space models of semantics.

journal of arti   cial intelligence research, 37:141   188, 2010.

weston, jason, bengio, samy, and usunier, nicolas. wsabie: scaling up to large vocabulary image

annotation. in proceedings of ijcai, pp. 2764   2770, 2011.

10

