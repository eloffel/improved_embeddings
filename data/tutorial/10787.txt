6
1
0
2

 
t
c
o
5

 

 
 
]
l
m

.
t
a
t
s
[
 
 

5
v
7
8
2
5
0

.

2
1
5
1
:
v
i
x
r
a

a theoretically grounded application of dropout in

recurrent neural networks

yarin gal

university of cambridge

{yg279,zg201}@cam.ac.uk

zoubin ghahramani

abstract

recurrent neural networks (id56s) stand at the forefront of many recent develop-
ments in deep learning. yet a major dif   culty with these models is their tendency to
over   t, with dropout shown to fail when applied to recurrent layers. recent results
at the intersection of bayesian modelling and deep learning offer a bayesian inter-
pretation of common deep learning techniques such as dropout. this grounding of
dropout in approximate bayesian id136 suggests an extension of the theoretical
results, offering insights into the use of dropout with id56 models. we apply this
new variational id136 based dropout technique in lstm and gru models,
assessing it on language modelling and id31 tasks. the new approach
outperforms existing techniques, and to the best of our knowledge improves on the
single model state-of-the-art in language modelling with the id32 (73.4
test perplexity). this extends our arsenal of variational tools in deep learning.

1

introduction

recurrent neural networks (id56s) are sequence-based models of key importance for natural language
understanding, language generation, video processing, and many other tasks [1   3]. the model   s input
is a sequence of symbols, where at each time step a simple neural network (id56 unit) is applied to a
single symbol, as well as to the network   s output from the previous time step. id56s are powerful
models, showing superb performance on many tasks, but over   t quickly. lack of regularisation in
id56 models makes it dif   cult to handle small data, and to avoid over   tting researchers often use
early stopping, or small and under-speci   ed models [4].
dropout is a popular regularisation technique with deep networks [5, 6] where network units are
randomly masked during training (dropped). but the technique has never been applied successfully
to id56s. empirical results have led many to believe that noise added to recurrent layers (connections
between id56 units) will be ampli   ed for long sequences, and drown the signal [4]. consequently,
existing research has concluded that the technique should be used with the inputs and outputs of the
id56 alone [4, 7   10]. but this approach still leads to over   tting, as is shown in our experiments.
recent results at the intersection of bayesian research and deep learning offer interpretation of
common deep learning techniques through bayesian eyes [11   16]. this bayesian view of deep
learning allowed the introduction of new techniques into the    eld, such as methods to obtain principled
uncertainty estimates from deep learning networks [14, 17]. gal and ghahramani [14] for example
showed that dropout can be interpreted as a variational approximation to the posterior of a bayesian
neural network (nn). their variational approximating distribution is a mixture of two gaussians
with small variances, with the mean of one gaussian    xed at zero. this grounding of dropout in
approximate bayesian id136 suggests that an extension of the theoretical results might offer
insights into the use of the technique with id56 models.
here we focus on common id56 models in the    eld (lstm [18], gru [19]) and interpret these
as probabilistic models, i.e. as id56s with network weights treated as random variables, and with

yt   1

yt

yt+1

yt   1

yt

yt+1

xt   1

xt

xt+1

xt   1

xt

xt+1

(a) naive dropout id56

(b) variational id56

figure 1: depiction of the dropout technique following our bayesian interpretation (right)
compared to the standard technique in the    eld (left). each square represents an id56 unit, with
horizontal arrows representing time dependence (recurrent connections). vertical arrows represent
the input and output to each id56 unit. coloured connections represent dropped-out inputs, with
different colours corresponding to different dropout masks. dashed lines correspond to standard
connections with no dropout. current techniques (naive dropout, left) use different masks at different
time steps, with no dropout on the recurrent layers. the proposed technique (variational id56, right)
uses the same dropout mask at each time step, including the recurrent layers.

suitably de   ned likelihood functions. we then perform approximate variational id136 in these
probabilistic bayesian models (which we will refer to as variational id56s). approximating the
posterior distribution over the weights with a mixture of gaussians (with one component    xed at
zero and small variances) will lead to a tractable optimisation objective. optimising this objective is
identical to performing a new variant of dropout in the respective id56s.
in the new dropout variant, we repeat the same dropout mask at each time step for both inputs, outputs,
and recurrent layers (drop the same network units at each time step). this is in contrast to the existing
ad hoc techniques where different dropout masks are sampled at each time step for the inputs and
outputs alone (no dropout is used with the recurrent connections since the use of different masks
with these connections leads to deteriorated performance). our method and its relation to existing
techniques is depicted in    gure 1. when used with discrete inputs (i.e. words) we place a distribution
over the id27s as well. dropout in the word-based model corresponds then to randomly
dropping word types in the sentence, and might be interpreted as forcing the model not to rely on
single words for its task.
we next survey related literature and background material, and then formalise our approximate
id136 for the variational id56, resulting in the dropout variant proposed above. experimental
results are presented thereafter.

2 related research

in the past few years a considerable body of work has been collected demonstrating the negative
effects of a naive application of dropout in id56s    recurrent connections. pachitariu and sahani [7],
working with language models, reason that noise added in the recurrent connections of an id56 leads
to model instabilities. instead, they add noise to the decoding part of the model alone. bayer et al. [8]
apply a deterministic approximation of dropout (fast dropout) in id56s. they reason that with dropout,
the id56   s dynamics change dramatically, and that dropout should be applied to the    non-dynamic   
parts of the model     connections feeding from the hidden layer to the output layer. pham et al. [9]
assess dropout with handwriting recognition tasks. they conclude that dropout in recurrent layers
disrupts the id56   s ability to model sequences, and that dropout should be applied to feed-forward
connections and not to recurrent connections. the work by zaremba, sutskever, and vinyals [4] was
developed in parallel to pham et al. [9]. zaremba et al. [4] assess the performance of dropout in id56s
on a wide series of tasks. they show that applying dropout to the non-recurrent connections alone

2

results in improved performance, and provide (as yet unbeaten) state-of-the-art results in language
modelling on the id32. they reason that without dropout only small models were used
in the past in order to avoid over   tting, whereas with the application of dropout larger models can
be used, leading to improved results. this work is considered a reference implementation by many
(and we compare to this as a baseline below). bluche et al. [10] extend on the previous body of work
and perform exploratory analysis of the performance of dropout before, inside, and after the id56   s
unit. they provide mixed results, not showing signi   cant improvement on existing techniques. more
recently, and done in parallel to this work, moon et al. [20] suggested a new variant of dropout in
id56s in the id103 community. they randomly drop elements in the lstm   s internal
cell ct and use the same mask at every time step. this is the closest to our proposed approach
(although fundamentally different to the approach we suggest, explained in   4.1), and we compare to
this variant below as well.
existing approaches are based on an empirical experimentation with different    avours of dropout,
following a process of trial-and-error. these approaches have led many to believe that dropout
cannot be extended to a large number of parameters within the recurrent layers, leaving them with
no regularisation. in contrast to these conclusions, we show that it is possible to derive a variational
id136 based variant of dropout which successfully regularises such parameters, by grounding our
approach in recent theoretical research.

3 background

we review necessary background in bayesian neural networks and approximate variational id136.
building on these ideas, in the next section we propose approximate id136 in the probabilistic
id56 which will lead to a new variant of dropout.

3.1 bayesian neural networks
given training inputs x = {x1, . . . , xn} and their corresponding outputs y = {y1, . . . , yn}, in
bayesian (parametric) regression we would like to infer parameters    of a function y = f   (x) that
are likely to have generated our outputs. what parameters are likely to have generated our data?
following the bayesian approach we would put some prior distribution over the space of parameters,
p(  ). this distribution represents our prior belief as to which parameters are likely to have generated
our data. we further need to de   ne a likelihood distribution p(y|x,   ). for classi   cation tasks we
may assume a softmax likelihood,

p(cid:0)y = d|x,   (cid:1) = categorical

(cid:32)

exp(f   

d (x))/

exp(f   

d(cid:48) (x))

(cid:33)

(cid:88)

d(cid:48)

or a gaussian likelihood for regression. given a dataset x, y, we then look for the posterior
distribution over the space of parameters: p(  |x, y). this distribution captures how likely various
function parameters are given our observed data. with it we can predict an output for a new input
point x    by integrating

p(y   |x   , x, y) =

p(y   |x   ,   )p(  |x, y)d  .

(1)

(cid:90)

one way to de   ne a distribution over a parametric set of functions is to place a prior distribution over
a neural network   s weights, resulting in a bayesian nn [21, 22]. given weight matrices wi and bias
vectors bi for layer i, we often place standard matrix gaussian prior distributions over the weight
matrices, p(wi) = n (0, i) and often assume a point estimate for the bias vectors for simplicity.

3.2 approximate variational id136 in bayesian neural networks

we are interested in    nding the distribution of weight matrices (parametrising our functions) that have
generated our data. this is the posterior over the weights given our observables x, y: p(  |x, y).
this posterior is not tractable in general, and we may use variational id136 to approximate it (as
was done in [23   25, 12]). we need to de   ne an approximating variational distribution q(  ), and then
minimise the kl divergence between the approximating distribution and the full posterior:

kl(cid:0)q(  )||p(  |x, y)(cid:1)        

(cid:90)

q(  ) log p(y|x,   )d   + kl(q(  )||p(  ))

3

(cid:90)

=     n(cid:88)

i=1

q(  ) log p(yi|f   (xi))d   + kl(q(  )||p(  )).

(2)

we next extend this approximate variational id136 to probabilistic id56s, and use a q(  ) distribu-
tion that will give rise to a new variant of dropout in id56s.

4 variational id136 in recurrent neural networks

in this section we will concentrate on simple id56 models for brevity of notation. derivations for
lstm and gru follow similarly. given input sequence x = [x1, ..., xt ] of length t , a simple id56
is formed by a repeated application of a function fh. this generates a hidden state ht for time step t:

ht = fh(xt, ht   1) =   (xtwh + ht   1uh + bh)

d  

(cid:90)

q(  ) log p(y|f   

y (ht ))d   =

for some non-linearity   . the model output can be de   ned, for example, as fy(ht ) = ht wy + by.
we view this id56 as a probabilistic model by regarding    = {wh, uh, bh, wy, by} as random
variables (following normal prior distributions). to make the dependence on    clear, we write f   
y
for fy and similarly for f   
h . we de   ne our probabilistic model   s likelihood as above (section 3.1).
the posterior over random variables    is rather complex, and we use variational id136 with
approximating distribution q(  ) to approximate it.
evaluating each sum term in eq. (2) above with our id56 model we get

(cid:90)
(cid:90)

y

y

q(  ) log p

(cid:18)
(cid:18)

(cid:12)(cid:12)(cid:12)(cid:12)f   
h (xt , ht   1)(cid:1)(cid:19)
(cid:0)f   
(cid:12)(cid:12)(cid:12)(cid:12)f   
(cid:0)f   
h (x1, h0)...))(cid:1)(cid:19)
h (...f(cid:98)  

h (xt , f   

q(  ) log p

h (...f   

y

y

,

d  

h (x1, h0)...))(cid:1)(cid:19)
(cid:98)       q(  )

=

(cid:18)

y

(cid:12)(cid:12)(cid:12)(cid:12)f(cid:98)  
(cid:18)

y

    log p

with h0 = 0. we approximate this with monte carlo (mc) integration with a single sample:

resulting in an unbiased estimator to each sum term.
this estimator is plugged into equation (2) to obtain our minimisation objective

(cid:0)f(cid:98)  
h (xt , f(cid:98)  
(cid:12)(cid:12)(cid:12)(cid:12)f(cid:98)  i
h (xi,1, h0)...))(cid:1)(cid:19)
y,(cid:98)bi
h,(cid:98)bi
note that for each sequence xi we sample a new realisation(cid:98)  i = {(cid:99)wi
h,(cid:99)wi
h,(cid:98)ui
y}, and that
each symbol in the sequence xi = [xi,1, ..., xi,t ] is passed through the function f(cid:98)  i
h,(cid:98)bi
h,(cid:98)ui
weight realisations(cid:99)wi
h with the same
h used at every time step t     t .

(cid:0)f(cid:98)  i
h (xi,t , f(cid:98)  i

l         n(cid:88)

+ kl(q(  )||p(  )).

h (...f(cid:98)  i

log p

(3)

yi

i=1

y

following [17] we de   ne our approximating distribution to factorise over the weight matrices and
their rows in   . for every weight matrix row wk the approximating distribution is:

q(wk) = pn (wk; 0,   2i) + (1     p)n (wk; mk,   2i)

with mk variational parameter (row vector), p given in advance (the dropout id203), and small
  2. we optimise over mk the variational parameters of the random weight matrices; these correspond
to the id56   s weight matrices in the standard view1. the kl in eq. (3) can be approximated as l2
regularisation over the variational parameters mk [17].

evaluating the model output f(cid:98)  
rows in each weight matrix w during the forward pass     i.e. performing dropout. our objective l is
identical to that of the standard id56. in our id56 setting with a sequence input, each weight matrix
row is randomly masked once, and importantly the same mask is used through all time steps.2

y (  ) with sample(cid:98)       q(  ) corresponds to randomly zeroing (masking)

1graves et al. [26] further factorise the approximating distribution over the elements of each row, and use a
gaussian approximating distribution with each element (rather than a mixture); the approximating distribution
above seems to give better performance, and has a close relation with dropout [17].

2in appendix a we discuss the relation of our dropout interpretation to the ensembling one.

4

predictions can be approximated by either propagating the mean of each layer to the next (referred to
as the standard dropout approximation), or by approximating the posterior in eq. (1) with q(  ),

p(y   |x   , x, y)    

p(y   |x   ,   )q(  )d       1
k

with(cid:98)  k     q(  ), i.e. by performing dropout at test time and averaging results (mc dropout).

k=1

(4)

k(cid:88)

p(y   |x   ,(cid:98)  k)

(cid:90)

4.1

implementation and relation to dropout in id56s

implementing our approximate id136 is identical to implementing dropout in id56s with the
same network units dropped at each time step, randomly dropping inputs, outputs, and recurrent
connections. this is in contrast to existing techniques, where different network units would be
dropped at different time steps, and no dropout would be applied to the recurrent connections (   g. 1).
certain id56 models such as lstms and grus use different gates within the id56 units. for
example, an lstm is de   ned using four gates:    input   ,    forget   ,    output   , and    input modulation   ,

i = sigm(cid:0)ht   1ui + xtwi
(cid:1)
o = sigm(cid:0)ht   1uo + xtwo
(cid:1)

ct = f     ct   1 + i     g

f = sigm(cid:0)ht   1uf + xtwf
(cid:1)
(cid:1)
g = tanh(cid:0)ht   1ug + xtwg

ht = o     tanh(ct)

(5)
with    = {wi, ui, wf , uf , wo, uo, wg, ug} weight matrices and     the element-wise product.
here an internal state ct (also referred to as cell) is updated additively.
alternatively, the model could be re-parametrised as in [26]:

          =

          i

f
o
g

         (cid:18)(cid:18) xt
         sigm

sigm
sigm
tanh

ht   1

(cid:19)

(cid:19)

   w

with    = {w}, w a matrix of dimensions 2k by 4k (k being the dimensionality of xt). we name
this parametrisation a tied-weights lstm (compared to the untied-weights lstm in eq. (5)).
even though these two parametrisations result in the same deterministic model, they lead to different
approximating distributions q(  ). with the    rst parametrisation one could use different dropout
masks for different gates (even when the same input xt is used). this is because the approximating
distribution is placed over the matrices rather than the inputs: we might drop certain rows in one
weight matrix w applied to xt and different rows in another matrix w(cid:48) applied to xt. with the
second parametrisations we would place a distribution over the single matrix w. this leads to a
faster forward-pass, but with slightly diminished results as we will see in the experiments section.
in more concrete terms, we may write our dropout variant with the second parametrisation (eq. (6)) as

(6)

(7)

with zx, zh random masks repeated at all time steps (and similarly for the parametrisation in eq. (5)).
in comparison, zaremba et al. [4]   s dropout variant replaces zx in eq. (7) with the time-dependent zt
x
which is sampled anew every time step (whereas zh is removed and the recurrent connection ht   1 is
not dropped):

          i

f
o
g

          =

         sigm
         (cid:18)(cid:18) xt     zx

ht   1     zh

(cid:19)

(cid:19)

   w

sigm
sigm
tanh

          =

          i

f
o
g

         sigm
         (cid:18)(cid:18)xt     zt

x
ht   1

sigm
sigm
tanh

(cid:19)

(cid:19)

   w

.

(8)

on the other hand, moon et al. [20]   s dropout variant changes eq. (5) by adapting the internal cell

(9)
with the same mask zc used at all time steps. note that unlike [20], by viewing dropout as an
operation over the weights our technique trivially extends to id56s and grus.

ct = ct     zc

5

4.2 id27s dropout

in datasets with continuous inputs we often apply dropout to the input layer     i.e. to the input vector
itself. this is equivalent to placing a distribution over the weight matrix which follows the input and
approximately integrating over it (the matrix is optimised, therefore prone to over   tting otherwise).
but for models with discrete inputs such as words (where every word is mapped to a continuous
vector     a id27) this is seldom done. with id27s the input can be seen as
either the id27 itself, or, more conveniently, as a    one-hot    encoding (a vector of zeros
with 1 at a single position). the product of the one-hot encoded vector with an embedding matrix
we     rv   d (where d is the embedding dimensionality and v is the number of words in the
vocabulary) then gives a id27. curiously, this parameter layer is the largest layer in most
language applications, yet it is often not regularised. since the embedding matrix is optimised it can
lead to over   tting, and it is therefore desirable to apply dropout to the one-hot encoded vectors. this
in effect is identical to dropping words at random throughout the input sentence, and can also be
interpreted as encouraging the model to not    depend    on single words for its output.
note that as before, we randomly set rows of the matrix we     rv   d to zero. since we repeat the
same mask at each time step, we drop the same words throughout the sequence     i.e. we drop word
types at random rather than word tokens (as an example, the sentence    the dog and the cat    might
become        dog and     cat    or    the     and the cat   , but never        dog and the cat   ). a possible
inef   ciency implementing this is the requirement to sample v bernoulli random variables, where
v might be large. this can be solved by the observation that for sequences of length t , at most t
embeddings could be dropped (other dropped embeddings have no effect on the model output). for
t (cid:28) v it is therefore more ef   cient to    rst map the words to the id27s, and only then to
zero-out id27s based on their word type.

5 experimental evaluation

we start by implementing our proposed dropout variant into the torch implementation of zaremba
et al. [4], that has become a reference implementation for many in the    eld. zaremba et al. [4] have
set a benchmark on the id32 that to the best of our knowledge hasn   t been beaten for
the past 2 years. we improve on [4]   s results, and show that our dropout variant improves model
performance compared to early-stopping and compared to using under-speci   ed models. we continue
to evaluate our proposed dropout variant with both lstm and gru models on a id31
task where labelled data is scarce. we    nish by giving an in-depth analysis of the properties of the
proposed method, with code and many experiments deferred to the appendix due to space constraints.

5.1 language modelling

we replicate the language modelling experiment of zaremba, sutskever, and vinyals [4]. the
experiment uses the id32, a standard benchmark in the    eld. this dataset is considered
a small one in the language processing community, with 887, 521 tokens (words) in total, making
over   tting a considerable concern. throughout the experiments we refer to lstms with the dropout
technique proposed following our bayesian interpretation as variational lstms, and refer to existing
dropout techniques as naive dropout lstms (eq. (8), different masks at different steps, applied to the
input and output of the lstm alone). we refer to lstms with no dropout as standard lstms.
we implemented a variational lstm for both the medium model of [4] (2 layers with 650 units in
each layer) as well as their large model (2 layers with 1500 units in each layer). the only changes
we   ve made to [4]   s setting are 1) using our proposed dropout variant instead of naive dropout, and
2) tuning weight decay (which was chosen to be zero in [4]). all other hyper-parameters are kept
identical to [4]: learning rate decay was not tuned for our setting and is used following [4]. dropout
parameters were optimised with grid search (tying the dropout id203 over the embeddings
together with the one over the recurrent layers, and tying the dropout id203 for the inputs and
outputs together as well). these are chosen to minimise validation perplexity3. we further compared
to moon et al. [20] who only drop elements in the lstm internal state using the same mask at all

3optimal probabilities are 0.3 and 0.5 respectively for the large model, compared [4]   s 0.6 dropout id203,

and 0.2 and 0.35 respectively for the medium model, compared [4]   s 0.5 dropout id203.

6

medium lstm

large lstm
test
127.4
118.7
86.0
78.4

validation

moon et al. [20]

zaremba et al. [4]

128.3
122.9
88.8
82.2

121.1
100.7
88.9
86.2

non-regularized (early stopping)

variational (tied weights)

test
121.7
97.0
86.5
82.7

moon et al. [20] +emb dropout

variational (tied weights, mc)
variational (untied weights)

wps validation
5.5k
4.8k
4.8k
5.5k
4.7k 77.3    0.2 75.0    0.1
   
74.1    0.0
2.7k 77.9    0.3 75.2    0.2

wps
2.5k
3k
3k
2.5k
2.4k
   
1.6k
73.4    0.0    
table 1: single model perplexity (on test and validation sets) for the id32 language
modelling task. two model sizes are compared (a medium and a large lstm, following [4]   s setup),
with number of processed words per second (wps) reported. both dropout approximation and mc
dropout are given for the test set with the variational model. a common approach for regularisation is
to reduce model complexity (necessary with the non-regularised lstm). with the variational models
however, a signi   cant reduction in perplexity is achieved by using larger models.

81.8    0.2 79.7    0.1
79.0    0.1
81.9    0.2 79.7    0.1

variational (untied weights, mc)

78.6    0.1    

   

   

   

   

time steps (in addition to performing dropout on the inputs and outputs, eq. (9)). we implemented
their dropout variant with each model size, and repeated the procedure above to    nd optimal dropout
probabilities (0.3 with the medium model, and 0.5 with the large model). we had to use early stopping
for the large model with [20]   s variant as the model starts over   tting after 16 epochs. moon et al.
[20] proposed their dropout variant within the id103 community, where they did not
have to consider embeddings over   tting (which, as we will see below, affect the recurrent layers
considerably). we therefore performed an additional experiment using [20]   s variant together with
our embedding dropout (referred to as moon et al. [20]+emb dropout).
our results are given in table 1. for the variational lstm we give results using both the tied weights
model (eq. (6)   (7), variational (tied weights)), and without weight tying (eq. (5), variational (untied
weights)). for each model we report performance using both the standard dropout approximation
(averaging the weights at test time     propagating the mean of each approximating distribution as input
to the next layer), and using mc dropout (obtained by performing dropout at test time 1000 times,
and averaging the model outputs following eq. (4), denoted mc). for each model we report average
perplexity and standard deviation (each experiment was repeated 3 times with different random seeds
and the results were averaged). model training time is given in words per second (wps).
it is interesting that using the dropout approximation, weight tying results in lower validation error
and test error than the untied weights model. but with mc dropout the untied weights model performs
much better. validation perplexity for the large model is improved from [4]   s 82.2 down to 77.3 (with
weight tying), or 77.9 without weight tying. test perplexity is reduced from 78.4 down to 73.4 (with
mc dropout and untied weights). to the best of our knowledge, these are currently the best single
model perplexities on the id32.
it seems that moon et al. [20] underperform even compared to [4]. with no embedding dropout the
large model over   ts and early stopping is required (with no early stopping the model   s validation
perplexity goes up to 131 within 30 epochs). adding our embedding dropout, the model performs
much better, but still underperforms compared to applying dropout on the inputs and outputs alone.
comparing our results to the non-regularised lstm (evaluated with early stopping, giving similar
performance as the early stopping experiment in [4]) we see that for either model size an improvement
can be obtained by using our dropout variant. comparing the medium sized variational model to the
large one we see that a signi   cant reduction in perplexity can be achieved by using a larger model.
this cannot be done with the non-regularised lstm, where a larger model leads to worse results.
this shows that reducing the complexity of the model, a possible approach to avoid over   tting,
actually leads to a worse    t when using dropout.
we also see that the tied weights model achieves very close performance to that of the untied weights
one when using the dropout approximation. assessing model run time though (on a titan x gpu),
we see that tying the weights results in a more time-ef   cient implementation. this is because the
single matrix product is implemented as a single gpu kernel, instead of the four smaller matrix

7

products used in the untied weights model (where four gpu kernels are called sequentially). note
though that a low level implementation should give similar run times.
we further experimented with a model averaging experiment following [4]   s setting, where several
large models are trained independently with their outputs averaged. we used variational lstms
with mc dropout following the setup above. using 10 variational lstms we improve [4]   s test set
perplexity from 69.5 to 68.7     obtaining identical perplexity to [4]   s experiment with 38 models.
lastly, we report validation perplexity with reduced learning rate decay (with the medium model).
learning rate decay is often used for regularisation by setting the optimiser to make smaller steps
when the model starts over   tting (as done in [4]). by removing it we can assess the regularisation
effects of dropout alone. as can be seen in    g. 2, even with early stopping, variational lstm achieves
lower perplexity than naive dropout lstm and standard lstm. note though that a signi   cantly
lower perplexity for all models can be achieved with learning rate decay scheduling as seen in table 1

5.2 id31

we next evaluate our dropout variant with both lstm and gru models on a id31 task,
where labelled data is scarce. we use mc dropout (which we compare to the dropout approximation
further in appendix b), and untied weights model parametrisations.
we use the raw cornell    lm reviews corpus collected by pang and lee [27]. the dataset is composed
of 5000    lm reviews. we extract consecutive segments of t words from each review for t = 200,
and use the corresponding    lm score as the observed output y. the model is built from one embedding
layer (of dimensionality 128), one lstm layer (with 128 network units for each gate; gru setting is
built similarly), and    nally a fully connected layer applied to the last output of the lstm (resulting
in a scalar output). we use the adam optimiser [28] throughout the experiments, with batch size 128,
and mc dropout at test time with 10 samples.
the main results can be seen in    g. 3. we compared variational lstm (with our dropout variant
applied with each weight layer) to standard techniques in the    eld. training error is shown in    g. 3a
and test error is shown in    g. 3b. optimal dropout probabilities and weight decay were used for each
model (see appendix b). it seems that the only model not to over   t is the variational lstm, which
achieves lowest test error as well. variational gru test error is shown in    g. 14 (with loss plot given
in appendix b). optimal dropout probabilities and weight decay were used again for each model.
variational gru avoids over   tting to the data and converges to the lowest test error. early stopping in
this dataset will result in smaller test error though (lowest test error is obtained by the non-regularised
gru model at the second epoch). it is interesting to note that standard techniques exhibit peculiar
behaviour where test error repeatedly decreases and increases. this behaviour is not observed with
the variational gru. convergence plots of the loss for each model are given in appendix b.
we next explore the effects of dropping-out different parts of the model. we assessed our variational
lstm with different combinations of dropout over the embeddings (pe = 0, 0.5) and recurrent
layers (pu = 0, 0.5) on the id31 task. the convergence plots can be seen in    gure 4a. it
seems that without both strong embeddings regularisation and strong regularisation over the recurrent
layers the model would over   t rather quickly. the behaviour when pu = 0.5 and pe = 0 is quite
interesting: test error decreases and then increases before decreasing again. also, it seems that when
pu = 0 and pe = 0.5 the model becomes very erratic.

figure 2: medium model validation perplexity for the id32 language modelling task.
learning rate decay was reduced to assess model over   tting using dropout alone. even with early
stopping, variational lstm achieves lower perplexity than naive dropout lstm and standard lstm.
lower perplexity for all models can be achieved with learning rate decay scheduling, seen in table 1.

8

(a) lstm train error: variational,
naive dropout, and standard lstm.

(b) lstm test error: variational,
naive dropout, and standard lstm.

(c) gru test error: variational,

naive dropout, and standard lstm.

figure 3: id31 error for variational lstm / gru compared to naive dropout lstm /
gru and standard lstm / gru (with no dropout).

lastly, we tested the performance of variational lstm with different recurrent layer dropout
probabilities,    xing the embedding dropout id203 at either pe = 0 or pe = 0.5 (   gs. 4b-4c).
these results are rather intriguing. in this experiment all models have converged, with the loss getting
near zero (not shown). yet it seems that with no embedding dropout, a higher dropout id203
within the recurrent layers leads to over   tting! this presumably happens because of the large number
of parameters in the embedding layer which is not regularised. regularising the embedding layer with
dropout id203 pe = 0.5 we see that a higher recurrent layer dropout id203 indeed leads to
increased robustness to over   tting, as expected. these results suggest that embedding dropout can be
of crucial importance in some tasks.
in appendix b we assess the importance of weight decay with our dropout variant. common practice
is to remove weight decay with naive dropout. our results suggest that weight decay plays an
important role with our variant (it corresponds to our prior belief of the distribution over the weights).

6 conclusions

we presented a new technique for recurrent neural network regularisation. our id56 dropout variant
is theoretically motivated and its effectiveness was empirically demonstrated. in future research
we aim to assess model uncertainty in variational lstms [17]. together with the developments
presented here, this will have important implications for modelling language ambiguity and modelling
dynamics in control tasks.

references
[1] martin sundermeyer, ralf schl  ter, and hermann ney. lstm neural networks for id38. in

interspeech, 2012.

[2] nal kalchbrenner and phil blunsom. recurrent continuous translation models. in emnlp, 2013.

(a) combinations of pe = 0, 0.5

with pu = 0, 0.5.

(b) pu = 0, ..., 0.5 with

   xed pe = 0.

(c) pu = 0, ..., 0.5 with

   xed pe = 0.5.

figure 4: test error for variational lstm with various settings on the id31 task.
different dropout probabilities are used with the recurrent layer (pu ) and embedding layer (pe).

9

[3] ilya sutskever, oriol vinyals, and quoc vv le. sequence to sequence learning with neural networks. in

nips, 2014.

[4] wojciech zaremba, ilya sutskever, and oriol vinyals. recurrent neural network id173. arxiv

preprint arxiv:1409.2329, 2014.

[5] geoffrey e others hinton. improving neural networks by preventing co-adaptation of feature detectors.

arxiv preprint arxiv:1207.0580, 2012.

[6] nitish srivastava, geoffrey hinton, alex krizhevsky, ilya sutskever, and ruslan salakhutdinov. dropout:

a simple way to prevent neural networks from over   tting. jmlr, 2014.

[7] marius pachitariu and maneesh sahani. id173 and nonlinearities for neural language models:

when are they needed? arxiv preprint arxiv:1301.5650, 2013.

[8] justin bayer, christian osendorfer, daniela korhammer, nutan chen, sebastian urban, and patrick van der
smagt. on fast dropout and its applicability to recurrent networks. arxiv preprint arxiv:1311.0701, 2013.
[9] vu pham, theodore bluche, christopher kermorvant, and jerome louradour. dropout improves recurrent

neural networks for handwriting recognition. in icfhr. ieee, 2014.

[10] th  odore bluche, christopher kermorvant, and j  r  me louradour. where to apply dropout in recurrent

neural networks for handwriting recognition? in icdar. ieee, 2015.

[11] danilo jimenez rezende, shakir mohamed, and daan wierstra. stochastic id26 and approxi-

mate id136 in deep generative models. in icml, 2014.

[12] charles blundell, julien cornebise, koray kavukcuoglu, and daan wierstra. weight uncertainty in neural

network. in icml, 2015.

[13] jose miguel hernandez-lobato and ryan adams. probabilistic id26 for scalable learning of

bayesian neural networks. in icml, 2015.

[14] yarin gal and zoubin ghahramani. bayesian convolutional neural networks with bernoulli approximate

variational id136. arxiv:1506.02158, 2015.

[15] diederik kingma, tim salimans, and max welling. variational dropout and the local reparameterization

trick. in nips. curran associates, inc., 2015.

[16] anoop korattikara balan, vivek rathod, kevin p murphy, and max welling. bayesian dark knowledge.

in nips. curran associates, inc., 2015.

[17] yarin gal and zoubin ghahramani. dropout as a bayesian approximation: representing model uncertainty

in deep learning. arxiv:1506.02142, 2015.

[18] sepp hochreiter and j  rgen schmidhuber. long short-term memory. neural computation, 9(8), 1997.
[19] kyunghyun cho et al. learning phrase representations using id56 encoder   decoder for statistical machine

translation. in emnlp, doha, qatar, october 2014. acl.

[20] taesup moon, heeyoul choi, hoshik lee, and inchul song. id56drop: a novel dropout for id56s in

asr. in asru workshop, december 2015.

[21] david jc mackay. a practical bayesian framework for id26 networks. neural computation, 4

(3):448   472, 1992.

[22] radford m neal. bayesian learning for neural networks. phd thesis, university of toronto, 1995.
[23] geoffrey e hinton and drew van camp. keeping the neural networks simple by minimizing the description

length of the weights. in colt, pages 5   13. acm, 1993.

[24] david barber and christopher m bishop. id108 in bayesian neural networks. nato asi

series f computer and systems sciences, 168:215   238, 1998.

[25] alex graves. practical variational id136 for neural networks. in nips, 2011.
[26] alan graves, abdel-rahman mohamed, and geoffrey hinton. id103 with deep recurrent

neural networks. in icassp. ieee, 2013.

[27] bo pang and lillian lee. seeing stars: exploiting class relationships for sentiment categorization with

respect to rating scales. in acl. acl, 2005.

[28] diederik kingma and jimmy ba. adam: a method for stochastic optimization. arxiv preprint

arxiv:1412.6980, 2014.

[29] james bergstra et al. theano: a cpu and gpu math expression compiler. in proceedings of the python

for scienti   c computing conference (scipy), june 2010. oral presentation.

[30] fchollet. keras. https://github.com/fchollet/keras, 2015.

10

a bayesian versus ensembling interpretation of dropout

apart from our bayesian approximation interpretation, dropout in deep networks can also be seen
as following an ensembling interpretation [6]. this interpretation also leads to mc dropout at test
time. but the ensembling interpretation does not determine whether the ensemble should be over the
network units or the weights. for example, in an id56 this view will not lead to our dropout variant,
unless the ensemble is de   ned to tie the weights of the network ad hoc. this is in comparison to the
bayesian approximation view where the weight tying is forced by the probabilistic interpretation of
the model.

b id31     further experiments

id31 hyper-parameters were obtained by evaluating each model with dropout probabili-
ties 0.25 and 0.5, and weight decays ranging from 10   6 to 10   4. the optimal setting for variational
lstm is dropout probabilities 0.25 and weight decay 10   3, and for naive dropout lstm the dropout
probabilities are 0.5 (no weight decay is used in reference implementations of naive dropout lstm
[4]).
we assess the dropout approximation in variational lstms. the dropout approximation is often
used in deep networks as means of approximating the mc estimate. in the approximation we replace
each weight matrix m by pm where p is the dropout id203, and perform a deterministic pass
through the network (without dropping out units). this can be seen as propagating the mean of the
random variables w through the network [17]. the approximation has been shown to work well for
deep networks [6], yet it fails with convolution layers [14]. we assess the approximation empirically
with our variational lstm model, repeating the    rst experiment with the approximation used at test
time instead of mc dropout. the results can be seen in    g. 9. it seems that the approximation gives a
good estimate to the test error, similar to the results in    gure 4a.
we further tested the variational lstm model with different weight decays, observing the effects of
different values for these. note that weight decay is applied to all layers, including the embedding
layer. in    gure 6 we can see that higher weight decay values result in lower test error, with signi   cant
differences for different weight decays. this suggests that weight decay still plays an important role
even when using dropout (whereas common practice is to remove weight decay with naive dropout).
note also that the weight decay can be optimised (together with the dropout parameters) as part of
the variational approximation. this is not done in practice in the deep learning community, where
grid-search or bayesian optimisation are often used for these instead.
testing the variational lstm with different sequence lengths (with sequences of lengths t =
20, 50, 200, 400) we can see that sequence length has a strong effect on model performance as well
(   g. 7). longer sequences result in much better performance but with the price of longer convergence
time. we hypothesised that the diminished performance on shorter sequences is caused by the high
dropout id203 on the embeddings. but a follow-up experiment with sequence lengths 50 and
200, and different embedding dropout probabilities, shows that lower dropout probabilities result in
even worse model performance (   gures 8 and 5).
in    g. 10a we see how different dropout probabilities and weight decays affect gru model perfor-
mance.

figure 5: pe = 0, ..., 0.5 with
   xed pu = 0.5.

figure 6: test error for vari-
ational lstm with different
weight decays.

11

7:

figure
lstm test error
ferent
sequence
(t
=
cut-offs).

variational
for dif-
lengths
20, 50, 200, 400

figure 8: test error for various
embedding dropout probabili-
ties, with sequence length 50.

figure 9: dropout approxima-
tion in variational lstm with
different dropout probabilities.

(a) various variational gru model con   gurations

figure 10: id31 error for variational gru compared to naive dropout gru and
standard gru (with no dropout). test error for the different models (left) and for different varia-
tional gru con   gurations (right).

we compare naive dropout lstm to variational lstm with dropout id203 in the recurrent
layers set to zero: pu = 0 (referred to as dropout lstm). both models apply dropout to the input and
outputs of the lstm alone, with no dropout applied to the embeddings. naive dropout lstm uses
different masks at different time steps though, tied across the gates, whereas dropout lstm uses the
same mask at different time steps. the test error for both models can be seen in    g. 11. it seems that
without dropout over the recurrent layers and embeddings both models over   t, and in fact result in
identical performance.
next, we assess the dropout approximation in the gru model. the approximation seems to give
similar results to mc dropout in the gru model (   g. 12).

figure 11: naive dropout lstm uses different
dropout masks at each time step, whereas dropout
lstm uses the same mask at each time step. both
models apply dropout to the inputs and outputs
alone, and result in identical performance.

12

figure 12: gru dropout approximation

lastly, we plot the train loss for various models from the main body of the paper. all models have
converged, with a stable train loss.

figure 13: train loss (as a func-
tion of batches) for    gure 3a

figure 14: gru train loss (as a
function of batches) (   gure 14)

figure 15: train loss (as a func-
tion of batches) for    gure 4a

13

c code

an ef   cient theano [29] implementation of the method above into keras [30] is as simple as:

def get_output(self, train=false):

x = self.get_input(train)

retain_prob_w = 1. - self.p_w[0]
retain_prob_u = 1. - self.p_u[0]
b_w = self.srng.binomial((4, x.shape[1], self.input_dim),

p=retain_prob_w, dtype=theano.config.floatx)

b_u = self.srng.binomial((4, x.shape[1], self.output_dim),

p=retain_prob_u, dtype=theano.config.floatx)

xi = t.dot(x * b_w[0], self.w_i) + self.b_i
xf = t.dot(x * b_w[1], self.w_f) + self.b_f
xc = t.dot(x * b_w[2], self.w_c) + self.b_c
xo = t.dot(x * b_w[3], self.w_o) + self.b_o

[outputs, memories], updates = theano.scan(

self._step,
sequences=[xi, xf, xo, xc],
outputs_info=[

t.unbroadcast(alloc_zeros_matrix(x.shape[1], self.output_dim), 1),
t.unbroadcast(alloc_zeros_matrix(x.shape[1], self.output_dim), 1)

],
non_sequences=[self.u_i, self.u_f, self.u_o, self.u_c, b_u],
truncate_gradient=self.truncate_gradient)

return outputs[-1]

def _step(self,

xi_t, xf_t, xo_t, xc_t,
h_tm1, c_tm1,
u_i, u_f, u_o, u_c, b_u):

i_t = self.inner_activation(xi_t + t.dot(h_tm1 * b_u[0], u_i))
f_t = self.inner_activation(xf_t + t.dot(h_tm1 * b_u[1], u_f))
c_t = f_t * c_tm1 + i_t * self.activation(xc_t + t.dot(h_tm1 * b_u[2], u_c))
o_t = self.inner_activation(xo_t + t.dot(h_tm1 * b_u[3], u_o))
h_t = o_t * self.activation(c_t)
return h_t, c_t

14

