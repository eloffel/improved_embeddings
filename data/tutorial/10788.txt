end-to-end neural sentence ordering using pointer network

jingjing gong*, xinchi chen*, xipeng qiu, xuanjing huang

shanghai key laboratory of intelligent information processing, fudan university

school of computer science, fudan university

825 zhangheng road, shanghai, china

{jjgong15, xinchichen13, xpqiu, xjhuang}@fudan.edu.cn

6
1
0
2

 

v
o
n
5
2

 

 
 
]
l
c
.
s
c
[
 
 

2
v
3
5
9
4
0

.

1
1
6
1
:
v
i
x
r
a

abstract

sentence ordering is one of important tasks
in nlp. previous works mainly focused on
improving its performance by using pair-wise
strategy. however, it is nontrivial for pair-
wise models to incorporate the contextual sen-
tence information. in addition, error proroga-
tion could be introduced by using the pipeline
strategy in pair-wise models.
in this paper,
we propose an end-to-end neural approach to
address the sentence ordering problem, which
uses the pointer network (ptr-net) to alleviate
the error propagation problem and utilize the
whole contextual information. experimental
results show the effectiveness of the proposed
model. source codes1 and dataset2 of this pa-
per are available.

introduction

1
recently, sentence ordering task attracts more focus
in nlp community (chen et al., 2016; agrawal et
al., 2016; li and jurafsky, 2016) as its importance on
many succeed applications such as multi-document
summarization, etc.

the goal of sentence ordering is to arrange a set
of sentences into a coherent text in a clear and con-
sistent manner (grosz et al., 1995; van berkum et
al., 1999; barzilay and lapata, 2008).

most of previous researches of sentence order-
ing are pair-wise models. chen et al. (2016) and
agrawal et al. (2016) proposed pair-wise models

1https://github.com/fudannlp
2http://nlp.fudan.edu.cn/data/
   jingjing gong and xinchi chen contributed equally to this

work.

followed by a id125 decoder to seek the
ground truth sentence order. li and jurafsky (2016)
additionally employed the graph based method (la-
pata, 2003) to rank sentences. however, their meth-
ods could be easily affected by the performance of
independent sentence pairs without their contextual
sentences.

in this paper, we propose an end-to-end neural
sentence ordering approach based on the pointer net-
work (ptr-net) (vinyals et al., 2015). pointer net-
work can deal with some combinatorial optimization
problems with attention mechanism, such as sorting
the elements of a given set. our proposed model can
take a set of random sorted sentences as input, and
generate an ordered sequences. with pointer net-
work, our proposed model could utilize the whole
contextual information to specify the sort order. in
addition, we further evaluate the robustness of our
model by adding unrelated noisy sentence to the
sentence set, which is nontrivial for pair-wise mod-
els. experimental results on two datasets show that
proposed model achieves the state-of-the-art perfor-
mance even it only works with the greedy decoding.
the contributions of this paper can be summa-

rized as follows:

1. instead of pair-wise sentence ordering, we pro-
pose an end-to-end neural approach to generate
the order of sentences, which could exploit the
contextual information and alleviate the error
propagation problem.

2. we perform extensive empirical experiments
and achieve the state-of-the-art performance
even it only works with the greedy decoding.

3. we design a new and harder experiment to eval-

the model architecture is shown in figure 1.
speci   cally, the id203 of a speci   c order o of
given sentences p (o|s) could be formalized as:

p (o|s) =

p (oi|oi   1, . . . , o1, s).

(3)

n(cid:89)

i=1

(cid:124)(cid:20) ej

(cid:21)

the id203 p (oi|oi   1, . . . , o1, s) is calculated
by ptr-net:

p (oi|oi   1, . . . , o1, s) = softmax(ui)oi

(4)

(cid:124)

ui

di

j = v

tanh(w

), j = (1, . . . , n)

(5)
where ej, di     rh are outputs of encoder and de-
coder of ptr-net respectively. v     rh and w    
r(2h)  h are trainable parameters.
encoder the encoding representation of ptr-net
could be formalized as:
ej = lstm(enc(soj ), ej   1), j = (1, . . . , n),
(6)
where enc(soj ) indicates the encoding of sentence
soj . the sentence encoding function enc(  ) and
function lstm(  ) will be further interpreted in sec-
tion 2.3.

notably, the initial state of encoder is e0 = 0.

decoder similarity, the decoding representation
of ptr-net is formalized as:

di = lstm(enc(soi), di   1), i = (1, . . . , n).
notably, the initial state of decoder is d0 = en.

(7)

2.3 sentence encoding
since ptr-net receives    xed length vectors as inputs,
we need    rstly encode sentences with variational
length. inspired by chen et al. (2016), we tried three
types of encoders: continues bag of words (cbow),
convolutional neural networks (id98s) and long
short-term (lstm) neural networks.
2.3.1 continues bag of words

continues bag of words (cbow) model (mikolov
et al., 2013) simply averages the embeddings of
words of a sentence. formally, given the embed-
dings of nw words of a sentence s, w1, . . . , wnw,
the sentence embedding enc(s) is:

enc(s) =

1
nw

wk,

(8)

nw(cid:88)

k=1

figure 1: id193 for neural sentence or-
dering.

uate the robustness of our model. we add unre-
lated noisy sentences to the candidate set, and
wish the system can sort the sentences correctly
while discarding the noisy sentences.

2 pointer network for neural sentence

ordering

2.1 task description
sentence ordering task aims to rank a set of sen-
tences in a clear and consistent manner. speci   cally,
given n sentences s = s1, s2, . . . , sn, the aim is to
   nd the gold order o    for these sentences:

(cid:31) so   

2

(cid:31)        (cid:31) so   
n,

so   

1

(1)

which has the maximal id203 of given sen-
tences p (o   |s):

p (o   |s) > p (o|s),   o       ,

(2)

where o indicates any order of these sentences and
   indicates the set of all possible orders.

2.2 model architecture
previous works mainly focused on pair-wise mod-
els (chen et al., 2016; agrawal et al., 2016; li and
jurafsky, 2016), which lack contextual information
and introduce error propagation for the pipe line
strategy. instead of decomposing the score to inde-
pendent pairs, in this paper, we propose an end-to-
end neural approach based on pointer network (ptr-
net) to score the entire sequence of sentences.

>sentence encoders1s2s3s1s2s3<encoderdecoderwhere enc(s), wk     rde. de is a hyper-parameter,
indicating id27 size.
2.3.2 convolutional neural networks

convolutional neural networks (id98s) (simard
et al., 2003) are biologically-inspired variants of
multiple layer perceptions (mlps). formally, sen-
tence s with nw words could be encoded as:

covk =   (w
enc(s) = max

k

cov(   lf   1
(cid:124)
covk,

u=0 wk+u) + bcov),

(9)
(10)

where wcov     r(d  lf )  df and bcov     rdf
are trainable parameters, and   (  ) is tanh function.
here, k = 1, . . . , nw     lf + 1, and lf and df
are hyper-parameters indicating the    lter length and
number of feature maps respectively. notably, max
operation in eq (10) is a element-wise operation.
2.3.3 long short-term neural networks

long

neural

short-term (lstm)

networks
(hochreiter and schmidhuber, 1997) are advanced
recurrent neural networks (id56s), which alleviate
the problems of gradient vanishment and explosion.
formally, lstm has memory cells c     rdr con-
trolled by three kinds of gates: input gate i     rdr,
forget gate f     rdr and output gate o     rdr:

             it

ot
ft
  ct

             =

               

  
  
  

            (cid:18)

(cid:21)

(cid:124)(cid:20) wt

ht   1

wg

(cid:19)

ct = ct   1 (cid:12) ft +   ct (cid:12) it,
ht = ot (cid:12)   (ct),

(12)
(13)
where wg     r(d+dr)  4dr and bg     r4dr are train-
able parameters. dr is a hyper-parameter indicating
the cell unit size as well as gate unit size.   (  ) is
sigmoid function and   (  ) is tanh function. here,
t = 1, . . . , nw. thus, we would represent sentence
s as:

enc(s) = hnw .

(14)

2.4 order prediction
given p (o|s), the predicted order   o is the one with
highest id203:

  o = arg max

o

p (o|s).

(15)

since the decoding process, to    nd   o, is a np hard
problem. instead, we use two strategies to decode
a sub optimal result: greedy decoding and beam
search decoding.
greedy decoding at decoding phase of prt-net,
greedy strategy determines   o =   o1, . . . ,   on step by
step as:

  oi = arg max

oi

p (oi|  oi   1, . . . ,   o1, s).

(16)

id125 decoding id125 strategy al-
ways keeps top b terms as candidates each step. for-
mally, at step t, each candidate   ot
1 =   o1, . . . ,   ot has
a id203:

p (  ot

1|s) =

p (  oi|  oi   1, . . . ,   o1, s),

(17)

t(cid:89)

i=1

and b candidates with higher probabilities will be
kept at t step in the beam.
3 training
assuming that we have m training examples
i=1, where xi indicates a sequence of sen-
(xi, yi)m
tences with a speci   c permutation of yi, and yi is in
gold order o   . for obtaining more training data, we
randomly generate new permutation for xi at each
epoch. the goal is to minimize the id168
j(  ):

m(cid:88)

i=1

where p (yi|xi;   ) = p (o   |s = xi;   ) and    is a
hyper-parameter of id173 term.    indicates
all trainable parameters.

in addition, we use adagrad (duchi et al., 2011)
with shuf   ed mini-batch to train our model. we also
use pre-trained embeddings (turian et al., 2010) as
initialization.
4 experiments
4.1 datasets
to evaluate proposed model, we adopt two datasets:
abstracts on arxiv (chen et al., 2016) and sind
(sequential image narrative dataset) (ferraro et al.,
2016). examples in sind dataset all contain 5 sen-
tences, and we only use captions in this paper. the
details of two datasets are shown in table 1.

+ bg

, (11)

j(  ) =     1
m

log p (yi|xi;   ) +

(cid:107)  (cid:107)2
2,

  
2

(18)

models

(chen et al., 2016)
(agrawal et al., 2016)
cbow+ptr-net
id98+ptr-net
lstm+ptr-net
+id125
cbow+ptr-net
id98+ptr-net
lstm+ptr-net

arxiv
pm lsr
82.97

-
-

78.47
81.49
82.15

78.69
81.85
82.40

-

81.93
85.00
85.62

82.10
85.26
85.79

pmr
33.43

-

33.09
38.62
40.00

33.43
39.28
40.44

sind
pm lsr
-

-
-

69.69
71.03
72.45

71.45
72.39
72.52

73.20
70.69
72.17
74.10

72.77
74.21
74.17

pmr

-
-

09.70
11.08
12.36

11.45
12.32
12.34

table 3: performances of different models on test sets of datasets. since there is no noisy sentence here, we
constrain the number of output sentences as the same as input, and three scores of pm and lsr are the same
in this table.

attributes

arxiv

sind

train
dev
test
train
dev
test

n

884,912
110,614
110,615
40,155
4,990
5,055

savg wavg
134.58
5.38
5.39
134.80
134.58
5.37
56.71
57.49
56.31

5

table 1: details of datasets. n indicates the number
of texts. savg is the average sentence number per
text. wavg is the average word count per text.

initial learning rate

id173

filter length of id98

   = 0.5
   = 10   5
hidden layer size of ptr-net h = 200
lf = 3, 4, 5
df = 128
dr = 200
de = 100
b = 64

number of feature maps
hidden size of lstm

size of embedding

beam size
batch size

128

table 2: hyper-parameter con   gurations.

4.2 hyper-parameters
table 2 gives the details of hyper-parameter con   g-
urations. the id98 sentence encoder uses three dif-
ferent    lter lengths as kim et al. (2015).

4.3 metrics
to evaluate our model, we use three different met-
rics: (1) pairwise metrics; (2) longest sequence ra-

tio; (3) perfect match ratio.

pairwise metrics pairwise metrics (pm) is the
fraction of pairs of sentences whose predicted rel-
ative order is the same as the ground truth order
(higher is better). formally, pairwise metrics can be
denoted as three scores: precision p , recall r and
f -value.

m(cid:88)
m(cid:88)

i=1

|s(  oi)(cid:84) s(o   
|s(  oi)(cid:84) s(o   

|s(  oi)|

i )|

i )|

|s(o   
i )|

p =

1
m

r =

f =

i=1

1
m
2     p     r
p + r

,

,

,

(19)

(20)

(21)

where function s(  ) denotes the set of all skip bi-
gram sentence pairs of a text, and function |    | indi-
cates the size of set.
concretely, we take {  o = (2, 3, 1, 4), o    =
(1, 3, 4)} as an example, where 2nd sentence is a
noisy term. the pairwise scores of this example are:
p = 1/6, r = 1/3, f = 2/9.

longest sequence ratio longest sequence ra-
tio (lsr) calculates the radio of longest correct
sub-sequence (consecutiveness is not necessary, and
higher is better). formally, lsr can be denoted as
three scores: precision p , recall r and f -value.

m(cid:88)

i=1

p =

1
m

|l(  oi, o   
i )|
|  oi|

,

(22)

models

(chen et al., 2016) 84.85 62.37
cbow+ptr-net
id98+ptr-net
lstm+ptr-net
+id125
cbow+ptr-net
id98+ptr-net
lstm+ptr-net

arxiv

sind

head tail head tail
-

-

85.18 59.93 69.89 45.67
89.59 64.48 72.54 48.80
90.77 65.80 75.24 52.62

84.70 60.54 72.61 51.23
89.43 65.36 73.53 53.26
90.47 66.49 74.66 53.30

table 4: performances of different models on test
sets of datasets.

m(cid:88)

|l(  oi, o   
i )|
i|
|o   

r =

f =

i=1

1
m
2     p     r
p + r

,

,

(23)

(24)

where function l(  ) denotes the number of elements
in longest correct sub-sequence. the value of func-
tion l(  o = (2, 3, 1, 4), o    = (1, 3, 4)) of the exam-
ple above is 2.

perfect match ratio perfect match ratio (pmr)
calculates the radio of exactly matching case (higher
is better):

pmr =

1
m

1{  oi = o   
i}

(25)

m(cid:88)

i=1

4.4 results
results on different datasets are shown in table 3.
since there is no noisy sentence here and the num-
ber of output sentences is the same as inputs, three
scores of pm and lsr are the same in this table.
thus, we only use pm and lsr to denote the same
p, r, f values. as we can see, our model outper-
forms previous works, and achieves the state-of-the-
art performance. our model with lstm sentence
encoder reaches 40.00% in pmr, which means that
2/5 texts in test set of arxiv dataset are ranked ex-
actly right, 6.57% boosted compared with work of
chen et al. (2016). with id125 strategy, the
performance of our model with lstm sentence en-
coder is further boosted to 40.44% in prm on test
set of arxiv dataset.

moreover, we investigate the performance of our
model when sentence number varies. as chen et
al. (2016) did not evaluate their results on lsr met-
rics, we only compare with them on pm and pmr.
as shown in figure 2, our model with id98 and
lstm sentence encoders outperforms the work of
(chen et al., 2016). although the accuracy of our
model drops as the number of sentences increases,
we could    nd that our model performs better on text
with more sentences compared to pair-wise model.
since    rst and last sentences are more special, we
also evaluate the accuracy of    nding the    rst and last
sentences. as shown in table 4, we also achieve
signi   cant boost compared to work of (chen et al.,
2016). as we can see, by using id125, our
model obtains higher accuracy on    nding last sen-
tence whereas performance of seeking    rst sentence
drops. however, the performance in total is boosted
by using id125 according to the results shown
in table 3, which implies that id125 strategy
concentrates more on the entire text.

according to the experimental results above, we
could    nd that ptr-net with lstm sentence encoder
almost outperforms the one with cbow or id98
sentence encoder. thus, we mainly focus on eval-
uating our model with lstm sentence encoder in
the rest of this paper.

4.5 visualization
to further understand how our model works, we do
some visualizations of ptr-net with id98 and lstm
sentence encoders. as shown in table 5, the left
side (   rst three columns) blue terms are input sen-
tences. they are    rstly encoded by sentence encoder
(id98 or lstm sentence encoder), then sent to the
decoder of ptr-net. the right side (last column) red
term is the    rst sentence that our model predicts. af-
ter that, this red item is encoded and sent to prt-net
decoder to generate the 2nd sentence. this visual-
ization shows how important each word is in gen-
erating the 2nd sentence. the more important the
words are, the darker the color is. as we can see,
by using id98 sentence encoder, the model focuses
more on individual words, like       rst   ,    second   ,
which give strong signal for ordering. contrastly,
the model with lstm sentence encoder trends to
   nd a speci   c pattern (or compositional features). in
this case, the model emphasizes the pattern    our x

100

90

80

70

cbow
id98
lstm

chen et al.

100

80

60

cbow
id98
lstm

100

50

0

cbow
id98
lstm

chen et al.

10

2

2

6

8
4
# of sentences
(a) pm

10

2

6

8
4
# of sentences
(b) lsr

10

6

8
4
# of sentences
(c) pmr

figure 2: performances of different sentence encoders on different numbers of sentences on test set of arxiv
dataset using greedy decoding strategy.

question regarding   , where x could be ordinal num-
bers like    rst and second here. however, both of
them have some signals like words    indices   ,    al-
gorithm   , which might be disturbances and hard to
interpret.

there still a question remains. how could we de-
termine the importance of each word (the color)? in-
spired by the back-propagation strategy (erhan et
al., 2009; simonyan et al., 2013; li et al., 2015;
chen et al., 2016), which measures how much each
input unit contributes to the    nal decision, we can
approximate the importance of words by their    rst
derivatives. formally, we aim to rank n sentences in
gold order s1, . . . , sn. assuming the predicted order
would be   o and we are predicting the i-th sentence
s  oi, the importance of each word wj
k (k-th word in
j-th sentence s  oj ) is:

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)    p (  oi|  oi   1, . . . ,   o1, s)

   wj
k

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ,

a(wj

k) =

(26)

k is id27 of word wj

where wj
k. func-
tion |    | is the second order id172 opera-
tion. p (  oi|  oi   1, . . . ,   o1, s) is detailed in eq. 5. for
cbow sentence encoder, the gradients of words in a
sentence are the same for the simple average opera-
tion. thus, we only visualize ptr-net with id98 and
lstm sentence encoders.

4.6 noisy sentences
interestingly, we    nd our model could deal with
the case well when additional noisy sentence exists.
to evaluate the performance of our model on noisy

100

80

60

40

20

pmr

60

0

20
40
beam size

figure 4: pmr metrics on test set of sind dataset
using lstm sentence encoder without noise. data
are selected with the beam sizes of 1, 2, 4, 8, 16, 32,
64 respectively.

sentences, we compare three different strategies in
adding noises: (1) 0 noisy sentence (0 noise); (2) 1
noisy sentence (1 noise); (3) 1 noisy sentence with
50% id203 (0/1 noise). all noisy sentences
come from own datasets. since all texts in sind
dataset contain 5 sentences, it is easy for model to
tell if there is noisy sentence. thus, we only evalu-
ate our model on arxiv dataset as shown in table 6.
as mentioned above, we only use the model with
lstm sentence encoder to evaluate the ability of
our model on disambiguating noisy sentences, since
it always performs better than the one with cbow
or id98 sentence encoder.

although 0 noise version seems the same as the
case in table 3, they are actually different with each
other. in this section, 0 noise version do not con-
strain the length of predicted sequence to be the
same with input. in addition, the p, r, f values of

2
second
the

question
our
regarding
function
which computes minimal
indices is whether one can
compute a short
list of
candidate indices which
includes a minimal index
for a given program

2

the

our

second question
regarding
function
which computes minimal
indices is whether one can
compute a short
list of
candidate indices which
includes a minimal index
for a given program

id98 sentence encoder

3

we give some negative re-
sults and leave the possi-
bility of positive results as
open questions

1

our    rst question regard-
ing the set of minimal in-
dices is whether there ex-
ists an algorithm which
can correctly label 1 out of
k indices as either minimal
or non minimal

1

our    rst question regard-
ing the set of minimal in-
dices is whether there ex-
ists an algorithm which
can correctly label 1 out of
k indices as either minimal
or non minimal

encoding

lstm sentence encoder

decoding

3

we give some negative re-
sults and leave the possi-
bility of positive results as
open questions

1

our    rst question regard-
ing the set of minimal in-
dices is whether there ex-
ists an algorithm which
can correctly label 1 out of
k indices as either minimal
or non minimal

1

our    rst question regard-
ing the set of minimal in-
dices is whether there ex-
ists an algorithm which
can correctly label 1 out of
k indices as either minimal
or non minimal

encoding

decoding

table 5: case study. color indicates importance of words in order prediction. the more important the
words are, the darker the color is. in this case, models with id98 and lstm sentence encoders all make
correct predictions.

pm and lsr of 0 noise and 1 noise version are not
the same actually, and very little difference exists (4
numbers after decimal point). it is because that our
model is very good at eliminating the noise terms on
0 noise and 1 noise cases.

as we can see, our model performs best on 0 noise
case, which implies that it is easier than 1 noise and
0/1 noise cases for our model. however, it is hard
to say which one is more dif   cult among 1 noise
and 0/1 noise cases, as the performance on different
metrics are not consistent.

similarly,

the performance of our model

is
boosted by using id125 instead of greedy de-
coding.

4.7 potential oracle
in this section, we evaluate the potential of our
model by    guring out what we    nd in the beam.
with id125 strategy, we additionally obtain
candidates in beam where the ground truth might ap-
pear.

according to the further analysis of candidates
in beam, we    nd our model could hit ground truth

with a very high id203. as show in figure 3,
our model could obtain 69.03% in pmr with beam
size of 8 on test set of arxiv dataset without noisy
sentences, and performance is further boosted with
the larger beam size (82.78% in pmr with beam
size of 64 on test set of arxiv dataset without noisy
sentences). notably, the performance boosts faster
when beam size is smaller, which shows our model
could rank ground truth in a top position in beam
(the ground truth has higher score). additionally,
according to the results in figure 3, we could tell
that 0/1 noise case is a more dif   cult task for our
model as the performance are worse than the 1 noise
case. notably, the results of potential oracle under
pm and lsr metrics shown in figure 3 are using f-
value. when determining the best case in beam, we
take the one with the highest f-value of pm or lsr
metrics for each speci   c case.

moreover, to identify whether the high perfor-
mance in potential oracle is cause by the short texts
(arxiv dataset has lots of texts with a few sentences)
or not, we also evaluate the performance on sind
dataset (whose texts all contain 5 sentences). as

pm
r

85.62
81.82
83.40

85.79
82.28
84.04

p

85.62
81.82
83.05

85.79
82.28
83.44

f

85.62
81.82
83.22

85.79
82.28
83.74

p

82.15
80.47
81.27

82.40
80.92
81.68

lsr
r

82.15
80.47
81.46

82.40
80.92
81.98

f

82.15
80.47
81.36

82.40
80.92
81.83

pmr

40.00
36.62
35.75

40.44
37.33
36.75

0 noise
1 noise
0/1 noise
+id125
0 noise
1 noise
0/1 noise

table 6: performances of different con   guration of noise on test sets of arxiv dataset using prt-net with
lstm sentence encoder.

95

90

85

80

0 noise
1 noise
0/1 noise

60

95

90

85

0

20
40
beam size
(a) pm

0

20
40
beam size
(b) lsr

80

60

40

0 noise
1 noise
0/1 noise

60

0 noise
1 noise
0/1 noise

60

0

20
40
beam size
(c) pmr

figure 3: oracles on different noise con   gurations on test set of arxiv dataset using lstm sentence encoder.
data are selected with the beam sizes of 1, 2, 4, 8, 16, 32, 64 respectively.

shown in figure 4, the performance in pmr met-
rics on sind dataset is very similar to that on arxiv
dataset, and our model could obtain 94.01% in pmr
with beam size of 64 on test set of sind dataset
without noisy sentences.

thus, we believe that we could further boost our
performance a lot if we design a model to rerank the
candidates.

is determined by its adjacent sentence and learns
constraints on sentence order from a corpus of do-
main speci   c texts. okazaki et al. (2004) improved
chronological ordering by resolving antecedent sen-
tences of arranged sentences and combining topical
segmentation. bollegala et al. (2010) presented a
bottom-up approach to arrange sentences extracted
for id57.

5 related work

previous works on sentence ordering mainly focused
on the external and downstream applications, such
as id57 and discourse
coherence (van dijk, 1985; grosz et al., 1995;
van berkum et al., 1999; elsner et al., 2007; barzi-
lay and lapata, 2008). barzilay and elhadad (2002)
proposed two naive sentence ordering techniques,
such as majority ordering and chronological order-
ing, in the context of multi-document summariza-
tion. lapata (2003) proposed a probabilistic model
that assumes the id203 of any given sentence

recently, increasing number of researches stud-
ied sentence ordering using neural models (agrawal
et al., 2016; li and jurafsky, 2016; chen et al.,
2016). chen et al. (2016) framed sentence order-
ing as an isolated task and    rstly applied neural
methods on sentence ordering.
in addition, they
designed an interesting task of ordering the coher-
ent sentences from academic abstracts. agrawal et
al. (2016) focused on a very similar ordering task
which ranks image-caption pairs, additionally con-
sidering the image information. li and jurafsky
(2016) mainly applied neural models to judge if a
given text is coherent.

despite of their success, they are all pair-wise
based models, lack of contextual information. un-
like these work, we propose an end-to-end neural
model based on ptr-net to address sentence order-
ing problem. a few days ago, logeswaran et al.
(2016) proposed a similar model based on recurrent
neural networks to address sentence ordering prob-
lem. however, their work did not consider neither
the case that noisy sentences involve nor alternative
sentence encoders exist.

6 conclusions
sentence ordering is an important factor in natural
language generation and attracts increasing focus re-
cently. previous works are mainly based on pair-
wise learning framework, which do not take contex-
tual information into consideration and always lead
to error propagation for their pipeline learning strat-
egy. in this paper, we propose an end-to-end neural
model based on ptr-net to address sentence ordering
problem. experimental results show that our model
achieves the state-of-the-art performance even using
greedy decoding strategy.

in the future, we would like to further improve the
performance by reranking the candidates derived by
id125.

references
[agrawal et al.2016] harsh agrawal, arjun chan-
drasekaran, dhruv batra, devi parikh, and mohit
sort story: sorting jumbled im-
bansal.
arxiv preprint
ages and captions into stories.
arxiv:1606.07493.

2016.

[barzilay and elhadad2002] regina barzilay and noemie
elhadad. 2002. inferring strategies for sentence order-
ing in multidocument news summarization. journal of
arti   cial intelligence research, pages 35   55.

[barzilay and lapata2008] regina barzilay and mirella
lapata. 2008. modeling local coherence: an entity-
based approach. computational linguistics, 34(1):1   
34.

[bollegala et al.2010] danushka

naoaki
okazaki, and mitsuru ishizuka. 2010. a bottom-up
approach to sentence ordering for multi-document
information processing & manage-
summarization.
ment, 46(1):89   109.

bollegala,

[chen et al.2016] xinchi chen, xipeng qiu, and xuan-
jing huang. 2016. neural sentence ordering. arxiv
preprint arxiv:1607.06952.

[duchi et al.2011] john duchi, elad hazan, and yoram
singer. 2011. adaptive subgradient methods for on-
line learning and stochastic optimization. the journal
of machine learning research, 12:2121   2159.

[elsner et al.2007] micha elsner, joseph l austerweil,
and eugene charniak. 2007. a uni   ed local and
in hlt-
global model for discourse coherence.
naacl, pages 436   443.

[erhan et al.2009] dumitru erhan, yoshua bengio, aaron
courville, and pascal vincent.
2009. visualizing
higher-layer features of a deep network. university
of montreal, 1341.

ferraro,

[ferraro et al.2016] francis

nasrin
mostafazadeh,
ishan misra, aishwarya agrawal,
jacob devlin, ross girshick, xiaodong he, pushmeet
kohli, dhruv batra, c lawrence zitnick, et al. 2016.
visual storytelling. arxiv preprint arxiv:1604.03968.
[grosz et al.1995] barbara j grosz, scott weinstein, and
aravind k joshi. 1995. centering: a framework for
modeling the local coherence of discourse. computa-
tional linguistics, 21(2):203   225.

[hochreiter and schmidhuber1997] sepp hochreiter and
j  urgen schmidhuber. 1997. long short-term memory.
neural computation, 9(8):1735   1780.

[kim et al.2015] yoon kim, yacine jernite, david son-
2015. character-
arxiv preprint

tag, and alexander m rush.
aware neural
arxiv:1508.06615.

language models.

[lapata2003] mirella lapata. 2003. probabilistic text
structuring: experiments with sentence ordering.
in
proceedings of the 41st annual meeting on associ-
ation for computational linguistics-volume 1, pages
545   552.

[li and jurafsky2016] jiwei li and dan jurafsky. 2016.
neural net models for open-domain discourse coher-
ence. arxiv preprint arxiv:1606.01545.

[li et al.2015] jiwei li, xinlei chen, eduard hovy,
visualizing and un-
arxiv preprint

and dan jurafsky.
derstanding neural models in nlp.
arxiv:1506.01066.

2015.

[logeswaran et al.2016] lajanugen

logeswaran,
honglak lee, and dragomir radev.
2016.
sen-
tence ordering using recurrent neural networks. arxiv
preprint arxiv:1611.02654.

[mikolov et al.2013] tomas mikolov, kai chen, greg
corrado, and jeffrey dean. 2013. ef   cient estimation
of word representations in vector space. arxiv preprint
arxiv:1301.3781.

[okazaki et al.2004] naoaki okazaki, yutaka matsuo,
and mitsuru ishizuka. 2004. improving chronological
sentence ordering by precedence relation. in proceed-
ings of the 20th international conference on computa-
tional linguistics, page 750.

[simard et al.2003] patrice y simard, dave steinkraus,
and john c platt. 2003. best practices for convo-
lutional neural networks applied to visual document
analysis. in null, page 958. ieee.

[simonyan et al.2013] karen simonyan, andrea vedaldi,
and andrew zisserman.
2013. deep inside con-
volutional networks: visualising image classi   ca-
arxiv preprint
tion models and saliency maps.
arxiv:1312.6034.

[turian et al.2010] joseph turian, lev ratinov,

and
yoshua bengio. 2010. word representations: a simple
and general method for semi-supervised learning. in
proceedings of acl.

[van berkum et al.1999] jos ja van berkum, peter ha-
goort, and colin brown. 1999. semantic integration
in sentences and discourse: evidence from the n400.
cognitive neuroscience, journal of, 11(6):657   671.

[van dijk1985] teun a van dijk. 1985. semantic dis-
course analysis. handbook of discourse analysis,
2:103   136.

[vinyals et al.2015] oriol vinyals, meire fortunato, and
in ad-
navdeep jaitly.
vances in neural information processing systems,
pages 2692   2700.

2015. id193.

