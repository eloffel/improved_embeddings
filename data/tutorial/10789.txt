3
1
0
2

 
r
p
a
8
2

 

 
 
]

.

o
l
h
t
a
m

[
 
 

2
v
3
2
8
5

.

4
0
3
1
:
v
i
x
r
a

towards a formal id65:

simulating logical calculi with tensors

edward grefenstette
university of oxford

department of computer science

wolfson building, parks road

oxford ox1 3qd, uk

edward.grefenstette@cs.ox.ac.uk

abstract

the development of compositional distribu-
tional models of semantics reconciling the em-
pirical aspects of id65 with
the compositional aspects of formal seman-
tics is a popular topic in the contemporary lit-
erature. this paper seeks to bring this rec-
onciliation one step further by showing how
the mathematical constructs commonly used
in compositional distributional models, such
as tensors and matrices, can be used to sim-
ulate di   erent aspects of predicate logic.
this paper discusses how the canonical iso-
morphism between tensors and multilinear
maps can be exploited to simulate a full-blown
quanti   er-free predicate calculus using ten-
sors. it provides tensor interpretations of the
set of logical connectives required to model
propositional calculi.
it suggests a variant
of these tensor calculi capable of modelling
quanti   ers, using few non-linear operations.
it    nally discusses the relation between these
variants, and how this relation should consti-
tute the subject of future work.

introduction

1
the topic of compositional id65
has been growing in popularity over the past few
years. this emerging sub-   eld of natural language
semantic modelling seeks to combine two seemingly
orthogonal approaches to modelling the meaning of
words and sentences, namely formal semantics and
id65.

these approaches, summarised in section 2, dif-
fer in that formal semantics, on the one hand, pro-

vides a neatly compositional picture of natural lan-
guage meaning, reducing sentences to logical rep-
resentations; one the other hand, distributional se-
mantics accounts for the ever-present ambiguity and
polysemy of words of natural language, and pro-
vides tractable ways of learning and comparing
word meanings based on corpus data.

recent e   orts, some of which are brie   y re-
ported below, have been made to unify both of
these approaches to language modelling to pro-
duce compositional distributional models of seman-
tics, leveraging the learning mechanisms of distri-
butional semantics, and providing syntax-sensitive
operations for the production of representations of
sentence meaning obtained through combination of
corpus-inferred word meanings. these e   orts have
been met with some success in evaluations such
as phrase similarity tasks (mitchell and lapata,
2008; mitchell and lapata, 2009; grefenstette and
sadrzadeh, 2011; kartsaklis et al., 2012), sentiment
prediction (socher et al., 2012), and paraphrase de-
tection (blacoe and lapata, 2012).

representations of

while these developments are promising with
regard to the goal of obtaining learnable-yet-
structured sentence-level
lan-
guage meaning, part of the motivation for unifying
formal and distributional models of semantics has
been lost. the compositional aspects of formal se-
mantics are combined with the corpus-based empir-
ical aspects of id65 in such mod-
els, yet the logical aspects are not. but it is these
logical aspects which are so appealing in formal se-
mantic models, and therefore it would be desirable
to replicate the inferential powers of logic within

compositional distributional models of semantics.

in this paper, i make steps towards addressing this
lost connection with logic in compositional distri-
butional semantics. in section 2, i provide a brief
overview of formal and distributional semantic mod-
els of meaning.
in section 3, i give mathemati-
cal foundations for the rest of the paper by intro-
ducing tensors and tensor contraction as a way of
modelling multilinear functions. in section 4, i dis-
cuss how predicates, relations, and logical atoms
of a quanti   er-free predicate calculus can be mod-
elled with tensors.
in section 5, i present tenso-
rial representations of logical operations for a com-
plete propositional calculus. in section 6, i discuss
a variant of the predicate calculus from section 4
aimed at modelling quanti   ers within such tensor-
based logics, and the limits of compositional for-
malisms based only on multilinear maps.
i con-
clude, in section 7, by suggesting directions for fur-
ther work based on the contents of this paper.

this paper does not seek to address the question
of how to determine how words should be trans-
lated into predicates and relations in the    rst place,
but rather shows how such predicates and relations
can be modelled using multiid202. as such,
it can be seen as a general theoretical contribution
which is independent from the approaches to com-
positional id65 it can be applied
to. it is directly compatible with the e   orts of co-
ecke et al. (2010) and grefenstette et al. (2013), dis-
cussed below, but is also relevant to any other ap-
proach making use of tensors or matrices to encode
semantic relations.

2 related work

formal semantics, from the montagovian school of
thought (montague, 1974; dowty et al., 1981), treats
natural languages as programming languages which
compile down to some formal language such as a
predicate calculus. the syntax of natural languages,
in the form of a grammar, is augmented by seman-
tic interpretations, in the form of expressions from
a higher order logic such as the lambda-beta calcu-
lus. the parse of a sentence then determines the
combinations of lambda-expressions, the reduction
of which yields a well-formed formula of a predi-
cate calculus, corresponding to the semantic repre-

sentation of the sentence. a simple formal semantic
model is illustrated in figure 1.

syntactic analysis
s     np vp
np     cats, milk, etc.
vp     vt np
vt     like, hug, etc.

semantic interpretation
[[vp]]([[np]])
[[cats]], [[milk]], . . .
[[vt]]([[np]])
  yx.[[like]](x, y), . . .

[[like]]([[cats]], [[milk]])

[[cats]]

  x.[[like]](x, [[milk]])

  yx.[[like]](x, y)

[[milk]]

figure 1: a simple formal semantic model.

formal semantic models are incredibly powerful,
in that the resulting logical representations of sen-
tences can be fed to automated theorem provers to
perform textual id136, consistency veri   cation,
id53, and a host of other tasks which
are well developed in the literature (e.g. see (love-
land, 1978) and (fitting, 1996)). however, the so-
phistication of such formal semantic models comes
at a cost:
the complex set of rules allowing for
the logical interpretation of text must either be pro-
vided a priori, or learned. learning such represen-
tations is a complex task, the di   culty of which is
compounded by issues of ambiguity and polysemy
which are pervasive in natural languages.

in contrast, distributional semantic models, best
summarised by the dictum of firth (1957) that    you
shall know a word by the company it keeps,    pro-
vide an elegant and tractable way of learning se-
mantic representations of words from text. word
meanings are modelled as high-dimensional vectors
in large semantic vector spaces, the basis elements
of which correspond to contextual features such as
other words from a lexicon. semantic vectors for
words are built by counting how many time a target
word occurs within a context (e.g. within k words
of select words from the lexicon). these context
counts are then normalised by a term frequency-
inverse document frequency-like measure (e.g. tf-
idf, pointwise mutual information, ratio of proba-
bilities), and are set as the basis weights of the vec-
tor representation of the word   s meaning. word vec-
tors can then be compared using geometric distance

furry

cat

dog

pet

stroke

snake

figure 2: a simple distributional semantic model.

metrics such as cosine similarity, allowing us to de-
termine the similarity of words, cluster semantically
related words, and so on. excellent overviews of dis-
tributional semantic models are provided by curran
(2004) and mitchell (2011). a simple distributional
semantic model showing the spacial representation
of words    dog   ,    cat    and    snake    within the context
of feature words    pet   ,    furry   , and    stroke    is shown
in figure 2.

distributional semantic models have been suc-
cessfully applied to tasks such as word-sense
discrimination (sch  utze, 1998),
thesaurus extrac-
tion (grefenstette, 1994), and automated essay
marking (landauer and dumais, 1997). however,
while such models provide tractable ways of learn-
ing and comparing word meanings, they do not natu-
rally scale beyond word length. as recently pointed
out by turney (2012), treating larger segments of
texts as lexical units and learning their representa-
tions distributionally (the    holistic approach   ) vio-
lates the principle of linguistic creativity, according
to which we can formulate and understand phrases
which we   ve never observed before, provided we
know the meaning of their parts and how they are
combined. as such, id65 makes
no e   ort to account for the compositional nature of
language like formal semantics does, and ignores is-
sues relating to syntactic and relational aspects of
language.

several proposals have been put forth over the
last few years to provide vector composition func-
tions for distributional models in order to introduce
compositionality, thereby replicating some of the as-

pects of formal semantics while preserving learn-
ability. simple operations such as vector addition
and multiplication, with or without scalar or matrix
weights (to take word order or basic relational as-
pects into account), have been suggested (zanzotto
et al., 2010; mitchell and lapata, 2008; mitchell and
lapata, 2009).

smolensky (1990) suggests using the tensor prod-
uct of word vectors to produce representations that
grow with sentence complexity. clark and pulman
(2006) extend this approach by including basis vec-
tors standing for dependency relations into tensor
product-based representations. both of these ten-
sor product-based approaches run into dimensional-
ity problems as representations of sentence mean-
ing for sentences of di   erent lengths or grammati-
cal structure do not live in the same space, and thus
cannot directly be compared. coecke et al. (2010)
develop a framework using category theory, solving
this dimensionality problem of tensor-based models
by projecting tensored vectors for sentences into a
unique vector space for sentences, using functions
dynamically generated by the syntactic structure of
the sentences. in presenting their framework, which
partly inspired this paper, they describe how a verb
can be treated as a logical relation using tensors in
order to evaluate the truth value of a simple sentence,
as well as how negation can be modelled using ma-
trices.

a related approach, by baroni and zamparelli
(2010), represents unary relations such as adjectives
as matrices learned by id75 from cor-
pus data, and models adjective-noun composition
as matrix-vector multiplication. grefenstette et al.
(2013) generalise this approach to relations of any
arity and relate it to the framework of coecke et al.
(2010) using a tensor-based approach to formal se-
mantic modelling similar to that presented in this pa-
per.

finally, socher et al. (2012) apply deep learning
techniques to model syntax-sensitive vector compo-
sition using non-linear operations, e   ectively turn-
ing parse trees into multi-stage neural networks.
socher shows that the non-linear activation func-
tion used in such a neural network can be tailored to
replicate the behaviour of basic logical connectives
such as conjunction and negation.

3 tensors and multilinear maps
tensors are the mathematical objects dealt with in
multiid202 just as vectors and matrices are
the objects dealt with in id202. in fact, ten-
sors can be seen as generalisations of vectors and
matrices by introducing the notion of tensor rank.
let the rank of a tensor be the number of indices re-
quired to describe a vector/matrix-like object in sum
i }i can
notation. a vector v in a space v with basis {bv
be written as the weighted sum of the basis vectors:

(cid:88)

i

v =

cv
i bv
i

where the cv
i elements are the scalar basis weights
of the vector. being fully described with one index,
vectors are rank 1 tensors. similarly, a matrix m is
an element of a space v     w with basis {(bv
j )}i j
(such pairs of basis vectors of v and w are com-
monly written as {bv
j }i j in multiid202).
such matrices are rank 2 tensors, as they can be fully
described using two indices (one for rows, one for
columns):

i     bw
(cid:88)

i , bw

i j bv
cm

i     bw

j

m =

i j

i j are just the i jth ele-

where the scalar weights cm
ments of the matrix.
a tensor t of rank k is just a geometric object with
a higher rank. let t be a member of v1   . . .   vk; we
can express t as follows, using k indices   1 . . .   k:

t =

ct
  1...  kbv1

  1     . . .     bvk

  k

  1...  k

in this paper, we will be dealing with tensors of rank
1 (vectors), rank 2 (matrices) and rank 3, which can
be pictured as cuboids (or a matrix of matrices).

tensor contraction is an operation which allows
us to take two tensors and produce a third. it is a
generalisation of inner products and matrix multipli-
cation to tensors of higher ranks. let t be a tensor in
v1   . . .   v j   vk and u be a tensor in vk   vm   . . .   vn.
the contraction of these tensors, written t    u, cor-
responds to the following calculation:
t    u =(cid:88)

    . . .     bv j

   j     bvm

  m

    . . .     bvn

  n

ct
  1...  kcu

  k...  nbv1

  1

  1...  n

tensor contraction takes a tensor of rank k and a
tensor of rank n     k + 1 and produces a tensor of

(cid:88)

rank n     1, corresponding to the sum of the ranks of
the input tensors minus 2. the tensors must satisfy
the following restriction: the left tensor must have
a rightmost index spanning the same number of di-
mensions as the leftmost index of the right tensor.
this is similar to the restriction that a m by n matrix
can only be multiplied with a p by q matrix if n = p,
i.e. if the index spanning the columns of the    rst ma-
trix covers the same number of columns as the index
spanning the rows of the second matrix covers rows.
similarly to how the columns of one matrix    merge   
with the rows of another to produce a third matrix,
the part of the    rst tensor spanned by the index k
merges with the part of the second tensor spanned by
k by    summing through    the shared basis elements
bvk
  k of each tensor. each tensor therefore loses a
rank while being joined, explaining how the tensor
produced by t  u is of rank k+(n   k+1)   2 = n   1.
there exists an isomorphism between tensors and
multilinear maps (bourbaki, 1989; lee, 1997), such
that any curried multilinear map

f : v1     . . .     v j     vk

can be represented as a tensor t f     vk   v j    . . .   v1
(note the reversed order of the vector spaces), with
tensor contraction acting as function application.
this isomorphism guarantees that there exists such a
tensor t f for every f , such that the following equal-
ity holds for any v1     v1, . . . , v j     v j:

f v1 . . . v j = vk = t f    v1    . . .    v j

4 tensor-based predicate calculi

in this section, i discuss how the isomorphism be-
tween multilinear maps and tensors described above
can be used to model predicates, relations, and log-
ical atoms of a predicate calculus. the four aspects
of a predicate calculus we must replicate here us-
ing tensors are as follows: truth values, the logical
domain and its elements (logical atoms), predicates,
and relations. i will discuss logical connectives in
the next section.

both truth values and domain objects are the ba-
sic elements of a predicate calculus, and therefore
it makes sense to model them as vectors rather than
higher rank tensors, which i will reserve for rela-
tions. we    rst must consider the vector space used

to model the boolean truth values of b. coecke et al.
(2010) suggest, as boolean vector space, the space b
with the basis {(cid:62),   }, where (cid:62) = [1 0](cid:62) is inter-
preted as    true   , and     = [0 1](cid:62) as    false   .
i assign to the domain d, the set of objects in
our logic, a vector space d on r|d| with basis vec-
tors {di}i which are in bijective correspondence with
elements of d. an element of d is therefore rep-
resented as a one-hot vector in d, the single non-
null value of which is the weight for the basis vector
mapped to that element of d. similarly, a subset of
d is a vector of d where those elements of d in the
subset have 1 as their corresponding basis weights in
the vector, and those not in the subset have 0. there-
fore there is a one-to-one correspondence between
the vectors in d and the elements of the power set
p(d), provided the basis weights of the vectors are
restricted to one of 0 or 1.
each unary predicate p in the logic is represented
in the logical model as a set mp     d containing the
elements of the domain for which the predicate is
true. predicates can be viewed as a unary function
fp : d     b where

    otherwise

(cid:40) (cid:62) if x     mp
                     (cid:88)
                      +

i

fp(x) =

                     (cid:88)

i

these predicate functions can be modelled as rank 2
tensors in b     d, i.e. matrices. such a matrix mp is
expressed in sum notation as follows:

mp =

1i (cid:62)     di
cmp

2i         di
cmp

                     

the basis weights are de   ned in terms of the set mp
as follows: cmp
= 1 if the logical atom xi associ-
1i
ated with basis weight di is in mp, and 0 otherwise;
conversely, cmp
= 1 if the logical atom xi associated
2i
with basis weight di is not in mp, and 0 otherwise.
to give a simple example, let   s consider a do-
main with three individuals, represented as the fol-
john = [1 0 0](cid:62),
lowing one-hot vectors in d:
chris = [0 1 0](cid:62), and tom = [0 0 1](cid:62). let   s
imagine that chris and john are mathematicians, but
tom is not. the predicate p for    is a mathemati-
cian    therefore is represented model-theoretically as
the set mp = {chris, john}. translating this into a
matrix gives the following tensor for p:

(cid:34) 1

0

mp =

1
0

0
1

(cid:35)

to compute the truth value of    john is a mathemati-
cian   , we perform predicate-argument application as
tensor contraction (matrix-vector multiplication, in
this case):

0

(cid:34) 1
(cid:34) 1

0

1
0

(cid:35)                               0
(cid:35)                               0

0
1

                               =
                               =

0

(cid:34) 1
(cid:34) 0

1

(cid:35)

= (cid:62)

(cid:35)

=    

1
0

0
1

1
0

0
1

mp    john =

mp    tom =

likewise for    tom is a mathematician   :

model theory for predicate calculus represents
any n-ary relation r, such as a verb, as the set mr
of n-tuples of elements from d for which r holds.
therefore such relations can be viewed as functions
fr : dn     b where:

fr(x1, . . . , xn) =

we can represent the boolean function for such a re-
lation r as a tensor tr in b     d     . . .     d
:

(cid:40) (cid:62) if (x1, . . . , xn)     mr
(cid:124)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:123)(cid:122)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:125)

    otherwise

n

ct r
1  1...  n

(cid:62)     d  1     . . .     d  n

ct r
2  1...  n

        d  1     . . .     d  n

                        
                        

                        (cid:88)
                        (cid:88)

  1...  n

  1...  n

tr =

+

1  1...  n

as was the case for predicates, the weights for re-
lational tensors are de   ned in terms of the set mod-
elling the relation: ct r
is 1 if the tuple (x, . . . , z)
associated with the basis vectors d  n . . . d  1 (again,
note the reverse order) is in mr and 0 otherwise; and
ct r
is 1 if the tuple (x, . . . , z) associated with
2  1...  n
the basis vectors d  n . . . d  1 is not in mr and 0 oth-
erwise.

to give an example involving relations, let our
domain be the individuals john ( j) and mary (m).
mary loves john and herself, but john only loves
himself. the logical model for this scenario is as
follows:

d = { j, m}

mloves = {( j, j), (m, m), (m, j)}

distributionally speaking, the elements of the do-
main will be mapped to the following one-hot vec-
tors in some two-dimensional space d as follows:

j = [1 0](cid:62) and m = [0 1](cid:62). the tensor for    loves   
can be written as follows, ignoring basis elements
with null-valued basis weights, and using the dis-
tributivity of the tensor product over addition:
tloves = (cid:62)     ((d1     d1) + (d2     d2) + (d1     d2))

+ (        d2     d1)

computing    mary loves john    would correspond to
the following calculation:
(tloves    m)    j =

(((cid:62)     d2) + ((cid:62)     d1))    j = (cid:62)

whereas    john loves mary    would correspond to the
following calculation:

(tloves    j)    m =

(((cid:62)     d1) + (        d2))    m =    
5 logical connectives with tensors

in this section, i discuss how the boolean connec-
tives of a propositional calculus can be modelled us-
ing tensors. combined with the predicate and rela-
tion representations discussed above, these form a
complete quanti   er-free predicate calculus based on
tensors and tensor contraction.

negation has already been shown to be modelled
in the boolean space described earlier by coecke et
al. (2010) as the swap matrix:

this can easily be veri   ed:

(cid:35)

1

(cid:34) 0
(cid:35)(cid:34) 1
(cid:35)(cid:34) 0

0

1

1
0
1
0

1
0

(cid:35)
(cid:35)

t   =

(cid:34) 0
(cid:34) 0

1

1

(cid:35)
(cid:35)

(cid:34) 0
(cid:34) 1

1

0

=

=

=    

= (cid:62)

t      (cid:62) =

t          =

all other logical operators are binary, and hence
modelled as rank 3 tensors. to make talking about
rank 3 tensors used to model binary operations eas-
ier, i will use the following block matrix notation for
2    2    2 rank 3 tensors t:
b1
d1

(cid:34) a1

a2
c2

b2
d2

t =

(cid:35)

c1

t    v =

which allows us to express tensor contractions as
follows:

=

  
  

b1
d1

(cid:35)(cid:34)

(cid:34) a1
(cid:34)
(cid:34) a1
(cid:34) a1

(cid:35)
a2
b2
c1
c2
d2
      a1 +       a2       b1 +       b2
      d1 +       d2
      c1 +       c2
(cid:35)
(cid:35)(cid:34) 1
(cid:35)
(cid:35)(cid:34) 0

(cid:34) a1
(cid:34) a2

c1

c1

0

=

b1
d1
b1
d1

a2
c2
a2
c2

=

1

c2

b2
d2
b2
d2

(cid:35)

b1
d1
b2
d2

(cid:35)
(cid:35)

or more concretely:
t    (cid:62) =

t        =

c1

using this notation, we can de   ne tensors for the

following operations:
(   ) (cid:55)    t    =

(   ) (cid:55)    t    =

(   ) (cid:55)    t    =

(cid:34) 1
(cid:34) 1
(cid:34) 1

0

0

0

(cid:35)
(cid:35)
(cid:35)

1
0
0
1
0
1

1
0
0
1
1
0

0
1
0
1
1
0

i leave the trivial proof by exhaustion that these    t
the bill to the reader.

it is worth noting here that these tensors pre-
serve normalised probabilities of truth. let us con-
sider a model such at that described in coecke et
al. (2010) which, in lieu of boolean truth values,
represents truth value vectors of the form [     ](cid:62)
where    +    = 1. applying the above logical op-
erations to such vectors produces vectors with the
same normalisation property. this is due to the fact
that the columns of the component matrices are all
normalised (i.e. each column sums to 1). to give
an example with conjunction, let v = [  1   1](cid:62) and
w = [  2   2](cid:62) with   1 +   1 =   2 +   2 = 1. the con-
junction of these vectors is calculated as follows:

(t       v)    w
0
1

0

=

(cid:34) 1
(cid:34)
(cid:34)

(cid:35)(cid:34)
(cid:35)(cid:34)

0
1

0
1
0

(cid:35)

  2
  2

(cid:35)(cid:34)
(cid:35)

  1
  1
  2
  2

(cid:35)

  1
  1   1 +   1
  1  2

=

=

  1  2 + (  1 +   1)  2

to check that the probabilities are normalised we
calculate:

  1  2 +   1  2 + (  1 +   1)  2
= (  1 +   1)  2 + (  1 +   1)  2
= (  1 +   1)(  2 +   2) = 1

we can observe that the resulting id203 distri-
bution for truth is still normalised. the same prop-
erty can be veri   ed for the other connectives, which
i leave as an exercise for the reader.

6 quanti   ers and non-linearity
the predicate calculus described up until this point
has repeatedly been quali   ed as    quanti   er-free   ,
for the simple reason that quanti   cation cannot be
modelled if each application of a predicate or rela-
tion immediately yields a truth value. in perform-
ing such reductions, we throw away the informa-
tion required for quanti   cation, namely the infor-
mation which indicates which elements of a domain
the predicate holds true or false for.
in this sec-
tion, i present a variant of the predicate calculus
developed earlier in this paper which allows us to
model simple quanti   cation (i.e. excluding embed-
ded quanti   ers) alongside a tensor-based approach
to predicates. however, i will prove that this ap-
proach to quanti   er modelling relies on non-linear
functions, rendering them non-suitable for compo-
sitional distributional models relying solely on mul-
tilinear maps for composition (or alternatively, ren-
dering such models unsuitable for the modelling of
quanti   ers by this method).

we saw, in section 4, that vectors in the seman-
tic space d standing for the logical domain could
model logical atoms as well as sets of atoms. with
this in mind, instead of modelling a predicate p as
a truth-function, let us now view it as standing for
some function fp : p(d)     p(d), de   ned as:

fp(x) = x     mp

where x is a set of domain objects, and mp is the set
modelling the predicate. the tensor form of such a
function will be some t fpin d     d. let this square
matrix be a diagonal matrix such that basis weights
ct fp
= 1 if the atom x corresponding to di is in mp
ii
and 0 otherwise. through tensor contraction, this

tensor maps subsets of d (elements of d) to subsets
of d containing only those objects of the original
subset for which p holds (i.e. yielding another vector
in d).

to give an example: let us consider a domain with
two dogs (a and b) and a cat (c). one of the dogs (b)
is brown, as is the cat. let s be the set of dogs, and p
the predicate    brown   . i represent these statements
in the model as follows:

d = {a, b, c} s = {a, b} mp = {b, c}

the set of dogs is represented as a vector s =
[1 1 0](cid:62) and the predicate    brown    as a tensor in
d     d:

                               0

0
0

                              

0
1
0

0
0
1

tp =

the set of brown dogs is obtained by computing
fb(s ), which distributionally corresponds to apply-
ing the tensor tp to the vector representation of s
via tensor contraction, as follows:

                               0

0
0

                              

                               1

1
0

                               =

                               0

1
0

                               = b

0
1
0

0
0
1

tp    s =

the result of this computation shows that the set of
brown dogs is the singleton set containing the only
brown dog, b. as for how logical connectives    t
into this picture, in both approaches discussed be-
low, conjunction and disjunction are modelled using
set-theoretic intersection and union, which are sim-
ply the component-wise min and max functions over
vectors, respectively.

using this new way of modelling predicates as
tensors, i turn to the problem of modelling quanti   -
cation. we begin by putting all predicates in vector
form by replacing each instance of the bound vari-
able with a vector 1    lled with ones, which extracts
the diagonal from the predicate matrix.

an intuitive way of modelling universal quanti   -
cation is as follows: expressions of the form    all xs
are ys    are true if and only if mx = mx    my, where
mx and my are the set of xs and the set of ys, re-
spectively. using this, we can de   ne the map forall
for distributional universal quanti   cation modelling
expressions of the form    all xs are ys    as follows:

forall(x, y) =

if x = min(x, y)
otherwise

(cid:40) (cid:62)

   

to give a short example, the sentence    all greeks are
human    is veri   ed by computing x = (mgreek    1),
y = (mhuman    1), and verifying the equality x =
min(x, y).

existential statements of the form    there exists
x    can be modelled using the function exists, which
tests whether or not mx is empty, and is de   ned as
follows:

(cid:40) (cid:62)

   

exists(x) =

if |x| > 0
otherwise

to give a short example, the sentence    there exists a
brown dog    is veri   ed by computing x = (mbrown   
1)     (mdog    1) and verifying whether or not x is of
strictly positive length.

an important point to note here is that neither of
these quanti   cation functions are multi-linear maps,
since a multilinear map must be linear in all argu-
ments. a counter example for forall is to consider
the case where mx and my are empty, and multi-
ply their vector representations by non-zero scalar
weights    and   .

  x = x
  y = y
forall(  x,   y) = forall(x, y) = (cid:62)
forall(  x,   y) (cid:44)     (cid:62)

i observe that the equations above demonstrate that
forall is not a multilinear map.

the proof that exists is not a multilinear map is
equally trivial. assume mx is an empty set and    is
a non-zero scalar weight:

  x = x
exists(  x) = exists(x) =    
exists(  x) (cid:44)      

it follows that exists is not a multi-linear function.

7 conclusions and future work
in this paper, i set out to demonstrate that it was
possible to replicate most aspects of predicate logic
using tensor-based models.
i showed that tensors
can be constructed from logical models to represent
predicates and relations, with vectors encoding ele-
ments or sets of elements from the logical domain.

i discussed how tensor contraction allows for evalu-
ation of logical expressions encoded as tensors, and
that logical connectives can be de   ned as tensors to
form a full quanti   er-free predicate calculus. i ex-
posed some of the limitations of this approach when
dealing with variables under the scope of quanti   ers,
and proposed a variant for the tensor representation
of predicates which allows us to deal with quanti   -
cation. further work on tensor-based modelling of
quanti   ers should ideally seek to reconcile this work
with that of barwise and cooper (1981). in this sec-
tion, i discuss how both of these approaches to pred-
icate modelling can be put into relation, and suggest
further work that might be done on this topic, and on
the topic of integrating this work into compositional
distributional models of semantics.

the    rst approach to predicate modelling treats
predicates as truth functions represented as tensors,
while the second treats them as functions from sub-
sets of the domain to subsets of the domain. yet both
representations of predicates contain the same infor-
mation. let mp and m(cid:48)p be the tensor represen-
tations of a predicate p under the    rst and second
approach, respectively. the relation between these
representations lies in the equality diag(pmp) =
m(cid:48)p, where p is the covector [1 0] (and hence pmp
yields the    rst row of mp). the second row of mp
being de   ned in terms of the    rst, one can also re-
cover mp from the diagonal of m(cid:48)p.

furthermore, both approaches deal with separate
aspects of predicate logic, namely applying predi-
cates to logical atoms, and applying them to bound
variables. with this in mind, it is possible to see how
both approaches can be used sequentially by noting
that tensor contraction allows for partial application
of relations to logical atoms. for example, apply-
ing a binary relation to its    rst argument under the
   rst tensor-based model yields a predicate. translat-
ing this predicate into the second model   s form using
the equality de   ned above then permits us to use it
in quanti   ed expressions. using this, we can eval-
uate expressions of the form    there exists someone
who john loves   . future work in this area should
therefore focus on developing a version of this ten-
sor calculus which permits seaid113ss transition be-
tween both tensor formulations of logical predicates.
finally, this paper aims to provide a starting point
for the integration of logical aspects into composi-

tional distributional semantic models. the work pre-
sented here serves to illustrate how tensors can sim-
ulate logical elements and operations, but does not
address (or seek to address) the fact that the vectors
and matrices in most compositional distributional
semantic models do not cleanly represent elements
of a logical domain. however, such distributional
representations can arguably be seen as represent-
ing the properties objects of a logical domain hold
in a corpus: for example the similar distributions of
   car    and    automobile    could serve to indicate that
these concepts are co-extensive. this suggests two
directions research based on this paper could take.
one could use the hypothesis that similar vectors in-
dicate co-extensive concepts to infer a (probabilis-
tic) logical domain and set of predicates, and use the
methods described above without modi   cation; al-
ternatively one could use the form of the logical op-
erations and predicate tensors described in this pa-
per as a basis for a higher-dimensional predicate cal-
culus, and investigate how such higher-dimensional
   logical    operations and elements could be de   ned
or learned. either way, the problem of reconciling
the fuzzy    messiness    of distributional models with
the sharp    cleanliness    of logic is a di   cult problem,
but i hope to have demonstrated in this paper that a
small step has been made in the right direction.

acknowledgments
thanks to ond  rej ryp  a  cek, nal kalchbrenner
and karl moritz hermann for their helpful com-
ments during discussions surrounding this pa-
per. this work is supported by epsrc project
ep/i03808x/1.

references
[baroni and zamparelli2010] m. baroni and r. zampar-
elli. nouns are vectors, adjectives are matrices:
representing adjective-noun constructions in seman-
tic space. in proceedings of the 2010 conference on
empirical methods in natural language processing,
pages 1183   1193. association for computational lin-
guistics, 2010.

[barwise and cooper1981] j. barwise and r. cooper
generalized quanti   ers and natural language. linguis-
tics and philosophy, pages 159   219. springer, 1981.

[blacoe and lapata2012] w. blacoe and m. lapata. a
comparison of vector-based representations for seman-

tic composition. proceedings of the 2012 conference
on empirical methods in natural language process-
ing, 2012.

[bourbaki1989] n. bourbaki.

commutative algebra:
chapters 1-7. springer-verlag (berlin and new york),
1989.

[clark and pulman2006] s. clark and s. pulman. com-
bining symbolic and distributional models of meaning.
in aaai spring symposium on quantum interaction,
2006.

[coecke et al.2010] b. coecke, m. sadrzadeh,

and
s. clark. mathematical foundations for a compo-
sitional distributional model of meaning. linguistic
analysis, volume 36, pages 345   384. march 2010.

[curran2004] j. r. curran. from distributional to seman-

tic similarity. phd thesis, 2004.

[dowty et al.1981] d. r. dowty, r. e. wall, and s. pe-
ters. introduction to montague semantics. dordrecht,
1981.

[firth1957] j. r. firth. a synopsis of linguistic theory

1930-1955. studies in linguistic analysis, 1957.

[fitting1996] m. fitting. id85 and automated

theorem proving. springer verlag, 1996.

[grefenstette et al.2013] e. grefenstette, g. dinu,
y. zhang, m. sadrzadeh, and m. baroni. multi-step
regression learning for compositional distributional
semantics. in proceedings of the tenth international
conference on computational semantics. association
for computational linguistics, 2013.

[grefenstette and sadrzadeh2011] e. grefenstette

and
m. sadrzadeh. experimental support for a categorical
compositional distributional model of meaning.
in
proceedings of the 2011 conference on empirical
methods in natural language processing, 2011.

[grefenstette1994] g. grefenstette. explorations in auto-

matic thesaurus discovery. 1994.

[kartsaklis et al.2012] d. kartsaklis, and m. sadrzadeh
and s. pulman. a uni   ed sentence space for cate-
gorical distributional-id152: the-
in proceedings of 24th in-
ory and experiments.
ternational conference on computational linguistics
(coling 2012): posters, 2012.

[landauer and dumais1997] t. k. landauer and s. t. du-
mais. a solution to plato   s problem: the latent seman-
tic analysis theory of acquisition, induction, and repre-
sentation of knowledge. psychological review, 1997.
[lee1997] j. lee. riemannian manifolds: an introduc-
tion to curvature, volume 176. springer verlag, 1997.
[loveland1978] d. w. loveland. automated theorem
proving: a logical basis. elsevier north-holland,
1978.

[mitchell and lapata2008] j. mitchell and m. lapata.
vector-based models of semantic composition. in pro-
ceedings of acl, volume 8, 2008.

[mitchell and lapata2009] j. mitchell and m. lapata.
language models based on semantic composition. in
proceedings of the 2009 conference on empirical
methods in natural language processing: volume 1-
volume 1, pages 430   439. association for computa-
tional linguistics, 2009.

[mitchell2011] j. j. mitchell. composition in distribu-

tional models of semantics. phd thesis, 2011.

[montague1974] r. montague. english as a formal lan-
guage. formal semantics: the essential readings,
1974.

[sch  utze1998] h. sch  utze. automatic word sense dis-
crimination. computational linguistics, 24(1):97   
123, 1998.

[smolensky1990] p. smolensky. tensor product variable
binding and the representation of symbolic structures
in connectionist systems. arti   cial intelligence, 46(1-
2):159   216, 1990.

[socher et al.2012] r. socher, b. huval, c.d. manning,
and a.y ng. semantic compositionality through re-
cursive matrix-vector spaces. proceedings of the 2012
conference on empirical methods in natural lan-
guage processing, pages 1201   1211, 2012.

[turney2012] p. d. turney. domain and function: a
dual-space model of semantic relations and compo-
journal of arti   cial intelligence research,
sitions.
44:533   585, 2012.

[zanzotto et al.2010] f. m. zanzotto,

i. korkontzelos,
f. fallucchi, and s. manandhar. estimating linear
models for compositional id65. in
proceedings of the 23rd international conference on
computational linguistics, pages 1263   1271. associ-
ation for computational linguistics, 2010.

