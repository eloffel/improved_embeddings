4
1
0
2

 
r
a

 

m
0
2

 
 
]
l
c
.
s
c
[
 
 

4
v
3
7
1
6

.

2
1
3
1
:
v
i
x
r
a

multilingual distributed representations without

word alignment

karl moritz hermann and phil blunsom

department of computer science

university of oxford
oxford, ox1 3qd, uk

{karl.moritz.hermann,phil.blunsom}@cs.ox.ac.uk

abstract

distributed representations of meaning are a natural way to encode covariance
relationships between words and phrases in nlp. by overcoming data sparsity
problems, as well as providing information about semantic relatedness which is
not available in discrete representations, distributed representations have proven
useful in many nlp tasks. recent work has shown how compositional semantic
representations can successfully be applied to a number of monolingual applica-
tions such as id31. at the same time, there has been some initial
success in work on learning shared word-level representations across languages.
we combine these two approaches by proposing a method for learning distributed
representations in a multilingual setup. our model learns to assign similar embed-
dings to aligned sentences and dissimilar ones to sentence which are not aligned
while not requiring word alignments. we show that our representations are seman-
tically informative and apply them to a cross-lingual document classi   cation task
where we outperform the previous state of the art. further, by employing parallel
corpora of multiple language pairs we    nd that our model learns representations
that capture semantic relationships across languages for which no parallel data
was used.

1

introduction

distributed representations of words are increasingly being used to achieve high levels of generalisa-
tion within language modelling tasks. successful applications of this approach include word-sense
disambiguation, word similarity and synonym detection (e.g. [10, 27]). subsequent work has also
attempted to learn distributed semantics of larger structures, allowing us to apply distributed rep-
resentation to tasks such as id31 or paraphrase detection (i.a. [1, 3, 12, 14, 21, 25]).
at the same time a second strand of work has focused on transferring linguistic knowledge across
languages, and particularly from english into low-resource languages, by means of distributed rep-
resentations at the word level [13, 16].
currently, work on compositional semantic representations focuses on monolingual data while the
cross-lingual work focuses on word level representations only. however, it appears logical that these
two strands of work should be combined as there exists a plethora of parallel corpora with aligned
data at the sentence level or beyond which could be exploited in such work. further, sentence aligned
data provides a plausible concept of semantic similarity, which can be harder to de   ne at the word
level. consider the case of alignment between a german compound noun (e.g.    schwerlastverkehr   )
and its english equivalent (   heavy goods vehicle traf   c   ). semantic alignment at the phrase level
here appears far more plausible than aligning individual tokens for semantic transfer.

1

using this rationale, and building on both work related to learning cross-lingual embeddings as well
as to compositional semantic representations, we introduce a model that learns cross-lingual em-
beddings at the sentence level. in the following section we will brie   y discuss prior work in these
two    elds before going on to describe the bilingual training signal that we developed for learning
multilingual compositional embeddings. subsequently, we will describe our model in greater detail
as well as its training procedure and experimental setup. finally, we perform a number of evalua-
tions and demonstrate that our training signal allows a very simple compositional vector model to
outperform the state of the art on a task designed to evaluate its ability to transfer semantic informa-
tion across languages. unlike other work in this area, our model does not require word aligned data.
in fact, while we evaluate our model on sentence aligned data in this paper, there is no theoretical
requirement for this and technically our algorithm could also be applied to document-level parallel
data or even comparable data only.

2 models of compositional distributed semantics

in the case of representing individual words as vectors, the distributional account of semantics pro-
vides a plausible explanation of what is encoded in a word vector. this follows the idea that the
meaning of a word can be determined by    the company it keeps    [11], that is by the context it ap-
pears in. such context can easily be encoded in vectors using collocational methods, and is also
underlying other methods of learning id27s [7, 20].
for a number of important problems, semantic representations of individual words do not suf   ce, but
instead a semantic representation of a larger structure   e.g. a phrase or a sentence   is required. this
was highlighted in [10], who proposed a mechanism for modifying a word   s representation based on
its individual context. the distributional account of semantics can, due to sparsity, not be applied to
such larger linguistic units. a notable exception perhaps is baroni and zamparelli [1], who learned
distributional representations for adjective noun pairs using a collocational approach on a corpus of
unprecedented size. the bigram representations learned from that corpus were subsequently used to
learn lexicalised composition functions for the constituent words.
most alternative attempts to extract such higher-level semantic representations have focused on
learning composition functions that represent the semantics of a larger structure as a function of
the representations of its parts. [21] provides an evaluation of a number of simple composition func-
tions applied to bigrams. applied recursively, such approaches can then easily be reconciled with
the co-occurrence based word level representations. there are a number of proposals motivating
such recursive or deep composition models. notably, [3] propose a tensor-based model for semantic
composition and, similarly, [4] develop a framework for semantic composition by combining dis-
tributional theory with pregroup grammars. the latter framework was empirically evaluated and
supported by the results in [12]. more recently, various forms of id56s have
successfully been used for semantic composition and related tasks such as id31. such
models include recursive autoencoders [24], matrix-vector id56s [25], untied
id56s [14] or convolutional networks [15].

2.1 multilingual embeddings

much research has been devoted to the task of inducing distributed semantic representations for
single languages. in particular english, with its large number of annotated resources, has enjoyed
most attention. recently, progress has been made at representation learning for languages with fewer
available resources. klementiev et al. [16] described a form of multitask learning on word-aligned
parallel data to transfer embeddings from one language to another. earlier work, haghighi et al.
[13], proposed a method for inducing cross-lingual lexica using monolingual feature representations
and a small initial lexicon to bootstrap with. this approach has recently been extended by [18, 19],
who developed a method for learning transformation matrices to convert semantic vectors of one
language into those of another. is was demonstrated that this approach can be applied to improve
tasks related to machine translation. their cbow model is also worth noting for its similarities to
the composition function used in this paper. using a slightly different approach, [29], also learned
bilingual embeddings for machine translation.
it is important to note that, unlike our proposed
system, all of these methods require word aligned parallel data for training.

2

two recent workshop papers deserve mention in this respect. both lauly et al. [17] and sarath chan-
dar et al. [23] propose methods for learning id27s by exploiting bilingual data, not unlike
the method proposed in this paper. instead of the noise-contrastive method developed in this paper,
both groups of authors make use of autoencoders to encode monolingual representations and to
support the bilingual transfer.
so far almost all of this work has been focused on learning multilingual representations at the word
level. as distributed representations of larger expressions have been shown to be highly useful for a
number of tasks, it seems to be a natural next step to also attempt to induce these using cross-lingual
data. this paper provides a    rst step in that direction.

3 model description

id146 in humans is widely seen as grounded in sensory-motor experience [22, 2].
based on this idea, there have been some attempts at using multi-modal data for learning better
vector representations of words (e.g. [26]). such methods, however, are not easily scalable across
languages or to large amounts of data for which no secondary or tertiary representation might exist.
we abstract the underlying principle one step further and attempt to learn semantics from multi-
lingual data. the idea is that, given enough parallel data, a shared representation would be forced
to capture the common elements between sentences from different languages. what two parallel
sentences have in common, of course, is the semantics of those two sentences. using this data, we
propose a novel method for learning vector representations at the word level and beyond.

3.1 bilingual signal

exploiting the semantic similarity of parallel sentences across languages, we can de   ne a simple
bilingual (and trivially multilingual) error function as follows: given a compositional sentence
model (cvm) ma, which maps a sentence to a vector, we can train a second cvm mb using
a corpus ca,b of parallel data from the language pair a, b. for each pair of parallel sentences
(a, b)     ca,b, we attempt to minimize

edist(a, b) = (cid:107)aroot     broot(cid:107)2

(1)

where aroot is the vector representing sentence a and broot the vector representing sentence b.

3.2 the bicvm model

a cvm learns semantic representations of larger syntactic units given the semantic representations
of their constituents. we assume individual words to be represented by vectors (x     rd).
previous methods employ binary parse trees on the data (e.g. [14, 25]) and use weighted or multi-
plicative composition functions. under such a setup, where each node in the tree is terminal or has
two children (p     c0, c1), a binary composition function could take the following form:

p = g (w e[c0; c1] + be)

(2)
where [c0; c1] is the concatenation of the two child vectors, w e     rd  2d and be     rd the encod-
ing matrix and bias, respectively, and g an element-wise activation function such as the hyperbolic
tangent. for the purposes of evaluation the bilingual signal proposed above, we simplify this com-
position function by setting all weight matrices to the identity and all biases to zero. thereby the
cvm reduces to a simple additive composition function:

aroot =

ai

(3)

of course, this is a very simpli   ed cvm, as such a bag-of-words approach no longer accounts
for word ordering and other effects which a more complex cvm might capture. however, for
the purposes of this evaluation (and with the experimental evaluation in mind), such a simplistic
composition function should be suf   cient to evaluate the novel objective function proposed here.

i=0

3

|a|(cid:88)

figure 1: description of a bilingual model with parallel input sentences a and b. the objective
function of this model is to minimize the distance between the sentence level encoding of the bi-
text. principally any composition function can be used to generate the compositional sentence level
representations. the composition function is represented by the cvm boxes in the diagram above.

using this additive cvm we want to optimize the bilingual error signal de   ned above (eq. 1). for
the moment, assume that ma is a perfectly trained cvm such that aroot represents the semantics
of the sentence a. further, due to the use of parallel data, we know that a and b are semantically
equivalent. hence we transfer the semantic knowledge contained in ma onto mb, by learning
  mb to minimize:

ebi(ca,b) =

edist(a, b)

(4)

(cid:88)

(a,b)   ca,b

of course, this objective function assumes a fully trained model which we do not have at this stage.
while this can be a useful objective for transferring linguistic knowledge into low-resource lan-
guages [16], this precondition is not helpful when there is no model to learn from in    rst place. we
resolve this issue by jointly training both models ma and mb.
applying ebi to parallel data ensures that both models learn a shared representation at the sentence
level. as the parallel input sentences share the same meaning, it is reasonable to assume that mini-
mizing ebi will force the model to learn their semantic representation. let   bi =   ma       mb . the
joint objective function j(  bi) thus becomes:

j(  bi) = ebi(ca,b) +

(cid:107)  bi(cid:107)2

  
2

(5)

where   (cid:107)  bi(cid:107)1 is the l2 id173 term.
it is apparent that this joint objective j(  bi) is degenerate. the models could learn to reduce all
embeddings and composition weights to zero and thereby minimize the objective function. we ad-
dress this issue by employing a form of contrastive estimation penalizing small distances between
non-parallel sentence pairs. for every pair of parallel sentences (a, b) we sample a number of ad-
ditional sentences n     cb, which   with high id203   are not exact translations of a. this is
comparable to the second term of the id168 of a large margin nearest neighbour classi   er
(see eq. 12 in [28]):

enoise(a, b, n) = [1 + edist(a, b)     edist(a, n)]+

(6)

4

where [x]+ = max(x, 0) denotes the standard hinge loss. thus, the    nal objective function to
minimize for the bicvm model is:

(cid:88)

(cid:32) k(cid:88)

(a,b)   ca,b

i=1

(cid:33)

enoise(a, b, ni)

+

(cid:107)  bi(cid:107)2

  
2

(7)

j(  bi) =

3.3 model learning

given the objective function as de   ned above, model learning can employ the same techniques as
any monolingual cvm. in particular, as the objective function is differentiable, we can use standard
id119 techniques such as stochastic id119, l-bfgs or the adaptive gradient
algorithm adagrad [8]. within each monolingual cvm, we use id26 through structure
after applying the joint error to each sentence level node.

4 experiments

4.1 data and parameters

all model weights were randomly initialised using a gaussian distribution. there are a number of
parameters that can in   uence model training. we selected the following values for simplicity and
comparability with prior work. in future work we will investigate the effect of these parameters
in greater detail. l2 id173 (1), step-size (0.1), number of noise elements (50), margin size
(50), embedding dimensionality (d=40). the noise elements samples were randomly drawn from
the corpus at training time, individually for each training sample and epoch. we use the europarl
corpus (v7)1 for training the bilingual model. the corpus was pre-processed using the set of tools
provided by cdec2 [9] for tokenizing and lowercasing the data. further, all empty sentences as well
as their translations were removed from the corpus.
we present results from two experiments. the bicvm model was trained on 500k sentence pairs of
the english-german parallel section of the europarl corpus. the bicvm+ model used this dataset
in combination with another 500k parallel sentences from the english-french section of the corpus,
resulting in 1 million english sentences, each paired up with either a german or a french sentence.
each language   s vocabulary used distinct encodings to avoid potential overlap.
the motivation behind bicvm+ is to investigate whether we can learn better embeddings by intro-
ducing additional data in a different language. this is similar to prior work in machine translation
where english was used as a pivot for translation between low-resource languages [5].
we use the adaptive gradient method, adagrad [8], for updating the weights of our models, and ter-
minate training after 50 iterations. earlier experiments indicated that the bicvm model converges
faster than the bicvm+ model, but we report results on the same number of iterations for better
comparability3.

4.2 cross-lingual document classi   cation

we evaluate our model using the cross-lingual document classi   cation (cldc) task of klementiev
et al. [16]. this task involves learning language independent embeddings which are then used for
document classi   cation across the english-german language pair. for this, cldc employs a par-
ticular kind of supervision, namely using supervised training data in one language and evaluating
without supervision in another. thus, cldc is a good task for establishing whether our learned
representations are semantically useful across multiple languages.
we follow the experimental setup described in [16], with the exception that we learn our embeddings
using solely the europarl data and only use the reuters rcv1/rcv2 corpora during the classi   er
training and testing stages. each document in the classi   cation task is represented by the average

1http://www.statmt.org/europarl/
2https://github.com/redpony/cdec
3these numbers were updated following comments in the iclr open review process. results for other

dimensionalities and our source code for our model are available at http://www.karlmoritz.com.

5

model
majority class
glossed
mt
i-matrix
bicvm
bicvm+

en     de
46.8
65.1
68.1
77.6
83.7
86.2

de     en
46.8
68.6
67.4
71.1
71.4
76.9

table 1: classi   cation accuracy for training on english and german with 1000 labeled examples.
cross-lingual compositional representations (bicvm and bicvm+), cross-lingual representations
using learned embeddings and an interaction matrix (i-matrix) [16] translated (mt) and glossed
(glossed) words, and the majority class baseline. the mt and glossed results are also taken from
klementiev et al. [16].

)

%

(

y
c
a
r
u
c
c
a
n
o
i
t
a
c
   

i
s
s
a
l
c

80

70

60

50

100

80

70

60

50

100

500

200
training documents (en)

1000 5000 10000

500

200
training documents (de)

1000 5000 10000

bicvm

bicvm+

i-matrix

mt

glossed

majority class

figure 2: classi   cation accuracy for a number of models (see table 1 for model descriptions). the
left chart shows results for these models when trained on english data and evaluated on german
data, the right chart vice versa.

of the d-dimensional representations of all its sentences. we train the multiclass classi   er using the
same settings and implementation of the averaged id88 classi   er [6] as used in [16].
we ran the cldc experiments both by training on english and testing on german documents and
vice versa. using the data splits provided by [16], we used varying training data sizes from 100 to
10,000 documents for training the multiclass classi   er. the results of this task across training sizes
are shown in figure 2. table 1 shows the results for training on 1,000 documents.
both models, bicvm and bicvm+ outperform all prior work on this task. further, the bicvm+
model outperforms the bicvm model, indicating the usefulness of adding training data even from
a separate language pair.

4.3 visualization

while the cldc experiment focused on establishing the semantic content of the sentence level
representations, we also want to brie   y investigate the induced id27s. in particular the
bicvm+ model is interesting for that purpose, as it allows us to evaluate our approach of using
english as a pivot language in a multilingual setup.
in figure 3 we show the id167 projections for a number of english, french and german words. of
particular interest should be the right chart, which highlights bilingual embeddings between french
and german words. even though the model did not use any parallel french-german data during
training, it still managed to learn semantic word-word similarity across these two languages.

6

figure 3: the left scatter plot shows id167 projections for a weekdays in all three languages using
the representations learned in the bicvm+ model. even though the model did not use any parallel
french-german data during training, it still learns semantic similarity between these two languages
using english as a pivot. to highlight this, the right plot shows another set of words (months of the
year) using only the german and french words.

5 conclusions

with this paper we have proposed a novel method for inducing cross-lingual distributed represen-
tations for id152. using a very simple method for semantic composition, we
nevertheless managed to obtain state of the art results on the cldc task, speci   cally designed to
evaluate semantic transfer across languages. after extending our approach to include multilingual
training data in the bicvm+ model, we were able to demonstrate that adding additional languages
further improves the model. furthermore, using some qualitative experiments and visualizations, we
showed that our approach also allows us to learn semantically related embeddings across languages
without any direct training data.
our approach provides great    exibility in training data and requires little to no annotation. hav-
ing demonstrated the successful training of semantic representations using sentence aligned data, a
plausible next step is to attempt training using document-aligned data or even corpora of comparable
documents. this may provide even greater possibilities for working with low-resource languages.
in the same vein, the success of our pivoting experiments suggest further work. unlike other pivot
approaches, it is easy to extend our model to have multiple pivot languages. thus some pivots could
preserve different aspects such as case, gender etc., and overcome other issues related to having a
single pivot language.
as we have achieved the results in this paper with a relatively simple cvm, it would also be inter-
esting to establish whether our objective function can be used in combination with more complex
compositional vector models such as mv-id56 [25] or tensor-based approaches, to see whether
these can further improve results on both mono- and multilingual tasks when used in conjunction
with our cross-lingual objective function. related to this, we will also apply our model to a wider
variety of tasks including machine translation and multilingual information extraction.

acknowledgements

the authors would like to thank alexandre klementiev and his co-authors for making their datasets
and averaged id88 implementation available, as well as answering a number of questions
related to their work on this task. this work was supported by epsrc grant ep/k036580/1 and a
xerox foundation award.

7

references
[1] marco baroni and roberto zamparelli. nouns are vectors, adjectives are matrices: represent-

ing adjective-noun constructions in semantic space. in proceedings of emnlp, 2010.

[2] paul bloom. precis of how children learn the meanings of words. behavioral and brain

sciences, 24:1095   1103, 2001.

[3] stephen clark and stephen pulman. combining symbolic and distributional models of mean-
ing. in proceedings of aaai spring symposium on quantum interaction. aaai press, 2007.
[4] bob coecke, mehrnoosh sadrzadeh, and stephen clark. mathematical foundations for a com-
positional distributional model of meaning. lambek festschrift. linguistic analysis, 36:345   
384, 2010.

[5] trevor cohn and mirella lapata. machine translation by triangulation: making effective use
of multi-parallel corpora. in proceedings of acl, pages 728   735, prague, czech republic,
june 2007. association for computational linguistics.

[6] michael collins. discriminative training methods for id48: theory and
in proceedings of acl-emnlp. association for

experiments with id88 algorithms.
computational linguistics, 2002. doi: 10.3115/1118693.1118694.

[7] ronan collobert and jason weston. a uni   ed architecture for natural language processing:

deep neural networks with multitask learning. in proceedings of icml, 2008.

[8] john duchi, elad hazan, and yoram singer. adaptive subgradient methods for online learning
and stochastic optimization. journal of machine learning research, 12:2121   2159, july 2011.
issn 1532-4435.

[9] chris dyer, adam lopez, juri ganitkevitch, johnathan weese, ferhan ture, phil blunsom,
hendra setiawan, vladimir eidelman, and philip resnik. cdec: a decoder, alignment, and
in proceedings of
learning framework for    nite-state and context-free translation models.
acl, 2010.

[10] k. erk and s. pad  o. a structured vector space model for word meaning in context. proceedings

of emnlp, 2008.

[11] j. r. firth. a synopsis of linguistic theory 1930-55. 1952-59:1   32, 1957.
[12] edward grefenstette and mehrnoosh sadrzadeh. experimental support for a categorical com-

positional distributional model of meaning. in proceedings of emnlp, 2011.

[13] aria haghighi, percy liang, taylor berg-kirkpatrick, and dan klein. learning bilingual

lexicons from monolingual corpora. in proceedings of acl-hlt, 2008.

[14] karl moritz hermann and phil blunsom. the role of syntax in vector space models of

id152. in proceedings of acl, 2013.

[15] nal kalchbrenner and phil blunsom. recurrent convolutional neural networks for discourse
compositionality. in proceedings of the workshop on continuous vector space models and
their compositionality, 2013.

[16] alexandre klementiev, ivan titov, and binod bhattarai.

representations of words. in proceedings of coling, 2012.

inducing crosslingual distributed

[17] stanislas lauly, alex boulanger, and hugo larochelle. learning id73 represen-

tations using a bag-of-words autoencoder. in deep learning workshop at nips, 2013.

[18] tomas mikolov, kai chen, greg corrado, and jeffrey dean. ef   cient estimation of word

representations in vector space. corr, 2013.

[19] tomas mikolov, quoc v. le, and ilya sutskever. exploiting similarities among languages for

machine translation. corr, 2013.

[20] tom  a  s mikolov, martin kara     at, luk  a  s burget, jan   cernock  y, and sanjeev khudanpur. re-

current neural network based language model. in proceedings of interspeech, 2010.

[21] jeff mitchell and mirella lapata. vector-based models of semantic composition. in in pro-

ceedings of acl, 2008.

[22] d. roy. grounded spoken id146: experiments in word learning. ieee trans-
actions on multimedia, 5(2):197   209, june 2003. issn 1520-9210. doi: 10.1109/tmm.2003.
811618.

8

[23] a p sarath chandar, m khapra mitesh, b ravindran, vikas raykar, and amrita saha. multi-

lingual deep learning. in deep learning workshop at nips, 2013.

[24] richard socher, jeffrey pennington, eric h. huang, andrew y. ng, and christopher d. man-
ning. semi-supervised recursive autoencoders for predicting sentiment distributions. in pro-
ceedings of emnlp, 2011.

[25] richard socher, brody huval, christopher d. manning, and andrew y. ng. semantic compo-
sitionality through recursive matrix-vector spaces. in proceedings of emnlp-conll, pages
1201   1211, 2012.

[26] nitish srivastava and ruslan salakhutdinov. multimodal learning with deep boltzmann ma-

chines. in proceedings of nips. 2012.

[27] p. d. turney and p. pantel. from frequency to meaning: vector space models of semantics.

journal of arti   cial intelligence research, 37(1):141   188, 2010.

[28] kilian q. weinberger and lawrence k. saul. distance metric learning for large margin nearest
neighbor classi   cation. journal of machine learning research, 10:207   244, june 2009. issn
1532-4435.

[29] will y. zou, richard socher, daniel cer, and christopher d. manning. bilingual word em-

beddings for phrase-based machine translation. in proceedings of emnlp, 2013.

9

