6
1
0
2

 

v
o
n
4

 

 
 
]
l
c
.
s
c
[
 
 

1
v
2
4
2
1
0

.

1
1
6
1
:
v
i
x
r
a

answering complicated question intents

expressed in decomposed question sequences

mohit iyyer   

wen-tau yih, ming-wei chang

department of computer science and umiacs

university of maryland, college park

microsoft research
redmond, wa 98052

{scottyih,minchang}@microsoft.com

miyyer@umd.edu

abstract

recent work in id29 for question
answering has focused on long and compli-
cated questions, many of which would seem
unnatural if asked in a normal conversation be-
tween two humans. in an effort to explore a
conversational qa setting, we present a more
realistic task: answering sequences of simple
but inter-related questions. we collect a dataset
of 6,066 question sequences that inquire about
semi-structured tables from wikipedia, with
17,553 question-answer pairs in total. ex-
isting qa systems face two major problems
when evaluated on our dataset: (1) handling
questions that contain coreferences to previous
questions or answers, and (2) matching words
or phrases in a question to corresponding en-
tries in the associated table. we conclude by
proposing strategies to handle both of these
issues.

1

introduction

id29, which maps natural language
text to meaning representations in formal logic, has
emerged as a key technical component for building
id53 systems (liang, 2016). once a
natural language question has been mapped to a for-
mal query, its answer can be retrieved simply by exe-
cuting the query on a back-end structured database.
one of the main focuses of id29 re-
search is how to address compositionality in language.
extremely complicated questions have been used to

   work done during an internship at microsoft research

demonstrate the sophistication of semantic parsers,1
and such questions have been speci   cally targeted in
the design of a recently-released qa dataset (pasupat
and liang, 2015). take for example the following
question:    of those actresses who won a tony after
1960, which one took the most amount of years after
winning the tony to win an oscar?    the correspond-
ing logical form is highly compositional; in order
to answer it, many sub-questions must be implicitly
answered in the process (e.g.,    who won a tony after
1960?   ).

while we agree that semantic parsers should be
able to answer very complicated questions, in reality
these questions are rarely issued by human users of
qa systems.2 because users can interact with a qa
system repeatedly, there is no need to assume a single-
turn qa setting where the exact question intent has
to be captured with just one complex question. the
same intent can be more naturally expressed through
a sequence of simpler questions, as shown below:

1. what actresses won a tony after 1960?
2. of those, who later won an oscar?
3. who had the biggest gap between their two

award wins?

decomposing complicated intents into multiple re-
lated but simpler questions is arguably a more ef-
fective strategy to explore a topic of interest, and it
reduces the cognitive burden on both the person who

1for example,    will it be warmer than 70 degrees near the
golden gate bridge after 5pm the day after tomorrow?    (dye,
2016)

2as indirect evidence, the percentage of questions with more
than 15 words is only 3.75% in the wikianswers questions
dataset (fader et al., 2014).

asks the question as well as the one who answers it.3
in this work, we study the id29
problem for answering sequences of simple related
questions. we collect a dataset of question se-
quences that we call sequentialqa (sqa)4 by ask-
ing crowdsourced workers to decompose compli-
cated questions sampled from the wikitableques-
tions dataset (pasupat and liang, 2015) into multiple
easier ones. in addition, each question is associated
with answers selected by workers from a correspond-
ing wikipedia html table. using the sqa dataset,
we investigate experimentally how we should modify
traditional semantic parser design to address different
properties in this new, multi-turn qa setting, such as
inter-question coreferences.

our contributions are twofold. first, to the best
of our knowledge, sqa is the    rst id29
dataset for sequential id53. we be-
lieve this dataset will be valuable to future research
on both id29 and id53 in
the more natural interactive setting. second, after
evaluating existing id53 systems on
sqa, we    nd that none of them performs adequately,
despite the relative lack of compositionality. we pro-
vide a detailed error analysis, which suggests that
improperly-resolved references and mismatches be-
tween question text and table entries are the main
sources of errors.

the rest of the paper is structured as follows.
sec. 2 contrasts the existing tasks and datasets to
sqa. sec. 3 describes how we collect the data in
detail. sec. 4 presents our experimental study, fol-
lowed by the discussion in sec. 5.2. finally, sec. 6
concludes the paper.

2 related work
our work is related to existing research on conver-
sational (or contextual) id29, as well as
more generally to interactive question-answering sys-
tems that operate on semi-structured data.

previous work on conversational qa has focused
on small, single-domain datasets. perhaps most re-
lated to our task is the context-dependent sentence

3while cognitive load has not been measured speci   cally for
complicated questions, there have been many studies linking
increased sentence complexity to longer reading times (hale,
2006; levy, 2008; frank, 2013).

4to be released at http://aka.ms/sqa

analysis described in zettlemoyer and collins (2009),
where conversations between customers and travel
agents are mapped to logical forms after resolving ref-
erential expressions. another dataset of travel book-
ing conversations is used by artzi and zettlemoyer
(2011) to learn a semantic parser for complicated
queries given user clari   cations. more recently, long
et al. (2016) collect three contextual semantic pars-
ing datasets (from synthetic domains) that contain
coreferences to entities and actions. we differentiate
ourselves from these prior works in two signi   cant
ways:    rst, our dataset is not restricted to a particular
domain, which results in major challenges further
detailed in section 5.2; and second, a major goal of
our work is to analyze the different types of sequence
progressions people create when they are trying to
express a complicated intent.

complex, interactive qa tasks have also been pro-
posed in the information retrieval community, where
the data source is a corpus of newswire text (kelly
and lin, 2007). we also build on aspects of some ex-
isting interactive question-answering systems. for ex-
ample, the system of harabagiu et al. (2005) includes
a module that predicts what a user will ask next given
their current question. a follow-up work (lacatusu
et al., 2006) proposes syntax-based heuristics to auto-
matically decompose complex questions into simpler
ones. both works rely on proprietary limited-domain
datasets; it is unlikely that the proposed heuristics
would scale across arbitrary domains.

3 a dataset of question sequences

since there are no previous publicly-available
datasets for our task, we collect the sequentialqa
(sqa) dataset via id104. we leverage
wikitablequestions (pasupat and liang, 2015,
henceforth wtq), which contains highly composi-
tional questions associated with html tables from
wikipedia. each id104 task contains a long,
complex question originally from wtq as the ques-
tion intent. the workers are asked to compose a
sequence of simpler questions that lead to the    nal
intent; an example of this process is shown in fig-
ure 1.

to simplify the task for workers, we only select
certain types of questions from wtq. in particular,
we only use questions from wtq whose answers

drastically from the intent.
no copying the intent: after adding the previous
constraint, we found that many workers were just
copying the intent as the    nal question of their
sequence, which resulted in unnatural-sounding
sequences. after we disallowed copying,
the
workers       nal questions contained many more
references to previous questions and answers.

we also encouraged (but did not enforce) the
following:
simplicity: when decomposing a complicated in-
tent into a sequence of questions, we expect that each
question in the sequence should be simpler than the
intent itself. however, de   ning    simple    is dif   cult,
and enforcing any de   nition is even harder. instead,
we told workers to try to limit their questions to those
that can be answered with just a single primitive
operation (e.g., column selection, argmax/argmin,    l-
tering with a single condition) and provided them
with examples of each primitive. following this def-
inition too closely, however, can result in unnatural
sequences, so we do not make any ui changes to
limit questions to single primitives.
inter-question coreferences: take the following
two sequences generated from the same question
intent:

1. what country won the world cup in 2014? of
the players on the team that won the world cup
in 2014, which ones were mid   elders?

2. what country won the world cup in 2014? of
the players on that team, which ones were mid-
   elders?

the second question of the    rst sequence clumsily
repeats information found in the preceding question,
while the second sequence avoids this repetition with
the referring expression    that team   . to encourage
more coreferences between questions, we showed
workers example sequences like these and stated that
the second one is preferred.

3.1 properties of sqa
in total, we used 2,022 question intents from the train
and test folds of the wtq for decomposition. we
had three workers decompose each intent, resulting in
6,066 unique questions sequences containing 17,553

figure 1: an example decomposition of a complicated intent
from wtq. workers must create a sequence of decomposed
questions where the answer to each question is a subset of cells
in the table.

are cells in the table, which excludes those involving
arithmetic and counting. we likewise also restrict
the questions our workers can write to those that are
answerable by only table cells. these restrictions
speed the annotation process because, instead of typ-
ing their answers, workers can just click on the table
to answer their question. they also allow us to col-
lect answer coordinates (row and column in the table)
as opposed to answer text, which removes many nor-
malization issues for answer string matching that are
present in the original wtq dataset. finally, we only
use intents that contain nine or more words; we    nd
that shorter questions tend to be simpler and are thus
less amenable to decomposition.

after iterating on the task design with many pilot
tasks, we found that the following constraints are
necessary for workers to produce good sequences:

minimum sequence length: workers must create
sequences that contain at least two questions. if the
intent is not easily decomposed into multiple ques-
tions, we instruct workers to create an alternate intent
whose answer is the same as that of the original. we
also encourage workers to write longer sequences if
possible.

final answer same as original answer: the    nal
question of a sequence must have the same answer
as that of the original intent. without this constraint,
some workers were writing sequences that diverged

nameegot completedyears to completeemmygrammyoscartonyrichard rodgers1962171962196019451951helen hayes1977451953197719321941rita moreno1977161977197219611975john gielgud1991301991197919811961audrey hepburn1994411993199419531954marvin hamlisch1995231995197419731976jonathan tunick1997201982198819771997mel brooks2001341967199819682001mike nichols2001402001196119671964whoopi goldberg2002172002198519902002scott rudin2012281984201220071994robert lopez2014102008201220142004what actresses have completed an egot?which of them won a tony after 1960?of those, who took the most years to complete the egot?list of people who have won academy, emmy, grammy, and tony awardsof those actresses who won a tony after 1960, which took the most amount of years to get their egot completed?decomposed sequence:original intent: total question-answer pairs (for an average of 2.9
questions per sequence). we divide the dataset into
train and test using the original wtq folds, resulting
in an 83/17 train/test split. importantly, just like in
wtq, none of the tables in the test set are seen in the
training set.

we identify three frequently-occurring question
classes: select column, select subset, and select row.
in select column questions, the answer is an entire
column of the table; these questions account for 23%
of all questions in sqa. subset and row selection are
more complicated than column selection, particularly
because they usually contain coreferences to the pre-
vious question   s answer. in select subset questions,
the answer is a subset of the previous question   s an-
swer; similarly, the answers to select row questions
occur in the same row(s) as the previous answer but
in a different column. select subset questions make
up 27% of sqa, while select row is 19%. the re-
maining 31% of sqa is comprised of more complex
questions that are combinations of these three types.
in the sequence    what are all of the tournaments? in
which one did he score the least points? on what date
was that?   , the    rst question is a column selection,
the second question is a subset selection, and the    nal
question is a row selection.

we also observe dramatic differences in the types
of questions that are asked at each position of the
sequence. for example, looking at just the    rst ques-
tion of each sequence, 51% of them are of the select
column variety (e.g.,    what are all of the teams?   ).
this number dwindles to just 18% when we look at
the second question of each sequence, which indi-
cates that the collected sequences start with general
questions and progress to more speci   c ones. by def-
inition, select subset and select row questions cannot
be the    rst question in a sequence.

4 baseline experiments

we evaluate two existing qa systems on sqa, a
id29 system called    oating parser and
an end-to-end neural network. the    oating parser
considers each question in a sequence independently
of the previous questions, while the neural network
leverages contextual information from the sequence.
our goals with these experiments are (1) to measure
the dif   culty of sqaand (2) to better understand the

behaviors of existing state-of-the-art systems.

4.1 floating parser
an obvious baseline is the    oating parser (fp) devel-
oped by pasupat and liang (2015), which fp maps
questions to logical forms and then executes them on
the table to retrieve the answers. it achieves 37.0%
accuracy on the wtq test set. one of the key chal-
lenges in id29 is the    semantic matching
problem   , where question text cannot be matched to
the corresponding answer column or cell verbatim.
without external knowledge, it is often hard to map
words or phrases in a question to predicates in its cor-
responding logical form. further compounding this
problem is that the train and test tables are disjoint,
which renders lexicon induction futile. therefore, fp
does not anchor predicates to tokens in the question,
relying instead on typing constraints to reduce the
search space.5

using fp as-is results in poor performance on
sqa. the main reason is that the system is con   g-
ured for questions with single answers, while sqa
contains a high percentage of questions with multiple-
cell answers. we address this issue by removing a
pruning hyperparameter (toomanyvalues that elimi-
nates all candidate parses with more than ten items
in their denotations, as well as by removing features
that add bias on the denotation size.

4.2 end-to-end neural network
recently, two different end-to-end neural network
architectures for question-answering on tables have
been proposed (neelakantan et al., 2015; yin et al.,
2016). both models show promising results on syn-
thetic datasets, but neither has been evaluated on
real data. we implement our own end-to-end neural
model (neural) by generally following both mod-
els but deviating when necessary to account for our
dataset characteristics.

as a brief description, we encode the question,
each column header, and each cell in the table with
a character-level lstm. we identify three high-level
operations based on our dataset characteristics (se-
lect column, select row, and select cell) and design
modules that perform each of these functions. a
module-level soft attention mechanism, effectively a

5see pasupat and liang (2015) for more details.

weighted sum of the module scores, decides which
module to use given a question.6 we also place an
additional lstm over the question sequence in order
to pass information about previous answers and ques-
tions to the current time step. finally, the output of
the attention mechanism and the question sequence
lstm is combined and fed to a binary classi   er that,
given each cell of the table, decides if the cell is part
of the answer to the current question or not.

fig. 2 shows an example of how the modules in
neural work together to answer a given question.
in particular, since the question    which of them won
a tony after 1960?    is asking for the names of the
actresses, the column selection module places most
of its weight on the    name    column, while the row
selection module highly weights rows that satisfy the
condition    tony after 1960   . the modules, which
take the question and table as input, are merged with
an attention mechanism a that also considers the an-
swer to the previous question. a full speci   cation of
neural can be found in appendix a.

in contrast to both the neural programmer of nee-
lakantan et al. (2015) and the neural enquirer of yin
et al. (2016), we make the simplifying assumption
that each question in a sequence can be solved with
just a single operation. another major difference is
that we use a character-level lstm, as the training
and test vocabulary are radically different.7

4.3 results
table 1 shows the results of both fp and neural
on the test set of sqa. we present both the overall
accuracy and the accuracy of answers to questions
at each position. although the accuracy of fp on
position-1 questions (48.7%) is much higher than its
performance on wtq (37.0%), the overall accuracy
(32.8%) is still lower, which indicates that our sqa
dataset remains dif   cult. in addition, the neural
model signi   cantly underperforms fp, suggesting
that it requires more data or more sophisticated archi-

6we did not design more speci   c modules to handle arith-
metic or aggregation like those of neelakantan et al. (2015),
although this is a potentially interesting direction for larger
datasets.

7due to the fact that much of our vocabulary (e.g., numbers,
entities) is not included in a regular corpus, we suspect that the
alternative of leveraging publicly-available id27s
will not be effective.

figure 2: diagram of neural architecture. small colored rect-
angles represent the output of the character-level lstm decoder.
the question, column header, and cell representations are passed
to three attentional modules. the output of these modules is
combined with the answer predictions for the previous question
to yield a    nal answer prediction for each cell.

model

fp

neural

all
32.8
17.4

pos 1 pos 2 pos 3 pos 4
17.5
48.7
27.6
12.2

26.2
11.8

25.8
13.4

table 1: accuracy of existing systems on our datasets on all
questions and questions at all positions within the sequence.

tectural design to generalize to all of sqa   s complex-
ities.

5 directions for improving sequential

id53

in this section, we explore possible directions for
improving the system performance in the sequential
id53 setting. we start from investigat-
ing different strategies for handling the coreference
issues of questions, and then revisited the semantic
matching issue by conducting some error analysis.

5.1 adapting existing semantic parsers
as we observed in sec. 4, existing semantic parsers
perform suboptimally on sqa. one possible expla-
nation for the suboptimal performance of existing
semantic parsers, shown in table 1, is that questions
that contain references to previous questions or an-
swers are not handled properly. by leveraging fp we
propose two ways to deal with this issue: question

answer to previous question: what actresses have completed an egot?which of them won a tony after 1960?nameegot completeyears to completeemmygrammyoscartonyhelen hayes1977451953197719321941rita moreno1977161977197219611975audrey hepburn1994411993199419531954whoopi goldberg2002172002198519902002select_row0.050.870.130.94select_col0.710.140.030.020.030.060.01select_cellarewriting and table rewriting.
question rewriting: take for example the partial
sequence    what are all the countries that participated
in the olympics? which ones won more than two gold
medals?    any system that treats these two questions
independently of each other has a high likelihood
of failing on the second question because    ones    is
not resolved to    countries   . the obvious solution is
to apply coreference resolution. however, existing
coreference resolution systems struggle at identifying
coreferences across two questions, potentially due to
the fact that their training data came from newswire
text with few questions.

an alternative approach is to create a set of com-
mon referential expressions (e.g.,    ones   ,    them   ,
   those   ) and replace them with noun phrases from the
previous question. as we do not have ground-truth
coreference annotations, we compute upper-bound
improvements on question rewriting instead. that is,
we rewrite a reference in a question with all possible
noun phrases in the previous question and count the
question as correct if any of the rewritten questions
are answered correctly. interestingly, we observe an
upper bound improvement of only    2% accuracy.
why is the upper bound so low? an error analysis
   nds that in many cases, the logical form predicted by
fp is wrong even when the referential expression is
correctly resolved. we will discuss this phenomenon
more in sec. 5.2, but here we concentrate on another
common scenario: the question contains a corefer-
ence to the answer of the previous question. if we
modify our example sequence to    what are all the
countries that participated in the olympics in 2012?
which ones won more than two gold medals?   , then
simply replacing    ones    with    countries    does not
resolve the reference.
table rewriting:
instead of building a model that
can learn to rewrite the second question to    which
countries won more than two gold medals in 2012   ,
or training a semantic parser that can incrementally
update the logical form from the previous question
as in zettlemoyer and collins (2009), we propose to
simply rewrite the table based on the    rst question   s
answer. speci   cally, if we know that a particular
question is a row or subset selection type, then we
also know that its answer must be located in the rows
that contain the previous answer. for example, take

the second question of the decomposed sequence in
fig. 1, which contains a coreference to the answer
of the    rst question (   which of them won a tony
after 1960   ) that refers to four actresses. the small-
est possible table from which we can still answer
this question is one that has four rows (for each of
the four actresses) and two columns (   name    and
   tony   ). however, identifying the columns neces-
sary to answer each question is often dif   cult, so we
leave this task to the semantic parser and remove only
rows (not columns) that do not contain the previous
question   s answers (see the rewritten table for this
example in fig. 2). in this way, we implicitly resolve
the coreference    of them   , as any rows that do not
correspond to actresses are excluded.

before rewriting the table, we have to    rst decide
whether the question contains a coreference to the an-
swer or not. we know that we should only rewrite the
table for subset and row selection questions. since
we can identify the question type in our dataset based
on the coordinates of the answers, we assume that
we know which questions should and should not be
rewritten and use this information to compute up-
per bounds for semantic parser improvement with
table rewriting. we evaluate    ve different rewriting
policies which vary in their knowledge of both the
question type and the correctness of the previous
predicted answer:

1. never rewrite the table
2. always rewrite the table based on the previous
predicted answer, regardless of whether table
rewriting is applicable to the question

3. rewrite row/subset: rewrite the table based on
the previous predicted answer only when table
rewriting is applicable (i.e., the question is sub-
set or row selection)

4. reference: same as rewrite row/subset, except
we only rewrite when we know the previous
predicted answer is correct

5. upper bound: same as rewrite row/subset, ex-
cept we rewrite using the previous ground-truth
answer instead of the previous predicted answer

table 2 shows the results of running these different
rewriting policies on our dev set. the oracle score
represents the percentage of questions for which at
least one candidate logical form generated by the

policy

dev acc dev oracle

never rewrite
always rewrite

rewrite row/subset

reference
upper bound

27.7
26.9
28.2
29.2
37.0

66.6
55.3
59.8
67.3
71.9

table 2: dev accuracy of different table rewriting policies; the
upper bound represents an almost 10% absolute improvement
that the other policies do not come close to reaching due to the
poor baseline performance of fp.

parser8 evaluates to the correct answer. the most
important takeaway is that accuracy improvements
are very small when we rewrite based on the pre-
vious predictions. intuitively, this makes sense: if
the parser only gets 30% accuracy, then 70% of the
time it will be incorrect on the previous question, and
rewriting the table based on a wrong answer could
make it impossible for the parser to get the right an-
swer (see the lower oracle scores for always rewrite
and rewrite row/subset). based on these results, ta-
ble rewriting will only be useful if the base parser   s
accuracy is high.

5.2 the semantic matching problem
the underwhelming improvements from question
and table rewriting force us to re-evaluate our origi-
nal hypothesis that reference resolution is the main
source of complexity in our dataset. we take 70
questions from our dev set and manually annotate
them with reasons why fp answered them incorrectly.
somewhat surprisingly, we    nd that only 15 of these
errors are due solely to coreferences! the majority
of errors are due to wrong logical forms that cannot
be corrected by simply resolving a coreference (e.g.,
the wrong operations are used, or the order of the
operations is incorrect).

when checking these questions in detail, we    nd
that the majority of the errors are due to the semantic
matching problem     mismatches between question
text and table text. the error analysis in (pasupat and
liang, 2015) on the more complicated wtq dataset
shows that 25% of errors are due to these mismatches
and an additional 29% to id172 issues (e.g.,

8the number of candidate parses considered by fp varies

and could sometimes be hundreds.

figure 3: example mismatches between question and table from
sqa. resolving the mismatches requires world knowledge such
as highway naming conventions and sports terminology that
must be provided externally or learned from a larger corpus.

an answer cell may contain    beijing, china    but the
crowdsourced answer is just    beijing   ). because
all answers in sqa are the exact text of cells in the
table, we avoid these id172 issues; however,
the results in table 1 show that the sequential nature
of sqa makes it equally as dif   cult as wtq for
machines. the examples in figure 3 suggest that
without solving the semantic matching problem, we
will not be able to properly take advantage of our
question or table rewriting adaptations.

6 conclusion

while most current qa systems assume a single-
turn setting, in this work we move towards a more
conversational, multi-turn scenario in which systems
must rely on prior context to answer the user   s cur-
rent question. to this end, we introduce sqa, a
dataset that consists of 6,066 unique sequences of
inter-related questions about wikipedia tables, with
17,553 questions-answer pairs in total. to the best
of our knowledge, sqa is the    rst id29
dataset that addresses sequential id53,
which is a more natural interface for information
access.

the unique setting and task scenario de   ned in
sqa immediately triggers several interesting re-
search questions, such as whether the simpler ques-
tions make the id29 problem easier and

what historic sites are located near a highway?namelocationcitybronson public library207 matteson streetbronsoncity of coldwater informational designationcity park at intersection of us-12 and us-27goldwateron what dates did the games end in a tie?dateopponentsresult f-a25 august 1984watford1-18 september 1984newcastle united5-0what stations play rock and jazz?locationcall signnetworkbeachkdprnews & classicalgrand forkskfjmroots, rock, and jazzlisbonkdsunews, classical, rock, and jazzhow should a system address the coreferences among
questions and answers. our preliminary experimental
study found that existing systems do not perform well
on sqa. moreover, the potential of various kinds of
question and table rewriting strategies for handling
coreferences is hindered by semantic matching errors
between question text and cells or column headers
in the table. in the near future, we plan to resolve
such errors by incorporating large external knowl-
edge sources into semantic parsers. longer-term, we
hope that research on sqa will push towards more
interactive settings where systems can ask users for
clari   cations and incorporate user feedback into fu-
ture predictions.

references
[artzi and zettlemoyer2011] yoav artzi and luke zettle-
moyer. 2011. id64 semantic parsers from
conversations. in proceedings of empirical methods in
natural language processing.

[dye2016] john dye.

2016.

the creator of siri
showcases viv, an impressive ai personal assis-
tant. http://www.androidauthority.com/
dag-kittlaus-showcase-viv-691539/,
may.

[fader et al.2014] anthony fader, luke zettlemoyer, and
oren etzioni. 2014. open id53 over
curated and extracted knowledge bases. in proceedings
of the 20th acm sigkdd international conference
on knowledge discovery and data mining, pages 1156   
1165. acm.

[frank2013] stefan l frank. 2013. uncertainty reduction
as a measure of cognitive load in sentence comprehen-
sion. topics in cognitive science, 5(3).

[hale2006] john hale. 2006. uncertainty about the rest

of the sentence. cognitive science, 30(4).

[harabagiu et al.2005] sanda harabagiu, andrew hickl,
john lehmann, and dan moldovan. 2005. experi-
ments with interactive question-answering. in proceed-
ings of the association for computational linguistics.
[kelly and lin2007] diane kelly and jimmy lin. 2007.
overview of the trec 2006 ciqa task. in acm sigir
forum, volume 41, pages 107   116. acm.

[kingma and ba2014] diederik kingma and jimmy ba.
2014. adam: a method for stochastic optimization. in
proceedings of the international conference on learn-
ing representations.

[lacatusu et al.2006] finley lacatusu, andrew hickl, and
sanda harabagiu. 2006. impact of question decompo-
sition on the quality of answer summaries. in interna-
tional language resources and evaluation.

[levy2008] roger levy. 2008. expectation-based syntac-

tic comprehension. cognition, 106(3).

[liang2016] percy liang. 2016. learning executable
semantic parsers for natural language understanding.
commun. acm, 59(9):68   76, august.

[long et al.2016] reginald long, panupong pasupat, and
percy liang. 2016. simpler context-dependent logical
forms via model projections. in proceedings of the
association for computational linguistics.

[neelakantan et al.2015] arvind neelakantan, quoc v le,
and ilya sutskever. 2015. neural programmer: in-
ducing latent programs with id119. in pro-
ceedings of the international conference on learning
representations.

[pasupat and liang2015] panupong pasupat and percy
liang. 2015. compositional id29 on semi-
structured tables. in proceedings of the association for
computational linguistics.

[yin et al.2016] pengcheng yin, zhengdong lu, hang li,
and ben kao. 2016. neural enquirer: learning to
query tables with natural language. in international
joint conference on arti   cial intelligence.

[zettlemoyer and collins2009] luke s zettlemoyer and
michael collins. 2009. learning context-dependent
mappings from sentences to logical form. in proceed-
ings of the association for computational linguistics.

a implementation detail in neural
we implement each of the three modules in neural with soft at-
tention mechanisms over columns, rows, and cells (mcol, mrow,
and mcell, respectively). while they are functionally similar,
each module differs from the others in both inputs and outputs.
before we present the equations de   ning each module, we intro-
duce some notation: say we have a r    c-dimensional table and
an lstm that encodes the question into a d-dimensional vector
q. we use the same lstm to encode the column headers into a
c    d matrix h and the table cell entries into an r    c    d tensor
t1. similar to the neural enquirer, we add type information to
the cell representations by computing a bilinear product with
the column headers, ti,j = relu(hjw1t1i,j ). before we can
implement our modules, we also have to integrate the previous
answer predictions (p1 of dimensionality r    c). we use a feed-
forward layer to determine how relevant the previous answers are
to the current question: p = relu(w3p1 + w4q). then, the
table representation is updated with the ground-truth previous
answers in a simple additive fashion: t = t + p     t.

our modules are de   ned as follows:

(cid:88)

mcol = softmax(hw5q),
ti,j)w6q),

mrow =   ((

j

mcell =   (tw7q)

(1)

note that mcol uses a softmax instead of a sigmoid; most of
the questions in sqa have answers that come from just a single

column of the table, so the softmax function   s predisposition
to select a single input is desirable here. finally, we compute
the    nal answer predictions a by merging the module outputs
with a soft attention mechanism that looks at the question to
generate a three-dimensional vector, matt, where each dimension
corresponds to the weight for one module.

(cid:88)

matt = softmax(w8q),

ai,j =

matt     [mcolj ; mrowi ; mcelli,j ]

(2)

the model parameters are optimized using adam (kingma
and ba, 2014); we train for 100 epochs and select the best-
performing model on the dev set. we set the dimensionality of
our lstm hidden state to d = 256 and the character embedding
dimensionality to 100.

