an empirical evaluation of doc2vec with

practical insights into document embedding generation

jey han lau1,2 and timothy baldwin2

1 ibm research

2 dept of computing and information systems,

the university of melbourne

jeyhan.lau@gmail.com, tb@ldwin.net

6
1
0
2

 
l
u
j
 

9
1

 
 
]
l
c
.
s
c
[
 
 

1
v
8
6
3
5
0

.

7
0
6
1
:
v
i
x
r
a

abstract

recently, le and mikolov (2014) pro-
posed doc2vec as an extension to
id97 (mikolov et al., 2013a) to
learn document-level embeddings. de-
spite promising results in the original pa-
per, others have struggled to reproduce
those results. this paper presents a rig-
orous empirical evaluation of doc2vec
over two tasks. we compare doc2vec
to two baselines and two state-of-the-art
document embedding methodologies. we
found that doc2vec performs robustly
when using models trained on large ex-
ternal corpora, and can be further im-
proved by using pre-trained word embed-
dings. we also provide recommendations
on hyper-parameter settings for general-
purpose applications, and release source
code to induce document embeddings us-
ing our trained doc2vec models.

1

introduction

neural embeddings were    rst proposed by bengio
et al. (2003), in the form of a feed-forward neu-
ral network language model. modern methods use
a simpler and more ef   cient neural architecture to
learn word vectors (id97: mikolov et al.
(2013b); glove: pennington et al. (2014)), based
on objective functions that are designed speci   -
cally to produce high-quality vectors.

neural embeddings learnt by these methods
have been applied in a myriad of nlp applica-
tions, including initialising neural network mod-
els for objective visual recognition (frome et
al., 2013) or machine translation (zhang et al.,
2014; li et al., 2014), as well as directly mod-
elling word-to-word relationships (mikolov et al.,

2013a; zhao et al., 2015; salehi et al., 2015; vy-
lomova et al., to appear),

paragraph vectors, or doc2vec, were pro-
posed by le and mikolov (2014) as a simple
extension to id97 to extend the learning
of embeddings from words to word sequences.1
doc2vec is agnostic to the granularity of the
word sequence     it can equally be a word id165,
sentence, paragraph or document.
in this paper,
we use the term    document embedding    to refer
to the embedding of a word sequence, irrespective
of its granularity.

doc2vec was proposed in two forms: dbow
and dmpv. dbow is a simpler model and ignores
word order, while dmpv is a more complex model
with more parameters (see section 2 for details).
although le and mikolov (2014) found that as a
standalone method dmpv is a better model, others
have reported contradictory results.2 doc2vec
has also been reported to produce sub-par per-
formance compared to vector averaging methods
based on informal experiments.3 additionally,
while le and mikolov (2014) report state-of-the-
art results over a id31 task using
doc2vec, others (including the second author of
the original paper in follow-up work) have strug-
gled to replicate this result.4

given this background of uncertainty regarding
the true effectiveness of doc2vec and confusion
about performance differences between dbow and
dmpv, we aim to shed light on a number of em-

1the term doc2vec was popularised by gensim
(   reh  u  rek and sojka, 2010), a widely-used implementation of
paragraph vectors: https://radimrehurek.com/gensim/
2the authors of gensim found dbow outperforms
https://github.com/piskvorky/gensim/blob/

dmpv:
develop/docs/notebooks/doc2vec-imdb.ipynb

3https://groups.google.com/forum/#!topic/

gensim/beskat45fxq

4for a detailed discussion on replicating the results of le
and mikolov (2014), see: https://groups.google.com/
forum/#!topic/id97-toolkit/q49firnoqro

pirical questions: (1) how effective is doc2vec
in different task settings?; (2) which is better out
of dmpv and dbow?; (3) is it possible to improve
doc2vec through careful hyper-parameter opti-
misation or with pre-trained id27s?;
and (4) can doc2vec be used as an off-the-shelf
model like id97? to this end, we present
a formal and rigorous evaluation of doc2vec
over two extrinsic tasks. our    ndings reveal that
dbow, despite being the simpler model, is supe-
rior to dmpv. when trained over large external
corpora, with pre-trained id27s and
hyper-parameter tuning, we    nd that doc2vec
performs very strongly compared to both a sim-
ple id27 averaging and id165 base-
line, as well as two state-of-the-art document em-
bedding approaches, and that doc2vec performs
particularly strongly over longer documents. we
additionally release source code for replicating our
experiments, and for inducing document embed-
dings using our trained models.

2 related work
id97 was proposed as an ef   cient neural
approach to learning high-quality embeddings for
words (mikolov et al., 2013a). negative sampling
was subsequently introduced as an alternative to
the more complex hierarchical softmax step at the
output layer, with the authors    nding that not only
is it more ef   cient, but actually produces better
word vectors on average (mikolov et al., 2013b).
the objective function of id97 is to max-
imise the log id203 of context word (wo)
given its input word (wi), i.e. log p (wo|wi ). with
negative sampling, the objective is to maximise the
dot product of the wi and wo while minimising
the dot product of wi and randomly sampled    neg-
ative    words. formally, log p (wo|wi ) is given as
follows:

log   (v(cid:48)

(cid:124)

wo

k(cid:88)

vwi )+
wi     pn(w)

(cid:104)

(cid:105)

vwi )

(1)

log   (   v(cid:48)

wi

(cid:124)

i=1

where    is the sigmoid function, k is the number of
negative samples, pn(w) is the noise distribution,
vw is the vector of word w, and v(cid:48)
w is the negative
sample vector of word w.

there are two approaches within id97:
skip-gram (   sg   ) and cbow. in skip-gram,
the input is a word (i.e. vwi is a vector of one word)

and the output is a context word. for each input
word, the number of left or right context words
to predict is de   ned by the window size hyper-
parameter. cbow is different to skip-gram in
one aspect: the input consists of multiple words
that are combined via vector addition to predict
the context word (i.e. vwi is a summed vector of
several words).

doc2vec is an extension to id97 for
learning document embeddings (le and mikolov,
2014).
there are two approaches within
doc2vec: dbow and dmpv.

dbow works in the same way as skip-gram,
except that the input is replaced by a special token
representing the document (i.e. vwi is a vector rep-
resenting the document). in this architecture, the
order of words in the document is ignored; hence
the name distributed bag of words.

dmpv works in a similar way to cbow. for the
input, dmpv introduces an additional document
token in addition to multiple target words. un-
like cbow, however, these vectors are not summed
but concatenated (i.e. vwi is a concatenated vector
containing the document token and several target
words). the objective is again to predict a context
word given the concatenated document and word
vectors..

more recently, kiros et al. (2015) proposed
skip-thought as a means of learning docu-
ment embeddings. skip-thought vectors are
inspired by abstracting the distributional hypothe-
sis from the word level to the sentence level. using
an encoder-decoder neural network architecture,
the encoder learns a dense vector presentation of a
sentence, and the decoder takes this encoding and
decodes it by predicting words of its next (or pre-
vious) sentence. both the encoder and decoder use
a gated recurrent neural network language model.
evaluating over a range of tasks, the authors found
that skip-thought vectors perform very well
against state-of-the-art task-optimised methods.

wieting et al. (2016) proposed a more direct
way of learning document embeddings, based on
a large-scale training set of paraphrase pairs from
the paraphrase database (ppdb: ganitkevitch et
al. (2013)). given a paraphrase pair, word em-
beddings and a method to compose the word em-
beddings for a sentence embedding, the objective
function of the neural network model is to opti-
mise the id27s such that the cosine
similarity of the sentence embeddings for the pair

is maximised. the authors explore several meth-
ods of combining id27s, and found
that simple averaging produces the best perfor-
mance.

3 evaluation tasks

we evaluate doc2vec in two task settings,
speci   cally chosen to highlight the impact of doc-
ument length on model performance.

for all tasks, we split the dataset into 2 par-
titions: development and test. the development
set is used to optimise the hyper-parameters of
doc2vec, and results are reported on the test set.
we use all documents in the development and test
set (and potentially more background documents,
where explicitly mentioned) to train doc2vec.
our rationale for this is that the doc2vec training
is completely unsupervised, i.e. the model takes
only raw text and uses no supervised or annotated
information, and thus there is no need to hold out
the test data, as it is unlabelled. we ultimately re-
lax this assumption in the next section (section 4),
when we train doc2vec using large external cor-
pora.

after training doc2vec, document embed-
dings are generated by the model.
for the
id97 baseline, we compute a document
embedding by taking the component-wise mean of
its component id27s. we experiment
with both variants of doc2vec (dbow and dmpv)
and id97 (skip-gram and cbow) for all
tasks.

in addition to id97, we experiment with
another baseline model that converts a document
into a distribution over words via maximum like-
lihood estimation, and compute pairwise docu-
ment similarity using the jensen shannon diver-
gence.5 for word types we explore id165s of or-
der n = {1, 2, 3, 4} and    nd that a combination of
unigrams, bigrams and trigrams achieves the best
results.6 henceforth, this second baseline will be
referred to as ngram.

3.1 forum question duplication
we    rst evaluate doc2vec over the task of du-
plicate question detection in a web forum setting,
using the dataset of hoogeveen et al. (2015). the
5we multiply the divergence value by    1.0 to invert the

value, so that a higher value indicates greater similarity.

6that is, the id203 distribution is computed over the
union of unigrams, bigrams and trigrams in the paired docu-
ments.

dataset has 12 subforums extracted from stackex-
change, and provides training and test splits in two
experimental settings: retrieval and classi   cation.
we use the classi   cation setting, where the goal is
to classify whether a given question pair is a du-
plicate.

the dataset is separated into the 12 subforums,
with a pre-compiled training   test split per subfo-
rum; the total number of instances (question pairs)
ranges from 50m to 1b pairs for the training par-
titions, and 30m to 300m pairs for the test par-
titions, depending on the subforum. the propor-
tion of true duplicate pairs is very small in each
subforum, but the setup is intended to respect the
distribution of true duplicate pairs in a real-world
setting.

we sub-sample the test partition to create a
smaller test partition that has 10m document
pairs.7 on average across all twelve subforums,
there are 22 true positive pairs per 10m ques-
tion pairs. we also create a smaller development
partition from the training partition by randomly
selecting 300 positive and 3000 negative pairs.
we optimise the hyper-parameters of doc2vec
and id97 using the development partition
on the tex subforum, and apply the same hyper-
parameter settings for all subforums when evalu-
ating over the test pairs. we use both the ques-
tion title and body as document content: on aver-
age the test document length is approximately 130
words. we use the default tokenised and lower-
cased words given by the dataset. all test, devel-
opment and un-sampled documents are pooled to-
gether during model training, and each subforum
is trained separately.

we compute cosine similarity between docu-
ments using the vectors produced by doc2vec
and id97 to score a document pair. we
then sort the document pairs in descending order
of similarity score, and evaluate using the area un-
der the curve (auc) of the receiver operating char-
acteristic (roc) curve . the roc curve tracks the
true positive rate against the false positive rate at
each point of the ranking, and as such works well
for heavily-skewed datasets. an auc score of 1.0
implies that all true positive pairs are ranked be-
fore true negative pairs, while an auc score of .5
indicates a random ranking. we present the full
results for each subforum in table 1.

7uniform random sampling is used so as to respect the

original distribution.

subforum

android
english
gaming

gis

mathematica

physics

programmers

stats
tex
unix

webmasters
wordpress

doc2vec
id97
dbow dmpv sg cbow
.97
.93
.73
.84
1.00
.97
.97
.93
.96
.81
.90
.96
.93
.84
1.00
.88
.94
.86
.98
.91
.92
.90
.97
.84

.86
.76
.97
.94
.81
.93
.84
.91
.79
.91
.92
.79

.96
.90
.98
.95
.90
.99
.83
.95
.91
.95
.91
.97

ngram

.80
.84
.94
.92
.70
.88
.68
.77
.78
.75
.79
.87

table 1: roc auc scores for each subforum.
boldface indicates the best score in each row.

domain

dls

headlines
ans-forums
ans-students

belief
images

.83
.74
.77
.74
.86

doc2vec
id97
dbow dmpv sg cbow
.69
.77
.66
.52
.65
.64
.76
.59
.78
.69

.78
.65
.60
.75
.75

.74
.62
.69
.72
.73

ngram

.61
.50
.65
.67
.62

table 2: pearson   s r of the sts task across 5 do-
mains. dls is the overall best system in the com-
petition. boldface indicates the best results be-
tween doc2vec and id97 in each row.

comparing doc2vec and id97 to
ngram, both embedding methods perform sub-
stantially better in most domains, with two excep-
tions (english and gis), where ngram has compa-
rable performance.

doc2vec outperforms id97 embed-
dings in all subforums except for gis. despite
the skewed distribution, simple cosine similarity
based on doc2vec embeddings is able to detect
these duplicate document pairs with a high degree
of accuracy. dbow performs better than or as well
as dmpv in 9 out of the 12 subforums, showing
that the simpler dbow is superior to dmpv.

one interesting exception is the english sub-
forum, where dmpv is substantially better, and
ngram     which uses only surface word forms
    also performs very well. we hypothesise that
the order and the surface form of words possibly
has a stronger role in this subforum, as questions
are often about grammar problems and as such the
position and semantics of words is less predictable
(e.g. where does    for the same    come from?)

hyper-parameter description

vector size dimension of word vectors

window size

left/right context window size
min count minimum frequency threshold

sub-sampling

for word types
threshold to downsample high
frequency words

negative sample no. of negative word samples

epoch number of training epochs

table 3: a description of doc2vec hyper-
paramters.

3.2 semantic textual similarity
the semantic textual similarity (sts) task is a
shared task held as part of *sem and semeval
over a number of iterations (agirre et al., 2013;
agirre et al., 2014; agirre et al., 2015). in sts,
the goal is to automatically predict the similarity
of a pair of sentences in the range [0, 5], where 0
indicates no similarity whatsoever and 5 indicates
semantic equivalence.

the top systems utilise word alignment, and fur-
ther optimise their scores using supervised learn-
ing (agirre et al., 2015). id27s are
employed, although sentence embeddings are of-
ten taken as the average of id27s (e.g.
sultan et al. (2015)).

we evaluate doc2vec and id97 embed-
dings over the english sts sub-task of semeval-
2015 (agirre et al., 2015). the dataset has 5 do-
mains, and each domain has 375   750 annotated
pairs. sentences are much shorter than our previ-
ous task, at an average of only 13 words in each
test sentence.

as the dataset is also much smaller, we com-
bine sentences from all 5 domains and also sen-
tences from previous years (2012   2014) to form
the training data. we use the headlines do-
main from 2014 as development, and test on all
2015 domains. for pre-processing, we tokenise
and lowercase the words using stanford corenlp
(manning et al., 2014).

as a benchmark, we include results from the
overall top-performing system in the competition,
referred to as    dls    (sultan et al., 2015). note,
however, that this system is supervised and highly
customised to the task, whereas our methods are
completely unsupervised. results are presented in
table 2.

unsurprisingly, we do not exceed the overall
performance of the supervised benchmark system
dls, although doc2vec outperforms dls over

training vector window min

method

task

dbow

dmpv

q-dup
sts
q-dup
sts

size
4.3m
.5m
4.3m
.5m

size
300
300
300
300

size
15
15
5
5

5
1
5
1

sub-
10   5
10   5
10   6
10   6

negative epoch
sample
20
400
600
1000

5
5
5
5

count sampling

table 4: optimal doc2vec hyper-parameter values used for each tasks.    training size    is the total word
count in the training data. for q-dup training size is an average word count across all subforums.

the domain of belief. ngram performs substan-
tially worse than all methods (with an exception
in ans-students where it outperforms dmpv and
cbow).

and

doc2vec

comparing

id97,
doc2vec performs better. however,
the per-
formance gap is lower compared to the previous
two tasks, suggesting that the bene   t of using
doc2vec is diminished for shorter documents.
comparing dbow and dmpv,
the difference is
marginal, although dbow as a whole is slightly
stronger,
consistent with the observation of
previous task.

3.3 optimal hyper-parameter settings

across the two tasks, we found that the optimal
hyper-parameter settings (as described in table 3)
are fairly consistent for dbow and dmpv, as de-
tailed in table 4 (task abbreviations: q-dup =
forum question duplication (section 3.1); and
sts = semantic textual similarity (section 3.2)).
note that we did not tune the initial and mini-
mum learning rates (   and   min, respectively),
and use the the following values for all experi-
ments:    = .025 and   min = .0001. the learning
rate decreases linearly per epoch from the initial
rate to the minimum rate.

in general, dbow favours longer windows for
possibly the most
context words than dmpv.
important hyper-parameter is the sub-sampling
threshold for high frequency words: in our experi-
ments we    nd that task performance dips consider-
ably when a sub-optimal value is used. dmpv also
requires more training epochs than dbow. as a
rule of thumb, for dmpv to reach convergence, the
number of epochs is one order of magnitude larger
than dbow. given that dmpv has more parameters
in the model, this is perhaps not a surprising    nd-
ing.

4 training with large external corpora

in section 3, all tasks were trained using small in-
domain document collections. doc2vec is de-
signed to scale to large data, and we explore the
effectiveness of doc2vec by training it on large
external corpora in this section.

we experiment with two external corpora: (1)
wiki, the full collection of english wikipedia;8
and (2) ap-news, a collection of associated press
english news articles from 2009 to 2015. we to-
kenise and lowercase the documents using stan-
ford corenlp (manning et al., 2014), and treat
each natural paragraph of an article as a document
for doc2vec. after pre-processing, we have ap-
proximately 35m documents and 2b tokens for
wiki, and 25m and .9b tokens for ap-news. see-
ing that dbow trains faster and is a better model
than dmpv from section 3, we experiment with
only dbow here.9

to test if doc2vec can be used as an off-the-
shelf model, we take a pre-trained model and in-
fer an embedding for a new document without up-
dating the hidden layer word weights.10 we have
three hyper-parameters for test id136:
initial
learning rate (  ), minimum learning rate (  min),
and number of id136 epochs. we optimise
these parameters using the development partitions
in each task; in general a small initial    (= .01)
with low   min (= .0001) and large epoch number
(= 500   1000) works well.

for id97, we train skip-gram on the

8using the dump dated 2015-12-01,

cleaned us-
https://github.com/attardi/

ing wikiextractor:
wikiextractor

9we use these hyper-parameter values for wiki (ap-
news): vector size = 300 (300), window size = 15 (15),
min count = 20 (10), sub-sampling threshold = 10   5 (10   5),
negative sample = 5, epoch = 20 (30). after removing low
frequency words, the vocabulary size is approximately 670k
for wiki and 300k for ap-news.

10that is, test data is held out and not including as part of

doc2vec training.

task metric

domain

android
english
gaming

gis

mathematica

physics

programmers

stats
tex
unix

webmasters
wordpress
headlines
ans-forums
ans-students

belief
images

q-dup

auc

sts

r

pp
ppdb
.92
.82
.96
.89
.80
.97
.88
.87
.88
.86
.89
.83
.77
.67
.78
.78
.83

skip-thought
book-corpus

.57
.56
.70
.58
.57
.61
.69
.60
.65
.74
.53
.66
.44
.35
.33
.24
.18

dbow

skip-gram

wiki ap-news wiki ap-news gl-news
.96
.80
.95
.85
.84
.92
.93
.92
.89
.95
.89
.99
.73
.59
.65
.58
.80

.77
.62
.88
.79
.65
.81
.75
.70
.75
.78
.77
.61
.73
.46
.67
.51
.72

.72
.61
.83
.79
.59
.74
.64
.66
.73
.66
.71
.58
.66
.42
.65
.52
.69

.76
.63
.85
.83
.58
.77
.72
.72
.64
.72
.73
.58
.74
.44
.69
.51
.73

.94
.81
.93
.86
.80
.94
.88
.98
.82
.94
.91
.98
.75
.60
.69
.62
.78

ngram

.80
.84
.94
.92
.70
.88
.68
.77
.78
.75
.79
.87
.61
.50
.65
.67
.62

table 5: results over all two tasks using models trained with external corpora.

same corpora.11 we also include the word vectors
trained on the larger google news by mikolov et
al. (2013b), which has 100b words.12 the google
news skip-gram vectors will henceforth be re-
ferred to as gl-news.

dbow, skip-gram and ngram results for all
two tasks are presented in table 5. between the
baselines ngram and skip-gram, ngram ap-
pears to do better over q-dup, while skip-gram
works better over sts.

as before, doc2vec outperforms id97
and ngram across almost all tasks. for tasks with
longer documents (q-dup), the performance gap
between doc2vec and id97 is more pro-
nounced, while for sts, which has shorter docu-
ments, the gap is smaller. in some sts domains
(e.g. ans-students) id97 performs just as
well as doc2vec. interestingly, we see that gl-
news id97 embeddings perform worse
than our wiki and ap-news id97 embed-
dings, even though the google news corpus is or-
ders of magnitude larger.

comparing doc2vec results with in-domain
results (1 and 2), the performance is in general
lower. as a whole, the performance difference be-
tween the dbow models trained using wiki and
ap-news is not very large, indicating the robust-
ness of these large external corpora for general-
purpose applications. to facilitate applications us-

11hyper-parameter values for wiki (ap-news): vector
size = 300 (300), window size = 5 (5), min count = 20 (10),
sub-sampling threshold = 10   5 (10   5), negative sample =
5, epoch = 100 (150)

12https://code.google.com/archive/p/id97/

ing off-the-shelf doc2vec models, we have pub-
licly released code and trained models to induce
document embeddings using the wiki and ap-
news dbow models.13

4.1 comparison with other document

embedding methodologies

we next calibrate the results for doc2vec
against skip-thought (kiros et al., 2015) and
paragram-phrase (pp: wieting et al. (2016)), two
recently-proposed competitor document embed-
ding methods. for skip-thought, we use the
pre-trained model made available by the authors,
based on the book-corpus dataset (zhu et al.,
2015); for pp, once again we use the pre-trained
model from the authors, based on ppdb (gan-
itkevitch et al., 2013). we compare these two
models against dbow trained on each of wiki
and ap-news. the results are presented in ta-
ble 5, along with results for the baseline method
of skip-gram and ngram.

skip-thought performs poorly:

its per-
formance is worse than the simpler method of
id97 vector averaging and ngram. dbow
outperforms pp over most q-dup subforums, al-
though the situation is reversed for sts. given
that pp is based on word vector averaging, these
observations support the conclusion that vector
averaging methods works best for shorter docu-
ments, while dbow handles longer documents bet-
ter.

it is worth noting that doc2vec has the upper-

13https://github.com/jhlau/doc2vec

hand compared to pp in that it can be trained on
in-domain documents. if we compare in-domain
doc2vec results (1 and 2) to pp (table 5), the
performance gain on q-dup is even more pro-
nounced.

5

improving doc2vec with pre-trained
id27s

q-dup

although not explicitly mentioned in the original
paper (le and mikolov, 2014), dbow does not
learn embeddings for words in the default con   gu-
ration. in its implementation (e.g. gensim), dbow
has an option to turn on id27 learn-
ing, by running a step of skip-gram to update
id27s before running dbow. with the
option turned off, id27s are randomly
initialised and kept at these randomised values.

even though dbow can in theory work with ran-
domised id27s, we found that perfor-
mance degrades severely under this setting. an in-
tuitive explanation can be traced back to its objec-
tive function, which is to maximise the dot prod-
uct between the document embedding and its con-
stituent id27s:
if id27s
are randomly distributed, it becomes more dif   cult
to optimise the document embedding to be close to
its more critical content words.

to illustrate this, consider the two-dimensional
id167 plot (van der maaten and hinton, 2008)
of doc2vec document and id27s in
figure 1(a).
in this case, the word learning op-
tion is turned on, and related words form clusters,
allowing the document embedding to selectively
position itself closer to a particular word cluster
(e.g. content words) and distance itself from other
clusters (e.g. function words). if id27s
are randomly distributed on the plane, it would be
harder to optimise the document embedding.

seeing that word vectors are essentially learnt
via skip-gram in dbow, we explore the pos-
sibility of using externally trained skip-gram
id27s to initialise the word embed-
dings in dbow. we repeat the experiments de-
scribed in section 3, training the dbow model us-
ing the smaller in-domain document collections
in each task, but this time initialise the word
vectors using pre-trained id97 embeddings
from wiki and ap-news. the motivation is that
with better initialisation, the model could converge
faster and improve the quality of embeddings.

results using pre-trained wiki and ap-news

task

domain

dbow

dbow + dbow +
ap-news

android
english
gaming

gis

mathematica

physics

programmers

stats
tex
unix

webmasters
wordpress
headlines
ans-forums
ans-students

belief
images

.97
.84
1.00
.93
.96
.96
.93
1.00
.94
.98
.92
.97
.77
.66
.65
.76
.78

wiki
.99
.90
1.00
.92
.96
.98
.92
1.00
.95
.98
.93
.96
.78
.68
.63
.77
.80

.98
.89
1.00
.94
.96
.97
.91
.99
.92
.97
.93
.98
.78
.68
.65
.78
.79

sts

table 6: comparison of dbow performance using
pre-trained wiki and ap-news skip-gram em-
beddings.

skip-gram embeddings are presented in ta-
ble 6. encouragingly, we see that using pre-
trained id27s helps the training of
dbow on the smaller in-domain document collec-
tion. across all tasks, we see an increase in perfor-
mance. more importantly, using pre-trained word
embeddings never harms the performance. al-
though not detailed in the table, we also    nd that
the number of epochs to achieve optimal perfor-
mance (based on development data) is fewer than
before.

we also experimented with using pre-trained
cbow id27s for dbow, and found sim-
ilar observations. this suggests that the initialisa-
tion of id27s of dbow is not sensitive
to a particular id27 implementation.

6 discussion

to date, we have focused on quantitative eval-
uation of doc2vec and id97.
the
qualitative difference between doc2vec and
id97 document embeddings, however, re-
mains unclear. to shed light on what is be-
ing learned, we select a random document from
sts     tech capital bangalore costliest indian
city to live in:
survey     and plot the docu-
ment and id27s induced by dbow and
skip-gram using id167 in figure 1.14

14we plotted a larger set of sentences as part of this analy-
sis, and found that the general trend was the same across all
sentences.

(a) doc2vec (dbow)

(b) id97 (skip-gram)

figure 1: two-dimentional id167 projection of doc2vec and id97 embeddings.

for id97, the document embedding is a
centroid of the id27s, given the sim-
ple word averaging method. with doc2vec, on
the other hand, the document embedding is clearly
biased towards the content words such as tech,
costliest and bangalore, and away from the func-
tion words. doc2vec learns this from its ob-
jective function with negative sampling: high fre-
quency function words are likely to be selected as
negative samples, and so the document embedding
will tend to align itself with lower frequency con-
tent words.
7 conclusion
we used two tasks
to empirically evaluate
the quality of document embeddings learnt by
doc2vec, as compared to two baseline meth-
ods     id97 word vector averaging and an
id165 model     and two competitor document
embedding methodologies. overall, we found
that doc2vec performs well, and that dbow is
a better model than dmpv. we empirically ar-
rived at recommendations on optimal doc2vec
hyper-parameter settings for general-purpose ap-
plications, and found that doc2vec performs ro-
bustly even when trained using large external cor-
pora, and bene   ts from pre-trained word embed-
dings. to facilitate the use of doc2vec and en-
able replication of these results, we release our
code and pre-trained models.

references
eneko agirre, daniel cer, mona diab, aitor gonzalez-
agirre, and weiwei guo. 2013. *sem 2013 shared

task: semantic textual similarity. in proceedings of
the second joint conference on lexical and compu-
tational semantics (*sem 2013), pages 32   43, at-
lanta, usa.

eneko agirre, carmen banea, claire cardie, daniel
cer, mona diab, aitor gonzalez-agirre, weiwei
guo, rada mihalcea, german rigau, and janyce
wiebe. 2014. semeval-2014 task 10: multilingual
in proceedings of the
semantic textual similarity.
8th international workshop on semantic evaluation
(semeval 2014), pages 81   91, dublin, ireland.

eneko agirre, carmen banea, claire cardie, daniel
cer, mona diab, aitor gonzalez-agirre, weiwei
guo, inigo lopez-gazpio, montse maritxalar, rada
mihalcea, german rigau, larraitz uria, and janyce
wiebe. 2015. semeval-2015 task 2: semantic tex-
tual similarity, english, spanish and pilot on inter-
pretability. in proceedings of the 9th international
workshop on semantic evaluation (semeval 2015),
pages 252   263, denver, usa.

yoshua bengio, r  ejean ducharme, pascal vincent, and
christian janvin. 2003. a neural probabilistic lan-
guage model. the journal of machine learning re-
search, 3:1137   1155.

andrea frome, greg s corrado, jon shlens, samy
bengio, jeff dean, marc aurelio ranzato, and
tomas mikolov. 2013. devise: a deep visual-
in advances in neu-
semantic embedding model.
ral information processing systems 26 (nips-13),
pages 2121   2129.

2013.

juri ganitkevitch, benjamin van durme, and chris
callison-burch.
ppdb: the paraphrase
database. in proceedings of the 2013 conference of
the north american chapter of the association for
computational linguistics: human language tech-
nologies (naacl hlt 2013), pages 758   764, at-
lanta, usa.

2.01.51.00.50.00.51.01.52.02.542024techcapitalbangalorecostliestindiancitytolivein:surveydoc2vec_sent_embtech capital bangalore costliest indian city to live in : survey32101234432101234techcapitalbangalorecostliestindiancitytolivein:surveyid97_sent_embtech capital bangalore costliest indian city to live in : surveydoris hoogeveen, karin verspoor, and timothy bald-
win.
2015. cqadupstack: a benchmark data
set for community question-answering research. in
proceedings of the twentieth australasian docu-
ment computing symposium (adcs 2015), pages
3:1   3:8, sydney, australia.

md arafat sultan, steven bethard, and tamara sum-
ner.
2015. dls@cu: sentence similarity from
word alignment and semantic vector composition.
in proceedings of the 9th international workshop on
semantic evaluation (semeval 2015), pages 148   
153, denver, colorado.

laurens van der maaten and geoffrey hinton. 2008.
visualizing data using id167. journal of machine
learning research, 9(2579   2605):85.

ekaterina vylomova, laura rimell, trevor cohn, and
timothy baldwin.
to appear. take and took, gag-
gle and goose, book and read: evaluating the utility
of vector differences for lexical relation learning. in
proceedings of the 54th annual meeting of the asso-
ciation for computational linguistics (acl 2016),
berlin, germany.

john wieting, mohit bansal, kevin gimpel, and karen
livescu. 2016. towards universal paraphrastic sen-
in proceedings of the inter-
tence embeddings.
national conference on learning representations
2016, san juan, puerto rico.

jiajun zhang, shujie liu, mu li, ming zhou, and
chengqing zong. 2014. bilingually-constrained
phrase embeddings for machine translation. in pro-
ceedings of the 52nd annual meeting of the asso-
ciation for computational linguistics (acl 2014),
pages 111   121, baltimore, usa.

jiang zhao, man lan, zheng-yu niu, and yue lu.
2015. integrating id27s and traditional
nlp features to measure id123 and se-
mantic relatedness of sentence pairs. in proceedings
of the international joint conference on neural net-
works (ijid982015), pages 1   7, killarney, ireland.

yukun zhu, ryan kiros, richard zemel, ruslan
salakhutdinov, raquel urtasun, antonio torralba,
and sanja fidler. 2015. aligning books and movies:
towards story-like visual explanations by watching
movies and reading books. arxiv, abs/1506.06724.

ryan kiros, yukun zhu, ruslan salakhutdinov,
richard s. zemel, antionio torralba, raquel ur-
tasun, and sanja fidler. 2015. skip-thought vec-
tors. in advances in neural information processing
systems 28 (nips-15), pages 3294   3302, montreal,
canada.

quoc le and tomas mikolov. 2014. distributed rep-
in pro-
resentations of sentences and documents.
ceedings of the 31st international conference on
machine learning (icml 2014), pages 1188   1196,
beijing, china.

peng li, yang liu, maosong sun, tatsuya izuha,
and dakun zhang.
2014. a neural reordering
model for phrase-based translation. in proceedings
of the 25th international conference on compu-
tational linguistics (coling 2014), pages 1897   
1907, dublin, ireland.

christopher d. manning, mihai surdeanu, john bauer,
jenny finkel, steven j. bethard, and david mc-
closky. 2014. the stanford corenlp natural lan-
guage processing toolkit. in association for compu-
tational linguistics (acl) system demonstrations,
pages 55   60, baltimore, usa.

tomas mikolov, kai chen, greg corrado, and jeffrey
dean. 2013a. ef   cient estimation of word represen-
tations in vector space. in proceedings of workshop
at the international conference on learning repre-
sentations, 2013, scottsdale, usa.

tomas mikolov, ilya sutskever, kai chen, greg s cor-
rado, and jeff dean. 2013b. distributed representa-
tions of words and phrases and their compositional-
ity. in advances in neural information processing
systems, pages 3111   3119.

2014.

jeffrey pennington, richard socher, and christopher
glove: global vectors for
manning.
in proceedings of the 2014
word representation.
conference on empirical methods in natural lan-
guage processing (emnlp 2014), pages 1532   
1543, doha, qatar.

radim   reh  u  rek and petr sojka.

software
framework for topic modelling with large cor-
pora. in proceedings of the lrec 2010 workshop
on new challenges for nlp frameworks, pages 45   
50, valletta, malta.

2010.

bahar salehi, paul cook, and timothy baldwin. 2015.
a id27 approach to predicting the com-
positionality of multiword expressions. in proceed-
ings of the 2015 conference of the north american
chapter of the association for computational lin-
guistics     human language technologies (naacl
hlt 2015), pages 977   983, denver, usa.

