dataset and neural recurrent sequence labeling model for open-domain

factoid id53

peng li, wei li, zhengyan he, xuguang wang, ying cao, jie zhou, wei xu

{lipeng17,liwei26,hezhengyan,wangxuguang,caoying03,

baidu research - institute of deep learning
zhoujie01,wei.xu}@baidu.com

6
1
0
2

 

p
e
s
1

 

 
 
]
l
c
.
s
c
[
 
 

2
v
5
7
2
6
0

.

7
0
6
1
:
v
i
x
r
a

abstract

while id53 (qa) with neural
network, i.e. neural qa, has achieved promis-
ing results in recent years, lacking of large
scale real-word qa dataset is still a challenge
for developing and evaluating neural qa sys-
tem. to alleviate this problem, we propose
a large scale human annotated real-world qa
dataset webqa with more than 42k questions
and 556k evidences. as existing neural qa
methods resolve qa either as sequence gen-
eration or classi   cation/ranking problem, they
face challenges of expensive softmax compu-
tation, unseen answers handling or separate
candidate answer generation component.
in
this work, we cast neural qa as a sequence la-
beling problem and propose an end-to-end se-
quence labeling model, which overcomes all
the above challenges. experimental results
on webqa show that our model outperforms
the baselines signi   cantly with an f1 score of
74.69% with word-based input, and the per-
formance drops only 3.72 f1 points with more
challenging character-based input.

introduction

1
id53 (qa) with neural network, i.e.
neural qa, is an active research direction along the
road towards the long-term ai goal of building gen-
eral dialogue agents (weston et al., 2016). unlike
conventional methods, neural qa does not rely on
feature engineering and is (at least nearly) end-to-
end trainable. it reduces the requirement for domain
speci   c knowledge signi   cantly and makes domain
adaption easier. therefore, it has attracted intensive
attention in recent years.

including neural

resolving qa problem requires several funda-
mental abilities including reasoning, memorization,
etc. various neural methods have been proposed
to improve such abilities,
ten-
sor networks (socher et al., 2013), recursive net-
works (iyyer et al., 2014), convolution neural net-
works (yih et al., 2014; dong et al., 2015; yin et
al., 2015), id12 (hermann et al., 2015;
yin et al., 2015; santos et al., 2016), and memo-
ries (graves et al., 2014; weston et al., 2015; kumar
et al., 2016; bordes et al., 2015; sukhbaatar et al.,
2015), etc. these methods achieve promising results
on various datasets, which demonstrates the high po-
tential of neural qa. however, we believe there are
still two major challenges for neural qa:

system development and/or evaluation on real-
world data: although several high quality and
well-designed qa datasets have been proposed in
recent years, there are still problems about using
them to develop and/or evaluate qa system under
real-world settings due to data size and the way
they are created. for example, babi (weston et
al., 2016) and the 30m factoid question-answer
corpus (serban et al., 2016) are arti   cially synthe-
sized; the trec datasets (harman and voorhees,
2006), free917 (cai and yates, 2013) and webques-
tions (berant et al., 2013) are human generated but
only have few thousands of questions; simpleques-
tions (bordes et al., 2015) and the id98 and daily
mail news datasets (hermann et al., 2015) are large
but generated under controlled conditions. thus, a
new large-scale real-world qa dataset is needed.

a new design choice for answer produc-
tion besides sequence generation and classi   ca-

tion/ranking: without loss of generality, the meth-
ods used for producing answers in existing neu-
ral qa works can be roughly categorized into
the sequence generation type and the classi   ca-
tion/ranking type. the former generates answers
word by word, e.g. (weston et al., 2016; kumar et
al., 2016; hermann et al., 2015). as it generally
involves softmax computation over a large vocabu-
lary, the computational cost is remarkably high and
it is hard to produce answers with out-of-vocabulary
word. the latter produces answers by classi   cation
over a prede   ned set of answers, e.g. (sukhbaatar
et al., 2015), or ranking given candidates by model
score, e.g. (yin et al., 2015). although it generally
has lower computational cost than the former, it ei-
ther also has dif   culties in handling unseen answers
or requires an extra candidate generating component
which is hard for end-to-end training. above all,
we need a new design choice for answer production
that is both computationally effective and capable of
handling unseen words/answers.

in this work, we address the above two challenges
by a new dataset and a new neural qa model. our
contributions are two-fold:

    we propose a new large-scale real-world fac-
toid qa dataset webqa with more than 42k
questions and 566k evidences, where an evi-
dence is a piece of text that contains relevant in-
formation to answer the question. on one hand,
our dataset is an order of magnitude larger
than existing real-world qa datasets (harman
and voorhees, 2006; cai and yates, 2013; be-
rant et al., 2013), which are generally insuf   -
cient to train an end-to-end qa system. on
the other hand, all questions in our dataset are
asked by real-world users in daily life, which is
signi   cantly more close to real-world settings
than those generated under controlled condi-
tions (bordes et al., 2015; hermann et al.,
2015). besides, as we also provide multiple hu-
man annotated evidences for each question, the
dataset can be used in research such as evidence
ranking and answer sentence selection as well.

    we introduce an end-to-end sequence label-
ing technique into neural qa as a new design
choice for answer production. mimicking how

q: who is the    rst wife of albert einstein ?
e:

einstein/o married/o his/o    rst/o wife/o mil-
eva/b mari  c/i in/o 1903/o

a: mileva mari  c

figure 1: factoid qa as sequence labeling.

humans    nd answers using search engine, we
use conditional random    eld (crf) (lafferty
et al., 2001) to label the answer of a question
from retrieved evidence. we avoid feature en-
gineering by computing features with a neural
model jointly trained with crf. as our model
does not rely on prede   ned vocabulary or can-
didates, it can handle unseen words/answers
easily and get rid of expensive softmax com-
putation.

experimental results show that our model outper-
forms baselines with a large margin on the webqa
dataset, indicating that it is effective. furthermore,
our model even achieves an f1 score of 70.97% on
character-based input, which is comparable with the
74.69% f1 score on word-based input, demonstrat-
ing that our model is robust.

2 factoid qa as sequence labeling
in this work, we focus on open-domain factoid qa.
taking figure 1 as an example, we formalize the
problem as follows: given each question q, we have
one or more evidences e, and the task is to produce
the answer a, where an evidence is a piece of text
of any length that contains relevant information to
answer the question. the advantage of this formal-
ization is that evidences can be retrieved from web
or unstructured knowledge base, which can improve
system coverage signi   cantly.

inspired by (yao et al., 2013), we introduce end-
to-end sequence labeling as a new design choice for
answer production in neural qa. given a question
and an evidence, we use crf (lafferty et al., 2001)
to assign a label to each word in the evidence to indi-
cate whether the word is at the beginning (b), inside
(i) or outside (o) of the answer (see figure 1 for ex-
ample). the key difference between our work and
(yao et al., 2013) is that (yao et al., 2013) needs a
lot work on feature engineering which further relies
on pos/ner tagging, id33, question
type analysis, etc. while we avoid feature engineer-

figure 2: neural recurrent sequence labeling model for factoid qa. the model consists of three components:    question lstm   
for computing question representation (rq),    evidence lstms    for analyzing evidence, and    crf    for producing label sequence
which indicates whether each word in the evidence is at the beginning (b), inside (i) or outside (o) of the answer. each word in the
evidence is also equipped with two 0-1 features (see section 3.4). we plot rq multiple times for clarity.

ing, and only use one single model to solve the prob-
lem. furthermore, compared with sequence genera-
tion and classi   cation/ranking methods for answer
production, our method avoids expensive softmax
computation and can handle unseen answers/words
naturally in a principled way.

1, xe

2,       , xq
m ), where xq

formally, we formalize qa as a sequence la-
suppose we have
beling problem as follows:
a vocabulary v of size |v |, given question
1, xq
xq = (xq
n ) and evidence xe =
2,       , xe
j are one-hot vec-
(xe
tors of dimension |v |, and n and m are the number
of words in the question and evidence respectively.
the problem is to    nd the label sequence   y which
maximizes the id155 under param-
eter   

i and xe

  y = arg max

(1)
in this work, we model p  (y|xq, xe) by a neural net-
work composed of lstms and crf.

y

p  (y|xq, xe).

3 recurrent sequence labeling model
3.1 overview
figure 2 shows the structure of our model. the
model consists of three components: (1) question
lstm for computing question representation; (2)
evidence lstms for evidence analysis; and (3) a

crf layer for sequence labeling. the question
lstm in a form of a single layer lstm equipped
with a single time attention takes the question as
input and generates the question representation rq.
the three-layer evidence lstms takes the evidence,
question representation rq and optional features as
input and produces    features    for the crf layer.
the crf layer takes the    features    as input and pro-
duces the label sequence. the details will be given
in the following sections.

3.2 long short-term memory (lstm)
following (graves, 2013), we de   ne (s(cid:48), y(cid:48)) =
lst m (x, s, y) as a function mapping its input x,
previous state s and output y to current state s(cid:48) and
output y(cid:48):

i =   (wxix + wyiy + wsis + bi)
f =   (wxf x + wyf y + wsf s + bf )
s(cid:48) = f s + i  (wxsx + wysy + bs)
o =   (wxox + wyoy + wsos(cid:48) + bo)
y(cid:48) = o tanh(s(cid:48))

(2)
(3)
(4)
(5)
(6)
where w        rh  h are parameter matrices, b       
rh are biases, h is lstm layer width,    is the
sigmoid function, i, f and o are the input gate, for-
get gate and output gate respectively.

oevidence lstmscrfquestion lstmwhoisthefirstwifeofalberteinsteineinstein10married00his01first11wife11mileva01mari  01in00190300oooobiooevidence 2albert 's three children were from his relation-ship with his first wife , milevamari  , his dau-ghterlieserlbeing born a year before they married .evidence 1rqrqrqrqrqrqrqrqrq(cid:88)

3.3 question lstm
the question lstm consists of a single-layer
lstm 1 and a single-time attention model. the
question xq = (xq
n ) is fed into the
lstm to produce a sequence of vector representa-
tions q1, q2,       , qn

2,       , xq

1, xq

(sq

i , sq

i   1, qi   1)

i , qi) = lst m (exq

(7)
where e     rd  |v | is the embedding matrix and d
is id27 dimension. then a weight   i
is computed by the single-time attention model for
each qi

  i = softmax(cid:0)vt

q tanh(waqi)(cid:1)

(8)
where vq     rd and wa     rd  d. and    nally the
weighted average rq of qi is used as the representa-
tion of the question

rq =

  iqi.

(9)

i

3.4 evidence lstms
the three-layer evidence lstms processes evidence
m ) to produce    features    for the
xe = (xe
1,xe
crf layer.

2,       , xe

the    rst lstm layer takes evidence xe, question
representation rq and optional features as input. we
   nd the following two simple common word indica-
tor features are effective:

    question-evidence common word feature
(q-e.comm): for each word in the evidence, the
feature has value 1 when the word also occurs
in the question, otherwise 0. the intuition is
that words occurring in questions tend not to
be part of the answers for factoid questions.

    evidence-evidence common word feature
(e-e.comm): for each word in the evidence, the
feature has value 1 when the word occurs in an-
other evidence, otherwise 0. the intuition is
that words shared by two or more evidences are
more likely to be part of the answers.

although counterintuitive, we found non-binary e-
e.comm feature values does not work well. because
the more evidences we considered, the more words
1multi-layer lstms can be used but no gain was observed.

tend to get non-zero feature values, and the less dis-
criminative the feature is.

the second lstm layer stacks on top of the    rst
lstm layer, but processes its output in a reverse or-
der. the third lstm layer stacks upon the    rst and
second lstm layers with cross layer links, and its
output serves as features for crf layer.

formally, the computations are de   ned as follows

(s1
(s2
(s3

j , e1
j , e2
j , e3
j and g2

j ; f 2g2
j ]
j   1, e1
j   1)
j+1, e2
j+1)
j   1, e3
j ], s3

xe1 = [exe
j; rq; f 1g1
j ) = lst m (xe1, s1
j ) = lst m (e1
j , s2
j ; e2
j ) = lst m ([e1

(10)
(11)
(12)
j   1) (13)
j are one-hot feature vectors, f 1    
where g1
rd1  2 and f 2     rd2  2 are embeddings for the
features, and d1 and d2 are the feature embedding
dimensions. note that we use the same word em-
bedding matrix e as in question lstm.

3.5 sequence labeling
following (huang et al., 2015; zhou and xu, 2015),
we use crf on top of evidence lstms for sequence
labeling. the id203 of a label sequence y given
question xq and evidence xe is computed as
p  (y|xq, xe)     exp(

  [yj   1, yj] +

(cid:88)

(cid:88)

ej[yj])

j

j

(14)
j, we     rl  d, l is the number of
where ej = wee3
label types,   [i, j] is the transition weight from label
i to j, and ej[i] is the i-th value of vector ej.
4 training
the objective function of our model is

l  (t ) =    (cid:88)

log(cid:0)p  (  yi|xq

i )(cid:1) +

i , xe

  ||  ||2

1
2

i , xe

where   yi is the golden label sequence, and t =
{(  yi, xq

i } is training set.

we use a minibatch stochastic id119
(sgd) (lecun et al., 1998) algorithm with rm-
sprop (tieleman and hinton, 2012) to minimize the
objective function. the initial learning rate is 0.001,
batch size is 120, and    = 0.016. we also ap-
ply dropout (hinton et al., 2012) to the output of
all the lstm layers. the dropout rate is 0.05. all
these hyper-parameters are determined empirically
via grid search on validation set.

i

dataset

train
validation
test

question
# word #
374,500
36,666
36,815

36,145
3,018
3,024

annotated evidence

positive
#
140,897
5,412
5,445

word #
10,757,652
233,911
234,258

negative
#
122,206
/
/

word #
14,808,758
/
/

retrieved evidence
word #
7,233,543
3,633,540
3,620,391

#
171,838
60,351
60,465

table 1: statistics of webqa dataset.

5 webqa dataset

in order to train and evaluate open-domain factoid
qa system for real-world questions, we build a new
chinese qa dataset named as webqa. the dataset
consists of tuples of (question, evidences, answer),
which is similar to example in figure 1. all the
questions, evidences and answers are collected from
web. table 1 shows some statistics of the dataset.

the questions and answers are mainly collected
from a large community qa website baidu zhi-
dao 2 and a small portion are from hand collected
web documents. therefore, all these questions are
indeed asked by real-world users in daily life in-
stead of under controlled conditions. all the ques-
tions are of single-entity factoid type, which means
(1) each question is a factoid question and (2) its
answer involves only one entity (but may have mul-
tiple words). the question in figure 1 is a positive
example, while the question    who are the children
of albert enistein?    is a counter example because
the answer involves three persons. the type and cor-
rectness of all the question answer pairs are veri   ed
by at least two annotators.

all the evidences are retrieved from internet by
using a search engine with questions as queries. we
download web pages returned in the    rst 3 result
pages and take all the text pieces which have no
more than 5 sentences and include at least one ques-
tion word as candidate evidences. as evidence re-
trieval is beyond the scope of this work, we simply
use tf-idf values to re-rank these candidates.

for each question in the training set, we provide
the top 10 ranked evidences to annotate (   annotated
evidence    in table 1). an evidence is annotated as
positive if the question can be answered by just read-
ing the evidence without any other prior knowledge,
otherwise negative. only evidences whose annota-
tions are agreed by at least two annotators are re-

2http://zhidao.baidu.com

tained. we also provide trivial negative evidences
(   retrieved evidence    in table 1), i.e. evidences
that do not contain golden standard answers.

for each question in the validation and test sets,
we provide one major positive evidence, and maybe
an additional positive one to compute features. both
of them are annotated. raw retrieved evidences
are also provided for evaluation purpose (   retrieved
evidence    in table 1).

the dataset will be released on the project page

http://idl.baidu.com/webqa.html.

6 evaluation on webqa dataset

6.1 baselines

we compare our model with two sets of baselines:

memn2n (sukhbaatar et al., 2015) is an end-to-
end trainable version of memory networks (weston
et al., 2015). it encodes question and evidence with
a bag-of-word method and stores the representations
of evidences in an external memory. a recurrent at-
tention model is used to retrieve relevant informa-
tion from the memory to answer the question.

attentive and impatient readers (hermann et
al., 2015) use bidirectional lstms to encode ques-
tion and evidence, and do classi   cation over a large
vocabulary based on these two encodings. the sim-
pler attentive reader uses a similar way as our work
to compute attention for the evidence. and the more
complex impatient reader computes attention after
processing each question word.

the key difference between our model and the
two readers is that they produce answer by doing
classi   cation over a large vocabulary, which is com-
putationally expensive and has dif   culties in han-
dling unseen words. however, as our model uses an
end-to-end trainable sequence labeling technique, it
avoids both of the two problems by its nature.

system

memn2n
attentive reader
impatient reader
ours

r

validation (strict)
f1
p
52.61
65.41
63.05
74.28

52.61
65.41
63.05
87.62

52.61
65.41
63.05
64.46

test (strict)

p

r

50.14
62.46
59.83
63.30

50.14
62.46
59.83
87.70

f1
50.14
62.46
59.83
73.53

table 2: comparison with baselines on the one-word answer subset of webqa.

model

softmax
softmax (k-1)
crf
softmax
softmax (k-1)
crf

noise

false
false
false
true
true
true

p

58.21
58.28
63.19
59.74
59.84
64.42

strict (val.)

r

72.90
71.80
79.21
69.11
67.51
75.84

f1
64.73
64.34
70.30
64.08
63.44
69.67

p

57.42
58.22
61.90
59.38
59.76
63.72

annotated evidence

strict (test)

r

72.32
71.83
77.33
68.77
67.61
76.09

f1
64.02
64.31
68.76
63.73
63.44
69.36

p

61.53
62.48
66.00
63.58
64.02
67.53

fuzzy (test)

r

77.49
77.08
82.45
73.63
72.44
80.63

f1
68.59
69.02
73.32
68.24
67.97
73.50

r

retrieved evidence
fuzzy (test, voting)
f1
p
70.70
70.11
71.69
72.15
71.44
74.69

74.18
73.47
74.64
74.72
73.93
76.83

67.53
67.06
68.97
69.75
69.11
72.66

table 3: evaluation results on the entire webqa dataset.

6.2 evaluation method
the performance is measured with precision (p), re-
call (r) and f1-measure (f1) 3

p =

|c|
|a| , r =

|c|
|q| , f 1 =

2p r
p + r

(15)

where c is the list of correctly answered questions,
a is the list of produced answers, and q is the list of
all questions 4.

as webqa is collected from web, the same an-
swer may be expressed in different surface forms in
the golden standard answer and the evidence, e.g.
          (beijing)    v.s.              (beijing province)   .
therefore, we use two ways to count correctly an-
swered questions, which are referred to as    strict   
and    fuzzy    in the tables:

    strict matching: a question is counted if and
only if the produced answer is identical to the
golden standard answer;

    fuzzy matching: a question is counted if and
only if the produced answer is a synonym 5 of
the golden standard answer;

and we also consider two evaluation settings:

3measures such map and mrr are often also used for eval-
uating qa system. however, as our model gives only condi-
tional probabilities which are not directly comparable for dif-
ferent answers, we will not include these measures in this work.
4as the baselines will produce exactly one answer for each

question, p, r and f1 will be identical for them.

5the synonyms will also be released.

    annotated evidence: each question has one
major annotated evidence and maybe another
annotated evidence for computing q-e.comm
and e-e.comm features (section 3.4);

    retrieved evidence: each question is provided
with at most 20 automatically retrieved evi-
dences (see section 5 for details). all the ev-
idences will be processed by our model inde-
pendently and answers are voted by frequency
to decide the    nal result. note that a large
amount of the evidences are negative and our
model should not produce any answer for them.

6.3 model settings
if not speci   ed, the following hyper-parameters will
be used in the reset of this section: lstm layer
width h = 64 (section 3.2), id27 di-
mension d = 64 (section 3.3), feature embedding
dimension d1 = d2 = 2 (section 3.3). the word
embeddings are initialized with pre-trained embed-
dings using a 5-gram neural language model (ben-
gio et al., 2003) and is    xed during training.

we will show that injecting noise data is impor-
tant for improving performance on retrieved evi-
dence setting in section 6.5. in the following exper-
iments, 20% of the training evidences will be neg-
ative ones randomly selected on the    y, of which
25% are annotated negative evidences and 75% are
retrieved trivial negative evidences (section 5). the
percentages are determined empirically. intuitively,

we provide the noise data to teach the model learn-
ing to recognize unreliable evidence.

for each evidence, we will randomly sample an-
other evidence from the rest evidences of the ques-
tion and compare them to compute the e-e.comm
feature (section 3.4). we will develop more power-
ful models to process multiple evidences in a more
principle way in the future.

as the answer for each question in our webqa
dataset only involves one entity (section 5), we dis-
tinguish label os before and after the    rst b in the
label sequence explicitly to discourage our model
to produce multiple answers for a question. for
example,
the golden labels for the example evi-
dence in figure 1 will became    einstein/o1 mar-
ried/o1 his/o1    rst/o1 wife/o1 mileva/b mari  c/i
in/o2 1903/o2   , where we use    o1    and    o2    to
denote label os before and after the    rst b 6.    fuzzy
matching    is also used for computing golden stan-
dard labels for training set.

for each setting, we will run three trials with dif-
ferent random seeds and report the average perfor-
mance in the following sections.

6.4 comparison with baselines

as the baselines can only predict one-word answers,
we only do experiments on the one-word answer
subset of webqa, i.e. only questions with one-word
answers are retained for training, validation and test.
as shown in table 2, our model achieves signi   cant
higher f1 scores than all the baselines.

the main reason for the relative low performance
of memn2n is that it uses a bag-of-word method to
encode question and evidence such that higher order
information like word order is absent to the model.
we think its performance can be improved by de-
signing more complex encoding methods (hill et al.,
2016) and leave it as a future work.

the attentive and impatient readers only have
access to the    xed length representations when do-
ing classi   cation. however, our model has access
to the outputs of all the time steps of the evidence
lstms, and scores the label sequence as a whole.
therefore, our model achieves better performance.

6all the words in a negative evidence will get label    o1   .

(cid:89)

6.5 evaluation on the entire webqa dataset
in this section, we evaluate our model on the entire
webqa dataset. the evaluation results are shown in
table 3. although producing multi-word answers is
harder, our model achieves comparable results with
the one-word answer subset (table 2), demonstrat-
ing that our model is effective for both single-word
and multi-word word settings.

   softmax    in table 3 means we replace crf with

softmax, i.e. replace eq. (14) with

p  (y|xq, xe) =

softmax(ek[yk])

(16)

k

crf outperforms softmax signi   cantly in all cases.
the reason is that softmax predicts each label in-
dependently, suggesting that modeling label tran-
sition explicitly is essential for improving perfor-
mance. a natural choice for modeling label tran-
sition in softmax is to take the last prediction into
account as in (bahdanau et al., 2015). the result
is shown in table 3 as    softmax(k-1)   . however,
its performance is only comparable with    softmax   
and signi   cantly lower than crf. the reason is that
we can enumerate all possible label sequences im-
plicitly by id145 for crf during
predicting but this is not possible for    softmax(k-
1)    7, which indicates crf is a better choice.

   noise    in table 3 means whether we inject noise
data or not (section 6.3). as all evidences are pos-
itive under the annotated evidence setting, the abil-
ity for recognizing unreliable evidence will be use-
less. therefore, the performance of our model with
and without noise is comparable under the annotated
evidence setting. however, the ability is important
to improve the performance under the retrieved evi-
dence setting because a large amount of the retrieved
evidences are negative ones. as a result, we observe
signi   cant improvement by injecting noise data for
this setting.

6.6 effect of id27
as stated in section 6.3, the id27 e
is initialized with lm embedding and kept    xed in
training. we evaluate different initialization and op-
timization methods in this section. the evaluation

7we think the performance of    softmax(k-1)    can be im-

proved by id125 and leave it as a future work.

settings

joint training

initialization
lm embedding
lm embedding
random

false
true
true

r

annotated evidence
f1
p
73.50
70.48
67.19

80.63
76.76
71.52

67.53
65.16
63.37

retrieved evidence (voting)

p

72.66
70.15
66.74

r

76.83
74.09
70.11

f1
74.69
72.06
68.38

table 4: effect of embedding initialization and training. only fuzzy matching results are shown.

annotated evidence (fuzzy)

p

settings
both
67.53
w/o q-e.comm 64.32
w/o e-e.comm 70.07

r

80.63
69.26
70.30

f1
73.50
66.69
70.18

retrieved evidence (voting, fuzzy)
f1
74.69
65.81
73.30

settings
72.66
both
w/o q-e.comm 63.97
w/o e-e.comm 71.05

76.83
67.77
75.69

r

p

annotated evidence (fuzzy)

p

r

67.53
67.80
67.30

80.63
78.84
78.03

f1
settings
73.50
attention
72.90
max
72.27
average
retrieved evidence (voting, fuzzy)
settings
f1
74.69
attention
74.15
max
73.30
average

72.66
72.08
71.31

76.83
76.34
75.41

r

p

table 5: effect of q-e.comm and e-e.comm features.

table 6: effect of question representations.

results are shown in table 4. the second row shows
the results when the embedding is optimized jointly
during training. the performance drops signi   -
cantly. detailed analysis reveals that the trainable
embedding enlarge trainable parameter number and
the model gets over    tting easily. the model acts
like a context independent entity tagger to some ex-
tend, which is not desired. for example, the model
will try to    nd any location name in the evidence
when the word           (where)    occurs in the ques-
tion. in contrary, pre-trained    xed embedding forces
the model to pay more attention to the latent syntac-
tic regularities. and it also carries basic priors such
as        (pear)    is fruit and              (lee sedol)    is a
person, thus the model will generalize better to test
data with    xed embedding. the third row shows
the result when the embedding is randomly initial-
ized and jointly optimized. the performance drops
signi   cantly further, suggesting that pre-trained em-
bedding indeed carries meaningful priors.

6.7 effect of q-e.comm and e-e.comm features
as shown in table 5, both the q-e.comm and e-
e.comm features are effective, and the q-e.comm
feature contributes more to the overall performance.
the reason is that the interaction between question
and evidence is limited and q-e.comm feature with
value 1, i.e. the corresponding word also occurs in
the question, is a strong indication that the word may
not be part of the answer.

6.8 effect of question representations
in this section, we compare the single-time atten-
tion method for computing rq (attention, eq.
(8,
9)) with two widely used options: element-wise max
operation max: rq = maxi qi and element-wise
average operation average: rq = 1
intu-
n
itively, attention can distill information in a more
   exible way from {qi}, while average tends to hide
the differences between them, and max lies between
attention and average. the results in table 6 sug-
gest that the more    exible and selective the opera-
tion is, the better the performance is.

(cid:80)

i qi.

6.9 effect of evidence lstms structures
we investigate the effect of evidence lstms layer
number, layer width and cross layer links in this sec-
tion. the results are shown in figure 3. for fair
comparison, we do not use cross layer links in fig-
ure 3 (a) (dotted lines in figure 2), and highlight the
results with cross layer links (layer width 64) with
circle and square for retrieved and annotated evi-
dence settings respectively. we can conclude that:
(1) generally the deeper and wider the model is, the
better the performance is; (2) cross layer links are ef-
fective as they make the third evidence lstm layer
see information in both directions.

6.10 word-based v.s. character-based input
our model achieves fuzzy matching f1 scores of
69.78% and 70.97% on character-based input in an-

annotated evidence (fuzzy)

model
word-based
char-based

p (%) r (%)
80.63
67.53
67.00
72.80

retrieved evidence (voting, fuzzy)

model
word-based
char-based

p (%) r (%)
76.83
72.66
68.88
73.20

f1 (%)
73.50
69.78

f1 (%)
74.69
70.97

figure 3: effect of evidence lstms structures. for fair com-
parison, cross layer links are not used in (a).

table 7: word-based v.s. character-based input.

notated and retrieved evidence settings respectively
(table 7), which are only 3.72 and 3.72 points lower
than the corresponding scores on word-based input
respectively. the performance is promising, demon-
strating that our model is robust and effective.

7 conclusion and future work
in this work, we build a new human annotated real-
world qa dataset webqa for developing and eval-
uating qa system on real-world qa data. we also
propose a new end-to-end recurrent sequence label-
ing model for qa. experimental results show that
our model outperforms baselines signi   cantly.

there are several future directions we plan to pur-
sue. first, multi-entity factoid and non-factoid qa
are also interesting topics. second, we plan to ex-
tend our model to multi-evidence cases. finally, in-
spired by residual network (he et al., 2016), we
will investigate deeper and wider models in the fu-
ture.

references
[bahdanau et al.2015] dzmitry bahdanau, kyunghyun
cho, and yoshua bengio.
2015. neural machine
translation by jointly learning to align and translate.
in proceedings of iclr 2015.

[bengio et al.2003] yoshua bengio, r  ejean ducharme,
pascal vincent, and christian janvin. 2003. a neural
probabilistic language model. the journal of machine
learning research, 3.

[berant et al.2013] jonathan berant, andrew chou, roy
frostig, and percy liang. 2013. id29 on
freebase from question-answer pairs. in proceedings
of the 2013 conference on empirical methods in natu-
ral language processing, pages 1533   1544, october.
[bordes et al.2015] antoine bordes, nicolas usunier,
sumit chopra, and jason weston. 2015. large-scale

simple id53 with memory networks.
arxiv:1506.02075.

[cai and yates2013] qingqing cai and alexander yates.
2013.
large-scale id29 via schema
matching and lexicon extension. in proceedings of the
51st annual meeting of the association for compu-
tational linguistics (volume 1: long papers), pages
423   433, august.

[dong et al.2015] li dong, furu wei, ming zhou, and
ke xu. 2015. id53 over freebase
with multi-column convolutional neural networks. in
proceedings of the 53rd annual meeting of the as-
sociation for computational linguistics and the 7th
international joint conference on natural language
processing (volume 1: long papers), pages 260   269,
july.

[graves et al.2014] alex graves, greg wayne, and ivo
id63s.

2014.

danihelka.
arxiv:1410.5401v2.

[graves2013] alex graves. 2013. generating sequences

with recurrent neural networks. arxiv:1308.0850v5.

[harman and voorhees2006] donna k harman

and
ellen m voorhees.
trec: an overview.
annual review of information science and technology,
40(1):113   155.

2006.

[he et al.2016] kaiming he, xiangyu zhang, shaoqing
ren, and jian sun. 2016. deep residual learning
for image recognition. in proceedings of ieee con-
ference on id161 and pattern recognition
(cvpr), pages 770   778, june.

[hermann et al.2015] karl moritz hermann, tomas ko-
cisky, edward grefenstette, lasse espeholt, will kay,
mustafa suleyman, and phil blunsom. 2015. teach-
in advances
ing machines to read and comprehend.
in neural information processing systems 28, pages
1693   1701.

[hill et al.2016] felix hill, antoine bordes, sumit
chopra, and jason weston. 2016. the goldilocks
principle: reading children   s books with explicit
in proceedings of iclr
memory representations.
2016.

(b) layer width64128256512717273747576(a) layer number1234fuzzy matching f1 (%)717273747576retrieved evidence (voting)annotated evidence[weston et al.2015] jason weston, sumit chopra, and
in pro-

bordes antoine. 2015. memory networks.
ceedings of iclr 2015.

[weston et al.2016] jason weston, antoine bordes, sumit
chopra, and tomas mikolov. 2016. towards ai-
complete id53: a set of prerequisite toy
tasks. in proceedings of iclr 2016.

[yao et al.2013] xuchen yao, benjamin van durme,
chris callison-burch, and peter clark. 2013. an-
swer extraction as sequence tagging with tree edit dis-
tance. in proceedings of the 2013 conference of the
north american chapter of the association for com-
putational linguistics: human language technolo-
gies, pages 858   867, june.

[yih et al.2014] wen-tau yih, xiaodong he, and christo-
pher meek. 2014. id29 for single-relation
id53. in proceedings of the 52nd an-
nual meeting of the association for computational
linguistics (volume 2: short papers), pages 643   648,
june.

[yin et al.2015] wenpeng yin, sebastian ebert, and hin-
rich sch  utze.
attention-based convolu-
tional neural network for machine comprehension.
arxiv:1602.04341v1.

2015.

[zhou and xu2015] jie zhou and wei xu. 2015. end-to-
end learning of id14 using recurrent
neural networks. in proceedings of the 53rd annual
meeting of the association for computational linguis-
tics and the 7th international joint conference on nat-
ural language processing (volume 1: long papers),
pages 1127   1137, beijing, china, july.

[hinton et al.2012] geoffrey e. hinton, nitish srivas-
ilya sutskever, and rus-
tava, alex krizhevsky,
lan salakhutdinov.
improving neural net-
works by preventing co-adaptation of feature detec-
tors. arxiv:1207.0580v1.

2012.

[iyyer et al.2014] mohit

[huang et al.2015] zhiheng huang, wei xu, and kai yu.
2015. bidirectional lstm-crf models for sequence
tagging. arxiv:1508.01991v1.
iyyer,

jordan boyd-graber,
leonardo claudino, richard socher,
and hal
daum  e iii. 2014. a neural network for factoid ques-
tion answering over paragraphs. in proceedings of the
2014 conference on empirical methods in natural
language processing (emnlp), pages 633   644,
october.

[kumar et al.2016] ankit kumar, ozan irsoy, jonathan
su, james bradbury, robert english, brian pierce, pe-
ter ondruska, ishaan gulrajani, and richard socher.
2016. ask me anything: dynamic memory networks
for natural language processing. in proceedings of the
33rd international conference on machine learning,
pages 1378   1387, june.

[lafferty et al.2001] john lafferty, andrew mccallum,
and fernando pereira. 2001. conditional random
   elds: probabilistic models for segmenting and la-
in proceedings of the eigh-
beling sequence data.
teenth international conference on machine learning
(icml    01), pages 282   289, san francisco, ca, usa.
[lecun et al.1998] yann lecun, l  eon bottou, yoshua
bengio, and patrick haffner. 1998. gradient-based
learning applied to document recognition. proceed-
ings of the ieee, 86(11):2278   2324, nov.

[santos et al.2016] cicero dos santos, ming tan, bing
xiang, and bowen zhou. 2016. attentive pooling net-
works. arxiv:1602.03609v1.

[serban et al.2016] iulian vlad serban, alberto garc    a-
dur  an, c   aglar g  ulc  ehre, sungjin ahn, sarath cjan-
dar, aaron c. courville, and yoshua bengio. 2016.
generating factoid questions with recurrent neural net-
works: the 30m factoid question-answer corpus. to
appear in proceedings of acl 2016.

[socher et al.2013] richard socher, danqi chen, christo-
pher d manning, and andrew ng. 2013. reasoning
with neural tensor networks for knowledge base com-
pletion. in advances in neural information process-
ing systems, pages 926   934.

[sukhbaatar et al.2015] sainbayar sukhbaatar, arthur
szlam, jason weston, and rob fergus. 2015. end-
in advances in neural
to-end memory networks.
information processing systems 28, pages 2440   2448.
[tieleman and hinton2012] tijmen tieleman and geof-
frey hinton. 2012. lecture 6.5-rmsprop: divide the
gradient by a running average of its recent magnitude.
coursera: neural networks for machine learning.

