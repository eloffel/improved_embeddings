6
1
0
2

 
l
u
j
 

1
2

 
 
]
l
c
.
s
c
[
 
 

1
v
8
0
2
6
0

.

7
0
6
1
:
v
i
x
r
a

exploring phrase-compositionality in skip-gram models

xiaochang peng and daniel gildea

xpeng,gildea@cs.rochester.edu

abstract

connect the hidden layer to the context word at each
relative position.

in this paper, we introduce a variation of
the skip-gram model which jointly learns dis-
tributed word vector representations and their
way of composing to form phrase embed-
dings.
in particular, we propose a learn-
ing procedure that
incorporates a phrase-
compositionality function which can capture
how we want to compose phrases vectors from
their component word vectors. our experi-
ments show improvement in word and phrase
similarity tasks as well as syntactic tasks like
id33 using the proposed joint
models.

1 introduction

and

translation

distributed word vector
representations learned
from large corpora of unlabeled data have been
shown to be effective in a variety of nlp tasks,
such as id52
(collobert et al., 2011),
(chen and manning, 2014;
parsing
durrett and klein, 2015),
ma-
(devlin et al., 2014;
chine
liu et al., 2014;
sutskever et al., 2014;
kalchbrenner and blunsom, 2013).
the
most widely used approaches to learn these vector
representations is the skip-gram model described in
mikolov et al. (2013a) and mikolov et al. (2013b).
the skip-gram model optimizes the id203 of
predicting words in the context given the current
center word. figure 1 shows a standard skip-gram
structure. ling et al. (2015) present a variation of
skip-gram which captures the relative position of
context words by using a different weight matrix to

one of

work

word

embeddings

usually
to data

are
to larger units due

hard
to
sparsity.
scale
recent
(mitchell and lapata, 2008;
baroni and zamparelli, 2010; coecke et al., 2010;
fyshe et al., 2015) deals with this issue by con-
structing distributional representations for phrases
from id27s. socher et al. (2013) use a
id56 to learn weight matrices
that capture compositionality. however, the learning
procedure is limited to the labeled id32
and the phrasal information from the unlabeled data
is not utilized.

lebret and collobert (2015) propose a learning
schedule which learns distributed vector
repre-
sentations for words and phrases jointly. how-
information
ever,
they haven   t used the context
during the optimization procedure.
recently
yu and dredze (2015) propose a feature-rich com-
positional transformation (fct) model which learns
weighted combination of word vectors to compose
phrase vectors, where they mainly focus on bigram
nps.

in this paper, we extend the skip-gram model to
use the phrase structure in a large corpus to capture
phrase compositionality and positional information
of both words and phrases. we jointly model words
in the context of words and phrases in the context
of phrases. additionally, we enforce a composition-
ality constraint on both the input and output phrase
embedding spaces which indicates how we build the
distributed vector representations for phrases from
their component word vectors. our results show

input projection

composition

output

distribution:

input projection

output

w(i)

w(i-2)

w(i-1)

p(i)

w(i+1)

w(i+2)

p(i-2)

p(i-1)

p(i+1)

p(i+2)

(a) word-level skip-gram

(b) phrase-level skip-gram

figure 1: architecture of the skip-gram model.

that using phrase level context information provides
gains in both word similarity and phrase similar-
ity tasks. additionally, if we model phrase-level
skip-gram over syntactic phrases, it would be helpful
for syntactic tasks like syntactic analogy and depen-
dency parsing.

2 skip-gram model

the skip-gram model learns distributed vector rep-
resentations for words by maximizing the probabil-
ity of predicting the context words given the current
word. according to the id97 implementation of
mikolov et al. (2013b), each input word w is associ-
ated with a d-dimensional vector vw     rd called the
input embedding and each context word wo is asso-
wo     rd called
ciated with a d-dimensional vector v   
the output embedding. w, wo are words from a vo-
cabulary v of size w . the id203 of observing
wo in the context of w is modeled with a softmax
function:

p (wo|w) =

t vw)

exp(v   

wo
i=1 exp(v   
wi

t vw)

pw

(1)

the denominator of this function involves a sum-
mation over the whole vocabulary, which is imprac-
tical. one alternative to deal with the complexity
issue is to sample several negative samples to avoid
computing all the vocabulary. the objective func-
tion after using negative sampling is:

ew = x

w   s

(log   (v   

wo

t vw) +

kx

i=1

log   (   v   

wi

t vw))

(2)

where s is a chunked sentence. wi, i = 1, 2, . . . , k,
are negative samples sampled from the following

3

4

p (w) = ep (w)

z

(3)

where ep (w) is the unigram distribution of words and
4 is

z is the id172 constant. the exponent 3
set empirically.

3 compositionality-aware skip-gram

model

to capture the way of composing phrase embed-
dings from distributed word vector representations,
we extend the skip-gram model to include informa-
tion from context of phrases and learn their composi-
tionality from word vectors during the optimization
procedure. our phrase-level skip-gram structure is
shown in figure 1b.

3.1 phrase-level skip-gram model
the word-level skip-gram model predicts the con-
text words given the current word vector. our
approach further models the prediction of context
phrases given the vector representation of the cur-
rent phrase vector (figure 1b). assume vp     rd
to be the d-dimensional input embedding for current
po     rd to be the output embedding
phrase p and v   
for context phrase po. using negative sampling, we
model the phrase-level id203 with:

ep = x

p   s

(log   (v   

po

t vp) +

nx

i=1

log   (   v   
pi

t vp))

(4)

where pi, i = 1, 2, . . . , n, are negative samples
sampled according to the unigram id203 of
4.
phrases raised to the same exponent 3

in this paper, we jointly model word-level skip-
gram and phrase-level skip-gram for each sentence:

e = ew +   ep

(5)

where    > 0 adjusts the relative importance of the
word-level and the phrase-level skipgram.

3.2 compositional model
assume a phrase p is composed of words
w1, . . . , wnp, where np is the number of component
words. the vector representation for p is computed
as:

vp =   (   (  (vw1), . . . ,   (vwi )))

(6)

where vp is the vector representation for p. the
function    is a component-wise manipulation over
each dimension. the symbol     is an operator over
the component word vectors, which can be linear
combination, summation, concatenation etc. the
mapping function    is a linear or non-linear manipu-
lation over the resulting vector after the     operation.
the same composition function is used to compute
pi, except
the output phrase embeddings v   
that the component word vectors are v   
wi instead of
vwi.

and v   

po

to show the effect of modeling phrase embed-
dings, we experiment with a composition function
where     is linear combination and    is passing
the resulting matrix to the left of a weight vector
lp = [lp
np] associated with each phrase p
showing how we combine the component word vec-
tors.

2, . . . , lp

1, lp

vp = [  (vw1), . . . ,   (vwnp )]

(7)

   
      

1

lp
...
lp
np

   
      

where the function    is a component-wise power
function over vector v = [v1, . . . , vn]:

  (v) = [  (v1), . . . ,   (vn)]

(8)

where   (vi) = sign(vi)|vi|  ,        1, is a power
function over each dimension. this manipulation
can be interpreted as adjusting dimensional values
of word vectors to the phrase vector space.

stochastic gradient ascent is used to update the
word vectors. in equation 4, for each word wj in p   ,
either context phrase po or negative phrase sample
pi, the gradient is:

   ep
   v   

wj

=      (v   

wj )(lp   

j (y       (v   
p   

t vp))vp)

(9)

where y = 1 for each word in po and 0 for each
wj ) is a diagonal matrix where the
word in pi.      (v   
i-th diagonal value is      (v   
). for each word wj in
the current phrase p, the gradient is:

wj i

   ep
   vwj

=      (vwj )(lp

j ((1       (v   

po

t vp))v   

po +

nx

i=1

(     (v   
pi

t vp))v   

pi))

(10)

3.3 output phrase embedding space
following ling et al. (2015) in using different out-
put embeddings at each relative position to capture
order information of context words (we call this

mitchell and lapata (2008)
id97
compositional
positional
compositional+ positional

  
0.19
0.23
0.25
0.23
0.26

table 1: spearman   s correlation using different embed-
dings

positional model), we use separate output embed-
dings to capture phrase-compositionality (we call
this compositional model). that is, we have a sep-
arate component word vector v       to compose the
phrase vectors in the context and the negative sam-
ples instead of using v   . the intuition of this choice
is that we don   t want the compositionality informa-
tion in the context or negative sample layer to be
distorted by word-level updates.

we further extend the phrase-level skipgram to
include the order information, which uses different
output id27s to compose phrases at each
relative position. phrases at the same relative posi-
tion share the same output embeddings (we call this
model positional+compositional). without loss of
generality, we experiment with the composition de-
scribed in equation 7. the coef   cients lp
i s are set to
be 1
np

.

4 experiments

we train the skip-gram model with negative sam-
pling using id97 as our baseline. we used
an april 2010 snapshot of the wikipedia cor-
pus (shaoul and westbury, 2010), which contains
approximately 2 million articles and 990 million to-
kens. we remove all words that have a frequency
less than 20 and use a context window size of 5 (5
words before and after the word occurrence). we set
the number of negative samples to be 10 and the di-
mensionality of vectors to be 300. for phrase-level
skip-gram, we also use a context window size of 5
(5 phrases before and after the phrase occurrence).

4.1 phrase compositionality
we    rst evaluate the compositional model on the
intransitive verb disambiguation dataset provided
by mitchell and lapata (2008). the dataset con-
sists of pairs of subject and intransitive verb and a
landmark intransitive verb is provided for each pair.

id97
positional
compositional
compositional+positional

word353 men
0.722
0.758
0.744
0.692
0.763
0.735
0.704
0.746

syn mixed
69.9
71.2
70.6
72.0

77.8
79.7
79.2
80.5
including

table 2: comparison of word-level tasks,
word similarity, analogy.

for example, this task requires one to identify when
taking    sale    as the subject, reference    slump    and
landmark    decline    are close to each other while
   slump    and landmark    slouch    are not. each pair
has multiple human ratings indicating how similar
the pairs are.

we use the senna toolkit to extract the pos tag la-
bels for each sentence and combine adjacent noun-
verb pairs. as the evaluation dataset is lemmatized,
we also lemmatize the wikipedia corpus to avoid
sparsity. we evaluate the cosine similarity between
the composed reference subject intransitive verb pair
and its landmarks. then we compute the spear-
man   s correlation between the similarity scores and
the human ratings. table 1 shows the result. we can
see that the compositions of subject and intransitive
verbs can be learned using the compositional model
and the best performance is achieved using the joint
model. we use    = 1 and varying    does not change
much of the performance.

4.2 word similarity and word analogy

we also consider word similarity and analogy tasks
for evaluating the quality of id27s.
word similarity measures spearman   s correlation
coef   cient between the human scores and the em-
beddings    cosine similarities for word pairs. word
analogy measures the accuracy on syntactic and se-
mantic analogy questions.

here we extract phrases that contain syntax infor-
mation, in the hope of learning additional syntac-
tic information by modeling phrase-level skip-gram.
we use the senna toolkit to identify the constituent
chunks in each sentence. we run the chunker on
20 cpus, and it takes less than 2 hours to chunk
the wikipedia 2010 corpus we use. the extracted
phrases are labeled with np, vp, pps etc.

we evaluate similarity on two tasks, wordsim-
353 and men, respectively containing 353 and 3000
word pairs. we use two word analogy datasets

id97
compositional
positional
compositional+ positional

dev
uas
92.21
92.34
92.29
92.39

test
uas
91.91
92.02
92.05
92.19

las
90.83
90.91
90.88
90.91

las
90.54
90.64
90.67
90.82

table 3: id33 results on ptb using dif-
ferent embeddings

that we call syn (8000 syntactic analogy questions)
and mixed (19544 syntactic and semantic analogy
questions).

the

on

wordsim-353

task,
neelakantan et al. (2014)
reported a spearman   s
correlation of 0.709, while their id97 baseline
is 0.704. from table 2, we can see that positional
information actually degrades the performance on
the similarity task, while only adding composi-
tional information performs the best. for syntactic
and mixed analogy tasks which involves syntax
information, we can see that using phrase-level
skip-gram and positional information both help and
combining both gives the best performance.

4.3 id33

we use the same preprocessing procedure as in the
previous task. the evaluation on dependency pars-
ing is performed on the english ptb, with the stan-
dard train, dev and test splits with stanford depen-
dencies. we use a neural network as described in
chen and manning (2014). as we are using embed-
dings with a different dimensionality, we tune the
hidden layer size and learning rate parameters for
the neural network parser by grid search for each
model and train for 15000 iterations. the other pa-
rameters are using the default settings. evaluation
is performed with the labeled (las) and unlabeled
(uas) attachment scores. we run each parameter
setting for 3 times and then average to prevent ran-
domness. we can see that by modeling phrase-level
skip-gram over syntactic phrases, the performance
on the id33 task can be improved.

5 conclusion

in this paper, we have presented a variation of
skip-gram model which learns compositionality of
phrase embeddings. our results show that model-
ing phrase-level co-occurrance and phrase composi-

tionality helps improve word and phrase similarity
tasks. if the phrases contain syntactic information,
it would also help improve syntactic tasks. as our
compositionality function is very general, it would
be interesting to see different variations and choices
of composition function in different applications.

references

[baroni and zamparelli2010] marco baroni and roberto
zamparelli. 2010. nouns are vectors, adjectives are
matrices: representing adjective-noun constructions
in semantic space. in proceedings of the 2010 confer-
ence on empirical methods in natural language pro-
cessing, emnlp    10, pages 1183   1193, stroudsburg,
pa, usa. association for computational linguistics.
[chen and manning2014] danqi chen and christopher d
manning. 2014. a fast and accurate dependency
parser using neural networks. in empirical methods
in natural language processing (emnlp).

[coecke et al.2010] bob coecke, mehrnoosh sadrzadeh,
and stephen clark. 2010. mathematical foundations
for a compositional distributional model of meaning.
corr, abs/1003.4394.

[collobert et al.2011] ronan collobert, jason weston,
leon bottou, michael karlen, koray kavukcuoglu,
and pavel kuksa. 2011. natural language process-
ing (almost) from scratch. the journal of machine
learning research, 12:2493   2537.

rabih

[devlin et al.2014] jacob
devlin,
zbib,
zhongqiang huang,
thomas lamar, richard
schwartz, and john makhoul.
fast and
robust neural network joint models for statistical
machine translation.
in proceedings of the 52nd
annual meeting of the association for computational
linguistics (volume 1: long papers), pages 1370   
1380, baltimore, maryland, june. association for
computational linguistics.

2014.

[durrett and klein2015] greg durrett and dan klein.
2015. neural crf parsing.
in proceedings of the
association for computational linguistics, beijing,
china, july. association for computational linguis-
tics.

[fyshe et al.2015] alona fyshe, leila wehbe, partha p.
talukdar, brian murphy, and tom m. mitchell. 2015.
a compositional and interpretable semantic space. in
proceedings of the 2015 conference of the north
american chapter of the association for computa-
tional linguistics: human language technologies,
pages 32   41, denver, colorado, may   june. associa-
tion for computational linguistics.

[kalchbrenner and blunsom2013] nal kalchbrenner and
phil blunsom. 2013. recurrent continuous translation
models. in emnlp, pages 1700   1709.

[lebret and collobert2015] r  emi lebret and ronan col-
lobert. 2015.    the sum of its parts   : joint learning
of word and phrase representations with autoencoders.
corr, abs/1506.05703.

[ling et al.2015] wang ling, chris dyer, alan w black,
and isabel trancoso. 2015. two/too simple adapta-
tions of id97 for syntax problems. in proceed-
ings of the 2015 conference of the north american
chapter of the association for computational lin-
guistics: human language technologies, pages 1299   
1304, denver, colorado, may   june. association for
computational linguistics.

[liu et al.2014] shujie liu, nan yang, mu li, and ming
zhou. 2014. a recursive recurrent neural network for
id151. in proceedings of the
52nd annual meeting of the association for compu-
tational linguistics (volume 1: long papers), pages
1491   1500, baltimore, maryland, june. association
for computational linguistics.

[mikolov et al.2013a] tomas mikolov, kai chen, greg
corrado, and jeffrey dean. 2013a. ef   cient estima-
tion of word representations in vector space. corr,
abs/1301.3781.

[mikolov et al.2013b] tomas mikolov,

ilya sutskever,
kai chen, greg s corrado, and jeff dean. 2013b.
distributed representations of words and phrases and
their compositionality. in advances in neural infor-
mation processing systems, pages 3111   3119.

[mitchell and lapata2008] jeff mitchell and mirella la-
pata. 2008. vector-based models of semantic com-
position.
in in proceedings of acl-08: hlt, pages
236   244.

[neelakantan et al.2014] arvind neelakantan,

jeevan
shankar, alexandre passos, and andrew mccallum.
2014. ef   cient non-parametric estimation of multiple
embeddings per word in vector space.
in proceed-
ings of the 2014 conference on empirical methods
in natural language processing (emnlp), pages
1059   1069, doha, qatar, october. association for
computational linguistics.

[shaoul and westbury2010] cyrus shaoul

and chris
westbury. 2010. the westbury lab wikipedia corpus.
[socher et al.2013] richard socher, john bauer, christo-
pher d. manning, and ng andrew y. 2013. pars-
ing with compositional vector grammars. in proceed-
ings of the 51st annual meeting of the association for
computational linguistics (volume 1: long papers),
pages 455   465, so   a, bulgaria, august. association
for computational linguistics.

[sutskever et al.2014] ilya sutskever, oriol vinyals, and
quoc v. le. 2014. sequence to sequence learning

with neural networks. in advances in neural informa-
tion processing systems, pages 3104   3112.

[yu and dredze2015] mo yu and mark dredze. 2015.
learning composition models for phrase embeddings.
transactions of the association for computational
linguistics, 3:227   242.

