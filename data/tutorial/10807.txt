6
1
0
2

 

p
e
s
4
2

 

 
 
]
l
c
.
s
c
[
 
 

2
v
3
3
6
8
0

.

4
0
6
1
:
v
i
x
r
a

word ordering without syntax

allen schmaltz and alexander m. rush and stuart m. shieber

harvard university

{schmaltz@fas,srush@seas,shieber@seas}.harvard.edu

abstract

neural networks via, in part, the network   s ability to
predict word order in simple sentences. he notes,

recent work on word ordering has argued that
syntactic structure is important, or even re-
quired, for effectively recovering the order of
a sentence. we    nd that, in fact, an id165
language model with a simple heuristic gives
strong results on this task. furthermore, we
show that a long short-term memory (lstm)
language model is even more effective at re-
covering order, with our basic model outper-
forming a state-of-the-art syntactic model by
11.5 id7 points. additional data and larger
beams yield further gains, at the expense of
training and search time.

the order of words in sentences re   ects a num-
ber of constraints. . . syntactic structure, selective
restrictions, subcategorization, and discourse con-
siderations are among the many factors which
join together to    x the order in which words oc-
cur. . . [t]here is an abstract structure which un-
derlies the surface strings and it is this structure
which provides a more insightful basis for under-
standing the constraints on word order. . . . it is,
therefore, an interesting question to ask whether a
network can learn any aspects of that underlying
abstract structure (elman, 1990).

1 introduction

recovering the origi-
we address the task of
nal word order of a shuf   ed sentence,
referred
to as bag generation (brown et al., 1990), shake-
and-bake generation (brew, 1992), or more re-
linearization, as standardized in a recent
cently,
line of
research as a method useful
iso-
lating the performance of text-to-text generation
models (zhang and clark, 2011; liu et al., 2015;
liu and zhang, 2015; zhang and clark, 2015). the
predominant argument of the more recent works is
that jointly recovering explicit syntactic structure is
crucial for determining the correct word order of the
original sentence. as such, these methods either
generate or rely on given parse structure to repro-
duce the order.

for

independently, elman (1990) explored lineariza-
tion in his seminal work on recurrent neural net-
works. elman judged the capacity of early recurrent

neural

recurrent

networks

recently,

have
reemerged as a powerful tool for learning the latent
structure of language.
in particular, work on long
short-term memory (lstm) networks for language
modeling has provided improvements in perplexity.
we revisit elman   s question by applying lstms
to the word-ordering task, without any explicit syn-
tactic modeling. we    nd that language models are
in general effective for linearization relative to ex-
isting syntactic approaches, with lstms in particu-
lar outperforming the state-of-the-art by 11.5 id7
points, with further gains observed when training
with additional text and decoding with larger beams.

2 background: linearization

the task of linearization is to recover the original or-
der of a shuf   ed sentence. we assume a vocabulary
v and are given a sequence of out-of-order phrases
x1, . . . , xn , with xn     v + for 1     n     n. de   ne
m as the total number of tokens (i.e., the sum of the
lengths of the phrases). we consider two varieties of

the task: (1) words, where each xn consists of a
single word and m = n, and (2) words+bnps,
where base noun phrases (noun phrases not con-
taining inner noun phrases) are also provided and
m     n. the second has become a standard for-
mulation in recent literature.

given input x, we de   ne the output set y to be
all possible permutations over the n elements of x,
where   y     y is the permutation generating the true
order. we aim to    nd   y, or a permutation close to
it. we produce a linearization by (approximately)
optimizing a learned scoring function f over the set
of permutations, y    = arg maxy   y f (x, y).

3 related work: syntactic linearization

recent approaches to linearization have been based
on reconstructing the syntactic structure to produce
the word order. let z represent all projective de-
pendency parse trees over m words. the objec-
tive is to    nd y   , z    = arg maxy   y ,z   z f (x, y, z)
where f is now over both the syntactic structure
and the linearization. the current state of the art
on the id32 (ptb) (marcus et al., 1993),
without external data, of liu et al. (2015) uses a
transition-based parser with id125 to con-
struct a sentence and a parse tree. the scoring func-
tion is a linear model f (x, y) =        (x, y, z) and
is trained with an early update structured percep-
tron to match both a given order and syntactic tree.
the feature function    includes features on the syn-
tactic tree. this work improves upon past work
which used best-   rst search over a similar objective
(zhang and clark, 2011).

in follow-up work, liu and zhang (2015) argue
that syntactic models yield improvements over pure
surface id165 models for the words+bnps case.
this result holds particularly on longer sentences
and even when the syntactic trees used in training
are of low quality. the id165 decoder of this work
utilizes a single beam, discarding the probabilities
of internal, non-boundary words in the bnps when
comparing hypotheses. we revisit this comparison
between syntactic models and surface-level models,
utilizing a surface-level decoder with heuristic fu-
ture costs and an alternative approach for scoring
partial hypotheses for the words+bnps case.

the
gram models for the word ordering task.
work of de gispert et al. (2014) demonstrates im-
provements over the earlier syntactic model of
zhang et al. (2012) by applying an id165 language
model over the space of word permutations re-
stricted to concatenations of phrases seen in a large
corpus. horvat and byrne (2014) models the search
for the highest id203 permutation of words un-
der an id165 model as a travelling salesman prob-
lem; however, direct comparisons to existing works
are not provided.

4 lm-based linearization

in contrast to the recent syntax-based approaches,
we use an lm directly for word ordering. we
consider two types of language models:
an n-
gram model and a long short-term memory network
(hochreiter and schmidhuber, 1997). for the pur-
pose of this work, we de   ne a common abstraction
for both models. let h     h be the current state of
the model, with h0 as the initial state. upon seeing
a word wi     v, the lm advances to a new state
hi =   (wi, hi   1). at any time, the lm can be
queried to produce an estimate of the id203 of
the next word q(wi, hi   1)     p(wi | w1, . . . , wi   1).
for id165 language models, h,   , and q can natu-
rally be de   ned respectively as the state space, tran-
sition model, and edge costs of a    nite-state ma-
chine.

lstms are a type of recurrent neural network
(id56) that are conducive to learning long-distance
dependencies through the use of an internal mem-
ory cell. existing work with lstms has gener-
ated state-of-the-art results in id38
(zaremba et al., 2014), along with a variety of other
nlp tasks.

in our notation we de   ne h as the hidden states
and cell states of a multi-layer lstm,    as the
lstm update function, and q as a    nal af   ne trans-
formation and softmax given as q(   , hi   1;   q) =
(l)
i   1 is the top hid-
softmax(w h
den layer and   q = (w , b) are parameters. we di-
rect readers to the work of graves (2013) for a full
description of the lstm update.

(l)
i   1 + b) where h

additional previous work has also explored n-

for both models, we simply de   ne the scoring

m

for m = 0, . . . , m     1 do
for k = 1, . . . , |bm| do
(y, r, s, h)     b(k)
for i     r do

algorithm 1 lm beam-search word ordering
1: procedure order(x1 . . . xn , k, g)
2: b0     h(hi, {1, . . . , n }, 0, h0)i
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:

j     m + |xi|
bj     bj + (y + xi, r     i, s   , h   )
keep top-k of bj by f (x, y) + g(r)

(s   , h   )     (s, h)
for word w in phrase xi do

s        s    + log q(w, h   )
h          (w, h   )

return bm

function as

f (x, y) =

n

x
n=1

log p(xy(n) | xy(1), . . . , xy(n   1))

where the phrase probabilities are calculated word-
by-word by our language model.

searching over all permutations y is

in-
tractable, so we instead follow past work on lin-
earization (liu et al., 2015) and lstm generation
(sutskever et al., 2014) in adapting id125 for
our generation step. our work differs from the beam
search approach for the words+bnps case of pre-
vious work in that we maintain multiple beams, as
in stack decoding for phrase-based machine trans-
lation (koehn, 2010), allowing us to incorporate
the probabilities of internal, non-boundary words
in the bnps. additionally, for both words and
words+bnps, we also include an estimate of fu-
ture cost in order to improve search accuracy.

beams,
id125 maintains m + 1
the top-
b0, . . . , bm , each containing at most
k partial hypotheses of that
length. a partial
hypothesis is a 4-tuple (y, r, s, h), where y is a
partial ordering, r is the set of remaining indices to
be ordered, s is the score of the partial linearization
f (x, y), and h is the current lm state. each step
consists of expanding all next possible phrases and
adding the next hypothesis to a later beam. the full
id125 is given in algorithm 1.

as part of the id125 scoring function we
also include a future cost g, an estimate of the score

model

zgen-64
zgen-64+pos

ngram-64 (no g)
ngram-64
ngram-512
lstm-64
lstm-512

zgen-64+lm+gw+pos
lstm-64+gw
lstm-512+gw

words words+bnps

30.9

   

32.0
37.0
38.6
40.5
42.7

   

41.1
44.5

49.4
50.8

51.3
54.3
55.6
60.9
63.2

52.4
63.1
65.8

table 1: id7 score comparison on the ptb test
set. results from previous works (for zgen) are
those provided by the respective authors, except for
the words task. the    nal number in the model
identi   er is the beam size, +gw indicates additional
gigaword data. models marked with +pos are pro-
vided with a pos dictionary derived from the ptb
training set.

contribution of the remaining elements in r. to-
gether, f (x, y) + g(r) gives a noisy estimate of the
total score, which is used to determine the k best
elements in the beam. in our experiments we use a
very simple unigram future cost estimate, g(r) =
pi   r pw   xi log p(w).
5 experiments

setup experiments are on ptb with sections 2-
21 as training, 22 as validation, and 23 as test1.
we utilize two unk types, one for initial upper-
case tokens and one for all other low-frequency to-
kens; end sentence tokens; and start/end tokens,
which are treated as words, to mark bnps for the
words+bnps task. we also use a special symbol
to replace tokens that contain at least one numeric
character. we otherwise train with punctuation and
the original case of each token, resulting in a vocab-
ulary containing around 16k types from around 1m
training tokens.

for experiments marked gw we augment

the
ptb with a subset of
the annotated giga-
word corpus (napoles et al., 2012). we follow

1in

practice,

and
liu and zhang (2015) use section 0 instead of 22 for vali-
dation (author correspondence).

in liu et al. (2015)

results

the

bnp

g

gw

1

10

64

128

256

512

   
   
   

   
   

   

   

   
   

   
   

   

   

41.7
47.6
48.4
15.4
25.0
23.8

40.6
45.7
14.6
27.1

53.6
59.4
60.1
26.8
36.8
35.5

49.7
53.6
27.1
34.6

lstm

58.0
62.2
64.2
33.8
40.7
40.7

59.1
62.9
64.9
35.3
41.7
41.7

ngram

52.6
55.6
32.6
37.5

53.2
56.2
33.8
38.1

60.0
63.6
65.6
36.5
42.0
42.9

54.0
56.6
35.1
38.4

60.6
64.3
66.2
38.0
42.5
43.7

54.7
56.6
35.8
38.7

table 2: id7 results on the validation set for
the lstm and ngram model with varying beam
sizes, future costs, additional data, and use of base
noun phrases.

liu and zhang (2015) and train on a sample of 900k
agence france-presse sentences combined with the
full ptb training set.
the gw models bene   t
from both additional data and a larger vocabulary of
around 25k types, which reduces unknowns in the
validation and test sets.

we compare the models of liu et al. (2015)
(known as zgen), a 5-gram lm using kneser-ney
smoothing (ngram)2, and an lstm. we experi-
ment on the words and words+bnps tasks, and
we also experiment with including future costs (g),
the gigaword data (gw), and varying beam size.
we retrain zgen using publicly available code3 to
replicate published results.
is

in size and
architecture
to the medium lstm setup of
zaremba et al. (2014)4. our implementation uses
the torch5 framework and is publicly available6.

the lstm model

similar

we compare the performance of the models using
the id7 metric (papineni et al., 2002). in gener-
ation if there are multiple tokens of identical unk
type, we randomly replace each with possible un-
used tokens in the original source before calculating
id7. for comparison purposes, we use the id7

2we use the kenlm language model toolkit (https://

kheafield.com/code/kenlm/).

3https://github.com/sutdnlp/zgen
4we hypothesize that additional gains are possible via a

larger model and model averaging, ceteris paribus.

5http://torch.ch
6https://github.com/allenschmaltz/word_

ordering

lstm-512

lstm-64

zgen-64

lstm-1

30

35

40

zgen-64

lstm-64

15

20

25
sentence length

)

%

(
u
e
l
b

90

70

50

30

s
n
e
k
o
t

12000

8000

4000

5

10

0
0.0

0.2

0.4

0.6

0.8

1.0

distortion rate

figure 1: experiments on the ptb validation on the
words+bnps models: (a) performance on the set
by length on sentences from length 2 to length 40.
(b) the distribution of token distortion, binned at 0.1
intervals.

script distributed with the publicly available zgen
code.

results our main results are shown in table 1.
on the words+bnps task the ngram-64 model
scores nearly 5 points higher than the syntax-
based model zgen-64.
the lstm-64 then
surpasses ngram-64 by more than 5 id7
differences on the words task are
points.
smaller, but show a similar pattern.
incorporat-
ing gigaword further increases the result another 2
points. notably, the ngram model outperforms the
combined result of zgen-64+lm+gw+pos from
liu and zhang (2015), which uses a 4-gram model
trained on gigaword. we believe this is because
the combined zgen model incorporates the id165
scores as discretized indicator features instead of us-
ing the id203 directly.7 a beam of 512 yields a
further improvement at the cost of search time.

to further explore the impact of search accuracy,

7in work of liu and zhang (2015), with the given decoder,
id165s only yielded a small further improvement over the syn-
tactic models when discretized versions of the lm probabilities
were incorporated as indicator features in the syntactic models.

model

words words+bnps

zgen-64(z    )
zgen-64

ngram-64
ngram-512
lstm-64
lstm-512

39.7
40.8

46.1
47.2
51.3
52.8

64.9
65.2

67.0
67.8
71.9
73.1

table 3: unlabeled attachment scores (uas) on
the ptb validation set after parsing and aligning
the output. for zgen we also include a result us-
ing the tree z    produced directly by the system.
for words+bnps, internal bnp arcs are always
counted as correct.

table 2 shows the results of various models with
beam widths ranging from 1 (greedy search) to 512,
and also with and without future costs g. we see that
for the better models there is a steady increase in ac-
curacy even with large beams, indicating that search
errors are made even with relatively large beams.

one proposed advantage of syntax in lineariza-
tion models is that it can better capture long-distance
relationships. figure 1 shows results by sentence
length and distortion, which is de   ned as the abso-
lute difference between a token   s index position in
y    and   y, normalized by m. the lstm model ex-
hibits consistently better performance than existing
syntax models across sentence lengths and generates
fewer long-range distortions than the zgen model.
finally, table 3 compares the syntactic    uency of
the output. as a lightweight test, we parse the output
with the yara parser (rasooli and tetreault, 2015)
and compare the unlabeled attachment scores (uas)
to the trees produced by the syntactic system. we
   rst align the gold head to each output token.
(in
cases where the alignment is not one-to-one, we ran-
domly sample among the possibilities.) the models
with no knowledge of syntax are able to recover a
higher proportion of gold arcs.

6 conclusion

strong surface-level language models recover word
order more accurately than the models trained with
explicit syntactic annotations appearing in a recent
series of papers. this has implications for the utility
of costly syntactic annotations in generation mod-

els, for both high- and low- resource languages and
domains.

acknowledgments

we thank yue zhang and jiangming liu for assis-
tance in using zgen, as well as veri   cation of the
task setup for a valid comparison. jiangming liu
also assisted in pointing out a discrepancy in the im-
plementation of an earlier version of our ngram
decoder, the resolution of which improved id7
performance.

references

[brew1992] chris brew. 1992. letting the cat out of the
bag: generation for shake-and-bake mt. in proceed-
ings of the 14th conference on computational lin-
guistics - volume 2, coling    92, pages 610   616,
stroudsburg, pa, usa. association for computational
linguistics.

[brown et al.1990] peter f brown, john cocke, stephen
a della pietra, vincent j della pietra, fredrick je-
linek, john d lafferty, robert l mercer, and paul s
roossin. 1990. a statistical approach to machine
translation. computational linguistics, 16(2):79   85.

[de gispert et al.2014] adri`a de gispert, marcus tomalin,
and bill byrne. 2014. word ordering with phrase-
based grammars. in proceedings of the 14th confer-
ence of the european chapter of the association for
computational linguistics, pages 259   268, gothen-
burg, sweden, april. association for computational
linguistics.

[elman1990] jeffrey l. elman. 1990. finding structure

in time. cognitive science, 14(2):179     211.

[graves2013] alex graves.

2013.

quences with recurrent neural networks.
abs/1308.0850.

generating se-
corr,

[hochreiter and schmidhuber1997] sepp hochreiter and
j  urgen schmidhuber. 1997. long short-term memory.
neural comput., 9(8):1735   1780, november.

[horvat and byrne2014] matic horvat

and william
byrne.
2014. a graph-based approach to string
regeneration. in proceedings of the student research
workshop at the 14th conference of the european
chapter of the association for computational lin-
guistics, pages 85   95, gothenburg, sweden, april.
association for computational linguistics.

[koehn2010] philipp koehn. 2010. statistical machine
translation. cambridge university press, new york,
ny, usa, 1st edition.

[zhang et al.2012] yue zhang, graeme blackwood, and
stephen clark. 2012. syntax-based word ordering in-
corporating a large-scale language model. in proceed-
ings of the 13th conference of the european chap-
ter of the association for computational linguistics,
eacl    12, pages 736   746, stroudsburg, pa, usa.
association for computational linguistics.

[liu and zhang2015] jiangming liu and yue zhang.
2015. an empirical comparison between id165 and
syntactic language models for word ordering. in pro-
ceedings of the 2015 conference on empirical meth-
ods in natural language processing, pages 369   378,
lisbon, portugal, september. association for compu-
tational linguistics.

[liu et al.2015] yijia liu, yue zhang, wanxiang che, and
bing qin. 2015. transition-based syntactic lineariza-
tion.
in proceedings of the 2015 conference of the
north american chapter of the association for com-
putational linguistics: human language technolo-
gies, pages 113   122, denver, colorado, may   june.
association for computational linguistics.

[marcus et al.1993] mitchell p. marcus, beatrice san-
torini, and mary ann marcinkiewicz. 1993. building
a large annotated corpus of english: the penn tree-
bank. computational linguistics, 19(2):313   330.

[napoles et al.2012] courtney napoles, matthew gorm-
ley, and benjamin van durme. 2012. annotated gi-
gaword. in proceedings of the joint workshop on au-
tomatic knowledge base construction and web-scale
knowledge extraction, akbc-wekex    12, pages
95   100, stroudsburg, pa, usa. association for com-
putational linguistics.

[papineni et al.2002] kishore papineni, salim roukos,
todd ward, and wei-jing zhu.
2002. id7: a
method for automatic evaluation of machine transla-
tion. in proceedings of the 40th annual meeting on
association for computational linguistics, acl    02,
pages 311   318, stroudsburg, pa, usa. association
for computational linguistics.

[rasooli and tetreault2015] mohammad sadegh rasooli
and joel r. tetreault. 2015. yara parser: a fast and
accurate dependency parser. corr, abs/1503.06733.
[sutskever et al.2014] ilya sutskever, oriol vinyals, and
quoc vv le. 2014. sequence to sequence learning
with neural networks. in advances in neural informa-
tion processing systems, pages 3104   3112.

[zaremba et al.2014] wojciech zaremba, ilya sutskever,
and oriol vinyals. 2014. recurrent neural network
id173. corr, abs/1409.2329.

[zhang and clark2011] yue zhang and stephen clark.
2011. syntax-based grammaticality improvement us-
ing id35 and guided search. in proceedings of the con-
ference on empirical methods in natural language
processing, emnlp    11, pages 1147   1157, strouds-
burg, pa, usa. association for computational lin-
guistics.

[zhang and clark2015] yue zhang and stephen clark.
2015. discriminative syntax-based word ordering for
text generation. comput. linguist., 41(3):503   538,
september.

