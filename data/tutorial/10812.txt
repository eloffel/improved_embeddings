stochastic language generation in dialogue using recurrent neural

networks with convolutional sentence reranking

tsung-hsien wen, milica ga  si  c, dongho kim, nikola mrk  si  c,

pei-hao su, david vandyke and steve young
cambridge university engineering department,
trumpington street, cambridge, cb2 1pz, uk

{thw28,mg436,dk449,nm480,phs26,djv27,sjy}@cam.ac.uk

5
1
0
2

 

g
u
a
7

 

 
 
]
l
c
.
s
c
[
 
 

1
v
5
5
7
1
0

.

8
0
5
1
:
v
i
x
r
a

abstract

the id86 (id86)
component of a spoken dialogue system
(sds) usually needs a substantial amount
of handcrafting or a well-labeled dataset to
be trained on. these limitations add sig-
ni   cantly to development costs and make
cross-domain, multi-lingual dialogue sys-
tems intractable. moreover, human lan-
guages are context-aware. the most nat-
ural response should be directly learned
from data rather than depending on pre-
de   ned syntaxes or rules.
this paper
presents a statistical language generator
based on a joint recurrent and convolu-
tional neural network structure which can
be trained on dialogue act-utterance pairs
without any semantic alignments or pre-
de   ned grammar trees. objective metrics
suggest that this new model outperforms
previous methods under the same experi-
mental conditions. results of an evalua-
tion by human judges indicate that it pro-
duces not only high quality but linguisti-
cally varied utterances which are preferred
compared to id165 and rule-based sys-
tems.

1

introduction

conventional spoken dialogue systems (sds) are
expensive to build because many of the process-
ing components require a substantial amount of
handcrafting (ward and issar, 1994; bohus and
rudnicky, 2009).
in the past decade, signif-
icant progress has been made in applying sta-
tistical methods to automate the speech under-
standing and dialogue management components of
an sds, including making them more easily ex-
tensible to other application domains (young et
al., 2013; ga  si  c et al., 2014; henderson et al.,

2014). however, due to the dif   culty of col-
lecting semantically-annotated corpora, the use of
data-driven id86 for sds remains relatively un-
explored and rule-based generation remains the
norm for most systems (cheyer and guzzoni,
2007; mirkovic and cavedon, 2011).

the goal of the id86 component of an sds is
to map an abstract dialogue act consisting of an
act type and a set of attribute-value pairs1 into
an appropriate surface text (see table 1 below
for some examples). an early example of a sta-
tistical id86 system is halogen by langkilde
and knight (1998) which uses an id165 language
model (lm) to rerank a set of candidates gener-
ated by a handcrafted generator.
in order to re-
duce the amount of handcrafting and make the
approach more useful in sds, oh and rudnicky
(2000) replaced the handcrafted generator with a
set of word-based id165 lm-based generators,
one for each dialogue type and then reranked the
generator outputs using a set of rules to produce
the    nal response. although oh and rudnicky
(2000)   s approach limits the amount of handcraft-
ing to a small set of post-processing rules, their
system incurs a large computational cost in the
over-generation phase and it is dif   cult to en-
sure that all of the required semantics are cov-
ered by the selected output. more recently, a
phrase-based id86 system called bagel trained
from utterances aligned with coarse-grained se-
mantic concepts has been described (mairesse et
al., 2010; mairesse and young, 2014). by im-
plicitly modelling paraphrases, bagel can generate
linguistically varied utterances. however, collect-
ing semantically-aligned corpora is expensive and
time consuming, which limits bagel   s scalability
to new domains.

this paper presents a neural network based
id86 system that can be fully trained from dia-

1here and elsewhere, attributes are frequently referred to

as slots.

log act-utterance pairs without any semantic align-
ments between the two. we start in section 3 by
presenting a generator based on a recurrent neural
network language model (id56lm) (mikolov et
al., 2010; mikolov et al., 2011a) which is trained
on a delexicalised corpus (henderson et al., 2014)
whereby each value has been replaced by a symbol
representing its corresponding slot. in a    nal post-
processing phase, these slot symbols are converted
back to the corresponding slot values.

while generating, the id56 generator is condi-
tioned on an auxiliary dialogue act feature and a
controlling gate to over-generate candidate utter-
ances for subsequent reranking.
in order to ac-
count for arbitrary slot-value pairs that cannot be
routinely delexicalized in our corpus, section 3.1
describes a convolutional neural network (id98)
(collobert and weston, 2008; kalchbrenner et al.,
2014) sentence model which is used to validate
the semantic consistency of candidate utterances
during reranking. finally, by adding a backward
id56lm reranker into the model in section 3.2,
output    uency is further improved. training and
decoding details of the proposed system are de-
scribed in section 3.3 and 3.4.

section 4 presents an evaluation of the proposed
system in the context of an application providing
information about restaurants in the san francisco
area. in section 4.2, we    rst show that new gener-
ator outperforms oh and rudnicky (2000)   s utter-
ance class lm approach using objective metrics,
whilst at the same time being more computation-
ally ef   cient. in order to assess the subjective per-
formance of our system, pairwise preference tests
are presented in section 4.3. the results show
that our approach can produce high quality utter-
ances that are considered to be more natural than
a rule-based generator. moreover, by sampling ut-
terances from the top reranked output, our system
can also generate linguistically varied utterances.
section 4.4 provides a more detailed analysis of
the contribution of each component of the system
to the    nal performance. we conclude with a brief
summary and future work in section 5.

2 related work

conventional approaches to id86 typically divide
the task into sentence planning, and surface re-
alisation. sentence planning maps input seman-
tic symbols into an intermediary tree-like or tem-
plate structure representing the utterance, then sur-

face realisation converts the intermediate structure
into the    nal text (walker et al., 2002; stent et
al., 2004; dethlefs et al., 2013). as noted above,
one of the    rst statistical id86 methods that re-
quires almost no handcrafting or semantic align-
ments was an id165 based approach by oh and
rudnicky (2000). ratnaparkhi (2002) later ad-
dressed the limitations of id165 lms in the over-
generation phase by using a more sophisticated
generator based on a syntactic dependency tree.

statistical approaches have also been studied
for sentence planning, for example, generating
the most likely context-free derivations given a
corpus (belz, 2008) or maximising the expected
reward using id23 (rieser and
lemon, 2010). angeli et al. (2010) train a set
of id148 to predict individual gen-
eration decisions given the previous ones, using
only domain-independent features. along simi-
lar lines, by casting id86 as a template extraction
and reranking problem, kondadadi et al. (2013)
show that outputs produced by an id166 reranker
are comparable to human-authored texts.

the use of neural network-based approaches to
id86 is relatively unexplored. the stock reporter
system ana by kukich (1987) is a network based
id86 system, in which the generation task is di-
vided into a sememe-to-morpheme network fol-
lowed by a morpheme-to-phrase network. recent
advances in recurrent neural network-based lan-
guage models (id56lm) (mikolov et al., 2010;
mikolov et al., 2011a) have demonstrated the
value of distributed representations and the abil-
ity to model arbitrarily long dependencies for both
id103 and machine translation tasks.
sutskever et al. (2011) describes a simple vari-
ant of the id56 that can generate meaningful sen-
tences by learning from a character-level corpus.
more recently, karpathy and fei-fei (2014) have
demonstrated that an id56lm is capable of gener-
ating image descriptions by conditioning the net-
work model on a pre-trained convolutional image
feature representation. this work provides a key
inspiration for the system described here. zhang
and lapata (2014) describes interesting work us-
ing id56s to generate chinese poetry.

a speci   c requirement of id86 for dialogue
systems is that the concepts encoded in the ab-
stract system dialogue act must be conveyed ac-
curately by the generated surface utterance, and
simple unconstrained id56lms which rely on em-

bedding at the word level (mikolov et al., 2013;
pennington et al., 2014) are rather poor at this.
as a consequence, new methods have been in-
vestigated to learn distributed representations for
phrases and even sentences by training models
using different structures (collobert and weston,
2008; socher et al., 2013). convolutional neural
networks (id98s) were    rst studied in computer
vision for object recognition (lecun et al., 1998).
by stacking several convolutional-pooling layers
followed by a fully connected feed-forward net-
work, id98s are claimed to be able to extract sev-
eral levels of translational-invariant features that
are useful in classi   cation tasks. the convolu-
tional sentence model (kalchbrenner et al., 2014;
kim, 2014) adopts the same methodology but col-
lapses the two dimensional convolution and pool-
ing process into a single dimension. the resulting
model is claimed to represent the state-of-the-art
for many speech and nlp related tasks (kalch-
brenner et al., 2014; sainath et al., 2013).

3 recurrent generation model

figure 1: an unrolled view of the id56-based
generation model. it operates on a delexicalised
utterance and a 1-hot encoded feature vector spec-
i   ed by a dialogue act type and a set of slot-value
pairs.     indicates the gate used for controlling the
on/off states of certain feature values. the output
connection layer is omitted here for simplicity.

the generation model proposed in this paper is
based on an id56lm architecture (mikolov et al.,
2010) in which a 1-hot encoding wt of a token2
wt is input at each time step t conditioned on a re-
current hidden layer ht and outputs the id203
distribution of the next token wt+1. therefore, by
sampling input tokens one by one from the output
distribution of the id56 until a stop sign is gen-

2we use token instead of word because our model oper-
ates on text for which slot names and values have been delex-
icalised.

erated (karpathy and fei-fei, 2014) or some re-
quired constraint is satis   ed (zhang and lapata,
2014), the network can produce a sequence of to-
kens which can be lexicalised to form the required
utterance.

in order to ensure that the generated utterance
represents the intended meaning, the input vec-
tors wt are augmented by a control vector f con-
structed from the concatenation of 1-hot encod-
ings of the required dialogue act and its associated
slot-value pairs. the auxiliary information pro-
vided by this control vector tends to decay over
time because of the vanishing gradient problem
(mikolov and zweig, 2012; bengio et al., 1994).
hence, f is reapplied to the id56 at every time step
as in karpathy and fei-fei (2014).

in detail, the recurrent generator shown in fig-

ure 1 is de   ned as follows:

ht = sigmoid(whhht   1 + wwhwt + wf hft)

p (wt+1|wt, wt   1, ...w0, ft) = sof tmax(whoht)

wt+1     p (wt+1|wt, wt   1, ...w0, ft)

(1)

(2)

(3)

where whh, wwh, wf h, and who are the
learned network weight matrices. ft is a gated ver-
sion of f designed to discourage duplication of in-
formation in the generated output in which each
segment fs of the control vector f corresponding
to slot s is replaced by

fs,t = fs (cid:12)   t   ts

(4)

where ts is the time at which slot s    rst appears
in the output,        1 is a decay factor, and (cid:12) de-
notes element-wise multiplication. the effect of
this gating is to decrease the id203 of regen-
erating slot symbols that have already been gener-
ated, and to increase the id203 of rendering
all of the information encoded in f.

the tokenisation resulting from delexicalising
slots and values does not work for all cases.
for example,
some slot-value pairs such as
food=dont care or kids allowed=false cannot be
directly modelled using this technique because
there is no explicit value to delexicalise in the
training corpus. as a consequence, the model is
prone to errors when these slot-value pairs are re-
quired. a further problem is that the id56lm gen-
erator selects words based only on the preceding
history, whereas some sentence forms depend on
the backward context.

figure 2: our simple variant of id98 sentence model as described in kalchbrenner et al. (2014).

to deal with these issues, candidates gener-
ated by the id56lm are reranked using two mod-
els. firstly, a convolutional neural network (id98)
sentence model (kalchbrenner et al., 2014; kim,
2014) is used to ensure that the required dialogue
act and slot-value pairs are represented in the gen-
erated utterance, including the non-standard cases.
secondly, a backward id56lm is used to rerank
utterances presented in reverse order.

3.1 convolutional sentence model
the id98 sentence model is shown in figure 2.
given a candidate utterance of length n, an utter-
ance matrix u is constructed by stacking embed-
dings wt of each token in the utterance:

            

u =

w0
w1
...
wn   1

             .

(5)

a set of k convolutional mappings are then ap-
plied to the utterance to form a set of feature detec-
tors. the outputs of these detectors are combined
and fed into a fully-connected feed-forward net-
work to classify the action type and whether each
required slot is mentioned or not.
each mapping k consists of a one-dimensional
convolution between a    lter mk     rm and the
utterance matrix u to produce another matrix ck:

ck

i,j = mk

(cid:124)

ui   m+1:i,j

(6)

column of ck are then pooled by averaging3 over
time:

:,0,   ck

:,1, ...,   ck

:,h   1

(7)

hk =(cid:2)   ck

(cid:3)

where h is the size of embedding and k = 1 . . . k.
last, the k pooled feature vectors hk are passed
through a nonlinearity function to obtain the    nal
feature map.

3.2 backward id56 reranking
as noted earlier, the quality of an id56 language
model may be improved if both forward and back-
ward contexts are considered. previously, bidi-
rectional id56s (schuster and paliwal, 1997) have
been shown to be effective for handwriting recog-
nition (graves et al., 2008), id103
(graves et al., 2013), and machine translation
(sundermeyer et al., 2014). however, applying
a bidirectional id56 directly in our generator is
not straightforward since the generation process is
sequential in time. hence instead of integrating
the bidirectional information into a single uni   ed
network, the forward and backward contexts are
utilised separately by    rstly generating candidates
using the forward id56 generator, then using the
log-likelihood computed by a backward id56lm
to rerank the candidates.

3.3 training
overall the proposed generation architecture re-
quires three models to be trained: a forward id56
generator, a id98 reranker, and a backward id56
reranker. the objective functions for training the

where m is the    lter size, and i,j is the row and
column index respectively. the outputs of each

3max pooling was also tested but was found to be inferior

to average pooling

two id56 models are the cross id178 errors be-
tween the predicted word distribution and the ac-
tual word distribution in the training corpus, whilst
the objective for the id98 model is the cross en-
tropy error between the predicted dialogue act and
the actual dialogue act, summed over the act type
and each slot. an l2 regularisation term is added to
the objective function for every 10 training exam-
ples as suggested in mikolov et al. (2011b). the
three networks share the same set of word em-
beddings, initialised with pre-trained word vectors
provided by pennington et al. (2014). all costs
and gradients are computed and stochastic gra-
dient descent is used to optimise the parameters.
both id56s were trained with back propagation
through time (werbos, 1990). in order to prevent
over   tting, early stopping was implemented using
a held-out validation set.

3.4 decoding
the decoding procedure is split into two phases:
(a) over-generation, and (b) reranking. in the over-
generation phase, the forward id56 generator con-
ditioned on the given dialogue act,
is used to
sequentially generate utterances by random sam-
pling of the predicted next word distributions. in
the reranking phase, the hamming loss costcn n
of each candidate is computed using the id98
sentence model and the log-likelihood costbrn n
is computed using the backward id56. together
with the log-likelihood costf rn n from the for-
ward id56, the reranking score r is computed as:

r =    (costf rn n + costbrn n + costcn n ).

(8)

this is the reranking criterion used to analyse each
individual model in section 4.4.

generation quality can be further improved by
introducing a slot error criterion err, which is
the number of slots generated that is either redun-
dant or missing. this is also used in oh and rud-
nicky (2000). adding this to equation (8) yields
the    nal reranking score r   :

r    =    (costf rn n + costbrn n +

costcn n +   err)

(9)

in order to severely penalise nonsensical utter-
ances,    is set to 100 for both the proposed id56
system and our implementation of oh and rud-
nicky (2000)   s id165 based system. this rerank-
ing criterion is used for both the automatic evalu-
ation in section 4.2 and the human evaluation in
section 4.3.

4 experiments
4.1 experimental setup
the target application area for our generation sys-
tem is a spoken dialogue system providing infor-
mation about restaurants in san francisco. there
are 8 system dialogue act types such as inform to
present information about restaurants, con   rm to
check that a slot value has been recognised cor-
rectly, and reject to advise that the user   s con-
straints cannot be met (table 1 gives the full list
with examples); and there are 12 attributes (slots):
name, count, food, near, price, pricerange, post-
code, phone, address, area, goodformeal, and kid-
sallowed, in which all slots are categorical except
kidsallowed which is binary.

to form a training corpus, dialogues from a set
of 3577 dialogues collected in a user trial of a
statistical dialogue manager proposed by young
et al. (2013) were randomly sampled and shown
to workers recruited via the amazon mechanical
turk service. workers were shown each dialogue
turn by turn and asked to enter an appropriate
system response in natural english corresponding
to each system dialogue act. the resulting cor-
pus contains 5193 hand-crafted system utterances
from 1006 randomly sampled dialogues. each cat-
egorical value was replaced by a token represent-
ing its slot, and slots that appeared multiple times
in a dialogue act were merged into one. this re-
sulted in 228 distinct dialogue acts.

the system was implemented using the theano
library (bergstra et al., 2010; bastien et al., 2012).
the system was trained by partitioning the 5193
utterances into a training set, validation set, and
testing set in the ratio 3:1:1, respectively. the
frequency of each action type and slot-value pair
differs quite markedly across the corpus, hence
up-sampling was used to make the corpus more
uniform. since our generator works stochasti-
cally and the trained networks can differ depend-
ing on the initialisation, all the results shown be-
low4 were averaged over 10 randomly initialised
networks. the id7-4 metric was used for the
objective evaluation (papineni et al., 2002). mul-
tiple references for each test dialogue act were ob-
tained by mapping them back to the 228 distinct
dialogue acts, merging those delexicalised tem-
plates that have the same dialogue act speci   ca-
tion, and then lexicalising those templates back to
4except human evaluation, in which only one set of net-

work was used.

table 1: the 8 system dialogue acts with example realisations

# dialogue act and example realisations of our system, by sampling from top-5 candidates
1

inform(name=   stroganoff restaurant   ,pricerange=cheap,near=      shermans wharf   )
stroganoff restaurant is a cheap restaurant near    shermans wharf .
stroganoff restaurant is in the cheap price range near    shermans wharf .
reject(kidsallowed=yes,food=   basque   )
unfortunately there are 0 restaurants that allow kids and serve basque .
informonly(name=   bund shanghai restaurant   , food=   shanghainese   )
i apologize , no other restaurant except bund shanghai restaurant that serves shanghainese .
sorry but there is no place other than the restaurant bund shanghai restaurant for shanghainese .
con   rm(goodformeal=dontcare)
i am sorry . just to con   rm . you are looking for a restaurant good for any meal ?
can i con   rm that you do not care about what meal they offer ?
request(near)
would you like to dine near a particular location ?
reqmore()
is there anything else i can do for you ?
select(kidsallowed=yes, kidsallowed=no)
are you looking for a restaurant that allows kids , or does not allow kids ?
goodbye()
thank you for calling . good bye .

2

3

4

5

6

7

8

table 2: comparison of top-1 utterance between
the id56-based system and three baselines. a
two-tailed wilcoxon rank sum test was applied to
compare the id56 model with the best o&r sys-
tem (the 3-slot, 5g con   guration) over 10 random
seeds. (*=p<.005)

method
handcrafted
knn
o&r,0-slot,5g
o&r,1-slot,5g
o&r,2-slot,5g
o&r,3-slot,3g
o&r,3-slot,4g
o&r,3-slot,5g
our model

0

beam id7 err
n/a
n/a
1/20
1/20
1/20
1/20
1/20
1/20
1/20

0.440
0.591
0.527
0.610
0.719
0.760
0.758
0.757
0.777*

17.2
635.2
460.8
142.0
74.4
53.2
47.8
0*

form utterances. in addition, the slot error (err)
as described in section 3.4, out of 1848 slots in
1039 testing examples, was computed alongside
the id7 score.

4.2 empirical comparison

as can be seen in table 2, we compare our pro-
posed id56-based method with three baselines:
a handcrafted generator, a k-nearest neighbour
method (knn), and oh and rudnicky (2000)   s
id165 based approach (o&r). the handcrafted
generator was tuned over a long period of time
and has been used frequently to interact with real
users. we found its performance is reliable and
robust. the knn was performed by computing

figure 3: comparison of our method (id56) with
o&r   s approach (5g) in terms of optimising top-5
results over different selection beams.

the similarity of the testing dialogue act 1-hot
vector against all training examples. the most
similar template in the training set was then se-
lected and lexicalised as the testing realisation.
we found our id56 generator signi   cantly out-
performs these two approaches. while compar-
ing with the o&r system, we found that by par-
titioning the corpus into more and more utterance
classes, the o&r system can also reach a id7
score of 0.76. however, the slot error cannot be
ef   ciently reduced to zero even when using the er-
ror itself as a reranking criterion. this problem is
also noted in mairesse and young (2014).

in contrast, the id56 system produces utter-
ances without slot errors when reranking using the
same number of candidates, and it achieves the
highest id7 score. figure 3 compares the id56
system with o&r   s system when randomly select-

table 3: pairwise comparison between four systems. two quality evaluations (rating out of 5) and one
preference test were performed in each case. statistical signi   cance was computed using a two-tailed
wilcoxon rank sum test and a two-tailed binomial test (*=p<.05, **=p<.005).
id565
metrics

id565

handcrafted
id561
148 dialogs, 829 utt.
3.81
3.74**
55.2%*

3.75
3.58
44.8%

handcrafted
id565
148 dialogs, 814 utt.
3.93*
3.94**
62.8%**

3.85
3.57
37.2%

id561
144 dialogs, 799 utt.
3.75
3.67
47.5%

3.72
3.58
52.5%

o&r5
145 dialogs, 841 utt.
4.02
3.91
47.1%

4.15*
4.02
52.9%

info.
nat.
pref.

ing from the top-5 ranked results in order to intro-
duce linguistic diversity. results suggest that al-
though o&r   s approach improves as the selection
beam increases, the id56-based system is still bet-
ter in both metrics. furthermore, the slot error of
the id56 system drops to zero when the selection
beam is around 50. this indicates that the id56
system is capable of generating paraphrases by
simply increasing the number of candidates dur-
ing the over-generation phase.

4.3 human evaluation
whilst automated metrics provide useful informa-
tion for comparing different systems, human test-
ing is needed to assess subjective quality. to do
this, about 60 judges were recruited using amazon
mechanical turk and system responses were gen-
erated for the remaining 2571 unseen dialogues
mentioned in section 4.1. each judge was then
shown a randomly selected dialogue, turn by turn.
at each turn, two utterances were generated from
two different systems and presented to the judge
who was asked to score each utterance in terms
of informativeness and naturalness (rating out of
5), and also asked to state a preference between
the two taking account of the given dialogue act
and the dialogue context. here informativeness is
de   ned as whether the utterance contains all the
information speci   ed in the dialogue act, and nat-
uralness is de   ned as whether the utterance could
have been produced by a human. the trial was run
pairwise across four systems: the id56 system us-
ing 1-best utterance id561, the id56 system sam-
pling from the top 5 utterances id565, the o&r
approach sampling from top 5 utterances o&r5,
and a handcrafted baseline.

the result is shown in table 3. as can be
seen, the human judges preferred both id561 and
id565 compared to the rule-based generator and
the preference is statistically signi   cant. further-
more, the id56 systems scored higher in both in-
formativeness and naturalness metrics, though the
difference for informativeness is not statistically

signi   cant. when comparing id561 with id565,
id561 was judged to produce higher quality ut-
terances but overall the diversity of output offered
by id565 made it the preferred system. even
though the preference is not statistically signi   -
cant, it echoes previous    ndings (pon-barry et al.,
2006; mairesse and young, 2014) that showed that
language variability by id141 in dialogue
systems is generally bene   cial. lastly, id565 was
thought to be signi   cantly better than o&r in
terms of informativeness. this result veri   ed our
   ndings in section 4.2 that o&r suffers from high
slot error rates compared to the id56 system.

4.4 analysis
in order to better understand the relative contribu-
tion of each component in the id56-based gener-
ation process, a system was built in stages train-
ing    rst only the forward id56 generator, then
adding the id98 reranker, and    nally the whole
model including the backward id56 reranker. ut-
terance candidates were reranked using equation
(8) rather than (9) to minimise manual interven-
tion. as previously, the id7 score and slot error
(err) were measured.
gate
the forward id56 generator was trained
   rst with different feature gating factors   . using
a selection beam of 20 and selecting the top 5 ut-
terances, the result is shown in figure 4 for   =1 is
(equivalent to not using the gate),   =0.7, and   =0
(equivalent to turning off the feature immediately
its corresponding slot has been generated). as can
be seen, use of the feature gating substantially im-
proves both id7 score and slot error, and the
best performance is achieved by setting   =0.
id98
the feature-gated forward id56 gen-
erator was then extended by adding a single
convolutional-pooling layer id98 reranker. as
shown in figure 5, evaluation was performed on
both the original dataset (all) and the dataset con-
taining only binary slots and don   t care values
(hard). we found that the id98 reranker can better
handle slots and values that cannot be explicitly

figure 4: feature gating effect

figure 5: id98 effect

figure 6: backward id56 effect

lastly,

delexicalised (1.5% improvement on hard com-
paring to 1% less on all).
backward id56
the backward id56
reranker was added and trained to give the full
generation model. the selection beam was    xed
at 100 and the n-best top results from which to
select the output utterance was varied as n = 1,
5 and 10, trading accuracy for linguistic diversity.
in each case, the id7 score was computed with
and without the backward id56 reranker. the re-
sults shown in figure 6 are consistent with sec-
tion 4.2, in which id7 score degraded as more
n-best utterances were chosen. as can be seen,
the backward id56 reranker provides a stable im-
provement no matter which value n is.
training corpus size
finally, figure 7 shows
the effect of varying the size of the training cor-
pus. as can be seen, if only the 1-best utterance
is offered to the user, then around 50% of the data
(2000 utterances) is suf   cient. however, if the lin-
guistic variability provided by sampling from the
top-5 utterances is required, then the    gure sug-
gest that more than 4156 utterances in the current
training set are required.

figure 7: networks trained with different propor-
tion of data evaluated on two selection schemes.

5 conclusion and future work

in this paper a neural network-based natural lan-
guage generator has been presented in which a for-
ward id56 generator, a id98 reranker, and back-
ward id56 reranker are jointly optimised to gen-
erate utterances conditioned by the required dia-
logue act. the model can be trained on any cor-
pus of dialogue act-utterance pairs without any se-
mantic alignment and heavy feature engineering or
handcrafting. the id56-based generator is com-
pared with an id165 based generator which uses
similar information. the id165 generator can
achieve similar id7 scores but it is less ef   cient
and prone to making errors in rendering all of the
information contained in the input dialogue act.

an evaluation by human judges indicated that
our system can produce not only high quality but
linguistically varied utterances. the latter is par-
ticularly important in spoken dialogue systems
where frequent repetition of identical output forms
t.

the work reported in this paper is part of a
larger programme to develop techniques for im-
plementing open domain spoken dialogue. a key
potential advantage of neural network based lan-
guage processing is the implicit use of distributed
representations for words and a single compact
parameter encoding of a wide range of syntac-
tic/semantic forms. this suggests that it should
be possible to transfer a well-trained generator of
the form proposed here to a new domain using a
much smaller set of adaptation data. this will be
the focus of our future work in this area.

6 acknowledgements
tsung-hsien wen and david vandyke are sup-
ported by toshiba research europe ltd, cam-
bridge research laboratory.

references
gabor angeli, percy liang, and dan klein. 2010. a
simple domain-independent probabilistic approach
to generation. in proceedings of the 2010 confer-
ence on empirical methods in natural language
processing, emnlp    10, pages 502   512. associa-
tion for computational linguistics.

fr  ed  eric bastien, pascal lamblin, razvan pascanu,
james bergstra, ian j. goodfellow, arnaud berg-
eron, nicolas bouchard, and yoshua bengio. 2012.
theano: new features and speed improvements.
deep learning and unsupervised id171
nips 2012 workshop.

anja belz. 2008. automatic generation of weather
forecast
texts using comprehensive probabilistic
generation-space models. natural language engi-
neering, 14(4):431   455, october.

yoshua bengio, patrice simard, and paolo frasconi.
1994. learning long-term dependencies with gra-
dient descent is dif   cult. neural networks, ieee
transactions on, 5(2):157   166.

james bergstra, olivier breuleux, fr  ed  eric bastien,
pascal lamblin, razvan pascanu, guillaume des-
jardins, joseph turian, david warde-farley, and
yoshua bengio. 2010. theano: a cpu and gpu
in proceedings of the
math expression compiler.
python for scienti   c computing conference.

dan bohus and alexander i. rudnicky. 2009. the
ravenclaw dialog management framework: archi-
tecture and systems. computer speech and lan-
guage, 23(3):332   361, july.

adam cheyer and didier guzzoni. 2007. method and
apparatus for building an intelligent automated as-
sistant. us patent app. 11/518,292.

ronan collobert and jason weston. 2008. a uni   ed
architecture for natural language processing: deep
in pro-
neural networks with multitask learning.
ceedings of the 25th international conference on
machine learning, icml    08, pages 160   167.

nina dethlefs, helen hastie, heriberto cuayhuitl, and
oliver lemon. 2013. conditional random    elds for
responsive surface realisation using global features.
in in proceedings of acl.

milica ga  si  c, dongho kim, pirros tsiakoulis, cather-
ine breslin, matthew henderson, martin szummer,
blaise thomson, and steve young. 2014.
incre-
mental on-line adaptation of pomdp-based dialogue
managers to extended domains. in in proceedings
on interspeech.

alex graves, marcus liwicki, horst bunke, j  urgen
schmidhuber, and santiago fern  andez. 2008. un-
constrained on-line handwriting recognition with re-
current neural networks. in advances in neural in-
formation processing systems, pages 577   584.

alex graves, a-r mohamed, and geoffrey hinton.
2013. id103 with deep recurrent neural
networks. in acoustics, speech and signal process-
ing (icassp), 2013 ieee international conference
on, pages 6645   6649. ieee.

matthew henderson, blaise thomson, and steve
young. 2014. robust dialog state tracking using
delexicalised recurrent neural networks and unsu-
pervised adaptation. in proceedings of ieee spoken
language technology.

nal kalchbrenner, edward grefenstette, and phil blun-
som. 2014. a convolutional neural network for
modelling sentences. proceedings of the 52nd an-
nual meeting of the association for computational
linguistics, june.

andrej karpathy and li fei-fei. 2014. deep visual-
semantic alignments for generating image descrip-
tions. corr, abs/1412.2306.

yoon kim.

2014. convolutional neural networks
in proceedings of the
for sentence classi   cation.
2014 conference on empirical methods in natu-
ral language processing (emnlp), pages 1746   
1751, doha, qatar, october. association for com-
putational linguistics.

ravi kondadadi, blake howald, and frank schilder.
2013. a statistical id86 framework for aggregated
planning and realization. in proceedings of the 51st
annual meeting of the association for computa-
tional linguistics (volume 1: long papers), pages
1406   1415, so   a, bulgaria, august. association for
computational linguistics.

karen kukich.

1987. where do phrases come
from: some preliminary experiments in connection-
ist phrase generation. in natural language gener-
ation, volume 135 of nato asi series, pages 405   
421. springer netherlands.

irene langkilde and kevin knight. 1998. generation
that exploits corpus-based statistical knowledge. in
proceedings of the 36th annual meeting of the as-
sociation for computational linguistics, acl    98,
pages 704   710.

yann lecun, le  on bottou, yoshua bengio, and patrick
haffner. 1998. gradient-based learning applied to
document recognition. proceedings of the ieee,
86(11):2278   2324, nov.

franc  ois mairesse and steve young. 2014. stochas-
tic language generation in dialogue using factored
language models. computer linguistics, 40(4):763   
799.

franc  ois mairesse, milica ga  si  c, filip jur  c      cek, simon
keizer, blaise thomson, kai yu, and steve young.
2010. phrase-based statistical language generation
using id114 and active learning. in pro-
ceedings of the 48th annual meeting of the associa-
tion for computational linguistics, acl    10, pages
1552   1561.

tom  a  s mikolov and geoffrey zweig. 2012. context
dependent recurrent neural network language model.
in in proceedings on ieee slt workshop.

tom  a  s mikolov, martin kara   t, luk  a  s burget, jan
  cernock  y, and sanjeev khudanpur. 2010. recur-
in in
rent neural network based language model.
proceedings on interspeech.

tom  a  s mikolov, stefan kombrink, luk  a  s burget,
jan h.   cernock  y, and sanjeev khudanpur. 2011a.
extensions of recurrent neural network language
model. in acoustics, speech and signal processing
(icassp), 2011 ieee international conference on.

tom  a  s mikolov, stefan kombrink, anoop deoras,
luk  a  s burget, and jan   cernock  y. 2011b. id56lm -
recurrent neural network id38 toolkit.
in in proceedings on asru.

tom  a  s mikolov, ilya sutskever, kai chen, greg s. cor-
rado, and jeff dean. 2013. distributed representa-
tions of words and phrases and their compositional-
ity. in advances in neural information processing
systems 26, pages 3111   3119.

danilo mirkovic and lawrence cavedon. 2011. dia-
logue management using scripts, february 16. ep
patent 1,891,625.

alice h. oh and alexander i. rudnicky.

2000.
stochastic language generation for spoken dialogue
systems. in proceedings of the 2000 anlp/naacl
workshop on conversational systems - volume 3,
anlp/naacl-convsyst    00, pages 27   32.

kishore papineni, salim roukos, todd ward, and wei-
jing zhu. 2002. id7: a method for automatic
evaluation of machine translation. in proceedings of
the 40th annual meeting on association for compu-
tational linguistics, pages 311   318. association for
computational linguistics.

jeffrey pennington, richard socher, and christopher
manning. 2014. glove: global vectors for word
in proceedings of the 2014 con-
representation.
ference on empirical methods in natural language
processing (emnlp), pages 1532   1543. associa-
tion for computational linguistics, october.

heather pon-barry, karl schultz, elizabeth owen
bratt, brady clark, and stanley peters. 2006. re-
sponding to student uncertainty in spoken tutorial di-
alogue systems. international journal of arti   cial
intelligence in education.

adwait ratnaparkhi. 2002. trainable approaches to
surface id86 and their appli-
cation to conversational id71. computer
speech and language. spoken language genera-
tion.

verena rieser and oliver lemon. 2010. natural lan-
guage generation as planning under uncertainty for
spoken dialogue systems. in empirical methods in
id86, pages 105   120.

tara n sainath, a-r mohamed, brian kingsbury, and
bhuvana ramabhadran. 2013. deep convolutional
neural networks for lvcsr. in acoustics, speech and
signal processing (icassp), 2013 ieee interna-
tional conference on, pages 8614   8618. ieee.

mike schuster and kuldip k paliwal. 1997. bidirec-
tional recurrent neural networks. signal processing,
ieee transactions on, 45(11):2673   2681.

richard socher, alex perelygin, jean y. wu, jason
chuang, christopher d. manning, andrew y. ng,
and christopher potts. 2013. recursive deep mod-
els for semantic compositionality over a sentiment
treebank. in proceedings of the 2014 conference on
empirical methods in natural language processing
(emnlp).

amanda stent, rashmi prasad, and marilyn walker.
2004. trainable sentence planning for complex in-
formation presentation in spoken id71. in
in proceedings of the annual meeting of the associ-
ation for computational linguistics, pages 79   86.

martin sundermeyer, tamer alkhouli, joern wue-
bker, and hermann ney. 2014. translation mod-
eling with id182.
in proceedings of the 2014 conference on em-
pirical methods in natural language processing
(emnlp), pages 14   25. association for computa-
tional linguistics.

ilya sutskever, james martens, and geoffrey e. hin-
ton. 2011. generating text with recurrent neural
networks. in proceedings of the 28th international
conference on machine learning (icml-11), pages
1017   1024, new york, ny, usa. acm.

marilyn a walker, owen c rambow, and monica ro-
gati. 2002. training a sentence planner for spo-
ken dialogue using boosting. computer speech and
language, 16(3):409   433.

wayne ward and sunil issar. 1994. recent improve-
ments in the cmu spoken language understanding
in proceedings of the workshop on hu-
system.
man language technology, hlt    94, pages 213   
216. association for computational linguistics.

paul j werbos. 1990. id26 through time:
what it does and how to do it. proceedings of the
ieee, 78(10):1550   1560.

steve young, milica ga  si  c, blaise thomson, and ja-
son d. williams. 2013. pomdp-based statistical
spoken id71: a review. proceedings of
the ieee, 101(5):1160   1179, may.

xingxing zhang and mirella lapata.

2014. chi-
nese poetry generation with recurrent neural net-
in proceedings of the 2014 conference on
works.
empirical methods in natural language processing
(emnlp), pages 670   680. association for compu-
tational linguistics, october.

