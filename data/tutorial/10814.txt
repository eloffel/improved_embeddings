6
1
0
2

 
t
c
o
8

 

 
 
]
l
c
.
s
c
[
 
 

2
v
4
4
1
8
0

.

9
0
6
1
:
v
i
x
r
a

google   s id4 system: bridging the gap

between human and machine translation

yonghui wu, mike schuster, zhifeng chen, quoc v. le, mohammad norouzi

yonghui,schuster,zhifengc,qvl,mnorouzi@google.com

wolfgang macherey, maxim krikun, yuan cao, qin gao, klaus macherey,
je    klingner, apurva shah, melvin johnson, xiaobing liu,   ukasz kaiser,
stephan gouws, yoshikiyo kato, taku kudo, hideto kazawa, keith stevens,

george kurian, nishant patil, wei wang, cli    young, jason smith, jason riesa,

alex rudnick, oriol vinyals, greg corrado, macdu    hughes, je   rey dean

abstract

id4 (id4) is an end-to-end learning approach for automated translation,
with the potential to overcome many of the weaknesses of conventional phrase-based translation systems.
unfortunately, id4 systems are known to be computationally expensive both in training and in translation
id136     sometimes prohibitively so in the case of very large data sets and large models. several authors
have also charged that id4 systems lack robustness, particularly when input sentences contain rare words.
these issues have hindered id4   s use in practical deployments and services, where both accuracy and
speed are essential. in this work, we present gid4, google   s id4 system, which
attempts to address many of these issues. our model consists of a deep lstm network with 8 encoder
and 8 decoder layers using residual connections as well as attention connections from the decoder network
to the encoder. to improve parallelism and therefore decrease training time, our attention mechanism
connects the bottom layer of the decoder to the top layer of the encoder. to accelerate the    nal translation
speed, we employ low-precision arithmetic during id136 computations. to improve handling of rare
words, we divide words into a limited set of common sub-word units (   wordpieces   ) for both input and
output. this method provides a good balance between the    exibility of    character   -delimited models and
the e   ciency of    word   -delimited models, naturally handles translation of rare words, and ultimately
improves the overall accuracy of the system. our id125 technique employs a length-id172
procedure and uses a coverage penalty, which encourages generation of an output sentence that is most
likely to cover all the words in the source sentence. to directly optimize the translation id7 scores,
we consider re   ning the models by using id23, but we found that the improvement
in the id7 scores did not re   ect in the human evaluation. on the wmt   14 english-to-french and
english-to-german benchmarks, gid4 achieves competitive results to state-of-the-art. using a human
side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of
60% compared to google   s phrase-based production system.

introduction

1
id4 (id4) [41, 2] has recently been introduced as a promising approach with the
potential of addressing many shortcomings of traditional machine translation systems. the strength of id4
lies in its ability to learn directly, in an end-to-end fashion, the mapping from input text to associated
output text. its architecture typically consists of two recurrent neural networks (id56s), one to consume the
input text sequence and one to generate translated output text. id4 is often accompanied by an attention
mechanism [2] which helps it cope e   ectively with long input sequences.

an advantage of id4 is that it sidesteps many brittle design choices in traditional
phrase-based machine translation [26]. in practice, however, id4 systems used to be worse in accuracy than
phrase-based translation systems, especially when training on very large-scale datasets as used for the very
best publicly available translation systems. three inherent weaknesses of id4 are

1

responsible for this gap: its slower training and id136 speed, ine   ectiveness in dealing with rare words,
and sometimes failure to translate all words in the source sentence. firstly, it generally takes a considerable
amount of time and computational resources to train an id4 system on a large-scale translation dataset,
thus slowing the rate of experimental turnaround time and innovation. for id136 they are generally
much slower than phrase-based systems due to the large number of parameters used. secondly, id4 lacks
robustness in translating rare words. though this can be addressed in principle by training a    copy model    to
mimic a traditional alignment model [31], or by using the attention mechanism to copy rare words [37], these
approaches are both unreliable at scale, since the quality of the alignments varies across languages, and the
latent alignments produced by the attention mechanism are unstable when the network is deep. also, simple
copying may not always be the best strategy to cope with rare words, for example when a id68
is more appropriate. finally, id4 systems sometimes produce output sentences that do not translate all
parts of the input sentence     in other words, they fail to completely    cover    the input, which can result in
surprising translations.

this work presents the design and implementation of gid4, a production id4 system at google, that
aims to provide solutions to the above problems. in our implementation, the recurrent networks are long
short-term memory (lstm) id56s [23, 17]. our lstm id56s have 8 layers, with residual connections
between layers to encourage gradient    ow [21]. for parallelism, we connect the attention from the bottom
layer of the decoder network to the top layer of the encoder network. to improve id136 time, we
employ low-precision arithmetic for id136, which is further accelerated by special hardware (google   s
tensor processing unit, or tpu). to e   ectively deal with rare words, we use sub-word units (also known as
   wordpieces   ) [35] for inputs and outputs in our system. using wordpieces gives a good balance between the
   exibility of single characters and the e   ciency of full words for decoding, and also sidesteps the need for
special treatment of unknown words. our id125 technique includes a length id172 procedure
to deal e   ciently with the problem of comparing hypotheses of di   erent lengths during decoding, and a
coverage penalty to encourage the model to translate all of the provided input.

our implementation is robust, and performs well on a range of datasets across many pairs of languages
without the need for language-speci   c adjustments. using the same implementation, we are able to achieve
results comparable to or better than previous state-of-the-art systems on standard benchmarks, while
delivering great improvements over google   s phrase-based production translation system. speci   cally, on
wmt   14 english-to-french, our single model scores 38.95 id7, an improvement of 7.5 id7 from a
single model without an external alignment model reported in [31] and an improvement of 1.2 id7 from
a single model without an external alignment model reported in [45]. our single model is also comparable
to a single model in [45], while not making use of any alignment model as being used in [45]. likewise on
wmt   14 english-to-german, our single model scores 24.17 id7, which is 3.4 id7 better than a previous
competitive baseline [6]. on production data, our implementation is even more e   ective. human evaluations
show that gid4 has reduced translation errors by 60% compared to our previous phrase-based system
on many pairs of languages: english     french, english     spanish, and english     chinese. additional
experiments suggest the quality of the resulting translation system gets closer to that of average human
translators.

2 related work
id151 (smt) has been the dominant translation paradigm for decades [3, 4, 5].
practical implementations of smt are generally phrase-based systems (pbmt) which translate sequences of
words or phrases where the lengths may di   er [26].

even prior to the advent of direct id4, neural networks have been used as a
component within smt systems with some success. perhaps one of the most notable attempts involved the use
of a joint language model to learn phrase representations [13] which yielded an impressive improvement when
combined with phrase-based translation. this approach, however, still makes use of phrase-based translation
systems at its core, and therefore inherits their shortcomings. other proposed approaches for learning phrase
representations [7] or learning end-to-end translation with neural networks [24] o   ered encouraging hints, but
ultimately delivered worse overall accuracy compared to standard phrase-based systems.

the concept of end-to-end learning for machine translation has been attempted in the past (e.g., [8]) with

2

limited success. following seminal papers in the area [41, 2], id4 translation quality has crept closer to
the level of phrase-based translation systems for common research benchmarks. perhaps the    rst successful
attempt at surpassing phrase-based translation was described in [31]. on wmt   14 english-to-french, this
system achieved a 0.5 id7 improvement compared to a state-of-the-art phrase-based system.

since then, many novel techniques have been proposed to further improve id4: using an attention
mechanism to deal with rare words [37], a mechanism to model translation coverage [42], multi-task and
semi-supervised training to incorporate more data [14, 29], a character decoder [9], a character encoder [11],
subword units [38] also to deal with rare word outputs, di   erent kinds of attention mechanisms [30], and
sentence-level loss minimization [39, 34]. while the translation accuracy of these systems has been encouraging,
systematic comparison with large scale, production quality phrase-based translation systems has been lacking.

3 model architecture
our model (see figure 1) follows the common sequence-to-sequence learning framework [41] with attention [2].
it has three components: an encoder network, a decoder network, and an attention network. the encoder
transforms a source sentence into a list of vectors, one vector per input symbol. given this list of vectors,
the decoder produces one symbol at a time, until the special end-of-sentence symbol (eos) is produced.
the encoder and decoder are connected through an attention module which allows the decoder to focus on
di   erent regions of the source sentence during the course of decoding.

for notation, we use bold lower case to denote vectors (e.g., v, oi), bold upper case to represent matrices
(e.g., u, w), cursive upper case to represent sets (e.g., v , t ), capital letters to represent sequences (e.g. x,
y ), and lower case to represent individual symbols in a sequence, (e.g., x1, x2).

let (x, y ) be a source and target sentence pair. let x = x1, x2, x3, ..., xm be the sequence of m symbols
in the source sentence and let y = y1, y2, y3, ..., yn be the sequence of n symbols in the target sentence. the
encoder is simply a function of the following form:

(1)
in this equation, x1, x2, ..., xm is a list of    xed size vectors. the number of members in the list is the same
as the number of symbols in the source sentence (m in this example). using the chain rule the conditional
id203 of the sequence p(y |x) can be decomposed as:

x1, x2, ..., xm = encoderrn n(x1, x2, x3, ..., xm)

p(y |x) = p(y |x1, x2, x3, ..., xm)

ny

=

p(yi|y0, y1, y2, ..., yi   1; x1, x2, x3, ..., xm)

(2)

i=1

where y0 is a special    beginning of sentence    symbol that is prepended to every target sentence.

during id136 we calculate the id203 of the next symbol given the source sentence encoding and

the decoded target sequence so far:

p(yi|y0, y1, y2, y3, ..., yi   1; x1, x2, x3, ..., xm)

(3)

our decoder is implemented as a combination of an id56 network and a softmax layer. the decoder id56
network produces a hidden state yi for the next symbol to be predicted, which then goes through the softmax
layer to generate a id203 distribution over candidate output symbols.

in our experiments we found that for id4 systems to achieve good accuracy, both the encoder and
decoder id56s have to be deep enough to capture subtle irregularities in the source and target languages. this
observation is similar to previous observations that deep lstms signi   cantly outperform shallow lstms [41].
in that work, each additional layer reduced perplexity by nearly 10%. similar to [31], we use a deep stacked
long short term memory (lstm) [23] network for both the encoder id56 and the decoder id56.

our attention module is similar to [2]. more speci   cally, let yi   1 be the decoder-id56 output from the
past decoding time step (in our implementation, we use the output from the bottom decoder layer). attention

3

figure 1: the model architecture of gid4, google   s id4 system. on the left
is the encoder network, on the right is the decoder network, in the middle is the attention module. the
bottom encoder layer is bi-directional: the pink nodes gather information from left to right while the green
nodes gather information from right to left. the other layers of the encoder are uni-directional. residual
connections start from the layer third from the bottom in the encoder and decoder. the model is partitioned
into multiple gpus to speed up training. in our setup, we have 8 encoder lstm layers (1 bi-directional layer
and 7 uni-directional layers), and 8 decoder layers. with this setting, one model replica is partitioned 8-ways
and is placed on 8 di   erent gpus typically belonging to one host machine. during training, the bottom
bi-directional encoder layers compute in parallel    rst. once both    nish, the uni-directional encoder layers
can start computing, each on a separate gpu. to retain as much parallelism as possible during running
the decoder layers, we use the bottom decoder layer output only for obtaining recurrent attention context,
which is sent directly to all the remaining decoder layers. the softmax layer is also partitioned and placed on
multiple gpus. depending on the output vocabulary size we either have them run on the same gpus as the
encoder and decoder networks, or have them run on a separate set of dedicated gpus.

context ai for the current time step is computed according to the following formulas:

st = attentionf unction(yi   1, xt)    t,

1     t     m

exp(st)

   t,

1     t     m

(4)

mx

pt = exp(st)/

mx

t=1

pt.xt

ai =

t=1

where attentionf unction in our implementation is a feed forward network with one hidden layer.

3.1 residual connections
as mentioned above, deep stacked lstms often give better accuracy over shallower models. however, simply
stacking more layers of lstm works only to a certain number of layers, beyond which the network becomes

4

too slow and di   cult to train, likely due to exploding and vanishing gradient problems [33, 22]. in our
experience with large-scale translation tasks, simple stacked lstm layers work well up to 4 layers, barely
with 6 layers, and very poorly beyond 8 layers.

figure 2: the di   erence between normal stacked lstm and our stacked lstm with residual connections.
on the left: simple stacked lstm layers [41]. on the right: our implementation of stacked lstm layers
with residual connections. with residual connections, input to the bottom lstm layer (x0
i    s to lstm1) is
element-wise added to the output from the bottom layer (x1
i    s). this sum is then fed to the top lstm layer
(lstm2) as the new input.

motivated by the idea of modeling di   erences between an intermediate layer   s output and the targets,
which has shown to work well for many projects in the past [16, 21, 40], we introduce residual connections
among the lstm layers in a stack (see figure 2). more concretely, let lstmi and lstmi+1 be the i-th and
(i + 1)-th lstm layers in a stack, whose parameters are wi and wi+1 respectively. at the t-th time step,
for the stacked lstm without residual connections, we have:

t   1, mi

t = lstmi(ci
t, mi
ci
xi
t = mi
t = lstmi+1(ci+1
, mi+1

t

ci+1

t

t   1, xi   1

t

; wi)

(5)

t   1, mi+1
t and ci

t; wi+1)

t   1, xi
t are the hidden states and memory states of

where xi
lstmi at time step t, respectively.

t is the input to lstmi at time step t, and mi

with residual connections between lstmi and lstmi+1, the above equations become:

t   1, mi

t = lstmi(ci
ci
t, mi
t + xi   1
xi
t = mi
, mi+1
t = lstmi+1(ci+1

t

ci+1

t

t   1, mi+1

t   1, xi

t; wi+1)

t   1, xi   1

t

; wi)

(6)

residual connections greatly improve the gradient    ow in the backward pass, which allows us to train very
deep encoder and decoder networks. in most of our experiments, we use 8 lstm layers for the encoder and
decoder, though residual connections can allow us to train substantially deeper networks (similar to what
was observed in [45]).

3.2 bi-directional encoder for first layer
for translation systems, the information required to translate certain words on the output side can appear
anywhere on the source side. often the source side information is approximately left-to-right, similar to

5

the target side, but depending on the language pair the information for a particular output word can be
distributed and even be split up in certain regions of the input side.

to have the best possible context at each point in the encoder network it makes sense to use a bi-directional
id56 [36] for the encoder, which was also used in [2]. to allow for maximum possible parallelization during
computation (to be discussed in more detail in section 3.3), bi-directional connections are only used for the
bottom encoder layer     all other encoder layers are uni-directional. figure 3 illustrates our use of bi-directional
lstms at the bottom encoder layer. the layer lstmf processes the source sentence from left to right, while
      
xf
the layer lstmb processes the source sentence from right to left. outputs from lstmf (
t) and lstmb
      
xb
(
t ) are    rst concatenated and then fed to the next layer lstm1.

figure 3: the structure of bi-directional connections in the    rst layer of the encoder. lstm layer lstmf
processes information from left to right, while lstm layer lstmb processes information from right to left.
output from lstmf and lstmb are    rst concatenated and then fed to the next lstm layer lstm1.

3.3 model parallelism
due to the complexity of our model, we make use of both model parallelism and data parallelism to speed
up training. data parallelism is straightforward: we train n model replicas concurrently using a downpour
sgd algorithm [12]. the n replicas all share one copy of model parameters, with each replica asynchronously
updating the parameters using a combination of adam [25] and sgd algorithms. in our experiments, n is
often around 10. each replica works on a mini-batch of m sentence pairs at a time, which is often 128 in our
experiments.

in addition to data parallelism, model parallelism is used to improve the speed of the gradient computation
on each replica. the encoder and decoder networks are partitioned along the depth dimension and are placed
on multiple gpus, e   ectively running each layer on a di   erent gpu. since all but the    rst encoder layer are
uni-directional, layer i + 1 can start its computation before layer i is fully    nished, which improves training
speed. the softmax layer is also partitioned, with each partition responsible for a subset of symbols in the
output vocabulary. figure 1 shows more details of how partitioning is done.

model parallelism places certain constraints on the model architectures we can use. for example, we
cannot a   ord to have bi-directional lstm layers for all the encoder layers, since doing so would reduce
parallelism among subsequent layers, as each layer would have to wait until both forward and backward
directions of the previous layer have    nished. this would e   ectively constrain us to make use of only 2 gpus

6

in parallel (one for the forward direction and one for the backward direction). for the attention portion of
the model, we chose to align the bottom decoder output to the top encoder output to maximize parallelism
when running the decoder network. had we aligned the top decoder layer to the top encoder layer, we would
have removed all parallelism in the decoder network and would not bene   t from using more than one gpu
for decoding.

4 segmentation approaches
id4 models often operate with    xed word vocabularies even though translation is
fundamentally an open vocabulary problem (names, numbers, dates etc.). there are two broad categories of
approaches to address the translation of out-of-vocabulary (oov) words. one approach is to simply copy
rare words from source to target (as most rare words are names or numbers where the correct translation is
just a copy), either based on the attention model [37], using an external alignment model [31], or even using
a more complicated special purpose pointing network [18]. another broad category of approaches is to use
sub-word units, e.g., chararacters [10], mixed word/characters [28], or more intelligent sub-words [38].

4.1 wordpiece model
our most successful approach falls into the second category (sub-word units), and we adopt the wordpiece
model (wpm) implementation initially developed to solve a japanese/korean segmentation problem for the
google id103 system [35]. this approach is completely data-driven and guaranteed to generate
a deterministic segmentation for any possible sequence of characters. it is similar to the method used in [38]
to deal with rare words in id4.

for processing arbitrary words, we    rst break words into wordpieces given a trained wordpiece model.
special word boundary symbols are added before training of the model such that the original word sequence
can be recovered from the wordpiece sequence without ambiguity. at decoding time, the model    rst produces
a wordpiece sequence, which is then converted into the corresponding word sequence.
here is an example of a word sequence and the corresponding wordpiece sequence:
    word: jet makers feud over seat width with big orders at stake
    wordpieces: _j et _makers _fe ud _over _seat _width _with _big _orders _at _stake
in the above example, the word    jet    is broken into two wordpieces    _j    and    et   , and the word    feud   
is broken into two wordpieces    _fe    and    ud   . the other words remain as single wordpieces.    _    is a special
character added to mark the beginning of a word.

the wordpiece model is generated using a data-driven approach to maximize the language-model likelihood
of the training data, given an evolving word de   nition. given a training corpus and a number of desired
tokens d, the optimization problem is to select d wordpieces such that the resulting corpus is minimal in the
number of wordpieces when segmented according to the chosen wordpiece model. our greedy algorithm to
this optimization problem is similar to [38] and is described in more detail in [35]. compared to the original
implementation used in [35], we use a special symbol only at the beginning of the words and not at both ends.
we also cut the number of basic characters to a manageable number depending on the data (roughly 500 for
western languages, more for asian languages) and map the rest to a special unknown character to avoid
polluting the given wordpiece vocabulary with very rare characters. we    nd that using a total vocabulary of
between 8k and 32k wordpieces achieves both good accuracy (id7 scores) and fast decoding speed across
all pairs of language pairs we have tried.

as mentioned above, in translation it often makes sense to copy rare entity names or numbers directly
from the source to the target. to facilitate this type of direct copying, we always use a shared wordpiece
model for both the source language and target language. using this approach, it is guaranteed that the same
string in source and target sentence will be segmented in exactly the same way, making it easier for the
system to learn to copy these tokens.

wordpieces achieve a balance between the    exibility of characters and e   ciency of words. we also    nd
that our models get better overall id7 scores when using wordpieces     possibly due to the fact that our
models now deal e   ciently with an essentially in   nite vocabulary without resorting to characters only. the

7

latter would make the average lengths of the input and output sequences much longer, and therefore would
require more computation.

4.2 mixed word/character model
a second approach we use is the mixed word/character model. as in a word model, we keep a    xed-size
word vocabulary. however, unlike in a conventional word model where oov words are collapsed into a single
unk symbol, we convert oov words into the sequence of its constituent characters. special pre   xes are
prepended to the characters, to 1) show the location of the characters in a word, and 2) to distinguish them
from normal in-vocabulary characters. there are three pre   xes: <b>,<m>, and <e>, indicating beginning of
the word, middle of the word and end of the word, respectively. for example, let   s assume the word miki is
not in the vocabulary. it will be preprocessed into a sequence of special tokens: <b>m <m>i <m>k <e>i. the
process is done on both the source and the target sentences. during decoding, the output may also contain
sequences of special tokens. with the pre   xes, it is trivial to reverse the id121 to the original words as
part of a post-processing step.

5 training criteria

given a dataset of parallel text containing n input-output sequence pairs, denoted d    (cid:8)(x(i), y    (i))(cid:9)n

i=1,
standard maximum-likelihood training aims at maximizing the sum of log probabilities of the ground-truth
outputs given the corresponding inputs,

oml(  ) =

log p  (y    (i) | x(i)) .

(7)

nx

i=1

the main problem with this objective is that it does not re   ect the task reward function as measured by the
id7 score in translation. further, this objective does not explicitly encourage a ranking among incorrect
output sequences     where outputs with higher id7 scores should still obtain higher probabilities under the
model     since incorrect outputs are never observed during training. in other words, using maximum-likelihood
training only, the model will not learn to be robust to errors made during decoding since they are never
observed, which is quite a mismatch between the training and testing procedure.

several recent papers [34, 39, 32] have considered di   erent ways of incorporating the task reward into
optimization of neural sequence-to-sequence models. in this work, we also attempt to re   ne a model pre-
trained on the maximum likelihood objective to directly optimize for the task reward. we show that, even on
large datasets, re   nement of state-of-the-art maximum-likelihood models using task reward improves the
results considerably.

we consider model re   nement using the expected reward objective (also used in [34]), which can be

expressed as

orl(  ) =

p  (y | x(i)) r(y, y    (i)).

(8)

nx

x

i=1

y    y

here, r(y, y    (i)) denotes the per-sentence score, and we are computing an expectation over all of the output
sentences y , up to a certain length.

the id7 score has some undesirable properties when used for single sentences, as it was designed to
be a corpus measure. we therefore use a slightly di   erent score for our rl experiments which we call the
   gleu score   . for the gleu score, we record all sub-sequences of 1, 2, 3 or 4 tokens in output and target
sequence (id165s). we then compute a recall, which is the ratio of the number of matching id165s to
the number of total id165s in the target (ground truth) sequence, and a precision, which is the ratio of
the number of matching id165s to the number of total id165s in the generated output sequence. then
gleu score is simply the minimum of recall and precision. this gleu score   s range is always between 0
(no matches) and 1 (all match) and it is symmetrical when switching output and target. according to our
experiments, gleu score correlates quite well with the id7 metric on a corpus level but does not have its
drawbacks for our per sentence reward objective.

8

as is common practice in id23, we subtract the mean reward from r(y, y    (i)) in equation
8. the mean is estimated to be the sample mean of m sequences drawn independently from distribution
p  (y | x(i)). in our implementation, m is set to be 15. to further stabilize training, we optimize a linear
combination of ml (equation 7) and rl (equation 8) objectives as follows:

omixed(  ) =        oml(  ) + orl(  )

(9)

   in our implementation is typically set to be 0.017.

in our setup, we    rst train a model using the maximum likelihood objective (equation 7) until convergence.
we then re   ne this model using a mixed maximum likelihood and expected reward objective (equation 9),
until id7 score on a development set is no longer improving. the second step is optional.

6 quantizable model and quantized id136
one of the main challenges in deploying our id4 model to our interactive production
translation service is that it is computationally intensive at id136, making low latency translation di   cult,
and high volume deployment computationally expensive. quantized id136 using reduced precision
arithmetic is one technique that can signi   cantly reduce the cost of id136 for these models, often providing
e   ciency improvements on the same computational devices. for example, in [43], it is demonstrated that a
convolutional neural network model can be sped up by a factor of 4-6 with minimal loss on classi   cation
accuracy on the ilsvrc-12 benchmark. in [27], it is demonstrated that neural network model weights can
be quantized to only three states, -1, 0, and +1.

many of those previous studies [19, 20, 43, 27] however mostly focus on id98 models with relatively few
layers. deep lstms with long sequences pose a novel challenge in that quantization errors can be signi   cantly
ampli   ed after many unrolled steps or after going through a deep lstm stack.

in this section, we present our approach to speed up id136 with quantized arithmetic. our solution
is tailored towards the hardware options available at google. to reduce quantization errors, additional
constraints are added to our model during training so that it is quantizable with minimal impact on
the output of the model. that is, once a model is trained with these additional constraints, it can be
subsequently quantized without loss to translation quality. our experimental results suggest that those
additional constraints do not hurt model convergence nor the quality of a model once it has converged.

recall from equation 6 that in an lstm stack with residual connections there are two accumulators: ci
along the time axis and xi
t along the depth axis. in theory, both of the accumulators are unbounded, but
in practice, we noticed their values remain quite small. for quantized id136, we explicitly constrain the
values of these accumulators to be within [-  ,   ] to guarantee a certain range that can be used for quantization
later. the forward computation of an lstm stack with residual connections is modi   ed to the following:

t

t

t + xi   1

t   1, xi   1
c0i
t, mi
t = lstmi(ci
t   1, mi
t = max(     , min(  , c0i
ci
t))
x0i
t = mi
t = max(     , min(  , x0i
xi
t))
t   1, mi+1
t = lstmi+1(ci+1
, mi+1
t   1, xi
t = max(     , min(  , c0i+1
ci+1
))

t

t

c0i+1

t

t; wi+1)

; wi)

(10)

let us expand lstmi in equation 10 to include the internal gating logic. for brevity, we drop all the
superscripts i.

9

w = [w1, w2, w3, w4, w5, w6, w7, w8]
it = sigmoid(w1xt + w2mt)
i0
t = tanh(w3xt + w4mt)
ft = sigmoid(w5xt + w6mt)
ot = sigmoid(w7xt + w8mt)
ct = ct   1 (cid:12) ft + i0
mt = ct (cid:12) ot

t (cid:12) it

(11)

when doing quantized id136, we replace all the    oating point operations in equations 10 and 11 with
   xed-point integer operations with either 8-bit or 16-bit resolution. the weight matrix w above is represented
using an 8-bit integer matrix wq and a    oat vector s, as shown below:

si = max(abs(w[i, :]))

wq[i, j] = round(w[i, j]/si    127.0)
t) are represented using 16-bit integers representing the range [     ,   ]. all
all accumulator values (ci
id127s (e.g., w1xt, w2mt, etc.) in equation 11 are done using 8-bit integer multiplication
accumulated into larger accumulators. all other operations, including all the activations (sigmoid, tanh) and
elementwise operations ((cid:12), +) are done using 16-bit integer operations.

we now turn our attention to the log-linear softmax layer. during training, given the decoder id56

network output yt, we compute the id203 vector pt over all candidate output symbols as follows:

t and xi

(12)

vt = ws     yt
t = max(     , min(  , vt))
v0
pt = sof tmax(v0
t)

(13)

in equation 13, ws is the weight matrix for the linear layer, which has the same number of rows as the
number of symbols in the target vocabulary with each row corresponding to one unique target symbol.
v represents the raw logits, which are    rst clipped to be between       and    and then normalized into a
id203 vector p. input yt is guaranteed to be between       and    due to the quantization scheme we
applied to the decoder id56. the clipping range    for the logits v is determined empirically, and in our case,
it is set to 25. in quantized id136, the weight matrix ws is quantized into 8 bits as in equation 12, and
the id127 is done using 8 bit arithmetic. the calculations within the sof tmax function and
the attention model are not quantized during id136.

it is worth emphasizing that during training of the model we use full-precision    oating point numbers.
the only constraints we add to the model during training are the clipping of the id56 accumulator values
into [     ,   ] and softmax logits into [     ,   ].    is    xed to be at 25.0, while the value for    is gradually annealed
from a generous bound of    = 8.0 at the beginning of training, to a rather stringent bound of    = 1.0 towards
the end of training. at id136 time,    is    xed at 1.0. those additional constraints do not degrade model
convergence nor the decoding quality of the model when it has converged. in figure 4, we compare the loss
vs. steps for an unconstrained model (the blue curve) and a constrained model (the red curve) on wmt   14
english-to-french. we can see that the loss for the constrained model is slightly better, possibly due to
id173 roles those constraints play.

our solution strikes a good balance between e   ciency and accuracy. since the computationally expensive
operations (the id127s) are done using 8-bit integer operations, our quantized id136 is
quite e   cient. also, since error-sensitive accumulator values are stored using 16-bit integers, our solution is
very accurate and is robust to quantization errors.

in table 1 we compare the id136 speed and quality when decoding the wmt   14 english-to-french
development set (a concatenation of newstest2012 and newstest2013 test sets for a total of 6003 sentences) on

10

figure 4: log perplexity vs. steps for normal (non-quantized) training and quantization-aware training on
wmt   14 english to french during maximum likelihood training. notice the training losses are similar, with
the quantization-aware loss being slightly better. our conjecture for quantization-aware training being slightly
better is that the clipping constraints act as additional id173 which improves the model quality.

cpu, gpu and google   s tensor processing unit (tpu) respectively.1 the model used here for comparison
is trained with quantization constraints on the ml objective only (i.e., without id23 based
model re   nement). when the model is decoded on cpu and gpu, it is not quantized and all operations are
done using full-precision    oats. when it is decoded on tpu, certain operations, such as embedding lookup
and attention module, remain on the cpu, and all other quantized operations are o   -loaded to the tpu. in
all cases, decoding is done on a single machine with two intel haswell cpus, which consists in total of 88
cpu cores (hyperthreads). the machine is equipped with an nvidia gpu (tesla k80) for the experiment
with gpu or a single google tpu for the experiment with tpu.

table 1 shows that decoding using reduced precision arithmetics on the tpu su   ers a very minimal loss
of 0.0072 on log perplexity, and no loss on id7 at all. this result matches previous work reporting that
quantizing convolutional neural network models can retain most of the model quality.

table 1 also shows that decoding our model on cpu is actually 2.3 times faster than on gpu. firstly,
our dual-cpus host machine o   ers a theoretical peak flop performance which is more than two thirds that
of the gpu. secondly, the id125 algorithm forces the decoder to incur a non-trivial amount of data
transfer between the host and the gpu at every decoding step. hence, our current decoder implementation

1https://cloudplatform.googleblog.com/2016/05/google-supercharges-machine-learning-tasks-with-custom-chip.html

11

02468101214x 10500.511.522.533.544.55stepslog perplexity  normal trainingquantized trainingis not fully utilizing the computation capacities that a gpu can theoretically o   er during id136.

finally, table 1 shows that decoding on tpus is 3.4 times faster than decoding on cpus, demonstrating

that quantized arithmetics is much faster on tpus than both cpus or gpus.

table 1: model id136 on cpu, gpu and tpu. the model used here for comparison is trained with
the ml objective only with quantization constraints. results are obtained by decoding the wmt en   fr
development set on cpu, gpu and tpu respectively.

id7 log perplexity decoding time (s)

cpu 31.20
gpu 31.20
tpu 31.21

1.4553
1.4553
1.4626

1322
3028
384

unless otherwise noted, we always train and evaluate quantized models in our experiments. because there
is little di   erence from a quality perspective between a model decoded on cpus and one decoded on tpus,
we use cpus to decode for model evaluation during training and experimentation and use tpus to serve
production tra   c.

7 decoder
we use id125 during decoding to    nd the sequence y that maximizes a score function s(y, x) given
a trained model. we introduce two important re   nements to the pure max-id203 based id125
algorithm: a coverage penalty [42] and length id172. with length id172, we aim to account for
the fact that we have to compare hypotheses of di   erent length. without some form of length-id172
regular id125 will favor shorter results over longer ones on average since a negative log-id203
is added at each step, yielding lower (more negative) scores for longer sentences. we    rst tried to simply
divide by the length to normalize. we then improved on that original heuristic by dividing by length  ,
with 0 <    < 1 where    is optimized on a development set (       [0.6     0.7] was usually found to be best).
eventually we designed the empirically-better scoring function below, which also includes a coverage penalty
to favor translations that fully cover the source sentence according to the attention module.

more concretely, the scoring function s(y, x) that we employ to rank candidate translations is de   ned as

follows:

s(y, x) = log(p(y |x))/lp(y ) + cp(x; y )
lp(y ) = (5 + |y |)  
|x|x
(5 + 1)  

|y |x

log(min(

pi,j, 1.0)),

cp(x; y ) =       

i=1

j=1

(14)

(equation 4),p|x|

where pi,j is the attention id203 of the j-th target word yj on the i-th source word xi. by construction
i=0 pi,j is equal to 1. parameters    and    control the strength of the length id172
and the coverage penalty. when    = 0 and    = 0, our decoder falls back to pure id125 by id203.
during id125, we typically keep 8-12 hypotheses but we    nd that using fewer (4 or 2) has only
slight negative e   ects on id7 scores. besides pruning the number of considered hypotheses, two other
forms of pruning are used. firstly, at each step, we only consider tokens that have local scores that are
not more than beamsize below the best token for this step. secondly, after a normalized best score has
been found according to equation 14, we prune all hypotheses that are more than beamsize below the best
normalized score so far. the latter type of pruning only applies to full hypotheses because it compares scores
in the normalized space, which is only available when a hypothesis ends. this latter form of pruning also has
the e   ect that very quickly no more hypotheses will be generated once a su   ciently good hypothesis has
been found, so the search will end quickly. the pruning speeds up search by 30%     40% when run on cpus

12

compared to not pruning (where we simply stop decoding after a predetermined maximum output length of
twice the source length). typically we use beamsize = 3.0, unless otherwise noted.

to improve throughput during decoding we can put many sentences (typically up to 35) of similar length
into a batch and decode all of those in parallel to make use of available hardware optimized for parallel
computations. in this case the id125 only    nishes if all hypotheses for all sentences in the batch are out
of beam, which is slightly less e   cient theoretically, but in practice is of negligible additional computational
cost.

id7
0.0
0.2
0.4
0.6
0.8
1.0

  

0.0
30.3
31.4
31.4
31.4
31.4
31.4

0.2
30.7
31.4
31.4
31.4
31.4
31.3

  

0.4
30.9
31.4
31.4
31.3
31.2
31.2

0.6
31.1
31.3
31.1
30.9
30.8
30.6

0.8
31.2
30.8
30.5
30.1
29.8
29.4

1.0
31.1
30.3
29.6
28.9
28.1
27.2

table 2: wmt   14 en   fr id7 score with respect to di   erent values of    and   . the model in this
experiment trained using ml without rl re   nement. a single wmt en   fr model achieves a id7 score
of 30.3 on the development set when the id125 scoring function is purely based on the sequence
id203 (i.e., both    and    are 0). slightly larger    and    values improve id7 score by up to +1.1
(   = 0.2,    = 0.2), with a wide range of    and    values giving results very close to the best id7 scores.

table 2 shows the impact of    and    on the id7 score when decoding the wmt   14 english-to-french
development set. the model used here for experiments is trained using the ml objective only (without
rl re   nement). as can be seen from the results, having some length id172 and coverage penalty
improves id7 score considerably (from 30.3 to 31.4).

we    nd that length id172 (  ) and coverage penalty (  ) are less e   ective for models with rl
re   nement. table 3 summarizes our results. this is understandable, as during rl re   nement, the models
already learn to pay attention to the full source sentence to not under-translate or over-translate, which
would result in a penalty on the id7 (or gleu) scores.

id7
0.0
0.2
0.4
0.6
0.8
1.0

  

0.0
0.320
0.322
0.322
0.322
0.322
0.322

0.2
0.321
0.322
0.322
0.322
0.322
0.321

  

0.4
0.322
0.322
0.322
0.321
0.321
0.321

0.6
0.322
0.322
0.321
0.321
0.321
0.320

0.8
0.322
0.321
0.321
0.319
0.316
0.313

1.0
0.322
0.321
0.316
0.309
0.302
0.295

table 3: wmt en   fr id7 score with respect to di   erent values of    and   . the model used here is
trained using ml, then re   ned with rl. compared to the results in table 2, coverage penalty and length
id172 appear to be less e   ective for models after rl-based model re   nements. results are obtained
on the development set.

we found that the optimal    and    vary slightly for di   erent models. based on tuning results using

internal google datasets, we use    = 0.2 and    = 0.2 in our experiments, unless noted otherwise.

8 experiments and results
in this section, we present our experimental results on two publicly available corpora used extensively as
benchmarks for id4 systems: wmt   14 english-to-french (wmt en   fr) and
english-to-german (wmt en   de). on these two datasets, we benchmark gid4 models with word-based,

13

character-based, and wordpiece-based vocabularies. we also present the improved accuracy of our models
after    ne-tuning with rl and model ensembling. our main objective with these datasets is to show the
contributions of various components in our implementation, in particular the wordpiece model, rl model
re   nement, and model ensembling.

in addition to testing on publicly available corpora, we also test gid4 on google   s translation production
corpora, which are two to three decimal orders of magnitudes bigger than the wmt corpora for a given
language pair. we compare the accuracy of our model against human accuracy and the best phrase-based
machine translation (pbmt) production system for google translate.

in all experiments, our models consist of 8 encoder layers and 8 decoder layers. (since the bottom encoder
layer is actually bi-directional, in total there are 9 logically distinct lstm passes in the encoder.) the
attention network is a simple feedforward network with one hidden layer with 1024 nodes. all of the models
use 1024 lstm nodes per encoder and decoder layers.

8.1 datasets
we evaluate our model on the wmt en   fr dataset, the wmt en   de dataset, as well as many google-
internal production datasets. on wmt en   fr, the training set contains 36m sentence pairs. on wmt
en   de, the training set contains 5m sentence pairs. in both cases, we use newstest2014 as the test sets to
compare against previous work [31, 37, 45]. the combination of newstest2012 and newstest2013 is used as
the development set.
in addition to wmt, we also evaluate our model on some google-internal datasets representing a wider
spectrum of languages with distinct linguistic properties: english     french, english     spanish and english
    chinese.

8.2 id74
we evaluate our models using the standard id7 score metric. to be comparable to previous work [41, 31, 45],
we report tokenized id7 score as computed by the multi-id7.pl script, downloaded from the public
implementation of moses (on github), which is also used in [31].

as is well-known, id7 score does not fully capture the quality of a translation. for that reason we also
carry out side-by-side (sxs) evaluations where we have human raters evaluate and compare the quality of
two translations presented side by side for a given source sentence. side-by-side scores range from 0 to 6,
with a score of 0 meaning    completely nonsense translation   , and a score of 6 meaning    perfect translation:
the meaning of the translation is completely consistent with the source, and the grammar is correct   . a
translation is given a score of 4 if    the sentence retains most of the meaning of the source sentence, but may
have some grammar mistakes   , and a translation is given a score of 2 if    the sentence preserves some of the
meaning of the source sentence but misses signi   cant parts   . these scores are generated by human raters
who are    uent in both languages and hence often capture translation quality better than id7 scores.

8.3 training procedure
the models are trained by a system we implemented using tensorflow[1]. the training setup follows the
classic data parallelism paradigm. there are 12 replicas running concurrently on separate machines. every
replica updates the shared parameters asynchronously.

we initialize all trainable parameters uniformly between [-0.04, 0.04]. as is common wisdom in training
id56 models, we apply gradient clipping (similar to [41]): all gradients are uniformly scaled down such that
the norm of the modi   ed gradients is no larger than a    xed constant, which is 5.0 in our case. if the norm of
the original gradients is already smaller than or equal to the given threshold, then gradients are not changed.
for the    rst stage of maximum likelihood training (that is, to optimize for objective function 7), we
use a combination of adam [25] and simple sgd learning algorithms provided by the tensorflow runtime
system. we run adam for the    rst 60k steps, after which we switch to simple sgd. each step in training is a
mini-batch of 128 examples.

we    nd that adam accelerates training at the beginning, but adam alone converges to a worse point
than a combination of adam    rst, followed by sgd (figure 5). for the adam part, we use a learning rate of

14

figure 5: log perplexity vs. steps for adam, sgd and adam-then-sgd on wmt en   fr during maximum
likelihood training. adam converges much faster than sgd at the beginning. towards the end, however,
adam-then-sgd is gradually better. notice the bump in the red curve (adam-then-sgd) at around 60k
steps where we switch from adam to sgd. we suspect that this bump occurs due to di   erent optimization
trajectories of adam vs. sgd. when we switch from adam to sgd, the model    rst su   ers a little, but is
able to quickly recover afterwards.

0.0002, and for the sgd part, we use a learning rate of 0.5. we    nd that it is important to also anneal the
learning rate after a certain number of total steps. for the wmt en   fr dataset, we begin to anneal the
learning rate after 1.2m steps, after which we halve the learning rate every 200k steps for an additional 800k
steps. on wmt en   fr, it takes around 6 days to train a basic model using 96 nvidia k80 gpus.

once a model is fully converged using the ml objective, we switch to rl based model re   nement, i.e., we
further optimize the objective function as in equation 9. we re   ne a model until the id7 score does not
change much on the development set. for this model re   nement phase, we simply run the sgd optimization
algorithm. the number of steps needed to re   ne a model varies from dataset to dataset. for wmt en   fr,
it takes around 3 days to complete 400k steps.
to prevent over   tting, we apply dropout during training with a scheme similar to [44]. for the wmt
en   fr and en   de datasets, we set the dropout id203 to be 0.2 and 0.3 respectively. due to various
technical reasons, dropout is only applied during the ml training phase, not during the rl re   nement phase.
the exact hyper-parameters vary from dataset to dataset and from model to model. for the wmt
en   de dataset, since it is signi   cantly smaller than the wmt en   fr dataset, we use a higher dropout

15

0246810121416x 10500.511.522.533.544.55stepslog perplexity  sgd onlyadam onlyadam then sgdid203, and also train smaller models for fewer steps overall. on the production data sets, we typically
do not use dropout, and we train the models for more steps.

8.4 evaluation after maximum likelihood training
the models in our experiments are word-based, character-based, mixed word-character-based or several
wordpiece models with varying vocabulary sizes.

for the word model, we selected the most frequent 212k source words as the source vocabulary and the
most popular 80k target words as the target vocabulary. words not in the source vocabulary or the target
vocabulary (unknown words) are converted into special <first_char>_unk_<last_char> symbols. note, in
this case, there is more than one unk (e.g., our production word models have roughly 5000 di   erent unks
in this case). we then use the attention mechanism to copy a corresponding word from the source to replace
these unknown words during decoding [37].

the mixed word-character model is similar to the word model, except the out-of-vocabulary (oov) words
are converted into sequences of characters with special delimiters around them as described in section 4.2 in
more detail. in our experiments, the vocabulary size for the mixed word-character model is 32k. for the pure
character model, we simply split all words into constituent characters, resulting typically in a few hundred
basic characters (including special symbols appearing in the data). for the wordpiece models, we train 3
di   erent models with vocabulary sizes of 8k, 16k, and 32k.
table 4 summarizes our results on the wmt en   fr dataset. in this table, we also compare against other
strong baselines without model ensembling. as can be seen from the table,    wpm-32k   , a wordpiece model
with a shared source and target vocabulary of 32k wordpieces, performs well on this dataset and achieves the
best quality as well as the fastest id136 speed.

the pure character model (char input, char output) works surprisingly well on this task, not much worse
than the best wordpiece models in id7 score. however, these models are rather slow to train and slow to
use as the sequences are much longer.

our best model, wpm-32k, achieves a id7 score of 38.95. note that this id7 score represents the
averaged score of 8 models we trained. the maximum id7 score of the 8 models is higher at 39.37. we
point out that our models are completely self-contained, as opposed to previous models reported in [45],
which depend on some external alignment models to achieve their best results. also note that all our test set
numbers were achieved by picking an optimal model on the development set which was then used to decode
the test set.

note that the timing numbers for this section are obtained on cpus, not tpus. we use here the same
cpu machine as described above, and run the decoder with a batchsize of 16 sentences in parallel and a
maximum of 4 concurrent hypotheses at any time per sentence. the time per sentence is the total decoding
time divided by the number of respective sentences in the test set.

table 4: single model results on wmt en   fr (newstest2014)
model id7 cpu decoding time

per sentence (s)

37.90
word
38.01
character
wpm-8k 38.27
wpm-16k 37.60
wpm-32k 38.95
38.39
37.0
31.5
33.1
37.7
39.2

mixed word/character
pbmt [15]
lstm (6 layers) [31]
lstm (6 layers + posunk) [31]
deep-att [45]
deep-att + posunk [45]

0.2226
1.0530
0.1919
0.1874
0.2118
0.2774

similarly, the results of wmt en   de are presented in table 5. again, we    nd that wordpiece models

16

achieves the best id7 scores.

table 5: single model results on wmt en   de (newstest2014)

model id7 cpu decoding time

per sentence (s)

word
character (512 nodes)

23.12
22.62
wpm-8k 23.50
wpm-16k 24.36
wpm-32k 24.61
24.17
20.7
16.5
16.9
16.9
20.6

mixed word/character
pbmt [6]
id56search [37]
id56search-lv [37]
id56search-lv [37]
deep-att [45]

0.2972
0.8011
0.2079
0.1931
0.1882
0.3268

wmt en   de is considered a more di   cult task than wmt en   fr as it has much less training data,
and german, as a more morphologically rich language, needs a huge vocabulary for word models. thus
it is more advantageous to use wordpiece or mixed word/character models, which provide a gain of more
than 2 id7 points on top of the word model and about 4 id7 points on top of previously reported
results in [6, 45]. our best model, wpm-32k, achieves a id7 score of 24.61, which is averaged over 8 runs.
consistently, on the production corpora, wordpiece models tend to be better than other models both in terms
of speed and accuracy.

8.5 evaluation of rl-re   ned models
the models trained in the previous section are optimized for log-likelihood of the next step prediction which
may not correlate well with translation quality, as discussed in section 5. we use rl training to    ne-tune
sentence id7 scores after normal maximum-likelihood training.
the results of rl    ne-tuning on the best en   fr and en   de models are presented in table 6, which
show that    ne-tuning the models with rl can improve id7 scores. on wmt en   fr, model re   nement
improves id7 score by close to 1 point. on en   de, rl-re   nement slightly hurts the test performance
even though we observe about 0.4 id7 points improvement on the development set. the results presented
in table 6 are the average of 8 independent models. we also note that there is an overlap between the
wins from the rl re   nement and the decoder    ne-tuning (i.e., the introduction of length id172 and
coverage penalty). on a less    ne-tuned decoder (e.g., if the decoder does id125 by log-id203
only), the win from rl would have been bigger (as is evident from comparing results in table 2 and table 3).

table 6: single model test id7 scores, averaged over 8 runs, on wmt en   fr and en   de

dataset trained with log-likelihood re   ned with rl
en   fr
en   de

39.92
24.60

38.95
24.67

8.6 model ensemble and human evaluation
we ensemble 8 rl-re   ned models to obtain a state-of-the-art result of 41.16 id7 points on the wmt
en   fr dataset. our results are reported in table 7.
en   de dataset. our results are reported in table 8.

we ensemble 8 rl-re   ned models to obtain a state-of-the-art result of 26.30 id7 points on the wmt

finally, to better understand the quality of our models and the e   ect of rl re   nement, we carried out a
four-way side-by-side human evaluation to compare our id4 translations against the reference translations

17

table 7: model ensemble results on wmt en   fr (newstest2014)

wpm-32k (8 models)
rl-re   ned wpm-32k (8 models)
lstm (6 layers) [31]
lstm (6 layers + posunk) [31]
deep-att + posunk (8 models) [45]

model id7
40.35
41.16
35.6
37.5
40.4

table 8: model ensemble results on wmt en   de (newstest2014). see table 5 for a comparison against
non-ensemble models.

wpm-32k (8 models)
rl-re   ned wpm-32k (8 models)

model id7
26.20
26.30

and the best phrase-based id151s. during the side-by-side comparison, humans
are asked to rate four translations given a source sentence. the four translations are: 1) the best phrase-
based translations as downloaded from http://matrix.statmt.org/systems/show/2065, 2) an ensemble of 8
ml-trained models, 3) an ensemble of 8 ml-trained and then rl-re   ned models, and 4) reference human
translations as taken directly from newstest2014, our results are presented in table 9.

table 9: human side-by-side evaluation scores of wmt en   fr models.

model id7

side-by-side
averaged score

pbmt [15]
id4 before rl
id4 after rl
human

37.0
40.35
41.16

3.87
4.46
4.44
4.82

the results show that even though rl re   nement can achieve better id7 scores, it barely improves the
human impression of the translation quality. this could be due to a combination of factors including: 1) the
relatively small sample size for the experiment (only 500 examples for side-by-side), 2) the improvement in
id7 score by rl is relatively small after model ensembling (0.81), which may be at a scale that human
side-by-side evaluations are insensitive to, and 3) the possible mismatch between id7 as a metric and
real translation quality as perceived by human raters. table 11 contains some example translations from
pbmt, "id4 before rl" and "human", along with the side-by-side scores that human raters assigned to
each translation (some of which we disagree with, see the table caption).

8.7 results on production data
we have carried out extensive experiments on many google-internal production data sets. as the experiments
above cast doubt on whether rl improves the real translation quality or simply the id7 metric, rl-based
model re   nement is not used during these experiments. given the larger volume of training data available in
the google corpora, dropout is also not needed in these experiments.

in this section we describe our experiments with human perception of the translation quality. we asked
human raters to rate translations in a three-way side-by-side comparison. the three sides are from: 1)
translations from the production phrase-based statistical translation system used by google, 2) translations
from our gid4 system, and 3) translations by humans    uent in both languages. reported here in table 10
are averaged rated scores for english     french, english     spanish and english     chinese. all the gid4
models are wordpiece models, without model ensembling, and use a shared source and target vocabulary with
32k wordpieces. on each pair of languages, the evaluation data consist of 500 randomly sampled sentences
from wikipedia and news websites, and the corresponding human translations to the target language. the

18

table 10: mean of side-by-side scores on production data
relative

pbmt gid4 human

improvement

english     spanish
english     french
english     chinese
spanish     english
french     english
chinese     english

4.885
4.932
4.035
4.872
5.046
3.694

5.428
5.295
4.594
5.187
5.343
4.263

5.504
5.496
4.987
5.372
5.404
4.636

87%
64%
58%
63%
83%
60%

results show that our model reduces translation errors by more than 60% compared to the pbmt model on
these major pairs of languages. a typical distribution of side-by-side scores is shown in figure 6.

figure 6: histogram of side-by-side scores on 500 sampled sentences from wikipedia and news websites for a
typical language pair, here english     spanish (pbmt blue, gid4 red, human orange). it can be seen that
there is a wide distribution in scores, even for the human translation when rated by other humans, which
shows how ambiguous the task is. it is clear that gid4 is much more accurate than pbmt.

as expected, on this metric the gid4 system improves also compared to the pbmt system. in some
cases human and gid4 translations are nearly indistinguishable on the relatively simplistic and isolated
sentences sampled from wikipedia and news articles for this experiment. note that we have observed that
human raters, even though    uent in both languages, do not necessarily fully understand each randomly
sampled sentence su   ciently and hence cannot necessarily generate the best possible translation or rate a
given translation accurately. also note that, although the scale for the scores goes from 0 (complete nonsense)
to 6 (perfect translation) the human translations get an imperfect score of only around 5 in table 10, which
shows possible ambiguities in the translations and also possibly non-calibrated raters and translators with a
varying level of pro   ciency.

testing our gid4 system on particularly di   cult translation cases and longer inputs than just single

sentences is the subject of future work.

19

9 conclusion
in this paper, we describe in detail the implementation of google   s id4 (gid4)
system, including all the techniques that are critical to its accuracy, speed, and robustness. on the public
wmt   14 translation benchmark, our system   s translation quality approaches or surpasses all currently
published results. more importantly, we also show that our approach carries over to much larger production
data sets, which have several orders of magnitude more data, to deliver high quality translations.

our key    ndings are: 1) that wordpiece modeling e   ectively handles open vocabularies and the challenge
of morphologically rich languages for translation quality and id136 speed, 2) that a combination of model
and data parallelism can be used to e   ciently train state-of-the-art sequence-to-sequence id4 models
in roughly a week, 3) that model quantization drastically accelerates translation id136, allowing the
use of these large models in a deployed production environment, and 4) that many additional details like
length-id172, coverage penalties, and similar are essential to making id4 systems work well on real
data.

using human-rated side-by-side comparison as a metric, we show that our gid4 system approaches the
accuracy achieved by average bilingual human translators on some of our test sets. in particular, compared
to the previous phrase-based production system, this gid4 system delivers roughly a 60% reduction in
translation errors on several popular language pairs.

acknowledgements
we would like to thank the entire google brain team and google translate team for their foundational
contributions to this project.

references
[1] abadi, m., barham, p., chen, j., chen, z., davis, a., dean, j., devin, m., ghemawat, s.,
irving, g., isard, m., kudlur, m., levenberg, j., monga, r., moore, s., murray, d. g.,
steiner, b., tucker, p., vasudevan, v., warden, p., wicke, m., yu, y., and zheng, x.
tensor   ow: a system for large-scale machine learning. tech. rep., google brain, 2016. arxiv preprint.
[2] bahdanau, d., cho, k., and bengio, y. id4 by jointly learning to align

and translate. in international conference on learning representations (2015).

[3] brown, p., cocke, j., pietra, s. d., pietra, v. d., jelinek, f., mercer, r., and roossin, p.
a statistical approach to language translation. in proceedings of the 12th conference on computational
linguistics - volume 1 (stroudsburg, pa, usa, 1988), coling    88, association for computational
linguistics, pp. 71   76.

[4] brown, p. f., cocke, j., pietra, s. a. d., pietra, v. j. d., jelinek, f., lafferty, j. d.,
mercer, r. l., and roossin, p. s. a statistical approach to machine translation. computational
linguistics 16, 2 (1990), 79   85.

[5] brown, p. f., pietra, v. j. d., pietra, s. a. d., and mercer, r. l. the mathematics of
id151: parameter estimation. comput. linguist. 19, 2 (june 1993), 263   311.
[6] buck, c., heafield, k., and van ooyen, b. id165 counts and language models from the common

crawl. in lrec (2014), vol. 2, citeseer, p. 4.

[7] cho, k., van merrienboer, b., g  l  ehre,   ., bougares, f., schwenk, h., and bengio,
y. learning phrase representations using id56 encoder-decoder for id151. in
conference on empirical methods in natural language processing (2014).

[8] chrisman, l. learning recursive distributed representations for holistic computation. connection

science 3, 4 (1991), 345   366.

20

[9] chung, j., cho, k., and bengio, y. a character-level decoder without explicit segmentation for

id4. arxiv preprint arxiv:1603.06147 (2016).

[10] chung, j., cho, k., and bengio, y. a character-level decoder without explicit segmentation for

id4. corr abs/1603.06147 (2016).

[11] costa-juss  , m. r., and fonollosa, j. a. r. character-based id4. corr

abs/1603.00810 (2016).

[12] dean, j., corrado, g. s., monga, r., chen, k., devin, m., le, q. v., mao, m. z., ranzato,
m., senior, a., tucker, p., yang, k., and ng, a. y. large scale distributed deep networks. in
nips (2012).

[13] devlin, j., zbib, r., huang, z., lamar, t., schwartz, r. m., and makhoul, j. fast and
robust neural network joint models for id151. in acl (1) (2014), citeseer,
pp. 1370   1380.

[14] dong, d., wu, h., he, w., yu, d., and wang, h. id72 for multiple language
translation. in proceedings of the 53rd annual meeting of the association for computational linguistics
(2015), pp. 1723   1732.

[15] durrani, n., haddow, b., koehn, p., and heafield, k. edinburgh   s phrase-based machine
translation systems for wmt-14. in proceedings of the ninth workshop on id151
(2014), association for computational linguistics baltimore, md, usa, pp. 97   104.

[16] fahlman, s. e., and lebiere, c. the cascade-correlation learning architecture. in advances in

neural information processing systems 2 (1990), morgan kaufmann, pp. 524   532.

[17] gers, f. a., schmidhuber, j., and cummins, f. learning to forget: continual prediction with

lstm. neural computation 12, 10 (2000), 2451   2471.

[18] g  l  ehre,   ., ahn, s., nallapati, r., zhou, b., and bengio, y. pointing the unknown words.

corr abs/1603.08148 (2016).

[19] gupta, s., agrawal, a., gopalakrishnan, k., and narayanan, p. deep learning with limited

numerical precision. corr abs/1502.02551 (2015).

[20] han, s., mao, h., and dally, w. j. deep compression: compressing deep neural network with

pruning, trained quantization and hu   man coding. corr abs/1510.00149 (2015).

[21] he, k., zhang, x., ren, s., and sun, j. deep residual learning for image recognition. in ieee

conference on id161 and pattern recognition (2015).

[22] hochreiter, s., bengio, y., frasconi, p., and schmidhuber, j. gradient    ow in recurrent nets:

the di   culty of learning long-term dependencies, 2001.

[23] hochreiter, s., and schmidhuber, j. long short-term memory. neural computation 9, 8 (1997),

1735   1780.

[24] kalchbrenner, n., and blunsom, p. recurrent continuous translation models. in conference on

empirical methods in natural language processing (2013).

[25] kingma, d. p., and ba, j. adam: a method for stochastic optimization. corr abs/1412.6980

(2014).

[26] koehn, p., och, f. j., and marcu, d. statistical phrase-based translation. in proceedings of the
2003 conference of the north american chapter of the association for computational linguistics (2003).

[27] li, f., and liu, b. ternary weight networks. corr abs/1605.04711 (2016).

21

[28] luong, m., and manning, c. d. achieving open vocabulary id4 with hybrid

word-character models. corr abs/1604.00788 (2016).

[29] luong, m.-t., le, q. v., sutskever, i., vinyals, o., and kaiser, l. multi-task sequence to

sequence learning. in international conference on learning representations (2015).

[30] luong, m.-t., pham, h., and manning, c. d. e   ective approaches to attention-based neural

machine translation. in conference on empirical methods in natural language processing (2015).

[31] luong, m.-t., sutskever, i., le, q. v., vinyals, o., and zaremba, w. addressing the rare word
problem in id4. in proceedings of the 53rd annual meeting of the association for
computational linguistics and the 7th international joint conference on natural language processing
(2015).

[32] norouzi, m., bengio, s., chen, z., jaitly, n., schuster, m., wu, y., and schuurmans,
d. reward augmented maximum likelihood for neural id170. in neural information
processing systems (2016).

[33] pascanu, r., mikolov, t., and bengio, y. understanding the exploding gradient problem. corr

abs/1211.5063 (2012).

[34] ranzato, m., chopra, s., auli, m., and zaremba, w. sequence level training with recurrent

neural networks. in international conference on learning representations (2015).

[35] schuster, m., and nakajima, k. japanese and korean voice search. 2012 ieee international

conference on acoustics, speech and signal processing (2012).

[36] schuster, m., and paliwal, k. id182. ieee transactions on

signal processing 45, 11 (nov. 1997), 2673   2681.

[37] s  bastien, j., kyunghyun, c., memisevic, r., and bengio, y. on using very large target
vocabulary for id4. in proceedings of the 53rd annual meeting of the association
for computational linguistics and the 7th international joint conference on natural language processing
(2015).

[38] sennrich, r., haddow, b., and birch, a. id4 of rare words with subword
units. in proceedings of the 54th annual meeting of the association for computational linguistics (2016).
[39] shen, s., cheng, y., he, z., he, w., wu, h., sun, m., and liu, y. minimum risk training
in proceedings of the 54th annual meeting of the association for

for id4.
computational linguistics (2016).

[40] srivastava, r. k., greff, k., and schmidhuber, j. id199. corr abs/1505.00387

(2015).

[41] sutskever, i., vinyals, o., and le, q. v. sequence to sequence learning with neural networks. in

advances in neural information processing systems (2014), pp. 3104   3112.

[42] tu, z., lu, z., liu, y., liu, x., and li, h. coverage-based id4. in proceedings

of the 54th annual meeting of the association for computational linguistics (2016).

[43] wu, j., leng, c., wang, y., hu, q., and cheng, j. quantized convolutional neural networks for

mobile devices. corr abs/1512.06473 (2015).

[44] zaremba, w., sutskever, i., and vinyals, o. recurrent neural network id173, 2014.
[45] zhou, j., cao, y., wang, x., li, p., and xu, w. deep recurrent models with fast-forward

connections for id4. corr abs/1606.04199 (2016).

22

table 11: some example translations from pbmt [15], our gid4 system (the "id4 before rl", table 9),
and human. source and target sentences (human translations) are from the public benchmark wmt en   fr
(newstest2014) data set. the right-hand column shows the human ratings on a scale of 0 (complete nonsense)
to 6 (perfect translation). we disagree with some of the human ratings, e.g., the translation    elle a   t  
rep  r   trois jours plus tard par un promeneur de chien pi  g   dans la carri  re    contains grammatical mistakes
and changes semantics, and is still scored 6. we present it to illustrate the potential problems of the scoring
process.

source

pbmt

gid4

human

source

pbmt

gid4

human

"the reason boeing are doing this is to cram more seats in to make their plane more
competitive with our products," said kevin keniston, head of passenger comfort at
europe   s airbus.
"la raison pour laquelle boeing sont en train de faire, c   est de concentrer davantage de
si  ges pour prendre leur avion plus comp  titive avec nos produits", a d  clar   kevin m.
keniston, chef du confort des passagers de l   airbus de l   europe.
"la raison pour laquelle boeing fait cela est de cr  er plus de si  ges pour rendre son
avion plus comp  titif avec nos produits", a d  clar   kevin keniston, chef du confort des
passagers chez airbus.
"boeing fait   a pour pouvoir caser plus de si  ges et rendre ses avions plus comp  titifs
par rapports    nos produits", a d  clar   kevin keniston, directeur de confort passager
chez l   avionneur europ  en airbus.
when asked about this, an o   cial of the american administration replied: "the united
states is not conducting electronic surveillance aimed at o   ces of the world bank and
imf in washington."
interrog      ce sujet, un responsable de l   administration am  ricaine a r  pondu : "les
etats-unis n   est pas e   ectuer une surveillance   lectronique destin   aux bureaux de la
banque mondiale et du fmi    washington".
interrog      ce sujet, un fonctionnaire de l   administration am  ricaine a r  pondu: "les
  tats-unis n   e   ectuent pas de surveillance   lectronique    l   intention des bureaux de la
banque mondiale et du fmi    washington".
interrog   sur le sujet, un responsable de l   administration am  ricaine a r  pondu: "les
etats-unis ne m  nent pas de surveillance   lectronique visant les si  ges de la banque
mondiale et du fmi    washington".

shu   e.

du remaniement potentiel.

le potentiel remaniement minist  riel.

courant du remaniement   ventuel.
she was spotted three days later by a dog walker trapped in the quarry

source martin told id98 that he asked daley whether his then-boss knew about the potential
pbmt martin a d  clar      id98 qu   il a demand   daley si son patron de l     poque connaissaient
gid4 martin a dit    id98 qu   il avait demand      daley si son patron d   alors   tait au courant
human martin a dit sur id98 qu   il avait demand      daley si son patron d   alors   tait au
source
pbmt elle a   t   rep  r   trois jours plus tard par un promeneur de chien pi  g   dans la carri  re
gid4 elle a   t   rep  r  e trois jours plus tard par un tra  neau    chiens pi  g   dans la carri  re.
human elle a   t   rep  r  e trois jours plus tard par une personne qui promenait son chien
source analysts believe the country is unlikely to slide back into full-blown con   ict, but recent
pbmt les analystes estiment que le pays a peu de chances de retomber dans un con   it total,
mais les   v  nements r  cents ont inqui  t   les investisseurs   trangers et locaux.
selon les analystes, il est peu probable que le pays retombe dans un con   it g  n  ralis  ,
mais les   v  nements r  cents ont attir   des investisseurs   trangers et des habitants
locaux.

gid4
human les analystes pensent que le pays ne devrait pas retomber dans un con   it ouvert, mais
les r  cents   v  nements ont   branl   les investisseurs   trangers et la population locale.

events have unnerved foreign investors and locals.

coinc  e dans la carri  re

23

3.0

6.0

6.0

3.0

6.0

6.0

2.0

6.0

5.0

6.0
2.0
5.0

5.0

2.0

5.0

