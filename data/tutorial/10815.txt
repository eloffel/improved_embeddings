achieving human parity in conversational id103

w. xiong, j. droppo, x. huang, f. seide, m. seltzer, a. stolcke, d. yu and g. zweig

microsoft research

technical report msr-tr-2016-71

revised february 2017

7
1
0
2

 

b
e
f
7
1

 

 
 
]
l
c
.
s
c
[
 
 

2
v
6
5
2
5
0

.

0
1
6
1
:
v
i
x
r
a

abstract

conversational id103 has served as a    agship
id103 task since the release of the switchboard
corpus in the 1990s. in this paper, we measure the human er-
ror rate on the widely used nist 2000 test set, and    nd that
our latest automated system has reached human parity. the
error rate of professional transcribers is 5.9% for the switch-
board portion of the data, in which newly acquainted pairs of
people discuss an assigned topic, and 11.3% for the callhome
portion where friends and family members have open-ended
conversations.
in both cases, our automated system estab-
lishes a new state of the art, and edges past the human bench-
mark, achieving error rates of 5.8% and 11.0%, respectively.
the key to our system   s performance is the use of various
convolutional and lstm acoustic model architectures, com-
bined with a novel spatial smoothing method and lattice-free
mmi acoustic training, multiple recurrent neural network lan-
guage modeling approaches, and a systematic use of system
combination.

index terms    conversational id103, con-
volutional neural networks, recurrent neural networks, vgg,
resnet, lace, blstm, spatial smoothing.

1. introduction

recent years have seen human performance levels reached
or surpassed in tasks ranging from the games of chess and
go [1, 2] to simple id103 tasks like carefully
read newspaper speech [3] and rigidly constrained small-
vocabulary tasks in noise [4, 5]. in the area of speech recog-
nition, much of the pioneering early work was driven by
a series of carefully designed tasks with darpa-funded
datasets publicly released by the ldc and nist [6]:    rst
simple ones like the    resource management    task [7] with
a small vocabulary and carefully controlled grammar; then
read id103 in the wall street journal task [8];
then broadcast news [9]; each progressively more dif   cult
for automatic systems. one of last big initiatives in this
area was in conversational telephone speech (cts), which
is especially dif   cult due to the spontaneous (neither read
nor planned) nature of the speech, its informality, and the

self-corrections, hesitations and other dis   uencies that are
pervasive. the switchboard [10] and later fisher [11] data
collections of the 1990s and early 2000s provide what is to
date the largest and best studied of the conversational corpora.
the history of work in this area includes key contributions
by institutions such as ibm [12], bbn [13], sri [14], at&t
[15], limsi [16], cambridge university [17], microsoft [18]
and numerous others.
in the past, human performance on
this task has been widely cited as being 4% [19]. however,
the error rate estimate in [19] is attributed to a    personal
communication,    and the actual source of this number is
ephemeral. to better understand human performance, we
have used professional transcribers to transcribe the actual
test sets that we are working with, speci   cally the callhome
and switchboard portions of the nist eval 2000 test set.
we    nd that the human error rates on these two parts are
different almost by a factor of two, so a single number is
inappropriate to cite. the error rate on switchboard is about
5.9%, and for callhome 11.3%. we improve on our recently
reported conversational id103 system [20] by
about 0.4%, and now exceed human performance by a small
margin. our progress is a result of the careful engineering
and optimization of convolutional and recurrent neural net-
works. while the basic structures have been well known for a
long period [21, 22, 23, 24, 25, 26, 27], it is only recently that
they have dominated the    eld as the best models for speech
recognition. surprisingly, this is the case for both acoustic
modeling [28, 29, 30, 31, 32, 33] and id38
[34, 35, 36, 37]. in comparison to the standard feed-forward
mlps or dnns that    rst demonstrated breakthrough per-
formance on conversational id103 [18], these
acoustic models have the ability to model a large amount of
acoustic context with temporal invariance, and in the case of
convolutional models, with frequency invariance as well. in
id38, recurrent models appear to improve over
classical id165 models through the use of an unbounded
word history, as well as the generalization ability of contin-
uous word representations [38]. in the meantime, ensemble
learning has become commonly used in several neural mod-
els [39, 40, 35], to improve robustness by reducing bias and
variance. this paper is an expanded version of [20], with the
following additional material:

1. a comprehensive assessment of human performance on

the nist eval 2000 test set

2. the description of a novel spatial id173 method
which signi   cantly boosts our lstm acoustic model
performance

3. the use of lstm rather than id56-lms, and the use

of a letter-trigram input representation

4. a two-level system combination, based on a subsystem
of blstm-system variants that, by itself, surpasses the
best previously reported results

5. expanded coverage of the microsoft cognitive toolkit

(cntk) used to build our models

6. an analysis of human versus machine errors, which
indicates substantial equivalence, with the exception
of the word classes of backchannel acknowledgments
(e.g.    uh-huh   ) and hesitations (e.g.    um   ).

the remainder of this paper describes our system in detail.
section 2 describes our measurement of human performance.
section 3 describes the convolutional neural net (id98) and
long-short-term memory (lstm) models. section 4 de-
scribes our implementation of i-vector adaptation. section
5 presents out lattice-free mmi training process. language
model rescoring is a signi   cant part of our system, and de-
scribed in section 6. we describe the cntk toolkit that
forms the basis of our neural network models in section 7.
experimental results are presented in section 8, followed by
an error analysis in section 9, a review of relevant prior work
in 10 and concluding remarks.

2. human performance

to measure human performance, we leveraged an existing
pipeline in which microsoft data is transcribed on a weekly
basis. this pipeline uses a large commercial vendor to per-
form two-pass transcription.
in the    rst pass, a transcriber
works from scratch to transcribe the data. in the second pass,
a second listener monitors the data to do error correction.
dozens of hours of test data are processed in each batch.
one week, we added the nist 2000 cts evaluation data to
the work-list, without further comment. the intention was to
measure the error rate of professional transcribers going about
their normal everyday business. aside from the standard two-
pass checking in place, we did not do a complex multi-party
transcription and adjudication process. the transcribers were
given the same audio segments as were provided to the speech
recognition system, which results in short sentences or sen-
tence fragments from a single channel. this makes the task
easier since the speakers are more clearly separated, and more
dif   cult since the two sides of the conversation are not in-
terleaved. thus, it is the same condition as reported for our

automated systems. the resulting numbers are 5.9% for the
switchboard portion, and 11.3% for the callhome portion
of the nist 2000 test set, using the nist scoring protocol.
these numbers should be taken as an indication of the    error
rate    of a trained professional working in industry-standard
speech transcript production. (we have submitted the human
transcripts thus produced to the linguistic data consortium
for publication, so as to facilitate research by other groups.)
past work [41] reports inter-transcriber error rates for data
taken from the later rt03 test set (which contains switch-
board and fisher, but no callhome data). error rates of 4.1
to 4.5% are reported for extremely careful multiple transcrip-
tions, and 9.6% for    quick transcriptions.    while this is a
different test set, the numbers are in line with our    ndings.
we note that the bulk of the fisher training data, and the bulk
of the data overall, was transcribed with the    quick transcrip-
tion    guidelines. thus, the current state of the art is actually
far exceeding the noise level in its own training data. per-
haps the most important point is the extreme variability be-
tween the two test subsets. the more informal callhome data
has almost double the human error rate of the switchboard
data.
interestingly, the same informality, multiple speakers
per channel, and recording conditions that make callhome
hard for computers make it dif   cult for people as well. no-
tably, the performance of our arti   cial system aligns almost
exactly with the performance of people on both sets.

3. convolutional and lstm neural

networks

3.1. id98s

we use three id98 variants. the    rst is the vgg architecture
of [42]. compared to the networks used previously in image
recognition, this network uses small (3x3)    lters, is deeper,
and applies up to    ve convolutional layers before pooling.
the second network is modeled on the resnet architecture
[43], which adds highway connections [44], i.e. a linear trans-
form of each layer   s input to the layer   s output [44, 45]. the
only difference is that we apply batch id172 before
computing relu activations. the last id98 variant is the
lace (layer-wise context expansion with attention) model
[46]. lace is a tdnn [23] variant in which each higher
layer is a weighted sum of nonlinear transformations of a win-
dow of lower layer frames. in other words, each higher layer
exploits broader context than lower layers. lower layers fo-
cus on extracting simple local patterns while higher layers
extract complex patterns that cover broader contexts. since
not all frames in a window carry the same importance, an at-
tention mask is applied. the lace model differs from the
earlier tdnn models e.g. [23, 47] in the use of a learned
attention mask and resnet like linear pass-through. as illus-
trated in detail in figure 1, the model is composed of four
blocks, each with the same architecture. each block starts

table 1. comparison of id98 architectures

table 2. accuracy improvements from spatial smoothing on
the nist 2000 cts test set. the model is a six layer blstm,
using i-vectors and 40 dimensional    lterbank features, and a
dimension of 512 in each direction of each layer.

wer (%)

model

baseline
spatial smoothing

9000 senones
swb
ch
21.4
9.9
9.3
19.2

27000 senones
swb
ch
20.5
10.6
9.2
19.5

model. first, each vector of activations is re-interpreted as
a 2-dimensional image. for example, if there are 512 neu-
rons, they are interpreted as the pixels of a 16 by 32 image.
second, this image is high-pass    ltered. the    lter is imple-
mented as a circular convolution with a 3 by 3 kernel. the
center tap of the kernel has a value of 1, and the remaining
eight having a value of    1/8. third, the energy of this high-
pass    ltered image is computed and added to the training
objective function. we have found empirically that a suitable
scale for this energy is 0.1 with respect to the existing cross
id178 objective function. the overall effect of this process
is to make the training algorithm prefer models that have
correlated neurons, and to improve the word error rate of the
acoustic model. table 2 shows the bene   t in error rate for
some of our early systems. we observed error reductions of
between 5 and 10% relative from spatial smoothing.

fig. 1. lace network architecture

with a convolution layer with stride 2 which sub-samples the
input and increases the number of channels. this layer is
followed by four relu-convolution layers with jump links
similar to those used in resnet. table 1 compares the layer
structure and parameters of the three id98 architectures. we
also trained a fused model by combining a resnet model and
a vgg model at the senone posterior level. both base models
are independently trained, and then the score fusion weight is
optimized on development data. the fused system is our best
single system.

3.2. lstms

while our best performing models are convolutional, the use
of id137 (lstms) is a close sec-
ond. we use a bidirectional architecture [48] without frame-
skipping [29]. the core model structure is the lstm de   ned
in [28]. we found that using networks with more than six
layers did not improve the word error rate on the develop-
ment set, and chose 512 hidden units, per direction, per layer,
as that provided a reasonable trade-off between training time
and    nal model accuracy.

3.3. spatial smoothing

inspired by the human auditory cortex, where neighboring
neurons tend to simultaneously activate, we employ a spatial
smoothing technique to improve the accuracy of our lstm
models. the smoothing is implemented as a regulariza-
tion term on the activations between layers of the acoustic

4. speaker adaptive modeling

speaker adaptive modeling in our system is based on con-
ditioning the network on an i-vector [49] characterization of
each speaker [50, 51]. a 100-dimensional i-vector is gener-
ated for each conversation side. for the lstm system, the
conversation-side i-vector vs is appended to each frame of
input. for convolutional networks, this approach is inappro-
priate because we do not expect to see spatially contiguous
patterns in the input. instead, for the id98s, we add a learn-
able weight matrix w l to each layer, and add w lvs to the
activation of the layer before the nonlinearity. thus, in the
id98, the i-vector essentially serves as an speaker-dependent
bias to each layer. note that the i-vectors are estimated us-
ing mfcc features; by using them subsequently in systems
based on log-   lterbank features, we may bene   t from a form
of feature combination. performance improvements from i-
vectors are shown in table 3. the full experimental setup is
described in section 8.

5. lattice-free sequence training

after standard cross-id178 training, we optimize the model
parameters using the maximum mutual information (mmi)
objective function. denoting a word sequence by w and its
corresponding acoustic realization by a, the training criterion
is

(cid:88)

log

w,a   data

(cid:80)(cid:48)
p (w)p (a|w)
w p (w(cid:48))p (a|w(cid:48))

3. estimate an unsmoothed, variable-length id165 lan-
guage model from this data, where the history state con-
sists of the previous phone and previous senones within
the current phone.

to illustrate this, consider the sample senone sequence
{s s2.1288, s s3.1061, s s4.1096}, {eh s2.527, eh s3.128,
eh s4.66}, {t s2.729, t s3.572, t s4.748}. when predict-
ing the state following eh s4.66 the history consists of (s,
eh s2.527, eh s3.128, eh s4.66), and following t s2.729, the
history is (eh, t s2.729). we construct the denominator graph
from this language model, and id48 transition probabilities
as determined by transition-counting in the senone sequences
found in the training data. our approach not only largely
reduces the complexity of building up the language model
but also provides very reliable training performance. we
have found it convenient to do the full computation, with-
out pruning, in a series of matrix-vector operations on the
gpu. the underlying acceptor is represented with a sparse
matrix, and we maintain a dense likelihood vector for each
time frame. the alpha and beta recursions are implemented
with cusparse level-2 routines: sparse-matrix, dense vec-
tor multiplies. run time is about 100 times faster than real
time. as in [54], we use cross-id178 id173. in all
the lattice-free mmi (lfmmi) experiments mentioned below
we use a trigram language model. most of the gain is usually
obtained after processing 24 to 48 hours of data.

.

6. lm rescoring and system combination

as noted in [52, 53], the necessary gradient for use in back-
propagation is a simple function of the posterior id203 of
a particular acoustic model state at a given time, as computed
by summing over all possible word sequences in an uncon-
strained manner. as    rst done in [12], and more recently in
[54], this can be accomplished with a straightforward alpha-
beta computation over the    nite state acceptor representing
the decoding search space. in [12], the search space is taken to
be an acceptor representing the composition hclg for a un-
igram language model l on words. in [54], a language model
on phonemes is used instead. in our implementation, we use
a mixed-history acoustic unit language model. in this model,
the id203 of transitioning into a new context-dependent
phonetic state (senone) is conditioned on both the senone and
phone history. we found this model to perform better than
either purely word-based or phone-based models. based on a
set of initial experiments, we developed the following proce-
dure:

an initial decoding is done with a wfst decoder, using
the architecture described in [55]. we use an id165 lan-
guage model trained and pruned with the srilm toolkit
[56]. the    rst-pass lm has approximately 15.9 million bi-
grams, trigrams, and 4grams, and a vocabulary of 30,500
words. it gives a perplexity of 69 on the 1997 cts evaluation
transcripts. the initial decoding produces a lattice with the
pronunciation variants marked, from which 500-best lists are
generated for rescoring purposes. subsequent n-best rescor-
ing uses an unpruned lm comprising 145 million id165s.
all id165 lms were estimated by a maximum id178 cri-
terion as described in [57]. the n-best hypotheses are then
rescored using a combination of the large id165 lm and
several neural net lms. we have experimented with both
id56 lms and lstm lms, and describe the details in the
following two sections.

6.1. id56-lm setup

1. perform a forced alignment of the training data to select
lexical variants and determine frame-aligned senone se-
quences.

our id56-lms are trained and evaluated using the cued-
id56lm toolkit [58]. our id56-lm con   guration has sev-
eral distinctive features, as described below.

2. compress consecutive framewise occurrences of a sin-

gle senone into a single occurrence.

1. we trained both standard, forward-predicting id56-
lms and backward id56-lms that predict words in

table 3. performance improvements from i-vector and lfmmi training on the nist 2000 cts test set

wer (%)

con   guration

baseline
i-vector
i-vector+lfmmi

relu-dnn
ch
swb
13.4
21.9
11.5
20.1
17.9
10.2

resnet-id98
ch
swb
11.1
17.5
10.0
16.6
15.2
8.6

blstm

lace

ch
17.3
17.6
16.3

swb
10.3
9.9
8.9

ch
16.9
16.4
15.2

swb
10.4
9.3
8.5

reverse temporal order. the log probabilities from both
models are added.

2. as is customary, the id56-lm id203 estimates are
interpolated at the word-level with corresponding n-
gram lm probabilities (separately for the forward and
backward models).
in addition, we trained a second
id56-lm for each direction, obtained by starting with
different random initial weights. the two id56-lms
and the id165 lm for each direction are interpolated
with weights of (0.375, 0.375, 0.25).

3. in order to make use of lm training data that is not fully
matched to the target conversational speech domain,
we start id56-lm training with the union of in-domain
(here, cts) and out-of-domain (e.g., web) data. upon
convergence, the network undergoes a second training
phase using the in-domain data only. both training
phases use in-domain validation data to regulate the
learning rate schedule and termination. because the
size of the out-of-domain data is a multiple of the in-
domain data, a standard training on a simple union of
the data would not yield a well-matched model, and
have poor perplexity in the target domain.

4. we found best results with an id56-lm con   guration
that had a second, non-recurrent hidden layer. this pro-
duced lower perplexity and word error than the stan-
dard, single-hidden-layer id56-lm architecture [34].1
the overall network architecture thus had two hidden
layers with 1000 units each, using relu nonlineari-
ties. training used noise-contrastive estimation (nce)
[59].

5. the id56-lm output vocabulary consists of all words
occurring more than once in the in-domain training
set. while the id56-lm estimates a id203 for un-
known words, we take a different approach in rescor-
ing: the number of out-of-set words is recorded for
each hypothesis and a penalty for them is estimated
for them when optimizing the relative weights for all
model scores (acoustic, lm, pronunciation), using the
srilm nbest-optimize tool.

1however, adding more hidden layers produced no further gains.

6.2. lstm-lm setup

after obtaining good results with id56-lms we also explored
the lstm recurrent network architecture for language mod-
eling, inspired by recent work showing gains over id56-lms
for conversational id103 [37]. in addition to ap-
plying the lessons learned from our id56-lm experiments,
we explored additional alternatives, as described below.

1. there are two types of input vectors our lstm lms
take, word based one-hot vector input and letter trigram
vector [60] input.
including both forward and back-
ward models, we trained four different lstm lms in
total.

2. for the word based input, we leveraged the approach
from [61] to tie the input embedding and output em-
bedding together.

3. here we also used a two-phase training schedule to
train the lstm lms. first we train the model on
the combination of in-domain and out-domain data for
four data passes without any learning rate adjustment.
we then start from the resulting model and train on
in-domain data until convergence.

4. overall the letter trigram based models perform a little
better than the word based language model. we tried
applying dropout on both types of language models but
didn   t see an improvement.

5. convergence was improved through a variation of self-
stabilization [62], in which each output vector x of non-
linearities are scaled by 1
4 ln(1 + e4  ), where a    is a
scalar that is learned for each output. this has a similar
effect as the scale of the well-known batch normaliza-
tion technique, but can be used in recurrent loops.

6. table 4 shows the impact of number of layers on the    -
nal perplexities. based on this, we proceeded with three
hidden layers, with 1000 hidden units each. the per-
plexities of each lstm-lm we used in the    nal com-
bination (before interpolating with the id165 model)
can be found in table 5.

for the    nal system, we interpolated two lstm-lms with
an id165 lm for the forward-direction lm, and similarly
for the backward-direction lm. all lstms use three hidden

table 4. lstm perplexities (ppl) as a function of hidden
layers, trained on in-domain data only, computed on 1997
cts eval transcripts.

language model
letter trigram input with one layer (baseline)

+ two hidden layers
+ three hidden layers
+ four hidden layers
+    ve hidden layers
+ six hidden layers

ppl
63.2
61.8
59.1
59.6
60.2
63.7

table 5. perplexities (ppl) of the four lstm lms used in
the    nal combination. ppl is computed on 1997 cts eval
transcripts. all the lstm lms are with three hidden layers.

language model
id56: 2 layers + word input (baseline)
lstm: word input in forward direction
lstm: word input in backward direction
lstm: letter trigram input in forward direction
lstm: letter trigram input in backward direction

ppl
59.8
54.4
53.4
52.1
52.0

layers and are trained on in-domain and web data. unlike for
the id56-lms, the two models being interpolated differ not
just in their random initialization, but also in the input encod-
ing (one uses a triletter encoding, the other a one-hot word
encoding). the forward and backward lm log id203
scores are combined additively.

6.3. training data

the 4-gram language model for decoding was trained on the
available cts transcripts from the darpa ears program:
switchboard (3m words), bbn switchboard-2 transcripts
(850k), fisher (21m), english callhome (200k), and the uni-

table 6. performance of various versions of neural-net-based
lm rescoring. perplexities (ppl) are computed on 1997 cts
eval transcripts; word error rates (wer) on the nist 2000
switchboard test set.

language model
4-gram lm (baseline)
+ id56lm, cts data only

+ web data training

+ 2nd hidden layer

+ 2-id56lm interpolation

+ backward id56lms
+ lstm-lm, cts + web data
+ 2-lstm-lm interpolation

+ backward lstm-lm

ppl wer
8.6
69.4
62.6
7.6
7.4
60.9
7.4
59.0
7.3
57.2
6.9
6.9
6.8
6.6

51.4
50.5

-

-

versity of washington conversational web corpus (191m). a
separate id165 model was trained from each source and
interpolated with weights optimized on rt-03 transcripts.
for the unpruned large rescoring 4-gram, an additional lm
component was added, trained on 133m word of ldc broad-
cast news texts. the id165 lm con   guration is modeled
after that described in [51], except that maxent smoothing
was used. the id56 and lstm lms were trained on switch-
board and fisher transcripts as in-domain data (20m words
for gradient computation, 3m for validation). to this we
added 62m words of uw web data as out-of-domain data,
for use in the two-phase training procedure described above.

6.4. id56-lm and lstm-lm performance

table 6 gives perplexity and word error performance for vari-
ous recurrent neural net lm setups, from simple to more com-
plex. the acoustic model used was the resnet id98. note
that, unlike the results in tables 4 and 5, the neural net lms
in table 6 are interpolated with the id165 lm. as can be
seen, each of the measures described earlier adds incremen-
tal gains, which, however, add up to a substantial improve-
ment overall. the total gain relative to a purely id165 based
system is a 20% relative error reduction with id56-lms, and
23% with lstm-lms. as shown later (see table 8) the gains
with different acoustic models are similar.

6.5. system combination

the lm rescoring is carried out separately for each acoustic
model. the rescored n-best lists from each subsystem are
then aligned into a single confusion network [63] using the
srilm nbest-rover tool. however, the number of potential
candidate systems is too large to allow an all-out combina-
tion, both for practical reasons and due to over   tting issues.
instead, we perform a greedy search, starting with the single
best system, and successively adding additional systems, to
   nd a small set of systems that are maximally complemen-
tary. the rt-02 switchboard set was used for this search pro-
cedure. we experimented with two search algorithms to    nd
good subsets of systems. we always start with the system
giving the best individual accuracy on the development set.
in one approach, a greedy forward search then adds systems
incrementally to the combination, giving each equal weight.
if no improvement is found with any of the unused systems,
we try adding each with successively lower relative weights
of 0.5, 0.2, and 0.1, and stop if none of these give an improve-
ment. a second variant of the search procedure that can give
lower error (as measured on the devset) estimates the best sys-
tem weights for each incremental combination candidate. the
weight estimation is done using an expectation-maximization
algorithm based on aligning the reference words to the con-
fusion networks, and maximizing the weighted id203 of
the correct word at each alignment position. to avoid over   t-
ting, the weights for an n-way combination are smoothed hi-

erarchically, i.e., interpolated with the weights from the (n    
1)-way system that preceded it. this tends to give robust
weights that are biased toward the early (i.e., better) subsys-
tems. the    nal system incorporated a variety of blstm
models with roughly similar performance, but differing in
various metaparameters (number of senones, use of spatial
smoothing, and choice of pronunciation dictionaries).2 to
further limit the number of free parameters to be estimated in
system combination, we performed system selection in two
stages. first, we selected the four best blstm systems. we
then combined these with equal weights and treated them as
a single subsystem in searching for a larger combination in-
cluding other acoustic models. this yielded our best overall
combined system, as reported in section 8.3.

7. microsoft cognitive toolkit (cntk)

all neural networks in the    nal system were trained with
the microsoft cognitive toolkit, or cntk [64, 65]. on a
linux-based multi-gpu server farm. cntk allows for    ex-
ible model de   nition, while at the same time scaling very
ef   ciently across multiple gpus and multiple servers. the
resulting fast experimental turnaround using the full 2000-
hour corpus was critical for our work.

7.1. flexible, terse model de   nition

in cntk, a neural network (and the training criteria) are
speci   ed by its formula, using a custom functional lan-
guage (brainscript), or python. a graph-based execution
engine, which provides automatic differentiation, then trains
the model   s parameters through sgd. leveraging a stock li-
brary of common layer types, networks can be speci   ed very
tersely. samples can be found in [64].

7.2. multi-server training using 1-bit sgd

training the acoustic models in this paper on a single gpu
would take many weeks or even months. cntk made train-
ing times feasible by parallelizing the sgd training with our
1-bit sgd parallelization technique [66]. this data-parallel
method distributes minibatches over multiple worker nodes,
and then aggregates the sub-gradients. while the necessary
communication time would otherwise be prohibitive, the 1-
bit sgd method eliminates the bottleneck by two techniques:
1-bit quantization of gradients and automatic minibatch-size
scaling. in [66], we showed that gradient values can be quan-
tized to just a single bit, if one carries over the quantization er-
ror from one minibatch to the next. each time a sub-gradient
is quantized, the quantization error is computed and remem-
bered, and then added to the next minibatch   s sub-gradient.

2we used two different dictionaries, one based on a standard phone set
and another with dedicated vowel and nasal phones used only in the pro-
nunciations of    lled pauses (   uh   ,    um   ) and backchannel acknowledgments
(   uh-huh   ,    mhm   ), similar to [63].

this reduces the required bandwidth 32-fold with minimal
loss in accuracy. secondly, automatic minibatch-size scaling
progressively decreases the frequency of model updates. at
regular intervals (e.g. every 72h of training data), the trainer
tries larger minibatch sizes on a small subset of data and picks
the largest that maintains training loss. these two techniques
allow for excellent multi-gpu/server scalability, and reduced
the acoustic-model training times on 2000h from months to
between 1 and 3 weeks, making this work feasible.

7.3. computational performance

table 7 compares the runtimes, as multiples of speech dura-
tion, of various processing steps associated with the differ-
ent acoustic model architectures (   gures for dnns are given
only as a reference point, since they are not used in our sys-
tem). acoustic model (am) training comprises the forward
and backward id145 passes, as well as pa-
rameter updates. am evaluation refers to the forward com-
putation only. decoding includes am evaluation along with
hypothesis search (only the former makes use of the gpu).
runtimes were measured on a 12-core intel xeon e5-2620v3
cpu clocked at 2.4ghz, with an nvidia titan x gpu. we
observe that the gpu gives a 10 to 100-fold speedup for am
evaluation over the cpu implementation. am evaluation is
thus reduced to a small faction of overall decoding time, mak-
ing near-realtime operation possible.

8. experiments and results

8.1. speech corpora

we train with the commonly used english cts (switchboard
and fisher) corpora. evaluation is carried out on the nist
2000 cts test set, which comprises both switchboard (swb)
and callhome (ch) subsets. the waveforms were segmented
according to the nist partitioned evaluation map (pem)    le,
with 150ms of dithered silence padding added in the case of
the callhome conversations.3 the switchboard-1 portion of
the nist 2002 cts test set was used for tuning and develop-
ment. the acoustic training data is comprised by ldc cor-
pora 97s62, 2004s13, 2005s13, 2004s11 and 2004s09; see
[12] for a full description.

8.2. acoustic model details

forty-dimensional log-   lterbank features were extracted ev-
ery 10 milliseconds, using a 25-millisecond analysis window.
the id98 models used window sizes as indicated in table 1,
and the lstms processed one frame of input at a time. the
bulk of our models use three state left-to-right triphone mod-
els with 9000 tied states. additionally, we have trained sev-
eral models with 27k tied states. the phonetic inventory in-

3using the sox tool options pad 0.15 0.15 dither -p 14.

table 7. runtimes as factor of speech duration for various aspects of acoustic modeling and decoding, for different types of
acoustic model

processing step hardware
am training
am evaluation
am evaluation
decoding

gpu
gpu
cpu
gpu

dnn
0.012
0.0064
0.052
1.04

resnet-id98 blstm lace
0.23
0.081
8.47
1.38

0.60
0.15
11.7
1.19

0.022
0.0081

n/a
1.40

table 8. word error rates (%) on the nist 2000 cts test set with different acoustic models. unless otherwise noted, models
are trained on the full 2000 hours of data and have 9k senones.

id56-lm

model

resnet, 300h training
resnet
resnet, gmm alignments
vgg
vgg + resnet
lace
blstm
blstm, spatial smoothing
blstm, spatial smoothing, 27k senones
blstm, spatial smoothing, 27k senones, alternate dictionary
blstm system combination
full system combination

id165 lm
ch
19.2
14.8
15.3
15.7
14.5
15.0
16.5
15.4
15.3
14.9
13.2
13.0

swb ch
17.7
10.0
13.2
8.6
13.7
8.8
9.1
14.1
13.0
8.4
13.5
8.4
15.2
9.0
8.6
13.7
13.8
8.3
13.7
8.3
7.3
12.1
11.7
7.3

lstm-lm
swb
7.7
6.6
6.9
7.1
6.4
6.7
7.0
7.0
6.8
6.7
6.0
5.8

swb ch
17.0
8.2
12.5
6.9
12.8
7.3
7.6
13.2
12.2
6.9
13.0
7.2
14.4
7.5
7.4
13.0
13.2
7.0
13.0
7.0
6.4
11.6
11.0
6.1

table 9. comparative error rates from the literature and hu-
man error as measured in this work

lattices and post-processing them, we consider lfmmi to be
a signi   cant advance in technology.

model

id165 lm neural net lm
ch
povey et al. [54] lstm 15.3
saon et al. [51] lstm 15.1
saon et al. [51] system 13.7
2016 microsoft system 13.3
human transcription

swb ch
8.5
9.0
7.6
7.4

swb
-
-
6.6
5.8
5.9

-
-
12.2
11.0
11.3

cludes special models for noise, vocalized-noise, laughter and
silence. we use a 30k-vocabulary derived from the most com-
mon words in the switchboard and fisher corpora. the de-
coder uses a statically compiled unigram graph, and dynam-
ically applies the language model score. the unigram graph
has about 300k states and 500k arcs. table 3 shows the result
of i-vector adaptation and lfmmi training on several of our
early systems. we achieve a 5   8% relative improvement from
i-vectors, including on id98 systems. the last row of table 3
shows the effect of lfmmi training on the different models.
we see a consistent 7   10% further relative reduction in error
rate for all models. considering the great increase in procedu-
ral simplicity of lfmmi over the previous practice of writing

8.3. overall results and discussion

the performance of all our component models is shown in
table 8, along with the blstm combination and full system
combination results. (recall that the four best blstm sys-
tems are combined with equal weights    rst, as described in
section 6.5.) key benchmarks from the literature, our own
best results, and the measured human error rates are com-
pared in table 9.4 all models listed in table 8 are selected
for the combined systems for one or more of the three rescor-
ing lms. the only exception is the vgg+resnet system,
which combines acoustic senone posteriors from the vgg
and resnet networks. while this yields our single best acous-
tic model, only the individual vgg and resnet models are
used in the overall system combination. we also observe that
the four model variants chosen for the combined blstm sub-
system differ incrementally by one hyperparameter (smooth-

4when comparing the last row in table 3 with the    id165 lm    results
in table 8, note that the former results were obtained with the pruned id165
lm used in the decoder and    xed score weights (during lattice generation),
whereas the latter results are from rescoring with the unpruned id165 lm
(during n-best generation), using optimized score weighting. accordingly,
the rescoring results are generally somewhat better.

ing, number of senones, dictionary), and that the blstms
alone achieve an error that is within 3% relative of the full
system combination. this validates the rationale that choos-
ing different hyperparameters is an effective way to obtain
complementary systems for combination purposes. we also
assessed the lower bound of performance for our lattice/n-
best rescoring paradigm. the 500-best lists from the lattices
generated with the resnet id98 system had an oracle (lowest
achievable) wer of 2.7% on the switchboard portion of the
nist 2000 evaluation set, and an oracle wer of 4.9% on the
callhome portion. the oracle error of the combined system
is even lower (though harder to quantify) since (1) n-best out-
put from all systems are combined and (2) confusion network
construction generates new possible hypotheses not contained
in the original n-best lists. with oracle error rates less than
half the currently achieved actual error rates, we conclude that
search errors are not a major limiting factor to even better ac-
curacy.

9. error analysis

in this section, we compare the errors made by our arti   -
cial recognizer with those made by human transcribers. we
   nd that the machine errors are substantially the same as
human ones, with one large exception: confusions between
backchannel words and hesitations. the distinction is that
backchannel words like    uh-huh    are an acknowledgment
of the speaker, also signaling that the speaker should keep
talking, while hesitations like    uh    are used to indicate that
the current speaker has more to say and wants to keep his
or her turn.5 as turn-management devices, these two classes
of words therefore have exactly opposite functions. table
10 shows the ten most common substitutions for both hu-
mans and the arti   cial system. tables 11 and 12 do the same
for deletions and insertions. focusing on the substitutions,
we see that by far the most common error in the asr sys-
tem is the confusion of a hesitation in the reference for a
backchannel in the hypothesis. people do not seem to have
this problem. we speculate that this is due to the nature of
the fisher training corpus, where the    quick transcription   
guidelines were predominately used [41]. we    nd that there
is inconsistent treatment of backchannel and hesitation in
the resulting data; the relatively poor performance of the au-
tomatic system here might simply be due to confusions in
the training data annotations. for perspective, there are over
twenty-one thousand words in each test set. thus the errors
due to hesitation/backchannel substitutions account for an
error rate of only about 0.2% absolute.

the most frequent substitution for people on the switch-
board corpus was mistaking a hesitation in the reference for

5the nist scoring protocol treats hesitation words as optional, and we
therefore delete them from our recognizer output prior to scoring. this ex-
plains why we see many substitutions of backchannels for hesitations, but not
vice-versa.

the word    id48.    the scoring guidelines treat    id48    as a
word distinct from backchannels and hesitations, so this is not
a scoring mistake. examination of the contexts in which the
error is made show that it is most often intended to acknowl-
edge the other speaker, i.e. as a backchannel. for both people
and our automated system, the insertion and deletion patterns
are similar: short function words are by far the most frequent
errors. in particular, the single most common error made by
the transcribers was to omit the word    i.    while we believe
further improvement in function and content words is possi-
ble, the signi   cance of the remaining backchannel/hesitation
confusions is unclear. table 13 shows the overall error rates
broken down by substitutions, insertions and deletions. we
see that the human transcribers have a somewhat lower sub-
stitution rate, and a higher deletion rate. the relatively higher
deletion rate might re   ect a human bias to avoid outputting
uncertain information, or the productivity demands on a pro-
fessional transcriber. in all cases, the number of insertions is
relatively small.

10. relation to prior work

compared to earlier applications of id98s to speech recog-
nition [67, 68], our networks are much deeper, and use linear
bypass connections across convolutional layers. they are
similar in spirit to those studied more recently by [31, 30,
51, 32, 33]. we improve on these architectures with the
lace model [46], which iteratively expands the effective
window size, layer-by-layer, and adds an attention mask to
differentially weight distant context. our spatial regulariza-
tion technique is similar in spirit to stimulated deep neural
networks [69]. whereas stimulated networks use a supervi-
sion signal to encourage locality of activations in the model,
our technique is automatic. our use of lattice-free mmi is
distinctive, and extends previous work [12, 54] by proposing
the use of a mixed triphone/phoneme history in the language
model. on the id38 side, we achieve a per-
formance boost by combining multiple lstm-lms in both
forward and backward directions, and by using a two-phase
training regimen to get best results from out-of-domain data.
for our best id98 system, lstm-lm rescoring yields a rela-
tive word error reduction of 23%, and a 20% relative gain for
the combined recognition system, considerably larger than
previously reported for conversational id103
[37].

11. conclusions

we have measured the human error rate on nist   s 2000 con-
versational telephone id103 task. we    nd that
there is a great deal of variability between the switchboard
and callhome subsets, with 5.8% and 11.0% error rates re-
spectively. for the    rst time, we report automatic recogni-
tion performance on par with human performance on this task.

table 10. most common substitutions for asr system and humans. the number of times each error occurs is followed by the
word in the reference, and what appears in the hypothesis instead.

ch

asr
45: (%hesitation) / %bcack
12: was / is
9: (%hesitation) / a
8: (%hesitation) / oh
8: a / the
7: and / in
7: it / that
6: in / and
5: a / to
5: aw / oh

human
12: a / the
10: (%hesitation) / a
10: was / is
7: (%hesitation) / id48
7: bentsy / bensi
7: is / was
6: could / can
6: well / oh
5: (%hesitation) / %bcack
5: (%hesitation) / oh

swb

asr
29: (%hesitation) / %bcack
9: (%hesitation) / oh
9: was / is
8: and / in
6: (%hesitation) / i
6: in / and
5: (%hesitation) / a
5: (%hesitation) / yeah
5: a / the
5: jeez / jeeze

human
12: (%hesitation) / id48
10: (%hesitation) / oh
9: was / is
8: (%hesitation) / a
5: in / and
4: (%hesitation) / %bcack
4: and / in
4: is / was
4: that / it
4: the / a

table 11. most common deletions for asr system and hu-
mans.

table 12. most common insertions for asr system and hu-
mans.

asr
44: i
33: it
29: a
29: and
25: is
19: he
18: are
17: oh
17: that
17: the

ch
human
73: i
59: and
48: it
47: is
45: the
41: %bcack
37: a
33: you
31: oh
30: that

swb

asr
31: it
26: i
19: a
17: that
15: you
13: and
12: have
12: oh
11: are
11: is

human
34: i
30: and
29: it
22: a
22: that
22: you
17: the
17: to
15: oh
15: yeah

ch

asr
15: a
15: is
11: i
11: the
11: you
9: it
7: oh
6: and
6: in
6: know 4: they

human
10: i
9: and
8: a
8: that
8: the
7: have
5: you
4: are
4: is

swb

asr
19: i
9: and
7: of
6: do
6: is
5: but
5: yeah
4: air
4: in
4: you

human
12: i
11: and
9: you
8: is
6: they
5: do
5: have
5: it
5: yeah
4: a

our system   s performance can be attributed to the systematic
use of lstms for both acoustic and id38, as
well as id98s in the acoustic model, and extensive combina-
tion of complementary system for both acoustic and language
modeling.

acknowledgments

table 13. overall substitution, deletion and insertion rates.

ch

asr human
6.5
3.3
1.4
11.1

4.1
6.5
0.7
11.3

sub
del
ins
all

swb

asr human
3.3
1.8
0.7
5.9

2.6
2.7
0.7
5.9

we thank arul menezes for access to the microsoft transcrip-
tion pipeline; chris basoglu, amit agarwal and marko rad-
milac for their invaluable assistance with cntk; jinyu li and
partha parthasarathy for many helpful conversations. we also
thank x. chen from cambridge university for valuable assis-
tance with the cued-id56lm toolkit, and the international
computer science institute for compute and data resources.

12. references

[1] m. campbell, a. j. hoane, and f.-h. hsu,    deep blue   ,

arti   cial intelligence, vol. 134, pp. 57   83, 2002.

[2] d. silver, a. huang, c. j. maddison, a. guez, l. sifre,
g. van den driessche, j. schrittwieser, i. antonoglou,
v. panneershelvam, m. lanctot, et al.,    mastering the
game of go with deep neural networks and tree search   ,
nature, vol. 529, pp. 484   489, 2016.

[3] d. amodei, r. anubhai, e. battenberg, c. case,
j. casper, b. catanzaro, j. chen, m. chrzanowski,
a. coates, g. diamos, et al.,    deep speech 2: end-to-
end id103 in english and mandarin   , arxiv
preprint arxiv:1512.02595, 2015.

[4] t. t. kristjansson, j. r. hershey, p. a. olsen, s. j. ren-

   super-human multi-talker
nie, and r. a. gopinath,
id103:
the ibm 2006 speech separation
challenge system   , in proc. interspeech, vol. 12, p. 155,
2006.

[5] c. weng, d. yu, m. l. seltzer, and j. droppo,    single-
channel mixed id103 using deep neural
in proc. ieee icassp, pp. 5632   5636.
networks   ,
ieee, 2014.

[6] d. s. pallett,    a look at nist   s benchmark asr tests:
in ieee automatic speech
past, present, and future   ,
recognition and understanding workshop, pp. 483   
488. ieee, 2003.

[7] p. price, w. m. fisher, j. bernstein, and d. s. pal-
   the darpa 1000-word resource management
in proc.

lett,
database for continuous id103   ,
ieee icassp, pp. 651   654. ieee, 1988.

[8] d. b. paul and j. m. baker,    the design for the wall
in proceedings of the
street journal-based csr corpus   ,
workshop on speech and natural language, pp. 357   
362. association for computational linguistics, 1992.

[9] d. graff, z. wu, r. macintyre, and m. liberman,    the
1996 broadcast news speech and language-model cor-
pus   , in proceedings of the darpa workshop on spo-
ken language technology, pp. 11   14, 1997.

[10] j. j. godfrey, e. c. holliman, and j. mcdaniel,    switch-
board: telephone speech corpus for research and devel-
in proc. ieee icassp, vol. 1, pp. 517   520.
opment   ,
ieee, 1992.

[11] c. cieri, d. miller, and k. walker,    the fisher corpus:
a resource for the next generations of speech-to-text   , in
lrec, vol. 4, pp. 69   71, 2004.

[12] s. f. chen, b. kingsbury, l. mangu, d. povey, g. saon,
h. soltau, and g. zweig,    advances in speech transcrip-
tion at ibm under the darpa ears program   , ieee
trans. audio, speech, and language processing, vol.
14, pp. 1596   1608, 2006.

[13] s. matsoukas, j.-l. gauvain, g. adda, t. colthurst, c.-
l. kao, o. kimball, l. lamel, f. lefevre, j. z. ma,
j. makhoul, et al.,    advances in transcription of broad-
cast news and conversational telephone speech within
ieee transac-
the combined ears bbn/limsi system   ,
tions on audio, speech, and language processing, vol.
14, pp. 1541   1556, 2006.

[14] a. stolcke, b. chen, h. franco, v. r. r. gadde,
m. graciarena, m.-y. hwang, k. kirchhoff, a. man-
dal, n. morgan, x. lei, et al.,
   recent innovations
in speech-to-text transcription at sri-icsi-uw   , ieee
transactions on audio, speech, and language process-
ing, vol. 14, pp. 1729   1744, 2006.

[15] a. ljolje,    the at&t 2001 lvcsr system   , nist

lvcsr workshop, 2001.

[16] j.-l. gauvain, l. lamel, h. schwenk, g. adda,
l. chen, and f. lefevre,
   conversational telephone
id103   , in proc. ieee icassp, vol. 1, pp.
i   212. ieee, 2003.

[17] g. evermann, h. y. chan, m. j. f. gales, t. hain,
x. liu, d. mrva, l. wang, and p. c. woodland,    de-
velopment of the 2003 cu-htk conversational telephone
in proc. ieee icassp,
speech transcription system   ,
vol. 1, pp. i   249. ieee, 2004.

[18] f. seide, g. li, and d. yu,

   conversational speech
transcription using context-dependent deep neural net-
works   , in proc. interspeech, pp. 437   440, 2011.

[19] r. p. lippmann,    id103 by machines and
humans   , speech communication, vol. 22, pp. 1   15,
1997.

[20] w. xiong, j. droppo, x. huang, f. seide, m. seltzer,
a. stolcke, d. yu, and g. zweig,
   the mi-
crosoft 2016 conversational id103 sys-
tem   ,
preprint at
https://arxiv.org/abs/1609.03528.

submitted to icassp, 2017,

[21] f. j. pineda,    generalization of back-propagation to re-
current neural networks   , physical review letters, vol.
59, pp. 2229, 1987.

[22] r. j. williams and d. zipser,    a learning algorithm
for continually running fully recurrent neural networks   ,
neural computation, vol. 1, pp. 270   280, 1989.

[23] a. waibel, t. hanazawa, g. hinton, k. shikano, and
k. j. lang,    phoneme recognition using time-delay neu-
ral networks   , ieee trans. acoustics, speech, and sig-
nal processing, vol. 37, pp. 328   339, 1989.

[24] y. lecun and y. bengio,    convolutional networks for
images, speech, and time series   , the handbook of brain
theory and neural networks, vol. 3361, pp. 1995, 1995.

[25] y. lecun, b. boser, j. s. denker, d. henderson, r. e.
howard, w. hubbard, and l. d. jackel,    backpropaga-
tion applied to handwritten zip code recognition   , neu-
ral computation, vol. 1, pp. 541   551, 1989.

[26] t. robinson and f. fallside,    a recurrent error propa-
gation network id103 system   , computer
speech & language, vol. 5, pp. 259   274, 1991.

[27] s. hochreiter and j. schmidhuber,    long short-term
memory   , neural computation, vol. 9, pp. 1735   1780,
1997.

[28] h. sak, a. w. senior, and f. beaufays,    long short-
term memory recurrent neural network architectures for
large scale acoustic modeling   , in proc. interspeech, pp.
338   342, 2014.

[29] h. sak, a. senior, k. rao, and f. beaufays,    fast and
accurate recurrent neural network acoustic models for
in proc. interspeech, pp. 1468   
id103   ,
1472, 2015.

[30] g. saon, h.-k. j. kuo, s. rennie, and m. picheny,    the
telephone speech
in interspeech, pp. 3140   3144,

ibm 2015 english conversational
recognition system   ,
2015.

[31] t. sercu, c. puhrsch, b. kingsbury, and y. lecun,
   very deep multilingual convolutional neural networks
in proc. ieee icassp, pp. 4955   4959.
for lvcsr   ,
ieee, 2016.

[32] m. bi, y. qian, and k. yu,    very deep convolutional
neural networks for lvcsr   , in proc. interspeech, pp.
3259   3263, 2015.

[33] y. qian, m. bi, t. tan, and k. yu,    very deep convolu-
tional neural networks for noise robust speech recogni-
tion   , ieee/acm trans. audio, speech, and language
processing, vol. 24, pp. 2263   2276, aug. 2016.

[34] t. mikolov, m. kara     at, l. burget, j. cernock`y, and
s. khudanpur,    recurrent neural network based lan-
in proc. interspeech, pp. 1045   1048,
guage model   ,
2010.

[35] t. mikolov and g. zweig,    context dependent recurrent
neural network language model   , in proc. interspeech,
pp. 901   904, 2012.

[36] m. sundermeyer, r. schl  uter, and h. ney,    lstm neural
in interspeech, pp.

networks for id38.   ,
194   197, 2012.

[37] i. medennikov, a. prudnikov, and a. zatvornitskiy,
   improving english conversational telephone speech
recognition   , in proc. interspeech, pp. 2   6, 2016.

[38] t. mikolov, w.-t. yih, and g. zweig,    linguistic reg-
in

ularities in continuous space word representations   ,
hlt-naacl, vol. 13, pp. 746   751, 2013.

[39] i. sutskever, o. vinyals, and q. v. le,    sequence to
in advances
sequence learning with neural networks   ,
in neural information processing systems, pp. 3104   
3112, 2014.

[40] a. hannun, c. case, j. casper, b. catanzaro, g. di-
amos, e. elsen, r. prenger, s. satheesh, s. sengupta,
a. coates, et al.,    deep speech: scaling up end-to-end
id103   , arxiv preprint arxiv:1412.5567,
2014.

[41] m. l. glenn, s. strassel, h. lee, k. maeda, r. zakhary,
and x. li,    transcription methods for consistency, vol-
ume and ef   ciency   , in lrec, 2010.

[42] k. simonyan and a. zisserman,

   very deep convo-
lutional networks for large-scale image recognition   ,
arxiv preprint arxiv:1409.1556, 2014.

[43] k. he, x. zhang, s. ren, and j. sun,

ual learning for image recognition   ,
arxiv:1512.03385, 2015.

   deep resid-
arxiv preprint

[44] r. k. srivastava, k. greff, and j. schmidhuber,    high-

way networks   , corr, vol. abs/1505.00387, 2015.

[45] p. ghahremani, j. droppo, and m. l. seltzer,

early augmented deep neural network   ,
icassp, pp. 5085   5089. ieee, 2016.

   lin-
in proc. ieee

[46] d. yu, w. xiong, j. droppo, a. stolcke, g. ye, j. li, and
g. zweig,    deep convolutional neural networks with
in proc.
layer-wise context expansion and attention   ,
interspeech, pp. 17   21, 2016.

[47] a. waibel, h. sawai, and k. shikano,

   consonant
recognition by modular construction of large phonemic
in proc. ieee icassp,
time-delay neural networks   ,
pp. 112   115. ieee, 1989.

[48] a. graves and j. schmidhuber,    framewise phoneme
classi   cation with bidirectional lstm and other neural
network architectures   , neural networks, vol. 18, pp.
602   610, 2005.

[49] n. dehak, p. j. kenny, r. dehak, p. dumouchel, and
p. ouellet,    front-end factor analysis for speaker ver-
ieee trans. audio, speech, and language
i   cation   ,
processing, vol. 19, pp. 788   798, 2011.

[50] g. saon, h. soltau, d. nahamoo, and m. picheny,
   speaker adaptation of neural network acoustic models
using i-vectors   , in ieee id103 and un-
derstanding workshop, pp. 55   59, 2013.

[51] g. saon, t. sercu, s. j. rennie, and h. j. kuo,    the
telephone speech
in proc. interspeech, pp. 7   11,

ibm 2016 english conversational
recognition system   ,
sep. 2016.

[52] g. wang and k. sim,    sequential classi   cation crite-
ria for nns in automatic id103   , in proc.
interspeech, pp. 441   444, 2011.

[53] k. vesel`y, a. ghoshal, l. burget, and d. povey,
   sequence-discriminative training of deep neural net-
works   , in proc. interspeech, pp. 2345   2349, 2013.

[67] t. n. sainath, a.-r. mohamed, b. kingsbury, and
b. ramabhadran,    deep convolutional neural networks
in proc. ieee icassp, pp. 8614   8618.
for lvcsr   ,
ieee, 2013.

[68] o. abdel-hamid, a.-r. mohamed, h. jiang, and
g. penn,    applying convolutional neural networks con-
cepts to hybrid nn-id48 model for speech recogni-
in proc. ieee icassp, pp. 4277   4280. ieee,
tion   ,
2012.

[69] c. wu, p. karanasou, m. j. gales, and k. c. sim,    stim-
ulated deep neural network for id103   , in
proc. interspeech, pp. 400   404, 2016.

[54] d. povey, v. peddinti, d. galvez, p. ghahrmani,
v. manohar, x. na, y. wang, and s. khudanpur,    purely
sequence-trained neural networks for asr based on
lattice-free mmi   , in proc. interspeech, pp. 2751   2755,
2016.

[55] c. mendis, j. droppo, s. maleki, m. musuvathi,
   parallelizing wfst
in proc. ieee icassp, pp. 5325   

t. mytkowicz, and g. zweig,
speech decoders   ,
5329. ieee, 2016.

[56] a. stolcke,    srilm   an extensible id38
toolkit   , in proc. interspeech, vol. 2002, p. 2002, 2002.

[57] t. alum  ae and m. kurimo,

   ef   cient estimation of
maximum id178 language models with id165 fea-
tures: an srilm extension   , in proc. interspeech, pp.
1820   1823, 2012.

[58] x. chen, x. liu, y. qian, m. j. f. gales, and p. c.
woodland,    cued-id56lm: an open-source toolkit
for ef   cient training and evaluation of recurrent neural
network language models   , in proc. ieee icassp, pp.
6000   6004. ieee, 2016.

[59] m. gutmann and a. hyv  arinen,    noise-contrastive es-
timation: a new estimation principle for unnormalized
statistical models   , aistats, vol. 1, pp. 6, 2010.

[60] p.-s. huang, x. he, j. gao, l. deng, a. acero, and
l. heck,    learning deep structured semantic models
in proceed-
for web search using clickthrough data   ,
ings of the 22nd acm international conference on con-
ference on information & knowledge management, pp.
2333   2338. acm, 2013.

[61] o. press and l. wolf,

ding to improve language models   ,
arxiv:1608.05859, 2016.

   using the output embed-
arxiv preprint

[62] p. ghahremani and j. droppo,    self-stabilized deep neu-

ral network   , in proc. ieee icassp. ieee, 2016.

[63] a. stolcke et al.,    the sri march 2000 hub-5 conver-
in proceedings
sational speech transcription system   ,
nist speech transcription workshop, college park,
md, may 2000.

[64] microsoft research,    the microsoft cognition toolkit

(cntk)   , https://cntk.ai.

[65] d. yu et al.,    an introduction to computational networks
and the computational network toolkit   , technical
report msr-tr-2014-112, microsoft research, 2014,
https://github.com/microsoft/cntk.

[66] f. seide, h. fu, j. droppo, g. li, and d. yu,    1-bit
stochastic id119 and its application to data-
parallel distributed training of speech dnns   , in proc.
interspeech, pp. 1058   1062, 2014.

