id4 with recurrent attention modeling

zichao yang, zhiting hu, yuntian deng, chris dyer, alex smola

carnegie mellon university

{zichaoy,zhitingh,yuntiand,cdyer}@cs.cmu.edu

alex@smola.org

6
1
0
2

 
l
u
j
 

8
1

 
 
]
e
n
.
s
c
[
 
 

1
v
8
0
1
5
0

.

7
0
6
1
:
v
i
x
r
a

abstract

knowing which words have been attended
to in previous time steps while generating a
translation is a rich source of information for
predicting what words will be attended to in
the future. we improve upon the attention
model of bahdanau et al.
(2014) by explic-
itly modeling the relationship between previ-
ous and subsequent attention levels for each
word using one recurrent network per input
word. this architecture easily captures in-
formative features, such as fertility and reg-
ularities in relative distortion. in experiments,
we show our parameterization of attention im-
proves translation quality.

1

introduction

in contrast to earlier approaches to neural machine
translation (id4) that used a    xed vector represen-
tation of the input (sutskever et al., 2014; kalch-
brenner and blunsom, 2013), attention mechanisms
provide an evolving view of the input sentence as
the output is generated (bahdanau et al., 2014). al-
though attention is an intuitively appealing concept
and has been proven in practice, existing models
of attention use content-based addressing and have
made only limited use of the historical attention
masks. however, lessons from better word align-
ment priors in latent variable translation models sug-
gests value for modeling attention independent of
content.

a challenge in modeling dependencies between
previous and subsequent attention decisions is that
source sentences are of different lengths, so we

need models that can deal with variable numbers
of predictions across variable lengths. while other
work has sought to address this problem (cohn
et al., 2016; tu et al., 2016; feng et al., 2016),
these models either rely on explicitly engineered
features (cohn et al., 2016), resort to indirect mod-
eling of the previous attention decisions as by look-
ing at the content-based id56 states that gener-
ated them (tu et al., 2016), or only models cover-
age rather than coverage together with ordering pat-
terns (feng et al., 2016).
in contrast, we propose
to model the sequences of attention levels for each
word with an id56, looking at a    xed window of
previous alignment decisions. this enables us both
to learn long range information about coverage con-
straints, and to deal with the fact that input sentences
can be of varying sizes.

in this paper, we propose to explicitly model the
dependence between attentions among target words.
when generating a target word, we use a id56 to
summarize the attention history of each source word.
the resultant summary vector is concatenated with
the context vectors to provide a representation which
is able to capture the attention history. the atten-
tion of the current target word is determined based
on the concatenated representation. alternatively, in
the viewpoint of the memory networks framework
(sukhbaatar et al., 2015), our model can be seen
as augmenting the static encoding memory with dy-
namic memory which depends on preceding source
word attentions. our method improves over plain at-
tentive neural models, which is demonstrated on two
mt data sets.

2 model

2.1 id4
id4 directly models the condition id203
p(y|x) of target sequence y = {y1, ..., yt} given
source sequence x = {x1, ..., xs}, where xi, yj are
tokens in source sequence and target sequence re-
spectively. sutskever et al. (2014) and bahdanau
et al. (2014) are slightly different in choosing the
encoder and decoder network. here we choose the
id56search model from (bahdanau et al., 2014) as
our baseline model. we make several modi   cations
to the id56search model as we    nd empirically that
these modi   cation lead to better results.

2.1.1 encoder

we use bidirectional lstms to encode the source
sentences. given a source sentence {x1, ..., xs}, we
embed the words into vectors through an embedding
matrix ws, the vector of i-th word is wsxi. we get
the annotations of word i by summarizing the in-
formation of neighboring words using bidirectional
lstms:

      h i =               lstm(         hi   1, wsxi)
      h i =               lstm(         hi+1, wsxi)

(1)
(2)

the forward and backward annotation are concate-
nated to get the annotation of word i as hi =
[      h i,      h i]. all the annotations of the source words
form a context set, c = {h1, ..., hs}, conditioned
on which we generate the target sentence. c can
also be seen as memory vectors which encode all
the information from the source sequences.

2.1.2 attention based decoder

the decoder generates one target word per time
step, hence, we can decompose the conditional prob-
ability as

log p(y|x) =

p(yj|y<j, x).

(3)

(cid:88)

j

for each step in the decoding process, the lstm
updates the hidden states as

sj = lstm(sj   1, wt yj   1, cj   1).

(4)

the attention mechanism is used to select the most
relevant source context vector,

eij =vt

a tanh(wahi + uasj),
exp(eij)
i exp(eij)

,

(cid:80)
(cid:88)

  ijhi.

  ij =

cj =

(5)

(6)

(7)

i

this can also seen as memory addressing and read-
ing process. content based addressing is used to get
weights   ij. the decoder then reads the memory
as weighted average of the vectors. cj is combined
with sj to predict the j-th target word. in our im-
plementation we concatenate them and then use one
layer mlp to predict the target word:

  sj = tanh(w1[sj, cj] + b1)
pj =softmax(w2  sj)

(8)
(9)

we feed cj to the next step, this explains the cj   1

term in eq. 4.

the above attention mechanism follows that of
vinyals et al. (2015). similar approach has been
used in (luong et al., 2015). this is slightly differ-
ent from the attention mechanism used in (bahdanau
et al., 2014), we    nd empirically it works better.

one major limitation is that attention at each time
step is not directly dependent of each other. how-
ever, in machine translation, the next word to at-
tend to highly depends on previous steps, neigh-
boring words are more likely to be selected in next
time step. this above attention mechanism fails to
capture these important characteristics and encoding
this in the lstm can be expensive. in the follow-
ing, we attach a dynamic memory vector to the orig-
inal static memory hi, to keep track of how many
times this word has been attended to and whether
the neighboring words are selected at previous time
steps, the information, together with hi, is used to
predict the next word to select.

2.2 dynamic memory
for each source word i, we attach a dynamic mem-
ory vector di to keep track of history attention maps.
let     ij = [  i   k,j, ...  i+k,j] be a vector of length
2k + 1 that centers at position i, this vector keeps
track of the attention maps status around word i, the

    english-german the german-english data
set contains europarl, common crawl and
news commentary corpus. we remove the
sentence pairs that are not german or english
in a similar way as in (jean et al., 2014). there
are about 4.5 million sentence pairs after pre-
processing. we use newstest2013 set as valida-
tion and newstest2014, newstest2015 as test.
    chinese-english we
and
ldc2004t08 hong kong news data set
for chinese-english translation.
there are
about 1.5 million sentences pairs. we use mt
02, 03 as validation and mt 05 as test.

fibs

use

for both data sets, we tokenize the text with
tokenizer.perl. translation quality is eval-
uated in terms of tokenized id7 scores with
multi-id7.perl.

3.2 experiments con   guration
we exclude the sentences that are longer than 50
words in training. we set the vocabulary size to
be 50k and 30k for english-german and chinese-
english. the words that do not appear in the vocab-
ulary are replaced with unk.

for both id56search model and our model, we
set the id27 size and lstm dimension
size to be 1000, the dynamic memory vector dij size
is 500. following (sutskever et al., 2014), we ini-
tialize all parameters uniformly within range [-0.1,
0.1]. we use plain sgd to train the model and set the
batch size to be 128. we rescale the gradient when-
ever its norm is greater than 3. we use an initial
learning rate of 0.7. for english-german, we start
to halve the learning rate every epoch after training
for 8 epochs. we train the model for a total of 12
epochs. for chinese-english, we start to halve the
learning rate every two epochs after training for 10
epochs. we train the model for a total of 18 epochs.
to investigate the effect of window size 2k + 1,
we report results for k = 0, 5, i.e., windows of size
1, 11.

3.3 results
the results of english-german and chinese-english
are shown in table 2 and 3 respectively. we com-
pare our results with our own baseline and with re-
sults from related works if the experimental setting

figure 1: model diagram

dynamic memory dij is updated as follows:

dij = lstm(di,j   1,     i,j),   i

(10)

the model is shown in fig. 1. we call the vec-
tor dij dynamic memory because at each decoding
step, the memory is updated while hi is static. dij
is assumed to keep track of the history attention sta-
tus around word i. we concatenate the dij with hi
in the addressing and the attention weight vector is
calculated as:

a tanh(wa[hi, dij] + uasj),
exp(eij)
i exp(eij)

,

eij =vt

  ij =

cj =

(cid:80)
(cid:88)

  ijhi.

(11)

(12)

(13)

i

note that we only used dynamic memory dij in the
addressing process, the actual memory cj that we
read does not include dij. we also tried to get the dij
through a fully connected layer or a convolutional
layer. we    nd empirically lstm works best.

3 experiments & results

3.1 data sets
we experiment with two data sets: wmt english-
german and nist chinese-english.

 !hi !hi  hi  hidijdij      sjsjstaticmemorydynamicmemory+         i,j 1   i,j 1   i k,j 1   i k,j 1   i k,j 1   i k,j 1         i,j 2   i,j 2   i k,j 2   i k,j 2   i k,j 2   i k,j 2         i,1   i,1   i k,1   i k,1   i k,1   i k,1      attentionmatrixxi kxi kxixixi+kxi+kyj 1yj 1yj 2yj 2y1y1src
ref
baseline
our model

she was spotted three days later by a dog walker trapped in the quarry
drei tage sp  ater wurde sie von einem spazierg  anger im steinbruch in ihrer misslichen lage entdeckt
sie wurde drei tage sp  ater von einem hund entdeckt .
drei tage sp  ater wurde sie von einem hund im steinbruch gefangen entdeckt .

src

ref

baseline

our model

at the metropolitan transportation commission in the san francisco bay area , of   cials say congress could very simply deal with
the bankrupt highway trust fund by raising gas taxes .
bei der metropolitan transportation commission f  ur das gebiet der san francisco bay erkl  arten beamte , der kongress k  onne das
problem des bankrotten highway trust fund einfach durch erh  ohung der kraftstoffsteuer lsen .
bei der metropolitan im san francisco bay area sagen of   zielle vertreter des kongresses ganz einfach den konkurs highway durch
steuererh  ohungen .
bei der metropolitan transportation commission in san francisco bay area sagen beamte , dass der kongress durch steuer-
erh  ohungen ganz einfach mit dem konkurs highway trust fund umgehen k  onnte .

table 1: english-german translation examples

model
id56search
id56search + unk replace
id56search + window 1
id56search + window 11
id56search + window 11 +
unk replace

(jean et al., 2014)
id56search
id56search + unk replace
(luong et al., 2015)
four-layer lstm + attention
four-layer lstm + attention +
unk replace
id56search + character
(chung et al., 2016)
(costa-juss`a
and
2016)

fonollosa,

test1
19.0
21.6
18.9
19.5
22.1

16.5
19.0

19.0
20.9

test2
21.3
24.3
21.4
22.0
25.0

-
-

-
-

21.3

-

23.4
20.2

table 2: english-german results.

model
id56search
id56search + window 1
id56search + window 11
id56search + window 11 + unk replace
table 3: chinese-english results.

mt 05
27.3
27.9
28.8
29.3

are the same. from table 2, we can see that adding
dependency improves id56search model by 0.5 and
0.7 on newstest2014 and newstest2015, which we
denote as test1 and test2 respectively. using win-
dow size of 1, in which coverage property is con-
sidered, does not improve much. following (jean et
al., 2014; luong et al., 2014), we replace the unk
token with the most probable target words and get
id7 score of 22.1 and 25.0 on the two data sets
respectively. we compare our results with related
works, including (luong et al., 2015), which uses
four layer lstm and local attention mechanism,
and (costa-juss`a and fonollosa, 2016; chung et al.,
2016), which uses character based encoding, we can
see that our model outperform the best of them by
0.8 and 1.6 id7 score respectively. table 1 shows
some english-german translation examples. we can
see the model with dependent attention can pick the
right part to translate better and has better translation
quality.

the improvement is more obvious for chinese-
english. even only considering coverage property
improves by 0.6. using a window size of 11 im-
proves by 1.5. further using unk replacement trick
improves the id7 score by 0.5, this improvement
is not as signi   cant as english-german data set, this
is because english and german share lots of words
which chinese and english don   t.

4 conclusions & future work

in this paper, we proposed a new attention mecha-
nism that explicitly takes the attention history into
consideration when generating the attention map.
our work is motivated by the intuition that in atten-
tion based id4, the next word to attend is highly

to attention-based id4. arxiv
preprint arxiv:1508.04025.

[sukhbaatar et al.2015] sainbayar sukhbaatar, arthur
2015.
arxiv preprint

szlam, jason weston, and rob fergus.
end-to-end memory networks.
arxiv:1503.08895.

[sutskever et al.2014] ilya sutskever, oriol vinyals, and
quoc vv le. 2014. sequence to sequence learning
with neural networks. in advances in neural informa-
tion processing systems, pages 3104   3112.

[tu et al.2016] zhaopeng tu, zhengdong lu, yang liu,
xiaohua liu, and hang li. 2016. modeling cover-
age for id4. arxiv eprints, jan-
uary.

[vinyals et al.2015] oriol vinyals,   ukasz kaiser, terry
koo, slav petrov, ilya sutskever, and geoffrey hinton.
2015. grammar as a foreign language. in advances in
neural information processing systems, pages 2755   
2763.

dependent on the previous steps. we use a recurrent
neural network to summarize the preceding atten-
tions which could impact the attention of the current
decoding attention. the experiments on two mt
data sets show that our method outperforms previ-
ous independent attentive models. we also    nd that
using a larger context attention window would result
in a better performance.

for future directions of our work, from the static-
dynamic memory view, we would like to explore
extending the model to a fully dynamic memory
model where we directly update the representations
for source words using the attention history when we
generate each target word.

references
[bahdanau et al.2014] dzmitry bahdanau, kyunghyun
cho, and yoshua bengio.
2014. neural machine
translation by jointly learning to align and translate.
arxiv preprint arxiv:1409.0473.

[chung et al.2016] junyoung chung, kyunghyun cho,
and yoshua bengio.
2016. a character-level de-
coder without explicit segmentation for neural ma-
chine translation. arxiv preprint arxiv:1603.06147.

[cohn et al.2016] trevor cohn, cong duy vu hoang,
ekaterina vymolova, kaisheng yao, chris dyer, and
gholamreza haffari. 2016.
incorporating structural
alignment biases into an attentional neural translation
model. arxiv preprint arxiv:1601.01085.

[costa-juss`a and fonollosa2016] marta r costa-juss`a
character-
arxiv preprint

and jos  e ar fonollosa.
based id4.
arxiv:1603.00810.

2016.

[feng et al.2016] shi feng, shujie liu, mu li, and ming
zhou. 2016.
implicit distortion and fertility mod-
els for attention-based encoder-decoder id4 model.
corr, abs/1601.03317.
[jean et al.2014] s  ebastien

jean, kyunghyun cho,
roland memisevic, and yoshua bengio. 2014. on
using very large target vocabulary for neural machine
translation. corr, abs/1412.2007.

[kalchbrenner and blunsom2013] nal kalchbrenner and
phil blunsom. 2013. recurrent continuous translation
models. in proc. emnlp.

[luong et al.2014] minh-thang luong, ilya sutskever,
quoc v le, oriol vinyals, and wojciech zaremba.
2014. addressing the rare word problem in neural ma-
chine translation. arxiv preprint arxiv:1410.8206.

[luong et al.2015] minh-thang luong, hieu pham, and
christopher d manning. 2015. effective approaches

