5
1
0
2

 

v
o
n
5

 

 
 
]
e
n
.
s
c
[
 
 

3
v
5
6
5
8
0

.

0
1
5
1
:
v
i
x
r
a

attention with intention for a neural network

conversation model

kaisheng yao    
microsoft research

kaisheny@microsoft.com

geoffrey zweig

microsoft research

gzweig@microsoft.com

baolin peng

chinese university of hong kong
blpeng@se.cuhk.edu.hk

abstract

in a conversation or a dialogue process, attention and intention play intrinsic roles.
this paper proposes a neural network based approach that models the attention
and intention processes. it essentially consists of three recurrent networks. the
encoder network is a word-level model representing source side sentences. the
intention network is a recurrent network that models the dynamics of the intention
process. the decoder network is a recurrent network that produces responses
to the input from the source side. it is a language model that is dependent on
the intention and has an attention mechanism to attend to particular source side
words, when predicting a symbol in the response. the model is trained end-to-
end without labeling data. experiments show that this model generates natural
responses to user inputs.

1

introduction

a conversation process is a process of communication of thoughts through words. it may be con-
sidered as a structural process that stresses the role of purpose and processing in discourse [7].
essentially, the discourse structure is intimately connected with two nonlinguistic notions: intention
and attention. in processing an utterance, attention explicates the processing of utterances, for ex-
ample, paying attention to particular words in a sentence. on the other hand, intention is higher level
than attention and has its primary role of explaining discourse structure and coherence. clearly, a
conversation process is inherently complicated because of the two levels of structures.
a conversation process may be cast as a sequence-to-sequence mapping task.
in this task, the
source side of the conversation is from one person and the target side of the conversation is from
another person. the sequence-to-sequence mapping task includes machine translation, grapheme-
to-phoneme conversion, named entity tagging, etc. however, an apparent difference of a dialogue
process from these tasks is that a dialogue process involves multiple turns, whereas usually the above
tasks involve only one turn of mapping a source sequence to its target sequence.
neural network based approaches have been successfully applied in sequence-to-sequence mapping
tasks. they have made signi   cant progresses in machine translation [1,6,13], language understand-
ing [8], and id103 [4]. among those neural network-based approaches, one particular
approach, which is called encoder-decoder framework [1, 13], aims at relaxing much requirement
on human labeling.
conversation models have been typically designed to be domain speci   c with much knowledge such
as rules [3,18]. recent methods [15] relax such requirement to some extent but their whole systems
   presented at nips workshop on machine learning for spoken language understanding and interaction

2015.

1

table 1: an example of dialogue process.

user my computer is infected
agent
user
agent
user
agent
user
agent

do you want to retrieve the    les that was deleted?
the ones that the virus deleted , yes.
i can help you resolve the issue with our virus removal and protection service
ok
here is a link how to run system restore
thank you .
you welcome

are still trained with manual labels because of their sub-components that require so. manual labels
are error prone and expensive. therefore, it is appealing to train a system end-to-end without manual
labels. recent works in [10, 12, 14] are in this approach.
in general, however, using knowledge is helpful. for example, the alignment information between
the source and target side is critical in grapheme-to-phoneme conversion [17] to outperform a strong
baseline using id165 models [2]. in a neural network based machine translation system [6], the
alignment information is used to outperform a strong phrase-based baseline [5].
in the context of modeling conversation process, a neural network model may be built with the
knowledge of the structural information of conversation processes. in particular, the network may
incorporate the notion of intention and attention. to test this, we developed a model that consists
of three recurrent neural networks (id56s). the source side id56, or encoder network, encodes the
source side inputs. the target side id56, or decoder network, uses an attention mechanism to attend
to particular words in the source side, when predicting a symbol in its response to the source side.
importantly, this attention in the target side is conditioned on the output from an intention id56.
this model, which has the structural knowledge of the conversation process, is trained end-to-end
without labels. we experimented with this model and observed that it generates natural responses to
user inputs.

2 background

in the theory of discourse in [7], discourse structure is composed of three separate but related com-
ponents. the    rst is the linguistic structure, which is the structure of the sequence of utterance.
the linguistic structure consists of segments of the discourse into which the utterances naturally
aggregate. the second structure is the intentional structure, which captures the discourse-relevant
purposes, expressed in each of the linguistic segments as well as relationships among them. the
third is the attentional state that is dynamic, and records the objects, properties, and relations that
are salient at each point of the discourse.
in many examples we observe, there are usually just one linguistic segment that consists of all the
utterances. therefore, in the following, we consider a discourse with two structures: intention and
attention.
in the example in table 1, there is a clear    ow of intentions. the user states the problem, with the
user   s intention of conveying the problem to the agent. the agent receives the words, processes
them, and communicates back to the user. the user responds to the agent afterwards. therefore,
the whole conversation process consists of three intentions processed sequentially. the    rst is the
intention of communication of the problem. the second intention is the process of resolving the
issue. the third is the intention of acknowledgment. in processing each of the intentions, the user
and the agent pay attention to particular words. for example, when resolving the issue, the agent
pays attention to words such as    virus   .

2

figure 1: the attention with intention (awi) model. the model is unrolled into three turns. in
each turn has id56s for encoder network and decoder network. each session is represented by a
   xed-dimension vector, which is a hidden state of an intention id56 network.

3 the model

3.1 the attention with intention (awi) model

we propose a model that attempts to represent the structural process of intentions and the associated
attentions. figure 1 illustrates the model. it shows three layers of processing: encoder network,
intention network, and decoder network.
the encoder network has inputs from the current source side input. because the source side in the
current turn is also dependent on the previous turn, the source side encoder network is linked with
the output from the previous target side. the encoder network creates a representation of the source
side in the current turn.
the intention network is dependent on its past state, so that it memories the history of intentions.
it therefore is a recurrent network, taking a representation of the source side in the current turn and
updating its hidden state.
the decoder is a recurrent network for id38 that outputs symbol at each time. this
output is dependent on the current intention from the intention network. it also pays attention to
particular words in the source side.
in more details, a conversation has in totoal u turns. at turn u, a user in the source side, denoted in
: t = 1,       , t ) with length t . an agent in
superscript (s), has an input sequence of (cid:126)x(s,u) = (x(s,u)
: j = 1,       j) with
the target side, denoted in superscript (t), responds to the user with (cid:126)y(t,u) = (y(t,u)
length j. the proposed model is a conditional model of the target given the source, p((cid:126)y(t,u)|(cid:126)x(s,u)).
if there is no confusion, we may omit the session index u in the following.

t

j

3.2 encoder network

the encoder network reads the input sentence (cid:126)x(s), and converts them into a    xed-length or a variable
length representation of the source side sequence. there are many choices to encode the source side.
the approach we use is an id56 such that
h(s)
t = f

x(s)
t

(cid:16)

(cid:17)

(1)

, h(s)
t-1

t

is the hidden state at time t in the source side. the initial state h(s)
, of the decoder network in the previous turn k     1.

where f (  ) is an id56. h(s)
t = 0 is the last hidden state activity, h(t,k-1)
one form of the output from this encoder is the last hidden state activity c(s)
t . this is used
as a representation of the source side in the current turn to the intention network. the other form is
a variable-length representation, to be used in the attention model described in sec. 3.4. a general
description of the variable length representation is as follows

t = h(s)

t with

t

t = q({h(s)
c(s)

t

,   t = 0,       t})

(2)

3

where q(  ) might be a linear network or a nonlinear network.

3.3

intention network

the signal from the encoder network is fed into an intention network to model the intention process.
following [7], the intention process is a dynamic process to model the intrinsic dynamics of con-
versation, in which an intention in one turn is dependent on the intention in the previous turn. this
property might be modeled using a markov model, but we choose an id56.
interestingly, the hidden state of an id56 in a certain turn may be considered as a distributed repre-
sentation of the intention. different from the usual process of training distributed representation of
words [9], the distribution representation of intentions are trained with previous turns as their con-
text. we use a    rst order id56 model, in which a hidden state is dependent explicitly on its previous
state.
the intention model in awi is therefore an id56 as follows

(cid:16)

(cid:17)

h(i,k) = f

c(s)
t , h(i,k-1), h(t,k-1)

t

where c(s)
index of the current turn. h(t,k-1)
previous turn k     1.

t

t is the    xed dimension representation of the source side described in sec. 3.2. k is the
is the last hidden layer activity of the decoder network in the

3.4 decoder network

the last step is to decode the sequence in the target side, which is framed as a language model
over each symbol, generated left to right.
in this framework, the decoder computes conditional
id203 as

j-1 , h(t)
j
where the hidden state in the decoder is computed using an id56

j-1 , (cid:126)x(s)) = g(y(t)

p(y(t)
j

, c(t)
j )

1 ,       , y(t)
|y(t)
(cid:16)

h(t)
j = f

(cid:17)

y(t)
j-1 , h(t)

j-1, c(t)
j

the initial state h(t)
j

at t = 0 is the last hidden state activity from the intention network.

(cid:16)

:    t = {1,       , t}}(cid:17)

the c(t)
j

is a vector to represent the context to generate y(t)
j

. it is dependent on the source side as

j-1,{c(s)
h(t)

t

c(t)
j

= z

(6)
t } using weighted aver-
where z(  ) summerizes the variable-length source side representations {c(s)
age. the weight is computed using a content-based alignment model [1] that produces high scores
if the target side hidden state in previous time h(t)
are similar. more formally, the weight
  jt for the context c(s)
t

is computed using

j-1 and c(s)
t

(cid:80)

  jt =

exp ejt
m exp(ejm)

where

ejt = a(h(t)

j-1, c(s)
t )

(3)

(4)

(5)

(7)

(8)

the alignment model enables an attention to particular words, represented as a vector c(s)
in the
t
source side. since the decoder network generates responses on condition of the attention and also
the intention, our model is called attention with intention (awi) model.

3.5

implementation details

all of the recurrent networks are implemented using a recently proposed depth-gated long-short-
term memory (lstm) network [16]. the context vector c(s)
is an embedding vector of the source
t
side word at time t.

4

table 2: perplexity results with awi model. models have 50 or 200 hidden layer dimension, 25
alignment dimension, and one layer of lstms.

# hidden dimension

50
200

ppl
30.8
22.1

the alignment model in eq. (8) follows the attention model in [1], in which ejt is calculated as

(cid:16)

(cid:17)

ejt = (cid:126)v(cid:62) tanh

w(ah)h(t)

j-1 + w(ae)c(s)
t

,

(9)

which is a neural network with one hidden layer of size a and a single output, parameterised by
w(ae)     ra  h, w(ah)     ra  h and (cid:126)v     ra. h and a are the hidden layer dimension and
alignment dimension.

4 evaluation

we used an in-house dialogue dataset. the dataset consists of dialogues from a helpdesk chat ser-
vice. in this service, costumers seeks helps on computer related issues from human agents. training
consists of 10000 dialogues with 96913 turns or conversations. number of tokens is 2215047 in the
source side and 2378950 in the target side. the vocabulary size is 9085 including words from both
side. development set data has 1000 dialogues with 9971 turns. test set data has 500 dialogues with
5232 turns.
we use sentence-level sgd without momentum. learning rate is initialized to 0.1. development set
is used to control the learning rate. the learning rate is halved when perplexity on the development
is increased. one epoch of training has one pass of the training data. the order of training dialogues
is randomly shuf   ed in the beginning of each epoch. the order of turns in the dialogue is however
kept.

4.1 performances measure in perplexity

an objective comparison of different models for conversation is still an open question. we report
perplexity (ppl), though it may have drawbacks, to compare different models. table 2 presents
results in perplexity with two models with different hiden layer sizes. results show that a larger
model with 200 hidden layer dimension has lower ppl than the model with 50 dimension.

4.2 examples of outputs from the trained model

table 3 lists an example of the conversation process between a human and the trained model. the
model has two layers of lstms and other setups are the same as used in sec 4.1. similarly as
observed in [11], the model produces natural responses to user inputs. the    ow of intentions is
clearly seen in this example.

5 related work

our work is related to the recent works in [10, 12, 14], which use an encoder-decoder framework
to model conversation. the work in [10] is a model for single turn conversation. the work in [14]
is a simple encoder-decoder method using a    xed-dimension representation of the source side. the
work in [12] also uses a    xed-dimension representaiton of the source side but has an additional id56
to model dialogue context. this additional id56 is similar to the intention id56 in awi model.
however, awi model differs from [12] in that it incorprates the concept of attention and intention
based on the theory in [7]. therefore, attention mechanism is essential to awi. the model in [12]
doesn   t have an attention model.

5

table 3: the generated response from the trained model.

hi
how may i help you today ?

oh ! i sorry to hear that . are you getting an error message ?
yes

i updated my computer
i see . is this on windows 7 ?
no it is on linux
ok . are you using the affected computer right now ?
no
i can de   nitely assist you on your issue .
thanks

user
agent
user my computer is not responding
agent
user
agent what recent changes did you made with this computer ?
user
agent
user
agent
user
agent
user
agent we can go ahead and start working on this issue within our of   ce premium software support service .
user
agent
agent
user
agent
user
agent
user
agent
user
agent
user
agent
user
agent
user
agent
user
agent
agent
agent
user
agent
user
agent
user
agent

that is allright
if in case the issue comes back at all within the next 30 days we will perform the service again for free .
let me go ahead and prepare the offer and let start working on this issue now , alright ?
please
sure . may i have your phone number please .
xxx-xxx-xxxx
thank you .
no problem
thank you for waiting .
no problem
i see .
what is the next step
i see . let me go ahead and prepare the offers for you .
okay
thank you .
no problem
thank you for contacting answer desk . have a great day !
bye
thanks again for working with me . id like to send you a quick survey .
it starts with a yes/no question , with a few more questions about your experience with me today.
the survey takes about a minute and really helps us improve our service . can i send it to you now ?
sure
thank you .
no problem
thank you for contacting answer desk . have a great day !
bye
goodbye .

because it is not yet clear what objective measure to use to compare different models, it is hard to
make claims of superiority of these models. we believe awi model is an alternative to the models
in [12, 14].

6 conclusions and discussions

we have presented a model that incorporates attention and intention processes in a neural network
model. preliminary experiments show that this model generates natural responses to user inputs. fu-
ture works include experiments on common dataset to compare different models and incorporating
objective functions such as goals.

6

references
[1] d. bahdanau, k. cho, and y. bengio. id4 by jointly learning to align
in proceedings of the international conference on learning representations

and translate.
(iclr), san diego, ca, 2015.

[2] m. bisani and h. ney. joint-sequence models for grapheme-to-phoneme conversion. speech

communication, 50(5), 2008.

[3] d. bohus and a. i. rudnicky. the ravenclaw dialog management framework: architecture and

systems. computer, speech and language, 23:332   361, 2009.

[4] w. chan, n. jaitly, q. v. le, and o. vinyals. listen, attend and spell. in arxiv:1508.01211

[cs.cl], 2015.

[5] d. chiang. hierarchical phrase-based translation. computational linguistics, 33(2):201   228,

1999.

[6] j. devlin, r. zbib, z. huang, t. lamar, r. schwartz, and j. makhoul. fast and robust neural

network joint models for id151. in acl, 2014.

[7] b. j. grosz and c. l. sidner. attention, intentions, and the structure of discourse. computa-

tional linguistics, 12:175   204, 1986.

[8] g. mesnil, y. dauphin, k. yao, y. bengio, l. deng, d. hakkani-tur, x. he, l. heck, g. tur,
d. yu, and g. zweig. using recurrent neural networks for slot    lling in spoken language
understanding. ieee/acm transactions on audio, speech, and language processing, 2015.
[9] t. mikolov, k. chen, g. corrado, and j. dean. effcient estimation of word representations in

vector space. in nips, 2013.

[10] l. shang, z. lu, and h. li. neural responding machine for short-text conversation. in acl,

2015.

[11] a. sordoni, y. bengio, h. vahabi, c. lioma, j. g. simonsen, and j.-y. nie. a hierarchical
recurrent encoder-decoder for generative context-aware query suggestion. in arxiv:1507   0222
[cd.ne], july 2015.

[12] a. sordoni, m. galley, m. auli, c. brockett, y. ji, m. mitchell, j.-y. nie, j. gao, and b. dolan.
in

a neural network approach to context-sensitive generation of conversation responses.
naacl, 2015.

[13] i. sutskever, o. vinyals, and q. v. le. sequence to sequence learning with neural networks.

in neural information processing systems (nips), pages 3104   3112, montr  eal, 2014.

[14] o. vinyals and q. v. le. a nerual converstion model. in icml deep learning workshop,

2015.

[15] t.-h. wen, m. gasic, d. kim, n. mrksic, p.-h. su, d. vandyke, and s. young. stochastic
language generation in dialogue using recurrent neural networks with convolutional sentence
reranking. technical report, may 2015.

[16] k. yao, t. cohn, e. vylomova, k. duh, and c. dyer. depth-gated lstm. in arxiv:1508.03790

[cs.ne], 2015.

[17] k. yao and g. zweig. sequence-to-sequence neural net models for grapheme-to-phoneme

conversion. in interspeech, 2015.

[18] s. young, m. gasic, b. thomson, and j. d. williams. pomdp-based statistical spoken dialog

systems: a review. proceedings of the ieee, 101:1160   1179, 2013.

7

