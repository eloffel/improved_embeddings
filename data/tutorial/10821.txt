6
1
0
2

 

v
o
n
1
2

 

 
 
]
l
c
.
s
c
[
 
 

1
v
9
3
6
6
0

.

1
1
6
1
:
v
i
x
r
a

text classi   cation improved by integrating bidirectional lstm

with two-dimensional max pooling

peng zhou1, zhenyu qi1    , suncong zheng1, jiaming xu1, hongyun bao1, bo xu1,2

(1) institute of automation, chinese academy of sciences, china

(2) center for excellence in brain science and intelligence technology, china

{zhoupeng2013, zhenyu.qi, zhengsuncong,
jiaming.xu, hongyun.bao, xubo}@ia.ac.cn

abstract

recurrent neural network (id56) is one of the most popular architectures used in natural lan-
guage processsing (nlp) tasks because its recurrent structure is very suitable to process variable-
length text. id56 can utilize distributed representations of words by    rst converting the tokens
comprising each text into vectors, which form a matrix. and this matrix includes two dimen-
sions: the time-step dimension and the feature vector dimension. then most existing models
usually utilize one-dimensional (1d) max pooling operation or attention-based operation only
on the time-step dimension to obtain a    xed-length vector. however, the features on the feature
vector dimension are not mutually independent, and simply applying 1d pooling operation over
the time-step dimension independently may destroy the structure of the feature representation.
on the other hand, applying two-dimensional (2d) pooling operation over the two dimensions
may sample more meaningful features for sequence modeling tasks. to integrate the features on
both dimensions of the matrix, this paper explores applying 2d max pooling operation to obtain
a    xed-length representation of the text. this paper also utilizes 2d convolution to sample more
meaningful information of the matrix. experiments are conducted on six text classi   cation tasks,
including id31, question classi   cation, subjectivity classi   cation and newsgroup
classi   cation. compared with the state-of-the-art models, the proposed models achieve excellent
performance on 4 out of 6 tasks. speci   cally, one of the proposed models achieves highest accu-
racy on stanford sentiment treebank binary classi   cation and    ne-grained classi   cation tasks.

1 introduction

text classi   cation is an essential component in many nlp applications, such as id31
(socher et al., 2013), id36 (zeng et al., 2014) and spam detection (wang, 2010). there-
fore, it has attracted considerable attention from many researchers, and various types of models have
been proposed. as a traditional method, the bag-of-words (bow) model treats texts as unordered sets
of words (wang and manning, 2012). in this way, however, it fails to encode word order and syntactic
feature.

recently, order-sensitive models based on neural networks have achieved tremendous success for
text classi   cation, and shown more signi   cant progress compared with bow models. the challenge
for textual modeling is how to capture features for different text units, such as phrases, sentences and
documents. bene   ting from its recurrent structure, id56, as an alternative type of neural networks, is
very suitable to process the variable-length text.

id56 can capitalize on distributed representations of words by    rst converting the tokens compris-
ing each text into vectors, which form a matrix. this matrix includes two dimensions: the time-step
dimension and the feature vector dimension, and it will be updated in the process of learning feature
representation. then id56 utilizes 1d max pooling operation (lai et al., 2015) or attention-based oper-
ation (zhou et al., 2016), which extracts maximum values or generates a weighted representation over

   correspondence author: zhenyu.qi@ia.ac.cn
this work is licenced under a creative commons attribution 4.0 international licence.

licence details:

http://creativecommons.org/licenses/by/4.0/

the time-step dimension of the matrix, to obtain a    xed-length vector. both of the two operators ignore
features on the feature vector dimension, which maybe important for sentence representation, therefore
the use of 1d max pooling and attention-based operators may pose a serious limitation.

convolutional neural networks (id98) (kalchbrenner et al., 2014; kim, 2014) utilizes 1d convolu-
tion to perform the feature mapping, and then applies 1d max pooling operation over the time-step
dimension to obtain a    xed-length output. however the elements in the matrix learned by id56 are not
independent, as id56 reads a sentence word by word, one can effectively treat the matrix as an    image   .
unlike in nlp, id98 in image processing tasks (lecun et al., 1998; krizhevsky et al., 2012) applies 2d
convolution and 2d pooling operation to get a representation of the input. it is a good choice to utilize
2d convolution and 2d pooling to sample more meaningful features on both the time-step dimension
and the feature vector dimension for text classi   cation.

above all,

this paper proposes bidirectional id137 with two-
dimensional max pooling (blstm-2dpooling) to capture features on both the time-step dimension
and the feature vector dimension.
it    rst utilizes bidirectional id137
(blstm) to transform the text into vectors. and then 2d max pooling operation is utilized to obtain a
   xed-length vector. this paper also applies 2d convolution (blstm-2did98) to capture more mean-
ingful features to represent the input text.

the contributions of this paper can be summarized as follows:

    this paper proposes a combined framework, which utilizes blstm to capture long-term sentence
dependencies, and extracts features by 2d convolution and 2d max pooling operation for sequence
modeling tasks. to the best of our knowledge, this work is the    rst example of using 2d convolution
and 2d max pooling operation in nlp tasks.

    this work introduces two combined models blstm-2dpooling and blstm-2did98, and veri-
   es them on six text classi   cation tasks, including id31, question classi   cation, sub-
jectivity classi   cation, and newsgroups classi   cation. compared with the state-of-the-art models,
blstm-2did98 achieves excellent performance on 4 out of 6 tasks. speci   cally, it achieves high-
est accuracy on stanford sentiment treebank binary classi   cation and    ne-grained classi   cation
tasks.

    to better understand the effect of 2d convolution and 2d max pooling operation, this paper conducts
experiments on stanford sentiment treebank    ne-grained task. it    rst depicts the performance of
the proposed models on different length of sentences, and then conducts a sensitivity analysis of 2d
   lter and max pooling size.

the remainder of the paper is organized as follows. in section 2, the related work about text classi   -
cation is reviewed. section 3 presents the blstm-2did98 architectures for text classi   cation in detail.
section 4 describes details about the setup of the experiments. section 5 presents the experimental re-
sults. the conclusion is drawn in the section 6.

2 related work

deep learning based neural network models have achieved great improvement on text classi   cation tasks.
these models generally consist of a projection layer that maps words of text to vectors. and then
combine the vectors with different neural networks to make a    xed-length representation. according
to the structure, they may divide into four categories: id56s (reid981), id56,
id98 and other neural networks.

id56s: reid98 is de   ned over recursive tree structures. in the type of recursive
models, information from the leaf nodes of a tree and its internal nodes are combined in a bottom-up man-
ner. socher et al. (2013) introduced recursive neural tensor network to build representations of phrases
and sentences by combining neighbour constituents based on the parsing tree. irsoy and cardie (2014)

1to avoid confusion with id56, we named id56s as reid98.

proposed deep id56, which is constructed by stacking multiple recursive layers on
top of each other, to modeling sentence.

recurrent neural networks: id56 has obtained much attention because of their superior ability
to preserve sequence information over time. tang et al. (2015) developed target dependent long short-
term memory networks (lstm (hochreiter and schmidhuber, 1997)), where target information is auto-
matically taken into account. tai et al. (2015) generalized lstm to tree-lstm where each lstm unit
gains information from its children units. zhou et al. (2016) introduced blstm with attention mech-
anism to automatically select features that have a decisive effect on classi   cation. yang et al. (2016)
introduced a hierarchical network with two levels of attention mechanisms, which are word attention and
sentence attention, for document classi   cation. this paper also implements an attention-based model
blstm-att like the model in zhou et al. (2016).

convolution neural networks: id98 (lecun et al., 1998) is a feedforward neural network with 2d
convolution layers and 2d pooling layers, originally developed for image processing. then id98 is ap-
plied to nlp tasks, such as sentence classi   cation (kalchbrenner et al., 2014; kim, 2014), and relation
classi   cation (zeng et al., 2014). the difference is that the common id98 in nlp tasks is made up of
1d convolution layers and 1d pooling layers. kim (2014) de   ned a id98 architecture with two chan-
nels. kalchbrenner et al. (2014) proposed a dynamic k-max pooling mechanism for sentence modeling.
(zhang and wallace, 2015) conducted a sensitivity analysis of one-layer id98 to explore the effect of
architecture components on model performance. yin and sch  utze (2016) introduced multichannel em-
beddings and unsupervised pretraining to improve classi   cation accuracy. (zhang and wallace, 2015)
conducted a sensitivity analysis of one-layer id98 to explore the effect of architecture components on
model performance.

usually there is a misunderstanding that 1d convolutional    lter in nlp tasks has one dimension.
actually it has two dimensions (k, d), where k, d     r. as d is equal to the id27s size dw,
the window slides only on the time-step dimension, so the convolution is usually called 1d convolution.
while d in this paper varies from 2 to dw, to avoid confusion with common id98, the convolution in this
work is named as 2d convolution. the details will be described in section 3.2.

other neural networks: in addition to the models described above, lots of other neural networks
have been proposed for text classi   cation.
iyyer et al. (2015) introduced a deep averaging network,
which fed an unweighted average of id27s through multiple hidden layers before classi   -
cation. zhou et al. (2015) used id98 to extract a sequence of higher-level phrase representations, then
the representations were fed into a lstm to obtain the sentence representation.

the proposed model blstm-2did98 is most relevant to dsid98 (zhang et al., 2016) and rid98
(wen et al., 2016). the difference is that the former two utilize lstm, bidirectional id56 respectively,
while this work applies blstm, to capture long-term sentence dependencies. after that the former two
both apply 1d convolution and 1d max pooling operation, while this paper uses 2d convolution and 2d
max pooling operation, to obtain the whole sentence representation.

3 model

as shown in figure 1, the overall model consists of four parts: blstm layer, two-dimensional con-
volution layer, two dimensional max pooling layer, and output layer. the details of different compo-
nents are described in the following sections.

3.1 blstm layer

lstm was    rstly proposed by hochreiter and schmidhuber (1997) to overcome the gradient vanishing
problem of id56. the main idea is to introduce an adaptive gating mechanism, which decides the degree
to keep the previous state and memorize the extracted features of the current data input. given a sequence
s = {x1, x2, . . . , xl}, where l is the length of input text, lstm processes it word by word. at time-step
t, the memory ct and the hidden state ht are updated with the following equations:

blstm layer

left context id27

right context

...

two-dimensional 
convolution layer

two-dimensional 
max pooling layer

output 
layer

figure 1: a blstm-2did98 for the seven word input sentence. id27s have size 3, and
blstm has 5 hidden units. the height and width of convolution    lters and max pooling operations are
2, 2 respectively.

   
it
ft
         
ot
  ct

   
         

=

   
         

  
  
  

tanh

   
         

w    [ht   1, xt]

ct = ft     ct   1 + it       ct
ht = ot     tanh(ct)

(1)

(2)
(3)

where xt is the input at the current time-step, i, f and o is the input gate activation, forget gate activation
and output gate activation respectively,   c is the current cell state,    denotes the logistic sigmoid function
and     denotes element-wise multiplication.

for the sequence modeling tasks, it is bene   cial to have access to the past context as well as the
future context. schuster and paliwal (1997) proposed blstm to extend the unidirectional lstm by
introducing a second hidden layer, where the hidden to hidden connections    ow in opposite temporal
order. therefore, the model is able to exploit information from both the past and the future.

in this paper, blstm is utilized to capture the past and the future information. as shown in figure
1, the network contains two sub-networks for the forward and backward sequence context respectively.
the output of the ith word is shown in the following equation:

hi = [

      
hi    

      
hi ]

(4)

here, element-wise sum is used to combine the forward and backward pass outputs.

3.2 convolutional neural networks
since blstm has access to the future context as well as the past context, hi is related to all the other
words in the text. one can effectively treat the matrix, which consists of feature vectors, as an    image   , so
2d convolution and 2d max pooling operation can be utilized to capture more meaningful information.

3.2.1 two-dimensional convolution layer
a matrix h = {h1, h2, . . . , hl}, h     rl  dw, is obtained from blstm layer, where dw is the size
of id27s. then narrow convolution is utilized (kalchbrenner et al., 2014) to extract local
features over h. a convolution operation involves a 2d    lter m     rk  d, which is applied to a window
of k words and d feature vectors. for example, a feature oi,j is generated from a window of vectors
hi:i+k   1, j:j+d   1 by

oi,j = f (m    hi:i+k   1, j:j+d   1 + b)

(5)

where i ranges from 1 to (l     k + 1), j ranges from 1 to (dw     d + 1),    represents dot product, b     r
is a bias and an f is a non-linear function such as the hyperbolic tangent. this    lter is applied to each
possible window of the matrix h to produce a feature map o:

o = [o1,1, o1,2,          , ol   k+1,dw   d+1]

(6)

with o     r(l   k+1)  (dw
   d+1). it has described the process of one convolution    lter. the convolution
layer may have multiple    lters for the same size    lter to learn complementary features, or multiple kinds
of    lter with different size.

3.2.2 two-dimensional max pooling layer
then 2d max pooling operation is utilized to obtain a    xed length vector. for a 2d max pooling p    
rp1  p2, it is applied to each possible window of matrix o to extract the maximum value:

pi,j = down(oi:i+p1, j:j+p2)

(7)

where down(  ) represents the 2d max pooling function, i = (1, 1 + p1,          , 1 + (l     k + 1/p1     1)    p1),
and j = (1, 1 + p2,          , 1 + (dw     d + 1/p2     1)    p2). then the pooling results are combined as follows:

h    = [p1,1, p1,1+p2,          , p1+(l   k+1/p1   1)  p1,1+(dw    d+1/p2   1)  p2]

(8)

where h        r, and the length of h    is    l     k + 1/p1          dw     d + 1/p2   .

3.3 output layer

for text classi   cation, the output h    of 2d max pooling layer is the whole representation of the input
text s. and then it is passed to a softmax classi   er layer to predict the semantic relation label   y from a
discrete set of classes y . the classi   er takes the hidden state h    as input:

  p (y|s) = sof tmax(cid:16)w (s)h    + b(s)(cid:17)

  y = arg max

y

  p (y|s)

(9)

(10)

a reasonable training objective to be minimized is the categorical cross-id178 loss. the loss is

calculated as a regularized sum:

j (  ) =    

1
m

m

x

i=1

ti log(yi) +   k  k2
f

(11)

where t     rm is the one-hot represented ground truth, y     rm is the estimated id203 for each class
by softmax, m is the number of target classes, and    is an l2 id173 hyper-parameter. training
is done through stochastic id119 over shuf   ed mini-batches with the adadelta (zeiler, 2012)
update rule. training details are further introduced in section 4.3.

data
c
5
sst-1
2
sst-2
2
subj
trec 6
2
mr
20ng
4

l
18
19
23
10
21
276

m
51
51
65
33
59

11468

train
8544
6920
10000
5452
10662
7520

dev
1101
872

-
-
-

836

test
|v |
17836
2210
1821
16185
cv 21057
500
9137
cv 20191
5563
51379

|vpre|
12745
11490
17671
5990
16746
30575

table 1: summary statistics for the datasets. c: number of target classes, l: average sentence length, m:
maximum sentence length, train/dev/test: train/development/test set size, |v |: vocabulary size, |vpre|:
number of words present in the set of pre-trained id27s, cv: 10-fold cross validation.

4 experimental setup

4.1 datasets

the proposed models are tested on six datasets. summary statistics of the datasets are in table 1.

    mr2: sentence polarity dataset from pang and lee (2005). the task is to detect positive/negative

reviews.

    sst-13: stanford sentiment treebank is an extension of mr from socher et al. (2013). the aim is
to classify a review as    ne-grained labels (very negative, negative, neutral, positive, very positive).

    sst-2: same as sst-1 but with neutral reviews removed and binary labels (negative, positive). for
both experiments, phrases and sentences are used to train the model, but only sentences are scored at
test time (socher et al., 2013; le and mikolov, 2014). thus the training set is an order of magnitude
larger than listed in table 1.

    subj4: subjectivity dataset (pang and lee, 2004). the task is to classify a sentence as being sub-

jective or objective.

    trec5: question classi   cation dataset (li and roth, 2002). the task involves classifying a ques-

tion into 6 question types (abbreviation, description, entity, human, location, numeric value).

    20newsgroups6: the 20ng dataset contains messages from twenty newsgroups. we use the bydate
version preprocessed by cachopo (2007). we select four major categories (comp, politics, rec and
religion) followed by hingmire et al. (2013).

4.2 id27s

the id27s are pre-trained on much larger unannotated corpora to achieve better generaliza-
tion given limited amount of training data (turian et al., 2010). in particular, our experiments utilize
the glove embeddings7 trained by pennington et al. (2014) on 6 billion tokens of wikipedia 2014 and
gigaword 5. words not present in the set of pre-trained words are initialized by randomly sampling from
uniform distribution in [   0.1, 0.1]. the id27s are    ne-tuned during training to improve the
performance of classi   cation.

2https://www.cs.cornell.edu/people/pabo/movie-review-data/
3http://nlp.stanford.edu/sentiment/
4http://www.cs.cornell.edu/people/pabo/movie-review-data/
5http://cogcomp.cs.illinois.edu/data/qa/qc/
6http://web.ist.utl.pt/acardoso/datasets/
7http://nlp.stanford.edu/projects/glove/

4.3 hyper-parameter settings
for datasets without a standard development set we randomly select 10% of the training data as the
development set. the evaluation metric of the 20ng is the macro-f1 measure followed by the state-of-
the-art work and the other    ve datasets use accuracy as the metric. the    nal hyper-parameters are as
follows.

the dimension of id27s is 300, the hidden units of lstm is 300. we use 100 convo-
lutional    lters each for window sizes of (3,3), 2d pooling size of (2,2). we set the mini-batch size as
10 and the learning rate of adadelta as the default value 1.0. for id173, we employ dropout
operation (hinton et al., 2012) with dropout rate of 0.5 for the id27s, 0.2 for the blstm
layer and 0.4 for the penultimate layer, we also use l2 penalty with coef   cient 10   5 over the parameters.
these values are chosen via a grid search on the sst-1 development set. we only tune these hyper-
parameters, and more    ner tuning, such as using different numbers of hidden units of lstm layer, or
using wide convolution (kalchbrenner et al., 2014), may further improve the performance.

5 results

5.1 overall performance
this work implements four models, blstm, blstm-att, blstm-2dpooling, and blstm-2did98.
table 2 presents the performance of the four models and other state-of-the-art models on six classi   cation
tasks. the blstm-2did98 model achieves excellent performance on 4 out of 6 tasks. especially, it
achieves 52.4% and 89.5% test accuracies on sst-1 and sst-2 respectively.

blstm-2dpooling performs worse than the state-of-the-art models. while we expect performance
gains through the use of 2d convolution, we are surprised at the magnitude of the gains. blstm-id98
beats all baselines on sst-1, sst-2, and trec datasets. as for subj and mr datasets, blstm-2did98
gets a second higher accuracies. some of the previous techniques only work on sentences, but not
paragraphs/documents with several sentences. our question becomes whether it is possible to use our
models for datasets that have a substantial number of words, such as 20ng and where the content consists
of many different topics. for that purpose, this paper tests the four models on document-level dataset
20ng, by treating the document as a long sentence. compared with rid98 (lai et al., 2015), blstm-
2did98 achieves a comparable result.

besides, this paper also compares with renn, id56, id98 and other neural networks:

    compared with renn, the proposed two models do not depend on external language-speci   c fea-

tures such as dependency parse trees.

    id98 extracts features from id27s of the input text, while blstm-2dpooling and
blstm-2did98 captures features from the output of blstm layer, which has already extracted
features from the original input text.

    blstm-2did98 is an extension of blstm-2dpooling, and the results show that blstm-2did98

can capture more dependencies in text.

    adasent utilizes a more complicated model to form a hierarchy of representations, and it outper-
forms blstm-2did98 on subj and mr datasets. compared with dsid98 (zhang et al., 2016),
blstm-2did98 outperforms it on    ve datasets.

compared with these results, 2d convolution and 2d max pooling operation are more effective for
modeling sentence, even document. to better understand the effect of 2d operations, this work conducts
a sensitivity analysis on sst-1 dataset.

5.2 effect of sentence length
figure 2 depicts the performance of the four models on different length of sentences.
in the    gure,
the x-axis represents sentence lengths and y-axis is accuracy. the sentences collected in test set are no

nn model

renn

id98

id56

other

ours

rntn (socher et al., 2013)
did56 (irsoy and cardie, 2014)
did98 (kalchbrenner et al., 2014)
id98-non-static (kim, 2014)
id98-mc (kim, 2014)
tbid98(mou et al., 2015)
molding-id98 (lei et al., 2015)
id98-ana (zhang and wallace, 2015)
mvid98 (yin and sch  utze, 2016)
rid98 (lai et al., 2015)
s-lstm (zhu et al., 2015)
lstm (tai et al., 2015)
blstm (tai et al., 2015)
tree-lstm (tai et al., 2015)
lstmn (cheng et al., 2016)
multi-task (liu et al., 2016)
pv (le and mikolov, 2014)
dan (iyyer et al., 2015)
combine-skip (kiros et al., 2015)
adasent (zhao et al., 2015)
lstm-id56 (le and zuidema, 2015)
c-lstm (zhou et al., 2015)
dsid98 (zhang et al., 2016)
blstm
blstm-att
blstm-2dpooling
blstm-2did98

sst-1 sst-2
45.7
85.4
86.6
49.8
86.8
48.5
48.0
87.2
88.1
47.4
87.9
51.4
88.6
51.2
45.98
85.45
49.6
89.4
47.21

-

-

46.4
49.1
51.0
49.3
49.6
48.7
48.2

-
-

49.9
49.2
49.7
49.1
49.8
50.5
52.4

81.9
84.9
87.5
88.0
87.3
87.9
87.8
86.8

-
-

88.0
87.8
89.1
87.6
88.2
88.3
89.5

subj trec mr 20ng

-
-
-

93.4
93.2

-
-

93.66
93.9

-
-
-
-
-
-

94.1

-
-

93.6
95.5

-
-

93.2
92.1
93.5
93.7
94.0

-
-

93.0
93.6
92
96.0

-

-
-
-
-
-
-
-

91.37

81.02

-
-
-
-
-
-
-
-
-
-

92.2
92.4

-

94.6
95.4
93.0
93.8
94.8
96.1

-
-
-
-
-
-
-
-
-
-

76.5
83.1

-
-

81.5
80.0
81.0
81.5
82.3

-
-
-
-
-
-
-
-
-

96.49

-
-
-
-
-
-
-
-
-
-
-
-
-

94.0
94.6
95.5
96.5

table 2: classi   cation results on several standard benchmarks. rntn: recursive deep models for
semantic compositionality over a sentiment treebank (socher et al., 2013). did56: deep recursive
neural networks for compositionality in language (irsoy and cardie, 2014). did98: a convolutional
neural network for modeling sentences (kalchbrenner et al., 2014). id98-nonstatic/mc: convolu-
tional neural networks for sentence classi   cation (kim, 2014). tbid98: discriminative neural sen-
tence modeling by tree-based convolution (mou et al., 2015). molding-id98: molding id98s for
text: non-linear, non-consecutive convolutions (lei et al., 2015). id98-ana: a sensitivity anal-
ysis of (and practitioners    guide to) convolutional neural networks for sentence classi   cation
(zhang and wallace, 2015). mvid98: multichannel variable-size convolution for sentence classi   ca-
tion (yin and sch  utze, 2016). rid98: recurrent convolutional neural networks for text classi   ca-
tion (lai et al., 2015). s-lstm: long short-term memory over recursive structures (zhu et al., 2015).
lstm/blstm/tree-lstm: improved semantic representations from tree-structured long short-
term memory networks (tai et al., 2015). lstmn: long short-term memory-networks for machine
reading (cheng et al., 2016). multi-task: recurrent neural network for text classi   cation with
id72 (liu et al., 2016). pv: distributed representations of sentences and documents
(le and mikolov, 2014). dan: deep unordered composition rivals syntactic methods for text classi-
   cation (iyyer et al., 2015). combine-skip: skip-thought vectors (kiros et al., 2015). adasent: self-
adaptive hierarchical sentence model (zhao et al., 2015). lstm-id56: compositional distributional
semantics with long short term memory (le and zuidema, 2015). c-lstm: a c-lstm neural net-
work for text classi   cation (zhou et al., 2015). dsid98: dependency sensitive convolutional neural
networks for modeling sentences and documents (zhang et al., 2016).

65

60

55

50

45

)

%

(
 
y
c
a
r
u
c
c
a

40
0

5

10

15

blstm
blstm-att
blstm-2dpooling
blstm-2did98

30

35

40

45

)

%

(
 
y
c
a
r
u
c
c
a

53.0

52.5

52.0

51.5

51.0

50.5

50.0

49.5

49.0

c_2

c_3

c_4

2d filter size

c_5

c_6

20

25

sentence length

figure 2: fine-grained sentiment classi   cation
accuracy vs. sentence length.

figure 3: prediction accuracy with different
size of 2d    lter and 2d max pooling.

longer than 45 words. the accuracy here is the average value of the sentences with length in the window
[l     2, l + 2]. each data point is a mean score over 5 runs, and error bars have been omitted for clarity.

it is found that both blstm-2dpooling and blstm-2did98 outperform the other two models. this
suggests that both 2d convolution and 2d max pooling operation are able to encode semantically-useful
structural information. at the same time, it shows that the accuracies decline with the length of sen-
tences increasing. in future work, we would like to investigate neural mechanisms to preserve long-term
dependencies of text.

5.3 effect of 2d convolutional filter and 2d max pooling size
we are interested in what is the best 2d    lter and max pooling size to get better performance. we conduct
experiments on sst-1 dataset with blstm-2did98 and set the number of feature maps to 100.

to make it simple, we set these two dimensions to the same values, thus both the    lter and the pooling
are square matrices. for the horizontal axis, c means 2d convolutional    lter size, and the    ve different
color bar charts on each c represent different 2d max pooling size from 2 to 6. figure 3 shows that dif-
ferent size of    lter and pooling can get different accuracies. the best accuracy is 52.6 with 2d    lter size
(5,5) and 2d max pooling size (5,5), this shows that    ner tuning can further improve the performance
reported here. and if a larger    lter is used, the convolution can detector more features, and the perfor-
mance may be improved, too. however, the networks will take up more storage space, and consume
more time.

6 conclusion

this paper introduces two combination models, one is blstm-2dpooling, the other is blstm-
2did98, which can be seen as an extension of blstm-2dpooling. both models can hold not only
the time-step dimension but also the feature vector dimension information. the experiments are con-
ducted on six text classi   caion tasks. the experiments results demonstrate that blstm-2did98 not
only outperforms reid98, id56 and id98 models, but also works better than the blstm-2dpooling
and dsid98 (zhang et al., 2016). especially, blstm-2did98 achieves highest accuracy on sst-1 and
sst-2 datasets. to better understand the effective of the proposed two models, this work also conducts a
sensitivity analysis on sst-1 dataset. it is found that large    lter can detector more features, and this may
lead to performance improvement.

acknowledgements

we thank anonymous reviewers for their constructive comments. this research was funded by the na-
tional high technology research and development program of china (no.2015aa015402), and the
national natural science foundation of china (no. 61602479), and the strategic priority research
program of the chinese academy of sciences (grant no. xdb02070005).

references
[cachopo2007] ana margarida de jesus cardoso cachopo. 2007. improving methods for single-label text catego-

rization. ph.d. thesis, universidade t  ecnica de lisboa.

[cheng et al.2016] jianpeng cheng, li dong, and mirella lapata. 2016. long short-term memory-networks for

machine reading. arxiv preprint arxiv:1601.06733.

[hingmire et al.2013] swapnil hingmire, sandeep chougule, girish k palshikar, and sutanu chakraborti. 2013.
document classi   cation by topic labeling. in proceedings of the 36th international acm sigir conference on
research and development in information retrieval, pages 877   880. acm.

[hinton et al.2012] geoffrey e hinton, nitish srivastava, alex krizhevsky, ilya sutskever, and ruslan r salakhut-
improving neural networks by preventing co-adaptation of feature detectors. arxiv preprint

dinov. 2012.
arxiv:1207.0580.

[hochreiter and schmidhuber1997] sepp hochreiter and j  urgen schmidhuber. 1997. long short-term memory.

neural computation, 9(8):1735   1780.

[irsoy and cardie2014] ozan irsoy and claire cardie. 2014. deep id56s for compositionality

in language. in advances in neural information processing systems, pages 2096   2104.

[iyyer et al.2015] mohit iyyer, varun manjunatha, jordan boyd-graber, and hal daum  e iii. 2015. deep un-
in proceedings of the association for

ordered composition rivals syntactic methods for text classi   cation.
computational linguistics.

[kalchbrenner et al.2014] nal kalchbrenner, edward grefenstette, and phil blunsom. 2014. a convolutional

neural network for modelling sentences. arxiv preprint arxiv:1404.2188.

[kim2014] yoon kim.

arxiv:1408.5882.

2014. convolutional neural networks for sentence classi   cation.

arxiv preprint

[kiros et al.2015] ryan kiros, yukun zhu, ruslan r salakhutdinov, richard zemel, raquel urtasun, antonio
torralba, and sanja fidler. 2015. skip-thought vectors. in advances in neural information processing systems,
pages 3294   3302.

[krizhevsky et al.2012] alex krizhevsky, ilya sutskever, and geoffrey e hinton. 2012. id163 classi   cation
with deep convolutional neural networks. in advances in neural information processing systems, pages 1097   
1105.

[lai et al.2015] siwei lai, liheng xu, kang liu, and jun zhao. 2015. recurrent convolutional neural networks

for text classi   cation. in aaai, pages 2267   2273.

[le and mikolov2014] quoc v le and tomas mikolov. 2014. distributed representations of sentences and docu-

ments. arxiv preprint arxiv:1405.4053.

[le and zuidema2015] phong le and willem zuidema. 2015. compositional id65 with long

short term memory. arxiv preprint arxiv:1503.02510.

[lecun et al.1998] yann lecun, l  eon bottou, yoshua bengio, and patrick haffner. 1998. gradient-based learning

applied to document recognition. proceedings of the ieee, 86(11):2278   2324.

[lei et al.2015] tao lei, regina barzilay, and tommi jaakkola. 2015. molding id98s for text: non-linear, non-

consecutive convolutions. arxiv preprint arxiv:1508.04112.

[li and roth2002] xin li and dan roth. 2002. learning question classi   ers. in proceedings of the 19th interna-
tional conference on computational linguistics-volume 1, pages 1   7. association for computational linguis-
tics.

[liu et al.2016] pengfei liu, xipeng qiu, and xuanjing huang. 2016. recurrent neural network for text classi   -

cation with id72. arxiv preprint arxiv:1605.05101.

[mou et al.2015] lili mou, hao peng, ge li, yan xu, lu zhang, and zhi jin. 2015. discriminative neural sentence

modeling by tree-based convolution. arxiv preprint arxiv:1504.01106.

[pang and lee2004] bo pang and lillian lee. 2004. a sentimental education: id31 using subjec-
tivity summarization based on minimum cuts. in proceedings of the 42nd annual meeting on association for
computational linguistics, page 271. association for computational linguistics.

[pang and lee2005] bo pang and lillian lee. 2005. seeing stars: exploiting class relationships for sentiment
in proceedings of the 43rd annual meeting on association for

categorization with respect to rating scales.
computational linguistics, pages 115   124. association for computational linguistics.

[pennington et al.2014] jeffrey pennington, richard socher, and christopher d manning. 2014. glove: global

vectors for word representation. in emnlp, volume 14, pages 1532   43.

[schuster and paliwal1997] mike schuster and kuldip k paliwal. 1997. id182.

signal processing, ieee transactions on, 45(11):2673   2681.

[socher et al.2013] richard socher, alex perelygin, jean y wu, jason chuang, christopher d manning, andrew y
ng, and christopher potts. 2013. recursive deep models for semantic compositionality over a sentiment
treebank. in proceedings of the conference on empirical methods in natural language processing (emnlp),
volume 1631, page 1642. citeseer.

[tai et al.2015] kai sheng tai, richard socher, and christopher d manning. 2015. improved semantic represen-

tations from tree-structured id137. arxiv preprint arxiv:1503.00075.

[tang et al.2015] duyu tang, bing qin, xiaocheng feng, and ting liu. 2015. target-dependent sentiment classi-

   cation with long short term memory. arxiv preprint arxiv:1512.01100.

[turian et al.2010] joseph turian, lev ratinov, and yoshua bengio. 2010. word representations: a simple and
general method for semi-supervised learning. in proceedings of the 48th annual meeting of the association for
computational linguistics, pages 384   394. association for computational linguistics.

[wang and manning2012] sida wang and christopher d manning. 2012. baselines and bigrams: simple, good
sentiment and topic classi   cation. in proceedings of the 50th annual meeting of the association for computa-
tional linguistics: short papers-volume 2, pages 90   94. association for computational linguistics.

[wang2010] alex hai wang. 2010. don   t follow me: spam detection in twitter. in security and cryptography

(secrypt), proceedings of the 2010 international conference on, pages 1   10. ieee.

[wen et al.2016] ying wen, weinan zhang, rui luo, and jun wang. 2016. learning text representation using

recurrent convolutional neural network with highway layers. arxiv preprint arxiv:1606.06905.

[yang et al.2016] zichao yang, diyi yang, chris dyer, xiaodong he, alex smola, and eduard hovy. 2016. hi-
erarchical attention networks for document classi   cation. in proceedings of the 2016 conference of the north
american chapter of the association for computational linguistics: human language technologies.

[yin and sch  utze2016] wenpeng yin and hinrich sch  utze. 2016. multichannel variable-size convolution for sen-

tence classi   cation. arxiv preprint arxiv:1603.04513.

[zeiler2012] matthew d zeiler.

arxiv:1212.5701.

2012. adadelta: an adaptive learning rate method.

arxiv preprint

[zeng et al.2014] daojian zeng, kang liu, siwei lai, guangyou zhou, jun zhao, et al. 2014. relation classi   ca-

tion via convolutional deep neural network. in coling, pages 2335   2344.

[zhang and wallace2015] ye zhang and byron wallace. 2015. a sensitivity analysis of (and practitioners    guide

to) convolutional neural networks for sentence classi   cation. arxiv preprint arxiv:1510.03820.

[zhang et al.2016] rui zhang, honglak lee, and dragomir radev. 2016. dependency sensitive convolutional

neural networks for modeling sentences and documents. in proceedings of naacl-hlt, pages 1512   1521.

[zhao et al.2015] han zhao, zhengdong lu, and pascal poupart. 2015. self-adaptive hierarchical sentence model.

arxiv preprint arxiv:1504.05070.

[zhou et al.2015] chunting zhou, chonglin sun, zhiyuan liu, and francis lau. 2015. a c-lstm neural network

for text classi   cation. arxiv preprint arxiv:1511.08630.

[zhou et al.2016] peng zhou, wei shi, jun tian, zhenyu qi, bingchen li, hongwei hao, and bo xu. 2016.
attention-based bidirectional id137 for relation classi   cation. in the 54th annual
meeting of the association for computational linguistics, page 207.

[zhu et al.2015] xiaodan zhu, parinaz sobhani, and hongyu guo. 2015. long short-term memory over recursive

structures. in proceedings of the 32nd international conference on machine learning, pages 1604   1612.

