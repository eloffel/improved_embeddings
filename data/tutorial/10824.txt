7
1
0
2

 

b
e
f
2
2

 

 
 
]

g
l
.
s
c
[
 
 

4
v
7
0
9
2
0

.

9
0
6
1
:
v
i
x
r
a

published as a conference paper at iclr 2017

semi-supervised classification with
id197

thomas n. kipf
university of amsterdam
t.n.kipf@uva.nl

max welling
university of amsterdam
canadian institute for advanced research (cifar)
m.welling@uva.nl

abstract

we present a scalable approach for semi-supervised learning on graph-structured
data that is based on an ef   cient variant of convolutional neural networks which
operate directly on graphs. we motivate the choice of our convolutional archi-
tecture via a localized    rst-order approximation of spectral graph convolutions.
our model scales linearly in the number of graph edges and learns hidden layer
representations that encode both local graph structure and features of nodes. in
a number of experiments on id191 and on a id13 dataset
we demonstrate that our approach outperforms related methods by a signi   cant
margin.

1

introduction

(cid:88)

we consider the problem of classifying nodes (such as documents) in a graph (such as a citation
network), where labels are only available for a small subset of nodes. this problem can be framed
as graph-based semi-supervised learning, where label information is smoothed over the graph via
some form of explicit graph-based id173 (zhu et al., 2003; zhou et al., 2004; belkin et al.,
2006; weston et al., 2012), e.g. by using a graph laplacian id173 term in the id168:

l = l0 +   lreg , with lreg =

aij(cid:107)f (xi)     f (xj)(cid:107)2 = f (x)

(cid:62)

   f (x) .

(1)

i,j

a degree matrix dii =(cid:80)

here, l0 denotes the supervised loss w.r.t. the labeled part of the graph, f (  ) can be a neural network-
like differentiable function,    is a weighing factor and x is a matrix of node feature vectors xi.
    = d     a denotes the unnormalized graph laplacian of an undirected graph g = (v,e) with
n nodes vi     v, edges (vi, vj)     e, an adjacency matrix a     rn  n (binary or weighted) and
j aij. the formulation of eq. 1 relies on the assumption that connected
nodes in the graph are likely to share the same label. this assumption, however, might restrict
modeling capacity, as graph edges need not necessarily encode node similarity, but could contain
additional information.
in this work, we encode the graph structure directly using a neural network model f (x, a) and
train on a supervised target l0 for all nodes with labels, thereby avoiding explicit graph-based
id173 in the id168. conditioning f (  ) on the adjacency matrix of the graph will
allow the model to distribute gradient information from the supervised loss l0 and will enable it to
learn representations of nodes both with and without labels.
our contributions are two-fold. firstly, we introduce a simple and well-behaved layer-wise prop-
agation rule for neural network models which operate directly on graphs and show how it can be
motivated from a    rst-order approximation of spectral graph convolutions (hammond et al., 2011).
secondly, we demonstrate how this form of a graph-based neural network model can be used for
fast and scalable semi-supervised classi   cation of nodes in a graph. experiments on a number of
datasets demonstrate that our model compares favorably both in classi   cation accuracy and ef   -
ciency (measured in wall-clock time) against state-of-the-art methods for semi-supervised learning.

1

published as a conference paper at iclr 2017

2 fast approximate convolutions on graphs

(cid:16)

2 h (l)w (l)(cid:17)

in this section, we provide theoretical motivation for a speci   c graph-based neural network model
f (x, a) that we will use in the rest of this paper. we consider a multi-layer graph convolutional
network (id197) with the following layer-wise propagation rule:

  d    1

2   a   d    1

h (l+1) =   

in is the identity matrix,   dii =(cid:80)

(2)
here,   a = a + in is the adjacency matrix of the undirected graph g with added self-connections.
  aij and w (l) is a layer-speci   c trainable weight matrix.   (  )
denotes an activation function, such as the relu(  ) = max(0,  ). h (l)     rn  d is the matrix of ac-
tivations in the lth layer; h (0) = x. in the following, we show that the form of this propagation rule
can be motivated1 via a    rst-order approximation of localized spectral    lters on graphs (hammond
et al., 2011; defferrard et al., 2016).

.

j

g  (cid:48)(  )     k(cid:88)

k=0

g  (cid:48) (cid:63) x     k(cid:88)

2.1 spectral graph convolutions
we consider spectral convolutions on graphs de   ned as the multiplication of a signal x     rn (a
scalar for every node) with a    lter g   = diag(  ) parameterized by        rn in the fourier domain,
i.e.:

g   (cid:63) x = u g  u

x ,

(cid:62)

(3)

where u is the matrix of eigenvectors of the normalized graph laplacian l = in     d    1
2 =
u   u(cid:62), with a diagonal matrix of its eigenvalues    and u(cid:62)x being the graph fourier transform
of x. we can understand g   as a function of the eigenvalues of l, i.e. g  (  ). evaluating eq. 3 is
computationally expensive, as multiplication with the eigenvector matrix u is o(n 2). furthermore,
computing the eigendecomposition of l in the    rst place might be prohibitively expensive for large
graphs. to circumvent this problem, it was suggested in hammond et al. (2011) that g  (  ) can be
well-approximated by a truncated expansion in terms of chebyshev polynomials tk(x) up to kth
order:

2 ad    1

(cid:48)
ktk(    ) ,

  

(4)

       in .   max denotes the largest eigenvalue of l.   (cid:48)     rk is now a
with a rescaled      = 2
  max
vector of chebyshev coef   cients. the chebyshev polynomials are recursively de   ned as tk(x) =
2xtk   1(x)     tk   2(x), with t0(x) = 1 and t1(x) = x. the reader is referred to hammond et al.
(2011) for an in-depth discussion of this approximation.
going back to our de   nition of a convolution of a signal x with a    lter g  (cid:48), we now have:

(cid:48)
ktk(   l)x ,

  

(5)

k=0

l     in ; as can easily be veri   ed by noticing that (u   u(cid:62))k = u   ku(cid:62). note that
with   l = 2
  max
this expression is now k-localized since it is a kth-order polynomial in the laplacian, i.e. it depends
only on nodes that are at maximum k steps away from the central node (kth-order neighborhood).
the complexity of evaluating eq. 5 is o(|e|), i.e. linear in the number of edges. defferrard et al.
(2016) use this k-localized convolution to de   ne a convolutional neural network on graphs.

2.2 layer-wise linear model

a neural network model based on graph convolutions can therefore be built by stacking multiple
convolutional layers of the form of eq. 5, each layer followed by a point-wise non-linearity. now,
imagine we limited the layer-wise convolution operation to k = 1 (see eq. 5), i.e. a function that is
linear w.r.t. l and therefore a linear function on the graph laplacian spectrum.

1we provide an alternative interpretation of this propagation rule based on the weisfeiler-lehman algorithm

(weisfeiler & lehmann, 1968) in appendix a.

2

g  (cid:48) (cid:63) x       

(cid:48)
0x +   
0 and   (cid:48)

published as a conference paper at iclr 2017

in this way, we can still recover a rich class of convolutional    lter functions by stacking multiple
such layers, but we are not limited to the explicit parameterization given by, e.g., the chebyshev
polynomials. we intuitively expect that such a model can alleviate the problem of over   tting on
local neighborhood structures for graphs with very wide node degree distributions, such as social
networks, id191, id13s and many other real-world graph datasets. addition-
ally, for a    xed computational budget, this layer-wise linear formulation allows us to build deeper
models, a practice that is known to improve modeling capacity on a number of domains (he et al.,
2016).
in this linear formulation of a id197 we further approximate   max     2, as we can expect that neural
network parameters will adapt to this change in scale during training. under these approximations
eq. 5 simpli   es to:

1 (l     in ) x =   
(cid:48)

0x       
(cid:48)

(cid:48)
1d

    1

2 ad

    1

2 x ,

(6)

(cid:17)

(cid:16)

with two free parameters   (cid:48)
1. the    lter parameters can be shared over the whole graph.
successive application of    lters of this form then effectively convolve the kth-order neighborhood of
a node, where k is the number of successive    ltering operations or convolutional layers in the neural
network model.
in practice, it can be bene   cial to constrain the number of parameters further to address over   tting
and to minimize the number of operations (such as id127s) per layer. this leaves us
with the following expression:

(7)

g   (cid:63) x       
0 =      (cid:48)

    1

    1

2

2 ad

in + d

x ,
2 ad    1
1. note that in + d    1

with a single parameter    =   (cid:48)
2 now has eigenvalues in
the range [0, 2]. repeated application of this operator can therefore lead to numerical instabilities
and exploding/vanishing gradients when used in a deep neural network model. to alleviate this
problem, we introduce the following reid172 trick: in + d    1
2 , with

2       d    1

2 ad    1

2   a   d    1

  a = a + in and   dii =(cid:80)

  aij.

j

we can generalize this de   nition to a signal x     rn  c with c input channels (i.e. a c-dimensional
feature vector for every node) and f    lters or feature maps as follows:

(8)
where        rc  f is now a matrix of    lter parameters and z     rn  f is the convolved signal
matrix. this    ltering operation has complexity o(|e|f c), as   ax can be ef   ciently implemented
as a product of a sparse matrix with a dense matrix.

z =   d

2 x   ,

2   a   d

    1

    1

3 semi-supervised node classification

having introduced a simple, yet    exible model f (x, a) for ef   cient information propagation on
graphs, we can return to the problem of semi-supervised node classi   cation. as outlined in the in-
troduction, we can relax certain assumptions typically made in graph-based semi-supervised learn-
ing by conditioning our model f (x, a) both on the data x and on the adjacency matrix a of the
underlying graph structure. we expect this setting to be especially powerful in scenarios where the
adjacency matrix contains information not present in the data x, such as citation links between doc-
uments in a citation network or relations in a id13. the overall model, a multi-layer
id197 for semi-supervised learning, is schematically depicted in figure 1.

3.1 example

in the following, we consider a two-layer id197 for semi-supervised node classi   cation on a graph
with a symmetric adjacency matrix a (binary or weighted). we    rst calculate   a =   d    1
2 in
a pre-processing step. our forward model then takes the simple form:

2   a   d    1

z = f (x, a) = softmax

.

(9)

(cid:16)

  axw (0)(cid:17)

w (1)(cid:17)

(cid:16)

  a relu

3

l =     (cid:88)

f(cid:88)

l   yl

f =1

published as a conference paper at iclr 2017

(a) graph convolutional network

(b) hidden layer activations

figure 1: left: schematic depiction of multi-layer graph convolutional network (id197) for semi-
supervised learning with c input channels and f feature maps in the output layer. the graph struc-
ture (edges shown as black lines) is shared over layers, labels are denoted by yi. right: id167
(maaten & hinton, 2008) visualization of hidden layer activations of a two-layer id197 trained on
the cora dataset (sen et al., 2008) using 5% of labels. colors denote document class.

here, w (0)     rc  h is an input-to-hidden weight matrix for a hidden layer with h feature maps.
w (1)     rh  f is a hidden-to-output weight matrix. the softmax activation function, de   ned as
i exp(xi), is applied row-wise. for semi-supervised multi-

softmax(xi) = 1z exp(xi) with z =(cid:80)

class classi   cation, we then evaluate the cross-id178 error over all labeled examples:

ylf ln zlf ,

(10)

where yl is the set of node indices that have labels.
the neural network weights w (0) and w (1) are trained using id119. in this work, we
perform batch id119 using the full dataset for every training iteration, which is a viable
option as long as datasets    t in memory. using a sparse representation for a, memory requirement
is o(|e|), i.e. linear in the number of edges. stochasticity in the training process is introduced via
dropout (srivastava et al., 2014). we leave memory-ef   cient extensions with mini-batch stochastic
id119 for future work.

3.2

implementation

in practice, we make use of tensorflow (abadi et al., 2015) for an ef   cient gpu-based imple-
mentation2 of eq. 9 using sparse-dense id127s. the computational complexity of
evaluating eq. 9 is then o(|e|chf ), i.e. linear in the number of graph edges.

4 related work

our model draws inspiration both from the    eld of graph-based semi-supervised learning and from
recent work on neural networks that operate on graphs. in what follows, we provide a brief overview
on related work in both    elds.

4.1 graph-based semi-supervised learning

a large number of approaches for semi-supervised learning using graph representations have been
proposed in recent years, most of which fall into two broad categories: methods that use some
form of explicit graph laplacian id173 and graph embedding-based approaches. prominent
examples for graph laplacian id173 include label propagation (zhu et al., 2003), manifold
id173 (belkin et al., 2006) and deep semi-supervised embedding (weston et al., 2012).

2code to reproduce our experiments is available at https://github.com/tkipf/id197.

4

cinputlayerx1x2x3x4foutputlayerz1z2z3z4hiddenlayersy1y4130201001020303020100102030published as a conference paper at iclr 2017

recently, attention has shifted to models that learn graph embeddings with methods inspired by
the skip-gram model (mikolov et al., 2013). deepwalk (perozzi et al., 2014) learns embeddings
via the prediction of the local neighborhood of nodes, sampled from id93 on the graph.
line (tang et al., 2015) and node2vec (grover & leskovec, 2016) extend deepwalk with more
sophisticated random walk or breadth-   rst search schemes. for all these methods, however, a multi-
step pipeline including random walk generation and semi-supervised training is required where each
step has to be optimized separately. planetoid (yang et al., 2016) alleviates this by injecting label
information in the process of learning embeddings.

4.2 neural networks on graphs

neural networks that operate on graphs have previously been introduced in gori et al. (2005);
scarselli et al. (2009) as a form of recurrent neural network. their framework requires the repeated
application of contraction maps as propagation functions until node representations reach a stable
   xed point. this restriction was later alleviated in li et al. (2016) by introducing modern practices
for recurrent neural network training to the original graph neural network framework. duvenaud
et al. (2015) introduced a convolution-like propagation rule on graphs and methods for graph-level
classi   cation. their approach requires to learn node degree-speci   c weight matrices which does not
scale to large graphs with wide node degree distributions. our model instead uses a single weight
matrix per layer and deals with varying node degrees through an appropriate id172 of the
adjacency matrix (see section 3.1).
a related approach to node classi   cation with a graph-based neural network was recently introduced
in atwood & towsley (2016). they report o(n 2) complexity, limiting the range of possible appli-
cations. in a different yet related model, niepert et al. (2016) convert graphs locally into sequences
that are fed into a conventional 1d convolutional neural network, which requires the de   nition of a
node ordering in a pre-processing step.
our method is based on spectral graph convolutional neural networks, introduced in bruna et al.
(2014) and later extended by defferrard et al. (2016) with fast localized convolutions. in contrast
to these works, we consider here the task of transductive node classi   cation within networks of
signi   cantly larger scale. we show that in this setting, a number of simpli   cations (see section 2.2)
can be introduced to the original frameworks of bruna et al. (2014) and defferrard et al. (2016) that
improve scalability and classi   cation performance in large-scale networks.

5 experiments

we test our model in a number of experiments: semi-supervised document classi   cation in cita-
tion networks, semi-supervised entity classi   cation in a bipartite graph extracted from a knowledge
graph, an evaluation of various graph propagation models and a run-time analysis on random graphs.

5.1 datasets

we closely follow the experimental setup in yang et al. (2016). dataset statistics are summarized
in table 1. in the citation network datasets   citeseer, cora and pubmed (sen et al., 2008)   nodes
are documents and edges are citation links. label rate denotes the number of labeled nodes that are
used for training divided by the total number of nodes in each dataset. nell (carlson et al., 2010;
yang et al., 2016) is a bipartite graph dataset extracted from a id13 with 55,864 relation
nodes and 9,891 entity nodes.

table 1: dataset statistics, as reported in yang et al. (2016).

dataset
citeseer
cora
pubmed
nell

type

citation network
citation network
citation network
id13

nodes
3,327
2,708
19,717
65,755

edges classes features label rate
4,732
0.036
5,429
0.052
44,338
0.003
266,144
0.001

3,703
1,433
500
5,414

6
7
3
210

5

published as a conference paper at iclr 2017

id191 we consider three citation network datasets: citeseer, cora and pubmed (sen
et al., 2008). the datasets contain sparse bag-of-words feature vectors for each document and a list
of citation links between documents. we treat the citation links as (undirected) edges and construct
a binary, symmetric adjacency matrix a. each document has a class label. for training, we only use
20 labels per class, but all feature vectors.

nell nell is a dataset extracted from the id13 introduced in (carlson et al., 2010).
a id13 is a set of entities connected with directed, labeled edges (relations). we follow
the pre-processing scheme as described in yang et al. (2016). we assign separate relation nodes
r1 and r2 for each entity pair (e1, r, e2) as (e1, r1) and (e2, r2). entity nodes are described by
sparse feature vectors. we extend the number of features in nell by assigning a unique one-hot
representation for every relation node, effectively resulting in a 61,278-dim sparse feature vector per
node. the semi-supervised task here considers the extreme case of only a single labeled example
per class in the training set. we construct a binary, symmetric adjacency matrix from this graph by
setting entries aij = 1, if one or more edges are present between nodes i and j.

random graphs we simulate random graph datasets of various sizes for experiments where we
measure training time per epoch. for a dataset with n nodes we create a random graph assigning
2n edges uniformly at random. we take the identity matrix in as input feature matrix x, thereby
implicitly taking a featureless approach where the model is only informed about the identity of each
node, speci   ed by a unique one-hot vector. we add dummy labels yi = 1 for every node.

5.2 experimental set-up

unless otherwise noted, we train a two-layer id197 as described in section 3.1 and evaluate pre-
diction accuracy on a test set of 1,000 labeled examples. we provide additional experiments using
deeper models with up to 10 layers in appendix b. we choose the same dataset splits as in yang et al.
(2016) with an additional validation set of 500 labeled examples for hyperparameter optimization
(dropout rate for all layers, l2 id173 factor for the    rst id197 layer and number of hidden
units). we do not use the validation set labels for training.
for the citation network datasets, we optimize hyperparameters on cora only and use the same set
of parameters for citeseer and pubmed. we train all models for a maximum of 200 epochs (training
iterations) using adam (kingma & ba, 2015) with a learning rate of 0.01 and early stopping with a
window size of 10, i.e. we stop training if the validation loss does not decrease for 10 consecutive
epochs. we initialize weights using the initialization described in glorot & bengio (2010) and
accordingly (row-)normalize input feature vectors. on the random graph datasets, we use a hidden
layer size of 32 units and omit id173 (i.e. neither dropout nor l2 id173).

5.3 baselines

we compare against the same baseline methods as in yang et al. (2016), i.e. label propagation
(lp) (zhu et al., 2003), semi-supervised embedding (semiemb) (weston et al., 2012), manifold
id173 (manireg) (belkin et al., 2006) and skip-gram based graph embeddings (deepwalk)
(perozzi et al., 2014). we omit tid166 (joachims, 1999), as it does not scale to the large number of
classes in one of our datasets.
we further compare against the iterative classi   cation algorithm (ica) proposed in lu & getoor
(2003) in conjunction with two id28 classi   ers, one for local node features alone and
one for relational classi   cation using local features and an aggregation operator as described in
sen et al. (2008). we    rst train the local classi   er using all labeled training set nodes and use
it to bootstrap class labels of unlabeled nodes for relational classi   er training. we run iterative
classi   cation (relational classi   er) with a random node ordering for 10 iterations on all unlabeled
nodes (bootstrapped using the local classi   er). l2 id173 parameter and aggregation operator
(count vs. prop, see sen et al. (2008)) are chosen based on validation set performance for each dataset
separately.
lastly, we compare against planetoid (yang et al., 2016), where we always choose their best-
performing model variant (transductive vs. inductive) as a baseline.

6

published as a conference paper at iclr 2017

6 results

6.1 semi-supervised node classification

results are summarized in table 2. reported numbers denote classi   cation accuracy in percent. for
ica, we report the mean accuracy of 100 runs with random node orderings. results for all other
baseline methods are taken from the planetoid paper (yang et al., 2016). planetoid* denotes the best
model for the respective dataset out of the variants presented in their paper.

table 2: summary of results in terms of classi   cation accuracy (in percent).

method
manireg [3]
semiemb [28]
lp [32]
deepwalk [22]
ica [18]
planetoid* [29]
id197 (this paper)
id197 (rand. splits)

citeseer
60.1
59.6
45.3
43.2
69.1
64.7 (26s)
70.3 (7s)
67.9    0.5

cora
59.5
59.0
68.0
67.2
75.1
75.7 (13s)
81.5 (4s)
80.1    0.5

pubmed
70.7
71.1
63.0
65.3
73.9
77.2 (25s)
79.0 (38s)
78.9    0.7

nell
21.8
26.7
26.5
58.1
23.1
61.9 (185s)
66.0 (48s)
58.4    1.7

we further report wall-clock training time in seconds until convergence (in brackets) for our method
(incl. evaluation of validation error) and for planetoid. for the latter, we used an implementation pro-
vided by the authors3 and trained on the same hardware (with gpu) as our id197 model. we trained
and tested our model on the same dataset splits as in yang et al. (2016) and report mean accuracy
of 100 runs with random weight initializations. we used the following sets of hyperparameters for
citeseer, cora and pubmed: 0.5 (dropout rate), 5    10   4 (l2 id173) and 16 (number of hid-
den units); and for nell: 0.1 (dropout rate), 1    10   5 (l2 id173) and 64 (number of hidden
units).
in addition, we report performance of our model on 10 randomly drawn dataset splits of the same
size as in yang et al. (2016), denoted by id197 (rand. splits). here, we report mean and standard
error of prediction accuracy on the test set split in percent.

6.2 evaluation of propagation model

we compare different variants of our proposed per-layer propagation model on the citation network
datasets. we follow the experimental set-up described in the previous section. results are summa-
rized in table 3. the propagation model of our original id197 model is denoted by reid172
trick (in bold). in all other cases, the propagation model of both neural network layers is replaced
with the model speci   ed under propagation model. reported numbers denote mean classi   cation
accuracy for 100 repeated runs with random weight matrix initializations. in case of multiple vari-
ables   i per layer, we impose l2 id173 on all weight matrices of the    rst layer.

table 3: comparison of propagation models.

description
chebyshev    lter (eq. 5) k = 3
k = 2

1st-order model (eq. 6)
single parameter (eq. 7)
reid172 trick (eq. 8)
1st-order term only
multi-layer id88

(cid:80)k

propagation model

x  0 + d    1
(in + d    1

k=0 tk(   l)x  k
2 ad    1
2 ad    1
2 x  

2 x  1
2 )x  

  d    1
d    1

2   a   d    1
2 ad    1
x  

2 x  

citeseer cora pubmed
74.4
73.8

79.5
81.2

69.8
69.6

68.3
69.3
70.3

68.7
46.5

80.0
79.2
81.5

80.5
55.1

77.5
77.4
79.0

77.8
71.4

3https://github.com/kimiyoung/planetoid

7

published as a conference paper at iclr 2017

6.3 training time per epoch

here, we report results for the mean training
time per epoch (forward pass, cross-id178
calculation, backward pass) for 100 epochs on
simulated random graphs, measured in seconds
wall-clock time. see section 5.1 for a detailed
description of the random graph dataset used
in these experiments. we compare results on
a gpu and on a cpu-only implementation4 in
tensorflow (abadi et al., 2015). figure 2 sum-
marizes the results.

7 discussion

7.1 semi-supervised model

figure 2: wall-clock time per epoch for random
graphs. (*) indicates out-of-memory error.

in the experiments demonstrated here, our method for semi-supervised node classi   cation outper-
forms recent related methods by a signi   cant margin. methods based on graph-laplacian regular-
ization (zhu et al., 2003; belkin et al., 2006; weston et al., 2012) are most likely limited due to their
assumption that edges encode mere similarity of nodes. skip-gram based methods on the other hand
are limited by the fact that they are based on a multi-step pipeline which is dif   cult to optimize.
our proposed model can overcome both limitations, while still comparing favorably in terms of ef-
   ciency (measured in wall-clock time) to related methods. propagation of feature information from
neighboring nodes in every layer improves classi   cation performance in comparison to methods like
ica (lu & getoor, 2003), where only label information is aggregated.
we have further demonstrated that the proposed renormalized propagation model (eq. 8) offers both
improved ef   ciency (fewer parameters and operations, such as multiplication or addition) and better
predictive performance on a number of datasets compared to a na    ve 1st-order model (eq. 6) or
higher-order graph convolutional models using chebyshev polynomials (eq. 5).

7.2 limitations and future work

here, we describe several limitations of our current model and outline how these might be overcome
in future work.

memory requirement
in the current setup with full-batch id119, memory requirement
grows linearly in the size of the dataset. we have shown that for large graphs that do not    t in gpu
memory, training on cpu can still be a viable option. mini-batch stochastic id119 can
alleviate this issue. the procedure of generating mini-batches, however, should take into account the
number of layers in the id197 model, as the kth-order neighborhood for a id197 with k layers has to
be stored in memory for an exact procedure. for very large and densely connected graph datasets,
further approximations might be necessary.

directed edges and edge features our framework currently does not naturally support edge fea-
tures and is limited to undirected graphs (weighted or unweighted). results on nell however
show that it is possible to handle both directed edges and edge features by representing the original
directed graph as an undirected bipartite graph with additional nodes that represent edges in the
original graph (see section 5.1 for details).

limiting assumptions through the approximations introduced in section 2, we implicitly assume
locality (dependence on the kth-order neighborhood for a id197 with k layers) and equal impor-
tance of self-connections vs. edges to neighboring nodes. for some datasets, however, it might be
bene   cial to introduce a trade-off parameter    in the de   nition of   a:

4hardware used: 16-core intel r(cid:13) xeon r(cid:13) cpu e5-2640 v3 @ 2.60ghz, geforce r(cid:13) gtx titan x

  a = a +   in .

(11)

8

1k10k100k1m10m# edges10-310-210-1100101sec./epoch*gpucpupublished as a conference paper at iclr 2017

this parameter now plays a similar role as the trade-off parameter between supervised and unsuper-
vised loss in the typical semi-supervised setting (see eq. 1). here, however, it can be learned via
id119.

8 conclusion

we have introduced a novel approach for semi-supervised classi   cation on graph-structured data.
our id197 model uses an ef   cient layer-wise propagation rule that is based on a    rst-order approx-
imation of spectral convolutions on graphs. experiments on a number of network datasets suggest
that the proposed id197 model is capable of encoding both graph structure and node features in a
way useful for semi-supervised classi   cation. in this setting, our model outperforms several recently
proposed methods by a signi   cant margin, while being computationally ef   cient.

acknowledgments

we would like to thank christos louizos, taco cohen, joan bruna, zhilin yang, dave herman,
pramod sinha and abdul-saboor sheikh for helpful discussions. this research was funded by sap.

references
mart    n abadi et al. tensorflow: large-scale machine learning on heterogeneous systems, 2015.

james atwood and don towsley. diffusion-convolutional neural networks. in advances in neural

information processing systems (nips), 2016.

mikhail belkin, partha niyogi, and vikas sindhwani. manifold id173: a geometric frame-
work for learning from labeled and unlabeled examples. journal of machine learning research
(jmlr), 7(nov):2399   2434, 2006.

ulrik brandes, daniel delling, marco gaertler, robert gorke, martin hoefer, zoran nikoloski,
and dorothea wagner. on modularity id91. ieee transactions on knowledge and data
engineering, 20(2):172   188, 2008.

joan bruna, wojciech zaremba, arthur szlam, and yann lecun. spectral networks and locally
connected networks on graphs. in international conference on learning representations (iclr),
2014.

andrew carlson, justin betteridge, bryan kisiel, burr settles, estevam r. hruschka jr, and tom m.
mitchell. toward an architecture for never-ending language learning. in aaai, volume 5, pp. 3,
2010.

micha  el defferrard, xavier bresson, and pierre vandergheynst. convolutional neural networks on
graphs with fast localized spectral    ltering. in advances in neural information processing systems
(nips), 2016.

brendan l. douglas. the weisfeiler-lehman method and graph isomorphism testing. arxiv preprint

arxiv:1101.5211, 2011.

david k. duvenaud, dougal maclaurin, jorge iparraguirre, rafael bombarell, timothy hirzel, al  an
aspuru-guzik, and ryan p. adams. convolutional networks on graphs for learning molecular
   ngerprints. in advances in neural information processing systems (nips), pp. 2224   2232, 2015.

xavier glorot and yoshua bengio. understanding the dif   culty of training deep feedforward neural

networks. in aistats, volume 9, pp. 249   256, 2010.

marco gori, gabriele monfardini, and franco scarselli. a new model for learning in graph domains.
in proceedings. 2005 ieee international joint conference on neural networks., volume 2, pp.
729   734. ieee, 2005.

aditya grover and jure leskovec. node2vec: scalable id171 for networks. in proceedings
of the 22nd acm sigkdd international conference on knowledge discovery and data mining.
acm, 2016.

9

published as a conference paper at iclr 2017

david k. hammond, pierre vandergheynst, and r  emi gribonval. wavelets on graphs via spectral

id207. applied and computational harmonic analysis, 30(2):129   150, 2011.

kaiming he, xiangyu zhang, shaoqing ren, and jian sun. deep residual learning for image recog-

nition. in ieee conference on id161 and pattern recognition (cvpr), 2016.

thorsten joachims. transductive id136 for text classi   cation using support vector machines. in

international conference on machine learning (icml), volume 99, pp. 200   209, 1999.

diederik p. kingma and jimmy lei ba. adam: a method for stochastic optimization. in interna-

tional conference on learning representations (iclr), 2015.

yujia li, daniel tarlow, marc brockschmidt, and richard zemel. gated graph sequence neural

networks. in international conference on learning representations (iclr), 2016.

qing lu and lise getoor. link-based classi   cation. in international conference on machine learn-

ing (icml), volume 3, pp. 496   503, 2003.

laurens van der maaten and geoffrey hinton. visualizing data using id167. journal of machine

learning research (jmlr), 9(nov):2579   2605, 2008.

tomas mikolov, ilya sutskever, kai chen, greg s. corrado, and jeff dean. distributed repre-
sentations of words and phrases and their compositionality. in advances in neural information
processing systems (nips), pp. 3111   3119, 2013.

mathias niepert, mohamed ahmed, and konstantin kutzkov. learning convolutional neural net-

works for graphs. in international conference on machine learning (icml), 2016.

bryan perozzi, rami al-rfou, and steven skiena. deepwalk: online learning of social repre-
in proceedings of the 20th acm sigkdd international conference on knowledge

sentations.
discovery and data mining, pp. 701   710. acm, 2014.

franco scarselli, marco gori, ah chung tsoi, markus hagenbuchner, and gabriele monfardini.

the graph neural network model. ieee transactions on neural networks, 20(1):61   80, 2009.

prithviraj sen, galileo namata, mustafa bilgic, lise getoor, brian galligher, and tina eliassi-rad.

collective classi   cation in network data. ai magazine, 29(3):93, 2008.

nitish srivastava, geoffrey e. hinton, alex krizhevsky, ilya sutskever, and ruslan salakhutdinov.
dropout: a simple way to prevent neural networks from over   tting. journal of machine learning
research (jmlr), 15(1):1929   1958, 2014.

jian tang, meng qu, mingzhe wang, ming zhang, jun yan, and qiaozhu mei. line: large-scale
information network embedding. in proceedings of the 24th international conference on world
wide web, pp. 1067   1077. acm, 2015.

boris weisfeiler and a. a. lehmann. a reduction of a graph to a canonical form and an algebra

arising during this reduction. nauchno-technicheskaya informatsia, 2(9):12   16, 1968.

jason weston, fr  ed  eric ratle, hossein mobahi, and ronan collobert. deep learning via semi-

supervised embedding. in neural networks: tricks of the trade, pp. 639   655. springer, 2012.

zhilin yang, william cohen, and ruslan salakhutdinov. revisiting semi-supervised learning with

graph embeddings. in international conference on machine learning (icml), 2016.

wayne w. zachary. an information    ow model for con   ict and    ssion in small groups. journal of

anthropological research, pp. 452   473, 1977.

dengyong zhou, olivier bousquet, thomas navin lal, jason weston, and bernhard sch  olkopf.
learning with local and global consistency. in advances in neural information processing systems
(nips), volume 16, pp. 321   328, 2004.

xiaojin zhu, zoubin ghahramani, and john lafferty. semi-supervised learning using gaussian    elds
and id94. in international conference on machine learning (icml), volume 3,
pp. 912   919, 2003.

10

published as a conference paper at iclr 2017

a relation to weisfeiler-lehman algorithm

a neural network model for graph-structured data should ideally be able to learn representations of
nodes in a graph, taking both the graph structure and feature description of nodes into account. a
well-studied framework for the unique assignment of node labels given a graph and (optionally) dis-
crete initial node labels is provided by the 1-dim weisfeiler-lehman (wl-1) algorithm (weisfeiler
& lehmann, 1968):

algorithm 1: wl-1 algorithm (weisfeiler & lehmann, 1968)
input: initial node coloring (h(0)
output: final node coloring (h(t )
t     0;
repeat

2 , ..., h(0)
n )
, h(t )
, ..., h(t )
n )

1 , h(0)

1

2

(cid:16)(cid:80)

(cid:17)

j   ni h(t)

j

;

for vi     v do

i     hash
h(t+1)
t     t + 1;

until stable node coloring is reached;

denotes the coloring (label assignment) of node vi (at iteration t) and ni is its set of
here, h(t)
i
neighboring node indices (irrespective of whether the graph includes self-connections for every node
or not). hash(  ) is a hash function. for an in-depth mathematical discussion of the wl-1 algorithm
see, e.g., douglas (2011).
we can replace the hash function in algorithm 1 with a neural network layer-like differentiable
function with trainable parameters as follows:

      (cid:88)

j   ni

       ,

h(l+1)
i

=   

1
cij

h(l)
j w (l)

(12)

where cij is an appropriately chosen id172 constant for the edge (vi, vj). further, we can
take h(l)
now to be a vector of activations of node i in the lth neural network layer. w (l) is a
layer-speci   c weight matrix and   (  ) denotes a differentiable, non-linear activation function.
i

by choosing cij =(cid:112)didj, where di = |ni| denotes the degree of node vi, we recover the propaga-

tion rule of our graph convolutional network (id197) model in vector form (see eq. 2)5.
this   loosely speaking   allows us to interpret our id197 model as a differentiable and parameter-
ized generalization of the 1-dim weisfeiler-lehman algorithm on graphs.

a.1 node embeddings with random weights

from the analogy with the weisfeiler-lehman algorithm, we can understand that even an untrained
id197 model with random weights can serve as a powerful feature extractor for nodes in a graph. as
an example, consider the following 3-layer id197 model:

(cid:16)

(cid:16)

(cid:16)

  axw (0)(cid:17)

w (1)(cid:17)

w (2)(cid:17)

z = tanh

  a tanh

  a tanh

,

(13)

with weight matrices w (l) initialized at random using the initialization described in glorot & bengio
(2010).   a, x and z are de   ned as in section 3.1.
we apply this model on zachary   s karate club network (zachary, 1977). this graph contains 34
nodes, connected by 154 (undirected and unweighted) edges. every node is labeled by one of
four classes, obtained via modularity-based id91 (brandes et al., 2008). see figure 3a for an
illustration.

5note that we here implicitly assume that self-connections have already been added to every node in the

graph (for a clutter-free notation).

11

published as a conference paper at iclr 2017

(a) karate club network

(b) random weight embedding

figure 3: left: zachary   s karate club network (zachary, 1977), colors denote communities obtained
via modularity-based id91 (brandes et al., 2008). right: embeddings obtained from an un-
trained 3-layer id197 model (eq. 13) with random weights applied to the karate club network. best
viewed on a computer screen.

we take a featureless approach by setting x = in , where in is the n by n identity matrix. n is
the number of nodes in the graph. note that nodes are randomly ordered (i.e. ordering contains no
information). furthermore, we choose a hidden layer dimensionality6 of 4 and a two-dimensional
output (so that the output can immediately be visualized in a 2-dim plot).
figure 3b shows a representative example of node embeddings (outputs z) obtained from an un-
trained id197 model applied to the karate club network. these results are comparable to embeddings
obtained from deepwalk (perozzi et al., 2014), which uses a more expensive unsupervised training
procedure.

a.2 semi-supervised node embeddings

on this simple example of a id197 applied to the karate club network it is interesting to observe how
embeddings react during training on a semi-supervised classi   cation task. such a visualization (see
figure 4) provides insights into how the id197 model can make use of the graph structure (and of
features extracted from the graph structure at later layers) to learn embeddings that are useful for a
classi   cation task.
we consider the following semi-supervised learning setup: we add a softmax layer on top of our
model (eq. 13) and train using only a single labeled example per class (i.e. a total number of 4 labeled
nodes). we train for 300 training iterations using adam (kingma & ba, 2015) with a learning rate
of 0.01 on a cross-id178 loss.
figure 4 shows the evolution of node embeddings over a number of training iterations. the model
succeeds in linearly separating the communities based on minimal supervision and the graph struc-
ture alone. a video of the full training process can be found on our website7.

6we originally experimented with a hidden layer dimensionality of 2 (i.e. same as output layer), but observed
that a dimensionality of 4 resulted in less frequent saturation of tanh(  ) units and therefore visually more
pleasing results.

7http://tkipf.github.io/graph-convolutional-networks/

12

published as a conference paper at iclr 2017

(a) iteration 25

(b) iteration 50

(c) iteration 75

(d) iteration 100

(e) iteration 200

(f) iteration 300

figure 4: evolution of karate club network node embeddings obtained from a id197 model after a
number of semi-supervised training iterations. colors denote class. nodes of which labels were
provided during training (one per class) are highlighted (grey outline). grey links between nodes
denote graph edges. best viewed on a computer screen.

13

published as a conference paper at iclr 2017

b experiments on model depth

in these experiments, we investigate the in   uence of model depth (number of layers) on classi   cation
performance. we report results on a 5-fold cross-validation experiment on the cora, citeseer and
pubmed datasets (sen et al., 2008) using all labels. in addition to the standard id197 model (eq. 2),
we report results on a model variant where we use residual connections (he et al., 2016) between
hidden layers to facilitate training of deeper models by enabling the model to carry over information
from the previous layer   s input:

(cid:16)

2 h (l)w (l)(cid:17)

h (l+1) =   

  d    1

2   a   d    1

+ h (l) .

(14)

on each cross-validation split, we train for 400 epochs (without early stopping) using the adam
optimizer (kingma & ba, 2015) with a learning rate of 0.01. other hyperparameters are chosen as
follows: 0.5 (dropout rate,    rst and last layer), 5    10   4 (l2 id173,    rst layer), 16 (number
of units for each hidden layer) and 0.01 (learning rate). results are summarized in figure 5.

figure 5: in   uence of model depth (number of layers) on classi   cation performance. markers
denote mean classi   cation accuracy (training vs. testing) for 5-fold cross-validation. shaded areas
denote standard error. we show results both for a standard id197 model (dashed lines) and a model
with added residual connections (he et al., 2016) between hidden layers (solid lines).

for the datasets considered here, best results are obtained with a 2- or 3-layer model. we observe
that for models deeper than 7 layers, training without the use of residual connections can become
dif   cult, as the effective context size for each node increases by the size of its kth-order neighbor-
hood (for a model with k layers) with each additional layer. furthermore, over   tting can become
an issue as the number of parameters increases with model depth.

14

12345678910number of layers0.500.550.600.650.700.750.800.850.90accuracyciteseertraintrain (residual)testtest (residual)12345678910number of layers0.550.600.650.700.750.800.850.900.95accuracycoratraintrain (residual)testtest (residual)12345678910number of layers0.760.780.800.820.840.860.88accuracypubmedtraintrain (residual)testtest (residual)