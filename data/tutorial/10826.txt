polyglot: distributed word representations for multilingual nlp

rami al-rfou

bryan perozzi

steven skiena

computer science dept. stony brook university stony brook, ny 11794

{ralrfou, bperozzi, skiena}@cs.stonybrook.edu

4
1
0
2

 

n
u
j
 

7
2

 
 
]
l
c
.
s
c
[
 
 

2
v
2
6
6
1

.

7
0
3
1
:
v
i
x
r
a

abstract

distributed word representations (word
embeddings) have recently contributed
to competitive performance in language
modeling and several nlp tasks.
in
this work, we train id27s for
more than 100 languages using their cor-
responding wikipedias. we quantitatively
demonstrate the utility of our word em-
beddings by using them as the sole fea-
tures for training a part of speech tagger
for a subset of these languages. we    nd
their performance to be competitive with
near state-of-art methods in english, dan-
ish and swedish. moreover, we inves-
tigate the semantic features captured by
these embeddings through the proximity
of word groupings. we will release these
embeddings publicly to help researchers in
the development and enhancement of mul-
tilingual applications.

1

introduction

building multilingual processing systems is a
challenging task. every nlp task involves dif-
ferent stages of preprocessing and calculating in-
termediate representations that will serve as fea-
tures for later stages. these stages vary in com-
plexity and requirements for each individual lan-
guage. despite recent momentum towards devel-
oping multilingual tools (nivre et al., 2007; haji  c
et al., 2009; pradhan et al., 2012), most of nlp
research still focuses on rich resource languages.
common nlp systems and tools rely heavily on
english speci   c features and they are infrequently
tested on multiple datasets. this makes them hard
to port to new languages and tasks (blitzer et al.,
2006).

a serious bottleneck in the current approach
for developing multilingual systems is the require-

ment of familiarity with each language under con-
sideration. these systems are typically carefully
tuned with hand-manufactured features designed
by experts in a particular language. this approach
can yield good performance, but tends to create
complicated systems which have limited portabil-
ity to new languages, in addition to being hard to
enhance and maintain.

recent advancements in unsupervised feature
learning present an intriguing alternative.
in-
stead of relying on expert knowledge, these ap-
proaches employ automatically generated task-
independent features (or id27s) given
large amounts of plain text. recent developments
have led to state-of-art performance in several
nlp tasks such as id38 (bengio
et al., 2006; mikolov et al., 2010), and syntactic
tasks such as sequence tagging (collobert et al.,
2011). these embeddings are generated as a result
of training    deep    architectures, and it has been
shown that such representations are well suited for
id20 tasks (glorot et al., 2011; chen
et al., 2012).

we believe two problems have held back the
research community   s adoption of these methods.
the    rst is that learning representations of words
involves huge computational costs. the process
usually involves processing billions of words over
weeks. the second is that so far, these systems
have been built and tested mainly on english.

in this work we seek to remove these barriers
to entry by generating id27s for over
a hundred languages using state-of-the-art tech-
niques. speci   cally, our contributions include:

    id27s - we will release word
embeddings for the hundred and seventeen
languages that have more than 10,000 ar-
ticles on wikipedia. each language   s vo-
cabulary will contain up to 100,000 words.
the embeddings will be publicly available at

(www.cs.stonybrook.edu/  dsl), for
the research community to study their charac-
teristics and build systems for new languages.
we believe our embeddings represent a valu-
able resource because they contain a minimal
amount of id172. for example, we
do not lower case words for european lan-
guages as other studies have done for en-
glish. this preserves features of the under-
lying language.

    quantitative analysis

- we investigate
the embedding   s performance on a part-of-
speech (pos) tagging task, and conduct qual-
itative investigation of the syntactic and se-
mantic features they capture. our experi-
ments represent a valuable chance to evalu-
ate distributed word representations for nlp
as the experiments are conducted in a consis-
tent manner and a large number of languages
are covered. as the embeddings capture in-
teresting linguistic features, we believe the
multilingual resource we are providing gives
researchers a chance to create multilingual
comparative experiments.

    ef   cient implementation - training these
models was made possible by our contri-
butions to theano (machine learning library
(bergstra et al., 2010)). these optimizations
empower researchers to produce word em-
beddings under different settings or for dif-
ferent corpora than wikipedia.

the rest of this paper is as follows. in section
2, we give an overview of semi-supervised learn-
ing and learning representations related work. we
then describe, in section 3, the network used to
generate the id27s and its characteris-
tics. section 4 discusses the details of the corpus
collection and preparation steps we performed.
next, in section 5, we discuss our experimental
setup and the training progress over time. in sec-
tion 6 we discuss the semantic features captured
by the embeddings by showing examples of the
word groupings in multiple languages. finally,
in section 7 we demonstrate the quality of our
learned features by training a pos tagger on sev-
eral languages and then conclude.

2 related work
there is a large body of work regarding semi-
supervised techniques which integrate unsuper-

vised id171 with discriminative learning
methods to improve the performance of nlp ap-
plications. word id91 has been used to learn
classes of words that have similar semantic fea-
tures to improve id38 (brown et al.,
1992) and knowledge transfer across languages
(t  ackstr  om et al., 2012). id33
and other nlp tasks have been shown to bene-
   t from such a large unannotated corpus (koo et
al., 2008), and a variety of unsupervised feature
learning methods have been shown to unilaterally
improve the performance of supervised learning
tasks (turian et al., 2010).
(klementiev et al.,
2012) induce distributed representations for a pair
of languages jointly, where a learner can be trained
on annotations present in one language and ap-
plied to test data in another.

learning distributed word representations is a
way to learn effective and meaningful information
about words and their usages. they are usually
generated as a side effect of training parametric
language models as probabilistic neural networks.
training these models is slow and takes a signif-
icant amount of computational resources (bengio
et al., 2006; dean et al., 2012). several sugges-
tions have been proposed to speed up the training
procedure, either by changing the model architec-
ture to exploit an algorithmic speedup (mnih and
hinton, 2009; morin and bengio, 2005) or by esti-
mating the error by sampling (bengio and senecal,
2008).

(collobert and weston, 2008) shows that word
embeddings can almost substitute nlp common
features on several tasks. the system they built,
senna, offers id52, chunking,
id39, id14
and id33 (collobert, 2011). the
system is built on top of id27s and per-
forms competitively compared to state of art sys-
tems. in addition to pure performance, the system
has a faster execution speed than comparable nlp
pipelines (al-rfou    and skiena, 2012).

to speed up the embedding generation process,
senna embeddings are generated through a pro-
cedure that is different from id38.
the representations are acquired through a model
that distinguishes between phrases and corrupted
versions of them. in doing this, the model avoids
the need to normalize the scores across the vocab-
ulary to infer probabilities.
(chen et al., 2013)
shows that the embeddings generated by senna

apple
dell
paramount
mac
flex

bush

apple
tomato kennedy
roosevelt
bean
nixon
onion
potato
fisher

bush
jungle
lobster
sponge
mud

corpora
notations
digraphs
usages
derivations

dangerous
costly
chaotic
bizarre
destructive

table 1: words nearest neighbors as they appear in the english embeddings.

perform well in a variety of term-based evaluation
tasks. given the training speed and prior perfor-
mance on nlp tasks in english, we generate our
multilingual embeddings using a similar network
architecture to the one senna used.

however, our work differs from senna in the
following ways. first, we do not limit our mod-
els to english, we train embeddings for a hundred
and seventeen languages. next, we preserve lin-
guistic features by avoiding excessive normaliza-
tion to the text. for example, our english model
places    apple    closer to it companies and    ap-
ple    to fruits. more examples of linguistic fea-
tures preserved by our model are shown in table
1. this gives us the chance to evaluate the embed-
dings performance over id52 without the
need for manufactured features. finally, we re-
lease the embeddings and the resources necessary
to generate them to the community to eliminate
any barriers.

despite the progress made in creating dis-
tributed representations, combining them to pro-
duce meaning is still a challenging task. sev-
eral approaches have been proposed to address
feature compositionality for semantic problems
such as paraphrase detection (socher et al., 2011),
and id31 (socher et al., 2012) using
id27s.

3 distributed word representation

distributed word representations (word embed-
dings) map the index of a word in a dictionary to a
feature vector in high-dimension space. every di-
mension contributes to multiple concepts, and ev-
ery concept is expressed by a combination of sub-
set of dimensions. such mapping is learned by
back-propagating the error of a task through the
model to update random initialized embeddings.
the task is usually chosen such that examples can
be automatically generated from unlabeled data
(i.e so it is unsupervised).
in case of language
modeling, the task is to predict the last word of
a phrase that consists of n words.

in our work, we start from the example con-
struction method outlined in (bengio et al., 2009).
they train a model by requiring it to distinguish
between the original phrase and a corrupted ver-
sion of the phrase.
if it does not score the
original one higher than the corrupted one (by
a margin), the model will be penalized. more
precisely, for a given sequence of words s =
[wi   n . . . wi . . . wi+n] observed in the corpus t ,
we will construct another corrupted sequence s(cid:48)
by replacing the word in the middle wi with a word
wj chosen randomly from the vocabulary. the
neural network represents a function score that
scores each phrase, the model is penalized through
the hinge id168 j(t ) as shown in 1.

j(t ) =

1
|t|

|1   score(s(cid:48))+score(s)|+ (1)

(cid:88)

i   t

figure 1 shows a neural network that takes a se-
quence of words with size 2n + 1 to compute a
score. first, each word is mapped through a vo-
cabulary dictionary with the size |v | to an index
that is used to index a shared matrix c with the
size |v |    m where m is the size of the vector rep-
resenting the word. once the vectors are retrieved,
they are concatenated into one vector called pro-
jection layer p with size (2n + 1)     m. the pro-
jection layer plays the role of an input to a hidden
layer with size |h|, the activations a of which are
calculated according to equation 3, where w1, b1
are the weights and bias of the hidden layer.

a = tanh(w1p + b1)

(2)
to calculate the phrase score, a linear combina-
tion of the hidden layer activations a is computed
using w2 and b2.

score(p ) = w2a + b2

(3)

therefore, the    ve parameters that have to be
learned are w1, w2, b1, b2, c with a total number
of parameters (2n + 1)     m     h + h + h + 1 +
|v |     m     m     (nh + |v |) .

gine1. next we must tokenize the text. we rely
on an opennlp probabilistic tokenizer whenever
possible, and default to the unicode text segmen-
tation2 algorithm offered by lucene when we have
no such opennlp model. after id121, we
normalize the tokens to reduce their sparsity. we
have two main id172 rules. the    rst re-
places digits with the symbol #, so    1999    be-
comes ####. in the second, we remove hyphens
and brackets that appear in the middle of a token.
as an additional rule for english, we map non-
latin characters to their unicode block groups.

in order to capture the syntactic and semantic
features of words, we must observe each word sev-
eral times in each of its valid contexts. this re-
quirement, when combined with the zip   an dis-
tribution of words in natural language, implies that
learning a meaningful representation of a language
requires a huge amount of unstructured text.
in
practice we deal with this limitation by restricting
ourselves to considering the most frequently oc-
curring tokens in each language.

table 2 shows the size of each language corpus
in terms of tokens, number of word types and cov-
erage of text achieved by building a vocabulary out
of the most frequent 100,000 tokens, |v |. out of
vocabulary (oov) words are replaced with a spe-
cial token (cid:104)unk(cid:105).

while wikipedia has 284 language speci   c en-
cyclopedias, only    ve of them have more than a
million articles. the size drops dramatically, such
that the 42nd largest wikipedia, hindi, has slightly
above 100,000 articles and the 100th, tatar, has
slightly over 16,000 articles3.

signi   cant wikipedias in size have a word cov-
erage over 92% except for german, russian, ara-
bic and czech which shows the effect of heavy us-
age of morphological forms in these languages on
the word usage distribution.

the highest word coverage we achieve is unsur-
prisingly for chinese. this is expected given the
limited size vocabulary of the language - the num-
ber of entries in the contemporary chinese dictio-
nary are estimated to be 65 thousand words (shux-
iang, 2004).

1java wikipedia api (bliki engine) - http://code.

google.com/p/gwtwiki/

2http://www.unicode.org/reports/tr29/
3http://meta.wikimedia.org/w/index.

php?title=list_of_wikipedias&oldid=
5248228

figure 1: neural network architecture. words are
retrieved from embeddings matrix c and concate-
nated at the projection layer as an input to com-
puter the hidden layer activation. the score is
the linear combination of the activation values of
the hidden layer. the scores of two phrases are
ranked according to hinge loss to distinguish the
corrupted phrase from the original one.

4 corpus preparation

we have chosen to generate our id27s
from wikipedia. in addition to size, there are other
desirable properties that we wish for the source of
our language model to have:

    size and variety of languages - as of this
writing (april, 2013), 42 languages had more
than 100,000 article pages, and 117 lan-
guages had more than 10,000 article pages.
    well studied - wikipedia is a proli   c re-
source in the literature, and has been used
for a variety of problems.
particularly,
wikipedia is well suited for multilingual ap-
plications (navigli and ponzetto, 2010).
    quality - wikipedians strive to write arti-
cles that are readable, accurate, and consist
of good grammar.
    openly accessible - wikipedia is a resource
    growing - as technology becomes more ac-
cessible, the size and scope of the multilin-
gual wikipedia effort continues to expand.

available for free use by researchers

to process wikipedia markup, we    rst extract
the text using a modi   ed version of the bliki en-

cimaginationciscgreatercthancdetailscorehidden layerhcm|v|projection layerlanguage

english
german
french
spanish
russian
italian
portuguese
dutch
chinese
swedish
czech
arabic
danish
bulgarian
slovene
hindi

tokens words coverage
   106
96.30%
1,888
91.78%
687
473
95.78%
96.07%
399
90.43%
328
95.52%
322
197
95.68%
93.81%
197
99.67%
196
92.36%
101
80
91.84%
91.78%
52
93.68%
44
39
94.35%
94.42%
30
23
96.25%

   103
12,125
9,474
4,675
3,978
5,959
3,642
2,870
3,712
423
2,707
2,081
1,834
1,414
1,114
920
702

table 2: statistics of a subset of the languages pro-
cessed. the second column reports the number of
tokens found in the corpus in millions while the
third column reports the word types found in thou-
sands. the coverage indicates the percentage of
the corpus that will be matching words in a vocab-
ulary consists of the most frequent 100 thousand
words.

5 training

for our experiments, we build a model as the one
described in section 3 using theano (bergstra et
al., 2010). we choose the following parameters,
context window size 2n + 1 = 5, vocabulary
|v | = 100, 000, id27 size m = 64,
and hidden layer size h = 32. the intuition, here,
is to maximize the relative size of the embeddings
compared to the rest of the network. this might
force the model to store the necessary information
in the embeddings matrix instead of the hidden
layer. another bene   t is that we will avoid over-
   tting on the smaller wikipedias. increasing the
window size or the embedding size slows down
the training speed, making it harder to converge
within a reasonable time.

the examples are generated by sweeping a win-
dow over sentences. for each sentence in the cor-
pus, all unknown words are replaced with a special
token (cid:104)unk(cid:105) and sentences are padded with (cid:104)s(cid:105),
(cid:104)/s(cid:105) tokens. in case the window exceeds the edges
of a sentence, the missing slots are    lled with our
padding token, (cid:104)pad(cid:105).

figure 2: training and test errors of the french
model after 23 days of training. we did not notice
any over   tting while training the model. the error
curves are smoother the larger the language corpus
is.

to train the model, we consider the data in mini-
batches of size 16. every 16 examples, we es-
timate the gradient using stochastic gradient de-
scent (bottou, 1991), and update the parameters
which contributed to the error using backpropaga-
tion (rumelhart et al., 2002). calculating an exact
gradient is prohibitive given that the dataset size is
in millions of examples. we calculate the devel-
opment error by sampling randomly 10000 mini-
batches from the development dataset.

for each language, we set the batch size to 16
examples, and the learning rate to be 0.1. follow-
ing, (collobert et al., 2011)   s advice, we divide
each layer by the fan in of that layer, and we con-
sider the embeddings layer to have a fan in of 1.
we divide the corpus to three sets, training, devel-
opment and testing with the following percentages
90, 5, 5 respectively.

one disadvantage of the approach used by (col-
lobert et al., 2011) is that there is no clear stop-
ping criteria for the model training process. we
have noticed that after a few weeks of training,
the model   s performance reaches the point where
there is no signi   cant decrease in the average loss
over the development set, and when this occurs we
manually stop the training. an interesting prop-
erty of this model is that we did not notice any
sign of over   tting for large wikipedias. this could
be explained by the in   nite amount of examples
we can generate by randomly choosing the re-

word
id8
juane
rose
blanc
orange
id7
jkr(cid:0)
   jkr(cid:0)
(cid:11)(cid:24)ya(cid:11)  
jkr(cid:0)(cid:190)
   jkr(cid:0)(cid:190)
(cid:155)r(cid:23)ba

translation
red
yellow
pink
white
orange
blue

thanks
and thanks
greetings
thanks + diacritic
and thanks + diacritic
hello

          
                
              
            
            
                

putin
yanukovych
trotsky
hitler
stalin
medvedev

word
dentista
peluquero
ginec  olog
camionero
oftalm  ologo
telegra   sta
   (cid:152)d(cid:0)(cid:160)
(cid:0)(cid:7)na(cid:160)
   (cid:152)d  (cid:159)
vf   (cid:160)
(cid:0)(cid:7)ny(cid:159)
(cid:0)(cid:7)nta(cid:160)
id68
dongzhi
chunfen
xiazhi
qiufen
ziye
chuxi

translation
dentist
barber
gynecologist
truck driver
ophthalmologist
telegraphist

two boys
two sons
two boys
two children
two sons
two daughters

winter solstice
vernal equinox
summer solstice
autumnal equinox
midnight
new year   s eve

h
s
i
l
g
n
e

n
a
m
r
e
g

n
a
i
l
a
t
i

word
mumbai
chennai
bangalore
kolkata
cairo
hyderabad

word
bombay
madras
shanghai
calultta
bangkok
hyderabad

eisenbahnbetrieb
fahrbetrieb
reisezugverkehr
f  ahrverkehr
handelsverkehr
sch  ulerverkehr

rail operations
driving
passenger trains
ferries
trade
students transport

papa
papa
ponte   ce
basileus
canridnale
frate

pope
pope
pontiff
basileus
cardinal
friar

h
s
i

n
a
p
s

c
i

b
a
r
a

e
s
e
n

i

h
c

h
c
n
e
r
f

c
i

b
a
r
a

n
a
i
s
s
u
r

table 3: examples of the nearest    ve neighbors of every word in several languages. translation is
retrieved from http://translate.google.com.

placement word in the corrupted phrase. figure
2 shows a typical learning curve of the training.
as the number of examples have been seen so far
increased both the training error and the develop-
ment error go down.

6 qualitative analysis

in order to understand how the embeddings space
is organized, we examine the subtle information
captured by the embeddings through investigating
the proximity of word groups. this information
has the potential to help researchers develop ap-
plications that use such semantic and syntactic in-
formation. the embeddings not only capture syn-
tactic features, as we will demonstrate in section
4, but also demonstrate the ability to capture in-
teresting semantic information. table 3 shows dif-
ferent words in several languages. for each word
on top of each list, we rank the vocabulary accord-
ing to their euclidean distance from that word and
show the closest    ve neighboring words.
    french & spanish - expected groupings of
    english - the example shows how the em-
bedding space is aware of the name change
that happened to a group of indian cities.
   mumbai    used to be called    bombay   ,
   chennai    used to be called    madras and
   kolkata    used to be called    calcutta   . on
the other hand,    hyderabad    stayed at a sim-
ilar distance from both names as they point to
the same conceptual meaning.
    arabic - the    rst example shows the word
   thanks   . despite not removing the diacrit-

colors and professions is clearly observed.

ics from the text, the model learned that the
two surface forms of the word mean similar
things and, therefore, grouped them together.
in arabic, conjunction words do not get sepa-
rated from the following word. usually,    and
thanks    serves as a letter signature as    sin-
cerely    is used in english. the model learned
that both words {   and thanks   ,    thanks    }
are similar, regardless their different forms.
the second example illustrates a speci   c syn-
tactic morphological feature of arabic, where
enumeration of couples has its own form.
    german - the example demonstrates that the
id152 of multi-unit words
are still preserved.
    russian - the model learned to group rus-
sian/soviet leaders and other    gures related
to the soviet history together.
    chinese - the list contains three solar terms
that are part of the traditional east asian lu-
nisolar calendars. the remaining two terms
correspond to traditional holidays that occur
at the same dates of these solar terms.
    italian - the model learned that the lower
and upper cases of the word has similar
meaning.

7 sequence tagging

here we analyze the quality of the models we have
generated. to test the quantitative performance of
the embeddings, we use them as the sole features
for a well studied nlp task, part of speech tag-
ging.

to demonstrate the capability of the learned dis-

language

source
tiger    (brants et al., 2002)
btb    (simov et al., 2002)
pdt 2.5 (bej  cek et al., 2012)
ddt    (kromann, 2003)
alpino    (van der beek et al., 2002)
penntreebank (marcus et al., 1993)

german
bulgarian
czech
danish
dutch
english
portuguese sint(c)tica    (afonso et al., 2002)
slovene
swedish

sdt    (d  zeroski et al., 2006)
talbanken05    (nivre et al., 2006)

test

unknown known

all

tnt
89.17% 98.60% 97.85% 98.10%
75.74% 98.33% 96.33% 97.50%
71.98% 99.15% 97.13% 99.10%
73.03% 98.07% 96.45% 96.40%
73.47% 95.85% 93.86% 95.00%
75.97% 97.74% 97.18% 96.80%
75.36% 97.71% 95.95% 96.80%
68.82% 95.17% 93.46% 94.60%
83.54% 95.77% 94.68% 94.70%

table 4: results of our model against several pos datasets. the performance is measured using accuracy
over the test datasets. third column represents the total accuracy of the tagger the former two columns
reports the accuracy over known words and oov words (unknown). the results are compared to the
tnt tagger results reported by (petrov et al., 2012).
   conll 2006 dataset

tributed representations in extracting useful word
features, we train a pos tagger over the subset of
languages that we were able to acquire free anno-
tated resources for. we choose our tagger for this
task to be a neural network because it has a fast
convergence rate based on our initial experiments.
the part of speech tagger has similar architec-
ture to the one used for training the embeddings.
however we have changed some of the network
parameters, speci   cally, we use a hidden layer of
size 300 and learning rate of 0.3. the network is
trained by minimizing the negative of the log like-
lihood of the labeled data. to tag a speci   c word
wi we consider a window with size 2n where n
in our experiment is equal to 2. equation 4 shows
how we construct a feature vector f by concate-
nating (   ) the embeddings of the words occurred
in the window, where c is the matrix that contains
the embeddings of the language vocabulary.

f =

c[wj]

(4)

i+2(cid:77)

j=i   2

the feature vector will be fed to the network and
the error will back propagated back to the embed-
dings.

the results of this experiment are presented in
table 4. we train and test our models on the uni-
versal tagset proposed by (petrov et al., 2012).
this universal tagset maps each original tag in a
treebank to one out of twelve general pos tags.
this simpli   es the comparison of classi   ers per-
formance across languages. we compare our re-
sults to a similar experiment conducted in their

work, where they trained a tnt tagger (brants,
2000) on several treebanks. the tnt tagger is
based on markov models and depends on trigram
counts observed in the labeled data. it was cho-
sen for its fast speed and (near to) state-of-the-art
accuracy, without language speci   c tuning.

the performance of embeddings is competitive
in general. surprisingly, it is doing better than the
tnt tagger in english and danish. moreover, our
performance is so close in the case of swedish.
this task is hard for our tagger for two reasons.
the    rst is that we do not add oov words seen
during training of the tagger to our vocabulary.
the second is that all oov words are substituted
with one representation, (cid:104)unk(cid:105) and there is no
character level information used to inform the tag-
ger about the characteristic of the oov words.

on the other hand,

the performance on the
known words is strong and consistent showing the
value of the features learned about these words
from the unsupervised stage. although the word
coverage of german and czech are low in the orig-
inal wikipedia corpora (see table 2), the features
learned are achieving great accuracy on the known
words. they both achieve above 98.5% accuracy.
it is noticeable that the slovene model performs
the worst, under both known and unknown words
categories. it achieves only 93.46% accuracy on
the test dataset. given that the slovene embed-
dings were trained on the least amount of data
among all other embeddings we test here, we ex-
pect the quality to go lower for the other smaller
wikipedias not tested here.

in table 5, we present how well the vocabulary
of each language   s embeddings covered the part of
speech datasets. the datasets come from a differ-
ent domain than wikipedia, and this is re   ected in
the results.

in table 6, we present the results of training the
same neural network part of speech tagger with-
out using our embeddings as initializations. we
found that the embeddings bene   ted all the lan-
guages we considered, and observed the greatest
bene   t in languages which had a small number of
training examples. we believe that these results
illustrate the performance

language % token % word
coverage coverage
77.70
bulgarian
65.61
czech
80.03
danish
german
60.68
79.73
english
77.76
dutch
72.66
portuguese
slovene
83.67
73.92
swedish

94.58
95.37
95.41
94.04
98.06
96.25
94.09
95.33
95.87

table 5: coverage statistics of the embedding   s
vocabulary on the part of speech datasets after nor-
malization. token coverage is the raw percentage
of words which were known, while the word cov-
erage ignores repeated words.

8 conclusion

distributed word representations represent a valu-
able resource for any language, but particularly for
resource-scarce languages. we have demonstrated
how id27s can be used as off-the-shelf
solution to reach near to state-of-art performance
over a fundamental nlp task, and we believe that
our embeddings will help researchers to develop
tools in languages with which they have no exper-
tise.

moreover, we showed several examples of in-
teresting semantic relations expressed in the em-
beddings space that we believe will lead to inter-
esting applications and improve tasks as semantic
compositionality.

while we have only considered the properties of
id27s as features in this work, it has
been shown that using id27s in con-
junction with traditional nlp features can signi   -

# training accuracy
examples
200,049
1,239,687

language

bulgarian
czech
danish
german
english
dutch
portuguese
slovene
swedish

96,581
735,826
950,561
208,418
212,749
27,284
199,509

drop
-2.01%
-0.86%
-1.77%
-0.89%
-0.25%
-1.37%
-0.91%
-2.68%
-0.82%

table 6: accuracy of randomly initialized tag-
ger compared to our results. using the embed-
dings was generally helpful, especially in lan-
guages where we did not have many training ex-
amples. the scores presented are the best we
found for each language (languages with more re-
sources could afford to train longer before over   t-
ting).

cantly improve results on nlp tasks (turian et al.,
2010; collobert et al., 2011). with this in mind,
we believe that the entire research community can
bene   t from our release of id27s for
over 100 languages.

we hope that these resources will advance the
study of possible pair-wise mappings between em-
beddings of several languages and their relations.
our future work in this area includes improving
the models by increasing the size of the context
window and their domain adaptivity through in-
corporating other sources of data. we will be
investigating better strategies for modeling oov
words. we see improvements to oov word han-
dling as essential to ensure robust performance of
the embeddings on real-world tasks.

acknowledgments
this research was partially supported by nsf
grants dbi-1060572 and iis-1017181, with ad-
ditional support from texeltek.

references
susana afonso, eckhard bick, renato haber, and di-
ana santos. 2002. floresta sint  a (c) tica   : a treebank
for portuguese. in proc. of the third intern. conf. on
language resources and evaluation (lrec), pages
1698   1703.

rami al-rfou    and steven skiena. 2012. speedread:
in pro-

a fast id39 pipeline.

ceedings of the 24th international conference on
computational linguistics (coling 2012), pages 53   
61, mumbai, india, december. coling 2012 orga-
nizing committee.

joelle pineau, editors, proceedings of the 29th inter-
national conference on machine learning (icml-
12), icml    12, pages 767   774. acm, new york,
ny, usa, july.

eduard bej  cek, jarmila panevov  a, jan popelka, pavel
stra  n  ak, magda   sev  c    kov  a, jan   st  ep  anek, and
zden  ek   zabokrtsk  y.
2012. prague dependency
treebank 2.5     a revisited version of pdt 2.0.
in proceedings of coling 2012, pages 231   246,
mumbai, india, december. the coling 2012 or-
ganizing committee.

yoshua bengio and j-s senecal. 2008. adaptive im-
portance sampling to accelerate training of a neu-
ral probabilistic language model. neural networks,
ieee transactions on, 19(4):713   722.

y. bengio, h. schwenk, j.s. sen  ecal, f. morin, and j.l.
gauvain. 2006. neural probabilistic language mod-
els. innovations in machine learning, pages 137   
186.

y. bengio, j. louradour, r. collobert, and j. weston.
2009. curriculum learning. in international con-
ference on machine learning, icml.

james bergstra, olivier breuleux, fr  ed  eric bastien,
pascal lamblin, razvan pascanu, guillaume des-
jardins, joseph turian, david warde-farley, and
a cpu and
yoshua bengio.
in proceedings
gpu math expression compiler.
of the python for scienti   c computing conference
(scipy), june. oral presentation.

theano:

2010.

john blitzer, ryan mcdonald, and fernando pereira.
2006. id20 with structural correspon-
dence learning. in conference on empirical meth-
ods in natural language processing, sydney, aus-
tralia.

l  eon bottou. 1991. stochastic gradient learning in
in proceedings of neuro-n    mes

neural networks.
91, nimes, france. ec2.

sabine brants, stefanie dipper, silvia hansen, wolf-
gang lezius, and george smith. 2002. the tiger
treebank. in in proceedings of the work-
shop on treebanks and linguistic theo-
ries, pages 24   41.

thorsten brants.
2000. tnt: a statistical part-of-
in proceedings of the sixth confer-
speech tagger.
ence on applied natural language processing, pages
224   231. association for computational linguis-
tics.

peter f brown, peter v desouza, robert l mercer,
vincent j della pietra, and jenifer c lai. 1992.
class-based id165 models of natural
language.
computational linguistics, 18(4):467   479.

yanqing chen, bryan perozzi, rami al-rfou   , and
steven skiena. 2013. the expressive power of word
embeddings. corr, abs/1301.3226.

r. collobert and j. weston. 2008. a uni   ed architec-
ture for natural language processing: deep neural
networks with multitask learning. in international
conference on machine learning, icml.

ronan collobert, jason weston, l  eon bottou, michael
karlen, koray kavukcuoglu, and pavel kuksa.
2011. natural language processing (almost) from
j. mach. learn. res., 12:2493   2537,
scratch.
november.

ronan collobert. 2011. deep learning for ef   cient

discriminative parsing. in aistats.

jeffrey dean, greg corrado, rajat monga, kai chen,
matthieu devin, quoc le, mark mao, marc   aurelio
ranzato, andrew senior, paul tucker, ke yang, and
andrew ng. 2012. large scale distributed deep net-
works. in p. bartlett, f.c.n. pereira, c.j.c. burges,
l. bottou, and k.q. weinberger, editors, advances
in neural information processing systems 25, pages
1232   1240.

sa  so d  zeroski, toma  z erjavec, nina ledinek, petr pa-
jas, zdenek   zabokrtsky, and andreja   zele. 2006.
towards a slovene dependency treebank. in proc. of
the fifth intern. conf. on language resources and
evaluation (lrec).

xavier glorot, antoine bordes, and yoshua bengio.
2011. id20 for large-scale sentiment
in pro-
classi   cation: a deep learning approach.
ceedings of the twenty-eight international confer-
ence on machine learning (icml   11), volume 27,
pages 97   110, june.

jan haji  c, massimiliano ciaramita, richard johans-
son, daisuke kawahara, maria ant`onia mart    , llu    s
m`arquez, adam meyers, joakim nivre, sebastian
pad  o, jan   st  ep  anek, pavel stra  n  ak, mihai surdeanu,
nianwen xue, and yi zhang. 2009. the conll-
2009 shared task: syntactic and semantic depen-
in proceedings of
dencies in multiple languages.
the 13th conference on computational natural lan-
guage learning (conll-2009), june 4-5, boulder,
colorado, usa.

alexandre klementiev, ivan titov, and binod bhat-
tarai. 2012. inducing crosslingual distributed rep-
resentations of words. in proceedings of coling
2012, pages 1459   1474, mumbai, india, december.
the coling 2012 organizing committee.

minmin chen, zhixiang xu, kilian weinberger, and
fei sha. 2012. marginalized denoising autoen-
coders for id20. in john langford and

terry koo, xavier carreras, and michael collins.
2008. simple semi-supervised id33.
in in proc. acl/hlt.

lu shuxiang. 2004. the contemporary chinese dic-
tionary (xiandai hanyu cidian). commercial press.

kiril simov, petya osenova, milena slavcheva,
sia kolkovska, elisaveta balabanova, dimitar
doikoff, krassimira ivanova, er simov, and milen
kouylekov. 2002. building a linguistically inter-
preted corpus of bulgarian: the bultreebank. in in:
proceedings of lrec 2002, canary islands.

richard socher, eric h. huang, jeffrey pennington,
andrew y. ng, and christopher d. manning. 2011.
dynamic pooling and unfolding recursive autoen-
in advances in
coders for paraphrase detection.
neural information processing systems 24.

richard socher, brody huval, christopher d. man-
ning, and andrew y. ng. 2012. semantic com-
positionality through recursive matrix-vector spaces.
in proceedings of the 2012 conference on em-
pirical methods in natural language processing
(emnlp).

oscar t  ackstr  om, ryan mcdonald, and jakob uszko-
reit. 2012. cross-lingual word clusters for direct
transfer of linguistic structure. in proceedings of the
2012 conference of the north american chapter of
the association for computational linguistics: hu-
man language technologies, pages 477   487. asso-
ciation for computational linguistics.

j. turian, l. ratinov, and y. bengio. 2010. word rep-
resentations: a simple and general method for semi-
supervised learning. in proceedings of the 48th an-
nual meeting of the association for computational
linguistics, pages 384   394. association for com-
putational linguistics.

leonoor van der beek, gosse bouma, rob malouf,
and gertjan van noord. 2002. the alpino depen-
dency treebank. language and computers, 45(1):8   
22.

matthias trautner kromann. 2003. the danish depen-
dency treebank and the dtag treebank tool. in pro-
ceedings of the second workshop on treebanks and
linguistic theories (tlt), page 217.

mitchell p marcus, mary ann marcinkiewicz, and
beatrice santorini. 1993. building a large anno-
tated corpus of english: the id32. compu-
tational linguistics, 19(2):313   330.

t. mikolov, m. kara     at, l. burget, j. cernocky, and
s. khudanpur.
2010. recurrent neural network
based language model. proceedings of interspeech.

andriy mnih and geoffrey e hinton. 2009. a scalable
hierarchical distributed language model. advances
in neural information processing systems, 21:1081   
1088.

frederic morin and yoshua bengio. 2005. hierarchi-
cal probabilistic neural network language model. in
proceedings of the international workshop on arti   -
cial intelligence and statistics, pages 246   252.

roberto navigli and simone paolo ponzetto. 2010.
babelnet: building a very large multilingual seman-
tic network. in proceedings of the 48th annual meet-
ing of the association for computational linguistics,
pages 216   225. association for computational lin-
guistics.

joakim nivre, jens nilsson, and johan hall. 2006.
talbanken05: a swedish treebank with phrase struc-
ture and dependency annotation. in proceedings of
the    fth international conference on language re-
sources and evaluation (lrec), pages 1392   1395.

joakim nivre, johan hall, sandra k  ubler, ryan mc-
donald, jens nilsson, sebastian riedel, and deniz
yuret. 2007. the conll 2007 shared task on de-
in proceedings of the conll
pendency parsing.
shared task session of emnlp-conll 2007, pages
915   932, prague, czech republic, june. associa-
tion for computational linguistics.

slav petrov, dipanjan das, and ryan mcdonald. 2012.
a universal part-of-speech tagset. in nicoletta cal-
zolari (conference chair), khalid choukri, thierry
declerck, mehmet u  gur do  gan, bente maegaard,
joseph mariani, jan odijk, and stelios piperidis, ed-
itors, proceedings of the eight international con-
ference on language resources and evaluation
(lrec   12), istanbul, turkey, may. european lan-
guage resources association (elra).

sameer pradhan, alessandro moschitti, nianwen xue,
olga uryupina, and yuchen zhang. 2012. conll-
2012 shared task: modeling multilingual unre-
in proceedings
stricted coreference in ontonotes.
of the sixteenth conference on computational natu-
ral language learning (conll 2012), jeju, korea.

david e rumelhart, geoffrey e hinton, and ronald j
williams. 2002. learning representations by back-
propagating errors. cognitive modeling, 1:213.

