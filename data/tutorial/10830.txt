a neural networks approach to predicting how things might have
turned out had i mustered the nerve to ask barry cotton   eld to the

junior prom back in 1997

eve armstrong   1

1biocircuits institute, university of california, san diego, la jolla, ca 92093-0374

(dated: april 1, 2017)

abstract

we use a feed-forward arti   cial neural network with back-propagation through a single hidden layer to predict
barry cotton   eld   s likely reply to this author   s invitation to the    once upon a daydream    junior prom at the conard
high school gymnasium back in 1997. to examine the network   s ability to generalize to such a situation beyond
speci   c training scenarios, we use a l2 id173 term in the cost function and examine performance over a
range of id173 strengths. in addition, we examine the nonsensical decision-making strategies that emerge
in barry at times when he has recently engaged in a    ght with his annoying kid sister janice. to simulate barry   s
inability to learn ef   ciently from large mistakes (an observation well documented by his algebra teacher during
sophomore year), we choose a simple quadratic form for the cost function, so that the weight update magnitude is
not necessary correlated with the magnitude of output error.

network performance on test data indicates that this author would have received an 87.2 (1)% chance of
   yes   given a particular set of environmental input parameters. most critically, the optimal method of question
delivery is found to be secret note rather than verbal speech. there also exists mild evidence that wearing
a burgundy mini-dress might have helped. the network performs comparably for all values of id173
strength, which suggests that the nature of noise in a high school hallway during passing time does not affect
much of anything. we comment on possible biases inherent in the output, implications regarding the functionality
of a real biological network, and future directions. over-training is also discussed, although the id202
teacher assures us that in barry   s case this is not possible.

i.

introduction

7
1
0
2

 
r
a

 

m
0
3

 
 
]
h
p
-
p
o
p

.
s
c
i
s
y
h
p
[
 
 

1
v
9
4
4
0
1

.

3
0
7
1
:
v
i
x
r
a

by most historical accounts, barry cotton   eld was a
brash, unprincipled individual (private communications
1983-1998; lunch ladies    assoc. v. cotton   eld 1995)
with slightly below-average learning capabilities (consol-
idated report cards: 1986-1998), and with a reputation
among the local police force for an alleged - yet unprov-
able - association with the serial disappearance of various
neighbors    cats (whpd records division 1988, 1989a,
1989b, 1996).

but they didn   t know him like i did. the moment
i    rst laid eyes on barry - across the see-saw in second
grade - i imagined that he was deeply multi-layered: a
mysterious soul with a hidden strength driven by latent,
and perhaps unstable, inclinations. i could tell. it was

   earmstrong@ucsd.edu

1

sure to be re   ected somewhere in those expressive eyes
of his, if one were to peer hard enough (see fig. 1). fur-
ther, this sentiment was in-part justi   ed when he left
me dangling up on the see-saw for several minutes until
the playground monitor intervened. as noted, not many
others saw any nuanced side of barry. this was due to
the fact that one had to exert enormous effort in order to
see it. obsessive, in fact.

throughout our four years at conard high school
in west hartford, connecticut, barry and i engaged in
numerous in-depth conversations - usually in the hallway
during the four-minute permitted passing time. during
these opportune intervals i peppered him with questions
regarding himself: habits, quirks, predilections, revul-

saved my copious notes on our question-and-answer ses-
sions - notes that might serve as a proxy of barry   s per-
sonality. might i harness this information to somehow
reconstruct barry - and use that representation to predict
backwards? to investigate this possibility, let us now turn
to the black art of arti   cial neural network construction.
arti   cial neural networks (anns) are a type of
machine learning algorithm in which a neurobiology-
inspired architecture is created, via exposure to training
data, to mimic the capability of a real brain to catego-
rize information (e.g. hop   eld 1988, jain & mao 1996).
this arti   cial brain, while astoundingly stupid by hu-
man standards (and by fruit-   y and sea-slug standards,
incidentally), can nevertheless learn to be stupid rather
quickly and thereafter serve as a powerful tool for partic-
ular predictive purposes.

the combined features of relatively low intelligence
and limited utility parallel barry cotton   eld quite readily.
this framework, then, appears well-suited for creating an
arti   cial barry and examining its decision-making strate-
gies. id90 and decision-tree/ann hybrids (e.g.
lee & yen 2002, biau et al. 2016, rota bulo & kond-
schieder 2014, kontschieder et al. 2015) may also mimic
such calculations, but for our purposes it seems impor-
tant to constrain the algorithm to a brain-like framework.
speci   cally, we aim to recreate the brain that was barry   s
at the time immediately preceding the junior prom in
may 1997. the    ve-volume high school diary can be
employed as a training set.

in this paper we examine a two-tiered learning ap-
proach to infer the likely outcome of inviting barry cot-
ton   eld to the junior prom twenty years ago. in the    rst
tier, a zeroth-order barry is set based on known answers
to simple questions of identity and person tastes, which
are taken to be independent of immediate environment.
in the second set, zeroth-order barry receives questions
that require a decision to be made; these decisions might
be environment dependent. in this phase, zeroth-order
barry is tweaked to account for environmental prefer-
ences, and a corresponding optimal set of environmental
parameter values is identi   ed. finally, the prediction
phase employs this optimal set to yield an 87.2 (1)% like-
lihood of    yes    to the prom invitation.

while the output of anns does not necessarily pro-
vide the right answer, we    nd ourselves quite enamored
of this one. consequently we take it to be right.

figure 1: barry cotton   eld in 1997 (yearbook pictures).

sions, sunniest plans, bloodiest secrets, and his essential
identity. i took copious notes on his responses, when-
ever he was willing to offer me any. i recorded them
instantly and later transferred them to a diary (well, di-
aries - ultimately    ve volumes). the frequency of my
entries tapered somewhat toward the end of senior year,
by which time barry had learned that he could maximize
the chances of evading me by ducking out of class two
minutes early (note that he required 3.5 years to learn
this; see results: over-training). catching up to him often
required an all-out sprint on my part. but it was worth it.
he was fascinating.

i never, however, mustered the courage to ask barry
to the prom junior year. this decision has haunted me;
i wonder what his answer would have been. while an
unparalleled number of poems has been written on this
topic (armstrong 1998-2017) and hundreds of sleep hours
wasted on what-if? dreams, no quantitative study has ever
been undertaken to investigate the question. recently i
decided to address this situation. i wanted an answer -
or at least some maximum-likelihood answer.

but how to obtain an answer? contacting barry to
simply pose the question was no longer a possibility;
it would have violated my restraining order (whpd
records division 2002-2017, renewable). i had, however,

ii. learning algorithm

a. framework

the architecture consists of one input, one hidden, and
one output layer (fig. 2). the number of input nodes
is 137. the inputs are: 1) a question, and 2) a set of 136
environmental parameters. the output is binary (yes
or no). to maximize the network   s ability to generalize
beyond the training set, we choose the number of hidden
nodes to be 1,000. now, a larger-than-necessary hidden
layer may increase computational demands and the like-
lihood of over   tting. these are not concerns for our
purposes, however, as we have no time constraints and it
is highly improbable that a realistic model of barry can
be over-trained (consolidated report cards: 1986-1998).
the input signal propagates forward only. synaptic
weights and node biases are updated via the gradient-
descent minimization of a cost function that represents
output error. updates are effected via a constant back-
propagation rule that relates output error to the cost
function derivative with respect to the weights and biases
(see materials and methods).

figure 2: basic brain architecture to be trained. there is one
input, one hidden, and one output layer. the number of input
nodes is 137. the inputs are: 1) a question, and 2) a set of
136 environmental parameters. the output is binary (yes or
no). to maximize the network   s ability to generalize beyond
the training set, the number of hidden nodes is chosen to be
1,000. while a larger-than-necessary hidden layer may increase
computational demands and the likelihood of over   tting, these
are not concerns for our purposes. first: we have plenty of
time, and second: it is highly improbable that a realistic model
of barry can be over-trained (consolidated report cards: 1986-
1998).

now, this choice of cost function implicitly takes
barry   s aim in life to be the minimization of error. this
assumption is not squarely consistent with his behavior
in a classroom setting (see discussion). to mitigate the
possible impact of this bias, we choose a simple quadratic
term for this cost function, so that the weight update
magnitude does not necessary correlate with the magni-
tude of output error. this choice aims to simulate barry   s
inability to learn ef   ciently from large mistakes (report
card: algebra 1996). finally, we include a id173
term to examine the network   s sensitivity to noise (see
materials and methods). when id173 strength is
chosen aptly, this term penalizes high synaptic weights
- and low weights may enable a model to more readily
recognize repeating patterns, rather than noise, in a data
set (nielsen 2015).

the algorithm converges upon a cost value that is
not guaranteed - nay, is highly unlikely - to be the global
minimum. it will be suf   cient for our standards, however,
provided that our standards are suf   ciently low.

b. two-stage learning

learning occurs in two stages, denoted: becoming barry
and tweaking barry, to be described below. following
each stage is a validation and a test step. in each stage,
a single data set is used, from which we select random
batches for training, validation, and testing. the per-
cent accuracy is measured continually, and learning is
stopped when that percentage begins to asymptote to
a stable value. note that as the questions were origi-
nally posed in a high school hallway during passing time,
these training sets provide an opportunity to probe the
network   s performance in a high-noise environment.

the use of a single data set from which to draw
training, validation, and test questions might reduce
barry   s ability to generalize to broad situations. we do
not consider this a problem, however. the inclusion
of a id173 term and an absurdly large number
of hidden nodes is intended to foster generalizability.
moreover, the prediction phase involves a single question
that is typical of the scenario at hand. that is: for the
purposes of this study, all barry needs to do is learn how
to be a high school junior.

stage 1: becoming barry

in this    rst stage of learning, the architecture adjusts
to a zeroth-order representation of barry   s brain as it ex-
isted in 1997, given a set of questions that are taken to be
independent of immediate environment. these questions
probe knowledge of barry   s basic identity, personal tastes,
interests, and emotional tendencies. there are 10,002 en-
tries in the question set. samples, including: is your name
barry?, are listed in table 1. it is assumed (well, hoped)
that these questions require no decision-making by the
time barry has reached age 17.

note that while the network contains 137 input nodes,

only input node 1 receives a signal during this stage -
the signal being a question. the other 136 nodes exist
to receive environmental input parameters, and we set
these aside for the moment. in this    rst stage, then, the
only weights that are updated are those associated with
input node 1. these consist of the input-to-hidden-layer
weights that emanate from input node 1 only, and all
of the hidden-to-output weights. fig. 3 (left) denotes
these weights in purple. note that while node bias is not
depicted in fig. 3, all nodes receiving input will receive
bias updates.

figure 3: two-stage learning procedure. left: setting zeroth-order barry. here we update weights and biases (biases are not
pictured) based on simple questions regarding basic identity, which we take to be environment-independent. right: here
we tweak zeroth-order barry using questions that require contemplation or decision-making. these questions may depend
in part on environmental parameters. we perform this step multiple times on all possible sets of environmental parameter
values. to identify barry   s preferred set, we assume that some information regarding barry   s environmental preferences have
been implicitly wired into his zeroth-order personality. thus, we take as optimal the set requiring the least tweaking of the
zeroth-order weights that were set during stage 1.

stage 2: tweaking barry

having constructed a zeroth-order barry, we now
expose him to questions that require a decision to be
made, where now the answer may depend on particular
environmental parameters.

we consider 136 parameters that, given this author   s
conversational history with barry, are suspected to affect
barry   s response. these include, for example: time of
day, question delivery method (e.g. secret note versus
verbal speech), and type of out   t the question-deliverer is

wearing. we would like to ascertain barry   s preferences
regarding the parameter values. in this arrangement, in-
put node 1 still receives each question, and input nodes
2-137 now receive the environmental input signals, which
are elements of a vector p.

as in stage 1, the questions and answers for stage 2
are taken from this author   s high school diary. further,
a signi   cant fraction of the questions were asked multi-
ple times over various environmental contexts; this point
shall prove pertinent momentarily. sample questions are
listed in table 2.

now, as barry   s basic identity has been programmed
in stage 1, we imagine that some sense of environmental
preferences are now implicit in his personality. for exam-
ple, consider this question from set 2: can i borrow    ve
dollars?. during stage 1 we had wired in the answer to is
your favorite color beige? as yes. it is possible that that an-
swer contains information regarding the likelihood that
barry will lend us    ve dollars if we are wearing a beige
sweater at the time. one environmental parameter to be
tuned, then, is out   t color.

what is the mapping between barry   s intrinsic wiring
from stage 1 and parameter preferences p in stage 2?
that is, what is the set of functions f that takes the ma-
trix barryzero such that: p = f (barryzero)? if we knew
f, we would be able to infer the optimal set p. we do
not know f, however, and we will not even attempt to
construct it. instead, we will assume that within zeroth-
order barry there exists already an implicit approximate
preference for set p, and thus that the optimal choice of
p should affect zeroth-order barry minimally.

our aim in learning stage 2, then, is to seek the set
of environmental parameter values p that requires the
least tweaking of the synapse weights that were estimated
in stage 1, assuming that this set will be the most faith-
ful representation of barry   s true preferences. fig. 3
(right) shows in purple the weights that are now to be
estimated: the weights estimated in stage 1 in addition to
the weights associated with the input nodes that receive
the 136 environmental signals.

for each question in set 2, we train barry indepen-
dently on all possible combinations of environmental
parameter values p. we choose the optimal set regardless

of the required training duration or the percent accuracy
of the output.

now, the optimal set p should be independent of the
question posed, with one salient exception. as noted
earlier, this author asked many of the questions in set 2
on multiple occasions, in various environmental contexts.
answers were found to be stable across all parameter
values, except one: instances in which barry had recently
engaged in a    ght with his annoying kid sister janice
(fig. 4). given this observation, we perform learning
stage 2 twice: once for the scenario in which a    ght with
janice had recently occurred, and one for the scenario in
which it had not. each version corresponds to a distinct
training set.

the stage 2 training begins with the weights and bi-
ases estimated in stage 1 at those starting values, and
with all as-yet unestimated weights and biases initialized
randomly. finally, it is assumed that since this author is
the individual who posed all 24,203 questions present in
the training data (see materials and methods), barry has
been conditioned to assume that any question posed is
coming from this author. that is: during the prediction
phase, he will know who is asking him to the prom.

figure 4: janice cotton   eld, barry   s kid sister, in 1997 (year-
book pictures).

table 1: samples from the 10,002-item training set 1: becoming barry

answer

question re: identity
is your name barry?
yes
no
do you have a dachshund named raskolnikov? yes
yes
do you own a peanut plant?
yes
no
do you play lacrosse?
yes
yes
is your house beige?
no
yes
yes would you like to have ten toes tomorrow? yes
do you have ten toes?

do you wish your name were jackson?

are you the best lacrosse player ever?

question re: personal inclinations

was    raskolnikov    your idea?

is your favorite color beige?

do you like peanuts?

answer

these questions concern information about basic identity and personal tastes; they are designed to set zeroth-order barry.

answers are taken to be independent of environment.

table 2: samples from training set 2: tweaking barry; answer depending on occurrence of a recent    ght with janice

question

answer

(janice-   ght: yes) (janice-   ght: no)

can i borrow a dollar?
can i borrow    ve dollars?
is this a good time?
are you going to hand in mrs. lindsey   s extra credit assignment?
will you look after the school mascot ferret over this weekend?
do you want to buy a kitten?
can you keep a secret?
please stop clicking your tongue behind me all through chem class?
would you consider going to the prom with mindy?
what about lydia?

no
no
no
yes
yes
yes
no
yes
yes
yes

no
no
no
no
no
no
no
no
yes
no

these questions are designed to probe barry   s decision-making strategies given various environmental parameters. they were
asked under two conditions: once at times when barry had recently fought with his kid sister janice, and once at times when
he had not. the training sets for    janice-fight: yes    and    janice-fight: no    contain 7,345 and 9,557 questions, respectively.
2,701 questions overlap between scenarios. the samples above are drawn from the ~ 50% of these 2,701 overlapping questions
for which barry gave a reliable answer in the    janice-   ght: yes    case. for the other 50%, barry   s answer failed to stabililize,

suggesting that    ghting with his sister rendered him quite unlike himself (see results).

iii. results

a. using the no-   ght-with-janice results to obtain

predictions

we chose to make predictions using results of the no-
fight-with-janice training set. this decision was made
for two related reasons: with this set, we obtained 1) less
updating of barry   s zeroth-order synaptic weights and 2)
greater stability of the output answer.

regarding the    rst point: the set for which the answer
to have you recently fought with janice? is yes modi   ed the
weights of stage 1 by roughly eight times as much as did
the set for which the answer is no. this suggests that
   ghting with janice made barry suf   ciently vexed that
he was signi   cantly not himself anymore.

secondly: on many trials of    janice-   ght: yes   , learn-
ing of a signi   cant fraction of the questions did not
asymptote to a stable answer. speci   cally: on 76% of
trials, at least 50% of the questions failed to converge dur-
ing the time required for the other fraction of questions
to attain stable values and for the percent accuracy of
those values to asymptote. for questions that did asymp-
tote to a stable value, the typical required duration for
learning, over all trials, was 31 (4) minutes. this lower
limit is consistent with the observation that the effects of
   ghts with barry   s sister tended to last at least 45 minutes
(consolidated guidance counselors    records 1995-1997).
furthermore, for the case in which a recent    ght with
janice had occurred, the back propagation resulted in

seemingly random redistributions of weights. this phe-
nomenon appears to be consistent with barry   s deviation
from zeroth order, as noted above; namely: the random
weight changes suggest that barry was forgetting himself
entirely. we do note, however, that for some brains, ran-
dom changing of synaptic weights can be just as    ne a
paradigm for learning as is a rigid back-propagation rule
(lillicrap et al. 2016).

one    nal reason we took to dismiss the results of the
   janice-   ght: yes    scenario was a simple examination of
the responses that did asymptote to stable values. within
this category, for any questions that represented requests
for favors, barry was 27 (8)% less likely to acquiesce fol-
lowing a    ght. to make predictions, then, we adopted
the optimal parameter set that emerged from the training
scenario in which barry had not recently fought with
janice.

for this optimal set, there emerged one parameter that
was consistently (34 (4)%) more likely to yield a favorable
reply to a request. this parameter was question delivery
method: secret note is favored over verbal speech. bur-
gundy mini-dress won out slightly (at the 2 (1)% level) to
other shades of mini-dress. the mini-dress category over-
all, meanwhile, beat 726 other wardrobe choices at the 6
(2)% level. the remaining 134 environmental parameter
choices had negligible effect on outcome.

b. prediction

given the optimal set of parameter values in a scenario in
which barry had not recently engaged in a    ght with jan-
ice, and over 1,000 trial predictions, the prom invitation
yielded an 87.2 (1)% success rate.

c. over-training

as noted, most questions were originally delivered in
a high school hallway, which is typically an extremely
noisy environment.
it is possible, then, that answers
barry gave during stage 2 were in   uenced by factors that
were not accounted for in the environmental parameter
set. to examine the model   s sensitivity to noise, we ex-
amined results for all choices of id173 strength.
regularized networks are constrained to build simple
models based on patterns seen often in training, whereas
unregularized networks may be more likely to learn the
noise.

for all choices of id173 strength, the network
performed comparably. this suggests that the nature of
noise in a high school hallway during passing time does
not affect much of anything. alternatively, barry simply
did not hear it, which is the same conclusion stated dif-
ferently. moreover, either way, it seems to con   rm the
algebra teacher   s report that to over-train barry would
require a monumental undertaking.

d.

inability to learn particular questions

we make a    nal observation that pertains again to the
algebra teacher   s comment on over-training. even for
the training set corresponding to the    janice-   ght: no   
scenario, we identi   ed a small subset of questions (2%
of the sample) for which barry   s response never attained
a stable value. in particular, barry learned quickly that
he had ten toes but never a reliable response to will you
have ten toes tomorrow?, despite trials over consecutive
days. this    nding might be indicative of one problem
inherent in attempting to teach a memory-less system to
learn something (see discussion.)

iv. discussion

a. a memory-less system that learns?

we comment on barry   s inability to yield a stable output
to a small subset of questions from stage 2: tweaking
barry, even in the regime in which he had not recently
fought with janice. these questions shared a common-
ality: they required imagining the future. one question,
for example, was: will you have ten toes tomorrow?. the
answer failed to stabilize despite training sessions over
consecutive days - perhaps re   ecting one of the inherent
dif   culties of teaching memory-less systems to learn.

it is possible that this    nding provides further evi-
dence in favor of his teacher   s theory that barry cannot be
over-trained. alternatively, however, perhaps our exper-
imental design underestimated barry   s learning capacity.
indeed, while it is highly likely that barry will have ten
toes tomorrow (given ten toes today), it is not guaran-
teed. barry may have apprehended this uncertainty. the
capacity for such aptitude was not captured in the ex-
perimental design, an omission that may underlie the
instability of the output.

b. possible biases in constructing barry

to examine the possibility of bias in results, let us con-
sider our construction of barry   s brain   s basic architec-
ture. we set barry as a feed-forward network, with zero
connectivity between nodes in a layer, and a constant
back-propagation rule that permits no stochasticity into
network evolution. it is possible that barry   s true cogni-
tive scheme is not as rigidly compartmentalized. further,
we have enacted the learning by assuming a particular
desire for ef   ciency: that it is output error that barry
seeks to minimize.

now, barry   s brain may be designed for ef   ciency
at something, but must it necessarily be minimizing out-
put error? perhaps output error reduction is not barry   s
agenda. perhaps instead, for example, barry seeks to
minimize the amount of work required to produce an
answer, regardless of whether that answer is correct. this
is a possibility that would be strongly supported by any
teacher or peer who shared a class with barry during
any time-frame within his grade school years (private
communications, 1987-98).

in short, other choices for constructing either the basic

architecture or the learning algorithm might have yielded
different outcomes (see future directions).

c.

insight into the functionality of real biological

networks

d. future directions

let us now ruminate for a moment. let   s imagine that we
know absolutely the cost function to be minimized. that
is: we know barry   s aim in life. let us further imagine
that we have developed a method to identify the global
minimum of this cost function - and to calculate it within
half a human lifetime. in this case, the emergent net-
work structure might contain clues regarding how nature
designs a real central nervous system.

what might this cost function be? how might it relate
to architectural complexity, robustness of certain activ-
ity patterns, variability of other patterns, redundancy,
and other variables that a nervous system must consider
when attempting to keep its host alive? further, is there
just one cost function, or a system of cost functions?

unfortunately, to use the methodology devised in this
paper to ascertain what cost function barry is minimizing,
we require a cost function to minimize. the constraints
we have erected are too rigid for such an exploration;
barry would require a bit of    eshing out. adding a feed-
back    ow of information, for example, would be a sound
place to begin. most people have memories, or at least
we are under the impression that we do.

as publishable as this adventure sounds, however, we
shall leave it in the hands of more wily souls. we have
obtained our result (87.2% yes!). it is a rather delightful
result and so we shall take it to be correct - and plan no
future directions.

v. materials and methods

a. feed-forward activation

    taking    c
   al
j
  l
j = (al

j     yj) yields:
= (al
j     yj)  (cid:48)(zl
j ).

we use a sigmoid activation function for the neurons because
we assume that barry is human to zeroth order and that a
sigmoid function - which saturates at both ends of the input
distribution - captures that feature. the sigmoid y(x) is written:

where

y(x) =   (w    x + b),

  (z) =

1

1 + e   z ,

where each neuron receives i inputs xi and i weights wi and
possesses a bias b. for each layer l, then, one can compute
output al given the input (zl):

zl = wlal   1 + bl;
al =   (zl)

b. back propagation

we use a quadratic cost function with a id173 term to
penalize high weights. a cross-id178 (logarithmic) cost func-
tion better approximates learning in most humans (e.g. golik
et al. 2013), but for barry   s case the quadratic form captures
barry   s tendency to learn extremely slowly from large mis-
takes (report card: algebra 1995). the cost function c(w, b)
is written as:

c(w, b) =

1
2n

   
j

[yj(xj)     al

j ]2 +

  
2n

   
i,j

w2
ij

where wij are the synapse weights, b are the node biases, x
are the training inputs, y(x) represents the desired output, al
is the activation of the    nal input (or, the ultimate output at
layer l), n is the number of training examples, and    is the
id173 strength.

a id119 is used to minimize the cost function.
this procedure entails a set of steps that iteratively relate the
output error   l
j of layer l to the updating of weights and biases
of layers l and (l + 1) - beginning from the    nal layer l and
proceeding backwards. the steps are standard:

      l

back-propagation steps
  (cid:48)(zl
j =    c
   al
j
tion function.

j ), where       is the derivative of the activa-

    to propagate this error, we de   ne   l

j for each layer l:

j        c
  l

   zl
j

.

j =   (cid:48)(zl)    j wl+1

    we obtain:   l
    we can relate the error   l

ij   l+1

j

.

the weights wij and biases bj - as follows:

j to the quantities of interest -

  l
j;

= al   1
k

       c
   wl
jk
=   l
j.

       c
   bl
j

c. choices of user-de   ned parameters

the id173 term    was varied from zero (the limit in
which high weight is irrelevant, or rather, is tamed only by the
sigmoid activation) through 1 in increments of 0.01 (a regime
of high penalty on large weights), and then from 1 to 100,000
in increments of 10 (a regime in which weight again becomes
increasingly irrelevant). this breadth of range was examined
in order to explore the network   s sensitivity to performance in
a noisy environment.

the step size for id119 (or    learning rate   ) was

set to 0.1, for no particular reason.

d. training data

question set 1 contained 10,002 questions. question set 2
contained 7,345 questions for the regime in which barry had
recently engaged in a    ght with janice, and 9,557 questions
for the regime in which he had not; 2701 of these questions
overlapped. (for access to these supplementary materials, please
email the author.)

batches were made of 100 questions each. learning pro-
ceeded until the percent accuracy on validation tests asymp-
toted, or until it was determined that learning would in fact
probably not occur at all (see results).

as these questions were originally posed in a high school
hallway during passing time, these training sets are able to
probe the network   s ability to perform in an extremely noisy
environment.

references

[1] abrams, mrs. betty. petty altercations: cotton   eld, janice; cotton   eld, barry. consolidated guidance

counselors    records (1995-7).

[2] armstrong, e. odes to barry. unpublished (1999-2017).

[3] biau, g  rard and scornet, erwan and welbl, johannes. neural id79s.

arxiv preprint

arxiv:1604.07143 (2016).

[4] connecticut judicial branch: small claims. lunch ladies    association of west hartford v. cotton   eld.

1st cir. 150 f.3d 1, 6-7 (1995).

[5] cranston, mister jim. semester report card: algebra. semester 1 (1996).
[6] garby, officer frank. disappearance of sparky. west hartford; whpd records division (12 mar 1988).
[7] garby, officer frank. order of restraint: armstrong, e. west hartford; whpd records division, stalking:

18-9-111, c.r.s. (9 jul 2002 - 31 dec 2017; renewable).

[8] golik, p., doetsch, p., and ney, h. cross-id178 vs. squared error training: a theoretical and experimental

comparison. in interspeech (2013), pp. 1756   1760.

[9] hopfield, john j. arti   cial neural networks. ieee circuits and devices magazine 4, 5 (1988), 3   10.
[10] kendrick, officer clara. disappearance of mittens. west hartford; whpd records division (27 nov 1989b).
[11] kontschieder, peter and fiterau, madalina and criminisi, antonio and rota bulo, samuel. deep
neural decision forests. in proceedings of the ieee international conference on id161 (2015), pp. 1467   
1475.

[12] lee, yue-shi and yen, show-jane. neural-based approaches for improving the accuracy of id90.

in international conference on data warehousing and knowledge discovery (2002), springer, pp. 114   123.

[13] lillicrap, timothy p and cownden, daniel and tweed, douglas b and akerman, colin j. random
synaptic feedback weights support error id26 for deep learning. nature communications 7 (2016).
[14] mitchell, officer douglass. disappearance of professor wagstaff. west hartford; whpd records division

(25 jul 1989a).

[15] nielsen, michael a. neural networks and deep learning. determination press (2015).
[16] rota bulo, samuel and kontschieder, peter. neural decision forests for semantic image labelling. in

proceedings of the ieee conference on id161 and pattern recognition (2014), pp. 81   88.

[17] terndrup, officer blake. disappearance of sparky junior. west hartford; whpd records division (9 jul

1996).

[18] west hartford public school system. consolidated report cards. west hartford, ct (1986-1998).
[19] yearbook pictures.

say cheese! the world   s worst high school yearbook photos range from
referencing a former post on worldwidein-
strange and scary to just plain hilarious.
terweb.com. http://www.dailymail.co.uk/news/article-2193399/say-cheese   the-worlds-worst-yearbook-photos-range-
strange-scary-just-plain-hilarious.html (accessed 22 mar 2017).

daily mail,

