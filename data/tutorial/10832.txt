id202ic structure of word senses, with applications to polysemy

sanjeev arora, yuanzhi li, yingyu liang, tengyu ma, andrej risteski

computer science department, princeton university

35 olden st, princeton, nj 08540

{arora,yuanzhil,yingyul,tengyu,risteski}@cs.princeton.edu

8
1
0
2
 
c
e
d
7

 

 
 
]
l
c
.
s
c
[
 
 

6
v
4
6
7
3
0

.

1
0
6
1
:
v
i
x
r
a

abstract

id27s are ubiquitous in nlp and
information retrieval, but it is unclear what
they represent when the word is polysemous.
here it is shown that multiple word senses re-
side in linear superposition within the word
embedding and simple sparse coding can re-
cover vectors that approximately capture the
senses. the success of our approach, which
applies to several embedding methods,
is
mathematically explained using a variant of
the random walk on discourses model (arora
et al., 2016). a novel aspect of our tech-
nique is that each extracted word sense is ac-
companied by one of about 2000    discourse
atoms    that gives a succinct description of
which other words co-occur with that word
sense. discourse atoms can be of indepen-
dent interest, and make the method potentially
more useful. empirical tests are used to verify
and support the theory.

1 introduction

id27s are constructed using firth   s hy-
pothesis that a word   s sense is captured by the distri-
bution of other words around it (firth, 1957). clas-
sical vector space models (see the survey by tur-
ney and pantel (2010)) use simple id202
on the matrix of word-word co-occurrence counts,
whereas recent neural network and energy-based
models such as id97 use an objective that in-
volves a nonconvex (thus, also nonlinear) function
of the word co-occurrences (bengio et al., 2003;
mikolov et al., 2013a; mikolov et al., 2013b).

this nonlinearity makes it hard to discern how
these modern embeddings capture the different
senses of a polysemous word. the monolithic view
of embeddings, with the internal information ex-
tracted only via inner product, is felt to fail in cap-
turing word senses (grif   ths et al., 2007; reisinger
and mooney, 2010; iacobacci et al., 2015). re-
searchers have instead sought to capture polysemy
using more complicated representations, e.g., by in-
ducing separate embeddings for each sense (murphy
et al., 2012; huang et al., 2012). these embedding-
per-sense representations grow naturally out of
classic word sense induction or wsi (yarowsky,
1995; schutze, 1998; reisinger and mooney, 2010;
di marco and navigli, 2013) techniques that per-
form id91 on neighboring words.

the current paper goes beyond this mono-
lithic view, by describing how multiple senses
of a word actually reside in linear superposi-
tion within the standard id27s (e.g.,
id97 (mikolov et al., 2013a) and glove (pen-
nington et al., 2014)). by this we mean the follow-
ing: consider a polysemous word, say tie, which can
refer to an article of clothing, or a drawn match, or a
physical act. let   s take the usual viewpoint that tie
is a single token that represents monosemous words
tie1, tie2, .... the theory and experiments in this
paper strongly suggest that id27s com-
puted using modern techniques such as glove and
id97 satisfy:

vtie       1 vtie1 +   2 vtie2 +   3 vtie3 +         

(1)

coef   cients   i   s

where
and
vtie1, vtie2, etc., are the hypothetical embeddings of

are nonnegative

the different senses   those that would have been
induced in the thought experiment where all oc-
currences of the different senses were hand-labeled
in the corpus. this linearity assertion, whereby
linear structure appears out of a highly nonlinear
embedding technique, is explained theoretically in
section 2, and then empirically tested in a couple of
ways in section 4.

section 3 uses the linearity assertion to show how
to do wsi via sparse coding, which can be seen as
a id202ic analog of the classic id91-
based approaches, albeit with overlapping clusters.
on standard testbeds it is competitive with earlier
embedding-for-each-sense approaches (section 6).
a novelty of our wsi method is that it automat-
ically links different senses of different words via
our atoms of discourse (section 3). this can be
seen as an answer to the suggestion in (reisinger
and mooney, 2010) to enhance one-embedding-per-
sense methods so that they can automatically link
together senses for different words, e.g., recognize
that the    article of clothing    sense of tie is connected
to shoe, jacket, etc.

this paper is inspired by the solution of word
analogies via id202ic methods (mikolov et
al., 2013b), and use of sparse coding on word em-
beddings to get useful representations for many nlp
tasks (faruqui et al., 2015). our theory builds
conceptually upon the random walk on discourses
model of arora et al. (2016), although we make
a small but important change to explain empirical
   ndings regarding polysemy. our wsi procedure
applies (with minor variation in performance) to
canonical embeddings such as id97 and glove
as well as the older vector space methods such as
pmi (church and hanks, 1990). this is not surpris-
ing since these embeddings are known to be interre-
lated (levy and goldberg, 2014; arora et al., 2016).

point in the corpus there is a micro-topic (   what is
being talked about   ) called discourse that is drawn
from the continuum of unit vectors in    d. the pa-
rameters of the model include a vector vw        d for
each word w. each discourse c de   nes a distribution
over words pr[w | c]     exp(c    vw). the model as-
sumes that the corpus is generated by the slow geo-
metric random walk of c over the unit sphere in    d :
when the walk is at c, a few words are emitted by
i.i.d. samples from the distribution (2), which, due to
its log-linear form, strongly favors words close to c
in cosine similarity. estimates for learning parame-
ters vw using id113 and moment methods correspond
to standard embedding methods such as glove and
id97 (see the original paper).

to study how id27s capture word
senses, we   ll need to understand the relationship be-
tween a word   s embedding and those of words it
co-occurs with.
in the next subsection, we pro-
pose a slight modi   cation to the above model and
shows how to infer the embedding of a word from
the embeddings of other words that co-occur with it.
this immediately leads to the linearity assertion,
as shown in section 2.2.

2.1 gaussian walk model

as alluded to before, we modify the random walk
model of (arora et al., 2016) to the gaussian ran-
dom walk model. again, the parameters of the model
include a vector vw        d for each word w. the
model assumes the corpus is generated as follows.
first, a discourse vector c is drawn from a gaussian
with mean 0 and covariance   . then, a window of
n words w1, w2, . . . , wn are generated from c by:

n

pr[w1, w2, . . . , wn| c] =

pr[wi| c],

yi=1

(2)

(3)

pr[wi | c] = exp(c    vwi)/zc,

2 justi   cation for linearity assertion

since id27s are solutions to nonconvex
optimization problems, at    rst sight it appears hope-
less to reason about their    ner structure. but it be-
comes possible to do so using a generative model for
language (arora et al., 2016)     a dynamic versions
by the log-linear topic model of (mnih and hinton,
2007)   which we now recall. it posits that at every

where zc = pw exp(hvw, ci) is the partition func-

tion. we also assume the partition function concen-
trates in the sense that zc     z exp(kck2) for some
constant z. this is a direct extension of (arora
et al., 2016, lemma 2.1) to discourse vectors with
norm other than 1, and causes the additional term
exp(kck2).1

1the formal proof of (arora et al., 2016) still applies in this
setting. the simplest way to informally justify this assumption

theorem 1. assume the above generative model,
and let s denote the random variable of a window
of n words. then, there is a linear transformation a

such that vw     a e(cid:2) 1

npwi   s vwi | w     s(cid:3).

proof. let cs be the discourse vector for the whole
window s. by the law of total expectation, we have

e [cs | w     s]

=e [e[cs | s = w1 . . . wj   1wwj+1 . . . wn] | w     s] .
(4)

we evaluate the two sides of the equation.

first, by bayes    rule and the assumptions on the
distribution of c and the partition function, we have:

p(c|w)     p(w|c)p(c)

   

   

1
zc
1
z

exp(hvw, ci)    exp(cid:18)   
exp(cid:18)hvw, ci     c   (cid:18) 1

1
2

c        1c(cid:19)
     1 + i(cid:19) c(cid:19) .

2

so c | w is a gaussian distribution with mean

e [c | w]     (     1 + 2i)   1vw.

(5)

next, we compute e[c|w1, . . . , wn]. again using
bayes    rule and the assumptions on the distribution
of c and the partition function,

p(c|w1, . . . , wn)

    p(w1, . . . , wn|c)p(c)

n

    p(c)

p(wi|c)

yi=1

   

1

z n exp  n
xi=1

v   

wic     c   (cid:18) 1

2

     1 + ni(cid:19) c! .

so c|w1 . . . wn is a gaussian distribution with mean

e[c|w1, . . . , wn]    (cid:0)     1 + 2ni(cid:1)   1

vwi.

(6)

n

xi=1

now plugging in equation (5) and (6) into equa-
tion (4), we conclude that

(     1 + 2i)   1vw     (     1 + 2ni)   1e" n
xi=1

vwi | w     s# .

is to assume vw are random vectors, and then zc can be shown
to concentrate around exp(kck2). such a condition enforces
the word vectors to be isotropic to some extent, and makes the
covariance of the discourse identi   able.

re-arranging the equation completes the proof with
a = n(     1 + 2i)(     1 + 2ni)   1.

note: interpretation. theorem 1 shows that there
exists a linear relationship between the vector of a
word and the vectors of the words in its contexts.
consider the following thought experiment. first,
choose a word w. then, for each window s contain-
ing w, take the average of the vectors of the words
in s and denote it as vs. now, take the average of vs
for all the windows s containing w, and denote the
average as u. theorem 1 says that u can be mapped
to the word vector vw by a linear transformation that
does not depend on w. this linear structure may also
have connections to some other phenomena related
to linearity, e.g., gittens et al. (2017) and tian et al.
(2017). exploring such connections is left for future
work.

the linear transformation is closely related to   ,
which describes the distribution of the discourses.
if we choose a coordinate system such that    is
a diagonal matrix with diagonal entries   i, then a
will also be a diagonal matrix with diagonal en-
tries (n + 2n  i)/(1 + 2n  i). this is smoothing the
spectrum and essentially shrinks the directions cor-
responding to large   i relatively to the other direc-
tions. such directions are for common discourses
and thus common words. empirically, we indeed
observe that a shrinks the directions of common
words. for example, its last right singular vector
has, as nearest neighbors, the vectors for words like
   with   ,    as   , and    the.    note that empirically, a is
not a diagonal matrix since the word vectors are not
in the coordinate system mentioned.
note:
implications for glove and id97.
repeating the calculation in arora et al. (2016)
for our new generative model, we can show that
the solutions to glove and id97 training ob-
jectives solve for the following vectors:   vw =

are the same as vw   s up to linear transformation,
theorem 1 (and the linearity assertion) still holds

(cid:0)     1 + 4i(cid:1)   1/2 vw. since these other embeddings
for them. empirically, we    nd that(cid:0)     1 + 4i(cid:1)   1/2

is close to a scaled identity matrix (since k     1k2 is
small), so   vw   s can be used as a surrogate of vw   s.
experimental note: using better sentence em-
beddings, sif embeddings. theorem 1 implicitly
uses the average of the neighboring word vectors as

an estimate (id113) for the discourse vector. this
estimate is of course also a simple sentence em-
bedding, very popular in empirical nlp work and
also reminiscent of id97   s training objective.
in practice, this naive sentence embedding can be
improved by taking a weighted combination (often
tf-idf) of adjacent words. the paper (arora et al.,
2017) uses a simple twist to the generative model
in (arora et al., 2016) to provide a better estimate of
the discourse c called sif embedding, which is bet-
ter for downstream tasks and surprisingly compet-
itive with sophisticated lstm-based sentence em-
beddings.
it is a weighted average of word em-
beddings in the window, with smaller weights for
more frequent words (reminiscent of tf-idf). this
weighted average is the id113 estimate of c if above
generative model is changed to:

p(w|c) =   p(w) + (1       )

exp(vw    c)

zc

,

where p(w) is the overall id203 of word w in
the corpus and    > 0 is a constant (hyperparameter).
the theory in the current paper works with sif
embeddings as an estimate of the discourse c; in
other words, in theorem 1 we replace the average
word vector with the sif vector of that window. em-
pirically we    nd that it leads to similar results in test-
ing our theory (section 4) and better results in down-
stream wsi applications (section 6). therefore, sif
embeddings are adopted in our experiments.

2.2 proof of linearity assertion

now we use theorem 1 to show how the linear-
ity assertion follows. recall the thought experiment
considered there. suppose word w has two distinct
senses s1 and s2. compute a id27 vw for
w. then hand-replace each occurrence of a sense of
w by one of the new tokens s1, s2 depending upon
which one is being used. next, train separate embed-
dings for s1, s2 while keeping the other embeddings
   xed. (nb: the classic id91-based sense induc-
tion (schutze, 1998; reisinger and mooney, 2010)
can be seen as an approximation to this thought ex-
periment.)

theorem 2 (main). assuming the model of sec-
tion 2.1, embeddings in the thought experiment
above will satisfy kvw       vwk2     0 as the corpus

length tends to in   nity, where   vw       vs1 +   vs2 for

   =

f1

f1 + f2

,

   =

f2

f1 + f2

,

where f1 and f2 are the numbers of occurrences of
s1, s2 in the corpus, respectively.

proof. suppose we pick a random sample of n win-
dows containing w in the corpus. for each window,
compute the average of the word vectors and then
apply the linear transformation in theorem 1. the
transformed vectors are i.i.d. estimates for vw, but
with high id203 about f1/(f1 + f2) fraction of
the occurrences used sense s1 and f2/(f1 + f2) used
sense s2, and the corresponding estimates for those
two subpopulations converge to vs1 and vs2 respec-
tively. thus by construction, the estimate for vw is a
linear combination of those for vs1 and vs2.

note. theorem 1 (and hence the linearity asser-
tion) holds already for the original model in arora
et al. (2016) but with a = i, where i is the iden-
tity transformation. in practice, we    nd inducing the
word vector requires a non-identity a, which is the
reason for the modi   ed model of section 2.1. this
also helps to address a nagging issue hiding in older
id91-based approaches such as reisinger and
mooney (2010) and huang et al. (2012), which iden-
ti   ed senses of a polysemous word by id91 the
sentences that contain it. one imagines a good rep-
resentation of the sense of an individual cluster is
simply the cluster center. this turns out to be false
    the closest words to the cluster center sometimes
are not meaningful for the sense that is being cap-
tured; see table 1. indeed, the authors of reisinger
and mooney (2010) seem aware of this because they
mention    we do not assume that clusters correspond
to traditional word senses. rather, we only rely
on clusters to capture meaningful variation in word
usage.    we    nd that applying a to cluster centers
makes them meaningful again. see also table 1.

3 towards wsi: atoms of discourse

now we consider how to do wsi using only word
embeddings and the linearity assertion. our ap-
proach is fully unsupervised, and tries to induce
senses for all words in one go, together with a vector
representation for each sense.

center 1

center 2

before
after
before
after

and provide providing a
providing provide opportunities provision
and a to the
access accessible allowing provide

table 1: four nearest words for some cluster cen-
ters that were computed for the word    access    by
applying 5-means on the estimated discourse vec-
tors (see section 2.1) of 1000 random windows from
wikipedia containing    access   . after applying the
linear transformation of theorem 1 to the center, the
nearest words become meaningful.

given embeddings for all words, it seems un-
clear at    rst sight how to pin down the senses of
tie using only (1) since vtie can be expressed in in-
   nitely many ways as such a combination, and this
is true even if   i   s were known (and they aren   t).
to pin down the senses we will need to interrelate
the senses of different words, for example, relate the
   article of clothing    sense tie1 with shoe, jacket, etc.
to do so we rely on the generative model of sec-
tion 2.1 according to which unit vector in the em-
bedding space corresponds to a micro-topic or dis-
course. empirically, discourses c and c    tend to look
similar to humans (in terms of nearby words) if their
inner product is larger than 0.85, and quite different
if the inner product is smaller than 0.5. so in the dis-
cussion below, a discourse should really be thought
of as a small region rather than a point.

one imagines that the corpus has a    clothing    dis-
course that has a high id203 of outputting the
tie1 sense, and also of outputting related words such
as shoe, jacket, etc. by (2) the id203 of be-
ing output by a discourse is determined by the inner
product, so one expects that the vector for    clothing   
discourse has a high inner product with all of shoe,
jacket, tie1, etc., and thus can stand as surrogate for
vtie1 in (1)! thus it may be suf   cient to consider the
following global optimization:

given word vectors {vw} in    d and two inte-
gers k, m with k < m,    nd a set of unit vectors
a1, a2, . . . , am such that

here k is the sparsity parameter, and m is the
number of atoms, and the optimization minimizes
the norms of   w   s (the    2-reconstruction error):

vw    

.

(8)

xw (cid:13)(cid:13)(cid:13)(cid:13)

m

xj=1

2

2

  w,jaj(cid:13)(cid:13)(cid:13)(cid:13)

both aj    s and   w,j   s are unknowns, and the opti-
mization is nonconvex. this is just sparse coding,
useful in neuroscience (olshausen and field, 1997)
and also in image processing, id161, etc.
this optimization is a surrogate for the desired ex-
pansion of vtie as in (1), because one can hope that
among a1, . . . , am there will be directions corre-
sponding to clothing, sports matches, etc., that will
have high inner products with tie1, tie2, etc., re-
spectively. furthermore, restricting m to be much
smaller than the number of words ensures that the
typical ai needs to be reused to express multiple
words.

we refer to ai   s, discovered by this procedure, as
atoms of discourse, since experimentation suggests
that the actual discourse in a typical place in text
(namely, vector c in (2)) is a linear combination of a
small number, around 3-4, of such atoms. implica-
tions of this for text analysis are left for future work.
relationship to id91.
sparse coding is
solved using alternating minimization to    nd the
ai   s that minimize (8). this objective function re-
veals sparse coding to be a id202ic analogue
of overlapping id91, whereby the ai   s act as
cluster centers and each vw is assigned in a soft way
to at most k of them (using the coef   cients   w,j, of
which at most k are nonzero). in fact this id91
viewpoint is also the basis of the alternating mini-
mization algorithm. in the special case when k = 1,
each vw has to be assigned to a single cluster, which
is the familiar geometric id91 with squared    2
distance.

similar overlapping id91 in a traditional
graph-theoretic setup    id91 while simultane-
ously cross-relating the senses of different words   
seems more dif   cult but worth exploring.

4 experimental tests of theory

vw =

m

xj=1

  w,jaj +   w

(7)

4.1 test of gaussian walk model: induced

embeddings

where at most k of the coef   cients   w,1, . . . ,   w,m
are nonzero, and   w   s are error vectors.

now we test the prediction of the gaussian walk
model suggesting a linear method to induce embed-

#paragraphs
cos similarity

250k
0.94

500k
0.95

750k
0.96

1 million

0.96

table 2: fitting the glove word vectors with aver-
age discourse vectors using a linear transformation.
the    rst row is the number of paragraphs used to
compute the discourse vectors, and the second row
is the average cosine similarities between the    tted
vectors and the glove vectors.

dings from the context of a word. start with the
let vw denote the embedding
glove embeddings;
for w. randomly sample many paragraphs from
wikipedia, and for each word w    and each occur-
rence of w    compute the sif embedding of text in
the window of 20 words centered around w   . aver-
age the sif embeddings for all occurrences of w    to
obtain vector uw   . the gaussian walk model says
that there is a linear transformation that maps uw    to
vw   , so solve the regression:

argminaxw

kauw     vwk2
2.

(9)

we call the vectors auw the induced embeddings.
we can test this method of inducing embeddings by
holding out 1/3 words randomly, doing the regres-
sion (9) on the rest, and computing the cosine sim-
ilarities between auw and vw on the heldout set of
words.

table 2 shows that the average cosine similar-
ity between the induced embeddings and the glove
vectors is large. by contrast the average similar-
ity between the average discourse vectors and the
glove vectors is much smaller (about 0.58), illus-
trating the need for the linear transformation. sim-
ilar results are observed for the id97 and sn
vectors (arora et al., 2016).

4.2 test of linearity assertion

we do two empirical tests of the linearity assertion
(theorem 2).
test 1. the    rst test involves the classic arti   cial
polysemous words (also called pseudowords). first,
pre-train a set w1 of word vectors on wikipedia with
existing embedding methods. then, randomly pick
m pairs of non-repeated words, and for each pair,
replace each occurrence of either of the two words

m pairs

relative error

cos similarity

10
0.32
0.29
0.90
0.91

103
0.63
0.32
0.72
0.91

3    104
0.67
0.51
0.75
0.77

sn

glove

sn

glove

table 3: the average relative errors and cosine sim-
ilarities between the vectors of pseudowords and
those predicted by theorem 2. m pairs of words are
randomly selected and for each pair, all occurrences
of the two words in the corpus is replaced by a pseu-
doword. then train the vectors for the pseudowords
on the new corpus.

with a pseudoword. third, train a set w2 of vectors
on the new corpus, while holding    xed the vectors
of words that were not involved in the pseudowords.
construction has ensured that each pseudoword has
two distinct    senses   , and we also have in w1 the
   ground truth    vectors for those senses.2 theorem 2
implies that the embedding of a pseudoword is a lin-
ear combination of the sense vectors, so we can com-
pare this predicted embedding to the one learned in
w2.3

suppose the trained vector for a pseudoword w
is uw and the predicted vector is vw,
then the
comparison criterion is the average relative error
1
where s is the set of all the
pseudowords. we also report the average cosine
similarity between vw   s and uw   s.

|s|pw   s

kuw   vwk2

kvwk2

2

2

table 3 shows the results for the glove and
sn (arora et al., 2016) vectors, averaged over 5
runs. when m is small, the error is small and the co-
sine similarity is as large as 0.9. even if m = 3    104

2note that this discussion assumes that the set of pseu-
dowords is small, so that a typical neighborhood of a pseu-
doword does not consist of other pseudowords. otherwise the
ground truth vectors in w1 become a bad approximation to the
sense vectors.

3here w2 is trained while    xing the vectors of words not
involved in pseudowords to be their pre-trained vectors in w1.
we can also train all the vectors in w2 from random initializa-
tion. such w2 will not be aligned with w1. then we can learn
a linear transformation from w2 to w1 using the vectors for the
words not involved in pseudowords, apply it on the vectors for
the pseudowords, and compare the transformed vectors to the
predicted ones. this is tested on id97, resulting in relative
errors between 20% and 32%, and cosine similarities between
0.86 and 0.92. these results again support our analysis.

vector type glove
0.72

cosine

skip-gram sn
0.76

0.73

table 4: the average cosine of the angles between
the vectors of words and the span of vector represen-
tations of its senses. the words tested are those in
the wsi task of semeval 2010.

(i.e., about 90% of the words in the vocabulary are
replaced by pseudowords), the cosine similarity re-
mains above 0.7, which is signi   cant in the 300 di-
mensional space. this provides positive support for
our analysis.
test 2. the second test is a proxy for what would
be a complete (but laborious) test of the linearity
assertion: replicating the thought experiment while
hand-labeling sense usage for many words in a cor-
pus. the simpler proxy is as follows. for each
word w, id138 (fellbaum, 1998) lists its vari-
ous senses by providing de   nition and example sen-
tences for each sense. this is enough text (roughly
a paragraph   s worth) for our theory to allow us to
represent it by a vector    speci   cally, apply the sif
sentence embedding followed by the linear transfor-
mation learned as in section 4.1. the text embed-
ding for sense s should approximate the ground truth
vector vs for it. then the linearity assertion pre-
dicts that embedding vw lies close to the subspace
spanned by the sense vectors.
(note that this is a
nontrivial event: in 300 dimensions a random vector
will be quite far from the subspace spanned by some
3 other random vectors.) table 4 checks this predic-
tion using the polysemous words appearing in the
wsi task of semeval 2010. we tested three stan-
dard id27 methods: glove, the skip-
gram variant of id97, and sn (arora et al.,
2016). the results show that the word vectors are
quite close to the subspace spanned by the senses.

5 experiments with atoms of discourse

the experiments use 300-dimensional embeddings
created using the sn objective in (arora et al., 2016)
and a wikipedia corpus of 3 billion tokens (wikime-
dia, 2012), and the sparse coding is solved by stan-
dard k-svd algorithm (damnjanovic et al., 2010).
experimentation showed that the best sparsity pa-
rameter k (i.e., the maximum number of allowed

senses per word) is 5, and the number of atoms m
is about 2000. for the number of senses k, we
tried plausible alternatives (based upon suggestions
of many colleagues) that allow k to vary for differ-
ent words, for example to let k be correlated with the
word frequency. but a    xed choice of k = 5 seems
to produce just as good results. to understand why,
realize that this method retains no information about
the corpus except for the low dimensional word em-
beddings. since the sparse coding tends to express
a word using fairly different atoms, examining (7)
w,j is bounded by approximately
kvwk2
2. so if too many   w,j   s are allowed to be
nonzero, then some must necessarily have small co-
ef   cients, which makes the corresponding compo-
nents indistinguishable from noise. in other words,
raising k often picks not only atoms corresponding
to additional senses, but also many that don   t.

shows that pj   2

the best number of atoms m was found to be
around 2000. this was estimated by re-running
the sparse coding algorithm multiple times with dif-
ferent random initializations, whereupon substantial
overlap was found between the two bases: a large
fraction of vectors in one basis were found to have
a very close vector in the other. thus combining
the bases while merging duplicates yielded a basis of
about the same size. around 100 atoms are used by
a large number of words or have no close-by words.
they appear semantically meaningless and are ex-
cluded by checking for this condition.4

the content of each atom can be discerned by
looking at the nearby words in cosine similarity.
some examples are shown in table 5. each word is
represented using at most    ve atoms, which usually
capture distinct senses (with some noise/mistakes).
the senses recovered for tie and spring are shown
in table 6. similar results can be obtained by using
other id27s like id97 and glove.

we also observe sparse coding procedures assign
nonnegative values to most coef   cients   w,j   s even
if they are left unrestricted. probably this is because
the appearances of a word are best explained by what
discourse is being used to generate it, rather than
what discourses are not being used.

4we think semantically meaningless atoms    i.e., unex-
plained inner products   exist because a simple language model
such as ours cannot explain all observed co-occurrences due to
grammar, stopwords, etc. it ends up needing smoothing terms.

616
membrane

231

825
instagram stakes
twitter
facebook
tumblr
vimeo
linkedin
reddit

atom 1978
drowning
suicides
overdose
murder
poisoning
commits
stabbing
strangulation myspace
gunshot

tweets

thoroughbred mitochondria
guineas
preakness
   lly
   llies
epsom
racecourse
sired

cytosol
cytoplasm
membranes
organelles
endoplasmic
proteins
vesicles

1638
slapping
pulling
plucking
squeezing
twisting
bowing
slamming
tossing
grabbing

330
149
conferences
orchestra
meetings
philharmonic
seminars
philharmonia
workshops
conductor
exhibitions
symphony
organizes
orchestras
toscanini
concerts
concertgebouw lectures
solti

presentations

table 5: some discourse atoms and their nearest 9 words. by equation (2), words most likely to appear in
a discourse are those nearest to it.

tie

season
teams

trousers
blouse
waistcoat winning
skirt
sleeved
pants

league
   nished
championship replay

operatic
soprano
mezzo

beginning dampers
scoreline wires
until
goalless
cables
months
equaliser wiring
earlier
clinching electrical contralto
baritone
scoreless wire
year
coloratura last
cable

brakes
summers
suspension    owering river
fork
absorbers
ppen
wheels
piney warm
damper

fragrant
lilies
   owered elk

creek humid
brook winters

temperatures

spring
   ower
   owers

table 6: five discourse atoms linked to the words tie and spring. each atom is represented by its nearest 6
words. the algorithm often makes a mistake in the last atom (or two), as happened here.

relationship to topic models. atoms of discourse
may be reminiscent of results from other automated
methods for obtaining a thematic understanding of
text, such as id96, described in the sur-
vey by blei (2012). this is not surprising since the
model (2) used to compute the embeddings is re-
lated to a log-linear topic model by mnih and hinton
(2007). however, the discourses here are computed
via sparse coding on id27s, which can
be seen as a id202ic alternative, resulting in
fairly    ne-grained topics. atoms are also reminis-
cent of coherent    word clusters    detected in the past
using brown id91, or even sparse coding (mur-
phy et al., 2012). the novelty in this paper is a clear
interpretation of the sparse coding results as atoms
of discourse, as well as its use to capture different
word senses.

6 testing wsi in applications

while the main result of the paper is to reveal the
id202ic structure of word senses within ex-
isting embeddings, it is desirable to verify that this
view can yield results competitive with earlier sense
embedding approaches. we report some tests be-

low. we    nd that common id27s per-
form similarly with our method; for concreteness we
use induced embeddings described in section 4.1.
they are evaluated in three tasks: word sense induc-
tion task in semeval 2010 (manandhar et al., 2010),
word similarity in context (huang et al., 2012), and
a new task we called police lineup test. the results
are compared to those of existing embedding based
approaches reported in related work (huang et al.,
2012; neelakantan et al., 2014; mu et al., 2017).

6.1 word sense induction

in the wsi task in semeval 2010, the algorithm is
given a polysemous word and about 40 pieces of
texts, each using it according to a single sense. the
algorithm has to cluster the pieces of text so that
those with the same sense are in the same cluster.
the evaluation criteria are f-score (artiles et al.,
2009) and v-measure (rosenberg and hirschberg,
2007). the f-score tends to be higher with a smaller
number of clusters and the v-measure tends to be
higher with a larger number of clusters, and fair eval-
uation requires reporting both.

given a word and its example texts, our algorithm
uses a bayesian analysis dictated by our theory to

compute a vector uc for the word in each context c
and and then applies id116 on these vectors, with
the small twist that sense vectors are assigned to
nearest centers based on inner products rather than
euclidean distances. table 7 shows the results.
computing vector uc. for word w we start by com-
puting its expansion in terms of atoms of discourse
(see (8) in section 3). in an ideal world the nonzero
coef   cients would exactly capture its senses, and
each text containing w would match to one of these
nonzero coef   cients. in the real world such deter-
ministic success is elusive and one must reason us-
ing bayes    rule.

for each atom a, word w and text c there is a joint
distribution p(w, a, c) describing the event that atom
a is the sense being used when word w was used
in text c. assuming that p(w, c|a) = p(w|a)p(c|a)
(similar to eqn (2)), the posterior distribution is:

method

v-measure

(huang et al., 2012)

(neelakantan et al., 2014)
(mu et al., 2017), k = 2
(mu et al., 2017), k = 5

ours, k = 2
ours, k = 3
ours, k = 4
ours, k = 5

10.60
9.00
7.30
14.50

6.1
7.4
9.9
11.5

f-score
38.05
47.26
57.14
44.07
58.55
55.75
51.85
46.38

table 7: performance of different vectors in the wsi
task of semeval 2010. the parameter k is the num-
ber of clusters used in the methods. rows are di-
vided into two blocks, the    rst of which shows the
results of the competitors, and the second shows
those of our algorithm. best results in each block
are in boldface.

p(a|c, w)     p(a|w)p(a|c)/p(a).

(10)

the last equation suggests de   ning the vector uc

we approximate p(a|w) using theorem 2, which
suggests that the coef   cients in the expansion of vw
with respect to atoms of discourse scale according to
probabilities of usage. (this assertion involves ig-
noring the low-order terms involving the logarithm
in the theorem statement.) also, by the random walk
model, p(a|c) can be approximated by exp(hva, vci)
where vc is the sif embedding of the context. fi-
nally, since p(a) = ec[p(a|c)], it can be empirically
estimated by randomly sampling c.

the posterior p(a|c, w) can be seen as a soft de-
coding of text c to atom a. if texts c1, c2 both contain
w, and they were hard decoded to atoms a1, a2 re-
spectively then their similarity would be hva1, va2 i.
with our soft decoding, the similarity can be de   ned
by taking the expectation over the full posterior:

similarity(c1, c2)

= e

ai   p(a|ci,w),i   {1,2}hva1, va2 i,

(11)

=*xa1

p(a1|c1, w)va1 ,xa2

p(a2|c2, w)va2+ .

at a high level this is analogous to the bayesian
polysemy model of reisinger and mooney (2010)
and brody and lapata (2009), except that they in-
troduced separate embeddings for each sense clus-
ter, while here we are working with structure already
existing inside id27s.

for the word w in the context c as

p(a|c, w)va,

(12)

uc =xa

which allows the similarity of the word in the two
contexts to be expressed via their inner product.
results. the results are reported in table 7. our
approach outperforms the results by huang et al.
(2012) and neelakantan et al. (2014). when com-
pared to mu et al. (2017), for the case with 2 centers,
we achieved better v-measure but lower f-score,
while for 5 centers, we achieved lower v-measure
but better f-score.

6.2 word similarity in context

the dataset consists of around 2000 pairs of words,
along with the contexts the words occur in and the
ground-truth similarity scores. the evaluation cri-
terion is the correlation between the ground-truth
scores and the predicted ones. our method computes
the estimated sense vectors and then the similarity as
in section 6.1. we compare to the baselines that sim-
ply use the cosine similarity of the glove/skip-gram
vectors, and also to the results of several existing
sense embedding methods.
results. table 8 shows that our result is better
than those of the baselines and mu et al. (2017),
but slightly worse than that of huang et al. (2012).

spearman coef   cient

word

method
glove

skip-gram

(huang et al., 2012)

(neelakantan et al., 2014)

(mu et al., 2017)

ours

0.573
0.622
0.657
0.567
0.637
0.652

table 8: the results for different methods in the task
of word similarity in context. the best result is in
boldface. our result is close to the best.

note that huang et al. (2012) retrained the vectors
for the senses on the corpus, while our method de-
pends only on senses extracted from the off-the-shelf
vectors. after all, our goal is to show word senses
already reside within off-the-shelf word vectors.

6.3 police lineup

evaluating wsi systems can run into well-known
dif   culties, as re   ected in the changing metrics over
the years (navigli and vannella, 2013). inspired by
word-intrusion tests for topic coherence (chang et
al., 2009), we proposed a new simple test, which has
the advantages of being easy to understand, and ca-
pable of being administered to humans.

the testbed uses 200 polysemous words and their
704 senses according to id138. each sense is
represented by 8 related words, which were col-
lected from id138 and online dictionaries by col-
lege students, who were told to identify most rele-
vant other words occurring in the online de   nitions
of this word sense as well as in the accompany-
ing illustrative sentences. these are considered as
ground truth representation of the word sense. these
8 words are typically not synonyms. for example,
for the tool/weapon sense of axe they were    handle,
harvest, cutting, split, tool, wood, battle, chop.   

the quantitative test is called police lineup. first,
randomly pick one of these 200 polysemous words.
second, pick the true senses for the word and then
add randomly picked senses from other words so
that there are n senses in total, where each sense is
represented by 8 related words as mentioned. fi-
nally, the algorithm (or human) is given the polyse-
mous word and a set of n senses, and has to identify
the true senses in this set. table 9 gives an example.

senses

1
2
3
4
5
6

navigate nocturnal mouse wing cave sonic    y dark
used hitting ball game match cricket play baseball
wink brie   y shut eyes wink bate quickly action
whereby legal court law lawyer suit bill judge
loose ends two loops shoelaces tie rope string
horny projecting bird oral nest horn hard food

bat

table 9: an example of the police lineup test with
n = 6. the algorithm (or human subject) is given
the polysemous word    bat    and n = 6 senses each of
which is represented as a list of words, and is asked
to identify the true senses belonging to    bat    (high-
lighted in boldface for demonstration).

algorithm 1 our method for the police lineup test

input: word w, list s of senses (each has 8 words)
output: t senses out of s

1: heuristically    nd in   ectional forms of w.
2: find 5 atoms for w and each in   ectional form. let

u denote the union of all these atoms.

3: initialize the set of candidate senses cw        , and

the score for each sense l to score(l)           

4: for each atom a     u do
rank senses l     s by
5:

6:

score(a, l) = s(a, l)   sl

a + s(w, l)     sl
v

add the two senses l with highest score(a, l) to
cw, and update their scores

score(l)     max{score(l), score(a, l)}

7: return the t senses l     cs with highest score(l)

our method (algorithm 1) uses the similarities
between any word (or atom) x and a set of words
y , de   ned as s(x, y ) = hvx, vy i where vy is the
sif embedding of y . it also uses the average simi-
larities:

sy

a = pa   a s(a, y )

|a|

, sy

v = pw   v s(w, y )

|v |

where a are all the atoms, and v are all the words.
we note two important practical details. first, while
we have been using atoms of discourse as a proxy
for word sense, these are too coarse-grained: the to-
tal number of senses (e.g., id138 synsets) is far
greater than 2000. thus the score(  ) function uses
both the atom and the word vector. second, some
words are more popular than the others   i.e., have
large components along many atoms and words   
which seems to be an instance of the smoothing

1

0.8

0.6

0.4

0.2

n
o
s

i

i

c
e
r
p

0

0

1

0.8

0.6

0.4

0.2

0

recall
precision

our method
mu et al, 2017
id97
native speaker
non-native speaker

0.2

0.4

0.6

0.8

1

10

20

recall

a

40

30
60
number of meanings m

50

70

80

b

figure 1: precision and recall in the police lineup test. (a) for each polysemous word, a set of n = 20 senses
containing the ground truth senses of the word are presented. human subjects are told that on average each
word has 3.5 senses and were asked to choose the senses they thought were true. the algorithms select t
senses for t = 1, 2, . . . , 6. for each t, each algorithm was run 5 times (standard deviations over the runs are
too small to plot). (b) the performance of our method for t = 4 and n = 20, 30, . . . , 70.

a and sl

phenomenon alluded to in footnote 4. the penalty
terms sl
v lower the scores of senses l con-
taining such words. finally, our algorithm returns t
senses where t can be varied.
results. the precision and recall for different n and
t (number of senses the algorithm returns) are pre-
sented in figure 1. our algorithm outperforms the
two selected competitors. for n = 20 and t = 4,
our algorithm succeeds with precision 65% and re-
call 75%, and performance remains reasonable for
n = 50. giving the same test to humans5 for n = 20
(see the left    gure) suggests that our method per-
forms similarly to non-native speakers.

other id27s can also be used in the
test and achieved slightly lower performance. for
n = 20 and t = 4, the precision/recall are lower by
the following amounts: glove 2.3%/5.76%, nnse
(id105 on pmi to rank 300 by murphy
et al. (2012)) 25%/28%.

7 conclusions

different senses of polysemous words have been
shown to lie in linear superposition inside standard
id27s like id97 and glove. this
has also been shown theoretically building upon

5human subjects are graduate students from science or engi-
neering majors at major u.s. universities. non-native speakers
have 7 to 10 years of english language use/learning.

previous generative models, and empirical tests of
this theory were presented. a priori, one imagines
that showing such theoretical results about the in-
ner structure of modern id27s would
be hopeless since they are solutions to complicated
nonid76.

a new wsi method is also proposed based upon
these insights that uses only the id27s
and sparse coding, and shown to provide very com-
petitive performance on some wsi benchmarks.
one novel aspect of our approach is that the word
senses are interrelated using one of about 2000 dis-
course vectors that give a succinct description of
which other words appear in the neighborhood with
that sense. our method based on sparse coding can
be seen as a id202ic analog of the cluster-
ing approaches, and also gives    ne-grained thematic
structure reminiscent of topic models.

a novel police lineup test was also proposed for
testing such wsi methods, where the algorithm is
given a word w and word clusters, some of which
belong to senses of w and the others are distractors
belonging to senses of other words. the algorithm
has to identify the ones belonging to w. we con-
jecture this police lineup test with distractors will
challenge some existing wsi methods, whereas our
method was found to achieve performance similar to
non-native speakers.

acknowledgements

we thank the reviewers and the action editors of
tacl for helpful feedbacks and thank the editors
for granting a special relaxation of the page limit for
our paper. this work was supported in part by nsf
grants ccf-1527371, dms-1317308, simons in-
vestigator award, simons collaboration grant, and
onr-n00014-16-1-2329. tengyu ma was addition-
ally supported by the simons award in theoretical
computer science and by the ibm ph.d. fellow-
ship.

references

arora et al. (2016 sanjeev arora, yuanzhi li, yingyu
liang, tengyu ma, and andrej risteski. 2016. a la-
tent variable model approach to pmi-based word em-
beddings. transaction of association for computa-
tional linguistics, pages 385   399.

arora et al. (2017 sanjeev arora, yingyu liang, and
tengyu ma. 2017. a simple but tough-to-beat base-
line for sentence embeddings. in in proceedings of in-
ternational conference on learning representations.
artiles et al. (2009 javier artiles, enrique amig  o, and
julio gonzalo. 2009. the role of named entities in
web people search. in proceedings of the 2009 con-
ference on empirical methods in natural language
processing, pages 534   542.

bengio et al. (2003 yoshua bengio, r  ejean ducharme,
pascal vincent, and christian jauvin. 2003. a neu-
ral probabilistic language model. journal of machine
learning research, pages 1137   1155.

blei (2012 david m. blei. 2012. probabilistic topic mod-
els. communication of the association for computing
machinery, pages 77   84.

2009. bayesian word sense induction.

brody and lapata (2009 samuel brody and mirella la-
pata.
in
proceedings of the 12th conference of the european
chapter of the association for computational linguis-
tics, pages 103   111.

chang et al. (2009 jonathan chang, sean gerrish, chong
wang, jordan l. boyd-graber, and david m. blei.
2009. reading tea leaves: how humans interpret topic
models. in advances in neural information process-
ing systems, pages 288   296.

church and hanks (1990 kenneth ward church and
patrick hanks.
1990. word association norms,
mutual information, and id69. computational
linguistics, pages 22   29.

damnjanovic et al. (2010 ivan damnjanovic, matthew
davies, and mark plumbley. 2010. smallbox     an

evaluation framework for sparse representations and
dictionary learning algorithms. in international con-
ference on latent variable analysis and signal sepa-
ration, pages 418   425.

di marco and navigli (2013 antonio di marco

and
roberto navigli.
2013. id91 and diversi-
fying web search results with graph-based word
sense induction. computational linguistics, pages
709   754.

faruqui et al. (2015 manaal faruqui, yulia tsvetkov,
dani yogatama, chris dyer, and noah a. smith.
2015. sparse overcomplete word vector representa-
tions.
in proceedings of association for computa-
tional linguistics, pages 1491   1500.

fellbaum (1998 christiane fellbaum. 1998. id138:

an electronic lexical database. mit press.

firth (1957 john rupert firth. 1957. a synopsis of lin-
guistic theory, 1930-1955. studies in linguistic anal-
ysis.

gittens et al. (2017 alex gittens, dimitris achlioptas,
and michael w mahoney. 2017. skip-gram     zipf
+ uniform = vector additivity. in proceedings of the
55th annual meeting of the association for computa-
tional linguistics (volume 1: long papers), volume 1,
pages 69   76.

grif   ths et al. (2007 thomas l. grif   ths, mark steyvers,
and joshua b. tenenbaum. 2007. topics in semantic
representation. psychological review, pages 211   244.
huang et al. (2012 eric h. huang, richard socher,
christopher d. manning, and andrew y. ng. 2012.
improving word representations via global context and
multiple word prototypes. in proceedings of the 50th
annual meeting of the association for computational
linguistics, pages 873   882.

iacobacci et al. (2015 ignacio

iacobacci,

moham-
mad taher pilehvar, and roberto navigli.
2015.
sensembed: learning sense embeddings for word and
relational similarity.
in proceedings of association
for computational linguistics, pages 95   105.

levy and goldberg (2014 omer levy and yoav gold-
berg. 2014. neural id27 as implicit ma-
trix factorization. in advances in neural information
processing systems, pages 2177   2185.

manandhar et al. (2010 suresh manandhar,

ioannis p
klapaftis, dmitriy dligach, and sameer s pradhan.
2010. semeval 2010: task 14: word sense induc-
tion & disambiguation. in proceedings of the 5th in-
ternational workshop on semantic evaluation, pages
63   68.

mikolov et al. (2013a tomas mikolov,

ilya sutskever,
kai chen, greg s. corrado, and jeff dean. 2013a.
distributed representations of words and phrases and
their compositionality. in advances in neural infor-
mation processing systems, pages 3111   3119.

mikolov et al. (2013b tomas mikolov, wen-tau yih, and
geoffrey zweig. 2013b. linguistic regularities in
continuous space word representations.
in proceed-
ings of the conference of the north american chapter
of the association for computational linguistics: hu-
man language technologies, pages 746   751.

schutze (1998 hinrich schutze. 1998. automatic word
computational linguistics,

sense discrimination.
pages 97   123.

tian et al. (2017 ran tian, naoaki okazaki, and kentaro
inui. 2017. the mechanism of additive composition.
machine learning, 106(7):1083   1130.

mnih and hinton (2007 andriy mnih and geoffrey hin-
ton. 2007. three new id114 for statistical
language modelling.
in proceedings of the 24th in-
ternational conference on machine learning, pages
641   648.

turney and pantel (2010 peter d. turney and patrick pan-
tel. 2010. from frequency to meaning: vector space
models of semantics. journal of arti   cial intelligence
research, pages 141   188.

wikimedia (2012 wikimedia. 2012. english wikipedia

dump. accessed march 2015.

yarowsky (1995 david yarowsky. 1995. unsupervised
id51 rivaling supervised meth-
ods. in proceedings of the 33rd annual meeting on as-
sociation for computational linguistics, pages 189   
196.

mu et al. (2017 jiaqi mu, suma bhat,

and pramod
viswanath.
in
proceedings of international conference on learning
representations.

2017. geometry of polysemy.

murphy et al. (2012 brian murphy, partha pratim taluk-
dar, and tom m. mitchell. 2012. learning effective
and interpretable semantic models using non-negative
sparse embedding.
in proceedings of the 24th in-
ternational conference on computational linguistics,
pages 1933   1950.

navigli and vannella (2013 roberto navigli and daniele
vannella. 2013. semeval 2013: task 11: word sense
induction and disambiguation within an end-user ap-
plication. in second joint conference on lexical and
computational semantics, pages 193   201.

neelakantan et al. (2014 arvind neelakantan,

jeevan
shankar, re passos, and andrew mccallum. 2014.
ef   cient nonparametric estimation of multiple em-
beddings per word in vector space.
in proceedings
of conference on empirical methods in natural
language processing, pages 1059   1069.

olshausen and field (1997 bruno olshausen and david
field. 1997. sparse coding with an overcomplete ba-
sis set: a strategy employed by v1? vision research,
pages 3311   3325.

pennington et al. (2014 jeffrey

pennington,

richard
socher, and christopher d. manning. 2014. glove:
global vectors for word representation. in proceed-
ings of the empiricial methods in natural language
processing, pages 1532   1543.

reisinger and mooney (2010 joseph reisinger and ray-
mond mooney. 2010. multi-prototype vector-space
models of word meaning. in proceedings of the con-
ference of the north american chapter of the associa-
tion for computational linguistics: human language
technologies, pages 107   117.

rosenberg and hirschberg (2007 andrew rosenberg and
julia hirschberg. 2007. v-measure: a conditional
id178-based external cluster evaluation measure. in
conference on empirical methods in natural lan-
guage processing and conference on computational
natural language learning, pages 410   420.

