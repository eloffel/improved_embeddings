7
1
0
2

 
r
a

 

m
2
2

 
 
]

g
l
.
s
c
[
 
 

3
v
7
6
1
2
0

.

1
1
6
1
:
v
i
x
r
a

published as a conference paper at iclr 2017

designing neural network architectures
using id23

bowen baker, otkrist gupta, nikhil naik & ramesh raskar
media laboratory
massachusetts institute of technology
cambridge ma 02139, usa
{bowen, otkrist, naik, raskar}@mit.edu

abstract

at present, designing convolutional neural network (id98) architectures requires
both human expertise and labor. new architectures are handcrafted by careful
experimentation or modi   ed from a handful of existing networks. we intro-
duce metaqnn, a meta-modeling algorithm based on id23 to
automatically generate high-performing id98 architectures for a given learning
task. the learning agent is trained to sequentially choose id98 layers using q-
learning with an  -greedy exploration strategy and experience replay. the agent
explores a large but    nite space of possible architectures and iteratively discovers
designs with improved performance on the learning task. on image classi   cation
benchmarks, the agent-designed networks (consisting of only standard convolu-
tion, pooling, and fully-connected layers) beat existing networks designed with
the same layer types and are competitive against the state-of-the-art methods that
use more complex layer types. we also outperform existing meta-modeling ap-
proaches for network design on image classi   cation tasks.

1

introduction

deep convolutional neural networks (id98s) have seen great success in the past few years on a
variety of machine learning problems (lecun et al., 2015). a typical id98 architecture consists
of several convolution, pooling, and fully connected layers. while constructing a id98, a network
designer has to make numerous design choices: the number of layers of each type, the ordering
of layers, and the hyperparameters for each type of layer, e.g., the receptive    eld size, stride, and
number of receptive    elds for a convolution layer. the number of possible choices makes the design
space of id98 architectures extremely large and hence, infeasible for an exhaustive manual search.
while there has been some work (pinto et al., 2009; bergstra et al., 2013; domhan et al., 2015) on
automated or computer-aided neural network design, new id98 architectures or network design ele-
ments are still primarily developed by researchers using new theoretical insights or intuition gained
from experimentation.
in this paper, we seek to automate the process of id98 architecture selection through a meta-
modeling procedure based on id23. we construct a novel id24 agent whose
goal is to discover id98 architectures that perform well on a given machine learning task with no
human intervention. the learning agent is given the task of sequentially picking layers of a id98
model. by discretizing and limiting the layer parameters to choose from, the agent is left with
a    nite but large space of model architectures to search from. the agent learns through random
exploration and slowly begins to exploit its    ndings to select higher performing models using the  -
greedy strategy (mnih et al., 2015). the agent receives the validation accuracy on the given machine
learning task as the reward for selecting an architecture. we expedite the learning process through
repeated memory sampling using experience replay (lin, 1993). we refer to this id24 based
meta-modeling method as metaqnn, which is summarized in figure 1.1
we conduct experiments with a space of model architectures consisting of only standard convolution,
pooling, and fully connected layers using three standard image classi   cation datasets: cifar-10,

1for more information, model    les, and code, please visit https://bowenbaker.github.io/metaqnn/

1

published as a conference paper at iclr 2017

figure 1: designing id98 architectures with id24: the agent begins by sampling a con-
volutional neural network (id98) topology conditioned on a prede   ned behavior distribution and
the agent   s prior experience (left block). that id98 topology is then trained on a speci   c task; the
topology description and performance, e.g. validation accuracy, are then stored in the agent   s mem-
ory (middle block). finally, the agent uses its memories to learn about the space of id98 topologies
through id24 (right block).

svhn, and mnist. the learning agent discovers id98 architectures that beat all existing networks
designed only with the same layer types (e.g., springenberg et al. (2014); srivastava et al. (2015)).
in addition, their performance is competitive against network designs that include complex layer
types and training procedures (e.g., clevert et al. (2015); lee et al. (2016)). finally, the metaqnn
selected models comfortably outperform previous automated network design methods (stanley &
miikkulainen, 2002; bergstra et al., 2013). the top network designs discovered by the agent on
one dataset are also competitive when trained on other datasets, indicating that they are suited for
id21 tasks. moreover, we can generate not just one, but several varied, well-performing
network designs, which can be ensembled to further boost the prediction performance.

2 related work

designing neural network architectures: research on automating neural network design goes
back to the 1980s when genetic algorithm-based approaches were proposed to    nd both architec-
tures and weights (schaffer et al., 1992). however, to the best of our knowledge, networks designed
with id107, such as those generated with the neat algorithm (stanley & miikkulainen,
2002), have been unable to match the performance of hand-crafted networks on standard bench-
marks (verbancsics & harguess, 2013). other biologically inspired ideas have also been explored;
motivated by screening methods in genetics, pinto et al. (2009) proposed a high-throughput network
selection approach where they randomly sample thousands of architectures and choose promising
ones for further training. in recent work, saxena & verbeek (2016) propose to sidestep the archi-
tecture selection process through densely connected networks of layers, which come closer to the
performance of hand-crafted networks.
bayesian optimization has also been used (shahriari et al., 2016) for automatic selection of network
architectures (bergstra et al., 2013; domhan et al., 2015) and hyperparameters (snoek et al., 2012;
swersky et al., 2013). notably, bergstra et al. (2013) proposed a meta-modeling approach based
on tree of parzen estimators (tpe) (bergstra et al., 2011) to choose both the type of layers and
hyperparameters of feed-forward networks; however, they fail to match the performance of hand-
crafted networks.
id23: recently there has been much work at the intersection of reinforcement
learning and deep learning. for instance, methods using id98s to approximate the id24 utility
function (watkins, 1989) have been successful in game-playing agents (mnih et al., 2015; silver
et al., 2016) and robotic control (lillicrap et al., 2015; levine et al., 2016). these methods rely on
phases of exploration, where the agent tries to learn about its environment through sampling, and
exploitation, where the agent uses what it learned about the environment to    nd better paths. in
traditional id23 settings, over-exploration can lead to slow convergence times, yet
over-exploitation can lead to convergence to local minima (kaelbling et al., 1996). however, in the
case of large or continuous state spaces, the  -greedy strategy of learning has been empirically shown
to converge (vermorel & mohri, 2005). finally, when the state space is large or exploration is costly,

2

agent samplesnetwork topologyagent learnsfrom memorytrain networkstore in replay memoryrqsamplememoryupdateq-valuesconvconvpoolsoftmaxtopology:    c(64,5,1)    c(128,3,1)    p(2,2)    sm(10)performance:     93.3%rpublished as a conference paper at iclr 2017

the experience replay technique (lin, 1993) has proved useful in experimental settings (adam et al.,
2012; mnih et al., 2015). we incorporate these techniques   id24, the  -greedy strategy and
experience replay   in our algorithm design.

3 background

our method relies on id24, a type of id23. we now summarize the theoret-
ical formulation of id24, as adopted to our problem. consider the task of teaching an agent
to    nd optimal paths as a markov decision process (mdp) in a    nite-horizon environment. con-
straining the environment to be    nite-horizon ensures that the agent will deterministically terminate
in a    nite number of time steps. in addition, we restrict the environment to have a discrete and
   nite state space s as well as action space u. for any state si     s, there is a    nite set of actions,
u(si)     u, that the agent can choose from. in an environment with stochastic transitions, an agent
in state si taking some action u     u(si) will transition to state sj with id203 ps(cid:48)|s,u(sj|si, u),
which may be unknown to the agent. at each time step t, the agent is given a reward rt, dependent
on the transition from state s to s(cid:48) and action u. rt may also be stochastic according to a distribution
pr|s(cid:48),s,u. the agent   s goal is to maximize the total expected reward over all possible trajectories, i.e.,
maxti   t rti, where the total expected reward for a trajectory ti is
er|s,u,s(cid:48)[r|s, u, s(cid:48)].

rti =(cid:80)

(1)

(s,u,s(cid:48))   ti

though we limit the agent to a    nite state and action space, there are still a combinatorially large
number of trajectories, which motivates the use of id23. we de   ne the maximiza-
tion problem recursively in terms of subproblems as follows. for any state si     s and subsequent
action u     u(si), we de   ne the maximum total expected reward to be q   (si, u). q   (  ) is known as
the action-value function and individual q   (si, u) are know as q-values. the recursive maximiza-
tion equation, which is known as bellman   s equation, can be written as

q   (si, u) = esj|si,u

(cid:2)er|si,u,sj [r|si, u, sj] +    maxu(cid:48)   u (sj ) q   (sj, u(cid:48))(cid:3) .
qt+1(si, u) = (1       )qt(si, u) +   (cid:2)rt +    maxu(cid:48)   u (sj ) qt(sj, u(cid:48))(cid:3) .

in many cases, it is impossible to analytically solve bellman   s equation (bertsekas, 2015), but it can
be formulated as an iterative update

(2)

(3)

equation 3 is the simplest form of id24 proposed by watkins (1989). for well formulated
problems, limt       qt(s, u) = q   (s, u), as long as each transition is sampled in   nitely many
times (bertsekas, 2015). the update equation has two parameters: (i)    is a id24 rate which
determines the weight given to new information over old information, and (ii)    is the discount fac-
tor which determines the weight given to short-term rewards over future rewards. the id24
algorithm is model-free, in that the learning agent can solve the task without ever explicitly con-
structing an estimate of environmental dynamics. in addition, id24 is off policy, meaning it
can learn about optimal policies while exploring via a non-optimal behavioral distribution, i.e. the
distribution by which the agent explores its environment.
we choose the behavior distribution using an  -greedy strategy (mnih et al., 2015). with this strat-
egy, a random action is taken with id203   and the greedy action, maxu   u (si) qt(si, u), is
chosen with id203 1      . we anneal   from 1     0 such that the agent begins in an exploration
phase and slowly starts moving towards the exploitation phase. in addition, when the exploration
cost is large (which is true for our problem setting), it is bene   cial to use the experience replay
technique for faster convergence (lin, 1992). in experience replay, the learning agent is provided
with a memory of its past explored paths and rewards. at a given interval, the agent samples from
the memory and updates its q-values via equation 3.

4 designing neural network architectures with id24

we consider the task of training a learning agent to sequentially choose neural network layers.
figure 2 shows feasible state and action spaces (a) and a potential trajectory the agent may take along
with the id98 architecture de   ned by this trajectory (b). we model the layer selection process as a
markov decision process with the assumption that a well-performing layer in one network should

3

published as a conference paper at iclr 2017

figure 2: markov decision process for id98 architecture generation: figure 2(a) shows the
full state and action space. in this illustration, actions are shown to be deterministic for clarity, but
they are stochastic in experiments. c(n, f, l) denotes a convolutional layer with n    lters, receptive
   eld size f, and stride l. p (f, l) denotes a pooling layer with receptive    eld size f and stride l. g
denotes a termination state (softmax/global average pooling). figure 2(b) shows a path the agent
may choose, highlighted in green, and the corresponding id98 topology.

also perform well in another network. we make this assumption based on the hierarchical nature of
the feature representations learned by neural networks with many hidden layers (lecun et al., 2015).
the agent sequentially selects layers via the  -greedy strategy until it reaches a termination state.
the id98 architecture de   ned by the agent   s path is trained on the chosen learning problem, and the
agent is given a reward equal to the validation accuracy. the validation accuracy and architecture
description are stored in a replay memory, and experiences are sampled periodically from the replay
memory to update q-values via equation 3. the agent follows an   schedule which determines its
shift from exploration to exploitation.
our method requires three main design choices: (i) reducing id98 layer de   nitions to simple state
tuples, (ii) de   ning a set of actions the agent may take, i.e., the set of layers the agent may pick next
given its current state, and (iii) balancing the size of the state-action space   and correspondingly, the
model capacity   with the amount of exploration needed by the agent to converge. we now describe
the design choices and the learning process in detail.

4.1 the state space

each state is de   ned as a tuple of all relevant layer parameters. we allow    ve different types of lay-
ers: convolution (c), pooling (p), fully connected (fc), global average pooling (gap), and softmax
(sm), though the general method is not limited to this set. table 1 shows the relevant parameters for
each layer type and also the discretization we chose for each parameter. each layer has a parameter
layer depth (shown as layer 1, 2, ... in figure 2). adding layer depth to the state space allows us
to constrict the action space such that the state-action graph is directed and acyclic (dag) and also
allows us to specify a maximum number of layers the agent may select before terminating.
each layer type also has a parameter called representation size (r-size). convolutional nets pro-
gressively compress the representation of the original signal through pooling and convolution. the
presence of these layers in our state space may lead the agent on a trajectory where the intermediate
signal representation gets reduced to a size that is too small for further processing. for example,    ve
2    2 pooling layers each with stride 2 will reduce an image of initial size 32    32 to size 1    1. at
this stage, further pooling, or convolution with receptive    eld size greater than 1, would be mean-
ingless and degenerate. to avoid such scenarios, we add the r-size parameter to the state tuple s,
which allows us to restrict actions from states with r-size n to those that have a receptive    eld size
less than or equal to n. to further constrict the state space, we chose to bin the representation sizes
into three discrete buckets. however, binning adds uncertainty to the state transitions: depending on
the true underlying representation size, a pooling layer may or may not change the r-size bin. as a
result, the action of pooling can lead to two different states, which we model as stochasticity in state
transitions. please see figure a1 in appendix for an illustrated example.

4

layer 1layer 2w11(1)w12(1)w13(1)w21(1)w22(1)w23(1)w31(1)w32(1)w33(1)input convolution64 filters3x3 receptive field1x1 stridesmax poolingsoftmaxinputc(64,3,1)p(2,2)c(64,3,1)ggggp(2,2)stateactioninputc(64,3,1)p(2,2)c(64,3,1)gggglayer 1layer 2c(64,3,1)c(64,3,1)gggglayer n-1layer np(2,2)p(2,2)p(2,2)(a)(b)published as a conference paper at iclr 2017

layer type

convolution (c)

pooling (p)

fully connected (fc)

termination state

layer parameters
i     layer depth
f     receptive    eld size
(cid:96)     stride
d     # receptive    elds
n     representation size
i     layer depth
(f, (cid:96))     (receptive    eld size, strides)
n     representation size
i     layer depth
n     # consecutive fc layers
d     # neurons
s     previous state
t     type

parameter values
< 12
square.     {1, 3, 5}
square. always equal to 1
    {64, 128, 256, 512}
    {(   , 8], (8, 4], (4, 1]}
< 12
    {(   , 8], (8, 4] and (4, 1]}
< 12

square.    (cid:8)(5, 3), (3, 2), (2, 2)(cid:9)

< 3    {512, 256, 128}

global avg. pooling/softmax

table 1: experimental state space. for each layer type, we list the relevant parameters and the
values each parameter is allowed to take.

4.2 the action space

we restrict the agent from taking certain actions to both limit the state-action space and make learn-
ing tractable. first, we allow the agent to terminate a path at any point, i.e. it may choose a termi-
nation state from any non-termination state. in addition, we only allow transitions for a state with
layer depth i to a state with layer depth i + 1, which ensures that there are no loops in the graph.
this constraint ensures that the state-action graph is always a dag. any state at the maximum layer
depth, as prescribed in table 1, may only transition to a termination layer.
next, we limit the number of fully connected (fc) layers to be at maximum two, because a large
number of fc layers can lead to too may learnable parameters. the agent at a state with type fc
may transition to another state with type fc if and only if the number of consecutive fc states is
less than the maximum allowed. furthermore, a state s of type fc with number of neurons d may
only transition to either a termination state or a state s(cid:48) of type fc with number of neurons d(cid:48)     d.
an agent at a state of type convolution (c) may transition to a state with any other layer type. an
agent at a state with layer type pooling (p) may transition to a state with any other layer type other
than another p state because consecutive pooling layers are equivalent to a single, larger pooling
layer which could lie outside of our chosen state space. furthermore, only states with representation
size in bins (8, 4] and (4, 1] may transition to an fc layer, which ensures that the number of weights
does not become unreasonably huge. note that a majority of these constraints are in place to enable
faster convergence on our limited hardware (see section 5) and not a limitation of the method in
itself.

4.3 id24 training procedure

for the iterative id24 updates (equation 3), we set the id24 rate (  ) to 0.01. in addition,
we set the discount factor (  ) to 1 to not over-prioritize short-term rewards. we decrease   from 1.0
to 0.1 in steps, where the step-size is de   ned by the number of unique models trained (table 2).
at   = 1.0, the agent samples id98 architecture with a random walk along a uniformly weighted
markov chain. every topology sampled by the agent is trained using the procedure described in
section 5, and the prediction performance of this network topology on the validation set is recorded.
we train a larger number of models at   = 1.0 as compared to other values of   to ensure that the
agent has adequate time to explore before it begins to exploit. we stop the agent at   = 0.1 (and not
at   = 0) to obtain a stochastic    nal policy, which generates perturbations of the global minimum.2
ideally, we want to identify several well-performing model topologies, which can then be ensembled
to improve prediction performance.
during the entire training process (starting at   = 1.0), we maintain a replay dictionary which stores
(i) the network topology and (ii) prediction performance on a validation set, for all of the sampled

2  = 0 indicates a completely deterministic policy. because we would like to generate several good models

for ensembling and analysis, we stop at   = 0.1, which represents a stochastic    nal policy.

5

published as a conference paper at iclr 2017

 

# models trained

1.0
1500

0.9
100

0.8
100

0.7
100

0.6
150

0.5
150

0.4
150

0.3
150

0.2
150

0.1
150

table 2:   schedule. the learning agent trains the speci   ed number of unique models at each  .

models. if a model that has already been trained is re-sampled, it is not re-trained, but instead the
previously found validation accuracy is presented to the agent. after each model is sampled and
trained, the agent randomly samples 100 models from the replay dictionary and applies the q-value
update de   ned in equation 3 for all transitions in each sampled sequence. the q-value update is
applied to the transitions in temporally reversed order, which has been shown to speed up q-values
convergence (lin, 1993).

5 experiment details

i

during the model exploration phase, we trained each network topology with a quick and aggressive
training scheme. for each experiment, we created a validation set by randomly taking 5,000 samples
from the training set such that the resulting class distributions were unchanged. for every network,
a dropout layer was added after every two layers. the ith dropout layer, out of a total n dropout
layers, had a dropout id203 of
2n. each model was trained for a total of 20 epochs with the
adam optimizer (kingma & ba, 2014) with   1 = 0.9,   2 = 0.999,    = 10   8. the batch size was
set to 128, and the initial learning rate was set to 0.001. if the model failed to perform better than a
random predictor after the    rst epoch, we reduced the learning rate by a factor of 0.4 and restarted
training, for a maximum of 5 restarts. for models that started learning (i.e., performed better than a
random predictor), we reduced the learning rate by a factor of 0.2 every 5 epochs. all weights were
initialized with xavier initialization (glorot & bengio, 2010). our experiments using caffe (jia
et al., 2014) took 8-10 days to complete for each dataset with a hardware setup consisting of 10
nvidia gpus.
after the agent completed the   schedule (table 2), we selected the top ten models that were found
over the course of exploration. these models were then    netuned using a much longer training
schedule, and only the top    ve were used for ensembling. we now provide details of the datasets
and the    netuning process.
the street view house numbers (svhn) dataset has 10 classes with a total of 73,257 samples
in the original training set, 26,032 samples in the test set, and 531,131 additional samples in the
extended training set. during the exploration phase, we only trained with the original training set,
using 5,000 random samples as validation. we    netuned the top ten models with the original plus
extended training set, by creating preprocessed training and validation sets as described by lee et al.
(2016). our    nal learning rate schedule after tuning on validation set was 0.025 for 5 epochs, 0.0125
for 5 epochs, 0.0001 for 20 epochs, and 0.00001 for 10 epochs.
cifar-10, the 10 class tiny image dataset, has 50,000 training samples and 10,000 testing samples.
during the exploration phase, we took 5,000 random samples from the training set for validation.
the maximum layer depth was increased to 18. after the experiment completed, we used the same
validation set to tune hyperparameters, resulting in a    nal training scheme which we ran on the
entire training set.
in the    nal training scheme, we set a learning rate of 0.025 for 40 epochs,
0.0125 for 40 epochs, 0.0001 for 160 epochs, and 0.00001 for 60 epochs, with all other parameters
unchanged. during this phase, we preprocess using global contrast id172 and use moderate
data augmentation, which consists of random mirroring and random translation by up to 5 pixels.
mnist, the 10 class handwritten digits dataset, has 60,000 training samples and 10,000 testing
samples. we preprocessed each image with global mean subtraction. in the    nal training scheme,
we trained each model for 40 epochs and decreased learning rate every 5 epochs by a factor of 0.2.
for further tuning details please see appendix c.

6 results

model selection analysis: from id24 principles, we expect the learning agent to improve
in its ability to pick network topologies as   reduces and the agent enters the exploitation phase. in

6

published as a conference paper at iclr 2017

figure 3: id24 performance. in the plots, the blue line shows a rolling mean of model
accuracy versus iteration, where in each iteration of the algorithm the agent is sampling a model.
each bar (in light blue) marks the average accuracy over all models that were sampled during the
exploration phase with the labeled  . as   decreases, the average accuracy goes up, demonstrating
that the agent learns to select better-performing id98 architectures.

method
maxout (goodfellow et al., 2013)
nin (lin et al., 2013)
fitnet (romero et al., 2014)
highway (srivastava et al., 2015)
vggnet (simonyan & zisserman, 2014)
all-id98 (springenberg et al., 2014)
metaqnn (ensemble)
metaqnn (top model)

cifar-10

9.38
8.81
8.39
7.72
7.25
7.25
7.32
6.92

svhn mnist cifar-100
2.47
2.35
2.42

38.57
35.68
35.04

0.45
0.47
0.51

-
-
-

2.06
2.28

-
-
-

0.32
0.44

-
-

33.71
27.14   

-

table 3: error rate comparison with id98s that only use convolution, pooling, and fully con-
nected layers. we report results for cifar-10 and cifar-100 with moderate data augmentation
and results for mnist and svhn without any data augmentation.

figure 3, we plot the rolling mean of prediction accuracy over 100 models and the mean accuracy
of models sampled at different   values, for the cifar-10 and svhn experiments. the plots show
that, while the prediction accuracy remains    at during the exploration phase (  = 1) as expected, the
agent consistently improves in its ability to pick better-performing models as   reduces from 1 to 0.1.
for example, the mean accuracy of models in the svhn experiment increases from 52.25% at   = 1
to 88.02% at   = 0.1. furthermore, we demonstrate the stability of the id24 procedure with
10 independent runs on a subset of the svhn dataset in section d.1 of the appendix. additional
analysis of id24 results can be found in section d.2.
the top models selected by the id24 agent vary in the number of parameters but all demon-
strate high performance (see appendix tables 1-3). for example, the number of parameters for the
top    ve cifar-10 models range from 11.26 million to 1.10 million, with only a 2.32% decrease
in test error. we    nd design motifs common to the top hand-crafted network architectures as well.
for example, the agent often chooses a layer of type c(n, 1, 1) as the    rst layer in the network.
these layers generate n learnable linear transformations of the input data, which is similar in spirit
to preprocessing of input data from rgb to a different color spaces such as yuv, as found in prior
work (sermanet et al., 2012; 2013).
prediction performance: we compare the prediction performance of the metaqnn networks dis-
covered by the id24 agent with state-of-the-art methods on three datasets. we report the accu-
racy of our best model, along with an ensemble of top    ve models. first, we compare metaqnn with
six existing architectures that are designed with standard convolution, pooling, and fully-connected
layers alone, similar to our designs. as seen in table 3, our top model alone, as well as the com-
mittee ensemble of    ve models, outperforms all similar models. next, we compare our results with
six top networks overall, which contain complex layer types and design ideas, including generalized
pooling functions, residual connections, and recurrent modules. our results are competitive with
these methods as well (table 4). finally, our method outperforms existing automated network de-

7

050010001500200025003000iterations0.000.100.200.300.400.500.600.700.800.901.00accuracyepsilon = 1.0.9.8.7.6.5.4.3.2.1svhn id24 performanceaverage accuracy per epsilonrolling mean model accuracy0500100015002000250030003500iterations0.000.100.200.300.400.500.600.700.800.901.00accuracyepsilon = 1.0.9.8.7.6.5.4.3.2.1cifar10 id24 performanceaverage accuracy per epsilonrolling mean model accuracypublished as a conference paper at iclr 2017

method
dropconnect (wan et al., 2013)
dsn (lee et al., 2015)
r-id98 (liang & hu, 2015)
metaqnn (ensemble)
metaqnn (top model)
resnet(110) (he et al., 2015)
resnet(1001) (he et al., 2016)
elu (clevert et al., 2015)
tree+max-avg (lee et al., 2016)

cifar-10

9.32
8.22
7.72
7.32
6.92
6.61
4.62
6.55
6.05

svhn mnist cifar-100
1.94
1.92
1.77
2.06
2.28

34.57
31.75
27.14   

0.57
0.39
0.31
0.32
0.44

-

-

-
-
-

-
-
-

1.69

0.31

-

22.71
24.28
32.37

table 4: error rate comparison with state-of-the-art methods with complex layer types. we re-
port results for cifar-10 and cifar-100 with moderate data augmentation and results for mnist
and svhn without any data augmentation.

dataset
training from scratch
finetuning
state-of-the-art

cifar-100

27.14
34.93

24.28 (clevert et al., 2015)

svhn
2.48
4.00

1.69 (lee et al., 2016)

mnist

0.80
0.81

0.31 (lee et al., 2016)

table 5: prediction error for the top metaqnn (cifar-10) model trained for other tasks. fine-
tuning refers to initializing training with the weights found for the optimal cifar-10 model.

sign methods. metaqnn obtains an error of 6.92% as compared to 21.2% reported by bergstra et al.
(2011) on cifar-10; and it obtains an error of 0.32% as compared to 7.9% reported by verbancsics
& harguess (2013) on mnist.
the difference in validation error between the top 10 models for mnist was very small, so we also
created an ensemble with all 10 models. this ensemble achieved a test error of 0.28%   which beats
the current state-of-the-art on mnist without data augmentation.
the best cifar-10 model performs 1-2% better than the four next best models, which is why the
ensemble accuracy is lower than the best model   s accuracy. we posit that the cifar-10 metaqnn
did not have adequate exploration time given the larger state space compared to that of the svhn
experiment, causing it to not    nd more models with performance similar to the best model. fur-
thermore, the coarse training scheme could have been not as well suited for cifar-10 as it was for
svhn, causing some models to under perform.
id21 ability: network designs such as vggnet (simonyan & zisserman, 2014) can
be adopted to solve a variety of id161 problems. to check if the metaqnn networks
provide similar id21 ability, we use the best metaqnn model on the cifar-10 dataset
for training other id161 tasks. the model performs well (table 5) both when training
from random initializations, and    netuning from existing weights.

7 concluding remarks

neural networks are being used in an increasingly wide variety of domains, which calls for scalable
solutions to produce problem-speci   c model architectures. we take a step towards this goal and
show that a meta-modeling approach using id23 is able to generate tailored id98
designs for different image classi   cation tasks. our metaqnn networks outperform previous meta-
modeling methods as well as hand-crafted networks which use the same types of layers.
while we report results for image classi   cation problems, our method could be applied to differ-
ent problem settings, including supervised (e.g., classi   cation, regression) and unsupervised (e.g.,
autoencoders). the metaqnn method could also aid constraint-based network design, by optimiz-
ing parameters such as size, speed, and accuracy. for instance, one could add a threshold in the
state-action space barring the agent from creating models larger than the desired limit. in addition,
   results in this column obtained with the top metaqnn architecture for cifar-10, trained from random

initialization with cifar-100 data.

8

published as a conference paper at iclr 2017

one could modify the reward function to penalize large models for constraining memory or penalize
slow forward passes to incentivize quick id136.
there are several future avenues for research in id23-driven network design as
well. in our current implementation, we use the same set of hyperparameters to train all network
topologies during the id24 phase and further    netune the hyperparameters for top models
selected by the metaqnn agent. however, our approach could be combined with hyperparameter
optimization methods to further automate the network design process. moreover, we constrict the
state-action space using coarse, discrete bins to accelerate convergence. it would be possible to
move to larger state-action spaces using methods for q-function approximation (bertsekas, 2015;
mnih et al., 2015).

acknowledgments

we thank peter downs for creating the project website and contributing to illustrations. we ac-
knowledge center for bits and atoms at mit for their help with computing resources. finally, we
thank members of camera culture group at mit media lab for their help and support.

references
sander adam, lucian busoniu, and robert babuska. experience replay for real-time reinforcement
learning control. ieee transactions on systems, man, and cybernetics, part c (applications and
reviews), 42(2):201   212, 2012.

james bergstra, daniel yamins, and david d cox. making a science of model search: hyperpa-
rameter optimization in hundreds of dimensions for vision architectures. icml (1), 28:115   123,
2013.

james s bergstra, r  emi bardenet, yoshua bengio, and bal  azs k  egl. algorithms for hyper-parameter

optimization. nips, pp. 2546   2554, 2011.

dimitri p bertsekas. id76 algorithms. athena scienti   c belmont, 2015.

djork-arn  e clevert, thomas unterthiner, and sepp hochreiter. fast and accurate deep network

learning by exponential linear units (elus). arxiv preprint arxiv:1511.07289, 2015.

tobias domhan, jost tobias springenberg, and frank hutter. speeding up automatic hyperparame-

ter optimization of deep neural networks by extrapolation of learning curves. ijcai, 2015.

xavier glorot and yoshua bengio. understanding the dif   culty of training deep feedforward neural

networks. aistats, 9:249   256, 2010.

ian j goodfellow, david warde-farley, mehdi mirza, aaron c courville, and yoshua bengio. max-

out networks. icml (3), 28:1319   1327, 2013.

kaiming he, xiangyu zhang, shaoqing ren, and jian sun. deep residual learning for image recog-

nition. arxiv preprint arxiv:1512.03385, 2015.

kaiming he, xiangyu zhang, shaoqing ren, and jian sun.

identity mappings in deep residual

networks. in european conference on id161, pp. 630   645. springer, 2016.

yangqing jia, evan shelhamer, jeff donahue, sergey karayev, jonathan long, ross girshick, ser-
gio guadarrama, and trevor darrell. caffe: convolutional architecture for fast feature embed-
ding. arxiv preprint arxiv:1408.5093, 2014.

leslie pack kaelbling, michael l littman, and andrew w moore. id23: a

survey. journal of arti   cial intelligence research, 4:237   285, 1996.

diederik kingma and jimmy ba. adam: a method for stochastic optimization. arxiv preprint

arxiv:1412.6980, 2014.

yann lecun, yoshua bengio, and geoffrey hinton. deep learning. nature, 521(7553):436   444,

2015.

9

published as a conference paper at iclr 2017

chen-yu lee, saining xie, patrick gallagher, zhengyou zhang, and zhuowen tu. deeply-

supervised nets. aistats, 2(3):6, 2015.

chen-yu lee, patrick w gallagher, and zhuowen tu. generalizing pooling functions in convolu-
tional neural networks: mixed, gated, and tree. international conference on arti   cial intelligence
and statistics, 2016.

sergey levine, chelsea finn, trevor darrell, and pieter abbeel. end-to-end training of deep visuo-

motor policies. jmlr, 17(39):1   40, 2016.

ming liang and xiaolin hu. recurrent convolutional neural network for object recognition. cvpr,

pp. 3367   3375, 2015.

timothy p lillicrap, jonathan j hunt, alexander pritzel, nicolas heess, tom erez, yuval tassa,
david silver, and daan wierstra. continuous control with deep id23. arxiv
preprint arxiv:1509.02971, 2015.

long-ji lin. self-improving reactive agents based on id23, planning and teaching.

machine learning, 8(3-4):293   321, 1992.

long-ji lin. id23 for robots using neural networks. technical report, dtic

document, 1993.

min lin, qiang chen, and shuicheng yan. network in network. arxiv preprint arxiv:1312.4400,

2013.

volodymyr mnih, koray kavukcuoglu, david silver, andrei a rusu, joel veness, marc g belle-
mare, alex graves, martin riedmiller, andreas k fidjeland, georg ostrovski, et al. human-level
control through deep id23. nature, 518(7540):529   533, 2015.

nicolas pinto, david doukhan, james j dicarlo, and david d cox. a high-throughput screening
approach to discovering good forms of biologically inspired visual representation. plos compu-
tational biology, 5(11):e1000579, 2009.

adriana romero, nicolas ballas, samira ebrahimi kahou, antoine chassang, carlo gatta, and

yoshua bengio. fitnets: hints for thin deep nets. arxiv preprint arxiv:1412.6550, 2014.

shreyas saxena and jakob verbeek. convolutional neural fabrics. in advances in neural informa-

tion processing systems 29, pp. 4053   4061. 2016.

j david schaffer, darrell whitley, and larry j eshelman. combinations of id107 and
neural networks: a survey of the state of the art. international workshop on combinations of
id107 and neural networks, pp. 1   37, 1992.

pierre sermanet, soumith chintala, and yann lecun. convolutional neural networks applied to

house numbers digit classi   cation. icpr, pp. 3288   3291, 2012.

pierre sermanet, koray kavukcuoglu, soumith chintala, and yann lecun. pedestrian detection

with unsupervised multi-stage id171. cvpr, pp. 3626   3633, 2013.

bobak shahriari, kevin swersky, ziyu wang, ryan p adams, and nando de freitas. taking the
human out of the loop: a review of bayesian optimization. proceedings of the ieee, 104(1):
148   175, 2016.

david silver, aja huang, chris j maddison, arthur guez, laurent sifre, george van den driessche,
julian schrittwieser, ioannis antonoglou, veda panneershelvam, marc lanctot, et al. mastering
the game of go with deep neural networks and tree search. nature, 529(7587):484   489, 2016.

karen simonyan and andrew zisserman. very deep convolutional networks for large-scale image

recognition. arxiv preprint arxiv:1409.1556, 2014.

jasper snoek, hugo larochelle, and ryan p adams. practical bayesian optimization of machine

learning algorithms. nips, pp. 2951   2959, 2012.

10

published as a conference paper at iclr 2017

jost tobias springenberg, alexey dosovitskiy, thomas brox, and martin riedmiller. striving for

simplicity: the all convolutional net. arxiv preprint arxiv:1412.6806, 2014.

rupesh kumar srivastava, klaus greff, and j  urgen schmidhuber. id199. arxiv preprint

arxiv:1505.00387, 2015.

kenneth o stanley and risto miikkulainen. evolving neural networks through augmenting topolo-

gies. evolutionary computation, 10(2):99   127, 2002.

kevin swersky, jasper snoek, and ryan p adams. multi-task bayesian optimization. nips, pp.

2004   2012, 2013.

phillip verbancsics and josh harguess. generative neuroevolution for deep learning. arxiv preprint

arxiv:1312.5355, 2013.

joannes vermorel and mehryar mohri. multi-armed bandit algorithms and empirical evaluation.

european conference on machine learning, pp. 437   448, 2005.

li wan, matthew zeiler, sixin zhang, yann l cun, and rob fergus. id173 of neural

networks using dropconnect. icml, pp. 1058   1066, 2013.

christopher john cornish hellaby watkins. learning from delayed rewards. phd thesis, university

of cambridge, england, 1989.

11

published as a conference paper at iclr 2017

appendix

a algorithm

we    rst describe the main components of the metaqnn algorithm. algorithm 1 shows the main
loop, where the parameter m would determine how many models to run for a given   and the
parameter k would determine how many times to sample the replay database to update q-values on
each iteration. the function train refers to training the speci   ed network and returns a validation
accuracy. algorithm 2 details the method for sampling a new network using the  -greedy strategy,
where we assume we have a function transition that returns the next state given a state and
action. finally, algorithm 3 implements the q-value update detailed in equation 3, with discounting
factor set to 1, for an entire state sequence in temporally reversed order.

algorithm 1 id24 for id98 topologies

initialize:

replay memory     [ ]
q     {(s, u)    s     s, u     u(s) : 0.5}
for episode = 1 to m do
s, u     sample new network( , q)
accuracy     train(s)
replay memory.append((s, u, accuracy))
for memory = 1 to k do

ssam p le, usam p le, accuracysam p le     uniform{replay memory}
q     update q values(q, ssam p le, usam p le, accuracysam p le)

end for

end for

algorithm 2 sample new network( , q)

initialize:

state sequence s = [sstart]
action sequence u = [ ]
while u [   1] (cid:54)= terminate do
       uniform[0, 1)
if    >   then

u = argmaxu   u (s[   1]) q[(s[   1], u)]
s(cid:48) = transition(s[   1], u)
u     uniform{u(s[   1])}
s(cid:48) = transition(s[   1], u)

else

end if
u.append(u)
if u != terminate then

s.append(s(cid:48))

end if
end while
return s, u

algorithm 3 update q values(q, s, u, accuracy)

q[s[   1], u [   1]] = (1       )q[s[   1], u [   1]] +       accuracy
for i = length(s)     2 to 0 do

q[s[i], u [i]] = (1       )q[s[i], u [i]] +    maxu   u (s[i+1]) q[s[i + 1], u]

end for
return q

12

published as a conference paper at iclr 2017

b representation size binning

as mentioned in section 4.1 of the main text, we introduce a parameter called representation size
to prohibit the agent from taking actions that can reduce the intermediate signal representation to
a size that is too small for further processing. however, this process leads to uncertainties in state
transitions, as illustrated in figure a1, which is handled by the standard id24 formulation.

(a)

(b)

(c)

figure a1: representation size binning: in this    gure, we show three example state transitions.
the true representation size (r-size) parameter is included in the    gure to show the true underlying
state. assuming there are two r-size bins, r-size bin1: [8,   ) and r-size bin2: (0, 7], figure a1a
shows the case where the initial state is in r-size bin1 and true representation size is 18. after the
agent chooses to pool with a 2   2    lter with stride 2, the true representation size reduces to 9 but the
r-size bin does not change. in figure a1b, the same 2    2 pooling layer with stride 2 reduces the
actual representation size of 14 to 7, but the bin changes to r-size bin2. therefore, in    gures a1a
and a1b, the agent ends up in different    nal states, despite originating in the same initial state and
choosing the same action. figure a1c shows that in our state-action space, when the agent takes an
action that reduces the representation size, it will have uncertainty in which state it will transition to.

c mnist experiment

we noticed that the    nal mnist models were prone to over   tting, so we increased dropout and
did a small grid search for the weight id173 parameter. for both tuning and    nal training,
we warmed the model with the learned weights from after the    rst epoch of initial training. the
   nal models and solvers can be found on our project website https://bowenbaker.github.io/metaqnn/.
figure a2 shows the id24 performance for the mnist experiment.

d further analysis of id24

figure 3 of the main text and figure a2 show that as the agent begins to exploit, it improves in
architecture selection. it is also informative to look at the distribution of models chosen at each  .
figure a4 gives further insight into the performance achieved at each   for both experiments.

d.1 id24 stability

because the id24 agent explores via a random or semi-random distribution, it is natural to
ask whether the agent can consistently improve architecture performance. while the success of the
three independent experiments described in the main text allude to stability, here we present further
evidence. we conduct 10 independent runs of the id24 procedure on 10% of the svhn
dataset (which corresponds to    7,000 training examples). we use a smaller dataset to reduce the
computation time of each independent run to 10gpu-days, as opposed to the 100gpu-days it would
take on the full dataset. as can be seen in figure a3, the id24 procedure with the exploration
schedule detailed in table 2 is fairly stable. the standard deviation at   = 1 is notably smaller
than at other stages, which we attribute to the large difference in number of samples at each stage.

13

p(2,2)      r-size: 18r-size bin: 1      r-size: 9r-size bin: 1p(2,2)      r-size: 7r-size bin: 2      r-size: 14r-size bin: 1statesactionsp12pr-size bin: 1r-size bin: 1r-size bin: 2p(2,2)published as a conference paper at iclr 2017

figure a2: mnist id24 performance. the blue line shows a rolling mean of model
accuracy versus iteration, where in each iteration of the algorithm the agent is sampling a model.
each bar (in light blue) marks the average accuracy over all models that were sampled during the
exploration phase with the labeled  . as   decreases, the average accuracy goes up, demonstrating
that the agent learns to select better-performing id98 architectures.

(a)

(b)

figure a3: figure a3a shows the mean model accuracy and standard deviation at each   over 10
independent runs of the id24 procedure on 10% of the svhn dataset. figure a3b shows the
mean model accuracy at each   for each independent experiment. despite some variance due to a
randomized exploration strategy, each independent run successfully improves architecture perfor-
mance.

furthermore, the best model found during each run had remarkably similar performance with a mean
accuracy of 88.25% and standard deviation of 0.58%, which shows that each run successfully found
at least one very high performing model. note that we did not use an extended training schedule to
improve performance in this experiment.

d.2 q-value analysis

we now analyze the actual q-values generated by the agent during the training process. the learning
agent iteratively updates the q-values of each path during the  -greedy exploration. each q-value
is initialized at 0.5. after the  -schedule is complete, we can analyze the    nal q-value associated
with each path to gain insights into the layer selection process. in the left column of figure a5, we
plot the average q-value for each layer type at different layer depths (for both svhn and cifar-
10) datasets. roughly speaking, a higher q-value associated with a layer type indicates a higher
id203 that the agent will pick that layer type. in figure a5, we observe that, while the average
q-value is higher for convolution and pooling layers at lower layer depths, the q-values for fully-
connected and termination layers (softmax and global average pooling) increase as we go deeper
into the network. this observation matches with traditional network designs.
we can also plot the average q-values associated with different layer parameters for further analysis.
in the right column of figure a5, we plot the average q-values for convolution layers with receptive

14

0500100015002000250030003500iterations0.000.100.200.300.400.500.600.700.800.901.00accuracyepsilon = 1.0.9.8.7.6.5.4.3.2.1mnist id24 performanceaverage accuracy per epsilonrolling mean model accuracy0.10.20.30.40.50.60.70.80.91.0epsilon0.450.500.550.600.650.700.750.80mean accuracyid24 stability (across 10 runs)0.10.20.30.40.50.60.70.80.91.0epsilon0.450.500.550.600.650.700.750.80mean accuracyid24 individual runspublished as a conference paper at iclr 2017

   eld sizes 1, 3, and 5 at different layer depths. the plots show that layers with receptive    eld size
of 5 have a higher q-value as compared to sizes 1 and 3 as we go deeper into the networks. this
indicates that it might be bene   cial to use larger receptive    eld sizes in deeper networks.
in summary, the id24 method enables us to perform analysis on the relative bene   ts of differ-
ent design parameters of our state space, and possibly gain insights for new id98 designs.

e top topologies selected by algorithm

in tables a1 through a3, we present the top    ve model architectures selected with id24
for each dataset, along with their prediction error reported on the test set, and their to-
tal number of parameters.
to download the caffe solver and prototext    les, please visit
https://bowenbaker.github.io/metaqnn/.

model architecture
[c(512,5,1), c(256,3,1), c(256,5,1), c(256,3,1), p(5,3), c(512,3,1),
c(512,5,1), p(2,2), sm(10)]
[c(128,1,1), c(512,3,1), c(64,1,1), c(128,3,1), p(2,2), c(256,3,1),
p(2,2), c(512,3,1), p(3,2), sm(10)]
[c(128,3,1), c(128,1,1), c(512,5,1), p(2,2), c(128,3,1), p(2,2),
c(64,3,1), c(64,5,1), sm(10)]
[c(256,3,1), c(256,3,1), p(5,3), c(256,1,1), c(128,3,1), p(2,2),
c(128,3,1), sm(10)]
[c(128,5,1), c(512,3,1), p(2,2), c(128,1,1), c(128,5,1), p(3,2),
c(512,3,1), sm(10)]

test error (%)

6.92

# params (106)

11.18

8.78

8.88

9.24

11.63

2.17

2.42

1.10

1.66

table a1: top 5 model architectures: cifar-10.

c(256,3,1),

c(256,5,1),

model architecture
[c(128,3,1), p(2,2), c(64,5,1), c(512,5,1), c(256,3,1), c(512,3,1),
p(2,2), c(512,3,1), c(256,5,1), c(256,3,1), c(128,5,1), c(64,3,1),
sm(10)]
[c(128,1,1), c(256,5,1), c(128,5,1), p(2,2), c(256,5,1), c(256,1,1),
c(256,3,1),
c(256,3,1),
c(128,3,1), sm(10)]
[c(128,5,1), c(128,3,1), c(64,5,1), p(5,3), c(128,3,1), c(512,5,1),
c(256,5,1), c(128,5,1), c(128,5,1), c(128,3,1), sm(10)]
[c(128,1,1), c(256,5,1), c(128,5,1), c(256,3,1), c(256,5,1), p(2,2),
c(128,1,1), c(512,3,1), c(256,5,1), p(2,2), c(64,5,1), c(64,1,1),
sm(10)]
[c(128,1,1), c(256,5,1), c(128,5,1), c(256,5,1), c(256,5,1),
c(256,1,1), p(3,2), c(128,1,1), c(256,5,1), c(512,5,1), c(256,3,1),
c(128,3,1), sm(10)]

c(512,5,1),

test error (%)

2.24

# params (106)

9.81

2.28

2.32

2.35

2.36

10.38

6.83

6.99

10.05

table a2: top 5 model architectures: svhn. note that we do not report the best accuracy on test
set from the above models in tables 3 and 4 from the main text. this is because the model that
achieved 2.28% on the test set performed the best on the validation set.

15

published as a conference paper at iclr 2017

model architecture
[c(64,1,1), c(256,3,1), p(2,2), c(512,3,1), c(256,1,1), p(5,3),
c(256,3,1), c(512,3,1), fc(512), sm(10)]
[c(128,3,1), c(64,1,1), c(64,3,1), c(64,5,1), p(2,2), c(128,3,1), p(3,2),
c(512,3,1), fc(512), fc(128), sm(10)]
[c(512,1,1), c(128,3,1), c(128,5,1), c(64,1,1), c(256,5,1), c(64,1,1),
p(5,3), c(512,1,1), c(512,3,1), c(256,3,1), c(256,5,1), c(256,5,1),
sm(10)]
[c(64,3,1), c(128,3,1), c(512,1,1), c(256,1,1), c(256,5,1), c(128,3,1),
p(5,3), c(512,1,1), c(512,3,1), c(128,5,1), sm(10)]
[c(64,3,1), c(128,1,1), p(2,2), c(256,3,1), c(128,5,1), c(64,1,1),
c(512,5,1), c(128,5,1), c(64,1,1), c(512,5,1), c(256,5,1), c(64,5,1),
sm(10)]
[c(64,1,1), c(256,5,1), c(256,5,1), c(512,1,1), c(64,3,1), p(5,3),
c(256,5,1), c(256,5,1), c(512,5,1), c(64,1,1), c(128,5,1), c(512,5,1),
sm(10)]
[c(128,3,1), c(512,3,1), p(2,2), c(256,3,1), c(128,5,1), c(64,1,1),
c(64,5,1), c(512,5,1), gap(10), sm(10)]
[c(256,3,1), c(256,5,1), c(512,3,1), c(256,5,1), c(512,1,1), p(5,3),
c(256,3,1), c(64,3,1), c(256,5,1), c(512,3,1), c(128,5,1), c(512,5,1),
sm(10)]
[c(512,5,1), c(128,5,1), c(128,5,1), c(128,3,1), c(256,3,1),
c(512,5,1), c(256,3,1), c(128,3,1), sm(10)]
[c(64,5,1), c(512,5,1), p(3,2), c(256,5,1), c(256,3,1), c(256,3,1),
c(128,1,1), c(256,3,1), c(256,5,1), c(64,1,1), c(256,3,1), c(64,3,1),
sm(10)]

test error (%)

0.35

# params (106)

5.59

0.38

0.40

0.41

0.43

0.44

0.44

0.46

0.55

0.56

7.43

8.28

6.27

8.10

9.67

3.52

12.42

7.25

7.55

table a3: top 10 model architectures: mnist. we report the top 10 models for mnist because
we included all 10 in our    nal ensemble. note that we do not report the best accuracy on test set
from the above models in tables 3 and 4 from the main text. this is because the model that achieved
0.44% on the test set performed the best on the validation set.

16

published as a conference paper at iclr 2017

(a)

(c)

(e)

(b)

(d)

(f)

figure a4: accuracy distribution versus  : figures a4a, a4c, and a4e show the accuracy dis-
tribution for each   for the svhn, cifar-10, and mnist experiments, respectively. figures a4b,
a4d, and a4f show the accuracy distributions for the initial   = 1 and the    nal   = 0.1. one can
see that the accuracy distribution becomes much more peaked in the high accuracy ranges at small  
for each experiment.

17

0.10.20.30.40.50.60.70.80.91.0validation accuracy0102030405060% modelsmodel accuracy distribution(svhn)epsilon0.10.20.30.40.50.60.70.80.91.00.10.20.30.40.50.60.70.80.91.0validation accuracy0102030405060% modelsmodel accuracy distribution(svhn)epsilon0.11.00.10.20.30.40.50.60.70.80.9validation accuracy05101520% modelsmodel accuracy distribution(cifar-10)epsilon0.10.20.30.40.50.60.70.80.91.00.10.20.30.40.50.60.70.80.9validation accuracy05101520% modelsmodel accuracy distribution(cifar-10)epsilon0.11.00.10.20.30.40.50.60.70.80.91.0validation accuracy020406080100% modelsmodel accuracy distribution(mnist)epsilon0.10.20.30.40.50.60.70.80.91.00.10.20.30.40.50.60.70.80.91.0validation accuracy020406080100% modelsmodel accuracy distribution(mnist)epsilon0.11.0published as a conference paper at iclr 2017

(a)

(c)

(e)

(b)

(d)

(f)

figure a5: average q-value versus layer depth for different layer types are shown in the left
column. average q-value versus layer depth for different receptive    eld sizes of the convolution
layer are shown in the right column.

18

02468101214layer depth0.00.20.40.60.81.0average q-valueaverage q-value vs. layer depth(svhn)convolutionfully connectedpoolingglobal average poolingsoftmax024681012layer depth0.50.60.70.80.91.0average q-valueaverage q-value vs. layer depthfor convolution layers (svhn)receptive field size 1receptive field size 3receptive field size 505101520layer depth0.00.20.40.60.81.0average q-valueaverage q-value vs. layer depth(cifar10)convolutionfully connectedpoolingglobal average poolingsoftmax024681012141618layer depth0.50.60.70.80.91.0average q-valueaverage q-value vs. layer depthfor convolution layers (cifar10)receptive field size 1receptive field size 3receptive field size 502468101214layer depth0.00.20.40.60.81.0average q-valueaverage q-value vs. layer depth(mnist)convolutionfully connectedpoolingglobal average poolingsoftmax024681012layer depth0.50.60.70.80.91.0average q-valueaverage q-value vs. layer depthfor convolution layers (mnist)receptive field size 1receptive field size 3receptive field size 5