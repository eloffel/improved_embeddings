listen, attend and spell

william chan

carnegie mellon university
williamchan@cmu.edu

navdeep jaitly, quoc v. le, oriol vinyals

{ndjaitly,qvl,vinyals}@google.com

google brain

5
1
0
2

 

g
u
a
0
2

 

 
 
]
l
c
.
s
c
[
 
 

2
v
1
1
2
1
0

.

8
0
5
1
:
v
i
x
r
a

abstract

we present listen, attend and spell (las), a neural network that learns to tran-
scribe speech utterances to characters. unlike traditional dnn-id48 models, this
model learns all the components of a speech recognizer jointly. our system has
two components: a listener and a speller. the listener is a pyramidal recurrent net-
work encoder that accepts    lter bank spectra as inputs. the speller is an attention-
based recurrent network decoder that emits characters as outputs. the network
produces character sequences without making any independence assumptions be-
tween the characters. this is the key improvement of las over previous end-to-
end ctc models. on a subset of the google voice search task, las achieves a
word error rate (wer) of 14.1% without a dictionary or a language model, and
10.3% with language model rescoring over the top 32 beams. by comparison, the
state-of-the-art cldnn-id48 model achieves a wer of 8.0%.

1

introduction

deep neural networks (dnns) have led to improvements in various components of speech recog-
nizers. they are commonly used in hybrid dnn-id48 id103 systems for acoustic
modeling [1, 2, 3, 4, 5, 6]. dnns have also produced signi   cant gains in pronunciation models that
map words to phoneme sequences [7, 8]. in id38, recurrent models have been shown
to improve id103 accuracy by rescoring n-best lists [9]. traditionally these compo-
nents     acoustic, pronunciation and language models     have all been trained separately, each with a
different objective. recent work in this area attempts to rectify this disjoint training issue by design-
ing models that are trained end-to-end     from speech directly to transcripts [10, 11, 12, 13, 14, 15].
two main approaches for this are connectionist temporal classi   cation (ctc) [10] and sequence
to sequence models with attention [16]. both of these approaches have limitations that we try to
address: ctc assumes that the label outputs are conditionally independent of each other; whereas
the sequence to sequence approach has only been applied to phoneme sequences [14, 15], and not
trained end-to-end for id103.
in this paper we introduce listen, attend and spell (las), a neural network that improves upon the
previous attempts [12, 14, 15]. the network learns to transcribe an audio sequence signal to a word
sequence, one character at a time. unlike previous approaches, las does not make independence
assumptions in the label sequence and it does not rely on id48s. las is based on the sequence to
sequence learning framework with attention [17, 18, 16, 14, 15]. it consists of an encoder recurrent
neural network (id56), which is named the listener, and a decoder id56, which is named the speller.
the listener is a pyramidal id56 that converts low level speech signals into higher level features.
the speller is an id56 that converts these higher level features into output utterances by specifying
a id203 distribution over sequences of characters using the attention mechanism [16, 14, 15].
the listener and the speller are trained jointly.
key to our approach is the fact that we use a pyramidal id56 model for the listener, which reduces
the number of time steps that the attention model has to extract relevant information from. rare and
out-of-vocabulary (oov) words are handled automatically, since the model outputs the character

1

sequence, one character at a time. another advantage of modeling characters as outputs is that the
network is able to generate multiple spelling variants naturally. for example, for the phrase    triple a   
the model produces both    triple a    and    aaa    in the top beams (see section 4.5). a model like ctc
may have trouble producing such diverse transcripts for the same utterance because of conditional
independence assumptions between frames.
in our experiments, we    nd that these components are necessary for las to work well. without the
attention mechanism, the model over   ts the training data signi   cantly, in spite of our large training
set of three million utterances - it memorizes the training transcripts without paying attention to the
acoustics. without the pyramid structure in the encoder side, our model converges too slowly - even
after a month of training, the error rates were signi   cantly higher than the errors we report here.
both of these problems arise because the acoustic signals can have hundreds to thousands of frames
which makes it dif   cult to train the id56s. finally, to reduce the over   tting of the speller to the
training transcripts, we use a sampling trick during training [19].
with these improvements, las achieves 14.1% wer on a subset of the google voice search task,
without a dictionary or a language model. when combined with language model rescoring, las
achieves 10.3% wer. by comparison, the google state-of-the-art cldnn-id48 system achieves
8.0% wer on the same data set [20].

2 related work

even though deep networks have been successfully used in many applications, until recently, they
have mainly been used in classi   cation: mapping a    xed-length vector to an output category [21].
for structured problems, such as mapping one variable-length sequence to another variable-length
sequence, neural networks have to be combined with other sequential models such as hidden
markov models (id48s) [22] and id49 (crfs) [23]. a drawback of this
combining approach is that the resulting models cannot be easily trained end-to-end and they make
simplistic assumptions about the id203 distribution of the data.
sequence to sequence learning is a framework that attempts to address the problem of learning
variable-length input and output sequences [17]. it uses an encoder id56 to map the sequential
variable-length input into a    xed-length vector. a decoder id56 then uses this vector to produce
the variable-length output sequence, one token at a time. during training, the model feeds the
groundtruth labels as inputs to the decoder. during id136, the model performs a id125 to
generate suitable candidates for next step predictions.
sequence to sequence models can be improved signi   cantly by the use of an attention mechanism
that provides the decoder id56 more information when it produces the output tokens [16]. at each
output step, the last hidden state of the decoder id56 is used to generate an attention vector over
the input sequence of the encoder. the attention vector is used to propagate information from the
encoder to the decoder at every time step, instead of just once, as with the original sequence to
sequence model [17]. this attention vector can be thought of as skip connections that allow the
information and the gradients to    ow more effectively in an id56.
the sequence to sequence framework has been used extensively for many applications: machine
translation [24, 25], image captioning [26, 27], parsing [28] and conversational modeling [29]. the
generality of this framework suggests that id103 can also be a direct application [14,
15].

3 model

in this section, we will
formally describe las which accepts acoustic features as in-
puts and emits english characters as outputs.
let x = (x1, . . . , xt ) be our input se-
quence of    lter bank spectra features, and let y = ((cid:104)sos(cid:105), y1, . . . , ys,(cid:104)eos(cid:105)), yi    
{a, b, c,       , z, 0,       , 9,(cid:104)space(cid:105),(cid:104)comma(cid:105),(cid:104)period(cid:105),(cid:104)apostrophe(cid:105),(cid:104)unk(cid:105)}, be the output se-
quence of characters. here (cid:104)sos(cid:105) and (cid:104)eos(cid:105) are the special start-of-sentence token, and end-of-
sentence tokens, respectively.

2

we want to model each character output yi as a conditional distribution over the previous characters
y<i and the input signal x using the chain rule:
p (y|x) =

p (yi|x, y<i)

(cid:89)

(1)

i

our listen, attend and spell (las) model consists of two sub-modules: the listener and the speller.
the listener is an acoustic model encoder, whose key operation is listen. the speller is an attention-
based character decoder, whose key operation is attendandspell. the listen function transforms
the original signal x into a high level representation h = (h1, . . . , hu ) with u     t , while the
attendandspell function consumes h and produces a id203 distribution over character se-
quences:

h = listen(x)

p (y|x) = attendandspell(h, y)

(2)
(3)

figure 1 visualizes las with these two components. we provide more details of these components
in the following sections.

speller

y2

y3

y4

(cid:104)eos(cid:105)

grapheme characters yi are
modelled by the
characterdistribution

c1

h

c2

h

s1

s2

(cid:104)sos(cid:105)

y2

y3

h

ys   1

attentioncontext creates
context vector ci from h
and si

long input sequence x is encoded with the pyramidal
blstm listen into shorter sequence h

h = (h1, . . . , hu )

listener

h1

h2

hu

x1

x2

x3

x4

x5

x6

x7

x8

xt

figure 1: listen, attend and spell (las) model: the listener is a pyramidal blstm encoding our input
sequence x into high level features h, the speller is an attention-based decoder generating the y characters
from h.

3

3.1 listen

the listen operation uses a bidirectional long short term memory id56 (blstm) [30, 31, 12]
with a pyramid structure. this modi   cation is required to reduce the length u of h, from t , the
length of the input x, because the input speech signals can be hundreds to thousands of frames long.
a direct application of blstm for the operation listen converged slowly and produced results
inferior to those reported here, even after a month of training time. this is presumably because the
operation attendandspell has a hard time extracting the relevant information from a large number
of input time steps.
we circumvent this problem by using a pyramid blstm (pblstm) similar to the clockwork id56
[33]. in each successive stacked pblstm layer, we reduce the time resolution by a factor of 2. in a
typical deep btlm architecture, the output at the i-th time step, from the j-th layer is computed as
follows:

i = blstm(hj
hj

)

(4)

in the pblstm model, we concatenate the outputs at consecutive steps of each layer before feeding
it to the next layer, i.e.:

i

i   1, hj   1
(cid:104)

hj   1

2i

(cid:105)

, hj   1

2i+1

hj
i = pblstm(hj

i   1,

)

(5)

in our model, we stack 3 pblstms on top of the bottom blstm layer to reduce the time resolution
23 = 8 times. this allows the attention model (see next section) to extract the relevant information
from a smaller number of times steps. in addition to reducing the resolution, the deep architecture al-
lows the model to learn nonlinear feature representations of the data. see figure 1 for a visualization
of the pblstm.
the pyramid structure also reduces the computational complexity. in the next section we show that
the attention mechanism over u features has a computational complexity of o(u s). thus, reducing
u speeds up learning and id136 signi   cantly.

3.2 attend and spell

we now describe the attendandspell function. the function is computed using an attention-based
lstm transducer [16, 15]. at every output step, the transducer produces a id203 distribution
over the next character conditioned on all the characters seen previously. the distribution for yi is
a function of the decoder state si and context ci. the decoder state si is a function of the previous
state si   1, the previously emitted character yi   1 and context ci   1. the context vector ci is produced
by an attention mechanism. speci   cally,

ci = attentioncontext(si, h)
si = id56(si   1, yi   1, ci   1)

p (yi|x, y<i) = characterdistribution(si, ci)

(6)
(7)
(8)

where characterdistribution is an mlp with softmax outputs over characters, and id56 is a 2
layer lstm.
at each time step, i, the attention mechanism, attentioncontext generates a context vector, ci
encapsulating the information in the acoustic signal needed to generate the next character. the
attention model is content based - the contents of the decoder state si are matched to the contents
of hu representing time step u of h, to generate an attention vector   i.   i is used to linearly blend
vectors hu to create ci.
speci   cally, at each decoder timestep i, the attentioncontext function computes the scalar energy
ei,u for each time step u, using vector hu     h and si. the scalar energy ei,u is converted into a
id203 distribution over times steps (or attention)   i using a softmax function. this is used to

4

(9)

(10)

(11)

(13)
(14)

create the context vector ci by linearly blending the listener features, hu, at different time steps:

ei,u = (cid:104)  (si),   (hu)(cid:105)

(cid:80)
(cid:88)

exp(ei,u)
u exp(ei,u)
  i,uhu

  i,u =

ci =

where    and    are mlp networks. on convergence, the   i distribution is typically very sharp, and
focused on only a few frames of h; ci can be seen as a continuous bag of weighted features of h.
figure 1 shows las architecture.

u

3.3 learning

the listen and attendandspell functions can be trained jointly for end-to-end id103.
the sequence to sequence methods condition the next step prediction on the previous characters [17,
16] and maximizes the log id203:

log p (yi|x, y   

<i;   )

(12)

(cid:88)

i

max

  

<i is the groundtruth of the previous characters.

where y   
however during id136, the groundtruth is missing and the predictions can suffer because the
model was not trained to be resilient to feeding in bad predictions at some time steps. to ameliorate
this effect, we use a trick that was proposed in [19]. during training, instead of always feeding in the
ground truth transcript for next step prediction, we sometimes sample from our previous character
distribution and use that as the inputs in the next step predictions:

  yi     characterdistribution(si, ci)

log p (yi|x,   y<i;   )

(cid:88)

i

max

  

where   yi   1 is the character chosen from the ground truth, or sampled from the model with a certain
sampling rate. unlike [19], we do not use a schedule and simply use a constant sampling rate of
10% right from the start of training.
as the system is a very deep network it may appear that some type of pretraining would be required.
however, in our experiments, we found no need for pretraining.
in particular, we attempted to
pretrain the listen function with context independent or context dependent phonemes generated
from a conventional gmm-id48 system. a softmax network was attached to the output units
hu     h of the listener and used to make multi-frame phoneme state predictions [34] but led to no
improvements. we also attempted to use the phonemes as a joint objective target [35], but found no
improvements.

3.4 decoding and rescoring

during id136 we want to    nd the most likely character sequence given the input acoustics:

  y = arg max

y

log p (y|x)

(15)

decoding is performed with a simple left-to-right id125 algorithm similar to [17]. we main-
tain a set of    partial hypotheses, starting with the start-of-sentence (cid:104)sos(cid:105) token. at each timestep,
each partial hypothesis in the beam is expanded with every possible character and only the    most
likely beams are kept. when the (cid:104)eos(cid:105) token is encountered, it is removed from the beam and added
to the set of complete hypothesis. a dictionary can optionally be added to constrain the search space
to valid words, however we found that this was not necessary since the model learns to spell real
words almost all the time.
we have vast quantities of text data [36], compared to the amount of transcribed speech utterances.
we can use language models trained on text corpora alone similar to conventional speech systems

5

[37]. to do so we can rescore our beams with the language model. we    nd that our model has a
small bias for shorter utterances so we normalize our probabilities by the number of characters |y|c
in the hypothesis and combine it with a language model id203 plm(y):

s(y|x) =

log p (y|x)

|y|c

+    log plm(y)

(16)

where    is our language model weight and can be determined by a held-out validation set.

4 experiments

we used a dataset approximately three million google voice search utterances (representing 2000
hours of data) for our experiments. approximately 10 hours of utterances were randomly selected as
a held-out validation set. data augmentation was performed using a room simulator, adding different
types of noise and reverberations; the noise sources were obtained from youtube and environmental
recordings of daily events [20]. this increased the amount of audio data by 20 times. 40-dimensional
log-mel    lter bank features were computed every 10ms and used as the acoustic inputs to the listener.
a separate set of 22k utterances representing approximately 16 hours of data were used as the test
data. a noisy test data set was also created using the same corruption strategy that was applied to
the training data. all training sets are anonymized and hand-transcribed, and are representative of
googles speech traf   c.
the text was normalized by converting all characters to lower case english alphanumerics (including
digits). the punctuations: space, comma, period and apostrophe were kept, while all other tokens
were converted to the unknown (cid:104)unk(cid:105) token. as mentioned earlier, all utterances were padded with
the start-of-sentence (cid:104)sos(cid:105) and the end-of-sentence (cid:104)eos(cid:105) tokens.
the state-of-the-art model on this dataset is a cldnn-id48 system that was described in [20].
the cldnn system achieves a wer of 8.0% on the clean test set and 8.9% on the noisy test set.
however, we note that the cldnn uses unidirectional cldnns and would certainly bene   t even
further from the use of a bidirectional cldnn architecture.
for the listen function we used 3 layers of 512 pblstm nodes (i.e., 256 nodes per direction) on
top of a blstm that operates on the input. this reduced the time resolution by 8 = 23 times. the
spell function used a two layer lstm with 512 nodes each. the weights were initialized with a
uniform distribution u(   0.1, 0.1).
asynchronous stochastic id119 (asgd) was used for training our model [38]. a learn-
ing rate of 0.2 was used with a geometric decay of 0.98 per 3m utterances (i.e., 1/20-th of an epoch).
we used the distbelief framework [38] with 32 replicas, each with a minibatch of 32 utterances.
in order to further speed up training, the sequences were grouped into buckets based on their frame
length [17].
the model was trained using groundtruth previous characters until results on the validation set
stopped improving. this took approximately two weeks. the model was decoded using beam width
   = 32 and achieved 16.2% wer on the clean test set and 19.0% wer on the noisy test set without
any dictionary or language model. we found that constraining the id125 with a dictionary had
no impact on the wer. rescoring the top 32 beams with the same id165 language model that was
used by the cldnn system using a language model weight of    = 0.008 improved the results for
the clean and noisy test sets to 12.6% and 14.7% respectively. note that for convenience, we did not
decode with a language model, but rather only rescored the top 32 beams. it is possible that further
gains could have been achieved by using the language model during decoding.
as mentioned in section 3.3, there is a mismatch between training and testing. during training
the model is conditioned on the correct previous characters but during testing mistakes made by
the model corrupt future predictions. we trained another model by sampling from our previous
character distribution with a id203 of 10% (we did not use a schedule as described in [19]).
this improved our results on the clean and noisy test sets to 14.1% and 16.5% wer respectively
when no language model rescoring was used. with language model rescoring, we achevied 10.3%
and 12.0% wer on the clean and noisy test sets, respectively. table 1 summarizes these results.
on the clean test set, this model is within 2.5% absolute wer of the state-of-the-art cldnn-id48
system, while on the noisy set it is less than 3.0% absolute wer worse. we suspect that convolu-

6

table 1: wer comparison on the clean and noisy google voice search task. the cldnn-id48 system is
the state-of-the-art system, the listen, attend and spell (las) models are decoded with a beam size of 32.
language model (lm) rescoring was applied to our beams, and a sampling trick was applied to bridge the gap
between training and id136.

model
cldnn-id48 [20]
las
las + lm rescoring
las + sampling
las + sampling + lm rescoring

clean wer noisy wer
8.0
16.2
12.6
14.1
10.3

8.9
19.0
14.7
16.5
12.0

tional    lters could lead to improved results, as they have been reported to improve performance by
5% relative wer on clean speech and 7% relative on noisy speech compared to non-convolutional
architectures [20].

4.1 attention visualization

figure 2: alignments between character outputs and audio signal produced by the listen, attend and spell
(las) model for the utterance    how much would a woodchuck chuck   . the content based attention mechanism
was able to identify the start position in the audio sequence for the    rst character correctly. the alignment
produced is generally monotonic without a need for any location based priors.

the content-based attention mechanism creates an explicit alignment between the characters and
audio signal. we can visualize the attention mechanism by recording the attention distribution on
the acoustic sequence at every character output timestep. figure 2 visualizes the attention align-
ment between the characters and the    lterbanks for the utterance    how much would a woodchuck
chuck   . for this particular utterance, the model learnt a monotonic distribution without any location
priors. the words    woodchuck    and    chuck    have acoustic similarities, the attention mechanism
was slightly confused when emitting    woodchuck    with a dilution in the distribution. the attention
model was also able to identify the start and end of the utterance properly.

7

in the following sections, we report results of control experiments that were conducted to understand
the effects of beam widths, utterance lengths and word frequency on the wer of our model.

4.2 effects of beam width

we investigate the correlation between the performance of the model and the width of id125,
with and without the language model rescoring. figure 3 shows the effect of the decode beam width,
  , on the wer for the clean test set. we see consistent wer improvements by increasing the beam
width up to 16, after which we observe no signi   cant bene   ts. at a beam width of 32, the wer
is 14.1% and 10.3% after language model rescoring. rescoring the top 32 beams with an oracle
produces a wer of 4.3% on the clean test set and 5.5% on the noisy test set.

figure 3: the effect of the decode beam width on wer for the clean google voice search task. the reported
wers are without a dictionary or language model, with language model rescoring and the oracle wer for
different beam widths. the    gure shows that good results can be obtained even with a relatively small beam
size.

4.3 effects of utterance length

we measure the performance of our model as a function of the number of words in the utterance. we
expect the model to do poorly on longer utterances due to limited number of long training utterances
in our distribution. hence it is not surprising that longer utterances have a larger error rate. the
deletions dominate the error for long utterances, suggesting we may be missing out on words. it is
surprising that short utterances (e.g., 2 words or less) perform quite poorly. here, the substitutions
and insertions are the main sources of errors, suggesting the model may split words apart.
figure 4 also suggests that our model struggles to generalize to long utterances when trained on a
distribution of shorter utterances. it is possible location-based priors may help in these situations as
reported by [15].

4.4 word frequency

we study the performance of our model on rare words. we use the recall metric to indicate whether
a word appears in the utterance regardless of position (higher is better). figure 5 reports the recall
of each word in the test distribution as a function of the word frequency in the training distribution.
rare words have higher variance and lower recall while more frequent words typically have higher

8

12481632beam width05101520werwerwer lmwer oraclebeam width vs. werfigure 4: the correlation between error rates (insertion, deletion, substitution and wer) and the number of
words in an utterance. the wer is reported without a dictionary or language model, with language model
rescoring and the oracle wer for the clean google voice search task. the data distribution with respect to the
number of words in an utterance is overlaid in the    gure. las performs poorly with short utterances despite
an abundance of data. las also fails to generalize well on longer utterances when trained on a distribution
of shorter utterances. insertions and substitutions are the main sources of errors for short utterances, while
deletions dominate the error for long utterances.

recall. the word    and    occurs 85k times in the training set, however it has a recall of only 80% even
after language model rescoring. the word    and    is frequently mis-transcribed as    in    (which has
95% recall). this suggests improvements are needed in the language model. by contrast, the word
   walkerville    occurs just once in the training set but it has a recall of 100%. this suggests that the
recall for a word depends both on its frequency in the training set and its acoustic uniqueness.

4.5

interesting decoding examples

in this section, we show the outputs of the model on several utterances to demonstrate the capabilities
of las. all the results in this section are decoded without a dictionary or a language model.
during our experiments, we observed that las can learn multiple spelling variants given the same
acoustics. table 2 shows top beams for the utterance that includes    triple a   . as can be seen,
the model produces both    triple a    and    aaa    within the top four beams. the decoder is able to
generate such varied parses, because the next step prediction model makes no assumptions on the
id203 distribution by using the chain rule decomposition. it would be dif   cult to produce such
differing transcripts using ctc due to the conditional independence assumptions, where p(yi|x)
is conditionally independent of p(yi+1|x). conventional dnn-id48 systems would require both
spellings to be in the pronunciation dictionary to generate both spelling permutations.
it can also be seen that the model produced    xxx    even though acoustically    x    is very different
from    a    - this is presumably because the language model overpowers the acoustic signal in this
case. in the training corpus    xxx    is a very common phrase and we suspect the language model

9

510152025+number of words in utterance0102030405060708090percentagedata distributioninsertiondeletionsubstitutionwerwer lmwer oracleutterance length vs. errorfigure 5: the correlation between word frequency in the training distribution and recall in the test distribution.
in general, rare words report worse recall compared to more frequent words.

table 2: example 1:    triple a    vs.    aaa    spelling variants.

beam text
truth
1
2
3
4

call aaa roadside assistance
call aaa roadside assistance
call triple a roadside assistance
call trip way roadside assistance
call xxx roadside assistance

log id203 wer
-
0.00
50.00
50.00
25.00

-
-0.5740
-1.5399
-3.5012
-4.4375

implicit in the speller learns to associate    triple    with    xxx   . we note that    triple a    occurs 4 times
in the training distribution and    aaa    (when pronounced    triple a    rather than    a   -   a   -   a   ) occurs
only once in the training distribution.
we are also surprised that the model is capable of handling utterances with repeated words despite
the fact that it uses content-based attention. table 3 shows an example of an utterance with a repeated
word. since las implements content-based attention, it is expected it to    lose its attention    during
the decoding steps and produce a word more or less times than the number of times the word was
spoken. as can be seen from this example, even though    seven    is repeated three times, the model
successfully outputs    seven    three times. this hints that location-based priors (e.g., location based
attention or location based id173) may not be needed for repeated contents.

table 3: example 2: repeated    seven   s.

beam text
truth
1
2
3
4

eight nine four minus seven seven seven
eight nine four minus seven seven seven
eight nine four nine seven seven seven
eight nine four minus seven seventy seven
eight nine four nine s seven seven seven

log id203 wer
-
0.00
14.29
14.29
28.57

-
-0.2145
-1.9071
-4.7316
-5.1252

10

100101102103104105106word frequency0102030405060708090100word recall percentageword frequency vs. recall5 conclusions

we have presented listen, attend and spell (las), an attention-based neural network that can di-
rectly transcribe acoustic signals to characters. las is based on the sequence to sequence framework
with a pyramid structure in the encoder that reduces the number of timesteps that the decoder has
to attend to. las is trained end-to-end and has two main components. the    rst component, the
listener, is a pyramidal acoustic id56 encoder that transforms the input sequence into a high level
feature representation. the second component, the speller, is an id56 decoder that attends to the
high level features and spells out the transcript one character at a time. our system does not use
the concepts of phonemes, nor does it rely on pronunciation dictionaries or id48s. we bypass the
conditional independence assumptions of ctc, and show how we can learn an implicit language
model that can generate multiple spelling variants given the same acoustics. to further improve
the results, we used samples from the softmax classi   er in the decoder as inputs to the next step
prediction during training. finally, we showed how a language model trained on additional text can
be used to rerank our top hypotheses.

acknowledgements

we thank tara sainath, babak damavandi for helping us with the data, language models and for
helpful comments. we also thank andrew dai, ashish agarwal, samy bengio, eugene brevdo,
greg corrado, andrew dai, jeff dean, rajat monga, christopher olah, mike schuster, noam
shazeer, ilya sutskever, vincent vanhoucke and the google brain team for helpful comments, sug-
gestions and technical assistance.

references

[1] nathaniel morgan and herve bourlard. continuous id103 using multilayer per-
ceptrons with id48. in ieee international conference on acoustics, speech
and signal processing, 1990.

[2] abdel-rahman mohamed, george e. dahl, and geoffrey e. hinton. id50 for
phone recognition. in neural information processing systems: workshop on deep learning
for id103 and related applications, 2009.

[3] george e. dahl, dong yu, li deng, and alex acero. large vocabulary continuous speech
recognition with context-dependent dbn-id48s. in ieee international conference on acous-
tics, speech and signal processing, 2011.

[4] abdel-rahman mohamed, george e. dahl, and geoffrey hinton. acoustic modeling us-
ing id50. ieee transactions on audio, speech, and language processing,
20(1):14   22, 2012.

[5] navdeep jaitly, patrick nguyen, andrew w. senior, and vincent vanhoucke. application
in inter-

of pretrained deep neural networks to large vocabulary id103.
speech, 2012.

[6] tara sainath, abdel-rahman mohamed, brian kingsbury, and bhuvana ramabhadran. deep
convolutional neural networks for lvcsr. in ieee international conference on acoustics,
speech and signal processing, 2013.

[7] kanishka rao, fuchun peng, hasim sak, and francoise beaufays. grapheme-to-phoneme
conversion using long short-term memory recurrent neural networks. in ieee international
conference on acoustics, speech and signal processing, 2015.

[8] kaisheng yao and geoffrey zweig. sequence-to-sequence neural net models for grapheme-

to-phoneme conversion. 2015.

[9] tomas mikolov, kara   at martin, burget luka, eernocky jan, and khudanpur sanjeev. recur-

rent neural network based language model. in interspeech, 2010.

[10] alex graves, santiago fernandez, faustino gomez, and jurgen schmiduber. connectionist
temporal classi   cation: labelling unsegmented sequence data with recurrent neural net-
works. in international conference on machine learning, 2006.

11

[11] alex graves. sequence transduction with recurrent neural networks. in international con-

ference on machine learning: representation learning workshop, 2012.

[12] alex graves and navdeep jaitly. towards end-to-end id103 with recurrent

neural networks. in international conference on machine learning, 2014.

[13] awni hannun, carl case, jared casper, bryan catanzaro, greg diamos, erich elsen, ryan
prenger, sanjeev satheesh, shubho sengupta, adam coates, and andrew ng. deep speech:
scaling up end-to-end id103. in http://arxiv.org/abs/1412.5567, 2014.

[14] jan chorowski, dzmitry bahdanau, kyunghyun cho, and yoshua bengio. end-to-end con-
tinuous id103 using attention-based recurrent nn: first results. in neural in-
formation processing systems: workshop deep learning and representation learning work-
shop, 2014.

[15] jan chorowski, dzmitry bahdanau, dmitriy serdyuk, kyunghyun cho, and yoshua bengio.

attention-based models for id103. in http://arxiv.org/abs/1506.07503, 2015.

[16] dzmitry bahdanau, kyunghyun cho, and yoshua bengio. id4 by
jointly learning to align and translate. in international conference on learning representa-
tions, 2015.

[17] ilya sutskever, oriol vinyals, and quoc le. sequence to sequence learning with neural

networks. in neural information processing systems, 2014.

[18] kyunghyun cho, bart van merrienboer, caglar gulcehre, dzmitry bahdanau, fethi bougares,
holger schwen, and yoshua bengio. learning phrase representations using id56 encoder-
decoder for id151. in conference on empirical methods in natural
language processing, 2014.

[19] samy bengio, oriol vinyals, navdeep jaitly, and noam shazeer. scheduled sampling for se-
quence prediction with recurrent neural networks. in http://arxiv.org/abs/1506.03099, 2015.
[20] tara n. sainath, oriol vinyals, andrew senior, and hasim sak. convolutional, long short-
term memory, fully connected deep neural networks. in ieee international conference on
acoustics, speech and signal processing, 2015.

[21] alex krizhevsky, ilya sutskever, and geoffrey e. hinton. id163 classi   cation with deep

convolutional neural networks. in neural information processing systems, 2012.

[22] leonard e. baum and ted petrie. statistical id136 for probabilistic functions of finite

state markov chains. the annals of mathematical statistics, 37:1554   1563, 1966.

[23] john lafferty, andrew mccallum, and fernando pereira. id49: proba-
bilistic models for segmenting and labeling sequence data. in international conference on
machine learning, 2001.

[24] minh-thang luong, ilya sutskever, quoc v. le, oriol vinyals, and wojciech zaremba. ad-
dressing the rare word problem in id4. in association for computa-
tional linguistics, 2015.

[25] sebastien jean, kyunghyun cho, roland memisevic, and yoshua bengio. on using very
large target vocabulary for id4. in association for computational
linguistics, 2015.

[26] oriol vinyals, alexander toshev, samy bengio, and dumitru erhan. show and tell: a neural
image caption generator. in ieee conference on id161 and pattern recognition,
2015.

[27] kelvin xu, jimmy ba, ryan kiros, kyunghyun cho, aaron courville, ruslan salakhutdinov,
richard zemel, and yoshua bengio. show, attend and tell: neural image id134
with visual attention. in international conference on machine learning, 2015.

[28] oriol vinyals, lukasz kaiser, terry koo, slav petrov, ilya sutskever, and geoffrey e. hinton.

grammar as a foreign language. in http://arxiv.org/abs/1412.7449, 2014.

[29] oriol vinyals and quoc v. le. a neural conversational model. in international conference

on machine learning: deep learning workshop, 2015.

[30] sepp hochreiter and jurgen schmidhuber. long short-term memory. neural computation,

9(8):1735   1780, november 1997.

12

[31] alex graves, navdeep jaitly, and abdel-rahman mohamed. hybrid id103 with
bidirectional lstm. in automatic id103 and understanding workshop, 2013.
[32] salah hihi and yoshua bengio. hierarchical recurrent neural networks for long-term de-

pendencies. in neural information processing systems, 1996.

[33] jan koutnik, klaus greff, faustino gomez, and jurgen schmidhuber. a clockwork id56. in

international conference on machine learning, 2014.

[34] navdeep jaitly, vincent vanhoucke, and geoffrey hinton. autoregressive product of multi-

frame predictions can improve the accuracy of hybrid models. in interspeech, 2014.

[35] hasim sak, andrew senior, kanishka rao, and francoise beaufays. fast and accurate recur-

rent neural network acoustic models for id103. in interspeech, 2015.

[36] tomas mikolov, ilya sutskever, kai chen, greg corrado, and jeffrey dean. distributed repre-
sentations of words and phrases and their compositionality. in neural information processing
systems, 2013.

[37] daniel povey, arnab ghoshal, gilles boulianne, lukas burget, ondrej glembek, nagendra
goel, mirko hannenmann, petr motlicek, yanmin qian, petr schwarz, jan silovsky, georg
in automatic speech
stemmer, and karel vesely. the kaldi id103 toolkit.
recognition and understanding workshop, 2011.

[38] jeffrey dean, greg s. corrado, rajat monga, kai chen, matthieu devin, quoc v. le, mark z.
mao, marc   aurelio ranzato, andrew senior, paul tucker, ke yang, and andrew y. ng. large
scale distributed deep networks. in neural information processing systems, 2012.

13

a alignment examples

in this section, we give additional visualization examples of our model and the attention distribution.

figure 6: the spelling variants of    aaa    vs    triple a    produces different attention distributions, both spelling
variants appear in our top beams. the ground truth is:    aaa emergency roadside service   .

14

figure 7: the spelling variants of    st    vs    saint    produces different attention distributions, both spelling
variants appear in our top beams. the ground truth is:    st mary   s animal clinic   .

15

figure 8: the phrase    cancel    is repeated three times. note the parallel diagonals, the content attention
mechanism gets slightly confused however the model still emits the correct hypothesis. the ground truth is:
   cancel cancel cancel   .

16

