id183 with locally-trained id27s

fernando diaz

bhaskar mitra

nick craswell

{fdiaz,bmitra,nickr}@microsoft.com

microsoft

6
1
0
2

 

n
u
j
 

3
2

 
 
]

r

i
.
s
c
[
 
 

2
v
1
9
8
7
0

.

5
0
6
1
:
v
i
x
r
a

abstract
continuous space id27s have received a great
deal of attention in the natural language processing and ma-
chine learning communities for their ability to model term
similarity and other relationships. we study the use of term
relatedness in the context of id183 for ad hoc in-
formation retrieval. we demonstrate that id27s
such as id97 and glove, when trained globally, under-
perform corpus and query speci   c embeddings for retrieval
tasks. these results suggest that other tasks bene   ting from
global embeddings may also bene   t from local embeddings.

1.

introduction

continuous space embeddings such as id97 [29] or
glove [33] project terms in a vocabulary to a dense, lower
dimensional space. recent results in the natural language
processing community demonstrate the e   ectiveness of these
methods for analogy and word similarity tasks. in general,
these approaches provide global representations of words;
each word has a    xed representation, regardless of any dis-
course context. while a global representation provides some
advantages, language use can vary dramatically by topic.
for example, ambiguous terms can easily be disambiguated
given local information in immediately surrounding words
[17, 49]. the window-based training of id97 style algo-
rithms exploits this distributional property.

a global id27, even when trained using lo-
cal windows, risks capturing only coarse representations of
those topics dominant in the corpus. while a particular
embedding may be appropriate for a speci   c word within a
sentence-length context globally, it may be entirely inappro-
priate within a speci   c topic. gale et al. refer to this as the
   one sense per discourse    property [15]. previous work by
yarowsky demonstrates that this property can be success-
fully combined with information from nearby terms for word
sense disambiguation [50]. our work extends this approach
to id97-style training in the context word similarity.

indeed,

for many tasks that require topic-speci   c linguistic anal-
ysis, we argue that topic-speci   c representations should out-
perform global representations.
it is di   cult to
imagine a natural language processing task that would not
bene   t from an understanding of the local topical structure.
our work focuses on a id183, an information re-
trieval task where we can study di   erent lexical similarity
methods with an extrinsic evaluation metric (i.e. retrieval
metrics). recent work has demonstrated that similarity
based on global id27s can be used to outper-
form classic pseudo-relevance feedback techniques [40, 2].

we propose that embeddings be learned on topically-constrained

corpora, instead of large topically-unconstrained corpora. in
a retrieval scenario, this amounts to retraining an embedding
on documents related to the topic of the query. we present
local embeddings which capture the nuances of topic-speci   c
language better than global embeddings. there is substan-
tial evidence that global methods underperform local meth-
ods for information retrieval tasks such as id183
[48], latent semantic analysis [20, 37, 39], cluster-based re-
trieval [41, 42, 47], and term id91 [5]. we demonstrate
that the same holds true when using id27s for
text retrieval.

2. motivation

for the purpose of motivating our approach, we will re-
strict ourselves to id97 although other methods be-
have similarly [26]. these algorithms involve discrimina-
tively training a neural network to predict a word given
small set of context words. more formally, given a target
word w and observed context c, the instance loss is de   ned
as,

(cid:96)(w, c) = log   (  (w)      (c))

+       ew     c [log   (     (w)      (w))]

where    : v     (cid:60)k projects a term into a k-dimensional
embedding space,    : v m     (cid:60)k projects a set of m terms
into a k-dimensional embedding space, and w is a randomly
sampled    negative    context. the parameter    controls the
sampling of random negative terms. these matrices are es-
timated over a set of contexts sampled from a large corpus
and minimize the expected loss,

lc = ew,c   pc [(cid:96)(w, c)]

(1)

where pc is the distribution of word-context pairs in the
training corpus and can be estimated from corpus statistics.
while using corpus statistics may make sense absent any
other information, oftentimes we know that our analysis will
be topically constrained. for example, we might be analyz-
ing the    sports    documents in a collection. the language
in this domain is more specialized and the distribution over
word-context pairs is unlikely to be similar to pc(w, c). in
fact, prior work in information retrieval suggests that doc-
uments on subtopics in a collection have very di   erent un-
igram distributions compared to the whole corpus [9]. let
pt(w, c) be the id203 of observing a word-context pair
conditioned on the topic t.

the expected loss under this distribution is [38],

figure 1: importance weights for terms occurring
in documents related to    argentina pegging dollar   
relative to frequency in gigaword.

(cid:20) pt(w, c)

pc(w, c)

lt = ew,c   pc

(cid:21)

(cid:96)(w, c)

(2)

in general, if our corpus consists of su   ciently diverse data
(e.g. wikipedia), the support of pt(w, c) is much smaller
than and contained in that of pc(w, c). the loss, (cid:96), of a con-
text that occurs more frequently in the topic, will be ampli-
   ed by the importance weight    = pt(w,c)
pc(w,c) . because topics
require specialized language, this is likely to occur; at the
same time, these contexts are likely to be underemphasized
in training a model according to equation 1.

in order to quantify this, we took a topic from a trec
ad hoc retrieval collection (see section 5 for details) and
computed the importance weight for each term occurring in
the set of on-topic documents. the histogram of weights
   is presented in figure 1. while larger probabilities are
expected since the size of a topic-constrained vocabulary is
smaller, there are a non-trivial number of terms with much
larger importance weights. if the loss, (cid:96)(w), of a id97
embedding is worse for these words with low pc(w), then we
expect these errors to be exacerbated for the topic.

of course, these highly weighted terms may have a low
value for pt(w) but a very high value relative to the cor-
pus. we can adjust the weights by considering the pointwise
id181 for each word w,

dw(pt(cid:107)pc) = pt(w) log

pt(w)
pc(w)

(3)

words which have a much higher value of pt(w) than pc(w)
and have a high absolute value of pt(w) will have high point-
wise kl divergence. figure 2 shows the divergences for the
top 100 most frequent terms in pt(w). the higher ranked
terms (i.e. good id183 candidates) tend to have
much higher probabilities than found in pc(w). if the loss
on those words is large, this may result in poor embeddings
for the most important words for the topic.

a dramatic change in distribution between the corpus and
the topic has implications for performance precisely because
of the objective used by id97 (i.e. equation 1). the

figure 2: pointwise id181 for
terms occurring in documents related to    argentina
pegging dollar    relative to frequency in gigaword.

global
cutting
squeeze
reduce
slash

local
tax

de   cit
vote

budget

reduction

reduction

spend
lower
halve
soften
freeze

house

bill
plan
spend
billion

figure 3: terms similar to    cut    for a id97
model trained on a general news corpus and another
trained only on documents related to    gasoline tax   .

training emphasizes word-context pairs occurring with high
frequency in the corpus. we will demonstrate that, even
with heuristic downsampling of frequent terms in id97,
these techniques result in inferior performance for speci   c
topics.

thus far, we have sketched out why using the corpus dis-
tribution for a speci   c topic may result in undesirable out-
comes. however, it is even unclear that pt(w|c) = pc(w|c).
in fact, we suspect that pt(w|c) (cid:54)= pc(w|c) because of the
   one sense per discourse    claim [15]. we can qualitatively
observe the di   erence in pc(w|c) and pt(w|c) by training two
id97 models: the    rst on the large, generic gigaword
corpus and the second on a topically-constrained subset of
the gigaword. we present the most similar terms to    cut   
using both a global embedding and a topic-speci   c embed-
ding in figure 3. in this case, the topic is    gasoline tax   . as
we can see, the    tax cut    sense of    cut    is emphasized in the
topic-speci   c embedding.

3. local id27s

the previous section described several reasons why a global
embedding may result in overgeneral id27s. in
order to perform topic-speci   c training, we need a set of
topic-speci   c documents.
in information retrieval scenar-
ios users rarely provide the system with examples of topic-
speci   c documents, instead providing a small set of key-
words.

log(weight)-1012345050100150kl0.000.050.100.15rankfortunately, we can use information retrieval techniques
to generate a query-speci   c set of topical documents. specif-
ically, we adopt a id38 approach to do so [8].
in this retrieval model, each document is represented as a
maximum likelihood language model estimated from doc-
ument term frequencies. query language models are esti-
mated similarly, using term frequency in the query. a docu-
ment score then, is the id181 between
the query and document language models,

table 1: corpora used for retrieval and local em-
bedding training.

trec12
robust
web
news
wiki

docs
469,949
528,155
50,220,423
9,875,524
3,225,743

words
438,338
665,128
90,411,624
2,645,367
4,726,862

queries

150
250
200

-
-

(cid:88)

w   v

d(pq(cid:107)pd) =

pq(w) log

pq(w)
pd(w)

(4)

5.1 data

documents whose language models are more similar to the
query language model will have a lower kl divergence score.
for consistency with prior work, we will refer to this as the
query likelihood score of a document.

the scores in equation 4 can be passed through a softmax
function to derive a multinomial over the entire corpus [25],

(cid:80)
exp(   d(pq(cid:107)pd))
d(cid:48) exp(   d(pq(cid:107)pd(cid:48) ))

p(d) =

(5)

recall in section 2 that training a id97 model weights
word-context pairs according to the corpus frequency. our
query-based multinomial, p(d), provides a weighting func-
tion capturing the documents relevant to this topic. al-
though an estimation of the topic-speci   c documents from
a query will be imprecise (i.e. some nonrelevant documents
will be scored highly), the language use tends to be consis-
tent with that found in the known relevant documents.

we can train a local id27 using an arbitrary
optimization method by sampling documents from p(d) in-
stead of uniformly from the corpus.
in this work, we use
id97, although any method that operates on a sample
of documents can be used.

4. id183 with word em-

beddings

when using language models for retrieval, query expan-
sion involves estimating an alternative to pq. speci   cally,
when each expansion term is associated with a weight, we
normalize these weights to derive the expansion language
model, pq+ . this language model is then interpolated with
the original query model,

q(w) =   pq(w) + (1       )pq+ (w)
p1

(6)

this interpolated language model can then be used with
equation 4 to rank documents [1]. we will refer to this as
the expanded query score of a document.
now we turn to using id27s for query expan-
sion. let u be an |v|    k term embedding matrix. if q is a
|v|    1 column term vector for a query, then the expansion
term weights are uutq. we then take the top k terms,
normalize their weights, and compute pq+ (w).

we consider the following alternatives for u. the    rst
approach is to use a global model trained by sampling doc-
uments uniformly. the second approach, which we propose
in this paper, is to use a local model trained by sampling
documents from p(d).

5. methods

to evaluate the di   erent retrieval strategies described in
section 3, we use the following datasets. two newswire
datasets, trec12 and robust, consist of the newswire doc-
uments and associated queries from trec ad hoc retrieval
evaluations. the trec12 corpus consists of tipster disks 1
and 2; and the robust corpus consists of tipster disks 4 and
5. our third dataset, web, consists of the clueweb 2009
category b web corpus. for the web corpus, we only re-
tain documents with a waterloo spam rank above 70.1 we
present corpus statistics in table 1.

we consider several publicly available global embeddings.
we use four glove embeddings of di   erent dimensional-
ity trained on the union of wikipedia and gigaword doc-
uments.2 we use one publicly available id97 embed-
ding trained on google news documents.3 we also trained
a global embedding for trec12 and robust using the entire
corpus. instead of training a global embedding on the large
web collection, we use a glove embedding trained on com-
mon crawl data.4

we train local embeddings with id97 using one of
three retrieval sources. first, we consider documents re-
trieved from the target corpus of the query (i.e. trec12, ro-
bust, or web). we also consider training a local embedding
by performing a retrieval on large auxiliary corpora. we
use the gigaword corpus as a large auxiliary news corpus.
we hypothesize that retrieving from a larger news corpus
will provide substantially more local training data than a
target retrieval. we also use a wikipedia snapshot from de-
cember 2014. we hypothesize that retrieving from a large,
high    delity corpus will provide cleaner language than that
found in lower    delity target domains such as the web. ta-
ble 1 shows the relative magnitude of these auxiliary corpora
compared to the target corpora.

all corpora in table 1 were stopped using the smart
stopword list5 and stemmed using the krovetz algorithm
[23]. we used the indri implementation for indexing and
retrieval.6
5.2 evaluation

we consider several standard retrieval id74,
including ndcg@10 and interpolated precision at standard
recall points [22, 45]. ndcg@10 provides insight into per-
formance speci   cally at higher ranks. an interpolated preci-
sion recall graph describes system performance throughout

1https://plg.uwaterloo.ca/  gvcormac/clueweb09spam/
2http://nlp.stanford.edu/data/glove.6b.zip
3https://code.google.com/archive/p/id97/
4http://nlp.stanford.edu/data/glove.840b.300d.zip
5http://jmlr.csail.mit.edu/papers/volume5/lewis04a/
a11-smart-stop-list/english.stop
6http://www.lemurproject.org/indri/

the entire ranked list.
5.3 training

all retrieval experiments were conducted by performing
10-fold cross-validation across queries. speci   cally, we cross-
validate the number of expansion terms, k     [5     500], and
interpolation weight,        [0, 1]. for local id97 training,
we cross-validate the learning rate        {0.1, 0.01, 0.001}.

all id97 training used the publicly available id97
cbow implementation.7 when training the local models,
we sampled 1000 documents from p(d) with replacement.
to compensate for the much smaller corpus size, we ran
id97 training for 80 iterations. local id97 mod-
els use a    xed embedding dimension of 400 although other
choices did not signi   cantly a   ect our results. unless other-
wise noted, default parameter settings were used.

in our experiments, expanded queries rescore the top 1000
documents from an initial query likelihood retrieval. previ-
ous results have demonstrated that this approach results in
performance nearly identical with an expanded retrieval at a
much lower cost [11]. because publicly available embeddings
may have id121 inconsistent with our target corpora,
we restricted the vocabulary of candidate expansion terms
to those occurring in the initial retrieval.
if a candidate
term was not found in the vocabulary of the embedding ma-
trix, we searched for the candidate in a stemmed version of
the embedding vocabulary. in the event that the candidate
term was still not found after this process, we removed it
from consideration.

6. results

we present results for retrieval experiments in table 2.
we    nd that embedding-based id183 outperforms
our query likelihood baseline across all conditions. when us-
ing the global embedding, the news corpora bene   t from the
various embeddings in di   erent situations. interestingly, for
trec12, using an embedding trained on the target corpus sig-
ni   cantly outperforms all other global embeddings, despite
using substantially less data to estimate the model. while
this performance may be due to the embedding having a
id121 consistent with the target corpus, it may also
come from the fact that the corpus is more representative of
the target documents than other embeddings which rely on
online news or are mixed with non-news content. to some
extent this supports our desire to move training closer to the
target distribution.

across all conditions, local embeddings signi   cantly out-
perform global embeddings for id183. for our two
news collections, estimating the local model using a retrieval
from the larger gigaword corpus led to substantial improve-
ments. this e   ect is almost certainly due to the gigaword
corpus being similar in writing style to the target corpus
but, at the same time, providing signi   cantly more relevant
content [12]. as a result, the local embedding is trained us-
ing a larger variety of topical material than if it were to use
a retrieval from the smaller target corpus. an embedding
trained with a retrieval from wikipedia tended to perform
worse most likely because the language is dissimilar from
news content. our web collection, on the other hand, ben-
e   tted more from embeddings trained using retrievals from
the general wikipedia corpus. the gigaword corpus was less

7https://code.google.com/p/id97/

table 3: kendall   s    and spearman   s    between im-
provement in ndcg@10 and local kl divergence
with the corpus language model. the improvement
is measured for the best local embedding over the
best global embedding.

trec12
robust
web

  

0.0585
0.0545
0.0204

  

0.0798
0.0792
0.0283

useful here because news-style language is almost certainly
not representative of general web documents.

figure 4 presents interpolated precision-recall curves com-
paring the baseline, the best global id183 method,
and the best local id183 method. interestingly, al-
though global methods achieve strong performance for ndcg@10,
these improvements over the baseline are not re   ected in our
precision-recall curves. local methods, on the other hand,
almost always strictly dominate both the baseline and global
expansion across all recall levels.

the results support the hypothesis that local embeddings
provide better similarity measures than global embeddings
for id183. in order to understand why, we    rst
compare the performance di   erences between local and global
embeddings. figure 2 suggests that we should adopt a local
embedding when the local unigram language model deviates
(cid:80)
from the corpus language model. to test this, we computed
the kl divergence between the local unigram distribution,
d p(w|d)p(d), and the corpus unigram language model [9].
we hypothesize that, when this value is high, the topic lan-
guage is di   erent from the corpus language and the global
embedding will be inferior to the local embedding. we tested
the rank correlation between this kl divergence and the rel-
ative performance of the local embedding with respect to the
global embedding. these correlations are presented in ta-
ble 3. unfortunately, we    nd that the correlation is low,
although it is positive across collections.

we can also qualitatively analyze the di   erences in the
behavior of the embeddings.
if we have access to the set
of documents labeled relevant to a query, then we can com-
pute the frequency of terms in this set and consider those
terms with high frequency (after stopping and id30) to
be good id183 candidates. we can then visualize
where these terms lie in the global and local embeddings.
in figure 5, we present a two-dimensional projection [44]
of terms for the query    ocean remote sensing   , with those
good candidates highlighted. our projection includes the
top 50 candidates by frequency and a sample of terms oc-
curring in the query likelihood retrieval. we notice that, in
the global embedding, the good candidates are spread out
amongst poorer candidates. by contrast, the local embed-
ding clusters the candidates in general but also situates them
closely around the query. as a result, we suspect that the
similar terms extracted from the local embedding are more
likely to include these good candidates.

7. discussion

the success of local embeddings on this task should alarm
natural language processing researchers using global embed-
dings as a representational tool. for one, the approach of
learning from vast amounts of data is only e   ective if the

table 2: retrieval results comparing id183 based on various global and local embeddings. bolded
numbers indicate the best expansion in that class of embeddings. wilcoxon signed rank test between bolded
numbers indicates statistically signi   cant improvements (p < 0.05) for all collections.

global

trec12
robust
web

ql
0.514
0.467
0.216

50

0.518
0.470
0.227

wiki+giga
200
100
0.518
0.530
0.469
0.463
0.229
0.230

300
0.531
0.468
0.232

gnews

300
0.530
0.472
0.218

target

target

400

0.545
0.465
0.216

400
0.535
0.475
0.234

local
giga
400

0.563*
0.517*
0.236

wiki
400
0.523
0.476
0.258*

figure 4: interpolated precision-recall curves for query likelihood, the best global embedding, and the best
local embedding from table 2.

data is appropriate for the task at hand. and, when pro-
vided, much smaller high-quality data can provide much bet-
ter performance. beyond this, our results suggest that the
approach of estimating global representations, while com-
putationally convenient, may overlook insights possible at
query time, or evaluation time in general. a similar local
embedding approach can be adopted for any natural lan-
guage processing task where topical locality is expected and
can be estimated. although we used a query to re-weight the
corpus in our experiments, we could just as easily use alter-
native contextual information (e.g. a sentence, paragraph,
or document) in other tasks.

despite these strong results, we believe that there are still
some open questions in this work. first, although local em-
beddings provide e   ectiveness gains, they can be quite in-
e   cient compared to global embeddings. we believe that
there is opportunity to improve the e   ciency by consider-
ing o   ine computation of local embeddings at a coarser level
than queries but more specialized than the corpus. if the re-
trieval algorithm is able to select the appropriate embedding
at query time, we can avoid training the local embedding.
second, although our supporting experiments (table 3, fig-
ure 5) add some insight into our intuition, the results are
not strong enough to provide a solid explanation. further
theoretical and empirical analysis is necessary.
8. related work

topical adaptation of models. the shortcomings of learn-
ing a single global vector representation, especially for poly-
semic words, have been pointed out before [36]. the problem
can be addressed by training a global model with multiple

vector embeddings per word [35, 19] or topic-speci   c em-
beddings [27]. the number of senses for each word may be
   xed [32], or determined using class labels [43]. however, to
the best of our knowledge, this is the    rst time that training
topic-speci   c id27s has been explored.

several methods exist in the id38 commu-
nity for topic-dependent adaptation of language models [6].
these can lead to performance improvements in tasks such
as machine translation [51] and id103 [31]. topic-
speci   c data may be gathered in advance, by identifying
corpus of topic-speci   c documents.
it may also be gath-
ered during the discourse, using multiple hypotheses from
n-best lists as a source of topic-speci   c language. then
a topic-speci   c language model is trained (or the global
model is adapted) online using the topic-speci   c training
data. a topic-dependent model may be combined with the
global model using linear interpolation [21] or other more
sophisticated approaches [14, 24]. similarly to the adapta-
tion work, we use topic-speci   c documents to train a topic-
speci   c model. in our case the documents come from a    rst
round of retrieval for the user   s current query, and the word
embedding model is trained based on sentences from the
topic-speci   c document set. unlike the past work, we do not
focus on interpolating the local and global models, although
this is a promising area for future work. in the current study
we focus on a direct comparison between the local-only and
global-only approach, for improving retrieval performance.

id27s for ir. information retrieval has a long
history of learning representations of words that are low-
dimensional dense vectors. these approaches can be broadly
classi   ed into two families based on whether they are learnt

0.00.20.40.60.81.00.00.10.20.30.40.50.60.7trec12recallprecisionqlgloballocal0.00.20.40.60.81.00.00.20.40.60.8robustrecallprecisionqlgloballocal0.00.20.40.60.81.00.00.10.20.30.40.50.6webrecallprecisionqlgloballocallocal latent semantic analysis. despite the mathemati-
cal appeal of latent semantic analysis, several experiments
suggest that its empirical performance may be no better
than that of ranking using standard term vectors [10, 13,
4]. in order to address the coarseness of corpus-level latent
semantic analysis, hull proposed restricting analysis to the
documents relevant to a query [20]. this approach signi   -
cantly improved over corpus-level analysis for routing tasks,
a result that has been reproduced in consequent research [37,
39]. our work can be seen as an extension of these results
to more recent techniques such as id97.

9. conclusion

we have demonstrated a simple and e   ective method for
performing id183 with id27s.
im-
portantly, our results highlight the value of locally-training
id27s in a query-speci   c manner. the strength
of these results suggests that other research adopting global
embedding vectors should consider local embeddings as a po-
tentially superior representation. instead of using a    sriracha
sauce of deep learning,    as embedding techniques like id97
have been called, we contend that the situation sometimes
requires, say, that we make a b  echamel or a mole verde or a
sambal   or otherwise learn to cook.

10. references
[1] n. abdul-jaleel, j. allan, w. b. croft, f. diaz,

l. larkey, x. li, d. metzler, m. d. smucker,
t. strohman, h. turtle, and c. wade. umass at trec
2004: novelty and hard. in online proceedings of 2004
text retrieval conference, 2004.

[2] m. al masri, c. berrut, and j.-p. chevallet. a

comparison of deep learning based id183
with pseudo-relevance feedback and mutual
information. in n. ferro, f. crestani, m.-f. moens,
j. mothe, f. silvestri, m. g. di nunzio, c. hau   , and
g. silvello, editors, proceedings of the 38th european
conference on ir research (ecir 2016), pages
709   715, cham, 2016. springer international
publishing.

[3] a. atreya and c. elkan. id45

(lsi) fails for trec collections. acm sigkdd
explorations newsletter, 12(2):5   10, 2011.

[4] a. atreya and c. elkan. id45

(lsi) fails for trec collections. sigkdd explor. newsl.,
12(2):5   10, mar. 2011.

[5] r. attar and a. s. fraenkel. local feedback in

full-text retrieval systems. j. acm, 24(3):397   417,
july 1977.

[6] j. r. bellegarda. statistical language model
adaptation: review and perspectives. speech
communication, 42(1):93   108, 2004.

[7] d. m. blei, a. y. ng, and m. i. jordan. latent

dirichlet allocation. j. mach. learn. res., 3:993   1022,
2003.

[8] w. b. croft and j. la   erty. id38 for
information retrieval. kluwer academic publishing,
2003.

[9] s. cronen-townsend, y. zhou, and w. b. croft.

predicting query performance. in sigir    02:
proceedings of the 25th annual international acm
sigir conference on research and development in

figure 5: global versus local embedding of highly
relevant terms. each point represents a candidate
expansion term. red points have high frequency
in the relevant set of documents. white points have
low or no frequency in the relevant set of documents.
the blue point represents the query. contours in-
dicate distance from the query.

based on a term-document matrix or term co-occurence data.
using the term-document matrix for embedding leads to sev-
eral well-studied approaches such as lsa [10], plsa [18],
and lda [7, 46]. the performance of these models varies
depending on the task, for example they are known to per-
form poorly for retrieval tasks unless combined with lexical
features [3]. term-cooccurence based embeddings, such as
id97 [29, 28] and [34], have recently been remarkably
popular for many natural language processing and logical
reasoning tasks. however, there are relatively less known
successful applications of these models in ir. ganguly et.
al. [16] used the word similarity in the id97 embedding
space as a way to estimate term transformation probabilities
in a language modelling setting for retrieval. more recently,
nalisnick et. al.
[30] proposed to model document about-
ness by computing the similarity between all pairs of query
and document terms using dual embedding spaces. both
these approaches estimate the semantic relatedness between
two terms as the cosine distance between them in the embed-
ding space(s). we adopt a similar notion of term relatedness
but focus on demonstrating improved retrieval performance
using locally trained embeddings.

globallocalinformation retrieval, pages 299   306, new york, ny,
usa, 2002. acm press.

development in information retrieval, pages 191   202,
new york, ny, usa, 1993. acm press.

[10] s. c. deerwester, s. t. dumais, t. k. landauer,
g. w. furnas, and r. a. harshman. indexing by
latent semantic analysis. journal of the american
society of information science, 41(6):391   407, 1990.

[24] r. kuhn and r. de mori. a cache-based natural

language model for id103. pattern
analysis and machine intelligence, ieee transactions
on, 12(6):570   583, 1990.

[11] f. diaz. condensed list relevance models. in

[25] v. lavrenko and w. b. croft. relevance based

proceedings of the 2015 international conference on
the theory of information retrieval, ictir    15,
pages 313   316, new york, ny, usa, may 2015. acm.

[12] f. diaz and d. metzler. improving the estimation of

relevance models using large external corpora. in
sigir    06: proceedings of the 29th annual
international acm sigir conference on research and
development in information retrieval, pages 154   161,
new york, ny, usa, 2006. acm press.

[13] s. t. dumais. id45 (lsi):
trec-3 report. in overview of the third text
retrieval conference (trec-3), pages 219   230, 1995.
[14] m. federico. bayesian estimation methods for id165

language model adaptation. in spoken language,
1996. icslp 96. proceedings., fourth international
conference on, volume 1, pages 240   243. ieee, 1996.

[15] w. a. gale, k. w. church, and d. yarowsky. one

sense per discourse. in proceedings of the workshop on
speech and natural language, hlt    91, pages
233   237, stroudsburg, pa, usa, 1992. association for
computational linguistics.

[16] d. ganguly, d. roy, m. mitra, and g. j. jones. word

embedding based generalized language model for
information retrieval. in proceedings of the 38th
international acm sigir conference on research
and development in information retrieval, sigir    15,
pages 795   798, new york, ny, usa, 2015. acm.

[17] z. s. harris. distributional structure. word,

10(2-3):146   162, 1954.

[18] t. hofmann. probabilistic id45. in

sigir    99: proceedings of the 22nd annual
international acm sigir conference on research and
development in information retrieval, pages 50   57,
new york, ny, usa, 1999. acm press.

[19] e. h. huang, r. socher, c. d. manning, and a. y. ng.
improving word representations via global context and
multiple word prototypes. in proceedings of the 50th
annual meeting of the association for computational
linguistics: long papers-volume 1, pages 873   882.
association for computational linguistics, 2012.
[20] d. hull. improving text retrieval for the routing

problem using id45. in proceedings
of the 17th annual international acm sigir
conference on research and development in
information retrieval, sigir    94, pages 282   291, new
york, ny, usa, 1994. springer-verlag new york, inc.

[21] r. iyer and m. ostendorf. modeling long distance

dependence in language: topic mixtures versus
dynamic cache models. speech and audio processing,
ieee transactions on, 7(1):30   39, jan 1999.

[22] k. j  arvelin and j. kek  al  ainen. cumulated gain-based
evaluation of ir techniques. tois, 20(4):422   446, 2002.

[23] r. krovetz. viewing morphology as an id136

process. in sigir    93: proceedings of the 16th annual
international acm sigir conference on research and

language models. in proceedings of the 24th annual
international acm sigir conference on research and
development in information retrieval, pages 120   127.
acm press, 2001.

[26] o. levy and y. goldberg. neural id27 as

implicit id105. in z. ghahramani,
m. welling, c. cortes, n. lawrence, and
k. weinberger, editors, advances in neural
information processing systems 27, pages 2177   2185.
curran associates, inc., 2014.

[27] y. liu, z. liu, t.-s. chua, and m. sun. topical word

embeddings. in proceedings of the twenty-ninth
aaai conference on arti   cial intelligence, aaai   15,
pages 2418   2424. aaai press, 2015.

[28] t. mikolov, k. chen, g. corrado, and j. dean.

e   cient estimation of word representations in vector
space. arxiv preprint arxiv:1301.3781, 2013.

[29] t. mikolov, i. sutskever, k. chen, g. s. corrado, and

j. dean. distributed representations of words and
phrases and their compositionality. in c. burges,
l. bottou, m. welling, z. ghahramani, and
k. weinberger, editors, advances in neural
information processing systems 26, pages 3111   3119.
curran associates, inc., 2013.

[30] e. nalisnick, b. mitra, n. craswell, and r. caruana.

improving document ranking with dual word
embeddings. in proc. www. international world
wide web conferences steering committee, 2016.
[31] h. nanjo and t. kawahara. language model and

speaking rate adaptation for spontaneous presentation
id103. speech and audio processing,
ieee transactions on, 12(4):391   400, 2004.
[32] a. neelakantan, j. shankar, a. passos, and

a. mccallum. e   cient non-parametric estimation of
multiple embeddings per word in vector space. arxiv
preprint arxiv:1504.06654, 2015.

[33] j. pennington, r. socher, and c. d. manning. glove:

global vectors for word representation. in empirical
methods in natural language processing (emnlp),
pages 1532   1543, 2014.

[34] j. pennington, r. socher, and c. d. manning. glove:
global vectors for word representation. proc. emnlp,
12:1532   1543, 2014.

[35] j. reisinger and r. mooney. a mixture model with

sharing for lexical semantics. in proceedings of the
2010 conference on empirical methods in natural
language processing, pages 1173   1182. association for
computational linguistics, 2010.

[36] j. reisinger and r. j. mooney. multi-prototype
vector-space models of word meaning. in human
language technologies: the 2010 annual conference
of the north american chapter of the association for
computational linguistics, pages 109   117. association
for computational linguistics, 2010.

[37] h. sch  utze, d. a. hull, and j. o. pedersen. a

comparison of classi   ers and id194s
for the routing problem. in proceedings of the 18th
annual international acm sigir conference on
research and development in information retrieval,
sigir    95, pages 229   237, new york, ny, usa, 1995.
acm.

[51] b. zhao, m. eck, and s. vogel. language model

adaptation for id151 with
structured query models. in proceedings of the 20th
international conference on computational
linguistics, coling    04, stroudsburg, pa, usa,
2004. association for computational linguistics.

[38] h. shimodaira. improving predictive id136 under

covariate shift by weighting the log-likelihood
function. journal of statistical planning and
id136, 90(2):227     244, 2000.

[39] a. singhal, m. mitra, and c. buckley. learning
routing queries in a query zone. sigir forum,
31(si):25   32, july 1997.

[40] a. sordoni, y. bengio, and j.-y. nie. learning

concept embeddings for id183 by quantum
id178 minimization. in proceedings of the
twenty-eighth aaai conference on arti   cial
intelligence, aaai   14, pages 1586   1592. aaai press,
2014.

[41] a. tombros and c. j. van rijsbergen. query-sensitive

similarity measures for the calculation of
interdocument relationships. in cikm    01:
proceedings of the tenth international conference on
information and knowledge management, pages 17   24,
new york, ny, usa, 2001. acm press.

[42] a. tombros, r. villa, and c. j. van rijsbergen. the
e   ectiveness of query-speci   c hierarchic id91 in
information retrieval. inf. process. manage.,
38(4):559   582, july 2002.

[43] a. trask, p. michalak, and j. liu. sense2vec-a fast

and accurate method for id51 in
neural id27s. arxiv preprint
arxiv:1511.06388, 2015.

[44] l. van der maaten and g. e. hinton. visualizing

high-dimensional data using id167. journal of machine
learning research, 9:2579   2605, 2008.

[45] c. j. van rijsbergen. information retrieval.

butterworths, 1979.

[46] x. wei and w. b. croft. lda-based document models

for ad-hoc retrieval. in sigir    06: proceedings of the
29th annual international acm sigir conference on
research and development in information retrieval,
pages 178   185, new york, ny, usa, 2006. acm
press.

[47] p. willett. query-speci   c automatic document

classi   cation. in international forum on information
and documentation, volume 10, pages 28   32, 1985.

[48] j. xu and w. b. croft. id183 using local
and global document analysis. in proceedings of the
19th annual international acm sigir conference on
research and development in information retrieval,
sigir    96, pages 4   11, new york, ny, usa, 1996.
acm.

[49] d. yarowsky. one sense per collocation. in

proceedings of the workshop on human language
technology, hlt    93, pages 266   271, stroudsburg, pa,
usa, 1993. association for computational linguistics.
[50] d. yarowsky. unsupervised id51
rivaling supervised methods. in proceedings of the 33rd
annual meeting on association for computational
linguistics, acl    95, pages 189   196, stroudsburg, pa,
usa, 1995. association for computational linguistics.

