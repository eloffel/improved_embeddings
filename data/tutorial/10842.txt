6
1
0
2
 
c
e
d
2
2

 

 
 
]
l
c
.
s
c
[
 
 

1
v
3
3
8
7
0

.

2
1
6
1
:
v
i
x
r
a

understanding image and text simultaneously: a dual vision-language

machine comprehension task

nan ding
google

sebastian goodman

google

fei sha
google

radu soricut

google

dingnan@google.com

seabass@google.com

fsha@google.com

rsoricut@google.com

abstract

we introduce a new multi-modal task for computer sys-
tems, posed as a combined vision-language comprehen-
sion challenge: identifying the most suitable text describ-
ing a scene, given several similar options. accomplishing
the task entails demonstrating comprehension beyond just
recognizing    keywords    (or key-phrases) and their corre-
sponding visual concepts.
instead, it requires an align-
ment between the representations of the two modalities that
achieves a visually-grounded    understanding    of various
linguistic elements and their dependencies. this new task
also admits an easy-to-compute and well-studied metric:
the accuracy in detecting the true target among the decoys.
the paper makes several contributions: an effective and
extensible mechanism for generating decoys from (human-
created) image captions; an instance of applying this
mechanism, yielding a large-scale machine comprehension
dataset (based on the coco images and captions) that we
make publicly available; human evaluation results on this
dataset, informing a performance upper-bound; and sev-
eral baseline and competitive learning approaches that il-
lustrate the utility of the proposed task and dataset in ad-
vancing both image and language comprehension. we also
show that, in a id72 setting, the performance
on the proposed task is positively correlated with the end-
to-end task of image captioning.

1. introduction

there has been a great deal of interest in multi-modal
arti   cial intelligence research recently, bringing together
the    elds of id161 and natural language pro-
cessing. this interest has been fueled in part by the avail-
ability of many large-scale image datasets with textual an-
notations. several vision+language tasks have been pro-
posed around these datasets [15, 16, 21, 3].
image cap-
tioning [15, 10, 16, 11, 17, 33, 25, 37] and visual question
answering [23, 24, 31, 3, 40, 35, 28, 13, 38, 41, 22] have

in particular attracted a lot of attention. the performances
on these tasks have been steadily improving, owing much
to the wide use of deep learning architectures [6].

a central theme underlying these efforts is the use of nat-
ural language to identify how much visual information is
perceived and understood by a computer system. presum-
ably, a system that understands a visual scene well enough
ought to be able to describe what the scene is about (thus
   captioning   ) or provide correct and visually-grounded an-
swers when queried (thus    question-answering   ).

in this paper, we argue for directly measuring how well
the semantic representations of the visual and linguistic
modalities align (in some abstract semantic space). for in-
stance, given an image and two captions     a correct one
and an incorrect yet-cunningly-similar one     can we both
qualitatively and quantitatively measure the extent to which
humans can dismiss the incorrect one but computer sys-
tems blunder? arguably, the degree of the modal align-
ment is a strong indicator of task-speci   c performance on
any vision+language task. consequentially, computer sys-
tems that can learn to maximize and exploit such alignment
should outperform those that do not.

we take a two-pronged approach for addressing this is-
sue. first, we introduce a new and challenging dual ma-
chine comprehension (dmc) task, in which a computer
system must identify the most suitable textual description
from several options: one being the target and the oth-
ers being    adversarialy   -chosen decoys. all options are
free-form, coherent, and    uent sentences with high degrees
of semantic similarity (hence, they are    cunningly simi-
lar   ). a successful computer system has to demonstrate
comprehension beyond just recognizing    keywords    (or key
phrases) and their corresponding visual concepts; they must
arrive at a coinciding and visually-grounded understanding
of various linguistic elements and their dependencies. what
makes the dmc task even more appealing is that it admits
an easy-to-compute and well-studied performance metric:
the accuracy in detecting the true target among the decoys.
second, we illustrate how solving the dmc task bene   ts
related vision+language tasks. to this end, we render the

1

dmc task as a classi   cation problem, and incorporate it in
a id72 framework for end-to-end training of
joint objectives.

our work makes the following contributions: (1) an ef-
fective and extensible algorithm for generating decoys from
human-created image captions (section 3.2); (2) an instan-
tiation of applying this algorithm to the coco dataset [21],
resulting in a large-scale dual machine-comprehension
dataset that we make publicly available (section 3.3); (3)
a human evaluation on this dataset, which provides an
upper-bound on performance (section 3.4); (4) a bench-
mark study of baseline and competitive learning approaches
(section 5), which underperform humans by a substantial
gap (about 20% absolute); and (5) a novel multi-task learn-
ing model that simultaneously learns to solve the dmc task
and the image captioning task (sections 4.3 and 5.4).

our empirical study shows that performance on the
dmc task positively correlates with performance on the
image captioning task. therefore, besides acting as a stan-
dalone benchmark, the new dmc task can be useful in im-
proving other complex vision+language tasks. both suggest
the dmc task as a fruitful direction for future research.

2. related work

image understanding is a long-standing challenge in
id161. there has recently been a great deal of in-
terest in bringing together vision and language understand-
ing. particularly relevant to our work are image captioning
(ic) and visual question-answering (vqa). both have insti-
gated a large body of publications, a detailed exposition of
which is beyond the scope of this paper. interested readers
should refer to two recent surveys [7, 34].

in ic tasks, systems attempt to generate a    uent and cor-
ic systems are
rect sentence describing an input image.
usually evaluated on how well the generated descriptions
align with human-created captions (ground-truth). the lan-
guage generation model of an ic system plays a crucial role;
it is often trained such that the probabilities of the ground-
truth captions are maximized (id113 training), though more
advanced methods based on techniques borrowed from re-
inforcement learning have been proposed [27]. to provide
visual grounding, image features are extracted and injected
into the language model. note that language generation
models need to both decipher the information encoded in
the visual features, and model id86.
in vqa tasks, the aim is to answer an input question
correctly with respect to a given input image.
in many
variations of this task, answers are limited to single words
or a binary response (   yes    or    no   ) [3]. the visual7w
dataset [41] contains anaswers in a richer format such as
phrases, but limits questions to    wh-   style (what, where,
who, etc). the visual genome dataset [18], on the other
hand, can potentially de   ne more complex questions and

answers due to its extensive textual annotations.

our dmc task is related but signi   cantly different. in
our task, systems attempt to discriminate the best caption
for an input image from a set of captions     all but one are
decoys. arguably, it is a form of vqa task, where the same
default (thus uninformative) question is asked: which of the
following sentences best describes this image? however,
unlike current vqa tasks, choosing the correct answer in
our task entails a deeper    understanding    of the available
answers. thus, to perform well, a computer system needs
to understand both complex scenes (visual understanding)
and complex sentences (language understanding), and be
able to reconcile them.

the dmc task admits a simple classi   cation-based eval-
uation metric: the accuracy of selecting the true target. this
is a clear advantage over the ic tasks, which often rely on
imperfect metrics such as id7 [26], id8 [20], me-
teor [5], cider [32], or spice [2].

related to our proposal is the work in [15], which frames
image captioning as a ranking problem. while both share
the idea of selecting captions from a large set, our frame-
work has some important and distinctive components. first,
we devise an algorithm for smart selection of candidate de-
coys, with the goal of selecting those that are suf   ciently
similar to the true targets to be challenging, and yet still
be reliably identi   able by human raters. second, we have
conducted a thorough human evaluation in order to estab-
lish a performance ceiling, while also quantifying the level
to which current learning systems underperform. lastly,
we show that there exists a positive correlation between the
performance on the dmc task and the performance on re-
lated vision+language tasks by proposing and experiment-
ing with a id72 model. our work is also sub-
stantially different from their more recent work [14], where
only one decoy is considered and its generation is either ran-
dom, or focusing on visual concept similarity (   switching
people or scenes   ) instead of our focus on both linguistic
surface and paragraph vector embedding similarity.

3. the dual machine comprehension task
3.1. design overview

we propose a new multi-modal machine comprehension
task to examine how well visual and textual semantic un-
derstanding are aligned. given an image, human evaluators
or machines must accurately identify the best sentence de-
scribing the scene from several decoy sentences. accuracy
on this task is de   ned as the percentage that the true targets
are identi   ed.

it seems straightforward to construct a dataset for this
task, as there are several existing datasets which are com-
posed of images and their (multiple) ground-truth captions,
including the popular coco dataset [21]. thus, for any

2

given image, it appears that one just needs to use the cap-
tions corresponding to other images as decoys. however,
this na    ve approach could be overly simplistic as it is pro-
vides no control over the properties of the decoys.

speci   cally, our desideratum is to recruit challenging de-
coys that are suf   ciently similar to the targets. however, for
a small number of decoys, e.g. 4-5, randomly selected cap-
tions could be signi   cantly different from the target. the
resulting dataset would be too    easy    to shed any insight on
the task. since we are also interested in human performance
on this task, it is thus impractical to increase the number of
decoys to raise the dif   culty level of the task at the expense
of demanding humans to examine tediously and unreliably
a large number of decoys. in short, we need an automatic
procedure to reliably create dif   cult sets of decoy captions
that are suf   ciently similar to the targets.

we describe such a procedure in the following. while it
focuses on identifying decoy captions, the main idea is po-
tentially adaptable to other settings. the algorithm is    ex-
ible in that the    dif   culty    of the dataset can be controlled
to some extent through the algorithm   s parameters.
3.2. algorithm to create an mc-ic dataset

the main idea behind our algorithm is to carefully de   ne
a    good decoy   . the algorithm exploits recent advances in
paragraph vector (pv) models [19], while also using lin-
guistic surface analysis to de   ne similarity between two
sentences. due to space limits, we omit a detailed intro-
duction of the pv model. it suf   ces to note that the model
outputs a continuously-valued embedding for a sentence, a
paragraph, or even a document.

the pseudo-code is given in algorithm 1 (the name
mc-ic stands for    machine-comprehension for image &
captions   ). as input,
the algorithm takes a set c of
(cid:104)image,{caption(s)} (cid:105) pairs1, as those extracted from a va-
riety of publicly-available corpora, including the coco
dataset [21]. the output of the algorithm is the mcic set.
concretely, the mc-ic algorithm has three main argu-
ments: a dataset c = {(cid:104)ii, ci(cid:105)|1     i     m} where ii is
an image and ci is its ground-truth caption2; an integer n
which controls the size of ci   s neighborhood in the embed-
ding space de   ned by the paragraph vector model pv; and
a function score which is used to score the n items in each
such neighborhood.

the    rst two steps of the algorithm tune several hyper-
parameters. the    rst step    nds optimal settings for the pv
model given the dataset c. the second    nds a weight pa-
rameter    given pv, dataset c, and the score function.

1on the order of at least hundreds of thousands of examples; smaller

sets result in less challenging datasets.

2for an image with multiple ground-truth captions, we split it to mul-
tiple instances with the same image for each one of the ground-truth cap-
tions; the train/dev/test splits are done such that they contain disjoint image
sets, as opposed to disjoint instance sets.

algorithm 1: mc-ic(c, n, score)
result: dataset mcic
pv     optimize-pv(c)
       optimize-score(pv, c, score)
mcic        
nr decoys = 4
for (cid:104)ii, ci(cid:105)     c do

a     []
tci     pv(ci)[1..n ]
for cd     tci do

score     score(pv,   , cd, ci)
if score > 0 then

a.append((cid:104)score, cd(cid:105))

end

end
if |a|    nr decoys then

r     descending-sort(a)
for l     [1..nr decoys] do

(cid:104)score, cd(cid:105)     r[l]
mcic     mcic    {((cid:104)ii, cd(cid:105), false)}

end
mcic     mcic    {((cid:104)ii, ci(cid:105), true)}

end

end

these hyperparameters are dataset-speci   c. details are dis-
cussed in the next section.

the main body of the algorithm, the outer for loop, gen-
erates a set of nr decoys (4 here) decoys for each ground-
truth caption.
it accomplishes this by    rst extracting n
candidates from the pv neighborhood of the ground-truth
caption, excluding those that belong to the same image. in
the inner for loop, it computes the similarity of each can-
didate to the ground-truth and stores them in a list a. if
enough candidates are generated, the list is sorted in de-
scending order of score. the top nr decoys captions are
marked as    decoys    (i.e. false), while the ground-truth cap-
tion is marked as    target    (i.e. true).
the score function score(pv,   , c(cid:48), c) is a crucial com-
ponent of the decoy selection mechanism.
its de   nition
leverages our linguistic intuition by combining linguistic
surface similarity, simsurf(c(cid:48), c), with the similarity sug-
gested by the embedding model, simpv(c(cid:48), c):

score =

   simpv +(1     ) simsurf otherwise

(1)

if simsurf     l

(cid:26)0

where the common argument (c(cid:48), c) is omitted. the higher
the similarity score, the more likely that c(cid:48) is a good decoy
for c. note that if the surface similarity is above the thresh-
old l, the function returns 0,    agging that the two captions
are too similar to be used as a pair of target and decoy.

3

split
#unique images
# instances

dev
2,000
9,999

test
2,000
10,253

train
110,800
554,063

total
114,800
574,315

table 1. mcic-coco dataset descriptive statistics

correct responses
3 out of 3
at least 2 out of 3
at least 1 out of 3
0 out of 3

# instances accuracy%

673
828
931
69

67.3
82.8
93.1
0.0

in this work, simsurf is computed as the id7 score
between the inputs [26] (with the brevity penalty set to 1).
the embedding similarity, simpv, is computed as the cosine
similarity between the two in the pv embedding space.

3.3. the mcic-coco dataset

we applied the mc-ic algorithm to the coco
dataset [21] to generate a dataset for the visual-language
dual machine comprehension task. the dataset is called
mcic-coco and it is made publicly available3. we de-
scribe the details of this dataset below.

we set the neighborhood size at n = 500, and the
threshold at l = 0.5 (see eq. 1). as the coco dataset
has a large body of images (thus captions) focusing on a
few categories (such as sports activities), this threshold is
important in discarding signi   cantly similar captions to be
decoys     otherwise, even human annotators will experience
dif   culty in selecting the ground-truth captions.

r. the total number of data examples m =(cid:80)n

the hyperparameters of the pv model, dim (embed-
ding dimension) and epochs (number of training epochs),
are optimized in the optimize-pv step of the mc-ic
algorithm. the main idea is to learn embeddings such
that ground-truth captions from the same image have sim-
ilar embeddings. concretely, the optimization step is a
grid-search over the hyper-parameters of the pv-dbow
model [19], which we train using a softmax loss. since
there are multiple ground-truth captions associated with
each image, the dataset is denoted by c = {(cid:104)irc , crc(cid:105)|1    
r     n, 1     c     sr}, where r is the index for each
unique image (irc     ir), n is the total number images
and sr > 1 is the number of unique captions for image
r=1 sr. here
the hyper-parameters are searched on a grid to minimize
   multiple ground-truth score    rank (mgs-rank):
the aver-
age rank (under the cosine-distance score) between crc and
{crl|1     l     sr, l (cid:54)= c}. the lower the mgs-rank, the better
the resulting paragraph vector model is at modeling mul-
tiple ground-truths for a given image as being similar. as
such, our grid-search over the mcic-coco dev dataset
yields a minimum mgs-rank at dim=1024 and epochs=5.
similarly, the optimize-score(pv, score) step is a
grid-search over the    parameter of the score function,
given a paragraph vector embedding model pv and a
dataset c of captions and images, as before. a well-chosen
   will ensure the multiple ground-truth captions for the

3http://www.github.com/google/mcic-coco

4

table 2. human performance on the dmc task with the mcic-
coco dataset. bold denotes the performance ceiling.

same image will be measured with high degree of simi-
larity with the score function. the        [0, 1] parameter
is searched on a grid to minimize the    weighted multiple
ground-truths score    rank (wmgs-rank):
the average rank
(under the score) between crc and {crl|1     l     sr, l (cid:54)= c},
relative to the top n-best closest-cosine neighbors in pv.
for example, if given    ve ground-truths for image ir, and
when considering cr1, ground-truths cr2 to cr5 are ranking
at #4, #10, #16, and #22 (in top-500 closest-cosine neigh-
bors in pv), then wmgs-rank(cr1 ) = 13 (the average of
these ranks). our grid-search over the mcic-coco dev
dataset yields a minimum wmgs-rank at   =0.3.
the resulting mcic-coco dataset has 574,315 in-
stances that are in the format of {i : ((cid:104)ii, cj
i ), j =
1 . . . 5} where labelj
i     {true, false}. for each such in-
stance, there is one and only one j such that the label is
true. we have created a train/dev/test split such that all of
the instances for the same image occur in the same split.
table 1 reports the basic statistics for the dataset.
3.4. human performance on mcic-coco
setup to measure how well humans can perform on the
dmc task, we randomly drew 1,000 instances from the
mcic-coco dev set and submitted those instances to hu-
man    raters   4 via a crowd-sourcing platform.

i(cid:105), labelj

three independent responses from 3 different rates were
gathered for each instance, for a total of 3,000 responses.
to ensure diversity, raters were prohibited from evaluating
more than six instances or from responding to the same task
instance twice. in total, 807 distinct raters were employed.
raters were shown one instance at a time. they were
shown the image and the    ve caption choices (ground-truth
and four decoys, in randomized order) and were instructed
to choose the best caption for the image. before starting
evaluation, the raters were trained with sample instances
from the train dataset, disjoint from the dev dataset on
which their performance data were collected. the training
process presents an image and    ve sentences, of which the
ground-truth caption is highlighted. in addition, speci   c in-
structions and clari   cation were given to the raters on how
to choose the best caption for the image. in figure 1, we
present three instances on how the rater instructions were

4raters are vetted, screened and tested before working on any tasks;

requirements include native-language pro   ciency level.

1. a herd of giraffe standing next to each other in a dirt    eld
2. a pack of elephants standing next to each other
3. animals are gathering next to each other in a dirt    eld
4. three giraffe standing next to each other on a grass    eld
5. two elephants standing next to each other in a grass    eld

instructions:
captions 1 and 4 are clearly incorrect. they do not match up with the image at all.
caption 2 calls the elephants a    pack   , which is vague and a bit subjective. it does not mention the grass at all.
caption 3 only uses the word    animals    to describe what is in the picture, when the picture clearly shows elephants.
caption 5 gives the exact count and correct animal type and even mentions the grass    eld. it is a more accurate,
descriptive, and objective caption than the other options.

1. a meal covered with a lot of broccoli and tomatoes
2. a pan    lled with a mixture of vegetables and meat
3. a piece of bread covered in a meat and sauces
4. a pizza smothered in cheese and meat with french fries
5. a plate of fries and a sandwich cut in half

instructions:
caption 3 does not mention the fork, the plate, or the eggs, but it is still the best option because the other captions are
inaccurate. captions 1, 2, 4, and 5 all describe items not present in the picture, such as broccoli, a pan, cheese, or fries.

1. the man in the picture is reaching toward a frisbee
2. a middle aged man in a    eld tossing a frisbee
3. a woman in stance to throw a frisbee
4. a man dives for a catch in this ultimate frisbee match
5. there is a male tennis player playing in a match

instructions:
caption 5 is clearly incorrect (the game being played here is not tennis). captions 2 and 3 describe the act of tossing,
but the picture shows the act of catching, so these captions are both inaccurate. captions 1 and 4 seem close, but the
phrase    dives for a catch    is more descriptive than the phrase    reaching toward a frisbee   . in addition, caption 4
mentions the name of the game that they are playing, so it better informs the reader about what is happening in the
rest of the image than the other caption does.

figure 1. examples of instances from the mcic-coco dataset (the ground-truth is in bold face), together with rater instructions.

presented for rater training.
quantitative results we assessed human performance in
two metrics: (1) percentage of correct rater responses (1-
human system): 81.1% (2432 out of 3000); (2) percentage
of instances with at least 50% (i.e. 2) correct responses (3-
human system): 82.8% (828 out of 1000).

table 2 gives a detailed breakdown on the statistics re-
lated to the inter-rater (dis)agreement. the    rst row, with
accuracy at 67.3%, suggests that this is the level at which
the correct answer is obvious (i.e., percentage of    easy    in-
stances). the second row, at 82.8%, indicates that this is
the performance ceiling in terms of accuracy that can be ex-
pected for the mcic-coco dataset; at the same time, it
suggests that the difference between 67.3% and 82.8% (i.e.,
about 15% of instances) is caused by    dif   cult    instances.
finally, the third row, at 93.1%, indicates that the level of

   unanswerable    instances is somewhere in the 10%-15%
range (combining the increase from 82.8% to 93.1% and
the remaining 6.9% that no one gets right).

we will investigate those instances in detail in the fu-
ture. the coco dataset has a signi   cant number of cap-
tions that    t more than one image in the dataset, given the
biased concentration on certain categories. thus, we sus-
pect that even with our threshold-check (cf. the introduction
of l in eq. 1), our procedure might have failed to    lter out
some impossible-to-distinguish decoys.
qualitative examples we present in figure 2 several exam-
ple instances from the mcic-coco dataset. the    rst ex-
ample illustrates how certain aspects of vqa are subsumed
by the dmc task: in order to correctly choose answer 3, a
system needs to implicitly answer questions like    how many
people are in the image?    (answer: three, thus choices 4.

5

1. three bikes on the shore while people talk on a small boat
2. three people and a dog are running on the beach
3. three people ride horses along the beach while a dog follows
4. two people on horseback ride along a beach
5. two people on horses trot along a sandy beach

1. a brown bear is standing in the grass
2. a brown bear standing in the water of a river
3. a dark brown bear standing in the woods
4. a small brown bear rolling in the grass
5. a small brown bear standing in the dirt

1. a family is playing the wii in a house
2. a man playing with a frisbee in a park
3. a small boy playing with kites in a    eld
4. three women play with frisbees in a shady park
5. boy    ies a kite with family in the park

figure 2. examples of instances from the mcic-coco dataset (the ground-truth is in bold face). the correct answers for the    rst two
examples are relatively obvious to humans, but less so to computer systems. the third example illustrates one of the dif   cult cases in which
the humans annotators did not agree (option 3. was also chosen).

and 5. are wrong), and    what are the people doing?    (an-
swer: riding horses, thus choices 1. and 2. are wrong). the
second example illustrates the extent to which a successful
computer system needs to be able to differentiate between
   standing    and    rolling    in a visually-grounded way, pre-
sumably via a pose model [39] combined with a translation
model between poses and their verbal correspondents. last
but not least, the third examples illustrates a dif   cult case,
which led to human annotator disagreement in our annota-
tion process (both choice 3. and 5. were selected by differ-
ent annotators).

4. learning methods

we describe several learning methods for the dual ma-
chine comprehension (dmc) task with the mcic dataset.
we start with linear models which will be used as base-
lines. we then present several neural-network based mod-
els. in particular, we describe a novel, hybrid neural net-
work model that combines the feedforward architecture and
the id195 architecture [29] for id72 of the
dmc task and the image captioning task. this new model
achieves the best performance in both tasks.
4.1. linear models as baselines
regression to examine how well the two embeddings are
aligned in    semantic understanding space   , a simple ap-

proach is to assume that the learners do not have access to
the decoys. instead, by accessing the ground-truth captions
only, the models learn a linear regressor from the image
embeddings to the target captions    embeddings (   forward
regression   ), or from the captions to the images (   back-
ward regression   ). with the former approach, referred as
baseline-i2c, we check whether the predicted caption for
any given image is closest to its true caption. with the latter,
referred as baseline-c2i, we check whether the predicted
image embedding by the ground-truth caption is the closest
among predicted ones by decoy captions to the real image
embeddings.
linear classi   er our next approach baseline-linm is a
linear classi   er learned to discriminate true targets from the
decoys. speci   cally, we learn a linear discriminant function
   c where    is a matrix measuring the
f (i, c;   ) = i
compatibility between two types of embeddings, cf.
[12].
the id168 is then given by

(cid:62)

l(  ) =

j(cid:54)=j    f (ii, cj
[max

i ;   )     f (ii, cj   

i ;   )]+

(2)

(cid:88)

i

where [ ]+ is the hinge function and j indexes over all
the available decoys and i indexes over all training in-
stances. the optimization tries to increase the gap between
the target cj   
and the worst    offending    decoy. we use
stochastic (sub)gradient methods to optimize   , and se-

i

6

th caption of the instance i is labeled as k, and(cid:80)

lect the best model in terms of accuracy on the mcic-
coco development set.
4.2. feedforward neural network (ffnn) models
to present our neural-network   based models, we use the
following notations. each training instance pair is a tuple
(cid:104)ii, cj
i(cid:105), where i denotes the image, and cj
i denotes the cap-
tion options, which can either be the target or the decoys.
we use a binary variable yijk     {0, 1} to denote whether j-
k yijk = 1.
we    rst employ the standard feedforward neural-
network models to solve the dmc task on the mcic-
for each instance pair (cid:104)ii, cj
i(cid:105),
coco dataset.
the
to the neural network is an embedding tuple
input
i ;    )(cid:105), where    denotes the parameters
(cid:104)dnn(ii;   ), emb(cj
of a deep convolutional neural network dnn. dnn takes
an image and outputs an image embedding vector.     is the
embedding matrix, and emb(.) denotes the mapping from a
list of word ids to a list of embedding vectors using    . the
id168 for our ffnn is given by:

yijk log fnk(dnn(ii;   ), emb(cj

i ;    ); u)

(3)

l(  ,    , u)=

(cid:88)
network, and(cid:80)

i,j,k

where fnk denotes the k-th output of a feedforward neural
k fnk(.) = 1. our architecture uses a two
hidden-layer fully connected network with recti   ed linear
hidden units, and a softmax layer on top.

the formula in eq. 3 is generic with respect to the num-
ber of classes. in particular, we consider a 2-class   classi   er
(k     {0, 1}, 1 for    yes   , this is a correct answer; 0 for    no   ,
this is an incorrect answer), applied independently on all the
(cid:104)ii, cj
i(cid:105) pairs and apply one ffnn-based binary classi   er
for each; the    nal prediction is the caption with the high-
est    yes    id203 among all instance pairs belonging to
instance i.
4.3. vec2seq + ffnn model

we describe here a hybrid neural-network model that
combines a recurrent neural-network with a feedforward
one. we encode the image into a single-cell id56 encoder,
and the caption into an id56 decoder. because the    rst se-
quence only contains one cell, we call this model a vector-
to-sequence (vec2seq) model as a special case of id195
model as in [29, 4]. the output of each unit cell of a
vec2seq model (both on the encoding side and the decoding
side) can be fed into an ffnn architecture for binary clas-
si   cation. see figure 3 for an illustration of the vec2seq +
ffnn model architecture.

id72
in addition to the classi   cation loss
(eq. 3), we also include a loss for generating an output se-
quence cj
i based on an input ii image. we de   ne a binary

7

figure 3. vec2seq + ffnn model architecture.

variable zijlv     {0, 1} to indicate whether the lth word of
cj
i is equal to word v. od
ijl denotes the l-th output of the
decoder of instance pair (cid:104)ii, cj
ij denotes the output of
the encoder, and od
ij: denotes the concatenation of decoder
outputs.

i(cid:105), oe

with these de   nitions, the id168 for the vec2seq

+ ffnn model is:
l(  , w, u)

(cid:88)

=

(cid:88)

i,j,k

+   gen

where(cid:80)

yijk log fnk(oe

ij(ii, cj

i ;   ), od

ij:(ii, cj

i ;   ); u)

yij1zijlv log softmaxv(od

ijl(ii, cj

i ;   ); w)

i,j,l,v

(4)

v softmaxv(.) = 1;    are the parameters of the
vec2seq model, which include the parameters within each
unit cell, as well as the elements in the embedding matrices
for images and target sequences; w are the output projection
parameters that transform the output space of the decoder
to the vocabulary space. u are the parameters of the ffnn
model (eq. 3);   gen is the weight assigned to the sequence-
to-sequence generation loss. only the true target candidates
(the ones with yij1 = 1) are included in this loss, as we do
not want the decoy target options to affect this computation.
the vec2seq model we use here is an instantiation of
the attention-enhanced models proposed in [4, 8]. however,
our current model does not support location-wise attention,
as in the show-attend-and-tell [36] model. in this sense,
our model is an extension of the show-and-tell model with
a single attention state representing the entire image, used
as image memory representation for all decoder decisions.

we apply gated recurrent unit (gru) as the unit cell [9].
we also compare the in   uence on performance of the   gen
parameter.

5. experiments
5.1. experimental setup
baseline models for the baseline models, we use the 2048-
dimensional outputs of google-inception-v3 [30] (pre-
trained on id163 ilssvr 2012) to represent the im-
ages, and 1024-dimensional paragraph-vector embeddings
(section 3.2) to represent captions. to reduce computation
time, both are reduced to 256-dimensional vectors using
random projections.
neural-nets based models the experiments with these
models are done using the tensor   ow package [1]. the
hyper-parameter choices are decided using the hold-out de-
velopment portion of the mcic-coco set. for modeling
the input tokens, we use a vocabulary size of 8,855 types,
selected as the most frequent tokens over the captions from
the coco training set (words occurring at least 5 times).
the models are optimized using adagrad with an ini-
tial learning rate of 0.01, and clipped gradients (maximum
norm 4). we run the training procedures for 3, 000, 000
steps, with a mini-batch size of 20. we use 40 workers for
computing the updates, and 10 parameter servers for model
storing and (asynchronous and distributed) updating.

2-class

we use the following notations to refer to the neural
network models: ffnnargmax 1..5
refers to the version of
feedforward neural network architecture with a 2-class   
classi   er (   yes    or    no    for answer correctness), over which
an argmax function computes a 5-way decision (i.e., the
choice with the highest    yes    id203); we henceforth
refer to this model simply as ffnn.

2-class

the vec2seq+ffnn refers to the hybrid model
combining vec2seq and
described in section 4.3,
ffnnargmax 1..5
. the id56 part of the model uses a two-
hidden   layer gru unit-cell [9] con   guration, while the
ffnn part uses a two-hidden   layer architecture. the   gen
hyper-parameter from the loss-function l(  , w, u) (eq. 4)
is by default set to 1.0 (except for section 5.4 where we
directly measure its effect on performance).
id74 the metrics we use to measure per-
formance come in two    avors. first, the accuracy in de-
tecting (the index of) the true target among the decoys pro-
vides a direct way of measuring the performance level on
the comprehension task. we use this metric as the main
indicator of comprehension performance. second, because
our vec2seq+ffnn models are multi-task models, they can
also generate new captions given the input image. the per-
formance level for the generation task is measured using the
standard scripts measuring id8-l [20] and cider [32],
using as reference the available captions from the coco

model

dim
baseline-i2c 256
baseline-c2i
256
baseline-linm 256
ffnn 256
256

vec2seq+ffnn

dev

19.6   0.4
32.8   0.5
44.6   0.5
56.3   0.5
60.5   0.5

test

19.3  0.4
32.0  0.5
44.5  0.5
55.1  0.5
59.0  0.5

table 3. performance on the dmc task, in accuracies (and stan-
dard deviations) on mcic-coco for baselines and nn models.

data (around 5 for most of the images). code for these met-
rics is available as part of the coco evaluation toolkit 5. as
usual, both the hypothesis strings and the reference strings
are preprocessed: remove all the non-alphabetic characters;
transform all letters to lowercase, and tokenize using white
space; replace all words occurring less than 5 times with an
unknown token (cid:104)unk(cid:105) (total vocabulary of 8,855 types);
truncate to the    rst 30 tokens.
5.2. results

table 3 summarizes our main results on the comprehen-
sion task. we report the accuracies (and their standard de-
viations) for random choice, baselines, and neural network-
based models.

interestingly, the baseline-i2c model performs at the
level of random choice, and much worse than the baseline-
c2i model. this discrepancy re   ects the inherent dif   culty
in vision-language tasks: for each image, there are several
possible equally good descriptions, thus a linear mapping
from the image embeddings to the captions might not be
enough     statistically, the linear model will just predict the
mean of those captions. however, for the reverse direc-
tion where the captions are the independent variables, the
learned model does not have to capture the variability in im-
age embeddings corresponding to the different but equally
good captions     there is only one such image embedding.

nonlinear neural networks overcome these modeling
limitations. the results clearly indicate their superior-
ity over the baselines. the vec2seq+ffnn model ob-
tains the best results, with accuracies of 60.5% (dev)
and 59.0% (test);
the accuracy numbers indicate that
the vec2seq+ffnn architecture is superior to the non-
recursive fully-connected ffnn architecture (at 55.1% ac-
curacy on test). we show next the impact on performance
of the embedding dimension and neural-network sizes, for
both the feedforward and the recurrent architectures.
5.3. analysis: embedding dimension and neural-

network sizes

in this section, we compare neural networks mod-
speci   cally, we compare em-
{64, 256, 512, 1024, 2048},
of

els of different sizes.
dimensions
bedding

5https://github.com/tylin/coco-caption

8

dim hidden-1

hidden-2 dev

test

64
256
256
512
1024
2048

64
64
256
512
1024
2048

vec2seq+ffnn
64
256
256
512
1024
2048

64
64
256
512
1024
2048

56.5
56.3
55.8
54.1
52.2
50.7

ffnn
53.9   0.5
16
55.1   0.5
16
54.3   0.5
64
52.5   0.5
128
51.3   0.5
256
50.7   0.5
512
(with default   gen = 1.0)
54.0   0.5
16
59.0   0.5
16
58.8   0.5
64
59.6   0.5
128
60.8   0.5
256
60.8   0.5
512

55.3
60.5
61.2
61.6
62.5
63.4

table 4. the impact of model sizes on mcic-coco accuracy
for the ffnn model.

two

sizes

hidden-layer

architectures with

and
of
{(64, 16), (256, 64), (512, 128), (1024, 256), (2048, 512)}.
the results in table 4 illustrate an interesting behav-
ior for the neural-network architectures. for the ffnn
models, contrary to expectations, bigger network sizes
leads to decreasing accuracy. on the other hand, for
vec2seq+ffnn models, accuracy increases with increased
size in model parameters, up until the embedding dimen-
sion of the id56 model matches the embedding dimension
of the inception model, at 2048.

at accuracy levels of 63.4% (dev) and 60.8% (test),
this performance establishes a high-bar for a computer
model performance on the dmc task using the mcic-
coco dataset. according to the estimate from ta-
ble 2, this level of performance is still signi   cantly below
the 82.8% accuracy achievable by humans, which makes
mcic-coco a challenging testbed for future models of
vision-language machine comprehension.

5.4. id72 for dmc and image cap-

tioning

in this section, we compare models with different values
of   gen in eq. 4. this parameter allows for a natural pro-
gression from learning for the dmc task only (  gen = 0)
to focusing on the image captioning loss (  gen     +   ).
in between the two extremes, we have a id72
objective for jointly learning related tasks.

the results in table 5 illustrate one of the main points of
this paper. that is, the ability to perform the comprehen-
sion task (as measured by the accuracy metric) positively
correlates with the ability to perform other tasks that re-
quire machine comprehension, such as id134.
at   gen = 4, the vec2seq+ffnn model not only has a
high accuracy of detecting the ground-truth option, but it
also generates its own captions given the input image, with

9

  gen

0.0
0.1
1.0
2.0
4.0
8.0
16.0

test

acc
50.7   0.5
59.0   0.5
60.8   0.5
61.3   0.5
60.9   0.5
60.1   0.5
59.6   0.5

dev
50.7
61.1
63.4
63.4
63.0
62.1
61.8

id8-l
test
dev

-

0.517
0.528
0.528
0.533
0.526
0.530

-

0.511
0.518
0.519
0.524
0.520
0.519

cider

dev

-

0.901
0.972
0.971
0.989
0.957
0.965

test

-

0.865
0.903
0.921
0.938
0.914
0.912

table 5. the impact of   gen on mcic-coco accuracy,
together with caption-generation performance (id8-l and
cider against 5 references). all results are obtained with a
vec2seq+ffnn model (embedding size 2048 and hidden-layer
sizes of 2048 and 512).

an accuracy measured on mcic-coco at 0.9890 (dev)
and 0.9380 (test) cider scores. on the other hand, at an
accuracy level of about 59% (on test, at   gen = 0.1), the
generation performance is at only 0.9010 (dev) and 0.8650
(test) cider scores.

we note that there is an inherent trade-off between pre-
diction accuracy and generation performance, as seen for
  gen values above 4.0. this agrees with the intuition that
training a vec2seq+ffnn model using a loss l(  , w, u)
with a larger   gen means that the ground-truth detection
loss (the    rst term of the loss in eq.4) may get overwhelmed
by the word-generation loss (the second term). however,
our empirical results suggest that there is value in train-
ing models with a multi-task setup, in which both the com-
prehension side as well as the generation side are carefully
tuned to maximize performance.

6. discussion

we have proposed and described in detail a new multi-
modal machine comprehension task (dmc), combining the
challenges of understanding visual scenes and complex lan-
guage constructs simultaneously. the underlying hypothe-
sis for this work is that computer systems that can be shown
to perform increasingly well on this task will do so by con-
structing a visually-grounded understanding of various lin-
guistic elements and their dependencies. this type of work
can therefore bene   t research in both machine visual under-
standing and language comprehension.

the vec2seq+ffnn architecture that we propose for
addressing this combined challenge is a generic multi-task
model. it can be trained end-to-end to display both the abil-
ity to choose the most likely text associated with an image
(thus enabling a direct measure of its    comprehension    per-
formance), as well as the ability to generate a complex de-
scription of that image (thus enabling a direct measure of
its performance in an end-to-end complex and meaningful
task). the empirical results we present validate the underly-
ing hypothesis of our work, by showing that we can measure

the decisions made by such a computer system and validate
that improvements in comprehension and generation hap-
pen in tandem.

the experiments presented in this work are done train-
ing our systems in an end-to-end fashion, starting directly
from raw pixels. we hypothesize that our framework can
be fruitfully used to show that incorporating specialized vi-
sion systems (such as id164, scene recognition,
pose detection, etc.) is bene   cial. more precisely, not only
it can lead to a direct and measurable impact on a computer
system   s ability to perform image understanding, but it can
express that understanding in an end-to-end complex task.

references
[1] m. abadi, a. agarwal, p. barham, e. brevdo,
z. chen, c. citro, g. corrado, a. davis, j. dean,
m. devin, s. ghemawat, i. goodfellow, a. harp,
g. irving, m. isard, y. jia, r. jozefowicz, l. kaiser,
m. kudlur, j. levenberg, d. man  e, r. monga,
s. moore, d. murray, c. olah, m. schuster, j. shlens,
b. steiner, i. sutskever, k. talwar, p. tucker, v. van-
houcke, v. vasudevan, f. vi  egas, o. vinyals, p. war-
den, m. wattenberg, m. wicke, y. yu, and x. zheng.
tensorflow: large-scale machine learning on hetero-
geneous systems, 2015. software available from ten-
sor   ow.org.

[2] p. anderson, b. fernando, m. johnson, and s. gould.
spice: semantic propositional image caption evalua-
tion. corr, abs/1607.08822, 2016.

[3] s. antol, a. agrawal, j. lu, m. mitchell, d. batra,
c. l. zitnick, and d. parikh. vqa: visual question
answering. in international conference on computer
vision (iccv), 2015.

[8] d. chen, j. bolton, and c. d. manning. a thorough
examination of the id98/daily mail reading com-
prehension task. in proceedings of acl, 2016.

[9] k. cho, b. van merrienboer, c   . g  ulc  ehre, d. bah-
danau, f. bougares, h. schwenk, and y. bengio.
learning phrase representations using id56 encoder-
in pro-
decoder for id151.
ceedings of emnlp, october 25-29, 2014, doha,
qatar, pages 1724   1734, 2014.

[10] j. donahue, l. a. hendricks, s. guadarrama,
m. rohrbach, s. venugopalan, k. saenko, and t. dar-
rell. long-term recurrent convolutional networks for
visual recognition and description. in proc. of ieee
conference on id161 and pattern recogni-
tion (cvpr), 2014.

[11] h. fang, s. gupta, f. iandola, r. srivastava, l. deng,
p. doll  ar, j. gao, x. he, m. mitchell, j. platt, et al.
from captions to visual concepts and back. in proc.
of ieee conference on id161 and pattern
recognition (cvpr), 2015.

[12] a. frome, g. s. corrado, j. shlens, s. bengio,
j. dean, t. mikolov, et al. devise: a deep visual-
in advances in neural
semantic embedding model.
information processing systems (nips), 2013.

[13] h. gao, j. mao, j. zhou, z. huang, l. wang, and
w. xu. are you talking to a machine? dataset and
methods for multilingual image id53.
in nips, 2015.

[14] m. hodosh and j. hockenmaier. focused evaluation
for image description with binary forced-choice tasks.
in proc. 5th vision and language workshop, 2016.

[4] d. bahdanau, k. cho, and y. bengio. neural machine
translation by jointly learning to align and translate. in
proceedings of iclr, 2015.

[15] m. hodosh, p. young, and j. hockenmaier. framing
image description as a ranking task: data, models and
id74. jair, 2013.

[5] s. banerjee and a. lavie. meteor: an automatic
metric for mt evaluation with improved correlation
in proceedings of the acl
with human judgments.
workshop on intrinsic and extrinsic evaluation mea-
sures for machine translation and/or summarization,
2005.

[6] y. bengio. learning deep architectures for ai. found.

trends mach. learn., 2(1):1   127, jan. 2009.

[16] a. karpathy and l. fei-fei. deep visual-semantic
alignments for generating image descriptions. in proc.
of ieee conference on id161 and pattern
recognition (cvpr), 2015.

[17] r. kiros, r. salakhutdinov, and r. s. zemel. unifying
visual-semantic embeddings with multimodal neural
language models. transactions of the association for
computational linguistics, 2015.

[7] r. bernardi, r. cakici, d. elliott, a. erdem, e. er-
dem, n. ikizler-cinbis, f. keller, a. muscat, and
b. plank. automatic description generation from im-
ages: a survey of models, datasets, and evaluation
measures. jair, 55, 2016.

[18] r. krishna, y. zhu, o. groth, j. johnson, k. hata,
j. kravitz, s. chen, y. kalantidis, l.-j. li, d. a.
shamma, m. bernstein, and l. fei-fei.
visual
genome: connecting language and vision using
crowdsourced dense image annotations. 2016.

10

[19] q. le and t. mikolov. distributed representations of
sentences and documents. in proceedings of the 31st
international conference on machine learning, bei-
jing, china, 2014.

[32] r. vedantam, c. lawrence zitnick, and d. parikh.
cider: consensus-based image description evaluation.
in the ieee conference on id161 and pat-
tern recognition (cvpr), june 2015.

[20] c.-y. lin and f. j. och. automatic evaluation of ma-
chine translation quality using longest common subse-
quence and skip-bigram statistics. in proceedings of
acl, 2004.

[33] o. vinyals, a. toshev, s. bengio, and d. erhan. show
and tell: a neural image caption generator. in proc.
of ieee conference on id161 and pattern
recognition (cvpr), 2015.

[21] t. lin, m. maire, s. j. belongie, l. d. bourdev, r. b.
girshick, j. hays, p. perona, d. ramanan, p. doll  ar,
and c. l. zitnick. microsoft coco: common objects
in context. corr, abs/1405.0312, 2014.

[34] q. wu, d. teney, p. wang, c. shen, a. r. dick, and
a. van den hengel. visual id53: a sur-
vey of methods and datasets. corr, abs/1607.05910,
2016.

[35] q. wu, p. wang, c. shen, a. dick, and a. van den
hengel. ask me anything: free-form visual question
answering based on knowledge from external sources.
in cvpr, 2016.

[36] k. xu, j. ba, r. kiros, k. cho, a. c. courville,
r. salakhutdinov, r. s. zemel, and y. bengio. show,
attend and tell: neural image id134 with
visual attention. corr, abs/1502.03044, 2015.

[37] k. xu, j. ba, r. kiros, a. courville, r. salakhutdi-
nov, r. zemel, and y. bengio. show, attend and tell:
neural image id134 with visual attention.
in proc. of the 32nd international conference on ma-
chine learning (icml), 2015.

[38] z. yang, x. he, j. gao, l. deng, and a. smola.
stacked attention networks for image question an-
swering. in cvpr, 2016.

[39] b. yao and f.-f. li. modeling mutual context of ob-
ject and human pose in human-object interaction ac-
tivities. in proceedings of the 2010 ieee conference
on id161 and pattern recognition (cvpr),
2010.

[40] l. yu, e. park, a. c. berg, and t. l. berg. visual
madlibs: fill-in-theblank description generation and
id53. in iccv, 2015.

[41] y. zhu, o. groth, m. bernstein, and l. fei-fei. vi-
sual7w: grounded id53 in images. in
cvpr, 2016.

[22] x. lin and d. parikh.

leveraging visual ques-
tion answering for image-caption ranking. corr,
abs/1605.01379, 2016.

[23] m. malinowski and m. fritz. a multi-world approach
to id53 about real-world scenes based
on uncertain input. in nips, 2014.

[24] m. malinowski, m. rohrbach, and m. fritz. ask your
neurons: a neural-based approach to answering ques-
tions about images. in iccv, 2015.

[25] j. mao, w. xu, y. yang, j. wang, and a. yuille. deep
captioning with multimodal recurrent neural networks
(mid56). in proc. int. conf. learn. representations,
2015.

[26] k. papineni, s. roukos, t. ward, and w.-j. zhu. id7:
a method for automatic evaluation of machine trans-
lation. in proceedings of acl, pages 311   318, 2002.

[27] m. ranzato, s. chopra, m. auli, and w. zaremba. se-
quence level training with recurrent neural networks.
corr, abs/1511.06732, 2015.

[28] m. ren, r. kiros, and r. zemel. image question an-
swering: a visual semantic embedding model and a
new dataset. in nips, 2015.

[29] i. sutskever, o. vinyals, and q. v. v. le. sequence to
sequence learning with neural networks. in advances
in neural information processing systems 27, pages
3104   3112. curran associates, inc., 2014.

[30] c. szegedy, v. vanhoucke, s. ioffe, j. shlens, and
z. wojna. rethinking the inception architecture for
id161. volume abs/1512.00567, 2015.

[31] k. tu, m. meng, m. w. lee, t. e. choe, and s. c.
zhu.
joint video and text parsing for understand-
ing events and answering queries. ieee multimedia,
2014.

11

