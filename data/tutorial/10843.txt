7
1
0
2
 
c
e
d
4

 

 
 
]
i

a
.
s
c
[
 
 

3
v
6
2
3
7
0

.

3
0
7
1
:
v
i
x
r
a

one-shotimitationlearningyanduan     ,marcinandrychowicz   ,bradlystadie      ,jonathanho     ,jonasschneider   ,ilyasutskever   ,pieterabbeel     ,wojciechzaremba      berkeleyairesearchlab,   openai  workdonewhileatopenai{rockyduan,jonathanho,pabbeel}@eecs.berkeley.edu{marcin,bstadie,jonas,ilyasu,woj}@openai.comabstractimitationlearninghasbeencommonlyappliedtosolvedifferenttasksinisolation.thisusuallyrequireseithercarefulfeatureengineering,orasigni   cantnumberofsamples.thisisfarfromwhatwedesire:ideally,robotsshouldbeabletolearnfromveryfewdemonstrationsofanygiventask,andinstantlygeneralizetonewsituationsofthesametask,withoutrequiringtask-speci   cengineering.inthispaper,weproposeameta-learningframeworkforachievingsuchcapability,whichwecallone-shotimitationlearning.speci   cally,weconsiderthesettingwherethereisaverylarge(maybein   nite)setoftasks,andeachtaskhasmanyinstantiations.forexample,ataskcouldbetostackallblocksonatableintoasingletower,anothertaskcouldbetoplaceallblocksonatableintotwo-blocktowers,etc.ineachcase,differentinstancesofthetaskwouldconsistofdifferentsetsofblockswithdifferentinitialstates.attrainingtime,ouralgorithmispresentedwithpairsofdemonstrationsforasubsetofalltasks.aneuralnetistrainedsuchthatwhenittakesasinputthe   rstdemonstrationdemonstrationandastatesampledfromtheseconddemonstration,itshouldpredicttheactioncorrespondingtothesampledstate.attesttime,afulldemonstrationofasingleinstanceofanewtaskispresented,andtheneuralnetisexpectedtoperformwellonnewinstancesofthisnewtask.ourexperimentsshowthattheuseofsoftattentionallowsthemodeltogeneralizetoconditionsandtasksunseeninthetrainingdata.weanticipatethatbytrainingthismodelonamuchgreatervarietyoftasksandsettings,wewillobtainageneralsystemthatcanturnanydemonstrationsintorobustpoliciesthatcanaccomplishanoverwhelmingvarietyoftasks.1introductionweareinterestedinroboticsystemsthatareabletoperformavarietyofcomplexusefultasks,e.g.tidyingupahomeorpreparingameal.therobotshouldbeabletoleaid56ewtaskswithoutlongsysteminteractiontime.toaccomplishthis,wemustsolvetwobroadproblems.the   rstproblemisthatofdexterity:robotsshouldlearnhowtoapproach,graspandpickupcomplexobjects,andhowtoplaceorarrangethemintoadesiredcon   guration.thesecondproblemisthatofcommunication:howtocommunicatetheintentofthetaskathand,sothattherobotcanreplicateitinabroadersetofinitialconditions.demonstrationsareanextremelyconvenientformofinformationwecanusetoteachrobotstoover-comethesetwochallenges.usingdemonstrations,wecanunambiguouslycommunicateessentiallyanymanipulationtask,andsimultaneouslyprovidecluesaboutthespeci   cmotorskillsrequiredtoperformthetask.wecancomparethiswithanalternativeformofcommunication,namelynaturallanguage.althoughlanguageishighlyversatile,effective,andef   cient,naturallanguageprocessing31stconferenceonneuralinformationprocessingsystems(nips2017),longbeach,ca,usa.(a) traditional imitation learningtask ae.g. stack blocks into towers of height 3manydemonstrationsimitation learning algorithmpolicy fortask aactionenvironmentobstask be.g. stack blocks into towers of height 2manydemonstrationsimitation learning algorithmpolicy fortask bactionenvironmentobsmany demonstrationsfor task ameta learning algorithm   more demonstrations for more tasksone-shot imitator(neural network)environmentactionobssingle demonstration for task fpolicy for task fmany demonstrationsfor task bmany demonstrationsfor task asamplemany demonstrationsfor task b(b) one-shot imitation learning(c) training the one-shot imitatorone-shot imitator(neural network)supervised lossdemo1observation fromdemo2correspondingaction in demo2predictedactionfigure1:(a)traditionally,policiesaretask-speci   c.forexample,apolicymighthavebeentrainedthroughanimitationlearningalgorithmtostackblocksintotowersofheight3,andthenanotherpolicywouldbetrainedtostackblocksintotowersofheight2,etc.(b)inthispaper,weareinterestedintrainingnetworksthatarenotspeci   ctoonetask,butrathercanbetold(throughasingledemonstration)whatthecurrentnewtaskis,andbesuccessfulatthisnewtask.forexample,whenitisconditionedonasingledemonstrationfortaskf,itshouldbehavelikeagoodpolicyfortaskf.(c)wecanphrasethisasasupervisedlearningproblem,wherewetrainthisnetworkonasetoftrainingtasks,andwithenoughexamplesitshouldgeneralizetounseen,butrelatedtasks.totrainthisnetwork,ineachiterationwesampleademonstrationfromoneofthetrainingtasks,andfeedittothenetwork.then,wesampleanotherpairofobservationandactionfromaseconddemonstrationofthesametask.whenconditionedonboththe   rstdemonstrationandthisobservation,thenetworkistrainedtooutputthecorrespondingaction.systemsarenotyetatalevelwherewecouldeasilyuselanguagetopreciselydescribeacomplextasktoarobot.comparedtolanguage,usingdemonstrationshastwofundamentaladvantages:   rst,itdoesnotrequiretheknowledgeoflanguage,asitispossibletocommunicatecomplextaskstohumansthatdon   tspeakone   slanguage.andsecond,therearemanytasksthatareextremelydif   culttoexplaininwords,evenifweassumeperfectlinguisticabilities:forexample,explaininghowtoswimwithoutdemonstrationandexperienceseemstobe,attheveryleast,anextremelychallengingtask.indeed,learningfromdemonstrationshavehadmanysuccessfulapplications.however,sofartheseapplicationshaveeitherrequiredcarefulfeatureengineering,orasigni   cantamountofsysteminteractiontime.thisisfarfromwhatwhatwedesire:ideally,wehopetodemonstrateacertaintaskonlyonceorafewtimestotherobot,andhaveitinstantlygeneralizetonewsituationsofthesametask,withoutlongsysteminteractiontimeordomainknowledgeaboutindividualtasks.inthispaperweexploretheone-shotimitationlearningsettingillustratedinfig.1,wheretheobjectiveistomaximizetheexpectedperformanceofthelearnedpolicywhenfacedwithanew,previouslyunseen,task,andhavingreceivedasinputonlyonedemonstrationofthattask.forthetasksweconsider,thepolicyisexpectedtoachievegoodperformancewithoutanyadditionalsysteminteraction,onceithasreceivedthedemonstration.wetrainapolicyonabroaddistributionovertasks,wherethenumberoftasksispotentiallyin   nite.foreachtrainingtaskweassumetheavailabilityofasetofsuccessfuldemonstrations.ourlearnedpolicytakesasinput:(i)thecurrentobservation,and(ii)onedemonstrationthatsuccessfullysolvesadifferentinstanceofthesametask(thisdemonstrationis   xedforthedurationoftheepisode).thepolicyoutputsthecurrentcontrols.wenotethatanypairofdemonstrationsforthesametaskprovidesasupervisedtrainingexamplefortheneuralnetpolicy,whereonedemonstrationistreatedastheinput,whiletheotherastheoutput.2tomakethismodelwork,wemadeessentialuseofsoftattention[6]forprocessingboththe(poten-tiallylong)sequenceofstatesandactionthatcorrespondtothedemonstration,andforprocessingthecomponentsofthevectorspecifyingthelocationsofthevariousblocksinourenvironment.theuseofsoftattentionoverbothtypesofinputsmadestronggeneralizationpossible.inparticular,onafamilyofblockstackingtasks,ourneuralnetworkpolicywasabletoperformwellonnovelblockcon   gurationswhichwerenotpresentinanytrainingdata.videosofourexperimentsareavailableathttp://bit.ly/nips2017-oneshot.2relatedworkimitationlearningconsiderstheproblemofacquiringskillsfromobservingdemonstrations.surveyarticlesinclude[48,11,3].twomainlinesofworkwithinimitationlearningarebehavioralcloning,whichperformssupervisedlearningfromobservationstoactions(e.g.,[41,44]);andinversereinforcementlearning[37],wherearewardfunction[1,66,29,18,22]isestimatedthatexplainsthedemonstrationsas(near)optimalbehavior.whilethispastworkhasledtoawiderangeofimpressiveroboticsresults,itconsiderseachskillseparately,andhavinglearnedtoimitateoneskilldoesnotacceleratelearningtoimitatethenextskill.one-shotandfew-shotlearninghasbeenstudiedforimagerecognition[61,26,47,42],generativemodeling[17,43],andlearning   fast   reinforcementlearningagentswithrecurrentpolicies[16,62].fastadaptationhasalsobeenachievedthroughfast-weights[5].likeouralgorithm,manyoftheaforementionedapproachesareaformofmeta-learning[58,49,36],wherethealgorithmitselfisbeinglearned.meta-learninghasalsobeenstudiedtodiscoverneuralnetworkweightoptimizationalgorithms[8,9,23,50,2,31].thispriorworkonone-shotlearningandmeta-learning,however,istailoredtorespectivedomains(imagerecognition,generativemodels,reinforcementlearning,optimization)andnotdirectlyapplicableintheimitationlearningsetting.recently,[19]proposeagenericframeworkformetalearningacrossseveralaforementioneddomains.howevertheydonotconsidertheimitationlearningsetting.reinforcementlearning[56,10]providesanalternativeroutetoskillacquisition,bylearningthroughtrialanderror.reinforcementlearninghashadmanysuccesses,includingbackgammon[57],helicoptercontrol[39],atari[35],go[52],continuouscontrolinsimulation[51,21,32]andonrealrobots[40,30].however,reinforcementlearningtendstorequirealargenumberoftrialsandrequiresspecifyingarewardfunctiontode   nethetaskathand.theformercanbetime-consumingandthelattercanoftenbesigni   cantlymoredif   cultthanprovidingademonstration[37].multi-taskandtransferlearningconsiderstheproblemoflearningpolicieswithapplicabilityandre-usebeyondasingletask.successstoriesincludedomainadaptationincomputervision[64,34,28,4,15,24,33,59,14]andcontrol[60,45,46,20,54].however,whileacquiringamultitudeofskillsfasterthanwhatitwouldtaketoacquireeachoftheskillsindependently,theseapproachesdonotprovidetheabilitytoreadilypickupanewskillfromasingledemonstration.ourapproachheavilyreliesonanattentionmodeloverthedemonstrationandanattentionmodeloverthecurrentobservation.weusethesoftattentionmodelproposedin[6]formachinetranslations,andwhichhasalsobeensuccessfulinimagecaptioning[63].theinteractionnetworksproposedin[7,12]alsoleveragelocalityofphysicalinteractioninlearning.ourmodelisalsorelatedtothesequencetosequencemodel[55,13],asinbothcasesweconsumeaverylongdemonstrationsequenceand,effectively,emitalongsequenceofactions.3oneshotimitationlearning3.1problemformalizationwedenoteadistributionoftasksbyt,anindividualtaskbyt   t,andadistributionofdemon-strationsforthetasktbyd(t).apolicyissymbolizedby      (a|o,d),whereaisanaction,oisanobservation,disademonstration,and   aretheparametersofthepolicy.ademonstrationd   d(t)isasequenceofobservationsandactions:d=[(o1,a1),(o2,a2),...,(ot,at)].weassumethatthedistributionoftaskstisgiven,andthatwecanobtainsuccessfuldemonstrationsforeachtask.weassumethatthereissomescalar-valuedevaluationfunctionrt(d)(e.g.abinaryvalue3indicatingsuccess)foreachtask,althoughthisisnotrequiredduringtraining.theobjectiveistomaximizetheexpectedperformanceofthepolicy,wheretheexpectationistakenovertaskst2t,anddemonstrationsd2d(t).3.2blockstackingtaskstoclarifytheproblemsetting,wedescribeaconcreteexampleofadistributionofblockstackingtasks,whichwewillalsolaterstudyintheexperiments.thecompositionalstructuresharedamongthesetasksallowsustoinvestigatenontrivialgeneralizationtounseentasks.foreachtask,thegoalistocontrola7-doffetchroboticarmtostackvariousnumbersofcube-shapedblocksintoaspeci   ccon   gurationspeci   edbytheuser.eachcon   gurationconsistsofalistofblocksarrangedintotowersofdifferentheights,andcanbeidenti   edbyastring.forexample,abcdefghmeansthatwewanttostack4towers,eachwithtwoblocks,andwewantblockatobeontopofblockb,blockcontopofblockd,blockeontopofblockf,andblockgontopofblockh.eachofthesecon   gurationscorrespondtoadifferenttask.furthermore,ineachepisodethestartingpositionsoftheblocksmayvary,whichrequiresthelearnedpolicytogeneralizeevenwithinthetrainingtasks.inatypicaltask,anobservationisalistof(x,y,z)objectpositionsrelativetothegripper,andinformationifgripperisopenedorclosed.thenumberofobjectsmayvaryacrossdifferenttaskinstances.wede   neastageasasingleoperationofstackingoneblockontopofanother.forexample,thetaskabcdefghhas4stages.3.3algorithminordertotraintheneuralnetworkpolicy,wemakeuseofimitationlearningalgorithmssuchasbehavioralcloninganddagger[44],whichonlyrequiredemonstrationsratherthanrewardfunctionstobespeci   ed.thishasthepotentialtobemorescalable,sinceitisofteneasiertodemonstrateataskthanspecifyingawell-shapedrewardfunction[38].westartbycollectingasetofdemonstrationsforeachtask,whereweaddnoisetotheactionsinordertohavewidercoverageinthetrajectoryspace.ineachtrainingiteration,wesamplealistoftasks(withreplacement).foreachsampledtask,wesampleademonstrationaswellasasmallbatchofobservation-actionpairs.thepolicyistrainedtoregressagainstthedesiredactionswhenconditionedonthecurrentobservationandthedemonstration,byminimizingan`2orcross-id178lossbasedonwhetheractionsarecontinuousordiscrete.ahigh-levelillustrationofthetrainingprocedureisgiveninfig.1(c).acrossallexperiments,weuseadamax[25]toperformtheoptimizationwithalearningrateof0.001.4architecturewhile,inprinciple,agenericneuralnetworkcouldlearnthemappingfromdemonstrationandcurrentobservationtoappropriateaction,wefounditimportanttouseanappropriatearchitecture.ourarchitectureforlearningblockstackingisoneofthemaincontributionsofthispaper,andwebelieveitisrepresentativeofwhatarchitecturesforone-shotimitationlearningcouldlooklikeinthefuturewhenconsideringmorecomplextasks.ourproposedarchitectureconsistsofthreemodules:thedemonstrationnetwork,thecontextnetwork,andthemanipulationnetwork.anillustrationofthearchitectureisshowninfig.2.wewilldescribethemainoperationsperformedineachmodulebelow,andafullspeci   cationisavailableintheappendix.4.1demonstrationnetworkthedemonstrationnetworkreceivesademonstrationtrajectoryasinput,andproducesanembeddingofthedemonstrationtobeusedbythepolicy.thesizeofthisembeddinggrowslinearlyasafunctionofthelengthofthedemonstrationaswellasthenumberofblocksintheenvironment.temporaldropout:forblockstacking,thedemonstrationscanspanhundredstothousandsoftimesteps,andtrainingwithsuchlongsequencescanbedemandinginbothtimeandmemoryusage.hence,werandomlydiscardasubsetoftimestepsduringtraining,anoperationwecalltemporaldropout,analogousto[53,27].wedenotepastheproportionoftimestepsthatarethrownaway.4hidden layershidden layerstemporal dropoutneighborhood attention+temporal convolutionattention overdemonstrationdemonstrationcurrent stateactionablock#bcdefghijattention overcurrent statecontext networkdemonstration networkmanipulation networkcontext embeddingfigure2:illustrationofthenetworkarchitecture.inourexperiments,weusep=0.95,whichreducesthelengthofdemonstrationsbyafactorof20.duringtesttime,wecansamplemultipledownsampledtrajectories,useeachofthemtocomputedownstreamresults,andaveragetheseresultstoproduceanensembleestimate.inourexperience,thisconsistentlyimprovestheperformanceofthepolicy.neighborhoodattention:afterdownsamplingthedemonstration,weapplyasequenceofopera-tions,composedofdilatedtemporalconvolution[65]andneighborhoodattention.wenowdescribethissecondoperationinmoredetail.sinceourneuralnetworkneedstohandledemonstrationswithvariablenumbersofblocks,itmusthavemodulesthatcanprocessvariable-dimensionalinputs.softattentionisanaturaloperationwhichmapsvariable-dimensionalinputsto   xed-dimensionaloutputs.however,bydoingso,itmayloseinformationcomparedtoitsinput.thisisundesirable,sincetheamountofinformationcontainedinademonstrationgrowsasthenumberofblocksincreases.therefore,weneedanoperationthatcanmapvariable-dimensionalinputstooutputswithcomparabledimensions.intuitively,ratherthanhavingasingleoutputasaresultofattendingtoallinputs,wehaveasmanyoutputsasinputs,andhaveeachoutputattendingtoallotherinputsinrelationtoitsowncorrespondinginput.westartbydescribingthesoftattentionmoduleasspeci   edin[6].theinputtotheattentionincludesaqueryq,alistofcontextvectors{cj},andalistofmemoryvectors{mj}.theithattentionweightisgivenbywi vttanh(q+ci),wherevisalearnedweightvector.theoutputofattentionisaweightedcombinationofthememorycontent,wheretheweightsaregivenbyasoftmaxoperationovertheattentionweights.formally,wehaveoutput pimiexp(wi)pjexp(wj).notethattheoutputhasthesamedimensionasamemoryvector.theattentionoperationcanbegeneralizedtomultiplequeryheads,inwhichcasetherewillbeasmanyoutputvectorsastherearequeries.nowweturntoneighborhoodattention.weassumetherearebblocksintheenvironment.wedenotetherobot   sstateassrobot,andthecoordinatesofeachblockas(x1,y1,z1),...,(xb,yb,zb).theinputtoneighborhoodattentionisalistofembeddingshin1,...,hinbofthesamedimension,whichcanbetheresultofaprojectionoperationoveralistofblockpositions,ortheoutputofapreviousneighborhoodattentionoperation.giventhislistofembeddings,weusetwoseparatelinearlayerstocomputeaqueryvectorandacontextembeddingforeachblock:qi linear(hini),andci linear(hini).thememorycontenttobeextractedconsistsofthecoordinatesofeachblock,concatenatedwiththeinputembedding.theithqueryresultisgivenbythefollowingsoftattentionoperation:resulti softattn(query:qi,context:{cj}bj=1,memory:{((xj,yj,zj),hinj))}bj=1).intuitively,thisoperationallowseachblocktoqueryotherblocksinrelationtoitself(e.g.   ndtheclosestblock),andextractthequeriedinformation.thegatheredresultsarethencombinedwitheachblock   sowninformation,toproducetheoutputembeddingperblock.concretely,wehave5outputi linear(concat(hini,resulti,(xi,yi,zi),srobot)).inpractice,weusemultiplequeryheadsperblock,sothatthesizeofeachresultiwillbeproportionaltothenumberofqueryheads.4.2contextnetworkthecontextnetworkisthecruxofourmodel.itprocessesboththecurrentstateandtheembeddingproducedbythedemonstrationnetwork,andoutputsacontextembedding,whosedimensiondoesnotdependonthelengthofthedemonstration,orthenumberofblocksintheenvironment.hence,itisforcedtocaptureonlytherelevantinformation,whichwillbeusedbythemanipulationnetwork.attentionoverdemonstration:thecontextnetworkstartsbycomputingaqueryvectorasafunctionofthecurrentstate,whichisthenusedtoattendoverthedifferenttimestepsinthedemonstrationembedding.theattentionweightsoverdifferentblockswithinthesametimesteparesummedtogether,toproduceasingleweightpertimestep.theresultofthistemporalattentionisavectorwhosesizeisproportionaltothenumberofblocksintheenvironment.wethenapplyneighborhoodattentiontopropagatetheinformationacrosstheembeddingsofeachblock.thisprocessisrepeatedmultipletimes,wherethestateisadvancedusinganlstmcellwithuntiedweights.attentionovercurrentstate:thepreviousoperationsproduceanembeddingwhosesizeisinde-pendentofthelengthofthedemonstration,butstilldependentonthenumberofblocks.wethenapplystandardsoftattentionoverthecurrentstatetoproduce   xed-dimensionalvectors,wherethememorycontentonlyconsistsofpositionsofeachblock,which,togetherwiththerobot   sstate,formsthecontextembedding,whichisthenpassedtothemanipulationnetwork.intuitively,althoughthenumberofobjectsintheenvironmentmayvary,ateachstageofthemanipulationoperation,thenumberofrelevantobjectsissmallandusually   xed.fortheblockstackingenvironmentspeci   cally,therobotshouldonlyneedtopayattentiontothepositionoftheblockitistryingtopickup(thesourceblock),aswellasthepositionoftheblockitistryingtoplaceontopof(thetargetblock).therefore,aproperlytrainednetworkcanlearntomatchthecurrentstatewiththecorrespondingstageinthedemonstration,andinfertheidentitiesofthesourceandtargetblocksexpressedassoftattentionweightsoverdifferentblocks,whicharethenusedtoextractthecorrespondingpositionstobepassedtothemanipulationnetwork.althoughwedonotenforcethisinterpretationintraining,ourexperimentanalysissupportsthisinterpretationofhowthelearnedpolicyworksinternally.4.3manipulationnetworkthemanipulationnetworkisthesimplestcomponent.afterextractingtheinformationofthesourceandtargetblocks,itcomputestheactionneededtocompletethecurrentstageofstackingoneblockontopofanotherone,usingasimplemlpnetwork.1thisdivisionoflaboropensupthepossibilityofmodulartraining:themanipulationnetworkmaybetrainedtocompletethissimpleprocedure,withoutknowingaboutdemonstrationsormorethantwoblockspresentintheenvironment.weleavethispossibilityforfuturework.5experimentsweconductexperimentswiththeblockstackingtasksdescribedinsection3.2.2theseexperimentsaredesignedtoanswerthefollowingquestions:   howdoestrainingwithbehavioralcloningcomparewithdagger?   howdoesconditioningontheentiredemonstrationcomparetoconditioningonthe   nalstate,evenwhenitalreadyhasenoughinformationtofullyspecifythetask?   howdoesconditioningontheentiredemonstrationcomparetoconditioningona   snapshot   ofthetrajectory,whichisasmallsubsetofframesthataremostinformative?1inprinciple,onecanreplacethismodulewithanid56module.butwedidnot   ndthisnecessaryforthetasksweconsider.2additionalexperimentresultsareavailableintheappendix,includingasimpleillustrativeexampleofparticlereachingtasksandfurtheranalysisofblockstacking6   canourframeworkgeneralizetotasksthatithasneverseenduringtraining?toanswerthesequestions,wecomparetheperformanceofthefollowingarchitectures:   bc:weusethesamearchitectureasprevious,butandthepolicyusingbehavioralcloning.   dagger:weusethearchitecturedescribedintheprevioussection,andtrainthepolicyusingdagger.   finalstate:thisarchitectureconditionsonthe   nalstateratherthanontheentiredemon-strationtrajectory.fortheblockstackingtaskfamily,the   nalstateuniquelyidenti   esthetask,andthereisnoneedforadditionalinformation.however,afulltrajectory,onewhichcontainsinformationaboutintermediatestagesofthetask   ssolution,canmakeiteasiertotraintheoptimalpolicy,becauseitcouldlearntorelyonthedemonstrationdirectly,withoutneedingtomemorizetheintermediatestepsintoitsparameters.thisisrelatedtothewayinwhichrewardshapingcansigni   cantlyaffectperformanceinreinforcementlearning[38].acomparisonbetweenthetwoconditioningstrategieswilltelluswhetherthishypothesisisvalid.wetrainthispolicyusingdagger.   snapshot:thisarchitectureconditionsona   snapshot   ofthetrajectory,whichincludesthelastframeofeachstagealongthedemonstrationtrajectory.thisassumesthatasegmentationofthedemonstrationintomultiplestagesisavailableattesttime,whichgivesitanunfairadvantagecomparedtotheotherconditioningstrategies.hence,itmayperformbetterthanconditioningonthefulltrajectory,andservesasareference,toinformuswhetherthepolicyconditionedontheentiretrajectorycanperformaswellasifthedemonstrationisclearlysegmented.again,wetrainthispolicyusingdagger.weevaluatethepolicyontasksseenduringtraining,aswellastasksunseenduringtraining.notethatgeneralizationisevaluatedatmultiplelevels:thelearnedpolicynotonlyneedstogeneralizetonewcon   gurationsandnewdemonstrationsoftasksseenalready,butalsoneedstogeneralizetonewtasks.concretely,wecollect140trainingtasks,and43testtasks,eachwithadifferentdesiredlayoutoftheblocks.thenumberofblocksineachtaskcanvarybetween2and10.wecollect1000trajectoriespertaskfortraining,andmaintainaseparatesetoftrajectoriesandinitialcon   gurationstobeusedforevaluation.thetrajectoriesarecollectedusingahard-codedpolicy.5.1performanceevaluation1234567numberofstages0%20%40%60%80%100%averagesuccessratepolicytypedemobcdaggersnapshotfinalstate(a)performanceontrainingtasks.245678numberofstages0%20%40%60%80%100%averagesuccessratepolicytypedemobcdaggersnapshotfinalstate(b)performanceontesttasks.figure3:comparisonofdifferentconditioningstrategies.thedarkestbarshowstheperformanceofthehard-codedpolicy,whichunsurprisinglyperformsthebestmostofthetime.forarchitecturesthatusetemporaldropout,weuseanensembleof10differentdownsampleddemonstrationsandaveragetheactiondistributions.thenforallarchitecturesweusethegreedyactionforevaluation.fig.3showstheperformanceofvariousarchitectures.resultsfortrainingandtesttasksarepresentedseparately,wherewegrouptasksbythenumberofstagesrequiredtocompletethem.thisisbecausetasksthatrequiremorestagestocompletearetypicallymorechallenging.infact,evenourscriptedpolicyfrequentlyfailsonthehardesttasks.wemeasuresuccessratepertaskbyexecutingthegreedypolicy(takingthemostcon   dentactionateverytimestep)in100differentcon   gurations,eachconditionedonadifferentdemonstrationunseenduringtraining.wereporttheaveragesuccessrateoveralltaskswithinthesamegroup.7fromthe   gure,wecanobservethatfortheeasiertaskswithfewerstages,allofthedifferentconditioningstrategiesperformequallywellandalmostperfectly.asthedif   culty(numberofstages)increases,however,conditioningontheentiredemonstrationstartstooutperformconditioningonthe   nalstate.onepossibleexplanationisthatwhenconditionedonlyonthe   nalstate,thepolicymaystruggleaboutwhichblockitshouldstack   rst,apieceofinformationthatisreadilyaccessiblefromdemonstration,whichnotonlycommunicatesthetask,butalsoprovidesvaluableinformationtohelpaccomplishit.moresurprisingly,conditioningontheentiredemonstrationalsoseemstooutperformconditioningonthesnapshot,whichweoriginallyexpectedtoperformthebest.wesuspectthatthisisduetotheid173effectintroducedbytemporaldropout,whicheffectivelyaugmentsthesetofdemonstrationsseenbythepolicyduringtraining.anotherinteresting   ndingwasthattrainingwithbehavioralcloninghasthesamelevelofperformanceastrainingwithdagger,whichsuggeststhattheentiretrainingprocedurecouldworkwithoutrequiringinteractivesupervision.inourpreliminaryexperiments,wefoundthatinjectingnoiseintothetrajectorycollectionprocesswasimportantforbehavioralcloningtoworkwell,henceinallexperimentsreportedhereweusenoiseinjection.inpractice,suchnoisecancomefromnaturalhuman-inducednoisethroughtele-operation,orbyarti   ciallyinjectingadditionalnoisebeforeapplyingitonthephysicalrobot.5.2visualizationwevisualizetheattentionmechanismsunderlyingthemainpolicyarchitecturetohaveabetterunderstandingabouthowitoperates.therearetwokindsofattentionwearemainlyinterestedin,onewherethepolicyattendstodifferenttimestepsinthedemonstration,andtheotherwherethepolicyattendstodifferentblocksinthecurrentstate.fig.4showssomeoftheattentionheatmaps.(a)attentionoverblocksinthecurrentstate.(b)attentionoverdownsampleddemonstration.figure4:visualizingattentionsperformedbythepolicyduringanentireexecution.thetaskbeingperformedisabcdefghij.notethatthepolicyhasmultiplequeryheadsforeachtypeofattention,andonlyonequeryheadpertypeisvisualized.(a)wecanobservethatthepolicyalmostalwaysfocusesonasmallsubsetoftheblockpositionsinthecurrentstate,whichallowsthemanipulationnetworktogeneralizetooperationsoverdifferentblocks.(b)wecanobserveasparsepatternoftimestepsthathavehighattentionweights.thissuggeststhatthepolicyhasessentiallylearnedtosegmentthedemonstrations,andonlyattendtoimportantkeyframes.notethatthereareroughly6regionsofhighattentionweights,whichnicelycorrespondstothe6stagesrequiredtocompletethetask.6conclusionsinthiswork,wepresentedasimplemodelthatmapsasinglesuccessfuldemonstrationofatasktoaneffectivepolicythatsolvessaidtaskinanewsituation.wedemonstratedeffectivenessofthisapproachonafamilyofblockstackingtasks.therearealotofexcitingdirectionsforfuturework.weplantoextendtheframeworktodemonstrationsintheformofimagedata,whichwillallowmoreend-to-endlearningwithoutrequiringaseparateperceptionmodule.wearealsointerestedinenablingthepolicytoconditiononmultipledemonstrations,incasewhereonedemonstrationdoesnotfullyresolveambiguityintheobjective.furthermoreandmostimportantly,wehopetoscaleup8ourmethodonamuchlargerandbroaderdistributionoftasks,andexploreitspotentialtowardsageneralroboticsimitationlearningsystemthatwouldbeabletoachieveanoverwhelmingvarietyoftasks.7acknowledgementwewouldliketothankourcolleaguesatucberkeleyandopenaiforinsightfuldiscussions.thisresearchwasfundedinpartbyonrthroughapecaseaward.yanduanwasalsosupportedbyahuaweifellowship.jonathanhowasalsosupportedbyannsffellowship.references[1]pieterabbeelandandrewng.apprenticeshiplearningviainversereinforcementlearning.ininternationalconferenceonmachinelearning(icml),2004.[2]marcinandrychowicz,mishadenil,sergiogomez,matthewwhoffman,davidpfau,tomschaul,andnandodefreitas.learningtolearnbygradientdescentbygradientdescent.inneuralinformationprocessingsystems(nips),2016.[3]brennadargall,soniachernova,manuelaveloso,andbrettbrowning.asurveyofrobotlearningfromdemonstration.roboticsandautonomoussystems,57(5):469   483,2009.[4]yusufaytarandandrewzisserman.tabularasa:modeltransferforobjectcategorydetection.in2011internationalconferenceoncomputervision,pages2252   2259.ieee,2011.[5]jimmyba,geoffreyehinton,volodymyrmnih,joelzleibo,andcatalinionescu.usingfastweightstoattendtotherecentpast.inneuralinformationprocessingsystems(nips),2016.[6]dzmitrybahdanau,kyunghyuncho,andyoshuabengio.neuralmachinetranslationbyjointlylearningtoalignandtranslate.arxivpreprintarxiv:1409.0473,2014.[7]peterbattaglia,razvanpascanu,matthewlai,danilojimenezrezende,etal.interactionnetworksforlearningaboutobjects,relationsandphysics.inadvancesinneuralinformationprocessingsystems,pages4502   4510,2016.[8]samybengio,yoshuabengio,jocelyncloutier,andjangecsei.ontheoptimizationofasynapticlearningrule.inoptimalityinarti   cialandbiologicalneuralnetworks,pages6   8,1992.[9]yoshuabengio,samybengio,andjocelyncloutier.learningasynapticlearningrule.universit  demontr  al,d  partementd   informatiqueetderechercheop  rationnelle,1990.[10]dimitripbertsekasandjohnntsitsiklis.neuro-dynamicprogramming:anoverview.indecisionandcontrol,1995.,proceedingsofthe34thieeeconferenceon,volume1,pages560   564.ieee,1995.[11]sylvaincalinon.robotprogrammingbydemonstration.epflpress,2009.[12]michaelbchang,tomerullman,antoniotorralba,andjoshuabtenenbaum.acompositionalobject-basedapproachtolearningphysicaldynamics.inint.conf.onlearningrepresentations(iclr),2017.[13]kyunghyuncho,bartvanmerri  nboer,caglargulcehre,dzmitrybahdanau,fethibougares,holgerschwenk,andyoshuabengio.learningphraserepresentationsusingid56encoder-decoderforstatisticalmachinetranslation.arxivpreprintarxiv:1406.1078,2014.[14]jeffdonahue,yangqingjia,oriolvinyals,judyhoffman,ningzhang,erictzeng,andtrevordarrell.decaf:adeepconvolutionalactivationfeatureforgenericvisualrecognition.inicml,pages647   655,2014.[15]lixinduan,dongxu,andivortsang.learningwithaugmentedfeaturesforheterogeneousdomainadaptation.arxivpreprintarxiv:1206.4660,2012.9[16]yanduan,johnschulman,xichen,peterlbartlett,ilyasutskever,andpieterabbeel.rl2:fastreinforcementlearningviaslowreinforcementlearning.arxivpreprintarxiv:1611.02779,2016.[17]harrisonedwardsandamosstorkey.towardsaneuralstatistician.internationalconferenceonlearningrepresentations(iclr),2017.[18]chelseafinn,sergeylevine,andpieterabbeel.guidedcostlearning:deepinverseoptimalcontrolviapolicyoptimization.inproceedingsofthe33rdinternationalconferenceonmachinelearning,volume48,2016.[19]chelseafinn,pieterabbeel,andsergeylevine.model-agnosticmeta-learningforfastadapta-tionofdeepnetworks.arxivpreprintarxiv:1703.03400,2017.[20]abhishekgupta,colinedevin,yuxuanliu,pieterabbeel,andsergeylevine.learninginvariantfeaturespacestotransferskillswithreinforcementlearning.inint.conf.onlearningrepresentations(iclr),2017.[21]nicolasheess,gregorywayne,davidsilver,timlillicrap,tomerez,andyuvaltassa.learningcontinuouscontrolpoliciesbystochasticvaluegradients.inadvancesinneuralinformationprocessingsystems,pages2944   2952,2015.[22]jonathanhoandstefanoermon.generativeadversarialimitationlearning.inadvancesinneuralinformationprocessingsystems,pages4565   4573,2016.[23]sepphochreiter,astevenyounger,andpeterrconwell.learningtolearnusinggradientdescent.ininternationalconferenceonarti   cialneuralnetworks.springer,2001.[24]judyhoffman,erikrodner,jeffdonahue,trevordarrell,andkatesaenko.ef   cientlearningofdomain-invariantimagerepresentations.arxivpreprintarxiv:1301.3224,2013.[25]diederikp.kingmaandjimmyba.adam:amethodforstochasticoptimization.inproceedingsofthe3rdinternationalconferenceonlearningrepresentations(iclr),2014.[26]gregorykoch.siameseneuralnetworksforone-shotimagerecognition.icmldeeplearningworkshop,2015.[27]davidkrueger,teganmaharaj,j  noskram  r,mohammadpezeshki,nicolasballas,nanrose-maryke,anirudhgoyal,yoshuabengio,hugolarochelle,aaroncourville,etal.zoneout:regularizingid56sbyrandomlypreservinghiddenactivations.arxivpreprintarxiv:1606.01305,2016.[28]briankulis,katesaenko,andtrevordarrell.whatyousawisnotwhatyouget:domainadaptationusingasymmetrickerneltransforms.incomputervisionandpatternrecognition(cvpr),2011ieeeconferenceon,pages1785   1792.ieee,2011.[29]s.levine,z.popovic,andv.koltun.nonlinearinversereinforcementlearningwithgaussianprocesses.inadvancesinneuralinformationprocessingsystems(nips),2011.[30]sergeylevine,chelseafinn,trevordarrell,andpieterabbeel.end-to-endtrainingofdeepvisuomotorpolicies.journalofmachinelearningresearch,17(39):1   40,2016.[31]keliandjitendramalik.learningtooptimize.arxivpreprintarxiv:1606.01885,2016.[32]timothyplillicrap,jonathanjhunt,alexanderpritzel,nicolasheess,tomerez,yuvaltassa,davidsilver,anddaanwierstra.continuouscontrolwithdeepreinforcementlearning.arxivpreprintarxiv:1509.02971,2015.[33]mingshenglongandjianminwang.learningtransferablefeatureswithdeepadaptationnetworks.corr,abs/1502.02791,1:2,2015.[34]yishaymansour,mehryarmohri,andafshinrostamizadeh.domainadaptation:learningboundsandalgorithms.arxivpreprintarxiv:0902.3430,2009.10[35]volodymyrmnih,koraykavukcuoglu,davidsilver,andreiarusu,joelveness,marcgbellemare,alexgraves,martinriedmiller,andreaskfidjeland,georgostrovski,etal.human-levelcontrolthroughdeepreinforcementlearning.nature,518(7540):529   533,2015.[36]devangknaikandrjmammone.meta-neuralnetworksthatlearnbylearning.ininternationaljointconferenceonneuralnetowrks(ijid98),1992.[37]andrewngandstuartrussell.algorithmsforinversereinforcementlearning.ininternationalconferenceonmachinelearning(icml),2000.[38]andrewyng,daishiharada,andstuartrussell.policyinvarianceunderrewardtransfor-mations:theoryandapplicationtorewardshaping.inicml,volume99,pages278   287,1999.[39]andrewyng,hjinkim,michaelijordan,shankarsastry,andshivballianda.autonomoushelicopter   ightviareinforcementlearning.innips,volume16,2003.[40]janpetersandstefanschaal.reinforcementlearningofmotorskillswithpolicygradients.neuralnetworks,21(4):682   697,2008.[41]deanapomerleau.alvinn:anautonomouslandvehicleinaneuralnetwork.inadvancesinneuralinformationprocessingsystems,pages305   313,1989.[42]sachinraviandhugolarochelle.optimizationasamodelforfew-shotlearning.inunderreview,iclr,2017.[43]danilojimenezrezende,shakirmohamed,ivodanihelka,karolgregor,anddaanwierstra.one-shotgeneralizationindeepgenerativemodels.internationalconferenceonmachinelearning(icml),2016.[44]st  phaneross,geoffreyjgordon,anddrewbagnell.areductionofimitationlearningandstructuredpredictiontono-regretonlinelearning.inaistats,volume1,page6,2011.[45]andreiarusu,neilcrabinowitz,guillaumedesjardins,hubertsoyer,jameskirkpatrick,koraykavukcuoglu,razvanpascanu,andraiahadsell.progressiveneuralnetworks.arxivpreprintarxiv:1606.04671,2016.[46]fereshtehsadeghiandsergeylevine.(cad)2rl:realsingle-image   ightwithoutasinglerealimage.2016.[47]adamsantoro,sergeybartunov,matthewbotvinick,daanwierstra,andtimothylillicrap.meta-learningwithmemory-augmentedneuralnetworks.ininternationalconferenceonmachinelearning(icml),2016.[48]stefanschaal.isimitationlearningtheroutetohumanoidrobots?trendsincognitivesciences,3(6):233   242,1999.[49]jurgenschmidhuber.evolutionaryprinciplesinself-referentiallearning.onlearninghowtolearn:themeta-meta-...hook.)diplomathesis,institutf.informatik,tech.univ.munich,1987.[50]j  rgenschmidhuber.learningtocontrolfast-weightmemories:analternativetodynamicrecurrentnetworks.neuralcomputation,1992.[51]johnschulman,sergeylevine,pieterabbeel,michaelijordan,andphilippmoritz.trustregionpolicyoptimization.inicml,pages1889   1897,2015.[52]davidsilver,ajahuang,chrisjmaddison,arthurguez,laurentsifre,georgevandendriess-che,julianschrittwieser,ioannisantonoglou,vedapanneershelvam,marclanctot,etal.mas-teringthegameofgowithdeepneuralnetworksandtreesearch.nature,529(7587):484   489,2016.[53]nitishsrivastava,geoffreyehinton,alexkrizhevsky,ilyasutskever,andruslansalakhutdi-nov.dropout:asimplewaytopreventneuralnetworksfromover   tting.journalofmachinelearningresearch,15(1):1929   1958,2014.11[54]bradliestadie,pieterabbeel,andilyasutskever.thirdpersonimitationlearning.inint.conf.onlearningrepresentations(iclr),2017.[55]ilyasutskever,oriolvinyals,andquocvle.sequencetosequencelearningwithneuralnetworks.inadvancesinneuralinformationprocessingsystems,pages3104   3112,2014.[56]richardssuttonandandrewgbarto.reinforcementlearning:anintroduction,volume1.mitpresscambridge,1998.[57]geraldtesauro.temporaldifferencelearningandtd-gammon.communicationsoftheacm,38(3):58   68,1995.[58]sebastianthrunandlorienpratt.learningtolearn.springerscience&businessmedia,1998.[59]erictzeng,judyhoffman,ningzhang,katesaenko,andtrevordarrell.deepdomainconfusion:maximizingfordomaininvariance.arxivpreprintarxiv:1412.3474,2014.[60]erictzeng,colinedevin,judyhoffman,chelseafinn,xingchaopeng,pieterabbeel,sergeylevine,katesaenko,andtrevordarrell.towardsadaptingdeepvisuomotorrepresentationsfromsimulatedtorealenvironments.arxivpreprintarxiv:1511.07111,2015.[61]oriolvinyals,charlesblundell,timlillicrap,daanwierstra,etal.matchingnetworksforoneshotlearning.inneuralinformationprocessingsystems(nips),2016.[62]janexwang,zebkurth-nelson,dhruvatirumala,hubertsoyer,joelzleibo,remimunos,charlesblundell,dharshankumaran,andmattbotvinick.learningtoreinforcementlearn.arxivpreprintarxiv:1611.05763,2016.[63]kelvinxu,jimmyba,ryankiros,kyunghyuncho,aaronccourville,ruslansalakhutdinov,richardszemel,andyoshuabengio.show,attendandtell:neuralimagecaptiongenerationwithvisualattention.inicml,volume14,pages77   81,2015.[64]junyang,rongyan,andalexanderghauptmann.cross-domainvideoconceptdetectionusingadaptiveid166s.inproceedingsofthe15thacminternationalconferenceonmultimedia,pages188   197.acm,2007.[65]fisheryuandvladlenkoltun.multi-scalecontextaggregationbydilatedconvolutions.ininternationalconferenceonlearningrepresentations(iclr),2016.[66]b.ziebart,a.maas,j.a.bagnell,anda.k.dey.maximumid178inversereinforcementlearning.inaaaiconferenceonarti   cialintelligence,2008.12aillustrativeexample:particlereachingtheparticlereachingproblemisaverysimplefamilyoftasks.ineachtask,wecontrolapointrobottoreachaspeci   clandmark,anddifferenttasksareidenti   edbydifferentlandmarks.asillustratedinfig.1,onetaskcouldbetoreachtheorangesquare,andanothertaskcouldbetoreachthegreentriangle.theagentreceivesitsown2dlocation,aswellasthe2dlocationsofeachofthelandmarks.withineachtask,theinitialpositionoftheagent,aswellasthepositionsofallthelandmarks,canvaryacrossdifferentinstancesofthetask.withoutademonstration,therobotdoesnotknowwhichlandmarkitshouldreach,andwillnotbeabletoaccomplishthetask.hence,thissettingalreadygetsattheessenceofone-shotimitation,namelytocommunicatethetaskviaademonstration.afterlearning,theagentshouldbeabletoidentifythetargetlandmarkfromthedemonstration,andreachthesamelandmarkinanewinstanceofthetask.figure1:therobotisapointmasscontrolledwith2-dimensionalforce.thefamilyoftasksistoreachatargetlandmark.theidentityofthelandmarkdiffersfromtasktotask,andthemodelhasto   gureoutwhichtargettopursuebasedonthedemonstration.(left)illustrationoftherobot;(middle)thetaskistoreachtheorangebox,(right)thetaskistoreachthegreentriangle.weconsiderthreearchitecturesforthisproblem:   plainlstm:the   rstarchitectureisasimplelstmwith512hiddenunits.itreadsthedemonstrationtrajectory,theoutputofwhichisthenconcatenatedwiththecurrentstate,andfedtoamulti-layerid88(mlp)toproducetheaction.   lstmwithattention:inthisarchitecture,thelstmoutputsaweightingoverthedif-ferentlandmarksfromthedemonstrationsequence.then,itappliesthisweightinginthetestscene,andproducesaweightedcombinationoverlandmarkpositionsgiventhecurrentstate.this2doutputisthenconcatenatedwiththecurrentagentposition,andfedtoanmlptoproducetheaction.   finalstatewithattention:ratherthanlookingattheentiredemonstrationtrajectory,thisarchitectureonlylooksatthe   nalstateinthedemonstration(whichisalreadysuf   cienttocommunicatethetask),andproduceaweightingoverlandmarks.itthenproceedslikethepreviousarchitecture.noticethatthesethreearchitecturesareincreasinglymorespecializedtothespeci   cparticlereach-ingsetting,whichsuggestsapotentialtrade-offbetweenexpressivenessandgeneralizability.theexperimentresultsareshowninfig.2.weobservethatasthearchitecturebecomesmorespecialized,weachievemuchbettergeneralizationperformance.forthissimpletask,itappearsthatconditioningontheentiredemonstrationhurtsgeneralizationperformance,andconditioningonjustthe   nalstateperformsthebestevenwithoutexplicitid173.thismakesintuitivesense,sincethe   nalstatealreadysuf   cientlycharacterizesthetaskathand.however,thesameconclusiondoesnotappeartoholdasthetaskbecomesmorecomplicated,asshownbytheblockstackingtasksinthemaintext.fig.3showsthelearningcurvesforthethreearchitecturesdesignedfortheparticlereachingtasks,asthenumberoflandmarksisvaried,byrunningthepoliciesover100differentcon   gurations,andcomputingsuccessratesoverbothtrainingandtestdata.wecanclearlyobservethatbothlstm-basedarchitecturesexhibitover   ttingasthenumberoflandmarksincreases.ontheotherhand,usingattentionclearlyimprovesgeneralizationperformance,andwhenconditioningononlythe   nalstate,itachievesperfectgeneralizationinallscenarios.itisalsointerestingtoobservethat12345678910numberoflandmarks0%20%40%60%80%100%successrateplainlstm(train)plainlstm(test)lstmwithattention(train)lstmwithattention(test)finalstatewithattention(train)finalstatewithattention(test)figure2:successratesofdifferentarchitecturesforparticlereaching.the   train   curvesshowthesuccessrateswhenconditionedondemonstrationsseenduringtraining,andrunningthepolicyoninitialconditionsseenduringtraining,whilethe   test   curvesshowthesuccessrateswhenconditionedonnewtrajectoriesandoperatinginnewsituations.bothattention-basedarchitecturesachieveperfecttrainingsuccessrates,andthecurvesareoverlapped.learningundergoesaphasetransition.intuitively,thismaybewhenthenetworkislearningtoinferthetaskfromthedemonstration.oncethisis   nished,thelearningofcontrolpolicyisalmosttrivial.table1andtable2showtheexactperformancenumbersforreference.#landmarksplainlstmlstmwithattentionfinalstatewithattention2100.0%100.0%100.0%3100.0%100.0%100.0%4100.0%100.0%100.0%5100.0%100.0%100.0%699.0%100.0%100.0%7100.0%100.0%100.0%8100.0%100.0%100.0%9100.0%100.0%100.0%1091.9%100.0%100.0%table1:successratesofparticlereachingconditionedonseendemonstrations,andrunningonseeninitialcon   gurations.bfurtherdetailsonblockstackingb.1fulldescriptionofarchitecturewenowspecifythearchitectureinpseudocode.weomitimplementationdetailswhichinvolvehan-dlingaminibatchofdemonstrationsandobservation-actionpairs,aswellasnecessarypaddingandmaskingtohandledataofdifferentdimensions.weuseweightid172withdata-dependentinitializationsalimansandkingma[2016]foralldenseandconvolutionoperations.202004006008001000epoch0%20%40%60%80%100%successratenumberofparticles2345678910(a)plainlstm(train)02004006008001000epoch0%20%40%60%80%100%successratenumberofparticles2345678910(b)plainlstm(test)02004006008001000epoch0%20%40%60%80%100%successratenumberofparticles2345678910(c)lstmwithattention(train)02004006008001000epoch0%20%40%60%80%100%successratenumberofparticles2345678910(d)lstmwithattention(test)02004006008001000epoch0%20%40%60%80%100%successratenumberofparticles2345678910(e)finalstatewithattention(train)02004006008001000epoch0%20%40%60%80%100%successratenumberofparticles2345678910(f)finalstatewithattention(test)figure3:learningcurvesforparticlereachingtasks.shownsuccessratesaremovingaveragesofpast10epochsforsmoothercurves.eachpolicyistrainedforupto1000epochs,whichtakesuptoanhourusingatitanxpascalgpu(ascanbeseenfromtheplot,mostexperimentscanbe   nishedsooner).b.1.1demonstrationnetworkassumethatthedemonstrationhasttimestepsandwehavebblocks.ourarchitectureonlymakeuseoftheobservationsintheinputdemonstrationbutnottheactions.eachobservationisa(3b+2)-dimensionalvector,containingthe(x,y,z)coordinatesofeachblockrelativetothecurrentpositionofthegripper,aswellasa2-dimensionalgripperstateindicatingwhetheritisopenorclosed.thefullsequenceofoperationsisgiveninmodule1.we   rstapplytemporaldropoutasdescribedinthemaintext.thenwesplittheobservationintoinformationabouttheblockandinformationabouttherobot,wherethe   rstdimensionistimeandtheseconddimensionistheblockid.therobotstateisbroadcastedacrossdifferentblocks.hencetheshapeofoutputsshouldbe  t   b   3and  t   b   2,respectively.then,weperforma1   1convolutionovertheblockstatestoprojectthemtothesamedimensionastheper-blockembedding.thenweperformasequenceofneighborhoodattentionoperationsand1   1convolutions,wheretheinputtotheconvolutionistheconcatenationoftheattentionresult,3module1demonstrationnetworkinput:demonstrationd2rt   (3b+2)hyperparameters:p=0.95,d=64output:demonstrationembedding2r  t   b   d,where  t=dt(1 p)eisthelengthofthedown-sampledtrajectory.d    temporaldropout(d,id203=p)blockstate,robotstate split(d   )h conv1d(blockstate,kernelsize=1,channels=d)fora2{1,2,4,8}do//residualconnectionsh    relu(h)attnresult neighborhoodattention(h   )h    concat({h   ,blockstate,robotstate},axis=-1)h    conv1d(h   ,kernelsize=2,channels=d,dilation=a)h    relu(h   )h h+h   endfordemoembedding hthecurrentblockposition,andtherobot   sstate.thisallowseachblocktoquerythestateofotherblocks,andreasonaboutthequeryresultincomparisonwithitsownstateandtherobot   sstate.weuseresidualconnectionsduringthisprocedure.b.1.2contextnetworkthepseudocodeisshowninmodule2.weperformaseriesofattentionoperationsoverthedemon-stration,followedbyattentionoverthecurrentstate,andweapplythemrepeatedlythroughanlstmwithdifferentweightspertimestep(wefoundthistobeslightlyeasiertooptimize).then,intheendweapplya   nalattentionoperationwhichproducesa   xed-dimensionalembeddingin-dependentofthelengthofthedemonstrationorthenumberofblocksintheenvironment.b.1.3manipulationnetworkgiventhecontextembedding,thismoduleissimplyamultilayerid88.pseudocodeisgiveninmodule3.b.2evaluatingpermutationinvarianceduringtrainingandinthepreviousevaluations,weonlyselectonetaskperequivalenceclass,wheretwotasksareconsideredequivalentiftheyarethesameuptopermutingdifferentblocks.thisisbasedontheassumptionthatourarchitectureisinvarianttopermutationsamongdifferentblocks.forexample,ifthepolicyisonlytrainedonthetaskabcd,itshouldperformwellontaskdcba,givenasingledemonstrationofthetaskdcba.wenowexperimentallyverifythispropertyby   xingatrainingtask,andevaluatingthepolicy   sperformanceunderallequivalentpermutationsofit.asfig.4shows,althoughthepolicyhasonlyseenthetaskabcd,itachievesthesamelevelofperformanceonallotherequivalenttasks.b.3effectofensemblingwenowevaluatetheimportanceofsamplingmultipledownsampleddemonstrationsduringevalu-ation.fig.5showstheperformanceacrossalltrainingandtesttasks,asthenumberofensemblesvariesfrom1to20.weobservethatmoreensembleshelpsthemostfortaskswithfewerstages.ontheotherhand,itconsistentlyimprovesperformanceforthehardertasks,althoughthegapissmaller.wesuspectthatthisisbecausethepolicyhaslearnedtoattendtoframesinthedemonstra-tiontrajectorywheretheblocksarealreadystackedtogether.intaskswithonly1stage,forexample,itisveryeasyfortheseframestobedroppedinasingledownsampleddemonstration.ontheotherhand,intaskswithmorestages,itbecomesmoreresilienttomissingframes.usingmorethan104module2contextnetworkinput:demonstrationembeddinghin2r  t   b   d,currentstates2r3b+2hyperparameters:d=64,tlstm=4,h=2output:contextembedding2r2+6h//splitthecurrentstateintoblockstate2rb   3androbotstatebroadcastedtoallblocks2rb   2blockstate,robotstate splitsingle(s)//initializelstmoutput2rb   dandstate(includinghiddenandcellstate)2rb   2doutput,state initlstmstate(size=b,hiddendim=d)fort=1totlstmdo//temporalattention:everyblockattendtothesametimestepx outputift>1thenx relu(x)endif//computingqueryforattentionoverdemonstration2rb   dq dense(x,outputdim=d)//computeresultfromattention2rh   b   dtemp softattention(query=q,context=hin,memory=hin,numheads=h)//reorganizeresultintoshapeb   (hd)temp reshape(transpose(temp,(1,0,2)),(b,h*d))//spatialattention:eachblockattendtoadifferentblockseparatelyx outputift>1thenx relu(x)endifx concat({x,temp},axis=-1)//computingcontextforattentionovercurrentstate2rb   dctx dense(x,outputdim=d)//computingqueryforattentionovercurrentstate2rb   dq dense(x,outputdim=d)//computingmemoryforattentionovercurrentstate2rb   (hd+3)mem concat({blockstate,temp},axis=-1)//computeresultfromattention2rb   h   (hd+3)spatial softattention(query=q,context=ctx,memory=mem,numheads=h)//reorganizeresultintoshapeb   h(hd+3)spatial reshape(spatial,(b,h*(h*d+3)))//forminputtothelstmcell2rb   (h(hd+3)+hd+8)input concat({robotstate,blockstate,spatial,temp},axis=-1)//runonestepofanlstmwithuntiedweights(meaningthatweusedifferentweightspertimestepoutput,state lstmonestep(input=input,state=state)endfor//finalattentionoverthecurrentstate,compressingano(b)representationdowntoo(1)//computethequeryvector.weusea   xed,trainablequeryvectorindependentoftheinputdata,withsize2r2   d(weusetwoqueries,originallyintendedtohaveoneforthesourceblockandoneforthetargetblock)q getfixedquery()//getattentionresult,whichshouldbeofshape2   h   3r softattention(query=q,context=output,memory=blockstate,numheads=h)//formthe   nalcontextembedding(wepickthe   rstrobotstatesincenoneedtobroadcasthere)contextembedding concat({robotstate[0],reshape(r,2*h*3)})5module3manipulationnetworkinput:contextembeddinghin2r2+6hhyperparameters:h=2output:predictedactiondistribution2r|a|h relu(dense(hin,outputdim=256))h relu(dense(h,outputdim=256))actiondist dense(h,outputdim=|a|)abcdabdcacbdacdbadbcadcbbacdbadcbcadbcdabdacbdcacabdcadbcbadcbdacdabcdbadabcdacbdbacdbcadcabdcbatask0%20%40%60%80%averagesuccessratefigure4:performanceofpolicyonasetoftasksequivalentuptopermutations.ensemblesappearstoprovidenosigni   cantimprovements,andhenceweused10ensemblesinourmainevaluation.12345678numberofstages0%20%40%60%80%100%averagesuccessratenumberofensembles1251020figure5:performanceofvariousnumberofensembles.6b.4breakdownoffailurecasestounderstandthelimitationsofthecurrentapproach,weperformabreakdownanalysisofthefailurecases.weconsiderthreefailurescenarios:   wrongmove   meansthatthepolicyhasarrangedalayoutincompatiblewiththedesiredlayout.thiscouldbebecausethepolicyhasmisinterpretedthedemonstration,orduetoanaccidentalbadmovethathappenstoscrambletheblocksintothewronglayout.   manipulationfailure   meansthatthepolicyhasmadeanirrecoverablefailure,forexampleiftheblockisshakenoffthetable,whichthecurrenthard-codedpolicydoesnotknowhowtohandle.   recoverablefailure   meansthatthepolicyrunsoutoftimebefore   nishingthetask,whichmaybeduetoanaccidentalfailureduringtheoperationthatwouldhavebeenrecoverablegivenmoretime.asshowninfig.6,conditioningononlythe   nalstatemakesmorewrongmovescomparedtootherarchitectures.apartfromthat,mostofthefailurecasesareactuallyduetomanipulationfailuresthataremostlyirrecoverable.1thissuggeststhatbettermanipulationskillsneedtobeacquiredtomakethelearnedone-shotpolicymorereliable.figure6:breakdownofthesuccessandfailurescenarios.theareathateachcoloroccupiesrepresenttheratioofthecorrespondingscenario.b.5learningcurvesfig.7showsthelearningcurvesfordifferentarchitecturesdesignedfortheblockstackingtasks.theselearningcurvesdonotre   ect   nalperformance:foreachevaluationpoint,wesampletasksanddemonstrationsfromtrainingdata,resettheenvironmenttothestartingpointofsomeparticularstage(sothatsomeblocksarealreadystacked),andonlyrunthepolicyforuptoonestage.ifthetrainingalgorithmisdagger,thesesampledtrajectoriesareannotatedandaddedtothetrainingset.hencethisevaluationdoesnotevaluategeneralization.wedidnotperformfullevaluationastrainingproceeds,becauseitisverytimeconsuming:eachevaluationrequirestensofthousandsof1notethattheactualratioofmisinterpreteddemonstrationsmaybedifferent,sincetherunsthathavecausedamanipulationfailurecouldlaterleadtoawrongmove,wereitsuccessfullyexecuted.ontheotherhand,byvisuallyinspectingthevideos,weobservedthatmostofthetrajectoriescategorizedas   wrongmove   areactuallyduetomanipulationfailures(exceptforpolicyconditioningonthe   nalstate,whichdoesseemtooccasionallyexecuteanactualwrongmove).7trajectoriesacrossover>100tasks.however,these   guresarestillusefultore   ectsomerelativetrend.fromthese   gures,wecanobservethatwhileconditioningonfulltrajectoriesgivesthebestper-formancewhichwasshowninthemaintext,itrequiresmuchlongertrainingtime,simplybecauseconditioningontheentiredemonstrationrequiresmorecomputation.inaddition,thismayalsobeduetothehighvarianceofthetrainingprocessduetodownsamplingdemonstrations,aswellasthefactthatthenetworkneedstolearntoproperlysegmentthedemonstration.itisalsointerestingthatconditioningonsnapshotsseemstolearnfasterthanconditioningonjustthe   nalstate,whichagainsuggeststhatconditioningonintermediateinformationishelpful,notonlyforthe   nalpolicy,butalsotofacilitatetraining.wealsoobservethatlearninghappensmostrapidlyfortheinitialstages,andmuchslowerforthelaterstages,sincemanipulationbecomesmorechallenginginthelaterstages.inaddition,therearefewertaskswithmorestages,andhencethelaterstagesarenotsampledasfrequentlyastheearlierstagesduringevaluation.b.6exactperformancenumbersexactperformancenumbersarepresentedforreference:   table3andtable4showthesuccessratesofdifferentarchitecturesontrainingandtesttasks,respectively;   table5showsthesuccessratesacrossalltasksasthenumberofensemblesisvaried;   table6showsthesuccessratesoftasksthatareequivalenttoabcduptopermutations;   table7,table8,table9,table10,andtable11showthebreakdownofdifferentsuccessandfailurescenariosforallconsideredarchitectures.b.7morevisualizationsfig.8andfig.9showthefullsetofheatmapsofattentionweights.interestingly,infig.8,weobservethatratherthanattendingtotwoblocksatatime,asweoriginallyexpected,thepolicyhaslearnedtomostlyattendtoonlyoneblockatatime.thismakessensebecauseduringeachofthegraspingandtheplacingphaseofasinglestackingoperation,thepolicyneedstoonlypayattentiontothesingleblockthatthegrippershouldaimtowards.forcontext,fig.10andfig.11showkeyframesoftheneuralnetworkpolicyexecutingthetask.referencestimsalimansanddiederikpkingma.weightid172:asimplereparameterizationtoaccel-eratetrainingofdeepneuralnetworks.inadvancesinneuralinformationprocessingsystems,pages901   901,2016.8#landmarksplainlstmlstmwithattentionfinalstatewithattention2100.0%100.0%100.0%3100.0%100.0%100.0%499.0%100.0%100.0%598.0%100.0%100.0%699.0%100.0%100.0%798.0%100.0%100.0%893.9%99.0%100.0%983.8%94.9%100.0%1050.5%85.9%100.0%table2:successratesofparticlereachingconditionedonunseendemonstrations,andrunningonunseeninitialcon   gurations.#stagesdemodaggerbcsnapshotfinalstate199.1%99.1%99.1%97.2%98.8%295.6%94.3%93.7%92.6%86.7%388.5%88.0%86.9%86.7%84.8%478.6%78.2%76.7%76.4%71.9%567.3%65.9%65.4%62.5%60.6%655.7%51.5%52.4%47.0%43.6%742.8%34.3%37.5%31.4%31.5%table3:successratesofdifferentarchitecturesontrainingtasksofblockstacking.#stagesdemodaggerbcsnapshotfinalstate295.8%94.9%95.9%92.8%94.1%477.6%77.0%74.8%77.2%75.8%565.9%65.9%64.3%61.1%51.9%649.4%50.6%46.5%42.6%35.9%746.5%36.5%38.5%32.8%32.0%829.0%18.0%24.0%19.0%20.0%table4:successratesofdifferentarchitecturesontesttasksofblockstacking.#stages1ens.2ens.5ens.10ens.20ens.191.9%95.4%98.8%99.1%98.7%292.3%92.2%94.5%94.6%94.1%386.0%86.8%87.9%88.0%87.9%476.6%77.4%77.9%78.0%78.3%565.1%65.0%65.3%65.9%65.5%649.0%50.4%50.1%51.3%50.8%734.4%36.1%36.0%34.9%36.8%820.0%21.0%21.0%18.0%20.0%table5:successratesofvaryingnumberofensemblesusingthedaggerpolicyconditionedonfulltrajectories,acrossbothtrainingandtesttasks.9taskidsuccessrateabcd83.0%abdc86.0%acbd92.0%acdb84.0%adbc91.0%adcb88.0%bacd92.0%badc90.0%bcad92.0%bcda88.0%bdac94.0%bdca88.0%cabd82.0%cadb87.0%cbad95.0%cbda87.0%cdab91.0%cdba93.0%dabc90.0%dacb92.0%dbac88.0%dbca90.0%dcab91.0%dcba84.0%table6:successratesofasetoftasksthatareequivalentuptopermutations,usingthedaggerpolicyconditionedonfulltrajectories.#stagessuccessrecoverablefailuremanipulationfailurewrongmove199.3%0.0%0.7%0.0%295.9%0.4%3.7%0.0%389.1%0.7%10.1%0.1%479.2%1.2%19.4%0.1%567.5%1.4%30.9%0.2%655.2%1.4%43.1%0.3%744.6%1.7%53.2%0.6%830.9%4.3%64.9%0.0%table7:breakdownofsuccessandfailurescenariosfordemopolicy.#stagessuccessrecoverablefailuremanipulationfailurewrongmove199.4%0.0%0.6%0.0%295.3%0.9%3.8%0.0%389.1%1.9%8.8%0.1%479.5%3.5%16.7%0.3%569.1%5.0%25.6%0.3%655.8%7.3%36.4%0.5%739.0%8.6%51.5%0.8%821.2%14.1%62.4%2.4%table8:breakdownofsuccessandfailurescenariosfordaggerpolicy.100100200300400500evaluationepoch0%20%40%60%80%100%singlestagesuccessratepolicytypedaggerbcsnapshotfinalstate(a)allstages0100200300400500evaluationepoch0%20%40%60%80%100%singlestagesuccessratepolicytypedaggerbcsnapshotfinalstate(b)stage00100200300400500evaluationepoch0%20%40%60%80%100%singlestagesuccessratepolicytypedaggerbcsnapshotfinalstate(c)stage10100200300400500evaluationepoch0%20%40%60%80%100%singlestagesuccessratepolicytypedaggerbcsnapshotfinalstate(d)stage20100200300400500evaluationepoch0%20%40%60%80%100%singlestagesuccessratepolicytypedaggerbcsnapshotfinalstate(e)stage30100200300400500evaluationepoch0%20%40%60%80%100%singlestagesuccessratepolicytypedaggerbcsnapshotfinalstate(f)stage40100200300400500evaluationepoch0%20%40%60%80%100%singlestagesuccessratepolicytypedaggerbcsnapshotfinalstate(g)stage50100200300400500evaluationepoch0%20%40%60%80%100%singlestagesuccessratepolicytypedaggerbcsnapshotfinalstate(h)stage6figure7:learningcurvesofblockstackingtask.the   rstplotshowstheaveragesuccessratesoverinitialcon   gurationsofallstages.thesubsequent   guresshowsthebreakdownofeachstage.forinstance,   stage3   meansthatthe   rst3stackingoperationsarealreadycompleted,andthepolicyisevaluatedonitsabilitytoperformthe4thstackingoperation.11#stagessuccessrecoverablefailuremanipulationfailurewrongmove199.6%0.0%0.4%0.0%295.6%1.1%3.2%0.1%388.1%2.2%9.5%0.2%478.5%4.5%16.8%0.2%567.2%6.6%25.7%0.4%653.9%8.3%37.1%0.6%740.6%9.8%48.7%0.9%827.0%13.5%58.4%1.1%table9:breakdownofsuccessandfailurescenariosforbcpolicy.#stagessuccessrecoverablefailuremanipulationfailurewrongmove199.1%0.0%0.9%0.0%294.5%1.6%3.8%0.1%388.0%2.5%9.3%0.2%478.9%4.6%16.2%0.3%565.6%8.0%25.8%0.6%650.8%8.3%40.2%0.7%736.1%9.2%54.2%0.4%821.6%11.4%65.9%1.1%table10:breakdownofsuccessandfailurescenariosforsnapshotpolicy.#stagessuccessrecoverablefailuremanipulationfailurewrongmove199.2%0.0%0.8%0.0%295.1%1.3%3.6%0.0%386.7%2.5%9.7%1.1%475.2%4.0%18.3%2.5%560.5%4.3%31.2%4.0%645.5%4.7%45.5%4.3%734.9%5.6%57.3%2.2%824.1%3.6%72.3%0.0%table11:breakdownofsuccessandfailurescenariosforfinalstatepolicy.(a)head0(b)head1(c)head2(d)head3figure8:heatmapofattentionweightsoverdifferentblocksofall4queryheads.12(a)head0(b)head1(c)head2(d)head3(e)head4(f)head5figure9:heatmapofattentionweightsoverdownsampleddemonstrationtrajectoryofall6queryheads.thereare2queryheadsperstepoflstm,and3stepsoflstmareperformed.13figure10:illustrationofthetaskusedforthevisualizationofattentionheatmaps(   rsthalf).thetaskisabcdefghij.theleftsideshowsthekeyframesinthedemonstration.therightsideshowshow,afterseeingtheentiredemonstration,tthepolicyreproducesthesamelayoutinanewinitializationofthesametask.14figure11:illustrationofthetaskusedforthevisualizationofattentionheatmaps(secondhalf).thetaskisabcdefghij.theleftsideshowsthekeyframesinthedemonstration.therightsideshowshow,afterseeingtheentiredemonstration,tthepolicyreproducesthesamelayoutinanewinitializationofthesametask.15