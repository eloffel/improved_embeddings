learning to parse and translate improves id4

akiko eriguchi   , yoshimasa tsuruoka   , and kyunghyun cho   
   the university of tokyo, 7-3-1 hongo, bunkyo-ku, tokyo, japan
{eriguchi, tsuruoka}@logos.t.u-tokyo.ac.jp

   new york university, new york, ny 10012, usa

kyunghyun.cho@nyu.edu

abstract

there has been relatively little attention
to incorporating linguistic prior to neu-
ral machine translation. much of the
previous work was further constrained to
considering linguistic prior on the source
side.
in this paper, we propose a hybrid
model, called id4+id56g, that learns
to parse and translate by combining the
recurrent neural network grammar into
the attention-based neural machine trans-
lation. our approach encourages the neu-
ral machine translation model to incorpo-
rate linguistic prior during training, and
lets it translate on its own afterward. ex-
tensive experiments with four language
pairs show the effectiveness of the pro-
posed id4+id56g.

1

introduction

id4 (id4) has enjoyed
impressive success without relying on much, if
any, prior linguistic knowledge. some of the most
recent studies have for instance demonstrated that
id4 systems work comparably to other systems
even when the source and target sentences are
given simply as    at sequences of characters (lee
et al., 2016; chung et al., 2016) or statistically, not
linguistically, motivated subword units (sennrich
et al., 2016; wu et al., 2016). shi et al. (2016)
recently made an observation that the encoder of
id4 captures syntactic properties of a source sen-
tence automatically, indirectly suggesting that ex-
plicit linguistic prior may not be necessary.

on the other hand,

there have only been a
couple of recent studies showing the potential
bene   t of explicitly encoding the linguistic prior
into id4. sennrich and haddow (2016) for in-
stance proposed to augment each source word with
its corresponding part-of-speech tag, lemmatized

form and dependency label. eriguchi et al. (2016)
instead replaced the sequential encoder with a
tree-based encoder which computes the represen-
tation of the source sentence following its parse
tree. stahlberg et al. (2016) let the lattice from a
hierarchical phrase-based system guide the decod-
ing process of id4, which
results in two separate models rather than a single
end-to-end one. despite the promising improve-
ments, these explicit approaches are limited in that
the trained translation model strictly requires the
availability of external tools during id136 time.
more recently, researchers have proposed meth-
ods to incorporate target-side syntax into id4
models. alvarez-melis and jaakkola (2017) have
proposed a doubly-recurrent neural network that
can generate a tree-structured sentence, but its ef-
fectiveness in a full scale id4 task is yet to be
shown. aharoni and goldberg (2017) introduced
a method to serialize a parsed tree and to train the
serialized parsed sentences.

we propose to implicitly incorporate linguis-
tic prior based on the idea of multi-task learn-
ing (caruana, 1998; collobert et al., 2011). more
speci   cally, we design a hybrid decoder for id4,
called id4+id56g1, that combines a usual con-
ditional
language model and a recently pro-
posed recurrent neural network grammars (rn-
ngs, dyer et al., 2016). this is done by plugging
in the conventional language model decoder in the
place of the buffer in id56g, while sharing a sub-
set of parameters, such as word vectors, between
the language model and id56g. we train this hy-
brid model to maximize both the log-id203 of
a target sentence and the log-id203 of a parse
action sequence. we use an external parser (an-
dor et al., 2016) to generate target parse actions,
but unlike the previous explicit approaches, we do
not need it during test time.

1our code is available at https://github.com/

tempra28/id4id56g.

7
1
0
2

 
r
p
a
3
2

 

 
 
]
l
c
.
s
c
[
 
 

2
v
5
2
5
3
0

.

2
0
7
1
:
v
i
x
r
a

we evaluate the proposed id4+id56g on four
language pairs ({jp, cs, de, ru}-en). we observe
signi   cant improvements in terms of id7 scores
on three out of four language pairs and ribes
scores on all the language pairs.

2 id4

state sj against each of the hidden states and as-
signs a scalar score:   i,j = exp(h(cid:62)
i wdsj) (lu-
ong et al., 2015). these scores are then normal-
  i,j =   i,j(cid:80)
ized across the hidden states to sum to 1, that is
these attention weights: cj =(cid:80)
. the time-dependent context vector
is then a weighted-sum of the hidden states with

i   i,jhi.

i   i,j

id4 is a recently proposed
framework for building a machine translation sys-
tem based purely on neural networks.
it is of-
ten built as an attention-based encoder-decoder
network (cho et al., 2015) with two recurrent
networks   encoder and decoder   and an atten-
tion model. the encoder, which is often imple-
mented as a bidirectional recurrent network with
long short-term memory units (lstm, hochre-
iter and schmidhuber, 1997) or gated recurrent
units (gru, cho et al., 2014),    rst reads a source
sentence represented as a sequence of words x =
(x1, x2, . . . , xn ). the encoder returns a sequence
of hidden states h = (h1, h2, . . . , hn ). each hid-
den state hi is a concatenation of those from the
forward and backward recurrent network: hi =

(cid:104)      

h i;

(cid:105)

      
h i

, where
      
h i =
      
h i =

      
f enc(
      
f enc(

      
h i   1, vx(xi)),
      
h i+1, vx(xi)).

(cid:88)

vx(xi) refers to the word vector of the i-th source
word.

the decoder is implemented as a conditional re-
current language model which models the target
sentence, or translation, as

log p(y|x) =

log p(yj|y<j, x),

j

where y = (y1, . . . , ym ). each of the conditional
probabilities in the r.h.s is computed by
p(yj = y|y<j, x) = sof tmax(w (cid:62)
  sj = tanh(wc[sj; cj]),
sj = fdec(sj   1, [vy(yj   1);   sj   1]),

(1)
(2)
(3)

y   sj),

3 recurrent neural network grammars
a recurrent neural network grammar (id56g,
dyer et al., 2016) is a probabilistic syntax-based
language model. unlike a usual recurrent lan-
guage model (see, e.g., mikolov et al., 2010), an
id56g simultaneously models both tokens and
their tree-based composition. this is done by
having a (output) buffer, stack and action his-
tory, each of which is implemented as a stack
lstm (slstm, dyer et al., 2015). at each time
step, the action slstm predicts the next action
based on the (current) hidden states of the buffer,
stack and action slstm. that is,
p(at = a|a<t)     ew (cid:62)

a faction(hbuffer

,haction

,hstack

(4)

),

t

t

t

where wa is the vector of the action a. if the se-
lected action is shift, the word at the beginning of
the buffer is moved to the stack. when the re-
duce action is selected, the top-two words in the
stack are reduced to build a partial tree. addi-
tionally, the action may be one of many possible
non-terminal symbols, in which case the predicted
non-terminal symbol is pushed to the stack.

the hidden states of the buffer, stack and action

slstm are correspondingly updated by

hbuffer
= stacklstm(hbuffer
top
t
t = stacklstm(hstack
hstack
top , rt),
haction
= stacklstm(haction
t

top

, va(at   1)),

, vy(yt   1)),

(5)

where vy and va are functions returning the target
word and action vectors. the input vector rt of the
stack slstm is computed recursively by

rt = tanh(wr[rd; rp; va(at)]),

where fdec is a recurrent activation function, such
as lstm or gru, and wy is the output word vec-
tor of the word y.

cj is a time-dependent context vector that is
computed by the attention model using the se-
quence h of hidden states from the encoder. the
attention model    rst compares the current hidden

where rd and rp are the corresponding vectors
of the parent and dependent phrases,
respec-
tively (dyer et al., 2015). this process is iter-
ated until a complete parse tree is built. note that
the original paper of id56g (dyer et al., 2016)
uses constituency trees, but we employ depen-
dency trees in this paper. both types of trees are

represented as a sequence of the three types of ac-
tions in a transition-based parsing model.

when the complete sentence is provided, the
buffer simply summarizes the shifted words.
when the id56g is used as a generator, the buffer
further generates the next word when the selected
action is shift. the latter can be done by replacing
the buffer with a recurrent language model, which
is the idea on which our proposal is based.

4 learning to parse and translate

4.1 id4+id56g
our main proposal in this paper is to hybridize the
decoder of the id4 and the
id56g. we continue from the earlier observation
that we can replace the buffer of id56g to a recur-
rent language model that simultaneously summa-
rizes the shifted words as well as generates future
words. we replace the id56g   s buffer with the
neural translation model   s decoder in two steps.

top

construction first, we replace the hidden state
of the buffer hbuffer (in eq. (5)) with the hidden
state of the decoder of the attention-based neural
machine translation from eq. (3). as is clear from
those two equations, both the buffer slstm and
the translation decoder take as input the previous
hidden state (hbuffer
and sj   1, respectively) and
the previously decoded word (or the previously
shifted word in the case of the id56g   s buffer),
and returns its summary state. the only difference
is that the translation decoder additionally consid-
ers the state   sj   1. once the buffer of the id56g
is replaced with the id4 decoder in our proposed
model, the id4 decoder is also under control of
the actions provided by the id56g.2 second, we
let the next word prediction of the translation de-
coder as a generator of id56g. in other words,
the generator of id56g will output a word, when
asked by the shift action, according to the condi-
tional distribution de   ned by the translation de-
coder in eq. (1). once the buffer slstm is re-
placed with the neural translation decoder, the ac-
tion slstm naturally takes as input the translation
decoder   s hidden state when computing the action
conditional distribution in eq. (4). we call this hy-
brid model id4+id56g.

2the j-th hidden state in eq. (3) is calculated only when
the action (shift) is predicted by the id56g. this is why our
proposed model can handle the sequences of words and ac-
tions which have different lengths.

learning and id136 after this integration,
our hybrid id4+id56g models the conditional
distribution over all possible pairs of transla-
tion and its parse given a source sentence, i.e.,
p(y, a|x). assuming the availability of parse
annotation in the target-side of a parallel cor-
pus, we train the whole model jointly to maxi-
mize e(x,y,a)   data [log p(y, a|x)]. in doing so, we
notice that there are two separate paths through
which the neural translation decoder receives er-
ror signal. first, the decoder is updated in or-
der to maximize the id155 of the
correct next word, which has already existed in
the original id4. second,
the decoder is updated also to maximize the con-
ditional id203 of the correct parsing action,
which is a novel learning signal introduced by the
proposed hybridization. furthermore, the second
learning signal affects the encoder as well, encour-
aging the whole neural translation model to be
aware of the syntactic structure of the target lan-
guage. later in the experiments, we show that this
additional learning signal is useful for translation,
even though we discard the id56g (the stack and
action slstms) in the id136 time.

4.2 knowledge distillation for parsing

a major challenge in training the proposed hybrid
model is that there is not a parallel corpus aug-
mented with gold-standard target-side parse, and
vice versa. in other words, we must either parse
the target-side sentences of an existing parallel
corpus or translate sentences with existing gold-
standard parses. as the target task of the proposed
model is translation, we start with a parallel cor-
pus and annotate the target-side sentences.
it is
however costly to manually annotate any corpus
of reasonable size (table 6 in alonso et al., 2016).
we instead resort to noisy, but automated an-
notation using an existing parser. this approach
of automated annotation can be considered along
the line of recently proposed techniques of knowl-
edge distillation (hinton et al., 2015) and distant
supervision (mintz et al., 2009). in knowledge dis-
tillation, a teacher network is trained purely on a
training set with ground-truth annotations, and the
annotations predicted by this teacher are used to
train a student network, which is similar to our ap-
proach where the external parser could be thought
of as a teacher and the proposed hybrid network   s
id56g as a student. on the other hand, what we

train.
134,453
166,313
131,492
100,000

dev.
2,656
2,169
2,818
1,790

test
2,999
2,999
2,998
1,812

voc. (src, tgt, act)
(33,867, 27,347, 82)
(33,820, 30,684, 80)
(32,442, 27,979, 82)
(23,509, 28,591, 80)

cs-en
de-en
ru-en
jp-en

table 1: statistics of parallel corpora.

propose here is a special case of distant supervi-
sion in that the external parser provides noisy an-
notations to otherwise an unlabeled training set.

speci   cally, we use syntaxnet, released by an-
dor et al. (2016), on a target sentence.3 we convert
a parse tree into a sequence of one of three tran-
sition actions (shift, reduce-l, reduce-r).
we label each reduce action with a correspond-
ing dependency label and treat it as a more    ne-
grained action.

5 experiments
5.1 language pairs and corpora
we compare the proposed id4+id56g against
the baseline model on four different language
pairs   jp-en, cs-en, de-en and ru-en. the ba-
sic statistics of the training data are presented in
table 1. we mapped all the low-frequency words
to the unique symbol    unk    and inserted a spe-
cial symbol    eos    at the end of both source and
target sentences.

ja we use the aspec corpus (   train1.txt   ) from
the wat   16 jp-en translation task. we tokenize
each japanese sentence with kytea (neubig et al.,
2011) and preprocess according to the recommen-
dations from wat   16 (wat, 2016). we use the
   rst 100k sentence pairs of length shorter than 50
for training. the vocabulary is constructed with
all the unique tokens that appear at least twice in
the training corpus. we use    dev.txt    and    test.txt   
provided by wat   16 respectively as development
and test sets.

cs, de and ru we use news commentary v8.
we removed noisy metacharacters and used the to-
kenizer from moses (koehn et al., 2007) to build a
vocabulary of each language using unique tokens
that appear at least 6, 6 and 5 times respectively for
cs, ru and de. the target-side (english) vocab-
ulary was constructed with all the unique tokens

3when the target sentence is parsed as id174,
we use all the vocabularies in a corpus and do not cut off
any words. we use the plain syntaxnet and do not train it
furthermore.

appearing more than three times in each corpus.
we also excluded the sentence pairs which include
empty lines in either a source sentence or a target
sentence. we only use sentence pairs of length 50
or less for training. we use    newstest2015    and
   newstest2016    as development and test sets re-
spectively.

5.2 models, learning and id136
in all our experiments, each recurrent network has
a single layer of lstm units of 256 dimensions,
and the word vectors and the action vectors are
of 256 and 128 dimensions, respectively. to re-
duce computational overhead, we use blackout (ji
et al., 2015) with 2000 negative samples and    =
0.4. when employing blackout, we shared the
negative samples of each target word in a sen-
tence in training time (hashimoto and tsuruoka,
2017), which is similar to the previous work (zoph
et al., 2016). for the proposed id4+id56g, we
share the target word vectors between the decoder
(buffer) and the stack slstm.
each weight is initialized from the uniform dis-
tribution [   0.1, 0.1]. the bias vectors and the
weights of the softmax and blackout are initial-
ized to be zero. the forget gate biases of lstms
and stack-lstms are initialized to 1 as recom-
mended in j  ozefowicz et al. (2015). we use
stochastic id119 with minibatches of
128 examples. the learning rate starts from 1.0,
and is halved each time the perplexity on the de-
velopment set increases. we clip the norm of the
gradient (pascanu et al., 2012) with the thresh-
old set to 3.0 (2.0 for the baseline models on ru-
en and cs-en to avoid nan and inf). when the
perplexity of development data increased in train-
ing time, we halved the learning rate of stochastic
id119 and reloaded the previous model.
the id56g   s stack computes the vector of a de-
pendency parse tree which consists of the gener-
ated target words by the buffer. since the complete
parse tree has a    root    node, the special token of
the end of a sentence (   eos   ) is considered as the
root. we use id125 in the id136 time,
with the beam width selected based on the devel-
opment set performance.

it took about 15 minutes per epoch and about 20
minutes respectively for the baseline and the pro-
posed model to train a full jp-en parallel corpus
in our implementation.4

4we run all the experiments on multi-core cpus (10

de-en ru-en cs-en

jp-en

id7

id4
id4+id56g

16.61
16.41

12.03
12.46   

11.22
12.06   

17.88
18.84   

ribes

id4
id4+id56g

73.75
75.03   

69.56
71.04   

69.59
70.39   

71.27
72.25   

table 2: id7 and ribes scores by the baseline
and proposed models on the test set. we use the
bootstrap resampling method from koehn (2004)
to compute the statistical signi   cance. we use     to
mark those signi   cant cases with p < 0.005.

jp-en (dev)
id7
id4+id56g 18.60
18.02
w/o buffer
17.94
w/o action
w/o stack
17.58
id4
17.75

table 3: effect of each component in id56g.

5.3 results and analysis
in table 2, we report the translation qualities of
the tested models on all the four language pairs.
we report both id7 (papineni et al., 2002) and
ribes (isozaki et al., 2010). except for de-
en, measured in id7, we observe the statis-
tically signi   cant improvement by the proposed
id4+id56g over the baseline model. it is worth-
while to note that these signi   cant improvements
have been achieved without any additional param-
eters nor computational overhead in the id136
time.
ablation since each component in id56g may
be omitted, we ablate each component in the pro-
posed id4+id56g to verify their necessity.5 as
shown in table 3, we see that the best performance
could only be achieved when all the three compo-
nents were present. removing the stack had the
most adverse effect, which was found to be the
case for parsing as well by kuncoro et al. (2017).
generated sentences with parsed actions
the decoder part of our proposed model consists
of two components: the id4 decoder to gener-

threads on intel(r) xeon(r) cpu e5-2680 v2 @2.80ghz)

5 since the buffer is the decoder, it is not possible to com-
pletely remove it. instead we simply remove the dependency
of the action distribution on it.

figure 1: an example of translation and its depen-
dency relations obtained by our proposed model.

ate a translated sentence and the id56g decoder
to predict its parsing actions. the proposed model
can therefore output a dependency structure along
with a translated sentence. figure 1 shows an
example of jp-en translation in the development
dataset and its dependency parse tree obtained by
the proposed model. the special symbol (   eos   )
is treated as the root node (   root   ) of the parsed
tree. the translated sentence was generated by
using id125, which is the same setting of
id4+id56g shown in table 3. the parsing ac-
tions were obtained by greedy search. the re-
sulting dependency structure is mostly correct but
contains a few errors; for example, dependency re-
lation between    the    and     transition    should not
be    pobj   .

6 conclusion

we propose a hybrid model, to which we refer
as id4+id56g, that combines the decoder of an
attention-based neural translation model with the
id56g. this model learns to parse and translate si-
multaneously, and training it encourages both the
encoder and decoder to better incorporate linguis-
tic priors. our experiments con   rmed its effec-
tiveness on four language pairs ({jp, cs, de, ru}-
en). the id56g can in principle be trained with-
out ground-truth parses, and this would eliminate
the need of external parsers completely. we leave
the investigation into this possibility for future re-
search.

acknowledgments

we thank yuchen qiao and kenjiro taura for their
help to speed up the implementations of training
and also kazuma hashimoto for his valuable com-
ments and discussions. this work was supported
by jst crest grant number jpmjcr1513 and
jsps kakenhi grant number 15j12597 and

16h01715. kc thanks support by ebay, face-
book, google and nvidia.

references
roee aharoni and yoav goldberg. 2017. towards
in pro-
string-to-tree id4.
ceedings of the 55th annual meeting of the asso-
ciation for computational linguistics. to appear.

h  ector mart    nez alonso, djam  e seddah, and beno    t
sagot. 2016. from noisy questions to minecraft
texts: annotation challenges in extreme syntax sce-
nario. in proceedings of the 2nd workshop on noisy
user-generated text (wnut). pages 13   23.

david alvarez-melis and tommi s. jaakkola. 2017.
tree-structured decoding with doubly-recurrent
in proceedings of international
neural networks.
conference on learning representations 2017.

daniel andor, chris alberti, david weiss, aliaksei
severyn, alessandro presta, kuzman ganchev, slav
petrov, and michael collins. 2016. globally nor-
in pro-
malized transition-based neural networks.
ceedings of the 54th annual meeting of the asso-
ciation for computational linguistics. pages 2442   
2452.

rich caruana. 1998. multitask learning. in learning

to learn, springer, pages 95   133.

kyunghyun cho, aaron courville, and yoshua ben-
gio. 2015. describing multimedia content using
ieee
attention-based encoder-decoder networks.
transactions on multimedia 17(11):1875   1886.

kyunghyun cho, bart van merrienboer, caglar gul-
cehre, dzmitry bahdanau, fethi bougares, holger
schwenk, and yoshua bengio. 2014.
learning
phrase representations using id56 encoder   decoder
for id151. in proceedings of
the 2014 conference on empirical methods in nat-
ural language processing. pages 1724   1734.

junyoung chung, kyunghyun cho, and yoshua ben-
gio. 2016. a character-level decoder without ex-
plicit segmentation for id4.
in proceedings of the 54th annual meeting of the
association for computational linguistics. associ-
ation for computational linguistics, pages 1693   
1703.

ronan collobert, jason weston, l  eon bottou, michael
karlen, koray kavukcuoglu, and pavel kuksa.
2011. natural language processing (almost) from
journal of machine learning research
scratch.
12:2493   2537.

chris dyer, miguel ballesteros, wang ling, austin
matthews, and a. noah smith. 2015. transition-
based id33 with stack long short-
in proceedings of the 53rd annual
term memory.

meeting of the association for computational lin-
guistics and the 7th international joint conference
on natural language processing. pages 334   343.

chris dyer, adhiguna kuncoro, miguel ballesteros,
and a. noah smith. 2016. recurrent neural net-
work grammars. in proceedings of the 2016 con-
ference of the north american chapter of the asso-
ciation for computational linguistics: human lan-
guage technologies. pages 199   209.

akiko eriguchi, kazuma hashimoto, and yoshimasa
tsuruoka. 2016. tree-to-sequence attentional neu-
ral machine translation. in proceedings of the 54th
annual meeting of the association for computa-
tional linguistics. pages 823   833.

kazuma hashimoto

and yoshimasa tsuruoka.
2017. id4 with source-
arxiv preprint
side latent graph parsing.
arxiv:1702.02265 .

geoffrey hinton, oriol vinyals, and jeff dean. 2015.
distilling the knowledge in a neural network. arxiv
preprint arxiv:1503.02531 .

sepp hochreiter and j  urgen schmidhuber. 1997. long
short-term memory. neural comput. 9(8):1735   
1780.

hideki isozaki, tsutomu hirao, kevin duh, katsuhito
sudoh, and hajime tsukada. 2010. automatic eval-
uation of translation quality for distant language
in proceedings of the 2010 conference on
pairs.
empirical methods in natural language process-
ing. pages 944   952.

shihao ji, s. v. n. vishwanathan, nadathur satish,
michael j. anderson, and pradeep dubey. 2015.
blackout: speeding up recurrent neural network lan-
guage models with very large vocabularies. pro-
ceedings of international conference on learning
representations 2015 .

rafal

j  ozefowicz, wojciech zaremba,

and ilya
sutskever. 2015. an empirical exploration of recur-
in proceedings of the
rent network architectures.
32nd international conference on machine learn-
ing. pages 2342   2350.

philipp koehn. 2004. statistical signi   cance tests for
in proceedings of
machine translation evaluation.
the 2004 conference on empirical methods in nat-
ural language processing. pages 388   395.

philipp koehn, hieu hoang, alexandra birch, chris
callison-burch, marcello federico, nicola bertoldi,
brooke cowan, wade shen, christine moran,
richard zens, chris dyer, ondrej bojar, alexandra
constantin, and evan herbst. 2007. moses: open
source toolkit for id151. in
proceedings of the 45th annual meeting of the as-
sociation for computational linguistics companion
volume proceedings of the demo and poster ses-
sions. pages 177   180.

xing shi, inkit padhi, and kevin knight. 2016. does
string-based neural mt learn source syntax? in pro-
ceedings of the 2016 conference on empirical meth-
ods in natural language processing. pages 1526   
1534.

felix stahlberg, eva hasler, aurelien waite, and bill
byrne. 2016. syntactically guided neural machine
translation. in proceedings of the 54th annual meet-
ing of the association for computational linguis-
tics. pages 299   305.

wat.

2016.

http://lotus.kuee.

kyoto-u.ac.jp/wat/baseline/
datapreparationje.html.

yonghui wu, mike schuster, zhifeng chen, quoc v.
le, mohammad norouzi, wolfgang macherey,
maxim krikun, yuan cao, qin gao, klaus
macherey, jeff klingner, apurva shah, melvin
johnson, xiaobing liu, lukasz kaiser, stephan
gouws, yoshikiyo kato, taku kudo, hideto
kazawa, keith stevens, george kurian, nishant
patil, wei wang, cliff young, jason smith, jason
riesa, alex rudnick, oriol vinyals, greg corrado,
macduff hughes, and jeffrey dean. 2016. google   s
id4 system: bridging the
gap between human and machine translation. arxiv
preprint arxiv:1609.08144 .

barret zoph, ashish vaswani, jonathan may, and
kevin knight. 2016. simple, fast noise-contrastive
in pro-
estimation for large id56 vocabularies.
ceedings of the 2016 conference of the north amer-
ican chapter of the association for computational
linguistics: human language technologies. pages
1217   1222.

adhiguna kuncoro, miguel ballesteros, lingpeng
kong, chris dyer, graham neubig, and noah a.
smith. 2017. what do recurrent neural network
in proceedings of
grammars learn about syntax?
the 15th conference of the european chapter of the
association for computational linguistics: volume
1, long papers. pages 1249   1258.

jason lee, kyunghyun cho, and thomas hofmann.
2016. fully character-level neural machine trans-
lation without explicit segmentation. arxiv preprint
arxiv:1610.03017 .

thang luong, hieu pham, and christopher d. man-
ning. 2015. effective approaches to attention-based
in proceedings of the
id4.
2015 conference on empirical methods in natural
language processing. pages 1412   1421.

tom  a  s mikolov, martin kara     at, luk  a  s burget, jan
  cernock  y, and sanjeev khudanpur. 2010. recurrent
neural network based language model. in proceed-
ings of the 11th annual conference of the interna-
tional speech communication association (inter-
speech 2010). international speech communica-
tion association, pages 1045   1048.

mike mintz, steven bills, rion snow, and dan juraf-
sky. 2009. distant supervision for relation extrac-
in proceedings of the
tion without labeled data.
joint conference of the 47th annual meeting of the
acl and the 4th international joint conference on
natural language processing of the afnlp. pages
1003   1011.

graham neubig, yosuke nakata, and shinsuke mori.
2011. pointwise prediction for robust, adaptable
japanese morphological analysis. in proceedings of
the 49th annual meeting of the association for com-
putational linguistics: human language technolo-
gies. pages 529   533.

kishore papineni, salim roukos, todd ward, and wei-
jing zhu. 2002. id7: a method for automatic eval-
uation of machine translation. in proceedings of the
40th annual meeting on association for computa-
tional linguistics. pages 311   318.

razvan pascanu, tomas mikolov, and yoshua ben-
understanding the exploding gra-
arxiv preprint arxiv:1211.5063

gio. 2012.
dient problem.
abs/1211.5063.

rico sennrich and barry haddow. 2016. linguistic
input features improve id4.
in proceedings of the first conference on machine
translation. pages 83   91.

rico sennrich, barry haddow, and alexandra birch.
2016. id4 of rare words with
subword units. in proceedings of the 54th annual
meeting of the association for computational lin-
guistics. pages 1715   1725.

