8
1
0
2

 

n
u
j
 

7

 
 
]

g
l
.
s
c
[
 
 

5
v
4
4
8
9
0

.

3
0
7
1
:
v
i
x
r
a

published as a conference paper at iclr 2018

multi-scale dense networks
for resource efficient image classification

gao huang
cornell university

danlu chen
fudan university

tianhong li
tsinghua university

felix wu
cornell university

laurens van der maaten
facebook ai research

kilian weinberger
cornell university

abstract

in this paper we investigate image classi   cation with computational resource lim-
its at test time. two such settings are: 1. anytime classi   cation, where the net-
work   s prediction for a test example is progressively updated, facilitating the out-
put of a prediction at any time; and 2. budgeted batch classi   cation, where a    xed
amount of computation is available to classify a set of examples that can be spent
unevenly across    easier    and    harder    inputs. in contrast to most prior work, such
as the popular viola and jones algorithm, our approach is based on convolutional
neural networks. we train multiple classi   ers with varying resource demands,
which we adaptively apply during test time. to maximally re-use computation
between the classi   ers, we incorporate them as early-exits into a single deep con-
volutional neural network and inter-connect them with dense connectivity. to fa-
cilitate high quality classi   cation early on, we use a two-dimensional multi-scale
network architecture that maintains coarse and    ne level features all-throughout
the network. experiments on three image-classi   cation tasks demonstrate that our
framework substantially improves the existing state-of-the-art in both settings.

1

introduction

recent years have witnessed a surge in demand for applications of visual object recognition, for
instance, in self-driving cars (bojarski et al., 2016) and content-based image search (wan et al.,
2014). this demand has in part been fueled through the promise generated by the astonishing
progress of convolutional networks (id98s) on visual object recognition benchmark competition
datasets, such as ilsvrc (deng et al., 2009) and coco (lin et al., 2014), where state-of-the-art
models may have even surpassed human-level performance (he et al., 2015; 2016).
however, the requirements of such competitions differ from real-
world applications, which tend to incentivize resource-hungry mod-
els with high computational demands at id136 time. for exam-
ple, the coco 2016 competition was won by a large ensemble of
computationally intensive id98s1     a model likely far too compu-
tationally expensive for any resource-aware application. although
much smaller models would also obtain decent error, very large,
computationally intensive models seem necessary to correctly clas-
sify the hard examples that make up the bulk of the remaining mis-
classi   cations of modern algorithms. to illustrate this point, fig-
ure 1 shows two images of horses. the left image depicts a horse
in canonical pose and is easy to classify, whereas the right image is
taken from a rare viewpoint and is likely in the tail of the data dis-
tribution. computationally intensive models are needed to classify
such tail examples correctly, but are wasteful when applied to canonical images such as the left one.
in real-world applications, computation directly translates into power consumption, which should
be minimized for environmental and economical reasons, and is a scarce commodity on mobile

figure 1: two images containing
a horse. the left image is canon-
ical and easy to detect even with
a small model, whereas the right
image requires a computationally
more expensive network archi-
tecture. (copyright pixel addict
and doyle (cc by-nd 2.0).)

1http://image-net.org/challenges/talks/2016/grmi-coco-slidedeck.pdf

1

published as a conference paper at iclr 2018

devices. this begs the question: why do we choose between either wasting computational resources
by applying an unnecessarily computationally expensive model to easy images, or making mistakes
by using an ef   cient model that fails to recognize dif   cult images? ideally, our systems should
automatically use small networks when test images are easy or computational resources limited, and
use big networks when test images are hard or computation is abundant.
such systems would be bene   cial in at least two settings with computational constraints at test-
time: anytime prediction, where the network can be forced to output a prediction at any given point
in time; and budgeted batch classi   cation, where a    xed computational budget is shared across a
large set of examples which can be spent unevenly across    easy    and    hard    examples. a prac-
tical use-case of anytime prediction is in mobile apps on android devices: in 2015, there existed
24, 093 distinct android devices2, each with its own distinct computational limitations. it is infea-
sible to train a different network that processes video frame-by-frame at a    xed framerate for each
of these devices. instead, you would like to train a single network that maximizes accuracy on all
these devices, within the computational constraints of that device. the budget batch classi   cation
setting is ubiquitous in large-scale machine learning applications. search engines, social media
companies, on-line advertising agencies, all must process large volumes of data on limited hardware
resources. for example, as of 2010, google image search had over 10 billion images indexed3,
which has likely grown to over 1 trillion since. even if a new model to process these images is
only 1/10s slower per image, this additional cost would add 3170 years of cpu time. in the budget
batch classi   cation setting, companies can improve the average accuracy by reducing the amount of
computation spent on    easy    cases to save up computation for    hard    cases.
motivated by prior work in id161 on resource-ef   cient recognition (viola & jones, 2001),
we aim to develop id98s that    slice    the computation and process these slices one-by-one, stopping
the evaluation once the cpu time is depleted or the classi   cation suf   ciently certain (through    early
exits   ). unfortunately, the architecture of id98s is inherently at odds with the introduction of early
exits. id98s learn the data representation and the classi   er jointly, which leads to two problems
with early exits: 1. the features in the last layer are extracted directly to be used by the classi   er,
whereas earlier features are not. the inherent dilemma is that different kinds of features need to be
extracted depending on how many layers are left until the classi   cation. 2. the features in different
layers of the network may have different scale. typically, the    rst layers of a deep nets operate on a
   ne scale (to extract low-level features), whereas later layers transition (through pooling or strided
convolution) to coarse scales that allow global context to enter the classi   er. both scales are needed
but happen at different places in the network.
we propose a novel network architecture that addresses both of these problems through careful
design changes, allowing for resource-ef   cient image classi   cation. our network uses a cascade of
intermediate classi   ers throughout the network. the    rst problem, of classi   ers altering the internal
representation, is addressed through the introduction of dense connectivity (huang et al., 2017). by
connecting all layers to all classi   ers, features are no longer dominated by the most imminent early-
exit and the trade-off between early or later classi   cation can be performed elegantly as part of the
id168. the second problem, the lack of coarse-scale features in early layers, is addressed by
adopting a multi-scale network structure. at each layer we produce features of all scales (   ne-to-
coarse), which facilitates good classi   cation early on but also extracts low-level features that only
become useful after several more layers of processing. our network architecture is illustrated in
figure 2, and we refer to it as multi-scale densenet (msdnet).
we evaluate msdnets on three image-classi   cation datasets. in the anytime classi   cation setting,
we show that it is possible to provide the ability to output a prediction at any time while maintain
high accuracies throughout. in the budget batch classi   cation setting we show that msdnets can be
effectively used to adapt the amount of computation to the dif   culty of the example to be classi   ed,
which allows us to reduce the computational requirements of our models drastically whilst perform-
ing on par with state-of-the-art id98s in terms of overall classi   cation accuracy. to our knowledge
this is the    rst deep learning architecture of its kind that allows dynamic resource adaptation with a
single model and obtains competitive results throughout.

2source: https://opensignal.com/reports/2015/08/android-fragmentation/
3https://en.wikipedia.org/wiki/google_images

2

published as a conference paper at iclr 2018

figure 2: illustration of the    rst four layers of an msdnet with three scales. the horizontal direction cor-
responds to the layer direction (depth) of the network. the vertical direction corresponds to the scale of the
feature maps. horizontal arrows indicate a regular convolution operation, whereas diagonal and vertical arrows
indicate a strided convolution operation. classi   ers only operate on feature maps at the coarsest scale. connec-
tions across more than one layer are not drawn explicitly: they are implicit through recursive concatenations.

2 related work

we brie   y review related prior work on computation-ef   cient networks, memory-ef   cient networks,
and resource-sensitive machine learning, from which our network architecture draws inspiration.
computation-ef   cient networks. most prior work on (convolutional) networks that are computa-
tionally ef   cient at test time focuses on reducing model size after training. in particular, many stud-
ies propose to prune weights (lecun et al., 1989; hassibi et al., 1993; li et al., 2017) or quantize
weights (hubara et al., 2016; rastegari et al., 2016) during or after training. these approaches are
generally effective because deep networks often have a substantial number of redundant weights that
can be pruned or quantized without sacri   cing (and sometimes even improving) performance. prior
work also studies approaches that directly learn compact models with less parameter redundancy.
for example, the knowledge-distillation method (bucilua et al., 2006; hinton et al., 2014) trains
small student networks to reproduce the output of a much larger teacher network or ensemble. our
work differs from those approaches in that we train a single model that trades off computation for
accuracy at test time without any re-training or    netuning. indeed, weight pruning and knowledge
distillation can be used in combination with our approach, and may lead to further improvements.
resource-ef   cient machine learning. various prior studies explore computationally ef   cient vari-
ants of traditional machine-learning models (viola & jones, 2001; grubb & bagnell, 2012; karayev
et al., 2014; trapeznikov & saligrama, 2013; xu et al., 2012; 2013; nan et al., 2015; wang et al.,
2015). most of these studies focus on how to incorporate the computational requirements of com-
puting particular features in the training of machine-learning models such as (gradient-boosted)
id90. whilst our study is certainly inspired by these results, the architecture we explore
differs substantially: most prior work exploits characteristics of machine-learning models (such as
id90) that do not apply to deep networks. our work is possibly most closely related to
recent work on fractalnets (larsson et al., 2017), which can perform anytime prediction by pro-
gressively evaluating subnetworks of the full network. fractalnets differ from our work in that they
are not explicitly optimized for computation ef   ciency and consequently our experiments show that
msdnets substantially outperform fractalnets. our dynamic evaluation strategy for reducing batch
computational cost is closely related to the the adaptive computation time approach (graves, 2016;
figurnov et al., 2016), and the recently proposed method of adaptively evaluating neural networks
(bolukbasi et al., 2017). different from these works, our method adopts a specially designed net-
work with multiple classi   ers, which are jointly optimized during training and can directly output
con   dence scores to control the evaluation process for each test example. the adaptive computation
time method (graves, 2016) and its extension (figurnov et al., 2016) also perform adaptive eval-
uation on test examples to save batch computational cost, but focus on skipping units rather than
layers. in (odena et al., 2017), a    composer   model is trained to construct the evaluation network
from a set of sub-modules for each test example. by contrast, our work uses a single id98 with
multiple intermediate classi   ers that is trained end-to-end. the feedback networks (zamir et al.,
2016) enable early predictions by making predictions in a recurrent fashion, which heavily shares
parameters among classi   ers, but is less ef   cient in sharing computation.
related network architectures. our network architecture borrows elements from neural fabrics
(saxena & verbeek, 2016) and others (zhou et al., 2015; jacobsen et al., 2017; ke et al., 2016)

3

concatenationregular convstrided conv.........classifieronelayerdepthh(  )`=1`=2  h(  )featuresscalexs`cidentitycccccccccccccc`=3`=4cf(  )ororpublished as a conference paper at iclr 2018

figure 3: relative accuracy of the intermediate classi   er (left) and the    nal classi   er (right) when introducing
a single intermediate classi   er at different layers in a resnet, densenet and msdnet. all experiments were
performed on the cifar-100 dataset. higher is better.

to rapidly construct a low-resolution feature map that is amenable to classi   cation, whilst also
maintaining feature maps of higher resolution that are essential for obtaining high classi   cation
accuracy. our design differs from the neural fabrics (saxena & verbeek, 2016) substantially in
that msdnets have a reduced number of scales and no sparse channel connectivity or up-sampling
paths. msdnets are at least one order of magnitude more ef   cient and typically more accurate
    for example, an msdnet with less than 1 million parameters obtains a test error below 7.0%
on cifar-10 (krizhevsky & hinton, 2009), whereas saxena & verbeek (2016) report 7.43% with
over 20 million parameters. we use the same feature-concatenation approach as densenets (huang
et al., 2017), which allows us to bypass features optimized for early classi   ers in later layers of
the network. our architecture is related to deeply supervised networks (lee et al., 2015) in that
it incorporates classi   ers at multiple layers throughout the network. in contrast to all these prior
architectures, our network is speci   cally designed to operate in resource-aware settings.

3 problem setup

we consider two settings that impose computational constraints at prediction time.
anytime prediction. in the anytime prediction setting (grubb & bagnell, 2012), there is a    nite
computational budget b > 0 available for each test example x. the computational budget is nonde-
terministic, and varies per test instance. it is determined by the occurrence of an event that requires
the model to output a prediction immediately. we assume that the budget is drawn from some joint
distribution p (x, b). in some applications p (b) may be independent of p (x) and can be estimated.
for example, if the event is governed by a poisson process, p (b) is an exponential distribution. we
denote the loss of a model f (x) that has to produce a prediction for instance x within budget b by
l(f (x), b). the goal of an anytime learner is to minimize the expected loss under the budget dis-
tribution: l(f ) = e [l(f (x), b)]p (x,b). here, l(  ) denotes a suitable id168. as is common
in the empirical risk minimization framework, the expectation under p (x, b) may be estimated by
an average over samples from p (x, b).
budgeted batch classi   cation.
in the budgeted batch classi   cation setting, the model needs to
classify a set of examples dtest = {x1, . . . , xm} within a    nite computational budget b > 0 that
is known in advance. the learner aims to minimize the loss across all examples in dtest within a
cumulative cost bounded by b, which we denote by l(f (dtest), b) for some suitable id168
m computation on classifying an    easy    example
l(  ). it can potentially do so by spending less than b
whilst using more than b
m computation on classifying a    dif   cult    example. therefore, the budget
b considered here is a soft constraint when we have a large batch of testing samples.

4 multi-scale dense convolutional networks

a straightforward solution to the two problems introduced in section 3 is to train multiple networks
of increasing capacity, and sequentially evaluate them at test time (as in bolukbasi et al. (2017)).
in the anytime setting the evaluation can be stopped at any point and the most recent prediction is
returned. in the batch setting, the evaluation is stopped prematurely the moment a network classi   es

4

0.00.20.40.60.81.0locationofintermediateclassi   er(relativetofulldepth)0.50.60.70.80.91.0relativeaccuracyrelativeaccuracyoftheintermediateclassi   ermsdnet(withintermediateclassi   er)densenet(withintermediateclassi   er)resnet(withintermediateclassi   er)0.00.20.40.60.81.0locationofintermediateclassi   er(relativetofulldepth)0.900.920.940.960.981.00relativeaccuracyrelativeaccuracyofthe   nalclassi   ermsdnet(withintermediateclassi   er)densenet(withintermediateclassi   er)resnet(withintermediateclassi   er)published as a conference paper at iclr 2018

the test sample with suf   cient con   dence. when the resources are so limited that the execution is
terminated after the    rst network, this approach is optimal because the    rst network is trained for
exactly this computational budget without compromises. however, in both settings, this scenario is
rare. in the more common scenario where some test samples can require more processing time than
others the approach is far from optimal because previously learned features are never re-used across
the different networks.
an alternative solution is to build a deep network with a cascade of classi   ers operating on the
features of internal layers:
in such a network features computed for an earlier classi   er can be
re-used by later classi   ers. however, na    vely attaching intermediate early-exit classi   ers to a state-
of-the-art deep network leads to poor performance.
there are two reasons why intermediate early-exit classi   ers hurt the performance of deep neural
networks: early classi   ers lack coarse-level features and classi   ers throughout interfere with the
feature generation process. in this section we investigate these effects empirically (see figure 3)
and, in response to our    ndings, propose the msdnet architecture illustrated in figure 2.
problem: the lack of coarse-level features. traditional neural networks learn features of    ne
scale in early layers and coarse scale in later layers (through repeated convolution, pooling, and
strided convolution). coarse scale features in the    nal layers are important to classify the content
of the whole image into a single class. early layers lack coarse-level features and early-exit clas-
si   ers attached to these layers will likely yield unsatisfactory high error rates. to illustrate this
point, we attached4 intermediate classi   ers to varying layers of a resnet (he et al., 2016) and a
densenet (huang et al., 2017) on the cifar-100 dataset (krizhevsky & hinton, 2009). the blue
and red dashed lines in the left plot of figure 3 show the relative accuracies of these classi   ers.
all three plots gives rise to a clear trend: the accuracy of a classi   er is highly correlated with its
position within the network. particularly in the case of the resnet (blue line), one can observe a
visible    staircase    pattern, with big improvements after the 2nd and 4th classi   ers     located right
after pooling layers.
solution: multi-scale feature maps. to address this issue, msdnets maintain a feature repre-
sentation at multiple scales throughout the network, and all the classi   ers only use the coarse-level
features. the feature maps at a particular layer5 and scale are computed by concatenating the re-
sults of one or two convolutions: 1. the result of a regular convolution applied on the same-scale
features from the previous layer (horizontal connections) and, if possible, 2. the result of a strided
convolution applied on the    ner-scale feature map from the previous layer (diagonal connections).
the horizontal connections preserve and progress high-resolution information, which facilitates the
construction of high-quality coarse features in later layers. the vertical connections produce coarse
features throughout that are amenable to classi   cation. the dashed black line in figure 3 shows that
msdnets substantially increase the accuracy of early classi   ers.
problem: early classi   ers interfere with later classi   ers. the right plot of figure 3 shows the
accuracies of the    nal classi   er as a function of the location of a single intermediate classi   er,
relative to the accuracy of a network without intermediate classi   ers. the results show that the
introduction of an intermediate classi   er harms the    nal resnet classi   er (blue line), reducing its
accuracy by up to 7%. we postulate that this accuracy degradation in the resnet may be caused by
the intermediate classi   er in   uencing the early features to be optimized for the short-term and not
for the    nal layers. this improves the accuracy of the immediate classi   er but collapses information
required to generate high quality features in later layers. this effect becomes more pronounced
when the    rst classi   er is attached to an earlier layer.
solution: dense connectivity. by contrast, the densenet (red line) suffers much less from this
effect. dense connectivity (huang et al., 2017) connects each layer with all subsequent layers and
allows later layers to bypass features optimized for the short-term, to maintain the high accuracy
of the    nal classi   er. if an earlier layer collapses information to generate short-term features, the
lost information can be recovered through the direct connection to its preceding layer. the    nal
classi   er   s performance becomes (more or less) independent of the location of the intermediate

4we select six evenly spaced locations for each of the networks to introduce the intermediate classi   er.
both the resnet and densenet have three resolution blocks; each block offers two tentative locations for the
intermediate classi   er. the loss of the intermediate and    nal classi   ers are equally weighted.

5here, we use the term    layer    to refer to a column in figure 2.

5

published as a conference paper at iclr 2018

figure 4: the output xs
operator, hs
(cid:96) and   hs
of hs

(cid:96) of layer (cid:96) at the sth scale in a msdnet. herein, [. . . ] denotes the concatenation
(cid:96)(  ) a strided convolutional. note that the outputs

(cid:96)(  ) a regular convolution transformation, and   hs
(cid:96) have the same feature map size; their outputs are concatenated along the channel dimension.

classi   er. as far as we know, this is the    rst paper that discovers that dense connectivity is an
important element to early-exit classi   ers in deep networks, and we make it an integral design choice
in msdnets.

4.1

the msdnet architecture

(cid:96) , . . . , xs

1 , . . . , xs

(cid:96) and the original input image as x1

1 of the    rst layer is formally given in the top row of figure 4.

the msdnet architecture is illustrated in figure 2. we present its main components below. addi-
tional details on the architecture are presented in appendix a.
first layer. the    rst layer ((cid:96) = 1) is unique as it includes vertical connections in figure 2. its main
purpose is to    seed    representations on all s scales. one could view its vertical layout as a miniature
   s-layers    convolutional network (s=3 in figure 2). let us denote the output feature maps at layer
(cid:96) and scale s as xs
0. feature maps at coarser scales are obtained
via down-sampling. the output xs
subsequent layers. following huang et al. (2017), the output feature maps xs
(cid:96) produced at subse-
quent layers, (cid:96) > 1, and scales, s, are a concatenation of transformed feature maps from all previous
feature maps of scale s and s     1 (if s > 1). formally, the (cid:96)-th layer of our network outputs a set of
(cid:96)(cid:9), given in the last row of figure 4.
features at s scales(cid:8)x1
classi   ers. the classi   ers in msdnets also follow the dense connectivity pattern within the coars-
est scale, s, i.e., the classi   er at layer (cid:96) uses all the features(cid:2)xs
(cid:96)(cid:3). each classi   er consists
of two convolutional layers, followed by one average pooling layer and one linear layer. in prac-
tice, we only attach classi   ers to some of the intermediate layers, and we let fk(  ) denote the kth
classi   er. during testing in the anytime setting we propagate the input through the network until the
budget is exhausted and output the most recent prediction. in the batch budget setting at test time,
an example traverses the network and exits after classi   er fk if its prediction con   dence (we use
the maximum value of the softmax id203 as a con   dence measure) exceeds a pre-determined
threshold   k. before training, we compute the computational cost, ck, required to process the net-
work up to the kth classi   er. we denote by 0 < q     1 a    xed exit id203 that a sample that
reaches a classi   er will obtain a classi   cation with suf   cient con   dence to exit. we assume that q is
constant across all layers, which allows us to compute the id203 that a sample exits at classi   er
k as: qk = z(1     q)k   1q, where z is a normalizing constant that ensures that(cid:80)k p(qk) = 1. at test
time, we need to ensure that the overall cost of classifying all samples in dtest does not exceed our
budget b (in expectation). this gives rise to the constraint |dtest|(cid:80)k qkck     b. we can solve this
constraint for q and determine the thresholds   k on a validation set in such a way that approximately
|dtest|qk validation samples exit at the kth classi   er.
id168s. during training we use cross id178 id168s l(fk) for all classi   ers and
1|d|(cid:80)(x,y)   d(cid:80)k wkl(fk). herein, d denotes the training
minimize a weighted cumulative loss:
set and wk     0 the weight of the k-th classi   er. if the budget distribution p (b) is known, we can use
the weights wk to incorporate our prior knowledge about the budget b in the learning. empirically,
we    nd that using the same weight for all id168s (i.e., setting    k : wk = 1) works well in
practice.
network reduction and lazy evaluation. there are two straightforward ways to further reduce the
computational requirements of msdnets. first, it is inef   cient to maintain all the    ner scales until

6

x34x32x33x31x23x22x21not connectedindirectly connecteddirectly connectedxs``=1`=2`=3`=4s=1h11(x10)h12 x11 h13    x11,x12    h14    x11,x12,x13    s=2  h21(x11)     h22 x11 ,h22 x21       h23    x11,x12    ,h23    x21,x22          h24    x11,x12,x13    ,h24    x21,x22,x23     s=3  h31(x21)     h32 x21 ,h32 x31       h33    x21,x22    ,h33    x31,x32          h34    x21,x22,x23    ,h34    x31,x32,x33     .........or`=1`=2`=3`=4h11(x10)h12 x11 h13 x11,x12 h14 x11,x12,x13 s=1s=2s=3  h21 x11      h22    x11    h22    x21          h23    x11,x12    h23    x21,x22          h24    x11,x12,x13    h24    x21,x22,x23       h31 x21      h32    x21    h32    x31          h33    x21,x22    h33    x31,x32     xsl     h34    x21,x22,x23    h34    x31,x32,x33     published as a conference paper at iclr 2018

the last layer of the network. one simple strategy to reduce the size of the network is by splitting it
into s blocks along the depth dimension, and only keeping the coarsest (s     i + 1) scales in the ith
block (a schematic layout of this structure is shown in figure 9). this reduces computational cost for
both training and testing. every time a scale is removed from the network, we add a transition layer
between the two blocks that merges the concatenated features using a 1  1 convolution and cuts the
number of channels in half before feeding the    ne-scale features into the coarser scale via a strided
convolution (this is similar to the densenet-bc architecture of huang et al. (2017)). second, since
a classi   er at layer (cid:96) only uses features from the coarsest scale, the    ner feature maps in layer (cid:96) (and
some of the    ner feature maps in the previous s   2 layers) do not in   uence the prediction of that
classi   er. therefore, we group the computation in    diagonal blocks    such that we only propagate
the example along paths that are required for the evaluation of the next classi   er. this minimizes
unnecessary computations when we need to stop because the computational budget is exhausted.
we call this strategy lazy evaluation.

5 experiments

we evaluate the effectiveness of our approach on three image classi   cation datasets, i.e., the cifar-
10, cifar-100 (krizhevsky & hinton, 2009) and ilsvrc 2012 (id163; deng et al. (2009))
datasets. code to reproduce all results is available at https://anonymous-url. details on
architectural con   gurations of msdnets are described in appendix a.
datasets. the two cifar datasets contain 50, 000 training and 10, 000 test images of 32  32 pixels;
we hold out 5, 000 training images as a validation set. the datasets comprise 10 and 100 classes,
respectively. we follow he et al. (2016) and apply standard data-augmentation techniques to the
training images: images are zero-padded with 4 pixels on each side, and then randomly cropped
to produce 32  32 images. images are    ipped horizontally with id203 0.5, and normalized
by subtracting channel means and dividing by channel standard deviations. the id163 dataset
comprises 1, 000 classes, with a total of 1.2 million training images and 50,000 validation images.
we hold out 50,000 images from the training set to estimate the con   dence threshold for classi   ers
in msdnet. we adopt the data augmentation scheme of he et al. (2016) at training time; at test
time, we classify a 224  224 center crop of images that were resized to 256  256 pixels.
training details. we train all models using the framework of gross & wilber (2016). on the two
cifar datasets, all models (including all baselines) are trained using stochastic id119
(sgd) with mini-batch size 64. we use nesterov momentum with a momentum weight of 0.9
without dampening, and a weight decay of 10   4. all models are trained for 300 epochs, with an
initial learning rate of 0.1, which is divided by a factor 10 after 150 and 225 epochs. we apply the
same optimization scheme to the id163 dataset, except that we increase the mini-batch size to
256, and all the models are trained for 90 epochs with learning rate drops after 30 and 60 epochs.

5.1 anytime prediction

in the anytime prediction setting, the model maintains a progressively updated distribution over
classes, and it can be forced to output its most up-to-date prediction at an arbitrary time.
baselines. there exist several baseline approaches for anytime prediction: fractalnets (larsson
et al., 2017), deeply supervised networks (lee et al., 2015), and ensembles of deep networks of
varying or identical sizes. fractalnets allow for multiple evaluation paths during id136 time,
which vary in computation time. in the anytime setting, paths are evaluated in order of increasing
computation. in our result    gures, we replicate the fractalnet results reported in the original paper
(larsson et al., 2017) for reference. deeply supervised networks introduce multiple early-exit classi-
   ers throughout a network, which are applied on the features of the particular layer they are attached
to. instead of using the original model proposed in lee et al. (2015), we use the more competitive
resnet and densenet architectures (referred to as densenet-bc in huang et al. (2017)) as the base
networks in our experiments with deeply supervised networks. we refer to these as resnetmc and
densenetmc, where m c stands for multiple classi   ers. both networks require about 1.3    108
flops when fully evaluated; the detailed network con   gurations are presented in the supplemen-
tary material. in addition, we include ensembles of resnets and densenets of varying or identical
sizes. at test time, the networks are evaluated sequentially (in ascending order of network size) to
obtain predictions for the test data. all predictions are averaged over the evaluated classi   ers. on

7

published as a conference paper at iclr 2018

figure 5: accuracy (top-1) of anytime prediction models as a function of computational budget on the id163
(left) and cifar-100 (right) datasets. higher is better.

id163, we compare msdnet against a highly competitive ensemble of resnets and densenets,
with depth varying from 10 layers to 50 layers, and 36 layers to 121 layers, respectively.
anytime prediction results are presented in figure 5. the left plot shows the top-1 classi   cation
accuracy on the id163 validation set. here, for all budgets in our evaluation, the accuracy of
msdnet substantially outperforms the resnets and densenets ensemble. in particular, when the
budget ranges from 0.1  1010 to 0.3  1010 flops, msdnet achieves     4%   8% higher accuracy.
we evaluate more baselines on cifar-100 (and cifar-10; see supplementary materials). we
observe that msdnet substantially outperforms resnetsmc and densenetsmc at any computational
budget within our range. this is due to the fact that after just a few layers, msdnets have produced
low-resolution feature maps that are much more suitable for classi   cation than the high-resolution
feature maps in the early layers of resnets or densenets. msdnet also outperforms the other
baselines for nearly all computational budgets, although it performs on par with ensembles when
the budget is very small. in the extremely low-budget regime, ensembles have an advantage because
their predictions are performed by the    rst (small) network, which is optimized exclusively for the
low budget. however, the accuracy of ensembles does not increase nearly as fast when the budget is
increased. the msdnet outperforms the ensemble as soon as the latter needs to evaluate a second
model: unlike msdnets, this forces the ensemble to repeat the computation of similar low-level
features repeatedly. ensemble accuracies saturate rapidly when all networks are shallow.

5.2 budgeted batch classification
in budgeted batch classi   cation setting, the predictive model receives a batch of m instances and a
computational budget b for classifying all m instances. in this setting, we use dynamic evaluation:
we perform early-exiting of    easy    examples at early classi   ers whilst propagating    hard    examples
through the entire network, using the procedure described in section 4.
baselines. on id163, we compare the dynamically evaluated msdnet with    ve resnets (he
et al., 2016) and    ve densenets (huang et al., 2017), alexnet (krizhevsky et al., 2012), and google-
lenet (szegedy et al., 2015); see the supplementary material for details. we also evaluate an ensem-
ble of the    ve resnets that uses exactly the same dynamic-evaluation procedure as msdnets at test
time:    easy    images are only propagated through the smallest resnet-10, whereas    hard    images
are classi   ed by all    ve resnet models (predictions are averaged across all evaluated networks in
the ensemble). we classify batches of m = 128 images.
on cifar-100, we compare msdnet with several highly competitive baselines,
including
resnets (he et al., 2016), densenets (huang et al., 2017) of varying sizes, stochastic depth net-
works (huang et al., 2016), wide resnets (zagoruyko & komodakis, 2016) and fractalnets (lars-
son et al., 2017). we also compare msdnet to the resnetmc and densenetmc models that were
used in section 5.1, using dynamic evaluation at test time. we denote these baselines as resnetmc
/ densenetmc with early-exits. to prevent the result plots from becoming too cluttered, we present
cifar-100 results with dynamically evaluated ensembles in the supplementary material. we clas-
sify batches of m = 256 images at test time.
budgeted batch classi   cation results on id163 are shown in the left panel of figure 7. we
trained three msdnets with different depths, each of which covers a different range of compu-

8

0.00.20.40.60.81.01.21.4budget(inmul-add)  101060626466687072747678accuracy(%)anytimepredictiononid163msdnetensembleofresnets(varyingdepth)ensembleofdensenets(varyingdepth)0.00.20.40.60.81.01.21.4budget(inmul-add)  1084550556065707580accuracy(%)anytimepredictiononcifar-100msdnetfractalnetresnetmcdensenetmcensembleofresnets(allshallow)ensembleofresnets(varyingdepth)ensembleofdensenets(varyingdepth)published as a conference paper at iclr 2018

figure 7: accuracy (top-1) of budgeted batch classi   cation models as a function of average computational
budget per image the on id163 (left) and cifar-100 (right) datasets. higher is better.

tational budgets. we plot the performance of each msdnet as a gray curve; we select the best
model for each budget based on its accuracy on the validation set, and plot the corresponding ac-
curacy as a black curve. the plot shows that the predictions of msdnets with dynamic evaluation
are substantially more accurate than those of resnets and densenets that use the same amount of
computation. for instance, with an average budget of 1.7  109 flops, msdnet achieves a top-1
accuracy of    75%, which is    6% higher than that achieved by a resnet with the same number of
flops. compared to the computationally ef   cient densenets, msdnet uses     2   3   times fewer
flops to achieve the same classi   cation accuracy. moreover, msdnet with dynamic evaluation
allows for very precise tuning of the computational budget that is consumed, which is not possible
with individual resnet or densenet models. the ensemble of resnets or densenets with dynamic
evaluation performs on par with or worse than their individual counterparts (but they do allow for
setting the computational budget very precisely).
the right panel of figure 7 shows our results on cifar-100. the results show that msdnets con-
sistently outperform all baselines across all budgets. notably, msdnet performs on par with a 110-
layer resnet using only 1/10th of the computational budget and it is up to     5 times more ef   cient
than densenets, stochastic depth networks, wide resnets, and fractalnets. similar to results in
the anytime-prediction setting, msdnet substantially outperform resnetsm c and densenetsm c
with multiple intermediate classi   ers, which provides further evidence that the coarse features in the
msdnet are important for high performance in earlier layers.
visualization. to illustrate the ability of our ap-
proach to reduce the computational requirements
for classifying    easy    examples, we show twelve
randomly sampled test images from two ima-
genet classes in figure 6. the top row shows
   easy    examples that were correctly classi   ed
and exited by the    rst classi   er. the bottom row
shows    hard    examples that would have been in-
correctly classi   ed by the    rst classi   er but were
passed on because its uncertainty was too high.
the    gure suggests that early classi   ers recog-
nize prototypical class examples, whereas the last
classi   er recognizes non-typical images.

figure 6: sampled images from the id163 classes
red wine and volcano. top row: images exited from
the    rst classi   er of a msdnet with correct predic-
tion; bottom row: images failed to be correctly clas-
si   ed at the    rst classi   er but were correctly pre-
dicted and exited at the last layer.

5.3 more computationally efficient densenets

here, we discuss an interesting    nding during our exploration of the msdnet architecture. we
found that following the densenet structure to design our network, i.e., by keeping the number of
output channels (or growth rate) the same at all scales, did not lead to optimal results in terms of the
accuracy-speed trade-off. the main reason for this is that compared to network architectures like
resnets, the densenet structure tends to apply more    lters on the high-resolution feature maps in
the network. this helps to reduce the number of parameters in the model, but at the same time, it
greatly increases the computational cost. we tried to modify densenets by doubling the growth rate

9

012345averagebudget(inmul-add)  1095659626568717477accuracy(%)budgetedbatchclassi   cationonid163msdnetwithdynamicevaluationearly-exitensembleofresnetsearly-exitensembleofdensenetsresnets(heetal.,2015)densenetsgooglenet(szegedyetal.,2015)alexnet(krizhevskyetal.,2012)0.00.51.01.52.02.5averagebudget(inmul-add)  10860626466687072747678accuracy(%)resnet-110densenet-88budgetedbatchclassi   cationoncifar-100msdnetwithdynamicevaluationmsdnetw/odynamicevaluationresnetmcwithearly-exitsdensenetmcwithearly-exitsresnets(heetal.,2015)densenets(huangetal.,2016)stochasticdepth(huangetal.,2016)wideresnet(zagoruykoetal.,2016)fractalnet(larssonetal.,2016)(cid:11)(cid:68)(cid:12)(cid:3)(cid:53)(cid:72)(cid:71)(cid:3)(cid:90)(cid:76)(cid:81)(cid:72)   easy      hard   (cid:11)(cid:69)(cid:12)(cid:3)(cid:57)(cid:82)(cid:79)(cid:70)(cid:68)(cid:81)(cid:82)(cid:11)(cid:68)(cid:12)(cid:3)(cid:53)(cid:72)(cid:71)(cid:3)(cid:90)(cid:76)(cid:81)(cid:72)   easy      hard   (cid:11)(cid:69)(cid:12)(cid:3)(cid:57)(cid:82)(cid:79)(cid:70)(cid:68)(cid:81)(cid:82)published as a conference paper at iclr 2018

figure 8: test accuracy of densenet* on cifar-100 under the anytime learning setting (left) and the budgeted
batch setting (right).

after each transition layer, so that more    lters are applied to low-resolution feature maps. it turns
out that the resulting network, which we denote as densenet*, signi   cantly outperform the original
densenet in terms of computational ef   ciency.
we experimented with densenet* in our two settings with test time budget constraints. the left
panel of figure 8 shows the anytime prediction performance of an ensemble of densenets* of vary-
ing depths. it outperforms the ensemble of original densenets of varying depth by a large margin,
but is still slightly worse than msdnets. in the budgeted batch budget setting, densenet* also
leads to signi   cantly higher accuracy over its counterpart under all budgets, but is still substantially
outperformed by msdnets.
6 conclusion
we presented the msdnet, a novel convolutional network architecture, optimized to incorporate
cpu budgets at test-time. our design is based on two high-level design principles, to generate and
maintain coarse level features throughout the network and to inter-connect the layers with dense
connectivity. the former allows us to introduce intermediate classi   ers even at early layers and
the latter ensures that these classi   ers do not interfere with each other. the    nal design is a two
dimensional array of horizontal and vertical layers, which decouples depth and feature coarseness.
whereas in traditional convolutional networks features only become coarser with increasing depth,
the msdnet generates features of all resolutions from the    rst layer on and maintains them through-
out. the result is an architecture with an unprecedented range of ef   ciency. a single network can
outperform all competitive baselines on an impressive range of computational budgets ranging from
highly limited cpu constraints to almost unconstrained settings.
as future work we plan to investigate the use of resource-aware deep architectures beyond object
classi   cation, e.g. image segmentation (long et al., 2015). further, we intend to explore approaches
that combine msdnets with model compression (chen et al., 2015; han et al., 2015), spatially
adaptive computation (figurnov et al., 2016) and more ef   cient convolution operations (chollet,
2016; howard et al., 2017) to further improve computational ef   ciency.

acknowledgments

the authors are supported in part by grants from the national science foundation ( iii-1525919,
iis-1550179, iis-1618134, s&as 1724282, and ccf-1740822), the of   ce of naval research dod
(n00014-17-1-2175), and the bill and melinda gates foundation. we are also thankful for generous
support by sap america inc.

references
mariusz bojarski, davide del testa, daniel dworakowski, bernhard firner, beat flepp, prasoon
goyal, lawrence d jackel, mathew monfort, urs muller, jiakai zhang, et al. end to end learning
for self-driving cars. arxiv preprint arxiv:1604.07316, 2016.

10

0.00.20.40.60.81.01.21.4budget(inmul-add)  1084550556065707580accuracy(%)anytimepredictiononcifar-100msdnetensembleofdensenets(varyingdepth)ensembleofdensenets*(varyingdepth)0.00.51.01.52.02.5averagebudget(inmul-add)  10860626466687072747678accuracy(densenet-88batchcomputationallearningoncifar-100msdnetwithearly-exitsdensenets(huangetal.,2016)densenets*published as a conference paper at iclr 2018

tolga bolukbasi, joseph wang, ofer dekel, and venkatesh saligrama. adaptive neural networks

for fast test-time prediction. arxiv preprint arxiv:1702.07811, 2017.

cristian bucilua, rich caruana, and alexandru niculescu-mizil. model compression.

sigkdd, pp. 535   541. acm, 2006.

in acm

wenlin chen, james t wilson, stephen tyree, kilian q weinberger, and yixin chen. compressing

neural networks with the hashing trick. in icml, pp. 2285   2294, 2015.

franc  ois chollet. xception: deep learning with depthwise separable convolutions. arxiv preprint

arxiv:1610.02357, 2016.

jia deng, wei dong, richard socher, li-jia li, kai li, and li fei-fei. id163: a large-scale

hierarchical image database. in cvpr, pp. 248   255, 2009.

michael figurnov, maxwell d collins, yukun zhu, li zhang, jonathan huang, dmitry vetrov, and
ruslan salakhutdinov. spatially adaptive computation time for residual networks. arxiv preprint
arxiv:1612.02297, 2016.

alex graves.

adaptive computation time for recurrent neural networks.

arxiv:1603.08983, 2016.

arxiv preprint

sam gross and michael wilber. training and investigating residual nets. 2016. url http:

//torch.ch/blog/2016/02/04/resnets.html.

alexander grubb and drew bagnell. speedboost: anytime prediction with uniform near-optimality.

in aistats, volume 15, pp. 458   466, 2012.

song han, huizi mao, and william j. dally. deep compression: compressing deep neural network

with pruning, trained quantization and huffman coding. corr, abs/1510.00149, 2015.

babak hassibi, david g stork, and gregory j wolff. optimal brain surgeon and general network

pruning. in ijid98, pp. 293   299, 1993.

kaiming he, xiangyu zhang, shaoqing ren, and jian sun. delving deep into recti   ers: surpassing

human-level performance on id163 classi   cation. in iccv, pp. 1026   1034, 2015.

kaiming he, xiangyu zhang, shaoqing ren, and jian sun. deep residual learning for image recog-

nition. in cvpr, pp. 770   778, 2016.

geoffrey hinton, oriol vinyals, and jeff dean. distilling the knowledge in a neural network. in

nips deep learning workshop, 2014.

andrew g howard, menglong zhu, bo chen, dmitry kalenichenko, weijun wang, tobias weyand,
marco andreetto, and hartwig adam. mobilenets: ef   cient convolutional neural networks for
mobile vision applications. arxiv preprint arxiv:1704.04861, 2017.

gao huang, yu sun, zhuang liu, daniel sedra, and kilian q weinberger. deep networks with

stochastic depth. in eccv, pp. 646   661. springer, 2016.

gao huang, zhuang liu, kilian q weinberger, and laurens van der maaten. densely connected

convolutional networks. in cvpr, 2017.

itay hubara, matthieu courbariaux, daniel soudry, ran el-yaniv, and yoshua bengio. binarized

neural networks. in nips, pp. 4107   4115, 2016.

sergey ioffe and christian szegedy. batch id172: accelerating deep network training by

reducing internal covariate shift. in icml, pp. 770   778, 2015.

j  orn-henrik jacobsen, edouard oyallon, st  ephane mallat, and arnold wm smeulders. multiscale

hierarchical convolutional networks. arxiv preprint arxiv:1703.04140, 2017.

sergey karayev, mario fritz, and trevor darrell. anytime recognition of objects and scenes. in

cvpr, pp. 572   579, 2014.

11

published as a conference paper at iclr 2018

tsung-wei ke, michael maire, and stella x. yu. neural multigrid. corr, abs/1611.07661, 2016.

url http://arxiv.org/abs/1611.07661.

alex krizhevsky and geoffrey hinton. learning multiple layers of features from tiny images. tech

report, 2009.

alex krizhevsky, ilya sutskever, and geoffrey e hinton. id163 classi   cation with deep convo-

lutional neural networks. in nips, pp. 1097   1105, 2012.

gustav larsson, michael maire, and gregory shakhnarovich. fractalnet: ultra-deep neural net-

works without residuals. in iclr, 2017.

yann lecun, john s denker, sara a solla, richard e howard, and lawrence d jackel. optimal

brain damage. in nips, volume 2, pp. 598   605, 1989.

chen-yu lee, saining xie, patrick w gallagher, zhengyou zhang, and zhuowen tu. deeply-

supervised nets. in aistats, volume 2, pp. 5, 2015.

hao li, asim kadav, igor durdanovic, hanan samet, and hans peter graf. pruning    lters for

ef   cient convnets. in iclr, 2017.

tsung-yi lin, michael maire, serge belongie, james hays, pietro perona, deva ramanan, piotr
doll  ar, and c lawrence zitnick. microsoft coco: common objects in context. in eccv, pp.
740   755. springer, 2014.

jonathan long, evan shelhamer, and trevor darrell. fully convolutional networks for semantic

segmentation. in cvpr, pp. 3431   3440, 2015.

feng nan, joseph wang, and venkatesh saligrama. feature-budgeted id79. in icml, pp.

1983   1991, 2015.

augustus odena, dieterich lawson, and christopher olah. changing model behavior at test-time

using id23. arxiv preprint arxiv:1702.07780, 2017.

mohammad rastegari, vicente ordonez, joseph redmon, and ali farhadi. xnor-net: id163
classi   cation using binary convolutional neural networks. in eccv, pp. 525   542. springer, 2016.

shreyas saxena and jakob verbeek. convolutional neural fabrics. in nips, pp. 4053   4061, 2016.
christian szegedy, wei liu, yangqing jia, pierre sermanet, scott reed, dragomir anguelov, du-
mitru erhan, vincent vanhoucke, and andrew rabinovich. going deeper with convolutions. in
cvpr, pp. 1   9, 2015.

kirill trapeznikov and venkatesh saligrama. supervised sequential classi   cation under budget

constraints. in ai-stats, pp. 581   589, 2013.

paul viola and michael jones. robust real-time id164. international journal of computer

vision, 4(34   47), 2001.

ji wan, dayong wang, steven chu hong hoi, pengcheng wu, jianke zhu, yongdong zhang, and
in acm

jintao li. deep learning for content-based id162: a comprehensive study.
multimedia, pp. 157   166, 2014.

joseph wang, kirill trapeznikov, and venkatesh saligrama. ef   cient learning by directed acyclic

graph for resource constrained prediction. in nips, pp. 2152   2160. 2015.

zhixiang xu, olivier chapelle, and kilian q. weinberger. the greedy miser: learning under test-

time budgets. in icml, pp. 1175   1182, 2012.

zhixiang xu, matt kusner, minmin chen, and kilian q. weinberger. cost-sensitive tree of classi-

   ers. in icml, volume 28, pp. 133   141, 2013.

sergey zagoruyko and nikos komodakis. wide residual networks. in bmvc, 2016.
a. r. zamir, t.-l. wu, l. sun, w. shen, b. e. shi, j. malik, and s. savarese. feedback networks.

arxiv e-prints, december 2016.

yisu zhou, xiaolin hu, and bo zhang. interlinked convolutional neural networks for face parsing.

in international symposium on neural networks, pp. 222   231. springer, 2015.

12

published as a conference paper at iclr 2018

a details of msdnet architecture and baseline networks

we use msdnet with three scales on the cifar datasets, and the network reduction method intro-
duced in 4.1 is applied. figure 9 gives an illustration of the reduced network. the convolutional
1, denote a sequence of 3  3 convolutions (conv), batch normaliza-
layer functions in the    rst layer, hs
tion (bn; ioffe & szegedy (2015)), and recti   ed linear unit (relu) activation. in the computation
of   hs
1, down-sampling is performed by applying convolutions using strides that are powers of two.
for subsequent feature layers, the transformations hs
(cid:96) are de   ned following the design in
densenets (huang et al., 2017): conv(1    1)-bn-relu-conv(3    3)-bn-relu. we set the num-
ber of output channels of the three scales to 6, 12, and 24, respectively. each classi   er has two
down-sampling convolutional layers with 128 dimensional 3  3    lters, followed by a 2  2 average
pooling layer and a linear layer.
the msdnet used for id163 has four scales, respectively producing 16, 32, 64, and 64 feature
maps at each layer. the network reduction is also applied to reduce computational cost. the original
images are    rst transformed by a 7  7 convolution and a 3  3 max pooling (both with stride 2),
before entering the    rst layer of msdnets. the classi   ers have the same structure as those used for
the cifar datasets, except that the number of output channels of each convolutional layer is set to
be equal to the number of its input channels.

(cid:96) and   hs

figure 9: illustration of an msdnet with network reduction. the network has s = 3 scales, and it is divided
into three blocks, which maintain a decreasing number of scales. a transition layer is placed between two
contiguous blocks.
network architecture for anytime prediction. the msdnet used in our anytime-prediction ex-
periments has 24 layers (each layer corresponds to a column in fig. 1 of the main paper), using
the reduced network with transition layers as described in section 4. the classi   ers operate on the
output of the 2  (i+1)th layers, with i = 1, . . . , 11. on id163, we use msdnets with four scales,
and the ith classi   er operates on the (k  i+3)th layer (with i = 1, . . . , 5 ), where k = 4, 6 and 7. for
simplicity, the losses of all the classi   ers are weighted equally during training.
network architecture for budgeted batch setting. the msdnets used here for the two cifar
datasets have depths ranging from 10 to 36 layers, using the reduced network with transition layers
i=1 i)th layer. the msdnets used

as described in section 4. the kth classi   er is attached to the ((cid:80)k

for id163 are the same as those described for the anytime learning setting.
resnetmc and densenetmc. the resnetmc has 62 layers, with 10 residual blocks at each spatial
resolution (for three resolutions): we train early-exit classi   ers on the output of the 4th and 8th
residual blocks at each resolution, producing a total of 6 intermediate classi   ers (plus the    nal
classi   cation layer). the densenetmc consists of 52 layers with three dense blocks and each of
them has 16 layers. the six intermediate classi   ers are attached to the 6th and 12th layer in each
block, also with dense connections to all previous layers in that block.

b additional results

b.1 ablation study

we perform additional experiments to shed light on the contributions of the three main components
of msdnet, viz., multi-scale feature maps, dense connectivity, and intermediate classi   ers.

13

ccccccccc......cc...c`=1ccccc.........depthscaleblock=1block=2block=3published as a conference paper at iclr 2018

we start from an msdnet with six intermediate
classi   ers and remove the three main components
one at a time. to make our comparisons fair, we
keep the computational costs of the full networks
similar, at around 3.0    108 flops, by adapting
the network width, i.e., number of output chan-
nels at each layer. after removing all the three
components in an msdnet, we obtain a regular
vgg-like convolutional network. we show the
classi   cation accuracy of all classi   ers in a model
in the left panel of figure 10. several observa-
tions can be made: 1.
the dense connectivity is
crucial for the performance of msdnet and re-
moving it hurts the overall accuracy drastically
(orange vs. black curve); 2. removing multi-scale
convolution hurts the accuracy only in the lower
budget regions, which is consistent with our mo-
tivation that the multi-scale design introduces discriminative features early on; 3. the    nal canonical
id98 (star) performs similarly as msdnet under the speci   c budget that matches its evaluation cost
exactly, but it is unsuited for varying budget constraints. the    nal id98 performs substantially bet-
ter at its particular budget region than the model without dense connectivity (orange curve). this
suggests that dense connectivity is particularly important in combination with multiple classi   ers.

figure 10: ablation study (on cifar-100) of ms-
dnets that shows the effect of dense connectiv-
ity, multi-scale features, and intermediate classi   ers.
higher is better.

b.2 results on cifar-10

for the cifar-10 dataset, we use the same msdnets and baseline models as we used for cifar-
100, except that the networks used here have a 10-way fully connected layer at the end. the results
under the anytime learning setting and the batch computational budget setting are shown in the left
and right panel of figure 11, respectively. similar to what we have observed from the results on
cifar-100 and id163, msdnets outperform all the baselines by a signi   cant margin in both
settings. as in the experiments presented in the main paper, resnet and densenet models with
multiple intermediate classi   ers perform relatively poorly.

figure 11: classi   cation accuracies on the cifar-10 dataset in the anytime-prediction setting (left) and the
budgeted batch setting (right).

14

0.00.30.60.91.21.5mul-add   108505560657075accuracy(%)msdnetsvariants(cifar-100)msdnetsmsdnetswithoutdenseconnectionmsdnetwithoutdense-connectionandmulti-scalemsdnetwithoutdense-connection,multi-scaleandintermediateclassi   ermultiple classi   ers, multi-scale, dense connectivity multiple classi   ers, multi-scale, dense connectivity multiple classi   ers, multi-scale, dense connectivity multiple classi   ers, multi-scale, dense connectivity 0.00.20.40.60.81.01.21.4budget(inmul-add)  108808386899295accuracy(%)anytimepredictiononcifar-10msdnetresnetmcdensenetmcensembleofresnets(allshallow)ensembleofresnets(varyingdepth)ensembleofdensenets(varyingdepth)0.00.51.01.52.02.5averagebudget(inmul-add)  1088889909192939495accuracy(%)resnet-110densenet-88batchcomputationallearningoncifar-10msdnetwithearly-exitsresnetmcwithearly-exitsdensenetmcwithearly-exitsresnets(heetal.,2015)densenets(huangetal.,2016)stochasticdepth-110(huangetal.,2016)wideresnet-40(zagoruykoetal.,2016)