published as a conference paper at iclr 2017

structured attention networks

carl denton   

yoon kim   
{yoonkim@seas,carldenton@college,lhoang@g,srush@seas}.harvard.edu
school of engineering and applied sciences
harvard university
cambridge, ma 02138, usa

alexander m. rush

luong hoang

7
1
0
2

 

b
e
f
6
1

 

 
 
]
l
c
.
s
c
[
 
 

3
v
7
8
8
0
0

.

2
0
7
1
:
v
i
x
r
a

abstract

attention networks have proven to be an effective approach for embedding cat-
egorical id136 within a deep neural network. however, for many tasks we
may want to model richer structural dependencies without abandoning end-to-end
training. in this work, we experiment with incorporating richer structural distri-
butions, encoded using id114, within deep networks. we show that
these structured attention networks are simple extensions of the basic attention
procedure, and that they allow for extending attention beyond the standard soft-
selection approach, such as attending to partial segmentations or to subtrees. we
experiment with two different classes of structured attention networks: a linear-
chain conditional random    eld and a graph-based parsing model, and describe
how these models can be practically implemented as neural network layers. ex-
periments show that this approach is effective for incorporating structural biases,
and structured attention networks outperform baseline id12 on a va-
riety of synthetic and real tasks:
tree transduction, id4,
id53, and natural language id136. we further    nd that mod-
els trained in this way learn interesting unsupervised hidden representations that
generalize simple attention.

1

introduction

attention networks are now a standard part of the deep learning toolkit, contributing to impressive
results in id4 (bahdanau et al., 2015; luong et al., 2015), image captioning
(xu et al., 2015), id103 (chorowski et al., 2015; chan et al., 2015), id53
(hermann et al., 2015; sukhbaatar et al., 2015), and algorithm-learning (graves et al., 2014; vinyals
et al., 2015), among many other applications (see cho et al. (2015) for a comprehensive review).
this approach alleviates the bottleneck of compressing a source into a    xed-dimensional vector by
equipping a model with variable-length memory (weston et al., 2014; graves et al., 2014; 2016),
thereby providing random access into the source as needed. attention is implemented as a hidden
layer which computes a categorical distribution (or hierarchy of categorical distributions) to make a
soft-selection over source elements.
noting the empirical effectiveness of attention networks, we also observe that the standard attention-
based architecture does not directly model any structural dependencies that may exist among the
source elements, and instead relies completely on the hidden layers of the network. while one might
argue that these structural dependencies can be learned implicitly by a deep model with enough data,
in practice, it may be useful to provide a structural bias. modeling structural dependencies at the
   nal, output layer has been shown to be important in many deep learning applications, most notably
in seminal work on graph transformers (lecun et al., 1998), key work on nlp (collobert et al.,
2011), and in many other areas (peng et al., 2009; do & arti  eres, 2010; jaderberg et al., 2014; chen
et al., 2015; durrett & klein, 2015; lample et al., 2016, inter alia).
in this work, we consider applications which may require structural dependencies at the attention
layer, and develop internal structured layers for modeling these directly. this approach generalizes
categorical soft-selection attention layers by specifying possible structural dependencies in a soft

   equal contribution.

1

published as a conference paper at iclr 2017

manner. key applications will be the development of an attention function that segments the source
input into subsequences and one that takes into account the latent recursive structure (i.e. parse tree)
of a source sentence.
our approach views the attention mechanism as a graphical model over a set of latent variables. the
standard attention network can be seen as an expectation of an annotation function with respect to a
single latent variable whose categorical distribution is parameterized to be a function of the source.
in the general case we can specify a graphical model over multiple latent variables whose edges
encode the desired structure. computing forward attention requires performing id136 to obtain
the expectation of the annotation function, i.e. the context vector. this expectation is computed over
an exponentially-sized set of structures (through the machinery of id114/structured pre-
diction), hence the name structured attention network. notably each step of this process (including
id136) is differentiable, so the model can be trained end-to-end without having to resort to deep
id189 (schulman et al., 2015).
the differentiability of id136 algorithms over id114 has previously been noted by
various researchers (li & eisner, 2009; domke, 2011; stoyanov et al., 2011; stoyanov & eisner,
2012; gorid113y et al., 2015), primarily outside the area of deep learning. for example, gorid113y
et al. (2015) treat an entire graphical model as a differentiable circuit and backpropagate risk through
variational id136 (loopy belief propagation) for minimium risk training of dependency parsers.
our contribution is to combine these ideas to produce structured internal attention layers within
deep networks, noting that these approaches allow us to use the resulting marginals to create new
features, as long as we do so a differentiable way.
we focus on two classes of structured attention: linear-chain conditional random    elds (crfs) (laf-
ferty et al., 2001) and    rst-order graph-based dependency parsers (eisner, 1996). the initial work
of bahdanau et al. (2015) was particularly interesting in the context of machine translation, as the
model was able to implicitly learn an alignment model as a hidden layer, effectively embedding
id136 into a neural network. in similar vein, under our framework the model has the capacity
to learn a segmenter as a hidden layer or a parser as a hidden layer, without ever having to see a
segmented sentence or a parse tree. our experiments apply this approach to a dif   cult synthetic re-
ordering task, as well as to machine translation, id53, and natural language id136.
we    nd that models trained with structured attention outperform standard id12. analy-
sis of learned representations further reveal that interesting structures emerge as an internal layer of
the model. all code is available at http://github.com/harvardnlp/struct-attn.

2 background: attention networks

a standard neural network consist of a series of non-linear transformation layers, where each layer
produces a    xed-dimensional hidden representation. for tasks with large input spaces, this paradigm
makes it hard to control the interaction between components. for example in machine translation,
the source consists of an entire sentence, and the output is a prediction for each word in the translated
sentence. utilizing a standard network leads to an information bottleneck, where one hidden layer
must encode the entire source sentence. attention provides an alternative approach.1 an attention
network maintains a set of hidden representations that scale with the size of the source. the model
uses an internal id136 step to perform a soft-selection over these representations. this method
allows the model to maintain a variable-length memory and has shown to be crucially important for
scaling systems for many tasks.
formally, let x = [x1, . . . , xn] represent a sequence of inputs, let q be a query, and let z be a
categorical latent variable with sample space {1, . . . , n} that encodes the desired selection among
these inputs. our aim is to produce a context c based on the sequence and the query. to do so, we
assume access to an attention distribution z     p(z | x, q), where we condition p on the inputs x and
a query q. the context over a sequence is de   ned as expectation, c = ez   p(z | x,q)[f (x, z)] where
f (x, z) is an annotation function. attention of this form can be applied over any type of input,
however, we will primarily be concerned with    deep    networks, where both the annotation function

1another line of work involves marginalizing over latent variables (e.g. latent alignments) for sequence-to-

sequence transduction (kong et al., 2016; lu et al., 2016; yu et al., 2016; 2017).

2

published as a conference paper at iclr 2017

and attention distribution are parameterized with neural networks, and the context produced is a
vector fed to a downstream network.
for example, consider the case of attention-based id4 (bahdanau et al., 2015).
here the sequence of inputs [x1, . . . , xn] are the hidden states of a recurrent neural network (id56),
running over the words in the source sentence, q is the id56 hidden state of the target decoder
(i.e. vector representation of the query q), and z represents the source position to be attended to
for translation. the attention distribution p is simply p(z = i| x, q) = softmax(  i) where        rn
is a parameterized potential typically based on a neural network, e.g.   i = mlp([xi; q]). the
annotation function is de   ned to simply return the selected hidden state, f (x, z) = xz. the context
vector can then be computed using a simple sum,

c = ez   p(z | x,q)[f (x, z)] =

p(z = i| x, q)xi

(1)

other tasks such as id53 use attention in a similar manner, for instance by replacing
source [x1, . . . , xn] with a set of potential facts and q with a representation of the question.
in summary we interpret the attention mechanism as taking the expectation of an annotation function
f (x, z) with respect to a latent variable z     p, where p is parameterized to be function of x and q.

i=1

3 structured attention

n(cid:88)

attention networks simulate selection from a set using a soft model. in this work we consider gener-
alizing selection to types of attention, such as selecting chunks, segmenting inputs, or even attending
to latent subtrees. one interpretation of this attention is as using soft-selection that considers all pos-
sible structures over the input, of which there may be exponentially many possibilities. of course,
this expectation can no longer be computed using a simple sum, and we need to incorporate the
machinery of id136 directly into our neural network.
de   ne a structured attention model as being an attention model where z is now a vector of discrete
latent variables [z1, . . . , zm] and the attention distribution is p(z | x, q) is de   ned as a conditional
random    eld (crf), specifying the independence structure of the z variables. formally, we assume
an undirected graph structure with m vertices. the crf is parameterized with clique (log-)potentials
  c(zc)     r, where the zc indicates the subset of z given by clique c. under this de   nition, the
c   c(zc)), where for symmetry we
z(cid:48) exp(g(z(cid:48))) is
use softmax in a general sense, i.e. softmax(g(z)) = 1
the implied partition function. in practice we use a neural crf, where    comes from a deep model
over x, q.
in structured attention, we also assume that the annotation function f factors (at least) into clique
c fc(x, zc). under standard conditions on the conditional inde-
pendence structure, id136 techniques from id114 can be used to compute the forward-
pass expectations and the context:

attention id203 is de   ned as, p(z | x, q;   ) = softmax((cid:80)

annotation functions f (x, z) =(cid:80)

z exp(g(z)) where z =(cid:80)

c = ez   p(z | x,q)[f (x, z)] =

ez   p(zc | x,q)[fc(x, zc)]

3.1 example 1: subsequence selection

c

suppose instead of soft-selecting a single input, we wanted to explicitly model the selection of con-
tiguous subsequences. we could naively apply categorical attention over all subsequences, or hope
the model learns a multi-modal distribution to combine neighboring words. structured attention
provides an alternate approach.
concretely, let m = n, de   ne z to be a random vector z = [z1, . . . , zn] with zi     {0, 1}, and de   ne
i=1 fi(x, zi) where fi(x, zi) = 1{zi = 1}xi. the
n(cid:88)

our annotation function to be, f (x, z) = (cid:80)n

explicit expectation is then,

ez1,...,zn [f (x, z)] =

p(zi = 1| x, q)xi

(2)

i=1

3

(cid:88)

published as a conference paper at iclr 2017

q

z1

x1

x2

x3

x4

(a)

z1

x1

q

z2

x2

z3

x3

(b)

z4

x4

z1

x1

z2

x2

q

(c)

z3

x3

z4

x4

figure 1: three versions of a latent variable attention model: (a) a standard soft-selection attention network,
(b) a bernoulli (sigmoid) attention network, (c) a linear-chain structured attention model for segmentation.
the input and query are denoted with x and q respectively.

(cid:32)n   1(cid:88)

(cid:33)

equation (2) is similar to equation (1)   both are a linear combination of the input representations
where the scalar is between [0, 1] and represents how much attention should be focused on each
input. however, (2) is fundamentally different in two ways: (i) it allows for multiple inputs (or no
inputs) to be selected for a given query; (ii) we can incorporate structural dependencies across the
zi   s. for instance, we can model the distribution over z with a linear-chain crf with pairwise edges,

p(z1, . . . , zn | x, q) = softmax

  i,i+1(zi, zi+1)

(3)

i=1

where   k,l is the pairwise potential for zi = k and zi+1 = l. this model is shown in figure 1c.
compare this model to the standard attention in figure 1a, or to a simple bernoulli (sigmoid) selec-
tion method, p(zi = 1| x, q) = sigmoid(  i), shown in figure 1b. all three of these methods can
use potentials from the same neural network or id56 that takes x and q as inputs.
in the case of the linear-chain crf in (3), the marginal distribution p(zi = 1| x) can be calculated
ef   ciently in linear-time for all i using message-passing, i.e. the forward-backward algorithm. these
marginals allow us to calculate (2), and in doing so we implicitly sum over an exponentially-sized
set of structures (i.e. all binary sequences of length n) through id145. we refer to
this type of attention layer as a segmentation attention layer.
note that the forward-backward algorithm is being used as parameterized pooling (as opposed to
output computation), and can be thought of as generalizing the standard attention softmax. crucially
this generalization from vector softmax to forward-backward is just a series of differentiable steps,2
and we can compute gradients of its output (marginals) with respect to its input (potentials). this
will allow the structured attention model to be trained end-to-end as part of a deep model.

3.2 example 2: syntactic tree selection

this same approach can be used for more involved structural dependencies. one popular structure
for natural language tasks is a dependency tree, which enforces a structural bias on the recursive
dependencies common in many languages. in particular a dependency tree enforces that each word
in a source sentence is assigned exactly one parent word (head word), and that these assignments do
not cross (projective structure). employing this bias encourages the system to make a soft-selection
based on learned syntactic dependencies, without requiring linguistic annotations or a pipelined
decision.
a dependency parser can be partially formalized as a graphical model with the following cliques
(smith & eisner, 2008): latent variables zij     {0, 1} for all i (cid:54)= j, which indicates that the i-th
word is the parent of the j-th word (i.e. xi     xj); and a special global constraint that rules out
con   gurations of zij   s that violate parsing constraints (e.g. one head, projectivity).
the parameters to the graph-based crf dependency parser are the potentials   ij, which re   ect
the score of selecting xi as the parent of xj. the id203 of a parse tree z given the sentence

2as are other id145 algorithms for id136 in id114, such as (loopy and non-

loopy) belief propagation.

4

published as a conference paper at iclr 2017

procedure forwardbackward(  )

  [0,(cid:104)t(cid:105)]     0
  [n + 1,(cid:104)t(cid:105)]     0
for i = 1, . . . , n; c     c do

  [i, c]    (cid:76)
  [i, c]    (cid:76)

for i = n, . . . , 1; c     c do

y   [i    1, y]      i   1,i(y, c)
y   [i + 1, y]      i,i+1(c, y)

a       [n + 1,(cid:104)t(cid:105)]
for i = 1, . . . , n; c     c do
        a)

p(zi = c| x)     exp(  [i, c]       [i, c]

return p

procedure backpropforwardbackward(  , p,   l
p )

p               a
p               a

   l
       log p     log    l
   l
       log p     log    l
    [0,(cid:104)t(cid:105)]     0
    [n + 1,(cid:104)t(cid:105)]     0
for i = n, . . . 1; c     c do
    [i, c]        l

for i = 1, . . . , n; c     c do

  [i, c]    (cid:76)
   [i, c]    (cid:76)

for i = 1, . . . , n; y, c     c do

    [i, c]        l
   l
  i   1,i(y,c)     signexp(     [i, y]       [i + 1, c]

      [i, y]         [i + 1, c]
      [i, y]       [i + 1, c]        a)

return    l

  

y   i,i+1(c, y)         [i + 1, y]
y   i   1,i(y, c)         [i     1, y]

figure 2: algorithms for linear-chain crf: (left) computation of forward-backward tables   ,   , and marginal
probabilities p from potentials    (forward-backward algorithm); (right) id26 of loss gradients with
respect to the marginals    l
p . c denotes the state space and (cid:104)t(cid:105) is the special start/stop state. id26
uses the identity    l
, where (cid:12) is the element-wise multiplication.
typically the forward-backward with marginals is performed in the log-space semi   eld r   {     } with binary
operations     = logadd and     = + for numerical precision. however, id26 requires working with
the log of negative values (since    l
p could be negative), so we extend to a    eld [r     {     }]    {+,   } with
special +/    log-space operations. binary operations applied to vectors are implied to be element-wise. the
signexp function is de   ned as signexp(la) = sa exp(la). see section 3.3 and table 1 for more details.

p to calculate    l

log p = p(cid:12)   l

log p   log p

   =    l

  

x = [x1, . . . , xn] is,

p(z | x, q) = softmax

      1{z is valid}(cid:88)

i(cid:54)=j

      

1{zij = 1}  ij

(4)

where z is represented as a vector of zij   s for all i (cid:54)= j. it is possible to calculate the marginal
id203 of each edge p(zij = 1| x, q) for all i, j in o(n3) time using the inside-outside algorithm
(baker, 1979) on the data structures of eisner (1996).

the parsing contraints ensure that each word has exactly one head (i.e.(cid:80)n
n(cid:88)

we want to utilize the soft-head selection of a position j, the context vector is de   ned as:

i=1 zij = 1). therefore if

n(cid:88)

cj = ez[fj(x, z)] =

p(zij = 1| x, q)xi

1{zij = 1}xi

fj(x, z) =

i=1

i=1

note that in this case the annotation function has the subscript j to produce a context vector for
each word in the sentence. similar types of attention can be applied for other tree properties (e.g.
soft-children). we refer to this type of attention layer as a syntactic attention layer.

3.3 end-to-end training

id114 of this form have been widely used as the    nal layer of deep models. our contri-
bution is to argue that these networks can be added within deep networks in place of simple attention
layers. the whole model can then be trained end-to-end.
the main complication in utilizing this approach within the network itself is the need to backprop-
agate the gradients through an id136 algorithm as part of the structured attention network. past
work has demonstrated the techniques necessary for this approach (see stoyanov et al. (2011)), but
to our knowledge it is very rarely employed.
consider the case of the simple linear-chain crf layer from equation (3). figure 2 (left) shows the
standard forward-backward algorithm for computing the marginals p(zi = 1| x, q;   ). if we treat the
forward-backward algorithm as a neural network layer, its input are the potentials   , and its output

5

published as a conference paper at iclr 2017

sb

sa

   

   

la+b

sa+b

after the forward pass are these marginals.3 to backpropagate a loss through this layer we need to
compute the gradient of the loss l with respect to   ,    l
   , as a function of the gradient of the loss with
respect to the marginals,    l
p .4 as the forward-backward algorithm consists of differentiable steps,
this function can be derived using reverse-mode automatic differentiation of the forward-backward
algorithm itself. note that this reverse-mode algorithm conveniently has a parallel structure to the
forward version, and can also be implemented using id145.
however, in practice, one cannot simply
use current off-the-shelf tools for this task.
for one, ef   ciency is quite important for
these models and so the bene   ts of hand-
optimizing the reverse-mode implementa-
tion still outweighs simplicity of automatic
differentiation. secondly, numerical pre-
cision becomes a major issue for struc-
tured attention networks. for computing
the forward-pass and the marginals, it is im-
portant to use the standard log-space semi-
   eld over r     {     } with binary opera-
tions (    = logadd,    = +) to avoid un-
der   ow of probabilities. for computing the
backward-pass, we need to remain in log-
space, but also handle log of negative values (since    l
p could be negative). this requires extending
to the signed log-space semi   eld over [r     {     }]    {+,   } with special +/    operations. ta-
ble 1, based on li & eisner (2009), demonstrates how to handle this issue, and figure 2 (right)
describes id26 through the forward-backward algorithm. for id33, the
forward pass can be computed using the inside-outside implementation of eisner   s algorithm (eis-
ner, 1996). similarly, the id26 parallels the inside-outside structure. forward/backward
pass through the inside-outside algorithm is described in appendix b.

table 1:
signed log-space semi   eld (from li & eis-
ner (2009)). each real number a is represented as a pair
(la, sa) where la = log |a| and sa = sign(a). therefore
a = sa exp(la). for the above we let d = exp(lb     la) and
assume |a| > |b|.

+ + la + log(1 + d)
+     la + log(1     d)
    + la + log(1     d)
        la + log(1 + d)

la + lb
la + lb
la + lb
la + lb

+
+
   
   

+
   
   
+

sa  b

la  b

4 experiments

we experiment with three instantiations of structured attention networks on four different tasks: (a)
a simple, synthetic tree manipulation task using the syntactic attention layer, (b) machine translation
with segmentation attention (i.e. two-state linear-chain crf), (c) id53 using an n-
state linear-chain crf for multi-step id136 over n facts, and (d) natural language id136 with
syntactic tree attention. these experiments are not intended to boost the state-of-the-art for these
tasks but to test whether these methods can be trained effectively in an end-to-end fashion, can yield
improvements over standard selection-based attention, and can learn plausible latent structures. all
model architectures, hyperparameters, and training details are further described in appendix a.

4.1 tree transduction

the    rst set of experiments look at a tree-transduction task. these experiments use synthetic data
to explore a failure case of soft-selection id12. the task is to learn to convert a random
formula given in pre   x notation to one in in   x notation, e.g.,
(     ( + ( + 15 7 ) 1 8 ) ( + 19 0 11 ) )     ( ( 15 + 7 ) + 1 + 8 )     ( 19 + 0 + 11 )
the alphabet consists of symbols {(, ), +,   }, numbers between 0 and 20, and a special root symbol
$. this task is used as a preliminary task to see if the model is able to learn the implicit tree structure
on the source side. the model itself is an encoder-decoder model, where the encoder is de   ned
below and the decoder is an lstm. see appendix a.2 for the full model.

3confusingly,    forward    in this case is different than in the forward-backward algorithm, as the marginals
themselves are the output. however the two uses of the term are actually quite related. the forward-backward
algorithm can be interpreted as a forward and id26 pass on the log partition function. see eisner
(2016) for further details (appropriately titled    inside-outside and forward-backward algorithms are just
backprop   ). as such our full approach can be seen as computing second-order information. this interpretation
is central to li & eisner (2009).

4in general we use    a

b to denote the jacobian of a with respect to b.

6

published as a conference paper at iclr 2017

figure 3: visualization of the source self-attention distribution for the simple (left) and structured (right)
id12 on the tree transduction task. $ is the special root symbol. each row delineates the distribution
over the parents (i.e. each row sums to one). the attention distribution obtained from the parsing marginals are
more able to capture the tree structure   e.g. the attention weights of closing parentheses are generally placed
on the opening parentheses (though not necessarily on a single parenthesis).

training uses 15k pre   x-in   x pairs where the maximum nesting depth is set to be between 2-4 (the
above example has depth 3), with 5k pairs in each depth bucket. the number of expressions in each
parenthesis is limited to be at most 4. test uses 1k unseen sequences with depth between 2-6 (note
speci   cally deeper than train), with 200 sequences for each depth. the performance is measured
as the average proportion of correct target tokens produced until the    rst failure (as in grefenstette
et al. (2015)).
for experiments we try using different forms of self -attention over embedding-only encoders. let
xj be an embedding for each source symbol; our three variants of the source representation   xj are:
(a) no atten, just symbol embeddings by themselves, i.e.   xj = xj; (b) simple attention, symbol
i=1 softmax(  ij)xi
is calculated using soft-selection; (c) structured attention, symbol embeddings and soft-parent, i.e.
i=1 p(zij = 1| x)xi is calculated using parsing marginals, obtained
from the syntactic attention layer. none of these models use an explicit query value   the potentials
come from running a bidirectional lstm over the source, producing hidden vectors hi, and then
computing

embeddings and soft-pairing for each symbol, i.e.   xj = [xj; cj] where cj =(cid:80)n
  xj = [xj; cj] where cj = (cid:80)n

  ij = tanh(s(cid:62)

tanh(w1hi + w2hj + b))

where s, b, w1, w2 are parameters (see appendix a.1).

the source representation [  x1, . . . ,   xn] are attended
over using the standard attention mechanism at each
decoding step by an lstm decoder.5 additionally,
symbol embedding parameters are shared between the
parsing lstm and the source encoder.

depth no atten

simple

structured

2
3
4
5
6

7.6
4.1
2.8
2.1
1.5

87.4
49.6
23.3
15.0
8.5

99.2
87.0
64.5
30.8
18.2

table 2: performance (average length to fail-
ure %) of models on the tree-transduction task.

results table 2 has the results for the task. note
that this task is fairly dif   cult as the encoder is quite
simple. the baseline model (unsurprisingly) performs
poorly as it has no information about the source order-
ing. the simple attention model performs better, but
is signi   cantly outperformed by the structured model
with a tree structure bias. we hypothesize that the
model is partially reconstructing the arithmetic tree. figure 3 shows the attention distribution for the
simple/structured models on the same source sequence, which indicates that the structured model is
able to learn boundaries (i.e. parentheses).

5thus there are two attention mechanisms at work under this setup. first, structured attention over the
source only to obtain soft-parents for each symbol (i.e. self-attention). second, standard softmax alignment
attention over the source representations during decoding.

7

published as a conference paper at iclr 2017

4.2 id4

our second set of experiments use a full id4 model utilizing attention over
subsequences. here both the encoder/decoder are lstms, and we replace standard simple attention
with a segmentation attention layer. we experiment with two settings: translating directly from
unsegmented japanese characters to english words (effectively using structured attention to perform
soft id40), and translating from segmented japanese words to english words (which
can be interpreted as doing phrase-based id4). japanese id40
is done using the kytea toolkit (neubig et al., 2011).
the data comes from the workshop on asian translation (wat) (nakazawa et al., 2016). we
randomly pick 500k sentences from the original training set (of 3m sentences) where the japanese
sentence was at most 50 characters and the english sentence was at most 50 words. we apply the
same length    lter on the provided validation/test sets for evaluation. the vocabulary consists of all
tokens that occurred at least 10 times in the training corpus.
the segmentation attention layer is a two-state crf where the unary potentials at the j-th decoder
step are parameterized as

(cid:26)hiwhj, k = 1

  i(k) =

here [h1, . . . , hn] are the encoder hidden states and h(cid:48)
query vector). the pairwise potentials are parameterized linearly with b, i.e. all together

k = 0
j is the j-th decoder hidden state (i.e.

0,

the

  i,i+1(zi, zi+1) =   i(zi) +   i+1(zi+1) + bzi,zi+1

therefore the segmentation attention layer requires just 4 additional parameters. appendix a.3
describes the full model architecture.
we experiment with three attention con   gurations:

(a) standard simple attention, i.e. cj =
i=1 softmax(  i)hi; (b) sigmoid attention: multiple selection with bernoulli random variables,
i=1 sigmoid(  i)hi; (c) structured attention, encoded with normalized crf marginals,

(cid:80)n
i.e. cj =(cid:80)n

n(cid:88)

p(zi = 1| x, q)

cj =

i=1

  

hi

   =

1
  

p(zi = 1| x, q)

n(cid:88)

i=1

the id172 term    is not ideal but we found it to be helpful for stable training.6    is a
hyperparameter (we use    = 2) and we further add an l2 penalty of 0.005 on the pairwise potentials
b. these values were found via grid search on the validation set.

simple

14.6
14.3

12.6
14.1

13.1
13.8

sigmoid

structured

char
word

table 3: translation performance as mea-
sured by id7 (higher is better) on character-
to-word and word-to-word japanese-english
translation for the three different models.

results results for the translation task on the test
set are given in table 3. sigmoid attention outper-
forms simple (softmax) attention on the character-to-
word task, potentially because it is able to learn many-
to-one alignments. on the word-to-word task, the op-
posite is true, with simple attention outperforming sig-
moid attention. structured attention outperforms both
models on both tasks, although improvements on the
word-to-word task are modest and unlikely to be sta-
tistically signi   cant.
for further analysis, figure 4 shows a visualization of
the different attention mechanisms on the character-to-word setup. the simple model generally
focuses attention heavily on a single character. in contrast, the sigmoid and structured models are
able to spread their attention distribution on contiguous subsequences. the structured attention
learns additional parameters (i.e. b) to smooth out this type of attention.

i=1 p(zi = 1| x, q)hi) we empirically observed the marginals
to quickly saturate. we tried various strategies to overcome this, such as putting an l2 penalty on the unary
potentials and initializing with a pretrained sigmoid attention model, but simply normalizing the marginals
proved to be the most effective. however, this changes the interpretation of the context vector as the expectation
of an annotation function in this case.

6with standard expectation (i.e. cj = (cid:80)n

8

published as a conference paper at iclr 2017

figure 4: visualization of the source attention distribution for the simple (top left), sigmoid (top right), and
structured (bottom left) id12 over the ground truth sentence on the character-to-word translation
task. manually-annotated alignments are shown in bottom right. each row delineates the attention weights
over the source sentence at each step of decoding. the sigmoid/structured id12 are able learn an
implicit segmentation model and focus on multiple characters at each time step.

4.3 id53

our third experiment is on id53 (qa) with the linear-chain crf attention layer for
id136 over multiple facts. we use the babi dataset (weston et al., 2015), where the input is a set
of sentences/facts paired with a question, and the answer is a single token. for many of the tasks
the model has to attend to multiple supporting facts to arrive at the correct answer (see figure 5 for
an example), and existing approaches use multiple    hops    to greedily attend to different facts. we
experiment with employing structured attention to perform id136 in a non-greedy way. as the
ground truth supporting facts are given in the dataset, we are able to assess the model   s id136
accuracy.
the baseline (simple) attention model is the end-to-end memory network (sukhbaatar et al.,
2015) (memn2n), which we brie   y describe here. see appendix a.4 for full model details. let
x1, . . . , xn be the input embedding vectors for the n sentences/facts and let q be the query embed-
ding. in memn2n, zk is the random variable for the sentence to select at the k-th id136 step
(i.e. k-th hop), and thus zk     {1, . . . , n}. the id203 distribution over zk is given by p(zk =
i=1 p(zk = i| x, q)ok
i| x, q) = softmax((xk
i ,
where xk
i are the input and output embedding for the i-th sentence at the k-th hop, respectively.
the k-th context vector is used to modify the query qk+1 = qk + ck, and this process repeats for
k = 1, . . . , k (for k = 1 we have xk
i = xi, qk = q, ck = 0). the k-th context and query vectors
are used to obtain the    nal answer. the attention mechanism for a k-hop memn2n network can
therefore be interpreted as a greedy selection of a length-k sequence of facts (i.e. z1, . . . , zk).
for structured attention, we use an n-state, k-step linear-chain crf.7 we experiment with two
different settings: (a) a unary crf model with node potentials

i )(cid:62)qk), and the context vector is given by ck = (cid:80)n

i , ok

7note that this differs from the segmentation attention for the id4 experiments de-

scribed above, which was a k-state (with k = 2), n-step linear-chain crf.

  k(i) = (xk
i )

(cid:62)qk

9

published as a conference paper at iclr 2017

task
task 02 - two supporting facts
task 03 - three supporting facts
task 07 - counting
task 08 - lists sets
task 11 - indefinite knowledge
task 13 - compound coreference
task 14 - time reasoning
task 15 - basic deduction
task 16 - basic induction
task 17 - positional reasoning
task 18 - size reasoning
task 19 - path finding
average

memn2n

binary crf

unary crf

k ans % fact % ans % fact % ans % fact %
2
3
3
3
2
2
2
2
3
2
2
2
   

87.3
52.6
83.2
94.1
97.8
95.6
99.9
100.0
97.1
61.1
86.4
21.3

84.7
40.5
83.5
93.3
97.7
97.0
99.7
100.0
97.9
60.6
92.2
24.4

43.5
28.2
79.3
87.1
88.6
94.4
90.5
100.0
98.0
59.7
92.0
24.3

81.8
0.1
   
   
80.8
36.4
98.2
89.5
85.6
49.6
3.9
11.5

22.3
0.0
   
   
0.0
9.3
30.2
51.4
41.4
10.5
1.4
7.8

46.8
1.4
   
   
38.2
14.8
77.6
59.3
91.0
23.9
3.3
10.2

81.4

53.7

17.4

81.0

73.8

39.6

table 4: answer accuracy (ans %) and supporting fact selection accuracy (fact %) of the three qa models
on the 1k babi dataset. k indicates the number of hops/id136 steps used for each task. task 7 and 8 both
contain variable number of facts and hence they are excluded from the fact accuracy measurement. supporting
fact selection accuracy is calculated by taking the average of 10 best runs (out of 20) for each task.

and (b) a binary crf model with pairwise potentials
(cid:62)qk + (xk
i )

  k,k+1(i, j) = (xk
i )

(cid:62)xk+1

j + (xk+1

j

(cid:62)qk+1
)

both (a) and (b), a single context vector is computed: c = (cid:80)
f (x, z) factors over the components of z (e.g. f (x, z) =(cid:80)k
above sum in terms of marginals: c =(cid:80)k

the binary crf model is designed to test the model   s ability to perform sequential reasoning. for
p(z1, . . . , zk | x, q)f (x, z)
(unlike memn2n which computes k context vectors). evaluating c requires summing over all nk
(cid:80)n
possible sequences of length k, which may not be practical for large values of k. however, if
k=1 fk(x, zk)) then one can rewrite the
i=1 p(zk = i| x, q)fk(x, zk). in our experiments,

z1,...,zk

k=1

we use fk(x, zk) = ok

zk. all three models are described in further detail in appendix a.4.

results we use the version of the dataset with 1k questions for each task. since all models reduce
to the same network for tasks with 1 supporting fact, they are excluded from our experiments. the
number of hops (i.e. k) is task-dependent, and the number of memories (i.e. n) is limited to be
at most 25 (note that many question have less than 25 facts   e.g.
the example in figure 5 has 9
facts). due to high variance in model performance, we train 20 models with different initializations
for each task and report the test accuracy of the model that performed the best on a 10% held-out
validation set (as is typically done for babi tasks).
results of the three different models are shown in table 4. for correct answer seletion (ans %),
we    nd that memn2n and the binary crf model perform similarly while the unary crf model
does worse, indicating the importance of including pairwise potentials. we also assess each model   s
ability to attend to the correct supporting facts in table 4 (fact %). since ground truth supporting
facts are provided for each query, we can check the sequence accuracy of supporting facts for each
model (i.e. the rate of selecting the exact correct sequence of facts) by taking the highest id203
sequence   z = argmax p(z1, . . . , zk | x, q) from the model and checking against the ground truth.
overall the binary crf is able to recover supporting facts better than memn2n. this improvement
is signi   cant and can be up to two-fold as seen for task 2, 11, 13 & 17. however we observed that
on many tasks it is suf   cient to select only the last (or    rst) fact correctly to predict the answer,
and thus higher sequence selection accuracy does not necessarily imply better answer accuracy (and
vice versa). for example, all three models get 100% answer accuracy on task 15 but have different
supporting fact accuracies.
finally, in figure 5 we visualize of the output edge marginals produced by the binary crf model
for a single question in task 16. in this instance, the model is uncertain but ultimately able to select
the right sequence of facts 5     6     8.

10

published as a conference paper at iclr 2017

figure 5: visualization of the attention distribution over supporting fact sequences for an example question
in task 16 for the binary crf model. the actual question is displayed at the bottom along with the correct
answer and the ground truth supporting facts (5     6     8). the edges represent the marginal probabilities
p(zk, zk+1 | x, q), and the nodes represent the n supporting facts (here we have n = 9). the text for the
supporting facts are shown on the left. the top three most likely sequences are: p(z1 = 5, z2 = 6, z3 =
8| x, q) = 0.0564, p(z1 = 5, z2 = 6, z3 = 3| x, q) = 0.0364, p(z1 = 5, z2 = 2, z3 = 3| x, q) = 0.0356.

4.4 natural language id136

the    nal experiment looks at the task of natural language id136 (nli) with the syntactic atten-
tion layer. in nli, the model is given two sentences (hypothesis/premise) and has to predict their
relationship: entailment, contradiction, neutral.
for this task, we use the stanford nli dataset (bowman et al., 2015) and model our approach off
of the decomposable attention model of parikh et al. (2016). this model takes in the matrix of
id27s as the input for each sentence and performs inter-sentence attention to predict the
answer. appendix a.5 describes the full model.
as in the transduction task, we focus on modifying the input representation to take into account soft
parents via self-attention (i.e. intra-sentence attention). in addition to the three baselines described
for tree transduction (no attention, simple, structured), we also explore two additional settings: (d)
hard pipeline parent selection, i.e.   xj = [xj; xhead(j)], where head(j) is the index of xj   s parent8;
(e) pretrained structured attention: structured attention where the parsing layer is pretrained for one
epoch on a parsed dataset (which was enough for convergence).

results results of our models are shown in table 5. simple attention improves upon the no
attention model, and this is consistent with improvements observed by parikh et al. (2016) with
their intra-sentence attention model. the pipelined model with hard parents also slightly improves
upon the baseline. structured attention outperforms both models, though surprisingly, pretraining
the syntactic attention layer on the parse trees performs worse than training it from scratch   it is
possible that the pretrained attention is too strict for this task.
we also obtain the hard parse for an example sentence by running the viterbi algorithm on the
syntactic attention layer with the non-pretrained model:

8the parents are obtained from running the dependency parser of andor et al. (2016), available at

https://github.com/tensorflow/models/tree/master/syntaxnet

11

$themenare   ghtingoutsideadeli.published as a conference paper at iclr 2017

model
handcrafted features (bowman et al., 2015)
lstm encoders (bowman et al., 2015)
tree-based id98 (mou et al., 2016)
stack-augmented parser-interpreter neural net (bowman et al., 2016)
lstm with word-by-word attention (rockt  aschel et al., 2016)
matching lstms (wang & jiang, 2016)
decomposable attention over id27s (parikh et al., 2016)
decomposable attention + intra-sentence attention (parikh et al., 2016)
attention over constituency tree nodes (zhao et al., 2016)
neural tree indexers (munkhdalai & yu, 2016)
enhanced bilstm id136 model (chen et al., 2016)
enhanced bilstm id136 model + ensemble (chen et al., 2016)
no attention
no attention + hard parent
simple attention
structured attention
pretrained structured attention

accuracy %

78.2
80.6
82.1
83.2
83.5
86.1
86.3
86.8
87.2
87.3
87.7
88.3

85.8
86.1
86.2
86.8
86.5

table 5: results of our models (bottom) and others (top) on the stanford nli test set. our baseline model has
the same architecture as parikh et al. (2016) but the performance is slightly different due to different settings
(e.g. we train for 100 epochs with a batch size of 32 while parikh et al. (2016) train for 400 epochs with a batch
size of 4 using asynchronous sgd.)

despite being trained without ever being exposed to an explicit parse tree, the syntactic attention
layer learns an almost plausible dependency structure. in the above example it is able to correctly
identify the main verb fighting, but makes mistakes on determiners (e.g. head of the should be
men). we generally observed this pattern across sentences, possibly because the verb structure is
more important for the id136 task.

5 conclusion

this work outlines structured attention networks, which incorporate id114 to generalize
simple attention, and describes the technical machinery and computational techniques for backprop-
agating through models of this form. we implement two classes of structured attention layers: a
linear-chain crf (for id4 and id53) and a more complicated
   rst-order dependency parser (for tree transduction and natural language id136). experiments
show that this method can learn interesting structural properties and improve on top of standard mod-
els. structured attention could also be a way of learning latent labelers or parsers through attention
on other tasks.
it should be noted that the additional complexity in computing the attention distribution increases
run-time   for example, structured attention was approximately 5   slower to train than simple at-
tention for the id4 experiments, even though both attention layers have the
same asymptotic run-time (i.e. o(n)).
embedding differentiable id136 (and more generally, differentiable algorithms) into deep mod-
els is an exciting area of research. while we have focused on models that admit (tractable) exact
id136, similar technique can be used to embed approximate id136 methods. many optimiza-
tion algorithms (e.g. id119, lbfgs) are also differentiable (domke, 2012; maclaurin
et al., 2015), and have been used as output layers for id170 in energy-based models
(belanger & mccallum, 2016; wang et al., 2016). incorporating them as internal neural network
layers is an interesting avenue for future work.

acknowledgments

we thank tao lei, ankur parikh, tim vieira, matt gorid113y, andr  e martins, jason eisner, yoav
goldberg, and the anonymous reviewers for helpful comments, discussion, notes, and code. we
additionally thank yasumasa miyamoto for verifying japanese-english translations.

12

published as a conference paper at iclr 2017

references
daniel andor, chris alberti, david weiss, aliaksei severyn, alessandro presta, kuzman ganchev,
slav petrov, and michael collins. globally normalized transition-based neural networks. in
proceedings of acl, 2016.

dzmitry bahdanau, kyunghyun cho, and yoshua bengio. id4 by jointly

learning to align and translate. in proceedings of iclr, 2015.

james k. baker. trainable grammars for id103. speech communication papers for

the 97th meeting of the acoustical society, 1979.

david belanger and andrew mccallum. id170 energy networks. in proceedings of

icml, 2016.

samuel r. bowman, christopher d. manning, and christopher potts. tree-structured composition
in neural networks without tree-structured architectures. in proceedings of the nips workshop
on cognitive computation: integrating neural and symbolic approaches, 2015.

samuel r. bowman, jon gauthier, abhinav rastogi, raghav gupta, christopher d. manning, and
christopher potts. a fast uni   ed model for parsing and sentence understanding. in proceedings
of acl, 2016.

william chan, navdeep jaitly, quoc le, and oriol vinyals.

arxiv:1508.01211, 2015.

listen, attend and spell.

liang-chieh chen, alexander g. schwing, alan l. yuille, and raquel urtasun. learning deep

structured models. in proceedings of icml, 2015.

qian chen, xiaodan zhu, zhenhua ling, si wei, and hui jiang. enhancing and combining se-

quential and tree lstm for natural language id136. arxiv:1609.06038, 2016.

kyunghyun cho, aaron courville, and yoshua bengio. describing multimedia content using

attention-based encoder-decoder networks. in ieee transactions on multimedia, 2015.

jan chorowski, dzmitry bahdanau, dmitriy serdyuk, kyunghyun cho, and yoshua bengio.

attention-based models for id103. in proceedings of nips, 2015.

ronan collobert, jason weston, leon bottou, michael karlen, koray kavukcuoglu, and pavel
kuksa. natural language processing (almost) from scratch. journal of machine learning re-
search, 12:2493   2537, 2011.

trinh-minh-tri do and thierry arti  eres. neural id49. in proceedings of

aistats, 2010.

justin domke. parameter learning with truncated message-passing.

2011.

in proceedings of cvpr,

justin domke. generic methods for optimization-based modeling. in aistats, pp. 318   326, 2012.

john duchi, elad hazan, and yoram singer. adaptive subgradient methods for online learning

and stochastic optimization. journal of machine learning research, 12:2021   2159, 2011.

greg durrett and dan klein. neural crf parsing. in proceedings of acl, 2015.

jason m. eisner. three new probabilistic models for id33: an exploration. in

proceedings of acl, 1996.

jason m. eisner. inside-outside and forward-backward algorithms are just backprop. in proceed-

ings of id170 workshop at emnlp, 2016.

matthew r. gorid113y, mark dredze, and jason eisner. approximation-aware id33

by belief propagation. in proceedings of tacl, 2015.

alex graves, greg wayne, and ivo danihelka. id63s. arxiv:1410.5401, 2014.

13

published as a conference paper at iclr 2017

alex graves, greg wayne, malcolm reynolds, tim harley, ivo danihelka, agnieszka grabska-
barwinska, sergio gomez colmenarejo, edward grefenstette, tiago ramalho, john agapiou,
adria puigdomenech badia, karl moritz hermann, yori zwols, georg ostrovski, adam cain,
helen king, christopher summer   eld, phil blunsom, koray kavukcuoglu, and demis hassabis.
hybrid computing using a neural network with dynamic external memory. nature, october
2016.

edward grefenstette, karl moritz hermann, mustafa suleyman, and phil blunsom. learning to

transduce with unbounded memory. in proceedings of nips, 2015.

karl moritz hermann, tomas kocisky, edward grefenstette, lasse espeholt, will kay, mustafa
suleyman, and phil blunsom. teaching machines to read and comprehend. in proceedings of
nips, 2015.

max jaderberg, karen simonyan, andrea vedaldi, and andrew zisserman. deep structured output

learning for unconstrained text recognition. in proceedings of iclr, 2014.

diederik kingma and jimmy ba. adam: a method for stochastic optimization. in proceedings of

iclr, 2015.

eliyahu kipperwasser and yoav goldberg. simple and accurate id33 using bidi-

rectional lstm feature representations. in tacl, 2016.

lingpeng kong, chris dyer, and noah a. smith. segmental recurrent neural networks. in pro-

ceedings of iclr, 2016.

john lafferty, andrew mccallum, and fernando pereira. id49: probabilistic

models for segmenting and labeling sequence data. in proceedings of icml, 2001.

guillaume lample, miguel ballesteros, sandeep subramanian, kazuya kawakami, and chris dyer.

neural architectures for id39. in proceedings of naacl, 2016.

yann lecun, leon bottou, yoshua bengio, and patrick haffner. gradient-based learning applied

to document recognition. in proceedings of ieee, 1998.

zhifei li and jason eisner. first- and second-order expectation semirings with applications to

minimum-risk training on translation forests. in proceedings of emnlp 2009, 2009.

liang lu, lingpeng kong, chris dyer, noah a. smith, and steve renals. segmental recurrent
neural networks for end-to-end id103. in proceedings of interspeech, 2016.

minh-thang luong, hieu pham, and christopher d. manning. effective approaches to attention-

based id4. in proceedings of emnlp, 2015.

dougal maclaurin, david duvenaud, and ryan p. adams. gradient-based hyperparameter opti-

mization through reversible learning. in proceedings of icml, 2015.

lili mou, rui men, ge li, yan xu, lu zhang, rui yan, and zhi jin. natural language id136 by

tree-based convolution and heuristic matching. in proceedings of acl, 2016.

tsendsuren munkhdalai and hong yu.

arxiv:1607.04492, 2016.

neural tree indexers for text understanding.

toshiaki nakazawa, manabu yaguchi, kiyotaka uchimoto, masao utiyama, eiichiro sumita, sadao
kurohashi, and hitoshi isahara. aspec: asian scienti   c paper excerpt corpus. in nicoletta calzo-
lari (conference chair), khalid choukri, thierry declerck, marko grobelnik, bente maegaard,
joseph mariani, asuncion moreno, jan odijk, and stelios piperidis (eds.), proceedings of the
ninth international conference on language resources and evaluation (lrec 2016), pp. 2204   
2208, portoro, slovenia, may 2016. european language resources association (elra). isbn
978-2-9517408-9-1.

graham neubig, yosuke nakata, and shinsuke mori. pointwise prediction for robust, adaptable

japanese morphological analysis. in proceedings of acl, 2011.

14

published as a conference paper at iclr 2017

ankur p. parikh, oscar tackstrom, dipanjan das, and jakob uszkoreit. a decomposable attention

model for natural language id136. in proceedings of emnlp, 2016.

jian peng, liefeng bo, and jinbo xu. conditional neural fields. in proceedings of nips, 2009.

jeffrey pennington, richard socher, and christopher d. manning. glove: global vectors for word

representation. in proceedings of emnlp, 2014.

tim rockt  aschel, edward grefenstette, karl moritz hermann, tomas kocisky, and phil blunsom.

reasoning about entailment with neural attention. in proceedings of iclr, 2016.

john schulman, nicolas heess, theophane weber, and pieter abbeel. gradient estimation using
stochastic computation graphs. in advances in neural information processing systems, pp. 3528   
3536, 2015.

david a. smith and jason eisner. id33 as belief propagation. in proceedings of

emnlp, 2008.

veselin stoyanov and jason eisner. minimum-risk training of approximate crf-based nlp sys-

tems. in proceedings of naacl, 2012.

veselin stoyanov, alexander ropson, and jason eisner. empirical risk minimization of graphical
model parameters given approximate id136, decoding, and model structure. in proceedings
of aistats, 2011.

sainbayar sukhbaatar, arthur szlam, jason weston, and rob fergus. end-to-end memory net-

works. in proceedings of nips, 2015.

oriol vinyals, meire fortunato, and navdeep jaitly. id193. in proceedings of nips,

2015.

shenlong wang, sanja fidler, and raquel urtasun. proximal deep structured models. in proceed-

ings of nips, 2016.

shuohang wang and jing jiang. learning natural language id136 with lstm. in proceedings

of naacl, 2016.

jason weston, sumit chopra, and antoine bordes. memory networks. arxiv:1410.3916, 2014.

jason weston, antoine bordes, sumit chopra, alexander m rush, bart van merri  enboer, armand
joulin, and tomas mikolov. towards ai-complete id53: a set of prerequisite
toy tasks. arxiv preprint arxiv:1502.05698, 2015.

kelvin xu, jimma ba, ryan kiros, kyunghyun cho, aaron courville, ruslan salakhutdinov,
richard zemel, and yoshua bengio. show, attend and tell: neural image id134
with visual attention. in proceedings of icml, 2015.

lei yu, jan buys, and phil blunsom. online segment to segment neural transduction. in proceed-

ings of emnlp, 2016.

lei yu, phil blunsom, chris dyer, edward grefenstette, and tomas kocisky. the neural noisy

channel. in proceedings of iclr, 2017.

kai zhao, liang huang, and minbo ma. id123 with structured attentions and com-

position. in proceedings of coling, 2016.

15

published as a conference paper at iclr 2017

appendices

a model details

a.1 syntactic attention

the syntactic attention layer (for tree transduction and natural language id136) is similar to the
   rst-order graph-based dependency parser of kipperwasser & goldberg (2016). given an input sen-
tence [x1, . . . , xn] and the corresponding word vectors [x1, . . . , xn], we use a bidirectional lstm
to get the hidden states for each time step i     [1, . . . , n],

hfwd
i = lstm(xi, hfwd
i   1)

i = lstm(xi, hbwd
hbwd
i+1)

hi = [hfwd

i

; hbwd

i

]

where the forward and backward lstms have their own parameters. the score for xi     xj (i.e. xi
is the parent of xj), is given by an mlp

  ij = tanh(s(cid:62)

tanh(w1hi + w2hj + b))

these scores are used as input to the inside-outside algorithm (see appendix b) to obtain the prob-
ability of each word   s parent p(zij = 1| x), which is used to obtain the soft-parent cj for each word
xj. in the non-structured case we simply have p(zij = 1| x) = softmax(  ij).

a.2 tree transduction

let [x1, . . . , xn], [y1, . . . , ym] be the sequence of source/target symbols, with the associated embed-
dings [x1, . . . , xn], [y1, . . . , ym] with xi, yj     rl. in the simplest baseline model we take the source
representation to be the matrix of the symbol embeddings. the decoder is a one-layer lstm which
j     rl. the hidden states are combined
produces the hidden states h(cid:48)
with the input representation via a bilinear map w     rl  l to produce the attention distribution used
to obtain the vector mi, which is combined with the decoder hidden state as follows,

j = lstm(yj, h(cid:48)

j   1), with h(cid:48)

(cid:80)n

exp xiwh(cid:48)
k=1 exp xkwh(cid:48)

j

j

  i =

n(cid:88)

i=1

mi =

  ixi

  hj = tanh(u[mi; h(cid:48)
j])

here we have w     rl  l and u     r2l  l. finally,   hj is used to to obtain a distribution over the next
symbol yj+1,

p(yj+1 | x1, . . . , xn, y1, . . . , yj) = softmax(v  hj + b)
for structured/simple models, the j-th source representation are respectively

(cid:34)

n(cid:88)

(cid:35)

(cid:34)

n(cid:88)

  xi =

xi;

p(zki = 1| x) xk

  xi =

xi;

softmax(  ki) xk

k=1

k=1

(cid:35)

where   ij comes from the bidirectional lstm described in a.1. then   i and mi changed accord-
ingly,

(cid:80)n

exp   xiwh(cid:48)
k=1 exp   xkwh(cid:48)

j

j

  i =

n(cid:88)

i=1

mi =

  i   xi

note that in this case we have w     r2l  l and u     r3l  l. we use l = 50 in all our experiments.
the forward/backward lstms for the parsing lstm are also 50-dimensional. symbol embeddings
are shared between the encoder and the parsing lstms.
additional training details include: batch size of 20; training for 13 epochs with a learning rate
of 1.0, which starts decaying by half after epoch 9 (or the epoch at which performance does not
improve on validation, whichever comes    rst); parameter initialization over a uniform distribution
u [   0.1, 0.1]; gradient id172 at 1 (i.e. renormalize the gradients to have norm 1 if the l2
norm exceeds 1). decoding is done with id125 (beam size = 5).

16

n(cid:88)

i=1

cj =

sigmoid(  i)hi

n(cid:88)
(cid:26)hiwh(cid:48)

i=1

  i(k) =

0,

j, k = 1
k = 0

published as a conference paper at iclr 2017

a.3 id4

the baseline id4 system is from luong et al. (2015). let [x1, . . . , xn], [y1, . . . , ym] be the
source/target sentence, with the associated id27s [x1, . . . , xn], [y1, . . . , ym]. the en-
coder is an lstm over the source sentence, which produces the hidden states [h1, . . . , hn] where

hi = lstm(xi, hi   1)

j     rl. in the simple
and hi     rl. the decoder is another lstm which produces the hidden states h(cid:48)
attention case with categorical attention, the hidden states are combined with the input representation
via a bilinear map w     rl  l and this distribution is used to obtain the context vector at the j-th
time step,

  i = hiwh(cid:48)

j

cj =

softmax(  i)hi

the bernoulli attention network has the same   i but instead uses a sigmoid to obtain the weights of
the linear combination, i.e.,

and    nally, the structured attention model uses a bilinear map to parameterize one of the unary
potentials

  i,i+1(zi, zi+1) =   i(zi) +   i+1(zi+1) + bzi,zi+1

where b are the pairwise potentials. these potentials are used as inputs to the forward-backward
algorithm to obtain the marginals p(zi = 1| x, q), which are further normalized to obtain the context
vector

n(cid:88)

p(zi = 1| x, q)

cj =

i=1

  

hi

   =

1
  

p(zi = 1| x, q)

n(cid:88)

i

we use    = 2 and also add an l2 penalty of 0.005 on the pairwise potentials b. the context vector
is then combined with the decoder hidden state

  hj = tanh(u[cj; h(cid:48)
j])

and   hj is used to obtain the distribution over the next target word yj+1

p(yj+1 | x1, . . . , xn, y1, . . . yj) = softmax(v  hj + b)

the encoder/decoder lstms have 2 layers and 500 hidden units (i.e. l = 500).
additional training details include: batch size of 128; training for 30 epochs with a learning rate of
1.0, which starts decaying by half after the    rst epoch at which performance does not improve
on validation; dropout with id203 0.3; parameter initialization over a uniform distribution
u [   0.1, 0.1]; gradient id172 at 1. we generate target translations with id125 (beam
size = 5), and evaluate with multi-id7.perl from moses.9

a.4 id53

our baseline model (memn2n) is implemented following the same architecture as described in
sukhbaatar et al. (2015). in particular, let x = [x1, . . . , xn] represent the sequence of n facts with
the associated embeddings [x1, . . . , xn] and let q be the embedding of the query q. the embeddings

9 https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/

multi-id7.perl

17

published as a conference paper at iclr 2017

are obtained by simply adding the id27s in each sentence or query. the full model with
k hops is as follows:

n(cid:88)

p(zk = i| x, q) = softmax((xk
i )

(cid:62)qk)

ck =

p(zk = i| x, q)ok

i

i=1

qk+1 = qk + ck
p(y | x, q) = softmax(w(qk + ck))

i } are
where p(y | x, q) is the distribution over the answer vocabulary. at each layer, {xk
computed using embedding matrices xk and ok. we use the adjacent weight tying scheme from
the paper so that xk+1 = ok, wt = ok. x1 is also used to compute the query embedding at the
   rst hop. for k = 1 we have xk
for both the unary and the binary crf models, the same input fact and query representations are
computed (i.e. same embedding matrices with weight tying scheme). for the unary model, the
potentials are parameterized as

i = xi, qk = q, ck = 0.

i } and {ok

(cid:62)qk
and for the binary model we compute pairwise potentials as
(cid:62)xk+1

  k(i) = (xk
i )

  k,k+1(i, j) = (xk
i )

(cid:62)qk + (xk
i )

the qk   s are updated simply with a linear mapping, i.e.
qk+1 = qqk

j + (xk+1

j

(cid:62)qk+1
)

k(cid:88)

(cid:88)

in the case of the binary crf, to discourage the model from selecting the same fact again we
additionally set   k,k+1(i, i) =        for all i     {1, . . . , n}. given these potentials, we compute the
marginals p(zk = i, zk+1 = j | x, q) using the forward-backward algorithm, which is then used to
compute the context vector:

c =

p(z1, . . . , zk | x, q)f (x, z)

f (x, z) =

fk(x, zk)

fk(x, zk) = ok
zk

z1,...,zk

k=1

note that if f (x, z) factors over the components of z (as is the case above) then computing c only
requires evaluating the marginals p(zk | x, q).
finally, given the context vector the prediction is made in a similar fashion to memn2n:

p(y | x, q) = softmax(w(qk + c))

other training setup is similar to sukhbaatar et al. (2015): we use stochastic id119 with
learning rate 0.01, which is divided by 2 every 25 epochs until 100 epochs are reached. capacity
of the memory is limited to 25 sentences. the embedding vectors are of size 20 and gradients are
renormalized if the norm exceeds 40. all models implement position encoding, temporal encoding,
and linear start from the original paper. for linear start, the softmax(  ) function in the attention
layer is removed at the beginning and re-inserted after 20 epochs for memn2n, while for the crf
models we apply a log(softmax(  )) layer on the qk after 20 epochs. each model is trained separately
for each task.

a.5 natural language id136

our baseline model/setup is essentially the same as that of parikh et al. (2016).
let
[x1, . . . , xn], [y1, . . . , ym] be the premise/hypothesis, with the corresponding input representations
[x1, . . . , xn], [y1, . . . , ym]. the input representations are obtained by a linear transformation of
the 300-dimensional pretrained glove embeddings (pennington et al., 2014) after normalizing the
glove embeddings to have unit norm.10 the pretrained embeddings remain    xed but the linear layer

10we use the glove embeddings pretrained over the 840 billion word common crawl, publicly available at

http://nlp.stanford.edu/projects/glove/

18

published as a conference paper at iclr 2017

(which is also 300-dimensional) is trained. words not in the pretrained vocabulary are hashed to one
of 100 gaussian embeddings with mean 0 and standard deviation 1.
we concatenate each input representation with a convex combination of the other sentence   s input
representations (essentially performing inter-sentence attention), where the weights are determined
through a dot product followed by a softmax,

m(cid:88)

j=1

(cid:80)m

exp eij
k=1 exp eik

yj

(cid:34)

yj;

n(cid:88)

i=1

(cid:80)n

exp eij
k=1 exp ekj

xi

(cid:35)

eij = f (xi)

(cid:62)f (yj)

  xi =

here f (  ) is an mlp. the new representations are fed through another mlp g(  ), summed, combined
with the    nal mlp h(  ) and fed through a softmax layer to obtain a distribution over the labels l,

         yj =
m(cid:88)

      xi;
n(cid:88)

i=1

  x =

g(  xi)

  y =

g(  yj)

j=1

p(l | x1, . . . , xn, y1, . . . , ym) = softmax(vh([  x;   y]) + b)

all the mlps have 2-layers, 300 relu units, and dropout id203 of 0.2. for structured/simple
models, we    rst employ the bidirectional parsing lstm (see a.1) to obtain the scores   ij. in the
structured case each word representation is simply concatenated with its soft-parent

(cid:34)

(cid:34)

n(cid:88)

k=1

n(cid:88)

k=1

  xi =

xi;

p(zki = 1| x)xk

  xi =

xi;

(cid:80)n

exp   ki
l=1 exp   li

xk

(cid:35)

(cid:35)

and   xi (and analogously   yj) is used as the input to the above model. in the simple case (which
closely corresponds to the intra-sentence attention model of parikh et al. (2016)), we have

the id27s for the parsing lstms are also initialized with glove, and the parsing layer
is shared between the two sentences. the forward/backward lstms for the parsing layer are 100-
dimensional.
additional training details include: batch size of 32; training for 100 epochs with adagrad (duchi
et al., 2011) where the global learning rate is 0.05 and sum of gradient squared is initialized to
0.1; parameter intialization over a gaussian distribution with mean 0 and standard deviation 0.01;
gradient id172 at 5. in the pretrained scenario, pretraining is done with adam (kingma &
ba, 2015) with learning rate equal to 0.01, and   1 = 0.9,   2 = 0.999.

b forward/backward through the inside-outside algorithm

figure 6 shows the procedure for obtaining the parsing marginals from the input potentials. this
corresponds to running the inside-outside version of eisner   s algorithm (eisner, 1996). the inter-
mediate data structures used during the id145 algorithm are the (log) inside tables
  , and the (log) outside tables   . both   ,    are of size n   n   2   2, where n is the sentence length.
first two dimensions encode the start/end index of the span (i.e. subtree). the third dimension
encodes whether the root of the subtree is the left (l) or right (r) index of the span. the fourth
dimension indicates if the span is complete (1) or incomplete (0). we can calculate the marginal
distribution of each word   s parent (for all words) in o(n3) using this algorithm.
backward pass through the inside-outside algorithm is slightly more involved, but still takes o(n3)
time. figure 7 illustrates the backward procedure, which receives the gradient of the loss l with
respect to the marginals,    l
p , and computes the gradient of the loss with respect to the potentials
   l
   . the computations must be performed in the signed log-space semi   eld to handle log of negative
values. see section 3.3 and table 1 for more details.

19

published as a conference paper at iclr 2017

procedure insideoutside(  )

  ,              
for i = 1, . . . , n do
  [i, i, l, 1]     0
  [i, i, r, 1]     0

  [1, n, r, 1]     0
for k = 1, . . . , n do

for s = 1, . . . , n     k do

t     s + k

  [s, t, r, 0]    (cid:76)
  [s, t, l, 0]    (cid:76)
  [s, t, r, 1]    (cid:76)
  [s, t, l, 1]    (cid:76)

for k = n, . . . , 1 do

for s = 1, . . . , n     k do

t     s + k
for u = s + 1, . . . , t do

(cid:46) initialize log of inside (  ), outside (  ) tables

u   [s,t   1]   [s, u, r, 1]       [u + 1, t, l, 1]       st
u   [s,t   1]   [s, u, r, 1]       [u + 1, t, l, 1]       ts
u   [s+1,t]   [s, u, r, 0]       [u, t, r, 1]
u   [s,t   1]   [s, u, l, 1]       [u, t, l, 0]

(cid:46) inside step

(cid:46) outside step

if s > 1 then

  [s, u, r, 0]          [s, t, r, 1]       [u, t, r, 1]
  [u, t, r, 1]          [s, t, r, 1]       [s, u, r, 0]
for u = s, . . . , t     1 do

  [s, u, l, 1]          [s, t, l, 1]       [u, t, l, 0]
  [u, t, l, 0]          [s, t, l, 1]       [s, u, l, 1]

for u = s, . . . , t     1 do

if s > 1 then

  [s, u, r, 1]          [s, t, r, 0]       [u + 1, t, l, 1]       st
  [u + 1, t, l, 1]          [s, t, r, 0]       [s, u, r, 1]       st
for u = s, . . . , t     1 do

  [s, u, r, 1]          [s, t, l, 0]       [u + 1, t, l, 1]       ts
  [u + 1, t, l, 1]          [s, t, l, 0]       [s, u, r, 1]       ts

a       [1, n, r, 1]
for s = 1, . . . , n     1 do

for t = s + 1, . . . , n do

p[s, t]     exp(  [s, t, r, 0]       [s, t, r, 0]        a)
if s > 1 then

p[t, s]     exp(  [s, t, l, 0]       [s, t, l, 0]        a)

(cid:46) log partition
(cid:46) compute marginals. note that p[s, t] = p(zst = 1| x)

return p

figure 6: forward step of the syntatic attention layer to compute the marginals, using the inside-outside
algorithm (baker, 1979) on the data structures of eisner (1996). we assume the special root symbol is the    rst
element of the sequence, and that the sentence length is n. calculations are performed in log-space semi   eld
with     = logadd and     = + for numerical precision. a, b     c means a     c and b     c. a        b means
a     a     b.

20

published as a conference paper at iclr 2017

procedure backpropinsideoutside(  , p,   l
p )
(cid:46) id26 uses the identity    l

for s, t = 1, . . . , n; s (cid:54)= t do

(cid:46) initialize inside (   l

  ), outside (   l

   = (p (cid:12)    l
p )   log p
(cid:46)    = log(p (cid:12)    l
p )
   ) gradients, and log of    l
   and    l

(cid:46) backpropagate    to    l

  

  

  

(cid:46) backpropagate through outside step

(cid:46)   ,    are temporary values

  [t, u, r, 1]                 [s, u, r, 1]       [t, u, r, 1]

  [u, s, l, 1]                 [u, t, l, 1]       [u, s, l, 1]

  [t, u, l, 0]                 [s, u, l, 1]       [t, u, l, 1]

  [u, s, r, 0]                 [u, t, r, 1]       [u, s, r, 0]

   [u, t]                 
   [t, u]                 

   [s, u]                 
   [u, s]                 

p [s, t]

   , log    l

  [s, t]     log p[s, t]     log    l
   l
  ,   l
             
for s = 1, . . . , n     1 do
for t = s + 1, . . . , n do
   l
  [s, t, r, 0],   l
  [1, n, r, 1]             [s, t]
   l
if s > 1 then
  [s, t, l, 0],   l
   l
   l
  [1, n, r, 1]             [s, t]

for k = 1, . . . , n do

for s = 1, . . . , n     k do

   [s, t, r, 0]       [s, t]

   [s, t, l, 0]       [t, s]

t     s + k
          l
   [s, t, r, 0]       [s, t, r, 0]
for u = t, . . . , n do
   l
   [s, u, r, 1],   l
if s > 1 then
   [s, t, l, 0]       [s, t, l, 0]
          l
for u = 1, . . . , s do
   [u, t, l, 1],   l
   l
   [s, t, l, 1]       [s, t, l, 1]
          l
for u = t, . . . , n do
   [s, u, l, 1],   l
   l
for u = 1, . . . , s     1 do
         [u, t, r, 0]       [u, s     1, r, 1]       ut
   [u, t, r, 0],   l
   l
  [u, s     1, r, 1], log    l
         [u, t, l, 0]       [u, s     1, r, 1]       tu
  [u, s     1, r, 1], log    l
   [u, t, l, 0],   l
   l
   [s, t, r, 1]       [s, t, r, 1]
          l
for u = 1, . . . , s do
   l
   [u, t, r, 1],   l
for u = t + 1, . . . , n do
         [s, u, r, 0]       [t + 1, u, l, 1]       su
  [t + 1, u, l, 1], log    l
   [s, u, r, 0],   l
   l
         [s, u, l, 0]       [t + 1, u, l, 1]       us
   [s, u, l, 0],   l
  [t + 1, u, l, 1], log    l
   l
for s = 1, . . . , n     k do

t     s + k
  [s, t, r, 1]       [s, t, r, 1]
          l
for u = s + 1, . . . , t do
   l
  [u, t, r, 0],   l
if s > 1 then
  [s, t, l, 1]       [s, t, l, 1]
          l
for u = s, . . . , t     1 do
   l
  [s, u, l, 1],   l
          l
  [s, t, l, 0]       [s, t, l, 0]
for u = s, . . . , t     1 do
         [s, u, r, 1]       [u + 1, t, l, 1]       ts
   l
  [s, u, r, 1],   l
  [u + 1, t, l, 1], log    l
          l
  [s, t, r, 0]       [s, t, r, 0]
for u = s, . . . , t     1 do
         [s, u, r, 1]       [u + 1, t, l, 1]       st
   l
  [s, u, r, 1],   l
  [u + 1, t, l, 1], log    l

  [u, t, r, 1]                 [s, u, r, 0]       [u, t, r, 1]

  [u, t, l, 0]                 [s, u, l, 1]       [u, t, l, 0]

   [t, s]                 

   [s, t]                 

for k = n, . . . , 1 do

(cid:46) backpropagate through inside step

return signexp log    l

  

(cid:46) exponentiate log gradient, multiply by sign, and return    l

  

figure 7: id26 through the inside-outside algorithm to calculate the gradient with respect to the
input potentials.    a
   is the gradient with respect to   ).
a, b        c means a     a     c and b     b     c.

b denotes the jacobian of a with respect to b (so    l

21

