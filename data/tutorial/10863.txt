language as a latent variable:

discrete generative models for sentence compression

yishu miao1, phil blunsom1,2

1university of oxford, 2google deepmind

{yishu.miao, phil.blunsom}@cs.ox.ac.uk

6
1
0
2

 
t
c
o
4
1

 

 
 
]
l
c
.
s
c
[
 
 

2
v
7
1
3
7
0

.

9
0
6
1
:
v
i
x
r
a

abstract

in this work we explore deep generative mod-
els of text in which the latent representation
of a document is itself drawn from a discrete
language model distribution. we formulate a
variational auto-encoder for id136 in this
model and apply it to the task of compressing
sentences. in this application the generative
model    rst draws a latent summary sentence
from a background language model, and then
subsequently draws the observed sentence con-
ditioned on this latent summary. in our em-
pirical evaluation we show that generative for-
mulations of both abstractive and extractive
compression yield state-of-the-art results when
trained on a large amount of supervised data.
further, we explore semi-supervised compres-
sion scenarios where we show that it is possi-
ble to achieve performance competitive with
previously proposed supervised models while
training on a fraction of the supervised data.

1

introduction

the recurrent sequence-to-sequence paradigm for
id86 (kalchbrenner and blun-
som, 2013; sutskever et al., 2014) has achieved re-
markable recent success and is now the approach
of choice for applications such as machine transla-
tion (bahdanau et al., 2015), id134 (xu
et al., 2015) and id103 (chorowski et
al., 2015). while these models have developed so-
phisticated conditioning mechanisms, e.g. attention,
fundamentally they are discriminative models trained
only to approximate the conditional output distribu-
tion of strings. in this paper we explore modelling the

joint distribution of string pairs using a deep genera-
tive model and employing a discrete variational auto-
encoder (vae) for id136 (kingma and welling,
2014; rezende et al., 2014; mnih and gregor, 2014).
we evaluate our generative approach on the task of
sentence compression. this approach provides both
alternative supervised objective functions and the
opportunity to perform semi-supervised learning by
exploiting the vaes ability to marginalise the latent
compressed text for unlabelled data.

auto-encoders (rumelhart et al., 1985) are a typi-
cal neural network architecture for learning compact
data representations, with the general aim of perform-
ing id84 on embeddings (hinton
and salakhutdinov, 2006). in this paper, rather than
seeking to embed inputs as points in a vector space,
we describe them with explicit natural language sen-
tences. this approach is a natural    t for summarisa-
tion tasks such as sentence compression. according
to this, we propose a generative auto-encoding sen-
tence compression (asc) model, where we intro-
duce a latent language model to provide the variable-
length compact summary. the objective is to perform
bayesian id136 for the posterior distribution of
summaries conditioned on the observed utterances.
hence, in the framework of vae, we construct an in-
ference network as the variational approximation of
the posterior, which generates compression samples
to optimise the variational lower bound.

the most common family of variational auto-
encoders relies on the reparameterisation trick, which
is not applicable for our discrete latent language
model.
instead, we employ the reinforce al-
gorithm (mnih et al., 2014; mnih and gregor, 2014)

figure 1: auto-encoding sentence compression model

to mitigate the problem of high variance during
sampling-based variational id136. nevertheless,
when directly applying the id56 encoder-decoder to
model the variational distribution it is very dif   cult to
generate reasonable compression samples in the early
stages of training, since each hidden state of the se-
quence would have |v | possible words to be sampled
from. to combat this we employ id193
(vinyals et al., 2015) to construct the variational dis-
tribution. this biases the latent space to sequences
composed of words only appearing in the source
sentence (i.e.
the size of softmax output for each
state becomes the length of current source sentence),
which amounts to applying an extractive compression
model for the variational approximation.

in order to further boost the performance on sen-
tence compression, we employ a supervised forced-
attention sentence compression model
(fsc)
trained on labelled data to teach the asc model to
generate compression sentences. the fsc model
shares the pointer network of the asc model and
combines a softmax output layer over the whole vo-
cabulary. therefore, while training on the sentence-
compression pairs, it is able to balance copying a
word from the source sentence with generating it
from the background distribution. more importantly,
by jointly training on the labelled and unlabelled
datasets, this shared pointer network enables the
model to work in a semi-supervised scenario.
in

this case, the fsc teaches the asc to generate rea-
sonable samples, while the pointer network trained
on a large unlabelled data set helps the fsc model to
perform better abstractive summarisation.

in section 6, we evaluate the proposed model by
jointly training the generative (asc) and discrimina-
tive (fsc) models on the standard gigaword sentence
compression task with varying amounts of labelled
and unlabelled data. the results demonstrate that by
introducing a latent language variable we are able to
match the previous benchmakers with small amount
of the supervised data. when we employ our mixed
discriminative and generative objective with all of the
supervised data the model signi   cantly outperforms
all previously published results.

2 auto-encoding sentence compression

in this section, we introduce the auto-encoding sen-
tence compression model (figure 1)1 in the frame-
work of variational auto-encoders. the asc model
consists of four recurrent neural networks     an en-
coder, a compressor, a decoder and a language model.
let s be the source sentence, and c be the compres-
sion sentence. the compression model (encoder-
compressor) is the id136 network q  (c|s) that
takes source sentences s as inputs and generates
extractive compressions c. the reconstruction

1the language model, layer connections and decoder soft

attentions are omitted in figure 1 for clarity.

s0s1s2s1s3s2s4s3h1dh2dh3dh4ddecoders1s2s3s4h1ec1c2c3c1c2c0h1ch2eh3eh4eh2ch3cencodercompressorh0ccompression (id193)3  2  1  ^h2c^h3c^h1creconstruction (soft attention)model (compressor-decoder) is the generative net-
work p  (s|c) that reconstructs source sentences s
based on the latent compressions c. hence, the for-
ward pass starts from the encoder to the compressor
and ends at the decoder. as the prior distribution, a
language model p(c) is pre-trained to regularise the
latent compressions so that the samples drawn from
the compression model are likely to be reasonable
natural language sentences.

2.1 compression
for the compression model (encoder-compressor),
q  (c|s), we employ a pointer network consisting of a
bidirectional lstm encoder that processes the source
sentences, and an lstm compressor that generates
compressed sentences by attending to the encoded
source words.

let si be the words in the source sentences, he
i be
the corresponding state outputs of the encoder. he
i are
the concatenated hidden states from each direction:

i = f      enc((cid:126)h e
he

i   1, si)||f      enc( (cid:126)h e

i+1, si)

(1)

further, let cj be the words in the compressed sen-
tences, hc
j be the state outputs of the compressor. we
construct the predictive distribution by attending to
the words in the source sentences:

hc
j =fcom(hc

j   1, cj   1)

uj(i) =wt

3 tanh(w1hc
q  (cj|c1:j   1, s) = softmax(uj)

j +w2he
i )

(2)
(3)
(4)

where c0 is the start symbol for each compressed
sentence and hc
0 is initialised by the source sentence
vector of he|s|. in this case, all the words cj sampled
from q  (cj|c1:j   1, s) are the subset of the words
appeared in the source sentence (i.e. cj     s).
2.2 reconstruction
for the reconstruction model (compressor-decoder)
p  (s|c), we apply a soft attention sequence-to-
sequence model to generate the source sentence s
based on the compression samples c     q  (c|s).

let sk be the words in the reconstructed sentences
k be the corresponding state outputs of the

and hd
decoder:

hd
k = fdec(hd

k   1, sk   1)

(5)

in this model, we directly use the recurrent cell of
the compressor to encode the compression samples2:

  h

c
j =fcom(  h

c
j   1, cj)

(6)

c
where the state outputs   h
j corresponding to the word
inputs cj are different from the outputs hc
j in the
compression model, since we block the information
from the source sentences. we also introduce a start
symbol s0 for the reconstructed sentence and hd
0
c
is initialised by the last state output   h
|c|. the soft
attention model is de   ned as:

vk(j) =wt
6 tanh(w 4hd
  k(j) = softmax(vk(j))

k + w 5  h

dk =

  k(j)  h

c
j(vk(j))

(cid:88)|c|

j

c
j)

(7)
(8)

(9)

we then construct the predictive id203 distribu-
tion over reconstructed words using a softmax:

(10)

p  (sk|s1:k   1, c) = softmax(w 7dk)
id136

2.3
in the asc model there are two sets of parameters,   
and   , that need to be updated during id136. due
to the non-differentiability of the model, the repa-
rameterisation trick of the vae is not applicable in
this case. thus, we use the reinforce algorithm
(mnih et al., 2014; mnih and gregor, 2014) to reduce
the variance of the gradient estimator.

the variational lower bound of the asc model is:
l =eq  (c|s)[log p  (s|c)]     dkl[q  (c|s)||p(c)]
p  (s|c)p(c)dc = log p(s) (11)
(cid:54) log

(cid:90) q  (c|s)

q  (c|s)

therefore, by optimising the lower bound (eq. 11),
the model balances the selection of keywords for the
summaries and the ef   cacy of the composed com-
pressions, corresponding to the reconstruction error
and kl divergence respectively.

in practise, the pre-trained language model prior
p(c) prefers short sentences for compressions. as
one of the drawbacks of vaes, the kl divergence
term in the lower bound pushes every sample drawn
from the variational distribution towards the prior.

2the recurrent parameters of the compressor are not updated

by the gradients from the reconstruction model.

figure 2: forced attention sentence compression model

thus acting to regularise the posterior, but also to
restrict the learning of the encoder. if the estimator
keeps sampling short compressions during id136,
the lstm decoder would gradually rely on the con-
texts from the decoded words instead of the informa-
tion provided by the compressions, which does not
yield the best performance on sentence compression.
here, we introduce a co-ef   cient    to scale the
learning signal of the kl divergence:
l=eq  (c|s)[log p  (s|c)]     dkl[q  (c|s)||p(c)] (12)
although we are not optimising the exact variational
lower bound, the ultimate goal of learning an effec-
tive compression model is mostly up to the recon-
struction error. in section 6, we empirically apply
   = 0.1 for all the experiments on asc model. in-
terestingly,    controls the compression rate of the
sentences which can be a good point to be explored
in future work.

during the id136, we have different strategies
for updating the parameters of    and   . for the pa-
rameters    in the reconstruction model, we directly
update them by the gradients:

   l
     

(cid:88)

= eq  (c|s)[
    1
m

m

    log p  (s|c)
]
    log p  (s|c(m))

     

     

(13)

where we draw m samples c(m)     q  (c|s) indepen-
dently for computing the stochastic gradients.

for the parameters    in the compression model,

we    rstly de   ne the learning signal,
l(s, c) = log p  (s|c)       (log q  (c|s)     log p(c)).

then, we update the parameters    by:
    log q  (c|s)

= eq  (c|s)[l(s, c)

]

   l
     

(cid:88)

m

    1
m

[l(s, c(m))

]

(14)

     

    log q  (c(m)|s)

     

however, this gradient estimator has a big variance
because the learning signal l(s, c(m)) relies on the
samples from q  (c|s). therefore, following the re-
inforce algorithm, we introduce two baselines
b and b(s), the centred learning signal and input-
dependent baseline respectively, to help reduce the
variance.

here, we build an mlp to implement the input-
dependent baseline b(s). during training, we learn
the two baselines by minimising the expectation:

eq  (c|s)[(l(s, c)     b     b(s))2].
(cid:88)
hence, the gradients w.r.t.    are derived as,

(l(s, c(m))   b   b(s))

   l
     

    1
m

m

    log q  (c(m)|s)

     

(16)

(15)

which is basically a likelihood-ratio estimator.

3 forced-attention sentence compression
in neural variational id136, the effectiveness of
training largely depends on the quality of the in-
ference network gradient estimator. although we
introduce a biased estimator by using pointer net-
works, it is still very dif   cult for the compression
model to generate reasonable natural language sen-
tences at the early stage of learning, which results in

s1s2s3s4h1ec1c2c3c1c2c0h1ch2eh3eh4eh2ch3cencodercompresserh0c    123    1    23compression (combined id193)selected fromvhigh-variance for the gradient estimator. here, we
introduce our supervised forced-attention sentence
compression (fsc) model to teach the compression
model to generate coherent compressed sentences.

neither directly replicating the pointer network
of asc model, nor using a typical sequence-to-
sequence model, the fsc model employs a force-
attention strategy (figure 2) that encourages the com-
pressor to select words appearing in the source sen-
tence but keeps the original full output vocabulary
v . the force-attention strategy is basically a com-
bined pointer network that chooses whether to select
a word from the source sentence s or to predict a
word from v at each recurrent state. hence, the
combined pointer network learns to copy the source
words while predicting the word sequences of com-
pressions. by sharing the id193 between
the asc and fsc model, the biased estimator obtains
further positive biases by training on a small set of
labelled source-compression pairs.

here, the fsc model makes use of the compres-

sion model (eq. 1 to 4) in the asc model,

  j = softmax(uj),

(17)
where   j(i), i     (1, . . . ,|s|) denotes the id203
of selecting si as the prediction for cj.

on the basis of the pointer network, we further
introduce the id203 of predicting cj that is se-
lected from the full vocabulary,

j),

  j = softmax(w hc

(18)
where   j(w), w     (1, . . . ,|v |) denotes the probabil-
ity of selecting the wth from v as the prediction for
cj. to combine these two probabilities in the id56,
we de   ne a selection factor t for each state output,
which computes the semantic similarities between
the current state and the attention vector,

(cid:88)|s|

  j =

i

  j(i)he
i
j m hc

j).

tj =   (  t

(19)

(20)

hence, the id203 distribution over compressed
words is de   ned as,

(cid:26)tj  j(i) + (1     tj)  j(cj), cj = si

p(cj|c1:j   1, s)=

(1     tj)  j(cj),

cj (cid:54)    s
(21)

essentially, the fsc model is the extended compres-
sion model of asc by incorporating the pointer net-
work with a softmax output layer over the full vocab-
ulary. so we employ    to denote the parameters of
the fsc model p  (c|s), which covers the parameters
of the variational distribution q  (c|s).
4 semi-supervised training

as the auto-encoding sentence compression (asc)
model grants the ability to make use of an unla-
belled dataset, we explore a semi-supervised train-
ing framework for the asc and fsc models. in
this scenario we have a labelled dataset that contains
source-compression parallel sentences, (s, c)     l,
and an unlabelled dataset that contains only source
sentences s     u. the fsc model is trained on l so
that we are able to learn the compression model by
maximising the log-id203,

log p  (c|s).

(22)

(cid:88)

f =

(c,s)   l

(cid:88)

s   u

l=

(cid:88)

s   u
+

j=

while the asc model is trained on u, where we
maximise the modi   ed variational lower bound,

(eq  (c|s)[log p  (s|c)]     dkl[q  (c|s)||p(c)]).
(23)

the joint objective function of the semi-supervised
learning is,

(eq  (c|s)[log p  (s|c)]     dkl[q  (c|s)||p(c)])
(cid:88)

log p  (c|s).

(24)

(c,s)   l

hence, the pointer network is trained on both un-
labelled data, u, and labelled data, l, by a mixed
criterion of reinforce and cross-id178.

5 related work

as one of the typical sequence-to-sequence tasks,
sentence-level summarisation has been explored by a
series of discriminative encoder-decoder neural mod-
els. filippova et al. (2015) carries out extractive
summarisation via deletion with lstms, while rush
et al. (2015) applies a convolutional encoder and an

attentional feed-forward decoder to generate abstrac-
tive summarises, which provides the benchmark for
the gigaword dataset. nallapati et al. (2016) further
improves the performance by exploring multiple vari-
ants of id56 encoder-decoder models. the recent
works gulcehre et al. (2016), ling et al. (2016), nal-
lapati et al. (2016) and gu et al. (2016) also apply
the similar idea of combining id193 and
softmax output. however, different from all these
discriminative models above, we explore generative
models for sentence compression. instead of training
the discriminative model on a big labelled dataset, our
original intuition of introducing a combined pointer
networks is to bridge the unsupervised generative
model (asc) and supervised model (fsc) so that we
could utilise a large additional dataset, either labelled
or unlabelled, to boost the compression performance.
dai and le (2015) also explored semi-supervised se-
quence learning, but in a pure deterministic model
focused on learning better vector representations.

recently variational auto-encoders have been ap-
plied in a variety of    elds as deep generative mod-
els. in id161 kingma and welling (2014),
rezende et al. (2014), and gregor et al. (2015) have
demonstrated strong performance on the task of im-
age generation and eslami et al. (2016) proposed
variable-sized variational auto-encoders to identify
multiple objects in images. while in natural language
processing, there are variants of vaes on modelling
documents (miao et al., 2016), sentences (bowman
et al., 2015) and discovery of relations (marcheg-
giani and titov, 2016). apart from the typical initi-
ations of vaes, there are also a series of works that
employs generative models for supervised learning
tasks. for instance, ba et al. (2015) learns visual
attention for multiple objects by optimising a varia-
tional lower bound, kingma et al. (2014) implements
a semi-supervised framework for image classi   cation
and miao et al. (2016) applies a conditional varia-
tional approximation in the task of factoid question
answering. dyer et al. (2016) proposes a generative
model that explicitly extracts syntactic relationships
among words and phrases which further supports the
argument that generative models can be a statistically
ef   cient method for learning neural networks from
small data.

6 experiments
6.1 dataset & setup
we evaluate the proposed models on the standard gi-
gaword3 sentence compression dataset. this dataset
was generated by pairing the headline of each article
with its    rst sentence to create a source-compression
pair. rush et al. (2015) provided scripts4 to    lter
out outliers, resulting in roughly 3.8m training pairs,
a 400k validation set, and a 400k test set. in the
following experiments all models are trained on the
training set with different data sizes5 and tested on a
2k subset, which is identical to the test set used by
rush et al. (2015) and nallapati et al. (2016). we
decode the sentences by k = 5 id125 and test
with full-length id8 score.

for the asc and fsc models, we use 256 for the
dimension of both hidden units and lookup tables.
in the asc model, we apply a 3-layer bidirectional
id56 with skip connections as the encoder, a 3-layer
id56 pointer network with skip connections as the
compressor, and a 1-layer vanilla id56 with soft at-
tention as the decoder. the language model prior is
trained on the article sentences of the full training
set using a 3-layer vanilla id56 with 0.5 dropout. to
lower the computational cost, we apply different vo-
cabulary sizes for encoder and compressor (119,506
and 68,897) which corresponds to the settings of
rush et al. (2015). speci   cally, the vocabulary of
the decoder is    ltered by taking the most frequent
10,000 words from the vocabulary of the encoder,
where the rest of the words are tagged as    <unk>   .
in further consideration of ef   ciency, we use only
one sample for the gradient estimator. we optimise
the model by adam (kingma and ba, 2015) with a
0.0002 learning rate and 64 sentences per batch. the
model converges in 5 epochs. except for the pre-
trained language model, we do not use dropout or
embedding initialisation for asc and fsc models.

6.2 extractive summarisation
the    rst set of experiments evaluate the models on
extractive summarisation. here, we denote the joint

3https://catalog.ldc.upenn.edu/ldc2012t21
4https://github.com/facebook/namas
5the hyperparameters where tuned on the validation set to
maximise the perplexity of the summaries rather than the recon-
structed source sentences.

precision

model

training data

-

r-1

r-1

fsc

fsc

500k
3.8m

labelled unlabelled

asc+fsc1
asc+fsc2

asc+fsc1
asc+fsc2

500k
500k
500k
1m
1m
1m
3.8m
3.8m

r-l
21.468
24.874
25.452
24.711
25.148
26.145
28.035
28.366
asc+fsc1
table 1: extractive summarisation performance. (1) the extractive summaries of these models are decoded
by the pointer network (i.e the shared component of the asc and fsc models). (2) r-1, r-2 and r-l
represent the id8-1, id8-2 and id8-l score respectively.

r-l
28.263
26.811
26.218
28.257
28.097
26.801
27.889
27.805

r-1
30.817
29.117
28.236
30.889
30.490
29.034
30.112
29.946

r-2
7.998
10.575
11.131
10.266
10.799
11.521
13.813
14.699

r-l
20.520
26.344
27.896
24.916
25.943
28.658
31.704
32.972

22.357
28.558
30.112
27.169
28.109
31.037
34.135
35.538

23.415
26.987
27.453
26.984
27.258
28.336
30.225
30.568

-
1m
3.8m

3.8m

fsc

-

recall
r-2
10.861
10.643
10.359
11.645
11.443
10.780
12.436
12.558

f-1
r-2
8.156
9.741
9.902
10.028
10.189
10.313
12.258
12.553

models by asc+fsc1 and asc+fsc2 where asc
is trained on unlabelled data and fsc is trained on
labelled data. the asc+fsc1 model employs equiv-
alent sized labelled and unlabelled datasets, where
the article sentences of the unlabelled data are the
same article sentences in the labelled data, so there
is no additional unlabelled data applied in this case.
the asc+fsc2 model employs the full unlabelled
dataset in addition to the existing labelled dataset,
which is the true semi-supervised setting.

table 1 presents the test id8 score on extractive
compression. we can see that the asc+fsc1 model
achieves signi   cant improvements on f-1 scores
when compared to the supervised fsc model only
trained on labelled data. moreover,    xing the labelled
data size, the asc+fsc2 model achieves better per-
formance by using additional unlabelled data than the
asc+fsc1 model, which means the semi-supervised
learning works in this scenario. interestingly, learn-
ing on the unlabelled data largely increases the preci-
sions (though the recalls do not bene   t from it) which
leads to signi   cant improvements on the f-1 id8
scores. and surprisingly, the extractive asc+fsc1
model trained on full labelled data outperforms the
abstractive nabs (rush et al., 2015) baseline model
(in table 4).

6.3 abstractive summarisation

the second set of experiments evaluate performance
on abstractive summarisation (table 2). consistently,
we see that adding the generative objective to the
discriminative model (asc+fsc1) results in a sig-
ni   cant boost on all the id8 scores, while em-
ploying extra unlabelled data increase performance

further (asc+fsc2). this validates the effectiveness
of transferring the knowledge learned on unlabelled
data to the supervised abstractive summarisation.

in figure 3, we present the validation perplexity
to compare the abilities of the three models to learn
the compression languages. the asc+fsc1(red)
employs the same dataset for unlabelled and labelled
training, while the asc+fsc2(black) employs the
full unlabelled dataset. here, the joint asc+fsc1
model obtains better perplexities than the single dis-
criminative fsc model, but there is not much dif-
ference between asc+fsc1 and asc+fsc2 when
the size of the labelled dataset grows. from the per-
spective of language modelling, the generative asc
model indeed helps the discriminative model learn to
generate good summary sentences. table 3 displays
the validation perplexities of the benchmark models,
where the joint asc+fsc1 model trained on the full
labelled and unlabelled datasets performs the best on
modelling compression languages.

table 4 compares the test id8 score on ab-
stractive summarisation. encouragingly, the semi-
supervised model asc+fsc2 outperforms the base-
line model nabs when trained on 500k supervised
pairs, which is only about an eighth of the super-
vised data. in nallapati et al. (2016), the authors
exploit the full limits of discriminative id56 encoder-
decoder models by incorporating a sampled soft-
max, expanded vocabulary, additional lexical fea-
tures, and combined id1936, which yields
the best performance listed in table 4. however,
when all the data is employed with the mixed ob-

6the idea of the combined id193 is similar to the

fsc model, but the implementations are slightly different.

precision

model

training data

recall
r-2

-

r-1

r-1

r-2

r-1

fsc

fsc

500k
3.8m

labelled unlabelled

asc+fsc1
asc+fsc2

asc+fsc1
asc+fsc2

500k
500k
500k
1m
1m
1m
3.8m
3.8m

r-l
26.955
27.072
27.99
27.439
28.431
28.967
30.087
31.915
asc+fsc1
table 2: abstractive summarisation performance. the abstractive summaries of these models are decoded by
the combined pointer network (i.e. the shared pointer network together with the softmax output layer over the
full vocabulary).

r-l
31.288
31.585
33.212
30.741
33.306
33.626
34.405
36.662

r-l
25.197
25.239
25.703
26.478
26.367
27.067
28.954
30.246

13.019
13.678
14.537
13.422
15.243
14.988
16.127
18.382

27.147
27.067
27.662
28.521
28.333
29.017
31.148
32.385

10.039
10.717
11.102
11.308
11.814
12.007
13.553
15.155

33.781
33.893
35.756
33.132
35.860
36.128
36.917
39.224

29.074
29.027
30.140
29.580
30.569
31.089
32.327
34.156

10.842
11.461
12.051
11.807
12.743
12.785
14.000
15.935

-
1m
3.8m

3.8m

fsc

-

f-1
r-2

model

labelled data perplexity

bag-of-word (bow)
convolutional (tdnn)
attention-based (nabs)
(rush et al., 2015)

forced-attention (fsc)
auto-encoding (asc+fsc1)

3.8m
3.8m
3.8m

3.8m
3.8m

43.6
35.9
27.1

18.6
16.6

table 3: comparison on validation perplexity. bow,
tdnn and nabs are the baseline neural compres-
sion models with different encoders in rush et al.
(2015)

labelled data r-1

model

(rush et al., 2015)

(nallapati et al., 2016)

3.8m
3.8m
500k
1m
3.8m

r-2

r-l
29.78 11.89 26.97
33.17 16.02 30.98
30.14 12.05 27.99
31.09 12.79 28.97
34.17 15.94 31.92

asc + fsc2
asc + fsc2
asc + fsc1
table 4: comparison on test id8 scores

jective asc+fsc1 model, the result is signi   cantly
better than this previous state-of-the-art. as the semi-
supervised asc+fsc2 model can be trained on un-
limited unlabelled data, there is still signi   cant space
left for further performance improvements.

table 5 presents the examples of the compression
sentences decoded by the joint model asc+fsc1
and the fsc model trained on the full dataset.

7 discussion

from the perspective of generative models, a sig-
ni   cant contribution of our work is a process for
reducing variance for discrete sampling-based vari-
ational id136. the    rst step is to introduce two
baselines in the control variates method due to the
fact that the reparameterisation trick is not applica-

figure 3: perplexity on validation dataset.

ble for discrete latent variables. however it is the
second step of using a pointer network as the biased
estimator that makes the key contribution. this re-
sults in a much smaller state space, bounded by the
length of the source sentence (mostly between 20
and 50 tokens), compared to the full vocabulary. the
   nal step is to apply the fsc model to transfer the
knowledge learned from the supervised data to the
pointer network. this further reduces the sampling
variance by acting as a sort of bootstrap or constraint
on the unsupervised latent space which could encode
almost anything but which thus becomes biased to-
wards matching the supervised distribution. by using
these variance reduction methods, the asc model is
able to carry out effective variational id136 for the
latent language model so that it learns to summarise
the sentences from the large unlabelled training data.
in a different vein, according to the reinforce-
ment learning interpretation of sequence level train-
ing (ranzato et al., 2016), the compression model
of the asc model acts as an agent which iteratively
generates words (takes actions) to compose the com-

0500k1m2m4mlabelled data size20406080100perplexity1004935.427.418.687.7433225.216.683.342.533.625.416.6fscasc+fsc1asc+fsc2pression sentence and the reconstruction model acts
as the reward function evaluating the quality of the
compressed sentence which is provided as a reward
signal. ranzato et al. (2016) presents a thorough
empirical evaluation on three different nlp tasks by
using additional sequence-level reward (id7 and
id8-2) to train the models. in the context of this
paper, we apply a variational lower bound (mixed re-
construction error and kl divergence regularisation)
instead of the explicit id8 score. thus the asc
model is granted the ability to explore unlimited unla-
belled data resources. in addition we introduce a su-
pervised fsc model to teach the compression model
to generate stable sequences instead of starting with
a random policy. in this case, the pointer network
that bridges the supervised and unsupervised model
is trained by a mixed criterion of reinforce and
cross-id178 in an incremental learning framework.
eventually, according to the experimental results, the
joint asc and fsc model is able to learn a robust
compression model by exploring both labelled and
unlabelled data, which outperforms the other sin-
gle discriminative compression models that are only
trained by cross-id178 reward signal.

8 conclusion

in this paper we have introduced a generative model
for jointly modelling pairs of sequences and evalu-
ated its ef   cacy on the task of sentence compression.
the variational auto-encoding framework provided
an effective id136 algorithm for this approach
and also allowed us to explore combinations of dis-
criminative (fsc) and generative (asc) compression
models. the evaluation results show that supervised
training of the combination of these models improves
upon the state-of-the-art performance for the giga-
word compression dataset. when we train the su-
pervised fsc model on a small amount of labelled
data and the unsupervised asc model on a large
set of unlabelled data the combined model is able to
outperform previously reported benchmarks trained
on a great deal more supervised data. these results
demonstrate that we are able to model language as a
discrete latent variable in a variational auto-encoding
framework and that the resultant generative model is
able to effectively exploit both supervised and unsu-
pervised data in sequence-to-sequence tasks.

src

the sri lankan government on wednesday announced the closure of
government schools with immediate effect as a military campaign
against tamil separatists escalated in the north of the country .
sri lanka closes schools as war escalates

ref
asca sri lanka closes government schools
asce sri lankan government closure schools escalated
fsca sri lankan government closure with tamil rebels closure
src

factory orders for manufactured goods rose #.# percent in septem-
ber , the commerce department said here thursday .
us september factory orders up #.# percent

ref
asca us factory orders up #.# percent in september
asce factory orders rose #.# percent in september
fsca factory orders #.# percent in september
src

hong kong signed a breakthrough air services agreement with the
united states on friday that will allow us airlines to carry freight to
asian destinations via the territory .
hong kong us sign breakthrough aviation pact

ref
asca us hong kong sign air services agreement
asce hong kong signed air services agreement with united states
fsca hong kong signed air services pact with united states
src

a swedish un soldier in bosnia was shot and killed by a stray bul-
let on tuesday in an incident authorities are calling an accident ,
military of   cials in stockholm said tuesday .
swedish un soldier in bosnia killed by stray bullet

ref
asca swedish un soldier killed in bosnia
asce swedish un soldier shot and killed
fsca swedish soldier shot and killed in bosnia
src

tea scores on the fourth day of the second test between australia
and pakistan here monday .
australia vs pakistan tea scorecard

ref
asca australia v pakistan tea scores
asce australia tea scores
fsca tea scores on #th day of #nd test
src

india won the toss and chose to bat on the opening day in the
opening test against west indies at the antigua recreation ground
on friday .
ref
india win toss and elect to bat in    rst test
asca india win toss and bat against west indies
asce india won toss on opening day against west indies
fsca india chose to bat on opening day against west indies
src

a powerful bomb exploded outside a navy base near the sri lankan
capital colombo tuesday , seriously wounding at least one person ,
military of   cials said .
bomb attack outside srilanka navy base

ref
asca bomb explodes outside sri lanka navy base
asce bomb outside sri lankan navy base wounding one
fsca bomb exploded outside sri lankan navy base
src

press freedom in algeria remains at risk despite the release on
wednesday of prominent newspaper editor mohamed <unk> after
a two-year prison sentence , human rights organizations said .
algerian press freedom at risk despite editor    s release <unk>
picture

ref

asca algeria press freedom remains at risk
asce algeria press freedom remains at risk
fsca press freedom in algeria at risk
table 5: examples of the compression sentences.
src and ref are the source and reference sentences
provided in the test set. asca and asce are the abstrac-
tive and extractive compression sentences decoded
by the joint model asc+fsc1, and fsca denotes the
abstractive compression obtained by the fsc model.

references
[ba et al.2015] jimmy ba, volodymyr mnih, and koray
kavukcuoglu. 2015. multiple object recognition with
visual attention. in proceedings of iclr.

[bahdanau et al.2015] dzmitry bahdanau, kyunghyun
cho, and yoshua bengio. 2015. neural machine trans-
lation by jointly learning to align and translate.
in
proceedings of iclr.

[bowman et al.2015] samuel r bowman, luke vilnis,
oriol vinyals, andrew m dai, rafal jozefowicz, and
samy bengio. 2015. generating sentences from a
continuous space. arxiv preprint arxiv:1511.06349.

[chorowski et al.2015] jan k chorowski, dzmitry bah-
danau, dmitriy serdyuk, kyunghyun cho, and yoshua
bengio. 2015. attention-based models for speech
recognition. in proceedings of nips, pages 577   585.
[dai and le2015] andrew m dai and quoc v le. 2015.
semi-supervised sequence learning. in proceedings of
nips, pages 3061   3069.

[dyer et al.2016] chris dyer, adhiguna kuncoro, miguel
ballesteros, and noah a smith. 2016. recurrent neural
network grammars. in proceedings of naacl.

[eslami et al.2016] sm eslami, nicolas heess, theophane
weber, yuval tassa, koray kavukcuoglu, and geof-
frey e hinton. 2016. attend, infer, repeat: fast scene
understanding with generative models. arxiv preprint
arxiv:1603.08575.

[filippova et al.2015] katja filippova, enrique alfonseca,
carlos a colmenares, lukasz kaiser, and oriol vinyals.
2015. sentence compression by deletion with lstms. in
proceedings of emnlp, pages 360   368.

[gregor et al.2015] karol gregor, ivo danihelka, alex
graves, and daan wierstra. 2015. draw: a recurrent
neural network for image generation. in proceedings
of icml.

[gu et al.2016] jiatao gu, zhengdong lu, hang li, and
victor ok li. 2016. incorporating copying mecha-
nism in sequence-to-sequence learning. arxiv preprint
arxiv:1603.06393.

[gulcehre et al.2016] caglar gulcehre, sungjin ahn,
ramesh nallapati, bowen zhou, and yoshua bengio.
2016. pointing the unknown words. arxiv preprint
arxiv:1603.08148.

[hinton and salakhutdinov2006] geoffrey e hinton and
ruslan r salakhutdinov. 2006. reducing the di-
mensionality of data with neural networks. science,
313(5786):504   507.

[kalchbrenner and blunsom2013] nal kalchbrenner and
phil blunsom. 2013. recurrent continuous translation
models. in proceedings of emnlp.

[kingma and ba2015] diederik p. kingma and jimmy ba.
2015. adam: a method for stochastic optimization. in
proceedings of iclr.

[kingma and welling2014] diederik p kingma and max
welling. 2014. auto-encoding id58. in
proceedings of iclr.

[kingma et al.2014] diederik p kingma, shakir mo-
hamed, danilo jimenez rezende, and max welling.
2014. semi-supervised learning with deep generative
models. in proceedings of nips.

[ling et al.2016] wang ling, edward grefenstette,
karl moritz hermann, tomas kocisky, andrew senior,
fumin wang, and phil blunsom.
2016. latent
predictor networks for code generation.

[marcheggiani and titov2016] diego marcheggiani and
ivan titov. 2016. discrete-state variational autoen-
coders for joint discovery and factorization of relations.
transactions of the association for computational lin-
guistics, 4.

[miao et al.2016] yishu miao, lei yu, and phil blunsom.
2016. neural variational id136 for text processing.
in proceedings of icml.

[mnih and gregor2014] andriy mnih and karol gregor.
2014. neural variational id136 and learning in be-
lief networks. in proceedings of icml.

[mnih et al.2014] volodymyr mnih, nicolas heess, and
alex graves. 2014. recurrent models of visual atten-
tion. in proceedings of nips.

[nallapati et al.2016] ramesh nallapati, bowen zhou,
  a glar gul  ehre, and bing xiang. 2016. abstrac-
tive text summarization using sequence-to-sequence
id56s and beyond. arxiv preprint arxiv:1602.06023.

[ranzato et al.2016] marc   aurelio ranzato,

sumit
chopra, michael auli, and wojciech zaremba. 2016.
sequence level training with recurrent neural networks.
[rezende et al.2014] danilo j rezende, shakir mohamed,
and daan wierstra. 2014. stochastic id26
and approximate id136 in deep generative models.
in proceedings of icml.

[rumelhart et al.1985] david e rumelhart, geoffrey e
hinton, and ronald j williams. 1985. learning in-
ternal representations by error propagation. technical
report, dtic document.

[rush et al.2015] alexander m rush, sumit chopra, and
jason weston. 2015. a neural attention model for
abstractive sentence summarization. in proceedings of
emnlp.

[sutskever et al.2014] ilya sutskever, oriol vinyals, and
quoc v le. 2014. sequence to sequence learning with
neural networks. in proceedings of nips.

[vinyals et al.2015] oriol vinyals, meire fortunato, and
navdeep jaitly. 2015. id193. in proceed-
ings of nips, pages 2674   2682.

[xu et al.2015] kelvin xu, jimmy ba, ryan kiros, aaron
courville, ruslan salakhutdinov, richard zemel, and
yoshua bengio. 2015. show, attend and tell: neural
image id134 with visual attention.

