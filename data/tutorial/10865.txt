coupling distributed and symbolic execution for natural language queries

lili mou 1 zhengdong lu 2 hang li 3 zhi jin 1

7
1
0
2

 

n
u
j
 

6
1

 
 
]

g
l
.
s
c
[
 
 

4
v
1
4
7
2
0

.

2
1
6
1
:
v
i
x
r
a

abstract

building neural networks to query a knowledge
base (a table) with natural language is an emerg-
ing research topic in deep learning. an execu-
tor for table querying typically requires multi-
ple steps of execution because queries may have
complicated structures. in previous studies, re-
searchers have developed either fully distributed
executors or symbolic executors for table query-
ing. a distributed executor can be trained in
an end-to-end fashion, but is weak in terms of
execution ef   ciency and explicit interpretability.
a symbolic executor is ef   cient in execution,
but is very dif   cult to train especially at initial
stages. in this paper, we propose to couple dis-
tributed and symbolic execution for natural lan-
guage queries, where the symbolic executor is
pretrained with the distributed executor   s inter-
mediate execution results in a step-by-step fash-
ion. experiments show that our approach signi   -
cantly outperforms both distributed and symbolic
executors, exhibiting high accuracy, high learn-
ing ef   ciency, high execution ef   ciency, and high
interpretability.

1. introduction
using natural language to query a knowledge base is an
important task in nlp and has wide applications in ques-
tion answering (qa) (yin et al., 2016a), human-computer
conversation (wen et al., 2017), etc. table 1 illustrates an
example of a knowledge base (a table) and a query    how
long is the game with the largest host country size?    to
answer the question, we should    rst    nd a row with the

1key laboratory of high con   dence software technolo-
gies (peking university), moe; software institute, peking uni-
versity, china 2deeplycurious.ai 3noah   s ark lab, huawei
technologies. work done when the    rst author was an
intern at huawei.
l.m. <double
power.mou@gmail.com>, z.l. <luz@deeplycurious.ai>, h.l.
<hangli.hl@huawei.com>, z.j. <zhijin@sei.pku.edu.cn>.

correspondence to:

proceedings of the 34 th international conference on machine
learning, sydney, australia, pmlr 70, 2017. copyright 2017
by the author(s).

query:

how long is the game with the largest host country size?

knowledge base (table):

year

city

2000
2004
2008
2012
2016 rio de janeiro

sydney
athens
beijing
london

area

200
250
350
300
200

      
      
      
      
      
      

      
      
      
      
      
      
      
      

duration

30
20
25
35
40

table 1. an example of a natural language query and a knowledge
base (table).

largest value in the column area, and then select the value
of the chosen row with the column being duration.
a typical approach to table querying is to convert a natural
language sentence to an    executable    logic form, known
as id29. traditionally, building a semantic
parser requires extensive human engineering of explicit
features (berant et al., 2013; pasupat & liang, 2015).
with the fast development of deep learning, an increasingly
number of studies use neural networks for semantic pars-
ing. dong & lapata (2016) and xiao et al. (2016) apply
sequence-to-sequence (id195) neural models to gen-
erate a logic form conditioned on an input sentence, but
the training requires groundtruth logic forms, which are
costly to obtain and speci   c to a certain dataset. in real-
istic settings, we only assume groundtruth denotations1 are
available, and that we do not know execution sequences or
intermediate execution results. liang et al. (2017) train a
id195 network by reinforce policy gradient. but it
is known that the reinforce algorithm is sensitive to the
initial policy; also, it could be very dif   cult to get started at
early stages.
yin et al. (2016b) propose a fully distributed neural en-
quirer, comprising several neuralized execution layers of
   eld attention, row annotation, etc. the model can be
trained in an end-to-end fashion because all components
are differentiable. however, it lacks explicit interpreta-
tion and is not ef   cient in execution due to intensive ma-
trix/vector operation during neural processing.
neelakantan et al. (2016) propose a neural programmer

1a denotation refers to an execution result.

coupling distributed and symbolic execution for natural language queries

by de   ning a set of symbolic operators (e.g., argmax,
greater than); at each step, all possible execution re-
sults are fused by a softmax layer, which predicts the prob-
ability of each operator at the current step. the step-by-step
fusion is accomplished by weighted average and the model
is trained with mean square error. hence, such approaches
work with numeric tables, but may not be suited for other
operations like string matching.
it also suffers from the
problem of    exponential numbers of combinatorial states,   
as the model explores the entire space at a time by step-by-
step weighted average.
in this paper, we propose to couple distributed and sym-
bolic execution for natural language queries. by    sym-
bolic execution,    we mean that we de   ne symbolic op-
erators and keep discrete intermediate execution results.2
our intuition rises from the observation that a fully dis-
tributed/neuralized executor also exhibits some (imperfect)
symbolic interpretation. for example, the    eld attention
gadget in yin et al. (2016b) generally aligns with column
selection. we therefore use the distributed model   s in-
termediate execution results as supervision signals to pre-
train a symbolic executor. guided by such imperfect step-
by-step supervision, the symbolic executor learns a fairly
meaningful initial policy, which largely alleviates the cold
start problem of reinforce. moreover, the improved
policy can be fed back to the distributed executor to im-
prove the neural network   s performance.
we evaluated the proposed approach on the qa dataset
in yin et al. (2016b). our experimental results show that
the reinforce algorithm alone takes long to get started.
even if it does, it is stuck in poor local optima. once
pretrained by imperfect supervision signals, the symbolic
executor can recover most execution sequences, and also
achieves the highest denotation accuracy. it should be em-
phasized that, in our experiment, neither the distributed ex-
ecutor nor the symbolic executor is aware of groundtruth
execution sequences, and that the entire model is trained
with weak supervision of denotations only.
to the best of our knowledge, we are the    rst to couple
distributed and symbolic execution for id29.
our study also sheds light on neural sequence prediction in
general.

2. approach
in subsection 2.1, we introduce the fully distributed neu-
ral executor, which is mostly based on yin et al. (2016b).
for symbolic execution, we design a set of operators that

2the sense of symbolic execution here should not be confused
with    symbolic execution of a program    (see https://en.
wikipedia.org/wiki/symbolic_execution for ex-
ample).

figure 1. an overview of the coupled distributed and symbolic
executors.

are complete to the task at hand; at each execution step, a
neural network predicts a particular operator and possibly
arguments (subsection 2.2).
subsection 2.3 provides a uni   ed view of distributed and
symbolic execution (figure 1). we explain how the sym-
bolic executor is pretrained by the distributed one   s inter-
mediate execution results, and then further trained with the
reinforce algorithm.

2.1. distributed executor

the distributed executor makes full use of neural networks
for table querying. by    distributed,    we mean that all se-
mantic units (including words in the query, entries in the
table, and execution results) are represented as distributed,
real-valued vectors and processed by neural networks. one
of the most notable studies of distributed semantics is word
embeddings, which map discrete words to vectors as mean-
ing representations (mikolov et al., 2013).
the distributed executor consists of the following main
components.

    query encoder. words are mapped to word em-
beddings and a bi-directional recurrent neural net-
work (id56) aggregates information over the sen-
tence. id56s    last states in both directions are con-
catenated as the query representation (detonated as q).
    table encoder. all table cells are also represented as
embeddings. for a cell c (e.g., beijing in table 1)
with its column/   eld name being f (e.g., city), the
cell vector is the concatenation of the embeddings of
c and f, further processed by a feed-forward neural
network (also known as multi-layer id88). we
denote the representation of a cell as c.
    executor. as shown in figure 1a, the neural network
comprises several steps of execution. in each execu-
tion step, the neural network selects a column by soft-

neuralexecutorintermediateresultoperator1operator2operatornoperator/argumentpredictoroperator/argumentpredictoroperator/argumentpredictor. . .differentiablenon-differentiableimperfectstep-by-stepsupervisionintermediateresultneuralexecutorintermediateresultoperator1operator2operatornintermediateresultneuralexecutoroutputoperator1operator2operatorn. . .output. . .(a)(b)querycoupling distributed and symbolic execution for natural language queries

where [i] indexes a row of the selected column.
the current row annotation is computed by another mlp,
based on the query q, previous execution results r(t   1)
,
previous global information g(t   1), as well as the selected
row in the current step c(t)
select[i] (the selection is in a soft
manner), i.e.,

i

(cid:16)(cid:104)

(cid:105)(cid:17)

figure 2. a single step of distributed execution.

r(t)
i = mlp

q, g(t   1), r(t   1)

i

, c(t)

select[i]

(5)

max attention, and annotates each row with a vector,
i.e., an embedding (figure 2). the row vector can be
intuitively thought of as row selection in query execu-
tion, but is represented by distributed semantics here.3
in the last step of execution, a softmax classi   er is ap-
plied to the entire table to select a cell as the answer.
details are further explained as below.

i

let r(t   1)
be the previous step   s row annotation result,
where the subscript i indexes a particular row. we sum-
marize global execution information (denoted as g(t   1)) by
max-pooling the row annotation r(t   1), i.e.,
r(t   1)

g(t   1) = maxpooli

(cid:110)

(cid:111)

(1)

i

in the current execution step, we    rst compute a distribu-
tion p(t)
f over all    elds as    soft       eld selction. the compu-
tation is based on the query q, the previous global informa-
tion g(t   1), and the    eld name embeddings f, i.e.,

p(t)
fj

= softmax

(cid:16)

mlp(cid:0)[q; fj; g(t   1)](cid:1)(cid:17)
exp(cid:8)mlp(cid:0)[q; fj; g(t   1)](cid:1)(cid:9)
j(cid:48) exp(cid:8)mlp(cid:0)[q; fj(cid:48); g(t   1)](cid:1)(cid:9)
(cid:80)

=

(2)

(3)

where [  ;   ;        ] denotes vector concatenation; mlp refers
to a multi-layer id88.
here, the weights of softmax are    eld embeddings (rather
than parameters indexed by positions). in this way, there is
no difference if one shuf   es table columns. besides, for a
same    eld name, its embedding is shared among all training
samples containing this    eld (but different tables may have
different    elds).
we represent the selected cell in each row as the sum of
all cells in that row, weighted by soft    eld selection p(t   1)
.
formally, for the i-th row, we have

f

c(t)
select[i] =

p(t)
fj

cij

(4)

j

3in a pilot experiment, we tried a gating mechanism to indi-
cate the results of row selection in hopes of aligning symbolic
table execution. however, our preliminary experiments show that
such gates do not exhibit much interpretation, but results in per-
formance degradation. the distributed semantics provide more
information than a 1-bit gate for a row.

(cid:88)

as said, the last execution layer applies a softmax classi   er
over all cells to select an answer. similar to equation 3,
the weights of softmax are not associated with positions,
but the cell embeddings. in other words, the id203 of
choosing the i-th row, j-th column is

(cid:80)
i(cid:48)(cid:80)

exp

mlp

(cid:110)

(cid:16)

q, g(t   1), r(t   1)

, cij
q, g(t   1), r(t   1)

i

i(cid:48)

j(cid:48) exp

mlp

pij =

(cid:17)(cid:111)

, ci(cid:48)j(cid:48)

(cid:17)(cid:111)

(cid:110)

(cid:16)

(6)
in this way, the neural executor is invariant with respect to
the order of rows and columns. while order-sensitive ar-
chitectures (e.g., convolutional/recurrent neural networks)
might also model a table by implicitly ignoring such or-
der information, the current treatment is more reasonable,
which also better aligns with symbolic interpretation.

2.2. symbolic executor

the methodology of designing a symbolic executor is to
de   ne a set of primitive operators for the task, and then to
use a machine learning model to predict the operator se-
quence and its arguments.
our symbolic executor is different from the neural pro-
grammer (neelakantan et al., 2016) in that we keep
discrete/symbolic operators as well as execution results,
whereas neelakantan et al. (2016) fuse execution results
by weighted average.

2.2.1. primitive operators

we design six operators for symbolic execution, which are
complete as they cover all types of queries in our scenario.
similar to the distributed executor, the result of one-step
symbolic execution is some information for a row; here,
we use a 0-1 boolean scalar, indicating whether a row is
selected or not after a particular step of execution. then
a symbolic execution step takes previous results as input,
with a column/   eld being the argument. the green boxes
in figure 1b illustrate the process and table 2 summarizes
our primitive operator set.
in table 1, for example,
the    rst step of execution is
argmax over the column area, with previous (initial)
row selection being all ones. this step yields a single

soft  column  selectiondistributed  row  annotationselected  columncoupling distributed and symbolic execution for natural language queries

explanation
choose a row whose value of a particular column is mentioned in the query
choose the row from previously selected candidate rows with the minimum value in a particular column
choose the row from previously selected candidate rows with the maximum value in a particular column

operator
select row
argmin
argmax
greater than choose rows whose value in a particular column is greater than a previously selected row
less than
select value choose the value of a particular column and of the previously selected row
eoe

choose rows whose value in a particular column is less than a previously selected row

terminate, indicating the end of execution

table 2. primitive operators for symbolic execution.

row (cid:104)2008, beijing,       , 350,       , 25(cid:105). the second execu-
tion operator is select value, with an argument col-
umn=duration, yielding the result 25. then the execu-
tor terminates (eoe).
stacked with multiple steps of primitive operators, the ex-
ecutor can answer fairly complicated questions like    how
long is the last game which has smaller country size than
the game whose host country gdp is 250?    in this exam-
ple, the execution sequence is

1. select row: select the row where the column is

gdp and the value is mentioned in the query.

2. less than: select rows whose country size is less

than that of the previously selected row.

3. argmax: select the row whose year is the largest

among previously selected rows.

4. select value: choose the value of the previously

selected row with the column being duration.

then the execution terminates. in our scenario, the execu-
tion is limited to four steps (eoe excluded) as such queries
are already very complicated in terms of logical depth.

2.2.2. operator and argument predictors

we also leverage neural models, in particular id56s, to pre-
dict the operator and its argument (a selected    eld/column).
let h(t   1) be the previous state   s hidden vectors. the cur-
rent hidden state is

op = sigmoid(w (rec)
h(t)

op h(t   1)

op

)

(7)

op

where w (rec)
is weight parameters. (a bias term is omitted
in the equation for simplicity.) the initial hidden state is
the query embedding, i.e., h(0)
the predicted id203 of an operator i is given by

op = q.

(cid:110)

(cid:111)

p(t)
opi

= softmax

w(out)
opi

(cid:62)h(t)
op

(8)

the operator with the largest predicted id203 is se-
lected for execution.
our id56 here does not have input, because the execution
sequence is not dependent on the result of the previous ex-
ecution step. such architecture is known as a jordan-type
id56 (jordan, 1997; mesnil et al., 2013).

likewise, another jordan-type id56 selects a    eld. the
only difference lies in the weight of the output softmax,
i.e., wi in equation 8 is substituted with the embedding of
a    eld/column name f, given by

(cid:111)
   eld h(t   1)
   eld = sigmoid(w (rec)
h(t)
   eld )
f(cid:62)
p(t)
j h(t)
   eld
fj

= softmax

(cid:110)

(9)

(10)

training a symbolic executor without step-by-step super-
vision signals is non-trivial. a typical training method is
id23 in a trial-and-error fashion. how-
ever, for a random initial policy, the id203 of recov-
ering an accurate execution sequence is extremely low.
given a 10    10 table, for example, the id203 is
1/(64    104)     7.7    10   8; the id203 of obtaining an
accurate denotation is 1%, which is also very low. there-
fore, symbolic executors are not ef   cient in learning.

2.3. a uni   ed view

we now have two worlds of execution:

    the distributed executor is end-to-end learnable, but
it is of low execution ef   ciency because of intensive
matrix/vector multiplication during neural informa-
tion processing. the fully neuralized execution also
lacks explicit interpretation.
    the symbolic executor has high execution ef   ciency
and explicit interpretation. however, it cannot be
trained in an end-to-end manner, and suffers from the
cold start problem of id23.

we propose to combine the two worlds by using the dis-
tributed executor   s intermediate execution results to pre-
train the symbolic executor for an initial policy; we then
use the reinforce algorithm to improve the policy. the
well-trained symbolic executor   s intermediate results can
also be fed back to the distributed executor to improve per-
formance.
2.3.1. distributed     symbolic
we observe that the    eld attention in equation 3 generally
aligns with column selection in equation 10. we therefore
pretrain the column selector in the symbolic executor with
labels predicted by a fully neuralized/distributed executor.

coupling distributed and symbolic execution for natural language queries

j =     m(cid:88)

label(cid:88)

n(i)

such pretraining can obtain up to 70% accurate    eld selec-
tion and largely reduce the search space during reinforce-
ment learning.
formally, the operator predictor (equation 8) and argu-
ment predictor (equation 10) in each execution step are
the actions (denoted as a) in id23 ter-
if we would like to pretrain m actions
minologies.
a1, a2,       , am     a, the cost function of a particular data
sample is

  t(i)
j

log p(i)
j

(11)

i=1

j=1

where n(j)
label is the number of labels (possible candidates)
for the j-th action. p(i)     rn(i)
label is the predicted prob-
ability by the operator/argument predictors in figure 1b.
  t(i)     rn(i)
label is the induced action from the fully distributed
model in figure 1a. in our scenario, we only pretrain col-
umn predictors.
after obtaining a meaningful, albeit imperfect, initial pol-
icy, we apply reinforce (williams, 1992) to improve
the policy.
we de   ne a binary reward r indicating whether the    nal
result of symbolic execution matches the groundtruth de-
notation. the id168 of a policy is the negative ex-
pected reward where actions are sampled from the current
predicted probabilities

j =    ea1,a2,       ,an     [r(a1, a2,       , an)]

(12)

the partial derivative for a particular sampled action is

=   r    (pi     1ai)

   j
   oi

(13)

where pi is the predicted id203 of all possible actions
at the time step i, 1ai is a onehot representation of a sam-
pled action ai, and oi is the input (also known as logit) of
softmax.   r is the adjusted reward, which will be described
shortly.
to help the training of reinforce, we have two tricks:
    we balance exploration and exploitation with a small
id203  .
in other words, we sample an action
from the predicted action distribution with id203
1      and from a uniform distribution over all possible
actions with id203  . the small fraction of uni-
form sampling helps the model to escape from poor
local optima, as it continues to explore the entire ac-
tion space during training.
    we adjust the reward by subtracting the mean reward,
averaged over sampled actions for a certain data point.
this is a common practice for reinforce (ranzato
et al., 2016). we also truncate negative rewards as

zero to prevent gradient from being messed up by in-
correct execution. this follows the idea of    reward-
inaction,    where unsuccessful trials are ignored (sec-
tion 2.4 in sutton & barto (1998)). the adjusted re-
ward is denoted as   r in equation 13.

notice that these tricks are applied to both coupled training
and baselines for fairness.
2.3.2. distributed     symbolic     distributed
after policy improvement by reinforce, we could fur-
ther feed back the symbolic executor   s intermediate results
to the distributed one, akin to the step-by-step supervision
setting in yin et al. (2016b). the loss is a combination of
denotation cross id178 loss jdenotation and    eld attention
cross id178 loss j   elds. (j   elds is similar to equation 11,
and details are not repeated). the overall training objective
is j = jdenotation +   j   elds, where    is a hyperparameter
balancing the two factors.
as will be seen in section 3.3.5, feeding back intermedi-
ate results improves the distributed executor   s performance.
this shows the distributed and symbolic worlds can indeed
be coupled well.

3. experiments
3.1. dataset

we evaluated our approach on a qa dataset
in yin
et al. (2016b). the dataset comprises 25k different ta-
bles and queries for training; validation and test sets con-
tain 10k samples, respectively, and do not overlap with the
training data. each table is of size 10    10, but differ-
ent samples have different tables; the queries can be di-
vided into four types: selectwhere, superlative,
wheresuperlative, and nestquery, requiring 2   4
execution steps (eoe excluded).
we have both groundtruth denotation and execution actions
(including operators and    elds), as the dataset is synthe-
sized by complicated rules and templates for research pur-
poses. however, only denotations are used as labels during
training, which is a realistic setting; execution sequences
are only used during testing. for the sake of simplicity, we
presume the number of execution steps is known a priori
during training (but not during testing). although we have
such (little) knowledge of execution, it is not a limitation of
our approach and out of our current focus. one can easily
design a dummy operator to    ll an unnecessary step or one
can also train a discriminative sentence model to predict the
number of execution steps if a small number of labels are
available.
we chose the synthetic datasets because it is magnitudes
larger than existing resources (e.g., webquestions).

coupling distributed and symbolic execution for natural language queries

query type
selectwhere
superlative
wheresuperlative
nestquery
overall

sempre    distributed   
sempre

denotation

symbolic coupled distributed

execution

symbolic coupled

93.8
97.8
34.8
34.4
65.2

96.2
98.9
80.4
60.5
84.0

99.2
100.0
51.9
52.5
75.8

99.6
100.0
99.9
100.0
99.8

   
   
   
   
   

99.1
100.0
0.0
0.0
49.5

99.6
100.0
91.0
100.0
97.6

table 3. accuracies (in percentage) of the sempre tookit, the distributed neural enquirer, the symbolic executor, and our coupled
approach.    results reported in yin et al. (2016b).

the process of data synthesizing also provides intermedi-
ate execution results for in-depth analysis. like babi for
machine comprehension, our dataset and setting are a pre-
requisite for general id29. the data are avail-
able at out project website4; the code for data generation
can also be downloaded to facilitate further development
of the dataset.

3.2. settings

the symbolic executor   s settings were generally derived
from yin et al. (2016b) so that we can have a fair com-
parison.5 the dimensions of all layers were in the range of
20   50; the learning algorithm was adadelta with default
hyperparameters.
for the pretraining of the symbolic executor, we applied
id113 for 40 epochs to column
selection with labels predicted by the distributed executor.
we then used the reinforce algorithm to improve the
policy, where we generated 10 action samples for each data
point with the exploration id203   being 0.1.
when feeding back to the distributed model, we chose   
from {0.1, 0.5, 1} by validation to balance denotation error
and    eld attention error. 0.5 outperforms the rest.
besides neural networks, we also included the sempre
system as a baseline for comparison. the results are re-
ported in yin et al. (2016b), where a sempre version that
is specially optimized for table query is adopted (pasupat
& liang, 2015). thus it is suited in our scenario.

3.3. results

3.3.1. overall performance

table 3 presents the experimental results of our coupled ap-
proach as well as baselines. because reinforcement learn-
ing is much more noisy to train, we report the test accuracy

4https://sites.google.com/site/

coupleneuralsymbolic/

5one exception is the query id56   s hidden states. yin
et al. (2016b) used 300d biid56, but we found it more likely
to over   t in the symbolic setting, and hence we used 50d. this
results in slower training, more rugged error surfaces, but higher
peak performance.

corresponding to highest validation accuracy among three
different random initializations (called trajectories). this
is also known as a restart strategy for non-convex optimiza-
tion.
as we see, both distributed and symbolic executors out-
perform the sempre system, showing that neural net-
works can capture query information more effectively than
human-engineered features. further, the coupled approach
also signi   cantly outperforms both of them.
if trained solely by reinforce, the symbolic execu-
tor can recover the execution sequences for simple ques-
tions (selectwhere and superlative). however,
for more complicated queries, it only learns last one or two
steps of execution and has trouble in recovering early steps,
even with the tricks in section 2.3.1. this results in low
execution accuracy but near 50% denotation accuracy be-
cause, in our scenario, we still have half chance to obtain
an accurate denotation even if the nested (early) execution
is wrong   the ultimate result is either in the candidate list
or not, given a wrong where-clause execution.
by contrast, the coupled training largely improves the sym-
bolic executor   s performance in terms of all query types.

3.3.2. interpretability

the accuracy of execution is crucial to the interpretability
of a model. we say an execution is accurate, if all actions
(operators and arguments) are correct. as shown above, an
accurate denotation does not necessarily imply an accurate
execution.
we    nd that coupled training recovers most correct exe-
cution sequences, that the symbolic executor alone cannot
recover complicated cases, and that a fully distributed en-
quirer does not have explicit interpretations of execution.
the results demonstrate high interpretability of our ap-
proach, which is helpful for human understanding of ex-
ecution processes.

3.3.3. learning efficiency

we plot in figure 3 the validation learning curves of the
symbolic executor, trained by either id23

coupling distributed and symbolic execution for natural language queries

fully

our approach

distributed op/arg pred. symbolic exe.    total
2.65
0.44

13.86
1.05

2.65
0.44

0.002

cpu
gpu

figure 3. validation learning curves.
(a) symbolic executor
trained by reinforce (rl) only. (b) symbolic executor with
40-epoch pretraining using a distributed executor in a supervised
learning (sl) fashion. both settings have three trajectories with
different random initializations. dotted lines: denotation accu-
racy. solid lines: execution accuracy.

alone or our coupled approach.
figure 3a shows that the symbolic executor is hard to train
by reinforce alone: one trajectory obtains near-zero
execution accuracy after 2000 epochs; the other two take
200 epochs to get started to escape from initial plateaus.
even if they achieve    50% execution accuracy (for simple
query types) and    75% denotation accuracy, they are stuck
in the poor local optima.
figure 3b presents the learning curves of the symbolic ex-
ecutor pretrained with intermediate    eld attention of the
distributed executor. since the operation predictors are still
hanging after pretraining, the denotation accuracy is near
0 before id23. however, after only a
few epochs of reinforce training, the performance in-
creases sharply and achieves high accuracy gradually.
notice that we have 40 epochs of (imperfectly) supervised
pretraining. however, its time is negligible compared with
id23 because in our experiments rein-
force generates 10 samples and hence is theoretically 10
times slower. the results show that our coupled approach
has much higher learning ef   ciency than a pure symbolic
executor.

3.3.4. execution efficiency

table 4 compares the execution ef   ciency of a distributed
executor and our coupled approach. all neural networks
are implemented in theano with a titan black gpu and
xeon e7-4820v2 (8-core) cpu; symbolic execution is as-
sessed in c++ implementation. the comparison makes
sense because the theano platform is not specialized in
symbolic execution, and fortunately, execution results do
not affect actions in our experiment. hence they can be
easily disentangled.
as shown in the table, the execution ef   ciency of our ap-
proach is 2   5 times higher than the distributed executor,
depending on the implementation. the distributed execu-
tor is when predicting because it maps every token to a

table 4. execution ef   ciency. we present the running time (in
seconds) of the test set, containing 10k samples.    the symbolic
execution is assessed in c++ implementation. others are imple-
mented in theano, including the fully distributed model as well
as the operator and argument predictors.

training method
end-to-end (w/ denotation labels)   
step-by-step (w/ execution labels)   
feeding back

accuracy (%)
84.0
96.4
96.5

table 5. the accuracy of a fully distributed model, trained by dif-
ferent methods. in the last row, we    rst train a distributed executor
and feed its intermediate execution results to the symbolic one;
then the symbolic executor   s intermediate results are fed back to
the distributed one.    reported in yin et al. (2016b).

distributed real-valued vector, resulting in intensive matrix-
vector operations. the symbolic executor only needs a neu-
ral network to predict actions (operators and arguments),
and thus is more lightweight. further, we observe the exe-
cution itself is blazingly fast, implying that, compared with
distributed models, our approach could achieve even more
ef   ciency boost with a larger table or more complicated
operation.

3.3.5. feeding back

we now feed back the well-trained symbolic executor   s in-
termediate results to the fully distributed one. as our well-
trained symbolic executor has achieved high execution ac-
curacy, this setting is analogous to strong supervision with
step-by-step groundtruth signals, and thus it also achieves
similar performance,6 shown in table 5.
we showcase the distributed executor   s    eld attention7 in
figure 4.
if trained in an end-to-end fashion, the neural
network exhibits interpretation in the last three steps of this
example, but in the early step, the    eld attention is incorrect
(also more uncertain as it scatters a broader range). af-
ter feeding back the symbolic executor   s intermediate re-
sults as step-by-step supervision, the distributed executor
exhibits near-perfect    eld attention.
this experiment further con   rms that the distributed and

6we even have 0.1% performance boost compared with the
step-by-step setting, but we think it should be better explained as
variance of execution.

7the last neural executor is a softmax layer over all cells. we

marginalize over rows to obtain the    eld id203.

coupling distributed and symbolic execution for natural language queries

query: how many people watched the earliest game whose host country gdp is larger than the game in cape town?

figure 4. distributed executor   s intermediate results of    eld attention. top: trained in an end-to-end fashion (a   d). bottom: one-round
co-training of distributed and symbolic executors (e   h). the red plot indicates incorrect    eld attention.

symbolic worlds can indeed be coupled well. in more com-
plicated applications, there could also be possibilities in it-
eratively training one model by leveraging the other in a
co-training fashion.

4. related work and discussions
neural execution has recently aroused much interest in
the deep learning community. besides sql-like execu-
tion as has been extensively discussed in previous sec-
tions, id63s (graves et al., 2014) and
neural programmer-interpreters (reed & de freitas, 2016)
are aimed at more general    programs.    the former is a
   distributed analog    to turing machines with soft operators
(e.g., read, write, and address); its semantics, how-
ever, cannot be grounded to actual operations. the latter
learns to generate an execution trace in a fully supervised,
step-by-step manner.
another related topic is incorporating neural networks with
external (hard) mechanisms. hu et al. (2016) propose to
better train a neural network by leveraging the classi   ca-
tion distribution induced from a rule-based system. lei
et al. (2016) propose to induce a sparse code by rein-
force to make neural networks focus on relevant infor-
mation. in machine translation, mi et al. (2016) use align-
ment heuristics to train the attention signal of neural net-
works in a supervised manner. in these studies, researchers
typically leverage external hard mechanisms to improve
neural networks    performance.
the uniqueness of our work is to train a fully neural-
ized/distributed model    rst, which takes advantage of its
differentiability, and then to guide a symbolic model to
achieve a meaningful initial policy. further trained by rein-
forcement learning, the symbolic model   s knowledge can

improve neural networks    performance by feeding back
step-by-step supervision. our work sheds light on neu-
ral sequence prediction in general, for example, explor-
ing word alignment (mi et al., 2016) or chunking informa-
tion (zhou et al., 2017) in machine translation by coupling
neural and external mechanisms.

5. conclusion and future work
in this paper, we have proposed a coupled view of
distributed and symbolic execution for natural language
queries. by pretraining with intermediate execution re-
sults of a distributed executor, we manage to accelerate
the symbolic model   s reinforce training to a large ex-
tent. the well-trained symbolic executor could also guide
a distributed executor to achieve better performance. our
proposed approach takes advantage of both distributed and
symbolic worlds, achieving high interpretability, high exe-
cution ef   ciency, high learning ef   ciency, as well as high
accuracy.
as a pilot study, our paper raises several key open ques-
tions: when do neural networks exhibit symbolic interpre-
tations? how can we better transfer knowledge between
distributed and symbolic worlds?
in future work, we would like to design interpretable op-
erators in the distributed model to better couple the two
worlds and to further ease the training with reinforce.
we would also like to explore different ways of transferring
knowledge, e.g., distilling knowledge from the action dis-
tributions (rather than using the max a posteriori action),
or sampling actions by following the distributed model   s
predicted distribution during symbolic one   s monte carlo
policy gradient training (reinforce).

coupling distributed and symbolic execution for natural language queries

pasupat, panupong and liang, percy. compositional se-
in acl-

mantic parsing on semi-structured tables.
ijcnlp, pp. 1470   1480, 2015.

ranzato, marcaurelio, chopra, sumit, auli, michael, and
zaremba, wojciech. sequence level training with recur-
rent neural networks. in iclr, 2016.

reed, scott and de freitas, nando. neural programmer-

interpreters. in iclr, 2016.

sutton, richard s and barto, andrew g. reinforcement
learning: an introduction. mit press cambridge, 1998.

wen, tsung-hsien, vandyke, david, mrk  si  c, nikola, ga-
sic, milica, rojas barahona, lina m., su, pei-hao,
ultes, stefan, and young, steve. a network-based end-
to-end trainable task-oriented dialogue system. in eacl,
pp. 438   449, 2017.

williams, ronald j. simple statistical gradient-following
learning.

algorithms for connectionist reinforcement
machine learning, 8(3):229   256, 1992.

xiao, chunyang, dymetman, marc, and gardent, claire.
sequence-based id170 for semantic pars-
ing. in acl, pp. 1341   1350, 2016.

yin, jun, jiang, xin, lu, zhengdong, shang, lifeng, li,
hang, and li, xiaoming. neural generative question an-
swering. in ijcai, pp. 2972   2978, 2016a.

yin, pengcheng, lu, zhengdong, li, hang, and kao, ben.
neural enquirer: learning to query tables with natural
language. in ijcai, pp. 2308   2314, 2016b.

zhou, hao, tu, zhaopeng, huang, shujian, liu, xiaohua,
li, hang, and chen, jiajun. chunk-based bi-scale de-
coder for id4. in acl, 2017.

acknowledgments
we thank pengcheng yin and jiatao gu for helpful dis-
cussions; we also thank the reviewers for insightful com-
ments. this research is partially supported by the national
basic research program of china (the 973 program) under
grant nos. 2014cb340301 and 2015cb352201, and the
national natural science foundation of china under grant
nos. 614201091, 61232015 and 61620106007.

references
berant, jonathan, chou, andrew, frostig, roy, and liang,
percy. id29 on freebase from question-
answer pairs. in emnlp, pp. 1533   1544, 2013.

dong, li and lapata, mirella. language to logical form

with neural attention. in acl, pp. 33   43, 2016.

graves, alex, wayne, greg, and danihelka, ivo. neural
turing machines. arxiv preprint arxiv:1410.5401, 2014.

hu, zhiting, ma, xuezhe, liu, zhengzhong, hovy, eduard,
and xing, eric. harnessing deep neural networks with
logic rules. in acl, pp. 2410   2420, 2016.

jordan, michael i. serial order: a parallel distributed pro-
cessing approach. advances in psychology, 121:471   
495, 1997.

lei, tao, barzilay, regina, and jaakkola, tommi. ratio-
in emnlp, pp. 107   117,

nalizing neural predictions.
2016.

liang, chen, berant, jonathan, le, quoc, forbus, ken-
neth d, and lao, ni. neural symbolic machines: learn-
ing semantic parsers on freebase with weak supervision.
in acl (to appear), 2017.

mesnil, gr  egoire, he, xiaodong, deng, li, and bengio,
yoshua. investigation of recurrent-neural-network archi-
tectures and learning methods for spoken language un-
derstanding. in interspeech, pp. 3771   3775, 2013.

mi, haitao, sankaran, baskaran, wang, zhiguo, and itty-
cheriah, abe. coverage embedding models for neural
machine translation. in emnlp, pp. 955   960, 2016.

mikolov, tomas, sutskever, ilya, chen, kai, corrado,
greg s, and dean, jeff. distributed representations of
words and phrases and their compositionality. in nips,
pp. 3111   3119, 2013.

neelakantan, arvind, le, quoc v, and sutskever, ilya.
neural programmer: inducing latent programs with gra-
dient descent. in iclr, 2016.

