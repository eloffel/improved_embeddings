8
1
0
2

 
r
a

 

m
9
1

 
 
]
l
c
.
s
c
[
 
 

2
v
7
1
4
1
0

.

2
0
7
1
:
v
i
x
r
a

published as a conference paper at iclr 2018

all-but-the-top: simple and effective post-
processing for word representations

jiaqi mu, pramod viswanath
university of illinois at urbana champaign
{jiaqimu2, pramodv}@illinois.edu

abstract

real-valued word representations have transformed nlp applications; popular
examples are id97 and glove, recognized for their ability to capture linguistic
regularities. in this paper, we demonstrate a very simple, and yet counter-intuitive,
postprocessing technique     eliminate the common mean vector and a few top
dominating directions from the word vectors     that renders off-the-shelf represen-
tations even stronger. the postprocessing is empirically validated on a variety of
lexical-level intrinsic tasks (word similarity, concept categorization, word analogy)
and sentence-level tasks (semantic textural similarity and text classi   cation) on
multiple datasets and with a variety of representation methods and hyperparameter
choices in multiple languages; in each case, the processed representations are
consistently better than the original ones.

1

introduction

words and their interactions (as sentences) are the basic units of natural language. although words
are readily modeled as discrete atomic units, this is unable to capture the relation between the
words. recent distributional real-valued representations of words (examples: id97, glove) have
transformed the landscape of nlp applications     for instance, text classi   cation (socher et al., 2013b;
maas et al., 2011; kim, 2014), machine translation (sutskever et al., 2014; bahdanau et al., 2014) and
knowledge base completion (bordes et al., 2013; socher et al., 2013a). the success comes from the
geometry of the representations that ef   ciently captures linguistic regularities: the semantic similarity
of words is well captured by the similarity of the corresponding vector representations.
a variety of approaches have been proposed in recent years to learn the word representations:
collobert et al. (2011); turian et al. (2010) learn the representations via semi-supervised learning by
jointly training the language model and downstream applications; bengio et al. (2003); mikolov et al.
(2010); huang et al. (2012) do so by    tting the data into a neural network language model; mikolov
et al. (2013); mnih & hinton (2007) by id148; and dhillon et al. (2012); pennington
et al. (2014); levy & goldberg (2014); stratos et al. (2015); arora et al. (2016) by producing a low-
dimensional representation of the cooccurrence statistics. despite the wide disparity of algorithms to
induce word representations, the performance of several of the recent methods is roughly similar on a
variety of intrinsic and extrinsic evaluation testbeds.
in this paper, we    nd that a simple processing renders the off-the-shelf existing representations even
stronger. the proposed algorithm is motivated by the following observation.

observation every representation we tested, in many languages, has the following properties:

vector (with norm up to a half of the average norm of word vector).

    the word representations have non-zero mean     indeed, word vectors share a large common
    after removing the common mean vector, the representations are far from isotropic     indeed,
much of the energy of most word vectors is contained in a very low dimensional subspace
(say, 8 dimensions out of 300).

implication since all words share the same common vector and have the same dominating direc-
tions, and such vector and directions strongly in   uence the word representations in the same way, we

1

published as a conference paper at iclr 2018

propose to eliminate them by: (a) removing the nonzero mean vector from all word vectors, effec-
tively reducing the energy; (b) projecting the representations away from the dominating d directions,
effectively reducing the dimension. experiments suggest that d depends on the representations (for
example, the dimension of the representation, the training methods and their speci   c hyperparameters,
the training corpus) and also depends on the downstream applications. nevertheless, a rule of thumb
of choosing d around d/100, where d is the dimension of the word representations, works uniformly
well across multiple languages and multiple representations and multiple test scenarios.
we emphasize that the proposed postprocessing is counter intuitive     typically denoising by dimen-
sionality reduction is done by eliminating the weakest directions (in a singular value decomposition
of the stacked word vectors), and not the dominating ones. yet, such postprocessing yields a    puri   ed   
and more    isotropic    word representation as seen in our elaborate experiments.

experiments by postprocessing the word representation by eliminating the common parts, we
   nd the processed word representations to capture stronger linguistic regularities. we demonstrate
this quantitatively, by comparing the performance of both the original word representations and the
processed ones on three canonical lexical-level tasks:

    word similarity task tests the extent to which the representations capture the similarity
between two words     the processed representations are consistently better on seven different
datasets, on average by 1.7%;
    concept categorization task tests the extent to which the clusters of word representations
capture the word semantics     the processed representations are consistently better on three
different datasets, by 2.8%, 4.5% and 4.3%;
    word analogy task tests the extent to which the difference of two representations captures
a latent linguistic relation     again, the performance is consistently improved (by 0.5%
on semantic analogies, 0.2% on syntactic analogies and 0.4% in total). since part of the
dominant components are inherently canceled due to the subtraction operation while solving
the analogy, we posit that the performance improvement is not as pronounced as earlier.

extrinsic evaluations provide a way to test the goodness of representations in speci   c downstream
tasks. we evaluate the effect of postprocessing on a standardized and important extrinsic evaluation
task on sentence modeling: semantic textual similarity task     where we represent a sentence by its
averaged word vectors and score the similarity between a pair of sentences by the cosine similarity
between the corresponding sentence representation. postprocessing improves the performance
consistently and signi   cantly over 21 different datasets (average improvement of 4%).
word representations have been particularly successful in nlp applications involving supervised-
learning, especially in conjunction with neural network architecture. indeed, we see the power
of postprocessing in an experiment on a standard text classi   cation task using a well established
convoluntional neural network (id98) classi   er (kim, 2014) and three id56 classi   ers (with vanilla
id56, gru (chung et al., 2015) and lstm greff et al. (2016) as recurrent units). across two
different pre-trained word vectors,    ve datasets and four different architectures, the performance with
processing improves on a majority of instances (34 out of 40) by a good margin (2.85% on average),
and the two performances with and without processing are comparable in the remaining ones.

related work. our work is directly related to word representation algorithms, most of which have
been elaborately cited.
aspects similar to our postprocessing algorithm have appeared in speci   c nlp contexts very recently
in (sahlgren et al., 2016) (centering the mean) and (arora et al., 2017) (nulling away only the    rst
principal component). although there is a super   cial similarity between our work and (arora et al.
2017), the nulling directions we take and the one they take are fundamentally different. speci   cally,
in arora et al. (2017), the    rst dominating vector is *dataset-speci   c*, i.e., they    rst compute the
sentence representation for the entire semantic textual similarity dataset, then extract the top direction
from those sentence representations and    nally project the sentence representation away from it.
by doing so, the top direction will inherently encode the common information across the entire
dataset, the top direction for the "headlines" dataset may encode common information about news
articles while the top direction for "twitter   15" may encode the common information about tweets.
in contrast, our dominating vectors are over the entire vocabulary of the language.

2

published as a conference paper at iclr 2018

more generally, the idea of removing the top principal components has been studied in the context of
positive-valued, high-dimensional data matrix analysis (bullinaria & levy, 2012; price et al., 2006).
bullinaria & levy (2012) posits that the highest variance components of the cooccurrence matrix are
corrupted by information other than lexical semantics, thus heuristically justifying the removal of
the top principal components. a similar idea appears in the context of population matrix analysis
(price et al., 2006), where the entries are also all positive. our postprocessing operation is on dense
low-dimensional representations (with both positive and negative entries).
we posit that the postprocessing operation makes the representations more    isotropic    with stronger
self-id172 properties     discussed in detail in section 2 and appendix a. our main point is
that this isotropy condition can be explicitly enforced to come up with new embedding algorithms (of
which our proposed post-processing is a simple and practical version).

2 postprocessing

we test our observations on various word representations: four publicly available word representations
(id971 (mikolov et al., 2013) trained using google news, glove2 (pennington et al., 2014)
trained using common crawl, rand-walk (arora et al., 2016) trained using wikipedia and
tscca3 trained using english gigaword) and two self-trained word representations using cbow
and skip-gram (mikolov et al., 2013) on the 2010 wikipedia corpus from (al-rfou et al., 2013). the
detailed statistics for all representations are listed in table 1. for completeness, we also consider the
representations on other languages: a detailed study is provided in appendix c.2.

language corpus
english
id97
glove
english
rand-walk english
english
cbow
skip-gram
english

google news
common crawl
wikipedia
wikipedia
wikipedia

dim vocab size
3,000,000
300
2,196,017
300
300
68, 430
1,028,961
300
300
1,028,961

avg. (cid:107)v(w)(cid:107)2
2.04
8.30
2.27
1.14
2.32

(cid:107)  (cid:107)2
0.69
3.15
0.70
0.29
1.25

table 1: a detailed description for the embeddings in this paper.

average of all v(w)   s, i.e.,    = 1/|v|(cid:80)

let v(w)     rd be a word representation for a given word w in the
vocabulary v. we observe the following two phenomena in each of
the word representations listed above:
    {v(w) : w     v} are not of zero-mean: i.e., all v(w) share a
non-zero common vector, v(w) =   v(w) +   , where    is the
w   v v(w). the norm of
   is approximately 1/6 to 1/2 of the average norm of all v(w) (cf.
table 1).
    {  v(w) : w     v} are not isotropic: let u1, ..., ud be the    rst to the
last components recovered by the principal component analysis
(pca) of {  v(w) : w     v}, and   1, ...,   d be the corresponding
normalized variance ratio. each   v(w) can be written as a linear
i=1   i(w)ui. as shown in figure 1,
we observe that   i decays near exponentially for small values of
i and remains roughly constant over the later ones. this suggests
there exists d such that   i (cid:29)   j for all i     d and j (cid:29) d; from figure 1 one observes that d is
roughly 10 with dimension d = 300.

combinations of u:   v(w) =(cid:80)d

figure 1: the decay of the
normalized singular values of
word representation.

angular asymmetry of representations a modern understanding of word representations in-
volves either pmi-based (including id97 (mikolov et al., 2010; levy & goldberg, 2014) and
glove (pennington et al., 2014)) or cca-based spectral factorization approaches. while cca-based

1https://code.google.com/archive/p/id97/
2https://github.com/stanfordnlp/glove
3http://www.pdhillon.com/code.html

3

100101102103index0.0000.0050.0100.0150.0200.0250.0300.0350.0400.045variance ratiogloverand_walkid97cbowskip-grampublished as a conference paper at iclr 2018

spectral factorization methods have long been understood from a probabilistic (i.e., generative model)
view point (browne, 1979; hotelling, 1936) and recently in the nlp context (stratos et al., 2015),
a corresponding effort for the pmi-based methods has only recently been conducted in an inspired
work (arora et al., 2016).
arora et al. (2016) propose a generative model (named rand-walk) of sentences, where every
word is parameterized by a d-dimensional vector. with a key postulate that the word vectors are
angularly uniform (   isotropic"), the family of pmi-based word representations can be explained
under the rand-walk model in terms of the maximum likelihood rule. our observation that word
vectors learnt through pmi-based approaches are not of zero-mean and are not isotropic (c.f. section
2) contradicts with this postulate. the isotropy conditions are relaxed in section 2.2 of (arora et al.,
2016), but the match with the spectral properties observed in figure 1 is not immediate.
this contradiction is explicitly resloved by relaxing the constraints on the word vectors to directly    t
the observed spectral properties. the relaxed conditions are: the word vectors should be isotropic
around a point (whose distance to the origin is a small fraction of the average norm of word vectors)
lying on a low dimensional subspace. our main result is to show that even with this enlarged
parameter-space, the maximum likelihood rule continues to be close to the pmi-based spectral
factorization methods. a brief summary of rand-walk, and the mathematical connection between
our work and theirs, are explored in detail in appendix a.

2.1 algorithm

since all word representations share the same common vector    and have the same dominating
directions and such vector and directions strongly in   uence the word representations in the same way,
we propose to eliminate them, as formally achieved as algorithm 1.

:word representations {v(w), w     v}, a threshold parameter d,

(cid:80)
w   v v(w),   v(w)     v(w)       
3 preprocess the representations: v(cid:48)(w)       v(w)    (cid:80)d

algorithm 1: postprocessing algorithm on word representations.
input
1 compute the mean of {v(w), w     v},        1|v|
2 compute the pca components: u1, ..., ud     pca({  v(w), w     v}).
output :processed representations v(cid:48)(w).

(cid:0)u(cid:62)
i v(w)(cid:1) ui

i=1

signi   cance of nulled vectors consider the representation of the words as viewed in terms of
the top d pca coef   cients   (cid:96)(w), for 1     (cid:96)     d. we    nd that these few coef   cients encode
the frequency of the word to a signi   cant degree; figure 2 illustrates the relation between the
(  1(w),   2(w)) and the unigram probabilty p(w), where the correlation is geometrically visible.

figure 2: the top two pca directions (i.e,   1(w) and   2(w)) encode frequency.

discussion in our proposed processing algorithm, the number of components to be nulled, d, is
the only hyperparameter that needs to be tuned. we    nd that a good rule of thumb is to choose d
approximately to be d/100, where d is the dimension of a word representation. this is empirically
justi   ed in the experiments of the following section where d = 300 is standard for published word
representations. we trained word representations for higher values of d using the id97 and

4

published as a conference paper at iclr 2018

glove algorithms and repeated these experiments; we see corresponding consistent improvements
due to postprocessing in appendix c.

2.2 postprocessing as a    rounding    towards isotropy

the idea of isotropy comes from the partition function de   ned in (arora et al., 2016),

exp(cid:0)c(cid:62)v(w)(cid:1) ,

(cid:88)

w   v

z(c) =

where z(c) should approximately be a constant with any unit vector c (c.f. lemma 2.1 in (arora
et al., 2016)). hence, we mathematically de   ne a measure of isotropy as follows,

i({v(w)}) =

min(cid:107)c(cid:107)=1 z(c)
max(cid:107)c(cid:107)=1 z(c)

,

(1)

where i({v(w)}) ranges from 0 to 1, and i({v(w)}) closer to 1 indicates that {v(w)} is more
isotropic. the intuition behind our postprocessing algorithm can also be motivated by letting
i({v(w)})     1.
let v be the matrix stacked by all word vectors, where the rows correspond to word vectors, and
1|v| be the |v|-dimensional vectors with all entries equal to one, z(c) can be equivalently de   ned as
follows,

z(c) = |v| + 1|v|(cid:62)v c +

1
k!
i({v(w)}) is, therefore, can be very coarsely approximated by,

c(cid:62)v (cid:62)v c +

1
2

k=3

   (cid:88)

(cid:88)

w   v

(c(cid:62)v(w))k.

    a    rst order approximation:

i({v(w)})    

|v| + min(cid:107)c(cid:107)=1 1(cid:62)
|v| + max(cid:107)c(cid:107)=1 1(cid:62)

|v|v c
|v|v c

=

|v|v (cid:107) = 0, which is equivalent to(cid:80)

|v|     (cid:107)1(cid:62)
|v| + (cid:107)1(cid:62)

|v|v (cid:107)
|v|v (cid:107) .

letting i({v(w)}) = 1 yields (cid:107)1(cid:62)
w   v v(w) = 0. the
intuition behind the    rst order approximation matches with the    rst step of the proposed
algorithm, where we enforce v(w) to have a zero mean.

    a second order approximation:
|v| + min(cid:107)c(cid:107)=1 1(cid:62)
|v| + max(cid:107)c(cid:107)=1 1(cid:62)

i({v(w)})    

|v|v c + min(cid:107)c(cid:107)=1
|v|v c + max(cid:107)c(cid:107)=1

1

2 c(cid:62)v (cid:62)v c
2 c(cid:62)v (cid:62)v c

1

=

|v|     (cid:107)1(cid:62)
|v| + (cid:107)1(cid:62)

|v|v (cid:107) + 1
|v|v (cid:107) + 1

2   2
2   2

min

,

max

where   min and   max are the smallest and largest singular value of v , respectively. letting
|v|v (cid:107) = 0 and   min =   max. the fact that   min =   max suggests
i({v(w)}) = 1 yields (cid:107)1(cid:62)
the spectrum of v(w)   s should be    at. the second step of the proposed algorithm removes
the highest singular values, and therefore explicitly    atten the spectrum of v .

empirical veri   cation indeed, we empirically validate the effect of postprocessing of on
i({v(w)}). since there is no closed-form solution for arg max(cid:107)c(cid:107)=1 z(c) or arg min(cid:107)c(cid:107)=1 z(c),
and it is impossible to enumerate all c   s, we estimate the measure by,

i({v(w)})     minc   c z(c)
maxc   c z(c)

,

where c is the set of eigenvectors of v (cid:62)v . the value of i({v(w)}) for the original vectors and
processed ones are reported in table 2, where we can observe that the degree of isotropy vastly
increases in terms of this measure.

5

published as a conference paper at iclr 2018

id97

glove

before

0.7
0.065

after
0.95
0.6

table 2: before-after on the measure of isotropy.

a formal way to verify the isotropy property is to
directly check if the    self-id172" property
(i.e., z(c) is a constant, independent of c (andreas
& klein, 2015)) holds more strongly. such a vali-
dation is seen diagrammatically in figure 3 where
we randomly sampled 1,000 c   s as (arora et al.,
2016).

(a) id97

(b) glove

figure 3: the histogram of z(c) for 1,000 randomly sampled vectors c of unit norm, where x-axis is
normalized by the mean of all values and d = 2 for glove and d = 3 for id97.

3 experiments

given the popularity and widespread use of id97 (mikolov et al., 2013) and glove
(pennington et al., 2014), we use their publicly available pre-trained reprepsentations in the following
experiments. we choose d = 3 for id97 and d = 2 for glove. the key underlying
principle behind word representations is that similar words should have similar representations.
following the tradition of evaluating word representations (schnabel et al., 2015; baroni et al.,
2014), we perform three canonical lexical-level tasks: (a) word similarity; (b) concept categorization;
(c) word analogy; and one sentence-level task: (d) semantic textual similarity. the processed
representations consistently improve performance on all three of them, and especially strongly on the
   rst two.

glove

proc.
74.36
76.79
52.04
81.78
70.85
44.97
32.23

orig.
76.96
73.79
46.41
80.49
69.29
40 83
28.33

id97
proc.
orig.
78.34
76.08
rg65
69.05
ws
68.29
54.33
rw 53.74
79.08
men 78.20
69.35
68.23
mturk
45.10
44.20
siid113x
36.50
simverb
36.35

word similarity the word similarity task is as
follows: given a pair of words, the algorithm as-
signs a    similarity" score     if the pair of words
are highly related then the score should also be
high and vice versa. the algorithm is evaluated in
terms of spearman   s rank correlation compared to
(a gold set of) human judgements.
for this experiment, we use seven standard
datasets: the    rst published rg65 dataset (ruben-
stein & goodenough, 1965); the widely used
wordsim-353 (ws) dataset (finkelstein et al.,
2001) which contains 353 pairs of commonly used
verbs and nouns; the rare-words (rw) dataset (lu-
ong et al., 2013) composed of rarely used words; the men dataset (bruni et al., 2014) where the 3000
pairs of words are rated by crowdsourced participants; the mturk dataset (radinsky et al., 2011)
where the 287 pairs of words are rated in terms of relatedness; the siid113x-999 (siid113x) dataset (hill
et al., 2016) where the score measures    genuine" similarity; and lastly the simverb-3500 (simverb)
dataset (gerz et al., 2016), a newly released large dataset focusing on similarity of verbs.
in our experiment, the algorithm scores the similarity between two words by the cosine similarity
1 v2/(cid:107)v1(cid:107)(cid:107)v2(cid:107)). the detailed
between the two corresponding word vectors (cossim(v1, v2) = v(cid:62)
performance on the seven datasets is reported in table 3, where we see a consistent and signi   -
cant performance improvement due to postprocessing, across all seven datasets. these statistics

table 3: before-after results (x100) on word
similarity task on seven datasets.

6

0.800.850.900.951.001.051.101.151.20partition function z(c)02004006008001000frequencybeforeafter0.70.80.91.01.11.21.3partition function z(c)02004006008001000frequencybeforeafterpublished as a conference paper at iclr 2018

(average improvement of 2.3%) suggest that by removing the common parts, the remaining word
representations are able to capture stronger semantic relatedness/similarity between words.

id97
proc.
orig.
57.72
54.43
84.09
75.00
81.71
71.97

ap
esslli
battig

glove

orig.
64 .18
81.82
86.59

proc.
65.42
81.82
86.59

table 4: before-after results (x100) on the
categorization task.

concept categorization this task is an indirect
evaluation of the similarity principle: given a set of
concepts, the algorithm needs to group them into
different categories (for example,    bear    and    cat   
are both animals and    city    and    country    are both
related to districts). the id91 performance is
then evaluated in terms of purity (manning et al.,
2008)     the fraction of the total number of the objects
that were classi   ed correctly.

we conduct this task on three different datasets: the almuhareb-poesio (ap) dataset (almuhareb,
2006) contains 402 concepts which fall into 21 categories; the esslli 2008 distributional semantic
workshop shared-task dataset (baroni et al., 2008) that contains 44 concepts in 6 categories; and the
battig test set (baroni & lenci, 2010) that contains 83 words in 10 categories.
here we follow the setting and the proposed algorithm in (baroni et al., 2014; schnabel et al., 2015)
    we cluster words (via their representations) using the classical id116 algorithm (with    xed
k). again, the processed vectors perform consistently better on all three datasets (with average
improvement of 2.5%); the full details are in table 4.

glove

proc.
75.40
79.25
77.15

orig.
74.95
79.22
76.89

syntax
semantics
all

id97
proc.
orig.
73.50
73.46
73.36
72.28
73.44
72.93

table 5: before-after results (x100) on the word
analogy task.

word analogy the analogy task tests to what
extent the word representations can encode latent
linguistic relations between a pair of words. given
three words w1, w2, and w3, the analogy task re-
quires the algorithm to    nd the word w4 such that
w4 is to w3 as w2 is to w1.
we use the analogy dataset introduced in (mikolov
et al., 2013). the dataset can be divided into two
parts: (a) the semantic part containing around 9k
questions, focusing on the latent semantic relation between pairs of words (for example, what is
to chicago as texas is to houston); and (b) the syntatic one containing roughly 10.5k questions,
focusing on the latent syntatic relation between pairs of words (for example, what is to    amazing    as
   apprently    is to    apparent   ).
in our setting, we use the original algorithm introduced in (mikolov et al., 2013) to solve this problem,
i.e., w4 is the word that maximize the cosine similarity between v(w4) and v(w2)     v(w1) + v(w3).
the average performance on the analogy task is provided in table 5 (with a detailed performance
provided in table 19 in appendix d). it can be noticed that while postprocessing continues to improve
the performance, the improvement is not as pronounced as earlier. we hypothesize that this is because
the mean and some dominant components get canceled during the subtraction of v(w2) from v(w1),
and therefore the effect of postprocessing is less relevant.

id97
proc.
orig.
57.67
57.22
2012
57.98
56.81
2013
63.30
62.89
2014
63.35
2015
62.74
70 20
sick 70.10
61.45
60.88

all

glove

orig.
48.27
44.83
51.11
47.23
65.14
49.19

proc.
54.06
57.71
59.23
57.29
67.85
56.76

table 6: before-after results (x100) on the
semantic textual similarity tasks.

semantic textual similarity extrinsic evalua-
tions measure the contribution of a word representa-
tion to speci   c downstream tasks; below, we study
the effect of postprocessing on a standard sentence
modeling task     semantic textual similarity (sts)
which aims at testing the degree to which the algo-
rithm can capture the semantic equivalence between
two sentences. for each pair of sentences, the algo-
rithm needs to measure how similar the two sentences
are. the degree to which the measure matches with
human judgment (in terms of pearson correlation) is
an index of the algorithm   s performance. we test the
word representations on 20 textual similarity datasets

7

published as a conference paper at iclr 2018

from the 2012-2015 semeval sts tasks (agirre et al., 2012; 2013; 2014; 2015), and the 2012
semeval semantic related task (sick) (marelli et al., 2014).
representing sentences by the average of their constituent word representations is surprisingly
effective in encoding the semantic information of sentences (wieting et al., 2015; adi et al., 2016)
and close to the state-of-the-art in these datasets. we follow this rubric and represent a sentence
s based on its averaged word representation, i.e., v(s) = 1|s|
w   s v(w), and then compute the
similarity between two sentences via the cosine similarity between the two representations. the
average performance of the original and processed representations is itemized in table 6 (with a
detailed performance in table 20 in appendix e)     we see a consistent and signi   cant improvement
in performance because of postprocessing (on average 4% improvement).

(cid:80)

4 postprocessing and supervised classification

supervised downstream nlp applications have greatly improved their performances in recent years
by combining the discriminative learning powers of neural networks in conjunction with the word
representations. we evaluate the performance of a variety of neural network architectures on a standard
and important nlp application: text classi   cation, with id31 being a particularly
important and popular example. the task is de   ned as follows: given a sentence, the algorithm needs
to decide which category it falls into. the categories can be either binary (e.g., positive/negative) or
can be more    ne-grained (e.g. very positive, positive, neutral, negative, and very negative).
we evaluate the word representations (with and without postprocessing) using four different neural
network architectures (id98, vanilla-id56, gru-id56 and lstm-id56) on    ve benchmarks: (a) the
movie review (mr) dataset (pang & lee, 2005); (b) the subjectivity (subj) dataset (pang & lee,
2004); (c) the trec question dataset (li & roth, 2002); (d) the imdb dataset (maas et al., 2011);
(e) the stanford sentiment treebank (sst) dataset (socher et al., 2013a). a detailed description of
these standard datasets, their training/test parameters and the cross validation methods adopted is
in appendix f. speci   cally, we allow the parameter d (i.e., the number of nulled components) to
vary between 0 and 4, and the best performance of the four neural network architectures with the
now-standard id98-based text classi   cation algorithm (kim, 2014) (implemented using tensor   ow4)
is itemized in table 7. the key observation is that the performance of postprocessing is better in a
majority (34 out of 40) of the instances by 2.32% on average, and in the rest the instances the two
performances (with and without postprocessing) are comparable.

id97
proc.
orig.
71.27
mr 70.80
87.33
subj
87.14
89.00
trec 87.80
sst 38.46
38.33
87.12
86.68

imdb

id98

glove

orig.
71.01
86.98
87.60
38.82
87.27

proc.
71.11
87.25
89.00
37.83
87.10

vanilla-id56

glove

gru-id56

glove

lstm-id56

glove

id97
proc.
orig.
74.95
74.01
87.60
82.85
89.20
80.60
42.08
39.91
53.14
50.15

orig.
71.14
81.45
85.20
41.45
52.76

proc.
72.56
87.37
89.00
41.90
76.07

id97
proc.
orig.
78.26
77.86
91.10
90.96
92.40
91.60
45.02
41.86
83.47
82.96

orig.
74.98
91.16
91.60
36.52
81.50

proc.
75.13
91.85
93.00
37.69
82.44

id97
proc.
orig.
77.34
75.69
90.54
90.23
91.20
88.00
43.08
42.08
82.60
81.29

orig.
72.02
90.74
85.80
37.51
79.10

proc.
71.84
90.82
91.20
38.05
81.33

table 7: before-after results (x100) on the text classi   cation task using id98 (kim, 2014) and vanilla
id56, gru-id56 and lstm-id56.

a further validation of the postprocessing operation in a variety of downstream applications (eg:
id39, syntactic parsers, machine translation) and classi   cation methods (eg:
id79s, neural network architectures) is of active research interest. of particular interest is
the impact of the postprocessing on the rate of convergence and generalization capabilities of the
classi   ers. such a systematic study would entail a concerted and large-scale effort by the research
community and is left to future research.

discussion all neural network architectures, ranging from feedforward to recurrent (either vanilla
or gru or lstm), implement at least linear processing of hidden/input state vectors at each of their
nodes; thus the postprocessing operation suggested in this paper can in principle be automatically
   learnt    by the neural network, if such internal learning is in-line with the end-to-end training examples.
yet, in practice this is complicated due to limitations of optimization procedures (sgd) and sample

4https://github.com/dennybritz/id98-text-classification-tf

8

published as a conference paper at iclr 2018

noise. we conduct a preliminary experiment in appendix b and show that subtracting the mean (i.e.,
the    rst step of postprocessing) is    effectively learnt" by neural networks within their nodes.

5 conclusion

we present a simple postprocessing operation that renders word representations even stronger, by
eliminating the top principal components of all words. such an simple operation could be used for
id27s in downstream tasks or as intializations for training task-speci   c embeddings. due
to their popularity, we have used the published representations of id97 and glove in en-
glish in the main text of this paper; postprocessing continues to be successful for other representations
and in multilingual settings     the detailed empirical results are tabulated in appendix c.

references
yossi adi, einat kermany, yonatan belinkov, ofer lavi, and yoav goldberg. fine-grained analysis
of sentence embeddings using auxiliary prediction tasks. arxiv preprint arxiv:1608.04207, 2016.

eneko agirre, mona diab, daniel cer, and aitor gonzalez-agirre. semeval-2012 task 6: a pilot
in proceedings of the first joint conference on lexical and
on semantic textual similarity.
computational semantics-volume 1: proceedings of the main conference and the shared task, and
volume 2: proceedings of the sixth international workshop on semantic evaluation, pp. 385   393.
association for computational linguistics, 2012.

eneko agirre, daniel cer, mona diab, aitor gonzalez-agirre, and weiwei guo. sem 2013 shared
task: semantic textual similarity, including a pilot on typed-similarity. in in* sem 2013: the
second joint conference on lexical and computational semantics. association for computational
linguistics. citeseer, 2013.

eneko agirre, carmen banea, claire cardie, daniel cer, mona diab, aitor gonzalez-agirre, weiwei
guo, rada mihalcea, german rigau, and janyce wiebe. semeval-2014 task 10: multilingual se-
mantic textual similarity. in proceedings of the 8th international workshop on semantic evaluation
(semeval 2014), pp. 81   91, 2014.

eneko agirre, carmen baneab, claire cardiec, daniel cerd, mona diabe, aitor gonzalez-agirrea,
weiwei guof, inigo lopez-gazpioa, montse maritxalara, rada mihalceab, et al. semeval-2015
task 2: semantic textual similarity, english, spanish and pilot on interpretability. in proceedings of
the 9th international workshop on semantic evaluation (semeval 2015), pp. 252   263, 2015.

rami al-rfou, bryan perozzi, and steven skiena. polyglot: distributed word representations for

multilingual nlp. arxiv preprint arxiv:1307.1662, 2013.

abdulrahman almuhareb. attributes in lexical acquisition. phd thesis, university of essex, 2006.

jacob andreas and dan klein. when and why are id148 self-normalizing? in hlt-

naacl, pp. 244   249, 2015.

sanjeev arora, yuanzhi li, yingyu liang, tengyu ma, and andrej risteski. a latent variable model
approach to pmi-based id27s. transactions of the association for computational
linguistics, 4:385   399, 2016.
issn 2307-387x. url https://transacl.org/ojs/
index.php/tacl/article/view/742.

sanjeev arora, yingyu liang, and tengyu ma. a simple but tough-to-beat baseline for sentence

embeddings. in international conference on learning representations., 2017.

dzmitry bahdanau, kyunghyun cho, and yoshua bengio. id4 by jointly

learning to align and translate. arxiv preprint arxiv:1409.0473, 2014.

m baroni, s evert, and a lenci. bridging the gap between semantic theory and computational
simulations: proceedings of the esslli workshop on distributional lexical semantics. hamburg,
germany: folli, 2008.

9

published as a conference paper at iclr 2018

marco baroni and alessandro lenci. distributional memory: a general framework for corpus-based

semantics. computational linguistics, 36(4):673   721, 2010.

marco baroni, georgiana dinu, and germ  n kruszewski. don   t count, predict! a systematic
comparison of context-counting vs. context-predicting semantic vectors. in acl (1), pp. 238   247,
2014.

yoshua bengio, r  jean ducharme, pascal vincent, and christian jauvin. a neural probabilistic

language model. journal of machine learning research, 3(feb):1137   1155, 2003.

antoine bordes, nicolas usunier, alberto garcia-duran, jason weston, and oksana yakhnenko.
translating embeddings for modeling multi-relational data. in advances in neural information
processing systems, pp. 2787   2795, 2013.

michael w browne. the maximum-likelihood solution in inter-battery factor analysis. british

journal of mathematical and statistical psychology, 32(1):75   86, 1979.

elia bruni, nam-khanh tran, and marco baroni. multimodal id65. j. artif. intell.

res.(jair), 49(1-47), 2014.

john a bullinaria and joseph p levy. extracting semantic representations from word co-occurrence

statistics: stop-lists, id30, and svd. behavior research methods, 44(3):890   907, 2012.

jos   camacho-collados, mohammad taher pilehvar, and roberto navigli. a framework for the
construction of monolingual and cross-lingual word similarity datasets. in acl (2), pp. 1   7, 2015.

junyoung chung, caglar g  l  ehre, kyunghyun cho, and yoshua bengio. gated feedback recurrent

neural networks. in icml, pp. 2067   2075, 2015.

ronan collobert, jason weston, l  on bottou, michael karlen, koray kavukcuoglu, and pavel kuksa.
natural language processing (almost) from scratch. journal of machine learning research, 12
(aug):2493   2537, 2011.

paramveer dhillon, jordan rodu, dean foster, and lyle ungar. two step cca: a new spectral method

for estimating vector models of words. arxiv preprint arxiv:1206.6403, 2012.

lev finkelstein, evgeniy gabrilovich, yossi matias, ehud rivlin, zach solan, gadi wolfman, and
eytan ruppin. placing search in context: the concept revisited. in proceedings of the 10th
international conference on world wide web, pp. 406   414. acm, 2001.

daniela gerz, ivan vuli  c, felix hill, roi reichart, and anna korhonen. simverb-3500: a large-scale

evaluation set of verb similarity. arxiv preprint arxiv:1608.00869, 2016.

klaus greff, rupesh k srivastava, jan koutn  k, bas r steunebrink, and j  rgen schmidhuber. lstm:

a search space odyssey. ieee transactions on neural networks and learning systems, 2016.

felix hill, roi reichart, and anna korhonen. siid113x-999: evaluating semantic models with

(genuine) similarity estimation. computational linguistics, 2016.

harold hotelling. relations between two sets of variates. biometrika, 28(3/4):321   377, 1936.

eric h huang, richard socher, christopher d manning, and andrew y ng.

improving word
representations via global context and multiple word prototypes. in proceedings of the 50th annual
meeting of the association for computational linguistics: long papers-volume 1, pp. 873   882.
association for computational linguistics, 2012.

yoon kim. convolutional neural networks for sentence classi   cation. arxiv preprint arxiv:1408.5882,

2014.

beatrice laurent and pascal massart. adaptive estimation of a quadratic functional by model selection.

annals of statistics, pp. 1302   1338, 2000.

omer levy and yoav goldberg. neural id27 as implicit id105. in advances

in neural information processing systems, pp. 2177   2185, 2014.

10

published as a conference paper at iclr 2018

xin li and dan roth. learning question classi   ers. in proceedings of the 19th international confer-
ence on computational linguistics-volume 1, pp. 1   7. association for computational linguistics,
2002.

thang luong, richard socher, and christopher d manning. better word representations with

id56s for morphology. in conll, pp. 104   113, 2013.

andrew l maas, raymond e daly, peter t pham, dan huang, andrew y ng, and christopher
potts. learning word vectors for id31. in proceedings of the 49th annual meeting
of the association for computational linguistics: human language technologies-volume 1, pp.
142   150. association for computational linguistics, 2011.

christopher d manning, prabhakar raghavan, hinrich sch  tze, et al. introduction to information

retrieval, volume 1. cambridge university press cambridge, 2008.

marco marelli, stefano menini, marco baroni, luisa bentivogli, raffaella bernardi, and roberto
zamparelli. a sick cure for the evaluation of compositional distributional semantic models. in
lrec, pp. 216   223, 2014.

tomas mikolov, martin kara     t, lukas burget, jan cernock`y, and sanjeev khudanpur. recurrent

neural network based language model. in interspeech, volume 2, pp. 3, 2010.

tomas mikolov, kai chen, greg corrado, and jeffrey dean. ef   cient estimation of word representa-

tions in vector space. arxiv preprint arxiv:1301.3781, 2013.

andriy mnih and geoffrey hinton. three new id114 for statistical language modelling. in
proceedings of the 24th international conference on machine learning, pp. 641   648. acm, 2007.

bo pang and lillian lee. a sentimental education: id31 using subjectivity summa-
rization based on minimum cuts. in proceedings of the 42nd annual meeting on association for
computational linguistics, pp. 271. association for computational linguistics, 2004.

bo pang and lillian lee. seeing stars: exploiting class relationships for sentiment categorization
in proceedings of the 43rd annual meeting on association for

with respect to rating scales.
computational linguistics, pp. 115   124. association for computational linguistics, 2005.

jeffrey pennington, richard socher, and christopher d manning. glove: global vectors for word

representation. in emnlp, volume 14, pp. 1532   43, 2014.

alkes l price, nick j patterson, robert m plenge, michael e weinblatt, nancy a shadick, and david
reich. principal components analysis corrects for strati   cation in genome-wide association studies.
nature genetics, 38(8):904   909, 2006.

kira radinsky, eugene agichtein, evgeniy gabrilovich, and shaul markovitch. a word at a
time: computing word relatedness using temporal semantic analysis. in proceedings of the 20th
international conference on world wide web, pp. 337   346. acm, 2011.

herbert rubenstein and john b goodenough. contextual correlates of synonymy. communications

of the acm, 8(10):627   633, 1965.

magnus sahlgren, amaru cuba gyllensten, fredrik espinoza, ola hamfors, jussi karlgren, fredrik
olsson, per persson, akshay viswanathan, and anders holst. the gavagai living lexicon. in
nicoletta calzolari (conference chair), khalid choukri, thierry declerck, sara goggi, marko
grobelnik, bente maegaard, joseph mariani, helene mazo, asuncion moreno, jan odijk, and
stelios piperidis (eds.), proceedings of the tenth international conference on language resources
and evaluation (lrec 2016), paris, france, may 2016. european language resources association
(elra). isbn 978-2-9517408-9-1.

tobias schnabel, igor labutov, david mimno, and thorsten joachims. evaluation methods for

unsupervised id27s. in proc. of emnlp, 2015.

richard socher, danqi chen, christopher d manning, and andrew ng. reasoning with neural tensor
networks for knowledge base completion. in advances in neural information processing systems,
pp. 926   934, 2013a.

11

published as a conference paper at iclr 2018

richard socher, alex perelygin, jean y wu, jason chuang, christopher d manning, andrew y ng,
and christopher potts. recursive deep models for semantic compositionality over a sentiment
treebank. in proceedings of the conference on empirical methods in natural language processing
(emnlp), volume 1631, pp. 1642. citeseer, 2013b.

karl stratos, michael collins, and daniel hsu. model-based id27s from decompositions

of count matrices. in proceedings of acl, pp. 1282   1291, 2015.

ilya sutskever, oriol vinyals, and quoc v le. sequence to sequence learning with neural networks.

in advances in neural information processing systems, pp. 3104   3112, 2014.

joseph turian, lev ratinov, and yoshua bengio. word representations: a simple and general method
for semi-supervised learning. in proceedings of the 48th annual meeting of the association for
computational linguistics, pp. 384   394. association for computational linguistics, 2010.

john wieting, mohit bansal, kevin gimpel, karen livescu, and dan roth. from paraphrase database

to compositional paraphrase model and back. arxiv preprint arxiv:1506.03487, 2015.

torsten zesch and iryna gurevych. automatically creating datasets for measures of semantic
relatedness. in proceedings of the workshop on linguistic distances, pp. 16   24. association for
computational linguistics, 2006.

12

published as a conference paper at iclr 2018

appendix: all-but-the-top: simple and effective postprocessing for

word representations

a angular asymmetry of representations

a modern understanding of word representations involves either pmi-based (including id97
(mikolov et al., 2010; levy & goldberg, 2014) and glove (pennington et al., 2014)) or cca-based
spectral factorization approaches. while cca-based spectral factorization methods have long been
understood from a probabilistic (i.e., generative model) view point (browne, 1979; hotelling, 1936)
and recently in the nlp context (stratos et al., 2015), a corresponding effort for the pmi-based
methods has only recently been conducted in an inspired work (arora et al., 2016).
(arora et al., 2016) propose a generative model (named rand-walk) of sentences, where every
word is parameterized by a d-dimensional vector. with a key postulate that the word vectors are
angularly uniform (   isotropic"), the family of pmi-based word representations can be explained
under the rand-walk model in terms of the maximum likelihood rule. our observation that word
vectors learnt through pmi-based approaches are not of zero-mean and are not isotropic (c.f. section
2) contradicts with this postulate. the isotropy conditions are relaxed in section 2.2 of (arora et al.,
2016), but the match with the spectral properties observed in figure 1 is not immediate.
in this section, we resolve this by explicitly relaxing the constraints on the word vectors to directly    t
the observed spectral properties. the relaxed conditions are: the word vectors should be isotropic
around a point (whose distance to the origin is a small fraction of the average norm of word vectors)
lying on a low dimensional subspace. our main result is to show that even with this enlarged
parameter-space, the maximum likelihood rule continues to be close to the pmi-based spectral
factorization methods. formally, the model, the original constraints of (arora et al., 2016) and the
enlarged constraints on the word vectors are listed below:

    a generative model of sentences: the word at time t, denoted by wt, is generated via a

log-linear model with a latent discourse variable ct (arora et al., 2016), i.e.,

p(wt|ct) =

1

(2)
where v(w)     rd is the vector representation for a word w in the vocabulary v , ct is the
latent variable which forms a    slowly moving" random walk, and the partition function:
    constraints on the word vectors: (arora et al., 2016) suppose that there is a bayesian

w   v exp(cid:0)c(cid:62)v(w)(cid:1).

z(c) =(cid:80)

z(ct)

exp(cid:0)c(cid:62)

t v(wt)(cid:1) ,

priori on the word vectors:

the ensemble of word vectors consists of i.i.d. draws generated by v = s      v,
where   v is from the spherical gaussian distribution, and s is a scalar random
variable.

a deterministic version of this prior is discussed in section 2.2 of (arora et al., 2016), but
part of these (relaxed) conditions on the word vectors are speci   cally meant for theorem 4.1
and not the main theorem (theorem 2.2). the geometry of the word representations is only
evaluated via the ratio of the quadratic mean of the singular values to the smallest one being
small enough. this meets the relaxed conditions, but not suf   cient to validate the proof
approach of the main result (theorem 2.2); what would be needed is that the ratio of the
largest singular value to the smallest one be small.
    revised conditions: we revise the bayesian prior postulate (and in a deterministic fashion)
formally as follows: there is a mean vector   , d orthonormal vectors u1, . . . , ud (that are
orthogonal and of unit norm), such that every word vector v(w) can be represented by,

d(cid:88)

v(w) =    +

  i(w)ui +   v(w),

(3)

where    is bounded,   i is bounded by a, d is bounded by da2 = o(d),   v(w) are
statistically isotropic. by statistical isotropy, we mean: for high-dimensional rectangles r,

i=1

13

published as a conference paper at iclr 2018

w   v 1(  v(w)     r)    (cid:82)
(cid:80)

1|v|
i.e., f (  v) is a function of (cid:107)  v(cid:107).

r f (  v)d  v, as |v|        , where f is an angle-independent pdf,

the revised postulate differs from the original one in two ways: (a) it imposes a formal deterministic
constraint on the word vectors; (b) the revised postulate allows the word vectors to be angularly
asymmetric: as long as the energy in the direction of u1,. . . ,ud is bounded, there is no constraint on
the coef   cients. indeed, note that there is no constraint on   v(w) to be orthogonal to u1, . . . ud.

empirical validation we can verify that the enlarged conditions are met by the existing word
representations. speci   cally, the natural choice for    is the mean of the word representations and
u1 . . . ud are the singular vectors associated with the top d singular values of the matrix of word
vectors. we pick d = 20 for id97 and d = 10 for glove, and the corresponding value of
da2 for id97 and glove vectors are both roughly 40, respectively; both values are small
compared to d = 300.

figure 4: spectrum of the published id97 and glove and random gaussian matrices,
ignoring the top d components; d = 10 for glove and d = 20 for id97.

this leaves us to check the statistical isotropy of the    remaining" vectors   v(w) for words w in the
vocabulary. we do this by plotting the remaining spectrum (i.e. the (d + 1)-th, ..., 300th singular
values) for the published id97 and glove vectors in figure 4. as a comparison, the
empirical spectrum of a random gaussian matrix is also plotted in figure 4. we see that both
spectra are    at (since the vocabulary size is much larger than the dimension d = 300). thus the
postprocessing operation can also be viewed as a way of making the vectors    more isotropic   .

mathematical contribution under the revised postulate, we show that the main theorem in (arora
et al., 2016) (c.f. theorem 2.2) still holds. formally:

theorem a.1 suppose the word vectors satisfy the constraints. then
    v(w1)(cid:62)v(w2)

pmi(w1, w2) def= log

,

p(w1, w2)
p(w1)p(w2)

d

as |v|        ,

(4)

where p(w) is the unigram distribution induced from the model (2), and p(w1, w2) is the id203
that two words w1 and w2 occur with each other within distance q.

the proof is in appendix g. theorem a.1 suggests that the rand-walk generative model and its
properties proposed by (arora et al., 2016) can be generalized to a broader setting (with a relaxed
restriction on the geometry of word representations)     relevantly, this relaxation on the geometry of
word representations is empirically satis   ed by the vectors learnt as part of the maximum likelihood
rule.

b neural networks learn to postprocess

every neural network family posseses the ability to conduct linear processing inside their nodes; this
includes feedforward and recurrent and convolutional neural network models. thus, in principle,
the postprocessing operation can be    learnt and implemented" within the parameters of the neural
network. on the other hand, due to the large number of parameters within the neural network, it

14

101102index0.0000.0010.0020.0030.0040.0050.0060.0070.008variance ratiogloveid97randompublished as a conference paper at iclr 2018

is unclear how to verify such a process, even if it were learnt (only one of the layers might be
implementing the postprocessing operation or via a combination of multiple effects).
to address this issue, we have adopted a comparative approach in the rest of this section. the
comparative approach involves adding an extra layer interposed in between the inputs (which are
word vectors) and the rest of the neural network. this extra layer involves only linear processing.
next we compare the results of the    nal parameters of the extra layer (trained jointly with the
rest of tne neural network parameters, using the end-to-end training examples) with and without
preprocessing of the word vectors. such a comparative approach allows us to separate the effect of
the postprocessing operation on the word vectors from the complicated    semantics    of the neural
network parameters.

vanilla

gru

lstm

w.
mr 82.07
subj
84.02
trec 81.68
sst 79.64
93.48

imdb

g.

49.23
49.94
52.99
46.59
66.37

w.
81.35
83.15
82.68
78.06
94.49

g.

47.63
50.60
50.42
43.21
55.24

w.
77.95
83.39
80.80
77.72
87.27

g.

44.92
48.95
46.77
42.82
46.74

figure 5: time-expanded id56 architecture with
an appended layer involving linear bias.

figure 6: the cosine similarity (x100) between
bproc. +    and borig., where w. and g. stand for
id97 and glove respectively.

experiment we construct a modi   ed neural network by explicitly adding a    postprocessing unit"
as the    rst layer of the id56 architecture (as in figure 5, where the appended layer is used to test the
   rst step (i.e., remove the mean vector)) of the postprocessing algorithm.
in the modi   ed neural network, the input word vectors are now v(w)     b instead of v(w). here
b is a bias vector trained jointly with the rest of the neural network parameters. note that this is
only a relabeling of the parameters from the perspective of the id56 architecture: the nonlinear
activation function of the node is now operated on a(v(w)     b) + b(cid:48) = av(w) + (b(cid:48)     ab) instead
of the previous av(w) + b(cid:48). let bproc. and borig. be the inferred biases when using the processed and
original word representations, respectively.
we itemize the cosine similarity between bproc. +    and borig. in table 6 for the 5 different datasets
and 3 different neural network architectures. in each case, the cosine similarity is remarkably large
(on average 0.66, in 300 dimensions)     in other words, trained neural networks implicitly postprocess
the word vectors nearly exactly as we proposed. this agenda is successfully implemented in the
context of verifying the removal of the mean vector.
the second step of our postprocessing algorithm (i.e., nulling away from the top principal components)
i on the word vectors, where ui is
the i-th principal component and d is the number of the removed directions. a comparative analysis
effort for the second step (nulling the dominant pca directions) is the following. instead of applying
a bias term b, we multiply by a matrix q to simulate the projection operation. the input word
vectors are now qorig.v(w) instead of v(w) for the original word vectors, and qproc.p v(w) instead
of p v(w) for the processed vectors. testing the similarity between qorig.p and qproc., allows us to
verify if the neural network learns to conduct the projection operation as proposed.
in our experiment, we found that such a result cannot be inferred. one possibility is that there are too
many parameters in both qproc. and qorig., which adds randomness to the experiment. alternatively,
the neural network weights may not be able to learn the second step of the postprocessing operation
(indeed, in our experiments postprocessing signi   cantly boosted end-performance of neural network
architectures). a more careful experimental setup to test whether the second step of the postprocessing
operation is learnt is left as a future research direction.

is equivalent to applying a projection matrix p = i    (cid:80)d

i=1 uiu(cid:62)

15

softmaxnonlinear	unitnonlinear	unitnonlinear	unitlinear	biasv(w1)v(w2)v(wt)      v(w1) bv(w2) bv(wt) bh1h2htpublished as a conference paper at iclr 2018

c experiments on various representations

in the main text, we have reported empirical results for two published word representations:
id97 and glove, each in 300 dimensions. in this section, we report the results of the same
experiments in a variety of other settings to show the generalization capability of the postprocessing
operation: representations trained via id97 and glove algorithms in dimensions other
than 300, other representations algorithms (speci   cally tscca and rand-walk) and in multiple
languages.

c.1 statistics of id73 representations

we use the publicly available tscca representations (dhillon et al., 2012) on german, french,
spanish, italian, dutch and chinese. the detailed statistics can be found in table 8 and the decay of
their singular values are plotted in figure 7.

language corpus
tscca-en
english
tscca-de german
french
tscca-fr
spanish
tscca-es
tscca-it
italian
tscca-nl dutch
tscca-zh chinese

gigawords
newswire
gigaword
gigaword
newswire+wiki
newswire+wiki
gigaword

dim vocab size
200
200
200
200
200
200
200

300,000
300,000
300,000
300,000
300,000
300,000
300,000

avg. (cid:107)v(w)(cid:107)2
4.38
4.52
4.34
4.17
4.34
4.46
4.51

(cid:107)  (cid:107)2
0.78
0.79
0.81
0.79
0.79
0.72
0.89

table 8: a detailed description for the tscca embeddings in this paper.

figure 7: the decay of the normalized singular values of id73 representation.

c.2 multilingual generalization

in this section, we perform the word similarity task with the original and the processed tscca word
representations in german and spanish on three german similarity datasets (gur65     a german
version of the rg65 dataset, gur350, and zg222 in terms of relatedness) (zesch & gurevych, 2006)
and the spanish version of rg65 dataset (camacho-collados et al., 2015). the choice of d is 2 for
both german and spanish.
the experiment setup and the similarity scoring algorithm are the same as those in section 3. the
detailed experiment results are provided in table 9, from which we observe that the processed repre-
sentations are consistently better than the original ones. this provides evidence to the generalization
capabilities of the postprocessing operation to multiple languages (similarity datasets in spanish and
german were the only ones we could locate).

c.3 generalization to different representation algorithms

given the popularity and widespread use of id97 (mikolov et al., 2013) and glove
(pennington et al., 2014), the main text has solely focused on their published publicly avalable

16

100101102103index0.000.010.020.030.040.050.060.07variance ratiotscca-detscca-entscca-estscca-frtscca-ittscca-nltscca-zhpublished as a conference paper at iclr 2018

rg65

language tscca
spanish
gur65 german
gur350 german
zg222 german

orig.
60.33
61.75
44.91
30.37

proc.
60.37
64.39
46.59
32.92

table 9: before-after results (x100) on the word similarity task in multiple languages.

300-dimension representations. in this section, we show that the proposed postprocessing algorithm
generalizes to other representation methods. speci   cally, we demonstrate this on rand-walk
(obtained via personal communication) and tscca (publicly available) on all the experiments of
section 3. the choice of d is 2 for both rand-walk and tscca.
in summary, the performance improvements on the similarity task, the concept categorization task,
the analogy task, and the semantic textual similarity dataset are on average 2.23%, 2.39%, 0.11% and
0.61%, respectively. the detailed statistics are provided in table 10, table 11, table 12 and table 13,
respectively. these results are a testament to the generalization capabilities of the postprocessing
algorithm to other representation algorithms.

rand-walk
proc.
orig.
82.96
80.66
rg65
74.37
ws
65.89
51.23
rw 45.11
77.22
men 73.56
66.11
mturk
64.35
36.55
34.05
siid113x
21.84
simverb
16.05

tscca

orig.
47.53
54.21
43.96
65.48
59.65
34.86
23.79

proc.
47.67
54.35
43.72
65.62
60.03
34.91
23.83

table 10: before-after results (x100) on the word similarity task on seven datasets.

rand-walk
proc.
orig.
62.36
59.83
72.73
72.73
81.82
75.73

ap
esslli
battig

tscca

orig.
60.00
68.18
70.73

proc.
63.42
70.45
70.73

table 11: before-after results (x100) on the categorization task.

rand-walk
proc.
orig.
60.48
60.39
83.82
83.55
70.67
70.50

syn.
sem.
all

tscca

orig.
37.72
14.54
27.30

proc.
37.80
14.55
27.35

table 12: before-after results (x100) on the word analogy task.

c.4 role of dimensions

the main text has focused on the dimension choice of d = 300, due to its popularity. in this section
we explore the role of the dimension in terms of both choice of d and the performance of the
postprocessing operation     we do this by using skip-gram model on the 2010 snapshot of wikipedia
corpus (al-rfou et al., 2013) to train word representations. we    rst observe that the two phenomena
of section 2 continue to hold:

    from table 14 we observe that the ratio between the norm of    and the norm average of all

v(w) spans from 1/3 to 1/4;

17

published as a conference paper at iclr 2018

rand-walk
proc.
orig.
38.03
37.66
2012
37.47
36.85
2013
48.32
46.06
2014
51.76
2015
47.82
51.76
sick 51.58
44.67
43.48

all

tscca

orig.
44.51
43.21
52.85
56.22
56.15
50.01

proc.
44.63
42.74
52.87
56.14
56.11
50.23

table 13: before-after results (x100) on the semantic textual similarity tasks.

dim
avg. (cid:107)v(w)(cid:107)2
(cid:107)  (cid:107)2

300
4.51
1.74

400
5.17
1.76

500
5.91
1.77

600
6.22
1.78

700
6.49
1.79

800
6.73
1.80

900
6.95
1.81

1000
7.15
1.83

table 14: statistics on word representation of dimensions 300, 400, ..., and 1000 using the skip-gram
model.

    from figure 8 we observe that the decay of the variance ratios   i is near exponential for

small values of i and remains roughly constant over the later ones.

figure 8: the decay of the normalized singular values of word representations.

a rule of thumb choice of d is around d/100. we validate this claim empirically by performing the
tasks in section 3 on word representations of higher dimensions, ranging from 300 to 1000, where
we set the parameter d = d/100. in summary, the performance improvement on the four itemized
tasks of section 3 are 2.27%, 3.37%, 0.01 and 1.92% respectively; the detailed results can be found
in table 15, table 16, table 17, and table 18. again, note that the improvement for analogy tasks is
marginal. these experimental results justify the rule-of-thumb setting of d = d/100, although we
emphasize that the improvements can be further accentuated by tuning the choice of d based on the
speci   c setting.

d experiments on word analogy task

the detailed performance on the analogy task is provided in table 19.

e experiments on semantic textual similarity task

the detailed performance on the semantic textual similarity is provided in table 20.

18

100101102103index0.0000.0050.0100.0150.0200.025variance ratio3004005006007008009001000published as a conference paper at iclr 2018

dim
orig.
73.57
rg65
ws
70.25
rw 46.25
men 75.66
75.66
mturk
34.02
siid113x
simverb
22.22
dim
orig.
77.3
rg65
ws
70.31
rw 45.86
men 75.84
67.47
mturk
siid113x
35.3
22.81
simverb

300

400

500

600

700

800

900

1000

proc.
74.72
71.95
49.11
77.59
77.59
36.19
24.98

proc.
81.07
73.02
48.4
78.21
67.79
37.59
25.6

orig.
75.64
70.8
45.97
76.07
67.68
35.17
22.91

orig.
77.52
71.52
44.96
75.84
67.67
36.54
23.48

proc.
79.87
72.88
47.63
77.89
68.11
37.1
25.32

proc.
81.07
74.65
49
77.96
68
37.85
25.57

orig.
77.72
70.39
46.6
75.9
66.89
35.73
23.03

orig.
79.75
71.19
44.44
76.16
67.98
36.62
23.68

proc.
81.97
72.73
48.59
78.15
68.25
37.65
25.82

proc.
82.34
73.06
49.22
78.35
68.87
38.44
25.76

orig.
77.59
71.64
45.7
75.88
67.6
35.76
23.35

orig.
78.18
71.5
44.5
76.72
68.34
36.67
23.24

proc.
80.7
74.04
47.81
78.15
67.87
38.04
25.97

proc.
79.07
74.78
49.03
78.1
69.44
38.58
26.58

table 15: before-after results (x100) on word similarity task on seven datasets.

dim
ap
esslli
battig
dim
ap
esslli
battig

300

400

500

600

orig.
46.1
68.18
71.6

orig.
38.04
54.55
62.96

proc.
48.61
72.73
77.78

proc.
41.31
54.55
66.67

orig.
42.57
64.2
68.18

orig.
34.76
68.18
67.9

proc.
45.34
82.72
75

proc.
39.8
56.82
69.14

orig.
46.85
64.2
68.18

orig.
34.76
72.73
49.38

proc.
50.88
65.43
70.45

proc.
27.46
72.73
59.26

orig.
40.3
65.91
46.91

orig.
27.96
52.27
51.85

proc.
45.84
72.73
66.67

proc.
28.21
52.27
46.91

700

800

900

1000

table 16: before-after results (x100) on the categorization task.

dim
syn.
sem.
all.
dim
syn.
sem.
all.

300

400

500

600

orig.
60.48
74.51
66.86

orig.
60.94
77.24
68.36

proc.
60.52
74.54
66.87

proc.
61.02
77.26
68.41

orig.
61.61
77.11
68.66

orig.
68.38
77.24
68.38

proc.
61.45
77.36
68.69

proc.
68.34
77.35
68.50

orig.
60.93
76.39
67.88

orig.
60.47
76.76
67.91

proc.
60.84
76.89
68.11

proc.
60.30
76.90
67.67

orig.
61.66
77.28
68.77

orig.
67.56
76.71
67.56

proc.
61.57
77.61
68.81

proc.
67.30
76.51
67.30

700

800

900

1000

table 17: before-after results (x100) on the word analogy task.

f statistics of text classification datasets

we evaluate the word representations (with and without postprocessing) using four different neural
network architectures (id98, vanilla-id56, gru-id56 and lstm-id56) on    ve benchmarks:

    the movie review (mr) dataset (pang & lee, 2005) where each review is composed by only

one sentence;

    the subjectivity (subj) dataset (pang & lee, 2004) where the algorithm needs to decide

whether a sentence is subjective or objective;

19

published as a conference paper at iclr 2018

dim
orig.
54.51
2012
56.58
2013
59.6
2014
2015
59.65
sick 68.89
58.32

all
dim
orig.
2012
55.52
2013
57.61
2014
61.57
2015
62.05
sick 68.38
59.96

all

300

400

500

600

700

800

900

1000

proc.
54.95
57.89
61.92
61.48
70.79
59.91

proc.
56.49
59.31
64.77
65.45
70.63
62.34

orig.
54.31
56.35
59.57
59.69
60.6
58.25

orig.
54.47
56.75
60.51
60.74
67.94
58.87

proc.
54.57
57.35
61.62
61.19
70.27
59.55

proc.
54.85
57.62
62.83
62.84
69.59
60.39

orig.
55.13
57.55
61.19
61.63
68.63
59.61

orig.
54.69
56.98
60.89
61.09
67.86
59.16

proc.
56.23
59.38
64.38
64.77
71.00
62.02

proc.
55.18
58.26
63.34
63.48
69.5
60.88

orig.
55.35
57.43
61.10
61.42
68.58
59.57

orig.
54.34
56.78
60.78
60.92
67.58
58.94

proc.
56.03
59.00
63.86
64.04
70.57
61.55

proc.
54.78
57.73
63.03
63.03
69.16
60.48

table 18: before-after results (x100) on the semantic textual similarity tasks.

capital-common-countries
capital-world
city-in-state
currency
family
gram1-adjective-to-adverb
gram2-opposite
gram3-comparative
gram4-superlative
gram5-present-participle
gram6-nationality-adjective
gram7-past-tense
gram8-plural
gram9-plural-verbs

id97
proc.
orig.
83.60
82.01
80.08
78.38
69.88
69.56
32.92
32.43
84.98
84.59
28.02
27.72
40.51
40.14
89.26
89.19
83.33
82.71
79.64
79.36
90.36
90.24
66.53
66.03
90.61
91.07
68.74
67.58

glove

orig.
95.06
91.89
69.56
21.59
95.84
40.42
31.65
86.93
90.46
82.95
90.24
63.91
95.27
67.24

proc.
95.96
92.31
70.45
21.36
95.65
39.21
30.91
87.09
90.59
82.76
90.24
64.87
95.36
68.05

table 19: before-after results (x100) on the word analogy task.

    the trec question dataset (li & roth, 2002) where all the questions in this dataset has to

be partitioned into six categories;

    the imdb dataset (maas et al., 2011)     each review consists of several sentences;
    the stanford sentiment treebank (sst) dataset (socher et al., 2013a), where we only use the

full sentences as the training data.

in trec, sst and imdb, the datasets have already been split into train/test sets. otherwise we use
10-fold cross validation in the remaining datasets (i.e., mr and subj). detailed statistics of various
features of each of the datasets are provided in table 21.

g proof of theorem a.1

given the similarity between the setup in theorem 2.2 in (arora et al., 2016) and theorem a.1,
many parts of the original proof can be reused except one key aspect     the concentration of z(c). we
summarize this part in the following lemma:

20

published as a conference paper at iclr 2018

2013.headlines

2012.smteuroparl
2012.smtnews

id97
proc.
orig.
43.85
42.12
2012.msrpar
72.16
2012.msrvid
72.07
69.48
2012.onwn 69.38
54.32
53.15
49.37
48.53
41.96
2013.fnwn 40.70
68.17
2013.onwn 67.87
63.81
61.88
74.78
2014.onwn 74.61
33.26
2014.deft-forum 32.19
66.83
65.96
2014.deft-news
59.58
2014.headlines
58.01
74.17
73.75
2014.images
72.07
2014.tweet-news
71.92
46.80
2015.answers-forum 46.35
68.07
67.99
60.42
59.72
63.45
61.47
78.08
78.09
70.20
sick 70.10

2015.answers-students
2015.belief
2015.headlines
2015.images

glove

orig.
44.54
64.47
53.07
41.74
37.54
37.54
47.22
49.73
57.41
21.55
65.14
47.05
57.22
58.32
30.02
49.20
44.05
46.22
66.63
65.14

proc.
44.09
68.05
65.67
45.28
47.22
39.34
58.60
57.20
67.56
29.39
71.45
52.60
68.28
66.13
39.86
62.38
57.68
53.31
73.20
67.85

table 20: before-after results (x100) on the semantic textual similarity tasks.

c
mr 2
2
subj
trec 6
sst 5
2

imdb

l
20
23
10
18
100

train
10,662
10,000
5,952
11,855
25,000

test
10-fold cross validation
10-fold cross validation
500
2,210
25,000

table 21: statistics for the    ve datasets after id121: c represents the number of classes; l
represents the average sentence length; train represents the size of the training set; and test represent
the size of the test set.

lemma g.1 let c be a random variable uniformly distributed over the unit sphere, we prove that
with high id203, z(c)/|v| converges to a constant z:

p((1      z)z     z(c)     (1 +  z)z)     1       ,

where  z =    ((d + 1)/|v|) and    =    ((da2 + (cid:107)  (cid:107)2)/d).

our proof differs from the one in (arora et al., 2016) in two ways: (a) we treat v(w) as deterministic
parameters instead of random variables and prove the lemma by showing a certain concentration of
measure; (b) the asymmetric parts    and u1,...,ud, (which did not exist in the original proof), need to
be carefully addressed to complete the proof.

21

since am,n can be represented by a union of countable disjoint rectangles, we know that for every
m, n     n+,

published as a conference paper at iclr 2018

g.1 proof of lemma g.1

given the constraints on the word vectors (3), the partition function z(c) can be rewritten as,

the equation above suggests that we can divide the proof into    ve parts.

step 1:

for every unit vector c, one has,

proof let m, n be a positive integer, and let am     rd such that,

v   v

(cid:88)
(cid:88)
(cid:88)

v   v

v   v

z(c) =

=

=

(cid:88)

w   v

1
|v|

exp(c(cid:62)v(w))

(cid:32)

exp

c(cid:62)

d(cid:88)

i=1

(cid:32)
(cid:34) d(cid:89)

   +

i=1

exp(c(cid:62)  )

exp(  i(w)c(cid:62)ui)

  i(w)ui +   v(w)

(cid:33)(cid:33)
exp(cid:0)c(cid:62)  v(w)(cid:1) .

(cid:35)

exp(cid:0)c(cid:62)  v(w)(cid:1)     ef
(cid:26)

  v     rd :

m     1

n

(cid:0)exp(cid:0)c(cid:62)  v(cid:1)(cid:1) , as |v|        .
(cid:27)

< exp(c(cid:62)  v)     m
n

.

am,n =

(cid:88)

w   v

1
|v|

(cid:90)

1(  v(w)     am,n ) =

f (  v)d  v.

am,n

m =1am,n , one has,
1(  v(w)     am,n ) exp(c(cid:62)  v(w))

1(  v(w)     am,n )

m
n

f (  v)d  v.

w   v

(cid:88)
(cid:88)
(cid:90)

w   v

1
|v|

1
|v|

m
n

am,n

m =1

   (cid:88)
   (cid:88)
   (cid:88)
(cid:90)

m =1

m =1

   

   

1
|v|

m =1

am,n

m
n

lim
n      

   (cid:88)
(cid:88)
exp(cid:0)c(cid:62)  v(w)(cid:1)     ef
(cid:88)
exp(cid:0)c(cid:62)  v(w)(cid:1)     lim

w   v

f (  v)d  v = ef

(cid:0)exp(cid:0)c(cid:62)  v(cid:1)(cid:1) ,
(cid:0)exp(cid:0)c(cid:62)  v(cid:1)(cid:1) , as |v|        .
   (cid:88)
(cid:0)exp(cid:0)c(cid:62)  v(cid:1)(cid:1) , as |v|        .

m     1

(cid:90)

am,n

m =1

n

f (  v)d  v

n      

= ef

w   v

which yields,

similarly, one has,

1
|v|

putting (6) and (7) proves (5).

22

further, since am,n are disjoint for different m   s and rd =       

(cid:88)

w   v

1
|v|

exp(cid:0)c(cid:62)  v(w)(cid:1) =

the above statement holds for every n. let n        , by de   nition of integration, one has,

(5)

(6)

(7)

published as a conference paper at iclr 2018

step 2:

the expected value, ef

(8)
proof let q     rd  d be a orthonormal matrix such that q(cid:62)c0 = c where c0 = (1, 0, ..., 0)(cid:62) and
det(q) = 1, then we have f (  v) = f (q  v), and,

(cid:0)exp(cid:0)c(cid:62)  v(cid:1)(cid:1) is a constant independent of c:

(cid:0)exp(cid:0)c(cid:62)  v(cid:1)(cid:1) = z0.
0   v(cid:1) d  v
f (  v) exp(cid:0)c(cid:62)
f (q  v) exp(cid:0)c(cid:62)q  v(cid:1) det(q)d  v
f (  v(cid:48)) exp(cid:0)c(cid:62)  v(cid:48)(cid:1) d  v(cid:48) = ef

ef

(cid:90)
(cid:90)
(cid:90)

  v

  v

  v(cid:48)

=

=

(cid:0)exp(cid:0)c(cid:62)  v(cid:1)(cid:1) ,

(cid:0)exp(cid:0)c(cid:62)

0   v(cid:1)(cid:1) =

ef

which proves (8).

for any vector   , one has the following concentration property,

step 3:

(cid:20)
proof let c1,...,cd be i.i.d. n (0, 1), and let c =(cid:80)d

p(cid:0)| exp(cid:0)c(cid:62)  (cid:1)     1| > k(cid:1)     2
(cid:17)

exp

(cid:16)(cid:107)  (cid:107)c1/

(cid:18)

(cid:19)

    1
4

c is uniform over
unit sphere. since c is uniform, then without loss of generality we can consider    = ((cid:107)  (cid:107), 0, ..., 0).
thus it suf   ces to bound exp

. we divide the proof into the following steps:

i=1 c2

   

c

(cid:107)  (cid:107)2
d     1

1

+

log2(1     k)
   
i , then c = (c1, ..., cd)/

(cid:21)

    c follows chi-square distribution with the degree of freedom of d, thus c can be bounded

by (laurent & massart, 2000),

   

p(c     d + 2

dx + 2x)     exp(   x),   x > 0.
dx)     exp(   x),   x > 0.

   

p(c     d     2

(9)

(10)
(11)

    therefore for any x > 0, one has,

p

let x = 1/4d, one has,

(cid:16)|c     d|     2

   

dx

(cid:17)     exp(   x)
(cid:19)
(cid:18)
(cid:18)
(cid:19)

    1
4d
    1
4d

,

.

p(c > d + 1)     exp

p(c < d     1)     exp

    since c1 is a gaussian random variable with variance 1, by chebyshev   s inequality, one has,

p (yci     k)     y2
k2 ,

p (yci        k)     y2

k2 ,   k > 0

and therefore thus,

p(exp(yci)     1 > k)    

y2

log2(1 + k)

,

p(exp(yci)     1 <    k)    

y2

log(1     k)2 ,    k > 0.

23

published as a conference paper at iclr 2018

(cid:17)

c

by,

(cid:16)(cid:107)  (cid:107)c1/
(cid:19)

    1 > k

   p (c > d + 1)

    therefore we can bound exp

(cid:18)

p

exp

(cid:18)(cid:107)  (cid:107)c1   

(cid:19)

c

+ p

    exp

(cid:19)

= exp

    exp

(cid:18)

p

exp

(cid:19)

(cid:18)(cid:107)  (cid:107)c1   

c

    1 <    k

   

(cid:18)
(cid:18)
(cid:18)
(cid:18)

c

(cid:18)(cid:107)  (cid:107)c1   
(cid:19)
(cid:18)
(cid:19)
(cid:19)
(cid:19)

+ p

exp
(cid:107)  (cid:107)2
d + 1
(cid:107)  (cid:107)2
d     1

+

+

exp

    1
4d
    1
4d
    1
4d

(cid:19)

    1 > k

(cid:12)(cid:12)(cid:12)(cid:12) c < d + 1
(cid:18) (cid:107)  (cid:107)c1   
(cid:19)

    1 > k

(cid:19)

d + 1
1

log(1     k)2 .

1

log2(1 + k)

.

p (c < d + 1)

combining the two inequalities above, one has (9) proved.

step 4: we are now ready to prove convergence of z(c). with (9), let c     rd such that,

c =(cid:8)c :(cid:12)(cid:12)exp(c(cid:62)  )     1(cid:12)(cid:12) < k,(cid:12)(cid:12)exp(ac(cid:62)ui)     1(cid:12)(cid:12) < k,(cid:12)(cid:12)exp(   ac(cid:62)ui)     1(cid:12)(cid:12) < k    i = 1, ..., d(cid:9)

p((cid:12)(cid:12)exp(ac(cid:62)ui)     1(cid:12)(cid:12) < k)     2d

then we can bound the id203 on c by,

p(c)     p(cid:0)(cid:12)(cid:12)exp(c(cid:62)  )     1(cid:12)(cid:12) < k(cid:1) +
(cid:19)

(cid:18)

d(cid:88)

i=1

    1     (2d + 1) exp

log2(1     k)
next, we need to show that for every w, the corresponding c(w), i.e.,

c(w) =(cid:8)c :(cid:12)(cid:12)exp(c(cid:62)  )     1(cid:12)(cid:12) < k,(cid:12)(cid:12)exp(  i(w)c(cid:62)ui)     1(cid:12)(cid:12) < k,    i = 1, ..., d(cid:9)

log2(1     k)

.

    1
4d

    2da2
d     1

1

1

    (cid:107)  (cid:107)2
d     1

we observe that   i(w) is bounded by a, therefore for any c that,

min(exp(   ac(cid:62)ui), exp(ac(cid:62)ui))     exp(  ic(cid:62)ui)     max(exp(   ac(cid:62)ui), exp(ac(cid:62)ui)),

and thus,
min(exp(   ac(cid:62)ui), exp(ac(cid:62)ui))     1     exp(  ic(cid:62)ui)     1     max(exp(   ac(cid:62)ui), exp(ac(cid:62)ui))     1,
which yields,

| exp(  ic(cid:62)ui)     1|     max(| exp(   ac(cid:62)ui)     1|,| exp(ac(cid:62)ui)     1|) < k.

therefore we prove c(w)     c. assembling everything together, one has,

exp(  i(w)c(cid:62)ui)     1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) > (d + 1)k,    i = 1, ..., d,   w     v

(cid:33)

(cid:32)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)exp(c(cid:62)  )

d(cid:89)

i=1

p

    p(   c)
    (2d + 1) exp(    1
4d

for every c     c, one has,

) +

2da2
d     1

1

log2(1     k)

(cid:107)  (cid:107)2
d     1

+

1

log2(1     k)

1

|v| |z(c)     z0|     (d + 1)k

|v|

z0.

let z = |v|z0, one can conclude that,

p((1      z)z     z(c)     (1 +  z)z)     1       ,

where  z =    ((d + 1)/|v|) and    =    (da2/d).

24

published as a conference paper at iclr 2018

g.2 proof of theorem a.1

having lemma g.1 ready, we can follow the same proof as in (arora et al., 2016) that both p(w) and
p(w, w(cid:48)) are correlated with (cid:107)v(w)(cid:107), formally
log p(w)     (cid:107)v(w)(cid:107)2

    log z, as |v|        ,

(12)

log p(w, w(cid:48))     (cid:107)v(w) + v(w(cid:48))(cid:107)2

2d

2d

    log z, as |v|        .

(13)

therefore, the id136 presented in (arora et al., 2016) (i.e., (4)) is obvious by assembling (12) and
(13) together:

pmi(w, w(cid:48))     v(w)(cid:62)v(w(cid:48))

d

, as |v|        .

25

