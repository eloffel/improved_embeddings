dynamic bernoulli embeddings for language evolution

maja rudolph, david blei

columbia university, new york, usa

7
1
0
2

 
r
a

 

m
3
2

 
 
]
l
m

.
t
a
t
s
[
 
 

1
v
2
5
0
8
0

.

3
0
7
1
:
v
i
x
r
a

abstract

id27s are a powerful approach for
unsupervised analysis of language. recently,
rudolph et al. (2016) developed exponential fam-
ily embeddings, which cast id27s in
a probabilistic framework. here, we develop dy-
namic embeddings, building on exponential family
embeddings to capture how the meanings of words
change over time. we use dynamic embeddings to
analyze three large collections of historical texts:
the u.s. senate speeches from 1858 to 2009, the
history of computer science acm abstracts from
1951 to 2014, and machine learning papers on
the arxiv from 2007 to 2015. we    nd dynamic
embeddings provide better    ts than classical em-
beddings and capture interesting patterns about
how language changes.

1. introduction
id27s are a collection of unsupervised learn-
ing methods for capturing latent semantic structure in lan-
guage. embedding methods analyze text data, learning dis-
tributed representations of the vocabulary to capture its co-
occurrence statistics. these learned representations are then
useful for reasoning about word usage and meaning (har-
ris, 1954; rumelhart et al., 1986). with large data sets and
approaches from neural networks, id27s have
become an important tool for analyzing language (bengio
et al., 2003; mikolov et al., 2013c;b;a; pennington et al.,
2014; levy & goldberg, 2014; arora et al., 2015).
recently, rudolph et al. (2016) developed exponential family
embeddings. exponential family embeddings distill the key
assumptions of an embedding problem, generalize them to
many types of data, and cast the distributed representations
as latent variables in a probabilistic model. they encom-
pass many existing methods for embeddings and open the
door to bringing expressive probabilistic modeling (bishop,
2006; murphy, 2012) to the problem of learning distributed
representations (bengio et al., 2003).
here we use exponential family embeddings to develop dy-
namic id27s, a method for learning distributed

representations that change over time. dynamic embeddings
analyze long-running texts, e.g., documents that span many
years, where the way words are used changes over time. the
goal of dynamic embeddings is to characterize and under-
stand those changes.
figure 1 illustrates the approach. it shows the changing rep-
resentation of intelligence in two corpora, the collection
of computer science abstracts from the acm 1951   2014
and the u.s. senate speeches 1858   2009. on the y-axis is
   meaning,    a proxy for the dynamic representation of the
word; in both corpora, its representation changes dramati-
cally over the years. to understand where it is located, the
plots also show similar words (according to their changing
representations) at various points. loosely, in the acm cor-
pus intelligence changes from government intelligence
to cognitive intelligence to arti   cial intelligence; in the con-
gressional record intelligence changes from psychologi-
cal intelligence to government intelligence. section 3 gives
other examples from these corpora, such as iraq, data, and
computer.
in more detail, a id27 uses representation vec-
tors to parameterize the conditional probabilities of words
in the context of other words. dynamic embeddings divide
the documents into time slices, e.g., one per year, and cast
the embedding vector as a latent variable that drifts via a
gaussian random walk. when    t to data, the dynamic em-
beddings capture how the representation of each word drifts
from slice to slice.
section 2 describes dynamic embeddings and how to    t them.
section 3 studies this approach on three datasets: 9 years
of arxiv machine learning papers (2007   2015), 64 years of
computer science abstracts (1951   2014), and 151 years of
u.s. senate speeches (1858   2009). dynamic embeddings
give better predictive performance than existing approaches
and provide an interesting exploratory window into how
language changes.
related work. language is known to evolve (aitchison,
2001; kirby et al., 2007) and there have been several lines
of research around capturing semantic shifts. mihalcea &
nastase (2012); tang et al. (2016) detect semantic changes
of words using features such as part-of-speech tags and en-

dynamic bernoulli embeddings for language evolution

dynamic id96 also studies text data over time
(blei & la   erty, 2006; wang & mccallum, 2006; wang
et al., 2008; gerrish & blei, 2010; wijaya & yeniterzi, 2011;
yogatama et al., 2014; mitra et al., 2014; 2015; frermann &
lapata, 2016). this class of models describes documents in
terms of topics, which are distributions over the vocabulary,
and then allows the topics to change over the course of the
collection. as in dynamic embeddings, some dynamic topic
models use a gaussian random walk to capture drift in the
underlying language model; for example, see blei & la   erty
(2006); wang et al. (2008); gerrish & blei (2010); frermann
& lapata (2016).
though topic models and id27s are related, they
are ultimately di   erent approaches to id38.
topic models capture co-occurrence of words at the docu-
ment level and focus on heterogeneity, i.e., that a document
can exhibit multiple topics (blei et al., 2003). word embed-
dings capture co-occurrence in terms of proximity in the text,
usually focusing on small neighborhoods around each word
(mikolov et al., 2013c). combining dynamic topic models
and dynamic id27s is an area for future study.

2. dynamic embeddings
we develop dynamic embeddings, a type of exponential
family embedding (efe) (rudolph et al., 2016) that captures
sequential changes in the representation of the data. we
focus on text data and the bernoulli embedding model.
in this section, we review bernoulli embeddings for text and
show how to include dynamics into the model. we then
derive the objective function for dynamic embeddings and
develop stochastic gradients to optimize it.
bernoulli embeddings for text. an exponential family
embedding is a conditionally speci   ed model. it has three
ingredients: the context, the conditional distribution of each
data point, and the parameter sharing structure.
in an efe for text, the data is a corpus of text. it is a collection
of words (x1, . . . , xn ) from a vocabulary of size v . each
word xi     {0, 1}v is an indicator vector (also called a    one-
hot    vector). it has exactly one nonzero entry at v, where v
is the vocabulary term at position i.
in an efe each data point has a context. in text, the context of
each word is its neighborhood; thus it is modelled conditional
on the words that come before and after it. typical context
sizes range between 2 and 10 words. (this is set in advance
or by cross-validation.)
we will build on bernoulli embeddings, which provide a
conditional model for the individual entries of the indicator
vectors xiv     {0, 1}. let ci be the set of positions in the
also resembles an uhlenbeck-ornstein process.

(a) intelligence in acm abstracts (1951   2014)

(b) intelligence in u.s. senate speeches (1858   2009)
figure 1. the dynamic embedding of intelligence reveals how
the term   s usage changes over the years. the y-axis is    meaning,    a
one dimensional projection of the embedding vectors. for selected
years, we list words with similar dynamic embeddings.

tropy and sagi et al. (2011); basile et al. (2014) employ
latent semantic analysis and temporal semantic indexing for
quantifying changes in meaning.
most closely related to our work are methods for dynamic
id27s (kim et al., 2014; kulkarni et al., 2015;
hamilton et al., 2016). these methods train a separate em-
bedding model for each time slice of the data. while in-
teresting, this requires enough data in each time slice such
that a high quality embedding can be trained for each. fur-
ther, because each time slice is trained independently, the
dimensions of the embeddings are not comparable across
time; they must use initialization (kim et al., 2014) or ad-hoc
alignment techniques (kulkarni et al., 2015; hamilton et al.,
2016; zhang et al., 2016) to stitch them together.
in contrast, the representations of dynamic embeddings are
sequential latent variables. dynamic embeddings naturally
accommodates time slices with sparse data and immediately
connect the latent dimensions across time. in section 3,
we found that dynamic embeddings provide quantitative
improvements over independently    tting each slice.1

1two similar models have been independently developed. bam-
ler & mandt (2016) model both the embeddings and the context
vectors using an uhlenbeck-ornstein process (uhlenbeck & orn-
stein, 1930). yao et al. (2017) factorize the pointwise mutual infor-
mation (pmi) matrix at di   erent time slices. their id173

dynamic bernoulli embeddings for language evolution

denote the collection
neighborhood of position i and let xci
of data points indexed by those positions. the conditional
distribution of xiv is

xiv|xci     bern(piv),

(1)

where piv     (0, 1) is the bernoulli id203.2
bernoulli embeddings specify the natural parameter, which
is the log odds   iv = log piv
. it is a function of the repre-
1   piv
sentation of term v and the terms in the context of position
i. speci   cally, each index (i, v) in the data is associated
with two parameter vectors, the embedding vector   v     rk
and the context vector   v     rk. together, the embedding
vectors and context vectors form the natural parameter of
the bernoulli. it is

(cid:16)(cid:80)

(cid:80)

(cid:17)

  iv =   (cid:62)

v

j   ci

v(cid:48)   v(cid:48)xjv(cid:48)

.

(2)

this is the inner product between the embedding   v and the
context vectors of the words that surround position i. (be-
cause xj is an indicator vector, the sum over the vocabulary
selects the appropriate context vector    at position j.) the
goal is to learn the embeddings and context vectors.
the index on the parameters does not depend on position
i, but only on term v; the embeddings are shared across all
positions in the text. this is what rudolph et al. (2016) call
the parameter sharing structure. it ensures, for example,
that the embedding vector for intelligence is the same
wherever it appears in the corpus. (dynamic embeddings
partially relax this restriction.)
finally, rudolph et al. (2016) regularize the bernoulli em-
bedding by placing priors on the embedding and context
vectors. they use gaussian priors with diagonal covariance,
i.e., (cid:96)2 id173. without the id173,    tting
a bernoulli embedding closely relates to other embedding
techniques such as cbow (mikolov et al., 2013a) and nega-
tive sampling (mikolov et al., 2013b). but the probabilistic
perspective of rudolph et al. (2016)   and in particular the
priors and the parameter sharing   allows us to extend this
setting to capture dynamics.
dynamic bernoulli embeddings. dynamic bernoulli em-
beddings extend bernoulli embeddings to text data over time.
each observation xiv is associated with a time slice ti, such
as the year of the observation. context vectors are shared
across all positions in the text but the embedding vectors are

2multinomial embeddings (rudolph et al., 2016) model each
indicator vector xi with a categorical conditional distribution, but
this requires expensive id172 in form of a softmax function.
for computational e   ciency, one can replace the softmax with the
hierarchical softmax (mikolov et al., 2013b) or employ approaches
related to noise contrastive estimation (gutmann & hyv  rinen,
2010; mnih & kavukcuoglu, 2013). bernoulli embeddings relax
the one-hot constraint of xi, and work well in practice; they relate
to the negative sampling (mikolov et al., 2013b).

figure 2. graphical representation of a dynamic embedding for text
data in t time slices, x (1),       , x (t ). the embedding vectors
  v of each term evolve over time. the context vectors are shared
across all time slices.

only shared within a time slice. thus dynamic embeddings
v     rk.
posit a sequence of embeddings for each term   (t)
the natural parameter of the conditional likelihood is similar
to equation (2) but with the embedding vector   v replaced
by the per-time-slice embedding vector   (ti)

,

v

(cid:16)(cid:80)

(cid:80)

(cid:17)

  iv =   (ti)(cid:62)

v

j   cj

v(cid:48)   v(cid:48)xjv(cid:48)

.

(3)

finally, dynamic embeddings use a gaussian random walk
as a prior on the embedding vectors,
v     n (0,      1
  v,   (0)
0 i)
v     n (  (t   1)
  (t)

,      1i).

(4)
(5)

v

given data, this leads to smoothly changing estimates of
each term   s embedding.3
figure 2 gives the graphical model for dynamic embeddings.
dynamic embeddings are a conditionally speci   ed model,
which in general are not guaranteed to imply a consistent
joint distribution. but dynamic bernoulli embeddings model
binary data, and thus a joint exists (arnold et al., 2001).
fitting dynamic embeddings. calculating the joint is com-
putationally intractable. rather, we    t dynamic embeddings
with the pseudo log likelihood, the sum of the log condi-
tionals. this is a commonly used objective for conditionally
speci   ed models (arnold et al., 2001).
in detail, we regularize the pseudo log likelihood with the log

3because    and    appear only as inner products in equation (2),
there is some redundancy in placing temporal dynamics on both
the embeddings and the context vectors. exploring dynamics in   
is a subject for future study.

dynamic bernoulli embeddings for language evolution

priors and then maximize to obtain a pseudo map estimate.
for dynamic bernoulli embeddings, this objective is the sum
of the log priors and the conditional log likelihoods of the
data xiv. we divide the data likelihood into two parts, the
contribution of nonzero data entries lpos and contribution
of zero data entries lneg,

l(  ,   ) = lpos + lneg + lprior.

(6)

table 1. time range and size of the three corpora analyzed in sec-
tion 3.

arxiv ml
acm
senate speeches

time range
2007     2015
1951     2014
1858     2009

slices

9
64
76

slice size
1 year
1 year
2 years

vocab size

50k
25k
25k

words
6.5m
21.6m
13.7m

the likelihoods are

n(cid:88)
n(cid:88)

i=1

v(cid:88)
v(cid:88)

v=1

lpos =

lneg =

xiv log   (  iv)

(1     xiv) log(1       (  iv)),

i=1

v=1

where   (  ) is the sigmoid, which maps natural parameters
to probabilities.
the prior is

lprior = log p(  ) + log p(  ),

where
log p(  ) =       0
2
log p(  ) =       0
2

(cid:88)
(cid:88)

v

v

||  v||2

||  (0)

v ||2       
2

(cid:88)

v,t

||  (t)

v       (t   1)

v

||2.

v

the parameters    and    appear in the natural parameters   iv
of equations (2) and (3) and in the log prior. the random
walk prior penalizes consecutive word vectors   (t   1)
and
v for drifting too far apart. it prioritizes parameter settings
  (t)
for which the norm of their di   erence is small.
the most expensive term in the objective is lneg, the contri-
bution of the zeroes to the conditional log likelihood. the
objective is cheaper if we subsample the zeros. rather than
summing over all words which are not at position i, we
sum over a subset of negative examples drawn at random.
mikolov et al. (2013b) call this negative sampling and rec-
ommend sampling from the unigram distribution raised to
the power of 0.75.
with negative sampling, we rede   ne lneg in equation (6).
denote the sampling distribution of zeros as   p,

lneg =

log(1       (  iv)).

(7)

n(cid:88)

(cid:88)

i=1

v      p

this sum has fewer terms and reduces the contribution of
the zeros to the objective. in a sense, this incurs a bias   the

expectation with respect to the negative samples is not equal
to the original objective   but    downweighting the zeros   
can improve prediction accuracy (hu et al., 2008; liang
et al., 2016).
we    t the objective (equation (6) with equation (7)) using
stochastic gradients (robbins & monro, 1951) and with
adaptive learning rates (duchi et al., 2011). pseudo code is in
appendix b. to avoid deriving the gradients of equation (6),
we implemented the algorithm in edward (tran et al., 2016).
edward is based on tensor   ow (team, 2015) and employs
automatic di   erentiation.4

3. empirical study
our empirical study contains two parts. in a quantitative
evaluation we benchmark dynamic embeddings against static
embeddings (mikolov et al., 2013a;b; rudolph et al., 2016).
dynamic embeddings improve over static embeddings in
terms of the conditional likelihood of held-out predictions.
further, dynamic embeddings perform better than embed-
dings trained on the individual time slices (hamilton et al.,
2016). in a qualitative evaluation we use a    tted dynamic
embedding model to extract which word vectors change most
and we visualize their dynamics. dynamic embeddings pro-
vide a new window into how language changes.

3.1. data
we studied three datasets. see table 1.
machine learning papers (2007 - 2015): this dataset con-
tains the full text from all machine learning papers (tagged
   stat.ml   ) published on the arxiv between april 2007 and
june 2015. it spans 9 years and we treat each year as a time
slice. the number of arxiv papers about machine learning
has increased over the years. there were 101 papers in 2007;
there were 1, 573 papers in 2014.
computer science abstracts (1951 - 2014): this dataset
contains abstracts of computer science papers published by
the association of computing machinery (acm) from 1951
to 2014. again, each year is considered a time slice and here
too the amount of data increases over the years. for 1953,
there are only around 10 abstracts and their combined length
4code available at http://github.com/mariru/dynamic_

bernoulli_embeddings

dynamic bernoulli embeddings for language evolution

is only 471 words; the combined length of the abstracts from
2009 is over 2m.
senate speeches (1858 - 2009): this dataset contains all
u.s. senate speeches from 1858 to mid 2009. here we treat
every 2 years as a time slice. in contrast to the other datasets,
this corpus is a transcript of spoken language.
for all datasets, we divide the observations into training,
validation, and testing. within each time slice we use 80%
for training, 10% for validation, and 10% for testing. ap-
pendix a provides details about preprocessing.

3.2. quantitative evaluation
we compare dynamic embeddings (d-emb) to time-binned
embeddings (t-emb) (hamilton et al., 2016) and static em-
beddings (s-emb) (rudolph et al., 2016). there are many
embedding techniques, without dynamics, that enjoy com-
parable performance. for the s-emb, we study bernoulli
embeddings (rudolph et al., 2016), which are similar to
continuous bag-of-words (cbow) with negative sampling
(mikolov et al., 2013a;b). for time-binned embeddings,
hamilton et al. (2016) train a separate embedding on each
time slice.
evaluation metric. we evaluate models by held-out
bernoulli id203. given a model, each held-out word
(validation or testing) is associated with a bernoulli proba-
bility. at that position, a better model assigns higher prob-
ability to the observed word and lower id203 to the
others. this metric is straightforward because the compet-
ing methods all produce bernoulli conditional likelihoods
(equation (1)).5 we report lpos, which considers only the
nonzero held-out data. to make results comparable, all meth-
ods are trained with the same number of negative samples.
model training and hyperparameters. each method takes
a maximum of 10 passes over the data. (the corresponding
number of stochastic gradient steps depends on the size of
the minibatches.) the parameters of s-emb are initialized
randomly. we initialize both d-emb and t-emb from a    t
of s-emb which has been trained from one pass, and then
train for 9 additional passes.
we set the dimension of the embeddings to 100 and the
number of negative samples to 20. we experiment with two
context sizes, 2 and 8.
other parameters are set by validation error. all methods
use validation error to set the initial learning rate    and mini-
batch sizes m. the model selects        [0.01, 0.1, 1, 10] and
m     [0.001n, 0.0001n, 0.00001n ], where n is the size of
training data. the only parameter speci   c to d-emb is the
5since we hold out chunks of consecutive words usually both a
word and its context are held out. for all methods we have to use
the words in the context to compute the conditional likelihoods.

precision of the random drift. to have one less hyper param-
eter to tune, we    x the precision on the context vectors and
the initial dynamic embeddings to   0 =   /1000, a constant
multiple of the precision on the dynamic embeddings. we
choose        [1, 10] by validation error.
results. we train each model on each training set and use
each validation set for model selection (e.g., selecting the
minibatch size and learning rate). table 2 reports the results
on the test set. dynamic embeddings consistently achieve
higher held-out likelihood.

table 2. dynamic embeddings (d-emb) consistently achieve high-
est held-out lpos (equation (6)). we compare to static embeddings
(s-emb) (mikolov et al., 2013b; rudolph et al., 2016), time-binned
embeddings (t-emb) (hamilton et al., 2016).

arxiv ml

s-emb (rudolph et al., 2016)
t-emb (hamilton et al., 2016)
d-emb [this paper]

s-emb (rudolph et al., 2016)
t-emb (hamilton et al., 2016)
d-emb [this paper]

s-emb (rudolph et al., 2016)
t-emb (hamilton et al., 2016)
d-emb [this paper]

context size 2

context size 8

   2.706    0.002
   2.491    0.002
   2.646    0.002
   2.454    0.002
   2.535    0.001    2.400    0.002

senate speeches

context size 2

context size 8

   2.366    0.001
   2.244    0.001
   2.212    0.001
   2.295    0.001
   2.263    0.001    2.204    0.001

acm
context size 2

context size 8

   2.427    0.001
   2.231    0.001
   2.242    0.001
   2.420    0.001
   2.396    0.001    2.228    0.001

3.3. qualitative exploration
we now show how to use dynamic embeddings to explore
the dataset. we use the    tted model to suggest ways that
language changes and visualize its discovered dynamic struc-
ture.
a word   s embedding neighborhood helps visualize its usage
and how it changes over time. it is simply a list of other
words with similar usage. for a given query word (e.g.,
computer) we take its index v and select the top ten words
according to

neighborhood(v, t) = argsort

w

(cid:0)sign(  (t)

v )(cid:62)  (t)
v ||    ||  (t)
w ||

w

||  (t)

(cid:1).

(8)

we    t a dynamic embedding    t to the senate speeches. ta-
ble 3 gives the embedding neighborhoods of computer for
the years 1858 and 1986. its usage changed dramatically
over the years. in 1858, a computer was a profession, a
person who was hired to compute things. now the profession
is obsolete; computer refers to the electronic device.
table 3 provides another example, bush. in 1858 this word
always referred to the plant. a bush still is a plant, but

dynamic bernoulli embeddings for language evolution

table 3. embedding neighborhoods (equation (8)) reveal how the
usage of a word changes over time. the embedding neighborhoods
of computer and bush were computed from a dynamic embed-
ding    tted to congress speeches (1858-2009). computer used to
be a profession but today it is used to refer to the electonic device.
the word bush is a plant but eventually in congress bush is used
to refer to the political    gures. the embedding neighborhood of
data comes from a dynamic embedding    tted to acm abstracts
(1951-2014).

computer (senate)
1986
1858

computer
draftsman
draftsmen
copyist

photographer
computers
copyists
janitor

accountant
bookkeeper

computer
software
computers
copyright

technological
innovation
mechanical
hardware

technologies

vehicles

bush (senate)

1858
bush

barberry

rust
bushes
borer

eradication
grasshoppers

cancer
tick

eradicate

1990
bush
cheney
nonsense

nixon
reagan
george
headed
criticized
clinton
blindness

1961
data

   les

directories

bibliographic

formatted
retrieval
publishing
archival
archives

manuscripts

1969
data

repositories
voluminous

lineage
metadata
snapshots
data streams

raw data
cleansing
data mining

data (acm)

1991
data

voluminous
raw data
repositories
data streams
data sources

volumes

dws
dsms

data access

2011
data

raw data
voluminous
data sources
data streams

dws

repositories
warehouses

marts
volumes

2014
data

data streams
voluminous
raw data
warehouses

dws

repositories
data sources
data mining

marts

figure 3. the dynamic embedding captures how the usage of the
word iraq changes over the years (1858-2009). the x-axis is time
and the y-axis is a one-dimensional projection of the embeddings
using pca. we include the embedding neighborhoods for iraq in
the years 1858, 1954, 1980 and 2008.

table 4. a list of the top 16 words whose dynamic embedding on
senate speeches changes most. the number represents the absolute
drift (equation (9)). the dynamics of the capitalized words are in
table 5 and discussed in the main text.

words with largest drift (senate)

iraq
tax cuts
health care
energy
medicare
discipline
text
values

3.09
2.84
2.62
2.55
2.55
2.44
2.41
2.40

coin
social security
fine
signal
program
moves
credit
unemployment

2.39
2.38
2.38
2.38
2.36
2.35
2.34
2.34

in the 1990   s, in the senate, it is usually refers to political
   gures. unlike computer, where the embedding neighbor-
hoods reveal two mutually exclusive meanings, the embed-
ding neighborhoods of bush re   ect which meaning is more
prevalent in a given period.
a    nal example in table 3 is the word data, from the acm
abstracts. the evolution of the embedding neighborhoods
of data re   ects how its meaning changes in the computer
science literature.
finding changing words with absolute drift. we have
highlighted example words whose usage changes. however,
not all words have changing usage. we now de   ne a metric
to discover which words change most.
one way to    nd words that change is to use absolute drift.
for word v, it is

drift(v) = ||  (t )

v       (0)
v ||.

(9)

this is the euclidean distance between the word   s embedding
at the last time slice and at the    rst time slice.
in the senate speeches, table 4 shows the 16 words that
have largest absolute drift. the word iraq has largest drift.
figure 3 highlights iraq   s embedding neighborhood in four
time slices: 1858, 1950, 1980, and 2008. (appendix c gives

the entire trajectory of its embedding neighborhood.) at
   rst the neighborhood contains other countries and regions.
later, arab countries move to the top of the neighborhood,
suggesting that the speeches start to use rhetoric more spe-
ci   c to arab countries. in 1980, iraq invades iran and the
iran-iraq war begins. in these years words such as aggres-
sors, troops, and invasion appear in the embedding
neighborhood. eventually, by 2008, the neighborhood con-
tains terror, terrorism, and saddam.
four other words with large drift are discipline, val-
ues, fine and unemployment (table 4). table 5 shows
their embedding neighborhoods for selected years. of these
words, discipline, values and, fine have multiple mean-
ings. their neighborhoods re   ect how the dominant mean-
ing changes over time. for example, values can be either
a numerical quantity or can be used to refer to moral values
and principles. in contrast, iraq and unemployment are
both words which have always had the same de   nition. yet,
the evolution of their neighborhood captures changes in the
way they are used.
dynamic embeddings as a tool to study a text. our hope
is that dynamic embeddings provide a suggestive tool for
understanding change in language. for example, researchers
interested in unemployment can complement their investi-
gation by looking at the embedding neighborhood of related

dynamic bernoulli embeddings for language evolution

table 5. embedding neighborhoods extracted from a dynamic embedding    tted to senate speeches (1858 - 2009). discipline, values,
fine, and unemployment are within the 16 words whose dynamic embedding has largest absolute drift. (table 4).

discipline

values

fine

unemployment

1858

discipline
hazing
westpoint
assaulting
disciplined
courtmartial
punishment

martial
mentally
summarily

2004

discipline
balanced
balancing

   scal
let

ourselves
structural
de   cit

administrations

restraint

1858
values

   uctuations

value

currencies
   uctuation
depreciation
   uctuating

purchasing power

   uctuate

basis

2000
values
sacred

inalienable

unique

preserving
exempli   ed
principles
philanthropy

virtues
historical

1858
   ne

luxurious

   nest
coarse
beautiful

   ner
lighter
weaves
spun

imprisonment

2004
   ne

punished

penitentiaries
imprisonment
misdemeanor
punishable

o   ense
guilty

conviction
penitentiary

1858

unemployment
unemployed
depression

acute

deplorable
alleviating
destitution

urban

employment
distressing

1940

unemployment
unemployed
depression
alleviating
destitution

acute

reemployment

deplorable
employment

distress

2000

unemployment

jobless
rate

depression
forecasts

crate
upward

lag

economists

predict

table 6. using dynamic embeddings we can study a social phenomenon of interest. we pick a target word of interest, such as jobs
or prostitution and create their embedding neighborhoods (equation (8)). looking at the neighborhood of jobs complements the
evolution of unemployment (table 5). or we might want to study prostitution. it used to be considered immoral and vile, evolved to
be indecent and its neighborhood in 1990 reveals a more concerned outlook as it includes words like servitude, harassment and
trafficking.

employment
unemployed

unemployed
employment

1858
jobs

overtime
positions

job

idleness
working

busy

civil service

jobs
1938
jobs

job

overtime
positions

shifts
idleness
busy
salaried

2008
jobs
job
create
creating

tremendously
economies
opportunities

created
pace

michigan

1858

prostitution
punishing
immoral
illegitimate

riotous
mobs
violence
assemblage
criminals
procures

1930

prostitution
punishing
immoral

bootlegging

riotous
forbidden
anarchists
assemblage

forbid
abet

prostitution
1962

1945

prostitution
indecent

vile

immoral
induces
incite
abortion
forbid

harboring
assemblage

prostitution
indecent
harassment
intimidation

sexual
vile

counterfeit
anarchists

mobs

lawbreakers

1988

harassment
intimidation
prostitution
counterfeit

illegal

tra   cking
indecent
disregard
anarchists
punishing

1990

prostitution
servitude
harassment
intimidation
tra   cking
harassing
apprehended

killings
labeled
naked

words such as employment, jobs or labor. in table 6 we
list the neighborhoods of jobs for the years 1858, 1938, and
2008. in 2008 the embedding neighborhood contains words
like create and opportunities, suggesting a di   erent
outlook on jobs than in earlier years.
another interesting example is prostitution. it used to
be immoral and vile, went to indecent, and in mod-
ern days it is considered harassment. we note the word
prostitution is not a frequent word. on average, it is used
once per time slice and, in two thirds of the time slices, it is
not mentioned at all. yet, the model is able to learn about
prostitution and the temporal evolution of the embed-
ding neighborhood reveals how over the years a judgemental
stance turns into concern over a social issue.

4. summary
we described dynamic embeddings, distributed representa-
tions of words that drift over the course of the collection.
building on rudolph et al. (2016), we formulate word em-
beddings with conditional probabilistic models and then
incorporate dynamics with a gaussian random walk prior.
we    t dynamic embeddings with stochastic optimization.
we used dynamic embeddings to analyze several datasets:

8 years of machine learning papers, 63 years of computer
science abstracts, and 151 years of speeches in the u.s. sen-
ate. dynamic embeddings provided a better    t than static
embeddings and other methods that account for time.
finally, we demonstrated how dynamic embeddings can
help identify interesting ways that language changes. a
word   s meaning can change (e.g., computer); its dominant
meaning can change (e.g., values); or its related subject
matter can change (e.g., iraq).

acknowledgements
we would like to thank francisco ruiz and liping liu for
discussion and helpful suggestiongs, elliot ash and suresh
naidu for access to the congress speeches, and aaron plasek
and matthew jones for access to the acm abstracts.

references
aitchison, jean. language change: progress or decay?

cambridge university press, 2001.

arnold, barry c, castillo, enrique, sarabia, jose maria,
et al. conditionally speci   ed distributions: an introduc-
tion (with comments and a rejoinder by the authors). sta-

dynamic bernoulli embeddings for language evolution

tistical science, 16(3):249   274, 2001.

arora, sanjeev, li, yuanzhi, liang, yingyu, ma, tengyu,
and risteski, andrej. rand-walk: a latent variable
model approach to id27s. arxiv preprint
arxiv:1502.03520, 2015.

baid113r, robert and mandt, stephan. dynamic word
arxiv preprint

embeddings via skip-gram    ltering.
arxiv:1702.08359, 2016.

basile, pierpaolo, caputo, annalina, and semeraro, gio-
vanni. analysing word meaning over time by exploiting
temporal random indexing. in first italian conference
on computational linguistics clic-it, 2014.

bengio, yoshua, ducharme, r  jean, vincent, pascal, and
jauvin, christian. a neural probabilistic language model.
journal of machine learning research, 3(feb):1137   1155,
2003.

bishop, christopher m. machine learning and pattern recog-
nition. information science and statistics. springer, hei-
delberg, 2006.

blei, david m and la   erty, john d. dynamic topic models.
in proceedings of the 23rd international conference on
machine learning, pp. 113   120. acm, 2006.

blei, david m, ng, andrew y, and jordan, michael i. la-
tent dirichlet allocation. journal of machine learning
research, 3(jan):993   1022, 2003.

duchi, john, hazan, elad, and singer, yoram. adaptive
subgradient methods for online learning and stochastic
optimization. journal of machine learning research, 12
(jul):2121   2159, 2011.

frermann, lea and lapata, mirella. a bayesian model of di-
achronic meaning change. transactions of the association
for computational linguistics, 4:31   45, 2016.

gerrish, s. and blei, d. a language-based approach to
measuring scholarly impact. in international conference
on machine learning, 2010.

gutmann, michael and hyv  rinen, aapo. noise-contrastive
estimation: a new estimation principle for unnormalized
statistical models. in aistats, 2010.

hamilton, william l, leskovec, jure, and jurafsky, dan.
diachronic id27s reveal statistical laws of
semantic change. arxiv preprint arxiv:1605.09096, 2016.
harris, zellig s. distributional structure. word, 10(2-3):

146   162, 1954.

hu, yifan, koren, yehuda, and volinsky, chris. collab-
orative    ltering for implicit feedback datasets. in data
mining, 2008. icdm   08. eighth ieee international con-
ference on, pp. 263   272. ieee, 2008.

kim, yoon, chiu, yi-i, hanaki, kentaro, hegde, darshan,
and petrov, slav. temporal analysis of language through
neural language models. arxiv preprint arxiv:1405.3515,
2014.

kirby, simon, dowman, mike, and gri   ths, thomas l.
innateness and culture in the evolution of language. pro-
ceedings of the national academy of sciences, 104(12):
5241   5245, 2007.

kulkarni, vivek, al-rfou, rami, perozzi, bryan, and skiena,
steven. statistically signi   cant detection of linguistic
change. in proceedings of the 24th international confer-
ence on world wide web, pp. 625   635. acm, 2015.

levy, omer and goldberg, yoav. neural id27
as implicit id105. in neural information
processing systems, pp. 2177   2185, 2014.

liang, dawen, charlin, laurent, mcinerney, james, and
blei, david m. modeling user exposure in recommenda-
tion. in proceedings of the 25th international conference
on world wide web, pp. 951   961. international world
wide web conferences steering committee, 2016.

mihalcea, rada and nastase, vivi. word epoch disambigua-
tion: finding how words change over time. in proceedings
of the 50th annual meeting of the association for compu-
tational linguistics: short papers-volume 2, pp. 259   263.
association for computational linguistics, 2012.

mikolov, tomas, chen, kai, corrado, greg, and dean, jef-
frey. e   cient estimation of word representations in vector
space. iclr workshop proceedings. arxiv:1301.3781,
2013a.

mikolov, tomas, sutskever, ilya, chen, kai, corrado,
greg s, and dean, je   . distributed representations of
words and phrases and their compositionality. in neural
information processing systems, pp. 3111   3119, 2013b.
mikolov, tomas, yih, wen-t au, and zweig, geo   rey. lin-
guistic regularities in continuous space word representa-
tions. in hlt-naacl, pp. 746   751, 2013c.

mitra, sunny, mitra, ritwik, riedl, martin, biemann,
chris, mukherjee, animesh, and goyal, pawan. that   s
sick dude!: automatic identi   cation of word sense
change across di   erent timescales.
arxiv preprint
arxiv:1405.4392, 2014.

mitra, sunny, mitra, ritwik, maity, suman kalyan, riedl,
martin, biemann, chris, goyal, pawan, and mukherjee,

dynamic bernoulli embeddings for language evolution

conference on knowledge discovery and data mining, pp.
424   433. acm, 2006.

wijaya, derry tanti and yeniterzi, reyyan. understanding
semantic change of words over centuries. in proceedings
of the 2011 international workshop on detecting and
exploiting cultural diversity on the social web, pp. 35   
40. acm, 2011.

yao, zijun, sun, yifan, ding, weicong, rao, nikhil, and
xiong, hui. discovery of evolving semantics through
dynamic id27 learning.
arxiv preprint
arxiv:1703.00607, 2017.

yogatama, d., wang, c., routledge, b., smith, n. a, and
xing, e. dynamic language models for streaming text.
transactions of the association for computational lin-
guistics, 2:181   192, 2014.

zhang, yating, jatowt, adam, bhowmick, sourav s, and
tanaka, katsumi. the past is not a foreign country: de-
tecting semantically similar terms across time.
ieee
transactions on knowledge and data engineering, 28
(10):2793   2807, 2016.

animesh. an automatic approach to identify word sense
changes in text media across timescales. natural lan-
guage engineering, 21(05):773   798, 2015.

mnih, andriy and kavukcuoglu, koray. learning word em-
beddings e   ciently with noise-contrastive estimation. in
neural information processing systems, pp. 2265   2273,
2013.

murphy, kevin p. machine learning: a probabilistic per-

spective. mit press, 2012.

pennington, je   rey, socher, richard, and manning, christo-
pher d. glove: global vectors for word representation. in
conference on empirical methods on natural language
processing, volume 14, pp. 1532   1543, 2014.

robbins, herbert and monro, sutton. a stochastic approx-
imation method. the annals of mathematical statistics,
pp. 400   407, 1951.

rudolph, maja, ruiz, francisco, mandt, stephan, and blei,
david. exponential family embeddings. in advances
in neural information processing systems, pp. 478   486,
2016.

rumelhart, david e, hintont, geo   rey e, and williams,
ronald j. learning representations by back-propagating
errors. nature, 323:9, 1986.

sagi, eyal, kaufmann, stefan, and clark, brady. tracing
semantic change with latent semantic analysis. current
methods in historical semantics, pp. 161   183, 2011.

tang, xuri, qu, weiguang, and chen, xiaohe. semantic
change computation: a successive approach. world wide
web, 19(3):375   415, 2016.

team, tensor   ow. tensorflow: large-scale machine
learning on heterogeneous systems, 2015. url http:
//tensorflow.org/. software available from tensor-
   ow.org.

tran, dustin, kucukelbir, alp, dieng, adji b., rudolph,
maja, liang, dawen, and blei, david m. edward: a
library for probabilistic modeling, id136, and criticism.
arxiv preprint arxiv:1610.09787, 2016.

uhlenbeck, george e and ornstein, leonard s. on the
theory of the brownian motion. physical review, 36(5):
823, 1930.

wang, c., blei, d., and heckerman, d. continuous time
dynamic topic models. in uncertainty in arti   cial intelli-
gence (uai), 2008.

wang, xuerui and mccallum, andrew. topics over time:
a non-markov continuous-time model of topical trends.
in proceedings of the 12th acm sigkdd international

dynamic bernoulli embeddings for language evolution

vocabulary. as in (mikolov et al., 2013b) we additionally remove each word with id203 p = 1    (cid:112)( 10   5

a. id174
we    x the vocabulary to the 25000 most frequent words and remove all words from the documents which are not in the
) where fi is
the frequency of the word. this e   ectively downsamples especially the frequent words and speeds up training. from each
time slice 80% of the words are used for training. a random subsample of 10% of the words is held out for validation and
another 10% for testing.

fi

b. pseudo code

algorithm 1: minibatch stochastic id119 for dynamic bernoulli embeddings.

input: t time slices of text data x (t) of size mt respectively. context size c, size of embedding k, number of negative
samples n, number of minibatch fractions m, initial learning rate   , precision   , vocabulary size v , smoothed unigram
distribution   p.
for v = 1 to v do

initialize entries of   v
(using draws from a normal distribution with zero mean and standard deviation 0.01).
for v = 1 to v do

initialize entries of   (t)
(using draws from a normal distribution with zero mean and standard deviation 0.01).

v

end for

end for
for number of passes over the data do

for number of minibatch fractions m do

for t = 1 to t do

sample minibatch of mt/m consecutive words {x(t)
word   s context to construct

mt/m} from each time slice x (t), and use each

1 ,       , x(t)
v(cid:88)
(cid:88)

c (t)

i =

  v(cid:48)xjv(cid:48).

j   ci
for each text position in the minibatch, draw a set s (t)

v(cid:48)=1

end for
update the parameters    = {  ,   } by ascending the stochastic gradient

i

(cid:16) v(cid:88)

v=1

(cid:40) t(cid:88)

     

mt/m(cid:88)
(cid:88)

i=1

v

m

t=1

      0
2

v c (t)

i ) +

iv log   (  (t)(cid:62)
x(t)
(cid:88)

||  v||2       0
2

||  (0)

v ||2       
2

v

v(cid:88)

v=1

(cid:88)
(cid:88)

xj   s (t)
||  (t)

i

v,t

(cid:41)

v       (t   1)

v

||2

of n negative samples from   p

(1     xjv) log(1       (  (t)(cid:62)

v c (t)

i ))

(10)

(cid:17)

end for

end for
any standard gradient-based learning rate schedule can be used. we use adagrad (duchi et al., 2011) in our experiments.

dynamic bernoulli embeddings for language evolution

c. entire embedding trajectory of iraq
here we give the entire trajectory of the embedding neighborhood of iraq. over the years it drifts smoothly. on average
iraq is mentioned only 10.6 times per time slice and in 64 out of the 76 time slices, iraq is not even mentioned at all. for
these years, the prior (equation (4)) ensures that the embedding at time t is the average of the embeddings at time t     1
and t + 1. when the embedding vector does not change between two consecutive time slices, the embedding neighborhood
might still    uctuate. this is because computing the embedding neighborhoods (equation (8)) involves also the embedding
vectors of the other words in the vocabulary.

table 7. embedding neighborhood of iraq extracted from a dynamic embedding    tted to the congress data. it is the word whose
embedding vector has largest absolute drift. by listing the neighborhood for all the time bins, we can see how iraq   s embedding vector
drifts smoothly. in 1858 iraq   s embedding neighborhood contains countries and regions. in 1950 a rethoric more speci   c to arabic
countries crystalizes. in 1980 iraq invades iran and words like invasion, aggressor and troops are in the neighborhood. by 2008 the
embedding neighborhood of iran contains words like terror, terrorism and saddam.

1858

1860

1862

1864

1866

1868

1870

1872

1874

1876

1878

1880

1882

1884

1886

1888

1890

1892

1894

1896

1898

1900

1902

1904

1906

poland, rumania, syria, yugoslavia, arabia,
lithuania, thrace, mesopotamia, hedjaz, albania
poland, rumania, syria, yugoslavia, arabia,
lithuania, thrace, mesopotamia, hedjaz, albania
poland, rumania, syria, yugoslavia, arabia,
lithuania, thrace, mesopotamia, hedjaz, albania
poland, rumania, syria, yugoslavia, arabia,
lithuania, thrace, mesopotamia, hedjaz, albania
poland, rumania, syria, yugoslavia, arabia,
lithuania, thrace, mesopotamia, hedjaz, albania
poland, rumania, syria, yugoslavia, arabia,
lithuania, thrace, mesopotamia, hedjaz, albania
poland, rumania, syria, yugoslavia, arabia,
lithuania, thrace, mesopotamia, hedjaz, albania
poland, rumania, syria, yugoslavia, arabia,
lithuania, thrace, mesopotamia, hedjaz, albania
poland, rumania, syria, yugoslavia, arabia,
lithuania, thrace, mesopotamia, hedjaz, albania
poland, rumania, syria, yugoslavia, arabia,
lithuania, thrace, mesopotamia, hedjaz, albania
poland, rumania, syria, yugoslavia, arabia,
lithuania, thrace, mesopotamia, hedjaz, albania
poland, rumania, syria, yugoslavia, arabia,
thrace, lithuania, mesopotamia, hedjaz, albania
poland, rumania, yugoslavia, syria, arabia,
thrace, lithuania, mesopotamia, hedjaz, albania
rumania, poland, yugoslavia, syria, arabia,
thrace, lithuania, mesopotamia, hedjaz, albania
rumania, poland, yugoslavia, syria, arabia,
thrace, lithuania, mesopotamia, hedjaz, albania
rumania, poland, yugoslavia, syria, arabia,
thrace, lithuania, mesopotamia, hedjaz, albania
rumania, poland, yugoslavia, arabia, syria,
thrace, lithuania, mesopotamia, hedjaz, albania
rumania, poland, yugoslavia, arabia, syria,
thrace, lithuania, mesopotamia, hedjaz, albania
rumania, yugoslavia, arabia, poland, syria,
thrace, lithuania, mesopotamia, hedjaz, albania
rumania, yugoslavia, arabia, syria, poland,
thrace, lithuania, mesopotamia, hedjaz, albania
rumania, yugoslavia, arabia, syria, poland,
thrace, lithuania, mesopotamia, hedjaz, albania
rumania, yugoslavia, arabia, syria, poland,
thrace, mesopotamia, lithuania, albania, hedjaz
rumania, yugoslavia, arabia, syria, poland,
thrace, mesopotamia, lithuania, albania, hedjaz
rumania, yugoslavia, arabia, syria, poland,
thrace, mesopotamia, albania, lithuania, hedjaz
rumania, yugoslavia, arabia, syria, poland,
thrace, mesopotamia, albania, lithuania, hedjaz

1908

1910

1912

1914

1916

1918

1920

1922

1924

1926

1928

1930

1932

1934

1936

1938

1940

1942

1944

1946

1948

1950

1952

1954

1956

rumania, yugoslavia, arabia, syria, poland,
thrace, mesopotamia, albania, hedjaz, lithuania
syria, rumania, arabia, yugoslavia, thrace,
mesopotamia, czecho, albania, poland, lithuania
syria, rumania, arabia, yugoslavia, mesopotamia,
thrace, czecho, albania, lithuania, poland
syria, rumania, arabia, yugoslavia, thrace,
mesopotamia, albania, czecho, poland, lithuania
syria, rumania, arabia, yugoslavia, thrace,
mesopotamia, albania, czecho, poland, lithuania
rumania, syria, arabia, yugoslavia, poland,
czecho, mesopotamia, lithuania, thrace, serbia
rumania, syria, arabia, yugoslavia, poland,
czecho, mesopotamia, thrace, lithuania, serbia
rumania, syria, arabia, yugoslavia, poland,
mesopotamia, czecho, thrace, lithuania, serbia
rumania, syria, arabia, yugoslavia, poland,
mesopotamia, czecho, thrace, lithuania, serbia
rumania, arabia, syria, mesopotamia, yugoslavia,
albania, thrace, salvador, persian, czecho
arabia, rumania, syria, yugoslavia, mesopotamia,
albania, thrace, persian, salvador, bulgaria
arabia, rumania, syria, yugoslavia, mesopotamia,
albania, thrace, persian, salvador, bulgaria
arabia, rumania, syria, yugoslavia, mesopotamia,
albania, thrace, salvador, persian, bulgaria
arabia, rumania, syria, yugoslavia, albania,
mesopotamia, thrace, salvador, persian, bulgaria
arabia, rumania, syria, yugoslavia, albania,
mesopotamia, thrace, salvador, persian, bulgaria
rumania, arabia, syria, yugoslavia, albania,
mesopotamia, bulgaria, salvador, alsace, lithuania
rumania, arabia, syria, yugoslavia, mesopotamia,
albania, salvador, guatemala, bulgaria, thrace
arabia, yugoslavia, guatemala, albania, salvador,
syria, rumania, bulgaria, iraq, balkans
yugoslavia, salvador, iraq, guatemala, arabia,
bulgaria, albania, rumania, syria, iran
iraq, albania, arabia, salvador, guatemala,
iran, bulgaria, afghanistan, rumania, syria
iraq, albania, arabia, salvador, guatemala,
iran, bulgaria, afghanistan, rumania, syria
iraq, arabia, albania, afghanistan, iran,
saudi, salvador, guatemala, ethiopia, cyprus
iraq, arabia, saudi, albania, afghanistan,
iran, salvador, guatemala, cyprus, ethiopia
iraq, albania, bulgaria, arabia, iran, salvador,
afghanistan, rumania, syria, cyprus
iraq, albania, iran, bulgaria, syria,
rumania, afghanistan, salvador, arabia, guatemala

1958

1960

1962

1964

1966

1968

1970

1972

1974

1976

1978

1980

1982

1984

1986

1988

1990

1992

1994

1996

1998

2000

2002

2004

2006

2008

iran, syria, albania, afghanistan, iraq,
bulgaria, arabia, rumania, cyprus, sultan
iran, syria, albania, afghanistan, iraq,
bulgaria, arabia, rumania, cyprus, sultan
iran, iraq, syria, afghanistan, invasion,
invaded, indochina, egypt, cyprus, arabia
iran, syria, iraq, afghanistan, invasion,
invaded, egypt, indochina, turkey, turkish
iran, iraq, syria, invasion, afghanistan,
invaded, indochina, korea, egypt, aggressors
iraq, iran, syria, invasion, invaded,
afghanistan, indochina, korea, aggressors, egypt
iraq, iran, invasion, syria, invaded,
afghanistan, korea, indochina, aggressors, egypt
iraq, invasion, iran, syria, invaded,
afghanistan, aggressors, korea, indochina, troops
iraq, invasion, iran, korea, syria,
invaded, troops, aggressors, afghanistan, indochina
iraq, iran, aggressors, aggression, syria,
invasion, troops, korea, invaded, indochina
iraq, aggressors, troops, iran, invasion,
syria, korea, aggression, indochina, invaded
iraq, aggressors, iran, troops, invasion,
syria, korea, aggression, indochina, invaded
iraq, aggressors, iran, troops, invasion,
syria, korea, aggression, indochina, invaded
iraq, iran, aggressors, syria, invasion,
troops, korea, aggression, invaded, allies
iraq, aggressors, syria, invasion, iran,
korea, troops, invaded, aggression, iraqi
iraq, aggressors, invasion, iran, allies,
invaded, korea, iraqi, syria, aggression
iraq, iraqi, iran, afghanistan, invaded,
aggressors, terror, allies, korea, invasion
iraq, iran, terror, allies, iraqi,
korea, aggressors, afghanistan, syria, invaded
iraq, iraqi, invaded, korea, allies,
aggressors, iran, exit, afghanistan, terror
iraq, iran, iraqi, allies, afghanistan,
terror, invaded, korea, aggressors, syria
iraq, terror, iran, iraqi, afghanistan,
occupation, allies, invaded, troops, invasion
iraq, iraqi, afghanistan, terrorism, terror,
iraqis, iran, reconstruction, saddam, bosnia
iraq, iraqi, afghanistan, terrorism, iran,
terror, iraqis, reconstruction, terrorist, terrorists
iraq, iraqi, afghanistan, terror, terrorism,
iran, reconstruction, iraqis, terrorists, saddam
iraq, iraqi, afghanistan, terror, terrorism,
iran, reconstruction, iraqis, terrorists, saddam
iraq, iraqi, afghanistan, terror, terrorism,
iran, reconstruction, iraqis, terrorists, saddam

