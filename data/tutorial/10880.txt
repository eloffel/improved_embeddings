search personalization with embeddings(cid:63)

thanh vu1, dat quoc nguyen2, mark johnson2, dawei song1, and alistair

willis1

1 the open university, milton keynes, united kingdom
{thanh.vu,dawei.song,alistair.willis}@open.ac.uk

2 department of computing, macquarie university, sydney, australia

dat.nguyen@students.mq.edu.au,mark.johnson@mq.edu.au

abstract. recent research has shown that the performance of search
personalization depends on the richness of user pro   les which normally
represent the user   s topical interests. in this paper, we propose a new
embedding approach to learning user pro   les, where users are embedded
on a topical interest space. we then directly utilize the user pro   les for
search personalization. experiments on query logs from a major com-
mercial web search engine demonstrate that our embedding approach
improves the performance of the search engine and also achieves better
search performance than other strong baselines.

1

introduction

users    personal data, such as a user   s historic interaction with the search engine
(e.g., submitted queries, clicked documents), have been shown useful to per-
sonalize search results to the users    information need [1,15]. crucial to e   ective
search personalization is the construction of user pro   les to represent individual
users    interests [1,3,6,7,12]. a common approach is to use main topics discussed
in the user   s clicked documents [1,6,12,15], which can be obtained by using a
human generated ontology as in [1,15] or using an unsupervised id96
technique as in [6,12].

however, using the user pro   le to directly personalize a search has been
not very successful with a minor improvement [6,12] or even deteriorate the
search performance [5]. the reason is that each user pro   le is normally built
using only the user   s relevant documents (e.g., clicked documents), ignoring user
interest-dependent information related to input queries. alternatively, the user
pro   le is utilized as a feature of a multi-id171-to-rank (l2r) frame-
work [1,13,14,15]. in this case, apart from the user pro   le, dozens of other fea-
tures has been proposed as the input of an l2r algorithm [1]. despite being
successful in improving search quality, the contribution of the user pro   le is not
very clear.

to handle these problems, in this paper, we propose a new embedding ap-
proach to constructing a user pro   le, using both the user   s input queries and
relevant documents. we represent each user pro   le using two projection matri-
ces and a user embedding. the two projection matrices is to identify the user
interest-dependent aspects of input queries and relevant documents while the

(cid:63)

in proceedings of the 39th european conference on information retrieval, ecir 2017, to appear.

6
1
0
2

 
c
e
d
2
1

 

 
 
]

r

i
.
s
c
[
 
 

1
v
7
9
5
3
0

.

2
1
6
1
:
v
i
x
r
a

2

thanh vu, dat quoc nguyen, mark johnson, dawei song, alistair willis

user embedding is to capture the relationship between the queries and docu-
ments in this user interest-dependent subspace. we then directly utilize the user
pro   le to re-rank the search results returned by a commercial search engine.
experiments on the query logs of a commercial web search engine demonstrate
that modeling user pro   le with embeddings helps to signi   cantly improve the
performance of the search engine and also achieve better results than other com-
parative baselines [1,11,14] do.

2 our approach

we start with our new embedding approach to building user pro   les in section
2.1, using pre-learned document embeddings and query embeddings. we then
detail the processes of using an unsupervised topic model (i.e., latent dirichlet
allocation (lda) [2]) to learn document embeddings and query embeddings in
sections 2.2 and 2.3, respectively. we    nally use the user pro   les to personalize
the search results returned by a commercial search engine in section 2.4.

2.1 building user pro   les with embeddings
let q denote the set of queries, u be the set of users, and d be the set of
documents. let (q, u, d) represent a triple (query, user, document). the query q    
q, user u     u and document d     d are represented by vector embeddings vq,
vu and vd     rk, respectively.

our goal is to select a score function f such that the implausibility value
f (q, u, d) of a correct triple (q, u, d) (i.e. d is a relevant document of u given q) is
smaller than the implausibility value f (q(cid:48), u(cid:48), d(cid:48)) of an incorrect triple (q(cid:48), u(cid:48), d(cid:48))
(i.e. d(cid:48) is not a relevant document of u(cid:48) given q(cid:48)). inspired by embedding models
of entities and relationships in knowledge bases [9,10], the score function f is
de   ned as follows:

f (q, u, d) = (cid:107)wu,1vq + vu     wu,2vd(cid:107)(cid:96)1/2

(1)
here we represent the pro   le for the user u by two matrices wu,1 and wu,2    
rk  k and a vector embedding vu, which represents the user   s topical interests.
speci   cally, we use the interest-speci   c matrices wu,1 and wu,2 to identify the
interest-dependent aspects of both query q and document d, and use vector vu
to describe the relationship between q and d in this interest-dependent subspace.
in this paper, vd and vq are pre-determined by employing the lda topic
model [2], which are detailed in next sections 2.2 and 2.3. our model parameters
are only the user embeddings vu and matrices wu,1 and wu,2. to learn these
user embeddings and matrices, we minimize the margin-based objective function:

max(cid:0)0,    + f (q, u, d)     f (q(cid:48), u, d(cid:48))(cid:1)

(cid:88)

l =

(2)

(q,u,d)   g
(q(cid:48),u,d(cid:48))   g(cid:48)

(q,u,d)

where    is the margin hyper-parameter, g is the training set that contains only
correct triples, and g(cid:48)
(q,u,d) is the set of incorrect triples generated by corrupt-
ing the correct triple (q, u, d) (i.e. replacing the relevant document/query d/q in

search personalization with embeddings

3

(q, u, d) by irrelevant documents/queries d(cid:48)/q(cid:48)). we use stochastic gradient de-
scent (sgd) to minimize l, and impose the following constraints during training:
(cid:107)vu(cid:107)2 (cid:54) 1, (cid:107)wu,1vq(cid:107)2 (cid:54) 1 and (cid:107)wu,2vd(cid:107)2 (cid:54) 1. first, we initialize user matri-
ces as identity matrices and then    x them to only learn the randomly initialized
user embeddings. then in the next step, we    ne-tune the user embeddings and
user matrices together. in all experiments shown in section 3, we train for 200
epochs during each two optimization step.

2.2 using lda to learn document embeddings

in this paper, we model document embeddings by using topics extracted from
relevant documents. we use lda [2] to automatically learn k topics from the
relevant document collection. after training an lda model to calculate the prob-
ability distribution over topics for each document, we use the topic proportion
vector of each document as its document embedding. speci   cally, the zth ele-
ment (z = 1, 2, ..., k) of the vector embedding for document d is: vd,z = p(z | d)
where p(z | d) is the id203 of the topic z given the document d.

2.3 modeling search queries with embeddings

we also represent each query as a id203 distribution vq over topics, i.e. the
zth element of the vector embedding for query q is de   ned as: vq,z = p(z | q)
where p(z | q) is the id203 of the topic z given the query q. following [1,14],
we de   ne p(z | q) as a mixture of lda topic probabilities of z given documents
related to q. let dq = {d1, d2, ..., dn} be the set of top n ranked documents
returned for a query q (in the experiments we select n = 10). we de   ne p(z | q)
as follows:

p(z | q) =

  ip(z | di)

(3)

(cid:88)n

i=1

j=1   j   1 is the exponential decay function of i which is the rank of
di in dq. and    is the decay hyper-parameter (0 <    < 1). the decay function
is to specify the fact that a higher ranked document is more relevant to user
in term of the lexical matching (i.e. we set the larger mixture weights to higher
ranked documents).

where   i =   i   1(cid:80)n

2.4 personalizing search results

we utilize the user pro   les (i.e., the learned user embeddings and matrices) to
re-rank the original list of documents produced by a commercial search engine
as follows: (1) we download the top n ranked documents given the input query
q. we denote a downloaded document as d. (2) for each document d we apply
the trained lda model to infer the topic distribution vd. we then model the
query q as a topic distribution vq as in section 2.3. (3) for each triple (q, u, d),
we calculate the implausibility value f (q, u, d) as de   ned in equation 1. we then
sort the values in the ascending order to achieve a new ranked list.

4

thanh vu, dat quoc nguyen, mark johnson, dawei song, alistair willis

3 experimental methodology

dataset: we evaluate our new approach using the search results returned by a
commercial search engine. we use a dataset of query logs of of 106 anonymous
users in 15 days from 01 july 2012 to 15 july 2012. a log entity contains a
user identi   er, a query, top-10 urls ranked by the search engine, and clicked
urls along with the user   s dwell time. we also download the content documents
of these urls for training lda [2] to learn document and query embeddings
(sections 2.2 and 2.3).

bennett et al. [1] indicate that short-term (i.e. session) pro   les achieved bet-
ter search performance than the longer-term pro   les. short-term pro   les are
usually constructed using the user   s search interactions within a search session
and used to personalize the search within the session [1]. to identify a search
session, we use 30 minutes of user inactivity to demarcate the session bound-
ary. in our experiments, we build short-term pro   les and utilize the pro   les to
personalize the returned results. speci   cally, we uniformly separate the last log
entries within search sessions into a test set and a validation set. the remainder
of log entities within search sessions are used for training (e.g. to learn user
embeddings and matrices in our approach).
evaluation methodology: we use the sat criteria detailed in [4] to identify
whether a clicked url is relevant from the query logs (i.e., a sat click). that
is either a click with a dwell time of at least 30 seconds or the last result click in
a search session. we assign a positive (relevant) label to a returned url if it is
a sat click. the remainder of the top-10 urls is assigned negative (irrelevant)
labels. we use the rank positions of the positive labeled urls as the ground
truth to evaluate the search performance before and after re-ranking. we also
apply a simple pre-processing on these datasets as follows. at    rst, we remove the
queries whose positive label set is empty from the dataset. after that, we discard
the domain-related queries (e.g. facebook, youtube). to this end, the training
set consists of 5,658 correct triples. the test and validation sets contain 1,210
and 1,184 correct triples, respectively. table 1 presents the dataset statistics
after pre-processing.

table 1. basic statistics of the dataset after pre-processing

#days #users #distinct queries #sat clicks #sessions #distinct documents

15

106

6,632

8,052

2,394

33,591

id74: we use two standard id74 in document
ranking [1,8]: mean reciprocal rank (mrr) and precision (p@1). for each met-
ric, the higher value indicates the better ranking performance.
baselines: we employ three comparative baselines with the same experimental
setup: (1) se: the original rank from the search engine (2) ci: we promote
returned documents previously clicked by the user. this baseline is similar to
the personalized navigation method in teevan et al. [11]. (3) sp: the search
personalization method using the short-term pro   le [1,14]. these are very com-

search personalization with embeddings

5

parative baselines given that they start with the ranking provided by the major
search engine and add other signals (e.g., clicked documents) to get a better
ranking performance [11,1].
hyper-parameter tuning: we perform a grid search to select optimal hyper-
parameters on the validation set. we train the lda model3 using only the rele-
vant documents (i.e., sat clicks) extracted from the query logs, with the number
of topics (i.e. the number of vector dimensions) k     {50, 100, 200}. we then apply
the trained lda model to infer document embeddings and query embeddings for
all documents and queries. we then choose either the (cid:96)1 or (cid:96)2 norm in the score
function f , and select sgd learning rate        {0.001, 0.005, 0.01}, the margin
hyper-parameter        {1, 3, 5} and the decay hyper-parameter        {0.7, 0.8, 0.9}.
the highest mrr on the validation set is obtained when using k = 200, (cid:96)1 in f ,
   = 0.005,    = 5, and    = 0.8.

4 experimental results

table 2 shows the performances of the baselines and our proposed method. using
the previously clicked documents ci helps to signi   cantly improve the search
performance (p < 0.05 with the paired t-test) with the relative improvements of
about 7+% in both mrr and p@1 metrics. with the use of short-term pro   les
as a feature of a learning-to-rank framework, sp [1,14] improves the mrr score
over the original rank signi   cantly (p < 0.01) and achieves a better performance
than ci    s.

table 2. overall performances of the methods in the test set. our method   w denotes
the simpli   ed version of our method. the subscripts denote the relative improvement
over the baseline se.

metric se ci [11]
mrr 0.559 0.597+6.9% 0.631+12.9% 0.656+17.3%
p@1
0.385 0.416+8.1% 0.452+17.4% 0.501+30.3%

sp [1,14] our method our method   w

0.645+15.4%
0.481+24.9%

by directly learning user pro   les and applying them to re-rank the search
results, our embedding approach achieves the highest performance of search
personalization. speci   cally, our mrr score is signi   cantly (p < 0.05) higher
than that of sp (with the relative improvement of 4% over sp). likewise, the
p@1 score obtained by our approach is signi   cantly higher than that of the
baseline sp (p < 0.01) with the relative improvement of 11%.

in table 2, we also present the performances of a simpli   ed version of our
embedding approach where we    x the user matrices as identity matrices and
then only learn the user embeddings. table 2 shows that our simpli   ed version
achieves second highest scores compared to all others.4 speci   cally, our simpli   ed
version obtains signi   cantly higher p@1 score (with p < 0.05) than sp.

3 we use the lda implementation in mallet toolkit: http://mallet.cs.umass.edu/.
4 our approach obtains signi   cantly higher p@1 score (p < 0.05) than our simpli   ed

version with 4% relative improvement.

6

thanh vu, dat quoc nguyen, mark johnson, dawei song, alistair willis

5 conclusions

in this paper, we propose a new embedding approach to building user pro   les. we
model each user pro   le using a user embedding together with two user matrices.
the user embedding and matrices are then learned using lda-based vector
embeddings of the user   s relevant documents and submitted queries. applying
it to web search, we use the pro   le to re-rank search results returned by a
commercial web search engine. our experimental results show that the proposed
method can stably and signi   cantly improve the ranking quality.

acknowledgments: the    rst two authors contributed equally to this work. dat
quoc nguyen is supported by an international postgraduate research scholar-
ship and a nicta nrpa top-up scholarship.

references

1. p. n. bennett, r. w. white, w. chu, s. t. dumais, p. bailey, f. borisyuk, and
x. cui. modeling the impact of short- and long-term behavior on search person-
alization. in sigir, 2012.

2. d. m. blei, a. y. ng, and m. i. jordan. id44. j. mach.

learn. res., 2003.

3. z. cheng, s. jialie, and s. c. hoi. on e   ective personalized music retrieval by

exploring online user behaviors. in sigir, 2016.

4. s. fox, k. karnawat, m. mydland, s. dumais, and t. white. evaluating implicit

measures to improve web search. acm trans. inf. syst., 23(2):147   168, 2005.

5. m. harvey, m. j. carman, i. ruthven, and f. crestani. bayesian latent variable

models for collaborative item rating prediction. in cikm, 2011.

6. m. harvey, f. crestani, and m. j. carman. building user pro   les from topic

models for personalised search. in cikm, 2013.

7. x. liu. modeling users    dynamic preference for personalized recommendation. in

ijcai, 2015.

8. c. d. manning, p. raghavan, and h. sch  utze. introduction to information re-

trieval. cambridge university press, new york, ny, usa, 2008.

9. d. q. nguyen, k. sirts, l. qu, and m. johnson. neighborhood mixture model for

knowledge base completion. in conll, 2016.

10. d. q. nguyen, k. sirts, l. qu, and m. johnson. stranse: a novel embedding

model of entities and relationships in knowledge bases. in naacl-hlt, 2016.

11. j. teevan, d. j. liebling, and g. ravichandran geetha. understanding and pre-

dicting personal navigation. in wsdm, 2011.

12. t. vu, d. song, a. willis, s. n. tran, and j. li. improving search personalisation

with dynamic group formation. in sigir, 2014.

13. t. vu, a. willis, u. kruschwitz, and d. song. personalised query suggestion for

intranet search with temporal user pro   ling. in chiir, 2017.

14. t. vu, a. willis, s. n. tran, and d. song. temporal latent topic user pro   les for

search personalisation. in ecir, 2015.

15. r. w. white, w. chu, a. hassan, x. he, y. song, and h. wang. enhancing

personalized search by mining and modeling task behavior. in www, 2013.

