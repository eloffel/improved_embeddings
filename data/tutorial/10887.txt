7
1
0
2

 

p
e
s
1
1

 

 
 
]
l
c
.
s
c
[
 
 

2
v
4
2
7
1
0

.

1
1
6
1
:
v
i
x
r
a

published as a conference paper at iclr 2017

words or characters? fine-grained gating
for reading comprehension

zhilin yang, bhuwan dhingra, ye yuan, junjie hu, william w. cohen, ruslan salakhutdinov
school of computer science
carnegie mellon university
{zhiliny,wcohen,rsalakhu}@cs.cmu.edu

abstract

previous work combines word-level and character-level representations using con-
catenation or scalar weighting, which is suboptimal for high-level tasks like read-
ing comprehension. we present a    ne-grained gating mechanism to dynamically
combine word-level and character-level representations based on properties of the
words. we also extend the idea of    ne-grained gating to modeling the interaction
between questions and paragraphs for reading comprehension. experiments show
that our approach can improve the performance on reading comprehension tasks,
achieving new state-of-the-art results on the children   s book test and who did
what datasets. to demonstrate the generality of our gating mechanism, we also
show improved results on a social media tag prediction task.1

1

introduction

finding semantically meaningful representations of the words (also called tokens) in a document is
necessary for strong performance in natural language processing tasks. in neural networks, tokens
are mainly represented in two ways, either using word-level representations or character-level repre-
sentations. word-level representations are obtained from a lookup table, where each unique token is
represented as a vector. character-level representations are usually obtained by applying recurrent
neural networks (id56s) or convolutional neural networks (id98s) on the character sequence of the
token, and their hidden states are combined to form the representation. word-level representations
are good at memorizing the semantics of the tokens while character-level representations are more
suitable for modeling sub-word morphologies (ling et al., 2015; yang et al., 2016a). for example,
considering    cat    and    cats   , word-level representations can only learn the similarities between the
two tokens by training on a large amount of training data, while character-level representations, by
design, can easily capture the similarities. character-level representations are also used to alleviate
the dif   culties of modeling out-of-vocabulary (oov) tokens (luong & manning, 2016).
hybrid word-character models have been proposed to leverage the advantages of both word-level
and character-level representations. the most commonly used method is to concatenate these two
representations (yang et al., 2016a). however, concatenating word-level and character-level repre-
sentations is technically problematic. for frequent tokens, the word-level representations are usually
accurately estimated during the training process, and thus introducing character-level representa-
tions can potentially bias the entire representations. for infrequent tokens, the estimation of word-
level representations have high variance, which will have negative effects when combined with the
character-level representations. to address this issue, recently miyamoto & cho (2016) introduced
a scalar gate conditioned on the word-level representations to control the ratio of the two repre-
sentations. however, for the task of reading comprehension, preliminary experiments showed that
this method was not able to improve the performance over concatenation. there are two possible
reasons. first, word-level representations might not contain suf   cient information to support the
decisions of selecting between the two representations. second, using a scalar gate means applying
the same ratio for each of the dimensions, which can be suboptimal.
in this work, we present a    ne-grained gating mechanism to combine the word-level and character-
level representations. we compute a vector gate as a linear projection of the token features followed

1code is available at https://github.com/kimiyoung/fg-gating

1

published as a conference paper at iclr 2017

by a sigmoid activation. we then multiplicatively apply the gate to the character-level and word-
level representations. each dimension of the gate controls how much information is    owed from
the word-level and character-level representations respectively. we use named entity tags, part-of-
speech tags, document frequencies, and word-level representations as the features for token proper-
ties which determine the gate. more generally, our    ne-grained gating mechanism can be used to
model multiple levels of structure in language, including words, characters, phrases, sentences and
paragraphs. in this work we focus on studying the effects on word-character gating.
to better tackle the problem of reading comprehension, we also extend the idea of    ne-grained
gating for modeling the interaction between documents and queries. previous work has shown the
importance of modeling interactions between document and query tokens by introducing various
attention architectures for the task (hermann et al., 2015; kadlec et al., 2016). most of these use
an inner product between the two representations to compute the relative importance of document
tokens. the gated-attention reader (dhingra et al., 2016a) showed improved performance by re-
placing the inner-product with an element-wise product to allow for better matching at the semantic
level. however, they use aggregated representations of the query which may lead to loss of infor-
mation. in this work we use a    ne-grained gating mechanism for each token in the paragraph and
each token in the query. the    ne-grained gating mechanism applies an element-wise multiplication
of the two representations.
we show improved performance on reading comprehension datasets, including children   s book test
(cbt), who did what, and squad. on cbt, our approach achieves new state-of-the-art results
without using an ensemble. our model also improves over state-of-the-art results on the who did
what dataset. to demonstrate the generality of our method, we apply our word-character    ne-
grained gating mechanism to a social media tag prediction task and show improved performance
over previous methods.
our contributions are two-fold. first, we present a    ne-grained word-character gating mechanism
and show improved performance on a variety of tasks including reading comprehension. second,
to better tackle the reading comprehension tasks, we extend our    ne-grained gating approach to
modeling the interaction between documents and queries.

2 related work

hybrid word-character models have been proposed to take advantages of both word-level and
character-level representations. ling et al. (2015) introduce a compositional character to word
(c2w) model based on bidirectional lstms. kim et al. (2016) describe a model that employs a
convolutional neural network (id98) and a highway network over characters for language model-
ing. miyamoto & cho (2016) use a gate to adaptively    nd the optimal mixture of the character-level
and word-level inputs. yang et al. (2016a) employ deep id149 on both character and
word levels to encode morphology and context information. concurrent to our work, ? employed a
similar gating idea to combine word-level and character-level representations, but their focus is on
low-level sequence tagging tasks and the gate is not conditioned on linguistic features.
the gating mechanism is widely used in sequence modeling. long short-term memory (lstm)
networks (hochreiter & schmidhuber, 1997) are designed to deal with vanishing gradients through
the gating mechanism. similar to lstm, gated recurrent unit (gru) was proposed by cho et al.
(2014), which also uses gating units to modulate the    ow of information. the gating mechanism
can also be viewed as a form of attention mechanism (bahdanau et al., 2015; yang et al., 2016b)
over two inputs.
similar to the idea of gating, multiplicative integration has also been shown to provide a bene   t
in various settings. yang et al. (2014)    nd that multiplicative operations are superior to additive
operations in modeling relations. wu et al. (2016) propose to use hadamard product to replace sum
operation in recurrent networks, which gives a signi   cant performance boost over existing id56
models. dhingra et al. (2016a) use a multiplicative gating mechanism to achieve state-of-the-art
results on id53 benchmarks.
reading comprehension is a challenging task for machines. a variety of models have been proposed
to extract answers from given text (hill et al., 2016; kadlec et al., 2016; trischler et al., 2016; chen
et al., 2016; sordoni et al., 2016; cui et al., 2016). yu et al. (2016) propose a dynamic chunk reader

2

published as a conference paper at iclr 2017

to extract and rank a set of answer candidates from a given document to answer questions. wang
& jiang (2016) introduce an end-to-end neural architecture which incorporates match-lstm and
id193 (vinyals et al., 2015).

3 fine-grained gating

in this section, we will describe our    ne-grained gating approach in the context of reading com-
prehension. we    rst introduce the settings of reading comprehension tasks and a general neural
network architecture. we will then describe our word-character gating and document-query gating
approaches respectively.

3.1 reading comprehension setting
the reading comprehension task involves a document p = (p1, p2,       , pm ) and a query q =
(q1, q2,       , qn ), where m and n are the lengths of the document and the query respectively. each
token pi is denoted as (wi, ci), where wi is a one-hot encoding of the token in the vocabulary and
ci is a matrix with each row representing a one-hot encoding of a character. each token in the query
qj is similarly de   ned. we use i as a subscript for documents and j for queries. the output of the
problem is an answer a, which can either be an index or a span of indices in the document.
now we describe a general architecture used in this work, which is a generalization of the gated
attention reader (dhingra et al., 2016a). for each token in the document and the query, we compute
a vector representation using a function f. more speci   cally, for each token pi in the document,
we have h0
i = f (wi, ci). the same function f is also applied to the tokens in the query. let
p and hq denote the vector representations computed by f for tokens in documents and queries
h0
respectively. in section 3.2, we will discuss the    word-character       ne-grained gating used to de   ne
the function f.
suppose that we have a network of k layers. at the k-th layer, we apply id56s on hk   1
and hq to
obtain hidden states pk and qk, where pk is a m    d matrix and qk is a n    d matrix with d being
the number of hidden units in the id56s. then we use a function r to compute a new representation
p = r(pk, qk). in section 3.3, we will introduce the    document-query       ne-
for the document hk
grained gating used to de   ne the function r.
after going through k layers, we predict the answer index a using a softmax layer over hidden
p. for datasets where the answer is a span of text, we use two softmax layers for the start
states hk
and end indices respectively.

p

3.2 word-character fine-grained gating

given a one-hot encoding wi and a character sequence ci, we now describe how to compute the
vector representation hi = f (wi, ci) for the token. in the rest of the section, we will drop the
subscript i for notation simplicity.
we    rst apply an id56 on c and take the hidden state in the last time step c as the character-level
representation (yang et al., 2016a). let e denote the token embedding lookup table. we perform a
matrix-vector multiplication ew to obtain a word-level representation. we assume c and ew have
the same length de in this work.
previous methods de   ned f using the word-level representation ew (collobert et al., 2011), the
character-level representation c (ling et al., 2015), or the concatenation [ew; c] (yang et al., 2016a).
unlike these methods, we propose to use a gate to dynamically choose between the word-level and
character-level representations based on the properties of the token. let v denote a feature vector
that encodes these properties. in this work, we use the concatenation of named entity tags, part-
of-speech tags, binned document frequency vectors, and the word-level representations to form the
feature vector v. let dv denote the length of v.
the gate is computed as follows:

g =   (wgv + bg)

3

published as a conference paper at iclr 2017

figure 1: word-character    ne-grained gating. the two lookup tables are shared.    ner   ,    pos   ,    frequency   
refer to named entity tags, part-of-speech tags, document frequency features.

where wg and bg are the model parameters with shapes de    dv and de, and    denotes an element-
wise sigmoid function.
the    nal representation is computed using a    ne-grained gating mechanism,

h = f (c, w) = g (cid:12) c + (1     g) (cid:12) (ew)

where (cid:12) denotes element-wise product between two vectors.
an illustration of our    ne-grained gating mechanism is shown in figure 1. intuitively speaking,
when the gate g has high values, more information    ows from the character-level representation to
the    nal representation; when the gate g has low values, the    nal representation is dominated by the
word-level representation.
though miyamoto & cho (2016) also use a gate to choose between word-level and character-level
representations, our method is different in two ways. first, we use a more    ne-grained gating mech-
anism, i.e., vector gates rather than scalar gates. second, we condition the gate on features that better
re   ect the properties of the token. for example, for noun phrases and entities, we would expect the
gate to bias towards character-level representations because noun phrases and entities are usually
less common and display richer morphological structure. experiments show that these changes are
key to the performance improvements for reading comprehension tasks.
our approach can be further generalized to a setting of multi-level networks so that we can combine
multiple levels of representations using    ne-grained gating mechanisms, which we leave for future
work.

3.3 document-query fine-grained gating

given the hidden states pk and qk, we now describe how to compute a representation hk that
encodes the interactions between the document and the query. in this section, we drop the superscript
k (the layer number) for notation simplicity. let pi denote the i-th row of p and qj denote the j-row
of q. let dh denote the lengths of pi and qj.
attention-over-attention (aoa) (cui et al., 2016) de   nes a dot product between each pair of tokens
in the document and the query, i.e., pt
i qj, followed by row-wise and column-wise softmax non-
linearities. aoa imposes pair-wise interactions between the document and the query, but using a
dot product is potentially not expressive enough and hard to generalize to multi-layer networks. the
gated attention (ga) reader (dhingra et al., 2016a) de   nes an element-wise product as pi(cid:12)gi where
gi is a gate computed by attention mechanism on the token pi and the entire query. the intuition
for the gate gi is to attend to important information in the document. however, there is no direct
pair-wise interaction between each token pair.

4

published as a conference paper at iclr 2017

figure 2: paragraph-question    ne-grained gating.

we present a    ne-grained gating method that combines the advantages of the above methods (i.e.,
both pairwise and element-wise). we compute the pairwise element-wise product between the hid-
den states in the document and the query, as shown in figure 2. more speci   cally, for pi and qj, we
have

iij = tanh(pi (cid:12) qj)

where qj can be viewed as a gate to    lter the information in pi. we then use an attention mechanism
over iij to output hidden states hi as follows

hi =

softmax(ut

h iij + wt

i wjbh1 + bh2)iij

j

where uh is a dv-dimensional model parameter, bh1 and bh2 are scalar model parameters, wi and
wj are one-hot encodings for pi and qj respectively. we additionally use one-hot encodings in the
attention mechanism to reinforce the matching between the same tokens since such information is
not fully preserved in iij when k is large. the softmax nonlinearity is applied over all j   s. the    nal
hidden states h are formed by concatenating the hi   s for each token pi.

4 experiments

we    rst present experimental results on the twitter dataset where we can rule out the effects of
different choices of network architectures, to demonstrate the effectiveness of our word-character
   ne-grained gating approach. later we show experiments on more challenging datasets on reading
comprehension to further show that our approach can be used to improve the performance on high-
level nlp tasks as well.

4.1 evaluating word-character gating on twitter

we evaluate the effectiveness of our word-character    ne-grained gating mechanism on a social media
tag prediction task. we use the twitter dataset and follow the experimental settings in dhingra et al.
(2016b). we also use the same network architecture upon the token representations, which is an
lstm layer followed by a softmax classi   cation layer (dhingra et al., 2016b). the twitter dataset
consists of english tweets with at least one hashtag from twitter. hashtags and html tags have
been removed from the body of the tweet, and user names and urls are replaced with special
tokens. the dataset contains 2 million tweets for training, 10k for validation and 50k for testing,
with a total of 2,039 distinct hashtags. the task is to predict the hashtags of each tweet.
we compare several different methods as follows. word char concat uses the concatenation of
word-level and character-level representations as in yang et al. (2016a); word char feat concat
concatenates the word-level and character-level representations along with the features described in

5

(cid:88)

published as a conference paper at iclr 2017

table 1: performance on the twitter dataset.    word    and    char    means using word-level and character-level
representations respectively.

model

precision@1 recall@10 mean rank

word (dhingra et al., 2016b)
char (dhingra et al., 2016b)
word char concat
word char feat concat
scalar gate
   ne-grained gate

0.241
0.284
0.2961
0.2951
0.2974
0.3069

0.428
0.485
0.4959
0.4974
0.4982
0.5119

133
104
105.8
106.2
104.2
101.5

table 2: performance on the cbt dataset. the    ga word char concat    results are extracted from dhingra
et al. (2016a). our results on    ne-grained gating are based on a single model.    cn    and    ne    are two widely
used question categories.    dev    means development set, and    test    means test set.

model

cn dev cn test ne dev ne test

ga word char concat
ga word char feat concat
ga scalar gate
ga    ne-grained gate
fg    ne-grained gate

sordoni et al. (2016)
trischler et al. (2016)
cui et al. (2016)
munkhdalai & yu (2016)

kadlec et al. (2016) ensemble
sordoni et al. (2016) ensemble
trischler et al. (2016) ensemble

0.731
0.7250
0.7240
0.7425
0.7530

0.721
0.715
0.722
0.743

0.711
0.741
0.736

0.696
0.6928
0.6908
0.7084
0.7204

0.692
0.674
0.694
0.719

0.689
0.710
0.706

0.768
0.7815
0.7810
0.7890
0.7910

0.752
0.753
0.778
0.782

0.762
0.769
0.766

0.725
0.7256
0.7260
0.7464
0.7496

0.686
0.697
0.720
0.732

0.710
0.720
0.718

section 3.2; scalar gate uses a scalar gate similar to miyamoto & cho (2016) but is conditioned on
the features;    ne-grained gate is our method described in section 3.2. we include word char feat
concat for a fair comparison because our    ne-grained gating approach also uses the token features.
the results are shown in table 1. we report three id74 including precision@1, re-
call@10, and mean rank. our method outperforms character-level models used in dhingra et al.
(2016b) by 2.29%, 2.69%, and 2.5 points in terms of precision, recall and mean rank respectively.
we can observe that scalar gating approach (miyamoto & cho, 2016) can only marginally improve
over the baseline methods, while    ne-grained gating methods can substantially improve model per-
formance. note that directly concatenating the token features with the character-level and word-level
representations does not boost the performance, but using the token features to compute a gate (as
done in    ne-grained gating) leads to better results. this indicates that the bene   t of    ne-grained
gating mainly comes from better modeling rather than using additional features.

4.2 performance on reading comprehension

after investigating the effectiveness of the word-character    ne-grained gating mechanism on the
twitter dataset, we now move on to a more challenging task, reading comprehension. in this section,
we experiment with two datasets, the children   s book test dataset (hill et al., 2016) and the squad
dataset (rajpurkar et al., 2016).

4.2.1 cloze-style questions

we evaluate our model on cloze-style id53 benchmarks.

6

published as a conference paper at iclr 2017

table 3: performance on the who did what dataset.    dev    means development set, and    test    means test set.
   wdw-r    is the relaxed version of wdw.

model

wdw dev wdw test wdw-r dev wdw-r test

kadlec et al. (2016)
chen et al. (2016)
munkhdalai & yu (2016)
dhingra et al. (2016a)

this paper

   
   
0.665
0.716

0.723

0.570
0.640
0.662
0.712

0.717

   
   
0.670
0.726

0.731

0.590
0.650
0.667
0.726

0.726

table 4: performance on the squad dev set. test set results are included in the brackets.

model

f1

ga word
ga word char concat
ga word char feat concat
ga scalar gate
ga    ne-grained gate
fg    ne-grained gate
fg    ne-grained gate + ensemble

0.6695
0.6857
0.6904
0.6850
0.6983
0.7125
0.7341 (0.733)

exact match

0.5492
0.5639
0.5711
0.5620
0.5804
0.5995
0.6238 (0.625)

yu et al. (2016)
wang & jiang (2016)

0.712 (0.710)
0.700 (0.703)

0.625 (0.625)
0.591 (0.595)

the children   s book test (cbt) dataset is built from children   s books. the whole dataset has
669,343 questions for training, 8,000 for validation and 10,000 for testing. we closely follow the
setting in dhingra et al. (2016a) and incrementally add different components to see the changes in
performance. for the    ne-grained gating approach, we use the same hyper-parameters as in dhingra
et al. (2016a) except that we use a character-level gru with 100 units to be of the same size as the
word lookup table. the id27s are updated during training.
in addition to different ways of combining word-level and character-level representations, we also
compare two different ways of integrating documents and queries: ga refers to the gated attention
reader (dhingra et al., 2016a) and fg refers to our    ne-grained gating described in section 3.3.
the results are reported in table 2. we report the results on common noun (cn) questions and
named entity (ne) questions, which are two widely used question categories in cbt. our    ne-
grained gating approach achieves new state-of-the-art performance on both settings and outperforms
the current state-of-the-art results by up to 1.76% without using ensembles. our method outperforms
the baseline ga reader by up to 2.4%, which indicates the effectiveness of the    ne-grained gating
mechanism. consistent with the results on the twitter dataset, using word-character    ne-grained
gating can substantially improve the performance over concatenation or scalar gating. furthermore,
we can see that document-query    ne-grained gating also contributes signi   cantly to the    nal results.
we also apply our    ne-grained gating model to the who did what (wdw) dataset (?). as shown in
table 3, our model achieves state-of-the-art results compared to strong baselines. we    x the word
embeddings during training.

4.2.2 squad

the stanford id53 dataset (squad) is a reading comprehension dataset collected
recently (rajpurkar et al., 2016). it contains 23,215 paragraphs come from 536 wikipedia articles.
unlike other reading comprehension datasets such as cbt, the answers are a span of text rather than
a single word. the dataset is partitioned into a training set (80%, 87,636 question-answer pairs), a
development set (10%, 10,600 question-answer pairs) and a test set which is not released.

7

published as a conference paper at iclr 2017

figure 3: visualization of the weight matrix wg. weights for each features are averaged. red means high and
yellow means low. high weight values favor character-level representations, and low weight values favor word-
level representations.    organization   ,       person   ,    location   , and    o    are named entity tags;    doclen-n   
are document frequency features (larger n means higher frequency, n from 0 to 4); others are pos tags.

figure 4: visualization of gate values in the text. red means high and yellow means low. high gate values
favor character-level representations, and low gate values favor word-level representations.

we report our results in table 4.    exact match    computes the ratio of questions that are answered
correctly by strict string comparison, and the f1 score is computed on the token level. we can
observe that both word-character    ne-grained gating and document-query    ne-grained gating can
substantially improve the performance, leading to state-of-the-art results among published papers.
note that at the time of submission, the best score on the leaderboard is 0.716 in exact match and
0.804 in f1 without published papers. a gap exists because our architecture described in section
3.1 does not speci   cally model the answer span structure that is unique to squad. in this work, we
focus on this general architecture to study the effectiveness of    ne-grained gating mechanisms.

4.3 visualization and analysis

we visualize the model parameter wg as described in section 3.2. for each feature, we average the
corresponding weight vector in wg. the results are described in figure 3. we can see that named
entities like    organization    and noun phrases (with tags    nnp    or    nnps   ) tend to use character-
level representations, which is consistent with human intuition because those tokens are usually
infrequent or display rich morphologies. also, doclen-4, wh-adverb (   wrb   ), and conjunction
(   in    and    cc   ) tokens tend to use word-level representations because they appear frequently.
we also sample random span of text from the squad dataset, and visualize the average gate values
in figure 4. the results are consistent with our observations in figure 3. rare tokens, noun phrases,
and named entities tend to use character-level representations, while others tend to use word-level
representations. to further justify this argument, we also list the tokens with highest and lowest gate
values in table 5.

8

published as a conference paper at iclr 2017

table 5: word tokens with highest and lowest gate values. high gate values favor character-level representa-
tions, and low gate values favor word-level representations.

gate values word tokens

lowest

highest

or but but these these however however among among that when when although
although because because until many many than though though this this since
since date where where have that and and such such number so which by by
how before before with with between between even even if

sweetgum untersee jianlong floresta chlorella obersee pht doctorin jumonville
wfts wtsp boven pharm nederrijn otrar rhin magicicada wbkb tanzler
kmbc wplg mainau merwede rmjm kleitman scheur bodensee kromme
horenbout vorderrhein chlamydomonas scantlebury qingshui funchess

5 conclusions

we present a    ne-grained gating mechanism that dynamically combines word-level and character-
level representations based on word properties. experiments on the twitter tag prediction dataset
show that    ne-grained gating substantially outperforms scalar gating and concatenation. our method
also improves the performance on reading comprehension and achieves new state-of-the-art results
on cbt and wdw. in our future work, we plan to to apply the    ne-grained gating mechanism for
combining other levels of representations, such as phrases and sentences. it will also be intriguing
to integrate ner and pos networks and learn the token representation in an end-to-end manner.

acknowledgments

this work was funded by nvidia, the of   ce of naval research scene understanding grant
n000141310721, the nsf grant iis1250956, and google research.

references
dzmitry bahdanau, kyunghyun cho, and yoshua bengio. id4 by jointly learning to

align and translate. in iclr, 2015.

danqi chen, jason bolton, and christopher d manning. a thorough examination of the id98/daily mail reading

comprehension task. in acl, 2016.

kyunghyun cho, bart van merri  enboer, dzmitry bahdanau, and yoshua bengio. on the properties of neural
machine translation: encoder-decoder approaches. in eighth workshop on syntax, semantics and structure
in statistical translation, 2014.

ronan collobert, jason weston, l  eon bottou, michael karlen, koray kavukcuoglu, and pavel kuksa. natural
language processing (almost) from scratch. journal of machine learning research, 12(aug):2493   2537,
2011.

yiming cui, zhipeng chen, si wei, shijin wang, ting liu, and guoping hu. attention-over-attention neural

networks for reading comprehension. arxiv preprint arxiv:1607.04423, 2016.

bhuwan dhingra, hanxiao liu, zhilin yang, william w cohen, and ruslan salakhutdinov. gated-attention

readers for text comprehension. arxiv preprint arxiv:1606.01549, 2016a.

bhuwan dhingra, zhong zhou, dylan fitzpatrick, michael muehl, and william w cohen. tweet2vec:

character-based distributed representations for social media. in acl, 2016b.

karl moritz hermann, tomas kocisky, edward grefenstette, lasse espeholt, will kay, mustafa suleyman,

and phil blunsom. teaching machines to read and comprehend. in nips, pp. 1693   1701, 2015.

felix hill, antoine bordes, sumit chopra, and jason weston. the goldilocks principle: reading children   s

books with explicit memory representations. in iclr, 2016.

9

published as a conference paper at iclr 2017

sepp hochreiter and j  urgen schmidhuber. long short-term memory. neural computation, 9(8):1735   1780,

1997.

rudolf kadlec, martin schmid, ondrej bajgar, and jan kleindienst. text understanding with the attention sum

reader network. in acl, 2016.

yoon kim, yacine jernite, david sontag, and alexander m rush. character-aware neural language models. in

aaai, 2016.

wang ling, tiago lu    s, lu    s marujo, ram  on fernandez astudillo, silvio amir, chris dyer, alan w black,
and isabel trancoso. finding function in form: compositional character models for open vocabulary word
representation. in emnlp, 2015.

minh-thang luong and christopher d manning. achieving open vocabulary id4 with

hybrid word-character models. in acl, 2016.

yasumasa miyamoto and kyunghyun cho. gated word-character recurrent language model. in emnlp, 2016.

tsendsuren munkhdalai and hong yu. neural semantic encoders. arxiv preprint arxiv:1607.04315, 2016.

pranav rajpurkar, jian zhang, konstantin lopyrev, and percy liang. squad: 100,000+ questions for machine

comprehension of text. in emnlp, 2016.

alessandro sordoni, phillip bachman, and yoshua bengio. iterative alternating neural attention for machine

reading. arxiv preprint arxiv:1606.02245, 2016.

adam trischler, zheng ye, xingdi yuan, and kaheer suleman. natural language comprehension with the

epireader. in emnlp, 2016.

oriol vinyals, meire fortunato, and navdeep jaitly. id193. in nips, pp. 2692   2700, 2015.

shuohang wang and jing jiang. machine comprehension using match-lstm and answer pointer. arxiv preprint

arxiv:1608.07905, 2016.

yuhuai wu, saizheng zhang, ying zhang, yoshua bengio, and ruslan salakhutdinov. on multiplicative inte-

gration with recurrent neural networks. in nips, 2016.

bishan yang, wen-tau yih, xiaodong he, jianfeng gao, and li deng. learning multi-relational semantics

using neural-embedding models. in nips 2014 workshop on learning semantics, 2014.

zhilin yang, ruslan salakhutdinov, and william cohen. multi-task cross-lingual sequence tagging from

scratch. arxiv preprint arxiv:1603.06270, 2016a.

zhilin yang, ye yuan, yuexin wu, ruslan salakhutdinov, and william w cohen. review networks for caption

generation. in nips, 2016b.

yang yu, wei zhang, kazi hasan, mo yu, bing xiang, and bowen zhou. end-to-end answer chunk extraction

and ranking for reading comprehension. arxiv preprint arxiv:1610.09996, 2016.

10

