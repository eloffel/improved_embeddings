6
1
0
2

 

v
o
n
8
2

 

 
 
]
l
c
.
s
c
[
 
 

1
v
0
0
1
9
0

.

1
1
6
1
:
v
i
x
r
a

under review as a conference paper at iclr 2017

learning to compose words into sentences
with id23

dani yogatama, phil blunsom, chris dyer, edward grefenstette, wang ling
deepmind
{dyogatama,pblunsom,cdyer,etg,lingwang}@google.com

abstract

we use id23 to learn tree-structured neural networks for com-
puting representations of natural language sentences. in contrast with prior work
on tree-structured models in which the trees are either provided as input or pre-
dicted using supervision from explicit treebank annotations, the tree structures
in this work are optimized to improve performance on a downstream task. ex-
periments demonstrate the bene   t of learning task-speci   c composition orders,
outperforming both sequential encoders and recursive encoders based on treebank
annotations. we analyze the induced trees and show that while they discover
some linguistically intuitive structures (e.g., noun phrases, simple verb phrases),
they are different than conventional english syntactic structures.

1

introduction

languages encode meaning in terms of hierarchical, nested structures on sequences of
words (chomsky, 1957). however, the degree to which neural network architectures that com-
pute representations of the meaning of sentences for practical applications should explicitly re   ect
such structures is a matter for debate.
there are three predominant approaches for constructing vector representations of sentences from a
sequence of words. the    rst composes words sequentially using a recurrent neural network, treat-
ing the id56   s    nal hidden state as the representation of the sentence (cho et al., 2014; sutskever
et al., 2014; kiros et al., 2015). in such models, there is no explicit hierarchical organization im-
posed on the words, and the id56   s dynamics must learn to simulate it. the second approach uses
tree-structured networks to recursively compose representations of words and phrases to form rep-
resentations of larger phrases and,    nally, the complete sentence. in contrast to sequential models,
these models    architectures are organized according to each sentence   s syntactic structure, that is, the
hierarchical organization of words into nested phrases that characterizes human intuitions about how
words combine to form grammatical sentences. prior work on tree-structured models has assumed
that trees are either provided together with the input sentences (clark et al., 2008; grefenstette &
sadrzadeh, 2011; socher et al., 2011; 2013; tai et al., 2015) or that they are predicted based on
explicit treebank annotations jointly with the downstream task (bowman et al., 2016; dyer et al.,
2016). the last approach for constructing sentence representations uses convolutional neural net-
works to produce the representation in a bottom up manner, either with syntactic information (ma
et al., 2015) or without (kim, 2014; kalchbrenner et al., 2014).
our work can be understood as a compromise between the    rst two approaches. rather than using
explicit supervision of tree structure, we use id23 to learn tree structures (and
thus, sentence-speci   c compositional architectures), taking performance on a downstream task that
uses the computed sentence representation as the reward signal. in contrast to sequential id56s,
which ignore tree structure, our model still generates a latent tree for each sentence and uses it to
structure the composition. our hypothesis is that encouraging the model to learn tree-structured
compositions will bias the model toward better generalizations about how words compose to form
sentence meanings, leading to better performance on downstream tasks.
this work is related to unsupervised grammar induction (klein & manning, 2004; blunsom & cohn,
2010; spitkovsky et al., 2011, inter alia), which seeks to infer a generative grammar of an in   nite
language from a    nite sample of strings from the language   but without any semantic feedback.

1

under review as a conference paper at iclr 2017

previous work on unsupervised grammar induction that incorporates semantic supervision involves
designing complex models for id35s (zettlemoyer & collins, 2005).
since semantic feedback has been proposed as crucial for the acquisition of syntax (pinker, 1984),
our model offers a simpler alternative.1 however, our primary focus is on improving performance
on the downstream model, so the learner may settle on a different solution than conventional english
syntax. we thus also explore what kind of syntactic structures are derivable from shallow semantics.
experiments on various tasks (i.e., id31, semantic relatedness, natural language infer-
ence, and sentence generation) show that id23 is a promising direction to discover
hierarchical structures of sentences. notably, representations learned this way outperformed both
conventional left-to-right models and tree-structured models based on linguistic syntax in down-
stream applications. this is in line with prior work showing the value of learning tree structures in
id151 models (chiang, 2007). although the induced tree structures mani-
fested a number of linguistically intuitive structures (e.g., noun phrases, simple verb phrases), there
are a number of marked differences to conventional analyses of english sentences (e.g., an overall
left-branching structure).

2 model

our model consists of two components: a sentence representation model and a reinforcement learn-
ing algorithm to learn the tree structure that is used by the sentence representation model.

2.1 tree lstm

our sentence representation model follows the stack-augmented parser-interpreter neural network
(spinn; bowman et al., 2016), spinn is a shift-reduce parser that uses long short-term memory
(lstm; hochreiter and schmidhuber, 1997) as its composition function. given an input sentence
of n words x = {x1, x2, . . . , xn}, we represent each word by its embedding vector xi     rd.
the parser maintains an index pointer p starting from the leftmost word (p = 1) and a stack. to
parse the sentence, it performs a sequence of operations a = {a1, a2, . . . , a2n   1}, where at    
{shift, reduce}. a shift operation pushes xp to the stack and moves the pointer to the next word
(p++); while a reduce operation pops two elements from the stack, composes them to a single
element, and pushes it back to the stack. spinn uses tree lstm (tai et al., 2015) as the reduce
composition function, which we follow. in tree lstm, each element of the stack is represented by
two vectors, a hidden state representation h and a memory representation c. two elements of the
stack (hi, ci) and (hj, cj) are composed as:

i =   (wi [hi, hj] + bi )

fl =   (wfl[hi, hj] + bfl )
fr =   (wfr [hi, hj] + bfr )
o =   (wo[hi, hj] + bi )
g = tanh(wg[hi, hj] + bg)
c = fl (cid:12) ci + fr (cid:12) cj + i (cid:12) g
h = o (cid:12) c

(1)

where [hi, hj] denotes concatenation of hi and hj, and    is the sigmoid activation function.
a unique sequence of {shift, reduce} operations corresponds to a unique binary parse tree of the
sentence. a shift operation introduces a new leaf node in the parse tree, while a reduce operation
combines two nodes by merging them into a constituent. see figure 1 for an example. we note that
for a sentence of length n, there are exactly n shift operations and n     1 reduce operations that
are needed to produce a binary parse tree of the sentence. the    nal sentence representation produced
by the tree lstm is the hidden state of the    nal element of the stack hn   1 (i.e., the topmost node
of the tree).

tracking lstm spinn optionally augments tree lstm with another lstm that incorporates
contextual information in sequential order called tracking lstm, which has been shown to improve

1our model only produces an interpretation grammar that parses language instead of a generative grammar.

2

under review as a conference paper at iclr 2017

figure 1: four examples of trees and their corresponding shift (s) and reduce (r) sequences. in
each of the examples, there are 4 input words (4 leaf nodes), so 7 operations (4 s, 3 r) are needed
to construct a valid tree. the nodes are labeled with the timesteps in which they are introduced to
the trees t     {1, . . . , 7}. a shift operation introduces a leaf node, whereas a reduce operation
introduces a non-leaf node by combining two previously introduced nodes. we can see that different
s-r sequences lead to different tree structures.

performance for id123. it is a standard recurrent lstm network that takes as input the
hidden states of the top two elements of the stack and the embedding vector of the word indexed by
the pointer at timestep t. every time a reduce operation is performed, the output of the tracking
lstm e is included as an additional input in eq. 1 (i.e., the input to the reduce composition
function is [hi, hj, e] instead of [hi, hj]).

2.2 id23

in previous work (tai et al., 2015; bowman et al., 2016), the tree structures that guided composition
orders of tree lstm models are given directly as input (i.e., a is observed and provided as an input).
formally, each training data is a triplet {x, a, y}. tai et al. (2015) consider models where a is also
given at test time, whereas bowman et al. (2016) explore models where a can be either observed or
not at test time. when it is only observed during training, a policy is trained to predict a at test time.
note that in this case the policy is trained to match explicit human annotations (i.e., id32
annotations), so the model learns to optimize representations according to structures that follows
human intuitions. they found that models that observe a at both training and test time are better
than models that only observe a during training.
our main idea is to use id23 (id189) to discover the best tree
structures for the task that we are interested in. we do not place any kind of restrictions when
learning these structures other than that they have to be valid binary parse trees, so it may result
in tree structures that match human linguistic intuition, heavily right or left branching, or other
solutions if they improve performance on the downstream task.
we parameterize each action a     {shift, reduce} by a policy network   (a | s; wr), where s is
a representation of the current state and wr is the parameter of the network. speci   cally, we use a
two-layer feedforward network that takes the hidden states of the top two elements of the stack hi
and hj and the embedding vector of the word indexed by the pointer xp as its input:

s = relu(w1
  (a | s; wr)     exp(w2(cid:62)

r s + b2
r)

r[hi, hj, xp] + b1
r)

where [hi, hj, xp] denotes concatenation of vectors inside the brackets.
if a is given as part of the training data, the policy network can be trained   in a supervised training
regime   to predict actions that result in trees that match human intuitions. our training data, on
the other hand, is a tuple {x, y}. we use reinforce (williams, 1992), which is an instance of a
broader class of algorithms called id189, to learn wr such that the sequence of
actions a = {a1, . . . , at} maximizes:

(cid:34) t(cid:88)

(cid:35)

r(w) = e  (a,s;wr)

rtat

,

where rt is the reward at timestep t. we use performance on a downstream task as the reward func-
tion. for example, if we are interested in using the learned sentence representations in a classi   cation

t=1

3

1245367124637512347561236457s,s,r,s,s,r,rs,s,s,r,r,s,rs,s,r,s,r,s,rs,s,s,s,r,r,runder review as a conference paper at iclr 2017

table 1: descriptive statistics of datasets used in our experiments.

dataset
sick
snli
sst
imdb

# of train

# of dev

4,500
550,152
98,794
441,617

500

10,000

872

223,235

# of test vocab size
4,927
10,000
1,821
223,236

2,172
18,461
8,201
29,209

task, our reward function is the id203 of predicting the correct label using a sentence represen-
tation composed in the order given by the sequence of actions sampled from the policy network, so
r(w) = log p(y | t-lstm(x); w), where we use w to denote all model parameters (tree lstm,
policy network, and classi   er parameters), y is the correct label for input sentence x, and x is rep-
resented by the tree lstm structure in   2.1. for a id86 task where the goal
is to predict the next sentence given the current sentence, we can use the id203 of predicting
words in the next sentence as the reward function, so r(w) = log p(xs+1 | t-lstm(xs); w).
note that in our setup we do not immediately receive a reward after performing an action at timestep
t. the reward is only observed at the end after we    nish creating a representation for the current
sentence with tree lstm and use the resulting representation for the downstream task. at each
timestep t, we sample a valid action according to   (a | s; wr). we add two simple constraints to
make the sequence of actions result in a valid tree: reduce is forbidden if there are fewer than two
elements on the stack, and shift is forbidden if there are no more words to read from the sentence.
after reaching timestep 2n     1, we construct the    nal representation and receive a reward that is
used to update our model parameters.
we experiment with two learning methods: unsupervised structures and semi-supervised structures.
suppose that we are interested in a classi   cation task. in the unsupervised case, the objective func-
tion that we maximize is log p(y | t-lstm(x); w). in the semi-supervised case, the objective
function for the    rst e epochs also includes a reward term for predicting the correct shift or re-
duce actions obtained from an external parser   in addition to performance on the downstream task,
so we maximize log p(y | t-lstm(x); w) + log   (a | s; wr). the motivation behind this model
is to    rst guide the model to discover tree structures that match human intuitions, before letting it
explore other structures close to these ones. after epoch e, we remove the second term from our ob-
jective function and continue maximizing the    rst term. note that unsupervised and semi-supervised
here refer to the tree structures, not the nature of the downstream task.

3 experiments

3.1 baselines

the goal of our experiments is to evaluate our hypothesis that we can discover useful task-speci   c
tree structures (composition orders) with id23. we compare the following com-
position methods (the last two are unique to our work):

network composition order.

embedding is an average of sentence embeddings produced by each of these models.

    right to left: words are composed from right to left.2
    left to right: words are composed from left to right. this is the standard recurrent neural
    bidirectional: a bidirectional right to left and left to right models, where the    nal sentence
    supervised syntax: words are composed according to a prede   ned parse tree of the sen-
tence. when parse tree information is not included in the dataset, we use stanford parser
(klein & manning, 2003) to parse the corpus.
    semi-supervised syntax: a variant of our id23 method, where for the
   rst e epochs we include rewards for predicting prede   ned parse trees given in the super-

2we choose to include right to left as a baseline since a right-branching tree structure   which is the output of
a right to left composition order   has been shown to be a reliable baseline for unsupervised grammar induction.
(klein & manning, 2004)

4

under review as a conference paper at iclr 2017

vised model, before letting the model explore other kind of tree structures at later epochs
(i.e., semi-supervised structures in   2.2).
    latent syntax: another variant of our id23 method where there is no
prede   ned structures given to the model at all (i.e., unsupervised structures in   2.2).

for learning, we use stochastic id119 with minibatches of size 1 and (cid:96)2 id173 con-
stant tune on development data from {10   4, 10   5, 10   6, 0}. we use performance on development
data to choose the best model and decide when to stop training.

3.2 tasks

we evaluate our method on four sentence representation tasks: sentiment classi   cation, semantic
relatedness, natural language id136 (entailment), and sentence generation. we show statistics of
the datasets in table 1 and describe each task in details in the followings.

stanford sentiment treebank we evaluate our model on a sentiment classi   cation task from the
stanford sentiment treebank (socher et al., 2013). we use the binary classi   cation task where the
goal is to predict whether a sentence is a positive or a negative movie review.
we set the id27 size to 100 and initialize them with glove vectors (pennington et al.,
2014)3. for each sentence, we create a 100-dimensional sentence representation s     r100 with
tree lstm, project it to a 200-dimensional vector and apply relu: q = relu(wps + bp), and
compute p(  y = c | q; wq)     exp(wq,cq + bq).
we run each model 3 times (corresponding to three different initialization points) and use the devel-
opment data to pick the best model. we show the results in table 2. our results agree with prior
work that have shown the bene   ts of using syntactic parse tree information on this dataset (i.e., su-
pervised recursive model is generally better than sequential models). the best model is the latent
syntax model, which is also competitive with results from other work on this dataset. both the latent
and semi-supervised syntax models outperform models with prede   ned structures, demonstrating
the bene   t of learning task-speci   c composition orders.
table 2: classi   cation accuracy on stanford sentiment treebank dataset. the number of parameters
includes id27 parameters and is our approximation when not reported in previous work.

100d-right to left
100d-left to right
100d-bidirectional
100d-supervised syntax
100d-semi-supervised syntax
100d-latent syntax
rntn (socher et al., 2013)
did98 (kalchbrenner et al., 2014)
id98-random(kim, 2014)
id98-id97 (kim, 2014)
id98-multichannel (kim, 2014)
nse (munkhdalai & yu, 2016a)
nti-slstm (munkhdalai & yu, 2016b)
nti-slstm-lstm (munkhdalai & yu, 2016b)
left to right lstm (tai et al., 2015)
bidirectional lstm (tai et al., 2015)
constituency tree   lstm   random (tai et al., 2015)
constituency tree   lstm   glove (tai et al., 2015)
dependency tree-lstm (tai et al., 2015)

model acc.
83.9
84.7
84.7
85.3
86.1
86.5
85.4
86.8
82.7
87.2
88.1
89.7
87.8
89.3
84.9
87.5
82.0
88.0
85.7

# params.
1.2m
1.2m
1.5m
1.2m
1.2m
1.2m
-
-
-
-
-
5.4m
4.4m
4.8m
2.8m
2.8m
2.8m
2.8m
2.8m

semantic relatedness the second task is to predict the degree of relatedness of two sentences
from the sentences involving compositional knowledge corpus (sick; marelli et al., 2014) . in
this dataset, each pair of sentences are given a relatedness score on a 5-point rating scale. for
each sentence, we use tree lstm to create its representations. denote the    nal representations

3http://nlp.stanford.edu/projects/glove/

5

under review as a conference paper at iclr 2017

by {s1, s2}     r100. we construct our prediction by computing: u = (s2     s1)2, v = s1 (cid:12) s2,
q q + bq, where wp     r200  200, bp     r200, wq    
q = relu(wp[u, v] + bp), and   y = w(cid:62)
r200, bq     r1 are model parameters, and [u, v] denotes concatenation of vectors inside the brackets.
we learn the model to minimize mean squared error.
we run each model 5 times and use the development data to pick the best model. our results
are shown in table 3. similar to the previous task, they clearly demonstrate that learning the tree
structures yield to better performance.
we also provide results from other work on this dataset for comparisons. some of these models (lai
& hockenmaier, 2014; jimenez et al., 2014; bjerva et al., 2014) rely on feature engineering and are
designed speci   cally for this task. our tree lstm implementation performs competitively with
most models in terms of mean squared error. our best model   semi-supervised syntax   is better
than most models except lstm models of tai et al. (2015) which were trained with a different
objective function.4 nonetheless, we observe the same trends with their results that show the bene   t
of using syntactic information on this dataset.

table 3: mean squared error on sick dataset.

100d-right to left
100d-left to right
100d-bidirectional
100d-supervised syntax
100d-semi-supervised syntax
100d-latent syntax
illinois-lh (lai & hockenmaier, 2014)
unal-nlp(jimenez et al., 2014)
meaning factory (bjerva et al., 2014)
dt-id56 (socher et al., 2014)
mean vectors (tai et al., 2015)
left to right lstm (tai et al., 2015)
bidirectional lstm (tai et al., 2015)
constituency tree-lstm (tai et al., 2015)
dependency tree-lstm (tai et al., 2015)

model mse
0.461
0.394
0.373
0.381
0.320
0.359
0.369
0.356
0.322
0.382
0.456
0.283
0.274
0.273
0.253

# params.
1.0m
1.0m
1.3m
1.0m
1.0m
1.0m
-
-
-
-
650k
1.0m
1.0m
1.0m
1.0m

stanford natural language id136 we next evaluate our model for natural language infer-
ence (i.e., recognizing id123) using the stanford natural language id136 corpus
(snli; bowman et al., 2015) . natural language id136 aims to predict whether two sentences
are entailment, contradiction, or neutral, which can be formulated as a three-way classiciation prob-
lem. given a pair of sentences, similar to the previous task, we use tree lstm to create sentence
representations {s1, s2}     r100 for each of the sentences. following bowman et al. (2016), we con-
struct our prediction by computing: u = (s2   s1)2, v = s1(cid:12)s2, q = relu(wp[u, v, s1, s2]+bp),
and p(  y = c | q; wq)     exp(wq,cq + bq), where wp     r200  400, bp     r200, wq     r200, bq     r1
are model parameters. the objective function that we maximize is the log likelihood of the correct
label under the models.
we show the results in table 4. the latent syntax method performs the best. interestingly, the se-
quential left to right model is better than the supervised recursive model in our experiments, which
contradicts results from bowman et al. (2016) that show 300d-lstm is worse than 300d-spinn.
a possible explanation is that our left to right model has identical number of parameters with the
supervised model due to the inclusion of the tracking lstm even in the left to right model (the only
difference is in the composition order), whereas the models in bowman et al. (2016) have differ-
ent number of parameters. due to the poor performance of the supervised model, semi-supervised
training does not help on this dataset, although it does signi   cantly close the gap. our models under-
perform state-of-the-art models on this dataset that have almost four times the number of parameters.
we only experiment with smaller models since tree-based models with dynamic structures (e.g., our
semi-supervised and latent syntax models) take longer to train. see   4 for details and discussions
about training time.

4our experiments with the regularized kl-divergence objective function (tai et al., 2015) do not result in
signi   cant improvements, so we choose to report results with the simpler mean squared error objective function.

6

under review as a conference paper at iclr 2017

table 4: classi   cation accuracy on snli dataset.

100d-right to left
100d-left to right
100d-bidirectional
100d-supervised syntax
100d-semi-supervised syntax
100d-latent syntax
100d-lstm (bowman et al., 2015)
300d-lstm (bowman et al., 2016)
300d-spinn (bowman et al., 2016)
1024d-gru (vendrov et al., 2016)
300d-id98 (mou et al., 2016)
300d-nti (munkhdalai & yu, 2016b)
300d-nse (munkhdalai & yu, 2016a)

model acc.
79.1
80.2
80.2
78.5
80.2
80.5
77.6
80.6
83.2
81.4
82.1
83.4
84.6

# params.
2.3m
2.3m
2.6m
2.3m
2.3m
2.3m
5.7m
8.5m
9.2m
15.0m
9m
9.5m
8.5m

sentence generation the last task that we consider is id86. given a sen-
tence, the goal is to maximize the id203 of generating words in the following sentence. this is
a similar setup to the skip thought objective (kiros et al., 2015), except that we do not generate the
previous sentence as well. given a sentence, we encode it with tree lstm to obtain s     r100. we
use a bag-of-words model as our decoder, so p(wi | s; v)     exp(v(cid:62)
i s), where v     r100  29,209
and vi     r100 is the i-th column of v. using a bag-of-words decoder as opposed to a recurrent
neural network decoder increases the importance of producing a better representation of the current
sentence, since the model cannot rely on a sophisticated decoder with a language model component
to predict better. this also greatly speeds up our training time.
we use imdb movie review corpus (diao et al., 2014) for this experiment, the corpus consists
of 280,593, 33,793, and 34,029 reviews in training, development, and test sets respectively. we
construct our data using the development and test sets of this corpus. for training, we process
33,793 reviews from the original development set to get 441,617 pairs of sentences. for testing,
we use 34,029 reviews in the test set (446,471 pairs of sentences). half of these pairs is used as
our development set to tune hyperparamaters, and the remaining half is used as our    nal test set.
our results in table 5 further demonstrate that methods that learn tree structures perform better than
methods that have    xed structures.

table 5: word perplexity on the sentence generation task. we also show perplexity of the model
that does not condition on the previous sentence (unconditional) when generating bags of words for
comparison.

model
100d-unconditional
100d-right to left
100d-left to right
100d-bidirectional
100d-supervised syntax
100d-semi-supervised syntax
100d-latent syntax

perplexity
105.6
101.4
101.1
100.2
100.8
98.4
99.0

# params.
30k
6m
6m
6.2m
6m
6m
6m

4 discussion
learned structures our results in   3 show that our proposed method outperforms competing
methods with prede   ned composition order on all tasks. the right to left model tends to perform
worse than the left to right model. this suggests that the left to right composition order, similar to
how human reads in practice, is better for neural network models. our latent syntax method is able
to discover tree structures that work reasonably well on all tasks, regardless of whether the task is
better suited for a left to right or supervised syntax composition order.
we inspect what kind of structures the latent syntax model learned and how closely they match
human intuitions. we    rst compute unlabeled bracketing f1 scores5 for the learned structures and

5we use evalb toolkit from http://nlp.cs.nyu.edu/evalb/.

7

under review as a conference paper at iclr 2017

figure 2: examples of tree structures learned by our model which show that the model discovers
simple concepts such as noun phrases and verb phrases.

figure 3: examples of unconventional tree structures.

parses given by stanford parser on snli and stanford sentiment treebank. in the snli dataset,
there are 10,000 pairs of test sentences (20,000 sentences in total), while the stanford sentiment
treebank test set contains 1,821 test sentences. the f1 scores for the two datasets are 41.73 and
40.51 respectively. for comparisons, f1 scores of a right (left) branching tree are 19.94 (41.37) for
snli and 12.96 (38.56) for sst.
we also manually inspect the learned structures. we observe that in snli, the trees exhibit overall
left-branching structure, which explains why the f1 scores are closer to a left branching tree struc-
ture. note that in our experiments on this corpus, the supervised syntax model does not perform
as well as the left-to-right model, which suggests why the latent syntax model tends to converge
towards the left-to-right model. we handpicked two examples of trees learned by our model and
show them in figure 2. we can see that in some cases the model is able to discover concepts such as
noun phrases (e.g., a boy, his sleds) and simple verb phrases (e.g., wearing sunglasses, is frowning).
of course, the model sometimes settles on structures that make little sense to humans. we show two
such examples in figure 3, where the model chooses to compose playing frisbee in and outside a as
phrases.

training time a major limitation of our proposed model is that it takes much longer to train com-
pared to models with prede   ned structures. we observe that our models only outperforms models
with    xed structures after several training epochs; and on some datasets such as snli or imdb, an
epoch could take a 5-7 hours (we use batch size 1 since the computation graph needs to be recon-
structed for every example at every iteration depending on the samples from the policy network).
this is also the main reason that we could only use smaller 100-dimensional tree lstm models in
all our experiments. while for smaller datasets such as sick the overall training time is approxi-
mately 6 hours, for snli or imdb it takes 3-4 days for the model to reach convergence. in general,
the latent syntax model and semi-supervised syntax models take about two or three times longer to
converge compared to models with prede   ned structures.

5 conclusion

we presented a id23 method to learn hierarchical structures of natural language
sentences. we demonstrated the bene   t of learning task-speci   c composition order on four tasks:
id31, semantic relatedness, natural language id136, and sentence generation. we
qualitatively and quantitatively analyzed the induced trees and showed that they both incorporate
some linguistically intuitive structures (e.g., noun phrases, simple verb phrases) and are different
than conventional english syntactic structures.

8

awomanwearingsunglassesisfrowning.aboydragshissledsthroughthesnow.twomenareplayingfrisbeeinthepark.familymembersstandingoutsideahome.under review as a conference paper at iclr 2017

references
johannes bjerva, johan bos, rob van der goot, and malvina nissim. the meaning factory: formal
in proc. of

semantics for recognizing id123 and determining semantic similarity.
semeval, 2014.

phil blunsom and trevor cohn. unsupervised induction of tree substitution grammars for depen-

dency parsing. in proc. of emnlp, 2010.

samuel r. bowman, gabor angeli, christopher potts, and christopher d. manning. a large anno-

tated corpus for learning natural language id136. in proc. of emnlp, 2015.

samuel r. bowman, jon gauthier, abhinav rastogi, raghav gupta, christopher d. manning, and
christopher potts. a fast uni   ed model for parsing and sentence understanding. in proc. of acl,
2016.

david chiang. hierarchical phrase-based translation. computational linguistics, 33(2):201   228,

2007.

kyunghyun cho, bart van merri  enboer, c   aglar g  ulc  ehre, dzmitry bahdanau, fethi bougares, hol-
ger schwenk, and yoshua bengio. learning phrase representations using id56 encoder-decoder
for id151. arxiv preprint, 2014.

noam chomsky. syntactic structures. mouton, 1957.

stephen clark, bob coecke, and mehrnoosh sadrzadeh. a compositional distributional model of

meaning. in proc. of the second symposium on quantum interaction, 2008.

qiming diao, minghui qiu, chao-yuan wu, alexander j. smola, jing jiang, and chong wang.
jointly modeling aspects, ratings and sentiments for movie recommendation (jmars). in proc.
of kdd, 2014.

chris dyer, adhiguna kuncoro, miguel ballesteros, and noah a. smith. recurrent neural network

grammars. in proc. of naacl, 2016.

edward grefenstette and mehrnoosh sadrzadeh. experimental support for a categorical composi-

tional distributional model of meaning. in proc. of emnlp, 2011.

sepp hochreiter and jurgen schmidhuber. long short-term memory. neural computation, 9(8):

1735   1780, 1997.

sergio jimenez, george duenas, julia baquero, alexander gelbukh, av juan dios batiz, and
av mendizabal. unal-nlp: combining soft cardinality features for semantic textual similarity,
relatedness and entailment. in proc. of semeval, 2014.

nal kalchbrenner, edward grefenstette, and phil blunsom. a convolutional neural network for

modelling sentences. in prof. of acl, 2014.

yoon kim. convolutional neural networks for sentence classi   cation. in proc. emnlp, 2014.

ryan kiros, yukun zhu, ruslan salakhutdinov, richard s. zemel, antonio torralba, raquel urta-

sun, and sanja fidler. skip-thought vectors. in proc. of nips, 2015.

dan klein and christopher d. manning. accurate unlexicalized parsing. in proc. of acl, 2003.

dan klein and christopher d. manning. corpus-based induction of syntactic structure: models of

dependency and constituency. in proc. of acl, 2004.

alice lai and julia hockenmaier. illinois-lh: a denotational and distributional approach to seman-

tics. in proc. of semeval, 2014.

mingbo ma, liang huang, bing xiang, and bowen zhou. dependency-based convolutional neural

networks for sentence embedding. in proc. acl, 2015.

9

under review as a conference paper at iclr 2017

marco marelli, luisa bentivogli, marco baroni, raffaella bernardi, stefano menini, and roberto
zamparelli. evaluation of compositional distributional semantic models on full sentences through
semantic relatedness and id123. in proc. of semeval, 2014.

lili mou, rui men, ge li, yan xu, lu zhang, rui yan, and zhi jin. natural language id136 by

tree-based convolution and heuristic matching. in proc. of acl, 2016.

tsendsuren munkhdalai and hong yu. neural semantic encoders. arxiv preprint, 2016a.

tsendsuren munkhdalai and hong yu. neural tree indexers for text understanding. arxiv preprint,

2016b.

jeffrey pennington, richard socher, and christopher d. manning. glove: global vectors for word

representation. in proc. of emnlp, 2014.

steven pinker. language learnability and language development. harvard, 1984.

richard socher, jeffrey pennington, eric h. huang, andrew y. ng, and christopher d. man-
in proc.

ning. semi-supervised recursive autoencoders for predicting sentiment distributions.
of emnlp, 2011.

richard socher, alex perelygin, jean wu, jason chuang, christopher manning, andrew ng, and
christopher potts. recursive deep models for semantic compositionality over a sentiment tree-
bank. in proc. of emnlp, 2013.

richard socher, andrej karpathy, quoc v le, christopher d manning, and andrew y ng.
grounded id152 for    nding and describing images with sentences. trans-
actions of the association for computational linguistics, 2:207   208, 2014.

valentin i. spitkovsky, hiyan alshawi, angel x. chang, and daniel jurafsky. unsupervised depen-

dency parsing without gold part-of-speech tags. in proc. of emnlp, 2011.

ilya sutskever, oriol vinyals, and quoc v. le. sequence to sequence learning with neural networks.

in proc. nips, 2014.

kai sheng tai, richard socher, and christopher d. manning. improved semantic representations

from tree-structured id137. in proc. of acl, 2015.

ivan vendrov, ryan kiros, sanja fidler, and raquel urtasun. order-embeddings of images and

language. in proc. of iclr, 2016.

ronald j. williams. simple statistical gradient-following algorithms for connectionist reinforcement

learning. machine learning, 8:229   256, 1992.

luke s. zettlemoyer and michael collins. learning to map sentences to logical form: structured

classi   cation with probabilistic categorial grammars. in proc. of uai, 2005.

10

