7
1
0
2

 

y
a
m
9
1

 

 
 
]
l
c
.
s
c
[
 
 

3
v
1
3
8
9
0

.

3
0
7
1
:
v
i
x
r
a

a deep compositional framework for human-like

id146 in virtual environment

haonan yu, haichao zhang, and wei xu
baidu research - institue of deep learning

sunnyvale, ca 94089

{haonanyu,zhanghaichao,xuwei06}@baidu.com

abstract

we tackle a task where an agent learns to navigate in a 2d maze-like environment
called xworld. in each session, the agent perceives a sequence of raw-pixel
frames, a natural language command issued by a teacher, and a set of rewards. the
agent learns the teacher   s language from scratch in a grounded and compositional
manner, such that after training it is able to correctly execute zero-shot commands:
1) the combination of words in the command never appeared before, and/or 2) the
command contains new object concepts that are learned from another task but never
learned from navigation. our deep framework for the agent is trained end to end:
it learns simultaneously the visual representations of the environment, the syntax
and semantics of the language, and the action module that outputs actions. the
zero-shot learning capability of our framework results from its compositionality
and modularity with parameter tying. we visualize the intermediate outputs of
the framework, demonstrating that the agent truly understands how to solve the
problem. we believe that our results provide some preliminary insights on how to
train an agent with similar abilities in a 3d environment.

introduction

1
the development of a sophisticated language
system is a very crucial part of achieving human-
level intelligence for a machine. language se-
mantics, when grounded in perception experi-
ence, can encode knowledge about perceiving
the world. this knowledge is transferred from
task to task, which empowers the machine with
generalization ability. it is argued that a machine
has to go through physical experience in order to
learn human-level semantics [kiela et al., 2016],
i.e., a process of human-like language acquisi-
tion. however, current machine learning tech-
niques do not have a reasonably fast learning
rate to make this happen. thus we model this
problem in a virtual environment, as the    rst step
towards training a physical intelligent machine.
human generalize surprisingly well when learn-
ing new concepts and skills through natural lan-
guage instructions. we are able to apply an
existing skill to newly acquired concepts with
little dif   culty. for example, a person who has
learned how to execute the command    cut x with knife    when    x    equals to apple, will do correctly

figure 1: illustration of our xworld environ-
ment and the zero-shot navigation tasks. (a) test
command contains an unseen word combination.
(b) test command contains completely new object
concepts that are learned from the recognition task
in some previous sessions (a).

(b)

(a)

navigation trainplease move to the west of cabbage.please move to the eastof fig.recognition trainq: what is in the southeast? a:watermelon.navigation testplease move to the west of fig.navigation traincould you please go to the coconut?could you please go to the apple?navigation testcould you please go to the watermelon???navigation trainplease move to the west of cabbage.please move to the eastof fig.recognition trainq: what is in the southeast? a:watermelon.navigation testplease move to the west of fig.navigation traincould you please go to the coconut?could you please go to the apple?navigation testcould you please go to the watermelon???when    x    is something else he knows (e.g., pear), even though he may have never been asked to cut
anything other than apple before. in other words, human have the zero-shot learning ability.
this paper describes a framework that demonstrates the zero-shot learning ability of an agent
in a speci   c task, namely, learning to navigate in a 2d maze-like environment called xworld
(figure 1). we are interested in solving a similar task that is faced by a baby who is learning to walk
and navigate, at the stage of learning his parents    language. the parents might give some simple
navigation command consisting of only two or three words in the beginning, and gradually increase
the complexity of the command as time goes by. meanwhile, the parents might teach the baby the
language in some other task such as object recognition. after the baby understands the language and
masters the navigation skill, he could immediately navigate to a new concept that is learned from
object recognition but never appeared in the navigation command before.
we train our baby agent across many learning sessions in xworld. in each session, the agent
perceives the environment through a sequence of raw-pixel images, a natural language command
issued by a teacher, and a set of rewards. the agent also occasionally receives the teacher   s questions
on object recognition whenever certain conditions are triggered. by exploring the environment, the
agent learns simultaneously the visual representations of the environment, the syntax and semantics
of the language, and how to navigate itself in the environment. the whole framework employed by
the agent is trained end to end from scratch by id119. we test the agent under four different
command conditions, three of which require that the agent generalizes to interpret unseen commands
and words, and that the framework architecture is modular so that other modules such as perception
and action will still work properly under such circumstance. our experiments show that the agent
performs equally well (    90% average success rate) in all conditions. moreover, several baselines
that simply learn a joint embedding for image and language yield poor results.
in summary, our main contributions are two-fold:
    a new deep id23 (drl) task that integrates both vision and language. the
language is not pre-parsed [sukhbaatar et al., 2016] or -linked [mikolov et al., 2015, sukhbaatar
et al., 2016] to the environment. instead, the agent has to learn everything from scratch and ground
the language in vision. this task models a similar scenario faced by a learning child.
    the zero-shot learning ability by leveraging the compositionality of both the language and the
network architecture. we believe that this ability is a crucial component of human-level intelligence.
2 related work
our work is inspired by the research of multiple disciplines. our xworld is similar to the mazebase
environment [sukhbaatar et al., 2016] in that both are 2d rectangular grid world. one big difference
is that their quasi-natural language is already parsed and linked to the environment. they put more
focus on reasoning and planning but not id146. on the contrary, we emphasize on how
to ground the language in vision and generalize the ability of interpreting the language. there are
several challenging 3d environments for rl such as vizdoom [kempka et al., 2016] and deepmind
lab [beattie et al., 2016]. the visual perception problems posed by them are much more dif   cult
than ours. however, these environments do not require language understanding. our agent needs to
learn to interpret different goals from different natural language commands.
our setting of language learning shares some similar ideas of the ai roadmap proposed by mikolov
et al. [2015]. like theirs, we also have a teacher in the environment that assigns tasks and rewards
to the agent. the teacher also provides additional questions and answers to the agent in an object
recognition task. unlike their proposal of entirely using linguistic channels, our tasks involve multiple
modalities and are more similar to human experience.
the importance of compositionality and modularity of a learning framework has been discussed
at length in cognitive science by lake et al. [2016]. the compositionality of our framework is
inspired by the ideas in neural programmer [neelakantan et al., 2016] and neural module networks
(nmns)[andreas et al., 2016a,b]. neural programmer is trained with id26 by employing
soft operations on databases. nmns assemble several primitive modules according to questions in
visual id53 (vqa). it depends on an external parser to convert each sentence to
one or several candidate parse trees and thus cannot be trained end to end. we adapt their primitive
modules to our framework with differentiable computation units to enable gradient calculation. their
subsequent work (n2nmns) [hu et al., 2017] learns end to end to optimize over the full space
of parse trees, but relies on candidate parse trees as an expert policy used by a behavior cloning
procedure to initialize the policy parameters.

2

our recognition task is essentially image vqa [antol et al., 2015, gao et al., 2015, ren et al., 2015,
lu et al., 2016, andreas et al., 2016a,b, teney and hengel, 2016, yang et al., 2016]. the navigation
task can also be viewed as a vqa problem if the actions are treated as answer labels. moreover, it is
a zero-shot vqa problem (i.e., test questions containing unseen concepts) which has not been well
addressed yet.
our id146 problem is closely related to some recent work on grounding language in
images and videos [yu and siskind, 2013, rohrbach et al., 2016, gao et al., 2016]. the navigation
task is also relevant to robotics navigation under natural language command [chen and mooney,
2011, tellex et al., 2011, barrett et al., 2015]. however, they either assume annotated navigation
paths in the training data or do not ground language in vision. as xworld is a virtual environment,
we currently do not address mechanics problems encountered by a physical robot, but focus on its
mental model building.
3 xworld environment
we    rst brie   y describe the xworld environment. more details are in appendix 8.5. xworld is a
2d grid world (figure 1). an agent interacts with the environment over a number of time steps t ,
with four actions: up, down, left, and right. it does so for many sessions. at the beginning of
each session, a teacher starts a timer and issues a natural language command asking the agent to reach
a location referred to by objects in the environment. there might be other objects as distractors. thus
the agent needs to differentiate and navigate to the right location. it perceives the entire environment
through rgb pixels with an egocentric view (figure 2c). if the agent correctly executes the command
before running out of time, it gets a positive reward. whenever it hits a wall or steps on an object that
is not the target, it gets a negative reward. the agent also receives a small negative reward at every
step as a punishment for taking too much time. after each session, the environment is reset randomly.
some example commands are (the parentheses contain environment con   gurations that are withheld
from the agent, same below):
    please navigate to the apple. (there is an apple, a banana, an orange, and a grape.)
    can you move to the grid between the apple and the banana? (there is an apple and a banana.
    could you please go to the red apple? (there is a green apple, a red apple, and a red cherry.)
the dif   culty of this navigation task is that, at the very beginning the agent knows nothing about the
language: every word appears equally meaningless. after trials and errors, it has to    gure out the
language syntax and semantics in order to correctly execute the command.
while the agent is exploring the environment, the teacher also asks object-related questions whenever
certain conditions are triggered (all conditions are listed in appendix 8.5). the answers are always
single words and provided by the teacher for supervision. some example qa pairs are:
    q:what is the object in the north? a:banana. (the agent is by the south of a banana, by the north
    q:where is the banana? a:north. (the agent is by the south of a banana and the east of an apple.)
    q:what is the color of the object in the west of the apple? a:yellow. (an apple has a banana on its

the apple and the banana are separated by one empty grid.)

of an apple, and by the west of a cucumber.)

west and a cucumber on its east.)

we expect the agent to transfer the knowledge exclusively learned from this recognition task to the
navigation task to execute zero-shot commands. both the commands and questions are generated
from templates (appendix 8.5) and are made like human-elicited sentences.
4 compositional framework for zero-shot navigation
the zero-shot navigation problem can be formulated as training a policy with a grounding model:
that works correctly for a new test command c(cid:48) which never appears in the training set. in the above
formulation, y is the agent   s navigation action, e is the perceived environment, a is the navigation
goal attention computed by grounding the teacher   s command c in the environment e. note that
the policy   (y|e, a) depends on a but not directly on c, thus our key problem is essentially how to
ground a new command to get the correct attention map.
the proposed framework for solving this problem contains four major modules: perception (sec-
tion 4.1), recognition (section 4.2), language (section 4.3), and action (section 4.4). the design of

  (y|e, a), with a = grounding(e, c)

3

figure 2: left: an overview of our framework. the inputs are an environment image and a sentence
(either a navigation command or a question). the output is either a navigation action or an answer to
the question, respectively. the red and blue lines in (a) indicate different tasks going through exactly
the same process. right: the pipeline of the programmer of the language module. the input is a
sequence of id27s. the output is the attention map at the    nal step.

the framework is driven by the need of navigating under new command c(cid:48). thus we will focus on
how language grounding (figure 2a) and recognition (figure 2b) are related, and how to transfer the
knowledge learned in the latter to the former. for an effective knowledge transfer, we believe that the
framework has to possess three crucial properties:

pi language grounding and recognition have to be reduced to (approximately) the same
problem. this ensures that a new word exclusively learned from recognition has already
been grounded for attention.

pii the language module must be compositional. it must process a sentence while preserving

the (major) sentence structure. one example would be a parser that outputs a parse tree.

piii inductive bias [lake et al., 2016] must be learned from training sentences. the language
module must know how to parse a sentence that has a previously seen structure even if a
word position of the sentence is    lled with a completely new word.

there is no existing framework for image captioning or vqa exhibits all such properties. while in
theory, binding parameters or embeddings between the language module and the recognition module
might satisfy the    rst property, in practice a naive binding without further architecture support hardly
works. it is also challenging for a simple (gated) recurrent neural network (id56) to satisfy the
last two properties when a sentence has a rich structure. our framework detailed below is designed
exactly for these three properties.
4.1 generating feature map
we compute a feature map f as an abstraction of the environment and the agent   s ego-centric
geometry, on which we perform the recognition and grounding of the entire dictionary. because
of this general computation purpose, the map should not only encode visual information, but also
provide spatial features. the feature map is formed by stacking two feature maps fv and fs along
the channel dimension. the perception module (figure 2c) transforms an environment image into the
visual feature map fv through a convolutional neural network (id98), whose output has a spatial
dimension equal to the number of grids n in the environment, where each pixel corresponds to a grid.
the spatial feature map fs is a set of randomly initialized parameters. intuitively, fv is responsible
for the agent   s visual sense while fs is responsible for its direction sense. the agent needs to learn to
associate visually-related words with fv while associate spatially-related words with fs.
4.2 recognizing and grounding a single word
to achieve zero-shot navigation, we start with reducing the single-word recognition and grounding to
the same problem, after which we will extend the reduction to the grounding of a sentence of multiple
words (section 4.3). the single-word grounding problem is de   ned as
(1)
where f     rd  n is a feature map, m is the word index, a     rn is a spatial attention map, and n
is the number of pixels. imagine that the feature map is a collection of slots of features, given a word
m, we need to    nd the corresponding slots (highlighted by attention) that match the word semantics.

f, m (cid:55)    a,

4

environment imageenvironment mapsentence   please move to the north of the lion .      say the color of the avocado .   embeddingattention map aweightedaveragesoftmax    "      which      white      will      yellow      question intentionsoftmaxupdownleftrightaction id98language module(a)recognition module(b)perception module(c)action module(d)state qestimated value    programmerperception id98feature map fvisualspatialfsyntaxfunctionalitybidirectional id56embeddingsyntax embeddingprogrammer bootings+1initprogrammer id56attentionfuncembeddingfeature map fmask computationword context*  *    "##    "#    "$%#    "oweighted averageelement-wisemultiplication*convolution  forget gatesprogramming stepo180-degree rotationin a reverse way, the recognition problem is de   ned as
f, a (cid:55)    m.

the recognition module (figure 2b) outputs the most possible word for a feature map weighted by
attention. this re   ects the id53 process after the agent attending to a region of the
environment image according to a question. in the extreme case, when a is a one-hot vector, the
module extracts and classi   es one slot of feature. suppose that the extracted feature f = fa is directly
fed to a softmax layer p (m|f ) = sof tmaxm (s
(cid:124)
(cid:124)
mf)a), where sof tmaxm
mf ) = sof tmaxm ((s
denotes the softmax function over m classes and sm is the parameter vector of the mth class. notice
mf essentially treats sm as a 1    1    lter and convolves it with every feature
(cid:124)
that the multiplication s
on f. this computation operates as if it is solving the grounding problem in eq. 1, and it produces an
(cid:124)
mf which is optimized towards the following property
attention map s

(cid:26) a

(cid:124)
mf    
s

m = m   
   a m (cid:54)= m    ,

where m    is the groundtruth class for f. in other words, this property implies that sm is grounded
in f where the feature of class m locates. this is exactly where we need to ground word m with
embedding em. thus we set the softmax matrix as the transpose of the id27 table (i.e.,
sm = em). so far, we have satis   ed property pi for the single-word grounding case, namely, if a new
word m + 1 is trained to be recognized accurately, then it can also be grounded correctly without
being trained to be grounded before.
there is still one    aw in the above de   nition of recognition. an attention map only tells the agent
where, but not what, to focus on. imagine a scenario in which the agent stands to the east of a red
apple. given an attention map that highlights the agent   s west, should the agent answer west, apple, or
red? it turns out that the answer should also depend on the question itself, i.e., the question intention
has to be understood. therefore, we rede   ne the recognition problem as:

f, a,   (q) (cid:55)    m,

(cid:124)

f, m,   (m) (cid:55)    a,

where q is the question and    computes its intention. in our framework,    is modeled as a gated id56
[cho et al., 2014] which outputs an embedding mask     [0, 1]d through sigmoid. then the softmax
distribution becomes p (m|f ) = sof tmaxm ((em       (q))
m(  (q)     fa))
(cid:124)
fa) = sof tmaxm (e
(figure 2b), where     denotes element-wise multiplication. to make property pi still possible, we
modify the grounding problem so that a id27 is masked before it is grounded:

(2)
where    is modeled as a projection1 that outputs an embedding mask through sigmoid. intuitively,
it enhances the grounding with the ability of selecting features that are relevant to the word being
grounded, and this selection is only decided by the word itself. both functions    and    serve as
the agent   s decision makers on which features to focus on given particular language tokens. they
operate on the same intermediate word representation referred to as functionality embedding that is a
projection of the id27 (figure 4 appendix 8.7). functionality embedding is exclusively
used for computing embedding masks. details of mask computation are in figure 5 appendix 8.7.
4.3 grounding a sentence
now we extend single-word grounding to sentence grounding while trying to preserve property pi.
the high-level idea is that we can ground a sentence by grounding individual words of that sentence
through several steps, one or more words per step at a time. the questions we will answer in this
section are how to: 1) decide which words to ground at each step, 2) combine the grounding results
of these steps to re   ect the grounding of the sentence, and 3) make the whole process differentiable.
the core of our language module (figure 2a) is a differentiable programmer (figure 2 right). we
treat each sentence (either navigation command or recognition question) as a program command.
the programmer converts the command implicitly to a set of operations (see appendix 8.2 for an
analogy between the operations of nmns and ours). the programming result is an attention map.
for navigation, the attended regions indicate the target locations. for recognition, they indicate which
part of the environment to be recognized. the programmer programs a command in several steps. at
each step, the programming consists of three substeps: attending, grounding, and combining. the
programmer    rst softly attends to a portion of a sentence. once one or more words are attended, they
are extracted, modulated by the embedding mask, and grounded in image. the resulting attention
map is selectively combined with the maps in the previous steps (detailed below). in this way, the
programmer uses the dynamic word attention, the grounding process modulated by masks, and the

1we use    projection    to denote the process of going through one or more fully-connected (fc) layers.

5

s )     a(cid:48)

s = (1       )a(cid:48)

s . assume that we cached an attention map a(cid:48)

selective map combination to implicitly output a network layout and model the structural process of
grounding a sentence in image. this approximately satis   es property pii.
the word attention is determined by the global sentential context, the local context at each word,
and the history of a programmer id56. both global and local contexts are computed based on an
intermediate word representation referred to as syntax embedding that is a projection of the word
embedding (figure 4 appendix 8.7). intuitively, the syntax embedding can be trained to encode a
group of words similarly if they have the same syntactic meaning. thus when a sentence contains
a new word that was only trained in recognition, the programmer id56 has the inductive bias of
interpreting the sentence as if the new word is an existing word with the same syntactic meaning
(property piii). the details of word attention are in appendix 8.1 and omitted here due to page limit.
with the word attention at each step s, we compute the averaged id27, mask it given
the corresponding averaged functionality embedding, and convolve it with the feature map as in
the single-word case (eq. 2). the convolution result is input to a softmax layer to get a sum-to-
one attention map a(cid:48)(cid:48)
s   1 in the previous step. the
programmer approximates the 2d translation of spatial attention by as = o(a(cid:48)(cid:48)
s   1, where o
denotes the 180-degree rotation. then the programmer caches a new attention map through a forget
s   1 +   as, where the gate        [0, 1] is computed from the current state of the
gate: a(cid:48)
programmer id56 (figure 2 right). we set a(cid:48)
0 = i where i is a map whose center pixel is one and the
rest are all zeros. finally, the attention map a at the last step is used as the output of the programmer.
4.4 navigation
a navigation attention map a only de   nes the target locations. to navigate, the agent needs to know
the surrounding environment including obstacles and distractors. our perception module (figure 2c)
convolves fv with a 1    1    lter to get the environment map. we stack the two maps and input them
to an action id98 whose output is projected to a state vector q that summarizes the environment.
the state vector is further projected to a distribution   (q, y) over the actions. at each time step, the
agent takes action y with a id203 of       0.25 + (1       )      (q, y), where    is the rate of random
exploration. the state is also projected to a scalar v (q) to approximate value function.
5 training
our training objective contains two sub-objectives l(  ) = lrl(  ) + lsl(  ), one for navigation
and the other for recognition, where    are the joint parameters of the framework. most parameters are
shared between the two tasks2. we model the recognition loss lsl as the multi-class cross id178
which has the gradients
where eq is the expectation over all the questions asked by the teacher in all training sessions, m is
the correct answer to each question, and f   is the corresponding feature. we compute the navigation
loss lrl(  ) as the negative expected reward    e     [r] the agent receives by following its policy     .
with the actor-critic (ac) algorithm [sutton and barto, 1998], we have the approximate gradients
where       are the target parameters that are periodically (every j minibatches) copied from   , r is the
immediate reward,    is the discount factor, q      is the next state after taking action y at state q  , and
     and v   are the policy and value output by the action module. since the expectations eq and e    
are different, we optimize the two sub-objectives separately over the same number of minibatches.
for effective training, we employ curriculum learning [bengio et al., 2009] and experience replay
[mnih et al., 2015] with prioritized sampling [schaul et al., 2016] (appendix 8.6).
6 experiments
we use adagrad [duchi et al., 2011] with a learning rate of 10   5 for stochastic id119
(sgd). in all experiments, we set the batch size to 16 and train 200k batches. the target parameters      
are updated every j = 2k batches. the reward discount factor    is set to 0.99. all the parameters have
a default weight decay equal to 10   4   batch size. for each layer, by default its parameters have zero
mean and a standard deviation of 1/
k, where k is the number of parameters of that layer. the agent
has 500k exploration steps in total, and the exploration rate    decreases linearly from 1 to 0. we    x
the number of programming steps as 3, and train each model with 4 random initializations. the whole
framework is implemented with paddlepaddle (https://github.com/paddlepaddle/paddle/)
and trained end to end. more implementation details are in appendix 8.3.

     lrl(  ) =    e     [(      log     (q  , y) +      v  (q  )) (r +   v      (q      )     v  (q  ))]

     lsl(  ) = eq [         log p  (m|f  )] ,

   

2in all the    gures of our framework, components with the same name share the same set of parameters.

6

baselines we    rst compare with    ve baselines in a normal setting.
    simpleattention we modify our framework by replacing the programmer with a simple attention
model. given a sentence, we use an id56 to output an embedding which is convolved as a 3    3
   lter with the visual feature map to get an attention map. the rest of our framework is unchanged
(figure 7 appendix 8.7). this baseline is an ablation to show the necessity of the programmer.
    notransshare we do not tie the id27 table to the transposed softmax matrix. in this
case, language grounding and recognition are not the same problem. this baseline is an ablation to
show the impact of the transposed sharing on the training convergence.
    vis-lstm following ren et al. [2015], we use id98 to get an image embedding which is then
projected to the id27 space and used as the    rst word of the sentence. the sentence goes
through an id56 whose last state is used for navigation and recognition (figure 8 appendix 8.7).
    multimodal we implement a multimodal framework [mao et al., 2015]. this framework uses id98
to get an image embedding and id56 to get a sentence embedding. then the two embeddings are
projected to a common feature space for navigation and recognition (figure 9 appendix 8.7).
    san we replace our language module with the attention process of stacked attention network
(san) [yang et al., 2016], with the difference of training a id98 from scratch, instead of using a
pretrained one, to accommodate to xworld. the rest of our framework is unchanged.

figure 3: training reward curves of the    ve base-
lines and our framework. the shown reward is
the accumulated discounted reward per session, av-
eraged every 8k training examples. the shaded
area of each curve denotes the variance among 4
random initializations.

for each method, we test 10k sessions in
total over four navigation subtasks nav_obj,
nav_col_obj, nav_nr_obj, and nav_bw_obj
(appendix 8.5). we compute the success rates
where success means that the agent reaches the
target location in time in a test session. the
training reward curves and the success rates are
shown in figure 3 and table 1a, respectively.
vis-lstm and multimodal have poor results be-
cause they do not ground language in vision.
san has the same overall architecture with our
framework but with a different language module,
which requires it to generate accurate naviga-
tion attention map because the navigation action
solely depends on attention maps rather than
visual image features as in vis-lstm and multi-
modal. this assumption is a quite strong one and
our experiment result shows that even though
san gets    100% recognition accuracy, its ac-
tion module hardly works. our explanation is
that san reasons spatial relationships based on the encoding of multiple neighboring objects in a
single feature at each convolution location (i.e., a broad receptive    eld), which can easily lead to
attention diffusion. its recognition module might be able to still correctly classify given diffused
attention, but the action module requires far more accurate attention as the navigation signal for the
agent. surprisingly, notransshare converges much slower than our framework does. one possible
reason is that the correct behavior of the language module is hard to be found by sgd if no constraint
on id27 is imposed. although simpleattention is able to perform well, without word-level
attention, its ability of sentence understanding is limited.
zero-shot navigation our primary question is whether the agent has the zero-shot navigation ability
of executing previously unseen commands. we setup four command conditions for training the agent:
    standard the training command set has the same distribution with the test command set.
    nc some word combinations are excluded from the training command set, even though all the
words are in it. we speci   cally consider three types of word combinations: (object, location),
(object, color), and (object, object). we enumerate all combinations for each type and randomly
hold out 10% from the teacher in navigation.
    nwnav&nwnavrec some object words are excluded from navigation training, and are trained
only in recognition and tested in navigation as new concepts. nwnavrec guarantees that the new
words will not appear in questions but only in answers while nwnav does not. we randomly hold
out 10% of the object words.

7

number of training samples#10600.511.522.53current reward-5-4-3-2-10ourssimpleattentionnotranssharevis-lstmmultimodalsannc

(a)

(b)

6.9

22.3

22.4

80.0

ours simpleattention notransshare vis-lstm multimodal san
89.2
10.0

ours/sa
standard
nwnav nwnavrec
94.6/91.6 94.6/91.8 94.6/91.7 94.2/83.4
nav_obj
nav_col_obj 94.3/88.8 93.2/89.6 94.1/89.6 94.0/83.4
93.1/74.5 92.7/74.2 93.0/77.2 93.0/70.8
nav_nr_obj
73.9/69.3 75.3/70.1 74.5/70.8 73.7/69.3
nav_bw_obj
*nav_obj
n/a 93.8/90.6 94.5/88.3 94.7/4.1
*nav_col_obj n/a 93.1/87.3 93.8/81.8 92.8/7.0
n/a 92.6/71.6 92.7/56.6 87.1/25.7
*nav_nr_obj
*nav_bw_obj
n/a 76.3/70.2 74.5/66.1 70.4/59.7

we use simpleattention for comparison as the other
baselines have far worse performance than it in the pre-
vious experiment. both simpleattention and our frame-
work are trained under each condition without chang-
ing the their hyperparameters. for testing, we put the
held-out combinations/words back to the commands
(i.e., standard condition) and again test 10k sessions.
table 1b contains the success rates. our framework
gets almost the same success rates for all the condi-
tions, and obtains high zero-shot success rates. the
table 1: success rates (%). (a) overall rates
results of nwnavrec show that although some new ob-
of all the methods in a standard training
ject concepts are learned from a completely different
command setting. (b) breakdown rates of
problem, they can be tested on navigation without any
our framework and simpleattention (sa) on
model retraining or    netuning. in contrast, simpleat-
the four subtasks under different training
tention almost fails on zero-shot commands (especially
command conditions (columns). the last
on nwnavrec). interestingly, in nav_bw_obj it has a
four rows show the rates of the test sessions
much higher zero-shot success rate than in the other
subtasks. our analysis reveals that with the 3    3    lter,
that contain commands not seen in training.
it always highlights the location between two objects without detecting their classes, because usually
there is only one quali   ed pair of objects in the environment for nav_bw_obj. as a result, it does
not really generalize to unseen commands.
visualization and analysis our framework produces intermediate results that can be easily visu-
alized and analyzed. one example has been shown in figure 2, where the environment map is
produced by a trained perception module. it detects exactly all the obstacles and goals. the map
constitutes the learned prior knowledge for navigation: all walls and goals should be avoided by
default because they incur negative rewards. this prior, combined with the attention map produced
for a command, contains all information the agent needs to navigate. the attention maps are usually
very precise (figure 10 appendix 8.7), with some rare cases in which there are    aws, e.g., when
the agent needs to navigate between two objects. this is due to our simpli   ed assumption on 2d
geometric translation: the attention map of a location word is treated as a    lter and the translation is
modeled as convolution. this results in attention diffusion in the above case. to address this issue,
more complicated transformation can be used (e.g., fc layers).
we also visualize the programming process. we observe that the programmer id56 is able to
shift its focus across steps (figure 11 appendix 8.7). with additional analysis of the ground-
ing and map operations, we    nd that in the    rst example, the programmer essentially does
transform[southeast](find[cabbage](f)), and in the second example, it essentially performs
transform[between](combineor(find[apple](f), find[coconut](f))).
we    nd the attention and environment maps very reliable by visualization. this is veri   ed by the
   100% qa accuracy in recognition. however, in table 1 the best success rate is still     5% away
from the perfect. further analysis reveals that the agent tends to stuck in loop if the target location is
behind a long wall, although it has a certain chance to bypass it (figure 12 appendix 8.7). thus we
believe that the discrepancy between the good map quality and the imperfect performance results
from our action module. currently the action module learns a direct mapping from an environment
state to an action. there is no support for either history remembering or route planning [tamar et al.,
2016]. since our focus here is zero-shot navigation, we leave such improvements to future work.
7 conclusion
we have demonstrated an end-to-end compositional framework for a virtual agent to generalize an
existing skill to new concepts without being retrained. such generalization is made possible by
reusing knowledge, encoded by language and learned in other tasks. by assembling words in different
ways, the agent is able to tackle new tasks while exploiting existing knowledge. we re   ect these ideas
in the design of our framework and apply it to a concrete example: zero-shot navigation in xworld.
our framework is just one possible implementation. our claim is not that an intelligent agent must
have a mental model as the presented one, but it has to possess several crucial properties discussed in
section 4. currently the agent explores in a 2d environment. in the future, we plan to migrate the
agent to a 3d world like malmo [johnson et al., 2016]. there will be several new challenges, e.g.,
perception and geometric transformation will be more dif   cult to model. we hope that the current
framework provides some preliminary insights on training a similar agent in a 3d environment.

8

acknowledgments

we thank yuanpeng li, liang zhao, yi yang, zihang dai, qing sun, jianyu wang, and xiaochen
lian for their helpful suggestions on writing.

references
j. andreas, m. rohrbach, t. darrell, and d. klein. neural module networks. in cvpr, pages 39   48, 2016a.
j. andreas, m. rohrbach, t. darrell, and d. klein. learning to compose neural networks for id53.

s. antol, a. agrawal, j. lu, m. mitchell, d. batra, c. lawrence zitnick, and d. parikh. vqa: visual question

in acl, pages 1545   1554, 2016b.

answering. in iccv, pages 2425   2433, 2015.

sion. arxiv preprint arxiv:1508.06161, 2015.

d. p. barrett, s. a. bronikowski, h. yu, and j. m. siskind. robot language learning, generation, and comprehen-

c. beattie, j. z. leibo, d. teplyashin, t. ward, m. wainwright, h. k  ttler, a. lefrancq, s. green, v. vald  s,
a. sadik, j. schrittwieser, k. anderson, s. york, m. cant, a. cain, a. bolton, s. gaffney, h. king,
d. hassabis, s. legg, and s. petersen. deepmind lab. corr, abs/1612.03801, 2016.

y. bengio, j. louradour, r. collobert, and j. weston. curriculum learning. in icml, pages 41   48, 2009.
d. l. chen and r. j. mooney. learning to interpret natural language navigation instructions from observations.

in aaai, volume 2, pages 1   2, 2011.

k. cho, b. van merrienboer,   . g  l  ehre, f. bougares, h. schwenk, and y. bengio. learning phrase representa-

tions using id56 encoder-decoder for id151. in emnlp, 2014.

j. duchi, e. hazan, and y. singer. adaptive subgradient methods for online learning and stochastic optimization.

jmlr, 12(jul):2121   2159, 2011.

h. gao, j. mao, j. zhou, z. huang, l. wang, and w. xu. are you talking to a machine? dataset and methods for

multilingual image question. in nips, pages 2296   2304, 2015.

q. gao, m. doering, s. yang, and j. y. chai. physical causality of action verbs in grounded language

understanding. in acl, volume 1, pages 1814   1824, 2016.

r. hu, j. andreas, m. rohrbach, t. darrell, and k. saenko. learning to reason: end-to-end module networks

for visual id53. arxiv preprint arxiv:1704.05526, 2017.

m. johnson, k. hofmann, t. hutton, and d. bignell. the malmo platform for arti   cial intelligence experimenta-

tion. in ijcai, 2016.

m. kempka, m. wydmuch, g. runc, j. toczek, and w. ja  skowski. vizdoom: a doom-based ai research
platform for visual id23. in ieee conference on computational intelligence and games,
pages 341   348, 2016.

d. kiela, l. bulat, a. l. vero, and s. clark. virtual embodiment: a scalable long-term strategy for arti   cial

intelligence research. in nips workshop, 2016.

b. m. lake, t. d. ullman, j. b. tenenbaum, and s. j. gershman. building machines that learn and think like

people. behavioral and brain sciences, pages 1   101, 11 2016.

j. lu, j. yang, d. batra, and d. parikh. hierarchical question-image co-attention for visual id53.

in nips, pages 289   297, 2016.

j. mao, x. wei, y. yang, j. wang, z. huang, and a. l. yuille. learning like a child: fast novel visual concept

learning from sentence descriptions of images. in iccv, pages 2533   2541, 2015.

t. mikolov, a. joulin, and m. baroni. a roadmap towards machine intelligence. arxiv preprint arxiv:1511.08130,

v. mnih, k. kavukcuoglu, d. silver, a. a. rusu, j. veness, m. g. bellemare, a. graves, m. riedmiller, a. k.
fidjeland, g. ostrovski, et al. human-level control through deep id23. nature, 518(7540):
529   533, 2015.

a. neelakantan, q. v. le, and i. sutskever. neural programmer: inducing latent programs with id119.

m. ren, r. kiros, and r. zemel. exploring models and data for image id53. in nips, pages

2015.

in iclr, 2016.

2953   2961, 2015.

a. rohrbach, m. rohrbach, r. hu, t. darrell, and b. schiele. grounding of textual phrases in images by

reconstruction. in eccv, pages 817   834, 2016.

t. schaul, j. quan, i. antonoglou, and d. silver. prioritized experience replay. in iclr, 2016.
m. schuster and k. k. paliwal. id182. ieee transactions on signal processing,

45(11):2673   2681, 1997.

s. sukhbaatar, a. szlam, g. synnaeve, s. chintala, and r. fergus. mazebase: a sandbox for learning from

games. arxiv preprint arxiv:1511.07401, 2016.

r. s. sutton and a. g. barto. id23: an introduction, volume 1. mit press cambridge, 1998.
a. tamar, s. levine, p. abbeel, y. wu, and g. thomas. value iteration networks. in nips, pages 2146   2154,

s. tellex, t. kollar, s. dickerson, m. r. walter, a. g. banerjee, s. teller, and n. roy. understanding natural

language commands for robotic navigation and mobile manipulation. in aaai, 2011.

2016.

9

d. teney and a. v. d. hengel. zero-shot visual id53. arxiv preprint arxiv:1611.05546, 2016.
z. yang, x. he, j. gao, l. deng, and a. smola. stacked attention networks for image id53. in

h. yu and j. m. siskind. grounded language learning from video described with sentences. in acl, pages

cvpr, pages 21   29, 2016.

53   63, 2013.

10

8 appendix

8.1 word attention

a sentence of length l is    rst converted to a sequence of id27s el by looking up the
embedding table e. then the embeddings are projected to syntax embeddings (figure 4). the syntax
embeddings are fed into a bidirectional id56 [schuster and paliwal, 1997] to obtain word context
vectors ec
l . the last forward state and the    rst backward state are concatenated and projected to a
booting vector (figure 6), which is used to initialize the programmer id56. given a    xed number of
programming steps, at each step s the programmer id56 computes the attention for each word l from
its context vector ec
l :

cs,l = sof tmaxl (cossim (hs, g(wec
cossim(z, z(cid:48)) = z

z(cid:48)

(cid:124)

(cid:107)z(cid:107)(cid:107)z(cid:48)(cid:107) ,

l + b))) ,

where w and b are projection parameters, hs is the id56 state, g is the activation function, and
sof tmaxl is the softmax function over l words. then the context vectors ec
l are weighted averaged
by the attention and fed back to the id56 to tell it how to update its hidden state.

8.2 programmer operations

the embedding masks produced by    and    support switching among the describecolor,
describelocation, and describename operations by masking id27s according to
given language tokens (section 4.2). the selective map combination in figure 2 right supports
nine different translateattention operations by convolution. our soft word attention supports
the combineand and combineor operations by attending to multiple words simultaneously. for
example, when multiple objects are attended, the resulting attention map would be a union of the
maps of the individual objects. when an object and the color modifying that object are both attended,
the resulting attention map would be an intersection of the two maps, thus selecting the object with
the correct color from a set of objects with the same name. thus compared to nmns [andreas et al.,
2016a], we end up with an implementation of an implicit and differentiable modular network.

implementation details

8.3
the agent at each time step receives a 156    156 rgb image. this image is egocentric and includes
both the environment and the black padding region. the agent processes the input image with a
id98 that has four convolutional layers: (3, 3, 64), (2, 2, 64), (2, 2, 512), (1, 1, 512), where (x, y, z)
represents z x    x    lters with stride y. all the four layers have the relu activation function. the
output is the visual feature map with 512 channels. we stack it along the channel dimension with
another parametric spatial feature map of the same sizes. this spatial feature map is initialized with
zero mean and standard deviation (figure 2c).
the agent also receives a navigation command at the beginning of a session. the same command is
repeated until the end of the session. the agent may or may not receive a question at every time step.
the dimensions of the id27, syntax embedding, and functionality embedding are 1024,
128, and 128, respectively. the id27 table is initialized with zero mean and a standard
deviation of 1. the hidden fc layers for computing the syntax and functionality embeddings have
512 units (figure 4). the bidirectional id56 for computing word contexts has a state size of 128 in
both directions. the output id56 booting vector and word context also have a length of 128. the
state size of the programmer id56 is equal to the length of the booting vector (figure 2 right). the
hidden fc layer for converting a functionality embedding to an embedding mask has a size of 128.
the id56 used for summarizing the question intention has 128 states (figure 5). all fc layers and
id56 states in the language and recognition module use tanh as the activation function. the only
exception is the fc layer that outputs the embedding mask (sigmoid).
in the action module, the action id98 for processing the attention map and the environment map has
two convolutional layers (3, 1, 64) and (3, 1, 4), both with paddings of 1. they are followed by three
fc layers that all have 512 units. all    ve layers use the relu activation function.

11

8.4 baseline models

the language module of simpleattention has the id27 size of 1024. the id56 has the
same size with the id27. the fc layer that produces the 3    3    lter has an output
size of 4608 which is 9 times the channel number of the visual feature map. the rest of the layer
con   guration is the same with our framework. vis-lstm has a id98 with four convolutional layers
(3, 2, 64), (3, 2, 64), (3, 2, 128), and (3, 1, 128). this is followed by three fc layers with size 1024.
the id27 and the id56 both have sizes of 1024. the id56 output goes through three
fc hidden layers of size 512 either for recognition or navigation. multimodal has the same layer
size con   guration with that of vis-lstm. san has a id98 with four convolutional layers (3, 3, 64),
(2, 2, 64), (2, 2, 512), and (3, 1, 1024), where the last layer makes each grid have a 3    3 receptive
   eld. its id27 size and id56 state size are both 512. following yang et al. [2016], we
use two attention layers.
the outputs of all layers of the above baselines are relu activated except for those that are designed
as linearly activated.

8.5 xworld setup

we con   gure square environments with sizes ranging from 3 to 7. we    x the size of the environment
image by padding walls for smaller environments. different sessions may have different map sizes.
in each session,
    the number of time steps t is four times the map size.
    the number of objects on the map is from 1 to 3.
    the number of wall grids on the map is from 0 to 10.
    the positive reward when the agent reaches the correct location is set to 1. the negative rewards
for hitting walls and for stepping on non-target objects are set to    0.2 and    1, respectively. the
time step penalty is set to    0.1.

the teacher has a vocabulary size of 104, including 2 punctuation marks. there are 9 locations, 4
colors, and 40 distinct object classes. each object class has 2.85 object instances on average. every
time the environment is reset, a number of object classes are randomly sampled and an object instance
is randomly sampled for each class. there are in total 16 types of sentences the teacher can speak,
including 4 types of navigation commands and 12 types of recognition questions. each sentence type
has multiple non-recursive natural-language templates, and corresponds to a subtask the agent must
learn to perform. in total there are 256,832 distinct sentences with 92,442 for the navigation task and
164,390 for the recognition task. the sentence length ranges from 2 to 12.
the object, location, and color words of the teacher   s language are listed below. these are the
content words with actual meanings that can be grounded in the environment. all the other words are
treated as grammatical words whose embeddings are only for interpreting sentence structures. the
differentiation between content and grammatical words is automatically learned by the agent based
on the teacher   s language and the environment. all words have the same form of representation.

object

location

color

other

apple, avocado, banana, blueberry, butter   y,
cabbage, cat, cherry, circle, coconut,
cucumber, deer, dog, elephant,    g,
   sh, frog, grape, hedgehog, ladybug,
lemon, lion, monkey, octopus, orange,
owl, panda, penguin, pineapple, pumpkin,
rabbit, snake, square, squirrel, star,
strawberry, triangle, turkey, turtle, watermelon

east, west,
north, south,
northeast, northwest,
southeast, southwest,
between

green,
red,
blue,
yellow located, location, me, move, name,

?, ., and, block, by, can, color, could,
destination, direction, does,    nd, go,
goal, grid, have, identify, in, is, locate,

navigate, near, nothing, object, of, on,
one, oov, please, property, reach, say,
side, target, tell, the, thing, three, to,
two, what, where, which, will, you, your

the sentence types that the teacher can speak are listed below. each sentence type corresponds to a
subtask. the triggering condition describes when the teacher says that type of sentences. besides
the conditions shown, an extra condition for navigation commands is that the target location must be
reachable from the current agent location. an extra condition for color-related questions is that the
object color must be one of the four de   ned colors, and objects with other colors will be ignored in
these questions. if at a time step there are multiple conditions triggered, we randomly sample one

12

sentence type for navigation and another for recognition. after the sentence type is sampled, we
generate the command or question according to the corresponding sentence templates.

sentence type
(subtask)

example

nav_obj

please go to the apple.

nav_col_obj

could you please move to the red apple?

nav_nr_obj
nav_bw_obj

the north of the apple is your destination.
navigate to the grid between apple and
banana please.

rec_col2obj

what is the red object?

rec_obj2col
rec_loc2obj

what is the color of the apple?
please tell the name of the object in the south.

rec_obj2loc
rec_loc2col
rec_col2loc
rec_loc_obj2obj

what is the location of the apple?
what color does the object in the east have?
where is the red object located?
identify the object which is in the east of the apple.

rec_loc_obj2col what is the color of the east to the apple?
rec_col_obj2loc where is the red apple?
rec_bw_obj2obj

what is the object between apple and banana?

rec_bw_obj2loc

where is the object between apple and banana?

rec_bw_obj2col

what is the color of the object between apple
and banana?

8.6 experience replay and curriculum learning

triggering condition

[c0] beginning of a session. &
[c1] the referred object has a unique
name.
[c0] & [c2] there are multiple objects
that either have the same name
but different colors, or have different
names but the same color.
[c0] & [c1]
[c0] & [c3] both referred objects have
unique names and are separated by
one grid.

[c4] there is only one object that
has the referred color.
[c1]
[c5] the agent is near the referred
object.
[c1] & [c5]
[c5]
[c4] & [c5]
[c1] & [c6] the referred object is
near another object
[c1] & [c6]
[c2] & [c5]
[c7] both referred objects have
unique names and are separated by
one object.
[c7] & [c8] the agent is near the
object in the middle.
[c7]

we employ experience replay [mnih et al., 2015] for training both the navigation and recognition
tasks. the environment inputs, rewards, and the actions taken by the agent at the most recent 10k
time steps are stored in a replay buffer. during training, every time two minibatches of the same
number of experiences are sampled from the buffer, one for computing      lsl(  ) and the other
for computing      lrl(  ). for the former, only individual experiences are sampled. we uniformly
sample experiences from a subset of the buffer which contains the teacher   s questions. for the latter,
we need to sample transitions (i.e., pairs of experiences) so that td error can be computed. we
sample from the entire buffer using the rank-based sampler [schaul et al., 2016] which has proven
to increase the learning ef   ciency by prioritizing rare experiences in the buffer.
because in the beginning the language is quite ambiguous, it is dif   cult for the agent to start learning
with a complex environment setup. thus we exploit curriculum learning [bengio et al., 2009] to
gradually increase the environment complexity. we gradually change the following things linearly in
proportional to min(1, g(cid:48) / g), where g(cid:48) is the number of sessions so far and g is the number of
curriculum sessions:
    the number of grids of the environment.
    the number of objects in the environment.
    the number of wall grids.
    the number of possible object classes that can appear in the environment.
    the length of a navigation command or a recognition question.
we    nd that this curriculum is crucial for ef   cient learning, because in the early phase the agent is
able to quickly master the meanings of the location and color words given only small ambiguity. after
this, these words are used to guide the optimization when more and more new sentence structures
and objects are added. in the experiments, we set g = 10k during training while do not use any
curriculum during test (maximal dif   culty).

13

8.7 figures

figure 4: the projections from id27 to syntax embedding and functionality embedding.

(a)

(b)

figure 5: details of computing the embedding masks. (a) the pipeline of question intention in
figure 2b, which computes an embedding mask according to a question. (b) the two fc layers used
by mask computation in both (a) and figure 2 right. they project a functionality embedding to a
mask.

figure 6: a bidirectional id56 that receives a sequence of syntax embeddings, and outputs a sequence
of word contexts and an id56 booting vector.

14

fc1fc2 syntax embeddingembeddingfc3fc4funcembeddingembeddingfunctionalityfuncembeddingquestion id56sequenceaveragemaskmask computationextracted featuremaskfc5fc6funcembeddingsyntax embeddingsentence contextcontroller bootingfc8fc7fc7fc7lastlastconcatfigure 7: the pipeline of the language module of the simpleattention baseline. an id56 processes
the input sentence and outputs a 3    3    lter which is convolved with the visual feature map generated
by the perception module. other modules are the same with our framework.

figure 8: our adapted version of the vis-lstm model [ren et al., 2015]. the framework treats the
projected image embedding as the    rst word of the sentence. navigation and recognition only differ
in the last several fc layers.

figure 9: a multimodal framework adapted from the one in mao et al. [2015]. the image and the
sentence are summarized by a id98 and an id56 respectively. their embeddings are concatenated
and projected to the same space. navigation and recognition only differ in the last several fc layers.

15

embedding   go to the apple .   id56sequenceaveragefcfiltervisual feature map*3x3attention mapimageembedding   go to the apple .   id56lastid98fc layersrecognition softmaxnavigation softmaximageembedding   go to the apple .   id56sequenceaverageid98fc layersrecognition softmaxnavigation softmaxconcatwatermelon is the
destination.

reach the grid between
grape and deer.

red ladybug is the
target.

can you go to the
south of the elephant?

figure 10: examples of attention maps in different sessions. top: the navigation commands. middle:
the current environment images. bottom: the corresponding attention maps output by the language
module. note that attention maps are all egocentric: the map center is the agent   s location.

figure 11: illustration of the word attention on two examples. given the current environment image
and a navigation command, the programmer generates an attention map in three steps. at each step
the programmer id56 focuses on a different portion of the sentence. the word attention is visualized
by a color strip where brighter portion means more attention. on the left of each color strip is the
corresponding attention map combining the current attention and the previously cached one (figure 2
right). the last attention map is used as the output of the programmer.

could you please go to the dog?

please reach south of the cucumber.

figure 12: examples of bypassing long walls. for each path, only three key steps are shown. (green
circles indicate successes.)

16

navigate  to  southeast  of   cabbage   .the  grid   between  apple  and coconut .