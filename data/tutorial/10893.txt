towards end-to-end learning for dialog state tracking and

management using deep id23

tiancheng zhao and maxine eskenazi

language technologies institute

carnegie mellon university

{tianchez, max+}@cs.cmu.edu

6
1
0
2

 

p
e
s
5
1

 

 
 
]
i

a
.
s
c
[
 
 

2
v
0
6
5
2
0

.

6
0
6
1
:
v
i
x
r
a

abstract

this paper presents an end-to-end frame-
work for
task-oriented id71
using a variant of deep recurrent q-
networks (drqn). the model
is able
to interface with a relational database
and jointly learn policies for both lan-
guage understanding and dialog strategy.
moreover, we propose a hybrid algorithm
that combines the strength of reinforce-
ment learning and supervised learning to
achieve faster learning speed. we evalu-
ated the proposed model on a 20 question
game conversational game simulator. re-
sults show that the proposed method out-
performs the modular-based baseline and
learns a distributed representation of the
latent dialog state.

introduction

1
task-oriented id71 have been an im-
portant branch of spoken dialog system (sds)
research (raux et al., 2005; young, 2006; bo-
hus and rudnicky, 2003). the sds agent has
to achieve some prede   ned targets (e.g. book-
ing a    ight) through natural language interac-
tion with the users. the typical structure of a
task-oriented dialog system is outlined in fig-
ure 1 (young, 2006). this pipeline consists of
several independently-developed modules: natural
language understanding (the nlu) maps the user
utterances to some semantic representation. this
information is further processed by the dialog state
tracker (dst), which accumulates the input of the
turn along with the dialog history. the dst out-
puts the current dialog state and the dialog policy
selects the next system action based on the dia-
log state. then id86 (id86)
maps the selected action to its surface form which

is sent to the tts (text-to-speech). this process
repeats until the agent   s goal is satis   ed.

figure 1: conventional pipeline of an sds.
the proposed model replaces the modules in the
dotted-line box with one end-to-end model.

the conventional sds pipeline has limitations.
the    rst issue is the credit assignment problem.
developers usually only get feedback from the end
users, who inform them about system performance
quality. determining the source of the error re-
quires tedious error analysis in each module be-
cause errors from upstream modules can propa-
gate to the rest of the pipeline. the second lim-
itation is process interdependence, which makes
online adaptation challenging. for example, when
one module (e.g. nlu) is retrained with new data,
all the others (e.g dm) that depend on it become
sub-optimal due to the fact that they were trained
on the output distributions of the older version of
the module. although the ideal solution is to re-
train the entire pipeline to ensure global optimal-
ity, this requires signi   cant human effort.

due to these limitations, the goal of this study
is to develop an end-to-end framework for task-
oriented sds that replaces 3 important modules:
the nlu, the dst and the dialog policy with a sin-
gle module that can be jointly optimized. devel-
oping such a model for task-oriented dialog sys-

tems faces several challenges. the foremost chal-
lenge is that a task-oriented system must learn a
strategic dialog policy that can achieve the goal of
a given task which is beyond the ability of standard
supervised learning (li et al., 2014). the second
challenge is that often a task-oriented agent needs
to interface with structured external databases,
which have symbolic query formats (e.g. sql
query). in order to    nd answers to the users    re-
quests from the databases, the agent must formu-
late a valid database query. this is dif   cult for
conventional neural network models which do not
provide intermediate symbolic representations.

this paper describes a deep reinforcement
learning based end-to-end framework for both dia-
log state tracking and dialog policy that addresses
the above-mentioned issues. we evaluated the pro-
posed approach on a conversational game sim-
ulator that requires both language understanding
and strategic planning. our studies yield promis-
ing results 1) in jointly learning policies for state
tracking and dialog strategies that are superior to
a modular-based baseline, 2) in ef   ciently incor-
porating various types of labelled data and 3) in
learning dialog state representations.

section 2 of the paper discusses related work;
section 3 reviews the basics of deep reinforce-
ment learning; section 4 describes the proposed
framework; section 5 gives experimental results
and model analysis; and section 6 concludes.

2 related work

dialog state tracking: the process of constantly
representing the state of the dialog is called di-
alog state tracking (dst). most industrial sys-
tems use rule-based heuristics to update the di-
alog state by selecting a high-con   dence output
from the nlu (williams et al., 2013). numerous
advanced statistical methods have been proposed
to exploit the correlation between turns to make
the system more robust given the uncertainty of
the automatic id103 (asr) and the
nlu (bohus and rudnicky, 2006; thomson and
young, 2010). the dialog state tracking chal-
lenge (dstc) (williams et al., 2013) formalizes
the problem as a supervised sequential labelling
task where the state tracker estimates the true slot
values based on a sequence of nlu outputs. in
practice the output of the state tracker is used by
a different dialog policy, so that the distribution
in the training data and in the live data are mis-

matched (williams et al., 2013). therefore one
of the basic assumptions of dstc is that the state
tracker   s performance will translate to better dia-
log policy performance. lee (2014) showed posi-
tive results following this assumption by showing
a positive correlation between end-to-end dialog
performance and state tracking performance.

id23 (rl): rl has been
a popular approach for learning the optimal dia-
log policy of a task-oriented dialog system (singh
et al., 2002; williams and young, 2007; georgila
and traum, 2011; lee and eskenazi, 2012). a
dialog policy is formulated as a partially observ-
able markov decision process (pomdp) which
models the uncertainty existing in both the users   
goals and the outputs of the asr and the nlu.
williams (2007) showed that pomdp-based sys-
tems perform signi   cantly better than rule-based
systems especially when the asr word error
rate (wer) is high. other work has explored
methods that
improve the amount of training
data needed for a pomdp-based dialog manager.
ga  si  c (2010) utilized gaussian process rl algo-
rithms and greatly reduced the data needed for
training. existing applications of rl to dialog
management assume a given dialog state represen-
tation. instead, our approach learns its own dia-
log state representation from the raw dialogs along
with a dialog policy in an end-to-end fashion.

end-to-end sdss: there have been many at-
tempts to develop end-to-end chat-oriented dialog
systems that can directly map from the history of a
conversation to the next system response (vinyals
and le, 2015; serban et al., 2015; shang et al.,
2015). these methods train sequence-to-sequence
models (sutskever et al., 2014) on large human-
human conversation corpora. the resulting mod-
els are able to do basic chatting with users. the
work in this paper differs from them by focusing
on building a task-oriented system that can inter-
face with structured databases and provide real in-
formation to users.

(2016)

trainable

end-to-end

recently, wen el al.

introduced a
network-based
tasked-
oriented dialog system. their approach treated
a dialog system as a mapping problem between
the dialog history and the system response. they
learned this mapping via a novel variant of the
encoder-decoder model. the main differences
between our models and theirs are that ours has
the advantage of learning a strategic plan using

rl and jointly optimizing state tracking beyond
standard supervised learning.

3 deep id23
before describing the proposed algorithms, we
brie   y review deep id23 (rl).
rl models are based on the markov decision pro-
cess (mdp). an mdp is a tuple (s, a, p,   , r),
where s is a set of states; a is a set of actions; p
de   nes the transition id203 p (s(cid:48)|s, a); r de-
   nes the expected immediate reward r(s, a); and
       [0, 1) is the discounting factor. the goal of
id23 is to    nd the optimal pol-
icy      , such that the expected cumulative return is
maximized (sutton and barto, 1998). mdps as-
sume full observability of the internal states of the
world, which is rarely true for real-world appli-
cations. the partially observable markov deci-
sion process (pomdp) takes the uncertainty in the
state variable into account. a pomdp is de   ned
by a tuple (s, a, p,   , r, o, z). o is a set of ob-
servations and z de   nes an observation probabil-
ity p (o|s, a). the other variables are the same as
the ones in mdps. solving a pomdp usually re-
quires computing the belief state b(s), which is the
id203 distribution of all possible states, such
s b(s) = 1. it has been shown that the belief
state is suf   cient for optimal control (monahan,
1982), so that the objective is to    nd       : b     a
that maximizes the expected future return.

that(cid:80)

3.1 deep q-network
the deep q-network (id25)
introduced by
mnih (2015) uses a deep neural network (dnn)
to parametrize the q-value function q(s, a;   )
and achieves human-level performance in playing
many atari games. id25 keeps two separate mod-
els: a target network      
i and a behavior network
  i. for every k new samples, id25 uses      
to
compute the target values yid25 and updates the
parameters in   i. only after every c updates, the
new weights of   i are copied over to      
i . further-
more, id25 utilizes experience replay to store all
previous experience tuples (s, a, r, s(cid:48)). before a
new model update, the algorithm samples a mini-
batch of experiences of size m from the memory
and computes the gradient of the following loss
function:

i

l(  i) = e(s,a,r,s(cid:48))[(yid25     q(s, a;   i))2]
yid25 = r +    max

a(cid:48) q(s(cid:48), a(cid:48);      
i )

(1)
(2)

recently, hasselt et al. (2015) leveraged the over-
estimation problem of standard id24 by in-
troducing double id25 and schaul et al. (2015)
improves the convergence speed of id25 via pri-
oritized experience replay. we found both modi   -
cations useful and included them in our studies.

3.2 deep recurrent q-network
an extension to id25 is a deep recurrent q-
network (drqn) which introduces a long short-
term memory (lstm) layer (hochreiter and
schmidhuber, 1997) on top of the convolutional
layer of the original id25 model (hausknecht
and stone, 2015) which allows drqn to solve
pomdps. the recurrent neural network can thus
be viewed as an approximation of the belief state
that can aggregate information from a sequence
of observations. hausknecht (2015) shows that
drqn performs signi   cantly better than id25
when an agent only observes partial states. a
similar model was proposed by narasimhan and
kulkarni (2015) and learns to play multi-user
dungeon (mud) games (curtis, 1992) with game
states hidden in natural language paragraphs.

4 proposed model
4.1 overview
end-to-end learning refers to models that can
back-propagate error signals from the end output
to the raw inputs. prior work in end-to-end state
tracking (henderson et al., 2014) learns a sequen-
tial classi   er that estimates the dialog state based
on asr output without the need of an nlu. in-
stead of treating state tracking as a standard su-
pervised learning task, we propose to unify dialog
state tracking with the dialog policy so that both
are treated as actions available to a reinforcement
learning agent. speci   cally, we learn an optimal
policy that either generates a verbal response or
modi   es the current estimated dialog state based
on the new observations. this formulation makes
it possible to obtain a state tracker even without
the labelled data required for dstc, as long as
the rewards from the users and the databases are
available. furthermore, in cases where dialog state
tracking labels are available, the proposed model
can incorporate them with minimum modi   cation
and greatly accelerate its learning speed. thus, the
following sections describe two models: rl and
hybrid-rl, corresponding to two labelling scenar-
ios: 1) only dialog success labels and 2) dialog

figure 2: an overview of the proposed end-to-end
task-oriented dialog management framework.

success labels with state tracking labels.

4.2 learning from the users and databases

figure 3: the network takes the observation ot at
turn t. the recurrent unit updates its hidden state
based on both the history and the current turn em-
bedding. then the model outputs the q-values for
all actions. the policy network in grey is masked
by the action mask

figure 2 shows an overview of the framework. we
consider a task-oriented dialog task, in which there
are s slots, each with cardinality ci, i     [0, s).
the environment consists of a user, eu and a
database edb. the agent can send verbal actions,
av     av to the user and the user will reply with
natural language responses ou and rewards ru. in
order to interface with the database environment
edb, the agent can apply special actions ah     ah
that can modify a query hypothesis h. the hy-
pothesis is a slot-   lling form that represents the
most likely slot values given the observed evi-
dence. given this hypothesis, h, the database can
perform a normal query and give the results as ob-
servations, odb and rewards rdb.
at each turn t, the agent applies its selected ac-
tion at     {av, ah} and receives the observations
from either the user or the database. we can then
de   ne the observation ot of turn t as,

      

       at

ou
t
odb
t

ot =

(3)

we then use the lstm network as the dialog state
tracker that is capable of aggregating information
over turns and generating a dialog state represen-
tation, bt = lst m (ot, bt   1), where bt is an ap-
proximation of the belief state at turn t. finally,
the dialog state representation from the lstm net-
work is the input to s + 1 policy networks imple-
mented as multilayer id88s (mlp). the    rst
policy network approximates the q-value function
for all verbal actions q(bt, av) while the rest esti-
mate the q-value function for each slot, q(bt, ah),
as shown in figure 3.

incorporating state tracking labels

4.3
the pure rl approach described in the previous
section could suffer from slow convergence when
the cardinality of slots is large. this is due to the
nature of id23: that it has to try
different actions (possible values of a slot) in order
to estimate the expected long-term payoff. on the
other hand, a supervised classi   er can learn much
more ef   ciently. a typical multi-class classi   ca-
tion id168 (e.g. categorical cross id178)
assumes that there is a single correct label such
that it encourages the id203 of the correct la-
bel and suppresses the probabilities of the all the
wrong ones. modeling dialog state tracking as a
q-value function has advantages over a local clas-
si   er. for instance, take the situation where a user
wants to send an email and the state tracker needs
to estimate the user   s goal from among three pos-
sible values: send, edit and delete. in a classi   -
cation task, all the incorrect labels (edit, delete)
are treated as equally undesirable. however, the
cost of mistakenly recognizing the user goal as
delete is much larger than edit, which can only
be learned from the future rewards.
in order to
train the slot-   lling policy with both short-term
and long-term supervision signals, we decompose
the reward function for ah into two parts:

q  (b, ah) =   r(b, a) +   

p (b(cid:48)|b, ah)v   (b(cid:48))

(cid:88)

b(cid:48)

(4)
  r(b, a, b(cid:48)) = r(b, ah) + p (ah|b)
(5)
where p (ah|b) is the id155 that
the correct label of the slots is ah given the cur-

rent belief state.
in practice, instead of training
a separate model estimating p (ah|b), we can re-
place p (ah|b) by 1(y = ah) as the sample re-
ward r, where y is the label. furthermore, a key
observation is that although it is expensive to col-
lect data from the user eu, one can easily sample
trajectories of interaction with the database since
p (b(cid:48)|b, ah) is known. therefore, we can acceler-
ate learning by generating synthetic experiences,
tuple (b, ah, r, b(cid:48))   ah     ah and add them
i.e.
to the experience replay buffer. this approach is
closely related to the dyna id24 proposed
in (sutton, 1990). the difference is that dyna q-
learning uses the estimated environment dynamics
to generating experiences, while our method only
uses the known transition function (i.e.
the dy-
namics of the database) to generate synthetic sam-
ples.

implementation details

4.4
we can optimize the network architecture in sev-
eral ways to improve its ef   ciency:

shared state tracking policies: it is more ef-
   cient to tie the weights of the policy networks
for similar slots and use the index of slot as an in-
put. this can reduce the number of parameters that
needs to be learned and encourage shared struc-
tures. the studies in section 5 illustrate one ex-
ample.

constrained action mask: we can constrain
the available actions at each turn to force the
agent to alternate between verbal response and
slot-   lling. we de   ne amask as a function that
takes state s and outputs a set of available actions
for:

amask(s) = ah
= av

new inputs from the user
otherwise

(6)
(7)

reward shaping based on the database: the
reward signals from the users are usually sparse
(at the end of a dialog), the database, however,
can provide frequent rewards to the agent. reward
shaping is a technique used to speed up learning.
ng et al. (1999) showed that potential-based re-
ward shaping does not alter the optimal solution;
it only impacts the learning speed. the pseudo re-
ward function f (s, a, s(cid:48)) is de   ned as:

  r(s, a, s(cid:48)) = r(s, a, s(cid:48)) + f (s, a, s(cid:48))
f (s, a, s(cid:48)) =     (s(cid:48))       (s)

(8)
(9)

let the total number of entities in the database
be d and pmax be the max potential, the potential
  (s) is:

  (st) = pmax(1     dt
d
if dt = 0
  (st) = 0

)

if dt > 0

(10)

(11)

the intuition of this potential function is to
encourage the agent to narrow down the possi-
ble range of valid entities as quickly as possible.
meanwhile, if no entities are consistent with the
current hypothesis, this implies that there are mis-
takes in previous slot    lling, which gives a poten-
tial of 0.

5 experiments

5.1

20q game as task-oriented dialog

in order to test the proposed framework, we chose
the 20 question game (20q). the game rules are
as follows: at the beginning of each game, the
user thinks of a famous person. then the agent
asks the user a series of yes/no questions. the
user honestly answers, using one of three answers:
yes, no or i don   t know.
in order to have this
resemble a dialog, our user can answer with any
natural utterance representing one of the three in-
tents. the agent can make guesses at any turn, but
a wrong guess results in a negative reward. the
goal is to guess the correct person within a max-
imum number of turns with the least number of
wrong guesses. an example game conversation is
as follows:

sys: is this person male?
user: yes i think so.
sys: is this person an artist?
user: he is not an artist.
...
sys: i guess this person is bill gates.
user: correct.
we can formulate the game as a slot-   lling di-
alog. assume the system has |q| available ques-
tions to select from at each turn. the answer to
each question becomes a slot and each slot has
three possible values: yes/no/unknown. due to the
length limit and wrong guess penalty, the optimal
policy does not allow the agent to ask all of the
questions regardless of the context or guess every
person in the database one by one.

5.2 simulator construction
we constructed a simulator for 20q. the simulator
has two parts: a database of 100 famous people
and a user simulator.

we selected 100 people from freebase (bol-
lacker et al., 2008), each of them has 6 attributes:
birthplace, degree, gender, profession and birth-
day. we manually designed several yes/no ques-
tions for each attribute that is available to the
agent. each question covers a different set of pos-
sible values for a given attribute and thus carries
a different discriminative power to pinpoint the
person that the user is thinking of. as a result,
the agent needs to judiciously select the question,
given the context of the game, in order to narrow
down the range of valid people. there are 31 ques-
tions. table 1 shows a summary.

qa example question
attribute
3 was he/she born before 1950?
birthday
9 was he/she born in usa?
birthplace
4 does he/she have a phd?
degree
2
gender
profession
8
nationality 5

is this person male?
is he/she an artist?
is he/she a citizen of an asian
country?

table 1: summary of the available questions. qa
is the number of questions for attribute a.

at the beginning of each game, the simula-
tor will    rst uniformly sample a person from the
database as the person it is thinking of. also there
is a 5% chance that the simulator will consider
unknown as an attribute and thus it will answer
with unknown intent for any question related to
it. after the game begins, when the agent asks
a question, the simulator    rst determines the an-
swer (yes, no or unknown) and replies using natu-
ral language. in order to generate realistic natural
language with the yes/no/unknown intent, we col-
lected utterances from the switchboard dialog act
(swda) corpus (jurafsky et al., 1997). table 2
presents the mapping from the swda dialog acts
to yes/no/unknown. we further post-processed re-
sults and removed irrelevant utterances, which led
to 508, 445 and 251 unique utterances with intent
respectively yes/no/unknown. we keep the fre-
quency counts for each unique expression. thus
at run time, the simulator can sample a response
according to the original distribution in the swda

corpus.

intent
yes

no

swda tags
agree, yes answers, af   rma-
tive non-yes answers
no answers, reject, negative
non-no answers

unknown maybe, other answer

table 2: dialog act mapping from swda to
yes/no/unknown

a game is terminated when one of the four con-
ditions is ful   lled: 1) the agent guesses the cor-
rect answer, 2) there are no people in the database
consistent with the current hypothesis, 3) the max
game length (100 steps) is reached and 4) the max
number of guesses is reached (10 guesses). only
if the agent guesses the correct answer (condition
1) treated as a game victory. the win and loss re-
wards are 30 and    30 and a wrong guess leads to
a    5 penalty.

5.3 training details
the user environment eu is the simulator that only
accepts verbal actions, either a yes/no question or
a guess, and replies with a natural language utter-
ance. therefore av contains |q| + 1 actions, in
which the    rst |q| actions are questions and the
last action makes a guess, given the results from
the database.

the database environment reads in a query hy-
pothesis h and returns a list of people that satisfy
the constraints in the query. h has a size of |q|
and each dimension can be one of the three values:
yes/no/unknown. since the cardinality for all slots
is the same, we only need 1 slot-   lling policy net-
work with 3 q-value outputs for yes/no/unknown,
to modify the value of the latest asked question,
which is the shared policy approach mentioned in
section 4. thus ah = {yes, no, unknown}. for
example, considering q = 3 and the hypothesis h
is: [unknown, unknown, unknown]. if the lat-
est asked question is q1 (1-based), then applying
action ah = yes will result in the new hypothesis:
[yes, unknown, unknown].

to represent

the observation ot

in vectorial
form, we use a bag-of-bigrams feature vector to
represent a user utterance; a one-hot vector to rep-
resent a system action and a single discrete num-
ber to represent the number of people satisfying
the current hypothesis.

the hyper-parameters of the neural network
model are as follows: the size of turn embedding is
30; the size of lstms is 256; each policy network
has a hidden layer of 128 with tanh activation.
we also add a dropout rate of 0.3 for both lstms
and tanh layer outputs. the network has a total
of 470,005 parameters. the network was trained
through rm sp rop (tieleman and hinton, 2012).
for hyper-parameters of drqn, the behavior net-
work was updated every 4 steps and the interval
between each target network update c is 1000.  -
greedy exploration is used for training, where   is
linearly decreased from 1 to 0.1. the reward shap-
ing constant pmax is 2 and the discounting factor
   is 0.99. the resulting network was evaluated
every 5000 steps and the model was trained up to
120,000 steps. each evaluation records the agent   s
performance with a greedy policy for 200 indepen-
dent episodes.

5.4 dialog policy analysis
we compare the performance of three models: a
strong modular baseline, rl and hybrid-rl. the
baseline has an independently trained state tracker
and dialog policy. the state tracker is also an
lstm-based classi   er that inputs a dialog history
and predicts the slot-value of the latest question.
the dialog policy is a drqn that assumes per-
fect slot-   lling during training and simply con-
trols the next verbal action. thus the essential
difference between the baseline and the proposed
models is that the state tracker and dialog policy
are not trained jointly. also, since hybrid-rl ef-
fectively changes the reward function, the typical
average cumulative reward metric is not applica-
ble for performance comparison. therefore, we
directly compare the win rate and average game
length in later discussions.

baseline
rl
hybrid-rl

win rate (%) avg turn
68.5
85.6
90.5

12.2
21.6
19.22

table 3: performance of the three systems

table 3 shows that both proposed models
achieve signi   cantly higher win rate than the base-
line by asking more questions before making
guesses. figure 4 illustrates the learning process
of the three models. the horizontal axis is the total
number of interaction between the agent and either

the user or the database. the baseline model has
the fastest learning speed but its performance sat-
urated quickly because the dialog policy was not
trained together with the state tracker. so the dia-
log policy is not aware of the uncertainty in slot-
   lling and the slot-   ller does not distinguish be-
tween the consequences of different wrong labels
(e.g classify yes to no versus to unknown). on the
other hand, although rl reaches high performance
at the end of the training, it struggles in the early
stages and suffers from slow convergence. this
is due to that fact that correct slot-   lling is a pre-
requisite for winning 20q, while the reward signal
has a long delayed horizon in the rl approach. fi-
nally, the hybrid-rl approach is able to converge
to the optimal solution much faster than rl due to
the fact that it ef   ciently exploits the information
in the state tracking label.

figure 4: graphs showing the evolution of the win
rate during training.

5.5 state tracking analysis
one of the hypotheses is that the rl approach can
learn a good state tracker using only dialog success
reward signals. we ran the best trained models
using a greedy policy and collected 10,000 sam-
ples. table 4 reports the precision and recall of
slot    lling in these trajectories. the results indi-

baseline
rl
hybrid-rl

unknown yes
0.99/0.60
0.21/0.77
0.54/0.60

0.96/0.97
1.00/0.93
0.98/0.92

no
0.94/0.95
0.95/0.51
0.94/0.93

table 4: state tracking performance of the three
systems. the results are in the format of preci-
sion/recall

cate that the rl model learns a completely dif-

ferent strategy compared to the baseline. the rl
model aims for high precision so that it predicts
unknown when the input is ambiguous, which is a
safer option than predicting yes/no, because con-
fusing between yes and no may potentially lead to
a contradiction and a game failure. this is very
different from the baseline which does not dis-
tinguish between incorrect labels. therefore, al-
though the baseline achieves better classi   cation
metrics, it does not take into account the long-
term payoff and performs sub-optimally in terms
of overall performance.

5.6 dialog state representation analysis
tracking the state over multiple turns is crucial be-
cause the agent   s optimal action depends on the
history, e.g.
the question it has already asked,
the number of guesses it has spent. furthermore,
one of the assumptions is that the output of the
lstm network is an approximation of the belief
state in the pomdp. we conducted two studies to
test these hypotheses. for both studies, we ran the
hybrid-rl models saved at 20k, 50k and 100k
steps against the simulator with a greedy policy
and recorded 10,000 samples for each model.

the    rst study checks whether we can recon-
the number of
struct an important state feature:
guesses the agent has made from the dialog state
embedding. we divide the collected 10,000 sam-
ples into 80% for training and 20% for testing. we
used the lstm output as input features to a lin-
ear regression model with l2 id173. ta-
ble 5 shows the correlation of determination r2 in-
creases for the model that was trained with more
data.

model
r2

20k
0.05

50k
0.51

100k
0.77

table 5: r2 of the id75 for predicting
the number of guesses in the test dataset.

the second study is a retrieval task. the la-
tent state of the 20q game is the true intent of the
users    answers to all the questions that have been
asked so far. therefore, the true state vector, s has
a size of 31 and each slot, s[k], k     [0, 31) is one
of the four values: not yet asked, yes, no, unknown.
therefore, if the lstm output b is in fact implic-
itly learning the distribution over this latent state s,
they must be highly correlated for a well-trained
model. therefore, for each bi, i     [0, 10, 000),

we    nd its nearest 5 neighbors based on cosine
distance measuring and record their latent states,
n (bi) : b     [s]. then we compute the empirical
id203 that each slot of the true state s differs
from the retrieved neighbors:

pdiff(s[k]) = esi

1(n (bi)[n][k] (cid:54)= si[k])

n=0

5

(12)
where 1 is the indicator function, k is the slot in-
dex and n is the neighbor index.

(cid:20)(cid:80)4

(cid:21)

figure 5: performance of retrieving similar true
dialog states using learned dialog state embedding.

figure 5 shows that the retrieval error contin-
ues to decrease for the model that was trained bet-
ter, which con   rms our assumption that the lstm
output is an approximation of the belief state.

6 conclusion

this paper identi   es the limitations of the conven-
tional sds pipeline and describes a novel end-to-
end framework for a task-oriented dialog system
using deep id23. we have as-
sessed the model on the 20q game. the proposed
models show superior performance for both nat-
ural language understanding and dialog strategy.
furthermore, our analysis con   rms our hypothe-
ses that the proposed models implicitly capture es-
sential information in the latent dialog states.

one limitation of the proposed approach is poor
scalability due to the large number of samples
needed for convergence. so future studies will in-
clude developing full-   edged task-orientated dia-
log systems and exploring methods to improve the
sample ef   ciency. also, investigating techniques
that allow easy integration of domain knowledge
so that the system can be more easily debugged
and corrected is another important direction.

7 acknowledgements

this work was funded by nsf grant cns-
1512973. the opinions expressed in this paper
do not necessarily re   ect those of nsf. we would
also like to thank alan w black for discussions on
this paper.

references
dan bohus and alexander i rudnicky. 2003. raven-
claw: dialog management using hierarchical task
decomposition and an expectation agenda.

dan bohus and alex rudnicky. 2006. a k hypothe-
in proc. of the
ses+ otherbelief updating model.
aaai workshop on statistical and empirical meth-
ods in spoken dialogue systems.

kurt bollacker, colin evans, praveen paritosh, tim
sturge, and jamie taylor. 2008. freebase: a col-
laboratively created graph database for structuring
human knowledge. in proceedings of the 2008 acm
sigmod international conference on management
of data, pages 1247   1250. acm.

pavel curtis. 1992. mudding: social phenomena in
text-based virtual realities. high noon on the elec-
tronic frontier: conceptual issues in cyberspace,
pages 347   374.

m ga  si  c, f jur  c      cek, simon keizer, franc  ois mairesse,
blaise thomson, kai yu, and steve young. 2010.
gaussian processes for fast policy optimisation of
in proceedings
pomdp-based dialogue managers.
of the 11th annual meeting of the special interest
group on discourse and dialogue, pages 201   204.
association for computational linguistics.

kallirroi georgila and david r traum. 2011. rein-
forcement learning of argumentation dialogue poli-
cies in negotiation. in interspeech, pages 2073   
2076.

matthew hausknecht and peter stone. 2015. deep
recurrent id24 for partially observable mdps.
arxiv preprint arxiv:1507.06527.

matthew henderson, blaise thomson, and steve
young. 2014. word-based dialog state tracking with
in proceedings of the
recurrent neural networks.
15th annual meeting of the special interest group
on discourse and dialogue (sigdial), pages 292   
299.

sepp hochreiter and j  urgen schmidhuber.

1997.
neural computation,

long short-term memory.
9(8):1735   1780.

dan jurafsky, elizabeth shriberg, and debra biasca.
1997. switchboard swbd-damsl shallow-discourse-
function annotation coders manual. institute of cog-
nitive science technical report, pages 97   102.

sungjin lee and maxine eskenazi. 2012. pomdp-
based let   s go system for spoken dialog challenge.
in spoken language technology workshop (slt),
2012 ieee, pages 61   66. ieee.

sungjin lee. 2014. extrinsic evaluation of dialog state
tracking and predictive metrics for dialog policy op-
in 15th annual meeting of the special
timization.
interest group on discourse and dialogue, page
310.

lihong li, he he, and jason d williams.

2014.
temporal supervised learning for inferring a dia-
log policy from example conversations. in spoken
language technology workshop (slt), 2014 ieee,
pages 312   317. ieee.

volodymyr mnih, koray kavukcuoglu, david silver,
andrei a rusu, joel veness, marc g bellemare,
alex graves, martin riedmiller, andreas k fidje-
land, georg ostrovski, et al. 2015. human-level
control through deep id23. na-
ture, 518(7540):529   533.

george e monahan. 1982. state of the arta survey of
partially observable id100: the-
ory, models, and algorithms. management science,
28(1):1   16.

karthik narasimhan, tejas kulkarni, and regina
barzilay. 2015. language understanding for text-
based games using deep reinforcement
learning.
arxiv preprint arxiv:1506.08941.

andrew y ng, daishi harada, and stuart russell.
1999. policy invariance under reward transforma-
tions: theory and application to reward shaping. in
icml, volume 99, pages 278   287.

antoine raux, brian langner, dan bohus, alan w
black, and maxine eskenazi. 2005. lets go pub-
lic! taking a spoken dialog system to the real world.
in in proc. of interspeech 2005. citeseer.

tom schaul, john quan, ioannis antonoglou, and
david silver. 2015. prioritized experience replay.
arxiv preprint arxiv:1511.05952.

iulian v serban, alessandro sordoni, yoshua bengio,
aaron courville, and joelle pineau. 2015. build-
ing end-to-end dialogue systems using generative hi-
erarchical neural network models. arxiv preprint
arxiv:1507.04808.

lifeng shang, zhengdong lu, and hang li. 2015.
neural responding machine for short-text conversa-
tion. arxiv preprint arxiv:1503.02364.

satinder singh, diane litman, michael kearns, and
marilyn walker. 2002. optimizing dialogue man-
agement with id23: experiments
with the njfun system. journal of arti   cial intelli-
gence research, pages 105   133.

ilya sutskever, oriol vinyals, and quoc v le. 2014.
sequence to sequence learning with neural net-
works. in advances in neural information process-
ing systems, pages 3104   3112.

richard s sutton and andrew g barto. 1998. intro-

duction to id23. mit press.

1990.

richard s sutton.

integrated architectures
for learning, planning, and reacting based on ap-
proximating id145. in proceedings
of the seventh international conference on machine
learning, pages 216   224.

blaise thomson and steve young. 2010. bayesian
update of dialogue state: a pomdp framework for
spoken dialogue systems. computer speech & lan-
guage, 24(4):562   588.

tijmen tieleman and geoffrey hinton. 2012. lecture
6.5-rmsprop: divide the gradient by a running av-
erage of its recent magnitude. coursera: neural
networks for machine learning, 4:2.

hado van hasselt, arthur guez, and david silver.
2015. deep id23 with double q-
learning. arxiv preprint arxiv:1509.06461.

oriol vinyals and quoc le. 2015. a neural conversa-

tional model. arxiv preprint arxiv:1506.05869.

tsung-hsien wen, milica gasic, nikola mrksic,
lina m rojas-barahona, pei-hao su, stefan ultes,
david vandyke, and steve young.
2016. a
network-based end-to-end trainable task-oriented di-
alogue system. arxiv preprint arxiv:1604.04562.

jason d williams and steve young. 2007. partially
observable id100 for spoken
id71. computer speech & language,
21(2):393   422.

jason williams, antoine raux, deepak ramachan-
dran, and alan black. 2013. the dialog state track-
ing challenge. in proceedings of the sigdial 2013
conference, pages 404   413.

steve j young. 2006. using pomdps for dialog man-

agement. in slt, pages 8   13.

