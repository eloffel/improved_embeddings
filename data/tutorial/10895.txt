7
1
0
2

 

b
e
f
5
1

 

 
 
]

g
l
.
s
c
[
 
 

2
v
8
7
5
1
0

.

1
1
6
1
:
v
i
x
r
a

under review as a conference paper at iclr 2017

neural architecture search with
id23

barret zoph   , quoc v. le
google brain
{barretzoph,qvl}@google.com

abstract

neural networks are powerful and    exible models that work well for many dif   -
cult learning tasks in image, speech and natural language understanding. despite
their success, neural networks are still hard to design. in this paper, we use a re-
current network to generate the model descriptions of neural networks and train
this id56 with id23 to maximize the expected accuracy of the
generated architectures on a validation set. on the cifar-10 dataset, our method,
starting from scratch, can design a novel network architecture that rivals the best
human-invented architecture in terms of test set accuracy. our cifar-10 model
achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than
the previous state-of-the-art model that used a similar architectural scheme. on
the id32 dataset, our model can compose a novel recurrent cell that out-
performs the widely-used lstm cell, and other state-of-the-art baselines. our cell
achieves a test set perplexity of 62.4 on the id32, which is 3.6 perplex-
ity better than the previous state-of-the-art model. the cell can also be transferred
to the character id38 task on ptb and achieves a state-of-the-art
perplexity of 1.214.

1

introduction

the last few years have seen much success of deep neural networks in many challenging appli-
cations, such as id103 (hinton et al., 2012), image recognition (lecun et al., 1998;
krizhevsky et al., 2012) and machine translation (sutskever et al., 2014; bahdanau et al., 2015; wu
et al., 2016). along with this success is a paradigm shift from feature designing to architecture
designing, i.e., from sift (lowe, 1999), and hog (dalal & triggs, 2005), to alexnet (krizhevsky
et al., 2012), vggnet (simonyan & zisserman, 2014), googlenet (szegedy et al., 2015), and
resnet (he et al., 2016a). although it has become easier, designing architectures still requires a
lot of expert knowledge and takes ample time.

figure 1: an overview of neural architecture search.

this paper presents neural architecture search, a gradient-based method for    nding good architec-
tures (see figure 1) . our work is based on the observation that the structure and connectivity of a

   work done as a member of the google brain residency program (g.co/brainresidency.)

1

under review as a conference paper at iclr 2017

neural network can be typically speci   ed by a variable-length string. it is therefore possible to use
a recurrent network     the controller     to generate such string. training the network speci   ed by the
string     the    child network        on the real data will result in an accuracy on a validation set. using
this accuracy as the reward signal, we can compute the policy gradient to update the controller. as a
result, in the next iteration, the controller will give higher probabilities to architectures that receive
high accuracies. in other words, the controller will learn to improve its search over time.
our experiments show that neural architecture search can design good models from scratch, an
achievement considered not possible with other methods. on image recognition with cifar-10,
neural architecture search can    nd a novel convnet model that is better than most human-invented
architectures. our cifar-10 model achieves a 3.65 test set error, while being 1.05x faster than the
current best model. on id38 with id32, neural architecture search can
design a novel recurrent cell that is also better than previous id56 and lstm architectures. the cell
that our model found achieves a test set perplexity of 62.4 on the id32 dataset, which is
3.6 perplexity better than the previous state-of-the-art.

2 related work

hyperparameter optimization is an important research topic in machine learning, and is widely used
in practice (bergstra et al., 2011; bergstra & bengio, 2012; snoek et al., 2012; 2015; saxena &
verbeek, 2016). despite their success, these methods are still limited in that they only search models
from a    xed-length space. in other words, it is dif   cult to ask them to generate a variable-length
con   guration that speci   es the structure and connectivity of a network. in practice, these methods
often work better if they are supplied with a good initial model (bergstra & bengio, 2012; snoek
et al., 2012; 2015). there are bayesian optimization methods that allow to search non    xed length
architectures (bergstra et al., 2013; mendoza et al., 2016), but they are less general and less    exible
than the method proposed in this paper.
modern neuro-evolution algorithms, e.g., wierstra et al. (2005); floreano et al. (2008); stanley et al.
(2009), on the other hand, are much more    exible for composing novel models, yet they are usually
less practical at a large scale. their limitations lie in the fact that they are search-based methods,
thus they are slow or require many heuristics to work well.
neural architecture search has some parallels to program synthesis and inductive programming, the
idea of searching a program from examples (summers, 1977; biermann, 1978). in machine learning,
probabilistic program induction has been used successfully in many settings, such as learning to
solve simple q&a (liang et al., 2010; neelakantan et al., 2015; andreas et al., 2016), sort a list of
numbers (reed & de freitas, 2015), and learning with very few examples (lake et al., 2015).
the controller in neural architecture search is auto-regressive, which means it predicts hyperpa-
rameters one a time, conditioned on previous predictions. this idea is borrowed from the decoder
in end-to-end sequence to sequence learning (sutskever et al., 2014). unlike sequence to sequence
learning, our method optimizes a non-differentiable metric, which is the accuracy of the child net-
work. it is therefore similar to the work on id7 optimization in id4 (ran-
zato et al., 2015; shen et al., 2016). unlike these approaches, our method learns directly from the
reward signal without any supervised id64.
also related to our work is the idea of learning to learn or meta-learning (thrun & pratt, 2012), a
general framework of using information learned in one task to improve a future task. more closely
related is the idea of using a neural network to learn the id119 updates for another net-
work (andrychowicz et al., 2016) and the idea of using id23 to    nd update policies
for another network (li & malik, 2016).

3 methods

in the following section, we will    rst describe a simple method of using a recurrent network to
generate convolutional architectures. we will show how the recurrent network can be trained with
a policy gradient method to maximize the expected accuracy of the sampled architectures. we will
present several improvements of our core approach such as forming skip connections to increase
model complexity and using a parameter server approach to speed up training. in the last part of

2

under review as a conference paper at iclr 2017

the section, we will focus on generating recurrent architectures, which is another key contribution
of our paper.

3.1 generate model descriptions with a controller recurrent neural

network

in neural architecture search, we use a controller to generate architectural hyperparameters of
neural networks. to be    exible, the controller is implemented as a recurrent neural network. let   s
suppose we would like to predict feedforward neural networks with only convolutional layers, we
can use the controller to generate their hyperparameters as a sequence of tokens:

figure 2: how our controller recurrent neural network samples a simple convolutional network. it
predicts    lter height,    lter width, stride height, stride width, and number of    lters for one layer and
repeats. every prediction is carried out by a softmax classi   er and then fed into the next time step
as input.

in our experiments, the process of generating an architecture stops if the number of layers exceeds
a certain value. this value follows a schedule where we increase it as training progresses. once the
controller id56    nishes generating an architecture, a neural network with this architecture is built
and trained. at convergence, the accuracy of the network on a held-out validation set is recorded.
the parameters of the controller id56,   c, are then optimized in order to maximize the expected
validation accuracy of the proposed architectures. in the next section, we will describe a policy
gradient method which we use to update parameters   c so that the controller id56 generates better
architectures over time.

3.2 training with reinforce

the list of tokens that the controller predicts can be viewed as a list of actions a1:t to design an
architecture for a child network. at convergence, this child network will achieve an accuracy r on
a held-out dataset. we can use this accuracy r as the reward signal and use id23
to train the controller. more concretely, to    nd the optimal architecture, we ask our controller to
maximize its expected reward, represented by j(  c):

j(  c) = ep (a1:t ;  c)[r]

since the reward signal r is non-differentiable, we need to use a policy gradient method to iteratively
update   c. in this work, we use the reinforce rule from williams (1992):

(cid:53)  cj(  c) =

ep (a1:t ;  c)

(cid:2) (cid:53)  c log p (at|a(t   1):1;   c)r(cid:3)

an empirical approximation of the above quantity is:

(cid:53)  c log p (at|a(t   1):1;   c)rk

t(cid:88)
t(cid:88)
m(cid:88)

t=1

k=1

t=1

1
m

where m is the number of different architectures that the controller samples in one batch and t is
the number of hyperparameters our controller has to predict to design a neural network architecture.

3

under review as a conference paper at iclr 2017

the validation accuracy that the k-th neural network architecture achieves after being trained on a
training dataset is rk.
the above update is an unbiased estimate for our gradient, but has a very high variance. in order to
reduce the variance of this estimate we employ a baseline function:

m(cid:88)

t(cid:88)

k=1

t=1

1
m

(cid:53)  c log p (at|a(t   1):1;   c)(rk     b)

as long as the baseline function b does not depend on the on the current action, then this is still an
unbiased gradient estimate. in this work, our baseline b is an exponential moving average of the
previous architecture accuracies.

accelerate training with parallelism and asynchronous updates:
in neural architecture
search, each gradient update to the controller parameters   c corresponds to training one child net-
work to convergence. as training a child network can take hours, we use distributed training and
asynchronous parameter updates in order to speed up the learning process of the controller (dean
et al., 2012). we use a parameter-server scheme where we have a parameter server of s shards, that
store the shared parameters for k controller replicas. each controller replica samples m different
child architectures that are trained in parallel. the controller then collects gradients according to the
results of that minibatch of m architectures at convergence and sends them to the parameter server
in order to update the weights across all controller replicas. in our implementation, convergence of
each child network is reached when its training exceeds a certain number of epochs. this scheme of
parallelism is summarized in figure 3.

figure 3: distributed training for neural architecture search. we use a set of s parameter servers
to store and send parameters to k controller replicas. each controller replica then samples m archi-
tectures and run the multiple child models in parallel. the accuracy of each child model is recorded
to compute the gradients with respect to   c, which are then sent back to the parameter servers.

3.3

increase architecture complexity with skip connections and other
layer types

in section 3.1, the search space does not have skip connections, or branching layers used in modern
architectures such as googlenet (szegedy et al., 2015), and residual net (he et al., 2016a). in this
section we introduce a method that allows our controller to propose skip connections or branching
layers, thereby widening the search space.
to enable the controller to predict such connections, we use a set-selection type attention (neelakan-
tan et al., 2015) which was built upon the attention mechanism (bahdanau et al., 2015; vinyals et al.,
2015). at layer n, we add an anchor point which has n     1 content-based sigmoids to indicate the
previous layers that need to be connected. each sigmoid is a function of the current hiddenstate of
the controller and the previous hiddenstates of the previous n     1 anchor points:

p(layer j is an input to layer i) = sigmoid(vttanh(wprev     hj + wcurr     hi)),

where hj represents the hiddenstate of the controller at anchor point for the j-th layer, where j
ranges from 0 to n     1. we then sample from these sigmoids to decide what previous layers to be
used as inputs to the current layer. the matrices wprev, wcurr and v are trainable parameters. as

4

under review as a conference paper at iclr 2017

these connections are also de   ned by id203 distributions, the reinforce method still applies
without any signi   cant modi   cations. figure 4 shows how the controller uses skip connections to
decide what layers it wants as inputs to the current layer.

figure 4: the controller uses anchor points, and set-selection attention to form skip connections.

in our framework, if one layer has many input layers then all input layers are concatenated in the
depth dimension. skip connections can cause    compilation failures    where one layer is not compat-
ible with another layer, or one layer may not have any input or output. to circumvent these issues,
we employ three simple techniques. first, if a layer is not connected to any input layer then the
image is used as the input layer. second, at the    nal layer we take all layer outputs that have not
been connected and concatenate them before sending this    nal hiddenstate to the classi   er. lastly,
if input layers to be concatenated have different sizes, we pad the small layers with zeros so that the
concatenated layers have the same sizes.
finally, in section 3.1, we do not predict the learning rate and we also assume that the architectures
consist of only convolutional layers, which is also quite restrictive. it is possible to add the learning
rate as one of the predictions. additionally, it is also possible to predict pooling, local contrast
id172 (jarrett et al., 2009; krizhevsky et al., 2012), and batchnorm (ioffe & szegedy, 2015)
in the architectures. to be able to add more types of layers, we need to add an additional step in the
controller id56 to predict the layer type, then other hyperparameters associated with it.

3.4 generate recurrent cell architectures

in this section, we will modify the above method to generate recurrent cells. at every time step t,
the controller needs to    nd a functional form for ht that takes xt and ht   1 as inputs. the simplest
way is to have ht = tanh(w1   xt +w2   ht   1), which is the formulation of a basic recurrent cell. a
more complicated formulation is the widely-used lstm recurrent cell (hochreiter & schmidhuber,
1997).
the computations for basic id56 and lstm cells can be generalized as a tree of steps that take xt
and ht   1 as inputs and produce ht as    nal output. the controller id56 needs to label each node in
the tree with a combination method (addition, elementwise multiplication, etc.) and an activation
function (tanh, sigmoid, etc.) to merge two inputs and produce one output. two outputs are then
fed as inputs to the next node in the tree. to allow the controller id56 to select these methods and
functions, we index the nodes in the tree in an order so that the controller id56 can visit each node
one by one and label the needed hyperparameters.
inspired by the construction of the lstm cell (hochreiter & schmidhuber, 1997), we also need cell
variables ct   1 and ct to represent the memory states. to incorporate these variables, we need the
controller id56 to predict what nodes in the tree to connect these two variables to. these predictions
can be done in the last two blocks of the controller id56.
to make this process more clear, we show an example in figure 5, for a tree structure that has two
leaf nodes and one internal node. the leaf nodes are indexed by 0 and 1, and the internal node is
indexed by 2. the controller id56 needs to    rst predict 3 blocks, each block specifying a combina-
tion method and an activation function for each tree index. after that it needs to predict the last 2
blocks that specify how to connect ct and ct   1 to temporary variables inside the tree. speci   cally,

5

under review as a conference paper at iclr 2017

figure 5: an example of a recurrent cell constructed from a tree that has two leaf nodes (base 2)
and one internal node. left: the tree that de   nes the computation steps to be predicted by controller.
center: an example set of predictions made by the controller for each computation step in the tree.
right:
the computation graph of the recurrent cell constructed from example predictions of the
controller.

according to the predictions of the controller id56 in this example, the following computation steps
will occur:

a0 = tanh(w1     xt + w2     ht   1).

elements in    cell inject   , which means we need to compute anew
notice that we don   t have any learnable parameters for the internal nodes of the tree.

compute a1 = relu(cid:0)(w3     xt) (cid:12) (w4     ht   1)(cid:1).

    the controller predicts add and t anh for tree index 0, this means we need to compute
    the controller predicts elemm ult and relu for tree index 1, this means we need to
    the controller predicts 0 for the second element of the    cell index   , add and relu for
0 = relu(a0 + ct   1).
    the controller predicts elemm ult and sigmoid for tree index 2, this means we need to
0 (cid:12) a1). since the maximum index in the tree is 2, ht is set to
compute a2 = sigmoid(anew
a2.
    the controller id56 predicts 1 for the    rst element of the    cell index   , this means that we
should set ct to the output of the tree at index 1 before the activation, i.e., ct = (w3     xt)(cid:12)
(w4     ht   1).

in the above example, the tree has two leaf nodes, thus it is called a    base 2    architecture. in our
experiments, we use a base number of 8 to make sure that the cell is expressive.

4 experiments and results

we apply our method to an image classi   cation task with cifar-10 and a id38 task
with id32, two of the most benchmarked datasets in deep learning. on cifar-10, our
goal is to    nd a good convolutional architecture whereas on id32 our goal is to    nd a good
recurrent cell. on each dataset, we have a separate held-out validation dataset to compute the reward
signal. the reported performance on the test set is computed only once for the network that achieves
the best result on the held-out validation dataset. more details about our experimental procedures
and results are as follows.

4.1 learning convolutional architectures for cifar-10

dataset:
in these experiments we use the cifar-10 dataset with id174 and aug-
mentation procedures that are in line with other previous results. we    rst preprocess the data by
whitening all the images. additionally, we upsample each image then choose a random 32x32 crop
of this upsampled image. finally, we use random horizontal    ips on this 32x32 cropped image.

search space: our search space consists of convolutional architectures, with recti   ed linear units
as non-linearities (nair & hinton, 2010), batch id172 (ioffe & szegedy, 2015) and skip
connections between layers (section 3.3). for every convolutional layer, the controller id56 has to
select a    lter height in [1, 3, 5, 7], a    lter width in [1, 3, 5, 7], and a number of    lters in [24, 36, 48,

6

under review as a conference paper at iclr 2017

64]. for strides, we perform two sets of experiments, one where we    x the strides to be 1, and one
where we allow the controller to predict the strides in [1, 2, 3].

training details: the controller id56 is a two-layer lstm with 35 hidden units on each layer.
it is trained with the adam optimizer (kingma & ba, 2015) with a learning rate of 0.0006. the
weights of the controller are initialized uniformly between -0.08 and 0.08. for the distributed train-
ing, we set the number of parameter server shards s to 20, the number of controller replicas k to
100 and the number of child replicas m to 8, which means there are 800 networks being trained on
800 gpus concurrently at any time.
once the controller id56 samples an architecture, a child model is constructed and trained for 50
epochs. the reward used for updating the controller is the maximum validation accuracy of the last
5 epochs cubed. the validation set has 5,000 examples randomly sampled from the training set,
the remaining 45,000 examples are used for training. the settings for training the cifar-10 child
models are the same with those used in huang et al. (2016a). we use the momentum optimizer
with a learning rate of 0.1, weight decay of 1e-4, momentum of 0.9 and used nesterov momentum
(sutskever et al., 2013).
during the training of the controller, we use a schedule of increasing number of layers in the child
networks as training progresses. on cifar-10, we ask the controller to increase the depth by 2 for
the child models every 1,600 samples, starting at 6 layers.

results: after the controller trains 12,800 architectures, we    nd the architecture that achieves the
best validation accuracy. we then run a small grid search over learning rate, weight decay, batchnorm
epsilon and what epoch to decay the learning rate. the best model from this grid search is then run
until convergence and we then compute the test accuracy of such model and summarize the results
in table 1. as can be seen from the table, neural architecture search can design several promising
architectures that perform as well as some of the best models on this dataset.

model

depth

parameters error rate (%)

network in network (lin et al., 2013)
all-id98 (springenberg et al., 2014)
deeply supervised net (lee et al., 2015)
highway network (srivastava et al., 2015)
scalable bayesian optimization (snoek et al., 2015)
fractalnet (larsson et al., 2016)
with dropout/drop-path
resnet (he et al., 2016a)
resnet (reported by huang et al. (2016c))
resnet with stochastic depth (huang et al., 2016c)

wide resnet (zagoruyko & komodakis, 2016)

resnet (pre-activation) (he et al., 2016b)

densenet (l = 40, k = 12) huang et al. (2016a)
densenet(l = 100, k = 12) huang et al. (2016a)
densenet (l = 100, k = 24) huang et al. (2016a)
densenet-bc (l = 100, k = 40) huang et al. (2016b)
neural architecture search v1 no stride or pooling
neural architecture search v2 predicting strides
neural architecture search v3 max pooling
neural architecture search v3 max pooling + more    lters

-
-
-
-
-
21
21
110
110
110
1202
16
28
164
1001
40
100
100
190
15
20
39
39

-
-
-
-
-

38.6m
38.6m
1.7m
1.7m
1.7m
10.2m
11.0m
36.5m
1.7m
10.2m
1.0m
7.0m
27.2m
25.6m
4.2m
2.5m
7.1m
37.4m

8.81
7.25
7.97
7.72
6.37
5.22
4.60
6.61
6.41
5.23
4.91
4.81
4.17
5.46
4.62
5.24
4.10
3.74
3.46
5.50
6.01
4.47
3.65

table 1: performance of neural architecture search and other state-of-the-art models on cifar-10.

7

under review as a conference paper at iclr 2017

first, if we ask the controller to not predict stride or pooling, it can design a 15-layer architecture
that achieves 5.50% error rate on the test set. this architecture has a good balance between accuracy
and depth. in fact, it is the shallowest and perhaps the most inexpensive architecture among the top
performing networks in this table. this architecture is shown in appendix a, figure 7. a notable
feature of this architecture is that it has many rectangular    lters and it prefers larger    lters at the
top layers. like residual networks (he et al., 2016a), the architecture also has many one-step skip
connections. this architecture is a local optimum in the sense that if we perturb it, its performance
becomes worse. for example, if we densely connect all layers with skip connections, its performance
becomes slightly worse: 5.56%. if we remove all skip connections, its performance drops to 7.97%.
in the second set of experiments, we ask the controller to predict strides in addition to other hyper-
parameters. as stated earlier, this is more challenging because the search space is larger. in this
case, it    nds a 20-layer architecture that achieves 6.01% error rate on the test set, which is not much
worse than the    rst set of experiments.
finally, if we allow the controller to include 2 pooling layers at layer 13 and layer 24 of the archi-
tectures, the controller can design a 39-layer network that achieves 4.47% which is very close to
the best human-invented architecture that achieves 3.74%. to limit the search space complexity we
have our model predict 13 layers where each layer prediction is a fully connected block of 3 layers.
additionally, we change the number of    lters our model can predict from [24, 36, 48, 64] to [6, 12,
24, 36]. our result can be improved to 3.65% by adding 40 more    lters to each layer of our archi-
tecture. additionally this model with 40    lters added is 1.05x as fast as the densenet model that
achieves 3.74%, while having better performance. the densenet model that achieves 3.46% error
rate (huang et al., 2016b) uses 1x1 convolutions to reduce its total number of parameters, which we
did not do, so it is not an exact comparison.

4.2 learning recurrent cells for id32

dataset: we apply neural architecture search to the id32 dataset, a well-known bench-
mark for id38. on this task, lstm architectures tend to excel (zaremba et al., 2014;
gal, 2015), and improving them is dif   cult (jozefowicz et al., 2015). as ptb is a small dataset, reg-
ularization methods are needed to avoid over   tting. first, we make use of the embedding dropout
and recurrent dropout techniques proposed in zaremba et al. (2014) and (gal, 2015). we also try to
combine them with the method of sharing input and output embeddings, e.g., bengio et al. (2003);
mnih & hinton (2007), especially inan et al. (2016) and press & wolf (2016). results with this
method are marked with    shared embeddings.   

search space: following section 3.4, our controller sequentially predicts a combination method
then an activation function for each node in the tree. for each node in the tree, the controller
id56 needs to select a combination method in [add, elem mult] and an activation method in
[identity, tanh, sigmoid, relu]. the number of input pairs to the id56 cell is called the    base
number    and set to 8 in our experiments. when the base number is 8, the search space is has ap-
proximately 6    1016 architectures, which is much larger than 15,000, the number of architectures
that we allow our controller to evaluate.

training details: the controller and its training are almost identical to the cifar-10 experiments
except for a few modi   cations: 1) the learning rate for the controller id56 is 0.0005, slightly smaller
than that of the controller id56 in cifar-10, 2) in the distributed training, we set s to 20, k to 400
and m to 1, which means there are 400 networks being trained on 400 cpus concurrently at any
time, 3) during asynchronous training we only do parameter updates to the parameter-server once
10 gradients from replicas have been accumulated.
in our experiments, every child model is constructed and trained for 35 epochs. every child model
has two layers, with the number of hidden units adjusted so that total number of learnable parameters
approximately match the    medium    baselines (zaremba et al., 2014; gal, 2015). in these experi-
ments we only have the controller predict the id56 cell structure and    x all other hyperparameters.
the reward function is

(validation perplexity)2 where c is a constant, usually set at 80.

c

after the controller id56 is done training, we take the best id56 cell according to the lowest val-
idation perplexity and then run a grid search over learning rate, weight initialization, dropout rates

8

under review as a conference paper at iclr 2017

and decay epoch. the best cell found was then run with three different con   gurations and sizes to
increase its capacity.

results:
in table 2, we provide a comprehensive list of architectures and their performance on
the ptb dataset. as can be seen from the table, the models found by neural architecture search
outperform other state-of-the-art models on this dataset, and one of our best models achieves a gain
of almost 3.6 perplexity. not only is our cell is better, the model that achieves 64 perplexity is also
more than two times faster because the previous best network requires running a cell 10 times per
time step (zilly et al., 2016).

model
mikolov & zweig (2012) - kn-5
mikolov & zweig (2012) - kn5 + cache
mikolov & zweig (2012) - id56
mikolov & zweig (2012) - id56-lda
mikolov & zweig (2012) - id56-lda + kn-5 + cache
pascanu et al. (2013) - deep id56
cheng et al. (2014) - sum-prod net
zaremba et al. (2014) - lstm (medium)
zaremba et al. (2014) - lstm (large)
gal (2015) - variational lstm (medium, untied)
gal (2015) - variational lstm (medium, untied, mc)
gal (2015) - variational lstm (large, untied)
gal (2015) - variational lstm (large, untied, mc)
kim et al. (2015) - charid98
press & wolf (2016) - variational lstm, shared embeddings
merity et al. (2016) - zoneout + variational lstm (medium)
merity et al. (2016) - pointer sentinel-lstm (medium)
inan et al. (2016) - vd-lstm + real (large)
zilly et al. (2016) - variational rhn, shared embeddings
neural architecture search with base 8
neural architecture search with base 8 and shared embeddings
neural architecture search with base 8 and shared embeddings

parameters test perplexity

2m   
2m   
6m   
7m   
9m   
6m
5m   
20m
66m
20m
20m
66m
66m
19m
51m
20m
21m
51m
24m
32m
25m
54m

141.2
125.7
124.7
113.7
92.0
107.5
100.0
82.7
78.4
79.7
78.6
75.2
73.4
78.9
73.2
80.6
70.9
68.5
66.0

67.9
64.0
62.4

table 2: single model perplexity on the test set of the id32 id38 task.
parameter numbers with     are estimates with reference to merity et al. (2016).

the newly discovered cell is visualized in figure 8 in appendix a. the visualization reveals that
the new cell has many similarities to the lstm cell in the    rst few steps, such as it likes to compute
w1     ht   1 + w2     xt several times and send them to different components in the cell.

id21 results: to understand whether the cell can generalize to a different task, we
apply it to the character id38 task on the same dataset. we use an experimental setup
that is similar to ha et al. (2016), but use variational dropout by gal (2015). we also train our own
lstm with our setup to get a fair lstm baseline. models are trained for 80k steps and the best test
set perplexity is taken according to the step where validation set perplexity is the best. the results
on the test set of our method and state-of-art methods are reported in table 3. the results on small
settings with 5-6m parameters con   rm that the new cell does indeed generalize, and is better than
the lstm cell.
additionally, we carry out a larger experiment where the model has 16.28m parameters. this model
has a weight decay rate of 1e     4, was trained for 600k steps (longer than the above models) and
the test perplexity is taken where the validation set perplexity is highest. we use dropout rates of 0.2
and 0.5 as described in gal (2015), but do not use embedding dropout. we use the adam optimizer
with a learning rate of 0.001 and an input embedding size of 128. our model had two layers with
800 hidden units. we used a minibatch size of 32 and bptt length of 100. with this setting, our
model achieves 1.214 perplexity, which is the new state-of-the-art result on this task.
finally, we also drop our cell into the gid4 framework (wu et al., 2016), which was previously
tuned for lstm cells, and train an wmt14 english     german translation model. the gid4

9

under review as a conference paper at iclr 2017

id56 cell type
ha et al. (2016) - layer norm hyperlstm
ha et al. (2016) - layer norm hyperlstm large embeddings
ha et al. (2016) - 2-layer norm hyperlstm
two layer lstm
two layer with new cell
two layer with new cell

parameters test bits per character

4.92m
5.06m
14.41m
6.57m
6.57m
16.28m

1.250
1.233
1.219
1.243
1.228
1.214

table 3: comparison between our cell and state-of-art methods on ptb character modeling. the
new cell was found on word level id38.

network has 8 layers in the encoder, 8 layers in the decoder. the    rst layer of the encoder has
bidirectional connections. the attention module is a neural network with 1 hidden layer. when a
lstm cell is used, the number of hidden units in each layer is 1024. the model is trained in a
distributed setting with a parameter sever and 12 workers. additionally, each worker uses 8 gpus
and a minibatch of 128. we use adam with a learning rate of 0.0002 in the    rst 60k training steps,
and sgd with a learning rate of 0.5 until 400k steps. after that the learning rate is annealed by
dividing by 2 after every 100k steps until it reaches 0.1. training is stopped at 800k steps. more
details can be found in wu et al. (2016).
in our experiment with the new cell, we make no change to the above settings except for dropping in
the new cell and adjusting the hyperparameters so that the new model should have the same compu-
tational complexity with the base model. the result shows that our cell, with the same computational
complexity, achieves an improvement of 0.5 test set id7 than the default lstm cell. though this
improvement is not huge, the fact that the new cell can be used without any tuning on the existing
gid4 framework is encouraging. we expect further tuning can help our cell perform better.

control experiment 1     adding more functions in the search space: to test the robustness of
neural architecture search, we add max to the list of combination functions and sin to the list
of id180 and rerun our experiments. the results show that even with a bigger search
space, the model can achieve somewhat comparable performance. the best architecture with max
and sin is shown in figure 8 in appendix a.

control experiment 2     comparison against random search:
instead of policy gradient, one
can use random search to    nd the best network. although this baseline seems simple, it is often very
hard to surpass (bergstra & bengio, 2012). we report the perplexity improvements using policy
gradient against random search as training progresses in figure 6. the results show that not only
the best model using policy gradient is better than the best model using random search, but also the
average of top models is also much better.

figure 6: improvement of neural architecture search over random search over time. we plot the
difference between the average of the top k models our controller    nds vs. random search every 400
models run.

10

0500010000150002000025000iteration0510152025303540perplexity improvementtop_1_unique_modelstop_5_unique_modelstop_15_unique_modelsunder review as a conference paper at iclr 2017

5 conclusion

in this paper we introduce neural architecture search, an idea of using a recurrent neural network to
compose neural network architectures. by using recurrent network as the controller, our method is
   exible so that it can search variable-length architecture space. our method has strong empirical per-
formance on very challenging benchmarks and presents a new research direction for automatically
   nding good neural network architectures. the code for running the models found by the controller
on cifar-10 and ptb will be released at https://github.com/tensor   ow/models . additionally, we
have added the id56 cell found using our method under the name nascell into tensorflow, so
others can easily use it.

acknowledgments

we thank greg corrado, jeff dean, david ha, lukasz kaiser and the google brain team for their
help with the project.

references
jacob andreas, marcus rohrbach, trevor darrell, and dan klein. learning to compose neural

networks for id53. in naacl, 2016.

marcin andrychowicz, misha denil, sergio gomez, matthew w hoffman, david pfau, tom schaul,
and nando de freitas. learning to learn by id119 by id119. arxiv preprint
arxiv:1606.04474, 2016.

dzmitry bahdanau, kyunghyun cho, and yoshua bengio. id4 by jointly

learning to align and translate. in iclr, 2015.

yoshua bengio, r  ejean ducharme, pascal vincent, and christian jauvin. a neural probabilistic

language model. jmlr, 2003.

james bergstra and yoshua bengio. random search for hyper-parameter optimization. jmlr, 2012.

james bergstra, r  emi bardenet, yoshua bengio, and bal  azs k  egl. algorithms for hyper-parameter

optimization. in nips, 2011.

james bergstra, daniel yamins, and david d cox. making a science of model search: hyperpa-

rameter optimization in hundreds of dimensions for vision architectures. icml, 2013.

alan w. biermann. the id136 of regular lisp programs from examples. ieee transactions on

systems, man, and cybernetics, 1978.

wei-chen cheng, stanley kok, hoai vu pham, hai leong chieu, and kian ming adam chai.

id38 with sum-product networks. in interspeech, 2014.

navneet dalal and bill triggs. histograms of oriented gradients for human detection. in cvpr,

2005.

jeffrey dean, greg corrado, rajat monga, kai chen, matthieu devin, mark mao, andrew senior,
paul tucker, ke yang, quoc v. le, et al. large scale distributed deep networks. in nips, 2012.

dario floreano, peter d  urr, and claudio mattiussi. neuroevolution: from architectures to learning.

evolutionary intelligence, 2008.

yarin gal. a theoretically grounded application of dropout in recurrent neural networks. arxiv

preprint arxiv:1512.05287, 2015.

david ha, andrew dai, and quoc v. le. hypernetworks. arxiv preprint arxiv:1609.09106, 2016.

kaiming he, xiangyu zhang, shaoqing ren, and jian sun. deep residual learning for image recog-

nition. in cvpr, 2016a.

kaiming he, xiangyu zhang, shaoqing ren, and jian sun.

networks. arxiv preprint arxiv:1603.05027, 2016b.

identity mappings in deep residual

11

under review as a conference paper at iclr 2017

geoffrey hinton, li deng, dong yu, george e. dahl, abdel-rahman mohamed, navdeep jaitly,
andrew senior, vincent vanhoucke, patrick nguyen, tara n. sainath, et al. deep neural networks
for acoustic modeling in id103: the shared views of four research groups. ieee
signal processing magazine, 2012.

sepp hochreiter and juergen schmidhuber. long short-term memory. neural computation, 1997.

gao huang, zhuang liu, and kilian q. weinberger. densely connected convolutional networks.

arxiv preprint arxiv:1608.06993, 2016a.

gao huang, zhuang liu, kilian q. weinberger, and laurens van der maaten. densely connected

convolutional networks. arxiv preprint arxiv:1608.06993, 2016b.

gao huang, yu sun, zhuang liu, daniel sedra, and kilian weinberger. deep networks with stochas-

tic depth. arxiv preprint arxiv:1603.09382, 2016c.

hakan inan, khashayar khosravi, and richard socher. tying word vectors and word classi   ers: a

loss framework for id38. arxiv preprint arxiv:1611.01462, 2016.

sergey ioffe and christian szegedy. batch id172: accelerating deep network training by

reducing internal covariate shift. in icml, 2015.

kevin jarrett, koray kavukcuoglu, yann lecun, et al. what is the best multi-stage architecture for

object recognition? in iccv, 2009.

rafal jozefowicz, wojciech zaremba, and ilya sutskever. an empirical exploration of recurrent

network architectures. in icml, 2015.

yoon kim, yacine jernite, david sontag, and alexander m. rush. character-aware neural language

models. arxiv preprint arxiv:1508.06615, 2015.

diederik p. kingma and jimmy ba. adam: a method for stochastic optimization. in iclr, 2015.

alex krizhevsky, ilya sutskever, and geoffrey e. hinton. id163 classi   cation with deep convo-

lutional neural networks. in nips, 2012.

brenden m. lake, ruslan salakhutdinov, and joshua b. tenenbaum. human-level concept learning

through probabilistic program induction. science, 2015.

gustav larsson, michael maire, and gregory shakhnarovich. fractalnet: ultra-deep neural net-

works without residuals. arxiv preprint arxiv:1605.07648, 2016.

yann lecun, l  eon bottou, yoshua bengio, and patrick haffner. gradient-based learning applied to

document recognition. proceedings of the ieee, 1998.

chen-yu lee, saining xie, patrick gallagher, zhengyou zhang, and zhuowen tu. deeply-

supervised nets. in aistats, 2015.

ke li and jitendra malik. learning to optimize. arxiv preprint arxiv:1606.01885, 2016.

percy liang, michael i. jordan, and dan klein. learning programs: a hierarchical bayesian ap-

proach. in icml, 2010.

min lin, qiang chen, and shuicheng yan. network in network. in iclr, 2013.

david g. lowe. object recognition from local scale-invariant features. in cvpr, 1999.

hector mendoza, aaron klein, matthias feurer, jost tobias springenberg, and frank hutter. to-
wards automatically-tuned neural networks. in proceedings of the 2016 workshop on automatic
machine learning, pp. 58   65, 2016.

stephen merity, caiming xiong, james bradbury, and richard socher. pointer sentinel mixture

models. arxiv preprint arxiv:1609.07843, 2016.

tomas mikolov and geoffrey zweig. context dependent recurrent neural network language model.

in slt, pp. 234   239, 2012.

12

under review as a conference paper at iclr 2017

andriy mnih and geoffrey hinton. three new id114 for statistical language modelling.

in icml, 2007.

vinod nair and geoffrey e. hinton. recti   ed linear units improve restricted id82s.

in icml, 2010.

arvind neelakantan, quoc v. le, and ilya sutskever. neural programmer: inducing latent programs

with id119. in iclr, 2015.

razvan pascanu, caglar gulcehre, kyunghyun cho, and yoshua bengio. how to construct deep

recurrent neural networks. arxiv preprint arxiv:1312.6026, 2013.

o   r press and lior wolf. using the output embedding to improve language models. arxiv preprint

arxiv:1608.05859, 2016.

marc   aurelio ranzato, sumit chopra, michael auli, and wojciech zaremba. sequence level train-

ing with recurrent neural networks. arxiv preprint arxiv:1511.06732, 2015.

scott reed and nando de freitas. neural programmer-interpreters. in iclr, 2015.

shreyas saxena and jakob verbeek. convolutional neural fabrics. in nips, 2016.

shiqi shen, yong cheng, zhongjun he, wei he, hua wu, maosong sun, and yang liu. minimum

risk training for id4. in acl, 2016.

karen simonyan and andrew zisserman. very deep convolutional networks for large-scale image

recognition. arxiv preprint arxiv:1409.1556, 2014.

jasper snoek, hugo larochelle, and ryan p. adams. practical bayesian optimization of machine

learning algorithms. in nips, 2012.

jasper snoek, oren rippel, kevin swersky, ryan kiros, nadathur satish, narayanan sundaram,
mostofa patwary, mostofa ali, ryan p. adams, et al. scalable bayesian optimization using deep
neural networks. in icml, 2015.

jost tobias springenberg, alexey dosovitskiy, thomas brox, and martin riedmiller. striving for

simplicity: the all convolutional net. arxiv preprint arxiv:1412.6806, 2014.

rupesh kumar srivastava, klaus greff, and j  urgen schmidhuber. id199. arxiv preprint

arxiv:1505.00387, 2015.

kenneth o. stanley, david b. d   ambrosio, and jason gauci. a hypercube-based encoding for

evolving large-scale neural networks. arti   cial life, 2009.

phillip d. summers. a methodology for lisp program construction from examples. journal of the

acm, 1977.

ilya sutskever, james martens, george dahl, and geoffrey hinton. on the importance of initializa-

tion and momentum in deep learning. in icml, 2013.

ilya sutskever, oriol vinyals, and quoc v. le. sequence to sequence learning with neural networks.

in nips, 2014.

christian szegedy, wei liu, yangqing jia, pierre sermanet, scott reed, dragomir anguelov, du-
mitru erhan, vincent vanhoucke, and andrew rabinovich. going deeper with convolutions. in
cvpr, 2015.

sebastian thrun and lorien pratt. learning to learn. springer science & business media, 2012.

oriol vinyals, meire fortunato, and navdeep jaitly. id193. in nips, 2015.

daan wierstra, faustino j gomez, and j  urgen schmidhuber. modeling systems with internal state

using evolino. in gecco, 2005.

ronald j. williams. simple statistical gradient-following algorithms for connectionist reinforcement

learning. in machine learning, 1992.

13

under review as a conference paper at iclr 2017

yonghui wu, mike schuster, zhifeng chen, quoc v. le, mohammad norouzi, et al. google   s
id4 system: bridging the gap between human and machine translation.
arxiv preprint arxiv:1609.08144, 2016.

sergey zagoruyko and nikos komodakis. wide residual networks. in bmvc, 2016.

wojciech zaremba, ilya sutskever, and oriol vinyals. recurrent neural network id173.

arxiv preprint arxiv:1409.2329, 2014.

julian georg zilly, rupesh kumar srivastava, jan koutn    k, and j  urgen schmidhuber. recurrent

id199. arxiv preprint arxiv:1607.03474, 2016.

14

under review as a conference paper at iclr 2017

a appendix

figure 7: convolutional architecture discovered by our method, when the search space does not
have strides or pooling layers. fh is    lter height, fw is    lter width and n is number of    lters. note
that the skip connections are not residual connections. if one layer has many input layers then all
input layers are concatenated in the depth dimension.

15

under review as a conference paper at iclr 2017

figure 8: a comparison of the original lstm cell vs. two good cells our model found. top left:
lstm cell. top right: cell found by our model when the search space does not include max and
sin. bottom: cell found by our model when the search space includes max and sin (the controller
did not choose to use the sin function).

16

