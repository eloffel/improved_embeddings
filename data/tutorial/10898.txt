8
1
0
2

 
r
a

 

m
1
2

 
 
]

g
l
.
s
c
[
 
 

3
v
8
9
7
2
0

.

4
0
7
1
:
v
i
x
r
a

bayesian recurrent neural networks

meire fortunato
deepmind
meirefortunato@google.com

oriol vinyals
deepmind
vinyals@google.com

charles blundell
deepmind
cblundell@google.com

abstract

in this work we explore a straightforward id58 scheme for recur-
rent neural networks. firstly, we show that a simple adaptation of truncated
id26 through time can yield good quality uncertainty estimates and
superior regularisation at only a small extra computational cost during training,
also reducing the amount of parameters by 80%. secondly, we demonstrate how
a novel kind of posterior approximation yields further improvements to the per-
formance of bayesian id56s. we incorporate local gradient information into the
approximate posterior to sharpen it around the current batch statistics. we show
how this technique is not exclusive to recurrent neural networks and can be applied
more widely to train bayesian neural networks. we also empirically demonstrate
how bayesian id56s are superior to traditional id56s on a language modelling
benchmark and an image captioning task, as well as showing how each of these
methods improve our model over a variety of other schemes for training them. we
also introduce a new benchmark for studying uncertainty for language models so
future methods can be easily compared.

1

introduction

recurrent neural networks (id56s) achieve state-of-the-art performance on a wide range of se-
quence prediction tasks (wu et al., 2016; amodei et al., 2015; jozefowicz et al., 2016; zaremba
et al., 2014; lu et al., 2016). in this work we examine how to add uncertainty and regularisation
to id56s by means of applying bayesian methods to training. this approach allows the network
to express uncertainty via its parameters. at the same time, by using a prior to integrate out the
parameters to average across many models during training, it gives a regularisation effect to the
network. recent approaches either justify dropout (srivastava et al., 2014) and weight decay as
a variational id136 scheme (gal & ghahramani, 2016), or apply stochastic gradient langevin
dynamics (welling & teh, 2011, sgld) to truncated id26 in time directly (gan et al.,
2016). interestingly, recent work has not explored further directly applying a id58 in-
ference scheme (beal, 2003) for id56s as was done in graves (2011). we derive a straightforward
approach based upon bayes by backprop (blundell et al., 2015) that we show works well on large
scale problems. our strategy is a simple alteration to truncated id26 through time that
results in an estimate of the posterior distribution on the weights of the id56.
this formulation explicitly leads to a cost function with an information theoretic justi   cation by
means of a bits-back argument (hinton & van camp, 1993) where a kl divergence acts as a regu-
lariser.
the form of the posterior in variational id136 shapes the quality of the uncertainty estimates
and hence the overall performance of the model. we shall show how performance of the id56 can
be improved by means of adapting (   sharpening   ) the posterior locally to a batch. this sharpening
adapts the variational posterior to a batch of data using gradients based upon the batch. this can
be viewed as a hierarchical distribution, where a local batch gradient is used to adapt a global
posterior, forming a local approximation for each batch. this gives a more    exible form to the

1

typical assumption of gaussian posterior when variational id136 is applied to neural networks,
which reduces variance. this technique can be applied more widely across other bayesian models.
the contributions of our work are as follows:

adopted in other maximum likelihood frameworks.

    we show how bayes by backprop (bbb) can be ef   ciently applied to id56s.
    we develop a novel technique which reduces the variance of bbb, and which can be widely
    we improve performance on two widely studied benchmarks outperforming established
    we introduce a new benchmark for studying uncertainty of language models.

regularisation techniques such as dropout by a big margin.

2 bayes by backprop

bayes by backprop (graves, 2011; blundell et al., 2015) is a variational id136 (wainwright et al.,
2008) scheme for learning the posterior distribution on the weights        rd of a neural network. this
posterior distribution is typically taken to be a gaussian with mean parameter        rd and standard
deviation parameter        rd, denoted n (  |  ,   2). note that we use a diagonal covariance matrix,
and d     the dimensionality of the parameters of the network     is typically in the order of millions.
let log p(y|  , x) be the log-likelihood of the model, then the network is trained by minimising the
variational free energy:

(cid:20)

(cid:21)

l(  ) = eq(  )

log

q(  )

p(y|  , x)p(  )

,

(1)

where p(  ) is a prior on the parameters.
to maximising the log-likelihood
minimising the variational free energy (1) is equivalent
log p(y|  , x) subject to a kl complexity term on the parameters of the network that acts as a regu-
lariser:

l(  ) =    eq(  ) [log p(y|  , x)] + kl [q(  ) || p(  )] .

(2)

in the gaussian case with a zero mean prior, the kl term can be seen as a form of weight decay on
the mean parameters, where the rate of weight decay is automatically tuned by the standard deviation
parameters of the prior and posterior. please refer to the supplementary material for the algorithmic
details on bayes by backprop.
the uncertainty afforded by bayes by backprop trained networks has been used successfully for
training feedforward models for supervised learning and to aid exploration by id23
agents (blundell et al., 2015; lipton et al., 2016; houthooft et al., 2016), but as yet, it has not been
applied to recurrent neural networks.

3 truncated bayes by backprop through time

the core of an id56, f, is a neural network that maps the id56 state st at step t, and an input
observation xt to a new id56 state st+1, f : (st, xt) (cid:55)    st+1. the exact equations of an lstm core
can be found in the supplemental material sec a.2.
an id56 can be trained on a sequence of length t by id26 through by unrolling t times
into a feedforward network. explicitly, we set si = f (si   1, xi), for i = 1, . . . , t . we shall refer to
an id56 core unrolled for t steps by s1:t = ft (x1:t , s0). note that the truncated version of the
algorithm can be seen as taking s0 as the last state of the previous batch, st .
id56 parameters are learnt in much the same way as in a feedforward neural network. a loss
(typically after further layers) is applied to the states s1:t of the id56, and then id26
is used to update the weights of the network. crucially, the weights at each of the unrolled steps
are shared. thus each weight of the id56 core receives t gradient contributions when the id56 is
unrolled for t steps.
applying bbb to id56s is depicted in figure 1 where the weight matrices of the id56 are drawn
from a distribution (learnt by bbb). however, this direct application raises two questions: when

2

algorithm: bayes by backprop for id56s
sample       n (0, i),       rd, and set network
parameters to    =    +    .
sample a minibatch of truncated sequences (x, y).
do forward and backward propagation as normal,
and let g be the gradient w.r.t   .
let
the
log n (  |  ,   2)     log p(  ) w.r.t.
respectively.
update    using the gradient g+ 1
update    using the gradient

gradients
of
  ,    and   

(cid:16) g+ 1

gkl
bc .
  
  + gkl
bc .

c gkl
b
c gkl
b

   , gkl

, gkl

gkl
  

(cid:17)

be

+

  

  

  

  

figure 1: illustration (left) and algorithm (right) of bayes by backprop applied to an id56.

to sample the parameters of the id56, and how to weight the contribution of the kl regulariser of
(2). we shall brie   y justify the adaptation of bbb to id56s, given in figure 1. the variational free
energy of (2) for an id56 on a sequence of length t is:

l(  ) =    eq(  ) [log p(y1:t|  , x1:t )] + kl [q(  ) || p(  )] ,

(3)
where p(y1:t|  , x1:t ) is the likelihood of a sequence produced when the states of an unrolled id56
ft are fed into an appropriate id203 distribution. the parameters of the entire network are
  . although the id56 is unrolled t times, each weight is penalised just once by the kl term,
rather than t times. also clear from (3) is that when a monte carlo approximation is taken to the
expectation, the parameters    should be held    xed throughout the entire sequence.
two complications arise to the above naive derivation in practice:    rstly, sequences are often long
enough and models suf   ciently large, that unrolling the id56 for the whole sequence is prohibitive.
secondly, to reduce variance in the gradients, more than one sequence is trained at a time. thus the
typical regime for training id56s involves training on mini-batches of truncated sequences.
let b be the number of mini-batches and c the number of truncated sequences (   cuts   ), then we
can write (3) as:

l(  ) =    eq(  )

log

p(y(b,c)|  , x(b,c))

+ kl [q(  ) || p(  )] ,

(4)

where the (b, c) superscript denotes elements of cth truncated sequence in the bth minibatch. thus
the free energy of mini-batch b of a truncated sequence c can be written as:

l(b,c)(  ) =    eq(  )

log p(y(b,c)|  , x(b,c), s(b,c)
prev )

+ w(b,c)

kl kl [q(  ) || p(  )] ,

(5)

where w(b,c)

quences (thus (cid:80)b

kl

(cid:80)c

b=1

c=1 w(b,c)

distributes the responsibility of the kl cost among minibatches and truncated se-
kl = 1), and s(b,c)
refers to the initial state of the id56 for the
prev
minibatch x(b,c). in practice, we pick w(b,c)
cb so that the kl penalty is equally distributed
kl = 1
among all mini-batches and truncated sequences. the truncated sequences in each subsequent mini-
batches are picked in order, and so s(b,c)
finally, the question of when to sample weights follows naturally from taking a monte carlo ap-
proximations to (5): for each minibatch, sample a fresh set of parameters.

prev is set to the last state of the id56 for x(b,c   1).

4 posterior sharpening

the choice of variational posterior q(  ) as described in section 3 can be enhanced by adding side
information that makes the posterior over the parameters more accurate, thus reducing variance of
the learning process.

3

b(cid:89)

c(cid:89)

b=1

c=1

(cid:34)

(cid:104)

(cid:35)

(cid:105)

akin to variational auto encoders (vaes) (kingma & welling, 2013; rezende et al., 2014), which
propose a powerful distribution q(z|x) to improve the gradient estimates of the (intractable) like-
lihood function p(x), here we propose a similar approach. namely, for a given minibatch of data
(inputs and targets) (x, y) sampled from the training set, we construct such q(  |(x, y)). thus, we
compute a proposal distribution where the latents (z in vaes) are the parameters    (which we wish
to integrate out), and the    privileged    information upon which we condition is a minibatch of data.
we could have chosen to condition on a single example (x, y) instead of a batch, but this would
have yielded different parameter vectors    per example. conditioning on the full minibatch has
the advantage of producing a single    per minibatch, so that matrix-matrix operations can still be
carried.
this    sharpened    posterior yields more stable optimisation, a common pitfall of bayesian ap-
proaches to train neural networks, and the justi   cation of this method follows from strong empirical
evidence and extensive work on vaes.
a challenging aspect of modelling the variational posterior q(  |(x, y)) is the large number of di-
mensions of        rd. when the dimensionality is not in the order of millions, a powerful non-linear
function (such as a neural network) can be used which transforms observations (x, y) to the param-
eters of a gaussian distribution, as proposed in kingma & welling (2013); rezende et al. (2014).
unfortunately, this neural network would have far too many parameters, making this approach un-
feasible.
given that the loss     log p(y|  , x) is differentiable with respect to   , we propose to parameterise q
as a linear combination of    and g   =          log p(y|  , x), both d-dimensional vectors.
thus, we can de   ne a hierarchical posterior of the form

q(  |(x, y)) =

q(  |  , (x, y))q(  )d  ,

(6)
with   ,        rd, and q(  ) = n (  |  ,   )     the same as in the standard bbb method. finally, let    
denote element-wise multiplication, we then have

q(  |  , (x, y)) = n (  |              g  ,   2

(7)
where        rd is a free parameter to be learnt and   0 a scalar hyper-parameter of our model.    can
be interpreted as a per-parameter learning rate.
during training, we get        q(  |(x, y)) via ancestral sampling to optimise the loss

0i),

l(  ,   ,   ) = e(x,y)[eq(  )q(  |  ,(x,y))[l(x, y,   ,   |  ,   ,   )]],

(8)

(cid:90)

with l(x, y,   ,   |  ,   ,   ) given by
l(x, y,   ,   |  ,   ,   ) =     log p(y|  , x) + kl [q(  |  , (x, y)) || p(  |  )] +

kl [q(  ) || p(  )] , (9)

1
c

where   ,   ,    are our model parameters, and p are the priors for the distributions de   ning q (for exact
details of these distributions see section 6). the constant c is the number of truncated sequences as
de   ned in section3. the bound on the true data likelihood which yields eq. (8) is derived in sec 4.1.
algorithm 1 presents how learning is performed in practice.

algorithm 1 bbb with posterior sharpening

sample a minibatch (x, y) of truncated sequences.
sample        q(  ) = n (  |  ,   ).
let g   =          log p(y|  , x).
sample        q(  |  , (x, y)) = n (  |              g  ,   2
compute the gradients of eq. (8) w.r.t. (  ,   ,   ).
update (  ,   ,   ).

0i).

as long as the improvement of the log likelihood log p(y|  , x) term along the gradient g   is greater
than the kl cost added for posterior sharpening (kl [q(  |  , (x, y)) || p(  |  )]), then the lower bound
in (8) will improve. this justi   es the effectiveness of the posterior over the parameters proposed in

4

eq. 7 which will be effective as long as the curvature of log p(y|  , x) is large. since    is learnt, it
controls the tradeoff between curvature improvement and kl loss. studying more powerful param-
eterisations is part of future research.
unlike regular bbb where the kl terms can be ignored during id136, there are two options
for doing id136 under posterior sharpening. the    rst involves using q(  ) and ignoring any
kl terms, similar to regular bbb. the second involves using q(  |  , (x, y)) which requires using
the term kl [q(  |  , (x, y)) || p(  |  )] yielding an upper bound on perplexity (lower bound in log
id203; see section 4.2 for details). this parameterisation involves computing an extra gradient
and incurs a penalty in training speed. a comparison of the two id136 methods is provided in
section 6. furthermore, in the case of id56s, the exact gradient cannot be ef   ciently computed, so
bptt is used.

4.1 derivation of free energy for posterior sharpening

here we turn to deriving the training id168 we use for posterior sharpening. the basic
idea is to take a variational approximation to the marginal likelihood p(x) that factorises hierar-
chically. hierarchical variational schemes for topic models have been studied previously in ran-
ganath et al. (2016). here, we shall assume a hierarchical prior for the parameters such that
and factorises as q(  ,   |x) = q(  |  , x)q(  ). the expected lower bound on p(x) is then as follows:

p(x) = (cid:82) p(x|  )p(  |  )p(  )d  d  . then we pick a variational posterior that conditions upon x,

(cid:18)(cid:90)

log p(x) = log

(cid:20)

p(x|  )p(  |  )p(  )d  d  
p(x|  )p(  |  )p(  )

(cid:19)
(cid:21)

(cid:21)

    eq(  ,  |x)

log

= eq(  |  ,x)q(  )

(cid:20)

q(  ,   |x)
p(x|  )p(  |  )p(  )
q(  |  , x)q(  )
log p(x|  ) + log

log

(cid:20)

(cid:20)
(cid:2)eq(  |  ,x) [log p(x|  )]     kl [q(  |  , x) || p(  |  )](cid:3)     kl [q(  ) || p(  )]

p(  |  )
q(  |  , x)

eq(  |  ,x)

p(  )
q(  )

+ log

(cid:21)

(cid:21)

= eq(  )
= eq(  )

4.2 derivation of predictions with posterior sharpening

now we consider making predictions. these are done by means of bayesian model averaging
over the approximate posterior. in the case of no posterior sharpening, predictions are made by
evaluating: eq(  ) [log p(  x|  )]. for posterior sharpening, we derive a bound on a bayesian model
average over the approximate posterior of   :

log

(cid:21)

p(  x|  )p(  |  )d  

(cid:20)
(cid:90)
(cid:20)
(cid:2)eq(  |  ,x) [log p(  x|  )]     kl [q(  |  , x) || p(  |  )](cid:3)

p(  x|  )p(  |  )
q(  |  , x)

eq(  |  ,x)

(cid:21)(cid:21)

(cid:20)

log

(10)

(11)

(12)

(13)

(14)

(15)

(16)

(17)

eq(  ) [log p(  x|  )] = eq(  )

    eq(  )
= eq(  )

5 related work

we note that the procedure of sharpening the posterior as explained above has similarities with
other techniques. perhaps the most obvious one is line search: indeed,    is a trained parameter that
does line search along the gradient direction. probabilistic interpretations have been given to line
search in e.g. mahsereci & hennig (2015), but ours is the    rst that uses a variational posterior with
the reparametrisation trick/perturbation analysis gradient. also, the probabilistic treatment to line
search can also be interpreted as a trust region method.
another related technique is dynamic evaluation (mikolov et al., 2010), which trains an id56 during
evaluation of the model with a    xed learning rate. the update applied in this case is cumulative, and

5

only uses previously seen data. thus, they can take a purely deterministic approach and ignore any
kl between a posterior with privileged information and a prior. as we will show in section 6,
performance gains can be signi   cant as the data exhibits many short term correlations.
lastly, learning to optimise (or learning to learn) (li & malik, 2016; andrychowicz et al., 2016) is
related in that a learning rate is learned so that it produces better updates than those provided by e.g.
adagrad (duchi et al., 2011) or adam (kingma & ba, 2014). whilst they train a parametric model,
we treat these as free parameters (so that they can adapt more quickly to the non-stationary distribu-
tion w.r.t. parameters). notably, we use gradient information to inform a variational posterior so as
to reduce variance of bayesian neural networks. thus, although similar in    avour, the underlying
motivations are quite different.
applying bayesian methods to neural networks has a long history, with most common approxima-
tions having been tried. buntine & weigend (1991) propose various maximum a posteriori schemes
for neural networks, including an approximate posterior centered at the mode. buntine & weigend
(1991) also suggest using second order derivatives in the prior to encourage smoothness of the re-
sulting network. hinton & van camp (1993) proposed using variational methods for compressing
the weights of neural networks as a regulariser. hochreiter et al. (1995) suggest an mdl loss for
single layer networks that penalises non-robust weights by means of an approximate penalty based
upon perturbations of the weights on the outputs. denker & lecun (1991); mackay (1995) investi-
gated using the laplace approximation for capturing the posterior of neural networks. neal (2012)
investigated the use of hybrid monte carlo for training neural networks, although it has so far been
dif   cult to apply these to the large sizes of networks considered here.
more recently graves (2011) derived a variational id136 scheme for neural networks and blun-
dell et al. (2015) extended this with an update for the variance that is unbiased and simpler to com-
pute. graves (2016) derives a similar algorithm in the case of a mixture posterior. several authors
have claimed that dropout (srivastava et al., 2014) and gaussian dropout (wang & manning, 2013)
can be viewed as approximate variational id136 schemes (gal & ghahramani, 2015; kingma
et al., 2015, respectively).
a few papers have investigated approximate bayesian recurrent neural networks. mirikitani & niko-
laev (2010) proposed a second-order, online training scheme for recurrent neural networks, while
chien & ku (2016) only capture a single point estimate of the weight distribution. gal & ghahra-
mani (2016) highlighted monte carlo dropout for lstms (we explicitly compare to these results in
our experiments), whilst graves (2011) proposed a variational scheme with biased gradients for the
variance parameter using the fisher matrix. our work extends this by using an unbiased gradient
estimator without need for approximating the fisher and also add a novel posterior approximation.
variational methods typically underestimate the uncertainty in the posterior (as they are mode seek-
ing, akin to the laplace approximation), whereas expectation propagation methods often average
over modes and so tend to overestimate uncertainty (although there are counter examples for each
depending upon the particular factorisation and approximations used; see for example (turner &
sahani, 2011)). nonetheless, several papers explore applying expectation propagation to neural
networks: soudry et al. (2014) derive a closed form approximate online expectation propagation
algorithm, whereas hern  andez-lobato & adams (2015) proposed using multiple passes of assumed
density    ltering (in combination with early stopping) attaining good performance on a number of
small data sets. hasenclever et al. (2015) derive a distributed expectation propagation scheme with
sgld (welling & teh, 2011) as an inner loop. others have also considered applying sgld to
neural networks (li et al., 2015) and gan et al. (2016) more recently used sgld for lstms (we
compare to these results in our experiments).

6 experiments

we present the results of our method for a language modelling and an image id134 task.

6.1 language modelling

we evaluated our model on the id32 marcus et al. (1993) benchmark, a task consisting
on next word prediction. we used the network architecture from zaremba et al. (2014), a simple

6

yet strong baseline on this task, and for which there is an open source implementation1. the base-
line consists of an id56 with lstm cells and a special regularisation technique, where the dropout
operator is only applied to the non-recurrent connections. we keep the network con   guration un-
changed, but instead of using dropout we apply our bayes by backprop formulation. our goal is to
demonstrate the effect of applying bbb to a pre-existing, well studied architecture.
to train our models, we tuned the parameters on the prior distribution, the learning rate and its
decay. the weights were initialised randomly and we used id119 with gradient clipping
for optimisation, closely following zaremba et al. (2014)   s    medium    lstm con   guration (2 layers
with 650 units each).
as in blundell et al. (2015), the prior of the network weights    was taken to be a scalar mixture of
two gaussian densities with zero mean and variances   2

(cid:89)

(cid:0)  n (  j|0,   2

1 and   2

2, explicitly
1) + (1       )n (  j|0,   2

2)(cid:1) ,

p(  ) =

(18)

j

where   j is the j-th weight of the network. we searched        {0.25, 0.5, 0.75}, log   1     {0,   1,   2}
and log   2     {   6,   7,   8}.
for speed purposes, during training we used one sample from the posterior for estimating the gradi-
ents and computing the (approximate) kl-divergence. for prediction, we experimented with either
computing the expected loss via monte carlo sampling, or using the mean of the posterior distribu-
tion as the parameters of the network (map estimate). we observed that the results improved as we
increased the number of samples but they were not signi   cantly better than taking the mean (as was
also reported by graves (2011); blundell et al. (2015)). for convenience, in table 1 we report our
numbers using the mean of the converged distribution, as there is no computation overhead w.r.t. a
standard lstm model.
table 1 compares our results to the lstm dropout baseline zaremba et al. (2014) we built from, and
to the variational lstms gal & ghahramani (2016), which is another bayesian approach to this
task. finally, we added dynamic evaluation mikolov et al. (2010) results with a learning rate of 0.1,
which was found via cross validation. the implementation for the bayesian id56 baseline which
replicates the results reported on table 1 can be found at https://github.com/deepmind/
sonnet/blob/master/sonnet/examples/bid56_ptb.py.
as with other vae-related id56s fabius & van amersfoort (2014); bayer & osendorfer (2014);
chung et al. (2015) perplexities using posterior sharpening are reported including a kl penalty
kl [q(  |  , (x, y)) || p(  |  )] in the log likelihood term (the kl is computed exactly, not sampled).
for posterior sharpening we use a hierarchical prior for   : p(  |  ) = n (  |  ,   2
0i) which expresses
our belief that a priori, the network parameters    will be much like the data independent parameters
   with some small gaussian perturbation. in our experiments we swept over   0 on the validation
set, and found   0 = 0.02 to perform well, although results were not particularly sensitive to this.
note that with posterior sharpening, the perplexities reported are upper bounds (as the likelihoods
are lower bounds).
lastly, we tested the variance reduction capabilities of posterior sharpening by analysing the per-
plexity attained by the best models reported in table 1. standard bbb yields 258 perplexity after
only one epoch, whereas the model with posterior sharpening is better at 227. we also implemented
it on mnist following blundell et al. (2015), and obtained small but consistent speed ups. lower
perplexities on the id32 task can be achieved by varying the model architecture, which
should be complementary to our work of treating weights as random variables   we are simply
interested in assessing the impact of our method on an existing architecture, rather than absolute
state-of-the-art. see kim et al. (2015); zilly et al. (2016); merity et al. (2016), for a report on recent
advances on this benchmark, where they achieve perplexities of 70.9 on the test set. furthermore
we note that the speed of our na    ve implementation of bayesian id56s was 0.7 times the original
speed and 0.4 times the original speed for posterior sharpening. notably, figure 2 shows the effect
of weight pruning: weights were ordered by their signal-to-noise ratio (|  i|/  i) and removed (set
to zero) in reverse order. we evaluated the validation set perplexity for each proportion of weights
dropped. as can be seen, around 80% of the weights can be removed from the network with little

1https://github.com/tensorflow/models/blob/master/tutorials/id56/ptb/

ptb_word_lm.py

7

table 1: word-level perplexity on the id32 language modelling task (lower is better),
where de indicates that dynamic evaluation was used.

model (medium)
lstm (zaremba et al., 2014)
lstm dropout (zaremba et al., 2014)
variational lstm (tied weights)
(gal & ghahramani, 2016)
variational lstm (tied weights, ms)
(gal & ghahramani, 2016)

val
120.7
86.2
81.8

-

test
114.5
82.1
79.7

79.0

val (de) test (de)

-

79.7

-

-

-

77.1

-

-

bayesian id56 (bid56)
bid56 w/ posterior sharpening

78.8
75.5
    77.8     74.8

73.4
    72.6

70.7
    69.8

figure 2: weight pruning experiment. no signif-
icant loss on performance is observed until prun-
ing more than 80% of weights.

figure 3: id178 gap    hp (eq. (20)) between
reversed and regular id32 test sets   
number of samples.

impact on validation perplexity. additional analysis on the existing patterns of the dropped weights
can be found in the supplementary material a.3.

6.1.1 uncertainty analysis
we used the id32 test set, which is a long sequence of     80k words, and reversed it. thus,
the    reversed    test set    rst few words are:    us with here them see not may we ...    which correspond
to the last words of the standard test set:    ... we may not see them here with us   .
let v be the vocabulary of this task. for a given input sequence x = x1:t and a probabilistic model
p, we de   ne the id178 of x under p, hp[x], by

p(w|x1:i   1) log p(w|x1:i   1)

(19)

hp[x] =     (cid:88)

i=1,...,t

(cid:88)

w   v

t hp[x] = h p[x] , i.e., the per word id178. let x be the standard id32 test set,

let 1
and xrev the reversed one. for a given probabilistic model p, we de   ne the id178 gap    hp by

   hp = h p[xrev]     h p[x].

(20)
since xrev clearly does not come from the training data distribution (reversed english does not look
like proper english), we expect    hp to be positive and large. namely, if we take the per word
id178 of a model as a proxy for the models    certainty (low id178 means the model is con   dent
about its prediction), then the overall certainty of well calibrated models over xrev should be lower
than over x. thus, h p[xrev] > h p[x]. when comparing two distributions, we expect the better
calibrated one to have a larger    hp.
in figure 3, we plotted    hp for the bbb and the baseline dropout lstm model. the bbb model
has a gap of about 0.67 nats/word when taking 10 samples, and slightly below 0.65 when using the

8

figure 4: image captioning results on mscoco development set.

posterior mean. in contrast, the model using mc dropout (gal & ghahramani, 2015) is less well
calibrated and is below 0.58 nats/word. however, when    turning off    dropout (i.e., using the mean
   eld approximation),    hp improves to below 0.62 nats/word.
we note that with the empirical likelihood of the words in the test set with size t (where for each
word w     v , p(w) = (# occurrences of w)
), we get an id178 of 6.33 nats/word. the bbb mean model
has id178 of 4.48 nats/word on the reversed set which is still far below the id178 we get by using
the empirical likelihood distribution.

t

6.2

image id134

we also applied bayes by backprop for id56s to image captioning. our experiments were based
upon the model described in vinyals et al. (2016), where a state-of-the-art pre-trained convolutional
neural network (id98) was used to map an image to a high dimensional space, and this representa-
tion was taken to be the initial state of an lstm. the lstm model was trained to predict the next
word on a sentence conditioned on the image representation and all the previous words in the im-
age caption. we kept the id98 architecture unchanged, and used an lstm trained using bayes by
backprop rather than the traditional lstm with dropout regularisation. as in the case for language
modelling, this work conveniently provides an open source implementation2. we used the same
prior distribution on the weights of the network (18) as we did for the language modelling task, and
searched over the same hyper-parameters.
we used the mscoco (lin et al., 2014) data set and report perplexity, blue-4, and cider scores
on compared to the show and tell model (vinyals et al., 2016), which was the winning entry of the
captioning challenge in 20153. the results are:

model
show and tell
bayes id56

perplexity blue-4 cider

8.3
8.1

28.8
30.2

89.8
96.0

we observe signi   cant improvements in blue and cider, outperforming the dropout baseline
by a large margin. moreover, a random sample of the captions that were different for both the
baseline and bbb is shown in figure 4. besides the clear quantitative improvement, it is useful
to visualise qualitatively the performance of bbb, which indeed generally outperforms the strong
baseline, winning in most cases. as in the case of id32, we chose a performant, open
source model. captioning models that use spatial attention, combined with losses that optimise
cider directly (rather than a surrogate loss as we do) achieve over 100 cider points (lu et al.,
2016; liu et al., 2016).
7 discussion
we have shown how to apply the bayes by backprop (bbb) technique to id56s. we enhanced it
further by introducing the idea of posterior sharpening: a hierarchical posterior on the weights of
neural networks that allows a network to adapt locally to batches of data by a gradient of the model.

2https://github.com/tensorflow/models/tree/master/im2txt
3the winning entry was an ensemble of many models, including some with    ne tuning w.r.t.

model. in this paper, though, we report single model performance.

the image

9

we showed improvements over two open source, widely available models in the language modelling
and image captioning domains. we demonstrated that not only do bbb id56s often have superior
performance to their corresponding baseline model, but are also better regularised and have superior
uncertainty properties in terms of uncertainty on out-of-distribution data. furthermore, bbb id56s
through their uncertainty estimates show signs of knowing what they know, and when they do not, a
critical property for many real world applications such as self-driving cars, healthcare, game playing,
and robotics. everything from our work can be applied on top of other enhancements to id56/lstm
models (and other non-recurrent architectures), and the empirical evidence combined with improve-
ments such as posterior sharpening makes id58 methods look very promising. we are
exploring further research directions and wider adoption of the techniques presented in our work.

acknowledgements

we would like to thank grzegorz swirszcz, daan wierstra, vinod gopal nair, koray kavukcuoglu,
chris shallue, james martens, danilo j. rezende, james kirkpatrick, alex graves, jacob menick,
yori zwols, frederick besse and many others at deepmind for insightful discussions and feedback
on this work.

references
dario amodei, rishita anubhai, eric battenberg, carl case, jared casper, bryan catanzaro, jing-
dong chen, mike chrzanowski, adam coates, greg diamos, et al. deep speech 2: end-to-end
id103 in english and mandarin. arxiv preprint arxiv:1512.02595, 2015.

marcin andrychowicz, misha denil, sergio gomez, matthew w hoffman, david pfau, tom schaul,
and nando de freitas. learning to learn by id119 by id119. in advances in
neural information processing systems, pp. 3981   3989, 2016.

justin bayer and christian osendorfer. learning stochastic recurrent networks. arxiv preprint

arxiv:1411.7610, 2014.

matthew james beal. variational algorithms for approximate bayesian id136. university of

london united kingdom, 2003.

charles blundell, julien cornebise, koray kavukcuoglu, and daan wierstra. weight uncertainty in

neural networks. arxiv preprint arxiv:1505.05424, 2015.

wray l buntine and andreas s weigend. bayesian back-propagation. complex systems, 5(6):

603   643, 1991.

jen-tzung chien and yuan-chu ku. bayesian recurrent neural network for id38.

ieee transactions on neural networks and learning systems, 27(2):361   374, 2016.

junyoung chung, kyle kastner, laurent dinh, kratarth goel, aaron c courville, and yoshua ben-
gio. a recurrent latent variable model for sequential data. in advances in neural information
processing systems, pp. 2980   2988, 2015.

john s denker and yann lecun. transforming neural-net output levels to id203 distributions.

in advances in neural information processing systems, pp. 853   859, 1991.

john duchi, elad hazan, and yoram singer. adaptive subgradient methods for online learning and

stochastic optimization. journal of machine learning research, 12(jul):2121   2159, 2011.

otto fabius and joost r van amersfoort. variational recurrent auto-encoders. arxiv preprint

arxiv:1412.6581, 2014.

yarin gal and zoubin ghahramani. dropout as a bayesian approximation: representing model

uncertainty in deep learning. arxiv preprint arxiv:1506.02142, 2, 2015.

yarin gal and zoubin ghahramani. a theoretically grounded application of dropout in recurrent
neural networks. in advances in neural information processing systems, pp. 1019   1027, 2016.

10

zhe gan, chunyuan li, changyou chen, yunchen pu, qinliang su, and lawrence carin. scal-
able bayesian learning of recurrent neural networks for id38. arxiv preprint
arxiv:1611.08034, 2016.

alex graves. practical variational id136 for neural networks. in advances in neural information

processing systems, pp. 2348   2356, 2011.

alex graves. stochastic id26 through mixture density distributions. arxiv preprint

arxiv:1607.05690, 2016.

leonard hasenclever, thibaut lienart, sebastian vollmer, stefan webb, balaji lakshminarayanan,
charles blundell, and yee whye teh. distributed bayesian learning with stochastic natural-
gradient expectation propagation and the posterior server. arxiv preprint arxiv:1512.09327, 2015.

jos  e miguel hern  andez-lobato and ryan adams. probabilistic id26 for scalable learn-

ing of bayesian neural networks. in icml, pp. 1861   1869, 2015.

geoffrey e hinton and drew van camp. keeping the neural networks simple by minimizing the
description length of the weights. in proceedings of the sixth annual conference on computational
learning theory, pp. 5   13. acm, 1993.

sepp hochreiter and j  urgen schmidhuber. long short-term memory. neural computation, 9(8):

1735   1780, 1997.

sepp hochreiter, j  urgen schmidhuber, et al. simplifying neural nets by discovering    at minima.

advances in neural information processing systems, pp. 529   536, 1995.

rein houthooft, xi chen, yan duan, john schulman, filip de turck, and pieter abbeel. curiosity-
driven exploration in deep id23 via bayesian neural networks. arxiv preprint
arxiv:1605.09674, 2016.

rafal jozefowicz, oriol vinyals, mike schuster, noam shazeer, and yonghui wu. exploring the

limits of id38. arxiv preprint arxiv:1602.02410, 2016.

yoon kim, yacine jernite, david sontag, and alexander m rush. character-aware neural language

models. arxiv preprint arxiv:1508.06615, 2015.

diederik kingma and jimmy ba. adam: a method for stochastic optimization. arxiv preprint

arxiv:1412.6980, 2014.

diederik p kingma and max welling. auto-encoding id58.

arxiv:1312.6114, 2013.

arxiv preprint

diederik p kingma, tim salimans, and max welling. variational dropout and the local reparame-

terization trick. in advances in neural information processing systems, pp. 2575   2583, 2015.

chunyuan li, changyou chen, david carlson, and lawrence carin. preconditioned stochastic
gradient langevin dynamics for deep neural networks. arxiv preprint arxiv:1512.07666, 2015.

ke li and jitendra malik. learning to optimize. arxiv preprint arxiv:1606.01885, 2016.

tsung-yi lin, michael maire, serge belongie, james hays, pietro perona, deva ramanan, piotr
in european

doll  ar, and c lawrence zitnick. microsoft coco: common objects in context.
conference on id161, pp. 740   755. springer, 2014.

zachary c lipton, jianfeng gao, lihong li, xiujun li, faisal ahmed, and li deng. ef   cient ex-
ploration for dialogue policy learning with bbq networks & replay buffer spiking. arxiv preprint
arxiv:1608.05081, 2016.

siqi liu, zhenhai zhu, ning ye, sergio guadarrama, and kevin murphy. optimization of image

description metrics using id189. arxiv preprint arxiv:1612.00370, 2016.

jiasen lu, caiming xiong, devi parikh, and richard socher. knowing when to look: adaptive

attention via a visual sentinel for image captioning. arxiv preprint arxiv:1612.01887, 2016.

11

david jc mackay. probable networks and plausible predictionsa review of practical bayesian meth-
ods for supervised neural networks. network: computation in neural systems, 6(3):469   505,
1995.

maren mahsereci and philipp hennig. probabilistic line searches for stochastic optimization. in

advances in neural information processing systems, pp. 181   189, 2015.

mitchell p marcus, mary ann marcinkiewicz, and beatrice santorini. building a large annotated

corpus of english: the id32. computational linguistics, 19(2):313   330, 1993.

stephen merity, caiming xiong, james bradbury, and richard socher. pointer sentinel mixture

models. arxiv preprint arxiv:1609.07843, 2016.

tomas mikolov, martin kara     at, lukas burget, jan cernock`y, and sanjeev khudanpur. recurrent

neural network based language model. in interspeech, volume 2, pp. 3, 2010.

derrick t mirikitani and nikolay nikolaev. recursive bayesian recurrent neural networks for time-

series modeling. ieee transactions on neural networks, 21(2):262   274, 2010.

radford m neal. bayesian learning for neural networks, volume 118. springer science & business

media, 2012.

rajesh ranganath, dustin tran, and david blei. hierarchical variational models. in proceedings of
the 33rd international conference on machine learning, volume 48 of proceedings of machine
learning research, pp. 324   333, 2016.

danilo jimenez rezende, shakir mohamed, and daan wierstra. stochastic id26 and
approximate id136 in deep generative models. in proceedings of the 31st international con-
ference on machine learning, pp. 1278   1286, 2014.

daniel soudry, itay hubara, and ron meir. expectation id26: parameter-free train-
ing of multilayer neural networks with continuous or discrete weights. in advances in neural
information processing systems, pp. 963   971, 2014.

nitish srivastava, geoffrey e hinton, alex krizhevsky, ilya sutskever, and ruslan salakhutdinov.
dropout: a simple way to prevent neural networks from over   tting. journal of machine learning
research, 15(1):1929   1958, 2014.

richard e turner and maneesh sahani. two problems with variational expectation maximisation

for time-series models. bayesian time series models, pp. 115   138, 2011.

oriol vinyals, alexander toshev, samy bengio, and dumitru erhan. show and tell: lessons learned
from the 2015 mscoco image captioning challenge. ieee transactions on pattern analysis and
machine intelligence, 2016.

martin j wainwright, michael i jordan, et al. id114, exponential families, and variational

id136. foundations and trends r(cid:13) in machine learning, 1(1   2):1   305, 2008.

sida i wang and christopher d manning. fast dropout training. in icml (2), pp. 118   126, 2013.

max welling and yee w teh. bayesian learning via stochastic gradient langevin dynamics. in
proceedings of the 28th international conference on machine learning (icml-11), pp. 681   688,
2011.

yonghui wu, mike schuster, zhifeng chen, quoc v le, mohammad norouzi, wolfgang macherey,
maxim krikun, yuan cao, qin gao, klaus macherey, et al. google   s neural machine trans-
arxiv preprint
lation system: bridging the gap between human and machine translation.
arxiv:1609.08144, 2016.

wojciech zaremba, ilya sutskever, and oriol vinyals. recurrent neural network id173.

arxiv preprint arxiv:1409.2329, 2014.

julian georg zilly, rupesh kumar srivastava, jan koutn    k, and j  urgen schmidhuber. recurrent

id199. arxiv preprint arxiv:1607.03474, 2016.

12

a supplementary material

a.1 bayes by backprop algorithm

algorithm 2 shows the bayes by backprop monte carlo procedure from section 2 for minimising
the variational free energy from eq. (1) with respect to the mean and standard deviation parameters
of the posterior q(  ).

algorithm 2 bayes by backprop
sample       n (0, i),       rd.
set network parameters to    =    +    .
do forward propagation and id26 as normal.
let g be the gradient w.r.t.    from id26.
let gkl
respectively.
update    according to the gradient g + gkl
update    according to the gradient (g + gkl

   .
   + gkl
  

   .
)  + gkl

   , gkl

, gkl

  

  

be the gradients of log n (  |  ,   2)     log p(  ) with respect to   ,    and   

a.2 lstm equations

the core of an id56, f, is a neural network that maps the id56 state at step t, st and an input
observation xt to a new id56 state st+1, f : (st, xt) (cid:55)    st+1.
an lstm core hochreiter & schmidhuber (1997) has a state st = (ct, ht) where c is an internal
core state and h is the exposed state. intermediate gates modulate the effect of the inputs on the
outputs, namely the input gate it, forget gate ft and output gate ot. the relationship between the
inputs, outputs and internal gates of an lstm cell (without peephole connections) are as follows:

it =   (wi[xt, ht   1]t + bi),
ft =   (wf [xt, ht   1]t + bf ),
ct = ftct   1 + it tanh(wc[xt, ht   1] + bc),
ot =   (wo[xt, ht   1]t + bo),
ht = ot tanh(ct),

where wi (bi), wf (bf ), wc (bc) and wo (bo) are the weights (biases) affecting the input gate, forget
gate, cell update, and output gate respectively.

a.3 weight pruning

as discussed in section 6.1, for the id32 task, we have taken the converged model an
performed weight pruning on the parameters of the network. weights were ordered by their signal-
to-noise ratio (|  i|/  i) and removed (set to zero) in reverse order. it was observed that around 80%
in
of the weights can be removed from the network with little impact on validation perplexity.
figure 5, we show the patterns of the weights dropped for one of the lstm cells from the model.

a.4 additional captioning examples

13

figure 5: pruning patterns for one lstm cell (with 650 untis) from converged model with 80% of
total weights dropped. a white dot indicates that particular parameter was dropped. in the middle
column, a horizontal white line means that row was set to zero. finally, the last column indicates
the total number of weights removed for each row.

figure 6: additional captions from ms coco validation set.

14

