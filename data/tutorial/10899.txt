5
1
0
2

 

p
e
s
2

 

 
 
]

v
c
.
s
c
[
 
 

2
v
6
7
5
6
0

.

8
0
5
1
:
v
i
x
r
a

a neural algorithm of artistic style

leon a. gatys,1,2,3    alexander s. ecker,1,2,4,5 matthias bethge1,2,4

1werner reichardt centre for integrative neuroscience

and institute of theoretical physics, university of t  ubingen, germany
2bernstein center for computational neuroscience, t  ubingen, germany
3graduate school for neural information processing, t  ubingen, germany

4max planck institute for biological cybernetics, t  ubingen, germany

5department of neuroscience, baylor college of medicine, houston, tx, usa
   to whom correspondence should be addressed; e-mail: leon.gatys@bethgelab.org

in    ne art, especially painting, humans have mastered the skill to create unique

visual experiences through composing a complex interplay between the con-

tent and style of an image. thus far the algorithmic basis of this process is

unknown and there exists no arti   cial system with similar capabilities. how-

ever, in other key areas of visual perception such as object and face recognition

near-human performance was recently demonstrated by a class of biologically

inspired vision models called deep neural networks.1,2 here we introduce an

arti   cial system based on a deep neural network that creates artistic images

of high perceptual quality. the system uses neural representations to sepa-

rate and recombine content and style of arbitrary images, providing a neural

algorithm for the creation of artistic images. moreover, in light of the strik-

ing similarities between performance-optimised arti   cial neural networks and

biological vision,3   7 our work offers a path forward to an algorithmic under-

standing of how humans create and perceive artistic imagery.

1

the class of deep neural networks that are most powerful in image processing tasks are

called convolutional neural networks. convolutional neural networks consist of layers of

small computational units that process visual information hierarchically in a feed-forward man-

ner (fig 1). each layer of units can be understood as a collection of image    lters, each of which

extracts a certain feature from the input image. thus, the output of a given layer consists of

so-called feature maps: differently    ltered versions of the input image.

when convolutional neural networks are trained on object recognition, they develop a

representation of the image that makes object information increasingly explicit along the pro-

cessing hierarchy.8 therefore, along the processing hierarchy of the network, the input image

is transformed into representations that increasingly care about the actual content of the im-

age compared to its detailed pixel values. we can directly visualise the information each layer

contains about the input image by reconstructing the image only from the feature maps in that

layer9 (fig 1, content reconstructions, see methods for details on how to reconstruct the im-

age). higher layers in the network capture the high-level content in terms of objects and their

arrangement in the input image but do not constrain the exact pixel values of the reconstruc-

tion. (fig 1, content reconstructions d,e). in contrast, reconstructions from the lower layers

simply reproduce the exact pixel values of the original image (fig 1, content reconstructions

a,b,c). we therefore refer to the feature responses in higher layers of the network as the content

representation.

to obtain a representation of the style of an input image, we use a feature space originally

designed to capture texture information.8 this feature space is built on top of the    lter responses

in each layer of the network. it consists of the correlations between the different    lter responses

over the spatial extent of the feature maps (see methods for details). by including the feature

correlations of multiple layers, we obtain a stationary, multi-scale representation of the input

image, which captures its texture information but not the global arrangement.

2

figure 1: convolutional neural network (id98). a given input image is represented as a set
of    ltered images at each processing stage in the id98. while the number of different    lters
increases along the processing hierarchy, the size of the    ltered images is reduced by some
downsampling mechanism (e.g. max-pooling) leading to a decrease in the total number of
units per layer of the network. content reconstructions. we can visualise the information
at different processing stages in the id98 by reconstructing the input image from only know-
ing the network   s responses in a particular layer. we reconstruct the input image from from
layers    conv1 1    (a),    conv2 1    (b),    conv3 1    (c),    conv4 1    (d) and    conv5 1    (e) of the orig-
inal vgg-network. we    nd that reconstruction from lower layers is almost perfect (a,b,c). in
higher layers of the network, detailed pixel information is lost while the high-level content of the
image is preserved (d,e). style reconstructions. on top of the original id98 representations
we built a new feature space that captures the style of an input image. the style representation
computes correlations between the different features in different layers of the id98. we recon-
struct the style of the input image from style representations built on different subsets of id98
layers (    conv1 1    (a),    conv1 1    and    conv2 1    (b),    conv1 1   ,    conv2 1    and    conv3 1    (c),
   conv1 1   ,    conv2 1   ,    conv3 1    and    conv4 1    (d),    conv1 1   ,    conv2 1   ,    conv3 1   ,    conv4 1   
and    conv5 1    (e)). this creates images that match the style of a given image on an increasing
scale while discarding information of the global arrangement of the scene.

3

again, we can visualise the information captured by these style feature spaces built on

different layers of the network by constructing an image that matches the style representation

of a given input image (fig 1, style reconstructions).10, 11 indeed reconstructions from the style

features produce texturised versions of the input image that capture its general appearance in

terms of colour and localised structures. moreover, the size and complexity of local image

structures from the input image increases along the hierarchy, a result that can be explained

by the increasing receptive    eld sizes and feature complexity. we refer to this multi-scale

representation as style representation.

the key    nding of this paper is that the representations of content and style in the convo-

lutional neural network are separable. that is, we can manipulate both representations inde-

pendently to produce new, perceptually meaningful images. to demonstrate this    nding, we

generate images that mix the content and style representation from two different source images.

in particular, we match the content representation of a photograph depicting the    neckarfront   

in t  ubingen, germany and the style representations of several well-known artworks taken from

different periods of art (fig 2).

the images are synthesised by    nding an image that simultaneously matches the content

representation of the photograph and the style representation of the respective piece of art (see

methods for details). while the global arrangement of the original photograph is preserved,

the colours and local structures that compose the global scenery are provided by the artwork.

effectively, this renders the photograph in the style of the artwork, such that the appearance of

the synthesised image resembles the work of art, even though it shows the same content as the

photograph.

as outlined above, the style representation is a multi-scale representation that includes mul-

tiple layers of the neural network. in the images we have shown in fig 2, the style representation

included layers from the whole network hierarchy. style can also be de   ned more locally by

4

figure 2: images that combine the content of a photograph with the style of several well-known
artworks. the images were created by    nding an image that simultaneously matches the content
representation of the photograph and the style representation of the artwork (see methods). the
original photograph depicting the neckarfront in t  ubingen, germany, is shown in a (photo:
andreas praefcke). the painting that provided the style for the respective generated image
is shown in the bottom left corner of each panel. b the shipwreck of the minotaur by j.m.w.
turner, 1805. c the starry night by vincent van gogh, 1889. d der schrei by edvard munch,
1893. e femme nue assise by pablo picasso, 1910. f composition vii by wassily kandinsky,
1913.

5

including only a smaller number of lower layers, leading to different visual experiences (fig 3,

along the rows). when matching the style representations up to higher layers in the network,

local images structures are matched on an increasingly large scale, leading to a smoother and

more continuous visual experience. thus, the visually most appealing images are usually cre-

ated by matching the style representation up to the highest layers in the network (fig 3, last

row).

of course, image content and style cannot be completely disentangled. when synthesising

an image that combines the content of one image with the style of another, there usually does

not exist an image that perfectly matches both constraints at the same time. however, the

id168 we minimise during image synthesis contains two terms for content and style

respectively, that are well separated (see methods). we can therefore smoothly regulate the

emphasis on either reconstructing the content or the style (fig 3, along the columns). a strong

emphasis on style will result in images that match the appearance of the artwork, effectively

giving a texturised version of it, but hardly show any of the photograph   s content (fig 3,    rst

column). when placing strong emphasis on content, one can clearly identify the photograph,

but the style of the painting is not as well-matched (fig 3, last column). for a speci   c pair of

source images one can adjust the trade-off between content and style to create visually appealing

images.

here we present an arti   cial neural system that achieves a separation of image content from

style, thus allowing to recast the content of one image in the style of any other image. we

demonstrate this by creating new, artistic images that combine the style of several well-known

paintings with the content of an arbitrarily chosen photograph.

in particular, we derive the

neural representations for the content and style of an image from the feature responses of high-

performing deep neural networks trained on object recognition. to our knowledge this is the

   rst demonstration of image features separating content from style in whole natural images.

6

figure 3: detailed results for the style of the painting composition vii by wassily kandinsky.
the rows show the result of matching the style representation of increasing subsets of the id98
layers (see methods). we    nd that the local image structures captured by the style represen-
tation increase in size and complexity when including style features from higher layers of the
network. this can be explained by the increasing receptive    eld sizes and feature complex-
ity along the network   s processing hierarchy. the columns show different relative weightings
between the content and style reconstruction. the number above each column indicates the
ratio   /   between the emphasis on matching the content of the photograph and the style of the
artwork (see methods).

7

previous work on separating content from style was evaluated on sensory inputs of much lesser

complexity, such as characters in different handwriting or images of faces or small    gures in

different poses.12, 13

in our demonstration, we render a given photograph in the style of a range of well-known

artworks. this problem is usually approached in a branch of id161 called non-

photorealistic rendering (for recent review see14). conceptually most closely related are meth-

ods using texture transfer to achieve artistic style transfer.15   19 however, these previous ap-

proaches mainly rely on non-parametric techniques to directly manipulate the pixel representa-

tion of an image. in contrast, by using deep neural networks trained on object recognition, we

carry out manipulations in feature spaces that explicitly represent the high level content of an

image.

features from deep neural networks trained on object recognition have been previously

used for style recognition in order to classify artworks according to the period in which they

were created.20 there, classi   ers are trained on top of the raw network activations, which we

call content representations. we conjecture that a transformation into a stationary feature space

such as our style representation might achieve even better performance in style classi   cation.

in general, our method of synthesising images that mix content and style from different

sources, provides a new, fascinating tool to study the perception and neural representation of

art, style and content-independent image appearance in general. we can design novel stimuli

that introduce two independent, perceptually meaningful sources of variation: the appearance

and the content of an image. we envision that this will be useful for a wide range of experimen-

tal studies concerning visual perception ranging from psychophysics over functional imaging

to even electrophysiological neural recordings. in fact, our work offers an algorithmic under-

standing of how neural representations can independently capture the content of an image and

the style in which it is presented. importantly, the mathematical form of our style representa-

8

tions generates a clear, testable hypothesis about the representation of image appearance down

to the single neuron level. the style representations simply compute the correlations between

different types of neurons in the network. extracting correlations between neurons is a bio-

logically plausible computation that is, for example, implemented by so-called complex cells

in the primary visual system (v1).21 our results suggest that performing a complex-cell like

computation at different processing stages along the ventral stream would be a possible way to

obtain a content-independent representation of the appearance of a visual input.

all in all it is truly fascinating that a neural system, which is trained to perform one of the

core computational tasks of biological vision, automatically learns image representations that

allow the separation of image content from style. the explanation could be that when learning

object recognition, the network has to become invariant to all image variation that preserves

object identity. representations that factorise the variation in the content of an image and the

variation in its appearance would be extremely practical for this task. thus, our ability to

abstract content from style and therefore our ability to create and enjoy art might be primarily a

preeminent signature of the powerful id136 capabilities of our visual system.

methods

the results presented in the main text were generated on the basis of the vgg-network,22

a convolutional neural network that rivals human performance on a common visual object

recognition benchmark task23 and was introduced and extensively described in.22 we used the

feature space provided by the 16 convolutional and 5 pooling layers of the 19 layer vgg-

network. we do not use any of the fully connected layers.the model is publicly available and

can be explored in the caffe-framework.24 for image synthesis we found that replacing the

max-pooling operation by average pooling improves the gradient    ow and one obtains slightly

more appealing results, which is why the images shown were generated with average pooling.

9

generally each layer in the network de   nes a non-linear    lter bank whose complexity in-

creases with the position of the layer in the network. hence a given input image (cid:126)x is encoded

in each layer of the id98 by the    lter responses to that image. a layer with nl distinct    lters

has nl feature maps each of size ml, where ml is the height times the width of the feature map.
so the responses in a layer l can be stored in a matrix f l     rnl  ml where f l
ij is the activation
of the ith    lter at position j in layer l. to visualise the image information that is encoded at

different layers of the hierarchy (fig 1, content reconstructions) we perform id119

on a white noise image to    nd another image that matches the feature responses of the original

image. so let (cid:126)p and (cid:126)x be the original image and the image that is generated and p l and f l their

respective feature representation in layer l. we then de   ne the squared-error loss between the

two feature representations

the derivative of this loss with respect to the activations in layer l equals

lcontent((cid:126)p, (cid:126)x, l) =

(cid:0)f l

ij     p l

ij

(cid:1)2 .

1
2

(cid:88)
(cid:40)(cid:0)f l     p l(cid:1)

i,j

   lcontent

   f l
ij

=

0

ij

if f l
if f l

ij > 0
ij < 0 .

(1)

(2)

from which the gradient with respect to the image (cid:126)x can be computed using standard error

back-propagation. thus we can change the initially random image (cid:126)x until it generates the same

response in a certain layer of the id98 as the original image (cid:126)p. the    ve content reconstructions

in fig 1 are from layers    conv1 1    (a),    conv2 1    (b),    conv3 1    (c),    conv4 1    (d) and    conv5 1   

(e) of the original vgg-network.

on top of the id98 responses in each layer of the network we built a style representation

that computes the correlations between the different    lter responses, where the expectation is

taken over the spatial extend of the input image. these feature correlations are given by the
gram matrix gl     rnl  nl, where gl

ij is the inner product between the vectorised feature map

10

i and j in layer l:

(cid:88)

k

gl

ij =

f l
ikf l

jk.

(3)

to generate a texture that matches the style of a given image (fig 1, style reconstructions),

we use id119 from a white noise image to    nd another image that matches the style

representation of the original image. this is done by minimising the mean-squared distance

between the entries of the gram matrix from the original image and the gram matrix of the

image to be generated. so let (cid:126)a and (cid:126)x be the original image and the image that is generated and

al and gl their respective style representations in layer l. the contribution of that layer to the

total loss is then

and the total loss is

el =

1
l m 2
4n 2
l

(cid:88)

i,j

(cid:1)2

(cid:0)gl
ij     al
l(cid:88)

wlel

ij

lstyle((cid:126)a, (cid:126)x) =

(4)

(5)

where wl are weighting factors of the contribution of each layer to the total loss (see below for

l=0

speci   c values of wl in our results). the derivative of el with respect to the activations in layer

l can be computed analytically:

(cid:40) 1

   el
   f l
ij

=

l m 2
l

n 2
0

(cid:0)(f l)t(cid:0)gl     al(cid:1)(cid:1)

ji

if f l
if f l

ij > 0
ij < 0 .

(6)

the gradients of el with respect to the activations in lower layers of the network can be readily

computed using standard error back-propagation. the    ve style reconstructions in fig 1 were

generated by matching the style representations on layer    conv1 1    (a),    conv1 1    and    conv2 1   

(b),    conv1 1   ,    conv2 1    and    conv3 1    (c),    conv1 1   ,    conv2 1   ,    conv3 1    and    conv4 1    (d),

   conv1 1   ,    conv2 1   ,    conv3 1   ,    conv4 1    and    conv5 1    (e).

to generate the images that mix the content of a photograph with the style of a painting

(fig 2) we jointly minimise the distance of a white noise image from the content representation

11

of the photograph in one layer of the network and the style representation of the painting in a

number of layers of the id98. so let (cid:126)p be the photograph and (cid:126)a be the artwork. the id168

we minimise is

ltotal((cid:126)p, (cid:126)a, (cid:126)x) =   lcontent((cid:126)p, (cid:126)x) +   lstyle((cid:126)a, (cid:126)x)

(7)

where    and    are the weighting factors for content and style reconstruction respectively. for

the images shown in fig 2 we matched the content representation on layer    conv4 2    and the

style representations on layers    conv1 1   ,    conv2 1   ,    conv3 1   ,    conv4 1    and    conv5 1    (wl =
1/5 in those layers, wl = 0 in all other layers) . the ratio   /   was either 1  10   3 (fig 2 b,c,d)
or 1    10   4 (fig 2 e,f). fig 3 shows results for different relative weightings of the content and
style reconstruction loss (along the columns) and for matching the style representations only

on layer    conv1 1    (a),    conv1 1    and    conv2 1    (b),    conv1 1   ,    conv2 1    and    conv3 1    (c),

   conv1 1   ,    conv2 1   ,    conv3 1    and    conv4 1    (d),    conv1 1   ,    conv2 1   ,    conv3 1   ,    conv4 1   

and    conv5 1    (e). the factor wl was always equal to one divided by the number of active layers

with a non-zero loss-weight wl.

acknowledgments this work was funded by the german national academic foundation

(l.a.g.), the bernstein center for computational neuroscience (fkz 01gq1002) and the ger-

man excellency initiative through the centre for integrative neuroscience t  ubingen (exc307)(m.b.,

a.s.e, l.a.g.)

references and notes

1. krizhevsky, a., sutskever, i. & hinton, g. e. id163 classi   cation with deep convolu-

tional neural networks. in advances in neural information processing systems, 1097   1105

(2012). url http://papers.nips.cc/paper/4824-id163.

12

2. taigman, y., yang, m., ranzato, m. & wolf, l. deepface: closing the gap to human-level

performance in face veri   cation.

in id161 and pattern recognition (cvpr),

2014 ieee conference on, 1701   1708 (ieee, 2014). url http://ieeexplore.

ieee.org/xpls/abs_all.jsp?arnumber=6909616.

3. g  uc  l  u, u. & gerven, m. a. j. v. deep neural networks reveal a gradient in the com-

plexity of neural representations across the ventral stream. the journal of neuroscience

35, 10005   10014 (2015). url http://www.jneurosci.org/content/35/27/

10005.

4. yamins, d. l. k. et al. performance-optimized id187 predict neural re-

sponses in higher visual cortex. proceedings of the national academy of sciences

201403112 (2014). url http://www.pnas.org/content/early/2014/05/

08/1403112111.

5. cadieu, c. f. et al. deep neural networks rival the representation of primate it cortex

for core visual object recognition. plos comput biol 10, e1003963 (2014). url http:

//dx.doi.org/10.1371/journal.pcbi.1003963.

6. k  ummerer, m., theis, l. & bethge, m. deep gaze i: boosting saliency prediction

with feature maps trained on id163.

in iclr workshop (2015). url /media/

publications/1411.1045v4.pdf.

7. khaligh-razavi, s.-m. & kriegeskorte, n. deep supervised, but not unsupervised, mod-

els may explain it cortical representation. plos comput biol 10, e1003915 (2014). url

http://dx.doi.org/10.1371/journal.pcbi.1003915.

13

8. gatys, l. a., ecker, a. s. & bethge, m. texture synthesis and the controlled generation of

natural stimuli using convolutional neural networks. arxiv:1505.07376 [cs, q-bio] (2015).

url http://arxiv.org/abs/1505.07376. arxiv: 1505.07376.

9. mahendran, a. & vedaldi, a. understanding deep image representations by inverting

them. arxiv:1412.0035 [cs] (2014). url http://arxiv.org/abs/1412.0035.

arxiv: 1412.0035.

10. heeger, d. j. & bergen, j. r. pyramid-based texture analysis/synthesis.

in pro-

ceedings of the 22nd annual conference on computer graphics and interactive tech-

niques, siggraph    95, 229   238 (acm, new york, ny, usa, 1995). url http:

//doi.acm.org/10.1145/218380.218446.

11. portilla, j. & simoncelli, e. p. a parametric texture model based on joint statis-

tics of complex wavelet coef   cients.

international journal of id161

40, 49   70 (2000). url http://link.springer.com/article/10.1023/a%

3a1026553619983.

12. tenenbaum, j. b. & freeman, w. t. separating style and content with bilinear models. neu-

ral computation 12, 1247   1283 (2000). url http://www.mitpressjournals.

org/doi/abs/10.1162/089976600300015349.

13. elgammal, a. & lee, c.-s. separating style and content on a nonlinear manifold.

in

id161 and pattern recognition, 2004. cvpr 2004. proceedings of the 2004

ieee computer society conference on, vol. 1, i   478 (ieee, 2004). url http://

ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1315070.

14. kyprianidis, j. e., collomosse, j., wang, t. & isenberg, t. state of the    art   : a taxon-

omy of artistic stylization techniques for images and video. visualization and computer

14

graphics, ieee transactions on 19, 866   885 (2013). url http://ieeexplore.

ieee.org/xpls/abs_all.jsp?arnumber=6243138.

15. hertzmann, a., jacobs, c. e., oliver, n., curless, b. & salesin, d. h. image analogies.

in proceedings of the 28th annual conference on computer graphics and interactive tech-

niques, 327   340 (acm, 2001). url http://dl.acm.org/citation.cfm?id=

383295.

16. ashikhmin, n. fast texture transfer. ieee computer graphics and applications 23, 38   43

(2003).

17. efros, a. a. & freeman, w. t. image quilting for texture synthesis and transfer. in pro-

ceedings of the 28th annual conference on computer graphics and interactive techniques,

341   346 (acm, 2001). url http://dl.acm.org/citation.cfm?id=383296.

18. lee, h., seo, s., ryoo, s. & yoon, k. directional texture transfer. in proceedings of the

8th international symposium on non-photorealistic animation and rendering, npar    10,

43   48 (acm, new york, ny, usa, 2010). url http://doi.acm.org/10.1145/

1809939.1809945.

19. xie, x., tian, f. & seah, h. s. feature guided texture synthesis (fgts) for artistic style

transfer. in proceedings of the 2nd international conference on digital interactive media

in entertainment and arts, dimea    07, 44   49 (acm, new york, ny, usa, 2007). url

http://doi.acm.org/10.1145/1306813.1306830.

20. karayev, s. et al. recognizing image style. arxiv preprint arxiv:1311.3715 (2013). url

http://arxiv.org/abs/1311.3715.

15

21. adelson, e. h. & bergen, j. r. spatiotemporal energy models for the perception of motion.

josa a 2, 284   299 (1985). url http://www.opticsinfobase.org/josaa/

fulltext.cfm?uri=josaa-2-2-284.

22. simonyan, k. & zisserman, a. very deep convolutional networks for large-scale image

recognition. arxiv:1409.1556 [cs] (2014). url http://arxiv.org/abs/1409.

1556. arxiv: 1409.1556.

23. russakovsky, o. et al.

id163 large scale visual recognition challenge.

arxiv:1409.0575 [cs] (2014). url http://arxiv.org/abs/1409.0575. arxiv:

1409.0575.

24. jia, y. et al. caffe: convolutional architecture for fast feature embedding. in proceedings

of the acm international conference on multimedia, 675   678 (acm, 2014). url http:

//dl.acm.org/citation.cfm?id=2654889.

16

