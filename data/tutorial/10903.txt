workshop track - iclr 2016

document context language models

yangfeng ji1, trevor cohn2, lingpeng kong3, chris dyer3 & jacob eisenstein1
1 school of interactive computing, georgia institute of technology
2 department of computing and information systems, university of melbourne
3 school of computer science, carnegie mellon university

abstract

text documents are structured on multiple levels of detail: individual words are re-
lated by syntax, and larger units of text are related by discourse structure. existing
language models generally fail to account for discourse structure, but it is crucial
if we are to have language models that reward coherence and generate coherent
texts. we present and empirically evaluate a set of multi-level recurrent neural net-
work language models, called document-context language models (dclms),
which incorporate contextual information both within and beyond the sentence.
in comparison with sentence-level recurrent neural network language models, the
dclms obtain slightly better predictive likelihoods, and considerably better as-
sessments of document coherence.

1

introduction

statistical language models are essential components of natural language processing systems, such
as machine translation (koehn, 2009), automatic id103 (jurafsky & martin, 2000),
text generation (sordoni et al., 2015) and information retrieval (manning et al., 2008). language
models estimate the id203 of a word for a given context. in conventional language models,
context is represented by id165s, so these models condition on a    xed number of preceding words.
recurrent neural network language models (id56lms; mikolov et al., 2010) use a dense vector
representation to summarize context across all preceding words within the same sentence. but
context operates on multiple levels of detail: on the syntactic level, a word   s immediate neighbors
are most predictive; but on the level of discourse and topic, all words in the document lend contextual
information.
recent research has developed a variety of ways to incorporate document-level contextual informa-
tion. for example, both mikolov & zweig (2012) and le & mikolov (2014) use topic information
extracted from the entire document to help predict words in each sentence; lin et al. (2015) propose
to construct contextual information by predicting the bag-of-words representation of the previous
sentence with a separate model; wang & cho (2015) build a bag-of-words context from the pre-
vious sentence and integrate it into the long short-term memory (lstm) generating the current
sentence. these models are all hybrid architectures in that they are recurrent at the sentence level,
but use a different architecture to summarize the context outside the sentence.
in this paper, we explore multi-level recurrent architectures for combining local and global infor-
mation in id38. the simplest such model would be to train a single id56, ignoring
sentence boundaries: as shown in figure 1, the last hidden state from the previous sentence t     1 is
used to initialize the    rst hidden state in sentence t. in such an architecture, the length of the id56
is equal to the number of tokens in the document; in typical genres such as news texts, this means
training id56s from sequences of several hundred tokens, which introduces two problems:

information decay in a sentence with thirty tokens (not unusual in news text), the contextual in-
formation from the previous sentence must be propagated through the recurrent dynamics
thirty times before it can reach the last token of the current sentence. meaningful document-
level information is unlikely to survive such a long pipeline.

6
1
0
2

 

b
e
f
1
2

 

 
 
]
l
c
.
s
c
[
 
 

4
v
2
6
9
3
0

.

1
1
5
1
:
v
i
x
r
a

learning it

is notoriously dif   cult

involve many time
steps (bengio et al., 1994). in the case of an id56 trained on an entire document, back-
propagation would have to run over hundreds of steps, posing severe numerical challenges.

to train recurrent architectures that

1

workshop track - iclr 2016

figure 1: a fragment of document-level recurrent neural network language model (did56lm). it is
also an extension of sentence-level id56lm to the document level by ignoring sentence boundaries.

in this paper, we use multi-level recurrent structures to solve both of these problems, thereby suc-
cessfully ef   ciently leveraging document-level context in id38. we present several
variant document-context language models (dclms), and evaluate them on predictive likelihood
and their ability to capture document coherence.

2 modeling framework

the core modeling idea of this work is to integrate contextual information from the id56 language
model of the previous sentence into the language model of the current sentence. we present three
alternative models, each with various practical or theoretical merits, and then evaluate them in sec-
tion 4.

2.1 recurrent neural network language models

we start from a recurrent neural network language model (id56lm) to explain some necessary terms.
given a sentence {xn}n

n=1, a recurrent neural network language model is de   ned as

hn =g (hn   1, xn)
yn =softmax (wohn + b) ,

(1)
(2)
where xn     rk is the distributed representation of the n-th word, hn     rh is the corresponding
hidden state computed from the word representation and the previous hidden state hn   1, and b
is the bias term. k and h are the input and hidden dimension respectively. as in the original
id56lm (mikolov et al., 2010), yn is a prediction of the (n + 1)-th word in the sequence.
the transition function g (  ) could be any nonlinear function used in neural networks, such as the
elementwise sigmoid function, or more complex recurrent functions such as the lstm (hochreiter
& schmidhuber, 1997) or gru (chung et al., 2014). in this work, we use lstm, as it consistently
gives the best performance in our experiments. by stacking two lstm together, we are able obtain
a even more powerful transition function, called multi-layer lstm (sutskever et al., 2014). in a
multi-layer lstm, the hidden state from a lower-layer lstm cell is used as the input to the upper-
layer, and the hidden state from the    nal-layer is used for prediction. in our following models, we
   x the number of layers as two.
in the rest of this section, we will consider different ways to employ the contextual information for
document-level id38. all models obtain the contextual representation from the hidden
states of the previous sentence, but they use this information in different ways.

2.2 model i: context-to-context dclm

the underlying assumption of this work is that contextual information from previous sentences
needs to be able to    short-circuit    the standard id56, so as to more directly impact the generation
of words across longer spans of text. we    rst consider the relevant contextual information to be the
   nal hidden representation from the previous sentence t     1, so that,

(3)
where m is the length of sentence t     1. we then create additional paths for this information to
impact each hidden representation in the current sentence t. writing xt,n for the word representation
of the n-th word in the t-th sentence, we have,

ct   1 = ht   1,m

ht,n =g   (ht,n   1, s (xt,n, ct   1))

(4)

2

yt   1,1xt   1,1yt   1,2xt   1,2yt   1,m   1xt   1,m   1yt   1,mxt   1,mxt,1yt,1xt,2xt,n   1xt,nyt,2yt,n   1yt,nworkshop track - iclr 2016

(a) ccdclm

(b) codclm

figure 2: context-to-context and context-to-output dclms

where g   (  ) is the activation function parameterized by    and s (  ) is a function that combines the
context vector with the input xt,n for the hidden state. in future work we may consider a variety of
forms for this function, but here we simply concatenate the representations,

s (xt,n, ct   1) = [xt,n, ct   1].

(5)
the emission id203 for yt,n is then computed from ht,n as in the standard id56lm (equa-
tion 2). the underlying assumption of this model is that contextual information should impact the
generation of each word in the current sentence. the model therefore introduces computational
   short-circuits    for cross-sentence information, as illustrated in figure 2(a). because information
   ows from one hidden vector to another, we call this the context-to-context document context
language model, abbreviated ccdclm.
with this speci   c architecture, the number of parameters is h(16h + 3k + 6) + v (h + k + 1),
where h is the size of the hidden representation, k is the size of the word representation, and v is
the vocabulary size. the constant factors come with the weight matrices within a two-layer lstm
unit. this is in the same complexity class as the standard id56lm. special handling is necessary
for the    rst sentence of the document. inspired by the idea of sentence-level id38, we
introduce a dummy contextual representation c0 as a start symbol for a document. this is another
parameter to be learned jointly with the other parameters in this model.
the training procedure of ccdclm is similar to a conventional id56lm: we move from left to
right through the document and compute a softmax loss on each output yt,n. we then backpropagate
this loss through the entire sequences.

2.2.1 model ii: context-to-output dclm

rather than incorporating the document context into the recurrent de   nition of the hidden state, we
can push it directly to the output, as illustrated in figure 2(b). let ht,n be the hidden state from a
conventional id56lm of sentence t,

ht,n = g   (ht,n   1, xt,n) .

(6)

then, the context vector ct   1 is directly used in the output layer as

yt,n     softmax (whht,n + wcct   1 + b)

(7)
where ct   1 is de   ned in equation 3. because the document context impacts the output directly, we
call this model the context-to-output dclm (codclm). the modi   cation on the model archi-
tecture from ccdclm to codclm leads to a notable change on the number of parameters. the
total number of parameters of codclm is h(13h + 3k + 6) + v (2h + k + 1). the difference
of the parameter numbers between these codclm and ccdclm is v h     3h 2. recall that v is
the vocabulary size and h is the size of latent representation, in most cases we have v     104 and
h     102. therefore v (cid:29) h in all reasonable cases, and codclm includes more parameters than
ccdclm in general.
while the codclm has more parameters that must be learned, it has a potentially important com-
putational advantage. by shifting ct   1 from hidden layer to output layer, the relationship of any two
hidden vectors ht and ht(cid:48) from different sentences is decoupled, so that each can be computed in
isolation. in a guided language generation scenario such as machine translation or speech recogni-
tion     the most common use case of neural language models     this means that decoding decisions
are only pairwise dependent across sentences. this is in contrast with the ccdclm, where the

3

yt   1,1xt   1,1yt   1,2xt   1,2yt   1,m   1xt   1,m   1yt   1,mxt   1,mxt,1yt,1xt,2xt,n   1xt,nyt,2yt,n   1yt,nyt   1,1xt   1,1yt   1,2xt   1,2yt   1,m   1xt   1,m   1yt   1,mxt   1,mxt,1yt,1xt,2xt,n   1xt,nyt,2yt,n   1yt,nworkshop track - iclr 2016

tying between each ht and ht+1 means that decoding decisions are jointly dependent across the
entire document. this joint dependence may have important advantages, as it propagates contextual
information further across the document; the ccdclm and codclm thereby offer two points on
a tradeoff between accuracy and decoding complexity.

2.3 attentional dclm

one potential shortcoming of ccdclm and codclm is the limited capacity of the context vec-
tor, ct   1, which is a    xed dimensional representation of the context. while this might suf   ce for
short sentences, as sentences grow longer, the amount of information needing to be carried forward
will also grow, and therefore a    xed size embedding may be insuf   cient. for this reason, we now
consider an attentional mechanism, based on conditional language models for translation (sutskever
et al., 2014; bahdanau et al., 2015) which allows for a dynamic capacity representation of the con-
text.
central to the attentional mechanism is the context representation, which is de   ned separately for
each word position in the output sentence,

m(cid:88)

ct   1,n =

  n,mht   1,m

m=1

  n = softmax (an)
an,m = w(cid:62)

a tanh (wa1ht,n + wa2ht   1,m)

where ct   1,n is formulated as a weighted linear combination of all the hidden states in the previous
sentence, with weights    constrained to lie on the simplex using the softmax transformation. each
weight   n,m encodes the importance of the context at position m for generating the current word
at n, de   ned as a neural network with a hidden layer and a single scalar output. consequently each
position in the generated output can    attend    to different elements of the context sentence, which
would arguably be useful to shift the focus to make best use of the context vector during generation.
the revised de   nition of the context in equation 8 requires some minor changes in the generat-
ing components. we include this as an additional input to both the recurrent function (similar to
ccdclm), and output generating function (akin to codclm), as follows

(cid:16)

ht,n   1,(cid:2)c(cid:62)

t   1,n, x(cid:62)

(cid:3)(cid:62)(cid:17)

ht,n = g  
yt,n     softmax (wo tanh (whht,n + wcct   1,n + b))

t,n

where the output uses a single hidden layer network to merge the local state and context, before
expanding the dimensionality to the size of the output vocabulary, using wo. the extended model
is named as attentional dclm (adclm).

3 data and implementation

we evaluate our models with perplexity and document-level coherence assessment. the    rst data
set used for evaluation is the id32 (ptb) corpus (marcus et al., 1993), which is a standard
data set used for evaluating language models (e.g., mikolov et al., 2010). we use the standard split:
sections 0-20 for training, 21-22 for development, and 23-24 for test (mikolov et al., 2010). we
keep the top 10,000 words to construct the vocabulary, and replace lower frequency words with
the special token unknown. the vocabulary also includes two special tokens start and end to
indicate the beginning and end of a sentence. in total, the vocabulary size is 10, 003.
to investigate the capacity of modeling documents with larger context, we use a subset of the north
american news text (nant) corpus (mcclosky et al., 2008) to construct another evaluation data
set. as shown in table 1, the average length of the training documents is more than 30 sentences.
we follow the same procedure to preprocess the dataset as for the ptb corpus, and keep the top
15,000 words from the training set in the vocabulary. some basic statistics of both data sets are
listed in table 1.

4

(8)

(9)
(10)

(11)
(12)

workshop track - iclr 2016

ptb

nant

training
development
test
training
development
test

# documents
2,000
155
155
26,462
148
2,753

average document length
# tokens
502
516
577
783
799
778

# sentences
21
22
24
32
33
32

table 1: basic statistics of the id32 (ptb) and north american news text (nant) data
sets

3.1

implementation

we use a two-layer lstm to build the recurrent architecture of our document language models,
which we implement in the id98 package (https://github.com/clab/id98). the rest of
this section includes some additional details of our implementation, which is available online at
https://github.com/jiyfeng/dclm.
initialization all parameters are initialized with random values drawn from the range

[   (cid:112)6/(d1 + d2),(cid:112)6/(d1 + d2)], where d1 and d2 are the input and output dimensions of the pa-

rameter matrix respectively, as suggested by glorot & bengio (2010).
learning online learning was performed using adagrad (duchi et al., 2011) with the initial learn-
ing    = 0.1. to avoid the exploding gradient problem, we used the norm clipping trick proposed by
pascanu et al. (2012) and    xed the norm threshold as    = 5.0.
hyper-parameters our models include two tunable hyper-parameters:
the dimension of word
representation k and the hidden dimension of lstm unit h. we consider the values
{32, 48, 64, 96, 128, 256} for both k and h. the best combination of k and h for each model
is selected by the development sets via grid search. in all experiments, we    x the hidden dimension
of the attentional component in adclm as 48.
document length as shown in table 1, the average length of documents is more than 500 tokens,
with extreme cases having over 1,000 tokens. in practice, we noticed that training on long documents
leads to a very slow convergence. we therefore segment documents into several non-overlapping
shorter documents, each with at most l sentences, while preserving the original sentence order. the
value of l used in most experiments is 5, although we compare with l = 10 in subsection 4.1.

4 experiments

we compare the three dclm-style models (ccdclm, codclm, adclm) with the following
competitive alternatives:

recurrent neural network language model (id56lm) the model is trained on individual sen-
tences without any contextual information (mikolov et al., 2010). the comparison between
our models and this baseline system highlights the contribution of contextual information.
id56lm w/o sentence boundary (did56lm) this is a straightforward extension of sentence-level
id56lm to document-level, as illustrated in figure 1. it can also be viewed a conventional
id56lm without considering sentence boundaries. the difference between id56lm and
did56lm is that did56lm is able to consider (a limited amount of) extra-sentential context.
hierarchical id56lm (hid56lm) we also adopt the model architecture of hid56lm (lin et al.,
2015) as another baseline system, and reimplemented it with several modi   cations for a
fair comparison. comparing to the original implementation (lin et al., 2015), we    rst re-
place the sigmoid recurrence function with a long short-term memory (lstm) as used in
dclms. furthermore, instead of using pretrained id27, we update word rep-
resentation during training. finally, we jointly train the language models on both sentence-
level and document-level. these changes resulted in substantial improvements over the

5

workshop track - iclr 2016

model
baselines
1. id56lm (mikolov et al., 2010)
2. id56lm w/o sentence boundary (did56lm)
3. hierarchical id56lm (hid56lm) (lin et al., 2015)
our models
4. attentional dclm (adclm)
5. context-to-output dclm (codclm)
6. context-to-context dclm (ccdclm)

ptb
dev

69.24
65.27
66.32

64.31
64.37
62.34

test

71.88
69.37
70.62

68.32
68.49
66.42

nant
dev

109.48
101.42
103.90

96.47
95.10
96.77

test

194.43
181.62
175.92

170.99
173.52
172.88

table 2: perplexities of the id32 (ptb) and north american news text (nant) data
sets.

figure 3: effect of length thresholds on predictive log-likelihood on the pdtb development set.

original version of the hid56lm; they allow us to isolate what we view as the most substan-
tive difference between the dclm and this modeling approach, which is how contextual
information is identi   ed and exploited.

4.1 perplexity

to make a fair comparison across different models, we follow the conventional way to compute
perplexity. particularly, the start and end tokens are only used for notational convenience. the
end token from the previous sentence was never used to predict the start token in the current
sentence. therefore, we have the same computation procedure on perplexity for the models with
and without contextual information.
table 2 present the results on id38 perplexity. the best perplexities are given by the
context-to-context dclm on the ptb data set (line 6 in table 2), and attentional dclm on the
nant data set (line 4 in table 2). all dclm-based models achieve better perplexity than the prior
work. while the improvements on the ptb dataset are small in an absolute sense, they consistently
point to the value of including multi-level context information in id38. the value of
context information is further veri   ed by the model performance on the nant dataset. of interest
is the behavior of the attentional dclm on two data sets. this model combines both the context-to-
context and context-to-output mechanisms. theoretically, adclmis considerably more expressive
then the codclm and ccdclm. on the other hand, it is also complex to learn and innately favors
large data sets.
in all the results reported in table 2, the document length threshold was    xed as l = 5, meaning
that documents were partitioned into subsequences of    ve sentences. we were interested to know
whether our results depended on this parameter. taking l = 1 would be identical to the standard
id56lm, run separately on each sentence. to test the effect of increasing l, we also did an empirical

6

020406080100120140# updates (   50)6.56.05.55.04.54.0normalized log-likelihoodlength threshold = 5length threshold = 10workshop track - iclr 2016

comparison between l = 5 and l = 10 with ccdclm. figure 3 shows the two curves on the ptb
development set. the x-axis is the number of updates on ccdclm with the ptb training set.
the y-axis is the mean per-token log-likelihood given by equation 2 on the development set. as
shown in this    gure, l = 10 seems to learn more quickly per iteration in the beginning, although
each iteration is more time-consuming, due to the need to backpropagate over longer documents.
however, after a suf   cient number of updates, the    nal performance results are nearly identical,
with a slight advantage to the l = 5 setting. this suggests a tradeoff between the amount of
contextual information and the ease of learning.

4.2 local coherence evaluation

the long-term goal of coherence evaluation is to predict which texts are more coherent, and then
to optimize for this criterion in multi-sentence generation tasks such as summarization and machine
translation. a well-known proxy to this task is to try to automatically distinguish an original docu-
ment from an alternative form in which the sentences are scrambled (barzilay & lapata, 2008; li &
hovy, 2014). multi-sentence language models can be applied to this task directly, by determining
whether the original document has a higher likelihood; no supervised training is necessary.
we adopt the speci   c experimental setup proposed by barzilay & lapata (2008). to give a robust
model comparison with the limited number of documents available in the ptb test set, we employ
id64 (davison & hinkley, 1997). first, a new test set d((cid:96)) is generated by sampling the
documents from the original test set with replacement. then, we shuf   ed the sentences in each
document d     d((cid:96)) to get a pseudo-document d(cid:48). the combination of d and d(cid:48) is a single test
example. we repeated the same procedure to produce 1,000 test sets, where each test set includes
155 pairs     one for each document in the ptb test set. since each test instance is a pairwise choice,
a random baseline will have expected accuracy of 50%.
to evaluate the models proposed in this paper, we use the con   guration with the best development set
perplexity, as shown in table 2. the results of accuracy and standard deviation are calculated over
1,000 resampled test sets. as shown in table 3, the best accuracy is 83.26% given by ccdclm,
which also gives the smallest standard deviation 3.77%. furthermore, all dclm-based models
signi   cantly outperform the id56lm with p < 0.01 given by a two-sample one-side z-test on the
bootstrap samples. in addition, the ccdclm and codclm are outperform the hid56lm with
p < 0.01 with statistic z = 36.55 and 31.26 respectively.
in addition, we also evaluated the models trained on the nant dataset on this coherence evaluation
task. with the same 1,000 test sets, the best accuracy number across different models is 72.85%
obtained from codclm. compare to the results in table 3, we believe the performance drop is
due to the domain mismatch. even though ptb and nant are both corpora collecting from news
articles, they have totally different distributions on words, sentence lengths and even document
lengths as shown in table 1.
unlike some prior work on coherence evaluation (li & hovy, 2014; li et al., 2015; lin et al.,
2015), our approach is not trained on supervised data. supervised training might therefore improve
performance further. however, we emphasize that the real goal is to make automatically-generated
translations and summaries more coherent, and we should therefore avoid over   tting on this arti   cial
proxy task.

5 related work

neural language models (nlms) learn the distributed representations of words together with the
id203 function of word sequences.
in the nlm proposed by bengio et al. (2003), a feed-
forward neural network with a single hidden layer was used to calculate the language model prob-
abilities. one limitation of this model is only    xed-length context can be used. recurrent neu-
ral network language models (id56lms) avoid this problem by recurrently updating a hidden
state (mikolov et al., 2010), thus enabling them to condition on arbitrarily long histories. in this
work, we make a further extension to include more context with a recurrent architecture, by allow-
ing multiple pathways for historical information to affect the current word. a comprehensive review
of recurrent neural networks language models is offered by de mulder et al. (2015).

7

workshop track - iclr 2016

model
baselines
1. id56lm w/o sentence boundary (did56lm)
2. hierarchical id56lm (hid56lm) (lin et al., 2015)
our models
3. attentional dclm (adclm)   
4. context-to-output dclm (codclm)      
5. context-to-context dclm (ccdclm)      
    signi   cantly better than did56lm with p-value < 0.01
    signi   cantly better than hid56lm with p-value < 0.01

accuracy
mean (%)

standard deviation (%)

72.54
75.32

75.51
81.72
83.26

8.46
4.42

4.12
3.81
3.77

table 3: coherence evaluation on the ptb test set. the reported accuracies are calculated from
1,000 id64 test sets (as explained in text).

conventional language models, including the models with recurrent structures (mikolov et al.,
2010), limit the context scope within a sentence. this ignores potentially important information from
preceding text, for example, the previous sentence. targeting id103, where contextual
information may be especially important, mikolov & zweig (2012) introduce the topic-conditioned
id56lm, which incorporates a separately-trained id44 topic model to capture
the broad themes of the preceding text. our focus here is on discriminatively-trained end-to-end
models.
lin et al. (2015) recently introduced a document-level language model, called hierarchical recurrent
neural network language model (hid56lm). as in our approach, there are two channels of informa-
tion: a id56 for modeling words in a sentence, and another recurrent model for modeling sentences,
based on a bag-of-words representation of each sentence. (contemporaneously to our paper, wang
& cho (2015) also construct a bag-of-words representation of previous sentences, which they then
insert into a sentence-level lstm.) our modeling approach is more uni   ed and compact, employ-
ing a single recurrent neural network architecture, but with multiple channels for information to feed
forward into the prediction of each word. we also go further than this prior work by exploring an
attentional architecture (bahdanau et al., 2015).
moving away from the speci   c problem of id38, we brie   y consider other approaches
for modeling document content. li & hovy (2014) propose to use a convolution kernel to summarize
sentence-level representations for modeling a document. the model is for coherence evaluation, in
which the parameters are learned via supervised training. related convolutional architectures for
document modeling are considered by denil et al. (2014) and tang et al. (2015). encoder-decoder
architectures provide an alternative perspective, compressing all the information in a sequence into
a single vector, and then attempting to decode the target information from this vector; while this
idea has notably applied in machine translation (cho et al., 2014), it can also be employed for
coherence modeling (li et al., 2015). the hierarchical sequence-to-sequence model of li et al.
(2015) conditions the start word of each sentence on contextual information provided by the encoder,
but does not apply this idea to id38. different from the models with hierarchical
structures, paragraph vector (le & mikolov, 2014) encodes a document to a numeric vector by
discarding document structure and only retaining topic information.

6 conclusion

contextual information beyond the sentence boundary is essential to document-level text generation
and coherence evaluation. we propose a set of document-context language models (dclms), which
provide various approaches to incorporate contextual information from preceding texts. empirical
evaluation with perplexity shows that the dclms give better word prediction as language models, in
comparison with conventional id56lms; performance is also good on unsupervised coherence as-
sessment. future work includes testing the applicability of these models to downstream applications
such as summarization and translation.

8

workshop track - iclr 2016

acknowledgments this work was initiated during the 2015 jelinek memorial summer work-
shop on speech and language technologies at the university of washington, seattle, and was sup-
ported by johns hopkins university via nsf grant no iis 1005411, darpa lorelei contract
no hr0011-15-2-0027, and gifts from google, microsoft research, amazon and mitsubishi elec-
tric research laboratory. it was also supported by a google faculty research award to je.

references
dzmitry bahdanau, kyunghyun cho, and yoshua bengio. id4 by jointly

learning to align and translate. in iclr, 2015.

regina barzilay and mirella lapata. modeling local coherence: an entity-based approach. com-

putational linguistics, 34(1):1   34, 2008.

yoshua bengio, patrice simard, and paolo frasconi. learning long-term dependencies with gradient

descent is dif   cult. neural networks, ieee transactions on, 5(2):157   166, 1994.

yoshua bengio, r  ejean ducharme, pascal vincent, and christian janvin. a neural probabilistic

language model. the journal of machine learning research, 3:1137   1155, 2003.

kyunghyun cho, bart van merri  enboer, caglar gulcehre, dzmitry bahdanau, fethi bougares, hol-
ger schwenk, and yoshua bengio. learning phrase representations using id56 encoder-decoder
for id151. in emnlp, 2014.

junyoung chung, caglar gulcehre, kyunghyun cho, and yoshua bengio. empirical evaluation of

gated recurrent neural networks on sequence modeling. arxiv preprint arxiv:1412.3555, 2014.

anthony christopher davison and david victor hinkley. bootstrap methods and their application,

volume 1. cambridge university press, 1997.

wim de mulder, steven bethard, and marie-francine moens. a survey on the application of re-
current neural networks to statistical id38. computer speech & language, 30(1):
61   98, 2015.

misha denil, alban demiraj, nal kalchbrenner, phil blunsom, and nando de freitas. modelling,
visualising and summarising documents with a single convolutional neural network. arxiv
preprint arxiv:1406.3830, 2014.

john duchi, elad hazan, and yoram singer. adaptive subgradient methods for online learning and

stochastic optimization. the journal of machine learning research, 12:2121   2159, 2011.

xavier glorot and yoshua bengio. understanding the dif   culty of training deep feedforward neural
networks. in international conference on arti   cial intelligence and statistics, pp. 249   256, 2010.

sepp hochreiter and j  urgen schmidhuber. long short-term memory. neural computation, 9(8):

1735   1780, 1997.

dan jurafsky and james h martin. speech and language processing. pearson education india, 2000.

philipp koehn. id151. cambridge university press, 2009.

quoc le and tomas mikolov. distributed representations of sentences and documents. in icml,

2014.

jiwei li and eduard hovy. a model of coherence based on distributed sentence representation. in

emnlp, 2014.

jiwei li, thang luong, and dan jurafsky. a hierarchical neural autoencoder for paragraphs
in acl-ijcnlp, pp. 1106   1115, beijing, china, july 2015. association for

and documents.
computational linguistics.

rui lin, shujie liu, muyun yang, mu li, ming zhou, and sheng li. hierarchical recurrent neural
network for document modeling. in emnlp, pp. 899   907, lisbon, portugal, september 2015.
association for computational linguistics.

9

workshop track - iclr 2016

christopher d manning, prabhakar raghavan, and hinrich sch  utze. introduction to information

retrieval. cambridge university press cambridge, 2008.

mitchell p. marcus, mary ann marcinkiewicz, and beatrice santorini. building a large annotated

corpus of english: the id32. computational linguistics, 19(2):313   330, 1993.

david mcclosky, eugene charniak, and mark johnson. bllip north american news text, com-

plete. linguistic data consortium, 2008.

tomas mikolov and geoffrey zweig. context dependent recurrent neural network language model.

in slt, pp. 234   239, 2012.

tomas mikolov, martin kara     at, lukas burget, jan cernock`y, and sanjeev khudanpur. recurrent

neural network based language model. in interspeech, 2010.

razvan pascanu, tomas mikolov, and yoshua bengio. on the dif   culty of training recurrent neural

networks. arxiv preprint arxiv:1211.5063, 2012.

alessandro sordoni, michel galley, michael auli, chris brockett, yangfeng ji, margaret mitchell,
jian-yun nie, jianfeng gao, and bill dolan. a neural network approach to context-sensitive
generation of conversational responses. in naacl, 2015.

ilya sutskever, oriol vinyals, and quoc vv le. sequence to sequence learning with neural net-

works. in nips, 2014.

duyu tang, bing qin, and ting liu. document modeling with gated recurrent neural network
in emnlp, pp. 1422   1432, lisbon, portugal, september 2015.

for sentiment classi   cation.
association for computational linguistics.

tian wang and kyunghyun cho.

arxiv:1511.03729, 2015.

larger-context language modelling.

arxiv preprint

10

