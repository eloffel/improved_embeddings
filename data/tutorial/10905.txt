4
1
0
2

 

v
o
n
0
1

 

 
 
]

g
l
.
s
c
[
 
 

1
v
9
3
5
2

.

1
1
4
1
:
v
i
x
r
a

unifying visual-semantic embeddings with

multimodal neural language models

ryan kiros, ruslan salakhutdinov, richard s. zemel

university of toronto

canadian institute for advanced research

{rkiros, rsalakhu, zemel}@cs.toronto.edu

abstract

inspired by recent advances in multimodal learning and machine translation, we
introduce an encoder-decoder pipeline that learns (a): a multimodal joint embed-
ding space with images and text and (b): a novel language model for decoding
distributed representations from our space. our pipeline effectively uni   es joint
image-text embedding models with multimodal neural language models. we in-
troduce the structure-content neural language model that disentangles the structure
of a sentence to its content, conditioned on representations produced by the en-
coder. the encoder allows one to rank images and sentences while the decoder
can generate novel descriptions from scratch. using lstm to encode sentences,
we match the state-of-the-art performance on flickr8k and flickr30k without
using id164s. we also set new best results when using the 19-layer ox-
ford convolutional network. furthermore we show that with linear encoders, the
learned embedding space captures multimodal regularities in terms of vector space
arithmetic e.g. *image of a blue car* - "blue" + "red" is near images of red cars.
sample captions generated for 800 images are made available for comparison.

1

introduction

generating descriptions for images has long been regarded as a challenging perception task integrat-
ing vision, learning and language understanding. one not only needs to correctly recognize what
appears in images but also incorporate knowledge of spatial relationships and interactions between
objects. even with this information, one then needs to generate a description that is relevant and
grammatically correct. with the recent advances made in deep neural networks, tasks such as object
recognition and detection have made signi   cant breakthroughs in only a short time. the task of
describing images is one that now appears tractable and ripe for advancement. being able to append
large image databases with accurate descriptions for each image would signi   cantly improve the
capabilities of content-based id162 systems. moreover, systems that can describe images
well, could in principle, be    ne-tuned to answer questions about images also.
this paper describes a new approach to the problem of image id134, casted into the
framework of encoder-decoder models. for the encoder, we learn a joint image-sentence embedding
where sentences are encoded using long short-term memory (lstm) recurrent neural networks [1].
image features from a deep convolutional network are projected into the embedding space of the
lstm hidden states. a pairwise ranking loss is minimized in order to learn to rank images and their
descriptions. for decoding, we introduce a new neural language model called the structure-content
neural language model (sc-nlm). the sc-nlm differs from existing models in that it disentangles
the structure of a sentence to its content, conditioned on distributed representations produced by the
encoder. we show that sampling from an sc-nlm allows us to generate realistic image captions,
signi   cantly improving over the generated captions produced by [2]. furthermore, we argue that
this combination of approaches naturally    ts into the experimentation framework of [3], that is, a
good encoder can be used to rank images and captions while a good decoder can be used to generate
new captions from scratch. our approach effectively uni   es image-text embedding models (encoder

1

figure 1: sample generated captions. the bottom row shows different error cases. additional results
can be found at http://www.cs.toronto.edu/~rkiros/lstm_scnlm.html

phase) [4, 5, 6] with multimodal neural language models (decoder phase) [2] [7]. furthermore, our
method builds on analogous approaches being used in machine translation [8, 9, 10, 11].
while the application focus of our work is on image description generation and ranking, we also
qualitatively analyse properties of multimodal vector spaces learned using images and sentences. we
show that using a linear sentence encoder, linguistic regularities [12] also carry over to multimodal
vector spaces. for example, *image of a blue car* - "blue" + "red" results in a vector that is near
images of red cars. we qualitatively examine several types of analogies and structures with pca
projections. consequently, even with a global image-sentence training objective the encoder can still
be used to retrieve locally (e.g. individual words). this is analogous to pairwise ranking methods
used in machine translation [13, 14].

1.1 multimodal representation learning

a large body of work has been done on learning multimodal representations of images and text.
popular approaches include learning joint image-id27s [4, 5] as well as embedding
images and sentences into a common space [6, 15]. our proposed pipeline makes direct use of
these ideas. other approaches to multimodal learning include the use of deep id82s
[16], log-bilinear neural language models [2], autoencoders [17], recurrent neural networks [7] and
topic-models [18]. several bi-directional approaches to ranking images and captions have also been
proposed, based off of kernel cca [3], normalized cca [19] and dependency tree recursive net-
works [6]. from an architectural standpoint, our encoder-decoder model is most similar to [20], who
proposed a two-step embedding and generation procedure for id29.

1.2 generating descriptions of images

we group together approaches to generation into three types of methods, each described here in
more detail:
template-based methods. template-based methods involve    lling in sentence templates, such as
triplets, based on the results of id164s and spatial relationships [21, 22, 23, 24, 25]. while

2

figure 2: encoder: a deep convolutional network (id98) and long short-term memory recurrent
network (lstm) for learning a joint image-sentence embedding. decoder: a new neural language
model that combines structure and content vectors for generating words one at a time in sequence.

these approaches can produce accurate descriptions, they are often more    robotic    in nature and do
not generalize to the    uidity and naturalness of captions written by humans.
composition-based methods. these approaches aim to harness existing image-caption databases
by extracting components of related captions and composing them together to generate novel de-
scriptions [26, 27]. the advantage of these approaches are that they allow for a much broader
and more expressive class of captions that are more    uent and human-like then template-based ap-
proaches.
neural network methods. these approaches aim to generate descriptions by sampling from condi-
tional neural language models. the initial work in this area, based off of multimodal neural language
models [2], generated captions by conditioning on feature vectors from the output of a deep con-
volutional network. these ideas were recently extended to multimodal recurrent networks with
signi   cant improvements [7]. the methods described in this paper produce descriptions that at least
qualitatively on par with current state-of-the-art composition-based methods [27].
description generation systems have been plagued with issues of evaluation. while id7 and id8
have been used in the past, [3] has argued that such automated evaluation methods are unreliable
and do not match human judgements. these authors instead proposed that the problem of ranking
images and captions can be used as a proxy for generation. since any generation system requires a
scoring function to access how well a caption and image match, optimizing this task should naturally
carry over to an improvement in generation. many recent methods have since used this approach
for evaluation. none the less, the question on how to transfer improvements on ranking to gen-
erating new descriptions remained. we argue that encoder-decoder methods naturally    t into this
experimentation framework. that is, the encoder gives us a way to rank images and captions and
develop good scoring functions, while the decoder can use the representations learned to optimize
the scoring functions as a way of generating and scoring new descriptions.

1.3 encoder-decoder methods for machine translation

our proposed pipeline, while new to id134, has already experienced several successes in
id4 (id4). the goal of id4 is to develop an end-to-end translation system
with a large neural network, as opposed to using a neural network as an additional feature function
to an existing phrase-based system. id4 methods are based on the encoder-decoder principle.
that is, an encoder is used to map an english sentence to a distributed vector. a decoder is then
conditioned on this vector to generate a french translation from the source text. current methods
include using a convolutional encoder and id56 decoder [8], id56 encoder and id56 decoder [9, 10]
and lstm encoder with lstm decoder [11]. while still a young research area, these methods have
already achieved performance on par with strong phrase-based systems and have improved on the
start-of-the-art when used for rescoring.
we argue that it is natural to think of image id134 as a translation problem. that is,
our goal is to translate an image into a description. this point of view has also been used by [28]
and allows us to make use of existing ideas in the machine translation literature. furthermore, there
is a natural correspondence between the concept of scoring functions (how well does a caption and
image match) and alignments (which parts of a description correspond to which parts of an image)
that can naturally be exploited for generating descriptions.

3

2 an encoder-decoder model for ranking and generation

in this section we describe our image id134 pipeline. we    rst review lstm id56s
which are used for encoding sentences, followed by how to learn multimodal distributed represen-
tations. we then review log-bilinear neural language models [29], multiplicative neural language
models [30] and then introduce our structure-content neural language model.

2.1 long short-term memory id56s

long short-term memory [1] is a recurrent neural network that incorporates a built in memory cell
to store information and exploit long range context. lstm memory cells are surrounded by gat-
ing units for the purpose of reading, writing and reseting information. lstms have been used to
achieve state-of-the-art performance in several tasks such as handwriting recognition [31], sequence
generation [32] id103 [33] and machine translation [11] among others. dropout [34]
strategies have also been proposed to prevent over   tting in deep lstms. [35]
let xt denote a matrix of training instances at time t.
in our case, xt is used to denote a
matrix of word representations for the t-th word of each sentence in the training batch. let
(it, ft, ct, ot, mt) denote the input, forget, cell, output and hidden states of the lstm at time
step t. the lstm architecture in this work is implemented using the following equations:

it =   (xt    wxi + mt   1    whi + ct   1    wci + bi)
ft =   (xt    wxf + mt   1    whf + ct   1    wcf + bf )
ct = ft     ct   1 + it     tanh(xt    wxc + mt   1    whc + bc)
ot =   (xt    wxo + mt   1    who + ct    wco + bo)
mt = ot     tanh(ct)

(1)
(2)
(3)
(4)
(5)
where (  ) denotes the sigmoid activation function, (  ) indicates id127 and (   ) indi-
cates component-wise multiplication. 1

2.2 multimodal distributed representations

suppose for training we are given image-description pairs each corresponding to an image and a
description that correctly describes the image. images are represented as the top layer (before the
softmax) of a convolutional network trained on the id163 classi   cation task [36].
let d be the dimensionality of an image feature vector (e.g. 4096 for alexnet [36]), k the di-
mensionality of the embedding space and let v be the number of words in the vocabulary. let
wi     rk  d and wt     rk  v be the image embedding matrix and id27 matri-
ces, respectively. given an image description s = {w1, . . . , wn} with words w1, . . . , wn , 2 let
{w1, . . . , wn}, wi     rk, i = 1, . . . , n denote the corresponding word representations to words
w1, . . . , wn (entries in the matrix wt ). the representation of a sentence v is the hidden state of
the lstm at time step n (i.e. the vector mt). we note that other approaches for computing sentence
representations for image-text embeddings have been proposed, including dependency tree id56s
[6] and bags of dependency parses [15]. let q     rd denote an image feature vector (for the image
corresponding to description s) and let x = wi    q     rk be the image embedding. we de   ne a
scoring function s(x, v) = x    v, where x and v are    rst scaled to have unit norm (making s equiv-
alent to cosine similarity). let    denote all the parameters to be learned (wi and all the lstm
weights) 3. we optimize the following pairwise ranking loss:

max{0,        s(x, v) + s(x, vk)} +

max{0,        s(v, x) + s(v, xk)}

(6)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

min

  

x

k

v

k

where vk is a contrastive (non-descriptive) sentence for image embedding x, and vice-versa with xk.
for all of our experiments, we initialize the id27s wt to be pre-computed k = 300
dimensional vectors learned using a continuous bag-of-words model [37]. the contrastive terms are
chosen randomly from the training set and resampled every epoch.

1for additional details on lstm: http://people.idsia.ch/~juergen/id56.html.
2as a slight abuse of notation, we refer to wi as both a word and an index into the id27 matrix.
3we keep the id27 matrix wt    xed.

4

2.3 log-bilinear neural language models

the log-bilinear language model (lbl) [29] is a deterministic model that may be viewed as a feed-
forward neural network with a single linear hidden layer. each word w in the vocabulary is repre-
sented as a k-dimensional real-valued vector w     rk, as in the case of the encoder. let r denote
a v    k matrix of word representation vectors 4 where v is the vocabulary size. let (w1, . . . wn   1)
be a tuple of n    1 words where n    1 is the context size. the lbl model makes a linear prediction
of the next word representation as

(7)
where c(i), i = 1, . . . , n     1 are k    k context parameter matrices. thus,   r is the predicted
representation of wn. the id155 p (wn = i|w1:n   1) of wn given w1, . . . , wn   1 is

c(i)wi,

  r =

i=1

p (wn = i|w1:n   1) =

(cid:80)v

exp(  rt ri + bi)
j=1 exp(  rt rj + bj)

,

(8)

where b     rv is a bias vector. learning is done with stochastic id119.

n   1(cid:88)

2.4 multiplicative neural language models
suppose now we are given a vector u     rk from the multimodal vector space, which has an
association with a word sequence s = {w1, . . . , wn}. for example, u may be the embedded
representation of an image whose description is given by s. a multiplicative neural language model
[30] models the distribution p (wn = i|w1:n   1, u) of a new word wn given context from the previous
words and the vector u. a multiplicative model has the additional property that the id27
matrix is instead replaced with a tensor t     rv   k  g where g is the number of slices. given u,
i=1 uit (i) i.e. word
representations with respect to u are computed as a linear combination of slices weighted by each
component ui of u. here, the number of slices g is equal to k, the dimensionality of u.
it is often unnecessary to use a fully unfactored tensor. as in e.g. [38, 39], we re-represent t in
terms of three matrices wf k     rf  k, wf d     rf  g and wf v     rf  v , such that

we can compute a word representation matrix as a function of u as t u =(cid:80)g

t u = (wf v)(cid:62)    diag(wf du)    wf k

(9)
where diag(  ) denotes the matrix with its argument on the diagonal. these matrices are parametrized
by a pre-chosen number of factors f . in [30], the conditioning vector u is referred to as an attribute
and using a third-order model of words allows one to model conditional similarity: how meanings
of words change as a function of the attributes they   re conditioned on.
let e = (wf k)(cid:62)wf v denote a    folded    k    v matrix of id27s. given the context
w1, . . . , wn   1, the predicted next word representation   r is given by:

  r =

c(i)e(:, wi),

(10)

where e(:, wi) denotes the column of e for the word representation of wi and c(i), i = 1, . . . , n   1
are k    k context matrices. given a predicted next word representation   r, the factor outputs
are f = (wf k  r)     (wf du), where     is a component-wise product. the id155
p (wn = i|w1:n   1, u) of wn given w1, . . . , wn   1 and u can be written as

p (wn = i|w1:n   1, u) =

exp(cid:0)(wf v(:, i))(cid:62)f + bi
(cid:1)
j=1 exp(cid:0)(wf v(:, j))(cid:62)f + bj
(cid:80)v

(cid:1) ,

where wf v(:, i) denotes the column of wf v corresponding to word i. in contrast to the log-bilinear
model, the matrix of word representations r from before is replaced with the factored tensor t that
we have derived. we compared the multiplicative model against an additive variant [2] and found on
large datasets, such as the sbu captioned photo dataset [40], the multiplicative variant signi   cantly
outperforms its additive counterpart. thus, the sc-nlm is derived from the multiplicative variant.

4note that this is a different matrix then that used by the encoder. we use the same vocabulary throughout

both models.

5

n   1(cid:88)

i=1

(a) multiplicative nlm

(b) structure-content nlm

(c) sc-nlm prediction

figure 3: left: multiplicative neural language model. middle: structure-content neural language
model (sc-nlm). right: the prediction problem of an sc-nlm.

2.5 structure-content neural language models

we now describe the structure-content neural language model. suppose that, along with a de-
scription s = {w1, . . . , wn}, we are also given a sequence of word-speci   c structure variables
t = {t1, . . . , tn}. throughout our experiments, each ti corresponds to the part-of-speech for word
wi, although other possibilities can be used instead. given an embedding u (the content vector), our
goal is to model the distribution p (wn = i|w1:n   1, tn:n+k, u) from previous word context w1:n   1
and forward structure context tn:n+k, where k is the forward context size. figure 3 gives an illus-
tration of the model and prediction problem. intuitively, the structure variables help guide the model
during the generation phrase and can be thought of as a soft template to help avoid the model from
generating grammatical nonsense. note that this model shares a resemblance with the nnjm of [41]
for machine translation, where the previous word context are predicted words in the target language,
and the forward context are words in the source language.
our model can be interpreted as a multiplicative neural language model but where the attribute
vector is no longer u but instead an additive function of u and the structure variables t . let
{tn, . . . , tn+k}, ti     rk, i = n, . . . , n + k be embedding vectors for the structure variables t .
these are obtained from a learned lookup table in the same way as words are. we introduce a se-
quence of g    g structure context matrices t(i), i = n, . . . , n + k which play the same role as the
word context matrices c(i). let tu denote a g    k context matrix for the multimodal vector u.
the attribute vector   u of combined structure and content information is computed as

(cid:34)(cid:32)n+k(cid:88)

(cid:33)

(cid:35)

  u =

t(i)ti

+ t(u)u + b

(11)

i=n

+

where [  ]+ = max{  , 0} is a relu non-linearity and b is a bias vector. the vector   u now plays the
same role as the vector u for the multiplicative model previously described and the remainder of the
model remains unchanged. our experiments use g = k = 300 and factors f = 100.
the sc-nlm is trained on a large collection of image descriptions (e.g. flickr30k). there are
several choices available for representing the conditioning vectors u. one choice would be to use
the embedding of the corresponding image. an alternative choice, which is the approach we take, is
to condition on the embedding vector for the description s computed with the lstm. the advantage
of this approach is that the sc-nlm can be trained purely on text alone. this allows us to make
use of large amounts of monolingual text (e.g. non image captions) to improve the quality of the
language model. since the embedding vectors of s share a joint space with the image embeddings,
we can also condition the sc-nlm on image embeddings (e.g. at test time, when no description
is available) after the model has been trained. this is a signi   cant advantage over a conditional
language model that explicitly requires image-caption pairs for training and highlights the strength
of a multimodal encoding space.
due to space limitations, we leave the full details of our id134 procedure to the supple-
mentary material.

6

flickr8k
image annotation

image search

model
random ranking
sdt-id56 [6]
    devise [5]
    sdt-id56 [6]
defrag [15]
    defrag [15]
m-id56 [7]
our model
our model (oxfordnet)

r@1 r@5 r@10 med r r@1 r@5 r@10 med r
500
0.1
29
4.5
4.8
29
25
6.0
32
5.9
15
12.6
15
14.5
13.5
14
10
18.0

1.1
28.6
27.3
34.0
27.3
44.0
48.5
45.7
55.0

1.0
29.0
29.6
31.7
26.5
42.5
42.4
43.7
51.5

0.6
18.0
16.5
22.7
19.2
32.9
37.2
36.2
40.9

0.5
18.5
20.1
21.6
17.6
29.6
31.0
31.0
37.0

0.1
6.1
5.9
6.6
5.2
9.7
11.5
10.4
12.5

631
32
28
23
34
14
11
13
8

table 1: flickr8k experiments. r@k is recall@k (high is good). med r is the median rank (low is good).
best results overall are bold while best results without oxfordnet features are underlined. a     infront of the
method indicates that id164s were used along with single frame features.

3 experiments

3.1

image-sentence ranking

our main quantitative results is to establish the effectiveness of using an lstm sentence encoder
for ranking image and descriptions. we perform the same experimental procedure as done by [15]
on the flickr8k [3] and flickr30k [42] datasets. these datasets come with 8,000 and 30,000 images
respectively with each image annotated using 5 sentences by independent annotators. as with [15],
we did not do any explicit text preprocessing. we used two convolutional network architectures
the toronto convnet 5 as well as the 19-layer
for extracting 4096 dimensional image features:
oxfordnet [43] which    nished 2nd place in the ilsvrc 2014 classi   cation competition. following
the protocol of [15], 1000 images are used for validation, 1000 for testing and the rest are used for
training. evaluation is performed using recall@k, namely the mean number of images for which
the correct caption is ranked within the top-k retrieved results (and vice-versa for sentences). we
also report the median rank of the closest ground truth result from the ranked list. we compare our
results to each of the following methods:
devise. the deep visual semantic embedding model [5] was proposed as a way of performing zero-
shot object recognition and was used as a baseline by [15]. in this model, sentences are represented
as the mean of their id27s and the objective function optimized matches ours.
sdt-id56. the semantic dependency tree id56 [6] is used to learn sentence
representations for embedding into a joint image-sentence space. the same objective is used.
defrag. deep fragment embeddings [15] were proposed as an alternative to embedding full-frame
image features and take advantage of id164s from the r-id98 [44] detector. descriptions
are represented as a bag of dependency parses. their objective incorporates both a global and
fragment objectives, for which their global objective matches ours.
m-id56. the multimodal recurrent neural network [7] is a recently proposed method that uses per-
plexity as a bridge between modalities, as    rst introduced by [2]. unlike all other methods, the
m-id56 does not use a ranking loss and instead optimizes the log-likelihood of predicting the next
word in a sequence conditioned on an image.
our lstms use 1 layer with 300 units and weights initialized uniformly from [-0.08, 0.08]. the
margin    was set to    = 0.2, which we found performed well on both datasets. training is done
using stochastic id119 with an initial learning rate of 1 and was exponentially decreased.
we used minibatch sizes of 40 on flickr8k and 100 on flickr30k. no momentum was used. the
same hyperparameters are used for the oxfordnet experiments.

3.1.1 results

tables 1 and 2 illustrate our results on flickr8k and flickr30k respectively. the performance of
our model is comparable to that of the m-id56. for some metrics we outperform or match existing
results while on others m-id56 outperforms our model. the m-id56 does not learn an explicit em-
bedding between images and sentences and relies on perplexity as a means of retrieval. methods that

5https://github.com/torontodeeplearning/convnet

7

flickr30k

image annotation

image search

model
random ranking
    devise [5]
    sdt-id56 [6]
    defrag [15]
    defrag + finetune id98 [15]
m-id56 [7]
our model
our model (oxfordnet)

r@1 r@5 r@10 med r r@1 r@5 r@10 med r
500
0.1
25
4.5
9.6
16
14
14.2
13
16.4
16
18.4
13
14.8
23.0
8

1.0
32.7
41.1
44.2
44.5
41.5
46.3
56.5

1.1
29.2
41.1
51.3
54.7
50.9
50.9
62.9

0.6
18.1
29.8
37.7
40.2
40.2
39.2
50.7

0.5
21.9
29.8
30.8
31.4
31.2
34.0
42.0

0.1
6.7
8.9
10.2
10.3
12.6
11.8
16.8

631
26
16
10
8
10
10
5

table 2: flickr30k experiments. r@k is recall@k (high is good). med r is the median rank (low is good).
best results overall are bold while best results without oxfordnet features are underlined. a     infront of the
method indicates that id164s were used along with single frame features.

learn explicit embedding spaces have a signi   cant speed advantage over perplexity-based retrieval
methods, since retrieval is easily done with a single matrix multiply of stored embedding vectors
from the dataset with the query vector. thus explicit embedding methods are much better suited for
scaling to large datasets.
perhaps more interestingly is the fact that both our method and the m-id56 outperform existing
models that integrate id164s. this is contradictory to [6], where recurrent networks are the
worst performing models. this highlights the effectiveness of lstm cells for encoding dependen-
cies across descriptions and learning meaningful distributed sentence representations. integrating
id164s into our framework should almost surely improve performance as well as allow for
interpretable retrievals, as in the case of defrag.
using image features from the oxfordnet model results in a signi   cant performance boost across
all metrics, giving new state-of-the-art numbers on these evaluation tasks.

3.2 multimodal linguistic regularities

id27s learned with skip-gram [37] or neural language models [45] were shown by [12]
to exhibit linguistic regularities that allow these models to perform analogical reasoning. for in-
stance, "man" is to "woman" as "king" is to ? can be answered by    nding the closest vector to
"king" - "man" + "woman". a natural question we ask is whether multimodal vector spaces exhibit
the same phenomenon. would *image of a blue car* - "blue" + "red" be near images of red cars?

suppose that we train an embedding model with a linear encoder, namely v =(cid:80)n

i=1 wi for word
vectors wi and sentence vector v (where both v and the image embedding are normalized to unit
length). using our example above, let vblue, vred and vcar denote the id27s for blue, red
and car respectively. let ibcar and ircar denote embeddings of images with blue and red cars. after
training a linear encoder, the model has the property that vblue + vcar     ibcar and vred + vcar    
ircar. it follows that

(12)
(13)
(14)
thus given a query image q, a negative word wn and a positive word wp (all with unit norm), we
seek an image x    such that:

vcar     ibcar     vblue
vred + vcar     ibcar     vblue + vred
ircar     ibcar     vblue + vred

x    = argmax

x

(q     wn + wp)(cid:62)x
(cid:107)q     wn + wp(cid:107)

(15)

the supplementary material contains qualitative evidence that the above holds for several types
of regularities and images. 6 in our examples, we consider retrieving the top-4 nearest images.
occasionally we observed that a poor result would be obtained within the top-4 among good results.
we found a simple strategy for removing these cases is to    rst retrieve the top n nearest images,
then re-sort these based on their distance to the mean of the n images.
it is worth noting that these kinds of regularities are not well observed with an lstm encoder, since
sentences are no longer just a sum of their words. the linear encoder is roughly equivalent to the

6for this model we    netune the word representations.

8

devise baselines in tables 1 and 2, which perform signi   cantly worse for retrieval than an lstm
encoder. so while these regularities are interesting the learned multimodal vector space is not well
apt for ranking sentences and images.

3.3

image id134

we generated image descriptions for roughly 800 images from the sbu captioned photo dataset [40].
these are the same images used to display results by the current state-of-the-art composition based
approach, treetalk [27]. 7 our lstm encoder and sc-nlm decoder were trained by concatenating
the flickr30k dataset with the recently released microsoft coco dataset [46], which combined
give us over 100,000 images and over 500,000 descriptions for training. the sbu dataset contains 1
million images each with a single description and was used by [27] for training their model. while
the sbu dataset is larger, the annotated descriptions are noisier and more personalized.
the generated results can be found at http://www.cs.toronto.edu/~rkiros/lstm_
scnlm.html 8. for each image we show the original caption, the nearest neighbour sentence
from the training set, the top-5 generated samples from our model and the best generated result from
treetalk. the nearest neighbour sentence is displayed to demonstrate that our model has not simply
learned to copy the training data. our generated descriptions are arguably the nicest ones to date.

4 discussion

when generating a description, it is often the case that only a small region is relevant at any given
time. we are developing an attention-based model that jointly learns to align parts of captions to
images and use these alignments to determine where to attend next, thus dynamically modifying the
vectors used for conditioning the decoder. we also plan on experimenting with lstm decoders as
well as deep and bidirectional lstm encoders.

acknowledgments

we would like to thank nitish srivastava for assistance with his convnet package as well as prepar-
ing the oxford convolutional network. we also thank the anonymous reviewers from the nips 2014
deep learning workshop for their comments and suggestions.

references
[1] sepp hochreiter and j  rgen schmidhuber. long short-term memory. neural computation,

1997.

[2] ryan kiros, richard s zemel, and ruslan salakhutdinov. multimodal neural language models.

icml, 2014.

[3] micah hodosh, peter young, and julia hockenmaier. framing image description as a ranking

task: data, models and id74. jair, 2013.

[4] jason weston, samy bengio, and nicolas usunier. large scale image annotation: learning to

rank with joint word-image embeddings. machine learning, 2010.

[5] andrea frome, greg s corrado, jon shlens, samy bengio, jeffrey dean, and tomas mikolov

marcaurelio ranzato. devise: a deep visual-semantic embedding model. nips, 2013.

[6] richard socher, q le, c manning, and a ng. grounded id152 for    nding

and describing images with sentences. in tacl, 2014.

[7] junhua mao, wei xu, yi yang, jiang wang, and alan l yuille. explain images with multi-

modal recurrent neural networks. arxiv preprint arxiv:1410.1090, 2014.

[8] nal kalchbrenner and phil blunsom. recurrent continuous translation models. in emnlp,

2013.

[9] kyunghyun cho, bart van merrienboer, caglar gulcehre, fethi bougares, holger schwenk,
and yoshua bengio. learning phrase representations using id56 encoder-decoder for statistical
machine translation. emnlp, 2014.

7http://ilp-cky.appspot.com/generation
8these results use features from the toronto convnet.

9

[10] dzmitry bahdanau, kyunghyun cho, and yoshua bengio. id4 by

jointly learning to align and translate. arxiv preprint arxiv:1409.0473, 2014.

[11] ilya sutskever, oriol vinyals, and quoc v le. sequence to sequence learning with neural

networks. nips, 2014.

[12] tomas mikolov, wen-tau yih, and geoffrey zweig. linguistic regularities in continuous space

word representations. in naacl-hlt, 2013.

[13] karl moritz hermann and phil blunsom. multilingual distributed representations without word

alignment. iclr, 2014.

[14] karl moritz hermann and phil blunsom. multilingual models for compositional distributional

semantics. in acl, 2014.

[15] andrej karpathy, armand joulin, and li fei-fei. deep fragment embeddings for bidirectional

image sentence mapping. nips, 2014.

[16] nitish srivastava and ruslan salakhutdinov. multimodal learning with deep boltzmann ma-

chines. in nips, 2012.

[17] jiquan ngiam, aditya khosla, mingyu kim, juhan nam, honglak lee, and andrew ng. mul-

timodal deep learning. in icml, 2011.

[18] yangqing jia, mathieu salzmann, and trevor darrell. learning cross-modality similarity for

multinomial data. in iccv, 2011.

[19] yunchao gong, liwei wang, micah hodosh, julia hockenmaier, and svetlana lazebnik. im-
proving image-sentence embeddings using large weakly annotated photo collections. in eccv.
2014.

[20] phil blunsom, nando de freitas, edward grefenstette, karl moritz hermann, et al. a deep

architecture for id29. in acl 2014 workshop on id29, 2014.

[21] girish kulkarni, visruth premraj, sagnik dhar, siming li, yejin choi, alexander c berg,
and tamara l berg. baby talk: understanding and generating simple image descriptions. in
cvpr, 2011.

[22] ali farhadi, mohsen hejrati, mohammad amin sadeghi, peter young, cyrus rashtchian, julia
hockenmaier, and david forsyth. every picture tells a story: generating sentences from
images. in eccv. 2010.

[23] siming li, girish kulkarni, tamara l berg, alexander c berg, and yejin choi. composing

simple image descriptions using web-scale id165s. in conll, 2011.

[24] yezhou yang, ching lik teo, hal daum   iii, and yiannis aloimonos. corpus-guided sentence

generation of natural images. in emnlp, 2011.

[25] margaret mitchell, xufeng han, jesse dodge, alyssa mensch, amit goyal, alex berg, kota
yamaguchi, tamara berg, karl stratos, and hal daum   iii. midge: generating image descrip-
tions from id161 detections. in eacl, 2012.

[26] polina kuznetsova, vicente ordonez, alexander c berg, tamara l berg, and yejin choi.

collective generation of natural image descriptions. acl, 2012.

[27] polina kuznetsova, vicente ordonez, tamara l. berg, and yejin choi. treetalk : composition

and compression of trees for image descriptions. tacl, 2014.

[28] marcus rohrbach, wei qiu, ivan titov, stefan thater, manfred pinkal, and bernt schiele.

translating video content to natural language descriptions. in iccv, 2013.

[29] andriy mnih and geoffrey hinton. three new id114 for statistical language mod-

elling. in icml, pages 641   648, 2007.

[30] ryan kiros, richard s zemel, and ruslan salakhutdinov. a multiplicative model for learning

distributed text-based attribute representations. nips, 2014.

[31] alex graves, marcus liwicki, santiago fern  ndez, roman bertolami, horst bunke, and j  r-
gen schmidhuber. a novel connectionist system for unconstrained handwriting recognition.
tpami, 2009.

[32] alex graves. generating sequences with recurrent neural networks.

arxiv:1308.0850, 2013.

arxiv preprint

[33] alex graves, navdeep jaitly, and abdel-rahman mohamed. hybrid id103 with

deep bidirectional lstm. in ieee workshop on asru, 2013.

10

[34] nitish srivastava, geoffrey hinton, alex krizhevsky, ilya sutskever, and ruslan salakhutdi-

nov. dropout: a simple way to prevent neural networks from over   tting. jmlr, 2014.

[35] wojciech zaremba, ilya sutskever, and oriol vinyals. recurrent neural network id173.

arxiv preprint arxiv:1409.2329, 2014.

[36] alex krizhevsky, ilya sutskever, and geoff hinton. id163 classi   cation with deep convo-

lutional neural networks. in nips, 2012.

[37] tomas mikolov, kai chen, greg corrado, and jeffrey dean. ef   cient estimation of word

representations in vector space. arxiv preprint arxiv:1301.3781, 2013.

[38] roland memisevic and geoffrey hinton. unsupervised learning of image transformations. in

cvpr, pages 1   8, 2007.

[39] alex krizhevsky, geoffrey e hinton, et al. factored 3-way restricted id82s for

modeling natural images. in aistats, pages 621   628, 2010.

[40] vicente ordonez, girish kulkarni, and tamara l berg. im2text: describing images using 1

million captioned photographs. in nips, 2011.

[41] jacob devlin, rabih zbib, zhongqiang huang, thomas lamar, richard schwartz, and john
makhoul. fast and robust neural network joint models for id151. acl,
2014.

[42] peter young alice lai micah hodosh and julia hockenmaier. from image descriptions to vi-
sual denotations: new similarity metrics for semantic id136 over event descriptions. tacl,
2014.

[43] karen simonyan and andrew zisserman. very deep convolutional networks for large-scale

image recognition. arxiv preprint arxiv:1409.1556, 2014.

[44] ross girshick, jeff donahue, trevor darrell, and jitendra malik. rich feature hierarchies for

accurate id164 and semantic segmentation. cvpr, 2014.

[45] yoshua bengio, r  jean ducharme, pascal vincent, and christian janvin. a neural probabilistic

language model. jmlr, 2003.

[46] tsung-yi lin, michael maire, serge belongie, james hays, pietro perona, deva ramanan,
piotr doll  r, and c lawrence zitnick. microsoft coco: common objects in context. arxiv
preprint arxiv:1405.0312, 2014.

11

5 supplementary material: additional experimentation and details

5.1 multimodal linguistic regularities

(a) simple cases

(b) colors

(c) image structure

(d) sanity check

figure 4: multimodal vector space arithmetic. query images were downloaded online and retrieved
images are from the sbu dataset.

(a) colors

(b) weather

figure 5: pca projection of the 300-dimensional word and image representations for (a) cars and
colors and (b) weather and temperature.

figure 4 illustrates sample results using a model trained on the sbu dataset. all queries were
downloaded online and retrieved images are from the sbu images used for training. what is of
interest to note is that the resulting images depend highly on the image used for the query. for
example, searching for the word    night    retrieves arbitrary images taken at night. on the other
hand, an image with a building predominantly as its focus will return night images when    day    is

12

subtracted and    night    is added. a similar phenomenon occurs with the example of cats, bowls and
boxes. as additional visualizations, we computed pca projections of cars and their corresponding
colors as well as images and the weather occurrences in figure 5. these results give us strong
evidence for the regularities apparent in multimodal vector spaces trained with linear encoders. of
course, sensible results are only likely to be obtained if (a) the content of the image is correctly
recognized, (b) the subtraction word is relevant to the image and (c) an image exists that is sensible
for the corresponding query.

5.2

image description generation

the sc-nlm was trained on the concatenation of training sentences from both flickr30k and mi-
crosoft coco. given an image, we    rst map it into the multimodal space. from this embedding,
we de   ne 2 sets of candidate conditioning vectors to the sc-nlm:
image embedding. the embedded image itself. note that the sc-nlm was not trained with images
but can be conditioned on images since the embedding space is multimodal.
top-n nearest words and sentences. after    rst computing the image embedding, we obtain the
top-n nearest neighbour words and training sentences using cosine similarity. these retrievals are
treated as a    bag of concepts    for which we compute an embedding vector as the mean of each
concept. all of our results use n = 5.
along with the candidate conditioning vectors, we also compute candidate pos sequences used by
the sc-nlm. for this, we obtain a set of all pos sequences from the training set whose lengths
were between 4 and 12, inclusive. captions are generated by    rst sampling a conditioning vector,
next sampling a pos sequence, then computing a map estimate from the sc-nlm. we generate
a large list of candidate descriptions (1000 for each image in our results) and rank these candidates
using a scoring function. our scoring function consists of two feature functions:
translation model. the candidate description is embedded into the multimodal space using the
lstm. we then compute a translation score as the cosine similarity between the image embedding
and the embedding of the candidate description. this scores how relevant the content of the candi-
date is to the image. we also augment to this score a multiplicative penalty to non-stopwords that
appear too frequently in the description. 9
language model. we trained a kneser-ney trigram model on a large corpus and compute the log-
id203 of the candidate under the model. this scores how reasonable of an english sentence is
the candidate.
the total score of a caption is then the weighted sum of the translation and language models. due to
the challenge of quantitatively evaluating generated descriptions, we tuned the weights by hand on
qualitative results alone. all of the candidate descriptions are ranked by their scores, and the top-5
captions are returned.

9for instance, given an image of a car, we would want a candidate to be ranked low if each noun in the

description was    car   .

13

