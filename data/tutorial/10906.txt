what do recurrent neural network grammars learn about syntax?

adhiguna kuncoro    miguel ballesteros    lingpeng kong   

chris dyer       graham neubig    noah a. smith   

   school of computer science, carnegie mellon university, pittsburgh, pa, usa

   ibm t.j. watson research center, yorktown heights, ny, usa

   deepmind, london, uk

   computer science & engineering, university of washington, seattle, wa, usa

{akuncoro,lingpenk,gneubig}@cs.cmu.edu

7
1
0
2

 

n
a
j
 

0
1

 
 
]
l
c
.
s
c
[
 
 

2
v
4
7
7
5
0

.

1
1
6
1
:
v
i
x
r
a

miguel.ballesteros@ibm.com, cdyer@google.com, nasmith@cs.washington.edu

abstract

neural

network

recurrent
grammars
(id56g) are a recently proposed prob-
abilistic generative modeling family for
natural language. they show state-of-
id38 and parsing
the-art
performance. we investigate what
in-
formation they learn, from a linguistic
perspective,
through various ablations
to the model and the data, and by aug-
menting the model with an attention
mechanism (ga-id56g) to enable closer
inspection. we    nd that explicit modeling
of composition is crucial for achieving the
best performance. through the attention
mechanism, we    nd that headedness
plays a central role in phrasal represen-
tation (with the model   s latent attention
largely agreeing with predictions made
by hand-crafted head rules, albeit with
some important differences). by training
grammars without nonterminal labels, we
   nd that phrasal representations depend
minimally on nonterminals, providing
support for the endocentricity hypothesis.

introduction

1
in this paper, we focus on a recently proposed
class of id203 distributions, recurrent neural
network grammars (id56gs; dyer et al., 2016),
designed to model syntactic derivations of sen-
tences. we focus on id56gs as generative proba-
bilistic models over trees, as summarized in   2.

fitting a probabilistic model to data has often
been understood as a way to test or con   rm some
aspect of a theory. we talk about a model   s as-
sumptions and sometimes explore its parameters
or posteriors over its latent variables in order to
gain understanding of what it    discovers    from the

data. in some sense, such models can be thought
of as mini-scientists.

neural networks, including id56gs, are capa-
ble of representing larger classes of hypotheses
than traditional probabilistic models, giving them
more freedom to explore. unfortunately, they tend
to be bad mini-scientists, because their parameters
are dif   cult for human scientists to interpret.

id56gs are striking because they obtain state-
of-the-art parsing and id38 perfor-
mance. their relative lack of independence as-
sumptions, while still incorporating a degree of
linguistically-motivated prior knowledge, affords
the model considerable freedom to derive its own
insights about syntax. if they are mini-scientists,
the discoveries they make should be of particular
interest as propositions about syntax (at least for
the particular genre and dialect of the data).

this paper manipulates the inductive bias of
id56gs to test linguistic hypotheses.1 we be-
gin with an ablation study to discover the impor-
tance of the composition function in   3. based
on the    ndings, we augment the id56g composi-
tion function with a novel gated attention mech-
anism (leading to the ga-id56g) to incorporate
more interpretability into the model in   4. using
the ga-id56g, we proceed by investigating the
role that individual heads play in phrasal represen-
tation (  5) and the role that nonterminal category
labels play (  6). our key    ndings are that lexi-
cal heads play an important role in representing
most phrase types (although compositions of mul-
tiple salient heads are not infrequent, especially

1id56gs have less inductive bias relative to traditional
unlexicalized id140, but more
than models that parse by transducing word sequences to
linearized parse trees represented as strings (vinyals et al.,
2015).
inductive bias is necessary for learning (mitchell,
1980); we believe the important question is not    how little
can a model get away with?    but rather the bene   t of differ-
ent forms of inductive bias as data vary.

for conjunctions) and that nonterminal labels pro-
vide little additional information. as a by-product
of our investigation, a variant of the id56g with-
out ensembling achieved the best reported super-
vised phrase-structure parsing (93.6 f1; english
ptb) and, through conversion, dependency pars-
ing (95.8 uas, 94.6 las; ptb sd). the code and
pretrained models to replicate our results are pub-
licly available2.

2 recurrent neural network grammars

an id56g de   nes a joint id203 distribution
over string terminals and phrase-structure nonter-
minals.3 formally, the id56g is de   ned by a
triple (cid:104)n,   ,   (cid:105), where n denotes the set of non-
terminal symbols (np, vp, etc.),    the set of all
terminal symbols (we assume that n        =    ),
and    the set of all model parameters. unlike
previous works that rely on hand-crafted rules to
compose more    ne-grained phrase representations
(collins, 1997; klein and manning, 2003), the
id56g implicitly parameterizes the information
passed through compositions of phrases (in    and
the neural network architecture), hence weakening
the strong independence assumptions in classical
id140.

the id56g is based on an abstract state ma-
chine like those used in transition-based parsing,
with its algorithmic state consisting of a stack
of partially completed constituents, a buffer of
already-generated terminal symbols, and a list of
past actions. to generate a sentence x and its
phrase-structure tree y, the id56g samples a se-
quence of actions to construct y top-down. given
y, there is one such sequence (easily identi   ed),
which we call the oracle, a = (cid:104)a1, . . . , an(cid:105) used
during supervised training.

the id56g uses three different actions:

    nt(x), where x     n, introduces an open non-

terminal symbol onto the stack, e.g.,    (np   ;

    gen(x), where x       , generates a terminal
symbol and places it on the stack and buffer; and
    reduce indicates a constituent is now com-
plete. the elements of the stack that comprise
the current constituent (going back to the last

2https://github.com/clab/id56g/tree/

master/interpreting-id56g

3dyer et al. (2016) also de   ned a conditional version of
the id56g that can be used only for parsing; here we focus
on the generative version since it is more    exible and (rather
surprisingly) even learns better estimates of p(y | x).

figure 1: the id56g consists of a stack, buffer of
generated words, and list of past actions that lead
to the current con   guration. each component is
embedded with lstms, and the parser state sum-
mary ut is used as top-layer features to predict a
softmax over all feasible actions. this    gure is
due to dyer et al. (2016).

open nonterminal) are popped, a composition
function is executed, yielding a composed rep-
resentation that is pushed onto the stack.

at each timestep, the model encodes the stack,
buffer, and past actions, with a separate lstm
(hochreiter and schmidhuber, 1997) for each
component as features to de   ne a distribution over
the next action to take (conditioned on the full
algorithmic state). the overall architecture is il-
lustrated in figure 1; examples of full action se-
quences can be found in dyer et al. (2016).

a key element of the id56g is the composition
function, which reduces a completed constituent
into a single element on the stack. this function
computes a vector representation of the new con-
stituent; it also uses an lstm (here a bidirectional
one). this composition function, which we con-
sider in greater depth in   3, is illustrated in fig. 2.

figure 2: id56g composition function on each
reduce operation; the network on the right mod-
els the structure on the left (dyer et al., 2016).

since the id56g is a generative model, it at-
tempts to maximize p(x, y), the joint distribution

thehungrycatnp(vp(sreducegennt(np)nt(vp)   cathungrythea<tp(at)utttz}|{stz}|{npuvwnpuvwnpxxof strings and trees, de   ned as

p(x, y) = p(a) =

p(at | a1, . . . , at   1).

n(cid:89)t=1

in other words, p(x, y) is de   ned as a product
of local probabilities, conditioned on all past ac-
tions. the joint id203 estimate p(x, y) can
be used for both phrase-structure parsing (   nding
arg maxy p(y | x)) and id38 (   nd-
ing p(x) by marginalizing over the set of possi-
ble parses for x). both id136 problems can be
solved using an importance sampling procedure.4
we report all id56g performance based on the
corrigendum to dyer et al. (2016).

3 composition is key

given the same data, under both the discrimina-
tive and generative settings id56gs were found to
parse with signi   cantly higher accuracy than (re-
spectively) the models of vinyals et al. (2015) and
choe and charniak (2016) that represent y as a
   linearized    sequence of symbols and parentheses
without explicitly capturing the tree structure, or
even constraining the y to be a well-formed tree
(see table 1). vinyals et al. (2015) directly predict
the sequence of nonterminals,    shifts    (which con-
sume a terminal symbol), and parentheses from
left to right, conditional on the input terminal se-
quence x, while choe and charniak (2016) used a
sequential lstm language model on the same lin-
earized trees to create a generative variant of the
vinyals et al. (2015) model. the generative model
is used to re-rank parse candidates.

model
vinyals et al. (2015)     ptb only
discriminative id56g
choe and charniak (2016)     ptb only
generative id56g

f1
88.3
91.2
92.6
93.3

table 1: phrase-structure parsing performance on
ptb   23. all results are reported using single-
model performance and without any additional
data.

the results in table 1 suggest that the id56g   s
explicit composition function (fig. 2), which

4importance sampling works by using a proposal distri-
bution q(y | x) that is easy to sample from. in dyer et al.
(2016) and this paper, the proposal distribution is the discrim-
inative variant of the id56g; see dyer et al. (2016).

vinyals et al. (2015) and choe and charniak
(2016) must learn implicitly, plays a crucial role in
the id56g   s generalization success. beyond this,
choe and charniak   s generative variant of vinyals
et al. (2015) is another instance where generative
models trained on the ptb outperform discrimina-
tive models.

3.1 ablated id56gs
on close inspection, it is clear that the id56g   s
three data structures   stack, buffer, and action
history   are redundant. for example, the action
history and buffer contents completely determine
the structure of the stack at every timestep. every
generated word goes onto the stack, too; and some
past words will be composed into larger structures,
but through the composition function, they are all
still    available    to the network that predicts the
next action. similarly, the past actions are redun-
dant with the stack. despite this redundancy, only
the stack incorporates the composition function.
since each of the ablated models is suf   cient to
encode all necessary partial tree information, the
primary difference is that ablations with the stack
use explicit composition, to which we can there-
fore attribute most of the performance difference.
we conjecture that the stack   the component
that makes use of the composition function   is
critical to the id56g   s performance, and that the
buffer and action history are not.
in transition-
based parsers built on expert-crafted features, the
most recent words and actions are useful if they
are salient, although neural representation learners
can automatically learn what information should
be salient.

to test this conjecture, we train ablated rn-
ngs that lack each of the three data structures (ac-
tion history, buffer, stack), as well as one that lacks
both the action history and buffer.5 if our conjec-
ture is correct, performance should degrade most
without the stack, and the stack alone should per-
form competitively.
experimental settings. we perform our exper-
iments on the english ptb corpus, with   02   21
for training,   24 for validation, and   23 for test;
no additional data were used for training. we fol-

5note that the ablated id56g without a stack is quite sim-
ilar to vinyals et al. (2015), who encoded a (partial) phrase-
structure tree as a sequence of open and close parentheses,
terminals, and nonterminal symbols; our action history is
quite close to this, with each nt(x) capturing a left parenthe-
sis and x nonterminal, and each reduce capturing a right
parenthesis.

low the same hyperparameters as the generative
model proposed in dyer et al. (2016).6 the gen-
erative model did not use any pretrained word em-
beddings or pos tags; a discriminative variant of
the standard id56g was used to obtain tree sam-
ples for the generative model. all further experi-
ments use the same settings and hyperparameters
unless otherwise noted.

experimental results. we trained each abla-
tion from scratch, and compared these models on
three tasks: english phrase-structure parsing (la-
beled f1), table 2; id33, table 3,
by converting parse output to stanford dependen-
cies (de marneffe et al., 2006) using the tool by
kong and smith (2014); and id38,
table 4. the last row of each table reports the
performance of a novel variant of the (stack-only)
id56g with attention, to be presented in   4.

model
vinyals et al. (2015)   
choe and charniak (2016)
choe and charniak (2016)   
baseline id56g
ablated id56g (no history)
ablated id56g (no buffer)
ablated id56g (no stack)
stack-only id56g
ga-id56g

f1
92.1
92.6
93.8
93.3
93.2
93.3
92.5
93.6
93.5

table 2: phrase-structure parsing performance on
ptb   23.     indicates systems that use additional
unparsed data (semisupervised). the ga-id56g
results will be discussed in   4.

discussion. the id56g with only a stack is the
strongest of the ablations, and it even outperforms
the    full    id56g with all three data structures.
ablating the stack gives the worst among the new
results. this strongly supports the importance of
the composition function: a proper reduce oper-
ation that transforms a constituent   s parts and non-
terminal label into a single explicit (vector) repre-
sentation is helpful to performance.

it is noteworthy that the stack alone is stronger
than the original id56g, which   in principle   
can learn to disregard the buffer and action his-

6the model is trained using stochastic id119,
with a learning rate of 0.1 and a per-epoch decay of 0.08. all
experiments with the generative id56g used 100 tree sam-
ples for each sentence, obtained by sampling from the local
softmax distribution of the discriminative id56g.

model
kiperwasser and goldberg (2016)
andor et al. (2016)
dozat and manning (2016)
choe and charniak (2016)   
baseline id56g
ablated id56g (no history)
ablated id56g (no buffer)
ablated id56g (no stack)
stack-only id56g
ga-id56g

uas las
93.9
91.9
92.8
94.6
93.8
95.4
95.9
94.1
95.6
94.4
94.2
95.4
94.4
95.6
93.8
95.1
95.8
94.6
94.5
95.7

table 3: id33 performance on
ptb   23 with stanford dependencies (de marn-
effe and manning, 2008).     indicates systems that
use additional unparsed data (semisupervised).

tory. since the stack maintains syntactically    re-
cent    information near its top, we conjecture that
the learner is over   tting to spurious predictors in
the buffer and action history that explain the train-
ing data but do not generalize well.

a similar performance degradation is seen in
id38 (table 4):
the stack-only
id56g achieves the best performance, and ablat-
ing the stack is most harmful.
indeed, model-
ing syntax without explicit composition (the stack-
ablated id56g) provides little bene   t over a se-
quential lstm language model.

model
ikn 5-gram
lstm lm
id56g
ablated id56g (no history)
ablated id56g (no buffer)
ablated id56g (no stack)
stack-only id56g
ga-id56g

test ppl. (ptb)

169.3
113.4
105.2
105.7
106.1
113.1
101.2
100.9

table 4: id38: perplexity.
refers to kneser-ney 5-gram lm.

ikn

we remark that

the stack-only results are
the best published ptb results for both phrase-
structure and id33 among super-
vised models.

4 gated attention id56g
having established that the composition function
is key to id56g performance (  3), we now seek
to understand the nature of the composed phrasal
representations that are learned. like most neural
networks, interpreting the composition function   s

behavior is challenging. fortunately, linguistic
theories offer a number of hypotheses about the
nature of representations of phrases that can pro-
vide a conceptual scaffolding to understand them.

4.1 linguistic hypotheses
we consider two theories about phrasal represen-
tation. the    rst is that phrasal representations are
strongly determined by a privileged lexical head.
augmenting grammars with lexical head informa-
tion has a long history in parsing, starting with
the models of collins (1997), and theories of syn-
tax such as the    bare phrase structure    hypothe-
sis of the minimalist program (chomsky, 1993)
posit that phrases are represented purely by sin-
gle lexical heads. proposals for multiple headed
phrases (to deal with tricky cases like conjunction)
likewise exist (jackendoff, 1977; keenan, 1987).
do the phrasal representations learned by rn-
ngs depend on individual lexical heads or mul-
tiple heads? or do the representations combine all
children without any salient head?

related to the question about the role of heads
in phrasal representation is the question of whether
phrase-internal material wholly determines the
representation of a phrase (an endocentric repre-
sentation) or whether nonterminal relabeling of a
constitutent introduces new information (exocen-
tric representations). to illustrate the contrast, an
endocentric representation is representing a noun
phrase with a noun category, whereas s     np vp
exocentrically introduces a new syntactic category
that is neither np nor vp (chomsky, 1970).

4.2 gated attention composition
to investigate what the stack-only id56g learns
about headedness (and later endocentricity), we
propose a variant of the composition function
that makes use of an explicit attention mechanism
(bahdanau et al., 2015) and a sigmoid gate with
multiplicative interactions, henceforth called ga-
id56g.

at every reduce operation, the ga-id56g as-
signs an    attention weight    to each of its chil-
dren (between 0 and 1 such that the total weight
off all children sums to 1), and the parent phrase
is represented by the combination of a sum of
each child   s representation scaled by its attention
weight and its nonterminal type. our weighted
sum is more expressive than traditional head rules,
however, because it allows attention to be divided
among multiple constituents. head rules, con-

versely, are analogous to giving all attention to one
constituent, the one containing the lexical head.

we now formally de   ne the ga-id56g   s com-
position function. recall that ut is the concatena-
tion of the vector representations of the id56g   s
data structures, used to assign probabilities to each
of the actions available at timestep t (see fig. 1, the
layer before the softmax at the top). for simplicity,
we drop the timestep index here. let ont denote
the vector embedding (learned) of the nonterminal
being constructed, for the purpose of computing
attention weights.

now let c1, c2, . . . denote the sequence of vec-
tor embeddings for the constituents of the new
phrase. the length of these vectors is de   ned by
the dimensionality of the bidirectional lstm used
in the original composition function (fig. 2). we
use semicolon (;) to denote vector concatenation
operations.

the attention vector is given by:

a = softmax(cid:16)[c1 c2        ]

(cid:62)

v [u; ont ](cid:17)

(1)

note that the length of a is the same as the num-
ber of constituents, and that this vector sums to
one due to the softmax. it divides a single unit of
attention among the constituents.
next, note that the constituent source vector
m = [c1; c2;       ]a is a convex combination of the
child-constituents, weighted by attention. we will
combine this with another embedding of the non-
terminal denoted as tnt (separate from ont) using
a sigmoid gating mechanism:

g =    (w1tnt + w2m + b)

(2)

the new phrase   s    nal

note that the value of the gate is bounded between
[0, 1] in each dimension.
representation uses
element-wise multiplication ((cid:12)) with respect to
both tnt and m, a process reminiscent of the
lstm    forget    gate:

c = g (cid:12) tnt + (1     g) (cid:12) m.

(3)

the intuition is that the composed represen-
tation should incorporate both nonterminal
in-
formation and information about the constituents
(through weighted sum and attention mechanism).
the gate g modulates the interaction between
them to account for varying importance between
the two in different contexts.

experimental results. we include this model   s
performance in tables 2   4 (last row in all ta-
bles).
it is clear that the model outperforms the
baseline id56g with all three structures present
and achieves competitive performance with the
strongest, stack-only, id56g variant.

5 headedness in phrases

we now exploit the attention mechanism to probe
what the id56g learns about headedness on the
wsj   23 test set (unseen before by the model).

5.1 the heads that ga-id56g learns
the attention weight vectors tell us which con-
stituents are most important to a phrase   s vector
representation in the stack. here, we inspect the
attention vectors to investigate whether the model
learns to center its attention around a single, or
by extension a few, salient elements, which would
con   rm the presence of headedness in ga-id56g.
first, we consider several major nonterminal
categories, and estimate the average perplexity of
the attention vectors. the average perplexity can
be interpreted as the average number of    choices   
for each nonterminal category; this value is only
computed for the cases where the number of com-
ponents in the composed phrase is at least two
(otherwise the attention weight would be trivially
1). the minimum possible value for the perplexity
is 1, indicating a full attention weight around one
component and zero everywhere else.

figure 3 (in blue) shows much less than 2
average    choices    across nonterminal categories,
which also holds true for all other categories not
shown. for comparison we also report the average
perplexity of the uniform distribution for the same
nonterminal categories (fig. 3 in red); this repre-
sents the highest id178 cases where there is no
headedness at all by assigning the same attention
weight to each constituent (e.g. attention weights
of 0.25 each for phrases with four constituents).
it is clear that the learned attention weights have
much lower perplexity than the uniform distribu-
tion baseline, indicating that the learned attention
weights are quite peaked around certain compo-
nents. this implies that phrases    vectors tend to
resemble the vector of one salient constituent, but
not exclusively, as the perplexity for most cate-
gories is still not close to one.

next, we consider the how attention is dis-
tributed for the major nonterminal categories in

table 5, where the    rst    ve rows of each category
represent compositions with highest id178, and
the next    ve rows are qualitatively analyzed. the
high-id178 cases where the attention is most di-
vided represent more complex phrases with con-
junctions or more than one plausible head.

nps. in most simple noun phrases (representa-
tive samples in rows 6   7 of table 5), the model
pays the most attention to the rightmost noun
and assigns near-zero attention on determiners and
possessive determiners, while also paying nontriv-
ial attention weights to the adjectives. this    nding
matches existing head rules and our intuition that
nouns head noun phrases, and that adjectives are
more important than determiners.

we analyze the case where the noun phrase con-
tains a conjunction in the last three rows of table
5. the syntax of conjunction is a long-standing
source of controversy in syntactic analysis (johan-
nessen, 1998, inter alia). our model suggests that
several representational strategies are used, when
coordinating single nouns, both the    rst noun (8)
and the last noun (9) may be selected. however, in
the case of conjunctions of multiple noun phrases
(as opposed to multiple single-word nouns), the
model consistently picks the conjunction as the
head. all of these representational strategies have
been argued for individually on linguistic grounds,
and since we see all of them present, id56gs face
the same confusion that linguists do.
vps. the attention weights on simple verb
phrases (e.g.,    vp     v np   , 9) are peaked around
the noun phrase instead of the verb. this implies
that the verb phrase would look most similar to the
noun under it and contradicts existing head rules
that unanimously put the verb as the head of the
verb phrase. another interesting    nding is that
the model pays attention to polarity information,
where negations are almost always assigned non-
trivial attention weights.7 furthermore, we    nd
that the model attends to the conjunction terminal
in conjunctions of verb phrases (e.g.,    vp     vp
and vp   , 10), reinforcing the similar    nding for
conjunction of noun phrases.

pps.

in almost all cases, the model attends
to the preposition terminal instead of the noun
phrases or complete clauses under it, regardless of
the type of preposition. even when the preposi-

7cf. li et al. (2016), where sequential lstms discover
polarity information in id31, although perhaps
more surprising as polarity information is less intuitively cen-
tral to syntax and id38.

tional phrase is only used to make a connection
between two noun phrases (e.g.,    pp     np after
np   , 10), the prepositional connector is still con-
sidered the most salient element. this is less con-
sistent with the collins and stanford head rules,
where prepositions are assigned a lower prior-
ity when composing pps, although more consis-
tent with the johansson head rule (johansson and
nugues, 2007).

3

2.5

2

1.5

1

adjp vp

np

pp

qp sbar

figure 3: average perplexity of the learned atten-
tion vectors on the test set (blue), as opposed to
the average perplexity of the uniform distribution
(red), computed for each major phrase type.

5.2 comparison to existing head rules
to better measure the overlap between the atten-
tion vectors and existing head rules, we converted
the trees in ptb   23 into a dependency represen-
tation using the attention weights.
in this case,
the attention weight functions as a    dynamic    head
rule, where all other constituents within the same
composed phrase are considered to modify the
constituent with the highest attention weight, re-
peated recursively. the head of the composed rep-
resentation for    s    at the top of the tree is attached
to a special root symbol and functions as the head
of the sentence.

we measure the overlap between the resulting
tree and conversion results of the same trees us-
ing the collins (1997) and stanford dependencies
(de marneffe et al., 2006) head rules. results are
evaluated using the standard evaluation script (ex-
cluding punctuation) in terms of uas, since the
attention weights do not provide labels.

results. the model has a higher overlap with
the conversion using collins head rules (49.8

uas) rather than the stanford head rules (40.4
uas). we attribute this large gap to the fact that
the stanford head rules incorporate more semantic
considerations, while the id56g is a purely syn-
tactic model. in general, the attention-based tree
output has a high error rate (    90%) when the de-
pendent is a verb, since the constituent with the
highest attention weight in a verb phrase is of-
ten the noun phrase instead of the verb, as dis-
cussed above. the conversion accuracy is better
for nouns (    50% error), and much better for de-
terminers (30%) and particles (6%) with respect to
the collins head rules.

discussion. ga-id56g has the power to in-
fer head rules, and to a large extent, it does.
it
follows some conventions that are established in
one or more previous head rule sets (e.g., preposi-
tions head prepositional phrases, nouns head noun
phrases), but attends more to a verb phrase   s nom-
inal constituents than the verb.
it is important
to note that this is not the by-product of learn-
ing a speci   c model for parsing; the training ob-
jective is joint likelihood, which is not a proxy
loss for parsing performance. these decisions
were selected because they make the data maxi-
mally likely (though admittedly only locally max-
imally likely). we leave deeper consideration of
this noun-centered verb phrase hypothesis to fu-
ture work.

6 the role of nonterminal labels

emboldened by our    nding that ga-id56gs learn
a notion of headedness, we next explore whether
heads are suf   cient to create representations of
phrases (in line with an endocentric theory of
phrasal representation) or whether extra nontermi-
nal information is necessary.
if the endocentric
hypothesis is true (that is, the representation of a
phrase is built from within depending on its com-
ponents but independent of explicit category la-
bels), then the nonterminal types should be easily
inferred given the endocentrically-composed rep-
resentation, and that ablating the nonterminal in-
formation would not make much difference in per-
formance. speci   cally, we train a ga-id56g on
unlabeled trees (only bracketings without nonter-
minal types), denoted u-ga-id56g.

this idea has been explored in research on
methods for learning syntax with less complete
annotation (pereira and schabes, 1992). a key
   nding from klein and manning (2002) was that,

noun phrases

verb phrases

1 canadian (0.09) auto (0.31) workers (0.2) union (0.22) president (0.18)
2

buying (0.31) and (0.25) selling (0.21) np (0.23)
no (0.29) major (0.05) eurobond (0.32) or (0.01) foreign (0.01) bond (0.1) offerings (0.22) advp (0.27) show (0.29) prt (0.23) pp (0.21)
saatchi (0.12) client (0.14) philips (0.21) lighting (0.24) co. (0.29)
nonperforming (0.18) commercial (0.23) real (0.25) estate (0.1) assets (0.25)
the (0.1) jamaica (0.1) tourist (0.03) board (0.17) ad (0.20) account (0.40)
the (0.0)    nal (0.18) hour (0.81)
their (0.0)    rst (0.23) test (0.77)

3

4

5

6

7
8 apple (0.62) , (0.02) compaq (0.1) and (0.01) ibm (0.25)
9
10 np (0.01) , (0.0) and (0.98) np (0.01)

both (0.02) stocks (0.03) and (0.06) futures (0.88)

pleaded (0.48) adjp (0.23) pp (0.15) pp (0.08) pp (0.06) advp (0.02) because (0.73) of (0.18) np (0.07)
received (0.33) pp (0.18) np (0.32) pp (0.17)
cut (0.27) np (0.37) pp (0.22) pp (0.14)
to (0.99) vp (0.01)
were (0.77) n   t (0.22) vp (0.01)
did (0.39) n   t (0.60) vp (0.01)
handle (0.09) np (0.91)
vp (0.15) and (0.83) vp 0.02)

such (0.31) as (0.65) np (0.04)
from (0.39) np (0.49) pp (0.12)
of (0.97) np (0.03)
in (0.93) np (0.07)
by (0.96) s (0.04)
at (0.99) np (0.01)
np (0.1) after (0.83) np (0.06)

prepositional phrases

advp (0.14) on (0.72) np (0.14)
advp (0.05) for (0.54) np (0.40)

table 5: attention weight vectors for some representative samples for nps, vps, and pps.

given bracketing structure, simple dimensional-
ity reduction techniques could reveal conventional
nonterminal categories with high accuracy; petrov
et al. (2006) also showed that latent variables can
be used to recover    ne-grained nonterminal cate-
gories. we therefore expect that the vector em-
beddings of the constituents that the u-ga-id56g
correctly recovers (on test data) will capture cate-
gories similar to those in the id32.

experiments. using the same hyperparameters
and the ptb dataset, we    rst consider unlabeled
f1 parsing accuracy. on test data (with the usual
split), the ga-id56g achieves 94.2%, while the
u-ga-id56g achieves 93.5%. this result sug-
gests that nonterminal category labels add a rel-
atively small amount of information compared to
purely endocentric representations.

visualization. if endocentricity is largely suf-
   cient to account for the behavior of phrases,
where do our robust intuitions for syntactic cate-
gory types come from? we use id167 (van der
maaten and hinton, 2008) to visualize composed
phrase vectors from the u-ga-id56g model ap-
plied to the unseen test data. fig. 4 shows that the
u-ga-id56g tends to recover nonterminal cate-
gories as encoded in the ptb, even when trained
without them.8 these results suggest nontermi-
nal types can be inferred from the purely endocen-
tric compositional process to a certain extent, and
that the phrase clusters found by the u-ga-id56g
largely overlap with nonterminal categories.

analysis of pp and sbar. figure 4 indicates
a certain degree of overlap between sbar (red)
and pp (yellow). as both categories are interest-
ing from the linguistic perspective and quite sim-
ilar, we visualize the learned phrase vectors of 40
randomly selected sbars and pps from the test
set (using u-ga-id56g), illustrated in figure 5.
first, we can see that phrase representations for
pps and sbars depend less on the nonterminal

8we see a similar result for the non-ablated ga-id56g

model, not shown for brevity.

figure 4: id167 on composed vectors when train-
ing without nonterminal categories. vectors in
dark blue are vps, red are sbars, yellow are pps,
light blue are nps, and green are ss.

categories9 and more on the connector. for in-
stance, the model learns to cluster phrases that
start with words that can be either prepositions
or complementizers (e.g., for, at, to, under, by),
regardless of whether the true nonterminal labels
are pps or sbars. this suggests that sbars that
start with    prepositional    words are similar to pps
from the model   s perspective.
second, the model learns to disregard the word
that, as    sbar     that s    and    sbar     s    are
close together. this    nding is intuitive, as comple-
mentizer that is often optional (jaeger, 2010), un-
like prepositional words that might describe rela-
tive time and location. third, certain categories of
pps and sbars form their own separate clusters,
such as those that involve the words because and
of. we attribute these distinctions to the fact that
these words convey different meanings than many
prepositional words; the word of indicates posses-
sion while because indicates cause-and-effect re-
lationship. these examples show that, to a cer-
tain extent, the ga-id56g is able to learn non-

9recall that u-ga-id56g is trained without access to the
nonterminal labels; training the model with nonterminal in-
formation would likely change the    ndings.

trivial semantic information, even when trained on
a fairly small amount of syntactic data.

figure 5: sample of pp and sbar phrase repre-
sentations.

7 related work

the problem of understanding neural network
models in nlp has been previously studied for se-
quential id56s (karpathy et al., 2015; li et al.,
2016). shi et al. (2016) showed that sequence-to-
sequence neural translation models capture a cer-
tain degree of syntactic knowledge of the source
language, such as voice (active or passive) and
tense information, as a by-product of the transla-
tion objective. our experiment on the importance
of composition function was motivated by vinyals
et al. (2015) and wiseman and rush (2016), who
achieved competitive parsing accuracy without ex-
plicit composition.
in another work, li et al.
(2015) investigated the importance of recursive
tree structures (as opposed to linear recurrent mod-
els) in four different tasks, including sentiment
and semantic relation classi   cation. their    ndings
suggest that recursive tree structures are bene   cial
for tasks that require identifying long-range rela-
tions, such as semantic relationship classi   cation,
with no conclusive advantage for sentiment classi-
   cation and discourse parsing. through the stack-
only ablation we demonstrate that the id56g com-
position function is crucial to obtaining state-of-
the-art parsing performance.

extensive prior work on phrase-structure
the probabilistic
parsing typically employs
context-free grammar formalism, with lexicalized
(collins, 1997) and nonterminal (johnson, 1998;
klein and manning, 2003) augmentations. the
conjecture that    ne-grained nonterminal rules and
labels can be discovered given weaker bracketing
structures was based on several studies (chiang

and bikel, 2002; klein and manning, 2002;
petrov et al., 2006).

in a similar work, sangati and zuidema (2009)
proposed id178 minimization and greedy famil-
iarity maximization techniques to obtain lexical
heads from labeled phrase-structure trees in an un-
supervised manner.
in contrast, we used neural
attention to obtain the    head rules    in the ga-
id56g; the whole model is trained end-to-end to
maximize the log id203 of the correct ac-
tion given the history. unlike prior work, ga-
id56g allows the attention weight to be divided
among phrase constituents, essentially propagat-
ing (weighted) headedness information from mul-
tiple components.

8 conclusion

we probe what recurrent neural network gram-
mars learn about syntax,
through ablation sce-
narios and a novel variant with a gated atten-
tion mechanism on the composition function. the
composition function, a key differentiator between
the id56g and other neural models of syntax, is
crucial for good performance. using the atten-
tion vectors we discover that the model is learning
something similar to heads, although the attention
vectors are not completely peaked around a sin-
gle component. we show some cases where the
attention vector is divided and measure the rela-
tionship with existing head rules. id56gs without
access to nonterminal information during training
are used to support the hypothesis that phrasal rep-
resentations are largely endocentric, and a visu-
alization of representations shows that traditional
nonterminal categories fall out naturally from the
composed phrase vectors. this con   rms previous
conjectures that bracketing annotation does most
of the work of syntax, with nonterminal categories
easily discoverable given bracketing.

acknowledgments

this work was sponsored in part by the defense advanced
research projects agency (darpa) information innovation
of   ce (i2o) under the low resource languages for emer-
gent incidents (lorelei) program issued by darpa/i2o
under contract no. hr0011-15-c-0114;
it was also sup-
ported in part by contract no. w911nf-15-1-0543 with
darpa and the army research of   ce (aro). approved for
public release, distribution unlimited. the views expressed
are those of the authors and do not re   ect the of   cial policy
or position of the department of defense or the u.s. govern-
ment.

sbar that ssbar that ssbar that ssbar ssbar ssbar ssbar ssbar because spp advp above npsbar advp whadvp ssbar for spp at nppp at nppp after nppp by nppp under nppp by npsbar as spp to nppp to nppp to nppp on nppp advp above npsbar sbar and sbarsbar than spp than nppp from spp about npsbar whadvppp about npsbar whnp ssbar whnp ssbar whnp ssbar spp of spp of nppp of nppp of nppp of npedward l. keenan.

1987. multiply-headed noun

phrases. linguistic inquiry, 18(3):481   490.

eliyahu kiperwasser and yoav goldberg. 2016. sim-
ple and accurate id33 using bidirec-
tional lstm feature representations. tacl.

dan klein and christopher d. manning. 2002. a
generative constituent-context model for improved
grammar induction. in proc. of acl.

dan klein and christopher d. manning. 2003. accu-

rate unlexicalized parsing. in proc. of acl.

lingpeng kong and noah a. smith. 2014. an em-
pirical comparison of parsing methods for stanford
dependencies. arxiv:1404.4314.

jiwei li, dan jurafsky, and eduard hovy. 2015. when
are tree structures necessary for deep learning of rep-
resentations? in proc. of emnlp.

jiwei li, xinlei chen, eduard hovy, and dan jurafsky.
2016. visualizing and understanding neural models
in nlp. in proc. of naacl.

tom m. mitchell. 1980. the need for biases in learn-

ing generalizations.

fernando pereira and yves schabes. 1992.

inside-
outside reestimation from partially bracketed cor-
pora. in proc. of acl.

slav petrov, leon barrett, romain thibaux, and dan
klein. 2006. learning accurate, compact, and inter-
pretable tree annotation. in proc. of coling-acl.

federico sangati and willem zuidema. 2009. unsu-
pervised methods for head assignments. in proc. of
eacl.

xing shi, inkit padhi, and kevin knight. 2016. does
in

string-based neural mt learn source syntax?
proc. of emnlp.

laurens van der maaten and geoffrey e. hinton.
2008. visualizing high-dimensional data using t-
sne. journal of machine learning research, 9.

oriol vinyals, lukasz kaiser, terry koo, slav petrov,
ilya sutskever, and geoffrey hinton. 2015. gram-
mar as a foreign language. in nips.

sam wiseman and alexander m. rush.

2016.
sequence-to-sequence learning as beam-search op-
timization. in proc. of emnlp.

references
daniel andor, chris alberti, david weiss, aliaksei
severyn, alessandro presta, kuzman ganchev, slav
petrov, and michael collins. 2016. globally nor-
malized transition-based neural networks. in proc.
of acl.

dzmitry bahdanau, kyunghyun cho, and yoshua ben-
gio. 2015. id4 by jointly
learning to align and translate. in proc. of iclr.

david chiang and daniel m. bikel. 2002. recovering
in proc. of col-

latent information in treebanks.
ing.

do kook choe and eugene charniak. 2016. parsing

as id38. in proc. of emnlp.

noam chomsky. 1970. remarks on nominalization.
in readings in english transformational grammar.

noam chomsky. 1993. a minimalist program for lin-

guistic theory.

michael collins. 1997. three generative, lexicalised

models for statistical parsing. in proc. of eacl.

marie-catherine de marneffe and christopher d.
manning. 2008. stanford typed dependencies man-
ual.

marie-catherine de marneffe, bill maccartney, and
christopher d. manning. 2006. generating typed
dependency parses from phrase structure parses. in
proc. of lrec.

timothy dozat and christopher d. manning. 2016.
deep biaf   ne attention for neural dependency pars-
ing. arxiv:1611.01734.

chris dyer, adhiguna kuncoro, miguel ballesteros,
and noah a. smith. 2016. recurrent neural net-
work grammars. in proc. of naacl.

sepp hochreiter and j  urgen schmidhuber. 1997. long

short-term memory. neural computation.

ray jackendoff. 1977. x    syntax.

t. florian jaeger.

2010. redundancy and reduc-
tion: speakers manage syntactic information den-
sity. cognitive psychology, 61(1).

janne bondi johannessen. 1998. coordination.

richard johansson and pierre nugues.

2007. ex-
tended constituent-to-dependency conversion for
english. in proc. of nodalida.

mark johnson. 1998. pid18 models of linguistic tree

representations. computational linguistics.

andrej karpathy, justin johnson, and fei-fei li. 2015.
visualizing and understanding recurrent networks.
arxiv:1506.02078.

