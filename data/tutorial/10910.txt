neural semantic encoders

tsendsuren munkhdalai

university of massachusetts, ma, usa

tsendsuren.munkhdalai@umassmed.edu

hong yu

university of massachusetts, ma, usa

hong.yu@umassmed.edu

7
1
0
2

 

n
a
j
 

5

 
 
]

g
l
.
s
c
[
 
 

3
v
5
1
3
4
0

.

7
0
6
1
:
v
i
x
r
a

abstract

we present a memory augmented neural network for natural language understand-
ing: neural semantic encoders. nse is equipped with a novel memory update rule
and has a variable sized encoding memory that evolves over time and maintains the
understanding of input sequences through read, compose and write operations. nse
can also access1 multiple and shared memories. in this paper, we demonstrated the
effectiveness and the    exibility of nse on    ve different natural language tasks:
natural language id136, id53, sentence classi   cation, document
id31 and machine translation where nse achieved state-of-the-art
performance when evaluated on publically available benchmarks. for example, our
shared-memory model showed an encouraging result on id4,
improving an attention-based baseline by approximately 1.0 id7.

1

introduction

recurrent neural networks (id56s) have been successful for modeling sequences [1]. particularly,
id56s equipped with internal short memories, such as long short-term memories (lstm) [2] have
achieved a notable success in sequential tasks [3, 4]. lstm is powerful because it learns to control
its short term memories. however, the short term memories in lstm are a part of the training
parameters. this imposes some practical dif   culties in training and modeling long sequences with
lstm.
recently several studies have explored ways of extending the neural networks with an external
memory [5   7]. unlike lstm, the short term memories and the training parameters of such a neural
network are no longer coupled and can be adapted. in this paper we propose a novel class of
memory augmented neural networks called neural semantic encoders (nse) for natural language
understanding. nse offers several desirable properties. nse has a variable sized encoding memory
which allows the model to access entire input sequence during the reading process; therefore ef   ciently
delivering long-term dependencies over time. the encoding memory evolves over time and maintains
the memory of the input sequence through read, compose and write operations. nse sequentially
processes the input and supports word compositionality inheriting both temporal and hierarchical
nature of human language. nse can read from and write to a set of relevant encoding memories
simultaneously or multiple nses can access a shared encoding memory effectively supporting
knowledge and representation sharing. nse is    exible, robust and suitable for practical nlu tasks
and can be trained easily by any id119 optimizer.
we evaluate nse on    ve different real tasks. for four of them, our models set new state-of-the-
art results. our results suggest that a nn model with the shared memory between encoder and
decoder is a promising approach for sequence transduction problems such as machine translation
and abstractive summarization. in particular, we observe that the attention-based neural machine
translation can be further improved by shared-memory models. we also analyze memory access
pattern and compositionality in nse and show that our model captures semantic and syntactic
structures of input sentence.

1by access we mean changing the memory states by the read, compose and write operations.

figure 1: high-level architectures of the neural semantic encoders. nse reads and writes its
own encoding memory in each time step (a). mma-nse accesses multiple relevant memories
simultaneously (b).

2 related work

one of the pioneering work that attempts to extend deep neural networks with an external memory
is id63s (ntm) [5]. ntm implements a centralized controller and a    xed-sized
random access memory. the ntm memory is addressable by both content (i.e. soft attention) and
location based access mechanisms. the authors evaluated ntm on algorithmic tasks such as copying
and sorting sequences.
comparison with id63s: nse addresses certain drawbacks of ntm. ntm has
a single centralized controller, which is usually an mlp or id56 while nse takes a modular approach.
the main controller in nse is decomposed into three separate modules, each of which performs
for read, compose or write operation. in nse, the compose module is introduced in addition to the
standard memory update operations (i.e. read-write) in order to process the memory entries and input
information.
the main advantage of nse over ntm is in its memory update. despite its sophisticated addressing
mechanism, the ntm controller does not have mechanism to avoid information collision in the
memory. particularly the ntm controller emits two separate set of access weights (i.e. read weight
and erase and write weights) that do not explicitly encode the knowledge about where information
is read from and written to. moreover the    xed-size memory in ntm has no memory allocation or
de-allocation protocol. therefore unless the controller is intelligent enough to track the previous
read/write information, which is hard for an id56 when processing long sequences, the memory
content is overlapped and information is overwritten throughout different time scales. we think that
this is a potential reason that makes ntm hard to train and makes the training not stable. we also
note that the effectiveness of the location based addressing introduced in ntm is unclear. in nse,
we introduce a novel and systematic memory update approach based on the soft attention mechanism.
nse writes new information to the most recently read memory locations. this is accomplished by
sharing the same memory key vector between the read and write modules. the nse memory update
is scalable and potentially more robust to train. nse is provided with a variable sized memory and
thus unlike ntm, the size of the nse memory is more relaxed. the novel memory update mechanism
and the variable sized memory together prevent nse from the information collision issue and avoid
the need of the memory allocation or de-allocation protocols. each memory location of the nse
memory stores a token representation in input sequence during encoding. this provides nse with an
anytime-access to the entire input sequence including the tokens from the future time scales, which is
not permitted in ntm, id56 and attention-based encoders.
lastly, ntm addresses small algorithmic problems while nse focuses on a set of large-scale language
understanding tasks.
the id56search model proposed in [8] can be seen as a variation of memory augmented networks due
to its ability to read the historic output states of id56s with soft attention. the work of [9] combines
the soft attention with memory networks (memnns) [6]. similar to id56search, memnns are
designed with non-writable memories. it constructs layered memory representations and showed
promising results on both arti   cial and real id53 tasks. we note that id56search
and memnns avoid the memory update and management overhead by simply using a non-writable

2

memoryreadinputoutputshared memoryinput(a)(b)composewritememoryreadoutputcomposewritememory storage. another variation of memnns is dynamic memory network [10] that is equipped
with an episodic memory and seems to be    exible in different settings.
although nse differs from other memory-augumented nn models in many aspects, they all use
soft attention mechanism with a type of similarity measures to retrieve relevant information from
the external memory. for example, ntm implements cosine similarity and memnns use vector dot
product. nse uses the vector dot product for the similarity measure in nse because it is faster to
compute.
other related work includes neural program-interpreters [11], which learns to run sub-programs and
to compose them for high-level programs. it uses execution traces to provide the full supervision.
researchers have also explored ways to add unbounded memory to lstm [7] using a particular data
structure. although this type of architecture provides a    exible capacity to store information, the
memory access is constrained by the data structure used for the memory bank, such as stack and
queue.
overall it is expensive to train and to scale the previously proposed memory-based models. most
models required a set of clever engineering tricks to work successfully. most of the aforementioned
memory augmented neural networks have been tested on synthetic tasks whereas in this paper we
evaluated nse on a wide range of real and large-scale natural language applications.

2, . . . , wi
ti

is a sequence
of tokens while the output y i can be either a single target or a sequence. we

3 proposed approach
our training set consists of n examples {x i, y i}n
1, wi
wi
transform each input token wt to its id27 xt.
our neural semantic encoders (nse) model has four main components: read, compose and write
modules and an encoding memory m     rk  l with a variable number of slots, where k is the
embedding dimension and l is the length of the input sequence. each memory slot vector mt     rk
corresponds to the vector representation of information about word wt in memory. in particular, the
memory is initialized by the embedding vectors {xt}l
t=1 and is evolved over time, through read,
compose and write operations. figure 1 (a) illustrates the architecture of nse.

i=1, where the input x i

3.1 read, compose and write

nse performs three main operations in every time step. after initializing the memory slots with the
corresponding input representations, nse processes an embedding vector xt and retrieves a memory
slot mr,t that is expected to be associatively coherent (i.e. semantically associated) with the current
input word wt.2 the slot location r (ranging from 1 to l) is de   ned by a key vector zt which the read
module emits by attending over the memory slots. the compose module implements a composition
operation that combines the memory slot with the current input. the write module then transforms
the composition output to the encoding memory space and writes the resulting new representation
into the slot location of the memory. instead of composing the raw embedding vector xt, we use the
hidden state ot produced by the read module at time t
concretely, let el     rl and ek     rk be vectors of ones and given a read function f lst m
composition f m lp
state ht, and the encoding memory mt in time step t as

, a
nse in figure 1 (a) computes the key vector zt, the output

and a write f lst m

w

r

c

ot = f lst m
zt = sof tmax(o(cid:62)

(xt)
t mt   1)

r

mr,t = z(cid:62)
ct = f m lp

t mt   1
(ot, mr,t)

c

ht = f lst m

w

(ct)

mt = mt   1(1     (zt     ek)(cid:62)) + (ht     el)(zt     ek)(cid:62)

2such a coherence is calculated by a soft attention with dot product similarity.

3

(1)
(2)
(3)
(4)
(5)
(6)

r

where 1 is a matrix of ones,     denotes the outer product which duplicates its left vector l or k times
to form a matrix. the read function f lst m
sequentially maps the id27s to the internal
space of the memory mt   1. then equation 2 looks for the slots related to the input by computing
association degree between each memory slot and the hidden state ot. we calculate the association
degree by the dot product and transform this scores to the fuzzy key vector zt by normalizing with
sof tmax function. since our key vector is fuzzy, the slot to be composed is retrieved by taking
weighted sum of the all slots as in equation 3. this process can also be seen as the soft attention
mechanism [8]. in equation 4 and 5, we compose and process the retrieved slot with the current
hidden state and map the resulting vector to the encoder output space. finally, we write the new
representation to the memory location pointed by the key vector in equation 6 where the key vector
zt emitted by the read module is reused to inform the write module of the most recently read slots.
first the slot information that was retrieved is erased and then the new representation is located. nse
performs this iterative process until all words in the input sequence are read. the encoding memories
{m}t
although nse reads a single word at a time, it has an anytime-access to the entire sequence stored
in the encoding memory. with the encoding memory, nse maintains a mental image of the input
sequence. the memory is initialized with the raw embedding vector at time t = 0. we term such
a freshly initialized memory a baby memory. as nse reads more input content in time, the baby
memory evolves and re   nes the encoded mental image.
the read f lst m
functions are neural networks and are
the training parameters in our nse. as the name suggests, we use lstm and multi-layer id88
(mlp) in this paper. since nse is fully differentiable, it can be trained with any id119
optimizer.

t=1 and output states {h}t

t=1 are further used for the tasks.

, the composition f m lp

and the write f lst m

w

r

c

3.2 shared and multiple memory accesses

for sequence to sequence transduction tasks like id53, natural language id136 and
machine translation, it is bene   cial to access other relevant memories in addition to its own one. the
shared or the multiple memory access allows a set of nses to exchange id99s
and to communicate with each other to accomplish a particular task throughout the encoding memory.
nse can be extended easily, so that it is able to read from and write to multiple memories simul-
taneously or multiple nses are able to access a shared memory. figure 1 (b) depicts a high-level
architectural diagram of a multiple memory access-nse (mma-nse). the    rst memory (in green)
is the shared memory accessed by more than one nses. given a shared memory m n     rk  n that
has been encoded by processing a relevant sequence with length n, mma-nse with the access to
one relevant memory is de   ned as

r

t   1)

ct = f m lp

t m n
t   1

mr,t = z(cid:62)

r,t = zn(cid:62)
mn

(xt)
t mt   1)

ot = f lst m
zt = sof tmax(o(cid:62)

t mt   1
t = sof tmax(o(cid:62)
zn
t m n

(7)
(8)
(9)
(10)
(11)
(12)
(13)
(14)
(15)
and this is almost the same as standard nse. the read module now emits the additional key vector
t for the shared memory and the composition function f m lp
zn
in mma-nse, the different memory slots are retrieved from the shared memories depending on
their encoded semantic representations. they are then composed together with the current input and
written back to their corresponding slots. note that mma-nse is capable of accessing a variable
number of relevant shared memories once a composition function that takes in dynamic inputs is
chosen.

mt = mt   1(1     (zt     ek)(cid:62)) + (ht     el)(zt     ek)(cid:62)
t     ek)(cid:62)
m n
t = m n

t     ek)(cid:62)) + (ht     en)(zn

combines more than one slots.

t   1(1     (zn

(ot, mr,t, mn

ht = f lst m

(ct)

r,t)

w

c

c

4

model

classi   er with handcrafted features [12]
lstm encoders [12]
dependency tree id98 encoders [13]
spinn-pi encoders [14]
nse
mma-nse
lstm attention [15]
lstm word-by-word attention [15]
mma-nse attention
mlstm word-by-word attention [16]
lstmn with deep attention fusion [17]
decomposable attention model [18]
full tree matching nti-slstm-lstm global attention [19]

d

-
300
300
300
300
300
100
100
300
300
450
200
300

|  |m
-

train
99.7
3.0m 83.9
3.5m 83.3
3.7m 89.2
3.4m 86.2
6.3m 87.1
85.4
242k
252k
85.3
6.5m 86.9
1.9m 92.0
3.4m 89.5
582k
90.5
3.2m 88.5

test
78.2
80.6
82.1
83.2
84.6
84.8
82.3
83.5
85.4
86.1
86.3
86.8
87.3

table 1: training and test accuracy on natural language id136 task. d is the id27 size
and |  |m the number of model parameters.

4 experiments

we describe in this section experiments on    ve different tasks, in order to show that nse can be
effective and    exible in different settings.3 we report results on natural language id136, question
answering (qa), sentence classi   cation, document id31 and machine translation. all
   ve tasks challenge a model in terms of language understanding and semantic reasoning.
the models are trained using adam [20] with hyperparameters selected on development set. we
chose two one-layer lstm for read/write modules on the tasks other than qa on which we used
two-layer lstm. the pre-trained 300-d glove 840b vectors and 100-d glove 6b vectors [21] were
obtained for the id27s.4 the id27s are    xed during training. the embeddings
for out-of-vocabulary words were set to zero vector. we crop or pad the input sequence to a    xed
length. a padding vector was inserted when padding. the models were regularized by using dropouts
and an l2 weight decay.5

4.1 natural language id136

the natural language id136 is one of the main tasks in language understanding. this task tests
the ability of a model to reason about the semantic relationship between two sentences. in order to
perform well on the task, nse should be able to capture sentence semantics and be able to reason the
relation between a sentence pair, i.e., whether a premise-hypothesis pair is entailing, contradictory or
neutral. we conducted experiments on the stanford natural language id136 (snli) dataset [12],
which consists of 549,367/9,842/9,824 premise-hypothesis pairs for train/dev/test sets and target label
indicating their relation.
following the setting in [13, 14] the nse output for each sentence was the input to a mlp, where
the input layer computes the concatenation [hp
l and elementwise
product hp
l of the two sentence representations. in addition, the mlp has a hidden layer with
1024 units with relu activation and a sof tmax layer. we set the batch size to 128, the initial
learning rate to 3e-4 and l2 regularizer strength to 3e-5, and train each model for 40 epochs. the
write/read neural nets and the last linear layer were regularized by using 30% dropouts.
we evaluated three different variations of nse show in table 1. the nse model encodes each
sentence simultaneously by using a separate memory for each sentence. the second model - mma-
nse    rst encodes the premise and then the hypothesis sentence by sharing the premise encoded
memory in addition to the hypothesis memory. for the third model, we use inter-sentence attention
which selectively reconstructs the premise representation.

l ], absolute difference hp

l     hh

l    hh

l ; hh

3code for the experiments and nses is available at https://bitbucket.org/tsendeemts/nse.
4http://nlp.stanford.edu/projects/glove/
5more detail on hyper-parameters can be found in code.

5

table 1 shows the results of our models along with the results of published methods for the task.
the classi   er with handcrafted features extracts a set of lexical features. the next group of models
are based on sentence encoding. while most of the sentence encoder models rely solely on word
embeddings, the dependency tree id98 and the spinn-pi models make use of sentence parser output.
the spinn-pi model is similar to nse in spirit that it also explicitly computes word composition.
however, the composition in the spinn-pi is guided by supervisions from a dependency parser. nse
outperformed the previous sentence encoders on this task. the mma-sne further slightly improved
the result, indicating that reading the premise memory is helpful while encoding the hypothesis.
the last set of methods designs inter-sentence relation with parameterized soft attention [8]. our
mma-nse attention model is similar to the lstm attention model. particularly, it attends over the
premise encoder outputs {hp}t
l and constructs
an attentively blended vector of the premise. this model obtained 85.4% accuracy score. the best
performing model for this task performs tree matching with attention mechanism and lstm.

t=1 in respect to the    nal hypothesis representation hh

4.2 answer sentence selection

answer sentence selection is an integral part of the open-domain id53. for this task,
a model is trained to identify the correct sentences that answer a factual question, from a set of
candidate sentences. we experiment on wikiqa dataset constructed from wikipedia [26]. the
dataset contains 20,360/2,733/6,165 qa pairs for train/dev/test sets.
the mlp setup used in the language id136 task is kept same, except that we now replace the
sof tmax layer with a sigmoid layer and model the following id155 distribution.

p  (y = 1|hq

l , ha

l ) = sigmoid(oqa)

(16)

l and ha

where hq
l are the question and the answer encoded vectors and oqa denotes the output of the
hidden layer of the mlp. we trained the mma-nse attention model to minimize the sigmoid cross
id178 loss. mma-nse    rst encodes the answers and then the questions by accessing its own and
the answer encoding memories. in our preliminary experiment, we found that the multiple memory
access and the attention over answer encoder outputs {ha}t
t=1 are crucial to this problem. following
previous work, we adopt map and mrr as the id74 for this task.6
we set the batch size to 4 and the initial learning rate to 1e-5, and train the model for 10 epochs.
we used 40% dropouts after id27s and no l2 weight decay. the id27s are
pre-trained 300-d glove 840b vectors. for this task, a linear mapping layer transforms the 300-d
id27s to the 512-d lstm inputs.
table 2 presents the results of our model and the previous models for the task.7 the classi   er with
handcrafted features is a id166 model trained with a set of features. the bigram-id98 model is
a simple convolutional neural net. while the lstm and lstm id12 outperform the
previous best result by nearly 5-6% by implementing deep lstm with three hidden layers, nasm
improves it further and sets a strong baseline by combining variational auto-encoder [27] with the
soft attention. our mma-nse attention model exceeds the nasm by approximately 1% on map
and 0.8% on mrr for this task.

6we used trec_eval script to calculate the id74
7inclusion of simple word count feature improves the performance by around 0.15-0.3 across the board

model

classi   er with features [22]
paragraph vector [23]
bigram-id98 [24]
3-layer lstm [25]
3-layer lstm attention [25]
nasm [25]
mma-nse attention

map
0.5993
0.5110
0.6190
0.6552
0.6639
0.6705
0.6811

mrr
0.6068
0.5160
0.6281
0.6747
0.6828
0.6914
0.6993

table 2: experiment results on answer sentence selection.

6

model

rntn [28]
paragraph vector [23]
id98-mc [29]
did56 [30]
2-layer lstm[31]
bi-lstm[31]
ct-lstm[31]
dmn [10]
nse

bin
85.4
87.8
88.1
86.6
86.3
87.5
88.0
88.6
89.7

fg
45.7
48.7
47.4
49.8
46.0
49.1
51.0
52.1
52.8

table 3: test accuracy for sentence classi   cation. bin: binary, fg:    ne-grained 5 classes.

4.3 sentence classi   cation

we evaluated nse on the stanford sentiment treebank (sst) [28]. this dataset comes with standard
train/dev/test sets and two subtasks: binary sentence classi   cation or    ne-grained classi   cation of
   ve classes. we trained our model on the text spans corresponding to labeled phrases in the training
set and evaluated the model on the full sentences.
the sentence representations were passed to a two-layer mlp for classi   cation. the    rst layer of the
mlp has relu activation and 1024 or 300 units for binary or    ne-grained setting. the second layer
is a sof tmax layer. the read/write modules are two one-layer lstm with 300 hidden units and the
id27s are the pre-trained 300-d glove 840b vectors. we set the batch size to 64, the
initial learning rate to 3e-4 and l2 regularizer strength to 3e-5, and train each model for 25 epochs.
the write/read neural nets and the last linear layer were regularized by 50% dropouts.
table 3 compares the result of our model with the state-of-the-art methods on the two subtasks.
most best performing methods exploited the parse tree provided in the treebank on this task with
the exception of the dmn. the dynamic memory network (dmn) model is a memory-augmented
network. our model outperformed the dmn and set the state-of-the-art results on both subtasks.

4.4 document id31

we evaluated our models for document-level id31 on two publically available large-
scale datasets: the imdb consisting of 335,018 movie reviews and 10 different classes and yelp 13
consisting of 348,415 restaurant reviews and 5 different classes. each document in the datasets is
associated with human ratings and we used these ratings as gold labels for sentiment classi   cation.
particularly, we used the pre-split datasets of [32].
we stack a nse or lstm on the top of another nse for document modeling. the    rst nse encodes
the sentences and the second nse or lstm takes sentence encoded outputs and constructs document
representations. the id194 is given to a output sof tmax layer. the whole network
is trained jointly by backpropagating the cross id178 loss. we used one-layer lstm with 100
hidden units for the read/write modules and the pre-trained 100-d glove 6b vectors for this task.
we set the batch size to 32, the initial learning rate to 3e-4 and l2 regularizer strength to 1e-5, and
trained each model for 50 epochs. the write/read neural nets and the document-level nse/lstm
were regularized by 15% dropouts and the softmax layer by 20% dropouts. in order to speedup the
training, we created document buckets by considering the number of sentences per document, i.e.,
documents with the same number of sentences were put together in the same bucket. the buckets
were shuf   ed and updated per epoch. we did not use curriculum scheduling [33], although it is
observed to help sequence training.
table 4 shows our results. we report two performance metrics: accuracy and mse. the best results
on the task were previously obtained by conv-gid56 and lstm-gid56, which are also stacked
models. these models    rst learn the sentence representations with a id98 or lstm and then
combine them for id194 using a gated recurrent neural network (gid56). our nse
models outperformed the previous state-of-the-art models in terms of both accuracy and mse, by
approximately 2-3%. on the other hand, all systems tend to show poor results on the imdb dataset.
that is, the imdb dataset contains longer documents than the yelp 13 and it has 10 classes while the

7

model
classi   er [32]
pv [32]
id98 [32]
conv-gid56 [32]
lstm-gid56 [32]
nse-nse
nse-lstm

yelp 13

acc mse
0.68
59.8
0.86
57.7
0.76
59.7
63.7
0.56
0.50
65.1
0.48
66.6
67.0
0.47

imdb

acc mse
3.56
40.5
4.69
34.1
3.30
37.6
42.5
2.71
3.00
45.3
1.94
48.3
48.1
1.98

table 4: results of document-level sentiment classi   cation. pv: paragraph vector, acc: accuracy,
and mse: mean squared error.

model

train
baseline lstm-lstm 28.06
28.73
nse-lstm
nse-nse
29.89

dev
17.96
17.67
18.53

test
17.02
17.13
17.93

table 5: id7 scores for english-german translation task.

yelp 13 dataset has    ve classes to distinguish.8 the stacked nses (nse-nse) performed slightly
better than the nse-lstm on the imdb dataset. this is possibly due to the encoding memory of
the document level nse that preserves the long dependency in documents with a large number of
sentences.

4.5 machine translation

lastly, we conducted an experiment on id4 (id4). the id4 problem is
mostly de   ned within the encoder-decoder framework [34, 3, 35]. the encoder provides the semantic
and syntactic information about the source sentences to the decoder and the decoder generates the
target sentences by conditioning on this information and its partially produced translation. for an
ef   cient encoding, the attention-based ntm was introduced [8].
for ntm, we implemented three different models. the    rst model is a baseline model and is
similar to the one proposed in [8] (id56search). this model (lstm-lstm) has two lstm for
the encoder/decoder and has the soft attention neural net, which attends over the source sentence
and constructs a focused encoding vector for each target word. the second model is an nse-lstm
encoder-decoder which encodes the source sentence with nse and generates the targets with the
lstm network by using the nse output states and the attention network. the last model is an
nse-nse setup, where the encoding part is the same as the nse-lstm while the decoder nse
now uses the output state and has an access to the encoder memory, i.e., the encoder and the decoder
nses access a shared memory. the memory is encoded by the    rst nses and then read/written by
the decoder nses. we used the english-german translation corpus from the iwslt 2014 evaluation
campaign [36]. the corpus consists of sentence-aligned translation of ted talks. the data was
pre-processed and lowercased with the moses toolkit.9 we merged the dev2010 and dev2012 sets
for development and the tst2010, tst2011 and tst2012 sets for test data10. sentence pairs with length
longer than 25 words were    ltered out. this resulted in 110,439/4,998/4,793 pairs for train/dev/test
sets. we kept the most frequent 25,000 words for the german dictionary. the english dictionary has
51,821 words. the 300-d glove 840b vectors were used for embedding the words in the source
sentence whereas a lookup embedding layer was used for the target german words. note that the
id27s are usually optimized along with the id4 models. however, for the evaluation
purpose we in this experiment do not optimize the english id27s. besides, we do not use
a id125 to generate the target sentences.
the lstm encoder/decoders have two layers with 300 units. the nse read/write modules are two
one-layer lstm with the same number of units as the lstm encoder/decoders. this ensures that

8the average number of sentences and words in a document for imdb: 14, 152 and yelp 13: 9, 326
9https://github.com/moses-smt/mosesdecoder
10we modi   ed preparedata.sh script: https://github.com/facebookresearch/mixer

8

figure 2: word association or composition graphs produced by nse memory access. the directed
arcs connect the words that are composed via compose module. the source nodes are input words
and the destination nodes (pointed by the arrows) correspond to the accessed memory slots. < s >
denotes the beginning of sequence.

the number of parameters of the models is roughly the equal. the models were trained to minimize
word-level cross id178 loss and were regularized by 20% input dropouts and the 30% output
dropouts. we set the batch size to 128, the initial learning rate to 1e-3 for lstm-lstm and 3e-4 for
the other models and l2 regularizer strength to 3e-5, and train each model for 40 epochs. we report
id7 score for each models.11
table 5 reports our results. the baseline lstm-lstm encoder-decoder (with attention) obtained
17.02 id7 on the test set. the nse-lstm improved the baseline slightly. given this very small
improvement of the nse-lstm, it is unclear whether the nse encoder is helpful in id4. however,
if we replace the lstm decoder with another nse and introduce the shared memory access to
the encoder-decoder model (nse-nse), we improve the baseline result by almost 1.0 id7. the
nse-nse model also yields an increasing id7 score on dev set. the result demonstrates that
the attention-based id4 systems can be improved by a shared-memory encoder-decoder model.
in addition, memory-based id4 systems should perform well on translation of long sequences by
preserving long term dependencies.

5 qualitative analysis

5.1 memory access and compositionality

nse is capabable of performing multiscale composition by retrieving associative slots for a particular
input at a time step. we analyzed the memory access order and the compositionality of memory slot
and the input word in the nse model trained on the snli data.
figure 2 shows the word association graphs for the two sentence picked from snli test set. the
association graph was constructed by inspecting the key vector z. for an input word, we connect it to
the most active slot pointed by z12.
note the graph components clustered around the semantically rich words: "sits", "wall" and "autumn"
(a) and "three", "puppies", "tub" and "vet" (b). the memory slots corresponding to words that are
semantically rich in the current context are the most frequently accessed. the graph is able to capture
certain syntactic structures including phrases (e.g., "hand built rock wall") and modi   er relations
(between "sits" and "quietly" and between "tub" and "sprayed with water"). another interesting
property is that the model tends to perform sensible compositions while processing the input sentence.
for example, nse retrieved the memory slot corresponding to "wall" or "three" when reading the
input "rock" or "are".
in appendix a, we show a step-by-step visualization of nse memory states for the    rst sentence.
note how the encoding memory is evolved over time. in time step four (t = 4), the memory slot for
"quietly" encodes information about "quiet(ly) little child". when t = 6, the model forms another
composition involving "quietly", "quietly sits". in the last time step, we are able to    nd the most or
the least frequently accessed slots in the memory. the least accessed slots correspond to function
words while the frequently accessed slots are content words and tend to carry out rich semantics and

11we computed the id7 score with multi-id7.perl script of the moses toolkit
12since z is fuzzy, we visualize the highest scoring slot. for a few inputs, z pointed to a slot corresponding to

the same word. in this case, we masked out those slots and showed the second best scoring slot.

9

(a)(b)intrinsic compositions found in the input sentence. overall the model is less constrained and is able
to compose multiword expressions.

6 conclusion

our proposed memory augmented neural networks have achieved the state-of-the-art results when
evaluated on    ve representative nlp tasks. nse is capable of building an ef   cient architecture of the
single, shared and multiple memory accesses for a speci   c nlp task. for example, for the nli task
nse accesses premise encoded memory when processing hypothesis. for the qa task, nse accesses
answer encoded memory when reading question for qa. in machine translation, nse shares a single
encoded memory between encoder and decoder. such    exibility in the architectural choice of the
nse memory access allows for the robust models for a better performance.
the initial state of the nse memory stores information about each word in the input sequence. we in
this paper used id27s to represent the words in the memory. different variations of word
representations such as character-based models are left to be evaluated for memory initialization
in the future. we plan to extend nse so that it learns to select and access a relevant subset from a
memory set. one could also explore unsupervised variations of nse, for example, to train them to
produce encoding memory and representation vector of entire sentences or documents using either
new or existing models such as the skip-gram model [37].

acknowledgments

we would like to thank abhyuday jagannatha and the anonymous reviewers for their insightful
comments and suggestions. this work was supported in part by the grant hl125089 from the
national institutes of health (nih). any opinions,    ndings and conclusions or recommendations
expressed in this material are those of the authors and do not necessarily re   ect those of the sponsor.

references
[1] jeffrey l elman. finding structure in time. cognitive science, 14(2):179   211, 1990.
[2] sepp hochreiter and j  rgen schmidhuber. long short-term memory. neural computation, 9(8):1735   1780,

1997.

2014.

[3] kyunghyun cho, bart van merri  nboer, caglar gulcehre, dzmitry bahdanau, fethi bougares, holger
schwenk, and yoshua bengio. learning phrase representations using id56 encoder-decoder for statistical
machine translation. arxiv preprint arxiv:1406.1078, 2014.

[4] oriol vinyals,   ukasz kaiser, terry koo, slav petrov, ilya sutskever, and geoffrey hinton. grammar as a

foreign language. in nips, 2015.

[5] alex graves, greg wayne, and ivo danihelka. id63s. arxiv preprint arxiv:1410.5401,

[6] jason weston, sumit chopra, and antoine bordes. memory networks. in icml 2015, 2015.
[7] edward grefenstette, karl moritz hermann, mustafa suleyman, and phil blunsom. learning to transduce

with unbounded memory. in nips 2015, pages 1819   1827, 2015.

[8] dzmitry bahdanau, kyunghyun cho, and yoshua bengio. id4 by jointly learning

[9] sainbayar sukhbaatar, jason weston, rob fergus, et al. end-to-end memory networks. in nips 2015,

to align and translate. in iclr, 2015.

pages 2431   2439, 2015.

[10] ankit kumar, ozan irsoy, jonathan su, james bradbury, robert english, brian pierce, peter ondruska,
ishaan gulrajani, and richard socher. ask me anything: dynamic memory networks for natural language
processing. corr, abs/1506.07285, 2016.

[11] scott reed and nando de freitas. neural programmer-interpreters. in iclr 2016, 2016.
[12] samuel r. bowman, gabor angeli, christopher potts, and christopher d. manning. a large annotated

corpus for learning natural language id136. in emnlp, 2015.

[13] lili mou, rui men, ge li, yan xu, lu zhang, rui yan, and zhi jin. recognizing entailment and

contradiction by tree-based convolution. in acl 2016, 2016.

[14] samuel r. bowman, jon gauthier, abhinav rastogi, raghav gupta, christopher d. manning, and
christopher potts. a fast uni   ed model for parsing and sentence understanding. corr, abs/1603.06021,
2016.

[15] tim rockt  schel, edward grefenstette, karl moritz hermann, tom     ko  cisk`y, and phil blunsom. reason-

ing about entailment with neural attention. in iclr 2016, 2016.

10

[16] shuohang wang and jing jiang. learning natural language id136 with lstm. corr, abs/1512.08849,

2015.

corr, abs/1601.06733, 2016.

[17] jianpeng cheng, li dong, and mirella lapata. long short-term memory-networks for machine reading.

[18] ankur p parikh, oscar t  ckstr  m, dipanjan das, and jakob uszkoreit. a decomposable attention model

for natural language id136. arxiv preprint arxiv:1606.01933, 2016.

[19] tsendsuren munkhdalai and hong yu. neural tree indexers for text understanding. arxiv preprint

arxiv:1607.04492, 2016.

[20] diederik kingma and jimmy ba. adam: a method for stochastic optimization. in iclr, 2014.
[21] jeffrey pennington, richard socher, and christopher d manning. glove: global vectors for word

representation. in emnlp, volume 14, pages 1532   1543, 2014.

[22] wen tau yih, ming-wei chang, christopher meek, and andrzej pastusiak. id53 using

enhanced lexical semantic models. in acl 2013, 2013.

[23] quoc v. le and tomas mikolov. distributed representations of sentences and documents. in icml 2014,

[24] lei yu, karl moritz hermann, phil blunsom, and stephen pulman. deep learning for answer sentence

selection. in nips deep learning workshop 2014, 2014.

[25] yishu miao, lei yu, and phil blunsom. neural variational id136 for text processing. in iclr 2016,

2014.

2016.

[26] yi yang, wen-tau yih, and christopher meek. wikiqa: a challenge dataset for open-domain question

answering. in emnlp 2015, 2015.

[27] diederik p kingma and max welling. auto-encoding id58. in iclr 2014, 2014.
[28] richard socher, alex perelygin, jean y wu, jason chuang, christopher d manning, andrew y ng, and
christopher potts. recursive deep models for semantic compositionality over a sentiment treebank. in
emnlp 2013, 2013.

[29] yoon kim. convolutional neural networks for sentence classi   cation. in emnlp, 2014.
[30] ozan irsoy and claire cardie. modeling compositionality with multiplicative recurrent neural networks.

in iclr 2015, 2015.

[31] kai sheng tai, richard socher, and christopher d manning. improved semantic representations from

tree-structured id137. in acl 2015, 2015.

[32] duyu tang, bing qin, and ting liu. document modeling with gated recurrent neural network for sentiment

classi   cation. in emnlp, 2015.

[33] yoshua bengio, j  r  me louradour, ronan collobert, and jason weston. curriculum learning.

in
proceedings of the 26th annual international conference on machine learning, pages 41   48. acm, 2009.
[34] nal kalchbrenner and phil blunsom. recurrent continuous translation models. in emnlp, volume 3, page

[35] ilya sutskever, oriol vinyals, and quoc v le. sequence to sequence learning with neural networks. in

[36] mauro cettolo, christian girardi, and marcello federico. wit3: web inventory of transcribed and translated

413, 2013.

nips, pages 3104   3112, 2014.

talks. in eamt, trento, italy, 2012.

[37] tomas mikolov, ilya sutskever, kai chen, greg s corrado, and jeff dean. distributed representations of

words and phrases and their compositionality. in nips, pages 3111   3119, 2013.

11

a step-by-step visualization of memory states in nse

each small table represents the memory state at a single time step. the current time step and input
token are listed on the top of the table. the memory slots pointed by the query vector is highlighted
in red color. the brackets represent the word composition order in each slot.

t=3
input: little
(a <s>)
(<s>a)
little
child
sits
(little quietly)
on
a
hand
built
rock
wall
in
autumn

t=9
input: hand
(a <s>)
(<s>a)
little
child
(quietly sits)
(child (little quietly))
on
a
hand
(a built)
rock
(hand wall)
in
(on (sits autumn))

t=4
input: child
(a <s>)
(<s>a)
little
child
sits
(child (little quietly))
on
a
hand
built
rock
wall
in
autumn

t=10
input: built
(a <s>)
(<s>a)
little
child
(quietly sits)
(child (little quietly))
on
a
hand
(a built)
rock
(built (hand wall))
in
(on (sits autumn))

t=5
input: sits
(a <s>)
(<s>a)
little
child
sits
(child (little quietly))
on
a
hand
built
rock
wall
in
(sits autumn)

t=11
input: rock
(a <s>)
(<s>a)
little
child
(quietly sits)
(child (little quietly))
on
a
hand
(a built)
rock
(rock (built (hand wall)))
in
(on (sits autumn))

t=0
input:
<s>
a
little
child
sits
quietly
on
a
hand
built
rock
wall
in
autumn

t=6
input: quietly
(a <s>)
(<s>a)
little
child
(quietly sits)
(child (little quietly))
on
a
hand
built
rock
wall
in
(sits autumn)

t=12
input: wall
(a <s>)
(<s>a)
little
child
(wall (quietly sits))
(child (little quietly))
on
a
hand
(a built)
rock
(rock (built (hand wall)))
in
(on (sits autumn))

t=1
input: <s>
<s>
(<s>a)
little
child
sits
quietly
on
a
hand
built
rock
wall
in
autumn

t=7
input: on
(a <s>)
(<s>a)
little
child
(quietly sits)
(child (little quietly))
on
a
hand
built
rock
wall
in
(on (sits autumn))

t=13
input: in
(a <s>)
(<s>a)
little
child
(wall (quietly sits))
(child (little quietly))
on
a
hand
(a built)
(in rock)
(rock (built (hand wall)))
in
(on (sits autumn))

t=2
input: a
(a <s>)
(<s>a)
little
child
sits
quietly
on
a
hand
built
rock
wall
in
autumn

t=8
input: a
(a <s>)
(<s>a)
little
child
(quietly sits)
(child (little quietly))
on
a
hand
(a built)
rock
wall
in
(on (sits autumn))

t=14
input: autumn
(a <s>)
(<s>a)
little
child
(wall (quietly sits))
(child (little quietly))
on
a
hand
(autumn (a built))
(in rock)
(rock (built (hand wall)))
in
(on (sits autumn))

12

