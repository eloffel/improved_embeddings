7
1
0
2

 
r
a

m
5

 

 
 
]
l
c
.
s
c
[
 
 

1
v
9
1
6
1
0

.

3
0
7
1
:
v
i
x
r
a

id4 and sequence-to-sequence models:

a tutorial

graham neubig

language technologies institute, carnegie mellon university

1 introduction

this tutorial introduces a new and powerful set of techniques variously called    neural machine
translation    or    neural sequence-to-sequence models   . these techniques have been used in
a number of tasks regarding the handling of human language, and can be a powerful tool
in the toolbox of anyone who wants to model sequential data of some sort. the tutorial
assumes that the reader knows the basics of math and programming, but does not assume
any particular experience with neural networks or natural language processing. it attempts to
explain the intuition behind the various methods covered, then delves into them with enough
mathematical detail to understand them concretely, and culiminates with a suggestion for an
implementation exercise, where readers can test that they understood the content in practice.

1.1 background

before getting into the details, it might be worth describing each of the terms that appear in
the title    id4 and sequence-to-sequence models   . machine trans-
lation is the technology used to translate between human language. think of the universal
translation device showing up in sci-    movies to allow you to communicate e   ortlessly with
those that speak a di   erent language, or any of the plethora of online translation web sites
that you can use to assimilate content that is not in your native language. this ability to re-
move language barriers, needless to say, has the potential to be very useful, and thus machine
translation technology has been researched from shortly after the advent of digital computing.
we call the language input to the machine translation system the source language, and
call the output language the target language. thus, machine translation can be described
as the task of converting a sequence of words in the source, and converting into a sequence of
words in the target. the goal of the machine translation practitioner is to come up with an
e   ective model that allows us to perform this conversion accurately over a broad variety of
languages and content.

the second part of the title, sequence-to-sequence models, refers to the broader class
of models that include all models that map one sequence to another. this, of course, includes
machine translation, but it also covers a broad spectrum of other methods used to handle
other tasks as shown in figure 1. in fact, if we think of a computer program as something
that takes in a sequence of input bits, then outputs a sequence of output bits, we could say
that every single program is a sequence-to-sequence model expressing some behavior (although
of course in many cases this is not the most natural or intuitive way to express things).

1

figure 1: an example of sequence-to-sequence modeling tasks.

the motivation for using machine translation as a representative of this larger class of

sequence-to-sequence models is many-fold:

1. machine translation is a widely-recognized and useful instance of sequence-to-sequence
models, and allows us to use many intuitive examples demonstrating the di   culties
encountered when trying to tackle these problems.

2. machine translation is often one of the main driving tasks behind the development of
new models, and thus these models tend to be tailored to mt    rst, then applied to
other tasks.

3. however, there are also cases where mt has learned from other tasks as well, and

introducing these tasks helps explain the techniques used in mt as well.

1.2 structure of this tutorial

this tutorial    rst starts out with a general mathematical de   nition of statistical techniques
for machine translation in section 2. the rest of this tutorial will sequentially describe
techniques of increasing complexity, leading up to attentional models, which represent the
current state-of-the-art in the    eld.

first, sections 3-6 focus on language models, which calculate the id203 of a target
sequence of interest. these models are not capable of performing translation or sequence
transduction, but will provide useful preliminaries to understand sequence-to-sequence mod-
els.

    section 3 describes id165 language models, simple models that calculate the prob-
ability of words based on their counts in a set of data. it also describes how we evaluate
how well these models are doing using measures such as perplexity.

    section 4 describes log-linear language models, models that instead calculate the
id203 of the next word based on features of the context. it describes how we can
learn the parameters of the models through stochastic id119     calculating
derivatives and gradually updating the parameters to increase the likelihood of the
observed data.

2

machine translation:kare wa ringo wo tabeta     he ate an appletagging:he ate an apple     prn vbd det ppdialog:he ate an apple     good, he needs to slim downid103:     he ate an appleand just about anything...:1010000111101     00011010001101    section 5 introduces the concept of neural networks, which allow us to combine
together multiple pieces of information more easily than id148, resulting in
increased modeling accuracy. it gives an example of feed-forward neural language
models, which calculate the id203 of the next word based on a few previous words
using neural networks.

    section 6 introduces recurrent neural networks, a variety of neural networks that
have mechanisms to allow them to remember information over multiple time steps.
these lead to recurrent neural network language models, which allow for the
handling of long-term dependencies that are useful when modeling language or other
sequential data.

finally, sections 7 and 8 describe actual sequence-to-sequence models capable of perform-

ing machine translation or other tasks.

    section 7 describes encoder-decoder models, which use a recurrent neural network
to encode the target sequence into a vector of numbers, and another network to decode
this vector of numbers into an output sentence. it also describes search algorithms
to generate output sequences based on this model.

    section 8 describes attention, a method that allows the model to focus on di   erent parts
of the input sentence while generating translations. this allows for a more e   cient and
intuitive method of representing sentences, and is often more e   ective than its simpler
encoder-decoder counterpart.

2 statistical mt preliminaries

first, before talking about any speci   c models, this chapter describes the overall framework
of id151 (smt) [16] more formally.

first, we de   ne our task of machine translation as translating a source sentence f =
|e|
1 .thus, any type of translation

into a target sentence e = e1, . . . , ei = e

|f|
f1, . . . , fj = f
1
system can be de   ned as a function

  e = mt(f ),

(1)

which returns a translation hypothesis   e given a source sentence f as input.
id151 systems are systems that perform translation by cre-
ating a probabilistic model for the id203 of e given f , p (e | f ;   ), and    nding the
target sentence that maximizes this id203:

  e = argmax

p (e | f ;   ),

e

(2)

where    are the parameters of the model specifying the id203 distribution. the pa-
rameters    are learned from data consisting of aligned sentences in the source and target
languages, which are called parallel corpora in technical terminology.within this frame-
work, there are three major problems that we need to handle appropriately in order to create
a good translation system:

3

modeling: first, we need to decide what our model p (e | f ;   ) will look like. what
parameters will it have, and how will the parameters specify a id203 distribution?

learning: next, we need a method to learn appropriate values for parameters    from training

data.

search: finally, we need to solve the problem of    nding the most probable sentence (solv-
ing    argmax   ). this process of searching for the best hypothesis and is often called
decoding.1

the remainder of the material here will focus on solving these problems.

3 id165 language models

while the    nal goal of a id151 system is to create a model of the
target sentence e given the source sentence f , p (e | f ), in this chapter we will take a step
back, and attempt to create a language model of only the target sentence p (e). basically,
this model allows us to do two things that are of practical use.

assess naturalness: given a sentence e, this can tell us, does this look like an actual,
natural sentence in the target language? if we can learn a model to tell us this, we can
use it to assess the    uency of sentences generated by an automated system to improve its
results. it could also be used to evaluate sentences generated by a human for purposes
of grammar checking or error correction.

generate text: language models can also be used to randomly generate text by sampling
a sentence e(cid:48) from the target distribution: e(cid:48)     p (e).2 randomly generating samples
from a language model can be interesting in itself     we can see what the model    thinks   
is a natural-looking sentences     but it will be more practically useful in the context of
the neural translation models described in the following chapters.

in the following sections, we   ll cover a few methods used to calculate this id203 p (e).

3.1 word-by-word computation of probabilities

as mentioned above, we are interested in calculating the id203 of a sentence e = et
1 .
formally, this can be expressed as

p (e) = p (|e| = t, et
1 ),

(3)
the joint id203 that the length of the sentence is (|e| = t ), that the identity of the
   rst word in the sentence is e1, the identity of the second word in the sentence is e2, up
until the last word in the sentence being et . unfortunately, directly creating a model of
this id203 distribution is not straightforward,3 as the length of the sequence t is not
determined in advance, and there are a large number of possible combinations of words.4

decoding an encoded cipher.

1this is based on the famous quote from warren weaver, likening the process of machine translation to
2    means    is sampled from   .
3although it is possible, as shown by whole-sentence language models in [88].
4question: if v is the size of the target vocabulary, how many are there for a sentence of length t ?

4

t +1(cid:89)

figure 2: an example of decomposing language model probabilities word-by-word.

as a way to make things easier, it is common to re-write the id203 of the full sen-
tence as the product of single-word probabilities. this takes advantage of the fact that a
joint id203     for example p (e1, e2, e3)     can be calculated by multiplying together con-
ditional probabilities for each of its elements. in the example, this means that p (e1, e2, e3) =
p (e1)p (e2 | e1)p (e3 | e1, e2).

figure 2 shows an example of this incremental calculation of probabilities for the sentence
   she went home   . here, in addition to the actual words in the sentence, we have introduced
an implicit sentence end (   (cid:104)/s(cid:105)   ) symbol, which we will indicate when we have terminated the
sentence. stepping through the equation in order, this means we    rst calculate the id203
of    she    coming at the beginning of the sentence, then the id203 of    went    coming next
in a sentence starting with    she   , the id203 of    home    coming after the sentence pre   x
   she went   , and then    nally the sentence end symbol    (cid:104)/s(cid:105)    after    she went home   . more
generally, we can express this as the following equation:

p (e) =

p (et | et   1

1

)

(4)

t=1

where et +1 = (cid:104)/s(cid:105). so coming back to the sentence end symbol (cid:104)/s(cid:105), the reason why we
introduce this symbol is because it allows us to know when the sentence ends. in other words,
by examining the position of the (cid:104)/s(cid:105) symbol, we can determine the |e| = t term in our
original lm joint id203 in equation 3. in this example, when we have (cid:104)/s(cid:105) as the 4th
word in the sentence, we know we   re done and our    nal sentence length is 3.
once we have the formulation in equation 4, the problem of id38 now
becomes a problem of calculating the next word given the previous words p (et | et   1
). this
is much more manageable than calculating the id203 for the whole sentence, as we now
have a    xed set of items that we are looking to calculate probabilities for. the next couple
of sections will show a few ways to do so.

1

3.2 count-based id165 language models

the    rst way to calculate probabilities is simple: prepare a set of training data from which
we can count word strings, count up the number of times we have seen a particular string of
words, and divide it by the number of times we have seen the context. this simple method,

5

p(|e| = 3, e1=   she   , e2=   went   , e3=   home   ) =p(e1=   she   )* p(e2=   went    | e1=   she   )* p(e3=   home    | e1=   she   , e2=   went   )* p(e4=   </s>    | e1=   she   , e2=   went   , e3=   home   )figure 3: an example of calculating probabilities using id113.

can be expressed by the equation below, with an example shown in figure 3

pml(et | et   1

cpre   x(et
1)
cpre   x(et   1

(5)
here cpre   x(  ) is the count of the number of times this particular word string appeared at the
beginning of a sentence in the training data. this approach is called maximum likelihood
estimation (id113, details later in this chapter), and is both simple and guaranteed to create
a model that assigns a high id203 to the sentences in training data.

) =

)

1

1

.

however, let   s say we want to use this model to assign a id203 to a new sentence
that we   ve never seen before. for example, if we want to calculate the id203 of the
sentence    i am from utah .    based on the training data in the example. this sentence is
extremely similar to the sentences we   ve seen before, but unfortunately because the string
   i am from utah    has not been observed in our training data, cpre   x(i, am, from, utah) = 0,
p (e4 = utah | e1 = i, e2 = am, e3 = from) becomes zero, and thus the id203 of the whole
sentence as calculated by equation 5 also becomes zero. in fact, this language model will
assign a id203 of zero to every sentence that it hasn   t seen before in the training corpus,
which is not very useful, as the model loses ability to tell us whether a new sentence a system
generates is natural or not, or generate new outputs.

to solve this problem, we take two measures. first, instead of calculating probabilities
from the beginning of the sentence, we set a    xed window of previous words upon which we
will base our id203 calculations, approximating the true id203.
if we limit our
context to n     1 previous words, this would amount to:

p (et | et   1

1

)     pml(et | et   1

t   n+1).

(6)

models that make this assumption are called id165 models. speci   cally, when models
where n = 1 are called unigram models, n = 2 bigram models, n = 3 trigram models, and
n     4 four-gram,    ve-gram, etc.
the parameters    of id165 models consist of probabilities of the next word given n     1

previous words:

  et

t   n+1

= p (et | et   1

t   n+1),

(7)

and in order to train an id165 model, we have to learn these parameters from data.5 in the
simplest form, these parameters can be calculated using id113 as
follows:

  et

t   n+1

= pml(et | et   1

t   n+1) =

c(et
c(et   1

t   n+1)
t   n+1)

,

(8)

5question: how many parameters does an id165 model with a particular n have?

6

i am from pittsburgh .i study at a university .my mother is from utah .p(e2=am | e1=i) = c(e1=i, e2=am)/c(e1=i) = 1 / 2 = 0.5p(e2=study | e1=i) = c(e1=i, e2=study)/c(e1=i) = 1 / 2 = 0.5where c(  ) is the count of the word string anywhere in the corpus. sometimes these equations
will reference et   n+1 where t     n + 1 < 0. in this case, we assume that et   n+1 = (cid:104)s(cid:105) where
(cid:104)s(cid:105) is a special sentence start symbol.

if we go back to our previous example and set n = 2, we can see that while the string
   i am from utah .    has never appeared in the training corpus,    i am   ,    am from   ,    from
utah   ,    utah .   , and    . (cid:104)/s(cid:105)    are all somewhere in the training corpus, and thus we can patch
together probabilities for them and calculate a non-zero id203 for the whole sentence.6
however, we still have a problem: what if we encounter a two-word string that has never
appeared in the training corpus? in this case, we   ll still get a zero id203 for that
particular two-word string, resulting in our full sentence id203 also becoming zero. n-
gram models    x this problem by smoothing probabilities, combining the maximum likelihood
estimates for various values of n.
in the simple case of smoothing unigram and bigram
probabilities, we can think of a model that combines together the probabilities as follows:

p (et | et   1) = (1       )pml(et | et   1) +   pml(et),

(9)

where    is a variable specifying how much id203 mass we hold out for the unigram
distribution. as long as we set    > 0, regardless of the context all the words in our vocabulary
will be assigned some id203. this method is called interpolation, and is one of the
standard ways to make probabilistic models more robust to low-frequency phenomena.

if we want to use even more context     n = 3, n = 4, n = 5, or more     we can recursively

de   ne our interpolated probabilities as follows:

p (et | et   1

t   m+1) = (1       m)pml(et | et   1

t   m+1) +   mp (et | et   1

t   m+2).

(10)

the    rst term on the right side of the equation is the maximum likelihood estimate for the
model of order m, and the second term is the interpolated id203 for all orders up to
m     1.

there are also more sophisticated methods for smoothing, which are beyond the scope of

this section, but summarized very nicely in [19].

context-dependent smoothing coe   cients: instead of having a    xed   , we condition
the interpolation coe   cient on the context:   et   1
. this allows the model to give
more weight to higher order id165s when there are a su   cient number of training
examples for the parameters to be estimated accurately and fall back to lower-order
id165s when there are fewer training examples. these context-dependent smoothing
coe   cients can be chosen using heuristics [118] or learned from data [77].

t   m+1

back-o   : in equation 9, we interpolated together two id203 distributions over the full
vocabulary v . in the alternative formulation of back-o   , the lower-order distribution
only is used to calculate probabilities for words that were given a id203 of zero
in the higher-order distribution. back-o    is more expressive but also more complicated
than interpolation, and the two have been reported to give similar results [41].

modi   ed distributions: it is also possible to use a di   erent distribution than pml. this
can be done by subtracting a constant value from the counts before calculating prob-
abilities, a method called discounting.
it is also possible to modify the counts of

6question: what is this id203?

7

lower-order distributions to re   ect the fact that they are used mainly as a fall-back for
when the higher-order distributions lack su   cient coverage.

currently, modi   ed kneser-ney smoothing (mkn; [19]), is generally considered one
of the standard and e   ective methods for smoothing id165 language models. mkn uses
context-dependent smoothing coe   cients, discounting, and modi   cation of lower-order distri-
butions to ensure accurate id203 estimates.

3.3 evaluation of language models

once we have a language model, we will want to test whether it is working properly. the way
we test language models is, like many other machine learning models, by preparing three sets
of data:

training data is used to train the parameters    of the model.

development data is used to make choices between alternate models, or to tune the hyper-
parameters of the model. hyper-parameters in the model above could include the
maximum length of n in the id165 model or the type of smoothing method.

test data is used to measure our    nal accuracy and report results.

for language models, we basically want to know whether the model is an accurate model
of language, and there are a number of ways we can de   ne this. the most straight-forward
way of de   ning accuracy is the likelihood of the model with respect to the development
or test data. the likelihood of the parameters    with respect to this data is equal to the
id203 that the model assigns to the data. for example, if we have a test dataset etest,
this is:

we often assume that this data consists of several independent sentences or documents e,
giving us

p (etest;   ) =

p (e;   ).

another measure that is commonly used is log likelihood

log p (etest;   ) =

log p (e;   ).

(11)

(12)

(13)

p (etest;   ).

(cid:89)
(cid:88)

e   etest

e   etest

the log likelihood is used for a couple reasons. the    rst is because the id203 of any
particular sentence according to the language model can be a very small number, and the
product of these small numbers can become a very small number that will cause numerical
precision problems on standard computing hardware. the second is because sometimes it is
more convenient mathematically to deal in log space. for example, when taking the derivative
in gradient-based methods to optimize parameters (used in the next section), it is more
convenient to deal with the sum in equation 13 than the product in equation 11.

it is also common to divide the log likelihood by the number of words in the corpus

|e|.

(14)

(cid:88)

e   etest

length(etest) =

8

this makes it easier to compare and contrast results across corpora of di   erent lengths.

the    nal common measure of language model accuracy is perplexity, which is de   ned

as the exponent of the average negative log likelihood per word

ppl(etest;   ) = e   (log p (etest;  ))/length(etest).

(15)

an intuitive explanation of the perplexity is    how confused is the model about its decision?   
more accurately, it expresses the value    if we randomly picked words from the id203
distribution calculated by the language model at each time step, on average how many words
would it have to pick to get the correct one?    one reason why it is common to see perplexities
in research papers is because the numbers calculated by perplexity are bigger, making the
di   erences in models more easily perceptible by the human eye.7

3.4 handling unknown words
finally, one important point to keep in mind is that some of the words in the test set etest
will not appear even once in the training set etrain. these words are called unknown words,
and need to be handeled in some way. common ways to do this in language models include:

assume closed vocabulary: sometimes we can assume that there will be no new words in
the test set. for example, if we are calculating a language model over ascii characters,
it is reasonable to assume that all characters have been observed in the training set.
similarly, in some id103 systems, it is common to simply assign a proba-
bility of zero to words that don   t appear in the training data, which means that these
words will not be able to be recognized.

interpolate with an unknown words distribution: as mentioned in equation 10, we
can interpolate between distributions of higher and lower order.
in the case of un-
known words, we can think of this as a distribution of order    0   , and de   ne the 1-gram
id203 as the interpolation between the unigram distribution and unknown word
distribution

p (et) = (1       1)pml(et) +   1punk(et).

(16)

here, punk needs to be a distribution that assigns a id203 to all words vall, not just
ones in our vocabulary v derived from the training corpus. this could be done by, for
example, training a language model over characters that    spells out    unknown words in
the case they don   t exist in in our vocabulary. alternatively, as a simpler approximation
that is nonetheless fairer than ignoring unknown words, we can guess the total number
of words |vall| in the language where we are modeling, where |vall| > |v |, and de   ne
punk as a uniform distribution over this vocabulary: punk(et) = 1/|vall|.

add an (cid:104)unk(cid:105) word: as a    nal method to handle unknown words we can remove some of
the words in etrain from our vocabulary, and replace them with a special (cid:104)unk(cid:105) symbol
representing unknown words. one common way to do so is to remove singletons, or
words that only appear once in the training corpus. by doing this, we explicitly predict
in which contexts we will be seeing an unknown word, instead of implicitly predicting
it through interpolation like mentioned above. even if we predict the (cid:104)unk(cid:105) symbol, we

7and, some cynics will say, making it easier for your research papers to get accepted.

9

will still need to estimate the id203 of the actual word, so any time we predict
(cid:104)unk(cid:105) at position i, we further multiply in the id203 of punk(et).

3.5 further reading

to read in more detail about id165 language models, [41] gives a very nice introduction
and comprehensive summary about a number of methods to overcome various shortcomings
of vanilla id165s like the ones mentioned above.

there are also a number of extensions to id165 models that may be nice for the interested

reader.

large-scale id38: language models are an integral part of many commer-
cial applications, and in these applications it is common to build language models using
massive amounts of data harvested from the web for other sources. to handle this data,
there is research on e   cient data structures [48, 82], distributed parameter servers [14],
and lossy compression algorithms [104].

language model adaptation: in many situations, we want to build a language model for
speci   c speaker or domain. adaptation techniques make it possible to create large
general-purpose models, then adapt these models to more closely match the target use
case [6].

longer-distance language count-based models: as mentioned above, id165 models
limit their context to n     1, but in reality there are dependencies in language that
can reach much farther back into the sentence, or even span across whole documents.
the recurrent neural network language models that we will introduce in section 6 are
one way to handle this problem, but there are also non-neural approaches such as cache
language models [61], topic models [13], and skip-gram models [41].

syntax-based language models: there are also models that take into account the syntax
of the target sentence. for example, it is possible to condition probabilities not on
words that occur directly next to each other in the sentence, but those that are    close   
syntactically [96].

3.6 exercise

the exercise that we will be doing in class will be constructing an id165 lm with linear
interpolation between various levels of id165s. we will write code to:

    read in and save the training and testing corpora.
    learn the parameters on the training corpus by counting up the number of times each
id165 has been seen, and calculating maximum likelihood estimates according to equa-
tion 8.

    calculate the probabilities of the test corpus using linearly interpolation according to

equation 9 or equation 10.

10

to handle unknown words, you can use the uniform distribution method described in sec-
tion 3.4, assuming that there are 10,000,000 words in the english vocabulary. as a sanity
check, it may be better to report the number of unknown words, and which portions of the
per-word log-likelihood were incurred by the main model, and which portion was incurred by
the unknown word id203 log punk.

in order to do so, you will    rst need data, and to make it easier to start out you can use
some pre-processed data from the german-english translation task of the iwslt evaluation
campaign8 here: http://phontron.com/data/iwslt-en-de-preprocessed.tar.gz.

potential improvements to the model include reading [19] and implementing a better
smoothing method, implementing a better method for handling unknown words, or imple-
menting one of the more advanced methods in section 3.5.

4 log-linear language models

this chapter will discuss another set of language models:
log-linear language models
[87, 20], which take a very di   erent approach than the count-based id165s described above.9

4.1 model formulation

like id165 language models, log-linear language models still calculate the id203 of a
particular word et given a particular context et   1
t   n+1. however, their method for doing so is
quite di   erent from count-based language models, based on the following procedure.

calculating features: log-linear language models revolve around the concept of fea-
tures. in short, features are basically,    something about the context that will be useful in
predicting the next word   . more formally, we de   ne a feature function   (et   1
t   n+1) that takes a
context as input, and outputs a real-valued feature vector x     rn that describe the context
using n di   erent features.10

for example, from our bi-gram models from the previous chapter, we know that    the
identity of the previous word    is something that is useful in predicting the next word. if we
want to express the identity of the previous word as a real-valued vector, we can assume that
each word in our vocabulary v is associated with a word id j, where 1     j     |v |. then, we
t   n+1) to return a feature vector x = r|v |, where if et   1 = j,
de   ne our feature function   (et
then the jth element is equal to one and the remaining elements in the vector are equal to
zero. this type of vector is often called a one-hot vector, an example of which is shown in
figure 4(a). for later user, we will also de   ne a function onehot(i) which returns a vector

8http://iwslt.org
9it should be noted that the cited papers call these maximum id178 language models. this is
because models in this chapter can be motivated in two ways: id148 that calculate un-normalized
log-id203 scores for each function and normalize them to probabilities, and maximum-id178 models
that spread their id203 mass as evenly as possible given the constraint that they must model the training
data. while the maximum-id178 interpretation is quite interesting theoretically and interested readers can
reference [11] to learn more, the explanation as id148 is simpler conceptually, and thus we will use
this description in this chapter.

10alternative formulations that de   ne feature functions that also take the current word as input   (et

t   n+1)
are also possible, but in this book, to simplify the transition into neural language models described in section 5,
we consider features over only the context.

11

figure 4: an example of feature values for a particular context.

where only the ith element is one and the rest are zero (assume the length of the vector is
the appropriate length given the context).

of course, we are not limited to only considering one previous word. we could also
calculate one-hot vectors for both et   1 and et   2, then concatenate them together, which
would allow us to create a model that considers the values of the two previous words. in fact,
there are many other types of feature functions that we can think of (more in section 4.4),
and the ability to    exibly de   ne these features is one of the advantages of log-linear language
models over standard id165 models.

calculating scores: once we have our feature vector, we now want to use these features
to predict probabilities over our output vocabulary v . in order to do so, we calculate a score
vector s     r|v | that corresponds to the likelihood of each word: words with higher scores in
the vector will also have higher probabilities. we do so using the model parameters   , which
speci   cally come in two varieties: a bias vector b     r|v |, which tells us how likely each
word in the vocabulary is overall, and a weight matrix w = r|v |  n which describes the
relationship between feature values and scores. thus, the    nal equation for calculating our
scores for a particular context is:

s = w x + b.

(17)

one thing to note here is that in the special case of one-hot vectors or other sparse vectors
where most of the elements are zero. because of this we can also think about equation 17
in a di   erent way that is numerically equivalent, but can make computation more e   cient.
speci   cally, instead of multiplying the large feature vector by the large weight matrix, we can
add together the columns of the weight matrix for all active (non-zero) features as follows:

s =

w  ,jxj + b,

(18)

(cid:88)

{j:xj(cid:54)=0}

where w  ,j is the jth column of w . this allows us to think of calculating scores as    look up
the vector for the features active for this instance, and add them together   , instead of writing
them as matrix math. an example calculation in this paradigm where we have two feature

12

j=1: aj=2: thej=3: hatj=4: giving...1000     (ei-1)=previous words:    giving a   word idsfeatures forthe previouswordfeatures forboth words0001     (ei-2)=  (ei-1,ei-2)=1000   0001   features fortwo wordsagofigure 5: an example of the weights for a log linear model in a certain context.

functions (one for the directly preceding word, and one for the word before that) is shown in
figure 5.

calculating probabilities: it should be noted here that scores s are arbitrary real
numbers, not probabilities: they can be negative or greater than one, and there is no restriction
that they add to one. because of this, we run these scores through a function that performs
the following transformation:

.

(19)

(cid:80)

pj =

exp(sj)
  j exp(s  j)

by taking the exponent and dividing by the sum of the values over the entire vocabulary,
these scores can be turned into probabilities that are between 0 and 1 and sum to 1.

this function is called the softmax function, and often expressed in vector form as follows:

through applying this to the scores calculated in the previous section, we now have a way to
go from features to language model probabilities.

p = softmax(s).

(20)

4.2 learning model parameters

now, the only remaining missing link is how to acquire the parameters   , consisting of the
weight matrix w and bias b. basically, the way we do so is by attempting to    nd parameters
that    t the training corpus well.
to do so, we use standard machine learning methods for optimizing parameters. first, we
de   ne a id168 (cid:96)(  )     a function expressing how poorly we   re doing on the training
data. in most cases, we assume that this loss is equal to the negative log likelihood:

(cid:96)(etrain,   ) =     log p (etrain |   ) =     (cid:88)

log p (e |   ).

e   etrain

we assume we can also de   ne the loss on a per-word level:
t   n+1,   ) = log p (et | et   1

(cid:96)(et

t   n+1).

13

(21)

(22)

w2,giving = w1,a = athetalkgifthat   3.02.5-0.20.11.2   b =-0.2-0.31.02.0-1.2   -6.0-5.10.20.10.6   s = -3.2-2.91.02.20.6   previous words:    giving a   wordswe'repredictinghow likelyare they?how likelygiven thepreviousword is    a   ?how likelygiven twowords beforeis    giving   ?totalscorenext, we optimize the parameters to reduce this loss. while there are many methods for
doing so, in recent years one of the go-to methods is stochastic id119 (sgd).
sgd is an iterative process where we randomly pick a single word et (or mini-batch, discussed
in section 5) and take a step to improve the likelihood with respect to et. in order to do
so, we    rst calculate the derivative of the loss with respect to each of the features in the full
feature set   :

d(cid:96)(et

t   n+1,   )
d  

.

(23)

we can then use this information to take a step in the direction that will reduce the loss
according to the objective function

                

d(cid:96)(et

t   n+1,   )
d  

,

(24)

where    is our learning rate, specifying the amount with which we update the parameters
every time we perform an update. by doing so, we can    nd parameters for our model that
reduce the loss, or increase the likelihood, on the training data.

this vanilla variety of sgd is quite simple and still a very competitive method for opti-
mization in large-scale systems. however, there are also a few things to consider to ensure
that training remains stable:

adjusting the learning rate: sgd requires also requires us to carefully choose   : if    is
too big, training can become unstable and diverge, and if    is too small, training may
become incredibly slow or fall into bad local optima. one way to handle this problem
is learning rate decay: starting with a higher learning rate, then gradually reducing
the learning rate near the end of training. other more sophisticated methods are listed
below.

early stopping: it is common to use a held-out development set, measure our log-likelihood
on this set, and save the model that has achieved the best log-likelihood on this held-
out set. this is useful in case the model starts to over-   t to the training set, losing its
generalization capability, we can re-wind to this saved model. as another method to
prevent over-   tting and smooth convergence of training, it is common to measure log
likelihood on a held-out development set, and when the log likelihood stops improving
or starts getting worse, reduce the learning rate.

shu   ing training order: one of the features of sgd is that it processes training data
one at a time. this is nice because it is simple and can be e   cient, but it also causes
problems if there is some bias in the order in which we see the data. for example,
if our data is a corpus of news text where news articles come    rst, then sports, then
entertainment, there is a chance that near the end of training our model will see hundreds
or thousands of entertainment examples in a row, resulting in the parameters moving to
a space that favors these more recently seen training examples. to prevent this problem,
it is common (and highly recommended) to randomly shu   e the order with which the
training data is presented to the learning algorithm on every pass through the data.

there are also a number of other update rules that have been proposed to improve gradient
descent and make it more stable or e   cient. some representative methods are listed below:

14

sgd with momentum [90]: instead of taking a single step in the direction of the current
gradient, sgd with momentum keeps an exponentially decaying average of past gradi-
ents. this reduces the propensity of simple sgd to    jitter    around, making optimization
move more smoothly across the parameter space.

adagrad [30]: adagrad focuses on the fact that some parameters are updated much more
frequently than others. for example, in the model above, columns of the weight matrix
w corresponding to infrequent context words will only be updated a few times for every
pass through the corpus, while the bias b will be updated on every training example.
based on this, adagrad dynamically adjusts the training rate    for each parameter
individually, with frequently updated (and presumably more stable) parameters such
as b getting smaller updates, and infrequently updated parameters such as w getting
larger updates.

adam [60]: adam is another method that computes learning rates for each parameter. it
does so by keeping track of exponentially decaying averages of the mean and variance
of past gradients, incorporating ideas similar to both momentum and adagrad. adam
is now one of the more popular methods for optimization, as it greatly speeds up con-
vergence on a wide variety of datasets, facilitating fast experimental cycles. however, it
is also known to be prone to over-   tting, and thus, if high performance is paramount,
it should be used with some caution and compared to more standard sgd methods.

[89] provides a good overview of these various methods with equations and notes a few other
concerns when performing stochastic optimization.

4.3 derivatives for id148

now, the    nal piece in the puzzle is the calculation of derivatives of the id168 with
respect to the parameters. to do so,    rst we step through the full id168 in one pass
as below:

(cid:88)

x =   (et   1
s =

t   m+1)

{j:xj !=0}

w  ,jxj + b

p = softmax(s)
(cid:96) =     log pet.

and thus, using the chain rule to calculate

d(cid:96)(et

t   n+1, w, b)

db

d(cid:96)(et

t   n+1, w, b)
dw  ,j

=

=

d(cid:96)
dp
d(cid:96)
dp

dp
ds
dp
ds

ds
db

ds
dw  ,j

15

(25)

(26)

(27)

(28)

(29)

(30)

we    nd that the derivative of the id168 for the bias and each column of the weight
matrix is:

d(cid:96)(et

t   n+1, w, b)

db

d(cid:96)(et

t   n+1, w, b)
dw  ,j

= p     onehot(et)
= xj(p     onehot(et))

(31)

(32)

con   rming these equations is left as a (highly recommended) exercise to the reader. hint:
when performing this derivation, it is easier to work with the log id203 log p than working
with p directly.

4.4 other features for id38

one reason why id148 are nice is because they allow us to    exibly design features
that we think might be useful for predicting the next word. for example, these could include:
context word features: as shown in the example above, we can use the identity of et   1

or the identity of et   2.

context class: context words can be grouped into classes of similar words (using a method
such as brown id91 [15]), and instead of looking up a one-hot vector with a separate
entry for every word, we could look up a one-hot vector with an entry for each class
[18]. thus, words from the same class could share statistical strength, allowing models
to generalize better.

context su   x features: maybe we want a feature that    res every time the previous word
ends with    ...ing    or other common su   xes. this would allow us to learn more gener-
alized patterns about words that tend to follow progressive verbs, etc.

bag-of-words features: instead of just using the past n words, we could use all previous
words in the sentence. this would amount to calculating the one-hot vectors for every
word in the previous sentence, and then instead of concatenating them simply summing
them together. this would lose all information about what word is in what position,
but could capture information about what words tend to co-occur within a sentence or
document.

it is also possible to combine together multiple features (for example et   1 is a particular
word and et   2 is another particular word). this is one way to create a more expressive feature
set, but also has a downside of greatly increasing the size of the feature space. we discuss
these features in more detail in section 5.1.

4.5 further reading

the language model in this section was basically a featurized version of an id165 language
model. there are quite a few other varieties of linear featurized models including:

whole-sentence language models: these models, instead of predicting words one-by-one,
predict the id203 over the whole sentence then normalize [88]. this can be con-
ducive to introducing certain features, such as a id203 distribution over lengths of
sentences, or features such as    whether this sentence contains a verb   .

16

discriminative language models: in the case that we want to use a language model to
determine whether the output of a system is good or not, sometimes it is useful to train
directly on this system output, and try to re-rank the outputs to achieve higher accuracy
[86]. even if we don   t have real negative examples, it can be possible to    hallucinate   
negative examples that are still useful for training [80].

4.6 exercise

in the exercise for this chapter, we will construct a log-linear language model and evaluate
its performance. i highly suggest that you try to use the numpy library to hold and perform
calculations over feature vectors, as this will make things much easier. if you have never used
numpy before, you can take a look at this tutorial to get started: https://docs.scipy.org/
doc/numpy-dev/user/quickstart.html.

writing the program will entail:
    writing a function to read in the training and test corpora, and converting the words

into numerical ids.

    writing the feature function   (et   1

t   n+1), which takes in a string and returns which fea-
tures are active (for example, as a baseline these can be features with the identity of
the previous two words).

    writing code to calculate the id168.
    writing code to calculate gradients and perform stochastic id119 updates.
    writing (or re-using from the previous exercise) code to evaluate the language models.

similarly to the id165 language models, we will measure the per-word log likelihood and
perplexity on our text corpus, and compare it to id165 language models. handling unknown
words will similarly require that you use the uniform distribution with 10,000,000 words in
the english vocabulary.

potential improvements to the model include designing better feature functions, adjusting
the learning rate and measuring the results, and researching and implementing other types of
optimizers such as adagrad or adam.

5 neural networks and feed-forward language models

in this chapter, we describe language models based on neural networks, a way to learn more
sophisticated functions to improve the accuracy of our id203 estimates with less feature
engineering.

5.1 potential and problems with combination features

before moving into the technical detail of neural networks,    rst let   s take a look at a motivating
example in figure 6. from the example, we can see et   2 =    farmers    is compatible with
et =    hay    (in the context    farmers grow hay   ), and et   1 =    eat    is also compatible (in the
context    cows eat hay   ). if we are using a log-linear model with one set of features dependent

17

figure 6: an example of the e   ect that combining multiple words can have on the id203
of the next word.

on et   1, and another set of features dependent on et   2, neither set of features can rule out
the unnatural phrase    farmers eat hay.   

one way we can    x this problem is by creating another set of features where we learn
one vector for each pair of words et   2, et   1. if this is the case, our vector for the context
et   2 =    farmers   , et   1 =    eat    could assign a low score to    hay   , resolving this problem.
however, adding these combination features has one major disadvantage: it greatly expands
the parameters: instead of o(|v |2) parameters for each pair ei   1, ei, we need o(|v |3) param-
eters for each triplet ei   2, ei   1, ei. these numbers greatly increase the amount of memory
used by the model, and if there are not enough training examples, the parameters may not
be learned properly.

because of both the importance of and di   culty in learning using these combination fea-
tures, a number of methods have been proposed to handle these features, such as kernelized
support vector machines [28] and neural networks [91, 39]. speci   cally in this section,
we will cover neural networks, which are both    exible and relatively easy to train on large
data, desiderata for sequence-to-sequence models.

5.2 a brief overview of neural networks

to understand neural networks in more detail, let   s take a very simple example of a function
that we cannot learn with a simple linear classi   er like the ones we used in the last chapter:
a function that takes an input x     {   1, 1}2 and outputs y = 1 if both x1 and x2 are equal
and y =    1 otherwise. this function is shown in figure 7.

figure 7: a function that cannot be solved by a linear transformation.

a    rst attempt at solving this function might de   ne a linear model (like the log-linear

models from the previous chapter) that solves this problem using the following form:

y = w x + b.

18

(33)

steak     highfarmers eathay       lowsteak     lowcows eathay       highsteak     lowfarmers growhay       highsteak     lowcows growhay       low-1+1+1-1x1x2figure 8: a simple neural network that represents the nonlinear function of figure 7.

however, this class of functions is not powerful enough to represent the function at hand.11
thus, we turn to a slightly more complicated class of functions taking the following form:

h = step(wxhx + bh)
y = whyh + by.

(34)

computation is split into two stages: calculation of the hidden layer, which takes in input
x and outputs a vector of hidden variables h, and calculation of the output layer, which
takes in h and calculates the    nal result y. both layers consist of an a   ne transform12
using weights w and biases b, followed by a step(  ) function, which calculates the following:

(cid:40)

step(x) =

1
   1

if x > 0,
otherwise.

(35)

this function is one example of a class of neural networks called multi-layer id88s
(mlps). in general, mlps consist one or more hidden layers that consist of an a   ne transform
followed by a non-linear function (such as the step function used here), culminating in an
output layer that calculates some variety of output.

figure 8 demonstrates why this type of network does a better job of representing the
non-linear function of figure 7. in short, we can see that the    rst hidden layer transforms
the input x into a hidden vector h in a di   erent space that is more conducive for modeling
our    nal function. speci   cally in this case, we can see that h is now in a space where we can
de   ne a linear function (using wy and by) that correctly calculates the desired output y.

as mentioned above, mlps are one speci   c variety of neural network. more generally,
neural networks can be thought of as a chain of functions (such as the a   ne transforms and
step functions used above, but also including many, many others) that takes some input and
calculates some desired output. the power of neural networks lies in the fact that chaining to-
gether a variety of simpler functions makes it possible to represent more complicated functions

11question: prove this by trying to solve the system of equations.
12a fancy name for a multiplication followed by an addition.

19

-1+1+1-1x1x2stepstepx1111-1-1-1-1h1wh,0bh,0wh,1bh,1x = {1,1}     h = {-1, 1}x2x11x2h2-1+1+1h1h2x = {1,-1}     h = {-1, -1}x = {-1,1}     h = {-1, -1}x = {-1,-1}     h = {1, -1}h11111ywybyh2original inputvariableshidden layertransformed variablesoutput layerfigure 9: types of non-linearities.

in an easily trainable, parameter-e   cient way. in fact, the simple single-layer mlp described
above is a universal function approximator [51], which means that it can approximate
any function to arbitrary accuracy if its hidden vector h is large enough.

we will see more about training in section 5.3 and give some more examples of how
these can be more parameter e   cient in the discussion of neural network language models in
section 5.5.

5.3 training neural networks

now that we have a model in equation 34, we would like to train its parameters wmh, bh, why,
and by. to do so, remembering our gradient-based training methods from the last chapter,
we need to de   ne the id168 (cid:96)(  ), calculate the derivative of the loss with respect to the
parameters, then take a step in the direction that will reduce the loss. for our id168,
let   s use the squared-error loss, a commonly used id168 for regression problems
which measures the di   erence between the calculated value y and correct value y    as follows

(36)
next, we need to calculate derivatives. here, we run into one problem: the step(  ) function

(cid:96)(y   , y) = (y        y)2.

is not very derivative friendly, with its derivative being:

dstep(x)

dx

=

unde   ned if x = 0,
0

otherwise.

because of this, it is more common to use other non-linear functions, such as the hyperbolic
tangent (tanh) function. the tanh function, as shown in figure 9, looks very much like
a softened version of the step function that has a continuous gradient everywhere, making
it more conducive to training with gradient-based methods. there are a number of other
alternatives as well, the most popular of which being the recti   ed linear unit (relu)

(cid:40)

(cid:40)

(37)

(38)

relu(x) =

x if x > 0,
0

otherwise.

shown in the left of figure 9. in short, relus solve the problem that the tanh function gets
   saturated    and has very small gradients when the absolute value of input x is very large (x is
a large negative or positive number). empirical results have often shown it to be an e   ective
alternative to tanh, including for the id38 task described in this chapter [110].

20

-4-3-2-101234-2-1012relu(x)y-4-3-2-101234-2-1012tanh(x)y-4-3-2-101234-2-1012step(x)yfigure 10: computation graphs for the function itself, and the id168.

so let   s say we swap in a tanh non-linearity instead of the step function to our network,
we can now proceed to calculate derivatives like we did in section 4.3. first, we perform the
full calculation of the id168:

h(cid:48) = wxhx + bh
h = tanh(h(cid:48))
y = whyh + by
(cid:96) = (y        y)2.

then, again using the chain rule, we calculate the derivatives of each set of parameters:

d(cid:96)
dby
d(cid:96)

dwhy
d(cid:96)
dbh
d(cid:96)

dwxh

=

=

=

=

d(cid:96)
dy
d(cid:96)
dy
d(cid:96)
dy
d(cid:96)
dy

dy
dby
dy

dwhy
dh
dy
dh(cid:48)
dh
dh
dy
dh(cid:48)
dh

dh(cid:48)
dbh
dh(cid:48)
dwxh

.

(39)

(40)

we could go through all of the derivations above by hand and precisely calculate the
gradients of all parameters in the model. interested readers are free to do so, but even for a
simple model like the one above, it is quite a lot of work and error prone. for more complicated
models, like the ones introduced in the following chapters, this is even more the case.

fortunately, when we actually implement neural networks on a computer, there is a very
useful tool that saves us a large portion of this pain: automatic di   erentiation (autodi   )
[116, 44]. to understand automatic di   erentiation, it is useful to think of our computation in
equation 39 as a data structure called a computation graph, two examples of which are
shown in figure 10. in these graphs, each node represents either an input to the network or
the result of one computational operation, such as a multiplication, addition, tanh, or squared
error. the    rst graph in the    gure calculates the function of interest itself and would be used
when we want to make predictions using our model, and the second graph calculates the loss
function and would be used in training.

21

whx  bh+tanhwy  by+y*sqr_errwhx  bh+tanhwy  by+ygraph for the training objectivegraph for the function itselfautomatic di   erentiation is a two-step id145 algorithm that operates

over the second graph and performs:

    forward calculation, which traverses the nodes in the graph in topological order,

calculating the actual result of the computation as in equation 39.

    back propagation, which traverses the nodes in reverse topological order, calculating

the gradients as in equation 40.

the nice thing about this formulation is that while the overall function calculated by the
graph can be relatively complicated, as long as it can be created by combining multiple
simple nodes for which we are able to calculate the function f (x) and derivative f(cid:48)(x), we are
able to use automatic di   erentiation to calculate its derivatives using this dynamic program
without doing the derivation by hand.

thus, to implement a general purpose training algorithm for neural networks, it is neces-
sary to implement these two dynamic programs, as well as the atomic forward function and
backward derivative computations for each type of node that we would need to use. while this
is not trivial in itself, there are now a plethora of toolkits that either perform general-purpose
auto-di   erentiation [7, 50], or auto-di   erentiation speci   cally tailored for machine learning
and neural networks [1, 12, 26, 105, 78]. these implement the data structures, nodes, back-
propogation, and parameter optimization algorithms needed to train neural networks in an
e   cient and reliable way, allowing practitioners to get started with designing their models.
in the following sections, we will take this approach, taking a look at how to create our mod-
els of interest in a toolkit called dynet,13 which has a programming interface that makes it
relatively easy to implement the sequence-to-sequence models covered here.14

5.4 an example implementation

figure 11 shows an example of implementing the above neural network in dynet, which
we   ll step through line-by-line. lines 1-2 import the necessary libraries. lines 4-5 specify
parameters of the models: the size of the hidden vector h and the number of epochs (passes
through the data) for which we   ll perform training. line 7 initializes a dynet model, which
will store all the parameters we are attempting to learn. lines 8-11 initialize parameters
wxh, bh, why, and by to be the appropriate size so that dimensions in the equations for
equation 39 match. line 12 initializes a    trainer   , which will update the parameters in the
model according to an update strategy (here we use simple stochastic id119, but
trainers for adagrad, adam, and other strategies also exist). line 14 creates the training
data for the function in figure 7.

lines 16-25 de   ne a function that takes input x and creates a computation graph to
calculate equation 39. first, line 17 creates a new computation graph to hold the computation
for this particular training example. lines 18-21 take the parameters (stored in the model) and
adds them to the computation graph as dynet variables for this particular training example.
line 22 takes a python list representing the current input and puts it into the computation
graph as a dynet variable. line 23 calculates the hidden vector h, line 24 calculates the
value y, and line 25 returns it.

13http://github.com/clab/dynet
14it is also developed by the author of these materials, so the decision might have been a wee bit biased.

22

1 import dynet as dy
2 import random
3 # parameters of the model and training
4 hidden_size = 20
5 num_epochs = 20
6 # define the model and sgd optimizer
7 model = dy.model()
8 w_xh_p = model.add_parameters((hidden_size, 2))
9 b_h_p = model.add_parameters(hidden_size)
10 w_hy_p = model.add_parameters((1, hidden_size))
11 b_y_p = model.add_parameters(1)
12 trainer = dy.simplesgdtrainer(model)
13 # define the training data, consisting of (x,y) tuples
14 data = [([1,1],1), ([-1,1],-1), ([1,-1],-1), ([-1,-1],1)]
15 # define the function we would like to calculate
16 def calc_function(x):

17

18

19

20

21

22

23

24

dy.renew_cg()
w_xh = dy.parameter(w_xh_p)
b_h = dy.parameter(b_h_p)
w_hy = dy.parameter(w_hy_p)
b_y = dy.parameter(b_y_p)
x_val = dy.inputvector(x)
h_val = dy.tanh(w_xh * x_val + b_h)
y_val = w_hy * h_val + b_y
return y_val

25
26 # perform training
27 for epoch in range(num_epochs):

28

29

30

31

32

33

34

35

epoch_loss = 0
random.shuffle(data)
for x, ystar in data:

y = calc_function(x)
loss = dy.squared_distance(y, dy.scalarinput(ystar))
epoch_loss += loss.value()
loss.backward()
trainer.update()

print("epoch %d: loss=%f" % (epoch, epoch_loss))

36
37 # print results of prediction
38 for x, ystar in data:

39

40

y = calc_function(x)
print("%r -> %f" % (x, y.value()))

figure 11: an example of training a neural network for a multi-layer id88 using the
toolkit dynet.

23

figure 12: a computation graph for a tri-gram feed-forward neural language model.

lines 27-36 perform training for num epochs passes over the data (one pass through the
training data is usually called an    epoch   ). line 28 creates a variable to keep track of the loss
for this epoch for later reporting. line 29 shu   es the data, as recommended in section 4.2.
lines 30-35 perform stochastic id119, looping over each of the training examples.
line 31 creates a computation for the function itself, and line 32 adds computation for the
id168. line 33 runs the forward calculation to calculate the loss and adds it to the loss
for this epoch. line 34 runs back propagation, and line 35 updates the model parameters.
at the end of the epoch, we print the loss for the epoch in line 36 to make sure that the loss
is going down and our model is converging.

finally, at the end of training in lines 38-40, we print the output results. in an actual

scenario, this would be done on a separate set of test data.

5.5 neural-network language models

now that we have the basics down, it is time to apply neural networks to id38
[76, 9]. a feed-forward neural network language model is very much like the log-linear language
model that we mentioned in the previous section, simply with the addition of one or more
non-linear layers before the output.

first, let   s recall the tri-gram log-linear language model. in this case, assume we have two
sets of features expressing the identity of et   1 (represented as w (1)) and et   2 (as w (2)), the
equation for the log-linear model looks like this:

  ,et   1 + w (2)

s = w (1)
p = softmax(s),

  ,et   2 + b

(41)

where we add the appropriate columns from the weight matricies to the bias to get the score,
then take the softmax to turn it into a id203.

compared to this, a tri-gram neural network model with a single layer is structured as

shown in figure 12 and described in equations below:

m = concat(m  ,et   2, m  ,et   1)
h = tanh(wmhm + bh)
s = whsh + bs
p = softmax(s)

(42)

in the    rst line, we obtain a vector m representing the context ei   1

i   n+1 (in the particular
case above, we are handling a tri-gram model so n = 3). here, m is a matrix with |v | columns,
and lm rows, where each column corresponds to an lm-length vector representing a single

24

mlookup(et-1)lookup(et-2)concatwhbhx+wpbpx+tanhsoftmaxp1 # define the lookup parameters at model definition time
2 # vocab_size is the number of words in the vocabulary
3 # embeddings_size is the length of the id27 vector
4 m_p = model.add_lookup_parameters((vocab_size, embedding_size))
5 # load the parameters into the computation graph
6 m = dy.lookup(m_p)
7 # and look up the vector for word i
8 m = m[i]

figure 13: code for looking things up in dynet.

word in the vocabulary. this vector is called a id27 or a word representation,
which is a vector of real numbers corresponding to particular words in the vocabulary.15 the
interesting thing about expressing words as vectors of real numbers is that each element of
the vector could re   ect a di   erent aspect of the word. for example, there may be an element
in the vector determining whether a particular word under consideration could be a noun, or
another element in the vector expressing whether the word is an animal, or another element
that expresses whether the word is countable or not.16 figure 13 shows an example of how
to de   ne parameters that allow you to look up a vector in dynet.
the vector m then results from the concatenation of the word vectors for all of the words
in the context, so |m| = lm     (n     1). once we have this m, we run the vectors through
a hidden layer to obtain vector h. by doing so, the model can learn combination features
that re   ect information regarding multiple words in the context. this allows the model to
be expressive enough to represent the more di   cult cases in figure 6. for example, given a
context is    cows eat   , and some elements of the vector m  ,cows identify the word as a    large
farm animal    (e.g.    cow   ,    horse   ,    goat   ), while some elements of m  ,eat corresponds to
   eat    and all of its relatives (   consume   ,    chew   ,    ingest   ), then we could potentially learn
a unit in the hidden layer h that is active when we are in a context that represents    things
farm animals eat   .
next, we calculate the score vector for each word: s     r|v |. this is done by performing an
a   ne transform of the hidden vector h with a weight matrix whs     r|v |  |h| and adding a bias
vector bs     r|v |. finally, we get a id203 estimate p by running the calculated scores
through a softmax function, like we did in the log-linear language models. for training, if we
know et we can also calculate the id168 as follows, similarly to the log-linear model:

(cid:96) =     log(pet).

(43)

dynet has a convenience function that, given a score vector s, will calculate the negative log
likelihood loss:

15for the purposes of the model in this chapter, these vectors can basically be viewed as one set of tunable
parameters in the neural language model, but there has also been a large amount of interest in learning these
vectors for use in other tasks. some methods are outlined in section 5.6.

16in reality, it is rare that single elements in the vector have such an intuitive meaning unless we impose

some sort of constraint, such as sparsity constraints [75].

25

1 loss = dy.pickneglogsoftmax(s, e_t)

the reasons why the neural network formulation is nice becomes apparent when we com-

pare this to id165 language models in section 3:

better generalization of contexts: id165 language models treat each word as its own
discrete entity. by using input embeddings m , it is possible to group together similar
words so they behave similarly in the prediction of the next word. in order to do the
same thing, id165 models would have to explicitly learn word classes and using these
classes e   ectively is not a trivial problem [15].

more generalizable combination of words into contexts: in an id165 language model,
we would have to remember parameters for all combinations of {cow, horse, goat}   
{consume, chew, ingest} to represent the context    things farm animals eat   . this would
be quadratic in the number of words in the class, and thus learning these parameters
is di   cult in the face of limited training data. neural networks handle this problem by
learning nodes in the hidden layer that can represent this quadratic combination in a
feature-e   cient way.

ability to skip previous words: id165 models generally fall back sequentially from longer
contexts (e.g.    the two previous words et   1
t   2   ) to shorter contexts (e.g.    the previous
words et   1   ), but this doesn   t allow them to    skip    a word and only reference for exam-
ple,    the word two words ago et   2   . id148 and neural networks can handle
this skipping naturally.

5.6 further reading

in addition to the methods described above, there are a number of extensions to neural-
network language models that are worth discussing.

softmax approximations: one problem with the training of log-linear or neural network
language models is that at every training example, they have to calculate the large score
vector s, then run a softmax over it to get probabilities. as the vocabulary size |v |
grows larger, this can become quite time-consuming. as a result, there are a number
of ways to reduce training time. one example are methods that sample a subset of the
vocabulary v (cid:48)     v where |v (cid:48)| (cid:28) v , then calculate the scores and approximate the loss
over this smaller subset. examples of these include methods that simply try to get the
true word et to have a higher score (by some margin) than others in the subsampled
set [27] and more probabilistically motivated methods, such as importance sampling
[10] or noise-contrastive estimation (nce; [74]). interestingly, for other objective
functions such as id75 and special variety of softmax called the spherical
softmax, it is possible to calculate the objective function in ways that do not scale
linearly with the vocabulary size [111].

other softmax structures: another interesting trick to improve training speed is to create
a softmax that is structured so that its id168s can be computed e   ciently. one

26

| ct, et   1

way to do so is the class-based softmax [40], which assigns each word et to a class ct,
then divides computation into two steps: predicting the id203 of class ct given the
context, then predicting the id203 of the word et given the class and the current
t   n+1). the advantage of this method is that we
context p (et
only need to calculate scores for the correct class ct out of |c| classes, then the correct
word et out of the vocabulary for class ct, which is size |vct|. thus, our computational
complexity becomes o(|c| + |vct|) instead of o(|v |).17 the hierarchical softmax [73]
takes this a step further by predicting words along a binary-branching tree, which results
in a computational complexity of o(log2|v |).

t   n+1)p (ct

| et   1

other models to learn word representations: as mentioned in section 5.5, we learn
id27s m as a by-product of training our language models. one very nice
feature of word representations is that language models can be trained purely on raw
text, but the resulting representations can capture semantic or syntactic features of the
words, and thus can be used to e   ectively improve down-stream tasks that don   t have a
lot of annotated data, such as part-of-speech tagging or parsing [107].18 because of their
usefulness, there have been an extremely large number of approaches proposed to learn
di   erent varieties of id27s,19 from early work based on distributional simi-
larity and id84 [93, 108] to more recent models based on predictive
models similar to language models [107, 71], with the general current thinking being
that predictive models are the more e   ective and    exible of the two [5].the most well-
known methods are the continuous-bag-of-words and skip-gram models implemented
in the software id97,20 which de   ne simple objectives for predicting words using
the immediately surrounding context or vice-versa. id97 uses a sampling-based
approach and parallelization to easily scale up to large datasets, which is perhaps the
primary reason for its popularity. one thing to note is that these methods are not
language models in themselves, as they do not calculate a id203 of the sentence
p (e), but many of the parameter estimation techniques can be shared.

5.7 exercise

in the exercise for this chapter, we will use dynet to construct a feed-forward language model
and evaluate its performance.

writing the program will entail:
    writing a function to read in the data and (turn it into numerical ids).
    writing a function to calculate the id168 by looking up id27s, then

running them through a multi-layer id88, then predicting the result.

    writing code to perform training using this function.
17question: what is the ideal class size to achieve the best computational e   ciency?
18manning

   sriracha

sauce

embeddings
can
it
naacl2015-vsm-compositional-deep-learning.pdf

called word
anything

(2015)
them to

to make

add

the
better

you
http://nlp.stanford.edu/~manning/talks/

of nlp   ,

because

19so many that daum  e iii (2016) called id27s the    sriracha sauce of nlp:

it sounds
like a good idea, you add too much, and now you   re crying    https://twitter.com/haldaume3/status/
706173575477080065

20https://code.google.com/archive/p/id97/

27

    writing evaluation code that measures the perplexity on a held-out data set.

id38 accuracy should be measured in the same way as previous exercises and
compared with the previous models.

potential improvements to the model include tuning the various parameters of the model.
how big should h be? should we add additional hidden layers? what optimizer with what
learning rate should we use? what happens if we implement one of the more e   cient versions
of the softmax explained in section 5.6?

6 recurrent neural network language models

the neural-network models presented in the previous chapter were essentially more powerful
and generalizable versions of id165 models. in this section, we talk about language models
based on recurrent neural networks (id56s), which have the additional ability to capture
long-distance dependencies in language.

6.1 long distance dependencies in language

figure 14: an example of long-distance dependencies in language.

before speaking about id56s in general, it   s a good idea to think about the various reasons
a model with a limited history would not be su   cient to properly model all phenomena in
language.

one example of a long-range grammatical constraint is shown in figure 14.

in this
example, there is a strong constraint that the starting    he    or    her    and the    nal    himself   
or    herself    must match in gender. similarly, based on the subject of the sentence, the
conjugation of the verb will change. these sorts of dependencies exist regardless of the
number of intervening words, and models with a    nite history ei   1
i   n+1, like the one mentioned
in the previous chapter, will never be able to appropriately capture this. these dependencies
are frequent in english but even more prevalent in languages such as russian, which has a
large number of forms for each word, which must match in case and gender with other words
in the sentence.21

another example where long-term dependencies exist is in selectional preferences [85].
in a nutshell, selectional preferences are basically common sense knowledge of    what will do
what to what   . for example,    i ate salad with a fork    is perfectly sensible with    a fork   
being a tool, and    i ate salad with my friend    also makes sense, with    my friend    being a
companion. on the other hand,    i ate salad with a backpack    doesn   t make much sense
because a backpack is neither a tool for eating nor a companion. these selectional preference
violations lead to nonsensical sentences and can also span across an arbitrary length due to
the fact that subjects, verbs, and objects can be separated by a great distance.

21see https://en.wikipedia.org/wiki/russian_grammar for an overview.

28

he doesn't have very much confidence in himselfshe doesn't have very much confidence in herselffigure 15: examples of computation graphs for neural networks. (a) shows a single time step.
(b) is the unrolled network. (c) is a simpli   ed version of the unrolled network, where gray
boxes indicate a function that is parameterized (in this case by wxh, whh, and bh).

finally, there are also dependencies regarding the topic or register of the sentence or
document. for example, it would be strange if a document that was discussing a technical
subject suddenly started going on about sports     a violation of topic consistency. it would
also be unnatural for a scienti   c paper to suddenly use informal or profane language     a lack
of consistency in register.

these and other examples describe why we need to model long-distance dependencies to

create workable applications.

6.2 recurrent neural networks

recurrent neural networks (id56s; [33]) are a variety of neural network that makes it
possible to model these long-distance dependencies. the idea is simply to add a connection
that references the previous hidden state ht   1 when calculating hidden state h, written in
equations as:

(cid:40)

ht =

tanh(wxhxt + whhht   1 + bh)
0

t     1,
otherwise.

(44)

as we can see, for time steps t     1, the only di   erence from the hidden layer in a standard
neural network is the addition of the connection whhht   1 from the hidden state at time step
t     1 connecting to that at time step t. as this is a recursive equation that uses ht   1 from
the previous time step. this single time step of a recurrent neural network is shown visually
in the computation graph shown in figure 15(a).

when performing this visual display of id56s, it is also common to    unroll    the neural
network in time as shown in figure 15(b), which makes it possible to explicitly see the
information    ow between multiple time steps. from unrolling the network, we can see that
we are still dealing with a standard computation graph in the same form as our feed-forward
networks, on which we can still do forward computation and backward propagation, making

29

xt  (a) a single id56 time stepbhwxh  whhht-1+httanhx1    h0+tanh(b) an unrolled id56wxhbhwhh    +tanhx2x3    +tanhx1id56h0(c) a simplified viewx2id56x3id56it possible to learn our parameters. it also makes clear that the recurrent network has to
start somewhere with an initial hidden state h0. this initial state is often set to be a vector
full of zeros, treated as a parameter hinit and learned, or initialized according to some other
information (more on this in section 7).

finally, for simplicity, it is common to abbreviate the whole recurrent neural network step
with a single block    id56    as shown in figure 15. in this example, the boxes corresponding
to id56 function applications are gray, to show that they are internally parameterized with
wxh, whh, and bh. we will use this convention in the future to represent parameterized
functions.

id56s make it possible to model long distance dependencies because they have the ability
to pass information between timesteps. for example, if some of the nodes in ht   1 encode the
information that    the subject of the sentence is male   , it is possible to pass on this information
to ht, which can in turn pass it on to ht+1 and on to the end of the sentence. this ability
to pass information across an arbitrary number of consecutive time steps is the strength of
recurrent neural networks, and allows them to handle the long-distance dependencies described
in section 6.1.

once we have the basics of id56s, applying them to id38 is (largely) straight-
forward [72]. we simply take the feed-forward language model of equation 42 and enhance
it with a recurrent connection as follows:

mt = m  ,et   1

(cid:40)

ht =

tanh(wmhmt + whhht   1 + bh)
0

t     1,
otherwise.

pt = softmax(whsht + bs).

(45)

one thing that should be noted is that compared to the feed-forward language model, we are
only feeding in the previous word instead of the two previous words. the reason for this is
because (if things go well) we can expect that information about et   2 and all previous words
are already included in ht   1, making it unnecessary to feed in this information directly.
function id56(  ), following the simpli   ed view of drawing id56s in figure 15(c):

also, for simplicity of notation, it is common to abbreviate the equation for ht with a

mt = m  ,et   1
ht = id56(mt, ht   1)
pt = softmax(whsht + bs).

(46)

6.3 the vanishing gradient and long short-term memory

however, while the id56s in the previous section are conceptually simple, they also have
problems: the vanishing gradient problem and the closely related cousin, the exploding
gradient problem.

a conceptual example of the vanishing gradient problem is shown in figure 16. in this
example, we have a recurrent neural network that makes a prediction after several times steps,
a model that could be used to classify documents or perform any kind of prediction over a
sequence of text. after it makes its prediction, it gets a loss that it is expected to back-
propagate over all time steps in the neural network. however, at each time step, when we run

30

figure 16: an example of the vanishing gradient problem.

the back propagation algorithm, the gradient gets smaller and smaller, and by the time we
get back to the beginning of the sentence, we have a gradient so small that it e   ectively has
no ability to have a signi   cant e   ect on the parameters that need to be updated. the reason
why this e   ect happens is because unless dht   1
is exactly one, it will tend to either diminish
dht
or amplify the gradient d(cid:96)
, and when this diminishment or ampli   cation is done repeatedly,
dht
it will have an exponential e   ect on the gradient of the loss.22

one method to solve this problem, in the case of diminishing gradients, is the use of
a neural network architecture that is speci   cally designed to ensure that the derivative of
the recurrent function is exactly one. a neural network architecture designed for this very
purpose, which has enjoyed quite a bit of success and popularity in a wide variety of sequential
processing tasks, is the long short-term memory (lstm; [49]) neural network architecture.
the most fundamental idea behind the lstm is that in addition to the standard hidden state
h used by most neural networks, it also has a memory cell c, for which the gradient
is
exactly one. because this gradient is exactly one, information stored in the memory cell does
not su   er from vanishing gradients, and thus lstms can capture long-distance dependencies
more e   ectively than standard recurrent neural networks.

dct
dct   1

so how do lstms do this? to understand this, let   s take a look at the lstm architecture

in detail, as shown in figure 17 and the following equations:

ut = tanh(wxuxt + whuht   1 + bu)
it =   (wxixt + whiht   1 + bi)
ot =   (wxoxt + whoht   1 + bo)
ct = it (cid:12) ut + ct   1
ht = ot (cid:12) tanh(ct).

(47)

(48)

(49)

(50)

(51)

taking the equations one at a time: equation 47 is the update, which is basically the same
as the id56 update in equation 44; it takes in the input and hidden state, performs an a   ne
transform and runs it through the tanh non-linearity.

equation 48 and equation 49 are the input gate and output gate of the lstm
respectively. the function of    gates   , as indicated by their name, is to either allow information
to pass through or block it from passing. both of these gates perform an a   ne transform

22this is particularly detrimental in the case where we receive a loss only once at the end of the sentence,
like the example above. one real-life example of such a scenario is document classi   cation, and because of this,
id56s have been less successful in this task than other methods such as convolutional neural networks, which
do not su   er from the vanishing gradient problem [59, 63]. it has been shown that pre-training an id56 as a
language model before attempting to perform classi   cation can help alleviate this problem to some extent [29].

31

x1id56h0x2id56x3id56y*square_errdldh3=largeh1h2h3dldh2=med.dldh1=smalldldh0=tinyfigure 17: a single time step of long short-term memory (lstm). the information    ow
between the h and cell c is modulated using parameterized input and output gates.

followed by the sigmoid function, also called the logistic function23

  (x) =

1

1 + exp(   x)

,

(52)

which squashes the input between 0 (which   (x) will approach as x becomes more negative)
and 1 (which   (x) will approach as x becomes more positive). the output of the sigmoid is
then used to perform a componentwise multiplication

z = x (cid:12) y
zi = xi     yi

with the output of another function. this results in the    gating    e   ect: if the result of the
sigmoid is close to one for a particular vector position, it will have little e   ect on the input
(the gate is    open   ), and if the result of the sigmoid is close to zero, it will block the input,
setting the resulting value to zero (the gate is    closed   ).

equation 50 is the most important equation in the lstm, as it is the equation that
implements the intuition that
must be equal to one, which allows us to conquer the
vanishing gradient problem. this equation sets ct to be equal to the update ut modulated by
the input gate it plus the cell value for the previous time step ct   1. because we are directly
adding ct   1 to ct, if we consider only this part of equation 50, we can easily con   rm that the
gradient will indeed be one.24

dct
dct   1

finally, equation 51 calculates the next hidden state of the lstm. this is calculated by
using a tanh function to scale the cell value between -1 and 1, then modulating the output

23 to be more accurate, the sigmoid function is actually any mathematical function having an s-shaped
curve, so the tanh function is also a type of sigmoid function. the logistic function is also a slightly broader
1+exp(   k(x   x0)) . however, in the machine learning literature, the    sigmoid    is usually
class of functions f (x) =
used to refer to the particular variety in equation 52.

l

24in actuality it (cid:12) ut is also a   ected by ct   1, and thus

dct

dct   1

is not exactly one, but the e   ect is relatively

indirect. especially for vector elements with it close to zero, the e   ect will be minimal.

32

ct-1ctu: tanh(  +)i:   (  +)ht-1htxto:   (  +)+      update u: what value do we try to add to the memory cell?input i: how much of the update do we allow to go through?output o: how much of the cell do we reflect in the next state?tanhusing the output gate value ot. this will be the value actually used in any downstream
calculation, such as the calculation of language model probabilities.

pt = softmax(whsht + bs).

(53)

6.4 other id56 variants

because of the importance of recurrent neural networks in a number of applications, many
variants of these networks exist. one modi   cation to the standard lstm that is used widely
(in fact so widely that most people who refer to    lstm    are now referring to this variant) is
the addition of a forget gate [38]. the equations for the lstm with a forget gate are shown
below:

ut = tanh(wxuxt + whuht   1 + bu)
it =   (wxixt + whiht   1 + bi)
ft =   (wxf xt + whf ht   1 + bf )
ot =   (wxoxt + whoht   1 + bo)
ct = it (cid:12) ut + ft (cid:12) ct   1
ht = ot (cid:12) tanh(ct).

(54)

(55)

compared to the standard lstm, there are two changes. first, in equation 54, we add
an additional gate, the forget gate. second, in equation 55, we use the gate to modulate
the passing of the previous cell ct   1 to the current cell ct. this forget gate is useful in that
it allows the cell to easily clear its memory when justi   ed:
for example, let   s say that the
model has remembered that it has seen a particular word strongly correlated with another
word, such as    he    and    himself    or    she    and    herself    in the example above. in this case,
we would probably like the model to remember    he    until it is used to predict    himself   ,
then forget that information, as it is no longer relevant. forget gates have the advantage of
allowing this sort of    ne-grained information    ow control, but they also come with the risk
that if ft is set to zero all the time, the model will forget everything and lose its ability to
handle long-distance dependencies. thus, at the beginning of neural network training, it is
common to initialize the bias bf of the forget gate to be a somewhat large value (e.g. 1), which
will make the neural net start training without using the forget gate, and only gradually start
forgetting content after the net has been trained to some extent.

while the lstm provides an e   ective solution to the vanishing gradient problem, it is
also rather complicated (as many readers have undoubtedly been feeling). one simpler id56
variant that has nonetheless proven e   ective is the gated recurrent unit (gru; [24]),
expressed in the following equations:

rt =   (wxrxt + whrht   1 + br)
zt =   (wxzxt + whzht   1 + bz)
  ht = tanh(wxhxt + whh(rt (cid:12) ht   1) + bh)
ht = (1     zt)ht   1 + zt  ht.

(56)

(57)

(58)

(59)

the most characteristic element of the gru is equation 59, which interpolates between a
candidate for the updated hidden state   ht and the previous state   ht   1. this interpolation is

33

modulated by an update gate zt (equation 57), where if the update gate is close to one,
the gru will use the new candidate hidden value, and if the update is close to zero, it will
use the previous value. the candidate hidden state is calculated by equation 58, which is
similar to a standard id56 update but includes an additional modulation of the hidden state
input by a reset gate rt calculated in equation 56. compared to the lstm, the gru has
slightly fewer parameters (it performs one less parameterized a   ne transform) and also does
not have a separate concept of a    cell   . thus, grus have been used by some to conserve
memory or computation time.

figure 18: an example of (a) stacked id56s and (b) stacked id56s with residual connections.

one other important modi   cation we can do to id56s, lstms, grus, or really any
other neural network layer is simple but powerful: stack multiple layers on top of each other
(stacked id56s figure 18(a)). for example, in a 3-layer stacked id56, the calculation at
time step t would look as follows:

h1,t = id561(xt, h1,t   1)
h2,t = id562(h1,t, h2,t   1)
h3,t = id563(h2,t, h3,t   1),

where hn,t is the hidden state for the nth layer at time step t, and id56(  ) is an abbreviation for
the id56 equation in equation 44. similarly, we could substitute this function for lstm(  ),
gru(  ), or any other recurrence step. the reason why stacking multiple layers on top of
each other is useful is for the same reason that non-linearities proved useful in the standard
neural networks introduced in section 5: they are able to progressively extract more abstract
features of the current words or sentences. for example, [98]    nd evidence that in a two-layer
stacked lstm, the    rst layer tends to learn granular features of words such as part of speech
tags, while the second layer learns more abstract features of the sentence such as voice or
tense.

while stacking id56s has potential bene   ts, it also has the disadvantage that it su   ers
from the vanishing gradient problem in the vertical direction, just as the standard id56 did in
the horizontal direction. that is to say, the gradient will be back-propagated from the layer

34

x1id56h1,0x2id56x3id56id56h2,0id56id56id56h3,0id56id56x1id56h1,0x2id56x3id56id56h2,0id56id56id56h3,0id56id56+++++++++(a) a stacked id56(b) with residual connectionsclose to the output (id563) to the layer close to the input (id561), and the gradient may
vanish in the process, causing the earlier layers of the network to be under-trained. a simple
solution to this problem, analogous to what the lstm does for vanishing gradients over time,
is residual networks (figure 18(b)) [47]. the idea behind these networks is simply to add
the output of the previous layer directly to the result of the next layer as follows:

h1,t = id561(xt, h1,t   1) + xt
h2,t = id562(h1,t, h2,t   1) + h1,t
h3,t = id563(h2,t, h3,t   1) + h2,t.

as a result, like the lstm, there is no vanishing of gradients due to passing through the
id56(  ) function, and even very deep networks can be learned e   ectively.

6.5 online, batch, and minibatch training

as the observant reader may have noticed, the previous sections have gradually introduced
more and more complicated models; we started with a simple linear model, added a hidden
layer, added recurrence, added lstm, and added more layers of lstms. while these more
expressive models have the ability to model with higher accuracy, they also come with a cost:
largely expanded parameter space (causing more potential for over   tting) and more compli-
cated operations (causing much greater potential computational cost). this section describes
an e   ective technique to improve the stability and computational e   ciency of training these
more complicated networks, minibatching.

up until this point, we have used the stochastic id119 learning algorithm in-
troduced in section 4.2 that performs updates according to the following iterative process.
this type of learning, which performs updates a single example at a time is called online
learning.

algorithm 1 a fully online training algorithm
1: procedure online
2:

for several epochs of training do

3:

4:

5:

6:

for each training example in the data do

calculate gradients of the loss
update the parameters according to this gradient

end for

end for

7:
8: end procedure

in contrast, we can also think of a batch learning algorithm, which treats the entire data
set as a single unit, calculates the gradients for this unit, then only performs update after
making a full pass through the data.

these two update strategies have trade-o   s.
    online training algorithms usually    nd a relatively good solution more quickly, as they

don   t need to make a full pass through the data before performing an update.

    however, at the end of training, batch learning algorithms can be more stable, as they

are not overly in   uenced by the most recently seen training examples.

35

algorithm 2 a batch learning algorithm
1: procedure batch
2:

for several epochs of training do

3:

4:

5:

6:

for each training example in the data do

calculate and accumulate gradients of the loss

end for
update the parameters according to the accumulated gradient

end for

7:
8: end procedure

figure 19: an example of combining multiple operations together when minibatching.

    batch training algorithms are also more prone to falling into local optima; the random-
ness in online training algorithms often allows them to bounce out of local optima and
   nd a better global solution.

minibatching is a happy medium between these two strategies. basically, minibatched
training is similar to online training, but instead of processing a single training example at a
time, we calculate the gradient for n training examples at a time. in the extreme case of n = 1,
this is equivalent to standard online training, and in the other extreme where n equals the
size of the corpus, this is equivalent to fully batched training. in the case of training language
models, it is common to choose minibatches of n = 1 to n = 128 sentences to process at a
single time. as we increase the number of training examples, each parameter update becomes
more informative and stable, but the amount of time to perform one update increases, so it
is common to choose an n that allows for a good balance between the two.

one other major advantage of minibatching is that by using a few tricks, it is actually
possible to make the simultaneous processing of n training examples signi   cantly faster than
processing n di   erent examples separately. speci   cally, by taking multiple training examples
and grouping similar operations together to be processed simultaneously, we can realize large
gains in computational e   ciency due to the fact that modern hardware (particularly gpus,
but also cpus) have very e   cient vector processing instructions that can be exploited with
appropriately structured inputs. as shown in figure 19, common examples of this in neural
networks include grouping together matrix-vector multiplies from multiple examples into a
single matrix-matrix multiply or performing an element-wise operation (such as tanh) over

36

x1operations w/o minibatchingwb+tanh(                 )wbx2+tanh(                 )wbx3+tanh(                 )operations with minibatchingx1b+tanh(                         )x2x3concatxbwbroadcastfigure 20: an example of minibatching in an id56 language model.

multiple vectors at the same time as opposed to processing single vectors individually. luckily,
in dynet, the library we are using, this is relatively easy to do, as much of the machinery
for each elementary operation is handled automatically. we   ll give an example of the changes
that we need to make when implementing an id56 language model below.

the basic idea in the batched id56 language model (figure 20) is that instead of processing
a single sentence, we process multiple sentences at the same time. so, instead of looking up
a single id27, we look up multiple id27s (in dynet, this is done by
replacing the lookup function with the lookup batch function, where we pass in an array of
word ids instead of a single word id). we then run these batched id27s through
the id56 and softmax as normal, resulting in two separate id203 distributions over
words in the    rst and second sentences. we then calculate the loss for each word (again
in dynet, replacing the pickneglogsoftmax function with the pickneglogsoftmax batch
function and pass word ids). we then sum together the losses and use this as the loss for our
entire sentence.

one sticking point, however, is that we may need to create batches with sentences of
di   erent sizes, also shown in the    gure.
in this case, it is common to perform sentence
padding and masking to make sure that sentences of di   erent lengths are treated properly.
padding works by simply adding the    end-of-sentence    symbol to the shorter sentences until
they are of the same length as the longest sentence in the batch. masking works by multiplying
all id168s calculated over these padded symbols by zero, ensuring that the losses for
sentence end symbols don   t get counted twice for the shorter sentences.

by taking these two measures, it becomes possible to process sentences of di   erent lengths,
if we perform lots of padding on sentences of vastly di   erent

but there is still a problem:

37

<s>     that    is       an example<s>     this    is    another </s>    id56id56id56id56id56softmaxsoftmaxsoftmaxsoftmaxsoftmaxlookuplookuplookuplookuplookuploss(  that  this)loss(  is  is)loss(  an  another)loss(  example  </s>)loss(  </s>  </s>)               +input:recurrence:estimate:loss:masking:sum time steps:final losslengths, we   ll end up wasting a lot of computation on these padded symbols. to    x this
problem, it is also common to sort the sentences in the corpus by length before creating
mini-batches to ensure that sentences in the same mini-batch are approximately the same
size.

6.6 further reading

because of the prevalence of id56s in a number of tasks both on natural language and other
data, there is signi   cant interest in extensions to them. the following lists just a few other
research topics that people are handling:

what can recurrent neural networks learn?: id56s are surprisingly powerful tools for
language, and thus many people have been interested in what exactly is going on inside
them.
[57] demonstrate ways to visualize the internal states of id137, and
   nd that some nodes are in charge of keeping track of length of sentences, whether a
parenthesis has been opened, and other salietn features of sentences. [65] show ways to
analyze and visualize which parts of the input are contributing to particular decisions
made by an id56-based model, by back-propagating information through the network.

other id56 architectures: there are also quite a few other recurrent network architec-
tures.
[42] perform an interesting study where they ablate various parts of the lstm
and attempt to    nd the best architecture for particular tasks. [123] take it a step further,
explicitly training the model to    nd the best neural network architecture.

6.7 exercise

in the exercise for this chapter, we will construct a recurrent neural network language model
using lstms.

writing the program will entail:
    writing a function such as lstm step or gru step that takes the input of the pre-
vious time step and updates it according to the appropriate equations. for refer-
ence, in dynet, the componentwise multiply and sigmoid functions are dy.cmult and
dy.logistic respectively.

    adding this function to the previous neural network language model and measuring the

e   ect on the held-out set.

    ideally, implement mini-batch training by using the functionality implemented in dynet,

lookup batch and pickneglogsoftmax batch.

id38 accuracy should be measured in the same way as previous exercises and
compared with the previous models.

potential improvements to the model include: measuring the speed/stability improve-
ments achieved by mini-batching. comparing the di   erences between recurrent architectures
such as id56, gru, or lstm.

38

7 neural encoder-decoder models

from section 3 to section 6, we focused on the id38 problem of calculating
the id203 p (e) of a sequence e. in this section, we return to the statistical machine
translation problem (mentioned in section 2) of modeling the id203 p (e | f ) of the
output e given the input f .

7.1 encoder-decoder models

the    rst model that we will cover is called an encoder-decoder model [22, 36, 53, 101].
the basic idea of the model is relatively simple: we have an id56 language model, but before
starting calculation of the probabilities of e, we    rst calculate the initial state of the language
model using another id56 over the source sentence f . the name    encoder-decoder    comes
from the idea that the    rst neural network running over f    encodes    its information as a
vector of real-valued numbers (the hidden state), then the second neural network used to
predict e    decodes    this information into the target sentence.

figure 21: a computation graph of the encoder-decoder model.

if the encoder is expressed as id56(f )(  ), the decoder is expressed as id56(e)(  ), and we
have a softmax that takes id56(e)   s hidden state at time step t and turns it into a id203,
then our model is expressed as follows (also shown in figure 21):

m(f )

t = m (f )
  ,ft
id56(f )(m(f )
0

t =

t

h(f )

m(e)

t = m (e)  ,et   1

(cid:40)
(cid:40)

, h(f )
t   1)

t     1,
otherwise.

h(e)

t =

id56(e)(m(e)
h(f )|f|

t

, h(e)

t   1)

t     1,
otherwise.

p(e)
t = softmax(whsh(e)

t + bs)

(60)

in the    rst two lines, we look up the embedding m(f )
h(f )

for the tth word in the source sequence f . we start with an empty vector h(f )

and calculate the encoder hidden state
0 = 0, and

t

t

39

f1id56(f)0lookup(f)f2id56(f)lookup(f)f|f|id56(f)lookup(f)   e0id56(e)lookup(e)e1id56(e)lookup(e)e|e|-1id56(e)lookup(e)   p(e)1p(e)2p(e)|e|softmax(e)softmax(e)softmax(e)h|f|encoderdecoderby h(f )|f|, the encoder has seen all the words in the source sentence. thus, this hidden state
should theoretically be able to encode all of the information in the source sentence.

t

t

. this is very similar to the encoder step, with the important di   erence that h(e)
0

in the decoder phase, we predict the id203 of word et at each time step. first, we
similarly look up m(e)
, but this time use the previous word et   1, as we must condition the
id203 of et on the previous word, not on itself. then, we run the decoder to calculate
h(e)
is set
to the    nal state of the encoder h(f )|f|, allowing us to condition on f . finally, we calculate the
id203 p(e)
while this model is quite simple (only 5 lines of equations), it gives us a straightforward
and powerful way to model p (e | f ). in fact, [101] have shown that a model that follows
this basic pattern is able to perform translation with similar accuracy to heavily engineered
systems specialized to the machine translation task (although it requires a few tricks over
the simple encoder-decoder that we   ll discuss in later sections: id125 (section 7.2), a
di   erent encoder (section 7.3), and ensembling (section 7.4)).

t by using a softmax on the hidden state h(e)

.

t

7.2 generating output
at this point, we have only mentioned how to create a id203 model p (e | f ) and haven   t
yet covered how to actually generate translations from it, which we will now cover in the next
section. in general, when we generate output we can do so according to several criteria:
random sampling: randomly select an output e from the id203 distribution p (e |

f ). this is usually denoted   e     p (e | f ).

1-best search: find the e that maximizes p (e | f ), denoted   e = argmax
n-best search: find the n outputs with the highest probabilities according to p (e | f ).

p (e | f ).

e

which of these methods we will choose will depend on our application, so we will discuss some
use cases along with the algorithms themselves.

7.2.1 random sampling

first, random sampling is useful in cases where we may want to get a variety of outputs for
a particular input. one example of a situation where this is useful would be in a sequence-
to-sequence model for a dialog system, where we would prefer the system to not always give
the same response to a particular user input to prevent monotony. luckily, in models like the
encoder-decoder above, it is simple to exactly generate samples from the distribution p (e | f )
using a method called ancestral sampling. ancestral sampling works by sampling variable
values one at a time, gradually conditioning on more context, so at time step t, we will
sample a word from the distribution p (et |   et   1
). in the encoder-decoder model, this means
we simply have to calculate pt according to the previously sampled inputs, leading to the
simple generation algorithm in algorithm 3.

1

one thing to note is that sometimes we also want to know the id203 of the sentence
that we sampled. for example, given a sentence   e generated by the model, we might want to
know how certain the model is in its prediction. during the sampling process, we can calculate

p (   e | f ) = (cid:81)|   e|

t p (  et | f,   et   1

1

) incrementally by stepping along and multiplying together

40

the probabilities of each sampled word. however, as we remember from the discussion of
id203 vs. log id203 in section 3.3, using probabilities as-is can result in very small
numbers that cause numerical precision problems on computers. thus, when calculating the
full-sentence id203 it is more common to instead add together log probabilities for each
word, which avoids this problem.

algorithm 3 generating random samples from a neural encoder-decoder
1: procedure sample
2:

for t from 1 to |f| do

3:

4:

5:

6:

7:

8:

9:

calculate m(f )

t

and h(f )

t

end for
set   e0 =   (cid:104)s(cid:105)    and t     0
while   et (cid:54)=   (cid:104)/s(cid:105)    do

t     t + 1
calculate m(e)
sample   et according to p(e)
t

, h(e)

, and p(e)

t

t

t

end while

10:
11: end procedure

7.2.2 greedy 1-best search

from   et   1

next, let   s consider the problem of generating a 1-best result. this variety of generation is
useful in machine translation, and most other applications where we simply want to output
the translation that the model thought was best. the simplest way of doing so is greedy
search, in which we simply calculate pt at every time step, select the word that gives us
the highest id203, and use it as the next word in our sequence. in other words, this
algorithm is exactly the same as algorithm 3, with the exception that on line 9, instead of
sampling   et randomly according to p(e)
t

, we instead choose the max:   et = argmax

p(e)
t,i .

interestingly, while ancestral sampling exactly samples outputs from the distribution ac-
cording to p (e | f ), greedy search is not guaranteed to    nd the translation with the highest
id203. an example of a case in which this is true can be found in the graph in fig-
ure 22, which is an example of search graph with a vocabulary of {a, b,(cid:104)/s(cid:105)}.25 as an
exercise, i encourage readers to    nd the true 1-best (or n-best) sentence according to the
id203 p (e | f ) and the id203 of the sentence found according to greedy search
and con   rm that these are di   erent.

i

7.2.3 id125

one way to solve this problem is through the use of id125. id125 is similar
to greedy search, but instead of considering only the one best hypothesis, we consider b best
hypotheses at each time step, where b is the    width    of the beam. an example of id125
where b = 2 is shown in figure 23 (note that we are using log probabilities here because they

25in reality, we will never have a id203 of exactly p (et = (cid:104)/s(cid:105) | f, et   1

1

) = 1.0, but for illustrative

purposes, we show this here.

41

figure 22: a search graph where greedy search fails.

figure 23: an example of id125 with b = 2. numbers next to arrows are log probabil-
ities for a single word log p (et | f, et   1
), while numbers above nodes are log probabilities for
the entire hypothesis up until this point.

1

42

e1p(e1|f)<s>   a      b   </s>   a      b   </s>   a      b   </s></s></s></s></s>e2p(e2|f,e1)e3p(e3|f,e1,e2)e00.350.40.250.150.80.10.50.41.01.00.051.01.0log p(e1|f)<s>   a      b   </s>   a      b   </s>   a      b   </s></s></s>log p(e2|f,e1)log p(e3|f,e1,e2)-1.05-0.92-1.39-1.90-0.22-2.30-0.69-0.920-3.000-1.05-0.92-1.39x-2.95-1.27-4.05-1.84-1.61-3.22xxxx-1.27-1.61are more conducive to comparing hypotheses over the entire sentence, as mentioned before).
in the    rst time step, we expand hypotheses for e1 corresponding to all of the three words in
the vocabulary, then keep the top two (   b    and    a   ) and delete the remaining one (   (cid:104)/s(cid:105)   ).
in the second time step, we expand hypotheses for e2 corresponding to the continuation of the
   rst hypotheses for all words in the vocabulary, temporarily creating b   |v | active hypotheses.
these active hypotheses are also pruned down to the b active hypotheses (   a b    and    b b   ).
this process of calculating scores for b     |v | continuations of active hypotheses, then pruning
back down to the top b, is continued until the end of the sentence.

machine translation, where p (e | f ) = (cid:81)|e|

one thing to be careful about when generating sentences using models, such as neural
) is that they tend to prefer
shorter sentences. this is because every time we add another word, we multiply in another
id203, reducing the id203 of the whole sentence. as we increase the beam size, the
search algorithm gets better at    nding these short sentences, and as a result, id125
with a larger beam size often has a signi   cant length bias towards these shorter sentences.
there have been several attempts to    x this length bias problem. for example, it is
possible to put a prior id203 on the length of the sentence given the length of the
previous sentence p (|e| | |f|), and multiply this with the standard sentence id203
p (e | f ) at decoding time [34]:

| f, et   1

t p (et

1

  e = argmax

log p (|e| | |f|) + log p (e | f ).

e

(61)

this prior id203 can be estimated from data, and [34] simply estimate this using a
multinomial distribution learned on the training data:

p (|e| | |f|) =

c(|e|,|f|)

c(|f|)

.

(62)

a more heuristic but still widely used approach normalizes the log id203 by the length
of the target sentence, e   ectively searching for the sentence that has the highest average log
id203 per word [21]:

  e = argmax

log p (e | f )/|e|.

e

(63)

7.3 other ways of encoding sequences

in section 7.1, we described a model that works by encoding sequences linearly, one word at
a time from left to right. however, this may not be the most natural or e   ective way to turn
the sentence f into a vector h. in this section, we   ll discuss a number of di   erent ways to
perform encoding that have been reported to be e   ective in the literature.

7.3.1 reverse and bidirectional encoders

first, [101] have proposed a reverse encoder. in this method, we simply run a standard
linear encoder over f , but instead of doing so from left to right, we do so from right to left.

(cid:40)            

      
h (f )

t =

id56(f )(m(f )
0

t

,

      
h (f )
t+1)

43

t     |f|,
otherwise.

(64)

figure 24: the distances between words with the same index in the forward and reverse
decoders.

the motivation behind this method is that for pairs of languages with similar ordering (such
as english-french, which the authors experimented on), the words at the beginning of f will
generally correspond to words at the beginning of e. assuming the extreme case that words
with identical indices correspond to each-other (e.g. f1 corresponds to e1, f2 to e2, etc.), the
distance between corresponding words in the linear encoding and decoding will be |f|, as
shown in figure 24(a). remembering the vanishing gradient problem from section 6.3, this
means that the id56 has to propagate the information across |f| time steps before making a
prediction, a di   cult feat. at the beginning of training, even id56 variants such as lstms
have trouble, as they have to essentially    guess    what part of the information encoded in their
hidden state is being used without any prior bias.

reversing the encoder helps solve this problem by reducing the length of dependencies for
a subset of the words in the sentence, speci   cally the ones at the beginning of the sentences.
as shown in figure 24(b), the length of the dependency for f1 and e1 is 1, and subsequent
pairs of ft and et have a distance of 2t    1. during learning, the model can    latch on    to these
short-distance dependencies and use them as a way to bootstrap the model training, after
which it becomes possible to gradually learn the longer dependencies for the words at the end
of the sentence. in [101], this proved critical to learn e   ective models in the encoder-decoder
framework.

however, this approach of reversing the encoder relies on the strong assumption that the
order of words in the input and output sequences are very similar, or at least that the words
at the beginning of sentences are the same. this is true for languages like english and french,
which share the same    subject-verb-object (svo)    word ordering, but may not be true for
more typologically distinct languages. one type of encoder that is slightly more robust to
in this method, we use two di   erent
these di   erences is the bi-directional encoder [4].

44

f1enc0f2enc   (a) dependency distances in forward encoderencf|f|e0dece1e1dece2   ence|e|e|e|-1|f||f||f|f|f|enc0f|f|-1enc   encf1e0dece1e1dece2   ence|e|e|e|-131|f|+|e|-1(b) dependency distances in reverse encoderfigure 25: examples of convolutional and tree-structured networks.

encoders: one traveling forward and one traveling backward over the input sentence

(cid:40)            
(cid:40)            

t

id56(f )(m(f )
0
id56(f )(m(f )
0

t

,

,

      
h (f )

t+   )

      
h (f )
t+1)

      
h (f )

t =

      
h (f )

t =

t     1,
otherwise.
t     |f|,
otherwise.

(65)

(66)

which are then combined into the initial vector h(e)
for the decoder id56. this combination
      
0
can be done by simply concatenating the two    nal vectors
h 1. however, this also
requires that the size of the vectors for the decoder id56 be exactly equal to the combined
size of the two encoder id56s. as a more    exible alternative, we can add an additional
parameterized hidden layer between the encoder and decoder states, which allows us to convert
the bidirectional encoder states into an appropriately-sized state for the decoder:

      
h |f| and

0 = tanh(w      
h(e)

f e

      
h |f| + w      

      
h 1 + be).

f e

(67)

7.3.2 convolutional neural networks

in addition, there are also methods for decoding that move beyond a simple linear view of
the input sentence. for example, convolutional neural networks (id98s; [37, 114, 62]),
figure 25(a)) are a variety of neural net that combines together information from spatially
or temporally local segments. they are most widely applied to image processing but have
also been used for speech processing, as well as the processing of textual sequences. while
there are many varieties of id98-based models of text (e.g.
[55, 63, 54]), here we will show
an example from [59]. this model has n    lters with a width w that are passed incrementally
over w-word segments of the input. speci   cally, given an embedding matrix m of width |f|,
we generate a hidden layer matrix h of width |f|     w + 1, where each column of the matrix
is equal to

ht = w concat(mt, mt+1, . . . , mt+w   1)

(68)

45

m1m2m3m4m5m|f|   h1h2h3h|f|-2   filtfiltfiltfiltpoolhm1m2m3m4m5m|f|   compcompcompcompcomp   h(a) convolutional neural net(b) tree-structured netfigure 26: an example of a syntax tree for a sentence showing the sentence structure and
phrase types (det=   determiner   , jj=   adjective   , nn=   noun   , vbd=   past tense verb   ,
np=   noun phrase   , np   =   part of a noun phrase   , vp=   verb phrase   , s=   sentence   ).

where w     rn  w|m| is a matrix where the ith row represents the parameters of    lter i that
will be multiplied by the embeddings of w consecutive words. if w = 3, we can interpret this
as h1 extracting a vector of features for f 3
2 , etc.
until the end of the sentence.

1 , h2 as extracting a vector of features for f 4

finally, we perform a pooling operation that converts this matrix h (which varies in
width according to the sentence length) into a single vector h (which is    xed-size and can
thus be used in down-stream processing). examples of pooling operations include average,
max, and k-max [55].

compared to id56s and their variants, id98s have several advantages and disadvantages:
    on the positive side, id98s provide a relatively simple way to detect features of short

word sequences in sentence text and accumulate them across the entire sentence.

    also on the positive side, id98s do not su   er as heavily from the vanishing gradient

problem, as they do not need to propagate gradients across multiple time steps.

    on the negative side, id98s are not quite as expressive and are a less natural way of

expressing complicated patterns that move beyond their    lter width.

in general, id98s have been found to be quite e   ective for text classi   cation, where it is more
important to pick out the most indicative features of the text and there is less of an emphasis
on getting an overall view of the content [59]. there have also been some positive results
reported using speci   c varieties of id98s for sequence-to-sequence modeling [54].

7.3.3 tree-structured networks

finally, one other popular form of encoder that is widely used in a number of tasks are tree-
structured networks ([83, 100], figure 25(b)). the basic idea behind these networks is
that the way to combine the information from each particular word is guided by some sort
of structure, usually the syntactic structure of the sentence, an example of which is shown
in figure 26. the reason why this is intuitively useful is because each syntactic phrase
usually also corresponds to a coherent semantic unit. thus, performing the calculation and
manipulation of vectors over these coherent units will be more appropriate compared to using
random substrings of words, like those used by id98s.

46

theredcatchasedthelittlebirddetjjnnvbddetjjnnnp'npnp'npvpsfor example, let   s say we have the phrase    the red cat chased the little bird    as shown
in the    gure. in this case, following a syntactic tree would ensure that we calculate vectors
for coherent units that correspond to a grammatical phrase such as    chased    and    the little
bird   , and combine these phrases together one by one to obtain the meaning of larger coherent
phrase such as    chased the little bird   . by doing so, we can take advantage of the fact
that language is compositional, with the meaning of a more complex phrase resulting from
regular combinations and transformation of smaller constituent phrases [102]. by taking
this linguistically motivated and intuitive view of the sentence, we hope will help the neural
networks learn more generalizable functions from limited training data.

perhaps the most simple type of tree-structured network is the recursive neural net-
work proposed by [100]. this network has very strong parallels to standard id56s, but
instead of calculating the hidden state ht at time t from the previous hidden state ht   1 as
follows:

ht = tanh(wxhxt + whhht   1 + bh),

(69)

we instead calculate the hidden state of the parent node hp from the hidden states of the left
and right children, hl and hr respectively:

hp = tanh(wxpxt + wlphl + wrphr + bp).

(70)

thus, the representation for each node in the tree can be calculated in a bottom-up fashion.
like standard id56s, these recursive networks su   er from the vanishing gradient problem.
to    x this problem there is an adaptation of lstms to tree-structured networks,    ttingly
called tree lstms [103], which    xes this vanishing gradient problem. there are also a wide
variety of other kinds of tree-structured composition functions that interested readers can
explore [99, 31, 32]. also of interest is the study by [66], which examines the various tasks in
which tree structures are necessary or unnecessary for nlp.

7.4 ensembling multiple models

one other method that is widely used in encoder-decoders, or other models of translation
is ensembling: the combination of the prediction of multiple independently trained models
to improve the overall prediction results. the intuition behind ensembling is that di   erent
models will make di   erent mistakes, and that on average it is more common for models to agree
when the answer is correct than when it is mistaken. thus, if we combine multiple models
together, it becomes possible to smooth over these mistakes,    nding the correct answer more
often.
the    rst step in ensembling encoder-decoder models is to independently train n di   erent
models p1(  ), p2(  ), . . . , pn (  ), for example, by randomly initializing the weights of the neural
network di   erently before training. next, during search, at each time step we calculate the
id203 of the next word as the average id203 of the n models:

p (et | f, et   1

1

) =

1
n

pi(et | f, et   1

1

).

(71)

n(cid:88)

i=1

this id203 is used in searching for our hypotheses.

47

7.5 exercise

in the exercise for this chapter, we will create an encoder-decoder translation model and make
it possible to generate translations.
writing the program will entail:
    extend your id56 language model code to    rst read in a source sentence to calculate

the initial hidden state.

    on the training set, write code to calculate the id168 and perform training.
    on the development set, generate translations using greedy search.
    evaluate your generated translations by comparing them to the reference translations
to see if they look good or not. translations can also be evaluated by automatic means,
such as id7 score [81]. a reference implementation of a id7 evaluation script can be
found here: https://github.com/moses-smt/mosesdecoder/blob/master/scripts/
generic/multi-id7.perl.

potential improvements to the model include: implementing id125 and comparing
the results with greedy search. implementing an alternative encoder. implementing ensem-
bling.

8 attentional neural mt

in the past chapter, we described a simple model for id4, which uses
an encoder to encode sentences as a    xed-length vector. however, in some ways, this view
is overly simpli   ed, and by the introduction of a powerful mechanism called attention, we
can overcome these di   culties. this section describes the problems with the encoder-decoder
architecture and what attention does to    x these problems.

8.1 problems of representation in encoder-decoders

theoretically, a su   ciently large and well-trained encoder-decoder model should be able to
perform machine translation perfectly. as mentioned in section 5.2, neural networks are
universal function approximators, meaning that they can express any function that we wish
to model, including a function that accurately predicts our predictive id203 for the next
word p (et | f, et   1
). however, in practice, it is necessary to learn these functions from limited
data, and when we do so, it is important to have a proper inductive bias     an appropriate
model structure that allows the network to learn to model accurately with a reasonable amount
of data.

1

there are two things that are worrying about the standard encoder-decoder architecture.
the    rst was described in the previous section: there are long-distance dependencies between
words that need to be translated into each other. in the previous section, this was alleviated
to some extent by reversing the direction of the encoder to bootstrap training, but still, a
large number of long-distance dependencies remain, and it is hard to guarantee that we will
learn to handle these properly.

48

the second, and perhaps more, worrying aspect of the encoder-decoder is that it attempts
to store information sentences of any arbitrary length in a hidden vector of    xed size. in other
words, even if our machine translation system is expected to translate sentences of lengths
from 1 word to 100 words, it will still use the same intermediate representation to store all
of the information about the input sentence. if our network is too small, it will not be able
to encode all of the information in the longer sentences that we will be expected to translate.
on the other hand, even if we make the network large enough to handle the largest sentences
in our inputs, when processing shorter sentences, this may be overkill, using needlessly large
amounts of memory and computation time. in addition, because these networks will have
large numbers of parameters, it will be more di   cult to learn them in the face of limited data
without encountering problems such as over   tting.

the remainder of this section discusses a more natural way to solve the translation problem

with neural networks: attention.

8.2 attention

the basic idea of attention is that instead of attempting to learn a single vector representation
for each sentence, we instead keep around vectors for every word in the input sentence, and
reference these vectors at each decoding step. because the number of vectors available to
reference is equivalent to the number of words in the input sentence, long sentences will have
many vectors and short sentences will have few vectors. as a result, we can express input
sentences in a much more e   cient way, avoiding the problems of ine   cient representations
for encoder-decoders mentioned in the previous section.

first we create a set of vectors that we will be using as this variably-lengthed represen-
tation. to do so, we calculate a vector for every word in the source sentence by running an
id56 in both directions:

      
h (f )
      
h (f )

j = id56(embed(fj),

j = id56(embed(fj),

      
h (f )
      
h (f )

j   1)

j+1).

      
h (f )

j

h(f )

j = [

j

      
h (f )
      
h (f )

;

j

and
      
h (f )

j

].

then we concatenate the two vectors

into a bidirectional representation h(f )

j

we can further concatenate these vectors into a matrix:

h (f ) = concat col(h(f )

1 , . . . , h(f )|f|).

this will give us a matrix where every column corresponds to one word in the input sentence.
however, we are now faced with a di   culty. we have a matrix h (f ) with a variable
number of columns depending on the length of the source sentence, but would like to use this
to compute, for example, the probabilities over the output vocabulary, which we only know
how to do (directly) for the case where we have a vector of input. the key insight of attention
is that we calculate a vector   t that can be used to combine together the columns of h into
a vector ct

(72)

ct = h (f )  t.

49

figure 27: an example of attention from [4]. english is the source, french is the target, and
a higher attention weight when generating a particular target word is indicated by a lighter
color in the matrix.

  t is called the attention vector, and is generally assumed to have elements that are between
zero and one and add to one.

the basic idea behind the attention vector is that it is telling us how much we are    fo-
cusing    on a particular source word at a particular time step. the larger the value in   t, the
more impact a word will have when predicting the next word in the output sentence. an ex-
ample of how this attention plays out in an actual translation example is shown in figure 27,
and as we can see the values in the alignment vectors generally align with our intuition.

8.3 calculating attention scores

the next question then becomes, from where do we get this   t? the answer to this lies in
the decoder id56, which we use to track our state while we are generating output. as before,
the decoder   s hidden state h(e)
is a    xed-length continuous vector representing the previous
target words et   1
0 = h(f )|f|+1. this is used to calculate a context vector ct
that is used to summarize the source attentional context used in choosing target word et, and
initialized as c0 = 0.

, initialized as h(e)

1

t

first, we update the hidden state to h(e)

t based on the word representation and context

vectors from the previous target time step

h(e)
t = enc([embed(et   1); ct   1], h(e)

t   1).

based on this h(e)

t

, we calculate an attention score at, with each element equal to

at,j = attn score(h(f )

j

, h(e)

t ).

50

(73)

(74)

figure 28: a computation graph for attention.

attn score(  ) can be an arbitrary function that takes two vectors as input and outputs a score
about how much we should focus on this particular input word encoding h(f )
at the time step
h(e)

. we describe some examples at a later point in section 8.4.
we then normalize this into the actual attention vector itself by taking a softmax over the

j

t

scores:

  t = softmax(at).

(75)

this attention vector is then used to weight the encoded representation h (f ) to create a
context vector ct for the current time step, as mentioned in equation 72.

we now have a context vector ct and hidden state h(e)
t

for time step t, which we can pass
on down to downstream tasks. for example, we can concatenate both of these together when
calculating the softmax distribution over the next words:

p(e)
t = softmax(whs[h(e)

t

; ct] + bs).

(76)

it is worth noting that this means that the encoding of each source word h(f )
is considered
much more directly in the calculation of output probabilities.
in contrast to the encoder-
decoder, where the encoder-decoder will only be able to access information about the    rst
encoded word in the source by passing it over |f| time steps, here the source encoding is
accessed (in a weighted manner) through the context vector equation 72.

j

this whole, rather involved, process is shown in figure 28.

8.4 ways of calculating attention scores

as mentioned in equation 74, the    nal missing piece to the puzzle is how to calculate the
attention score at,j.

[68] test three di   erent attention functions, all of which have their own merits:

51

thisaispenlookuplookuplookuplookupid56id56id56id56id56id56id56id5600concatconcatconcatconcatconcat_colhtattn_scoresoftmaxxconcatsoftmax(x+)ptdot product: this is the simplest of the functions, as it simply calculates the similarity

between h(e)

t

and h(f )

j

as measured by the dot product:

attn score(h(f )

j

, h(e)

t ) := h(f )(cid:124)

j h(e)

t

.

(77)

this model has the advantage that it adds no additional parameters to the model.
however, it also has the intuitive disadvantage that it forces the input and output
encodings to be in the same space (because similar h(e)
j must be close in space
in order for their dot product to be high). it should also be noted that the dot product
can be calculated e   ciently for every word in the source sentence by instead de   ning
the attention score over the concatenated matrix h (f ) as follows:

and h(f )

t

attn score(h (f ), h(e)

t ) := h (f )(cid:124)

j h(e)

t

.

(78)

combining the many attention operations into one can be useful for e   cient impemen-
tation, especially on gpus. the following attention functions can also be calculated
like this similarly.26

bilinear functions: one slight modi   cation to the dot product that is more expressive is the
bilinear function. this function helps relax the restriction that the source and target
embeddings must be in the same space by performing a linear transform parameterized
by wa before taking the dot product:

attn score(h(f )

j

, h(e)

t ) := h(f )(cid:124)

j wah(e)

t

.

(79)

this has the advantage that if wa is not a square matrix, it is possible for the two
vectors to be of di   erent sizes, so it is possible for the encoder and decoder to have
di   erent dimensions. however, it does introduce quite a few parameters to the model
(|h(f )|    |h(e)|), which may be di   cult to train properly.

multi-layer id88s: finally, it is also possible to calculate the attention score us-
ing a multi-layer id88, which was the method employed by [4] in their original
implementation of attention:

attn score(h(e)

t

, h(f )

j

) := w

(cid:124)
a2tanh(wa1[h(e)

t

; h(f )

j

]),

(80)

where wa1 and wa2 are the weight matrix and vector of the    rst and second layers of
the mlp respectively. this is more    exible than the dot product method, usually has
fewer parameters than the bilinear method, and generally provides good results.

in addition to these methods above, which are essentially the defacto-standard, there are
a few more sophisticated methods for calculating attention as well. for example, it is possible
to use recurrent neural networks [120], tree-structured networks based on document structure
[121], convolutional neural networks [2], or structured models [58] to calculate attention.

26question: what do the equations look like for the combined versions of the following functions?

52

8.5 copying and unknown word replacement

one pleasant side-e   ect of attention is that it not only increases translation accuracy, but
also makes it easier to tell which words are translated into which words in the output. one
obvious consequence of this is that we can draw intuitive graphs such as the one shown in
figure 27, which aid error analysis.

another advantage is that it also becomes possible to handle unknown words in a more
elegant way, performing unknown word replacement [67]. the idea of this method is
simple, every time our decoder chooses the unknown word token (cid:104)unk(cid:105) in the output, we look
up the source word with the highest attention weight at this time step, and output that word
instead of the unknown token (cid:104)unk(cid:105). if we do so, at least the user can see which words have
been left untranslated, which is better than seeing them disappear altogether or be replaced
by a placeholder.

it is also common to use alignment models such as those described in [16] to obtain a
translation dictionary, then use this to aid unknown word replacement even further. specif-
ically, instead of copying the word as-is into the output, if the chosen source word is f , we
output the word with the highest translation id203 pdict(e | f ). this allows words that
are included in the dictionary to be mapped into their most-frequent counterpart in the target
language.

8.6 intuitive priors on attention

because of the importance of attention in modern id4 systems, there have also been a num-
ber of proposals to improve accuracy of estimating the attention itself through the introduction
of intuitively motivated prior probabilities. [25] propose several methods to incorporate biases
into the training of the model to ensure that the attention weights match our belief of what
alignments between languages look like.

these take several forms, and are heavily inspired by the alignment models used in more
traditional smt systems such as those proposed by [16]. these models can be brie   y sum-
marized as:

position bias: if two languages have similar word order, then it is more likely that align-
ments should fall along the diagonal. this is demonstrated strongly in figure 27. it
is possible to encourage this behavior by adding a prior id203 over attention that
makes it easier for things near the diagonal to be aligned.

markov condition: in most languages, we can assume that most of the time if two words
in the target are contiguous, the aligned words in the source will also be contiguous. for
example, in figure 27, this is true for all contiguous pairs of english words except    the,
european    and    area, was   . to take advantage of this property, it is possible to impose
a prior that discourages large jumps and encourages local steps in attention. a model
that is similar in motivation, but di   erent in implementation, is the local attention
model [68], which selects which part of the source sentence to focus on using the neural
network itself.

fertility: we can assume that some words will be translated into a certain number words in
the other langauge. for example, the english word    cats    will be translated into two
words    les chats    in french. priors on fertility takes advantage of this fact by giving

53

the model a penalty when particular words are not attended too much, or attended to
too much. in fact one of the major problems with poorly trained neural mt systems is
that they repeat the same word over and over, or drop words, a violation of this fertility
constraint. because of this, several other methods have been proposed to incorporate
coverage in the model itself [106, 69], or as a constraint during the decoding process
[119].

bilingual symmetry: finally, we expect that words that are aligned when performing
translation from f to e should also be aligned when performing translation from e
to f . this can be enforced by training two models in parallel, and enforcing constraints
that the alignment matrices look similar in both directions.

[25] experiment extensively with these approaches, and    nd that the bilingual symmetry
constraint is particularly e   ective among the various methods.

8.7 further reading

this section outlines some further directions for reading more about improvements to atten-
tion:

hard attention: as shown in equation 75, standard attention uses a soft combination of
various contents. there are also methods for hard attention that make a hard binary
decision about whether to focus on a particular context, with motivations ranging from
learning explainable models [64], to processing text incrementally [122, 45].

supervised training of attention: in addition, sometimes we have hand-annotated data
showing us true alignments for a particular language pair. it is possible to train atten-
tional models using this data by de   ning a id168 that penalizes the model when
it does not predict these alignments correctly [70].

other ways of memorizing input: finally, there are other ways of accessing relevant
information other than attention.
[115] propose a method using memory networks,
which have a separate set of memory that can be written to or read from as the processing
continues.

8.8 exercise

in the exercise for this chapter, we will create code to train and generate translations with an
attentional neural mt model.

writing the program will entail extending your encoder-decoder code to add attention.

you can then generate translations and compare them to others.

    extend your encoder-decoder code to add attention.
    on the training set, write code to calculate the id168 and perform training.
    on the development set, generate translations using greedy search.
    evaluate these translations, either manually or automatically.

54

it is also highly recommended, but not necessary, that you attempt to implement unknown
word replacement.

potential improvements to the model include implementing any of the improvements to

attention mentioned in section 8.6 or section 8.7.

9 conclusion

this tutorial has covered the basics of id4 and sequence-to-sequence
models. it gradually stepped through models of increasing sophistication, starting with n-
gram language models, and culminating in attention, which now represents the state-of-the-art
in many sequence-to-sequence modeling tasks.

it should be noted that this is a very active reserach    eld, and there are a number of
advanced research topics that are beyond the scope of this tutorial, but may be of interest to
readers who have mastered the basics and would like to learn more.

handling large vocabularies: one di   culty of neural mt models is that they perform
badly when using large vocabularies; it is hard to learn how to properly translate rare
words with limited data, and computation becomes a burden. one method to handle
this is to break words into smaller units such as characters [23] or subwords [94]. it
is also possible to incorporate translation dictionaries with broad coverage to handle
low-frequency phenomena [3].

optimizing translation performance: while the models presented in this tutorial are
trained to maximize the likelihood of the target sentence given the source p (e | f ), in
reality what we actually care about is the accuracy of the generated sentences. there
have been a number of works proposed to resolve this disconnect by directly considering
the accuracy of the generated results when training our models. these include methods
that sample translation results from the current model and move towards parameters
that result in good translations [84, 97], methods that optimize parameters towards
partially mistaken hypotheses to try to improve robustness to mistakes in generation
[8, 79], or methods that try to prevent mistakes that may occur during the search process
[117].

multi-lingual learning: up until now we assumed that we were training a model between
two languages f and e. however, in reality there are many languages in the world, and
some work has shown that we can bene   t by using data from all these languages to learn
models together [35, 52, 46]. it is also possible to perform transfer across languages,
training a model    rst on one language pair, then    ne-tuning it to others [124].

other applications: similar sequence-to-sequence models have been used for a wide variety
of tasks, from id71 [112, 95] to text summarization [92], id103
[17], id133 [109], image captioning [56, 113], image generation [43], and more.

this is just a small sampling of topics from this exciting and rapidly expanding    eld, and
hopefully this tutorial gave readers the tools to strike out on their own and apply these models
to their applications of interest.

55

acknowledgements

i am extremely grateful to qinlan shen and dongyeop kang for their careful reading of these
materials and useful comments about unclear parts. i also thank the students in the machine
translation and sequence-to-sequence models class at cmu for pointing out various bugs in
the materials when a preliminary version was used in the class.

references

[1] mart  n abadi, ashish agarwal, paul barham, eugene brevdo, zhifeng chen, craig
citro, greg s corrado, andy davis, je   rey dean, matthieu devin, et al. tensor-
   ow: large-scale machine learning on heterogeneous distributed systems. arxiv preprint
arxiv:1603.04467, 2016.

[2] miltiadis allamanis, hao peng, and charles sutton. a convolutional attention network

for extreme summarization of source code. arxiv preprint arxiv:1602.03001, 2016.

[3] philip arthur, graham neubig, and satoshi nakamura. incorporating discrete transla-
tion lexicons into id4. in proceedings of the 2016 conference on
empirical methods in natural language processing (emnlp), 2016.

[4] dzmitry bahdanau, kyunghyun cho, and yoshua bengio. id4
by jointly learning to align and translate. in proceedings of the international conference
on learning representations (iclr), 2015.

[5] marco baroni, georgiana dinu, and germ  an kruszewski. don   t count, predict! a sys-
tematic comparison of context-counting vs. context-predicting semantic vectors. in pro-
ceedings of the 52nd annual meeting of the association for computational linguistics
(acl), pages 238   247, baltimore, maryland, june 2014. association for computational
linguistics.

[6] jerome r bellegarda. statistical language model adaptation: review and perspectives.

speech communication, 42(1):93   108, 2004.

[7] claus bendtsen and ole stauning. fadbad, a    exible c++ package for automatic dif-
ferentiation. department of mathematical modelling, technical university of denmark,
1996.

[8] samy bengio, oriol vinyals, navdeep jaitly, and noam shazeer. scheduled sampling for
sequence prediction with recurrent neural networks. in proceedings of the 29th annual
conference on neural information processing systems (nips), pages 1171   1179, 2015.

[9] yoshua bengio, holger schwenk, jean-s  ebastien sen  ecal, fr  ederic morin, and jean-luc
gauvain. neural probabilistic language models. in innovations in machine learning,
volume 194, pages 137   186. 2006.

[10] yoshua bengio and jean-s  ebastien sen  ecal. adaptive importance sampling to accel-
erate training of a neural probabilistic language model. ieee transactions on neural
networks, 19(4):713   722, 2008.

56

[11] adam l. berger, stephen a. della pietra, and vincent j. della pietra. a maximum
id178 approach to natural language processing. computational linguistics, 22, 1996.

[12] james bergstra, olivier breuleux, fr  ed  eric bastien, pascal lamblin, razvan pascanu,
guillaume desjardins, joseph turian, david warde-farley, and yoshua bengio. theano:
a cpu and gpu math compiler in python. in proc. 9th python in science conf, pages
1   7, 2010.

[13] david m. blei, andrew y. ng, and michael i. jordan. id44.

journal of machine learning research, 3, 2003.

[14] thorsten brants, ashok c. popat, peng xu, franz j. och, and je   rey dean. large
language models in machine translation. in proceedings of the 2007 joint conference
on empirical methods in natural language processing and computational natural lan-
guage learning (emnlp-conll), pages 858   867, 2007.

[15] peter f. brown, peter v. desouza, robert l. mercer, vincent j. della pietra, and
jenifer c. lai. class-based id165 models of natural language. comput. linguist.,
18(4):467   479, 1992.

[16] peter f. brown, vincent j.della pietra, stephen a. della pietra, and robert l. mercer.
the mathematics of id151: parameter estimation. computa-
tional linguistics, 19:263   312, 1993.

[17] william chan, navdeep jaitly, quoc le, and oriol vinyals. listen, attend and spell: a
neural network for large vocabulary conversational id103. in proceedings
of the international conference on acoustics, speech, and signal processing (icassp),
pages 4960   4964. ieee, 2016.

[18] stanley chen. shrinking exponential language models. in proceedings of the human
language technologies: the 2009 conference of the north american chapter of the
association for computational linguistics, pages 468   476, 2009.

[19] stanley f. chen and joshua goodman. an empirical study of smoothing techniques for
id38. in proceedings of the 34th annual meeting of the association for
computational linguistics (acl), pages 310   318, 1996.

[20] stanley f. chen and roni rosenfeld. a survey of smoothing techniques for me models.

speech and audio processing, ieee transactions on, 8(1):37   50, jan 2000.

[21] kyunghyun cho, bart van merrienboer, dzmitry bahdanau, and yoshua bengio. on the
properties of id4: encoder   decoder approaches. in proceedings
of the workshop on syntax and structure in statistical translation, pages 103   111,
2014.

[22] lonnie chrisman. learning recursive distributed representations for holistic computa-

tion. connection science, 3(4):345   366, 1991.

[23] junyoung chung, kyunghyun cho, and yoshua bengio. a character-level decoder
in proceedings of the

without explicit segmentation for id4.

57

54th annual meeting of the association for computational linguistics (acl), pages
1693   1703, 2016.

[24] junyoung chung, caglar gulcehre, kyunghyun cho, and yoshua bengio. empirical
evaluation of gated recurrent neural networks on sequence modeling. arxiv preprint
arxiv:1412.3555, 2014.

[25] trevor cohn, cong duy vu hoang, ekaterina vymolova, kaisheng yao, chris dyer,
and gholamreza ha   ari. incorporating structural alignment biases into an attentional
neural translation model. in proceedings of the 2016 conference of the north american
chapter of the association for computational linguistics: human language technolo-
gies (naacl-hlt), pages 876   885, 2016.

[26] ronan collobert, samy bengio, and johnny mari  ethoz. torch: a modular machine

learning software library. technical report, idiap, 2002.

[27] ronan collobert, jason weston, l  eon bottou, michael karlen, koray kavukcuoglu, and
pavel kuksa. natural language processing (almost) from scratch. journal of machine
learning research, 12:2493   2537, 2011.

[28] corinna cortes and vladimir vapnik. support-vector networks. machine learning,

20(3):273   297, 1995.

[29] andrew m dai and quoc v le. semi-supervised sequence learning. in proceedings of
the 29th annual conference on neural information processing systems (nips), pages
3079   3087, 2015.

[30] john duchi, elad hazan, and yoram singer. adaptive subgradient methods for online
learning and stochastic optimization. journal of machine learning research, 12:2121   
2159, 2011.

[31] chris dyer, miguel ballesteros, wang ling, austin matthews, and noah a. smith.
transition-based id33 with stack long short-term memory. in proceedings
of the 53rd annual meeting of the association for computational linguistics (acl),
pages 334   343, 2015.

[32] chris dyer, adhiguna kuncoro, miguel ballesteros, and noah a. smith. recurrent neu-
ral network grammars. in proceedings of the 2016 conference of the north american
chapter of the association for computational linguistics: human language technolo-
gies (naacl-hlt), pages 199   209, 2016.

[33] je   rey l elman. finding structure in time. cognitive science, 14(2):179   211, 1990.

[34] akiko eriguchi, kazuma hashimoto, and yoshimasa tsuruoka. tree-to-sequence at-
tentional id4. in proceedings of the 54th annual meeting of the
association for computational linguistics (acl), pages 823   833, 2016.

[35] orhan firat, kyunghyun cho, and yoshua bengio. multi-way, multilingual neural
in proceedings of the 2016
machine translation with a shared attention mechanism.
conference of the north american chapter of the association for computational lin-
guistics: human language technologies (naacl-hlt), pages 866   875, 2016.

58

[36] mikel l forcada and ram  on p   neco. recursive hetero-associative memories for trans-
lation. in international work-conference on arti   cial neural networks, pages 453   462.
springer, 1997.

[37] kunihiko fukushima. neocognitron: a hierarchical neural network capable of visual

pattern recognition. neural networks, 1(2):119   130, 1988.

[38] felix a gers, j  urgen schmidhuber, and fred cummins. learning to forget: continual

prediction with lstm. neural computation, 12(10):2451   2471, 2000.

[39] yoav goldberg. a primer on neural network models for natural language processing.

arxiv preprint arxiv:1510.00726, 2015.

[40] joshua goodman. classes for fast maximum id178 training. in proceedings of the
international conference on acoustics, speech, and signal processing (icassp), pages
561   564. ieee, 2001.

[41] joshua t. goodman. a bit of progress in id38. computer speech &

language, 15(4):403   434, 2001.

[42] klaus gre   , rupesh kumar srivastava, jan koutn    k, bas r. steunebrink, and j  urgen

schmidhuber. lstm: a search space odyssey. corr, abs/1503.04069, 2015.

[43] karol gregor, ivo danihelka, alex graves, danilo jimenez rezende, and daan wier-
arxiv preprint

stra. draw: a recurrent neural network for image generation.
arxiv:1502.04623, 2015.

[44] andreas griewank. automatic di   erentiation of algorithms: theory, implementation,
and application. in proceedings of the    rst siam workshop on automatic di   erentia-
tion, 1991.

[45] jiatao gu, graham neubig, kyunghyun cho, and victor ok li. learning to translate

in real-time with id4. 2017.

[46] thanh-le ha, jan niehues, and alexander waibel. toward multilingual neural machine
translation with universal encoder and decoder. arxiv preprint arxiv:1611.04798, 2016.

[47] kaiming he, xiangyu zhang, shaoqing ren, and jian sun. deep residual learning for
image recognition. in proceedings of the 29th ieee conference on id161
and pattern recognition (cvpr), pages 770   778, 2016.

[48] kenneth hea   eld. kenlm: faster and smaller language model queries. in proceedings
of the 6th workshop on id151 (wmt), pages 187   197, 2011.

[49] sepp hochreiter and j  urgen schmidhuber. long short-term memory. neural computa-

tion, 9(8):1735   1780, 1997.

[50] robin j hogan. fast reverse-mode automatic di   erentiation using expression templates

in c++. acm transactions on mathematical software (toms), 40(4):26, 2014.

[51] kurt hornik, maxwell stinchcombe, and halbert white. multilayer feedforward net-

works are universal approximators. neural networks, 2(5):359   366, 1989.

59

[52] melvin johnson, mike schuster, quoc v le, maxim krikun, yonghui wu, zhifeng chen,
nikhil thorat, fernanda vi  egas, martin wattenberg, greg corrado, et al. google   s
multilingual id4 system: enabling zero-shot translation. arxiv
preprint arxiv:1611.04558, 2016.

[53] nal kalchbrenner and phil blunsom. recurrent continuous translation models. in pro-
ceedings of the 2013 conference on empirical methods in natural language processing
(emnlp), pages 1700   1709, 2013.

[54] nal kalchbrenner, lasse espeholt, karen simonyan, aaron van den oord, alex graves,
and koray kavukcuoglu. id4 in linear time. arxiv preprint
arxiv:1610.10099, 2016.

[55] nal kalchbrenner, edward grefenstette, and phil blunsom. a convolutional neural
in proceedings of the 52nd annual meeting of the

network for modelling sentences.
association for computational linguistics (acl), pages 655   665, 2014.

[56] andrej karpathy and li fei-fei. deep visual-semantic alignments for generating image

descriptions. pages 3128   3137, 2015.

[57] andrej karpathy, justin johnson, and li fei-fei. visualizing and understanding recur-

rent networks. arxiv preprint arxiv:1506.02078, 2015.

[58] y. kim, c. denton, l. hoang, and a. m. rush. structured attention networks. arxiv

e-prints, february 2017.

[59] yoon kim. convolutional neural networks for sentence classi   cation. in proceedings of
the 2014 conference on empirical methods in natural language processing (emnlp),
pages 1746   1751, 2014.

[60] diederik kingma and jimmy ba. adam: a method for stochastic optimization. arxiv

preprint arxiv:1412.6980, 2014.

[61] roland kuhn and renato de mori. a cache-based natural language model for speech
recognition. ieee transactions on pattern analysis and machine intelligence, 12(6):570   
583, 1990.

[62] yann lecun, l  eon bottou, yoshua bengio, and patrick ha   ner. gradient-based learn-
ing applied to document recognition. proceedings of the ieee, 86(11):2278   2324, 1998.

[63] tao lei, regina barzilay, and tommi jaakkola. molding id98s for text: non-linear, non-
consecutive convolutions. in proceedings of the 2015 conference on empirical methods
in natural language processing (emnlp), pages 1565   1575, 2015.

[64] tao lei, regina barzilay, and tommi jaakkola. rationalizing neural predictions. in
proceedings of the 2016 conference on empirical methods in natural language process-
ing (emnlp), pages 107   117, 2016.

[65] jiwei li, xinlei chen, eduard hovy, and dan jurafsky. visualizing and understanding

neural models in nlp. arxiv preprint arxiv:1506.01066, 2015.

60

[66] jiwei li, thang luong, dan jurafsky, and eduard hovy. when are tree structures
necessary for deep learning of representations? in proceedings of the 2015 conference
on empirical methods in natural language processing (emnlp), pages 2304   2314,
2015.

[67] minh-thang luong, ilya sutskever, quoc le, oriol vinyals, and wojciech zaremba.
addressing the rare word problem in id4. in proceedings of the
53rd annual meeting of the association for computational linguistics (acl), pages
11   19, 2015.

[68] thang luong, hieu pham, and christopher d. manning. e   ective approaches to
attention-based id4. in proceedings of the 2015 conference on
empirical methods in natural language processing (emnlp), pages 1412   1421, 2015.

[69] haitao mi, baskaran sankaran, zhiguo wang, and abe ittycheriah. coverage embed-
ding models for id4. in proceedings of the 2016 conference on
empirical methods in natural language processing (emnlp), pages 955   960, 2016.

[70] haitao mi, zhiguo wang, and abe ittycheriah. supervised attentions for neural machine
translation. in proceedings of the 2016 conference on empirical methods in natural
language processing (emnlp), pages 2283   2288, 2016.

[71] tomas mikolov, kai chen, greg corrado, and je   rey dean. e   cient estimation of

word representations in vector space. arxiv preprint arxiv:1301.3781, 2013.

[72] tomas mikolov, martin kara     at, lukas burget, jan cernock`y, and sanjeev khudanpur.
recurrent neural network based language model.
in proceedings of the 11th annual
conference of the international speech communication association (interspeech), pages
1045   1048, 2010.

[73] tomas mikolov, ilya sutskever, kai chen, greg s corrado, and je    dean. distributed
in proceedings of
representations of words and phrases and their compositionality.
the 27th annual conference on neural information processing systems (nips), pages
3111   3119, 2013.

[74] andriy mnih and yee whye teh. a fast and simple algorithm for training neural

probabilistic language models. arxiv preprint arxiv:1206.6426, 2012.

[75] brian murphy, partha talukdar, and tom mitchell. learning e   ective and interpretable
semantic models using non-negative sparse embedding. in proceedings of the 24th inter-
national conference on computational linguistics (coling), pages 1933   1950, 2012.

[76] masami nakamura, katsuteru maruyama, takeshi kawabata, and kiyohiro shikano.
neural network approach to word category prediction for english texts. in proceedings
of the 13th international conference on computational linguistics (coling), 1990.

[77] graham neubig and chris dyer. generalizing and hybridizing count-based and neu-
ral language models. in proceedings of the 2016 conference on empirical methods in
natural language processing (emnlp), 2016.

61

[78] graham neubig, chris dyer, yoav goldberg, austin matthews, waleed ammar, an-
tonios anastasopoulos, miguel ballesteros, david chiang, daniel clothiaux, trevor
cohn, kevin duh, manaal faruqui, cynthia gan, dan garrette, yangfeng ji, lingpeng
kong, adhiguna kuncoro, gaurav kumar, chaitanya malaviya, paul michel, yusuke
oda, matthew richardson, naomi saphra, swabha swayamdipta, and pengcheng yin.
dynet: the dynamic neural network toolkit. arxiv preprint arxiv:1701.03980, 2017.

[79] mohammad norouzi, samy bengio, navdeep jaitly, mike schuster, yonghui wu, dale
schuurmans, et al. reward augmented maximum likelihood for neural structured pre-
diction. in proceedings of the 30th annual conference on neural information processing
systems (nips), pages 1723   1731, 2016.

[80] daisuke okanohara and jun   ichi tsujii. a discriminative language model with pseudo-
in proceedings of the 45th annual meeting of the association for

negative samples.
computational linguistics (acl), pages 73   80, 2007.

[81] kishore papineni, salim roukos, todd ward, and wei-jing zhu. id7: a method for
automatic evaluation of machine translation. in proceedings of the 40th annual meeting
of the association for computational linguistics (acl), pages 311   318, 2002.

[82] adam pauls and dan klein. faster and smaller id165 language models. pages 258   267,

2011.

[83] jordan b pollack. recursive distributed representations. arti   cial intelligence,

46(1):77   105, 1990.

[84] marcaurelio ranzato, sumit chopra, michael auli, and wojciech zaremba. sequence
level training with recurrent neural networks. proceedings of the international confer-
ence on learning representations (iclr), 2016.

[85] philip resnik. selectional preference and sense disambiguation. in proceedings of the
acl siglex workshop on tagging text with lexical semantics: why, what, and
how, pages 52   57. washington, dc, 1997.

[86] brian roark, murat saraclar, michael collins, and mark johnson. discriminative lan-
guage modeling with conditional random    elds and the id88 algorithm. in pro-
ceedings of the 42nd annual meeting of the association for computational linguistics
(acl), pages 47   54, 2004.

[87] ronald rosenfeld. a maximum id178 approach to adaptive statistical language mod-

elling. computer speech and language, 10(3):187     228, 1996.

[88] ronald rosenfeld, stanley f chen, and xiaojin zhu. whole-sentence exponential lan-
guage models: a vehicle for linguistic-statistical integration. computer speech & lan-
guage, 15(1):55   73, 2001.

[89] sebastian ruder. an overview of id119 optimization algorithms. arxiv

preprint arxiv:1609.04747, 2016.

62

[90] d. e. rumelhart, g. e. hinton, and r. j. williams. parallel distributed processing:
explorations in the microstructure of cognition, vol. 1. chapter learning internal rep-
resentations by error propagation, pages 318   362. mit press, 1986.

[91] david e rumelhart, geo   rey e hinton, and ronald j williams. learning representa-

tions by back-propagating errors. cognitive modeling, 5(3):1, 1988.

[92] alexander m. rush, sumit chopra, and jason weston. a neural attention model for
abstractive sentence summarization. in proceedings of the 2015 conference on empirical
methods in natural language processing (emnlp), pages 379   389, 2015.

[93] hinrich sch utze. word space. 5:895   902, 1993.

[94] rico sennrich, barry haddow, and alexandra birch. id4 of rare
words with subword units. in proceedings of the 54th annual meeting of the association
for computational linguistics (acl), pages 1715   1725, 2016.

[95] lifeng shang, zhengdong lu, and hang li. neural responding machine for short-
in proceedings of the 53rd annual meeting of the association for

text conversation.
computational linguistics (acl), pages 1577   1586, 2015.

[96] libin shen, jinxi xu, and ralph weischedel. a new string-to-dependency machine
translation algorithm with a target dependency language model. in proceedings of the
46th annual meeting of the association for computational linguistics (acl), pages
577   585, 2008.

[97] shiqi shen, yong cheng, zhongjun he, wei he, hua wu, maosong sun, and yang
liu. minimum risk training for id4. in proceedings of the 54th
annual meeting of the association for computational linguistics (acl), pages 1683   
1692, 2016.

[98] xing shi, inkit padhi, and kevin knight. does string-based neural mt learn source syn-
tax? in proceedings of the 2016 conference on empirical methods in natural language
processing (emnlp), pages 1526   1534, 2016.

[99] richard socher, john bauer, christopher d. manning, and ng andrew y. parsing
with compositional vector grammars. in proceedings of the 51st annual meeting of the
association for computational linguistics (acl), pages 455   465, 2013.

[100] richard socher, cli    c lin, chris manning, and andrew y ng. parsing natural scenes
and natural language with id56s. in proceedings of the 28th inter-
national conference on machine learning (icml), pages 129   136, 2011.

[101] ilya sutskever, oriol vinyals, and quoc vv le. sequence to sequence learning with
neural networks. in proceedings of the 28th annual conference on neural information
processing systems (nips), pages 3104   3112, 2014.

[102] zolt  an gendler szab  o. compositionality. stanford encyclopedia of philosophy, 2010.

63

[103] kai sheng tai, richard socher, and christopher d. manning. improved semantic rep-
resentations from tree-structured id137. in proceedings of
the 53rd annual meeting of the association for computational linguistics (acl), 2015.

[104] david talbot and thorsten brants. randomized language models via perfect hash func-
tions. in proceedings of the 46th annual meeting of the association for computational
linguistics (acl), pages 505   513, 2008.

[105] seiya tokui, kenta oono, shohei hido, and justin clayton. chainer: a next-generation
open source framework for deep learning.
in proceedings of workshop on machine
learning systems (learningsys) in the twenty-ninth annual conference on neural
information processing systems (nips), 2015.

[106] zhaopeng tu, zhengdong lu, yang liu, xiaohua liu, and hang li. modeling cover-
age for id4. in proceedings of the 54th annual meeting of the
association for computational linguistics (acl), pages 76   85, 2016.

[107] joseph turian, lev ratinov, and yoshua bengio. word representations: a simple and
general method for semi-supervised learning. in proceedings of the 48th annual meeting
of the association for computational linguistics (acl), pages 384   394. association for
computational linguistics, 2010.

[108] peter d turney and patrick pantel. from frequency to meaning: vector space models

of semantics. journal of arti   cial intelligence research, 37:141   188, 2010.

[109] a  aron van den oord, sander dieleman, heiga zen, karen simonyan, oriol vinyals,
alex graves, nal kalchbrenner, andrew senior, and koray kavukcuoglu. wavenet: a
generative model for raw audio. corr abs/1609.03499, 2016.

[110] ashish vaswani, yinggong zhao, victoria fossum, and david chiang. decoding with
large-scale neural language models improves translation.
in proceedings of the 2013
conference on empirical methods in natural language processing (emnlp), pages
1387   1392, 2013.

[111] pascal vincent, alexandre de br  ebisson, and xavier bouthillier. e   cient exact gradient
update for training deep networks with very large sparse targets.
in proceedings of
the 29th annual conference on neural information processing systems (nips), pages
1108   1116, 2015.

[112] oriol vinyals and quoc le.

a neural conversational model.

arxiv preprint

arxiv:1506.05869, 2015.

[113] oriol vinyals, alexander toshev, samy bengio, and dumitru erhan. show and tell: a

neural image caption generator. pages 3156   3164, 2015.

[114] alex waibel, toshiyuki hanazawa, geo   rey hinton, kiyohiro shikano, and kevin j

lang. phoneme recognition using time-delay neural networks. 37(3):328   339, 1989.

[115] mingxuan wang, zhengdong lu, hang li, and qun liu. memory-enhanced decoder
in proceedings of the 2016 conference on empirical

for id4.
methods in natural language processing (emnlp), pages 278   286, 2016.

64

[116] r.e. wengert. a simple automatic derivative evaluation program. communications of

the acm, 7(8):463   464, 1964.

[117] sam wiseman and alexander m. rush. sequence-to-sequence learning as beam-search
optimization. in proceedings of the 2016 conference on empirical methods in natural
language processing (emnlp), pages 1296   1306, 2016.

[118] i.h. witten and t.c. bell. the zero-frequency problem: estimating the probabilities
of novel events in adaptive text compression. id205, ieee transactions
on, 37(4):1085   1094, 1991.

[119] yonghui wu, mike schuster, zhifeng chen, quoc v le, mohammad norouzi, wolfgang
macherey, maxim krikun, yuan cao, qin gao, klaus macherey, et al. google   s neural
machine translation system: bridging the gap between human and machine translation.
arxiv preprint arxiv:1609.08144, 2016.

[120] zichao yang, zhiting hu, yuntian deng, chris dyer, and alex smola. neural machine
translation with recurrent attention modeling. arxiv preprint arxiv:1607.05108, 2016.

[121] zichao yang, diyi yang, chris dyer, xiaodong he, alex smola, and eduard hovy.
hierarchical attention networks for document classi   cation. in proceedings of the 2016
conference of the north american chapter of the association for computational lin-
guistics: human language technologies, pages 1480   1489, san diego, california, june
2016. association for computational linguistics.

[122] lei yu, jan buys, and phil blunsom. online segment to segment neural transduc-
tion. in proceedings of the 2016 conference on empirical methods in natural language
processing (emnlp), pages 1307   1316, 2016.

[123] barret zoph and quoc v le. neural architecture search with id23.

arxiv preprint arxiv:1611.01578, 2016.

[124] barret zoph, deniz yuret, jonathan may, and kevin knight. id21 for low-
resource id4. in proceedings of the 2016 conference on empirical
methods in natural language processing (emnlp), pages 1568   1575, 2016.

65

