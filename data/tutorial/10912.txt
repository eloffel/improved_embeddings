task-oriented query reformulation with id23

tandon school of engineering

courant institute of mathematical sciences

rodrigo nogueira

new york university

7
1
0
2

 

p
e
s
4
2

 

 
 
]

r

i
.
s
c
[
 
 

4
v
2
7
5
4
0

.

4
0
7
1
:
v
i
x
r
a

rodrigonogueira@nyu.edu

abstract

search engines play an important role in
our everyday lives by assisting us in    nd-
ing the information we need. when we
input a complex query, however, results
are often far from satisfactory.
in this
work, we introduce a query reformula-
tion system based on a neural network that
rewrites a query to maximize the number
of relevant documents returned. we train
this neural network with reinforcement
learning. the actions correspond to select-
ing terms to build a reformulated query,
and the reward is the document recall. we
evaluate our approach on three datasets
against strong baselines and show a rel-
ative improvement of 5-20% in terms of
recall. furthermore, we present a simple
method to estimate a conservative upper-
bound performance of a model in a partic-
ular environment and verify that there is
still large room for improvements.

introduction

1
search engines help us    nd what we need among
the vast array of available data. when we request
some information using a long or inexact descrip-
tion of it, these systems, however, often fail to de-
liver relevant items.
in this case, what typically
follows is an iterative process in which we try to
express our need differently in the hope that the
system will return what we want. this is a major
issue in information retrieval. for instance, huang
and efthimiadis (2009) estimate that 28-52% of
all the web queries are modi   cations of previous
ones.

to a certain extent, this problem occurs be-
cause search engines rely on matching words in
the query with words in relevant documents, to

kyunghyun cho

center for data science
new york university

kyunghyun.cho@nyu.edu

figure 1: a graphical illustration of the proposed
framework for query reformulation. a set of doc-
uments d0 is retrieved from a search engine using
the initial query q0. our reformulator selects terms
from q0 and d0 to produce a reformulated query
q(cid:48) which is then sent to the search engine. docu-
ments d(cid:48) are returned, and a reward is computed
against the set of ground-truth documents. the re-
formulator is trained with id23
to produce a query, or a series of queries, to maxi-
mize the expected return.

perform retrieval. if there is a mismatch between
them, a relevant document may be missed.

one way to address this problem is to automati-
cally rewrite a query so that it becomes more likely
to retrieve relevant documents. this technique is
known as automatic query reformulation. it typi-
cally expands the original query by adding terms
from, for instance, dictionaries of synonyms such
as id138 (miller, 1995), or from the initial set
of retrieved documents (xu and croft, 1996). this
latter type of reformulation is known as pseudo (or
blind) relevance feedback (prf), in which the rel-
evance of each term of the retrieved documents is
automatically inferred.

the proposed method is built on top of prf but
differs from previous works as we frame the query

q'scorerreformulatorgroundtruthd'search engineq' :  cancer treatment state-of-the-art frontiers surveyrewardq0 :what are the most promising directions to cure cancer and why?d0reformulation problem as a reinforcement learn-
ing (rl) problem. an initial query is the natural
language expression of the desired goal, and an
agent (i.e. reformulator) learns to reformulate an
initial query to maximize the expected return (i.e.
retrieval performance) through actions (i.e. select-
ing terms for a new query). the environment is a
search engine which produces a new state (i.e. re-
trieved documents). our framework is illustrated
in fig. 1.

the most important implication of this frame-
work is that a search engine is treated as a black
box that an agent learns to use in order to retrieve
more relevant items. this opens the possibility
of training an agent to use a search engine for a
task other than the one it was originally intended
for. to support this claim, we evaluate our agent
on the task of id53 (q&a), citation
recommendation, and passage/snippet retrieval.

as for training data, we use two publicly avail-
able datasets (trec-car and jeopardy) and in-
troduce a new one (ms academic) with hundreds
of thousands of query/relevant document pairs
from the academic domain.

furthermore, we present a method to estimate
the upper bound performance of our rl-based
model. based on the estimated upper bound, we
claim that this framework has a strong potential
for future improvements.

here we summarize our main contributions:
    a id23 framework for au-

tomatic query reformulation.

    a simple method to estimate the upper-bound
performance of an rl-based model in a given
environment.

    a new large dataset with hundreds of thou-

sands of query/relevant document pairs.1

2 a id23 approach
2.1 model description
in this section we describe the proposed method,
illustrated in fig. 2.

the inputs are a query q0 consisting of a se-
quence of words (w1, ..., wn) and a candidate term
ti with some context words (ti   k, ..., ti+k), where
k     0 is the context window size. candidate terms
1the dataset and code to run the experiments are
https://github.com/nyu-dl/

available
queryreformulator.

at

figure 2: an illustration of our neural network-
based reformulator.

are from q0     d0, the union of the terms in the
original query and those from the documents d0
retrieved using q0.

we use a dictionary of pretrained word embed-
dings (mikolov et al., 2013) to convert the sym-
bolic terms wj and ti to their vector representa-
tions vj and ei     rd, respectively. we map out-
of-vocabulary terms to an additional vector that is
learned during training.
we convert the sequence {vj} to a    xed-size
vector   a(v) by using either a convolutional neu-
ral network (id98) followed by a max pooling op-
eration over the entire sequence (kim, 2014) or by
using the last hidden state of a recurrent neural
network (id56).2

similarly, we fed the candidate term vectors
ei to a id98 or id56 to obtain a vector repre-
sentation   b(ei) for each term ti. the convolu-
tional/recurrent layers serve an important role of
capturing context information, especially for out-
of-vocabulary and rare terms. id98s can pro-
cess candidate terms in parallel, and, therefore, are
faster for our application than id56s. id56s, on
the other hand, can encode longer contexts.

finally, we compute the id203 of selecting

2to deal with variable-length inputs in a mini-batch, we
pad smaller ones with zeros on both ends so they end up as
long as the largest sample in the mini-batch.

id98/id56id98/id56candidate terms  q0     d0original query q0wp(ti | q0)+uv+sv1v2vn...ei+2ei+1 eiei-1ei-2valuenetworkw1w2wn...ti-2ti-1titi+1ti+2ti as:
p (ti|q0) =   (u t tanh(w (  a(v)(cid:107)  b(ei)) + b)),
(1)
where    is the sigmoid function, (cid:107) is the vector
concatenation operation, w     rd  2d and u     rd
are weights, and b     r is a bias.
at test time, we de   ne the set of terms used in
the reformulated query as t = {ti | p (ti|q0) >
 }, where   is a hyper-parameter. at training time,
we sample the terms according to their id203
distribution, t = {ti |    = 1          p (ti|q0)}. we
concatenate the terms in t to form a reformulated
query q(cid:48), which will then be used to retrieve a new
set of documents d(cid:48).
2.2 sequence generation
one problem with the method previously de-
scribed is that terms are selected independently.
this may result in a reformulated query that con-
tains duplicated terms since the same term can ap-
pear multiple times in the feedback documents.
another problem is that the reformulated query
can be very long, resulting in a slow retrieval.

to solve these problems, we extend the model to
sequentially generate a reformulated query, as pro-
posed by buck et al. (2017). we use a recurrent
neural network (id56) that selects one term at a
time from the pool of candidate terms and stops
when a special token is selected. the advantage of
this approach is that the model can remember the
terms previously selected through its hidden state.
it can, therefore, produce more concise queries.

we de   ne the id203 of selecting ti as the

k-th term of a reformulated query as:

p (tk

i |q0)     exp(  b(ei)thk),

(2)

where hk is the hidden state vector at the k-th step,
computed as:
hk = tanh(wa  a(v) + wb  b(tk   1) + whhk   1),
(3)
where tk   1 is the term selected in the previous
step and wa     rd  d, wb     rd  d, and wh    
rd  d are weight matrices. in practice, we use an
lstm (hochreiter and schmidhuber, 1997) to en-
code the hidden state as this variant is known to
perform better than a vanilla id56.

we avoid normalizing over a large vocabulary
by using only terms from the retrieved documents.
this makes id136 faster and training practi-
cal since learning to select words from the whole

vocabulary might be too slow with reinforcement
learning, although we leave this experiment for a
future work.

2.3 training
we train the proposed model using rein-
force (williams, 1992) algorithm. the per-
example stochastic objective is de   ned as
    log p (t|q0),

ca = (r       r)

(cid:88)

(4)

t   t

where r is the reward and   r is the baseline, com-
puted by the value network as:

  r =   (st tanh(v (  a(v)(cid:107)  e) + b)),

(cid:80)n
(5)
i=1   b(ei), n = |q0     d0|, v    
where   e = 1
rd  2d and s     rd are weights and b     r is a
n
bias. we train the value network to minimize

cb =   ||r       r||2,

(6)

where    is a small constant (e.g. 0.1) multiplied to
the loss in order to stabilize learning. we conjec-
ture that the stability is due to the slowly evolving
value network which directly affects the learning
of the policy. this effectively prevents the value
network to    t extreme cases (unexpectedly high or
low reward.)

we minimize ca and cb using stochastic gra-
dient descent (sgd) with the gradient computed
by id26 (rumelhart et al., 1988). this
allows the entire model to be trained end-to-end
directly to optimize the retrieval performance.
id178 id173 we observed that the
id203 distribution in eq.(1) became highly
peaked in preliminary experiments. this phe-
nomenon led to the trained model not being able
to explore new terms that could lead to a better-
reformulated query. we address this issue by reg-
ularizing the negative id178 of the id203
distribution. we add the following id173
term to the original cost function in eq. (4):
p (t|q0) log p (t|q0),

ch =      

(cid:88)

(7)

t   q0   d0

where    is a id173 coef   cient.

3 related work
query reformulation techniques are either based
on a global method, which ignores a set of doc-
uments returned by the original query, or a local

method, which adjusts a query relative to the doc-
uments that initially appear to match the query. in
this work, we focus on local methods.

a popular
instantiation of a local method
the relevance model, which incorporates
is
pseudo-relevance feedback into a language model
form (lavrenko and croft, 2001). the proba-
bility of adding a term to an expanded query is
proportional to its id203 of being generated
by the language models obtained from the orig-
inal query and the document the term occurs in.
this framework has the advantage of not requiring
query/relevant documents pairs as training data
since id136 is based on word co-occurrence
statistics.

unlike the relevance model, algorithms can
be trained with supervised learning, as proposed
by cao et al. (2008). a training dataset is auto-
matically created by labeling each candidate term
as relevant or not based on their individual contri-
bution to the retrieval performance. then a binary
classi   er is trained to select expansion terms. in
section 4, we present a neural network-based im-
plementation of this supervised approach.

a generalization of this supervised framework
is to iteratively reformulate the query by selecting
one candidate term at each retrieval step. this can
be viewed as navigating a graph where the nodes
represent queries and associated retrieved results
and edges exist between nodes whose queries are
simple reformulations of each other (diaz, 2016).
however, it can be slow to reformulate a query this
way as the search engine must be queried for each
newly added term.
in our method, on the con-
trary, the search engine is queried with multiple
new terms at once.

an alternative technique based on supervised
learning is to learn a common latent representation
of queries and relevant documents terms by us-
ing a click-through dataset (sordoni et al., 2014).
neighboring document terms of a query in the la-
tent space are selected to form an expanded query.
instead of using a click-through dataset, which is
often proprietary, it is possible to use an alterna-
tive dataset consisting of anchor text/title pairs. in
contrast, our approach does not require a dataset of
paired queries as it learns term selection strategies
via id23.

perhaps the closest work to ours is that by
narasimhan et al. (2016), in which a reinforce-
ment learning based approach is used to reformu-

late queries iteratively. a key difference is that
in their work the reformulation component uses
domain-speci   c template queries. our method, on
the other hand, assumes open-domain queries.

4 experiments

in this section we describe our experimental setup,
including baselines against which we compare the
proposed method, metrics, reward for rl-based
models, datasets and implementation details.

4.1 baseline methods
raw: the original query is given to a search
engine without any modi   cation. we evaluate
two search engines in their default con   gura-
tion: lucene3 (raw-lucene) and google search4
(raw-google).

pseudo relevance feedback (prf-tfidf):
a query is expanded with terms from the docu-
ments retrieved by a search engine using the orig-
inal query. in this work, the top-n tf-idf terms
from each of the top-k retrieved documents are
added to the original query, where n and k are
selected by a grid search on the validation data.

prf-relevance model (prf-rm): this is a
popular relevance model for id183
by lavrenko and croft (2001). the id203 of
using a term t in an expanded query is given by:

p (t|q0) = (1       )p (cid:48)(t|q0)

+   

p (d)p (t|d)p (q0|d),

(8)

(cid:88)

d   d0

where p (d) is the id203 of retrieving the
document d, assumed uniform over the set, p (t|d)
and p (q0|d) are the probabilities assigned by the
language model obtained from d to t and q0, re-
spectively. p (cid:48)(t|q0) = tf(t   q)
, where tf(t, d) is the
|q|
term frequency of t in d. we set the interpolation
parameter    to 0.5, following zhai and lafferty
(2001).

we use

a dirichlet

smoothed language
model (zhai and lafferty, 2001) to compute a
language model from a document d     d0:

p (t|d) =

tf(t, d) + up (t|c)

|d| + u

,

(9)

3https://lucene.apache.org/
4https://cse.google.com/cse/

where u is a scalar constant (u = 1500 in our ex-
periments), and p (t|c) is the id203 of t oc-
curring in the entire corpus c.
we use the n terms with the highest p (t|q0) in
an expanded query, where n is a hyper-parameter.
embeddings similarity:
inspired by the meth-
ods proposed by roy et al. (2016) and kuzi et al.
(2016), the top-n terms are selected based on
the cosine similarity of their embeddings against
the original query embedding. candidate terms
come from documents retrieved using the orig-
inal query (prf-emb), or from a    xed vocab-
ulary (vocab-emb). we use pretrained embed-
dings from mikolov et al. (2013), and it contains
374,000 words.

4.2 proposed methods
supervised learning (sl): here we detail a
deep learning-based variant of the method pro-
posed by cao et al. (2008). it assumes that query
terms contribute independently to the retrieval per-
formance. we thus train a binary classi   er to se-
lect a term if the retrieval performance increases
beyond a preset threshold when that term is added
to the original query. more speci   cally, we mark a
term as relevant if (r(cid:48)     r)/r > 0.005, where r
and r(cid:48) are the retrieval performances of the orig-
inal query and the query expanded with the term,
respectively.

we experiment with two variants of

this
method: one in which we use a convolutional net-
work for both original query and candidate terms
(sl-id98), and the other in which we replace the
convolutional network with a single hidden layer
feed-forward neural network (sl-ff). in this vari-
ant, we average the output vectors of the neural
network to obtain a    xed size representation of q0.
id23 (rl): we use multi-
ple variants of the proposed rl method. rl-id98
and rl-id56 are the models described in sec-
tion 2.1, in which the former uses id98s to encode
query and term features and the latter uses id56s
(more speci   cally, bidirectional lstms). rl-ff
is the model in which term and query vectors are
encoded by single hidden layer feed-forward neu-
ral networks. in the rl-id56-seq model, we add
the sequential generator described in section 2.2
to the rl-id56 variant.

4.3 datasets
we summarize in table 1 the datasets.

trec - complex answer retrieval (trec-
car) this is a publicly available dataset auto-
matically created from wikipedia whose goal is
to encourage the development of methods that re-
spond to more complex queries with longer an-
swers (dietz and ben, 2017). a query is the con-
catenation of an article title and one of its section
titles. the ground-truth documents are the para-
graphs within that section. for example, a query
is    sea turtle, diet    and the ground truth docu-
ments are the paragraphs in the section    diet    of
the    sea turtle    article. the corpus consists of all
the english wikipedia paragraphs, except the ab-
stracts. the released dataset has    ve prede   ned
folds, and we use the    rst three as the training set
and the remaining two as validation and test sets,
respectively.

jeopardy this is a publicly available q&a
dataset introduced by nogueira and cho (2016). a
query is a question from the jeopardy! tv show
and the corresponding document is a wikipedia
article whose title is the answer. for example,
a query is    for the last eight years of his life,
galileo was under house arrest for espousing this
mans theory    and the answer is the wikipedia arti-
cle titled    nicolaus copernicus   . the corpus con-
sists of all the articles in the english wikipedia.

microsoft academic (msa) this dataset con-
sists of academic papers crawled from microsoft
academic api.5 the crawler started at the pa-
per silver et al. (2016) and traversed the graph of
references until 500,000 papers were crawled. we
then removed papers that had no reference within
or whose abstract had less than 100 characters. we
ended up with 480,000 papers.

a query is the title of a paper, and the ground-
truth answer consists of the papers cited within.
each document in the corpus consists of its title
and abstract.6

4.4 metrics and reward
three metrics are used to evaluate performance:

recall@k: recall of the top-k retrieved docu-
ments:

r@k =

,

(10)

|dk     d   |

|d   |

5https://www.microsoft.com/cognitive-services/en-

us/academic-knowledge-api

6this was done to avoid a large computational overhead

for indexing full papers.

corpus

dataset
trec-car wikipedia paragraphs
jeopardy
msa

wikipedia articles
academic papers

docs
3.5m 585k
5.9m 118k
480k
270k

queries
train valid
195k
10k
20k

test
195k
10k
20k

relevant docs/query words/doc
avg.
std.
68
3.6
990
1.0
17.9
158

avg.
84
462
165

std.
5.7
0.0
21.5

table 1: summary of the datasets.

method
raw-lucene
raw-google
prf-tfidf
prf-rm
prf-emb
vocab-emb
sl-ff
sl-id98
sl-oracle
rl-ff
rl-id98
rl-id56
rl-id56-seq
rl-oracle

r@40
43.6

-

44.3
45.1
44.5
44.2
44.1
45.3
50.8
44.1
47.3
47.9
47.4
55.9

trec-car
p@10 map@40
7.24

19.6

-

-

7.31
7.35
7.32
7.30
7.29
7.35
8.25
7.29
7.45
7.52
7.48
9.06

19.9
19.5
19.0
19.1
19.7
19.8
21.0
20.0
20.3
20.6
20.3
23.0

r@40
23.4
30.1
29.9
30.5
30.1
29.4
30.8
31.1
38.8
31.0
33.4
33.7
33.4
42.4

jeopardy
p@10 map@40
1.47
1.92
1.91
1.96
1.92
1.87
1.95
1.98
2.50
1.98
2.14
2.12
2.13
2.74

7.40
7.71
7.65
7.64
7.74
7.80
7.70
7.79
9.92
7.81
8.02
8.07
8.01
10.3

r@40
12.9

-

13.2
12.3
12.2
12.0
13.2
14.0
17.3
13.9
14.9
15.1
14.8
24.6

msa

p@10 map@40
7.24

3.36

-

-

7.27
7.22
7.22
7.21
7.28
7.42
10.12
7.33
7.63
7.68
7.63
12.83

3.50
3.38
3.20
3.21
3.88
3.99
4.89
3.81
4.30
4.35
4.27
6.33

table 2: results on test sets. we use r@40 as a reward to the rl-based models.

where dk are the top-k retrieved documents and
d    are the relevant documents. since one of the
goals of query reformulation is to increase the pro-
portion of relevant documents returned, recall is
our main metric.

precision@k: precision of the top-k retrieved
documents:

p@k =

|dk     d   |

|dk|

(11)

precision captures the proportion of relevant doc-
uments among the returned ones. despite not be-
ing the main goal of a reformulation method, im-
provements in precision are also expected with a
good query reformulation method. therefore, we
include this metric.

mean average precision: the average preci-
sion of the top-k retrieved documents is de   ned
as:

(cid:80)k
k=1 p@k    rel(k)

ap@k =

,

(12)

|d   |

where

rel(k) =

(cid:40)

1,
0,

if the k-th document is relevant;
otherwise.

(13)

the mean average precision of a set of queries q
is then:

map@k =

1
|q|

ap@kq,

(14)

(cid:88)

q   q

where ap@kq is the average precision at k for a
query q. this metric values the position of a rele-
vant document in a returned list and is, therefore,
complementary to precision and recall.
reward we use r@k as a reward when train-
ing the proposed rl-based models as this metric
has shown to be effective in improving the other
metrics as well.
sl-oracle
in addition to the baseline methods
and proposed id23 approach,
we report two oracle performance bounds. the
   rst oracle is a supervised learning oracle (sl-
oracle).
it is a classi   er that perfectly selects
terms that will increase performance according to
the procedure described in section 4.2. this mea-
sure serves as an upper-bound for the supervised
methods. notice that this heuristic assumes that
each term contributes independently from all the
other terms to the retrieval performance. there
may be, however, other ways to explore the de-
pendency of terms that would lead to a higher per-
formance.

trec-car jeopardy msa
11%
31%

13%
29%

5%
27%

sl-oracle
rl-oracle

table 3: percentage of relevant terms over all the
candidate terms according to sl- and rl-oracle.

rl-oracle second, we introduce a reinforce-
ment learning oracle (rl-oracle) which estimates
a conservative upper-bound performance for the
rl models. unlike the sl-oracle, it does not
assume that each term contributes independently
to the retrieval performance. it works as follows:
   rst, the validation or test set is divided into n
small subsets {ai}n
i=1 (each with 100 examples,
for instance). an rl model is trained on each sub-
set ai until it over   ts, that is, until the reward r   
i
stops increasing or an early stop mechanism ends
training.7 finally, we compute the oracle perfor-
mance r    as the average reward over all the sub-
sets: r    = 1

(cid:80)n
i=1 r   
i .

this upper bound by the rl-oracle is, however,
conservative since there might exist better refor-
mulation strategies that the rl model was not able
to discover.

n

implementation details

4.5
search engine we use lucene and bm25 as the
search engine and the ranking function, respec-
tively, for all prf, sl and rl methods. for raw-
google, we restrict the search to the wikipedia.org
domain when evaluating its performance on the
jeopardy dataset. we could not apply the same re-
striction to the two other datasets as google does
not index wikipedia paragraphs, and as it is not
trivial to match papers from ms academic to the
ones returned by google search.
candidate terms we use wikipedia articles as
a source for candidate terms since it is a well cu-
rated, clean corpus, with diverse topics.

at

training and test

times of sl methods,
and at test time of rl methods, the candidate
terms are from the    rst m words of the top-k
wikipedia articles retrieved. we select m and
k using grid search on the validation set over
{50, 100, 200, 300} and {1, 3, 5, 7}, respectively.
the best values are m = 300 and k = 7. these
correspond to the maximum number of terms we
could    t in a single gpu.

7the subset should be small enough, or the model should

be large enough so it can over   t.

figure 3: our rl-based model continues to im-
prove recall as more candidate terms are added,
whereas a classical prf method saturates.

at training time of an rl model, we use only
one document uniformly sampled from the top-k
retrieved ones as a source for candidate terms, as
this leads to a faster learning.

for the prf methods,

the top-m terms ac-
cording to a relevance metric (i.e., tf-idf for
prf-tfidf, cosine similarity for prf-emb, and
id155 for prf-rm) from each
of the top-k retrieved documents are added to
the original query. we select m and k using
grid search over {10, 50, 100, 200, 300, 500} and
{1, 3, 5, 9, 11}, respectively. the best values are
m = 300 and k = 9.

multiple reformulation rounds although our
framework supports multiple rounds of search and
reformulation, we did not    nd any signi   cant im-
provement in reformulating a query more than
once. therefore, the numbers reported in the re-
sults section were all obtained from models run-
ning two rounds of search and reformulation.

neural network setup for sl-id98 and rl-
id98 variants, we use a 2-layer convolutional net-
work for the original query. each layer has a win-
dow size of 3 and 256    lters. we use a 2-layer con-
volutional network for candidate terms with win-
dow sizes of 9 and 3, respectively, and 256    lters
in each layer. we set the dimension d of the weight
matrices w, s, u, and v to 256. for the opti-
mizer, we use adam (kingma and ba, 2014) with
   = 10   4,   1 = 0.9,   2 = 0.999, and   = 10   8.
we set the id178 id173 coef   cient    to
10   3.

for rl-id56 and rl-id56-seq, we use a 2-
layer bidirectional lstm with 256 hidden units
in each layer. we clip the gradients to unit norm.
for rl-id56-seq, we set the maximum possible

number of generated terms to 50 and we use beam
search of size four at test time.

we    x the dictionary of pre-trained word em-
beddings during training, except the vector for out-
of-vocabulary words. we found that this led to
faster convergence and observed no difference in
the overall performance when compared to learn-
ing embeddings during training.

5 results and discussion

table 2 shows the main result. as expected, re-
formulation based methods work better than us-
ing the original query alone. supervised methods
(sl-ff and sl-id98) have in general a better per-
formance than unsupervised ones (prf-tfidf,
prf-rm, prf-emb, and emb-vocab), but per-
form worse than rl-based models (rl-ff, rl-
id98, rl-id56, and rl-id56-seq).

rl-id56-seq performs slightly worse than
rl-id56 but produces queries that are three times
shorter, on average (15 vs 47 words). thus, rl-
id56-seq is faster in retrieving documents and
therefore might be a better candidate for a produc-
tion implementation.

the performance gap between the oracle and
best performing method (table 2, rl-oracle vs.
rl-id56) suggests that there is a large room for
improvement. the cause for this gap is unknown
but we suspect, for instance, an inherent dif   culty
in learning a good selection strategy and the par-
tial observability from using a black box search
engine.

5.1 relevant terms per document
the proportion of relevant terms selected by the
sl- and rl-oracles over the total number of can-
didate terms (table 3) indicates that only a small
subset of terms are useful for the reformulation.
thus, we may conclude that the proposed method
was able to learn an ef   cient term selection strat-
egy in an environment where relevant terms are
infrequent.

5.2 scalability: number of terms vs recall
fig. 3 shows the improvement in recall as more
candidate terms are provided to a reformulation
method.
the rl-based model bene   ts from
more candidate terms, whereas the classical prf
method quickly saturates. in our experiments, the
best performing rl-based model uses the maxi-
mum number of candidate terms that we could    t

query
(original) the cross
id178 method for
fast policy search

(reformulated) cross
id178 fast policy
reinforcement
learning policies
global search
optimization biased

(original) daikon
cultivation

(reformulated) daikon
cultivation root seed
grow fast-growing
chinese leaves

top-3 retrieved documents
-the cross id178 method
for network reliability estim.
-robot weightlifting by
direct policy search
-off-policy policy search
-near optimal reinforcement
learning in polynom. time
-the cross id178 method
for network reliability estim.
-robot weightlifting by
direct policy search

   ...many types of pickles are
made with daikon, includ...   
   certain varieties of daikon
can be grown as a winter...   
   in chinese cuisine, turnip
cake and chai tow kway...   
   ...many types of pickles are
   made with daikon, includ...   
   certain varieties of daikon
can be grown as a winter...   
   the chinese and indian
varieties tolerate higher....   

table 4: top-3 retrieved documents using the orig-
inal query and a query reformulated by our rl-
id98 model. in the    rst example, we only show
the titles of the retrieved msa papers.
in the
second example, we only show some words of
the retrieved trec-car paragraphs. bold cor-
responds to ground-truth documents.

on a single gpu. we, therefore, expect further im-
provements with more computational resources.

5.3 qualitative analysis
we show two examples of queries and the proba-
bilities of each candidate term of being selected by
the rl-id98 model in fig. 4.

notice that terms that are more related to the
query have higher probabilities, although common
words such as    the    are also selected. this is a
consequence of our choice of a reward that does

trained on
trec-car

jeopardy
msa

selected terms
serves american national winsted
accreditation
tunxis quinebaug winsted nccc
hospital library arts center cancer center
summer programs

table 5: given the query    northwestern con-
necticut community college   , models trained on
different tasks choose different terms.

figure 4: probabilities assigned by the rl-id98 to candidate terms of two sample queries:    learning
intersections of halfspaces with a margin    (top) and    sea turtle diet    (bottom). we show the original
query terms and the top-10 and bottom-10 document terms with respect to their probabilities.

not penalize the selection of neutral terms.

in table 4 we show an original and reformu-
lated query examples extracted from the ms aca-
demic and trec-car datasets, and their top-3
retrieved documents. notice that the reformulated
query retrieves more relevant documents than the
original one. as we conjectured earlier, we see
that a search engine tends to return a document
simply with the largest overlap in the text, neces-
sitating the reformulation of a query to retrieve se-
mantically relevant documents.

same query, different tasks we compare in ta-
ble 5 the reformulation of a sample query made by
models trained on different datasets. the model
trained on trec-car selects terms that are sim-
ilar to the ones in the original query, such as
   serves    and    accreditation   . these selections are
expected for this task since similar terms can be
effective in retrieving similar paragraphs. on the
other hand, the model trained on jeopardy prefers
to select proper nouns, such as    tunxis   , as these
have a higher chance of being an answer to the
question. the model trained on msa selects terms
that cover different aspects of the entity being
queried, such as    arts center    and    library   , since
retrieving a diverse set of documents is necessary
for the task the of citation recommendation.

5.4 training and id136 times
our best model, rl-id56, takes 8-10 days to train
on a single k80 gpu. at id136 time, it takes

approximately one second to reformulate a batch
of 64 queries. approximately 40% of this time is
to retrieve documents from the search engine.

6 conclusion

we introduced a id23 frame-
work for task-oriented automatic query reformu-
lation. an appealing aspect of this framework is
that an agent can be trained to use a search en-
gine for a speci   c task. the empirical evaluation
has con   rmed that the proposed approach outper-
forms strong baselines in the three separate tasks.
the analysis based on two oracle approaches has
revealed that there is a meaningful room for fur-
ther development. in the future, more research is
necessary in the directions of (1) iterative refor-
mulation under the proposed framework, (2) using
information from modalities other than text, and
(3) better id23 algorithms for a
partially-observable environment.

acknowledgements

rn is funded by coordenao de aperfeioamento
de pessoal de nvel superior (capes). kc thanks
support by facebook, google and nvidia. this
work was partly funded by the defense advanced
research projects agency (darpa) d3m pro-
gram. any opinions,    ndings, and conclusions
or recommendations expressed in this material are
those of the authors and do not necessarily re   ect
the views of darpa.

rodrigo nogueira and kyunghyun cho. 2016. end-
in advances
to-end goal-driven web navigation.
in neural information processing systems, pages
1903   1911.

dwaipayan roy, debjyoti paul, mandar mitra, and
using id27s
arxiv preprint

utpal garain. 2016.
for automatic id183.
arxiv:1606.07608.

david e rumelhart, geoffrey e hinton, and ronald j
williams. 1988. learning representations by back-
propagating errors. cognitive modeling, 5(3):1.

david silver, aja huang, chris j maddison, arthur
guez, laurent sifre, george van den driessche, ju-
lian schrittwieser, ioannis antonoglou, veda pan-
neershelvam, marc lanctot, et al. 2016. mastering
the game of go with deep neural networks and tree
search. nature, 529(7587):484   489.

alessandro sordoni, yoshua bengio, and jian-yun nie.
2014. learning concept embeddings for query ex-
pansion by quantum id178 minimization. in aaai,
pages 1586   1592.

ronald j williams. 1992. simple statistical gradient-
following algorithms for connectionist reinforce-
ment learning. machine learning, 8(3-4):229   256.

jinxi xu and w bruce croft. 1996. id183
using local and global document analysis. in pro-
ceedings of the 19th annual international acm si-
gir conference on research and development in in-
formation retrieval, pages 4   11. acm.

chengxiang zhai and john lafferty. 2001. a study
of smoothing methods for language models applied
in proceedings of
to ad hoc information retrieval.
the 24th annual international acm sigir confer-
ence on research and development in information
retrieval, pages 334   342. acm.

references
christian buck, jannis bulian, massimiliano cia-
ramita, andrea gesmundo, neil houlsby, wojciech
gajewski, and wei wang. 2017. ask the right ques-
tions: active question reformulation with reinforce-
ment learning. arxiv preprint arxiv:1705.07830.

guihong cao,

jian-yun nie,

jianfeng gao, and
stephen robertson. 2008. selecting good expansion
in proceed-
terms for pseudo-relevance feedback.
ings of the 31st annual international acm sigir
conference on research and development in infor-
mation retrieval, pages 243   250. acm.

fernando diaz. 2016. pseudo-query reformulation.
in european conference on information retrieval,
pages 521   532. springer.

laura dietz and gamari ben. 2017. trec car: a
data set for complex answer retrieval. http://trec-
car.cs.unh.edu.

sepp hochreiter and j  urgen schmidhuber. 1997.
neural computation,

long short-term memory.
9(8):1735   1780.

jeff huang and efthimis n efthimiadis. 2009. analyz-
ing and evaluating query reformulation strategies in
web search logs. in proceedings of the 18th acm
conference on information and knowledge manage-
ment, pages 77   86. acm.

yoon kim. 2014.

works for sentence classi   cation.
arxiv:1408.5882.

convolutional neural net-
arxiv preprint

diederik kingma and jimmy ba. 2014. adam: a
method for stochastic optimization. arxiv preprint
arxiv:1412.6980.

saar kuzi, anna shtok, and oren kurland. 2016.
id183 using id27s. in pro-
ceedings of the 25th acm international on confer-
ence on information and knowledge management,
pages 1929   1932. acm.

victor lavrenko and w bruce croft. 2001. rele-
in proceedings of
vance based language models.
the 24th annual international acm sigir confer-
ence on research and development in information
retrieval, pages 120   127. acm.

tomas mikolov, kai chen, greg corrado, and jef-
ef   cient estimation of word
arxiv preprint

frey dean. 2013.
representations in vector space.
arxiv:1301.3781.

george a miller. 1995. id138: a lexical database for
english. communications of the acm, 38(11):39   
41.

karthik narasimhan, adam yala, and regina barzilay.
2016. improving information extraction by acquir-
ing external evidence with id23.
arxiv preprint arxiv:1603.07954.

