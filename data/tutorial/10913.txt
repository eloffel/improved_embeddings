on the expressive power of deep neural networks

maithra raghu 1 2 ben poole 3 jon kleinberg 1 surya ganguli 3 jascha sohl dickstein 2

7
1
0
2

 

n
u
j
 

8
1

 
 
]
l
m

.
t
a
t
s
[
 
 

6
v
6
3
3
5
0

.

6
0
6
1
:
v
i
x
r
a

abstract

we propose a new approach to the problem of
neural network expressivity, which seeks to char-
acterize how structural properties of a neural net-
work family affect the functions it is able to com-
pute. our approach is based on an interrelated
set of measures of expressivity, uni   ed by the
novel notion of trajectory length, which mea-
sures how the output of a network changes as the
input sweeps along a one-dimensional path. our
   ndings can be summarized as follows:
(1) the complexity of the computed function
grows exponentially with depth. we de-
sign measures of expressivity that capture
the non-linearity of the computed func-
tion. due to how the network transforms
its input, these measures grow exponentially
with depth.

(2) all weights are not equal (initial layers mat-
ter more). we    nd that trained networks
are far more sensitive to their lower (ini-
tial) layer weights: they are much less ro-
bust to noise in these layer weights, and also
perform better when these weights are opti-
mized well.

(3) trajectory id173 works like batch
id172. we    nd that batch norm
stabilizes the learnt
representation, and
based on this propose a new id173
scheme, trajectory id173.

1. introduction
deep neural networks have proved astoundingly effective
at a wide range of empirical tasks, from image classi   ca-
tion (krizhevsky et al., 2012) to playing go (silver et al.,
2016), and even modeling human learning (piech et al.,
2015).

1cornell university 2google brain 3stanford university. cor-

respondence to: maithra raghu <maithrar@gmail.com>.

proceedings of the 34 th international conference on machine
learning, sydney, australia, pmlr 70, 2017. copyright 2017
by the author(s).

despite these successes, understanding of how and why
neural network architectures achieve their empirical suc-
cesses is still lacking. this includes even the fundamen-
tal question of neural network expressivity, how the archi-
tectural properties of a neural network (depth, width, layer
type) affect the resulting functions it can compute, and its
ensuing performance.
this is a foundational question, and there is a rich history
of prior work addressing expressivity in neural networks.
however, it has been challenging to derive conclusions that
provide both theoretical generality with respect to choices
of architecture as well as meaningful insights into practical
performance.
indeed, the very    rst results on this question take a highly
theoretical approach, from using functional analysis to
show universal approximation results (hornik et al., 1989;
cybenko, 1989), to analysing expressivity via comparisons
to boolean circuits (maass et al., 1994) and studying net-
work vc dimension (bartlett et al., 1998). while these
results provided theoretically general conclusions, the shal-
low networks they studied are very different from the deep
models that have proven so successful in recent years.
in response, several recent papers have focused on under-
standing the bene   ts of depth for neural networks (pas-
canu et al., 2013; montufar et al., 2014; eldan and shamir,
2015; telgarsky, 2015; martens et al., 2013; bianchini and
scarselli, 2014). these results are compelling and take
modern architectural changes into account, but they only
show that a speci   c choice of weights for a deeper network
results in inapproximability by a shallow (typically one or
two hidden layers) network.
in particular, the goal of this new line of work has been
to establish lower bounds     showing separations between
shallow and deep networks     and as such they are based
on hand-coded constructions of speci   c network weights.
even if the weight values used in these constructions are
robust to small perturbations (as in (pascanu et al., 2013;
montufar et al., 2014)), the functions that arise from these
constructions tend toward extremal properties by design,
and there is no evidence that a network trained on data ever
resembles such a function.
this has meant that a set of fundamental questions about

on the expressive power of deep neural networks

neural network expressivity has remained largely unan-
swered. first, we lack a good understanding of the    typ-
ical    case rather than the worst case in these bounds for
deep networks, and consequently have no way to evalu-
ate whether the hand-coded extremal constructions provide
a re   ection of the complexity encountered in more stan-
dard settings. second, we lack an understanding of upper
bounds to match the lower bounds produced by this prior
work; do the constructions used to date place us near the
limit of the expressive power of neural networks, or are
there still large gaps? finally, if we had an understanding
of these two issues, we might begin to draw connections
between network expressivity and observed performance.

our contributions: measures of expressivity and their
applications
in this paper, we address this set of chal-
lenges by de   ning and analyzing an interrelated set of mea-
sures of expressivity for neural networks; our framework
applies to a wide range of standard architectures, indepen-
dent of speci   c weight choices. we begin our analysis at
the start of training, after random initialization, and later
derive insights connecting network expressivity and perfor-
mance.
our    rst measure of expressivity is based on the notion of
an activation pattern: in a network where the units compute
functions based on discrete thresholds, we can ask which
units are above or below their thresholds (i.e. which units
are    active    and which are not). for the range of standard
architectures that we consider, the network is essentially
computing a linear function once we    x the activation pat-
tern; thus, counting the number of possible activation pat-
terns provides a concrete way of measuring the complexity
beyond linearity that the network provides. we give an up-
per bound on the number of possible activation patterns,
over any setting of the weights. this bound is tight as it
matches the hand-constructed lower bounds of earlier work
(pascanu et al., 2013; montufar et al., 2014).
key to our analysis is the notion of a transition, in which
changing an input x to a nearby input x +    changes the
activation pattern. we study the behavior of transitions as
we pass the input along a one-dimensional parametrized
trajectory x(t). our central    nding is that the trajectory
length grows exponentially in the depth of the network.
trajectory length serves as a unifying notion in our mea-
sures of expressivity, and it leads to insights into the be-
havior of trained networks. speci   cally, we    nd that the
exponential growth in trajectory length as a function of
depth implies that small adjustments in parameters lower
in the network induce larger changes than comparable ad-
justments higher in the network. we demonstrate this phe-
nomenon through experiments on mnist and cifar-10,
where the network displays much less robustness to noise

in the lower layers, and better performance when they are
trained well. we also explore the effects of id173
methods on trajectory length as the network trains and pro-
pose a less computationally intensive method of regulariza-
tion, trajectory id173, that offers the same perfor-
mance as batch id172.
the contributions of this paper are thus:

(1) measures of expressivity: we propose easily com-
putable measures of neural network expressivity that
capture the expressive power inherent in different
neural network architectures, independent of speci   c
weight settings.

(2) exponential

trajectories: we    nd an exponen-
tial depth dependence displayed by these measures,
through a unifying analysis in which we study how the
network transforms its input by measuring trajectory
length

(3) all weights are not equal (the lower layers matter
more): we show how these results on trajectory length
suggest that optimizing weights in lower layers of the
network is particularly important.

(4) trajectory id173 based on understanding the
effect of batch norm on trajectory length, we propose
a new method of id173, trajectory regulariza-
tion, that offers the same advantages as batch norm,
and is computationally more ef   cient.

in prior work (poole et al., 2016), we studied the propa-
gation of riemannian curvature through random networks
by developing a mean    eld theory approach. here, we take
an approach grounded in computational geometry, present-
ing measures with a combinatorial    avor and explore the
consequences during and after training.

2. measures of expressivity
given a neural network of a certain architecture a (some
depth, width, layer types), we have an associated function,
fa(x; w ), where x is an input and w represents all the
parameters of the network. our goal is to understand how
the behavior of fa(x; w ) changes as a changes, for values
of w that we might encounter during training, and across
inputs x.
the    rst major dif   culty comes from the high dimension-
ality of the input. precisely quantifying the properties of
fa(x; w ) over the entire input space is intractable. as a
tractable alternative, we study simple one dimensional tra-
jectories through input space. more formally:
de   nition: given two points, x0, x1     rm, we say x(t)
is a trajectory (between x0 and x1) if x(t) is a curve

on the expressive power of deep neural networks

parametrized by a scalar t     [0, 1], with x(0) = x0 and
x(1) = x1.
simple examples of a trajectory would be a line (x(t) =
tx1 + (1     t)x0) or a circular arc (x(t) = cos(  t/2)x0 +
sin(  t/2)x1), but in general x(t) may be more compli-
cated, and potentially not expressible in closed form.
armed with this notion of trajectories, we can begin to de-
   ne measures of expressivity of a network fa(x; w ) over
trajectories x(t).

2.1. neuron transitions and activation patterns

in (montufar et al., 2014) the notion of a    linear region   
is introduced. given a neural network with piecewise lin-
ear activations (such as relu or hard tanh), the function
it computes is also piecewise linear, a consequence of the
fact that composing piecewise linear functions results in a
piecewise linear function. so one way to measure the    ex-
pressive power    of different architectures a is to count the
number of linear pieces (regions), which determines how
nonlinear the function is.
in fact, a change in linear region is caused by a neuron
transition in the output layer. more precisely:
de   nition for    xed w , we say a neuron with piecewise
linear region transitions between inputs x, x +    if its acti-
vation function switches linear region between x and x +   .
so a relu transition would be given by a neuron switching
from off to on (or vice versa) and for hard tanh by switch-
ing between saturation at    1 to its linear middle region to
saturation at 1. for any generic trajectory x(t), we can thus
de   ne t (fa(x(t); w )) to be the number of transitions un-
dergone by output neurons (i.e.
the number of linear re-
gions) as we sweep the input x(t). instead of just concen-
trating on the output neurons however, we can look at this
pattern over the entire network. we call this an activation
patten:
de   nition we can de   ne ap(fa(x; w )) to be the activa-
tion pattern     a string of form {0, 1}num neurons (for relus)
and {   1, 0, 1}num neurons (for hard tanh) of the network en-
coding the linear region of the activation function of every
neuron, for an input x and weights w .
overloading notation slightly, we can also de   ne (similarly
to transitions) a(fa(x(t); w )) as the number of distinct
activation patterns as we sweep x along x(t). as each
distinct activation pattern corresponds to a different linear
function of the input, this combinatorial measure captures
how much more expressive a is over a simple linear map-
ping.
returning to montufar et al, they provide a construction
i.e. a speci   c set of weights w0, that results in an exponen-

tial increase of linear regions with the depth of the archi-
tectures. they also appeal to zaslavsky   s theorem (stan-
ley, 2011) from the theory of hyperplane arrangements to
show that a shallow network, i.e. one hidden layer, with the
same number of parameters as a deep network, has a much
smaller number of linear regions than the number achieved
by their choice of weights w0 for the deep network.
more formally, letting a1 be a fully connected network
with one hidden layer, and al a fully connected network
with the same number of parameters, but l hidden layers,
they show

   wt (fa1 ([0, 1]; w )) < t (fa1([0, 1]; w0)

(*)

we derive a much more general result by considering the
   global    activation patterns over the entire input space, and
prove that for any fully connected network, with any num-
ber of hidden layers, we can upper bound the number of lin-
ear regions it can achieve, over all possible weight settings
w . this upper bound is asymptotically tight, matched by
the construction given in (montufar et al., 2014). our result
can be written formally as:
theorem 1. (tight) upper bound for number of activa-
tion patterns let a(n,k) denote a fully connected network
with n hidden layers of width k, and inputs in rm. then the
number of activation patterns a(fan,k (rm; w ) is upper
bounded by o(kmn) for relu activations, and o((2k)mn)
for hard tanh.

from this we can derive a chain of inequalities. firstly,
from the theorem above we    nd an upper bound of
a(fan,k (rm; w )) over all w , i.e.

    w a(fa(n,k))(rm; w )     u (n, k, m).

next, suppose we have n neurons in total. then we want to
compare (for wlog relus), quantities like u (n(cid:48), n/n(cid:48), m)
for different n(cid:48).
but u (n(cid:48), n/n(cid:48), m) = o((n/n(cid:48))mn(cid:48)

(cid:1)mx (for a > e) is x = a/e, we get, (for

the maxima of(cid:0) a

), and so, noting that

n, k > e), in comparison to (*),

x

u (1, n, m) < u (2,

, m) <       

n
2

       < u (n     1,

n
n     1

, m) < u (n, k, m)

we prove this via an inductive proof on regions in a hy-
perplane arrangement. the proof can be found in the ap-
pendix. as noted in the introduction, this result differs
from earlier lower-bound constructions in that it is an upper
bound that applies to all possible sets of weights. via our
analysis, we also prove

on the expressive power of deep neural networks

figure 1. deep networks with piecewise linear activations subdi-
vide input space into convex polytopes. we take a three hidden
layer relu network, with input x     r2, and four units in each
layer. the left pane shows activations for the    rst layer only. as
the input is in r2, neurons in the    rst hidden layer have an associ-
ated line in r2, depicting their activation boundary. the left pane
thus has four such lines. for the second hidden layer each neuron
again has a line in input space corresponding to on/off, but this
line is different for each region described by the    rst layer activa-
tion pattern. so in the centre pane, which shows activation bound-
ary lines corresponding to second hidden layer neurons in green
(and    rst hidden layer in black), we can see the green lines    bend   
at the boundaries. (the reason for this bending becomes appar-
ent through the proof of theorem 2.) finally, the right pane adds
the on/off boundaries for neurons in the third hidden layer, in pur-
ple. these lines can bend at both black and green boundaries, as
the image shows. this    nal set of convex polytopes corresponds
to all activation patterns for this network (with its current set of
weights) over the unit square, with each polytope representing a
different linear function.
theorem 2. regions in input space given the correspond-
ing function of a neural network fa(rm; w ) with relu
or hard tanh activations, the input space is partitioned into
convex polytopes, with fa(rm; w ) corresponding to a dif-
ferent linear function on each region.

this result is of independent interest for optimization     a
linear function over a convex polytope results in a well be-
haved id168 and an easy optimization problem. un-
derstanding the density of these regions during the training
process would likely shed light on properties of the loss
surface, and improved optimization methods. a picture of
a network   s regions is shown in figure 1.

2.1.1. empirically counting transitions

we empirically tested the growth of the number of acti-
vations and transitions as we varied x along x(t) to under-
stand their behavior. we found that for bounded non linear-
ities, especially tanh and hard-tanh, not only do we observe
exponential growth with depth (as hinted at by the upper
bound) but that the scale of parameter initialization also af-
fects the observations (figure 2). we also experimented
with sweeping the weights w of a layer through a trajec-
tory w (t), and counting the different labellings output by
the network. this    dichotomies    measure is discussed fur-
ther in the appendix, and also exhibits the same growth
properties, figure 14.

figure 2. the number of transitions seen for fully connected net-
works of different widths, depths and initialization scales, with
a circular trajectory between mnist datapoints. the number of
transitions grows exponentially with the depth of the architecture,
as seen in (left). the same rate of growth is not seen with increas-
ing architecture width, plotted in (right). there is a surprising
dependence on the scale of initialization, explained in 2.2.

figure 3. picture showing a trajectory increasing with the depth
of a network. we start off with a circular trajectory (left most
pane), and feed it through a fully connected tanh network with
width 100. pane second from left shows the image of the circular
trajectory (projected down to two dimensions) after being trans-
formed by the    rst hidden layer. subsequent panes show pro-
jections of the latent image of the circular trajectory after being
transformed by more hidden layers. the    nal pane shows the the
trajectory after being transformed by all the hidden layers.
2.2. trajectory length

in fact, there turns out to be a reason for the exponential
growth with depth, and the sensitivity to initialization scale.
returning to our de   nition of trajectory, we can de   ne an
immediately related quantity, trajectory length
de   nition: given a trajectory, x(t), we de   ne its length,
l(x(t)), to be the standard arc length:

(cid:90)

t

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) dx(t)

dt

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) dt

l(x(t)) =

intuitively, the arc length breaks x(t) up into in   nitesimal
intervals and sums together the euclidean length of these
intervals.
if we let a(n,k) denote, as before, fully connected networks
with n hidden layers each of width k, and initializing with
weights     n (0,   2
w/k) (accounting for input scaling as
typical), and biases     n (0,   2
theorem 3. bound on growth of trajectory length let
fa(x(cid:48), w ) be a relu or hard tanh random neural network
and x(t) a one dimensional trajectory with x(t +   ) having
a non trival perpendicular component to x(t) for all t,   

b ), we    nd that:

-101x0-101x1layer 0-101x0-101layer 1-101x0-101layer 202468101214network depth10-1100101102transitions numbernumber of transitions with increasing depthw50 scl10w100 scl8w500 scl5w700 scl5w700 scl10w1000 scl10w1000 scl1602004006008001000layer width10-1100101number of transitionsnumber of transitions with increasing widthlay2 scl5lay2 scl10lay4 scl5lay4 scl8lay6 scl5lay6 scl8lay8 scl8lay10 scl8lay12 scl8on the expressive power of deep neural networks

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)  z(d+1)   

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12), which results in the condition on x(t) in the

(t)

statement.
in figures 4, 12, we see the growth of an input trajectory
for relu networks on cifar-10 and mnist. the cifar-
10 network is convolutional but we observe that these lay-
ers also result in similar rates of trajectory length increases
to the fully connected layers. we also see, as would be
expected, that pooling layers act to reduce the trajectory
length. we discuss upper bounds in the appendix.

figure 4. we look at trajectory growth with different initializa-
tion scales as a trajectory is propagated through a convolutional
architecture for cifar-10, with relu activations. the analy-
sis of theorem 3 was for fully connected networks, but we see
that trajectory growth holds (albeit with slightly higher scales) for
convolutional architectures also. note that the decrease in trajec-
tory length, seen in layers 3 and 7 is expected, as those layers are
pooling layers.

(i.e, not a line). then de   ning z(d)(x(t)) = z(d)(t) to be
the image of the trajectory in layer d of the network, we
have

(a)

e(cid:104)

(cid:105)     o

l(z(d)(t))

for relus

(cid:33)d

(cid:32)

   
k   
  w
k + 1

l(x(t))

(b)

e(cid:104)

l(z(d)(t))

(cid:105)     o

      

(cid:113)

   

k

  w

b + k(cid:112)  2

w +   2
  2

w +   2
b

      d

l(x(t))

figure 5. the number of transitions is linear in trajectory length.
here we compare the empirical number of transitions to the length
of the trajectory, for different depths of a hard-tanh network. we
repeat this comparison for a variety of network architectures, with
different network width k and weight variance   2
w.

for the hard tanh case (and more generally any bounded
non-linearity), we can formally prove the relation of trajec-
tory length and transitions under an assumption: assume
that while we sweep x(t) all neurons are saturated un-
less transitioning saturation endpoints, which happens very
rapidly. (this is the case for e.g. large initialization scales).
then we have:
theorem 4. transitions proportional to trajectory length
let fan,k be a hard tanh network with n hidden layers each
of width k. and let

          
k(cid:113)

1 +   2
b
  2
w

      n

for hard tanh

g(k,   w,   b, n) = o

that is, l(x(t) grows exponentially with the depth of the
network, but the width only appears as a base (of the expo-
nent). this bound is in fact tight in the limits of large   w
and k.
a schematic image depicting this can be seen in figure 3
and the proof can be found in the appendix. a rough out-
line is as follows: we look at the expected growth of the
difference between a point z(d)(t) on the curve and a small
perturbation z(d)(t+dt), from layer d to layer d+1. denot-

ing this quantity(cid:12)(cid:12)(cid:12)(cid:12)  z(d)(t)(cid:12)(cid:12)(cid:12)(cid:12), we derive a recurrence relat-
ing(cid:12)(cid:12)(cid:12)(cid:12)  z(d+1)(t)(cid:12)(cid:12)(cid:12)(cid:12) and(cid:12)(cid:12)(cid:12)(cid:12)  z(d)(t)(cid:12)(cid:12)(cid:12)(cid:12) which can be composed

to give the desired growth rate.
the analysis is complicated by the statistical dependence
on the image of the input z(d+1)(t). so we instead form
a recursion by looking at the component of the difference
perpendicular to the image of the input in that layer, i.e.

then t (fan,k (x(t); w ) = o(g(k,   w,   b, n)) for w ini-
tialized with weight and bias scales   w,   b.

note that the expression for g(k,   w,   b, n) is exactly the
expression given by theorem 3 when   w is very large and
dominates   b. we can also verify this experimentally in
settings where the simpilfying assumption does not hold,
as in figure 5.

3. insights from network expressivity
here we explore the insights gained from applying
our measurements of expressivity, particularly trajectory
length, to understand network performance. we examine
the connection of expressivity and stability, and inspired
by this, propose a new method of id173, trajectory

123456789network depth101102103104105trajectory lengthtrajectory length growth with increasing depthscl5scl8scl12scl16scl20scl32on the expressive power of deep neural networks

figure 6. we then pick a single layer of a conv net trained to high
accuracy on cifar10, and add noise to the layer weights of in-
creasing magnitudes, testing the network accuracy as we do so.
we    nd that the initial (lower) layers of the network are least ro-
bust to noise     as the    gure shows, adding noise of 0.25 magni-
tude to the    rst layer results in a 0.7 drop in accuracy, while the
same amount of noise added to the    fth layer barely results in a
0.02 drop in accuracy. this pattern is seen for many different ini-
tialization scales, even for a (typical) scaling of   2
w = 2, used in
the experiment.

id173 that offers the same advantages as the more
computationally intensive batch id172.

3.1. expressivity and network stability

the analysis of network expressivity offers interesting
takeaways related to the parameter and functional stabil-
ity of a network. from the proof of theorem 3, we saw
that a perturbation to the input would grow exponentially
in the depth of the network. it is easy to see that this anal-
ysis is not limited to the input layer, but can be applied to
any layer. in this form, it would say

a perturbation at a layer grows exponentially in the

remaining depth after that layer.

this means that perturbations to weights in lower layers
should be more costly than perturbations in the upper lay-
ers, due to exponentially increasing magnitude of noise,
and result in a much larger drop of accuracy. figure 6, in
which we train a conv network on cifar-10 and add noise
of varying magnitudes to exactly one layer, shows exactly
this.
we also    nd that the converse (in some sense) holds: after
initializing a network, we trained a single layer at different
depths in the network and found monotonically increasing
performance as layers lower in the network were trained.
this is shown in figure 7 and figure 17 in the appendix.

3.2. trajectory length and id173: the effect

of batch id172

expressivity measures, especially trajectory length, can
also be used to better understand the effect of regulariza-

figure 7. demonstration of expressive power of remaining depth
on mnist. here we plot train and test accuracy achieved by train-
ing exactly one layer of a fully connected neural net on mnist.
the different lines are generated by varying the hidden layer cho-
sen to train. all other layers are kept frozen after random initial-
ization. we see that training lower hidden layers leads to better
performance. the networks had width k = 100, weight variance
w = 1, and hard-tanh nonlinearities. note that we only train
  2
from the second hidden layer (weights w (1)) onwards, so that
the number of parameters trained remains    xed.

tion. one id173 technique that has been extremely
successful for training neural networks is batch normal-
ization (ioffe and szegedy, 2015).

figure 8. training increases trajectory length even for typical
(  2
w = 2) initialization values of   w. here we propagate a cir-
cular trajectory joining two cifar10 datapoints through a conv
net without batch norm, and look at how trajectory length changes
through training. we see that training causes trajectory length
to increase exponentially with depth (exceptions only being the
pooling layers and the    nal fc layer, which halves the number of
neurons.) note that at step 0, the network is not in the exponen-
tial growth regime. we observe (discussed in figure 9) that even
networks that aren   t initialized in the exponential growth regime
can be pushed there through training.

by taking measures of trajectories during training we    nd
that without batch norm, trajectory length tends to increase
during training, as shown in figures 8 and figure 18 in
in these experiments, two networks were
the appendix.
initialized with   2
w = 2 and trained to high test accuracy on
cifar10 and mnist. we see that in both cases, trajectory
length increases as training progresses.
a surprising observation is   2
w = 2 is not in the exponential
growth increase regime at initialization for the cifar10

0.00.51.01.52.0noise magnitude0.10.20.30.40.50.60.70.80.91.0accuracycifar 10 accuracy against noise in diff layerslay01lay02lay03lay04lay05lay06lay070100200300400500epoch number0.30.40.50.60.70.80.91.0accuracytrain accuracy against epoch0100200300400500epoch numbertest accuracy against epochlay 2lay 3lay 4lay 5lay 6lay 7lay 8lay 9inc1c2p1c3c4c5p2fc1fc2trajectory length101102103104105layer depthcifar10 trajectory growth  without batch norm01189105124074472389104035137262142009156250on the expressive power of deep neural networks

architecture (figure 8 at step 0.). but note that even with a
smaller weight initialization, weight norms increase during
training, shown in figure 9, pushing typically initialized
networks into the exponential growth regime.

then scales the outgoing activations by   , where    is a pa-
rameter to be learnt. in implementation, we typically scale
the additional loss above with a constant (0.01) to reduce
magnitude in comparison to classi   cation loss. our results,
figure 11 show that both trajectory id173 and batch
norm perform comparably, and considerably better than not
using batch norm. one advantage of using trajectory reg-
ularization is that we don   t require different computations
to be performed for train and test, enabling more ef   cient
implementation.

figure 9. this    gure shows how the weight scaling of a cifar10
network evolves during training. the network was initialized with
w = 2, which increases across all layers during training.
  2

while the initial growth of trajectory length enables greater
functional expressivity, large trajectory growth in the learnt
representation results in an unstable representation, wit-
nessed in figure 6.
in figure 10 we train another conv
net on cifar10, but this time with batch id172.
we see that the batch norm layers reduce trajectory length,
helping stability.

figure 10. growth of circular trajectory between two datapoints
with batch norm layers for a conv net on cifar10. the network
was initialized as typical, with   2
w = 2. note that the batch norm
layers in step 0 are poorly behaved due to division by a close to 0
variance. but after just a few hundred gradient steps and contin-
uing onwards, we see the batch norm layers (dotted lines) reduce
trajectory length, stabilising the representation without sacri   cing
expressivity.

3.3. trajectory id173

motivated by the fact that batch id172 decreases
trajectory length and hence helps stability and generaliza-
tion, we consider directly regularizing on trajectory length:
we replace every batch norm layer used in the conv net
in figure 10 with a trajectory id173 layer. this
layer adds to the loss   (current length/orig length), and

figure 11. we replace each batch norm layer of the cifar10
conv net with a trajectory id173 layer, described in sec-
tion 3.3. during training trajectory length is easily calculated as
a piecewise linear trajectory between adjacent datapoints in the
minibatch. we see that trajectory id173 achieves the same
performance as batch norm, albeit with slightly more train time.
however, as trajectory id173 behaves the same during
train and test time, it is simpler and more ef   cient to implement.

4. discussion
characterizing the expressiveness of neural networks, and
understanding how expressiveness varies with parameters
of the architecture, has been a challenging problem due to
the dif   culty in identifying meaningful notions of expres-
sivity and in linking their analysis to implications for these
networks in practice. in this paper we have presented an
interrelated set of expressivity measures; we have shown
tight exponential bounds on the growth of these measures
in the depth of the networks, and we have offered a uni-
fying view of the analysis through the notion of trajectory
length. our analysis of trajectories provides insights for the
performance of trained networks as well, suggesting that
networks in practice may be more sensitive to small per-
turbations in weights at lower layers. we also used this to
explore the empirical success of batch norm, and developed
a new id173 method     trajectory id173.
this work raises many interesting directions for future
work. at a general level, continuing the theme of    prin-
cipled deep understanding   , it would be interesting to link

020000400006000080000100000120000train step2.02.53.03.54.04.55.05.56.0scaled weight variancecifar-10 scaled weight variances with traininglay 01lay 02lay 03lay 04lay 05lay 06lay 07inc1c2bn1p1c3c4c5bn2p2fc1bn3fc2bn4layer depth102103104105106107trajectory lengthcifar10: trajectory growth with  batch norm03957858156250020000400006000080000100000120000140000160000train step0.700.750.800.850.90test accuracycifar10 accuracy for  trajectory and batch norm reguarlizersbatch normtraj regno batch norm or traj regon the expressive power of deep neural networks

neural networks. in advances in neural information pro-
cessing systems, pages 2924   2932, 2014.

ronen eldan and ohad shamir. the power of depth
arxiv preprint

feedforward neural networks.

for
arxiv:1512.03965, 2015.

matus telgarsky. representation bene   ts of deep feedfor-
ward networks. arxiv preprint arxiv:1509.08101, 2015.
james martens, arkadev chattopadhya, toni pitassi, and
richard zemel. on the representational ef   ciency of re-
stricted id82s. in advances in neural in-
formation processing systems, pages 2877   2885, 2013.
monica bianchini and franco scarselli. on the complex-
ity of neural network classi   ers: a comparison between
shallow and deep architectures. neural networks and
learning systems, ieee transactions on, 25(8):1553   
1565, 2014.

ben poole, subhaneil lahiri, maithra raghu, jascha sohl-
dickstein, and surya ganguli. exponential expressivity
in deep neural networks through transient chaos. in ad-
vances in neural information processing systems, pages
3360   3368, 2016.

richard stanley. hyperplane arrangements. enumerative

combinatorics, 2011.

sergey ioffe and christian szegedy. batch id172:
accelerating deep network training by reducing inter-
in proceedings of the 32nd inter-
nal covariate shift.
national conference on machine learning, icml 2015,
lille, france, 6-11 july 2015, pages 448   456, 2015.

ian j. goodfellow, jonathon shlens, and christian szegedy.
explaining and harnessing adversarial examples. corr,
abs/1412.6572, 2014.

d. kershaw. some extensions of w. gautschi   s inequalities
for the gamma function. mathematics of computation,
41(164):607   611, 1983.

andrea laforgia and pierpaolo natalini. on some inequal-
ities for the gamma function. advances in dynamical
systems and applications, 8(2):261   267, 2013.

norbert sauer. on the density of families of sets. journal of
combinatorial theory, series a, 13(1):145   147, 1972.

measures of expressivity to other properties of neural net-
work performance. there is also a natural connection be-
tween adversarial examples, (goodfellow et al., 2014), and
trajectory length: adversarial perturbations are only a small
distance away in input space, but result in a large change in
classi   cation (the output layer). understanding how trajec-
tories between the original input and an adversarial pertur-
bation grow might provide insights into this phenomenon.
another direction, partially explored in this paper, is regu-
larizing based on trajectory length. a very simple version
of this was presented, but further performance gains might
be achieved through more sophisticated use of this method.

acknowledgements
we thank samy bengio, ian goodfellow, laurent dinh,
and quoc le for extremely helpful discussion.

references
alex krizhevsky, ilya sutskever, and geoffrey e hinton.
id163 classi   cation with deep convolutional neural
networks. in advances in neural information processing
systems, pages 1097   1105, 2012.

david silver, aja huang, chris j maddison, arthur guez,
laurent sifre, george van den driessche, julian schrit-
twieser,
ioannis antonoglou, veda panneershelvam,
marc lanctot, et al. mastering the game of go with deep
neural networks and tree search. nature, 529(7587):
484   489, 2016.

chris piech, jonathan bassen, jonathan huang, surya gan-
guli, mehran sahami, leonidas j guibas, and jascha
sohl-dickstein. deep knowledge tracing. in advances in
neural information processing systems, pages 505   513,
2015.

kurt hornik, maxwell stinchcombe, and halbert white.
multilayer feedforward networks are universal approxi-
mators. neural networks, 2(5):359   366, 1989.

george cybenko. approximation by superpositions of a
sigmoidal function. mathematics of control, signals and
systems, 2(4):303   314, 1989.

wolfgang maass, georg schnitger, and eduardo d son-
tag. a comparison of the computational power of sig-
moid and boolean threshold circuits. springer, 1994.

peter l bartlett, vitaly maiorov, and ron meir. almost lin-
ear vc-dimension bounds for piecewise polynomial net-
works. neural computation, 10(8):2159   2173, 1998.

razvan pascanu, guido montufar, and yoshua bengio. on
the number of response regions of deep feed forward net-
works with piece-wise linear activations. arxiv preprint
arxiv:1312.6098, 2013.

guido f montufar, razvan pascanu, kyunghyun cho, and
yoshua bengio. on the number of linear regions of deep

on the expressive power of deep neural networks

appendix

here we include the full proofs from sections in the paper.

a. proofs and additional results from section 2.1
proof of theorem 2

i

i

i

be the ith row of w (0), b(0)

the bias, we have the hyperplane w (0)

proof. we show inductively that fa(x; w ) partitions the input space into convex polytopes via hyperplanes. consider
the image of the input space under the    rst hidden layer. each neuron v(1)
de   nes hyperplane(s) on the input space:
letting w (0)
i x + bi = 0 for a relu and hyperplanes
i x + bi =   1 for a hard-tanh. considering all such hyperplanes over neurons in the    rst layer, we get a hyperplane
w (0)
arrangement in the input space, each polytope corresponding to a speci   c activation pattern in the    rst hidden layer.
now, assume we have partitioned our input space into convex polytopes with hyperplanes from layers     d     1. consider
and a speci   c polytope ri. then the activation pattern on layers     d     1 is constant on ri, and so the input to v(d)
v(d)
i
j   jxj + b and some constant term, comprising of the bias and the output of
saturated units. setting this expression to zero (for relus) or to   1 (for hard-tanh) again gives a hyperplane equation,
but this time, the equation is only valid in ri (as we get a different linear function of the inputs in a different region.) so
the de   ned hyperplane(s) either partition ri (if they intersect ri) or the output pattern of v(d)
is also constant on ri. the
theorem then follows.

on ri is a linear function of the inputs(cid:80)

i

i

this implies that any one dimensional trajectory x(t), that does not    double back    on itself (i.e. reenter a polytope it has
previously passed through), will not repeat activation patterns. in particular, after seeing a transition (crossing a hyperplane
to a different region in input space) we will never return to the region we left. a simple example of such a trajectory is a
straight line:
corollary 1. transitions and output patterns in an af   ne trajectory for any af   ne one dimensional trajectory x(t) =
x0 + t(x1     x0) input into a neural network fw , we partition r (cid:51) t into intervals every time a neuron transitions. every
interval has a unique network activation pattern on fw .

generalizing from a one dimensional trajectory, we can ask how many regions are achieved over the entire input     i.e. how
many distinct activation patterns are seen? we    rst prove a bound on the number of regions formed by k hyperplanes in
rm (in a purely elementary fashion, unlike the proof presented in (stanley, 2011))
theorem 5. upper bound on regions in a hyperplane arrangement suppose we have k hyperplanes in rm - i.e. k
equations of form   ix =   i. for   i     rm,   i     r. let the number of regions (connected open sets bounded on some sides
by the hyperplanes) be r(k, m). then

r(k, m)     m(cid:88)

(cid:18)k

(cid:19)

i

proof of theorem 5

i=0

proof. let the hyperplane arrangement be denoted h, and let h     h be one speci   c hyperplane. then the number of
regions in h is precisely the number of regions in h     h plus the number of regions in h     h. (this follows from the
fact that h subdivides into two regions exactly all of the regions in h     h, and does not affect any of the other regions.)
in particular, we have the recursive formula

r(k, m) = r(k     1, m) + r(k     1, m     1)

we now induct on k + m to assert the claim. the base cases of r(1, 0) = r(0, 1) = 1 are trivial, and assuming the claim

for     k + m     1 as the induction hypothesis, we have

on the expressive power of deep neural networks

i

+

i=0

(cid:18)k     1
(cid:19)
(cid:19)
(cid:18)k     1
r(k     1, m) + r(k     1, m     1)     m(cid:88)
m   1(cid:88)
(cid:18)k     1
(cid:19)
(cid:18)k     1
(cid:18)k     1
(cid:19)
(cid:19)
d   1(cid:88)
(cid:19)
(cid:18)k
(cid:19)
(cid:18) k
m   1(cid:88)
(cid:18)a + 1

(cid:18) a

(cid:18)a

(cid:19)

(cid:19)

(cid:19)

i + 1

i + 1

   

   

i=0

i=0

i=0

+

+

+

0

0

i

i

+

b

=

b + 1

b + 1

where the last equality follows by the well known identity

this concludes the proof.

with this result, we can easily prove theorem 1 as follows:

proof of theorem 1

proof. first consider the relu case. each neuron has one hyperplane associated with it, and so by theorem 5, the    rst
hidden layer divides up the inputs space into r(k, m) regions, with r(k, m)     o(km).
now consider the second hidden layer. for every region in the    rst hidden layer, there is a different activation pattern in
the    rst layer, and so (as described in the proof of theorem 2) a different hyperplane arrangement of k hyperplanes in an
m dimensional space, contributing at most r(k, m) regions.
in particular, the total number of regions in input space as a result of the    rst and second hidden layers is     r(k, m)    
r(k, m)     o(k2m). continuing in this way for each of the n hidden layers gives the o(kmn) bound.
a very similar method works for hard tanh, but here each neuron produces two hyperplanes, resulting in a bound of
o((2k)mn).

b. proofs and additional results from section 2.2
proof of theorem 3

b.1. notation and preliminary results
difference of points on trajectory given x(t) = x, x(t + dt) = x +   x in the trajectory, let   z(d) = z(d)(x +   x)    z(d)(x)
parallel and perpendicular components: given vectors x, y, we can write y = y    + y(cid:107) where y    is the component of y
perpendicular to x, and y(cid:107) is the component parallel to x. (strictly speaking, these components should also have a subscript
x, but we suppress it as the direction with respect to which parallel and perpendicular components are being taken will be
explicitly stated.)
this notation can also be used with a matrix w , see lemma 1.
before stating and proving the main theorem, we need a few preliminary results.
lemma 1. matrix decomposition let x, y     rk be    xed non-zero vectors, and let w be a (full rank) matrix. then, we
can write

w = (cid:107)w(cid:107) + (cid:107)w    +    w(cid:107) +    w   

such that

on the expressive power of deep neural networks

(cid:107)w   x = 0
yt    w(cid:107) = 0

   w   x = 0
yt    w    = 0

i.e. the row space of w is decomposed to perpendicular and parallel components with respect to x (subscript on right),
and the column space is decomposed to perpendicular and parallel components of y (superscript on left).
proof. let v, u be rotations such that v x = (||x|| , 0..., 0)t and u y = (||y|| , 0...0)t . now let   w = u w v t , and let
  w = (cid:107)   w(cid:107) + (cid:107)   w    +       w(cid:107) +       w   , with (cid:107)   w(cid:107) having non-zero term exactly   w11, (cid:107)   w    having non-zero entries exactly
  w1i for 2     i     k. finally, we let       w(cid:107) have non-zero entries exactly   wi1, with 2     i     k and       w    have the remaining
entries non-zero.
if we de   ne   x = v x and   y = u y, then we see that

(cid:107)   w      x = 0
  yt       w(cid:107) = 0

      w      x = 0
  yt       w    = 0

as   x,   y have only one non-zero term, which does not correspond to a non-zero term in the components of   w in the equations.
then, de   ning (cid:107)w(cid:107) = u t (cid:107)   w(cid:107)v , and the other components analogously, we get equations of the form

(cid:107)w   x = u t (cid:107)   w   v x = u t (cid:107)   w      x = 0

observation 1. given w, x as before, and considering w(cid:107), w    with respect to x (wlog a unit vector) we can express
them directly in terms of w as follows: letting w (i) be the ith row of w , we have

         ((w (0))t    x)x

((w (k))t    x)x

...

         

w(cid:107) =

i.e. the projection of each row in the direction of x. and of course

w    = w     w(cid:107)

the motivation to consider such a decomposition of w is for the resulting independence between different components, as
shown in the following lemma.
lemma 2. independence of projections let x be a given vector (wlog of unit norm.)
wij     n (0,   2), then w(cid:107) and w    with respect to x are independent random variables.

if w is a random matrix with

proof. there are two possible proof methods:
(a) we use the rotational invariance of random gaussian matrices, i.e. if w is a gaussian matrix, iid entries n (0,   2),
and r is a rotation, then rw is also iid gaussian, entries n (0,   2). (this follows easily from af   ne transformation
rules for multivariate gaussians.)
let v be a rotation as in lemma 1. then   w = w v t is also iid gaussian, and furthermore,   w(cid:107) and   w    partition
the entries of   w , so are evidently independent. but then w(cid:107) =   w(cid:107)v t and w    =   w   v t are also independent.

(b) from the observation note that w(cid:107) and w    have a centered multivariate joint gaussian distribution (both consist of
linear combinations of the entries wij in w .) so it suf   ces to show that w(cid:107) and w    have covariance 0. because
both are centered gaussians, this is equivalent to showing e(< w(cid:107), w    >) = 0. we have that

e(< w(cid:107), w    >) = e(w(cid:107)w t    ) = e(w(cid:107)w t )     e(w(cid:107)w t(cid:107) )

on the expressive power of deep neural networks

as any two rows of w are independent, we see from the observation that e(w(cid:107)w t ) is a diagonal matrix, with the
ith diagonal entry just ((w (0))t    x)2. but similarly, e(w(cid:107)w t(cid:107) ) is also a diagonal matrix, with the same diagonal
entries - so the claim follows.

in the following two lemmas, we use the rotational invariance of gaussians as well as the chi distribution to prove results
about the expected norm of a random gaussian vector.
lemma 3. norm of a gaussian vector let x     rk be a random gaussian vector, with xi iid,     n (0,   2). then

e [||x||] =   

   

2

  ((k + 1)/2)

  (k/2)

proof. we use the fact that if y is a random gaussian, and yi     n (0, 1) then ||y || follows a chi distribution. this means
that e(||x/  ||) =
2  ((k + 1)/2)/  (k/2), the mean of a chi distribution with k degrees of freedom, and the result
follows by noting that the expectation in the lemma is    multiplied by the above expectation.

   

we will    nd it useful to bound ratios of the gamma function (as appear in lemma 3) and so introduce the following
inequality, from (kershaw, 1983) that provides an extension of gautschi   s inequality.
theorem 6. an extension of gautschi   s inequality for 0 < s < 1, we have

(cid:16)

(cid:17)1   s       (x + 1)

  (x + s)

x +

s
2

(cid:32)

(cid:18)

2(cid:33)1   s
(cid:19) 1

   

x     1
2

+

s +

1
4

we now show:
lemma 4. norm of projections let w be a k by k random gaussian matrix with iid entries     n (0,   2), and x, y two
given vectors. partition w into components as in lemma 1 and let x    be a nonzero vector perpendicular to x. then

(b) if 1a is an identity matrix with non-zeros diagonal entry i iff i     a     [k], and |a| > 2, then

e(cid:2)(cid:12)(cid:12)(cid:12)(cid:12)   w   x   (cid:12)(cid:12)(cid:12)(cid:12)(cid:3) = ||x   ||   
e(cid:2)(cid:12)(cid:12)(cid:12)(cid:12)1a   w   x   (cid:12)(cid:12)(cid:12)(cid:12)(cid:3)     ||x   ||   

2

2

2

   

   

  (k/2)

    3
4

(cid:19)1/2

    ||x   ||   

  ((k     1)/2

(cid:18) k
(cid:18)|a|
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)   w   a
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) = ||w a||. so in particular
e(cid:2)(cid:12)(cid:12)(cid:12)(cid:12)   w   x   (cid:12)(cid:12)(cid:12)(cid:12)(cid:3) = e(cid:104)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)      w      x   
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:105)

  ((|a|     1)/2)

    ||x   ||   

(cid:19)1/2

  (|a|/2)

    3
4

   

   

2

2

2

proof.

(a) let u, v,   w be as in lemma 1. as u, v are rotations,   w is also iid gaussian. furthermore for any    xed w ,

with   a = v a, by taking inner products, and square-rooting, we see that

(a)

but from the de   nition of non-zero entries of       w   , and the form of   x    (a zero entry in the    rst coordinate), it follows
that       w      x    has exactly k    1 non zero entries, each a centered gaussian with variance (k    1)  2 ||x   ||2. by lemma
3, the expected norm is as in the statement. we then apply theorem 6 to get the lower bound.

(b) first note we can view 1a   w    =    1aw   . (projecting down to a random (as w is random) subspace of    xed size
|a| = m and then making perpendicular commutes with making perpendicular and then projecting everything down
to the subspace.)
so we can view w as a random m by k matrix, and for x, y as in lemma 1 (with y projected down onto m dimensions),
we can again de   ne u, v as k by k and m by m rotation matrices respectively, and   w = u w v t , with analogous

on the expressive power of deep neural networks

properties to lemma 1. now we can    nish as in part (a), except that       w      x may have only m    1 entries, (depending
on whether y is annihilated by projecting down by1a) each of variance (k     1)  2 ||x   ||2.

lemma 5. norm and translation let x be a centered multivariate gaussian, with diagonal covariance matrix, and    a
constant vector.

e(||x       ||)     e(||x||)

proof. the inequality can be seen intuitively geometrically: as x has diagonal covariance matrix, the contours of the pdf
of ||x|| are circular centered at 0, decreasing radially. however, the contours of the pdf of ||x       || are shifted to be
centered around ||  ||, and so shifting back    to 0 reduces the norm.
a more formal proof can be seen as follows: let the pdf of x be fx (  ). then we wish to show

now we can pair points x,   x, using the fact that fx (x) = fx (   x) and the triangle inequality on the integrand to get

(||x       || + ||   x       ||) fx (x)dx    

||2x|| fx (x)dx =

(||x|| + ||   x||) fx (x)dx

(cid:90)

|x|

(cid:90)

|x|

(cid:90)

x

||x       || fx (x)dx    

||x|| fx (x)dx

(cid:90)

x

(cid:90)

|x|

b.2. proof of theorem
we use v(d)
at layer d, and    the non-linearity. the weights and bias are called w (d) and b(d) respectively. so we have the relations

to denote the ith neuron in hidden layer d. we also let x = z(0) be an input, h(d) be the hidden representation

i

h(d) = w (d)z(d) + b(d),

z(d+1) =   (h(d)).

proof. we    rst prove the zero bias case. to do so, it is suf   cient to prove that

e(cid:104)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)  z(d+1)(t)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:105)     o

      (cid:32)    

  k   
   + k

(cid:33)d+1      (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)  z(0)(t)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(1)

(**)

as integrating over t gives us the statement of the theorem.
for ease of notation, we will suppress the t in z(d)(t).
we    rst write

w (d) = w (d)    + w (d)(cid:107)

where the division is done with respect to z(d). note that this means h(d+1) = w (d)(cid:107) z(d) as the other component annihilates
(maps to 0) z(d).
we can also de   ne aw (d)
saturated. letting wi denote the ith row of matrix w , we now claim that:

| < 1} i.e. the set of indices for which the hidden representation is not

= {i : i     [k],|h(d+1)

(cid:107)

i

(cid:104)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)  z(d+1)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:105)

e
w (d)

= e

w (d)

(cid:107)

e
w (d)   

               
             (cid:88)

i   a

w

(d)(cid:107)

            1/2               

((w (d)    )i  z(d) + (w (d)(cid:107)

)i  z(d))2

(*)

indeed, by lemma 2 we    rst split the expectation over w (d) into a tower of expectations over the two independent parts
of w to get

(cid:104)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)  z(d+1)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:105)

e
w (d)

(cid:104)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)  (w (d)  z(d))
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:105)

= e

e
w (d)   

w (d)

(cid:107)

on the expressive power of deep neural networks

in the inner expectation gives us h(d+1) and aw (d)

but conditioning on w (d)(cid:107)
  (w (d)  z(d)) with the sum in the term on the right hand side of the claim.
till now, we have mostly focused on partitioning the matrix w (d). but we can also set   z(d) =   z(d)(cid:107) +   z(d)    where the
perpendicular and parallel are with respect to z(d). in fact, to get the expression in (**), we derive a recurrence as below:

, allowing us to replace the norm over

(cid:107)

(cid:104)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)  z(d+1)   

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:105)     o

(cid:32)    

(cid:33)

  k   
   + k

e
w (d)

(cid:104)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)  z(d)   

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:105)

e
w (d)

to get this, we    rst need to de   ne   z(d+1) = 1a

w

(d)(cid:107)

h(d+1) - the latent vector h(d+1) with all saturated units zeroed out.

we then split the column space of w (d) =    w (d) + (cid:107)w (d), where the split is with respect to   z(d+1). letting   z(d+1)   
the part perpendicular to z(d+1), and a the set of units that are unsaturated, we have an important relation:
claim

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)  z(d+1)   

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)    (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)   w (d)  z(d)1a

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(where the indicator in the right hand side zeros out coordinates not in the active set.)
to see this,    rst note, by de   nition,

  z(d+1)   

= w (d)  z(d)    1a     (cid:104)w (d)  z(d)    1a,   z(d+1)(cid:105)  z(d+1)

where the      indicates a unit vector.
similarly

   w (d)  z(d) = w (d)  z(d)     (cid:104)w (d)  z(d),     z(d+1)(cid:105)    z(d+1)

be

(1)

(2)

now note that for any index i     a, the right hand sides of (1) and (2) are identical, and so the vectors on the left hand side
agree for all i     a. in particular,

now the claim follows easily by noting that
returning to (*), we split   z(d) =   z(d)    +   z(d)(cid:107) , w (d)    = (cid:107)w (d)    +    w (d)    (and w (d)(cid:107)
cancellation, we have

analogously), and after some

(cid:104)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)  z(d+1)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:105)

e
w (d)

= e

w (d)

(cid:107)

e
w (d)   

(cid:16)

(   w (d)    + (cid:107)w (d)    )i  z(d)    + (   w (d)(cid:107) + (cid:107)w (d)(cid:107)

)i  z(d)(cid:107)

(cid:17)2

            1/2               

   1a =    w (d)  z(d)    1a

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)    (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)  z(d+1)   

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12).

   1a

  z(d+1)   

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)  z(d+1)   
               
             (cid:88)

i   a

w

(d)(cid:107)

we would like a recurrence in terms of only perpendicular components however, so we    rst drop the (cid:107)w (d)    , (cid:107)w (d)(cid:107)
(which
can be done without decreasing the norm as they are perpendicular to the remaining terms) and using the above claim, have

(cid:104)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)  z(d+1)   

e
w (d)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:105)     e

w (d)

(cid:107)

e
w (d)   

(cid:16)

(   w (d)    )i  z(d)    + (   w (d)(cid:107)

)i  z(d)(cid:107)

(cid:17)2

               
             (cid:88)

i   a

w

(d)(cid:107)

            1/2               

(cid:16)

(   w (d)    )i  z(d)   

(cid:17)2

2

   

(cid:107)
2

(cid:113)2|aw (d)
       k(cid:88)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:105)     1   
(cid:18)k
(cid:19)
(cid:112)j =    

j=2

2

1
   

on the expressive power of deep neural networks

e
w (d)   

(   w (d)    )i  z(d)    + (   w (d)(cid:107)

)i  z(d)(cid:107)

but in the inner expectation, the term    w (d)(cid:107)   z(d)(cid:107)
we have

               
             (cid:88)

i   a

w

(d)(cid:107)

(cid:16)
               
             (cid:88)

i   a

w

(d)(cid:107)

e
w (d)   

we can then apply lemma 4 to get

is just a constant, as we are conditioning on w (d)(cid:107)

. so using lemma 5

            1/2               

            1/2                    e
(cid:17)2
            1/2                         

   

k

2

w (d)   

               
             (cid:88)
(cid:113)2|aw (d)

i   a

w

(d)(cid:107)

|     3

(cid:16)

(   w (d)    )i  z(d)   

(cid:17)2

e(cid:104)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)  z(d)   

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:105)

the outer expectation on the right hand side only affects the term in the expectation through the size of the active set of
| < 1), and noting that we get a non-zero
units. for relus, p = p(h(d+1)
norm only if |aw (d)

|     2 (else we cannot project down a dimension), and for |aw (d)

> 0) and for hard tanh, we have p = p(|h(d+1)

|     2,

i

i

(cid:107)

(cid:107)
2

(cid:107)

we get

e
w (d)

(cid:104)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)  z(d+1)   

|     3

    1   
2

(cid:113)|aw (d)

(cid:107)

|

(cid:19)

(cid:18)k

j

pj(1     p)k   j      
k

       e(cid:104)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)  z(d)   

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:105)
(cid:112)j

pj(1     p)k   j      
k

(cid:18)k     1

(cid:19)

j     1

pj   1(1     p)k   j

(cid:112)j
(cid:19)
(cid:18)k
k(cid:88)

j

j=1

1   
j

k(cid:88)

j=0

we use the fact that we have the id203 mass function for an (k, p) binomial random variable to bound the

   

j term:

(cid:18)k

(cid:19)

k(cid:88)

j

j=2

pj(1     p)k   j      
k

p(1     p)k   1      
k

+

=      

kp(1     p)k   1 + kp         
k

but by using jensen   s inequality with 1/

(cid:18)k     1

(cid:19)

j     1

1   
j

k(cid:88)

j=1

   

x, we get

pj   1(1     p)k   j    

(cid:113)(cid:80)k
j=1 j(cid:0)k   1

j   1

1

(cid:1)pj   1(1     p)k   j

=

where the last equality follows by recognising the expectation of a binomial(k    1, p) random variable. so putting together,
we get

(cid:104)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)  z(d+1)   

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:105)     1   

2

(cid:32)

e
w (d)

   

     

kp(1     p)k   1 +      

   

(cid:112)1 + (k     1)p

kp

from here, we must analyse the hard tanh and relu cases separately. first considering the hard tanh case:

to lower bound p, we    rst note that as h(d+1)

i

is a normal random variable with variance       2, if a     n (0,   2)

p(|h(d+1)

i

   
| < 1)     p(|a| < 1)     1

  

2  

(a)

(b)

1(cid:112)(k     1)p + 1
(cid:33)
e(cid:104)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)  z(d)   
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:105)

on the expressive power of deep neural networks

where the last inequality holds for        1 and follows by taylor expanding e   x2/2 around 0. similarly, we can also show
that p     1
   .
so this becomes

e(cid:104)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)  z(d+1)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:105)    

       1   
       1
(cid:32)    
(cid:33)

2

(2  )1/4

= o

  k   
   + k

   

  k

   

(cid:113)
e(cid:104)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)  z(d)   

  

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:105)

2   + (k     1)

(cid:18)

   

k

   

1     1
  

(cid:19)k   1             e(cid:104)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)  z(d)   

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:105)

finally, we can compose this, to get

e(cid:104)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)  z(d+1)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:105)    

       1   

2

       1

(2  )1/4

(cid:113)

  

   

   

  k

2   + (k     1)

(cid:18)

1     1
  

(cid:19)k   1            d+1

   

k

   

c    ||  x(t)||

(c)

with the constant c being the ratio of ||  x(t)   || to ||  x(t)||. so if our trajectory direction is almost orthogonal to x(t)
(which will be the case for e.g. random circular arcs, c can be seen to be     1 by splitting into components as in lemma 1,
and using lemmas 3, 4.)
the relu case (with no bias) is even easier. noting that for random weights, p = 1/2, and plugging in to equation (a), we
get

(cid:104)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)  z(d+1)   

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:105)     1   

2

(cid:32)     
   
2k +      

k

   

k(cid:112)2(k + 1)

(cid:33)

e(cid:104)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)  z(d)   

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:105)

e
w (d)

(d)

but the expression on the right hand side has exactly the asymptotic form o(  

   

   

k/

k + 1), and we    nish as in (c).

result for non-zero bias
in fact, we can easily extend the above result to the case of non-zero bias. the insight is to
note that because   z(d+1) involves taking a difference between z(d+1)(t + dt) and z(d+1)(t), the bias term does not enter
at all into the expression for   z(d+1). so the computations above hold, and equation (a) becomes

(cid:104)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)  z(d+1)   

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:105)     1   

2

(cid:32)

e
w (d)

   

kp(1     p)k   1 +   w   

   

(cid:112)1 + (k     1)p

kp

     w

(cid:33)

e(cid:104)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)  z(d)   

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:105)

for relus, we require h(d+1)
n (0,   2
through results in the same asymptotic behavior as without bias.

b ) and
w) respectively. but with p     1/4, this holds as the signs for w, b are purely random. substituting in and working

> 0 where the bias and weight are drawn from n (0,   2

z(d)
i + b(d+1)

= w(d+1)

i

i

i

for hard tanh, not that as h(d+1)

i

is a normal random variable with variance       2

w +   2

b (as equation (b) becomes

p(|h(d+1)

i

| < 1)    

(cid:112)(  2

1

w +   2
b )

   

2  

this gives theorem 3

e(cid:104)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)  z(d+1)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:105)     o

      

  w
w +   2

(  2

b )1/4

  

       e(cid:104)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)  z(d)   

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:105)

   

k

(cid:113)(cid:112)  2

w +   2

b + k

on the expressive power of deep neural networks

figure 12. the    gure above shows trajectory growth with different initialization scales as a trajectory is propagated through a fully
connected network for mnist, with relu activations. note that as described by the bound in theorem 3 we see that trajectory growth
is 1) exponential in depth 2) increases with initialization scale and width, 3) increases faster with scale over width, as expected from   w

compared to(cid:112)k/(k + 1) in the theorem.

statement and proof of upper bound for trajectory growth for hard tanh replace hard-tanh with a linear
coordinate-wise identity map, h(d+1)
= (w (d)z(d))i + bi. this provides an upper bound on the norm. we also then
recover a chi distribution with k terms, each with standard deviation   w
k

1
2

i

e(cid:104)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)  z(d+1)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:105)    

   

2

,

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)  z(d)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:19) 1
2(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)  z(d)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ,

  w
k 1

2

   ((k + 1)/2)

   (k/2)

(cid:18) k + 1

k

      w

(2)

(3)

where the second step follows from (laforgia and natalini, 2013), and holds for k > 1.

proof of theorem 4

i

i

proof. for   b = 0:
for hidden layer d < n, consider neuron v(d)
we assume that |z(d   1)
assume wlog that z(d   1)
wlog    1) will induce a transition in node v(d)

| = 1. furthermore, as signs for z(d   1)
i1
= 1. for a particular input, we can de   ne v(d)

1 . this has as input(cid:80)k
1 . a suf   cient condition for this to happen is if |wi1|     |(cid:80)

. as we are in the large    case,
are both completely random, we can also
transitioning (to
j(cid:54)=i wj1|. but
j(cid:54)=i wj1 = y (cid:48)     n (0, (k     1)  2/k). so we want to compute p(|x| > |y (cid:48)|). for ease of

x = wi1     n (0,   2/k) and(cid:80)

computation, we instead look at p(|x| > |y |), where y     n (0,   2).
but this is the same as computing p(|x|/|y | > 1) = p(x/y <    1) + p(x/y > 1). but the ratio of two centered
   
independent normals with variances   2
2 follows a cauchy distribution, with parameter   1/  2, which in this case is
1/

as sensitive to v(d   1)

i=1 w (d   1)

and w (d   1)

if v(d   1)

z(d   1)

1,   2

k. substituting this in to the cdf of the cauchy distribution, we get that
   

(cid:19)

i1

1

i

i

i

i

(cid:18)|x|

p

|y | > 1

= 1     2
  

arctan(

k)

2468101214network depth100101102103104105106trajectory lengthtrajectory length growth with increasing depthw50 scl3w1000 scl3w50 scl5w500 scl5w100 scl8w700 scl8w300 scl10w700 scl10on the expressive power of deep neural networks

(a)

(c)

(b)

(d)

figure 13. the exponential growth of trajectory length with depth, in a random deep network with hard-tanh nonlinearities. a circular
trajectory is chosen between two random vectors. the image of that trajectory is taken at each layer of the network, and its length
measured. (a,b) the trajectory length vs. layer, in terms of the network width k and weight variance   2
w, both of which determine its
growth rate. (c,d) the average ratio of a trajectory   s length in layer d + 1 relative to its length in layer d. the solid line shows simulated
data, while the dashed lines show upper and lower bounds (theorem 3). growth rate is a function of layer width k, and weight variance
w.
  2

024681012depth100101102103104105trajectory lengthtrajectory length, k=32  2w=0.5  2w=1  2w=4  2w=16  2w=64024681012depth100101102trajectory lengthtrajectory length,   2w=4k=1k=4k=16k=64k=2560100200300400500600width2024681012||  xd+1||||  xd||perturbation growth  2w=1  2w=16  2w=64050100150200250300  2w505101520||  xd+1||||  xd||perturbation growthk=2k=32k=512   
finally, using the identity arctan(x) + arctan(1/x) and the laurent series for arctan(1/x), we can evaluate the right hand
side to be o(1/

k). in particular

on the expressive power of deep neural networks

(cid:18)|x|

p

(cid:19)

|y | > 1

    o

(cid:19)

(cid:18) 1   

k

   

(c)

i

i

(cid:105)

t (d)
1

k neurons in the layer below.
might    ip very quickly from say    1 to 1, the gradation in the transition ensures
will transition at distinct times, we get the desired growth rate in expectation as

this means that in expectation, any neuron in layer d will be sensitive to the transitions of
using this, and the fact the while v(d   1)
that neurons in layer d sensitive to v(d   1)
follows:
let t (d) be a random variable denoting the number of transitions in layer d. and let t (d)

number of transitions of neuron i in layer d. note that by linearity of expectation and symmetry, e(cid:2)t (d)(cid:3) =(cid:80)
ke(cid:104)
(cid:105)     e(cid:104)(cid:80)
now, e(cid:104)
= ke(cid:104)
but by the independence of these two events, e(cid:104)
(cid:105)
(cid:3)    e(cid:104)
= e(cid:2)1(1,1)
(cid:105)        
(cid:105)
ke(cid:104)
k) by (c), so putting it all together, e(cid:104)
written in terms of the entire layer, we have e(cid:2)t (d+1)(cid:3)        
ke(cid:2)t (d)(cid:3) as desired.
k with (cid:112)k(1 +   2

i 1(1,i)t (d)
being sensitive to neuron i in layer d.

w), by noting that y     n (0,   2

for   b > 0:
we replace

   
side is o(1/

1(1,1)t (d)
t (d+1)
1

1(1,1)t (d)

t (d+1)
1

w +   2

be a random variable denoting the
=

where 1(1,i) is the indicator function of neuron 1 in layer d + 1

b ). this results in a growth rate of form

. but the    rt time on the right hand

e(cid:104)

t (d)
i

t (d)
1

t (d)
1

(cid:105)

(cid:105)

(cid:105)

(cid:105)

b /  2

   

.

1

1

i

i

i

(cid:113)

   

o(

k/

1 +   2
b
  2
w

).

b.3. dichotomies: a natural dual

our measures of expressivity have mostly concentrated on sweeping the input along a trajectory x(t) and taking measures
of fa(x(t); w ). instead, we can also sweep the weights w along a trajectory w (t), and look at the consequences (e.g.
binary labels     i.e. dichotomies), say for a    xed set of inputs x1, ..., xs.
in fact, after random initialization, sweeping the    rst layer weights is statistically very similar to sweeping the input along
a trajectory x(t). in particular, letting w (cid:48) denote the    rst layer weights, for a particular input x0, x0w (cid:48) is a vector, each
coordinate is iid,     n (0,||x0||2  2
w). extending this observation, we see that (providing norms are chosen appropriately),
x0w (cid:48) cos(t) + x1w (cid:48) sin(t) (   xed x0, x1, w ) has the same distribution as x0w (cid:48)
0, w (cid:48)
1).
so we expect that there will be similarities between results for sweeping weights and for sweeping input trajectories, which
we explore through some synthetic experiments, primarily for hard tanh, in figures 15, 16. we    nd that the proportionality
of transitions to trajectory length extends to dichotomies, as do results on the expressive power afforded by remaining
depth.
for non-random inputs and non-random functions, this is a well known question upper bounded by the sauer-shelah lemma
(sauer, 1972). we discuss this further in appendix ??. in the random setting, the statistical duality of weight sweeping
and input sweeping suggests a direct proportion to transitions and trajectory length for a    xed input. furthermore, if the
xi     s are suf   ciently uncorrelated (e.g. random) class label transitions should occur independently for each xi indeed,
we show this in figure 14.

1 sin(t) (   xed x0, w (cid:48)

0 cos(t) + x0w (cid:48)

c. addtional experiments from section 3
here we include additional experiments from section 3

on the expressive power of deep neural networks

(a)

(b)

figure 14. we sweep the weights w of a layer through a trajectory w (t) and count the number of labellings over a set of datapoints.
when w is the    rst layer, this is statistically identical to sweeping the input through x(t) (see appendix). thus, similar results are
observed, with exponential increase with the depth of an architecture, and much slower increase with width. here we plot the number
of classi   cation dichotomies over s = 15 input vectors achieved by sweeping the    rst layer weights in a hard-tanh network along a
one-dimensional great circle trajectory. we show this (a) as a function of depth for several widths, and (b) as a function of width for
several depths. all networks were generated with weight variance   2

w = 8, and bias variance   2

b = 0.

024681012141618remaining depth dr100101102103104105unique patternsdichotomies vs. remaining depthk=2k=8k=32k=128k=5120100200300400500600width k100101102103104105unique patternsdichotomies vs. widthdr=1dr=3dr=5dr=7dr=9dr=11dr=13dr=15dr=17on the expressive power of deep neural networks

figure 15. expressive power depends only on remaining network depth. here we plot the number of dichotomies achieved by sweeping
the weights in different network layers through a 1-dimensional great circle trajectory, as a function of the remaining network depth.
the number of achievable dichotomies does not depend on the total network depth, only on the number of layers above the layer swept.
w = 8, number of datapoints s = 15, and hard-tanh nonlinearities. the blue dashed
all networks had width k = 128, weight variance   2
line indicates all 2s possible dichotomies for this random dataset.

0246810121416remaining depth dr101102103104105unique dichotomiesdichotomies vs. remaining depthlayer swept = 1layer swept = 4layer swept = 8layer swept = 12all dichotomieson the expressive power of deep neural networks

figure 16. here we plot the number of unique dichotomies that have been observed as a function of the number of transitions the
network has undergone. each datapoint corresponds to the number of transitions and dichotomies for a hard-tanh network of a different
depth, with the weights in the    rst layer undergoing interpolation along a great circle trajectory w (0)(t). we compare these plots to a
random walk simulation, where at each transition a single class label is    ipped uniformly at random. dichotomies are measured over
a dataset consisting of s = 15 random samples, and all networks had weight variance   2
w = 16. the blue dashed line indicates all 2s
possible dichotomies.

100101102103104105transitions100101102103104105unique dichotomiesdichotomies vs. transitionsk=2k=8k=32k=128k=512all dichotomiesrandom walkon the expressive power of deep neural networks

figure 17. we repeat a similar experiment in figure 7 with a fully connected network on cifar-10, and mostly observe that training
lower layers again leads to better performance, although, as expected, overall performance is impacted by training only a single layer.
the networks had width k = 200, weight variance   2
w = 1, and hard-tanh nonlinearities. we again only train from the second hidden
layer on so that the number of parameters remains    xed. the theory only applies to training error (the ability to    t a function), and
generalisation accuracy remains low in this very constrained setting.

0100200300400500epoch number0.20.30.40.50.6accuracytrain accuracy against epoch0100200300400500epoch numbertest accuracy against epochlay 2lay 3lay 4lay 5lay 6lay 7lay 8on the expressive power of deep neural networks

figure 18. training increases the trajectory length for smaller initialization values of   w. this experiment plots the growth of trajectory
length as a circular interpolation between two mnist datapoints is propagated through the network, at different train steps. red indicates
the start of training, with purple the end of training. we see that the training process increases trajectory length, likely to increase the
expressivity of the input-output map to enable greater accuracy.

