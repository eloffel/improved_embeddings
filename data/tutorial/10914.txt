neural paraphrase identi   cation of questions

with noisy pretraining

gaurav singh tomar thyago duque oscar t  ackstr  om

jakob uszkoreit dipanjan das

google inc.

{gtomar, duque, oscart, uszkoreit, dipanjand}@google.com

7
1
0
2

 

g
u
a
0
2

 

 
 
]
l
c
.
s
c
[
 
 

2
v
5
6
5
4
0

.

4
0
7
1
:
v
i
x
r
a

abstract

we present a solution to the problem of
paraphrase identi   cation of questions. we
focus on a recent dataset of question pairs
annotated with binary paraphrase labels
and show that a variant of the decompos-
able attention model (parikh et al., 2016)
results in accurate performance on this task,
while being far simpler than many com-
peting neural architectures. furthermore,
when the model is pretrained on a noisy
dataset of automatically collected question
paraphrases, it obtains the best reported
performance on the dataset.

introduction

1
question paraphrase identi   cation is a widely use-
ful nlp application. for example, in question-and-
answer (qa) forums ubiquitous on the web, there
are vast numbers of duplicate questions. identi-
fying these duplicates and consolidating their an-
swers increases the ef   ciency of such qa forums.
moreover, identifying questions with the same se-
mantic content could help web-scale question an-
swering systems that are increasingly concentrating
on retrieving focused answers to users    queries.

here, we focus on a recent dataset published by
the qa website quora.com containing over 400k
annotated question pairs containing binary para-
phrase labels.1 we believe that this dataset presents
a great opportunity to the nlp research commu-
nity and practitioners due to its scale and quality; it
can result in systems that accurately identify dupli-
cate questions, thus increasing the quality of many
qa forums. we examine a simple model family,
the decomposable attention model of parikh et al.
(2016), that has shown promise in modeling natural

1see https://data.quora.com/first-quora-dataset-release-

question-pairs.

language id136 and has inspired recent work on
similar tasks (chen et al., 2016; kim et al., 2017).
we present two contributions. first, to mitigate
data sparsity, we modify the input representation
of the decomposable attention model to use sums
of character id165 embeddings instead of word
embeddings. we show that this model trained on
the quora dataset produces comparable or better
results with respect to several complex neural ar-
chitectures, all using pretrained id27s.
second, to signi   cantly improve our model perfor-
mance, we pretrain all our model parameters on the
noisy, automatically collected question-paraphrase
corpus paralex (fader et al., 2013), followed by
   ne-tuning the parameters on the quora dataset.
this two-stage training procedure achieves the best
result on the quora dataset to date, and is also sig-
ni   cantly better than learning only the character
id165 embeddings during the pretraining stage.

2 related work

paraphrase identi   cation is a well-studied task in
nlp (das and smith, 2009; chang et al., 2010; he
et al., 2015; wang et al., 2016, inter alia). here,
we focus on an instance, that of    nding questions
with identical meaning. lei et al. (2016) consider
a related task leveraging the askubuntu corpus
(dos santos et al., 2015), but it contains two or-
ders of magnitude less annotations, thus limiting
the quality of any model. most relevant to this
work is that of wang et al. (2017), who present the
best results on the quora dataset prior to this work.
the bilateral multi-perspective matching model
(bimpm) of wang et al. uses a character-based
lstm (hochreiter and schmidhuber, 1997) at its
input representation layer, a layer of bi-lstms
for computing context information, four different
types of multi-perspective matching layers, an ad-
ditional bi-lstm aggregation layer, followed by a

two-layer feedforward network for prediction. in
contrast, the decomposable attention model uses
four simple feedforward networks to (self-)attend,
compare and predict, leading to a more ef   cient
architecture. bimpm falls short of our best per-
forming model pretrained on noisy paraphrase data
and uses more parameters than our best model.

character-level modeling of text is a popular
approach. while conceptually simple, character
id165 embeddings are a highly competitive repre-
sentation (huang et al., 2013; wieting et al., 2016;
bojanowski et al., 2016). more complex representa-
tions built directly from individual characters have
also been proposed (sennrich et al., 2016; luong
and manning, 2016; kim et al., 2016; chung et al.,
2016; ling et al., 2015). these representations are
robust to out-of-vocabulary items, often produc-
ing improved results. our pretraining procedure is
reminiscent of several recent papers (wieting et al.,
2016, inter alia) who aim for general purpose char-
acter id165 embeddings. in contrast, we pretrain
all model parameters on automatic but in-domain
paraphrase data. we employ the same neural ar-
chitecture as our end task, similar to prior work on
id72 (s  gaard and goldberg, 2016,
inter alia), but use a simpler learning setup.

3 approach

our starting point is the decomposable attention
model (parikh et al., 2016, decatt henceforth),
which despite its simplicity and ef   ciency has been
shown to work remarkably well for the related
task of natural language id136 (bowman et al.,
2015). we extend this model with character id165
embeddings and noisy pretraining for the task of
question paraphrase identi   cation.

3.1 problem formulation
let a = (a1, . . . , a(cid:96)a) and b = (b1, . . . , b(cid:96)b) be
two input texts consisting of (cid:96)a and (cid:96)b tokens, re-
spectively. we assume that each ai, bj     rd is
encoded in a vector of dimension d. a context win-
dow of size c is subsequently applied, such that the
input to the model (  a,   b) consists of partly overlap-
ping phrases   ai = [ai   c, . . . , ai, . . . , ai+c],   bj =
[bj   c, . . . , bj, . . . , bj+c]     r2c+1  d. the model is
estimated using training data in the form of labeled
pairs {a(n), b(n), y(n)}n
n=1, where y(n)     {0, 1} is
a binary label indicating whether a is a paraphrase
of b or not. our goal is to predict the correct label
y given a pair of previously unseen texts (a, b).

3.2 the decomposable attention model
the decatt model divides the prediction into
three steps: attend, compare and aggregate. due
to lack of space, we only provide a brief outline
below and refer to parikh et al. (2016) for further
details on each of these steps.
attend. first, the elements of   a and   b are aligned
using a variant of neural attention (bahdanau et al.,
2015) to decompose the problem into the compari-
son of aligned phrases.

eij := f (  ai)(cid:62)f (  bj) .

(1)

the function f is a feedforward network. the
aligned phrases are computed as follows:

(cid:96)b(cid:88)
(cid:96)a(cid:88)

j=1

i=1

(cid:80)(cid:96)b
(cid:80)(cid:96)a

  i :=

  j :=

exp(eij)
k=1 exp(eik)

  bj ,

exp(eij)
k=1 exp(ekj)

  ai .

(2)

here   i is the subphrase in   b that is (softly) aligned
to   ai and vice versa for   j. optionally, the inputs
  a and   b to (1) can be replaced by input representa-
tions passed through a    self-attention    step to cap-
ture longer context. in this optional step, we modify
the input representations using    self-attention    to
encode compositional relationships between words
within each sentence, as proposed by (cheng et al.,
2016). similar to (1), we de   ne
fij := fself (  ai)(cid:62)f (cid:48)

self (  aj) .

(3)

the function fself and f (cid:48)
self are feedforward net-
works. the self-aligned phrases are then computed
as follows:

(cid:96)a(cid:88)

j=1

(cid:80)(cid:96)a

a(cid:48)
i :=

exp(fij + di   j)
k=1 exp(fik + di   k)

aj .

(4)

where di   j is a learned distance-sensitive bias term.
subsequent steps then use modi   ed input represen-
tations de   ned as   ai := [ai,a(cid:48)
compare. second, we separately compare the
aligned phrases {(  ai,   i)}(cid:96)a
j=1 us-
ing a feedforward network g:

i=1 and {(  bj,   j)}(cid:96)b

i] and   bi := [bi,b(cid:48)
i].

v1,i := g([  ai,   i])    i     (cid:104)1, . . . , (cid:96)a(cid:105) ,
v2,j := g([  bj,   j])    j     (cid:104)1, . . . , (cid:96)b(cid:105) .

(5)

where the brackets [  ,  ] denote concatenation.

the sets {v1,i}(cid:96)a

aggregate. finally,
i=1 and
{v2,j}(cid:96)b
j=1 are aggregated by summation. the sum
of two sets is concatenated and passed through an-
other feedforward network followed by a linear
layer, to predict the label   y.

3.3 character id165 word encodings
parikh et al. assume that each token ai, bj     rd
is directly embedded in a vector of dimension d;
in practice, they used pretrained id27s.
inspired by prior work mentioned in section 2, we
use an alternative approach and instead represent
each token as a sum of its embedded character n-
grams. this allows for more effective parameter
sharing at a small additional computational cost.
as observed in section 4, this leads to better results
compared to id27s.

3.4 noisy pretraining

while character id165 encodings help in effective
parameter sharing, data sparsity remains an issue.
pretraining embeddings with a task-agnostic ob-
jective on large-scale corpora (pennington et al.,
2014) is a common remedy to this problem. how-
ever, such pretraining is limited in the following
ways. first, it only applies to the input represen-
tation, leaving subsequent parts of the model to
random initialization. second, there may be a do-
main mismatch unless embeddings are pretrained
on the same domain as the end task (e.g., questions).
finally, since the objective used for pretraining
differs from that of the end task (e.g., paraphrase
identi   cation), the embeddings may be suboptimal.
as an alternative to task-agnostic pretraining
of embeddings on a very large corpus, we pro-
pose to pretrain all parameters of the model on
a modest-sized corpus of automatically gathered,
and therefore noisy examples, drawn from a simi-
lar domain.2 as observed in section 4, such noisy
pretraining of the full model results in more ac-
curate performance compared to using pretrained
embeddings, as well as compared to only pretrain-
ing embeddings on the noisy in-domain corpus.3

2paralex is gathered from wikianswers, a qa forum.
3the quora data is similar to the paralex corpus and we
exploit this by pretraining our entire model on the latter. it can
be argued that not all sentence pair modeling tasks may bene   t
similarly from the paralex corpus and a detailed empirical
study is warranted to investigate that; in this work, we restrict
our scope to only the question paraphrase identi   cation task,
a very useful nlp application by itself.

figure 1: learning curves for the quora develop-
ment set with and without pretraining on paralex.

implementation details

4 experiments
4.1
datasets we evaluate our models on the quora
question paraphrase dataset which contains over
400,000 question pairs with binary labels. we use
the same data and split as wang et al. (2017), with
10,000 question pairs each for development and
test, who also provide preprocessed and tokenized
question pairs.4 we duplicated the training set,
which has approximately 36% positive and 64%
negative pairs, by adding question pairs in reverse
order (since our model is not symmetric). when
pretraining the full model parameters, we use the
paralex corpus (fader et al., 2013), which consists
of 36 million noisy paraphrase pairs including du-
plicate reversed paraphrases. we created 64 million
arti   cial negative paraphrase pairs (re   ecting the
class balance of the quora training set) by combin-
ing the following three types of negatives in equal
proportions: (1) random unrelated questions, (2)
random questions that share a single word, and (3)
random questions that share all but one word.5
hyperparameters we tuned the following hyper-
parameters by grid search on the development set
(settings for our best model are in parenthesis):
embedding dimension (300), shape of all feedfor-
ward networks (two layers with 400 and 200 width),
character id165 sizes (5), context size (1), learn-
ing rate (0.1 for both pretraining and tuning), batch
size (256 for pretraining and 64 for tuning), dropout
ratio (0.1 for tuning) and prediction threshold (pos-
itive paraphrase for a score     0.3). we examined
whether self-attention helps or not for all model
variants, and found that it does for our best model.
note that we tried multiple orders of character n-

4this split is available at https://zhiguowang.github.io.
5more complex sampling procedures are possible, for ex-

ample, by using pretrained id27s.

102103104105106number of quora training examples (log scale)0.50.60.70.80.91.0accuracypretrainednot pretrainedid question 1

a

how shall i start my preparation for iit-jee
in class 10?

b what is fama french three factor model?

c how does paypal work in india?

d

e

f

what are the similarities between british en-
glish and american english?

how is buying land on the moon a good in-
vestment? why do people buy land on the
moon?
what can wrestlers do to prevent cauli   ower
ears?

question 2
should i start preparing for the iit jee in class
10 only?
what is fama-french three factor model?

does paypal work in india? what features of
paypal are available in india?
what are the similarities between american
english and british english?

at $20 an acre, isn   t buying moon plots a solid
investment?

why do wrestlers have deformed ears?

decattglove decattchar

pt-decattchar gold

n

n

y

n

n

n

y

y

y

n

n

n

y

y

n

y

n

n

y

y

n

y

y

y

table 1: example wins and losses from the decattglove, decattchar and the pt-decattchar models.

method
siamese-id98
multi-perspective id98
siamese-lstm
multi-perspective-lstm
l.d.c
bimpm
ffnnword
ffnnchar
decattword
decattglove
decattchar
decattparalex   char
pt-decattword
pt-decattchar

dev acc test acc

-
-
-
-
-

88.69
85.07
86.01
86.04
87.42
87.78
87.80
88.44
88.89

79.60
81.38
82.58
83.21
85.55
88.17
84.35
85.06
85.27
86.52
86.84
87.77
87.54
88.40

table 2: results on the quora development and
test sets in terms of accuracy. the    rst six rows are
taken from (wang et al., 2017).

grams with n     {3, 4, 5} both individually and
separately but 5-grams alone worked better than
these alternatives.
baselines we implemented several baseline mod-
els. in our    rst two baselines, each question is
represented by concatenating the sum of its uni-
gram id27s and the sum of its trigram
vectors, where each trigram vector is a concate-
nation of 3 adjacent id27s. the two
question representations are then concatenated and
fed to a feedforward network of shape [800, 400,
200]. we call these ffnnword and ffnnchar;
in the latter, id27s are just sums of
character id165 embeddings. second, we com-
pare purely supervised variants of decomposable
attention model, namely a word-based model with-

out any pretrained embeddings (decattword), a
word-based model with glove (pennington et al.,
2014) embeddings (decattglove), a character n-
gram model (decattchar) without pretrained em-
beddings and decattparalex   char whose charac-
ter id165 embeddings are pretrained with paralex
while all other parameters are learned from scratch
on quora. finally we present a baseline where a
word-based model is pretrained completely on par-
alex (pt-decattword) and our best model which
is a character id165 model pretrained completely
on paralex (pt-decattchar). note that in case of
character id165 based models, for tokens shorter
than n characters, we backoff and emit the token
itself. also, boundary markers were added at the
beginning and end of each word.

4.2 results
other than our baselines, we compare with wang
et al. (2017) in table 2. we observe that the sim-
ple ffnn baselines work better than more com-
plex siamese and multi-perspective id98 or lstm
models, more so if character id165 based em-
beddings are used. our basic decomposable at-
tention model decattword without pre-trained
embeddings is better than most of the models, all
of which used glove embeddings. an interest-
ing observation is that decattchar model with-
out any pretrained embeddings outperforms de-
cattglove that uses task-agnostic glove embed-
dings. furthermore, when character id165 em-
beddings are pre-trained in a task-speci   c manner
in decattparalex   char model, we observe a signif-
icant boost in performance. 6

the    nal two rows of the table show results
achieved by pt-decattword and pt-decattchar.
6note that paralex is orders of magnitude smaller than the

corpus used to pretrain glove.

we note that the former falls short of the de-
cattparalex   char, which shows that character n-
gram representations are powerful. finally, we note
that our best performing model is pt-decattchar,
which leverages the full power of character embed-
dings and pretraining the model on paralex.

noisy pretraining gives more signi   cant gains
in case of smaller human annotated data as can be
seen in figure 1 where non-pretrained decattchar
and pretrained pt-decattchar are compared on a
logarithmic scale of number of quora examples. it
also gives an important insight into trade off be-
tween having more but costly human annotated
data versus cheap but noisy pretraining. table 1
shows some example predictions from the de-
cattglove, decattchar and the pt-decattchar
models. the glove-trained model often makes mis-
takes related to spelling and id121 artifacts.
we observed that hyperparameter tuning resulted in
settings where non-pretrained models did not use
self-attention while the pretrained character based
model did, thus learning better long term context at
its input layer; this is re   ected in example d which
shows an alternation that our best model captures.
finally, e and f show pairs that present complex
paraphrases that none of our models capture.

5 conclusion and future work

we presented a focused contribution on question
paraphrase identi   cation, on the recently published
quora corpus. first, we showed that replacing the
id27s of the decomposable attention
model of parikh et al. (2016) with character id165
embeddings results in signi   cantly better accuracy
on this task. second, we showed that pretraining
the full model on automatically labeled noisy, but
task-speci   c data results in further improvements.
our methods perform better than several complex
neural architectures and achieve state of the art.
while conceptually simple, we believe that these
are two important insights that may be more widely
applicable within the    eld of natural language un-
derstanding. we leave investigation of this claim to
future work that may involve evaluation on related
tasks such as recognizing id123.

references
dzmitry bahdanau, kyunghyun cho, and yoshua ben-
gio. 2015. id4 by jointly
in proceedings of
learning to align and translate.
iclr.

piotr bojanowski, edouard grave, armand joulin, and
tomas mikolov. 2016. enriching word vectors with
subword information. arxiv 1607.04606.

samuel r. bowman, gabor angeli, christopher potts,
and christopher d. manning. 2015. a large anno-
tated corpus for learning natural language id136.
in proceedings of emnlp.

ming-wei chang, dan goldwasser, dan roth, and
vivek srikumar. 2010. discriminative learning over
in proceedings
constrained latent representations.
of hlt-naacl.

qian chen, xiaodan zhu, zhen-hua ling, si wei, and
hui jiang. 2016. enhancing and combining sequen-
tial and tree lstm for natural language id136.
arxiv 1609.06038 .

jianpeng cheng, li dong,

and mirella lapata.
long short-term memory-networks for
2016.
the 2016
machine reading.
conference on empirical methods
in natural
language processing. association for computa-
tional linguistics, austin, texas, pages 551   561.
https://aclweb.org/anthology/d16-1053.

in proceedings of

junyoung chung, kyunghyun cho, and yoshua ben-
gio. 2016. a character-level decoder without ex-
plicit segmentation for id4.
in proceedings of acl.

dipanjan das and noah a. smith. 2009. paraphrase
identi   cation as probabilistic quasi-synchronous
recognition. in proceedings of acl-ijcnlp.

cicero dos santos, luciano barbosa, dasha bog-
danova, and bianca zadrozny. 2015. learning hy-
brid representations to retrieve semantically equiva-
lent questions. in proceedings of acl.

anthony fader, luke zettlemoyer, and oren etzioni.
2013. paraphrase-driven learning for open question
answering. in proceedings of acl.

hua he, kevin gimpel, and jimmy lin. 2015. multi-
perspective sentence similarity modeling with con-
in proceedings of
volutional neural networks.
emnlp.

sepp hochreiter and j  urgen schmidhuber. 1997.
neural computation

long short-term memory.
9(8):1735   1780.

po-sen huang, xiaodong he, jianfeng gao, li deng,
alex acero, and larry heck. 2013. learning deep
structured semantic models for web search using
clickthrough data. in proceedings of cikm.

yoon kim, carl denton, loung hoang, and alexan-
der m. rush. 2017. id4 by
in proceed-
jointly learning to align and translate.
ings of iclr.

yoon kim, yacine jernite, david sontag, and alexan-
der m. rush. 2016. character-aware neural lan-
guage models. in proceedings of aaai.

tao lei, hrishikesh joshi, regina barzilay, tommi
jaakkola, kateryna tymoshenko, alessandro mos-
chitti, and llu    s m`arquez. 2016. semi-supervised
question retrieval with gated convolutions. in pro-
ceedings of naacl.

wang ling, chris dyer, alan w black, isabel tran-
coso, ramon fermandez, silvio amir, luis marujo,
and tiago luis. 2015. finding function in form:
compositional character models for open vocab-
in proceedings of
ulary word representation.
emnlp.

minh-thang luong and christopher d. manning. 2016.
achieving open vocabulary neural machine transla-
tion with hybrid word-character models. in proceed-
ings of acl.

ankur parikh, oscar t  ackstr  om, dipanjan das, and
jakob uszkoreit. 2016. a decomposable attention
in proceed-
model for natural language id136.
ings of emnlp.

jeffrey pennington, richard socher, and christopher d.
manning. 2014. glove: global vectors for word rep-
resentation. in proceedings of emnlp.

rico sennrich, barry haddow, and alexandra birch.
2016. id4 of rare words with
subword units. in proceedings of acl.

anders s  gaard and yoav goldberg. 2016. deep multi-
task learning with low level tasks supervised at lower
layers. in proceedings of acl.

zhiguo wang, wael hamza, and radu florian. 2017.
bilateral multi-perspective matching for natural lan-
guage sentences. in proceedings of ijcai.

zhiguo wang, haitao mi, and abraham ittycheriah.
2016. sentence similarity learning by lexical decom-
position and composition. in proceedings of col-
ing.

john wieting, mohit bansal, kevin gimpel, and karen
livescu. 2016. charagram: embedding words and
sentences via character id165s. in proceedings of
emnlp.

