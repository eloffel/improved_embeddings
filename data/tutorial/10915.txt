7
1
0
2

 
r
a

m
4

 

 
 
]
l
m

.
t
a
t
s
[
 
 

4
v
1
9
6
5
0

.

3
0
6
1
:
v
i
x
r
a

published as a conference paper at iclr 2017

do deep convolutional nets really need to
be deep and convolutional?

gregor urban1, krzysztof j. geras2, samira ebrahimi kahou3, ozlem aslan4, shengjie wang5,
abdelrahman mohamed6, matthai philipose6, matt richardson6, rich caruana6
1uc irvine, usa
2university of edinburgh, uk
3ecole polytechnique de montreal, ca
4university of alberta, ca
5university of washington, usa
6microsoft research, usa

abstract

yes, they do. this paper provides the    rst empirical demonstration that deep
convolutional models really need to be both deep and convolutional, even when
trained with methods such as distillation that allow small or shallow models of
high accuracy to be trained. although previous research showed that shallow
feed-forward nets sometimes can learn the complex functions previously learned
by deep nets while using the same number of parameters as the deep models they
mimic, in this paper we demonstrate that the same methods cannot be used to train
accurate models on cifar-10 unless the student models contain multiple layers
of convolution. although the student models do not have to be as deep as the
teacher model they mimic, the students need multiple convolutional layers to learn
functions of comparable accuracy as the deep convolutional teacher.

1

introduction

cybenko (1989) proved that a network with a large enough single hidden layer of sigmoid units can
approximate any decision boundary. empirical work, however, suggests that it can be dif   cult to
train shallow nets to be as accurate as deep nets. dauphin and bengio (2013) trained shallow nets
on sift features to classify a large-scale id163 dataset and found that it was dif   cult to train
large, high-accuracy, shallow nets. a study of deep convolutional nets suggests that for vision tasks
deeper models are preferred under a parameter budget (e.g. eigen et al. (2014); he et al. (2015);
simonyan and zisserman (2014); srivastava et al. (2015)). similarly, seide et al. (2011) and geras
et al. (2015) show that deeper models are more accurate than shallow models in speech acoustic
modeling. more recently, romero et al. (2015) showed that it is possible to gain increases in accuracy
in models with few parameters by training deeper, thinner nets (fitnets) to mimic much wider nets.
cohen and shashua (2016); liang and srikant (2016) suggest that the representational ef   ciency of
deep networks scales exponentially with depth, but it is unclear if this applies only to pathological
problems, or is encountered in practice on data sets such as timit and cifar.
ba and caruana (2014), however, demonstrated that shallow nets sometimes can learn the functions
learned by deep nets, even when restricted to the same number of parameters as the deep nets. they
did this by    rst training state-of-the-art deep models, and then training shallow models to mimic
the deep models. surprisingly, and for reasons that are not well understood, the shallow models
learned more accurate functions when trained to mimic the deep models than when trained on the
original data used to train the deep models. in some cases shallow models trained this way were as
accurate as state-of-the-art deep models. but this demonstration was made on the timit speech
recognition benchmark. although their deep teacher models used a convolutional layer, convolution
is less important for timit than it is for other domains such as image classi   cation.
ba and caruana (2014) also presented results on cifar-10 which showed that a shallow model
could learn functions almost as accurate as deep convolutional nets. unfortunately, the results on
cifar-10 are less convincing than those for timit. to train accurate shallow models on cifar-10

1

published as a conference paper at iclr 2017

they had to include at least one convolutional layer in the shallow model, and increased the number
of parameters in the shallow model until it was 30 times larger than the deep teacher model. despite
this, the shallow convolutional student model was several points less accurate than a teacher model
that was itself several points less accurate than state-of-the-art models on cifar-10.
in this paper we show that the methods ba and caruana used to train shallow students to mimic deep
teacher models on timit do not work as well on problems such as cifar-10 where multiple layers
of convolution are required to train accurate teacher models. if the student models have a similar
number of parameters as the deep teacher models, high accuracy can not be achieved without multiple
layers of convolution even when the student models are trained via distillation.
to ensure that the shallow student models are trained as accurately as possible, we use bayesian
optimization to thoroughly explore the space of architectures and learning hyperparameters. although
this combination of distillation and hyperparameter optimization allows us to train the most accurate
shallow models ever trained on cifar-10, the shallow models still are not as accurate as deep
models. our results clearly suggest that deep convolutional nets do, in fact, need to be both deep and
convolutional, even when trained to mimic very accurate models via distillation (hinton et al., 2015).

2 training shallow nets to mimic deeper convolutional nets

in this paper, we revisit the cifar-10 experiments in ba and caruana (2014). unlike in that work,
here we compare shallow models to state-of-the-art deep convolutional models, and restrict the
number of parameters in the shallow student models to be comparable to the number of parameters in
the deep convolutional teacher models. because we anticipated that our results might be different,
we follow their approach closely to eliminate the possibility that the results differ merely because of
changes in methodology. note that the goal of this paper is not to train models that are small or fast
as in bucila et al. (2006), hinton et al. (2015), and romero et al. (2015), but to examine if shallow
models can be as accurate as deep convolutional models given the same parameter budget.
there are many steps required to train shallow student models to be as accurate as possible: train
state-of-the-art deep convolutional teacher models, form an ensemble of the best deep models, collect
and combine their predictions on a large transfer set, and then train carefully optimized shallow
student models to mimic the teacher ensemble. for negative results to be informative, it is important
that each of these steps be performed as well as possible. in this section we describe the experimental
methodology in detail. readers familiar with distillation (model compression), training deep models
on cifar-10, data augmentation, and bayesian hyperparameter optimization may wish to skip to the
empirical results in section 3.

2.1 model compression and distillation

the key idea behind model compression is to train a compact model to approximate the function
learned by another larger, more complex model. bucila et al. (2006) showed how a single neural net
of modest size could be trained to mimic a much larger ensemble. although the small neural nets
contained 1000   fewer parameters, often they were as accurate as the large ensembles they were
trained to mimic.
model compression works by passing unlabeled data through the large, accurate teacher model to
collect the real-valued scores it predicts, and then training a student model to mimic these scores.
hinton et al. (2015) generalized the methods of bucila et al. (2006) and ba and caruana (2014)
by incorporating a parameter to control the relative importance of the soft targets provided by the
teacher model to the hard targets in the original training data, as well as a temperature parameter that
regularizes learning by pushing targets towards the uniform distribution. hinton et al. (2015) also
demonstrated that much of the knowledge passed from the teacher to the student is conveyed as dark
knowledge contained in the relative scores (probabilities) of outputs corresponding to other classes,
as opposed to the scores given to just the output for the one correct class.
surprisingly, distillation often allows smaller and/or shallower models to be trained that are nearly
as accurate as the larger, deeper models they are trained to mimic, yet these same small models are
not as accurate when trained on the 1-hot hard targets in the original training set. the reason for
this is not yet well understood. similar compression and distillation methods have also successfully

2

published as a conference paper at iclr 2017

been used in id103 (e.g. chan et al. (2015); geras et al. (2015); li et al. (2014)) and
id23 parisotto et al. (2016); rusu et al. (2016). romero et al. (2015) showed that
distillation methods can be used to train small students that are more accurate than the teacher models
by making the student models deeper, but thinner, than the teacher model.

2.2 mimic learning via l2 regression on logits

mimic models are not trained with cross-id178 on the ten p values where pk = ezk /(cid:80)

we train shallow mimic nets using data labeled by an ensemble of deep teacher nets trained on the
original 1-hot cifar-10 training data. the deep teacher models are trained in the usual way using
softmax outputs and cross-id178 cost function. following ba and caruana (2014), the student
j ezj output
by the softmax layer from the deep teacher model, but instead are trained on the un-normalized log
id203 values z (the logits) before the softmax activation. training on the logarithms of predicted
probabilities (logits) helps provide the dark knowledge that regularizes students by placing emphasis
on the relationships learned by the teacher model across all of the outputs.
as in ba and caruana (2014), the student is trained as a regression problem given training data
{(x(1), z(1)),...,(x(t ), z(t ))}:

l(w ) =

1
t

||g(x(t); w )     z(t)||2
2,

(1)

(cid:88)

t

where w represents all of the weights in the network, and g(x(t); w ) is the model prediction on the
tth training data sample.

2.3 using a linear bottleneck to speed up training

a shallow net has to have more hidden units in each layer to match the number of parameters in
a deep net. ba and caruana (2014) found that training these wide, shallow mimic models with
id26 was slow, and introduced a linear bottleneck layer between the input and non-linear
layers to speed learning. the bottleneck layer speeds learning by reducing the number of parameters
that must be learned, but does not make the model deeper because the linear terms can be absorbed
back into the non-linear weight matrix after learning. see ba and caruana (2014) for details. to match
their experiments we use linear bottlenecks when training student models with 0 or 1 convolutional
layers, but did not    nd the linear bottlenecks necessary when training student models with more than
1 convolutional layer.

2.4 bayesian hyperparameter optimization

the goal of this work is to determine empirically if shallow nets can be trained to be as accurate as
deep convolutional models using a similar number of parameters in the deep and shallow models. if
we succeed in training a shallow model to be as accurate as a deep convolutional model, this provides
an existence proof that shallow models can represent and learn the complex functions learned by
deep convolutional models. if, however, we are unable to train shallow models to be as accurate as
deep convolutional nets, we might fail only because we did not train the shallow nets well enough.
in all our experiments we employ bayesian hyperparameter optimization using gaussian process
regression to ensure that we thoroughly and objectively explore the hyperparameters that govern
learning. the implementation we use is spearmint (snoek et al., 2012). the hyperparameters we
optimize with bayesian optimization include the initial learning rate, momentum, scaling of the initial
random weights, scaling of the inputs, and terms that determine the width of each of the network   s
layers (i.e. number of convolutional    lters and neurons). more details of the hyperparameter
optimization can be found in sections 2.5, 2.7, 2.8 and in the appendix.

2.5 training data and data augmentation

the cifar-10 (krizhevsky, 2009) data set consists of a set of natural images from 10 different object
classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck. the dataset is a labeled
subset of the 80 million tiny images dataset (torralba et al., 2008) and is divided into 50,000 train and

3

published as a conference paper at iclr 2017

1

1

1+as

1+av

, 1 + as), av     u (

10,000 test images. each image is 32  32 pixels in 3 color channels, yielding input vectors with 3072
dimensions. we prepared the data by subtracting the mean and dividing by the standard deviation
of each image vector. we train all models on a subset of 40,000 images and use the remaining
10,000 images as the validation set for the bayesian optimization. the    nal trained models only
used 80% of the theoretically available training data (as opposed to retraining on all of the data after
hyperparameter optimization).
we employ the hsv-data augmentation technique as described by snoek et al. (2015). thus
we shift hue, saturation and value by uniform random values:    h     u (   dh, dh),    s    
u (   ds, ds),    v     u (   dv, dv). saturation and value values are scaled globally: as    
, 1 + av). the    ve constants dh, ds, dv, as, av are treated
u (
as additional hyperparameters in the bayesian hyperparameter optimization.
all training images are mirrored left-right randomly with a id203 of 0.5. the input images are
further scaled and jittered randomly by cropping windows of size 24  24 up to 32  32 at random
locations and then scaling them back to 32  32. the procedure is as follows: we sample an integer
value s     u (24, 32) and then a pair of integers x, y     u (0, 32     s). the transformed resulting
image is r = fspline,3(i[x : x + s, y : y + s]) with i denoting the original image and fspline,3
denoting the 3rd order spline interpolation function that maps the 2d array back to 32  32 (applied to
the three color channels separately).
all data augmentations for the teacher models are computed on the    y using different random seeds.
for student models trained to mimic the ensemble (see section 2.7 for details of the ensemble teacher
model), we pre-generated 160 epochs worth of randomly augmented training data, evaluated the
ensemble   s predictions (logits) on these samples, and saved all data and predictions to disk. all student
models thus see the same training data in the same order. the parameters for hsv-augmentation in
this case had to be selected beforehand; we chose to use the settings found with the best single model
(dh = 0.06, ds = 0.26, dv = 0.20, as = 0.21, av = 0.13). pre-saving the logits and augmented
data is important to reduce the computational cost at training time, and to ensure that all student
models see the same training data
because augmentation allows us to generate large training sets from the original 50,000 images, we
use augmented data as the transfer set for model compression. no extra unlabeled data is required.

2.6 learning-rate schedule

we train all models using sgd with nesterov momentum. the initial learning rate and momentum
are chosen by bayesian optimization. the learning rate is reduced according to the evolution of the
model   s validation error: it is halved if the validation error does not drop for ten epochs in a row. it is
not reduced within the next eight epochs following a reduction step. training ends if the error did not
drop for 30 epochs in a row or if the learning rate was reduced by a factor of more than 2000 in total.
this schedule provides a way to train the highly varying models in a fair manner (it is not feasible to
optimize all of the parameters that de   ne the learning schedule). it also decreases the time spent to
train each model compared to using a hand-selected overestimate of the number of epochs to train,
thus allowing us to train more models in the hyperparameter search.

2.7 super teacher: an ensemble of 16 deep convolutional cifar-10 models

one limitation of the cifar-10 experiments performed in ba and caruana (2014) is that the teacher
models were not state-of-the-art. the best deep models they trained on cifar-10 had only 88%
accuracy, and the ensemble of deep models they used as a teacher had only 89% accuracy. the
accuracies were not state-of-the-art because they did not use augmentation and because their deepest
models had only three convolutional layers. because our goal is to determine if shallow models can
be as accurate as deep convolutional models, it is important that the deep models we compare to (and
use as teachers) are as accurate as possible.
we train deep neural networks with eight convolutional layers, three intermittent max-pooling layers
and two fully-connected hidden layers. we include the size of these layers in the hyperparameter
optimization, by allowing the    rst two convolutional layers to contain from 32 to 96    lters each, the
next two layers to contain from 64 to 192    lters, and the last four convolutional layers to contain

4

published as a conference paper at iclr 2017

from 128 to 384    lters. the two fully-connected hidden layers can contain from 512 to 1536 neurons.
we parametrize these model-sizes by four scalars (the layers are grouped as 2-2-4) and include the
scalars in the hyperparameter optimization. all models are trained using theano (bastien et al., 2012;
bergstra et al., 2010).
we optimize eighteen hyperparameters overall: initial learning rate on [0.01, 0.05], momentum on
[0.80, 0.91], l2 weight decay on [5    10   5,4    10   4], initialization coef   cient on [0.8, 1.35] which
scales the initial weights of the id98, four separate dropout rates,    ve constants controlling the
hsv data augmentation, and the four scaling constants controlling the networks    layer widths. the
learning rate and momentum are optimized on a log-scale (as opposed to linear scale) by optimizing
the exponent with appropriate bounds, e.g. lr = e   x optimized over x on [3.0, 4.6]. see the
appendix for more details about hyperparameter optimization.
we trained 129 deep id98 models with spearmint. the best model obtained an accuracy of 92.78%;
the    fth best achieved 92.67%. see table 1 for the sizes and architectures of the three best models.
we are able to construct a more accurate model on cifar-10 by forming an ensemble of multiple
deep convolutional neural nets, each trained with different hyperparameters, and each seeing slightly
different training data (as the augmentation parameters vary). we experimented with a number of
ensembles of the many deep convnets we trained, using accuracy on the validation set to select the
best combination. the    nal ensemble contained 16 deep convnets and had an accuracy of 94.0% on
the validation set, and 93.8% on the    nal test set. we believe this is among the top published results
for deep learning on cifar-10. the ensemble averages the logits predicted by each model before
the softmax layers.
we used this very accurate ensemble model as the teacher model to label the data used to train the
shallower student nets. as described in section 2.2, the logits (the scores just prior to the    nal softmax
layer) from each of the id98 teachers in the ensemble model are averaged for each class, and the
average logits are used as    nal regression targets to train the shallower student neural nets.

2.8 training shallow student models to mimic an ensemble of deep

convolutional models

we trained student mimic nets with 1, 3.161, 10 and 31.6 million trainable parameters on the
pre-computed augmented training data (section 2.5) that was re-labeled by the teacher ensemble
(section 2.7). for each of the four student sizes we trained shallow fully-connected student mlps
containing 1, 2, 3, 4, or 5 layers of non-linear units (relu), and student id98s with 1, 2, 3 or 4
convolutional layers. the convolutional student models also contain one fully-connected relu layer.
models with zero or only one convolutional layer contain an additional linear bottleneck layer to
speed up learning (cf. section 2.3). we did not need to use a bottleneck to speed up learning for the
deeper models as the number of learnable parameters is naturally reduced by the max-pooling layers.
the student id98s use max-pooling and bayesian optimization controls the number of convolutional
   lters and hidden units in each layer. the hyperparameters we optimized in the student models are:
initial learning rate, momentum, scaling of the initially randomly distributed learnable parameters,
scaling of all pixel values of the input, and the scale factors that control the width of all hidden
and convolutional layers in the model. weights are initialized as in glorot and bengio (2010). we
intentionally do not optimize and do not make use of weight decay and dropout when training student
models because preliminary experiments showed that these consistently reduced the accuracy of
student models by several percent. please refer to the appendix for more details on the individual
architectures and hyperparameter ranges.

3 empirical results

table 1 summarizes results after bayesian hyperparameter optimization for models trained on the
original 0/1 hard cifar-10 labels. all of these models use weight decay and are trained with the
dropout hyperparameters included in the bayesian optimization. the table shows the accuracy of
the best three deep convolutional models we could train on cifar-10, as well as the accuracy of

13.16     sqrt(10) falls halfway between 1 and 10 on log scale.

5

published as a conference paper at iclr 2017

table 1: accuracy on cifar-10 of shallow and deep models trained on the original 0/1 hard class
labels using bayesian optimization with dropout and weight decay. key: c = convolution layer; mp
= max-pooling layer; fc = fully-connected layer; lfc = linear bottleneck layer; exponents indicate
repetitions of a layer. the last two models (*) are numbers reported by ba and caruana (2014). the
models with 1-4 convolutional layers at the top of the table are included for comparison with student
models of similar architecture in table 2 . all of the student models in table 2 with 1, 2, 3, and 4
convolutional layers are more accurate than their counterparts in this table that are trained on the
original 0/1 hard targets     as expected distillation yields shallow models of higher accuracy than
shallow models trained on the original training data.

model
1 conv. layer
2 conv. layer
3 conv. layer
4 conv. layer
teacher id98 1st
teacher id98 2nd
teacher id98 3rd
ensemble of 16 id98s
teacher id98 (*)
ensemble, 4 id98s (*)

architecture
c-mp-lfc-fc
c-mp-c-mp-fc

c-mp-c-mp-c-mp-fc
c-mp-c-c-mp-c-mp-fc

76c2-mp-126c2-mp-148c4-mp-1200fc2
96c2-mp-171c2-mp-128c4-mp-512fc2
54c2-mp-158c2-mp-189c4-mp-1044fc2

c2-mp-c2-mp-c4-mp-fc2

128c-mp-128c-mp-128c-mp-1k fc
128c-mp-128c-mp-128c-mp-1k fc

# parameters accuracy

10m
10m
10m
10m
5.3m
2.5m
5.8m
83.4m
2.1m
8.6m

84.6%
88.9%
91.2%
91.75%
92.78%
92.77%
92.67%
93.8%
88.0%
89.0%

table 2: comparison of student models with varying number of convolutional layers trained to mimic
the ensemble of 16 deep convolutional cifar-10 models in table 1 . the best performing student
models have 3     4 convolutional layers and 10m     31.6m parameters. the student models in this
table are more accurate than the models of the same architecture in table 1 that were trained on the
original 0/1 hard targets     shallow models trained with distillation are more accurate than shallow
models trained on 0/1 hard targets. the student model trained by ba and caruana (2014) is shown in
the last line for comparison; it is less accurate and much larger than the student models trained here
that also have 1 convolutional layer.

bottleneck, 1 hidden layer
2 hidden layers
3 hidden layers
4 hidden layers
5 hidden layers
1 conv. layer, 1 max-pool, bottleneck
2 conv. layers, 2 max-pool
3 conv. layers, 3 max-pool
4 conv. layers, 3 max-pool
snn-eid98-mimic-30k 128c-p-1200l-30k
trained on ensemble (ba and caruana, 2014)

1 m 3.16 m 10 m 31.6 m 70 m
65.8% 68.2% 69.5% 70.2%
66.2% 70.9% 73.4% 74.3%
66.8% 71.0% 73.0% 73.9%
66.7% 69.5% 71.6% 72.0%
66.4% 70.0% 71.4% 71.5%
84.5% 86.3% 87.3% 87.7%
87.9% 89.3% 90.0% 90.3%
90.7% 91.6% 91.9% 92.3%
91.3% 91.8% 92.6% 92.6%

   
   
   
   
   
   
   
   
   

   

   

   

   

85.8%

6

published as a conference paper at iclr 2017

the ensemble of 16 deep id98s. for comparison, the accuracy of the ensemble trained by ba and
caruana (2014)) is included at the bottom of the table.
table 2 summarizes the results after bayesian
hyperparameter optimization for student mod-
els of different depths and number of parameters
trained on soft targets (average logits) to mimic
the teacher ensemble of 16 deep id98s. for
comparison, the student model trained by ba
and caruana (2014) also is shown.
the    rst four rows in table 1 show the accuracy
of convolutional models with 10 million param-
eters and 1, 2, 3, and 4 convolutional layers.
the accuracies of these same architectures with
1m, 3.16m, 10m, and 31.6m parameters when
trained as students on the soft targets predicted
by the teacher ensemble are shown in table 2.
comparing the accuracies of the models with 10
million parameters in both tables, we see that
training student models to mimic the ensemble
leads to signi   cantly better accuracy in every
case. the gains are more pronounced for shal-
lower models, most likely because their learn-
able internal representations do not naturally
lead to good generalization in this task when
trained on the 0/1 hard targets: the difference
in accuracy for models with one convolutional
layer is 2.7% (87.3% vs. 84.6%) and only 0.8%
(92.6% vs. 91.8%) for models with four convo-
lutional layers.
figure 1 summarizes the results in table 2 for
student models of different depth, number of
convolutional layers, and number of parame-
ters when trained to mimic the ensemble teacher
model. student models trained on the ensemble
logits are able to achieve accuracies previously
unseen on cifar-10 for models with so few
layers. also, it is clear that there is a huge gap
between the convolutional student models at the
top of the    gure, and the non-convolutional stu-
dent models at the bottom of the    gure: the most
accurate student mlp has accuracy less than
75%, while the least accurate convolutional stu-
dent model with the same number of parameters
but only one convolutional layer has accuracy
above 87%. and the accuracy of the convolu-
tional student models increases further as more
layers of convolution are added. interestingly,
the most accurate student mlps with no convo-
lutional layers have only 2 or 3 hidden layers;
the student mlps with 4 or 5 hidden layers are
not as accurate.
comparing the student mlp with only one hidden layer (bottom of the graph) to the student id98
with 1 convolutional layer clearly suggests that convolution is critical for this problem even when
models are trained via distillation, and that it is very unlikely that a shallow non-convolutional model
with 100 million parameters or less could ever achieve accuracy comparable to a convolutional model.
it appears that if convolution is critical for teacher models trained on the original 0/1 hard targets, it

figure 1: accuracy of student models with differ-
ent architectures trained to mimic the cifar10
ensemble. the average performance of the    ve
best models of each hyperparameter-optimization
experiment is shown, together with dashed lines
indicating the accuracy of the best and the    fth
best model from each setting. the short horizontal
lines at 10m parameters are the accuracy of mod-
els trained without compression on the original 0/1
hard targets.

7

   
   :2-07 41 !,7,209078  2    438(   
   
   
 ..:7,. 90,. 07 03802- 0.4257088 43  ,523451         .43;4 :9 43,   ,5               .4257088 43  ,5       .43;4 :9 43,   , 07       .43;4 :9 43,   , 078       .43;4 :9 43,   , 078       .43;4 :9 43,   , 078  !      //03  , 07  !      //03  , 07  !      //03  , 07  !      //03  , 07  !      //03  , 07published as a conference paper at iclr 2017

is likely to be critical for student models trained to mimic these teacher models. adding depth to the
student mlps without adding convolution does not signi   cantly close this    convolutional gap   .
furthermore, comparing student id98s with 1, 2, 3, and 4 convolutional layers, it is clear that id98
students bene   t from multiple convolutional layers. although the students do not need as many
layers as teacher models trained on the original 0/1 hard targets, accuracy increases signi   cantly as
multiple convolutional layers are added to the model. for example, the best student with only one
convolutional layer has 87.7% accuracy, while the student with the same number of parameters (31m)
and 4 convolutional layers has 92.6% accuracy.
figure 1 includes short horizontal lines at 10m parameters indicating the accuracy of non-student
models trained on the original 0/1 hard targets instead of on the soft targets. this    compression
gap    is largest for shallower models, and as expected disappears as the student models become
architecturally more similar to the teacher models with multiple layers of convolution. the bene   ts of
distillation are most signi   cant for shallow models, yielding an increase in accuracy of 3% or more.
one pattern that is clear in the graph is that all student models bene   t when the number of parameters
increases from 1 million to 31 million parameters. it is interesting to note, however, that the largest
student (31m) with a one convolutional layer is less accurate than the smallest student (1m) with two
convolutional layers, further demonstrating the value of depth in convolutional models.
in summary, depth-constrained student models trained to mimic a high-accuracy ensemble of deep
convolutional models perform better than similar models trained on the original hard targets (the
   compression    gaps in figure 1), student models need at least 3-4 convolutional layers to have high
accuracy on cifar-10, shallow students with no convolutional layers perform poorly on cifar-10,
and student models need at least 3-10m parameters to perform well. we are not able to compress
deep convolutional models to shallow student models without signi   cant loss of accuracy.
we are currently running a reduced set of experiments on id163, though the chances of shallow
models performing well on a more challenging problem such as id163 appear to be slim.

4 discussion
although we are not able to train shallow models to be as accurate as deep models, the models trained
via distillation are the most accurate models of their architecture ever trained on cifar-10. for
example, the best single-layer fully-connected mlp (no convolution) we trained achieved an accuracy
of 70.2%. we believe this to be the most accurate shallow mlp ever reported for cifar-10 (in
comparison to 63.1% achieved by le et al. (2013), 63.9% by memisevic et al. (2015) and 64.3% by
geras and sutton (2015)). although this model cannot compete with convolutional models, clearly
distillation helps when training models that are limited by architecture and/or number of parameters.
similarly, the student models we trained with 1, 2, 3, and 4 convolutional layers are, we believe,
the most accurate convnets of those depths reported in the literature. for example, the ensemble
teacher model in ba and caruana (2014) was an ensemble of four id98s, each of which had 3
convolutional layers, but only achieved 89% accuracy, whereas the single student id98s we train via
distillation achieve accuracies above 90% with only 2 convolutional layers, and above 92% with 3
convolutional layers. the only other work we are aware of that achieves comparable high accuracy
with non-convolutional mlps is recent work by lin et al. (2016). they train multi-layer z-lin
networks, and use a powerful form of data augmentation based on deformations that we did not use.
interestingly, we noticed that mimic networks perform consistently worse when trained using dropout.
this surprised us, and suggests that training student models on the soft-targets from a teacher provides
signi   cant id173 for the student models obviating the need for extra id173 methods
such as dropout. this is consistent with the observation made by ba and caruana (2014) that student
mimic models did not seem to over   t. hinton et al. (2015) claim that soft targets convey more
information per sample than boolean hard targets. the also suggest that the    dark knowledge    in the
soft targets for other classes further helped id173, and that early stopping was unnecessary.
romero et al. (2015) extend distillation by using the intermediate representations learned by the
teacher as hints to guide training deep students, and teacher con   dences further help id173
by providing a measure of sample    simplicity    to the student, akin to curriculum learning. in other
work, pereyra et al. (2017) suggest that the soft targets provided by a teacher provide a form of
con   dence penalty that penalizes low id178 distributions and label smoothing, both of which
improve id173 by maintaining a reasonable ratio between the logits of incorrect classes.

8

published as a conference paper at iclr 2017

zhang et al. (2016) question the traditional view of id173 in deep models. although they do
not discuss distillation, they suggest that in deep learning traditional function approximation appears
to be deeply intertwined with massive memorization. the multiple soft targets used to train student
models have a high information density (hinton et al., 2015) and thus provide id173 by
reducing the impact of brute-force memorization.

5 conclusions

we train shallow nets with and without convolution to mimic state-of-the-art deep convolutional
nets. if one controls for the number of learnable parameters, nets containing a single fully-connected
non-linear layer and no convolutional layers are not able to learn functions as accurate as deeper
convolutional models. this result is consistent with those reported in ba and caruana (2014).
however, we also    nd that shallow nets that contain only 1-2 convolutional layers also are unable
to achieve accuracy comparable to deeper models if the same number of parameters are used in
the shallow and deep models. deep convolutional nets are signi   cantly more accurate than shallow
convolutional models, given the same parameter budget. we do, however, see evidence that model
compression allows accurate models to be trained that are shallower and have fewer convolutional
layers than the deep convolutional architectures needed to learn high-accuracy models from the
original 1-hot hard-target training data. the question remains why extra layers are required to train
accurate models from the original training data.

references
jimmy ba and rich caruana. do deep nets really need to be deep? in nips, 2014.

fr  d  ric bastien, pascal lamblin, razvan pascanu, james bergstra, ian j. goodfellow, arnaud bergeron,
nicolas bouchard, and yoshua bengio. theano: new features and speed improvements. deep learning and
unsupervised id171 nips 2012 workshop, 2012.

james bergstra, olivier breuleux, fr  d  ric bastien, pascal lamblin, razvan pascanu, guillaume desjardins,
joseph turian, david warde-farley, and yoshua bengio. theano: a cpu and gpu math expression compiler.
in scipy, 2010.

cristian bucila, rich caruana, and alexandru niculescu-mizil. model compression. in kdd, 2006.

william chan, nan rosemary ke, and ian laner. transferring knowledge from a id56 to a dnn.

arxiv:1504.01483, 2015.

nadav cohen and amnon shashua. convolutional recti   er networks as generalized tensor decompositions.

arxiv preprint arxiv:1603.00162, 2016.

george cybenko. approximation by superpositions of a sigmoidal function. mathematics of control, signals

and systems, 2(4):303   314, 1989.

yann n. dauphin and yoshua bengio. big neural networks waste capacity. arxiv:1301.3583, 2013.

david eigen, jason rolfe, rob fergus, and yann lecun. understanding deep architectures using a recursive

convolutional network. in iclr (workshop track), 2014.

krzysztof j. geras and charles sutton. scheduled denoising autoencoders. in iclr, 2015.

krzysztof j. geras, abdel-rahman mohamed, rich caruana, gregor urban, shengjie wang, ozlem aslan,
matthai philipose, matthew richardson, and charles sutton. blending lstms into id98s. arxiv:1511.06433,
2015.

xavier glorot and yoshua bengio. understanding the dif   culty of training deep feedforward neural networks.

in aistats, 2010.

kaiming he, xiangyu zhang, shaoqing ren, and jian sun. deep residual learning for image recognition.

arxiv:1512.03385, 2015.

geoffrey hinton, oriol vinyals, and jeff dean. distilling the knowledge in a neural network. arxiv:1503.02531,

2015.

alex krizhevsky. learning multiple layers of features from tiny images, 2009.

9

published as a conference paper at iclr 2017

quoc le, tam  s sarl  s, and alexander smola. fastfood-computing hilbert space expansions in loglinear time.

in icml, 2013.

jinyu li, rui zhao, jui-ting huang, and yifan gong. learning small-size dnn with output-distribution-based

criteria. in interspeech, 2014.

shiyu liang and r srikant. why deep neural networks? arxiv preprint arxiv:1610.04161, 2016.

zhouhan lin, roland memisevic, shaoqing ren, and kishore konda. how far can we go without convolution:

improving fully-connected networks. arxiv:1511.02580v1, 2016.

roland memisevic, kishore konda, and david krueger. zero-bias autoencoders and the bene   ts of co-adapting

features. in iclr, 2015.

emilio parisotto, jimmy lei ba, and ruslan salakhutdinov. actor-mimic: deep multitask and transfer reinforce-

ment learning. in iclr, 2016.

gabriel pereyra, george tucker, jan chorowski, lukasz kaiser, and geoffrey hinton. regularizing neural

networks by penalizing output distributions. iclr, 2017.

adriana romero, ballas nicolas, samira ebrahimi kahou, antoine chassang, carlo gatta, and yoshua bengio.

fitnets: hints for thin deep nets. iclr, 2015.

andrei a. rusu, sergio gomez colmenarejo,   aglar g  l  ehre, guillaume desjardins, james kirkpatrick,
razvan pascanu, volodymyr mnih, koray kavukcuoglu, and raia hadsell. policy distillation. in iclr, 2016.

frank seide, gang li, and dong yu. conversational speech transcription using context-dependent deep neural

networks. in interspeech, 2011.

karen simonyan and andrew zisserman. very deep convolutional networks for large-scale image recognition.

in iclr, 2014.

jasper snoek, hugo larochelle, and ryan p adams. practical bayesian optimization of machine learning

algorithms. nips, 2012.

jasper snoek, oren rippel, kevin swersky, ryan kiros, nadathur satish, narayanan sundaram, md patwary,
mostofa ali, ryan p adams, et al. scalable bayesian optimization using deep neural networks. in icml,
2015.

rupesh k srivastava, klaus greff, and juergen schmidhuber. training very deep networks. in nips, 2015.

antonio torralba, robert fergus, and william t. freeman. 80 million tiny images: a large data set for

nonparametric object and scene recognition. tpami, 30(11), 2008.

chiyuan zhang, samy bengio, moritz hardt, benjamin recht, and oriol vinyals. understanding deep learning

requires rethinking generalization. arxiv preprint arxiv:1611.03530, 2016.

10

published as a conference paper at iclr 2017

6 appendix

6.1 details of training the teacher models

weights of trained nets are initialized as in glorot and bengio (2010). the models trained in section 2.7
contain eight convolutional layers organized into three groups (2-2-4) and two fully-connected hidden layers.
the bayesian hyperparameter optimization controls four constants c1, c2, c3, h1 all in the range [0, 1] that
are then linearly transformed to the number of    lters/neurons in each layer. the hyperparameters for which
ranges were not shown in section 2.7 are: the four separate dropout rates (doc1, doc2, doc3, dof) and
the    ve constants dh, ds, dv, as, av controlling the hsv data augmentation. the ranges we selected are
doc1     [0.1, 0.3], doc2     [0.25, 0.35], doc3     [0.3, 0.44], dof 1     [0.2, 0.65], dof 2     [0.2, 0.65], dh    
[0.03, 0.11], ds     [0.2, 0.3], dv     [0.0, 0.2], as     [0.2, 0.3], av     [0.03, 0.2], partly guided by snoek et al.
(2015) and visual inspection of the resulting augmentations.
the number of    lters and hidden units for the models have the following bounds:
1 conv. layer: 50 - 500    lters, 200 - 2000 hidden units, number of units in bottleneck is the dependent variable.
2 conv. layers: 50 - 500    lters, 100 - 400    lters, number of hidden units is the dependent variable.
3 conv. layers: 50 - 500    lters (layer 1), 100 - 300    lters (layers 2-3), # of hidden units is dependent the variable.
4 conv. layers: 50 - 300    lters (layers 1-2), 100 - 300    lters (layers 3-4), # of hidden units is the dependent
variable.
all convolutional    lters in the model are sized 3  3, max-pooling is applied over windows of 2  2 and we use
relu units throughout all our models. we apply dropout after each max-pooling layer with the three rates
doc1, doc2, doc3 and after each of the two fully-connected layers with the same rate dof.

table 3: optimization bounds for student models. (models trained on 0/1 hard targets were described
in sections 6.1 and 6.2.) abbreviations: fc (fully-connected layer, relu), c (convolutional, relu),
linear (fully-connected bottleneck layer, linear activation function), dependent (dependent variable,
chosen s.t. parameter budget is met).

1st layer

2nd layer

3rd layer

4th layer

5th layer

no conv. layer (1m)
no conv. layer (3.1m)
no conv. layer (10m)
no conv. layer (31m)
1 conv. layer (1m)
1 conv. layer (3.1m)
1 conv. layer (10m)
1 conv. layer (31m)
2 conv. layers (1m)
2 conv. layers (3.1m)
2 conv. layers (10m)
2 conv. layers (31m)
3 conv. layers (1m)
3 conv. layers (3.1m)
3 conv. layers (10m)
3 conv. layers (31m)
4 conv. layers (1m)
4 conv. layers (3.1m)
4 conv. layers (10m)
4 conv. layers (31m)

500 - 5000 (fc)
1000 - 20000 (fc)
5000 - 30000 (fc)
5000 - 45000 (fc)

40 - 150 (c)
50 - 300 (c)
50 - 450 (c)
200 - 600 (c)
20 - 120 (c)
50 - 250 (c)
50 - 350 (c)
50 - 800 (c)
20 - 110 (c)
40 - 200 (c)
50 - 350 (c)
50 - 650 (c)
25 - 100 (c)
50 - 150 (c)
50 - 300 (c)
50 - 500 (c)

dependent (linear)
dependent (linear)
dependent (linear)
dependent (linear)
dependent (linear)
dependent (linear)
dependent (linear)
dependent (linear)

20 - 120 (c)
20 - 120 (c)
20 - 120 (c)
20 - 120 (c)
20 - 110 (c)
40 - 200 (c)
50 - 350 (c)
50 - 650 (c)
25 - 100 (c)
50 - 150 (c)
50 - 300 (c)
50 - 500 (c)

200 - 1600 (fc)
100 - 4000 (fc)
500 - 20000 (fc)
1000 - 4100 (fc)
dependent (fc)
dependent (fc)
dependent (fc)
dependent (fc)
20 - 110 (c)
40 - 200 (c)
50 - 350 (c)
50 - 650 (c)
25 - 100 (c)
50 - 200 (c)
50 - 350 (c)
50 - 650 (c)

dependent (fc)
dependent (fc)
dependent (fc)
dependent (fc)
25 - 100 (c)
50 - 200 (c)
50 - 350 (c)
50 - 650 (c)

dependent (fc)
dependent (fc)
dependent (fc)
dependent (fc)

6.2 details of training models of various depths on cifar-10 hard 0/1 labels

models in the    rst four rows in table 1 are trained similarly to those in section 6.1, and are architecturally
equivalent to the four convolutional student models shown in table 2 with 10 million parameters. the following
hyperparameters are optimized: initial learning rate [0.0015, 0.025] (optimized on a log scale), momentum
[0.68, 0.97] (optimized on a log scale), constants c1, c2     [0, 1] that control the number of    lters or neurons
in different layers, and up to four different dropout rates doc1     [0.05, 0.4], doc2     [0.1, 0.6], doc3    
[0.1, 0.7], dof 1     [0.1, 0.7] for the different layers. weight decay was set to 2    10   4 and we used the same
data augmentation settings as for the student models. we use 5  5 convolutional    lters, one nonlinear hidden
layer in each model and each max-pooling operation is followed by dropout with a separately optimized rate.
we use 2  2 max-pooling except in the model with only one convolutional layer where we apply 3  3 pooling as
this seemed to boost performance and reduces the number of parameters.

11

published as a conference paper at iclr 2017

6.3 details of training student models of various depths on ensemble labels

our student models have the same architecture as models in section 6.2. the model without convolutional layers
consists of one linear layer that acts as a bottleneck followed by a hidden layer of relu units. the following
hyperparameters are optimized: initial learning rate [0.0013, 0.016] (optimized on a log scale), momentum
[0.68, 0.97] (optimized on a log scale), input-scale     [0.8, 1.25], global initialization scale (after initialization)
    [0.4, 2.0], layer-width constants c1, c2     [0, 1] that control the number of    lters or neurons. the exact ranges
for the number of    lters and implicitly resulting number of hidden units was chosen for all twenty optimization
experiments independently, as architectures, number of units and number of parameters strongly interact.
for the non-convolutional models we chose a slightly different hyper-parameterization. given that all layers (in
models with    two layers    or more) are nonlinear and fully connected we treat all of them similarly from the
hyperparameter-optimizer   s point of view. in order to smoothly enforce the parameter budgets without rejecting
any samples from the bayesian optimizer we instead optimize the ratios of hidden units in each layer (numbers
between 0 and 1), and then re-normalize and scale them to the    nal number of neurons in each layer to match
the target parameter budget.

12

published as a conference paper at iclr 2017

figure 2 is similar to 1 but includes preliminary re-
sults from experiments for models with 100m param-
eters. we are also running experiments with 300m
parameters. unfortunately, bayesian optimization
on models with 100m and 300m parameters is even
more expensive than for the other points in the graph.
as expected, adding capacity to the convolutional
students (top of the    gure) modestly increases their
accuracy. preliminary results for the mlps however
(too preliminary to include in the graph) may not
show the same increase in accuracy with increasing
model size. models with two or three hidden layers
may bene   t from adding capacity to each layer, but
we have yet to see any bene   t from adding capacity
to the mlps with four or    ve hidden layers.

figure 2: see    gure 1.

13

   
   

 :2-07 41 !,7,209078  2    438(   
   
   
 ..:7,. 90,. 07 03802- 0.4257088 43  ,5         .43;4 :9 43,   ,5               .4257088 43  ,5       .43;4 :9 43,   , 07       .43;4 :9 43,   , 078       .43;4 :9 43,   , 078       .43;4 :9 43,   , 078  !      //03  , 07  !      //03  , 078  !      //03  , 078  !      //03  , 078  !      //03  , 078