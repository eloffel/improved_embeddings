improving id4 with conditional sequence

generative adversarial nets

zhen yang1,2, wei chen1   , feng wang1,2, bo xu1

1institute of automation, chinese academy of sciences

{yangzhen2014, wei.chen.media, feng.wang, xubo}@ia.ac.cn

2university of chinese academy of sciences

8
1
0
2

 
r
p
a
8

 

 
 
]
l
c
.
s
c
[
 
 

4
v
7
8
8
4
0

.

3
0
7
1
:
v
i
x
r
a

abstract

this paper proposes an approach for apply-
ing gans to id4. we build a conditional se-
quence generative adversarial net which com-
prises of two adversarial sub models, a gen-
erator and a discriminator. the generator
aims to generate sentences which are hard to
be discriminated from human-translated sen-
tences ( i.e., the golden target sentences); and
the discriminator makes efforts to discrim-
inate the machine-generated sentences from
human-translated ones. the two sub models
play a mini-max game and achieve the win-
win situation when they reach a nash equi-
librium. additionally, the static sentence-level
id7 is utilized as the reinforced objective
for the generator, which biases the generation
towards high id7 points. during training,
both the dynamic discriminator and the static
id7 objective are employed to evaluate the
generated sentences and feedback the evalu-
ations to guide the learning of the generator.
experimental results show that the proposed
model consistently outperforms the traditional
id56search and the newly emerged state-of-
the-art transformer on english-german and
chinese-english translation tasks.

1

introduction

id4 (kalchbrenner and
blunsom, 2013; sutskever et al., 2014; cho et al.,
2014; bahdanau et al., 2014) which directly lever-
ages a single neural network to transform the
source sentence into the target sentence, has drawn
more and more attention in both academia and in-
dustry (shen et al., 2015; wu et al., 2016; john-
son et al., 2016; gehring et al., 2017; vaswani
et al., 2017). this end-to-end id4 typically
consists of two sub neural networks. the en-
coder network reads and encodes the source sen-
tence into the context vector representation; and
the decoder network generates the target sentence

word by word based on the context vector. to
dynamically generate a context vector for a tar-
get word being generated, the attention mecha-
nism which enables the model to focus on the rel-
evant words in the source-side sentence is usu-
ally deployed. under the encoder-decoder frame-
work, many variants of the model structure, such
as convolutional neural network (id98) and re-
current neural network (id56) are proposed (bah-
danau et al., 2014; gehring et al., 2017). recently,
(gehring et al., 2017) propose the transformer,
the    rst sequence transduction model based en-
tirely on attention, achieving state-of-the-art per-
formance on the english-german and english-
french translation tasks. despite its success, the
transformer, similar to traditional id4 models,
is still optimized to maximize the likelihood es-
timation of the ground word (id113) at each time
step. such an objective poses a hidden danger to
id4 models. that is, the model may generate
the best candidate word for the current time step
yet a bad component of the whole sentence in the
long run. minimum risk training (mrt) (shen
et al., 2015) is proposed to alleviate such a lim-
itation by adopting the sequence level objective,
i.e., the sentence-level id7, for traditional id4
models. yet somewhat improved, this objective
still does not guarantee the translation results to
be natural and suf   cient. since the id7 point is
computed as the geometric mean of the modi   ed
id165 precisions (papineni et al., 2002), almost
all of the existing objectives essentially train id4
models to generate sentences with id165 preci-
sions as high as possible (id113 can be viewed to
generate sentences with high 1-gram precisions).
while id165 precisions largely tell the good sen-
tence apart from the bad one, it is widely acknowl-
edged that higher id165 precisions do not guar-
antee better sentences (callison-burch and os-
borne, 2006; chatterjee et al., 2007). addition-

ally, the manually de   ned objective, i.e., the n-
gram precision, is unable to cover all crucial as-
pects of the data distribution and id4 models
may be trained to generate suboptimal sentences
(luc et al., 2016)1.

in this paper, to address the limitation men-
tioned above, we borrow the idea of generative ad-
versarial training from id161 (goodfel-
low et al., 2014; denton et al., 2015) to directly
train the id4 model generating sentences which
are hard to be discriminated from human transla-
tions. the motivation behind is that while we can
not manually de   ne the data distribution of golden
sentences comprehensively, we are able to utilize
a discriminative network to learn automatically
what the golden sentences look like. following
this motivation, we build a conditional sequence
generative adversarial net where we jointly train
two sub adversarial models: a generator gener-
ates the target-language sentence based on the in-
put source-language sentence; and a discrimina-
tor, conditioned on the source-language sentence,
predicts the id203 of the target-language sen-
tence being a human-generated one. during the
training process, the generator aims to fool the
discriminator into believing that its output is a
human-generated sentence, and the discriminator
makes efforts not to be fooled by improving its
ability to distinguish the machine-generated sen-
tence from the human-generated one. this kind
of adversarial training achieves a win-win situa-
tion when the generator and discriminator reach a
nash equilibrium (zhao et al., 2016; arora et al.,
2017; guimaraes et al., 2017). besides generat-
ing the desired distribution, we also want to di-
rectly guide the generator with a static and spe-
ci   c objective, such as generating sentences with
high id7 points. to this end, the smoothed
sentence-level id7 (nakov et al., 2012) is uti-
lized as the reinforced objective for the generator.
during training, we employ both the dynamic dis-
criminator and the static id7 objective to evalu-
ate the generated sentences and feedback the eval-
uations to guide the learning of the generator. in
summary, we mainly make the following contribu-
tions:

    to the best of our knowledge,

this work
is among the    rst endeavors to introduce
the generative adversarial training into id4.
we directly train the id4 model to gener-

1this paper has been accepted by naacl-hlt2018.

ate sentences which are hard to be discrimi-
nated from human translations. the proposed
model can be applied to any end-to-end id4
systems.

    we

conduct

extensive

experiments on
english-german and chinese-english trans-
lation tasks and we test two different id4
models,
the traditional id56search (bah-
danau et al., 2014) and the state-of-the-art
transformer. experimental results show that
the proposed approach consistently achieves
great success.

    last but not least, we propose the smoothed
sentence-level id7 as the static and spe-
ci   c objective for the generator which biases
the generation towards achieving high id7
points. we show that the proposed approach
is a weighted combination of the naive gan
and mrt.

2 background and related work
2.1 id56search and transformer
the id56search is the traditional id4 model
which has been widely explored. we follow the
de facto standard implementation by (bahdanau
et al., 2014). the encoder is a bidirectional gated
recurrent units that encodes the input sequence
x = (x1, . . . , xm) and calculates the forward se-
      
      
hm), and a back-
quence of hidden states (
      
      
h1, . . . ,
ward sequence of hidden states (
hm). the
h1, . . . ,
   nal annotation vector hj is calculated by con-
      
hj. the decoder is a recur-
catenating
rent neural network that predicts a target sequence
y = (y1, . . . , yn). each word yi is predicted on a
recurrent hidden state si, the previously predicted
word yi   1 and a context vector ci. the ci is com-
puted as a weighted sum of the encoded annota-
tions hj. the weight aij of each annotation hj
is computed by the attention mechanism, which
models the alignment between yi and xj.

      
hj and

recently

proposed

the transformer,

by
(vaswani et al., 2017), achieves state-of-the-art
results on both wmt2014 english-german and
wmt2014 english-french translation tasks. the
encoder of transformer is composed of a stack
of six identical layers. each layer consists of a
multi-head self-attention and a simple position-
wise fully connected feed-forward network.
the decoder is also composed of a stack of six

identical layers. in addition to the two sub-layers
in each encoder layer, the decoder inserts a third
sub-layer, which performs multi-head attention
over the output of the encoder stack. the trans-
former can be trained signi   cantly faster than
architectures based on recurrent or convolutional
layers since it allows for signi   cantly more
parallelization.

2.2 generative adversarial nets

generative adversarial network, has enjoyed great
success in id161 and has been widely
applied to image generation (zhu et al., 2017;
radford et al., 2015). the conditional generative
adversarial nets (gauthier, 2014) apply an exten-
sion of generative adversarial network to a con-
ditional setting, which enables the networks to
condition on some arbitrary external data. some
recent works have begun to apply the genera-
tive adversarial training into the nlp area: (chen
et al., 2016) apply the idea of generative adversar-
ial training to id31 and (zhang et al.,
2017) use the idea to id20 tasks. for
sequence generation problem, (yu et al., 2016)
leverage policy gradient id23 to
back-propagate the reward from the discrimina-
tor, showing presentable results for poem gener-
ation, speech language generation and music gen-
eration. similarly, (zhang et al., 2016) generate
the text from random noise via adversarial train-
ing. a striking difference from the works men-
tioned above is that, our work is in the conditional
setting where the target-language sentence is gen-
erated conditioned on the source-language one. in
parallel to our work, (li et al., 2017) propose a
similar conditional sequence generative adversar-
ial training for dialogue generation. they use a
hierarchical long-short term memory (lstm) ar-
chitecture for the discriminator. in contrast to their
approach, we apply the id98-based discriminator
for the machine translation task. furthermore, we
propose to utilize the sentence-level id7 as the
speci   c objective for the generator. detailed train-
ing strategies for the proposed model and exten-
sive quantitative results are reported. we noticed
that (wu et al., 2017) is exploring the potential of
gan in id4 too. there are some differences
in training strategies and experimental settings be-
tween (wu et al., 2017) and this work. and the
most signi   cant difference is that we propose a
novel id7-reinforced gan for id4.

3 the approach
3.1 model overview
in this section, we describe the architecture of
the proposed id7 reinforced conditional se-
quence generative adversarial net (referred to as
br-id19an) in detail. the sentence generation
process is viewed as a sequence of actions that are
taken according to a policy regulated by the gen-
erator. in this work, we take the policy gradient
training strategies following (yu et al., 2016). the
whole architecture of the proposed model is de-
picted in    gure 1. the model mainly consists of
three sub modules:

generator based on the source-language sen-
tences, the generator g aims to generate target-
language sentences indistinguishable from human
translations.

discriminator the discriminator d, condi-
tioned on the source-language sentences, tries to
distinguish the machine-generated sentences from
human translations. d can be viewed as a dynamic
objective since it is updated synchronously with g.
id7 objective the sentence-level id7 q
serves as the reinforced objective, guiding the gen-
eration towards high id7 points. q is a static
function which will not be updated during train-
ing.

3.2 generator
resembling id4 models, the generator g de-
   nes the policy that generates the target sentence y
given the source sentence x. the generator takes
exactly the same architecture with id4 models.
note that we do not assume the speci   c architec-
ture of the generator. to verify the effectiveness of
the proposed method, we take two different archi-
tectures for the generator, the id56search 2 and
transformer 3.

3.3 discriminator
recently,
the deep discriminative models such
as the id98 and id56 have shown a high per-
formance in complicated sequence classi   cation
tasks. here,
the discriminator is implemented
based on the id98 architecture.

since sentences generated by the generator have
variable lengths, the id98 padding is used to trans-
form the sentences to sequences with    xed length
t , which is the maximum length set for the output

2https://github.com/nyu-dl/dl4mt-tutorial
3https://github.com/tensor   ow/tensor2tensor

figure 1: the illustration of the proposed br-id19an.
left: d is trained over the real sentence pairs translated
by the human and the generated sentence pairs by g.
note that d is a conditional discriminator. right: g
is trained by police gradient where the    nal reward is
provided by d and q.

of the generator. given the source-language se-
quence x1, . . . , xt and target-language sequence
y1, . . . , yt , we build the source matrix x1:t and
target matrix y1:t respectively as:

x1:t = x1; x2; . . . ; xt

(1)

and

y1:t = y1; y2; . . . ; yt

(2)
where xt, yt     rk is the k-dimensional word em-
bedding and the semicolon is the concatenation
operator. for the source matrix x1:t , a kernel
wj     rl  k applies a convolutional operation to
a window size of l words to produce a series of
feature maps:

cji =   (bn (wj     xi:i+l   1 + b))

(3)
where     operator is the summation of element-
wise production and b is a bias term.    is a non-
linear activation function which is implemented as
relu in this paper. note that the batch normaliza-
tion (ioffe and szegedy, 2015) which accelerates
the training signi   cantly, is applied to the input of
the activation function (bn in equation 3). to get
the    nal feature with respect to kernel wj, a max-
over-time pooling operation is leveraged over the

feature maps:(cid:101)cj = max{cj1, . . . , cjt   l+1}

(4)

we use various numbers of kernels with different
window sizes to extract different features, which
are then concatenated to form the source-language
sentence representation cx. identically, the target-
language sentence representation cy can be ex-
tracted from the target matrix y1:t . finally, given
the source-language sentence, the id203 that
the target-language sentence is being real can be
computed as:

p =   (v [cx; cy])

(5)

where v is the transform matrix which transforms
the concatenation of cx and cy into a 2-dimension
embedding and    is the logistic function.

3.4 id7 objective
we apply the smoothed sentence-level id7 as
the speci   c objective for the generator. given
the generated sentence yg and the the ground true
sentence yd, the objective q calculates a reward
q(yg, yd), which measures the id165 precisions
of the generated sentence yg. identical to the out-
put of the discriminator, the q(yg, yd) also ranges
from zero to one, which makes it easier to fuse q
and d.

3.5 policy gradient training
following (yu et al., 2016), the objective of the
generator g is de   ned as to generate a sequence
from the start state to maximize its expected end
reward. formally, the objective function is com-
puted as:

g  (y1:t|x)    rg  

d,q(y1:t   1, x, yt , y    )

j(  ) = (cid:80)

y1:t

where    represents the parameters in g, y1:t =
y1, . . . , yt indicates the generated target se-
quence, x is the source-language sentence, y    
represents the ground true target sentence. rg  
d,q
is the action-value function of a target-language
sentence given the source sentence x, i.e.
the
expected accumulative reward starting from the
state (y1:t   1, x), taking action yt , and follow-
ing the policy g  . to estimate the action-value
function, we consider the estimated id203 of
being real by the discriminator d and the output
of the id7 objective q as the reward:

d,q(y1:t   1, x, yt , y    ) =
rg  
  (d(x, y1:t )     b(x, y1:t )) + (1       )q(y1:t , y    )

where b(x,y) denotes the baseline value to reduce
the variance of the reward. practically, we take
b(x,y) as a constant, 0.5 for simplicity. and the   
is a hyper-parameter. the question is that, given
the source sequence, d only provides a reward
value for a    nished target sequence. if y1:t is not a
   nished target sequence, the value of d(x, y1:t )
makes no sense. therefore, we cannot get the
action-value for an intermediate state directly. to
evaluate the action-value for an intermediate state,
the monte carlo search under the policy of g  
is applied to sample the unknown tokens. each

gx,gxyhuman,dxydgnext actionmc searchrewardd with qstaterewardrewardrewardsearch ends until the end of sentence token is sam-
pled or the sampled sentence reaches the maxi-
mum length. to obtain more stable reward and re-
duce the variance, we represent an n-time monte
carlo search as:

{y 1

1:t1

, . . . , y n

1:tn

} = m cg   ((y1:t, x), n )

t+1:tn

represents the length of

the sen-
where ti
tence sampled by the i   th monte carlo search.
(y1:t, x) = (y1, . . . , yt, x) is the current state
and y n
is sampled based on the policy g  .
the discriminator provides n rewards for the
sampled n sentences respectively. the    nal re-
ward for the intermediate state is calculated as the
average of the n rewards. hence, for the target
sentence with the length t , we compute the re-
ward for yt in the sentence level as:

d,q(y1:t   1, x, yt , y    ) =
rg  

n(cid:80)

          1

n

  (d(x, y n

1:tn

)     b(x, y n

1:tn

)) + (1       )q(y1:tn, y    )

n=1

  (d(x, y1:t)     b(x, y1:t)) + (1       )q(y1:t, y    )

t < t

t = t

using the discriminator as a reward function can
further improve the generator iteratively by dy-
namically updating the discriminator. once we get
more realistic generated sequences, we re-train the
discriminator as:

for decoding. next, pre-train the discriminator
on the combination of the true parallel data and
the machine-generated data until the classi   cation
accuracy achieves at   . finally, we jointly train
the generator and discriminator. the generator is
trained with the policy gradient training method.
however, in our practice, we    nd that updating
the generator only with the simple policy gradient
training leads to unstableness. to alleviate this is-
sue, we adopt the teacher forcing approach which
is similar to (lamb et al., 2016; li et al., 2017).
we directly make the discriminator to automati-
cally assign a reward of 1 to the golden target-
language sentence and the generator uses this re-
ward to update itself on the true parallel example.
we run the teacher forcing training for one time
once the generator is updated by the policy gra-
dient training. after the generator gets updated,
we use the new stronger generator to generate   
more realistic sentences, which are then used to
train the discriminator. following (arjovsky et al.,
2017), we clamp the weights of the discriminator
to a    xed box ( [    , ] ) after each gradient update.
we perform one optimization step for the discrim-
inator for each step of the generator. in our prac-
tice, we set    as 0.82,    as 5000,   as 1.0 and the
n for monte carlo search as 20.

min   ex,y    pdata[log d(x, y )]     ex,y    g[log(1     d(x, y ))]

4 experiments and results

after updating the discriminator, we are ready to
re-train the generator. the gradient of the objec-
tive function j(  ) w.r.t the generator   s parameter
   is calculated as:

t(cid:80)

t=1

(cid:80)
d,q(y1:t   1, x, yt , y    )         (g  (yt|y1:t   1, x))
rg  
d,q(y1:t   1, x, yt , y    )          log p(yt|y1:t   1, x)]

yt

eyt   g   [rg  

   j(  ) = 1

t

t(cid:80)

t=1

= 1
t

3.6 training strategies
gans are widely criticized for its unstable train-
ing since the generator and discriminator need to
be carefully synchronized. to make this work eas-
ier to reproduce, this paper gives detailed strate-
gies for training the proposed model.

firstly, we use the maximum likelihood estima-
tion to pre-train the generator on the parallel train-
ing set until the best translation performance is
achieved. then, generate the machine-generated
sentences by using the generator to decode the
training data. we simply use the greedy sam-
pling method instead of the id125 method

we evaluate our br-id19an on english-german
and chinese-english translation tasks and we test
two different architectures for the generator, the
traditional id56search and the newly emerged
state-of-the-art transformer.

4.1 data sets and preprocessing
english-german: for english-german translation,
we conduct our experiments on the publicly avail-
able corpora used extensively as benchmark for
id4 systems, wmt   14 en-de. this data set
contains 4.5m sentence pairs 4. sentences are en-
coded using byte-pair encoding (sennrich et al.,
2015), which has a shared source-target vocabu-
lary of about 37000 tokens. we report results on
newstest2014. the newstest2013 is used as vali-
dation.

chinese-english: for chinese-english transla-
tion, our training data consists of 1.6m sentence
pairs randomly extracted from ldc corpora 5.

4http://nlp.stanford.edu/projects/id4
5ldc2002l27,
ldc2002t01,

ldc2002e18,

model

id56search (bahdanau et al., 2014)
transformer (vaswani et al., 2017)
id56search+br-id19an(   = 1.0)
id56search+br-id19an(   = 0.7)
id56search+br-id19an(   = 0)
transformer+br-id19an(   = 1.0)
transformer+br-id19an(   = 0.8)
transformer+br-id19an(   = 0)

chinese-english
nist03 nist04 nist05
32.24
33.93
41.02
42.23
35.21
33.45
34.03
35.97
33.07
34.57
41.54
42.67
43.01
41.86
41.29
42.41

35.67
42.17
36.51
37.32
35.93
42.79
42.96
42.74

average
33.94
41.80
35.05
35.77
34.52
42.30
42.61
42.14

english-german
newstest2014

21.2
27.30
22.1
22.89
21.75
27.75
27.92
27.49

table 1: id7 score on chinese-english and english-german translation tasks. the hyper-parameter    is selected
according to the development set. for the transformer, following (vaswani et al., 2017), we report the result of a
single model obtained by averaging the 5 checkpoints around the best model selected on the development set.

both the source and target sentences are encoded
with byte-pair encoding and the tokens in the
source and target vocabulary is about 38000 and
34000 respectively 6. we choose the nist02 as
the development set. for testing, we use nist03,
nist04 and nist05 data sets.

to speed up the training procedure, sentences of
length over 50 words are removed when we con-
duct experiments on the id56search model. this
is widely used by previous works (ranzato et al.,
2015; shen et al., 2015; yang et al., 2016).

4.2 model parameters and evaluation
for the transformer, following the base model in
(vaswani et al., 2017), we set the dimension of
id27 as 512, dropout rate as 0.1 and
the head number as 8. the encoder and decoder
both have a stack of 6 layers. we use id125
with a beam size of 4 and length penalty    = 0.6.
for the id56search, following (bahdanau et al.,
2014), we set the hidden units for both encoders
and decoders as 512. the dimension of the word
embedding is also set as 512. we do not apply
dropout for training the id56search. during test-
ing, we use id125 with a beam size of 10
and length penalty is not applied.

all models are implemented in tensorflow
(abadi et al., 2015) and trained on up to four
k80 gpus synchronously in a multi-gpu setup
on a single machine 7. we stop training when the
model achieves no improvement for the tenth eval-

ldc2003e07, ldc2004t08, ldc2004e12, ldc2005t10
6when doing bpe for chinese, we need to do word seg-
mentation    rst and the following steps are the same with bpe
for english.

7the code we used to train and evaluate our models can
be found at https://github.com/zhenyangiacas/id4 gan

uation on the development set. id7 (papineni
et al., 2002) is utilized as the evaluation metric.
we apply the script mteval-v11b.pl to evaluate the
chinese-english translation and utilize the script
multi-belu.pl for english-german translation 8.

4.3 main results
the model of id56search is optimized with the
mini-batch of 64 examples.
it takes about 30
hours to pre-train the id56search on the chinese-
english data set and 46 hours on the english-
german data set. during generative adversarial
training, it takes about 35 hours on the chinese-
english data set and about 50 hours on the
english-german data set. for the transformer,
each training batch contains a set of sentence
pairs containing approximately 25000 source to-
kens and 25000 target tokens. on the chinese-
english data set, it takes about 15 hours to do pre-
training and 20 hours to do generative adversarial
training. on the english-german data set, it takes
about 35 hours for the pre-training and 40 hours
for the generative adversarial training.

table 1 shows the id7 score on chinese-
english and english-german test sets. on the
id56search model, the naive gan (i.e., the line
of id56search+br-id19an (  =1) in table 1)
achieves improvement up to +1.11 id7 points
averagely on chinese-english test sets and +0.9
id7 points on english-german test set. armed
with the id7 objective, the br-id19an (the
line of id56search+br-id19an (  =0.7)) leads
to more signi   cant improvements, +1.83 id7

8https://github.com/moses-

smt/mosesdecoder/blob/617e8c8/scripts/generic/multi-
id7.perl;mteval-v11b.pl

points averagely on chinese-english translation
and +1.69 id7 points on english-german trans-
lation. we also test the translation performance
when the id56search is only guided by the static
id7 objective (the line of id56search+br-
id19an (  =0)), and we only get +0.58 id7
points improvement on chinese-english transla-
tion and +0.55 id7 points improvement on
english-german. experiments on the transformer
show the same trends. while the transformer has
achieved state-of-the-art translation performances,
our approach still achieves +0.81 id7 points
improvement on chinese-english translation and
+0.62 id7 points improvement on english-
german.

these results indicate that the proposed br-
id19an consistently outperforms the baselines
and it shows better translation performance than
the naive gan and the model guided only by the
id7 objective.

5 analysis
5.1 compared with mrt
we show that mrt (shen et al., 2015) is an ex-
treme case of our approach. considering a sen-
tence pair (x, y), the training objective of mrt is
calculated as

(cid:98)j(  (cid:48)) =

(cid:88)

ys   s(x)

p(ys|x;   (cid:48))   (ys, y)

with mrt. the only difference is that the rein-
forcement learning procedure is utilized in br-
id19an to maximize the total reward and mrt
instead applies random sampling to approximate
the risk. actually, the br-id19an is a weighted
sum of the naive gan (  =1) and mrt (  =0),
and it incorporates the advantages of the two ap-
proaches. speci   cally, compared to naive gan
which is trained without speci   c objective guid-
ance, br-id19an utilizes the id7 objective
to guide the generator to generate sentences with
higher id7 points. and compared to mrt
which is trained only with the static objective,
the br-id19an applies a dynamic discriminator
which updates synchronously with the generator,
to feedback the dynamic rewards for the genera-
tor. table 2 compares the translation performance
between the mrt and br-id19an on chinese-
english and english-german translation tasks. we
only conduct experiments on the id56search be-
cause we only get the open-source implementa-
tion of mrt on the id56search 9. results show
that the proposed br-id19an consistently out-
performs the mrt on the chinese-english and
english-german translations.

chinese-english english-german
newstest2014

model

id56search

mrt (shen et al., 2015)
br-id19an(   = 0.7)

average
33.94
34.64
35.77

21.2
21.6
22.89

where    (ys, y) is a id168 (i.e.,
the
sentence-level id7 used in this paper) that mea-
sures the discrepancy between a predicted trans-
lation ys and the training example y, s(x) rep-
resents the set which contains all of the predic-
tions given the input x, and   (cid:48) is the parameters
of the id4 model. unfortunately, this objective
is usually intractable due to the exponential search
space. to alleviate this problem, a subset of the
search space is sampled to approximate this ob-
jective. in this paper, when we set    as zero, the
objective for the proposed br-id19an comes to

j(  )  =0 =

g  (y1:t|x)    q(y1:t , y    )

y1:t

where the q(y1:t , y    ) is also a id168 be-
tween the predicted translation y1:t and the train-
ing example y    . it is easy to be found that, under
this condition (i.e.,    set as zero), the proposed
br-id19an optimizes almost the same objective

(cid:88)

table 2: id7 score on chinese-english and english-
german translation tasks for mrt and br-id19an.

5.2 when to stop pre-training
the initial accuracy    of the discriminator which
is viewed as a hyper-parameter, can be set care-
fully during the process of pre-training. a nat-
ural question is that when shall we end the pre-
training. do we need to pre-train the discriminator
with the highest accuracy? to answer this ques-
tion, we test the impact of the initial accuracy of
the discriminator. we pre-train    ve discriminators
which have the accuracy as 0.6, 0.7, 0.8, 0.9 and
0.95 respectively. with the    ve discriminators, we
train    ve different br-id19an models (with the
generator as id56search and    set as 0.7) and test
their translation performances on the development
set at regular intervals. figure 2 reports the results

9the open-source implementation can be found at:

https://github.com/edinburghnlp/nematus

-
-

35.67

33.93

36.87

n nist02 nist03 nist04 nist05
0
32.24
5
10
15
20
25
30

37.34
38.58
38.65
38.74

34.91
35.97
36.04
36.01

36.09
37.32
37.52
37.54

33.45
34.03
33.91
33.76

-
-

-
-

-
-

figure 2: id7 score on the development set for the
br-id19an where the discriminators have different
initial accuracy.    0.6-acc    means the initial accuracy
is 0.6. we report the results on the chinese-english
translation tasks. id56search is taken as the generator.

and we can    nd that the initial accuracy of the dis-
criminator shows great impacts on the translation
performance of the proposed br-id19an. from
   gure 2, we show that the initial accuracy of the
discriminator needs to be set carefully and either
it is too high (0.9 and 0.95) or too low (0.6 and
0.7), the model performs badly 10. this suggests
that it is important for the generator and discrim-
inator to keep a balanced relationship at the be-
ginning of the generative adversarial training. if
the discriminator is too strong, the generator is al-
ways penalized for its bad predictions and gets no
idea about right predictions. hence, the generator
is discouraged all the time and the performance
gets worse and worse. on the other hand, if the
discriminator is too weak, the discriminator is un-
able to give right guidance for the generator, i.e.
the gradient direction for updating the generator is
random. empirically, we pre-train the discrimina-
tor until its accuracy reaches around 0.8.

5.3 sample times for monto carol search
we are also curious about how the sample times
n for monte carlo search affects the translation
performance.
intuitively, if n is set as a small
number, the intermediate reward for each word
may be incorrect since there is a large variance
for the monto carol search when the sample time
is too small. and if otherwise, the computation
shall be very time consuming because we need
to do much more sampling. therefore, there is

10to make the illustration simple and clear, we only depict

the results when the id56search acts as the generator.

table 3:
the translation performance of the br-
id19an with different n for monte carlo search.    -   
means that the proposed model shows no improvement
than the pre-trained generator or it can not be trained
stably. with n set as 0, it is referred to as the pre-
trained generator. similarly, we only report results on
the id56search and    is set as 0.7.

a trade-off between the accuracy and computa-
tion complexity here. we investigate this prob-
lem on the chinese-english translation task. table
3 presents the translation performance of the br-
id19an on the test sets when the n are set from
5 to 30 with interval 5. from table 3, the proposed
model achieves no improvement than the baseline
(i.e., the pre-trained generator) when n are set
less than 15 and the id7 scores are not reported
on the table. as a matter of fact, the translation
performance of the model gets worse and worse.
we conjecture that the approximated reward is far
from the expected reward due to the large variance
when n is set too small, and gives wrong gradi-
ent directions for model updating. since the train-
ing for gan is not stable, the wrong gradient di-
rection exacerbates the unstableness and results in
the id7 getting worse and worse. with the in-
creasing of n, the translation performance of the
model gets improved. however, with n set larger
than 20, we get little improvement than the model
with n set as 20 and the training time exceeds our
expectation.

6 conclusion and future work

in this work, we propose the br-id19an which
leverages the id7 reinforced generative adver-
sarial net to improve the id4. we show that
the proposed approach is a weighted combination
of the naive gan and mrt. to verify the ef-
fectiveness of our approach, we test two differ-
ent architectures for the generator, the traditional
id56search and the state-of-the-art transformer.
extensive experiments on chinese-english and
english-german translation tasks show that our

024681012141618test step510152025303540id70.6-acc0.7-acc0.8-acc0.9-acc0.95-accapproach consistently achieves signi   cant
im-
provements.
in the future, we would like to
try multi-adversarial framework which consists of
multi discriminators and generators for gan.

acknowledgements
this work is supported by the national key re-
search and development program of china un-
der grant no. 2017yfb1002102, and beijing
engineering research center under grant no.
z171100002217015. we would like to thank xu
shuang for her preparing data used in this work.
additionally, we also want to thank chen zhineng,
wang wenfu and zhao yuanyuan for their invalu-
able discussions on this work.

references
mart    n abadi, ashish agarwal, paul barham, eugene
brevdo, zhifeng chen, craig citro, greg s. cor-
rado, andy davis, jeffrey dean, matthieu devin,
sanjay ghemawat, ian goodfellow, andrew harp,
geoffrey irving, michael isard, yangqing jia, rafal
jozefowicz, lukasz kaiser, manjunath kudlur, josh
levenberg, dan man  e, rajat monga, sherry moore,
derek murray, chris olah, mike schuster, jonathon
shlens, benoit steiner, ilya sutskever, kunal tal-
war, paul tucker, vincent vanhoucke, vijay vasude-
van, fernanda vi  egas, oriol vinyals, pete warden,
martin wattenberg, martin wicke, yuan yu, and xi-
aoqiang zheng. 2015. tensorflow: large-scale ma-
chine learning on heterogeneous systems .

martin arjovsky, soumith chintala, and l  eon bot-
arxiv preprint

tou. 2017. wasserstein gan.
arxiv:1701.07875 .

sanjeev arora, rong ge, yingyu liang, tengyu ma,
and yi zhang. 2017. generalization and equilibrium
in generative adversarial nets. icml .

dzmitry bahdanau, kyunghyun cho, and yoshua ben-
gio. 2014. id4 by jointly
arxiv preprint
learning to align and translate.
arxiv:1409.0473 .

chris callison-burch and miles osborne. 2006. re-
evaluating the role of id7 in machine translation re-
search .

niladri chatterjee, anish johnson, and madhav kr-
ishna. 2007.
some improvements over the id7
metric for measuring translation quality for hindi.
in computing: theory and applications, 2007. ic-
cta   07. international conference on. ieee, pages
485   490.

kyunghyun cho, bart van merri  enboer, caglar gul-
cehre, dzmitry bahdanau, fethi bougares, holger
schwenk, and yoshua bengio. 2014.
learning
phrase representations using id56 encoder-decoder
for id151. arxiv preprint
arxiv:1406.1078 .

emily l denton, soumith chintala, rob fergus, et al.
2015. deep generative image models using a?
in ad-
laplacian pyramid of adversarial networks.
vances in neural information processing systems.
pages 1486   1494.

jon gauthier. 2014. conditional generative adversar-
ial nets for convolutional face generation. class
project for stanford cs231n: convolutional neural
networks for visual recognition, winter semester
2014(5):2.

jonas gehring, michael auli, david grangier, denis
yarats, and yann n dauphin. 2017. convolutional
sequence to sequence learning .

ian goodfellow, jean pouget-abadie, mehdi mirza,
bing xu, david warde-farley, sherjil ozair, aaron
courville, and yoshua bengio. 2014. generative ad-
in advances in neural information
versarial nets.
processing systems. pages 2672   2680.

gabriel lima guimaraes, benjamin

sanchez-
lengeling, pedro luis cunha farias, and al  an
aspuru-guzik. 2017. objective-reinforced genera-
tive adversarial networks (organ) for sequence gen-
eration models. arxiv preprint arxiv:1705.10843
.

sergey ioffe and christian szegedy. 2015. batch nor-
malization: accelerating deep network training by
arxiv preprint
reducing internal covariate shift.
arxiv:1502.03167 .

melvin johnson, mike schuster, quoc v le, maxim
krikun, yonghui wu, zhifeng chen, nikhil thorat,
fernanda vi  egas, martin wattenberg, greg corrado,
et al. 2016. google   s multilingual neural machine
translation system: enabling zero-shot translation.
arxiv preprint arxiv:1611.04558 .

nal kalchbrenner and phil blunsom. 2013. recur-
rent continuous translation models. emnlp pages
1700   1709.

alex lamb, anirudh goyal, ying zhang, saizheng
zhang, aaron courville, and yoshua bengio. 2016.
professor forcing: a new algorithm for training re-
current networks. advances in neural information
processing systems pages 4601   4609.

jiwei li, will monroe, tianlin shi, alan ritter,
learning
arxiv preprint

and dan jurafsky. 2017.
for neural dialogue generation.
arxiv:1701.06547 .

adversarial

xilun chen, yu sun, ben athiwaratkun, claire cardie,
and kilian weinberger. 2016. adversarial deep av-
eraging networks for cross-lingual sentiment classi-
   cation. arxiv preprint arxiv:1606.01614 .

pauline luc, camille couprie, soumith chintala,
semantic segmenta-
arxiv preprint

and jakob verbeek. 2016.
tion using adversarial networks.
arxiv:1611.08408 .

yuan zhang, regina barzilay, and tommi jaakkola.
net-
arxiv preprint

2017.
aspect-augmented
works for id20.
arxiv:1701.00188 .

adversarial

junbo zhao, michael mathieu, and yann lecun. 2016.
energy-based generative adversarial network. arxiv
preprint arxiv:1609.03126 .

jun-yan zhu, taesung park, phillip isola, and alexei a
efros. 2017. unpaired image-to-image translation
using cycle-consistent adversarial networks. arxiv
preprint arxiv:1703.10593 .

preslav nakov, francisco guzman, and stephan vogel.
2012. optimizing for sentence-level id7+ 1 yields
in coling. volume 12, pages
short translations.
1979   1994.

kishore papineni, salim roukos, todd ward, and wei-
jing zhu. 2002. id7: a method for automatic eval-
uation of machine translation. association for com-
putational linguistics pages 311   318.

alec radford, luke metz, and soumith chintala.
2015. unsupervised representation learning with
deep convolutional id3.
arxiv preprint arxiv:1511.06434 .

marc   aurelio ranzato, sumit chopra, michael auli,
and wojciech zaremba. 2015. sequence level train-
ing with recurrent neural networks. arxiv preprint
arxiv:1511.06732 .

rico sennrich, barry haddow, and alexandra birch.
2015. id4 of rare words with
subword units. computer science .

shiqi shen, yong cheng, zhongjun he, wei he, hua
wu, maosong sun, and yang liu. 2015. minimum
risk training for id4. arxiv
preprint arxiv:1512.02433 .

ilya sutskever, oriol vinyals, and quoc vv le. 2014.
sequence to sequence learning with neural net-
works. advances in neural information processing
systems pages 3104   3112.

ashish vaswani, noam shazeer, niki parmar, jakob
uszkoreit, llion jones, aidan n gomez, lukasz
kaiser, and illia polosukhin. 2017. attention is all
you need .

lijun wu, yingce xia, li zhao, fei tian, tao qin,
jianhuang lai, and tie-yan liu. 2017. adver-
arxiv preprint
sarial id4.
arxiv:1704.06933 .

yonghui wu, mike schuster, zhifeng chen, quoc v
le, mohammad norouzi, wolfgang macherey,
maxim krikun, yuan cao, qin gao, klaus
macherey, et al. 2016.
google   s neural ma-
chine translation system: bridging the gap between
arxiv preprint
human and machine translation.
arxiv:1609.08144 .

zhen yang, wei chen, feng wang, and bo xu. 2016.
a character-aware encoder for neural machine trans-
lation. in coling.

lantao yu, weinan zhang, jun wang, and yong yu.
2016. seqgan: sequence generative adversarial nets
with policy gradient. the association for the ad-
vancement of arti   cial intelligence 2017 .

yizhe zhang, zhe gan, and lawrence carin. 2016.

generating text via adversarial training. nips .

