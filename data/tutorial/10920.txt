reward shaping with recurrent neural networks for speeding up

on-line policy learning in spoken dialogue systems

pei-hao su, david vandyke, milica ga  si  c,

nikola mrk  si  c, tsung-hsien wen and steve young

department of engineering, university of cambridge, cambridge, uk
{phs26, djv27, mg436, nm480, thw28, sjy}@cam.ac.uk

5
1
0
2

 

g
u
a
8
1

 

 
 
]

g
l
.
s
c
[
 
 

2
v
1
9
3
3
0

.

8
0
5
1
:
v
i
x
r
a

abstract

statistical spoken dialogue systems have
the attractive property of being able to
be optimised from data via interactions
with real users. however in the rein-
forcement learning paradigm the dialogue
manager (agent) often requires signi   cant
time to explore the state-action space to
learn to behave in a desirable manner.
this is a critical issue when the system is
trained on-line with real users where learn-
ing costs are expensive. reward shaping
is one promising technique for addressing
these concerns. here we examine three re-
current neural network (id56) approaches
for providing reward shaping information
in addition to the primary (task-orientated)
environmental feedback. these id56s are
trained on returns from dialogues gener-
ated by a simulated user and attempt to
diffuse the overall evaluation of the dia-
logue back down to the turn level to guide
the agent towards good behaviour faster.
in both simulated and real user scenarios
these id56s are shown to increase policy
learning speed.
importantly, they do not
require prior knowledge of the user   s goal.
introduction

1
spoken dialogue systems (sds) offer a natural
way for people to interact with computers. with
the ability to learn from data (interactions) sta-
tistical sds can theoretically be created faster
and with less man-hours than a comparable hand-
crafted rule based system. they have also been
shown to perform better (young et al., 2013).
central to this is the use of partially observable
id100 (pomdp) to model di-
alogue, which inherently manage the uncertainty
created by errors in id103 and seman-
tic decoding (williams and young, 2007).

the dialogue manager is a core component of
an sds and largely determines the quality of in-
its behaviour is controlled by a pol-
teraction.
icy which maps belief states to system actions (or
distributions over sets of actions) and this policy
is trained in a id23 framework
(sutton and barto, 1999) where rewards are re-
ceived from the environment, the most informative
of which occurs only at the dialogues conclusion,
indicating task success or failure.1

it is the sparseness of this environmental re-
ward function which, by not providing any infor-
mation at intermediate turns, requires exploration
to traverse deeply many sub-optimal paths. this
is a signi   cant concern when training sds on-
line with real users where one wishes to minimise
client exposure to sub-optimal system behaviour.
in an effort to counter this problem, reward shap-
ing (ng et al., 1999) introduces domain knowl-
edge to provide earlier informative feedback to the
agent (additional to the environmental feedback)
for the purpose of biasing exploration for discov-
ering optimal behaviour quicker.2 reward shaping
is brie   y reviewed in section 2.1.

in the context of sds, ferreira and lef`evre
(2015) have motivated the use of reward shap-
ing via analogy to the    social signals    naturally
produced and interpreted throughout a human-
human dialogue. this non-statistical reward shap-
ing model used heuristic features for speeding up
policy learning.

as an alternative, one may consider attempting
to handcraft a    ner grained environmental reward

1a uniform reward of -1 is common for all other, non-

terminal turns, which promotes faster task completion.

2learning algorithms are another central element in im-
proving the speed of convergence during policy training. in
particular the sample-ef   ciency of the learning algorithm can
be the deciding factor in whether it can realistically be em-
ployed on-line. see e.g. the gp-sarsa (gasic and young,
2014) and kalman temporal-difference (daubigney et al.,
2014) methods which bootstrap estimates of sparse value
functions from minimal numbers of samples (dialogues).

function. for example, asri et al. (2014) proposed
diffusing expert ratings of dialogues to the state
transition level to produce a richer reward func-
tion. policy convergence may occur faster in this
altered pomdp and dialogues generated by a task
based simulated user may also alleviate the need
for expert ratings. however, unlike reward shap-
ing, modifying the environmental reward function
also modi   es the resulting optimal policy.

we recently proposed convolutional and recur-
rent neural network (id56) approaches for deter-
mining dialogue success. this was used to pro-
vide a reinforcement signal for learning on-line
from real users without requiring any prior knowl-
edge of the user   s task (su et al., 2015). here
we extend the id56 approach by introducing new
training constraints in order to combine the merits
of the above three works: (1) diffusing dialogue
level ratings down to the turn level to (2) add re-
ward shaping information for faster policy learn-
ing, whilst (3) not requiring prior task knowledge
which is simply unavailable on-line.

in section 2 we brie   y describe potential based
reward shaping before introducing the id56s we
explore for producing reward shaping signals (ba-
sic id56, long short-term memory (lstm) and
gated recurrent unit (gru)). the features the
id56s use along with the training constraint and
loss are also described. the experimental evalu-
ation is then presented in section 3. firstly, the
estimation accuracy of the id56s is assessed. the
bene   t of using the id56 for reward shaping in
both simulated and real user scenarios is then also
demonstrated. finally, conclusions are presented
in section 4.

2 id56s for reward shaping
2.1 reward shaping
reward shaping provides the system with an ex-
tra reward signal f in addition to environmental
reward r, making the system learn from the com-
posite signal r + f . the shaping reward f often
encodes expert knowledge that complements the
sparse signal r. since the reward function de   nes
the system   s objective, changing it may result in
a different task. when the task is modelled as a
fully observable markov decision process (mdp),
ng et al. (1999) de   ned formal requirements on
the shaping reward as a difference of any potential
function    on consecutive states s and s(cid:48) which
preserves the optimality of policies. based on this

figure 1: id56 with three types of hidden units:
basic, lstm and gru. the feature vectors ft ex-
tracted at turns t = 1, . . . , t are labelled ft.

property, eck et al. (2015) further extended it to
pomdp by proof and empirical experiments:

f (bt, a, bt+1) =     (bt+1)       (bt)

(1)
where    is the discount factor, bt the belief state at
turn t, and a the action leading bt to bt+1.

however determining an appropriate potential
function for an sds is non-trivial. rather than
hand-crafting the function with heuristic knowl-
edge, we propose using an id56 to predict proper
values as in the following.

2.2 recurrent neural network models
the id56 model is a subclass of neural network
de   ned by the presence of feedback connections.
the ability to succinctly retain history information
makes it suitable for modelling sequential data. it
has been successfully used for language modelling
(mikolov et al., 2011) and spoken language under-
standing (mesnil et al., 2015).

however, bengio et al. (1994) observed that ba-
sic id56s suffer from vanishing/exploding gradi-
ent problems that limit their capability of mod-
elling long context dependencies. to address this,
long short-term memory (hochreiter and schmid-
huber, 1997) and gated recurrent unit (chung et
al., 2014) id56s have been proposed. in this pa-
per, all three types of id56 (basic/lstm/gru)
are compared.

2.3 reward shaping with id56 prediction
the role of the id56 is to solve the regression
problem of predicting the scalar return of each
completed dialogue. at every turn t, input fea-
ture ft are extracted from the belief/action pair and
used to update the hidden layer ht. from dialogues
generated by a simulated user (schatzmann and
young, 2009) supervised training pairs are created
which consist of the turn level sequence of these
feature vectors ft along with the scalar dialogue

return as scored by an objective measure of task
completion. whilst the id56 models are trained
on dialogue level supervised targets, we hypothe-
sise that their subsequent turn level predictions can
guide policy exploration via acting as informative
reward shaping potentials.

to encourage good turn level predictions, all
three id56 variants are trained to predict the di-
alogue return not with the    nal output of the net-
work, but with the constraint that their scalar out-
puts from each turn t should sum to predict the
return for the whole dialogue. this is shown in
figure 1. a mean-square-error (mse) loss is used
(see appendix a). the trained id56s are then used
directly as the reward shaping potential function   ,
using the id56 scalar output at each turn.

the feature inputs ft for all id56s consisted of
the following sections: the real-valued belief state
vector formed by concatenating the distributions
over user discourse act, method and goal variables
(thomson and young, 2010), one-hot encodings
of the user and summary system actions, and the
normalised turn number. this feature vector was
extracted at every turn (system + user exchange).

3 experiments

3.1 experimental setup

in all experiments the cambridge restaurant do-
main was used, which consists of approximately
150 venues each having 6 attributes (slots) of
which 3 can be used by the system to constrain the
search and the remaining 3 are informable proper-
ties once a database entity has been found.

the shared core components of the sds in all
experiments were a domain independent asr, a
confusion network (cnet) semantic input decoder
(henderson et al., 2012), the buds (thomson and
young, 2010) belief state tracker that factorises the
dialogue state using a dynamic id110
and a template based natural language generator.
all policies were trained by gp-sarsa (gasic
and young, 2014) and the summary action space
contains 20 actions. per turn reward was set to -1
and    nal reward 20 for task success else 0.

with this ontology, the size of the full feature
vector was 147. the turn number was expressed as
a percentage of the maximum number of allowed
turns, here 30. the one-hot user dialogue act en-
coding was formed by taking only the most likely
user act estimated by the cnet decoder.

figure 2: rmse of return prediction by using
id56/lstm/gru, trained on 18k and 1k dia-
logues and tested on sets testa and testb (see text).

3.2 neural network training
here results of training the 3 id56s on the simu-
lated user dialogues are presented.3 two training
sets were used consisting of 18k and 1k dialogues
to verify the model robustness. in all cases a sepa-
rate validation set consisting of 1k dialogues was
used for controlling over   tting. training and val-
idation sets were approximately balanced regard-
ing objective success/failure labels and collected
at a 15% semantic error rate (ser). prediction re-
sults are shown in figure 2 on two test sets; testa:
1k dialogues, balanced regarding objective labels,
at 15% ser and testb: containing 12k dialogues
collected at sers of 0, 15, 30 and 45 as the data
occurred (i.e. with no balancing regarding labels).
root-mse (rmse) results of predicting the di-
alogue return are depicted in figure 2. the models
with lstm and gru units achieved a slight im-
provement in most cases over the basic id56. no-
tice that the model with gru even reached com-
parable results when trained with 1k training data
compared to 18k. the results from the 1k train-
ing set indicate that the model can be developed
from limited data. this enables datasets to be cre-
ated by human annotation, avoiding the need for
a simulated user. the results on set testb also
show that the models can perform well in situa-
tions with varying error rates as would be encoun-
tered in real operating environments. note that the
dataset could also be created from human   s anno-
tation which avoids the need for a simulated user.
we next examine the id56-based reward shaping
for policy training with a simulated user.

3all id56s were implemented using the theano library
(bergstra et al., 2010). in all cases the hidden layer contained
100 units with a sigmoid non-linearity and used stochastic
id119 (per dialogue) for training.

figure 3: policy training via simulated user with
(gru/hdc) and without (baseline) reward shap-
ing. standard errors are also shown.

figure 4: learning curves of reward with standard
errors during on-line policy optimisation for the
baseline (black) and proposed (green) systems.

3.3 policy learning with simulated user
since the aim of reward shaping is to enhance
policy learning speed, we focus on the    rst 1000
training dialogues. figure 2 shows that the gru
id56 attained slightly better performance than the
other two id56 models, albeit with no statistical
signi   cance. thus for clearer presentation of the
policy training results we plot only the gru re-
sults, using the model trained on 18k dialogues.

to show the effectiveness of using id56 with
gru for predicting reward shaping potentials, we
compare it with the hand-crafted (hdc) method
for reward shaping proposed by ferreira and
lef`evre (2013) that requires prior knowledge of
the user   s task, and a baseline policy using only the
environmental reward. figure 3 shows the learn-
ing curve of the reward for the three systems. after
every 50 training iterations each system was tested
with 1000 dialogues and averaged over 10 poli-
cies. the simulated user   s ser was set to 15%.

we see that reward shaping indeed provides
the agent with more information, increasing the
learning speed. furthermore, our proposed id56
method further outperforms the hand-crafted sys-
tem, whilst also being able to be applied on-line.

3.4 policy learning with human users
based on the above results, the same gru model
was selected for training a policy on-line with hu-
mans. two systems were trained with users re-
cruited via amazon mechanical turk: a baseline
was trained with only the environmental reward,
and another system was trained with an additional
shaping reward predicted by the proposed gru.
learning began from a random policy in all cases.

figure 4 shows the on-line learning curve of the
reward when training the systems with 400 dia-
logues. the moving average was calculated using
a window of 100 dialogues and each result was av-
eraged over three policies in order to reduce noise.
it can be seen that by adding the id56 based shap-
ing reward, the policy learnt quicker in the impor-
tant initial phase of policy learning.

4 conclusions
this paper has shown that id56 models can be
trained to predict the dialogue return with a con-
straint such that subsequent turn level predictions
act as good reward shaping signals that are effec-
tive for accelerating policy learning on-line with
real users. as in many other applications, we
found that gated id56s such as lstm and gru
perform a little better than basic id56s.

in the work described here, the id56s were
trained using a simulated user and this simulator
could have been used to bootstrap a policy for
use with real users. however our supposition is
that id56s could be trained for reward prediction
which are substantially domain independent and
hence have wider applications via domain adapta-
tion and extension (ga  si  c et al., 2015; brys et al.,
2015). testing this supposition will be the subject
of future work.

5 acknowledgements
pei-hao su is supported by cambridge trust
and the ministry of education, taiwan. david
vandyke and tsung-hsien wen are supported by
toshiba research europe ltd, cambridge re-
search lab.

references
[asri et al.2014] layla el asri, romain laroche, and
olivier pietquin. 2014. task completion transfer
learning for reward id136. in proc of mlis.

[bengio et al.1994] yoshua bengio, patrice simard,
and paolo frasconi. 1994. learning long-term de-
pendencies with id119 is dif   cult. neural
networks, ieee transactions on, 5(2):157   166.

[bergstra et al.2010] james bergstra, olivier breuleux,
fr  ed  eric bastien, pascal lamblin, razvan pascanu,
guillaume desjardins, joseph turian, david warde-
farley, and yoshua bengio. 2010. theano: a cpu
and gpu math expression compiler. in proceedings
of the python for scienti   c computing conference.

[brys et al.2015] tim brys, anna harutyunyan,
matthew e. taylor, and ann now  e. 2015. policy
transfer using reward shaping. in proc of aamas.

[chung et al.2014] junyoung chung, caglar gulcehre,
kyunghyun cho, and yoshua bengio. 2014. em-
pirical evaluation of gated recurrent neural net-
arxiv preprint
works on sequence modeling.
arxiv:1412.3555.

[daubigney et al.2014] lucie daubigney, matthieu
geist, senthilkumar chandramohan, and olivier
pietquin. 2014. a comprehensive reinforcement
learning framework for dialogue management
optimisation. journal of selected topics in signal
processing, 6(8).

[eck et al.2015] adam eck, leen-kiat soh, sam de-
vlin, and daniel kudenko. 2015. potential-based
reward shaping for    nite horizon online pomdp plan-
ning. autonomous agents and multi-agent systems,
pages 1   43.

[ferreira and lef`evre2013] emmanuel ferreira

and
fabrice lef`evre.
social signal and user
adaptation in id23-based dialogue
management. in proc of mlis.

2013.

[ferreira and lef`evre2015] emmanuel ferreira

and
fabrice lef`evre.
2015. reinforcement-learning
based dialogue system for id176s
with socially-inspired rewards. computer speech &
language, 34(1):256   274.

[gasic and young2014] milica gasic and stephanie
young. 2014. gaussian processes for pomdp-based
dialogue manager optimization. taslp, 22(1):28   
40.

[ga  si  c et al.2015] milica ga  si  c, dongho kim, pirros
tsiakoulis, and steve young. 2015. distributed di-
alogue policies for multi-domain statistical dialogue
management. in icassp.

[henderson et al.2012] matthew henderson, milica
ga  si  c, blaise thomson, pirros tsiakoulis, kai yu,
and steve young. 2012. discriminative spoken
language understanding using word confusion
networks. in ieee slt.

[hochreiter and schmidhuber1997] sepp hochreiter
and j  urgen schmidhuber. 1997. long short-term
memory. neural computation, 9(8):1735   1780.

[mesnil et al.2015] gr  egoire mesnil, yann dauphin,
kaisheng yao, yoshua bengio, li deng, dilek
hakkani-tur, xiaodong he, larry heck, gokhan
tur, dong yu, et al. 2015. using recurrent neural
networks for slot    lling in spoken language under-
standing. taslp, 23(3):530   539.

[mikolov et al.2011] tomas mikolov, stefan kom-
brink, lukas burget, jan h cernocky, and sanjeev
khudanpur. 2011. extensions of recurrent neural
network language model. in icassp.

[ng et al.1999] andrew y. ng, daishi harada, and stu-
art russell. 1999. policy invariance under reward
transformations: theory and application to reward
shaping. in icml.

[schatzmann and young2009] j.

and
s. young. 2009. the hidden agenda user simulation
model. ieee talsp, 17(4):733   747.

schatzmann

[su et al.2015] pei-hao su, david vandyke, milica
ga  si  c, dongho kim, nikola mrk  si  c, tsung-hsien
wen, and steve young. 2015. learning from real
users: rating dialogue success with neural networks
for id23 in spoken dialogue sys-
tems. in proc of interspeech.

[sutton and barto1999] richard s. sutton and an-
drew g. barto. 1999. id23: an
introduction. mit press.

[thomson and young2010] b. thomson and s. young.
2010. bayesian update of dialogue state: a pomdp
framework for spoken dialogue systems. computer
speech and language, 24:562   588.

[williams and young2007] jason d. williams

and
steve young. 2007. partially observable markov
decision processes for spoken id71.
computer speech and language, 21(2):393   422.

[young et al.2013] steve young, milica ga  sic, blaise
thomson, and jason williams. 2013. pomdp-based
statistical spoken dialogue systems: a review.
in
proc of the ieee, volume 99, pages 1   20.

a training constraint/id168
for all id56 models the following mse loss func-
tion is used on a per-dialogue basis:

(cid:32)
r     t(cid:88)

(cid:33)2

mse =

rt

(2)

t=1

where the current dialogue has t turns, r is the
return and training target, and rt is the scalar pre-
diction output by the id56 model at each turn.

