semantically conditioned lstm-based id86 for

spoken dialogue systems

tsung-hsien wen, milica ga  si  c, nikola mrk  si  c,
pei-hao su, david vandyke and steve young
cambridge university engineering department,
trumpington street, cambridge, cb2 1pz, uk

{thw28,mg436,nm480,phs26,djv27,sjy}@cam.ac.uk

abstract

id86 (id86) is a
critical component of spoken dialogue and
it has a signi   cant impact both on usabil-
ity and perceived quality. most id86 sys-
tems in common use employ rules and
heuristics and tend to generate rigid and
stylised responses without the natural vari-
ation of human language. they are also
not easily scaled to systems covering mul-
tiple domains and languages. this pa-
per presents a statistical language gener-
ator based on a semantically controlled
long short-term memory (lstm) struc-
ture. the lstm generator can learn from
unaligned data by jointly optimising sen-
tence planning and surface realisation us-
ing a simple cross id178 training crite-
rion, and language variation can be eas-
ily achieved by sampling from output can-
didates. with fewer heuristics, an objec-
tive evaluation in two differing test do-
mains showed the proposed method im-
proved performance compared to previ-
ous methods. human judges scored the
lstm system higher on informativeness
and naturalness and overall preferred it to
the other systems.
introduction

1
the id86 (id86) compo-
nent provides much of the persona of a spoken
dialogue system (sds), and it has a signi   cant
impact on a user   s impression of the system. as
noted in stent et al. (2005), a good generator usu-
ally depends on several factors: adequacy,    u-
ency, readability, and variation.
previous ap-
proaches attacked the id86 problem in different
ways. the most common and widely adopted
today is the rule-based (or template-based) ap-
proach (cheyer and guzzoni, 2007; mirkovic and

cavedon, 2011). despite its robustness and ade-
quacy, the frequent repetition of identical, rather
stilted, output forms make talking to a rule-based
generator rather tedious. furthermore, the ap-
proach does not easily scale to large open domain
systems(young et al., 2013; ga  si  c et al., 2014;
henderson et al., 2014). hence approaches to
id86 are required that can be readily scaled whilst
meeting the above requirements.

the trainable generator approach exempli   ed
by the halogen (langkilde and knight, 1998)
and sparky system (stent et al., 2004) provides
a possible way forward. these systems include
speci   c trainable modules within the generation
framework to allow the model to adapt to different
domains (walker et al., 2007), or reproduce cer-
tain style (mairesse and walker, 2011). however,
these approaches still require a handcrafted gen-
erator to de   ne the decision space within which
statistics can be used for optimisation. the result-
ing utterances are therefore constrained by the pre-
de   ned syntax and any domain-speci   c colloquial
responses must be added manually.

more recently, corpus-based methods (oh and
rudnicky, 2000; mairesse and young, 2014; wen
et al., 2015) have received attention as access to
data becomes increasingly available. by de   n-
ing a    exible learning structure, corpus-based
methods aim to learn generation directly from
data by adopting an over-generation and rerank-
ing paradigm (oh and rudnicky, 2000), in which
   nal responses are obtained by reranking a set of
candidates generated from a stochastic generator.
learning from data directly enables the system to
mimic human responses more naturally, removes
the dependency on prede   ned rules, and makes
the system easier to build and extend to other do-
mains. as detailed in sections 2 and 3, however,
these existing approaches have weaknesses in the
areas of training data ef   ciency, accuracy and nat-
uralness.

5
1
0
2

 

g
u
a
6
2

 

 
 
]
l
c
.
s
c
[
 
 

2
v
5
4
7
1
0

.

8
0
5
1
:
v
i
x
r
a

this paper presents a statistical id86 based on
a semantically controlled long short-term mem-
ory (lstm) recurrent network. it can learn from
unaligned data by jointly optimising its sentence
planning and surface realisation components us-
ing a simple cross id178 training criterion with-
out any heuristics, and good quality language vari-
ation is obtained simply by randomly sampling
the network outputs. we start in section 3 by
de   ning the framework of the proposed neural lan-
guage generator. we introduce the semantically
controlled lstm (sc-lstm) cell in section 3.1,
then we discuss how to extend it to a deep structure
in section 3.2. as suggested in wen et al. (2015),
a backward reranker is introduced in section 3.3
to improve    uency. training and decoding details
are described in section 3.4 and 3.5.

section 4 presents an evaluation of the proposed
approach in the context of an application provid-
ing information about venues in the san francisco
area. in section 4.2, we    rst show that our genera-
tor outperforms several baselines using objective
metrics. we experimented on two different on-
tologies to show not only that good performance
can be achieved across domains, but how easy and
quick the development lifecycle is. in order to as-
sess the subjective performance of our system, a
quality test and a pairwise preference test are pre-
sented in section 4.3. the results show that our
approach can produce high quality utterances that
are considered to be more natural and are preferred
to previous approaches. we conclude with a brief
summary and future work in section 5.

2 related work

conventional approaches to id86 typically divide
the task into sentence planning and surface real-
isation. sentence planning maps input semantic
symbols into an intermediary form representing
the utterance, e.g. a tree-like or template struc-
ture, then surface realisation converts the interme-
diate structure into the    nal text (walker et al.,
2002; stent et al., 2004). although statistical sen-
tence planning has been explored previously, for
example, generating the most likely context-free
derivations given a corpus (belz, 2008) or max-
imising the expected reward using reinforcement
learning (rieser and lemon, 2010), these meth-
ods still rely on a pre-existing, handcrafted gener-
ator. to minimise handcrafting, stent and molina
(2009) proposed learning sentence planning rules

directly from a corpus of utterances labelled with
rhetorical structure theory (rst) discourse rela-
tions (mann and thompson, 1988). however, the
required corpus labelling is expensive and addi-
tional handcrafting is still needed to map the sen-
tence plan to a valid syntactic form.

as noted above, corpus-based id86 aims at
learning generation decisions from data with min-
imal dependence on rules and heuristics. a pi-
oneer in this direction is the class-based id165
language model (lm) approach proposed by oh
and rudnicky (2000). ratnaparkhi (2002) later
addressed some of the limitations of class-based
lms in the over-generation phase by using a mod-
i   ed generator based on a syntactic dependency
tree. mairesse and young (2014) proposed a
phrase-based id86 system based on factored lms
that can learn from a semantically aligned corpus.
although active learning (mairesse et al., 2010)
was also proposed to allow learning online directly
from users, the requirement for human annotated
alignments limits the scalability of the system.
another similar approach casts id86 as a template
extraction and matching problem, e.g., angeli et
al. (2010) train a set of id148 to make
a series of generation decisions to choose the most
suitable template for realisation. kondadadi et al.
(2013) later show that the outputs can be further
improved by an id166 reranker making them com-
parable to human-authored texts. however, tem-
plate matching approaches do not generalise well
to unseen combinations of semantic elements.

the use of neural network-based (nn) ap-
proaches to id86 is relatively unexplored. the
stock reporter system ana by kukich (1987) is
perhaps the    rst nn-based generator, although
generation was only done at the phrase level. re-
cent advances in recurrent neural network-based
language models (id56lm) (mikolov et al., 2010;
mikolov et al., 2011a) have demonstrated the
value of distributed representations and the ability
to model arbitrarily long dependencies. sutskever
et al. (2011) describes a simple variant of the id56
that can generate meaningful sentences by learn-
ing from a character-level corpus. more recently,
karpathy and fei-fei (2014) have demonstrated
that an id56lm is capable of generating image
descriptions by conditioning the network model
on a pre-trained convolutional image feature rep-
resentation. zhang and lapata (2014) also de-
scribes interesting work using id56s to generate

chinese poetry. a forerunner of the system pre-
sented here is described in wen et al. (2015), in
which a forward id56 generator, a id98 reranker,
and a backward id56 reranker are trained jointly
to generate utterances. although the system was
easy to train and extend to other domains, a heuris-
tic gate control was needed to ensure that all of
the attribute-value information in the system   s re-
sponse was accurately captured by the generated
utterance. furthermore, the handling of unusual
slot-value pairs by the id98 reranker was rather
arbitrary. in contrast, the lstm-based system de-
scribed in this paper can deal with these problems
automatically by learning the control of gates and
surface realisation jointly.

training an id56 with long range dependencies
is dif   cult because of the vanishing gradient prob-
lem (bengio et al., 1994). hochreiter and schmid-
huber (1997) mitigated this problem by replacing
the sigmoid activation in the id56 recurrent con-
nection with a self-recurrent memory block and a
set of multiplication gates to mimic the read, write,
and reset operations in digital computers. the re-
sulting architecture is dubbed the long short-term
memory (lstm) network. it has been shown to
be effective in a variety of tasks, such as speech
recognition (graves et al., 2013b), handwriting
recognition (graves et al., 2009), spoken language
understanding (yao et al., 2014), and machine
translation (sutskever et al., 2014). recent work
by graves et al. (2014) has demonstrated that an
nn structure augmented with a carefully designed
memory block and differentiable read/write op-
erations can learn to mimic computer programs.
moreover, the ability to train deep networks pro-
vides a more sophisticated way of exploiting rela-
tions between labels and features, therefore mak-
ing the prediction more accurate (hinton et al.,
2012). by extending an lstm network to be both
deep in space and time, graves (2013) shows the
resulting network can used to synthesise handwrit-
ing indistinguishable from that of a human.

3 the neural language generator

the generation model proposed in this paper is
based on a recurrent nn architecture (mikolov et
al., 2010) in which a 1-hot encoding wt of a token1
wt is input at each time step t conditioned on a re-

1we use token instead of word because our model operates
on text for which slot values are replaced by its corresponding
slot tokens. we call this procedure delexicalisation.

current hidden layer ht and outputs the id203
distribution of the next token wt+1. therefore, by
sampling input tokens one by one from the output
distribution of the id56 until a stop sign is gen-
erated (karpathy and fei-fei, 2014) or some con-
straint is satis   ed (zhang and lapata, 2014), the
network can produce a sequence of tokens which
can be lexicalised 2 to form the required utterance.

3.1 semantic controlled lstm cell

figure 1: semantic controlled lstm cell pro-
posed in this paper. the upper part is a traditional
lstm cell in charge of surface realisation, while
the lower part is a sentence planning cell based on
a sigmoid control gate and a dialogue act (da).

long short-term memory (hochreiter and
schmidhuber, 1997) is a recurrent nn architecture
which uses a vector of memory cells ct     rn and
a set of elementwise multiplication gates to control
how information is stored, forgotten, and exploited
inside the network. of the various different con-
nectivity designs for an lstm cell (graves, 2013;
zaremba et al., 2014), the architecture used in this
paper is illustrated in figure 3.1 and de   ned by the
following equations,,

it =   (wwiwt + whiht   1)
ft =   (wwf wt + whf ht   1)
ot =   (wwowt + whoht   1)
  ct = tanh(wwcwt + whcht   1)
ct = ft (cid:12) ct   1 + it (cid:12)   ct
ht = ot (cid:12) tanh(ct)

(1)
(2)
(3)
(4)
(5)
(6)

2the process of replacing slot token by its value.

where    is the sigmoid function, it, ft, ot     [0, 1]n
are input, forget, and output gates respectively, and
  ct and ct are proposed cell value and true cell
value at time t. note that each of these vectors
has a dimension equal to the hidden layer h.

in order to ensure that

the generated utter-
ance represents the intended meaning, the gen-
erator is further conditioned on a control vec-
tor d, a 1-hot representation of the dialogue act
(da) type and its slot-value pairs. although a re-
lated work (karpathy and fei-fei, 2014) has sug-
gested that reapplying this auxiliary information
to the id56 at every time step can increase perfor-
mance by mitigating the vanishing gradient prob-
lem (mikolov and zweig, 2012; bengio et al.,
1994), we have found that such a model also omits
and duplicates slot information in the surface re-
alisation. in wen et al. (2015) simple heuristics
are used to turn off slot feature values in the con-
trol vector d once the corresponding slot token
has been generated. however, these heuristics can
only handle cases where slot-value pairs can be
identi   ed by exact matching between the delexi-
calised surface text and the slot value pair encoded
in d. cases such as binary slots and slots that take
don   t care values cannot be explicitly delexicalised
in this way and these cases frequently result in
generation errors.

to address this problem, an additional control
cell is introduced into the lstm to gate the da
as shown in figure 1. this cell plays the role
of sentence planning since it manipulates the da
features during the generation process in order to
produce a surface realisation which accurately en-
codes the input information. we call the result-
ing architecture semantically controlled lstm
(sc-lstm). starting from the original da 1-hot
vector d0, at each time step the da cell decides
what information should be retained for future
time steps and discards the others,

rt =   (wwrwt +   whrht   1)
dt = rt (cid:12) dt   1

(7)
(8)

where rt     [0, 1]d is called the reading gate, and
   is a constant. here wwr and whr act like key-
word and key phrase detectors that learn to asso-
ciate certain patterns of generated tokens with cer-
tain slots. figure 3 gives an example of how these
detectors work in affecting da features inside the
network. equation 5 is then modi   ed so that the

cell value ct also depends on the da,

ct = ft (cid:12) ct   1 + it (cid:12)   ct + tanh(wdcdt)

(9)

after updating equation 6 by equation 9, the out-
put distribution is formed by applying a softmax
function g, and the distribution is sampled to ob-
tain the next token,
p (wt+1|wt, wt   1, ...w0, dt) = g(whoht) (10)

wt+1     p (wt+1|wt, wt   1, ...w0, dt).

(11)

3.2 the deep structure
deep neural networks (dnn) enable increased
discrimination by learning multiple layers of fea-
tures, and represent the state-of-the-art for many
applications such as id103 (graves et
al., 2013b) and natural language processing (col-
lobert and weston, 2008). the neural language
generator proposed in this paper can be easily ex-
tended to be deep in both space and time by stack-
ing multiple lstm cells on top of the original
structure. as shown in figure 2, skip connections
are applied to the inputs of all hidden layers as
well as between all hidden layers and the outputs
(graves, 2013). this reduces the number of pro-
cessing steps between the bottom of the network
and the top, and therefore mitigates the vanishing
gradient problem (bengio et al., 1994) in the ver-
tical direction. to allow all hidden layer informa-
tion to in   uence the reading gate, equation 7 is
changed to

rt =   (wwrwt +

  lwl

hrhl

t   1)

(12)

l

where l is the hidden layer index and   l is a
layer-wise constant. since the network tends to
over   t when the structure becomes more complex,
the dropout technique (srivastava et al., 2014) is
used to regularise the network. as suggested in
(zaremba et al., 2014), dropout was only applied
to the non-recurrent connections, as shown in the
figure 2. it was not applied to id27s
since pre-trained word vectors were used.

3.3 backward lstm reranking
one remaining problem in the structure described
so far is that the lstm generator selects words
based only on the preceding history, whereas some
sentence forms depend on the backward context.
previously, bidirectional networks (schuster and

(cid:88)

figure 2: the deep lstm generator structure by stacking multiple lstm layers on top of the da cell.
the skip connection was adopted to mitigate the vanishing gradient, while the dropout was applied on
dashed connections to prevent co-adaptation and over   tting.

paliwal, 1997) have been shown to be effective for
sequential problems (graves et al., 2013a; sunder-
meyer et al., 2014). however, applying a bidirec-
tional network directly in the sc-lstm generator
is not straightforward since the generation process
is sequential in time. hence instead of integrating
the bidirectional information into one network, we
trained another sc-lstm from backward context
to choose best candidates from the forward gen-
erator outputs. in our experiments, we also found
that by tying the keyword detector weights wwr
(see equations 7 and 12) of both the forward and
backward networks together makes the generator
less sensitive to random initialisation.

3.4 training

the forward generator and the backward reranker
were both trained by treating each sentence as a
mini-batch. the objective function was the cross
id178 error between the predicted word distri-
bution pt and the actual word distribution yt in
the training corpus. an l2 regularisation term
was added to the objective function for every 10
training examples as suggested in mikolov et al.
(2011b). however, further regularisation was re-
quired for the reading gate dynamics. this re-
sulted in the following modi   ed cost function for
each mini-match (ignoring standard l2),

f (  ) =(cid:80)

t log(yt) + (cid:107)dt(cid:107) +(cid:80)t   1

(cid:124)
t p

t=0     (cid:107)dt+1   dt(cid:107)

(13)

where dt is the da vector at the last word index
t , and    and    are constants set to 10   4 and 100,
respectively. the second term is used to penalise
generated utterances that failed to render all the re-
quired slots, while the third term discourages the
network from turning more than one gate off in
a single time step. the forward and backward
networks were structured to share the same set
of id27s, initialised with pre-trained
word vectors (pennington et al., 2014). the hid-
den layer size was set to be 80 for all cases, and
deep networks were trained with two hidden lay-
ers and a 50% dropout rate. all costs and gradients
were computed and stochastic id119
was used to optimise the parameters. both net-
works were trained with back propagation through
time (werbos, 1990).
in order to prevent over-
   tting, early stopping was implemented using a
held-out validation set.

3.5 decoding

the decoding procedure is split into two phases:
(a) over-generation, and (b) reranking.
in the
over-generation phase, the forward generator con-
ditioned on the given da, is used to sequentially
generate utterances by random sampling of the
predicted next word distributions. in the reranking
phase, the cost of the backward reranker fb(  ) is
computed. together with the cost ff (  ) from the
forward generator, the reranking score r is com-

puted as:

r =    (ff (  ) + fb(  ) +   err)

(14)

where    is a tradeoff constant, and the slot error
rate err is computed by exact matching the slot
tokens in the candidate utterances,

err =

p + q

n

(15)

where n is the total number of slots in the da, and
p, q is the number of missing and redundant slots
in the given realisation. note that the err rerank-
ing criteria cannot handle arbitrary slot-value pairs
such as binary slots or slots that take the don   t care
value because they cannot be delexicalised and ex-
actly matched.    is set to a large value in order to
severely penalise nonsensical outputs.

4 experiments
4.1 experimental setup
the target application for our generation system
is a spoken dialogue system providing informa-
tion about certain venues in san francisco. in or-
der to demonstrate the scalability of the proposed
method and its performance in different domains,
we tested on two domains that talk about restau-
rants and hotels respectively. there are 8 system
dialogue act types such as inform to present infor-
mation about restaurants, con   rm to check that a
slot value has been recognised correctly, and re-
ject to advise that the user   s constraints cannot be
met. each domain contains 12 attributes (slots),
some are common to both domains and the oth-
ers are domain speci   c. the detailed ontologies
for the two domains are provided in table 1. to
form a training corpus for each domain, dialogues
collected from a previous user trial (ga  si  c et al.,
2015) of a statistical dialogue manager were ran-
domly sampled and shown to workers recruited
via the amazon mechanical turk (amt) service.
workers were shown each dialogue turn by turn
and asked to enter an appropriate system response
in natural english corresponding to each system
da. for each domain around 5k system utter-
ances were collected from about 1k randomly
sampled dialogues. each categorical value was re-
placed by a token representing its slot, and slots
that appeared multiple times in a da were merged
into one. after processing and grouping each ut-
terance according to its delexicalised da, we ob-
tained 248 distinct das in the restaurant domain

p
y
t

t
c
a

e
r
a
h
s

table 1: ontologies used in the experiments.

sf restaurant

sf hotel

e inform, inform only, reject,

con   rm, select, request,
reqmore, goodbye

d name, type, *pricerange, price,

phone, address, postcode,
*area, *near
*food

c
   

*hasinternet
*acceptscards
*dogs-allowed
bold=binary slots, *=slots can take    don   t care    value

*goodformeal
*kids-allowed

i
c
e
p
s

and 164 in the hotel domain. the average number
of slots per da for each domain is 2.25 and 1.95,
respectively.

the system was implemented using the theano
library (bergstra et al., 2010; bastien et al., 2012),
and trained by partitioning each of the collected
corpus into a training, validation, and testing set
in the ratio 3:1:1. the frequency of each ac-
tion type and slot-value pair differs quite markedly
across the corpus, hence up-sampling was used to
make the corpus more uniform. since our gener-
ator works stochastically and the trained networks
can differ depending on the initialisation, all the
results shown below3 were averaged over 5 ran-
domly initialised networks. for each da, we over-
generated 20 utterances and selected the top 5 real-
isations after reranking. the id7-4 metric was
used for the objective evaluation (papineni et al.,
2002). multiple references for each test da were
obtained by mapping them back to the distinct
set of das, grouping those delexicalised surface
forms that have the same da speci   cation, and
then lexicalising those surface forms back to ut-
terances. in addition, the slot error rate (err) as
described in section 3.5 was computed as an aux-
iliary metric alongside the id7 score. however,
for the experiments it is computed at the corpus
level, by averaging slot errors over each of the top
5 realisations in the entire corpus. the trade-off
weights    between keyword and key phrase detec-
tors as mentioned in section 3.1 and 3.2 were set
to 0.5.

4.2 objective evaluation
we compared the single layer semantically con-
trolled lstm (sc-lstm) and a deep version with
3except human evaluation, in which only one set of net-

works was used.

table 2: objective evaluation of the top 5 re-
alisations. except for handcrafted (hdc) and k-
nearest neighbour (knn) baselines, all the other
approaches ranked their realisations from 20 over-
generated candidates.

table 3: real user trial for utterance quality
assessment on two metrics (rating out of 3),
averaging over top 5 realisations. statistical
signi   cance was computed using a two-tailed
student   s t-test, between deep and all others.

method

hdc
knn
classlm
id56 w/o
lstm w/o
id56 w/
lstm w/
sc-lstm
+deep

sf restaurant

id7 err(%)
0.451
0.602
0.627
0.706
0.714
0.710
0.717
0.711
0.731

0.0
0.87
8.70
4.15
1.79
1.52
0.63
0.62
0.46

sf hotel

id7 err(%)
0.560
0.676
0.734
0.813
0.817
0.815
0.818
0.802
0.832

0.0
1.87
5.35
3.14
1.93
1.74
1.53
0.78
0.41

two hidden layers (+deep) against several base-
the handcrafted generator (hdc), k-nearest
lines:
neighbour (knn), class-based lms (classlm) as
proposed in oh and rudnicky (2000), the heuris-
tic gated id56 as described in wen et al. (2015)
and a similar lstm variant (id56 w/ & lstm w/),
and the same id56/lstm but without gates (id56
w/o & lstm w/o). the handcrafted generator was
developed over a long period of time and is the
standard generator used for trialling end-to-end di-
alogue systems (for example (ga  si  c et al., 2014)).
the knn was implemented by computing the sim-
ilarity of the test da 1-hot vector against all of
the training da 1-hot vectors, selecting the nearest
and then lexicalising to generate the    nal surface
form. the objective results are shown in table
2. as can be seen, none of the baseline systems
shown in the    rst block (hdc, knn, & classlm)
are comparable to the systems described in this
paper (sc-lstm & +deep) if both metrics are con-
sidered. setting aside the dif   culty of scaling to
large domains, the handcrafted generator   s (hdc)
use of prede   ned rules yields a    xed set of sen-
tence plans, which can differ markedly from the
real colloquial human responses collected from
amt, while the class lm approach suffers from
inaccurate rendering of information. although
the knn method provides reasonable adequacy i.e.
low err, the id7 is low, probably because of
the errors in the collected corpus which knn can-
not handle but statistical approaches such as lms
can by suppressing unlikely outputs.

the last three blocks in table 2 compares the
proposed method with previous id56 approaches.

method informativeness naturalness
+deep
sc-lstm
id56 w/
classlm
* p < 0.05 ** p < 0.005

2.58
2.59
2.53
2.46**

2.51
2.50
2.42*
2.45

table 4: pairwise preference test among four sys-
tems. statistical signi   cance was computed using
two-tailed binomial test.

-

pref.% classlm id56 w/
classlm
46.0
id56 w/
54.0
59.1*
sc-lstm
62.3**
+deep
* p < 0.05 ** p < 0.005

-
57
64.3**

sc-lstm +deep
40.9**
37.7**
35.7*
43.0
47.6

-

52.4

-

lstm generally works better than vanilla id56
due to its ability to model long range dependen-
cies more ef   ciently. we also found that by us-
ing gates, whether learned or heuristic, gave much
lower slot error rates. as an aside, the ability of
the sc-lstm to learn gates is also exempli   ed in
figure 3. finally, by combining the learned gate
approach with the deep architecture (+deep), we
obtained the best overall performance.

4.3 human evaluation
since automatic metrics may not consistently
agree with human perception (stent et al., 2005),
human testing is needed to assess subjective qual-
ity. to do this, a set of judges were recruited using
amt. for each task, two systems among the four
(classlm, id56 w/, sc-lstm, and +deep) were ran-
domly selected to generate utterances from a set of
newly sampled dialogues in the restaurant domain.
in order to evaluate system performance in the
presence of language variation, each system gen-
erated 5 different surface realisations for each in-
put da and the human judges were asked to score
each of them in terms of informativeness and nat-
uralness (rating out of 3), and also asked to state a
preference between the two. here informativeness

(a) an example realisation from sf restaurant domain

(b) an example realisation from sf hotel domain

figure 3: examples showing how the sc-lstm controls the da features    owing into the network via
its learned semantic gates. despite errors due to sparse training data for some slots, each gate generally
learned to detect words and phrases describing a particular slot-value pair.

is de   ned as whether the utterance contains all the
information speci   ed in the da, and naturalness
is de   ned as whether the utterance could plausibly
have been produced by a human. in order to de-
crease the amount of information presented to the
judges, utterances that appeared identically in both
systems were    ltered out. we tested 1000 das in
total, and after    ltering there were approximately
1300 generated utterances per system.

table 3 shows the quality assessments which
exhibit the same general trend as the objective re-
sults. the sc-lstm systems (sc-lstm & +deep)
outperform the class-based lms (classlm) and the
id56 with heuristic gates (id56 w/) in both metrics.
the deep sc-lstm system (+deep) is signi   -
cantly better than the class lms (classlm) in terms
of informativeness, and better than the id56 with
heuristic gates (id56 w/) in terms of naturalness.
the preference test results are shown in table 4.
again, the sc-lstm systems (sc-lstm & +deep)
were signi   cantly preferred by the judges. more-
over, the judges recorded a strong preference for

the deep approach (+deep) compared to the others,
though the preference is not signi   cant when com-
paring to its shallow counterpart (sc-lstm). exam-
ple dialogue acts and their top-5 realisations are
shown in table 5.

5 conclusion and future work

in this paper we have proposed a neural network-
based generator that is capable of generating natu-
ral linguistically varied responses based on a deep,
semantically controlled lstm architecture which
we call sc-lstm. the generator can be trained
on unaligned data by jointly optimising its sen-
tence planning and surface realisation components
using a simple cross id178 criterion without any
heuristics or handcrafting. we found that the sc-
lstm model achieved the best overall perfor-
mance on two objective metrics across two differ-
ent domains. an evaluation by human judges also
con   rmed that the sc-lstm approach is strongly
preferred to a variety of existing methods.

this work represents a line of research that tries

table 5: samples of top 5 realisations from the deep sc-lstm (+deep) system output.

#
1

2

#
3

4

example dialogue acts and realizations from sf restaurant domain
inform(name=   red door cafe   , goodformeal=   breakfast   , area=   cathedral hill   , kidsallowed=   no   )
red door cafe is a good restaurant for breakfast in the area of cathedral hill and does not allow children .
red door cafe is a good restaurant for breakfast in the cathedral hill area and does not allow children .
red door cafe is a good restaurant for breakfast in the cathedral hill area and does not allow kids .
red door cafe is good for breakfast and is in the area of cathedral hill and does not allow children .
red door cafe does not allow kids and is in the cathedral hill area and is good for breakfast .
informonly(name=   dosa on    llmore and kiss seafood   , pricerange=   expensive   , near=   lower paci   c heights   )
there is no place other than dosa on    llmore and kiss seafood that are expensive near to lower paci   c heights .
dosa on    llmore and kiss seafood is the only expensive restaurant near lower paci   c heights .
the only listed restaurant near lower paci   c heights in the expensive price range is dosa on    llmore and kiss seafood .
i apologize , dosa on    llmore and kiss seafood is the only expensive restaurant near lower paci   c heights .
i apologize , dosa on    llmore and kiss seafood are the only expensive restaurants near lower paci   c heights .
example dialogue acts and realizations from sf hotel domain
inform(type=   hotel   ,count=   182   ,dogsallowed=   dontcare   )
there are 182 hotels if you do not care whether dogs are allowed .
there are 182 hotels if you do not care whether they allow dogs .
182 hotels are available if dogs allowed or not is not an issue .
there are 182 hotels if allowing dogs or not is not an issue .
there are 182 hotels if whether dogs are allowed does not matter .
informonly(name=   red victorian bed breakfast   ,acceptscreditcards=   yes   ,near=   haight   ,hasinternet=   yes   )
red victorian bed breakfast is the only hotel near haight and accepts credit cards and has internet .
red victorian bed breakfast is the only hotel near haight and has internet and accepts credit cards .
red victorian bed breakfast is the only hotel near haight that accept credit cards and offers internet .
the red victorian bed breakfast has internet and near haight , it does accept credit cards .
the red victorian bed breakfast is the only hotel near haight that accepts credit cards , and offers internet .

to model the id86 problem in a uni   ed architec-
ture, whereby the entire model is end-to-end train-
able from data. we contend that this approach can
produce more natural responses which are more
similar to colloquial styles found in human conver-
sations. another key potential advantage of neu-
ral network based language processing is the im-
plicit use of distributed representations for words
and a single compact parameter encoding of the
information to be conveyed. this suggests that it
should be possible to further condition the gener-
ator on some dialogue features such discourse in-
formation or social cues during the conversation.
furthermore, adopting a corpus based regime en-
ables domain scalability and multilingual id86 to
be achieved with less cost and a shorter lifecycle.
these latter aspects will be the focus of our future
work in this area.

6 acknowledgements

tsung-hsien wen and david vandyke are sup-
ported by toshiba research europe ltd, cam-
bridge research laboratory.

ference on emnlp, emnlp    10. association for
computational linguistics.

fr  ed  eric bastien, pascal lamblin, razvan pascanu,
james bergstra, ian j. goodfellow, arnaud berg-
eron, nicolas bouchard, and yoshua bengio. 2012.
theano: new features and speed improvements.
deep learning and unsupervised id171
nips 2012 workshop.

anja belz. 2008. automatic generation of weather
forecast
texts using comprehensive probabilistic
generation-space models. natural language engi-
neering.

yoshua bengio, patrice simard, and paolo frasconi.
1994. learning long-term dependencies with gra-
dient descent is dif   cult. neural networks, ieee
transactions on.

james bergstra, olivier breuleux, fr  ed  eric bastien,
pascal lamblin, razvan pascanu, guillaume des-
jardins, joseph turian, david warde-farley, and
yoshua bengio. 2010. theano: a cpu and gpu
in proceedings of the
math expression compiler.
python for scienti   c computing conference.

adam cheyer and didier guzzoni. 2007. method and
apparatus for building an intelligent automated as-
sistant. us patent app. 11/518,292.

references
gabor angeli, percy liang, and dan klein. 2010. a
simple domain-independent probabilistic approach
in proceedings of the 2010 con-
to generation.

ronan collobert and jason weston. 2008. a uni   ed
architecture for natural language processing: deep
in pro-
neural networks with multitask learning.
ceedings of the 25th international conference on
machine learning.

milica ga  si  c, dongho kim, pirros tsiakoulis, cather-
ine breslin, matthew henderson, martin szummer,
blaise thomson, and steve young. 2014.
incre-
mental on-line adaptation of pomdp-based dialogue
managers to extended domains. in in proceedings
on interspeech.

milica ga  si  c, dongho kim, pirros tsiakoulis, and
steve young. 2015. distributed dialogue policies
for multi-domain statistical dialogue management.
in in proceedings on icassp.

alex graves, marcus liwicki, santiago fern  andez,
and j  urgen
roman bertolami, horst bunke,
schmidhuber. 2009. a novel connectionist system
for unconstrained handwriting recognition. pattern
analysis and machine intelligence, ieee transac-
tions on.

alex graves, abdel-rahman mohamed, and geoffrey
hinton. 2013a. id103 with deep recur-
rent neural networks. in acoustics, speech and sig-
nal processing (icassp), 2013 ieee international
conference on.

alex graves, abdel-rahman mohamed, and geof-
id103
corr,

frey e. hinton.
with deep recurrent neural networks.
abs/1303.5778.

2013b.

alex graves, greg wayne, and ivo danihelka. 2014.

id63s. corr, abs/1410.5401.

alex graves. 2013. generating sequences with recur-

rent neural networks. corr, abs/1308.0850.

matthew henderson, blaise thomson, and steve
young. 2014. robust dialog state tracking using
delexicalised recurrent neural networks and unsu-
pervised adaptation. in proceedings of ieee spoken
language technology.

geoffrey hinton, li deng, dong yu, george dahl,
abdel-rahman mohamed, navdeep jaitly, andrew
senior, vincent vanhoucke, patrick nguyen, tara
sainath, and brian kingsbury. 2012. deep neural
networks for acoustic modeling in speech recogni-
tion: the shared views of four research groups. sig-
nal processing magazine, ieee.

sepp hochreiter and j  urgen schmidhuber. 1997. long

short-term memory. neural computation.

andrej karpathy and li fei-fei. 2014. deep visual-
semantic alignments for generating image descrip-
tions. corr.

ravi kondadadi, blake howald, and frank schilder.
2013. a statistical id86 framework for aggregated
planning and realization. in proceedings of the 51st
annual meeting of the acl. association for com-
putational linguistics.

karen kukich.

1987. where do phrases come
from: some preliminary experiments in connection-
ist phrase generation. in natural language genera-
tion. springer netherlands.

irene langkilde and kevin knight. 1998. generation
that exploits corpus-based statistical knowledge. in
proceedings of the 36th annual meeting of the acl,
acl    98.

franc  ois mairesse and marilyn a. walker. 2011. con-
trolling user perceptions of linguistic style: train-
able generation of personality traits. computer lin-
guistics.

franc  ois mairesse and steve young. 2014. stochastic
language generation in dialogue using factored lan-
guage models. computer linguistics.

franc  ois mairesse, milica ga  si  c, filip jur  c      cek, simon
keizer, blaise thomson, kai yu, and steve young.
2010. phrase-based statistical language generation
using id114 and active learning. in pro-
ceedings of the 48th acl, acl    10.

william c. mann and sandra a. thompson. 1988.
rhetorical structure theory: toward a functional the-
ory of text organization. text.

tom  a  s mikolov and geoffrey zweig. 2012. context
dependent recurrent neural network language model.
in in proceedings on ieee slt workshop.

tom  a  s mikolov, martin kara   t, luk  a  s burget, jan
  cernock  y, and sanjeev khudanpur. 2010. recur-
in in
rent neural network based language model.
proceedings on interspeech.

tom  a  s mikolov, stefan kombrink, luk  a  s burget,
jan h.   cernock  y, and sanjeev khudanpur. 2011a.
extensions of recurrent neural network language
in icassp, 2011 ieee international con-
model.
ference on.

tom  a  s mikolov, stefan kombrink, anoop deoras,
luk  a  s burget, and jan   cernock  y. 2011b. id56lm -
recurrent neural network id38 toolkit.
in in proceedings on asru.

danilo mirkovic and lawrence cavedon.

dialogue management using scripts.
1,891,625.

2011.
ep patent

alice h. oh and alexander i. rudnicky.

2000.
stochastic language generation for spoken dialogue
systems. in proceedings of the 2000 anlp/naacl
workshop on conversational systems - volume 3,
anlp/naacl-convsyst    00.

kishore papineni, salim roukos, todd ward, and wei-
jing zhu. 2002. id7: a method for automatic eval-
uation of machine translation. in proceedings of the
40th annual meeting on acl. association for com-
putational linguistics.

jeffrey pennington, richard socher, and christopher
manning. 2014. glove: global vectors for word
in proceedings of the 2014 con-
representation.
ference on emnlp. association for computational
linguistics.

adwait ratnaparkhi. 2002. trainable approaches to
surface id86 and their appli-
cation to conversational id71. computer
speech and language.

verena rieser and oliver lemon. 2010. natural lan-
guage generation as planning under uncertainty for
spoken dialogue systems. in empirical methods in
id86. springer-verlag.

mike schuster and kuldip k paliwal. 1997. bidirec-
tional recurrent neural networks. signal processing,
ieee transactions on.

nitish srivastava, geoffrey hinton, alex krizhevsky,
ilya sutskever, and ruslan salakhutdinov. 2014.
dropout: a simple way to prevent neural networks
from over   tting. journal of machine learning re-
search.

paul j werbos. 1990. id26 through time:
what it does and how to do it. proceedings of the
ieee.

kaisheng yao, baolin peng, yu zhang, dong yu, ge-
offrey zweig, and yangyang shi. 2014. spoken lan-
guage understanding using long short-term memory
in in proceedings on ieee slt
neural networks.
workshop. ieee institute of electrical and electron-
ics engineers.

steve young, milica ga  si  c, blaise thomson, and ja-
son d. williams. 2013. pomdp-based statistical
spoken id71: a review. proceedings of
the ieee.

wojciech zaremba, ilya sutskever, and oriol vinyals.
recurrent neural network id173.

2014.
corr, abs/1409.2329.

amanda stent and martin molina. 2009. evaluating
automatic extraction of rules for sentence plan con-
in proceedings of sigdial. association
struction.
for computational linguistics.

xingxing zhang and mirella lapata. 2014. chinese
poetry generation with recurrent neural networks.
in proceedings of the 2014 conference on emnlp.
association for computational linguistics, october.

amanda stent, rashmi prasad, and marilyn walker.
2004. trainable sentence planning for complex in-
formation presentation in spoken id71. in
in proceedings of the annual meeting of the acl.

amanda stent, matthew marge, and mohit singhai.
2005. evaluating evaluation methods for generation
in in proceedings of
in the presence of variation.
cicling 2005.

martin sundermeyer, tamer alkhouli, joern wuebker,
and hermann ney. 2014. translation modeling
with id182. in pro-
ceedings of the 2014 conference on emnlp. asso-
ciation for computational linguistics.

ilya sutskever, james martens, and geoffrey e. hin-
ton. 2011. generating text with recurrent neural
networks. in proceedings of the 28th international
conference on machine learning (icml-11). acm.

ilya sutskever, oriol vinyals, and quoc v. le. 2014.
sequence to sequence learning with neural net-
works. corr.

marilyn a walker, owen c rambow, and monica ro-
gati. 2002. training a sentence planner for spo-
ken dialogue using boosting. computer speech and
language.

marilyn walker, amanda stent, franois mairesse, and
rashmi prasad. 2007. individual and domain adap-
tation in sentence planning for dialogue. journal of
arti   cial intelligence research (jair.

tsung-hsien wen, milica ga  si  c, dongho kim, nikola
mrk  si  c, pei-hao su, david vandyke, and steve
young. 2015. stochastic language generation in di-
alogue using recurrent neural networks with convo-
lutional sentence reranking. in proceedings of sig-
dial. association for computational linguistics.

