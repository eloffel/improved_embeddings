6
1
0
2

 
r
p
a
9
1

 

 
 
]
l
c
.
s
c
[
 
 

6
v
1
3
9
6
0

.

1
1
5
1
:
v
i
x
r
a

published as a conference paper at iclr 2016

evaluating prerequisite qualities for learn-
ing end-to-end id71

jesse dodge   , andreea gane   , xiang zhang   , antoine bordes,
sumit chopra, alexander h. miller, arthur szlam & jason weston
facebook ai research
770 broadway
new york, usa
{jessedodge,agane,xiangz,abordes,spchopra,ahm,aszlam,jase}@fb.com

abstract

a long-term goal of machine learning is to build intelligent conversational agents.
one recent popular approach is to train end-to-end models on a large amount of
real dialog transcripts between humans (sordoni et al., 2015; vinyals & le, 2015;
shang et al., 2015). however, this approach leaves many questions unanswered as
an understanding of the precise successes and shortcomings of each model is hard
to assess. a contrasting recent proposal are the babi tasks (weston et al., 2015b)
which are synthetic data that measure the ability of learning machines at various
reasoning tasks over toy language. unfortunately, those tests are very small and
hence may encourage methods that do not scale. in this work, we propose a suite
of new tasks of a much larger scale that attempt to bridge the gap between the two
regimes. choosing the domain of movies, we provide tasks that test the ability
of models to answer factual questions (utilizing omdb), provide personalization
(utilizing movielens), carry short conversations about the two, and    nally to per-
form on natural dialogs from reddit. we provide a dataset covering    75k movie
entities and with    3.5m training examples. we present results of various models
on these tasks, and evaluate their performance.

1

introduction

with the recent employment of recurrent neural networks (id56s) and the large quantities of con-
versational data available on websites like twitter or reddit, a new type of dialog system is emerg-
ing. such end-to-end id71 (ritter et al., 2011; shang et al., 2015; vinyals & le, 2015;
sordoni et al., 2015) directly generate a response given the last user utterance and (potentially) the
context from previous dialog turns without relying on the intermediate use of a dialog state tracking
component like in traditional id71 (e.g. in henderson (2015)). these methods are trained
to imitate user-user conversations and do not need any hand-coding of attributes and labels for dia-
log states and goals like state tracking methods do. being trained on large corpora, they are robust
to many language variations and seem to mimic human conversations to some extent.

in spite of their    exibility and representational power, these neural network based methods lack
pertinent goal-oriented frameworks to validate their performance. indeed, traditional systems have
a wide range of well de   ned evaluation paradigms and benchmarks that measure their ability to
track user states and/or to reach user-de   ned goals (walker et al., 1997; paek, 2001; griol et al.,
2008; williams et al., 2013). recent end-to-end models, on the other hand, rely either on very
few human scores (vinyals & le, 2015), id104 (ritter et al., 2011; shang et al., 2015) or
machine translation metrics like id7 (sordoni et al., 2015) to judge the quality of the generated
language only. this is problematic because these evaluations do not assess if end-to-end systems
can conduct dialog to achieve pre-de   ned objectives, but simply whether they can generate correct
language that could    t in the context of the dialog; in other words, they quantify their chit-chatting
abilities.

   the    rst three authors contributed equally.

1

published as a conference paper at iclr 2016

to    ll in this gap, this paper proposes a collection of four tasks designed to evaluate different pre-
requisite qualities of end-to-end id71. focusing on the movie domain, we propose to test
if systems are able to jointly perform: (1) question-answering (qa), (2) recommendation, (3) a mix
of recommendation and qa and (4) general dialog about the topic, which we call chit-chat. all four
tasks have been chosen because they test basic capabilities we expect a dialog system performing
insightful movie recommendation should have while evaluation on each of them can be well de   ned
without the need of human-in-the-loop (e.g. via wizard-of-oz strategies (whittaker et al., 2002)).
our ultimate goal is to validate if a single model can solve the four tasks at once, which we assert is a
pre-requisite for an end-to-end dialog system supposed to act as a movie recommendation assistant,
and by extension a general dialog agent as well. at the same time we advocate developing methods
that make no special engineering for this domain, and hence should generalize to learning on tasks
and data from other domains easily.

to the babi tasks which test basic capabilities of story understanding systems
in contrast
(weston et al., 2015b), the tasks have been created using large-scale real-world sources (omdb1,
movielens2 and reddit3). overall, the dataset covers    75k movie entities (movie, actor, director,
genre, etc.) with    3.5m training examples: even if the dataset is restricted to a single domain, it is
large and allows a great variety of discussions, language and user goals. we evaluate on these tasks
the performance of various neural network models that can potentially create end-to-end dialogs,
ranging from simple supervised embedding models (bai et al., 2009), id56s with long short-term
memory (lstms) (hochreiter & schmidhuber, 1997), and attention-based models, in particular
memory networks (sukhbaatar et al., 2015). to validate the quality of our results, we also apply
our best performing model, memory networks, in other conditions by comparing it on the ubuntu
dialog corpus (lowe et al., 2015) against baselines trained by the authors of the corpus. we show
that they outperform all baselines by a wide margin.

2 the movie dialog dataset

we introduce a set of four tasks to test the ability of end-to-end id71, focusing on the
domain of movies and movie related entities. they aim to test    ve abilities which we postulate as
being key towards a fully functional general dialog system (i.e., not speci   c to movies per se):

    qa dataset: tests the ability to answer factoid questions that can be answered without

relation to previous dialog. the context consists of the question only.

    recommendation dataset: tests the ability to provide personalized responses to the user

via recommendations (in this case, of movies) rather than universal facts as above.

    qa+recommendation dataset: tests the ability of maintaining short dialogs involving

both factoid and personalized content where conversational state has to be maintained.

    reddit dataset: tests the ability to identify most likely replies in discussions on reddit.
    joint dataset: all our tasks are dialogs. they can be combined into a single dataset,

testing the ability of an end-to-end model to perform well at all skills at once.

sample input contexts and target replies from the tasks are given in tables 1-4. the datasets are
available at: http://fb.ai/babi.

2.1 id53 (qa)

the    rst task we build is to test whether a dialog agent is capable of answering simple factual
questions. the dataset was built from the open movie database (omdb)4 which contains metadata
about movies. the subset we consider contains    15k movies,    10k actors and    6k directors. we
also matched these movies to the movielens dataset5 to attribute tags to each movie. we build a
knowledge base (kb) directly from the combined data, stored as triples such as (the dark horse,

1http://en.omdb.org
2http://movielens.org
3http://reddit.com/r/movie
4downloaded from http://beforethecode.com/projects/omdb/download.aspx.
5http://grouplens.org/datasets/movielens/

2

published as a conference paper at iclr 2016

starred actor, bette davis) and (moonraker, has tag, james bond), with 8 different
relation types involving director, writer, actor, release date, genre, tags, rating and imdb votes.

we distinguish 11 classes of question, corresponding to different kinds of edges in our kb: actor
to movie (   what movies did michael j fox star in?   ), movie to actors (   who starred in back to
the future?   ), movie to director, director to movie, movie to writer, writer to movie, movie to tags,
tag to movie, movie to year, movie to genre and movie to language. for each question type there
is a set of possible answers. using simplequestions, an existing open-domain id53
dataset based on freebase (bordes et al., 2015) we identi   ed the subset of questions posed by those
human annotators that covered our question types. we expanded this set to cover all of our kb by
substituting the actual entities in those questions to also apply them to other questions, e.g. if the
original question written by an annotator was    what movies did michael j fox star in?   , we created
a pattern    what movies did [@actor] star in?    which we substitute for any other actors in our set,
and repeat this for all annotations. we split the questions into training, development and test sets
with    96k, 10k and 10k examples, respectively.

task 1: factoid id53 (qa)

what movies are about open source? revolution os
ruggero raimondi appears in which movies? carmen
what movies did darren mcgavin star in? billy madison, the night stalker, mrs. pollifax-spy
can you name a    lm directed by stuart ortiz? grave encounters
who directed the    lm white elephant? pablo trapero
what is the genre of the    lm dial m for murder? thriller, crime
what language is whity in? german

table 1: sample input contexts and target replies (in red) from task 1.

to simplify evaluation rather than requiring the generation of sentences containing the answers, we
simply ask a model to output a list, which is ranked as the possible set of answers. we then use
standard ranking metrics to evaluate the list, making the results easy to interpret. our main results
report the hits@1 metric (i.e. is the top answer correct); other metrics are given in the appendix.

2.2 recommendation dataset

not all questions about movies in dialogs have an objective answer, independent of the person
asking; indeed much of human dialog is based on opinons and personalized responses. one of the
simplest dialogs of this type to evaluate is that of recommendation, where we can utilize existing
data resources. we again employ the movielens dataset which features a user    item matrix of
movie ratings, rated from 1 to 5. we    ltered the set of movies to be the same set as in the qa task
and additionally only kept movies that had at least 2 ratings, giving around     11k movies.
to use this data for evaluating dialog, we then use it to generate dialog exchanges. we    rst select a
user at random; this will be the user who is participating in the dialog, and then sample 1-8 movies
that the user has rated 5. we then form a statement intended to express the user   s feelings about these
movies, according to a    xed set of natural language templates, one of which is selected randomly.
see table 2 for some examples. from the remaining set of movies the same user gave a rating of 5,
we select one to be the answer.

task 2: recommendation

schindler   s list, the fugitive, apocalypse now, pulp fiction, and the godfather are    lms i really liked.
can you suggest a    lm? the hunt for red october

some movies i like are heat, kids, fight club, shaun of the dead, the avengers, skyfall, and jurassic park.
can you suggest something else i might like? ocean   s eleven

table 2: sample input contexts and target replies (in red) from task 2.

there are    110k users in the training,    1k users in the development set and    1k for test. we follow
the procedure above sampling users with replacement and generate 1m training examples and 10k
development and test set examples, respectively. to evaluate the performance of a model, just as in
the    rst task, we evaluate a ranked list of answers. in our main results we measure hits@100, i.e. 1

3

published as a conference paper at iclr 2016

if the provided answer is in the top 100, and 0 otherwise, rather than hits@1 as this task is harder
than the last.

note that we expect absolute hits@k numbers to be lower for this task than for qa due to incom-
plete labeling (   missing ratings   ): in recommendation there is no exact right answer, and it is not
surprising the actual single true label is not always at the top position, i.e. the top predictions of
the model may be good as well, but we do not have their labels. one can thus view the ranking
metric as a kind of lower bound on performance of actually labeling all the predictions using human
annotations, which would be time consuming and no longer automatic, and hence undesirable for
algorithm development. this is standard in recommendation, see e.g. cremonesi et al. (2010).

2.3 qa+recommendation dialog

the tasks presented so far only involve questions followed by responses, with no context from pre-
vious dialog. this task aims at evaluating responses in the context of multiple previous exchanges,
while remaining straightforward enough that evaluation and analysis are still tractable. we hence
combine the id53 and recommendation tasks from before in a multi-response dialog,
where dialogs consist of 3 exchanges (3 turns from each participant).

the    rst exchange requires a recommendation similar to task 1 except that they also specify what
genre or topic they are interested in, e.g.    i   m looking for a music movie   , where the answer might
be    school of rock   , as in the example of table 3.

in the second exchange, given the model   s response (movie suggestion), the user asks a factoid
question about that suggestion, e.g.    what else is that about?   ,    who stars in that?    and so on. this
question refer back to the previous dialog, making context important.

in the third exchange, the user asks for a alternative recommendation, and provides extra information
about their tastes, e.g.    i like tim burton movies more   . again, context of the last two exchanges
should help for best performance.

task 3: qa + recommendation dialog

i loved billy madison, my neighbor totoro, blades of glory, bio-dome, clue, and happy gilmore.
i   m looking for a music movie. school of rock
what else is that about? music, musical, jack black, school, teacher, richard linklater, rock, guitar
i like rock and roll movies more. do you know anything else? little richard

tombstone, legends of the fall, braveheart, the net, outbreak, and french kiss are    lms i really liked.
i   m looking for a fantasy movie. jumanji
who directed that? joe johnston
i like tim burton movies more. do you know anything else? big fish

table 3: sample input contexts and target replies (in red) from task 3.

we thus generate 1m examples of such 6 line dialogs (3 turns from each participant) for training,
and    10k for development and testing respectively. we can evaluate the performance of models
across all the lines of dialog (e.g., all    30k responses from the test set), but also only on the 1st
(recommendation), 2nd (qa) or 3rd exchange (similarity) for a more    ne-grained analysis. we
again use a ranking metric (here, hits@10), just as in our previous tasks.

2.4 reddit discussion

our fourth task is to predict responses in movie discussions using real conversation data taken di-
rectly from reddit, a website where registered community members can submit content in various
areas of interest, called    subreddits   . we selected the movie subreddit6 to match our other tasks.
the original discussion data is potentially between multiple participants. to simplify the setup, we
   atten this to appear as two participants (parent and comment), just as in our other tasks. in this way
we collected    1m dialogs, of which 10k are reserved for a development set, and another 10k for the

6https://www.reddit.com/r/movies,

from
https://www.reddit.com/r/datasets/comments/3bxlg7.

selecting

the

dataset

available

at

4

published as a conference paper at iclr 2016

test set. of the dialogs,    76% involve a single exchange,    17% have at least two exchanges, and
7% have at least three exchanges (the longest exchange is length 50).

task 4: reddit discussion

i think the terminator movies really suck, i mean the    rst one was kinda ok, but after that they got really
cheesy. even the second one which people somehow think is great. and after that... forgeddabotit.
c   mon the second one was still pretty cool.. arny was still so badass, as was sararah connor   s character..
and the way they blended real action and effects was perhaps the last of its kind...

table 4: sample input contexts and target replies (in red) from task 4.

to evaluate the performance of models, we again separate the problem of evaluating the quality of a
response from that of language generation by considering a ranking setup, in line with other recent
works (sordoni et al., 2015). we proceed as follows: we select a further 10k comments for the
development set and another 10k for the test set which have not appeared elsewhere in our dataset,
and use these as potential candidates for ranking during evaluation. for each exchange, given the
input context, we rank 10001 possible candidates: the true response given in the dataset, plus the 10k
   negative    candidates just described. the model has to rank the true response as high as possible.
similar to recommendation as described before we do not expect absolute hits@k performance to
be as high as for qa due to incomplete labeling. as with task 3, we can evaluate on all the data,
or only on the 1st, 2nd or 3rd exchange, and so on. we also identi   ed the subset of the test set
where there is an entity match with at least two entities from tasks 1-3, where one of the entities
appears in the input, and the other in the response: this subset serves to evaluate the impact of using
a knowledge base for conducting such a dialog.

2.5

joint task

finally, we consider a task made of the combination of all four of the previous ones. at both training
and test time examples consist of exchanges from any of the datasets, sampled at random, whereby
the conversation is    reset    at each sample, so that the context history only ever includes exchanges
from the current conversation.

we consider this to be the most important task, as it tests whether a model can not only produce
chit-chat (task 4) but also can provide meaningful answers during dialog (tasks 1-3). on the other
hand, the point of delineating the separate tasks is to evaluate exactly which types of dialog a model
is succeeding at or not. that all the datasets are in the same domain is crucial to testing the ability of
models at performing well on all tasks jointly. if the domains were different, then the vocabularies
would be trivially non-overlapping, allowing to learn effectively separate models inside a single one.

2.6 relation to existing evaluation frameworks

traditional id71 consist of two main modules: (1) a dialog state tracking component that
tracks what has happened in a dialog, incorporating into a pre-de   ned explicit state structure sys-
tem outputs, user utterances, context from previous turns, and other external information, and (2)
a response generator. evaluation of the dialog state tracking stage is well de   ned since the par-
adise framework (walker et al., 1997) and subsequent initiatives (paek, 2001; griol et al., 2008),
including recent competitons (williams et al., 2013; henderson et al., 2014) as well as situated vari-
ants (rojas-barahona et al., 2012). however, they require    ne grained data annotations in terms of
labeling internal dialog state and precisely de   ned user intent (goals). as a result, they do not really
scale to large domains and dialogs with high variability in terms of language. because of language
ambiguity and variation, evaluation of the response generation step is complicated and usually relies
on human judgement (walker et al., 2003).

end-to-end id71 do not rely on explicit internal state and hence do not have state tracking
modules, they directly generate responses given user utterances and dialog context and hence can
not be evaluated using state tracking test-beds. unfortunately, as for response generator modules,
their evaluation is ill-de   ned as it is dif   cult to objectively rate at scale the    t of returned responses.
most existing work (ritter et al., 2011; shang et al., 2015; vinyals & le, 2015; sordoni et al., 2015)
chose to use human ratings, which does not easily scale. sordoni et al. (2015) also use the id7
score to compare to actual user utterances but this is not a completely satisfying measure of success,

5

published as a conference paper at iclr 2016

especially when used in a chit-chat setting where there are no clear goals and hence measures of
success. lowe et al. (2015) use a similar ranking evaluation to ours, but only in a chit-chat setting.

our approach of providing a collection of tasks to be jointly solved is related to the evaluation
framework of the babi tasks (weston et al., 2015a) and of the collection of sequence prediction tasks
of joulin & mikolov (2015). however, unlike them, our tasks 1-3 are much closer to real dialog,
being built from human-written text, and with task 4 actually involving real dialog from reddit. the
design of our tasks is such that all test one or more key characteristics a dialog system should have
but also that an unambiguous answer is expected after each dialog act. in that sense, it follows the
the notion of dialog evaluation by a reference answer introduced in (hirschman et al., 1990). the
application of movie recommender systems is connected to that of tv program suggestion proposed
by ramachandran et al. (2014), except that we frame it so that we can generate systematic evaluation
from it, where they only rely on human judgement at small scale.

3 models

3.1 memory networks

memory networks (weston et al., 2015c; sukhbaatar et al., 2015) are a recent class of models that
perform language understanding by incorporaring a memory component that potentially includes
both long-term memory (e.g., to remember facts about the world) and short-term context (e.g.,
the last few turns of dialog). they have only been evaluated in a few setups: id53
(bordes et al., 2015), id38 (sukhbaatar et al., 2015; hill et al., 2015), and language
understanding on the babi tasks (weston et al., 2015a), but not so far on dialog tasks such as ours.

we employ the memn2n architecture of sukhbaatar et al. (2015) in our experiments, with some
additional modi   cations to construct both long-term and short-term context memories. at any given
time step we are given as input the history of the current conversation: messages from the user cu
i at
i from the model itself at the corresponding time steps,
time step i and the corresponding responses cr
t and the model has to respond.
i = 1, . . . , t     1. at the current time t we are only given the input cu

retrieving long-term memories for each word in the last n messages we perform a hash lookup
to return all long-term memories (sentences) from a database that also contain that word. words
above a certain frequency cutoff can be ignored to avoid sentences that only share syntax or unim-
portant words. we employ the movie knowledge base of sec. 2.1 for our long-term memories, but
potentially any text dataset could be used. see figure 5 for an example of this process.

attention over memories the sentences hj, j = 1, . . . , h returned from the hashing step plus
the messages from the current conversation form the memory of the memory network7:

x = (cu

1 , . . . , cu

t   1, cr

1, . . . , cr

t   1, h1, . . . , hh ).

t is embedded using a matrix a of size d    v where d is the embedding
the last user input cu
dimension and v is the size of the vocabulary, giving u = acu
t . each memory xi is embedded using
the same matrix, giving mi = axi. the match between the input and the memories is then computed
by taking the inner product followed by a softmax: pi = softmax(u   mi) giving a id203 vector
over the memories. the output memory representation is then constructed with o = r pi pimi
where r is a d  d rotation matrix8. the memory output is then added to the original input q = o+cu
t .
this procedure can then be stacked in what is called multiple    hops    of attention over the memory.

generating the    nal prediction the    nal prediction is
  a =
softmax(q   w y1, . . . , q   w yc ) where there are c candidate responses in y, and w is of dimension
v    d. for tasks 1-3 the candidates are the set of words in the vocabulary, which are ranked for
   nal evaluation, whereas for task 4 the candidates are target respones (sentences).

then de   ned as:

the whole model is trained using stochastic id119 by minimizing a standard cross-id178
loss between   a and the true label a.

7we also add time features to each memory to denote their position following (sukhbaatar et al., 2015).
8optionally, different dictionaries can be used for inputs, memories and outputs instead of being shared.

6

published as a conference paper at iclr 2016

long-term
memories hi

1

short-term cu
memories
cr
input

cu

1

2

output

y

shaolin soccer directed by stephen chow
shaolin soccer written by stephen chow
shaolin soccer starred actors stephen chow
shaolin soccer release year 2001
shaolin soccer has genre comedy
shaolin soccer has tags martial arts, kung fu soccer, stephen chow
kung fu hustle directed by stephen chow
kung fu hustle written by stephen chow
kung fu hustle starred actors stephen chow
kung fu hustle has genre comedy action
kung fu hustle has imdb votes famous
kung fu hustle has tags comedy, action, martial arts, kung fu, china, soccer, hong kong, stephen chow
the god of cookery directed by stephen chow
the god of cookery written by stephen chow
the god of cookery starred actors stephen chow
the god of cookery has tags hong kong stephen chow
from beijing with love directed by stephen chow
from beijing with love written by stephen chow
from beijing with love starred actors stephen chow, anita yuen

. . . <and more> . . .

1) i   m looking a fun comedy to watch tonight, any ideas?
2) have you seen shaolin soccer? that was zany and great.. really funny but in a whacky way.
3) yes! shaolin soccer and kung fu hustle are so good i really need to    nd some more stephen chow
   lms i feel like there is more awesomeness out there that i haven   t discovered yet ...
4) god of cookery is pretty great, one of his mid 90   s hong kong martial art comedies.

table 5: memory network long-term and short-term memories. blue underlined text indicates
those words that hashed into the knowledge base to recall sentences from the long-term memory.
those, along with the recent short-term context (lines labeled 1 and 2) are used as input memories
to the memory network along with the input (labeled 3). the desired goal is to output dialog line 4.

3.2 supervised embedding models

while one of the major uses of id27 models is to learn unsupervised embeddings over
large unlabeled datasets such as in id97 (mikolov et al., 2013) there are also very effective
id27 models for training supervised models when labeled data is available. the sim-
plest approach which works suprisingly well is to sum the id27s of the input and the
target independently and then compare them with a similarity metric such as inner product or co-
sine similarity. a ranking loss is used to ensure the correct targets are ranked higher than any other
targets. several variants of this approach exist. for matching two documents supervised semantic
indexing (ssi) was shown to be superior to unsupervised id45 (lsi) (bai et al.,
2009). similar methods were shown to outperform svd for recommendation (weston et al., 2013).
however, we do not expect this method to work as well on id53 tasks, as all the
memorization must occur in the individual id27s, which was shown to perform poorly
in (bordes et al., 2014). for example, consider asking the question    who was born in paris?    and re-
quiring the id27 for paris to effectively contain all the pertinent information. however,
for rarer items requiring less storage, performance may not be as degraded. in general we believe
this is a surprisingly strong baseline that is often neglected in evaluations. our implementation
corresponds to a memory network with no attention over memory.

3.3 recurrent language models

recurrent neural networks (id56s) have proven successful at several tasks involving natural
language, id38 (mikolov et al., 2011), and have been applied recently to dialog
(sordoni et al., 2015; vinyals & le, 2015; shang et al., 2015). lstms are not known however for
tasks such as qa or item recommendation, and so we expect them to    nd our datasets challenging.

there are a large number of variants of id56s,
including long-short term memory activa-
tion units (lstms) (hochreiter & schmidhuber, 1997), bidirectional lstms (graves et al., 2012),
id195 models (sutskever et al., 2014), id56s that take into account the document context
(mikolov & zweig, 2012) and id56s that perform attention over their input in various different
ways (bahdanau et al., 2015; hermann et al., 2015; rush et al., 2015). evaluating all these variants

7

published as a conference paper at iclr 2016

is beyond the scope of this work and we instead use standard lstms as our baseline method9. how-
ever, we note that lstms with attention have many properties in common with memory networks
if the attention is applied over the same memory setup.

3.4 id53 systems

for the particular case of task 1 we can apply existing id53 systems. there has
been a recent surge in interest in such systems that try to answer a question posed in natural
language by converting it into a database search over a knowledge base (berant & liang, 2014;
kwiatkowski et al., 2013; fader et al., 2014), which is a setup natural for our qa task also. how-
ever, such systems cannot easily solve any of our other tasks, for example our recommendation
task 2 does not involve looking up a factoid answer in a database. nevertheless, this allows us
to compare the performance of end-to-end systems performant on all our tasks to a standard qa
benchmark. we chose the method of bordes et al. (2014)10 as our baseline. this system learns em-
beddings that match questions to database entries, and then ranks the set of entries, and has been
shown to achieve good performance on the webquestions benchmark (berant et al., 2013).

3.5 singular value decomposition

singular value decomposition (svd) is a standard benchmark for recommendation, being at the
core of the best ensemble results in the net   ix challenge, see koren & bell (2011) for a review.
however, it has been shown to be outperformed by other    avors of id105, in particular
by using a ranking loss rather than squared loss (weston et al., 2013) which we will compare to (cf.
sec 3.2), as well as improvements like svd++ (koren, 2008). collaborative    ltering methods are
applicable to task 2, but cannot easily be used for any of the other tasks. even for task 2, while
our dialog models use textual input, as shown in table 2, svd requires a user    item matrix, so for
this baseline we preprocessed the text to assign each entity an id, and throw away all other text. in
contrast, the end-to-end dialog models have to learn to process the text as part of the task.

3.6

information retrieval models

to select candidate responses a standard baseline is nearest neighbour information retrieval (ir)
(isbell et al., 2000; jafarpour et al., 2010; ritter et al., 2011; sordoni et al., 2015). two simple vari-
ants are often tried: given an input message, either (i)    nd the most similar message in the (training)
dataset and output the response from that exchange; or (ii)    nd the most similar response to the
input directly. in both cases the standard measure of similarity is tf-idf weighted cosine similarity
between the bags of words. note that that the supervised embedding models of sec. 3.2 effectively
implement the same kind of model (ii) but with a learnt similarity measure. it has been shown pre-
viously that method (ii) performs better (ritter et al., 2011), and our initial ir experiments showed
the same result. note that while (non-learning) ir systems can also be applied to other tasks such as
qa (kolomiyets & moens, 2011) they require signi   cant tuning to do so. here we stick to a vanilla
vector space model and hence only apply an ir baseline to task 4.

4 results

our main results across all the models and tasks are given in table 4. supervised embeddings and
memory networks are tested in two settings: trained and tested on all tasks separately, or jointly on
the combined task 5. other methods are only evaluated on independent tasks. in all cases, parameter
search was performed on the development sets; parameter choices are provided in the appendix.

answering factual questions memory networks and the baseline qa system are the two meth-
ods that have an explicit long-term memory via access to the knowledge base (kb). on the task of
answering factual questions where the answers are contained in the kb, they outperform the other
methods convincingly, with lstms being particularly poor. the latter is not unexpected as that
method is good at id38, not id53, see e.g. weston et al. (2015b). the

9we used the code available at: https://github.com/facebook/scid56s
10we used the    path representation    for the knowledge base, as described in sec. 3.1 of bordes et al. (2014).

8

published as a conference paper at iclr 2016

methods
qa system (bordes et al., 2014)
svd
ir
lstm
supervised embeddings
memn2n
joint supervised embeddings
joint memn2n

qa task
(hits@1)

recs task
(hits@100)

qa+recs task reddit task
(hits@10)

(hits@10)

90.7
n/a
n/a
6.5
50.9
79.3
43.6
83.5

n/a
19.2
n/a
27.1
29.2
28.6
28.1
26.5

n/a
n/a
n/a
19.9
65.9
81.7
58.9
78.9

n/a
n/a
23.7
11.8
27.6
29.2
14.5
26.6

table 6: test results across all tasks. of those methods tested, supervised embeddings, lstms
and memn2n are easily applicable to all tasks. the other methods are standard benchmarks for
individual tasks. the    nal two rows are models trained on the combined task, of all tasks at once.
evaluation uses the hits@k metric (in percent) with the value of k given in the second row.

baseline qa system, which is designed for this task, is superior to memory networks, indicating
there is still room for improvement in that model. on the other hand, the latter   s much more general
design allows it to perform well on our other dialog tasks, whereas the former is task speci   c.

making recommendations
in this task a long-term memory does not bring any improvement,
with lstms, supervised embeddings and memory networks all performing similarly, and all out-
performing the svd baseline. here, we conjecture lstms can perform well because it looks much
more like a id38 task, i.e. the input is a sequence of similar recommendations.

using dialog history in both qa+recommendations (task 3) and reddit (task 4) memory net-
works outperform supervised embeddings due to their better use of context. this can be seen by
breaking down the results by length of context: in the    rst response they perform similarly, but
memory networks show a relative improvement on the second and third responses, see tables 9
and 10 in the appendix. note that these improvements come from the short term memory (dialog
history), not from the use of the kb, as we show memory networks results without access to the
kb and they perform similarly. we believe the qa performance in these cases is not hindered by
the lack of a kb because we ask questions based on fewer relations than in task 1 and it is easier
to store the knowledge directly in the id27s. the baseline ir model in task 4 bene-
   ts from context too, it is compared with and without in table 10. lstms perform poorly: the
posts in reddit are quite long and the memory of the lstm is relatively short, as pointed out by
sordoni et al. (2015). in that work they employed a linear reranker that used lstm prediction as
features to better effect. testing more powerful recurrent networks such as lstms with attention on
these benchmarks remains as future work (although the latter is related to memory networks, which
we do report).

joint learning a truly end-to-end dialog system has to be good at all the skills in tasks 1-4 (and
more besides, i.e. this is necessary, but not suf   cient). we thus report results on our combined task
for supervised embeddings and memory networks. supervised embeddings still have the same
failings as before on tasks 1 and 3, but now seem to perform even more poorly due to the dif   culty
of encoding all the necessary skills in the id27s, so e.g., they now do signi   cantly
worse on task 4. this is despite us trying id27s of up to 2000 dimensions. memory
networks fare better, having only a slight loss in performance on tasks 2-4 and a slight gain in
task 1.
in their case, the modeling power is not only in the id27s, but also in the
attention over the long-term and short-term memory, so it does not need as much capacity in the
id27s. however, the best achievable models would presumably have some improvement
from training across all the tasks, not a loss, and would perform at least as well as all the individual
task baselines (i.e. in this case, perform better at task 1).

5 ubuntu dialogue corpus results

as no other authors have yet published results on our new benchmark, to validate the quality of our
results we also apply our best performing model in other conditions by comparing it on the ubuntu
dialog corpus (lowe et al., 2015). in particular, this also allows us to compare to more sophisticated

9

published as a conference paper at iclr 2016

methods
ir   
id56   
lstm   
memn2n 1-hop
memn2n 2-hops
memn2n 3-hops
memn2n 4-hops

validation
(hits@1)

test

(hits@1)

n/a
n/a
n/a
57.23
64.28
64.31
64.01

48.81
37.91
55.22
56.25
63.51
63.72
62.82

table 7: ubuntu dialog corpus results. the evaluation is retrieval-based, similar to that of reddit
(task 4). for each dialog, the correct answer is mixed among 10 random candidates; hits@1 (in %)
are reported. methods with     have been ran by lowe et al. (2015).

lstms models that are trained discriminatively using metric learning, as well as additional baseline
methods all trained by the authors. the ubuntu dialog corpus contains almost 1m dialogs of more
than 7 turns on average (900k dialogs for training, 20k for validation and 20k for testing), and 100m
million words. the corpus was scraped from the ubuntu irc channel logs where users ask questions
about issues they are having with ubuntu and get answers by other users. most chats can involve
more than two users but a series of heuristics to disentangle them into dyadic dialogs was used.

the evaluation is similar to that of reddit (task 4): each correct answer has to be retrieved among a
set of 10, mixed with 9 randomly chosen candidate utterances. we report the hits@1 in table 7.11
we used the same memn2n architecture as before. all models were selected using validation accu-
racy. on this dataset, which has longer dialogs than those from the movie dialog corpus, we can
see that running more hops on the memory with the memn2n improves performance: the 1-hop
model performs similarly to the lstm but with 2-hops and more we can gain more than a +8%
increase over the previous best reported model. using even more hops still improves over 1-hop but
not much over 2-hops.

6 conclusion

we have presented a new set of benchmark tasks designed to evaluate end-to-end id71.
the movie dialog dataset measures how well such models can perform at both goal driven dialog,
of both objective and subjective goals thanks to id74 on id53 and rec-
ommendation tasks, and at less goal driven chit-chat. a true end-to-end model should perform well
at all these tasks, being a necessary but not suf   cient condition for a fully functional dialog agent.

we showed that some end-to-end neural networks models can perform reasonably across all tasks
compared to standard per-task baselines. speci   cally, memory networks that incorporate short and
long term memory can utilize local context and knowledge bases of facts to boost performance.
we believe this is promising because we showed these same architectures also perform well on a
separate dialog task, the ubuntu dialog corpus, and have been shown previously to work well on
the synthetic but challenging babi tasks of weston et al. (2015a), and have no special engineering
for the tasks or domain. however, some limitations remain, in particular they do not perform as well
as stand-alone qa systems for qa, and performance is also degraded rather than improved when
training on all four tasks at once. future work should try to overcome these problems.

while our dataset focused on movies, there is nothing speci   c to the task design which could not
be transferred immediately to other domains, for example sports, music, restaurants, and so on.
future work should create new tasks in this and other domains to ensure that models are    rstly not
overtuned for these goals, and secondly to test further skills     and to motivate the development of
algorithms to be skillful at them.

references

bahdanau, dzmitry, cho, kyunghyun, and bengio, yoshua. id4 by jointly

learning to align and translate. iclr 2015, 2015.

11results for the baselines from (lowe et al., 2015) differ to that from the v3 of the arxiv paper, because the

corpus has been updated since then. all results in table 7 use the latest version of the corpus.

10

published as a conference paper at iclr 2016

bai, bing, weston, jason, grangier, david, collobert, ronan, sadamasa, kunihiko, qi, yanjun,
chapelle, olivier, and weinberger, kilian. supervised semantic indexing. in proceedings of the
18th acm conference on information and knowledge management, pp. 187   196. acm, 2009.

berant, jonathan and liang, percy. id29 via id141. in proceedings of the 52nd
annual meeting of the association for computational linguistics (acl   14), baltimore, usa,
2014.

berant, jonathan, chou, andrew, frostig, roy, and liang, percy. id29 on freebase from

question-answer pairs. in emnlp, pp. 1533   1544, 2013.

bordes, antoine, chopra, sumit, and weston, jason. id53 with subgraph embed-

dings. in proc. emnlp, 2014.

bordes, antoine, usunier, nicolas, chopra, sumit, and weston, jason. large-scale simple question

answering with memory networks. arxiv preprint arxiv:1506.02075, 2015.

cremonesi, paolo, koren, yehuda, and turrin, roberto. performance of recommender algorithms
on top-n recommendation tasks. in proceedings of the fourth acm conference on recommender
systems, pp. 39   46. acm, 2010.

fader, anthony, zettlemoyer, luke, and etzioni, oren. open id53 over curated and
extracted knowledge bases. in proceedings of 20th sigkdd conference on knowledge discovery
and data mining (kdd   14), new york city, usa, 2014. acm.

graves, alex et al. supervised sequence labelling with recurrent neural networks, volume 385.

springer, 2012.

griol, david, hurtado, llu    s f, segarra, encarna, and sanchis, emilio. a statistical approach to

spoken id71 design and evaluation. speech communication, 50(8):666   682, 2008.

henderson, matthew. machine learning for dialog state tracking: a review. in proceedings of the

first international workshop on machine learning in spoken language processing, 2015.

henderson, matthew, thomson, blaise, and williams, jason. the second dialog state tracking
challenge. in 15th annual meeting of the special interest group on discourse and dialogue, pp.
263, 2014.

hermann, karl moritz, ko  cisk  y, tom  a  s, grefenstette, edward, espeholt, lasse, kay, will,
teaching machines to read and compre-
url

in advances in neural information processing systems (nips), 2015.

suleyman, mustafa, and blunsom, phil.
hend.
http://arxiv.org/abs/1506.03340.

hill, felix, bordes, antoine, chopra, sumit, and weston, jason. the goldilocks principle: reading
children   s books with explicit memory representations. arxiv preprint arxiv:1511.02301, 2015.

hirschman, lynette, dahl, deborah a, mckay, donald p, norton, lewis m, and linebarger, mar-
cia c. beyond class a: a proposal for automatic evaluation of discourse. technical report, dtic
document, 1990.

hochreiter, sepp and schmidhuber, j  urgen. long short-term memory. neural computation, 9(8):

1735   1780, 1997.

isbell, charles lee, kearns, michael, kormann, dave, singh, satinder, and stone, peter. cobot in

lambdamoo: a social statistics agent. in aaai/iaai, pp. 36   41, 2000.

jafarpour, sina, burges, christopher jc, and ritter, alan. filter, rank, and transfer the knowledge:

learning to chat. advances in ranking, 10, 2010.

joulin, armand and mikolov, tomas. inferring algorithmic patterns with stack-augmented recurrent

nets. arxiv preprint: 1503.01007, 2015.

kolomiyets, oleksandr and moens, marie-francine. a survey on id53 technology

from an information retrieval perspective. information sciences, 181(24):5412   5434, 2011.

11

published as a conference paper at iclr 2016

koren, yehuda. factorization meets the neighborhood: a multifaceted collaborative    ltering model.
in proceedings of the 14th acm sigkdd international conference on knowledge discovery and
data mining, pp. 426   434. acm, 2008.

koren, yehuda and bell, robert. advances in collaborative    ltering.

handbook, pp. 145   186. springer, 2011.

in recommender systems

kwiatkowski, tom, choi, eunsol, artzi, yoav, and zettlemoyer, luke. scaling semantic parsers
with on-the-   y ontology matching. in proceedings of the 2013 conference on empirical methods
in natural language processing (emnlp   13), seattle, usa, october 2013.

lowe, ryan, pow, nissan, serban, iulian, and pineau, joelle. the ubuntu dialogue corpus:
arxiv preprint

a large dataset for research in unstructured multi-turn dialogue systems.
arxiv:1506.08909, 2015.

mikolov, tomas and zweig, geoffrey. context dependent recurrent neural network language model.

in slt, pp. 234   239, 2012.

mikolov, tom  a  s, kombrink, stefan, burget, luk  a  s,   cernock`y, jan honza, and khudanpur, san-
jeev. extensions of recurrent neural network language model. in acoustics, speech and signal
processing (icassp), 2011 ieee international conference on, pp. 5528   5531. ieee, 2011.

mikolov, tomas, chen, kai, corrado, greg, and dean, jeffrey. ef   cient estimation of word repre-

sentations in vector space. arxiv:1301.3781, 2013.

narasimhan, karthik, kulkarni, tejas, and barzilay, regina. language understanding for text-based

games using deep id23. arxiv preprint arxiv:1506.08941, 2015.

paek, tim. empirical methods for evaluating id71. in proceedings of the workshop on
evaluation for language and dialogue systems-volume 9, pp. 2. association for computational
linguistics, 2001.

ramachandran, deepak, yeh, peter z, jarrold, william, douglas, benjamin, ratnaparkhi, adwait,
provine, ronald, mendel, jeremy, and em   eld, adam. an end-to-end dialog system for tv pro-
gram discovery.
in spoken language technology workshop (slt), 2014 ieee, pp. 602   607.
ieee, 2014.

ritter, alan, cherry, colin, and dolan, william b. data-driven response generation in social media.
in proceedings of the conference on empirical methods in natural language processing, pp.
583   593. association for computational linguistics, 2011.

rojas-barahona, lina m, lorenzo, alejandra, and gardent, claire. an end-to-end evaluation of two
situated id71. in proceedings of the 13th annual meeting of the special interest group
on discourse and dialogue, pp. 10   19. association for computational linguistics, 2012.

rush, alexander m, chopra, sumit, and weston, jason. a neural attention model for abstractive

sentence summarization. proceedings of emnlp, 2015.

shang, lifeng, lu, zhengdong, and li, hang. neural responding machine for short-text conversa-

tion. arxiv preprint arxiv:1503.02364, 2015.

sordoni, alessandro, galley, michel, auli, michael, brockett, chris, ji, yangfeng, mitchell, mar-
garet, nie, jian-yun, gao, jianfeng, and dolan, bill. a neural network approach to context-
sensitive generation of conversational responses. proceedings of naacl, 2015.

sukhbaatar, sainbayar, szlam, arthur, weston, jason, and fergus, rob. end-to-end memory net-

works. proceedings of nips, 2015.

sutskever, ilya, vinyals, oriol, and le, quoc vv. sequence to sequence learning with neural net-

works. in advances in neural information processing systems, pp. 3104   3112, 2014.

vinyals, oriol and le, quoc. a neural conversational model. arxiv preprint arxiv:1506.05869,

2015.

12

published as a conference paper at iclr 2016

walker, marilyn a, litman, diane j, kamm, candace a, and abella, alicia. paradise: a framework
for evaluating spoken dialogue agents. in proceedings of the eighth conference on european chap-
ter of the association for computational linguistics, pp. 271   280. association for computational
linguistics, 1997.

walker, marilyn a, prasad, rashmi, and stent, amanda. a trainable generator for recommendations

in multimodal dialog. in interspeech, 2003.

weston, j., bordes, a., chopra, s., and mikolov, t. towards ai-complete id53: a

set of prerequisite toy tasks. arxiv preprint: 1502.05698, 2015a.

weston, jason, yee, hector, and weiss, ron j. learning to rank recommendations with the k-order
statistic loss. in proceedings of the 7th acm conference on recommender systems, pp. 245   248.
acm, 2013.

weston, jason, bordes, antoine, chopra, sumit, and mikolov, tomas. towards ai-complete question

answering: a set of prerequisite toy tasks. arxiv preprint arxiv:1502.05698, 2015b.

weston, jason, chopra, sumit, and bordes, antoine. memory networks. proceedings of iclr,

2015c.

whittaker, steve, walker, marilyn a, and moore, johanna d. fish or fowl: a wizard of oz evaluation

of dialogue strategies in the restaurant domain. in lrec, 2002.

williams, jason, raux, antoine, ramachandran, deepak, and black, alan. the dialog state tracking

challenge. in proceedings of the sigdial 2013 conference, pp. 404   413, 2013.

13

published as a conference paper at iclr 2016

a further experimental details

dictionary for all models we built a dictionary using all the known entities in the kb (e.g.    bruce
willis    and    die hard    are single dictionary elements). this allows us to output a single symbol for
qa and recommendation in order to predict an entity, rather than having to construct the answer
out of words, making training and evaluation of the task simpler. the rest of the dictionary is built
of unigrams that are not covered by our entity dictionary, where we removed other words (but not
entities) with frequency less than 5. overall this gives a dictionary of size 189472, which includes
75542 entities. all entries and texts were lower-cased. our text parser to convert to the dictionary
representation is then very simple: it goes left to right, consuming the largest id165 at each step.

memory networks for most of the tasks the optimal number of hops was 1, except for task 3
where 2 or 3 hops outperform 1. see table 9 and the parameter choices in sec. b. for the joint
task (task 5), to achieve best performance we increased the capacity compared to the individual
task models by using different dictionaries for the input, memory and output layers, see sec. b.
additionally, we pre-trained the weights by training without the long-term memory for speed.

supervised embedding models we tried two    avors of supervised embedding model: (i) a model
f (x, y) = x   u    u y (   single dictionary model   ); and (ii) a model f (x, y) = x   u    v y (   two
dictionary model   ). that is, the latter has two sets of id27s depending on whether the
word is in the input+context, or the label. the input and context are concatenated together to form
a bag of words in either case. it turns out method (i) works better on tasks 1 and 4, and method
(ii) works better on tasks 2 & 3. some of the reasons why that is so are easy to understand: on
tasks 2 and 3 (recommendations) a single dictionary model favors predicting the same movies that
are already in the input context, which are never correct. however, it appears that on tasks 1 and 4
the two dictionary model appears to over   t to some degree. this partially explains why the model
overall is worse on the joint dataset (task 5). see sec. b for more details.

lstms lstms performed poorly on task 4 and we spent some time trying to improve these re-
sults. despite the perplexity looking reasonable (   96 on the training set, and    105 on the validation
set) after training for    6 days, we still obtain poor results distinguishing between candidates. we
also tried id195 models (without attention or metric learning) and did not obtain improvements.
part of the problem is that posts in reddit vary from very short (a few words) to very long (several
paragraphs) and one natural procedure to try     computing the id203 of those sequences seeded
by the input     gives very unbalanced results, and tends to select the shorter ones, ending up with
worse than random performance. further, computationally the whole procedure is then very slow
compared to all other methods tested. memory networks and supervised embeddings need to com-
pute the inner product between embedded inputs and outputs, and hence the the candidates can be
embedded once and cached for the whole test set. this trick is not applicable to the method described
above rendering it much slower. to deal with the speed issue one can use our supervised embedding
model as a    rst step, and then only reranking the top 100 results with the lstm to make it tractable,
however performance is still poor as mentioned. we obtained improved results by instead adopting
the approach of narasimhan et al. (2015): we take the representation for a dialog message as the
average embedding over the hidden states as the symbols are consumed (at each step of the recur-
rence). we also note that lowe et al. (2015) report good results (on a different dataset, the ubuntu
corpus) by training an additional metric learner on top of an lstm representation, which we have
not tried. however, we do compare that approach to memory networks on that corpus in section 5.

information retrieval aside from the models described in the main paper, we also experimented
with a hybrid relevance feedback approach:    nd the most similar message in the history, add the
response to the query (with a certain weight) and then score candidate responses with the combined
input. however, the relevance feedback model did not help: as we increase the feedback parameter
(how much to use the retrieved response) the model only degrades, see table 10 for the performance
adding with a weight of 0.5.

b optimal hyper-parameter values

hyperparameters of all learning models have been set using grid search on the validation set. the
main hyperparameters are embedding dimension d, learning rate   , number of dictionaries w, num-

14

published as a conference paper at iclr 2016

ber of hops k for memnns and unfolding depth blen for lstms. all models are implemented in
the torch library (see torch.ch).

task 1 (qa)

    qa system of bordes et al. (2014):    = 0.001, d = 50.
    supervised embedding model:    = 0.05, d = 50, w = 1.
    memn2n:    = 0.005, d = 50, w = 1, k = 1.
    lstm:    = 0.001, d = 100, blen = 10.

task 2 (recomendation)

    svd: d = 50.
    supervised embedding model:    = 0.005, d = 200, w = 2.
    memn2n:    = 0.01, d = 1000, w = 1, k = 1.
    lstm:    = 0.01, d = 100, blen = 10.

task 3 (qa+recommendation)

    supervised embedding model:    = 0.005, d = 1000, w = 2.
    memn2n:    = 0.001, d = 50, w = 1, k = 3.
    lstm:    = 0.001, d = 100, blen = 10.

task 4 (reddit)

    supervised embedding model:    = 0.1, d = 1000, w = 1.
    memn2n:    = 0.01, d = 1000, w = 1, k = 1.
    lstm:    = 0.01, d = 512, blen = 15.

joint task we chose hyperparameters by taking the mean performance over the four tasks, after
scaling each task by the best performing model on that task on the development set in order to
normalize the metrics.

    supervised embedding model:    = 0.01, d = 1000, w = 2.
    memn2n:    = 0.005, d = 1000, w = 3.

ubuntu dialog corpus hyperparameters of the memn2n have been set using grid search on the
validation set. we report the best models with k = 1, 2, 3, 4 in the paper; other hyperparameters
were    = 0.001, d = 256.

15

published as a conference paper at iclr 2016

c further detailed results

c.1 breakdown of task 1 (qa) results by question type

task
writer to movie
tag to movie
movie to year
movie to writer
movie to tags
movie to language
movie to genre
movie to director
movie to actors
director to movie
actor to movie
total

qa system of

bordes et al. (2014)
h@1
98.7
71.8
89.8
88.8
84.5
94.6
93.0
88.2
88.5
98.3
98.9
90.7

h@10
98.7
71.8
89.8
89.5
85.3
94.8
93.5
88.2
88.5
98.3
98.9
91.0

supervised
embeddings
h@10
h@1
90.8
77.3
96.1
53.4
3.4
25.4
93.6
61.7
92.0
36.8
84.7
45.2
95.0
46.4
90.1
52.3
64.5
95.2
93.8
61.4
89.4
79.0
50.9
82.97

memn2n

h@1
77.6
61.4
87.3
73.5
79.9
90.1
92.5
78.3
68.4
71.5
76.7
78.9

h@10
95.5
88.6
92.1
84.1
95.1
97.6
99.4
87.1
87.2
91.0
96.7
91.8

table 8: qa task test performance per question type (h@1 / h@10 metrics).

c.2 breakdown of task 3 (qa+recommendation) results by response type

response 1 response 2 response 3
(similar)

methods
supervised embeddings
lstm
memn2n (1 hop)
memn2n (2 hops)
memn2n (3 hops)
memn2n (3 hops, -kb)

whole
test set

56.0
19.9
70.5
76.8
75.4
75.9

(recs)
56.7
35.3
47.0
53.4
52.6
54.3

(qa)
76.2
14.3
89.2
90.1
90.0
85.0

38.8
9.2
76.5
88.6
84.2
91.5

table 9: qa+recommendation task test results (h@10 metric). the last row shows memn2n
without access to a long-term memory (kb).

c.3 breakdown of task 4 (reddit) results by response type

methods
ir (query+context)
ir (query)
ir (query) rf=0.05
supervised embeddings
memn2n (-kb)
memn2n

whole
test set matched response 1 response 2 response 3+

entity

23.7
23.1
19.2
27.6
29.6
29.2

49.0
48.3
40.8
54.1
57.0
56.4

21.1
21.1
18.3
24.8
25.6
25.4

26.4
25.7
21.2
30.4
34.2
32.9

30.0
27.9
21.4
33.1
37.2
37.0

table 10: reddit task test results (h@10 metric). memn2n (-kb) is the memory network
model without access to the knowledge base.

16

