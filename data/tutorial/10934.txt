multi-domain neural network language generation for

spoken dialogue systems

tsung-hsien wen, milica ga  si  c, nikola mrk  si  c, lina m. rojas-barahona,

pei-hao su, david vandyke, steve young

cambridge university engineering department,
trumpington street, cambridge, cb2 1pz, uk

{thw28,mg436,nm480,lmr46,phs26,djv27,sjy}@cam.ac.uk

6
1
0
2

 
r
a

m
3

 

 
 
]
l
c
.
s
c
[
 
 

1
v
2
3
2
1
0

.

3
0
6
1
:
v
i
x
r
a

abstract

moving from limited-domain natural
lan-
guage generation (id86) to open domain is
dif   cult because the number of semantic in-
put combinations grows exponentially with
the number of domains. therefore, it is im-
portant to leverage existing resources and ex-
ploit similarities between domains to facilitate
id20. in this paper, we propose
a procedure to train multi-domain, recurrent
neural network-based (id56) language gen-
erators via multiple adaptation steps. in this
procedure, a model is    rst trained on counter-
feited data synthesised from an out-of-domain
dataset, and then    ne tuned on a small set of
in-domain utterances with a discriminative ob-
jective function. corpus-based evaluation re-
sults show that the proposed procedure can
achieve competitive performance in terms of
id7 score and slot error rate while signi   -
cantly reducing the data needed to train gen-
erators in new, unseen domains. in subjective
testing, human judges con   rm that the proce-
dure greatly improves generator performance
when only a small amount of data is available
in the domain.

1

introduction

modern spoken dialogue systems (sds) are typi-
cally developed according to a well-de   ned ontol-
ogy, which provides a structured representation of
the domain data that the dialogue system can talk
about, such as searching for a restaurant or shop-
ping for a laptop. unlike conventional approaches
employing a substantial amount of handcrafting for

each individual processing component (ward and is-
sar, 1994; bohus and rudnicky, 2009), statistical ap-
proaches to sds promise a domain-scalable frame-
work which requires a minimal amount of human in-
tervention (young et al., 2013). mrk  si  c et al. (2015)
showed improved performance in belief tracking by
training a general model and adapting it to speci   c
domains. similar bene   t can be observed in ga  si  c
et al. (2015), in which a bayesian committee ma-
chine (tresp, 2000) was used to model policy learn-
ing in a multi-domain sds regime.

in past decades, adaptive id86 has been stud-
ied from linguistic perspectives, such as systems
that learn to tailor user preferences (walker et al.,
2007), convey a speci   c personality trait (mairesse
and walker, 2008; mairesse and walker, 2011),
or align with their conversational partner (isard
et al., 2006). id20 was    rst ad-
dressed by hogan et al. (2008) using a generator
based on the lexical functional grammar (lfg) f-
structures (kaplan and bresnan, 1982). although
these approaches can model rich linguistic phe-
nomenon,
they are not readily adaptable to data
since they still require many handcrafted rules to
de   ne the search space. recently, id56-based lan-
guage generation has been introduced (wen et al.,
2015a; wen et al., 2015b). this class of statistical
generators can learn generation decisions directly
from dialogue act (da)-utterance pairs without any
semantic annotations (mairesse and young, 2014)
or hand-coded grammars (langkilde and knight,
1998; walker et al., 2002). many existing adapta-
tion approaches (wen et al., 2013; shi et al., 2015;
chen et al., 2015) can be directly applied due to the

   exibility of the underlying id56 language model
(id56lm) architecture (mikolov et al., 2010).

discriminative training (dt) has been success-
fully used to train id56s for various tasks. by op-
timising directly against the desired objective func-
tion such as id7 score (auli and gao, 2014) or
word error rate (kuo et al., 2002), the model can
explore its output space and learn to discriminate be-
tween good and bad hypotheses. in this paper we
show that dt can enable a generator to learn more
ef   ciently when in-domain data is scarce.

the paper presents an incremental recipe for
training multi-domain language generators based on
a purely data-driven, id56-based generation model.
following a review of related work in section 2, sec-
tion 3 describes the detailed id56 generator archi-
tecture. the data counterfeiting approach for syn-
thesising an in-domain dataset is introduced in sec-
tion 4, where it is compared to the simple model
   ne-tuning approach.
in section 5, we describe
our proposed dt procedure for training natural lan-
guage generators. following a brief review of the
data sets used in section 6, corpus-based evaluation
results are presented in section 7. in order to assess
the subjective performance of our system, a quality
test and a pairwise preference test are presented in
section 8. the results show that the proposed adap-
tation recipe improves not only the objective scores
but also the user   s perceived quality of the system.
we conclude with a brief summary in section 9.

2 related work

id20 problems arise when we have a
suf   cient amount of labeled data in one domain (the
source domain), but have little or no labeled data in
another related domain (the target domain). domain
adaptability for real world speech and language ap-
plications is especially important because both lan-
guage usage and the topics of interest are constantly
evolving. historically, id20 has been
less well studied in the id86 community. the most
relevant work was done by hogan et al. (2008).
they showed that an lfg f-structure based gener-
ator could yield better performance when trained on
in-domain sentences paired with pseudo parse tree
inputs generated from a state-of-the-art, but out-of-
domain parser. the spot-based generator proposed

by walker et al. (2002) has the potential to address
id20 problems. however, their pub-
lished work has focused on tailoring user prefer-
ences (walker et al., 2007) and mimicking person-
ality traits (mairesse and walker, 2011). lemon
(2008) proposed a id23 (rl)
framework in which policy and id86 components
can be jointly optimised and adapted based on on-
line user feedback.
in contrast, mairesse et al.
(2010) has proposed using active learning to miti-
gate the data sparsity problem when training data-
driven id86 systems. furthermore, cuayhuitl et al.
(2014) trained statistical surface realisers from unla-
belled data by an automatic slot labelling technique.
in general, feature-based adaptation is perhaps the
most widely used technique (blitzer et al., 2007;
pan and yang, 2010; duan et al., 2012). by ex-
ploiting correlations and similarities between data
points, it has been successfully applied to problems
like speaker adaptation (gauvain and lee, 1994;
leggetter and woodland, 1995) and various tasks in
natural language processing (daum  e iii, 2009). in
contrast, model-based adaptation is particularly use-
ful for id38 (lm) (bellegarda, 2004).
mixture-based topic lms (gildea and hofmann,
1999) are widely used in id165 lms for domain
adaptation. similar ideas have been applied to appli-
cations that require adapting lms, such as machine
translation (mt) (koehn and schroeder, 2007) and
personalised id103 (wen et al., 2012).
id20 for neural network (nn)-
based lms has also been studied in the past.
a feature augmented id56lm was    rst proposed
by mikolov and zweig (2012), but later applied to
multi-genre broadcast id103 (chen et
al., 2015) and personalised id38 (wen
et al., 2013). these methods are based on    ne-
tuning existing network parameters on adaptation
data. however, careful regularisation is often nec-
essary (yu et al., 2013).
in a slightly different
area, shi et al. (2015) applied curriculum learning
to id56lm adaptation.

discriminative training (dt) (collins, 2002) is an
alternative to the maximum likelihood (ml) crite-
rion. for classi   cation, dt can be split into two
phases: (1) decoding training examples using the
current model and scoring them, and (2) adjusting
the model parameters to maximise the separation

between the correct target annotation and the com-
peting incorrect annotations.
it has been success-
fully applied to many research problems, such as
id103 (kuo et al., 2002; voigtlaender
et al., 2015) and mt (he and deng, 2012; auli et
al., 2014). recently, auli and gao (2014) trained
an id56lm with a dt objective and showed im-
proved performance on an mt task. however, their
id56 probabilities only served as input features to a
phrase-based mt system.

3 the neural language generator

the neural language generation model (wen et al.,
2015a; wen et al., 2015b) is a id56lm (mikolov
et al., 2010) augmented with semantic input features
such as a dialogue act1 (da) denoting the required
semantics of the generated output. at every time
step t, the model consumes the 1-hot representation
2 to update its in-
of both the da dt and a token wt
ternal state ht. based on this new state, the output
distribution over the next output token is calculated.
the model can thus generate a sequence of tokens
by repeatedly sampling the current output distribu-
tion to obtain the next input token until an end-of-
sentence sign is generated. finally, the generated
sequence is lexicalised3 to form the target utterance.
the semantically conditioned long short-term
memory network (sc-lstm) (wen et al., 2015b)
the lstm net-
is a specialised extension of
work (hochreiter and schmidhuber, 1997)
for
language generation which has previously been
shown capable of learning generation decisions from
paired da-utterances end-to-end without a modu-
lar pipeline (walker et al., 2002; stent et al., 2004).
like lstm, sc-lstm relies on a vector of mem-
ory cells ct     rn and a set of elementwise multi-
plication gates to control how information is stored,
forgotten, and exploited inside the network. the sc-
lstm architecture used in this paper is de   ned by

1a combination of an action type and a set of slot-value

pairs. e.g. inform(name=   seven days   ,food=   chinese   )

2 we use token instead of word because our model operates
on text for which slot values are replaced by their corresponding
slot tokens. we call this procedure delexicalisation.
3the process of replacing slot token by its value.

the following equations,

                   =

                  

                  

it
ft
ot
rt
  ct

                   w5n,2n

sigmoid
sigmoid
sigmoid
sigmoid
tanh
dt = rt (cid:12) dt   1

(cid:19)

(cid:18) wt

ht   1

ct = ft (cid:12) ct   1 + it (cid:12)   ct + tanh(wdcdt)

ht = ot (cid:12) tanh(ct)

where n is the hidden layer size, it, ft, ot, rt    
[0, 1]n are input, forget, output, and reading gates re-
spectively,   ct and ct are proposed cell value and true
cell value at time t, w5n,2n and wdc are the model
parameters to be learned. the major difference of
the sc-lstm compared to the vanilla lstm is the
introduction of the reading gates for controlling the
semantic input features presented to the network. it
was shown in wen et al. (2015b) that these reading
gates act like keyword and key phrase detectors that
learn the alignments between individual semantic
input features and their corresponding realisations
without additional supervision.

after the hidden layer state is obtained, the com-
putation of the next word distribution and sampling
from it is straightforward,

p(wt+1|wt, wt   1, ...w0, dt) = sof tmax(whoht)

wt+1     p(wt+1|wt, wt   1, ...w0, dt).

(cid:124)

t p

t log(yt) + (cid:107)dt(cid:107) +(cid:80)t   1

where who is another weight matrix to learn. the
entire network is trained end-to-end using a cross
id178 cost function, between the predicted word
distribution pt and the actual word distribution yt,
with regularisations on da transition dynamics,
t=0     (cid:107)dt+1   dt(cid:107)

f (  ) =(cid:80)
(1)
where    = {w5n,2n, wdc, who}, dt is the da
vector at the last index t, and    and    are constants
set to 10   4 and 100, respectively.
4 training multi-domain models
given training instances (represented by da and
sentence tuples {di,    i}) from the source domain s
(rich) and the target domain t (limited), the goal is
to    nd a set of sc-lstm parameters   t that can
perform acceptably well in the target domain.

figure 1: an example of data counterfeiting algorithm. both slots and values are delexicalised. slots and
values that are not in the target domain are replaced during data counterfeiting (shown in red with * sign).
the pre   x inside bracket <> indicates the slot   s functional class (i for informable and r for requestable).

4.1 model fine-tuning
a straightforward way to adapt nn-based models to
a target domain is to continue training or    ne-tuning
a well-trained generator on whatever new target do-
main data is available. this training procedure is as
follows:

1. train a source domain generator   s on source
domain data {di,    i}     s with all values delex-
icalised4.

2. divide the adaptation data into training and val-
idation sets. re   ne parameters by training on
adaptation data {di,    i}     t with early stop-
ping and a smaller starting learning rate. this
yields the target domain generator   t.

although this method can bene   t from parameter
sharing of the lm part of the network, the parame-
ters of similar input slot-value pairs are not shared4.
in other words, realisation of any unseen slot-value
pair in the target domain can only be learned from
scratch. adaptation offers no bene   t in this case.

4.2 data counterfeiting
in order to maximise the effect of domain adapta-
tion, the model should be able to (1) generate accept-
able realisations for unseen slot-value pairs based
on similar slot-value pairs seen in the training data,

4we have tried training with both slots and values delexi-
calised and then using the weights to initialise unseen slot-value
pairs in the target domain. however, this yielded even worse
results since the learned semantic alignment stuck at local min-
ima. pre-training only the lm parameters did not produce better
results either.

and (2) continue to distinguish slot-value pairs that
are similar but nevertheless distinct. instead of ex-
ploring weight tying strategies in different training
stages (which is complex to implement and typically
relies on ad hoc tying rules), we propose instead a
data counterfeiting approach to synthesise target do-
main data from source domain data. the procedure
is shown in figure 1 and described as following:

1. categorise slots in both source and target do-
main into classes, according to some similarity
measure. in our case, we categorise them based
on their functional type to yield three classes:
informable, requestable, and binary5.

2. delexicalise all slots and values.
3. for each slot s in a source instance (di,    i)    
s, randomly select a new slot s(cid:48) that belongs
to both the target ontology and the class of s
to replace s. repeat this process for every slot
in the instance and yield a new pseudo instance
(   di,      i)     t in the target domain.

4. train a generator     t on the counterfeited

dataset {   di,      i}     t.

5. re   ne parameters on real in-domain data. this

yields    nal model parameters   t.

this approach allows the generator to share realisa-
tions among slot-value pairs that have similar func-
tionalities, therefore facilitates the id21

5informable class include all non-binary informable slots

while binary class includes all binary informable slots.

informable slots

requestable slots

act type

laptop

family, *pricerange, batteryrating,

driverange, weightrange,
isforbusinesscomputing

*name, *type, *price, warranty, battery,

design, dimension, utility, weight,
platform, memory, drive, processor

television

family, *pricerange, screensizerange,

ecorating, hdmiport, hasusbport

*name, *type, *price, resolution,

powerconsumption, accessories, color,

screensize, audio

*inform, *inform only match, *inform on match, inform all, *inform count,
inform no info, *recommend, compare, *select, suggest, *con   rm, *request,
*request more, *goodbye

bold=binary slots, *=overlap with sf restaurant and hotel domains, all informable slots can take    dontcare    value

table 1: ontologies for laptop and tv domains

of rare slot-value pairs in the target domain. further-
more, the approach also preserves the co-occurrence
statistics of slot-value pairs and their realisations.
this allows the model to learn the gating mechanism
even before adaptation data is introduced.

5 discriminative training
in contrast to the traditional ml criteria (equation 1)
whose goal is to maximise the log-likelihood of cor-
rect examples, dt aims at separating correct exam-
ples from competing incorrect examples. given a
training instance (di,    i), the training process starts
by generating a set of candidate sentences gen(di)
using the current model parameter    and da di. the
discriminative cost function can therefore be written
as

f (  ) =    e[l(  )]

=     (cid:88)

      gen(di)

p  (   |di)l(   ,    i)

(2)

exp[   log p(   |di,  )]

where l(   ,    i) is the scoring function evaluating
candidate     by taking ground truth    i as reference.
p  (   |di) is the normalised id203 of the candi-
date and is calculated by
(cid:80)
p  (   |di) =
   (cid:48)   gen(di) exp[   log p(   (cid:48)|di,  )] (3)
       [0,   ] is a tuned scaling factor that    attens the
distribution for    < 1 and sharpens it for    > 1. the
unnormalised candidate likelihood log p(   |di,   ) is
produced by summing token likelihoods from the
id56 generator output,
log p(   |di,   ) =

log p(wt|di,   )

(cid:88)

(4)

wt      

the scoring function l(   ,    i) can be further gen-
eralised to take several scoring functions into ac-
count

l(   ,    i) =

lj(   ,    i)  j

(5)

(cid:88)

j

where   j is the weight for j-th scoring function.
since the cost function presented here (equation 2)
is differentiable everywhere, back propagation can
be applied to calculate the gradients and update pa-
rameters directly.

6 datasets

in order to test our proposed recipe for training
multi-domain language generators, we conducted
experiments using four different domains:    nding a
restaurant,    nding a hotel, buying a laptop, and buy-
ing a television. datasets for the restaurant and hotel
domains have been previously released by wen et al.
(2015b). these were created by workers recruited
by amazon mechanical turk (amt) by asking them
to propose an appropriate natural language realisa-
tion corresponding to each system dialogue act ac-
tually generated by a dialogue system. however,
the number of actually occurring da combinations
in the restaurant and hotel domains were rather lim-
ited (   200) and since multiple references were col-
lected for each da, the resulting datasets are not suf-
   ciently diverse to enable the assessment of the gen-
eralisation capability of the different training meth-
ods over unseen semantic inputs.

in order to create more diverse datasets for the
laptop and tv domains, we enumerated all possible
combinations of dialogue act types and slots based
on the ontology shown in table 1. this yielded

(a) id7 score curve

(a) id7 score curve

(b) slot error rate curve

(b) slot error rate curve

figure 2: results evaluated on tv domain by
adapting models from laptop domain. compar-
ing train-from-scratch model (scratch) with model
   ne-tuning approach (tune) and data counterfeiting
method (counterfeit). 10%     700 examples.

about 13k distinct das in the laptop domain and 7k
distinct das in the tv domain. we then used amt
workers to collect just one realisation for each da.
since the resulting datasets have a much larger input
space but only one training example for each da,
the system must learn partial realisations of con-
cepts and be able to recombine and apply them to
unseen das. also note that the number of act types
and slots of the new ontology is larger, which makes
id86 in both laptop and tv domains much harder.

7 corpus-based evaluation

we    rst assess generator performance using two ob-
jective id74, the id7-4 score (pap-
ineni et al., 2002) and slot error rate err (wen et
al., 2015b). slot error rates were calculated by aver-
aging slot errors over each of the top 5 realisations
in the entire corpus. we used multiple references to
compute the id7 scores when available (i.e. for

figure 3: the same set of comparison as in figure 2,
but the results were evaluated by adapting from sf
restaurant and hotel joint dataset to laptop and tv
joint dataset. 10%     2k examples.

the restaurant and hotel domains). in order to better
compare results across different methods, we plotted
the id7 and slot error rate curves against different
amounts of adaptation data. note that in the graphs
the x-axis is presented on a log-scale.

7.1 experimental setup

the generators were implemented using the theano
library (bergstra et al., 2010; bastien et al., 2012),
and trained by partitioning each of the collected cor-
pora into a training, validation, and testing set in the
ratio 3:1:1. all the generators were trained by treat-
ing each sentence as a mini-batch. an l2 regulari-
sation term was added to the objective function for
every 10 training examples. the hidden layer size
was set to be 100 for all cases. stochastic gradient
descent and back propagation through time (werbos,
1990) were used to optimise the parameters. in or-
der to prevent over   tting, early stopping was imple-
mented using the validation set.

during decoding, we over-generated 20 utter-
ances and selected the top 5 realisations for each da
according to the following reranking criteria,

r =    (f (  ) +   err)

(6)

where    is a tradeoff constant, f (  ) is the cost gen-
erated by network parameters   , and the slot error
rate err is computed by exact matching of the slot
tokens in the candidate utterances.    is set to a large
value (10) in order to severely penalise nonsensical
outputs. since our generator works stochastically
and the trained networks can differ depending on the
initialisation, all the results shown below were aver-
aged over 5 randomly initialised networks.

7.2 data counterfeiting
we    rst compared the data counterfeiting (coun-
terfeit) approach with the model    ne-tuning (tune)
method and models trained from scratch (scratch).
figure 2 shows the result of adapting models be-
tween similar domains, from laptop to tv. because
of the parameter sharing in the lm part of the
network, model    ne-tuning (tune) achieves a bet-
ter id7 score than training from scratch (scratch)
when target domain data is limited. however, if we
apply the data counterfeiting (counterfeit) method,
we obtain an even greater id7 score gain. this is
mainly due to the better realisation of unseen slot-
value pairs. on the other hand, data counterfeit-
ing (counterfeit) also brings a substantial reduction
in slot error rate. this is because it preserves the
co-occurrence statistics between slot-value pairs and
realisations, which allows the model to learn good
semantic alignments even before adaptation data is
introduced. similar results can be seen in figure 3,
in which adaptation was performed on more disjoint
domains: restaurant and hotel joint domain to laptop
and tv joint domain. the data counterfeiting (coun-
terfeit) method is still superior to the other methods.

7.3 discriminative training
the generator parameters obtained from data coun-
terfeiting and ml adaptation were further tuned by
applying dt. in each case, the models were opti-
mised using two objective functions: id7-4 score
and slot error rate. however, we used a soft version
of id7 called sentence id7 as described in auli

(a) effect of dt on id7

(b) effect of dt on slot error rate

figure 4: effect of applying dt training after ml
adaptation. the results were evaluated on laptop to
tv adaptation. 10%     700 examples.

and gao (2014), to mitigate the sparse id165 match
problem of id7 at the sentence level. in our ex-
periments, we set    to 5.0 and   j to 1.0 and -1.0 for
id7 and err, respectively. for each da, we ap-
plied our generator 50 times to generate candidate
sentences. repeated candidates were removed. we
treated the remaining candidates as a single batch
and updated the model parameters by the procedure
described in section 5. we evaluated performance
of the algorithm on the laptop to tv adaptation sce-
nario, and compared models with and without dis-
criminative training (ml+dt & ml). the results
are shown in figure 4 where it can be seen that
dt consistently improves generator performance on
both metrics. another interesting point to note is
that slot error rate is easier to optimise compared to
id7 (err    0 after dt). this is probably be-
cause the sentence id7 optimisation criterion is
only an approximation of the corpus id7 score
used for evaluation.

method

tv to laptop
nat.
info.
2.37
2.64
2.52**
2.25**
2.22**
2.51**
2.24**
2.03**

scrall
dt-10%
ml-10%
scr-10%
* p <0.05, ** p <0.005

laptop to tv
nat.
info.
2.36
2.54
2.19**
2.51
2.22**
2.45**
2.00**
1.92**

table 2: human evaluation for utterance quality in
two domains. results are shown in two metrics
(rating out of 3). statistical signi   cance was com-
puted using a two-tailed student   s t-test, between the
model trained with full data (scrall) and all others.

8 human evaluation

since automatic metrics may not consistently agree
with human perception (stent et al., 2005), human
testing is needed to assess subjective quality. to do
this, a set of judges were recruited using amt. we
tested our models on two adaptation scenarios: lap-
top to tv and tv to laptop. for each task, two
systems among the four were compared:
training
from scratch using full dataset (scrall), adapting
with dt training but only 10% of target domain data
(dt-10%), adapting with ml training but only 10%
of target domain data (ml-10%), and training from
scratch using only 10% of target domain data (scr-
10%). in order to evaluate system performance in
the presence of language variation, each system gen-
erated 5 different surface realisations for each input
da and the human judges were asked to score each
of them in terms of informativeness and naturalness
(rating out of 3), and also asked to state a prefer-
ence between the two. here informativeness is de-
   ned as whether the utterance contains all the infor-
mation speci   ed in the da, and naturalness is de-
   ned as whether the utterance could plausibly have
been produced by a human. in order to decrease the
amount of information presented to the judges, utter-
ances that appeared identically in both systems were
   ltered out. we tested about 2000 das for each sce-
nario distributed uniformly between contrasts except
that allowed 50% more comparisons between ml-
10% and dt-10% because they were close.

table 2 shows the subjective quality assessments
which exhibit the same general trend as the objective
results. if a large amount of target domain data is

scr-10% ml-10% dt-10% scrall
22.4**
36.8**
35.9**

33.9**
44.9

34.5**

-

-

-

pref.%
scr-10%
ml-10%
dt-10%
scrall
* p <0.05, ** p <0.005

65.5**
66.1**
77.6**

64.1**

-

55.1
63.2**

(a) preference test on tv to laptop adaptation scenario

scr-10% ml-10% dt-10% scrall
14.8**
37.1**
41.6*

14.2**
48.1

17.4**

-

-

-

pref.%
scr-10%
ml-10%
dt-10%
scrall
* p <0.05, ** p <0.005

82.6**
85.8**
85.2**

58.4*

-

51.9
62.9**

(b) preference test on laptop to tv adaptation scenario
table 3: pairwise preference test among four ap-
proaches in two domains. statistical signi   cance
was computed using two-tailed binomial test.

available, training everything from scratch (scrall)
achieves a very good performance and adaptation is
not necessary. however, if only a limited amount
of in-domain data is available, ef   cient adaptation
is critical (dt-10% & ml-10% > scr-10%). more-
over, judges also preferred the dt trained genera-
tor (dt-10%) compared to the ml trained genera-
tor (ml-10%), especially for informativeness. in the
laptop to tv scenario, the informativeness score of
dt method (dt-10%) was considered indistinguish-
able when comparing to the method trained with full
training set (scrall). the preference test results are
shown in table 3. again, adaptation methods (dt-
10% & ml-10%) are crucial to bridge the gap be-
tween domains when the target domain data is scarce
(dt-10% & ml-10% > scr-10%). the results also
suggest that the dt training approach (dt-10%) was
preferred compared to ml training (ml-10%), even
though the preference in this case was not statisti-
cally signi   cant.

9 conclusion and future work
in this paper we have proposed a procedure for train-
ing multi-domain, id56-based language generators,
by data counterfeiting and discriminative training.
the procedure is general and applicable to any data-
driven language generator. both corpus-based eval-
uation and human assessment were performed. ob-
jective measures on corpus data have demonstrated

that by applying this procedure to adapt models be-
tween four different dialogue domains, good perfor-
mance can be achieved with much less training data.
subjective assessment by human judges con   rm the
effectiveness of the approach.

the proposed id20 method requires
a small amount of annotated data to be collected of-
   ine. in our future work, we intend to focus on train-
ing the generator on the    y with real user feedback
during conversation.

acknowledgments

tsung-hsien wen and david vandyke are supported
by toshiba research europe ltd, cambridge re-
search laboratory.

references
[auli and gao2014] michael auli and jianfeng gao.
2014. decoder integration and expected id7 training
for recurrent neural network language models. in pro-
ceedings of acl. association for computational lin-
guistics.

[auli et al.2014] michael auli, michel galley, and jian-
feng gao. 2014. large-scale expected id7 training
of phrase-based reordering models. in proceedings of
emnlp. association for computational linguistics.

[bastien et al.2012] fr  ed  eric bastien, pascal lamblin,
razvan pascanu, james bergstra, ian j. goodfellow,
arnaud bergeron, nicolas bouchard, and yoshua ben-
gio.
2012. theano: new features and speed im-
provements. deep learning and unsupervised fea-
ture learning nips 2012 workshop.

[bellegarda2004] jerome r. bellegarda. 2004. statistical
language model adaptation: review and perspectives.
speech communication.

[bergstra et al.2010] james bergstra, olivier breuleux,
fr  ed  eric bastien, pascal lamblin, razvan pascanu,
guillaume desjardins, joseph turian, david warde-
farley, and yoshua bengio. 2010. theano: a cpu
in proceedings
and gpu math expression compiler.
of the python for scienti   c computing conference.

[blitzer et al.2007] john blitzer, mark dredze, and fer-
nando pereira. 2007. biographies, bollywood, boom-
boxes and blenders: id20 for sentiment
classi   cation. in proceedings of acl. association for
computational linguistics.

[bohus and rudnicky2009] dan bohus and alexander i.
rudnicky.
2009. the ravenclaw dialog manage-
ment framework: architecture and systems. com-
puter speech and language.

[chen et al.2015] xie chen, tan tian, liu xunying, lan-
chantin pierre, wan moquan, mark gales, and wood-
land phil. 2015. recurrent neural network language
model adaptation for multi-genre broadcast speech
recognition. in proceedings of interspeech.

[collins2002] michael collins.

2002. discriminative
training methods for id48: theory
and experiments with id88 algorithms. in pro-
ceedings of emnlp. association for computational
linguistics.

[cuayhuitl et al.2014] h. cuayhuitl,

n. dethlefs,
h. hastie, and x. liu. 2014. training a statistical
surface realiser from automatic slot labelling.
in
spoken language technology workshop (slt), 2014
ieee, pages 112   117, dec.

[daum  e iii2009] hal daum  e iii.

2009. frustratingly

easy id20. corr, abs/0907.1815.

[duan et al.2012] lixin duan, dong xu, and ivor w.
tsang.
learning with augmented fea-
tures for heterogeneous id20. corr,
abs/1206.4660.

2012.

[ga  si  c et al.2015] milica ga  si  c, nikola mrk  si  c, pei-hao
su, david vandyke, tsung-hsien wen, and steve j.
young. 2015. policy committee for adaptation in
in proceed-
multi-domain spoken dialogue systems.
ings of asru.

[gauvain and lee1994] jean-luc gauvain and chin-hui
lee.
1994. maximum a posteriori estimation for
multivariate gaussian mixture observations of markov
chains. speech and audio processing, ieee transac-
tions on.

[gildea and hofmann1999] daniel gildea and thomas
hofmann. 1999. topic-based language models using
em. in proceedings of eurospeech.

[he and deng2012] xiaodong he and li deng. 2012.
maximum expected id7 training of phrase and lexi-
con translation models. in proceedings of acl. asso-
ciation for computational linguistics.

[hochreiter and schmidhuber1997] sepp hochreiter and
j  urgen schmidhuber. 1997. long short-term memory.
neural computation.

[hogan et al.2008] deirdre hogan,

jennifer foster,
joachim wagner, and josef van genabith.
2008.
parser-based retraining for id20 of
in proceedings of iid86.
probabilistic generators.
association for computational linguistics.

[isard et al.2006] amy isard, carsten brockmann, and
jon oberlander. 2006.
individuality and alignment
in generated dialogues. in proceedings of iid86. as-
sociation for computational linguistics.

[kaplan and bresnan1982] ronald m. kaplan and joan
bresnan. 1982. lexical-functional grammar: a for-
mal system for grammatical representation.
in joan

bresnan, editor, the mental representation of gram-
matical relations. mit press.

[koehn and schroeder2007] philipp koehn and josh
schroeder. 2007. experiments in id20
for id151. in proceedings of
statmt. association for computational linguistics.

[kuo et al.2002] hong-kwang kuo, eric fosler-lussier,
hui jiang, and chin-hui lee. 2002. discriminative
training of language models for id103. in
proceedings of icassp.

[langkilde and knight1998] irene langkilde and kevin
knight. 1998. generation that exploits corpus-based
statistical knowledge. in proceedings of acl. associ-
ation for computational linguistics.

[leggetter and woodland1995] chris

and
philip woodland. 1995. maximum likelihood linear
regression for
speaker adaptation of continuous
density id48. computer speech
and language.

leggetter

[lemon2008] oliver lemon.

2008. adaptive natural
language generation in dialogue using reinforcement
learning. in proceedings of semdial.

[mairesse and walker2008] franois mairesse and mari-
lyn walker. 2008. trainable generation of big-   ve
personality styles through data-driven parameter esti-
mation. in proceedings of acl. association for com-
putational linguistics.

[mairesse and walker2011] franc  ois mairesse and mari-
lyn a. walker. 2011. controlling user perceptions
of linguistic style: trainable generation of personality
traits. computer linguistics.

[mairesse and young2014] franc  ois mairesse and steve
young. 2014. stochastic language generation in dia-
logue using factored language models. computer lin-
guistics.

[mairesse et al.2010] franc  ois mairesse, milica ga  si  c,
filip jur  c      cek, simon keizer, blaise thomson, kai
yu, and steve young. 2010. phrase-based statistical
language generation using id114 and ac-
tive learning. in proceedings of acl. association for
computational linguistics.

[mikolov and zweig2012] tom  a  s mikolov and geoffrey
zweig. 2012. context dependent recurrent neural net-
work language model. in proceedings of slt.

[mikolov et al.2010] tom  a  s mikolov, martin kara   t,
luk  a  s burget, jan   cernock  y, and sanjeev khudan-
pur. 2010. recurrent neural network based language
model. in proceedings of interspeech.

[mrk  si  c et al.2015] nikola mrk  si  c,

  o
s  eaghdha, blaise thomson, milica ga  si  c, pei-
hao su, david vandyke, tsung-hsien wen, and
2015. multi-domain dialog state
steve j. young.
corr,
tracking using recurrent neural networks.
abs/1506.07190.

diarmuid

[pan and yang2010] sinno jialin pan and qiang yang.
2010. a survey on id21. ieee trans. on
knowledge and data engineering.

[papineni et al.2002] kishore papineni, salim roukos,
todd ward, and wei-jing zhu. 2002. id7: a method
for automatic evaluation of machine translation.
in
proceedings of acl. association for computational
linguistics.

[shi et al.2015] yangyang shi, martha larson,

and
catholijn m. jonker. 2015. recurrent neural network
language model adaptation with curriculum learning.
computer, speech and language.

[stent et al.2004] amanda stent, rashmi prasad, and
marilyn walker. 2004. trainable sentence planning
for complex information presentation in spoken dia-
log systems. in proceedings of acl. association for
computational linguistics.

[stent et al.2005] amanda stent, matthew marge, and
mohit singhai. 2005. evaluating evaluation methods
in pro-
for generation in the presence of variation.
ceedings of cicling 2005.

[tresp2000] volker tresp. 2000. a bayesian committee

machine. neural computation.

[voigtlaender et al.2015] p. voigtlaender, p. doetsch,
s. wiesler, r. schluter, and h. ney. 2015. sequence-
discriminative training of recurrent neural networks.
in proceedings of icassp.

[walker et al.2002] marilyn a walker, owen c rambow,
and monica rogati. 2002. training a sentence planner
for spoken dialogue using boosting. computer speech
and language.

[walker et al.2007] marilyn walker, amanda stent, fra-
nois mairesse, and rashmi prasad. 2007.
individ-
ual and id20 in sentence planning for
dialogue. journal of arti   cial intelligence research
(jair).

[ward and issar1994] wayne ward and sunil issar. 1994.
recent improvements in the cmu spoken language un-
in proceedings of workshop on
derstanding system.
hlt. association for computational linguistics.

[wen et al.2012] tsung-hsien wen, hung-yi lee, tai-
yuan chen, and lin-shan lee. 2012. personalized
id38 by crowd sourcing with social net-
work data for voice access of cloud applications.
in
proceedings of slt.

[wen et al.2013] tsung-hsien wen, aaron heidel, hung
yi lee, yu tsao, and lin-shan lee. 2013. recurrent
neural network based language model personalization
by social network id104. in proceedings of
interspeech.

[wen et al.2015a] tsung-hsien wen, milica ga  si  c,
dongho kim, nikola mrk  si  c, pei-hao su, david
vandyke, and steve young.
stochastic

2015a.

language generation in dialogue using recurrent
neural networks with convolutional sentence rerank-
in proceedings of sigdial. association for
ing.
computational linguistics.

[wen et al.2015b] tsung-hsien wen, milica ga  si  c,
nikola mrk  si  c, pei-hao su, david vandyke, and
steve young. 2015b. semantically conditioned lstm-
based id86 for spoken dialogue
systems. in proceedings of emnlp. association for
computational linguistics.

[werbos1990] paul j werbos. 1990. id26
through time: what it does and how to do it. proceed-
ings of the ieee.

[young et al.2013] steve young, milica ga  si  c, blaise
thomson, and jason d. williams. 2013. pomdp-
based statistical spoken id71: a review.
proceedings of the ieee.

[yu et al.2013] dong yu, kaisheng yao, hang su, gang
li, and frank seide.
2013. kl-divergence regu-
larized deep neural network adaptation for improved
large vocabulary id103. in proceedings
of icassp.

