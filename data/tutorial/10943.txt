6
1
0
2

 

n
u
j
 

8

 
 
]
l
c
.
s
c
[
 
 

1
v
9
8
6
2
0

.

6
0
6
1
:
v
i
x
r
a

continuously learning neural dialogue management

pei-hao su, milica ga  si  c, nikola mrk  si  c, lina rojas-barahona,
stefan ultes, david vandyke, tsung-hsien wen and steve young
department of engineering, university of cambridge, cambridge, uk

{phs26, mg436, nm480, lmr46, su259, djv27, thw28, sjy}@cam.ac.uk

abstract

we describe a two-step approach for dialogue
management in task-oriented spoken dialogue
systems. a uni   ed neural network framework
is proposed to enable the system to    rst learn
by supervision from a set of dialogue data
and then continuously improve its behaviour
via id23, all using gradient-
based algorithms on one single model. the ex-
periments demonstrate the supervised model   s
effectiveness in the corpus-based evaluation,
with user simulation, and with paid human
subjects. the use of reinforcement learn-
ing further improves the model   s performance
in both interactive settings, especially under
higher-noise conditions.

introduction

1
developing a robust spoken dialogue system
(sds) traditionally requires a substantial amount
of hand-crafted rules combined with various statis-
tical components.
in a task-oriented sds, teach-
ing a system how to respond appropriately is non-
trivial. more recently, this dialogue management
task has been formulated as a reinforcement learn-
ing (rl) problem which can be automatically opti-
mised through human interaction (levin and pierac-
cini, 1997; roy et al., 2000; williams and young,
2007; jur  c      cek et al., 2011; young et al., 2013). in
this framework, the system learns by a trial and er-
ror process governed by a potentially delayed learn-
ing objective, a reward function, that determines di-
alogue success (el asri et al., 2014; su et al., 2015;
vandyke et al., 2015; su et al., 2016). to enable
the system to be trained on-line, sample-ef   cient

learning algorithms have been proposed (ga  si  c and
young, 2014; daubigney et al., 2014) which can
learn policies from a minimal number of dialogues.
however, even with such methods, performance is
still poor in the early training stages, and this can im-
pact negatively on the user experience. for these and
other reasons, most commercial systems still hand-
craft the dialogue management to ensure its stability.
supervised learning (sl) has also been used
in dialogue research where a policy is trained to
produce an example response given the dialogue
state. wizard-of-oz (woz) methods (kelley, 1984;
dahlb  ack et al., 1993) have been widely used for
collecting domain-speci   c training corpora. re-
cently an emerging line of research has focused on
training a network-based dialogue model, mostly in
text-input schemes (vinyals and le, 2015; serban
et al., 2015; wen et al., 2016; bordes and weston,
2016). these systems were directly trained on past
dialogues without detailed speci   cation of the inter-
nal dialogue state. however, there are two key lim-
itations of using the sl approach for sds. firstly,
the effects of selecting an action on the future course
of the dialogue are not considered. secondly, there
may be a very large number of dialogue states for
which an appropriate response must be generated.
hence, the sl training set may lack suf   cient cover-
age. another issue is that there is no reason to sup-
pose a human wizard is acting optimally, especially
at high noise levels. these problems exacerbate in
larger domains where multi-step planning is needed.
thus, learning to mimic a human wizard does not
necessary lead to optimal behaviour.

to get the best of both sl- and rl-based dialogue

management, this paper describes a network-based
model which is initially trained with a supervised
spoken dialogue dataset. since the training data may
be mismatched to the deployment environment, the
model is further improved by rl in interaction with
a simulated user or human users. the advantage of
the proposed framework is that a single model can
be trained using both sl and rl without modifying
the system architecture. this resembles the train-
ing process used in alphago (silver et al., 2016) for
the game of go.
in addition, unlike most of the-
state-of-the-art rl-based dialogue systems (ga  si  c
and young, 2014; cuay  ahuitl et al., 2015) which
operate on a constrained set of summary actions to
limit the policy space and minimise expensive train-
ing costs, our model operates on a full action set.

2 neural dialogue management

the proposed framework addresses the dialogue
management component in a modular sds. as de-
picted in figure 1, the input to the model is the be-
lief state s which encodes the understood user in-
tents along with the dialogue history (henderson et
al., 2014b; mrk  si  c et al., 2015), and the output is
the master dialogue action a that decides the seman-
tic reply. this is subsequently passed to the natural
language generator (wen et al., 2015).

dialogue management is represented as a policy
network, a neural network with one hidden layer
exploiting tanh non-linearities, an output layer con-
sisting of two softmax partitions and six sigmoid
partitions. for the softmax outputs, one is for pre-
dicting diaact, a multi-class label over    ve dialogue
acts: {request, offer, confirm, select,
bye}, and the other for predicting query, contain-
ing four options for the search constraint: {food,
pricerange, area, none}. query options only
matter if the dialogue act in {request, confirm,
select} is used. the sigmoid partitions are for
offer, each of which is used to determine a binary
prediction when making system offer1.

given the system   s understanding of the user, the
model   s role is to determine what the intent of the
system response should be and which slot to talk
about. the exact value in each slot is decided by a

figure 1: network-based system architecture.

separate database parser, where the query is the top
prediction of each user-informable slot2 from the di-
alogue state tracker and the output is a matched en-
tity. this output forms the system   s semantic reply,
the master dialogue action.

2.1 phase i: supervised learning
in the    rst phase, the policy network is trained on
corpus data. this data may come from woz collec-
tion or from interactions between users and an al-
ready existing policy, such as a hand-crafted base-
line. the objective here is to    mimic    the response
behaviour within the supervised data.
the training objective for each sample is to min-
imise a joint cross-id178 loss l(  ) between model
action labels y de   ned in   2 and predictions p:

l(  ) =

h(yk, pk),

(1)

k   {da,q,os}

(cid:88)

where diaact da and query q outputs are categor-
ical distributions, and the offer set os contains six
binary offer slots.    are the network parameters.

2.2 phase ii: id23
the policy trained in phase i on a    xed dataset
may not generalise well.
in spoken dialogue, the
noise level may vary across conditions and thus
can signi   cantly affect performance. hence, the
second phase of the training pipeline aims at im-
proving the sl trained policy network by further
training using policy-gradient based rl. the model
is given the freedom to select any combination of

1system-offer slots are slots the system can mention, such

2user-informable slots are slots used by the user to constrain

as area, phone number and postcode.

the search, such as area and price range.

(cid:105)

(cid:12)(cid:12)(cid:12)    

t=1   tr(st, at)

(cid:104)(cid:80)t

master action. the training objective is to    nd a
parametrised policy      that maximises the expected
reward j(  ) of a dialogue with t turns: j(  ) =
, where    is the discount
e
factor and r(st, at) is the reward when taking master
action at in dialogue state st. note that the struc-
ture and initial weights of      are    xed by the sl
pre-training phase, since the rl training aims to im-
prove the sl trained model.

here a batch algorithm is adopted, and all tran-
sitions are sampled under the current policy. at
each update iteration, n episodes were generated,
where the ith episode consists of a set of transition
tuples {(si
t=0. the estimated gradient is
estimated using the likelihood ratio trick:
     j(  ) =
t|si

      log   (ai

t;   )ri

t, (2)

t, ai

t, ri

1

t)}ti
ti(cid:88)
n(cid:88)
t(cid:48)=t   t(cid:48)   tri

t=0

i=1

n ti

t =(cid:80)ti

t(cid:48) is the cumulative return
where ri
from time-step t to ti. id119 is, however,
slow and has poor convergence properties.

natural gradient (amari, 1998) improves upon
the above    vanilla    gradient method by computing an
ascent direction that approximately ensures a small
change in the policy distribution. this direction is
w = f (  )   1     j(  ), where f (  ) is the fisher in-
formation matrix (fim). based on this, peters and
schaal (2006) developed the natural actor-critic
(nac). in its episodic case (enac), the fim does
not need to be explicitly computed to obtain the nat-
ural gradient w. enac uses a least square method:

   w + c,

      log   (ai

t|si

t=0

rn =

t;   )t

(3)
where c is a constant and    n     {1, ..., n} an ana-
lytical solution can be obtained. for larger models
with more parameters, a truncated variant (schul-
man et al., 2015) can also be used to practically cal-
culate the natural gradient.

experience replay (lin, 1992) is utilised to ran-
domly sample mini-batches of experiences from a
reply pool p. this increases data ef   ciency by re-
using experience samples in multiple updates and re-
duces the data correlation. as the gradient is highly
correlated with the return r, to ensure stable train-
ing, a unity-based reward normalisation is adopted
to normalise the total return rn between 0 and 1.

(cid:34) t(cid:88)

(cid:35)

3 experimental results

the target application is a live telephone-based sds
providing restaurant information for the cambridge
(uk) area. the domain consists of approximately
150 venues, each having 6 slots out of which 3 can
be used by the system to constrain the search (food-
type, area and price-range) and 3 are informable
properties (phone-number, address and postcode)
available once a database entity has been found.

the model was implemented using the theano li-
brary (theano development team, 2016). the size
of the hidden layer was set to 32 and all the weights
were randomly initialised between -0.1 and 0.1.

3.1 supervised learning on corpus data
a corpus consisting of 720 user dialogues in the
cambridge restaurant domain was split into 4:1:1 for
training, validation and testing. this corpus was col-
lected via the amazon mechanical turk (amt) ser-
vice, where paid subjects interacted through speech
with a well-behaved dialogue system as proposed in
(su et al., 2016). the raw data contains the top n
id103 (asr) results which were passed
to a rule-based semantic decoder and the focus be-
lief state tracker (henderson et al., 2014a) to ob-
tain the belief state that serves as the input feature
to the proposed policy network. the turn-level la-
bels were tagged according to   2. adagrad (duchi
et al., 2011) per dialogue was used during backprop-
agation to train each model based on the objective in
equation 1. to prevent over-   tting, early stopping
was applied based on the held-out validation set.

table 1 shows the weighted f-1 scores computed
on the test set for each label. we can clearly see that
the model accurately determines the type of reply
(diaact) and generally provides the right informa-
tion (offer). the hypothesised reason for the lower
accuracy of query is that the sl training data con-
tains robust asr results and thus the system exam-
ples contain more offers and less queries. this can
be mitigated with a larger dataset covering more di-
verse situations, or improved via an rl approach.

table 1: model performance based on f-measure.

output diaact query offer
92.51

97.73

87.39

f-1

3.2 policy network in simulation
the policy network was tested with a simulated
user (schatzmann et al., 2006) which provides the
interaction at the semantic level. as shown in figure
2, the    rst grid points labelled    sl:0    represent the
performance of the sl model under various seman-
tic error rates (ser), averaged over 500 dialogues.
the sl model was then further trained using rl
at different sers. as the sl model is already per-
forming well, the exploration parameter   was set
to 0.1. the size of the experience replay pool l
was 2,000, and the mini-batch size was set to 32.
for each update, natural gradient was calculated by
enac to update the model weights of size    2600.
the total return given to each dialogue was set to
20    1(d)     t , where t is the dialogue turn num-
ber and 1(d) is the success indicator for dialogue
d. maximum dialogue length was set to 30 turns.
return normalisation was used to stabilise training.
the success rate of the sl model can be seen
to increase for all sers during 6,000 training di-
alogues, spreading between 1-8% improvement.
generally speaking, the greatest improvement oc-
curs when the ser is most different to the sl train-
ing set, which are the higher ser conditions here.
in this case, as the semantic hypotheses were more
corrupted, the model learned to double-check more
on what the user really wanted. this indicates the
model   s ability to re   ne its own behaviour via rl.

3.3 policy network with real users
starting from the same sl policy network as in   3.2,
the model was improved via rl using human sub-
jects recruited via amt. the policy network was
plugged-in to a modular sds, comprising the mi-
crosoft   s bing speech recogniser 3, a rule-based se-
mantic decoder, the focus belief state tracker, and a
template-based natural language generator.

to ensure the dialogue quality, only those dia-
logues whose objective system check matched with
the user rating were considered (ga  si  c et al., 2013).
based on this, two parallel policies were trained with
200 dialogues. to evaluate the resulting policies,
policy learning was disabled and a further 110 di-
alogues were collected with both the sl only and
sl+rl models. the amt users were asked to rate

3www.microsoft.com/cognitive-services/en-us/speech-api.

figure 2: the success rate of the policy network in
user simulation under various semantic error rates
trained with sl and further improved via rl.

the dialogue quality by answering the question    do
you think this dialogue was successful?    on a 6-point
likert scale and also providing a binary rating on di-
alogue success. the average quality rating (scaled
from 0 to 5) is shown in table 2 with one standard
error. the results indicate that the sl-model could
work quite well with humans, but was improved by
rl on the 200 training dialogues. this demonstrates
that on-line rl is a viable approach to adapt a dia-
logue system to changing environmental conditions.

table 2: user evaluation on the policies. quality:
6-point likert scale, success: binary rating.

policy

quality (0-5)
success (%)

sl

3.97    0.12
94.5    2.2

sl+rl

4.04    0.12
98.2    1.2

4 conclusion
this paper has presented a two-step development
for the dialogue management in sds using a uni-
   ed neural network framework, where the model can
be trained on a    xed dialogue dataset using sl and
subsequently improved via rl through simulated or
spoken interactions. the experiments demonstrated
the ef   ciency of the proposed model with only a few
hundred supervised dialogue examples. the model
was further tested in simulated and real user settings.
in a mismatched environment, the model was capa-
ble of modifying its behaviour via a delayed reward
signal and achieved better success rate.

acknowledgments

pei-hao su is supported by cambridge trust and the
ministry of education, taiwan.

references
[amari1998] shun-ichi amari. 1998. natural gradient
works ef   ciently in learning. neural computation,
10(2):251   276.

[bordes and weston2016] antoine bordes and jason we-
ston. 2016. learning end-to-end goal-oriented dialog.
arxiv preprint: 1605.07683, may.

[cuay  ahuitl et al.2015] heriberto cuay  ahuitl,

simon
keizer, and oliver lemon. 2015. strategic dialogue
management via deep id23. arxiv
preprint arxiv:1511.08099.

[dahlb  ack et al.1993] nils dahlb  ack, arne j  onsson, and
lars ahrenberg. 1993. wizard of oz studies: why and
how. in proc of intelligent user interfaces.

[daubigney et al.2014] lucie daubigney, matthieu geist,
senthilkumar chandramohan, and olivier pietquin.
2014. a comprehensive id23 frame-
work for dialogue management optimisation. journal
of selected topics in signal processing, 6(8).

[duchi et al.2011] john duchi, elad hazan, and yoram
singer. 2011. adaptive subgradient methods for on-
line learning and stochastic optimization. the journal
of machine learning research, 12:2121   2159.

[el asri et al.2014] layla el asri, romain laroche, and
2014. task completion transfer

olivier pietquin.
learning for reward id136. in proc of mlis.

[ga  si  c and young2014] milica ga  si  c and steve young.
2014. gaussian processes for pomdp-based dialogue
manager optimization. taslp, 22(1):28   40.

[ga  si  c et al.2013] milica ga  si  c, catherine breslin,
matthew henderson, dongho kim, martin szummer,
blaise thomson, pirros tsiakoulis, and steve j.
young. 2013. on-line policy optimisation of bayesian
spoken dialogue systems via human interaction.
in
proc of icassp.

[henderson et al.2014a] m. henderson, b. thomson, and
j. williams. 2014a. the second dialog state track-
ing challenge. in proc of sigdial.

[henderson et al.2014b] m. henderson, b. thomson, and
s. j. young. 2014b. word-based dialog state track-
ing with recurrent neural networks. in proc of sig-
dial.

[jur  c      cek et al.2011] filip jur  c      cek, blaise thomson, and
steve young. 2011. natural actor and belief critic:
reinforcement algorithm for learning parameters of
dialogue systems modelled as pomdps. acm tslp,
7(3):6.

[kelley1984] john f. kelley. 1984. an iterative design
methodology for user-friendly natural language of   ce
information applications. acm transaction on infor-
mation systems.

[levin and pieraccini1997] esther levin and roberto
pieraccini. 1997. a stochastic model of computer-
human interaction for learning dialogue strategies. eu-
rospeech.

[lin1992] long-ji lin. 1992. self-improving reactive
agents based on id23, planning and
teaching. machine learning, 8(3-4):293   321.

[mrk  si  c et al.2015] nikola mrk  si  c,

  o
s  eaghdha, blaise thomson, milica ga  si  c, pei-
hao su, david vandyke, tsung-hsien wen, and steve
young. 2015. multi-domain dialog state tracking
using recurrent neural networks. in proc of acl.

diarmuid

[peters and schaal2006] jan peters and stefan schaal.
2006. id189 for robotics. in ieee
rsj.

[roy et al.2000] nicholas roy, joelle pineau, and sebas-
tian thrun. 2000. spoken dialogue management using
probabilistic reasoning. in proc of sigdial.

[schatzmann et al.2006] jost schatzmann, karl weil-
hammer, matt stuttle, and steve young.
2006.
a survey of statistical user simulation techniques
for reinforcement-learning of dialogue management
the knowledge engineering review,
strategies.
21(02):97   126.

[schulman et al.2015] john schulman, sergey levine,
philipp moritz, michael i jordan, and pieter abbeel.
2015. trust region policy optimization. proc of icml.
[serban et al.2015] iulian v serban, alessandro sor-
doni, yoshua bengio, aaron courville, and joelle
pineau. 2015. hierarchical neural network gener-
arxiv preprint
ative models for movie dialogues.
arxiv:1507.04808.

[silver et al.2016] david silver, aja huang, chris j
maddison, arthur guez, laurent sifre, george
van den driessche, julian schrittwieser,
ioannis
antonoglou, veda panneershelvam, marc lanctot,
et al. 2016. mastering the game of go with deep neu-
ral networks and tree search. nature, 529(7587):484   
489.

[su et al.2015] pei-hao su, david vandyke, milica
ga  si  c, dongho kim, nikola mrk  si  c, tsung-hsien
wen, and steve young. 2015. learning from real
users: rating dialogue success with neural networks
for id23 in spoken dialogue systems.
in proc of interspeech.

[su et al.2016] pei-hao su, milica ga  si  c, nikola mrk  si  c,
lina rojas-barahona, stefan ultes, david vandyke,
tsung-hsien wen, and steve young. 2016. on-line
active reward learning for policy optimisation in spo-
ken dialogue systems. in proc of acl.

[theano development team2016] theano development
team. 2016. theano: a python framework for fast
computation of mathematical expressions. arxiv e-
prints, abs/1605.02688.

[vandyke et al.2015] david vandyke, pei-hao su, milica
ga  si  c, nikola mrk  si  c, tsung-hsien wen, and steve
young. 2015. multi-domain dialogue success classi-
   ers for policy training. in asru.

[vinyals and le2015] oriol vinyals and quoc le. 2015.
arxiv preprint

a neural conversational model.
arxiv:1506.05869.

[wen et al.2015] tsung-hsien wen, milica ga  si  c, nikola
mrk  si  c, pei-hao su, david vandyke, and steve
young. 2015. semantically conditioned lstm-based
id86 for spoken dialogue sys-
in proceedings of the 2015 conference on
tems.
empirical methods in natural language processing
(emnlp). association for computational linguistics,
september.

[wen et al.2016] tsung-hsien wen, milica ga  si  c, nikola
mrk  si  c, lina m. rojas-barahona, pei-hao su, stefan
ultes, david vandyke, and steve young. 2016. a
network-based end-to-end trainable task-oriented dia-
logue system. arxiv preprint: 1604.04562, april.

[williams and young2007] jason d. williams and steve
young.
partially observable markov deci-
sion processes for spoken id71. computer
speech and language, 21(2):393   422.

2007.

[young et al.2013] steve young, milica ga  sic, blaise
thomson, and jason williams. 2013. pomdp-based
statistical spoken dialogue systems: a review. in proc
of ieee, volume 99, pages 1   20.

