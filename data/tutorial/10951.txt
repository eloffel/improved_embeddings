1192

proceedings of the 2016 conference on empirical methods in natural language processing, pages 1192   1202,

austin, texas, november 1-5, 2016. c(cid:13)2016 association for computational linguistics

deepreinforcementlearningfordialoguegenerationjiweili1,willmonroe1,alanritter2,michelgalley3,jianfenggao3anddanjurafsky11stanforduniversity,stanford,ca,usa2ohiostateuniversity,oh,usa3microsoftresearch,redmond,wa,usa{jiweil,wmonroe4,jurafsky}@stanford.edu,ritter.1492@osu.edu{mgalley,jfgao}@microsoft.comabstractrecentneuralmodelsofdialoguegenerationoffergreatpromiseforgeneratingresponsesforconversationalagents,buttendtobeshort-sighted,predictingutterancesoneatatimewhileignoringtheirin   uenceonfutureout-comes.modelingthefuturedirectionofadi-alogueiscrucialtogeneratingcoherent,inter-estingdialogues,aneedwhichledtraditionalnlpmodelsofdialoguetodrawonreinforce-mentlearning.inthispaper,weshowhowtointegratethesegoals,applyingdeepreinforce-mentlearningtomodelfuturerewardinchat-botdialogue.themodelsimulatesdialoguesbetweentwovirtualagents,usingpolicygradi-entmethodstorewardsequencesthatdisplaythreeusefulconversationalproperties:infor-mativity,coherence,andeaseofanswering(re-latedtoforward-lookingfunction).weevalu-ateourmodelondiversity,lengthaswellaswithhumanjudges,showingthattheproposedalgorithmgeneratesmoreinteractiveresponsesandmanagestofosteramoresustainedconver-sationindialoguesimulation.thisworkmarksa   rststeptowardslearninganeuralconversa-tionalmodelbasedonthelong-termsuccessofdialogues.1introductionneuralresponsegeneration(sordonietal.,2015;shangetal.,2015;vinyalsandle,2015;lietal.,2016a;wenetal.,2015;yaoetal.,2015;luanetal.,2016;xuetal.,2016;wenetal.,2016;lietal.,2016b;suetal.,2016)isofgrowinginter-est.thelstmsequence-to-sequence(id195)model(sutskeveretal.,2014)isonetypeofneuralgenerationmodelthatmaximizestheid203ofgeneratingaresponsegiventhepreviousdialogueturn.thisapproachenablestheincorporationofrichcontextwhenmappingbetweenconsecutivedialogueturns(sordonietal.,2015)inawaynotpossible,forexample,withmt-baseddialoguemodels(ritteretal.,2011).despitethesuccessofid195modelsindi-aloguegeneration,twoproblemsemerge:first,id195modelsaretrainedbypredictingthenextdialogueturninagivenconversationalcontextusingthemaximum-likelihoodestimation(id113)objectivefunction.however,itisnotclearhowwellid113approximatesthereal-worldgoalofchatbotdevelop-ment:teachingamachinetoconversewithhumans,whileprovidinginteresting,diverse,andinformativefeedbackthatkeepsusersengaged.oneconcreteexampleisthatid195modelstendtogeneratehighlygenericresponsessuchas   idon   tknow   re-gardlessoftheinput(sordonietal.,2015;serbanetal.,2016;lietal.,2016a).thiscanbeascribedtothehighfrequencyofgenericresponsesfoundinthetrainingsetandtheircompatibilitywithadiverserangeofconversationalcontexts.yet   idon   tknow   isapparentlynotagoodactiontotake,sinceitclosestheconversationdown.anothercommonproblem,illustratedinthetwosampleconversationsontheleftoftable1,isthatthesystembecomesstuckinanin   niteloopofrepet-itiveresponses.thisisduetoid113-basedid195models   inabilitytoaccountforrepetition.inexam-ple2(bottoid113ft),thedialoguefallsintoanin   niteloopafterthreeturns,withbothagentsgeneratingdull,genericutteranceslikeidon   tknowwhatyouaretalkingaboutandyoudon   tknowwhatyouaresaying.lookingattheentireconversation,utterance(4)turnsouttobeabadactiontotakebecauseitoffersnowayofcontinuingtheconversation.11asimilarruleisoftensuggestedinimprovisationalcomedy:https://en.wikipedia.org/wiki/yes,_and...1193

baselinemutualinformationmodel(lietal.2015)proposedreinforcementlearningmodela:whereareyougoing?(1)a:whereareyougoing?(1)b:i   mgoingtotherestroom.(2)b:i   mgoingtothepolicestation.(2)a:seeyoulater.(3)a:i   llcomewithyou.(3)b:seeyoulater.(4)b:no,no,no,no,you   renotgoinganywhere.(4)a:seeyoulater.(5)a:why?(5)b:seeyoulater.(6)b:ineedyoutostayhere.(6)...a:idon   tknowwhatyouaretalkingabout.(7)......a:howoldareyou?(1)a:howoldareyou?(1)b:i   m16.(2)b:i   m16.whyareyouasking?(2)a:16?(3)aithoughtyouwere12.(3)b:idon   tknowwhatyouaretalkingabout.(4)b:whatmadeyouthinkso?(4)a:youdon   tknowwhatyouaresaying.(5)a:idon   tknowwhatyouaretalkingabout.(5)b:idon   tknowwhatyouaretalkingabout.(6)b:youdon   tknowwhatyouaresaying.(6)a:youdon   tknowwhatyouaresaying.(7).........table1:leftcolumn:dialoguesimulationbetweentwoagentsusinga4-layerlstmencoder-decodertrainedontheopensubtitlesdataset.the   rstturn(index1)isinputbytheauthors.thenthetwoagentstaketurnsconversing,takingasinputtheotheragent   spriorgeneratedturn.theoutputisgeneratedusingthemutualinformationmodel(lietal.,2015)inwhichann-bestlistis   rstobtainedusingbeamsearchbasedonp(t|s)andrerankedbylinearlycombiningthebackwardid203p(s|t),wheretandsrespectivelydenotetargetsandsources.rightcolumn:dialoguesimulatedusingtheproposedreinforcementlearningmodel.thenewmodelhasmoreforward-lookingutterances(questionslike   whyareyouasking?   andofferslike   i   llcomewithyou   )andlastslongerbeforeitfallsintoconversationalblackholes.thesechallengessuggestweneedaconversa-tionframeworkthathastheabilityto(1)integratedeveloper-de   nedrewardsthatbettermimicthetruegoalofchatbotdevelopmentand(2)modelthelong-termin   uenceofageneratedresponseinanongoingdialogue.toachievethesegoals,wedrawontheinsightsofreinforcementlearning,whichhavebeenwidelyap-pliedinmdpandpomdpdialoguesystems(seere-latedworksectionfordetails).weintroduceaneu-ralreinforcementlearning(rl)generationmethod,whichcanoptimizelong-termrewardsdesignedbysystemdevelopers.ourmodelusestheencoder-decoderarchitectureasitsbackbone,andsimulatesconversationbetweentwovirtualagentstoexplorethespaceofpossibleactionswhilelearningtomaxi-mizeexpectedreward.wede   nesimpleheuristicap-proximationstorewardsthatcharacterizegoodcon-versations:goodconversationsareforward-looking(allwoodetal.,1992)orinteractive(aturnsuggestsafollowingturn),informative,andcoherent.thepa-rametersofanencoder-decoderid56de   neapolicyoveranin   niteactionspaceconsistingofallpossiid7tterances.theagentlearnsapolicybyoptimizingthelong-termdeveloper-de   nedrewardfromongo-ingdialoguesimulationsusingpolicygradientmeth-ods(williams,1992),ratherthantheid113objectivede   nedinstandardid195models.ourmodelthusintegratesthepowerofid195systemstolearncompositionalsemanticmeaningsofutteranceswiththestrengthsofreinforcementlearn-inginoptimizingforlong-termgoalsacrossaconver-sation.experimentalresults(sampledresultsattherightpaneloftable1)demonstratethatourapproachfostersamoresustaineddialogueandmanagestoproducemoreinteractiveresponsesthanstandardid195modelstrainedusingtheid113objective.2relatedworkeffortstobuildstatisticaldialogsystemsfallintotwomajorcategories.the   rsttreatsdialoguegenerationasasource-to-targettransductionproblemandlearnsmappingrulesbetweeninputmessagesandresponsesfromamassiveamountoftrainingdata.ritteretal.(2011)framestheresponsegenerationproblemasastatisti-1194

calmachinetranslation(smt)problem.sordonietal.(2015)improvedritteretal.   ssystembyrescor-ingtheoutputsofaphrasalsmt-basedconversationsystemwithaneuralmodelthatincorporatespriorcontext.recentprogressinid195modelsinspireseveralefforts(vinyalsandle,2015)tobuildend-to-endconversationalsystemswhich   rstapplyanencodertomapamessagetoadistributedvectorrep-resentingitssemanticsandgeneratearesponsefromthemessagevector.serbanetal.(2016)proposeahierarchicalneuralmodelthatcapturesdependen-ciesoveranextendedconversationhistory.lietal.(2016a)proposemutualinformationbetweenmes-sageandresponseasanalternativeobjectivefunctioninordertoreducetheproportionofgenericresponsesproducedbyid195systems.theotherlineofstatisticalresearchfocusesonbuildingtask-orienteddialoguesystemstosolvedomain-speci   ctasks.effortsincludestatisticalmodelssuchasmarkovdecisionprocesses(mdps)(levinetal.,1997;levinetal.,2000;walkeretal.,2003;pieraccinietal.,2009),pomdp(youngetal.,2010;youngetal.,2013;ga  sicetal.,2013a;ga  sicetal.,2014)models,andmodelsthatstatisti-callylearngenerationrules(ohandrudnicky,2000;ratnaparkhi,2002;banchsandli,2012;nioetal.,2014).thisdialogueliteraturethuswidelyappliesreinforcementlearning(walker,2000;schatzmannetal.,2006;gasicetal.,2013b;singhetal.,1999;singhetal.,2000;singhetal.,2002)totraindialoguepolicies.buttask-orientedrldialoguesystemsof-tenrelyoncarefullylimiteddialogueparameters,orhand-builttemplateswithstate,actionandrewardsig-nalsdesignedbyhumansforeachnewdomain,mak-ingtheparadigmdif   culttoextendtoopen-domainscenarios.alsorelevantispriorworkonreinforcementlearn-ingforlanguageunderstanding-includinglearningfromdelayedrewardsignalsbyplayingtext-basedgames(narasimhanetal.,2015;heetal.,2016),executinginstructionsforwindowshelp(branavanetal.,2011),orunderstandingdialoguesthatgivenavigationdirections(vogelandjurafsky,2010).ourgoalistointegratetheid195andrein-forcementlearningparadigms,drawingontheadvan-tagesofboth.wearethusparticularlyinspiredbyrecentworkthatattemptstomergetheseparadigms,includingwenetal.(2016)   traininganend-to-endtask-orienteddialoguesystemthatlinksinputrepre-sentationstoslot-valuepairsinadatabase   orsuetal.(2016),whocombinereinforcementlearningwithneuralgenerationontaskswithrealusers,show-ingthatreinforcementlearningimprovesdialogueperformance.3reinforcementlearningforopen-domaindialogueinthissection,wedescribeindetailthecomponentsoftheproposedrlmodel.thelearningsystemconsistsoftwoagents.weuseptodenotesentencesgeneratedfromthe   rstagentandqtodenotesentencesfromthesecond.thetwoagentstaketurnstalkingwitheachother.adialoguecanberepresentedasanalternatingse-quenceofsentencesgeneratedbythetwoagents:p1,q1,p2,q2,...,pi,qi.weviewthegeneratedsen-tencesasactionsthataretakenaccordingtoapolicyde   nedbyanencoder-decoderrecurrentneuralnet-worklanguagemodel.theparametersofthenetworkareoptimizedtomaximizetheexpectedfuturerewardusingpolicysearch,asdescribedinsection4.3.policygradi-entmethodsaremoreappropriateforourscenariothanid24(mnihetal.,2013),becausewecaninitializetheencoder-decoderid56usingid113pa-rametersthatalreadyproduceplausibleresponses,beforechangingtheobjectiveandtuningtowardsapolicythatmaximizeslong-termreward.id24,ontheotherhand,directlyestimatesthefutureex-pectedrewardofeachaction,whichcandifferfromtheid113objectivebyordersofmagnitude,thusmak-ingid113parametersinappropriateforinitialization.thecomponents(states,actions,reward,etc.)ofoursequentialdecisionproblemaresummarizedinthefollowingsub-sections.3.1actionanactionaisthedialogueutterancetogenerate.theactionspaceisin   nitesincearbitrary-lengthse-quencescanbegenerated.3.2stateastateisdenotedbytheprevioustwodialogueturns[pi,qi].thedialoguehistoryisfurthertransformedtoavectorrepresentationbyfeedingtheconcatena-tionofpiandqiintoanlstmencodermodelas1195

describedinlietal.(2016a).3.3policyapolicytakestheformofanlstmencoder-decoder(i.e.,prl(pi+1|pi,qi))andisde   nedbyitsparam-eters.notethatweuseastochasticrepresentationofthepolicy(aid203distributionoveractionsgivenstates).adeterministicpolicywouldresultinadiscontinuousobjectivethatisdif   culttooptimizeusinggradient-basedmethods.3.4rewardrdenotestherewardobtainedforeachaction.inthissubsection,wediscussmajorfactorsthatcontributetothesuccessofadialogueanddescribehowapprox-imationstothesefactorscanbeoperationalizedincomputablerewardfunctions.easeofansweringaturngeneratedbyamachineshouldbeeasytorespondto.thisaspectofaturnisrelatedtoitsforward-lookingfunction:thecon-straintsaturnplacesonthenextturn(schegloffandsacks,1973;allwoodetal.,1992).weproposetomeasuretheeaseofansweringageneratedturnbyusingthenegativeloglikelihoodofrespondingtothatutterancewithadullresponse.wemanuallycon-structedalistofdullresponsessconsisting8turnssuchas   idon   tknowwhatyouaretalkingabout   ,   ihavenoidea   ,etc.,thatweandothershavefoundoccurveryfrequentlyinid195modelsofcon-versations.therewardfunctionisgivenasfollows:r1=   1nsxs   s1nslogpid195(s|a)(1)wherensdenotesthecardinalityofnsandnsde-notesthenumberoftokensinthedullresponses.althoughofcoursetherearemorewaystogeneratedullresponsesthanthelistcancover,manyoftheseresponsesarelikelytofallintosimilarregionsinthevectorspacecomputedbythemodel.asysteid113sslikelytogenerateutterancesinthelististhusalsolesslikelytogenerateotherdullresponses.pid195representsthelikelihoodoutputbyid195models.itisworthnotingthatpid195isdifferentfromthestochasticpolicyfunctionprl(pi+1|pi,qi),sincetheformerislearnedbasedontheid113objectiveoftheid195modelwhilethelatteristhepolicyoptimizedforlong-termfuturerewardintherlsetting.r1isfurtherscaledbythelengthoftargets.informationflowwewanteachagenttocon-tributenewinformationateachturntokeepthedi-aloguemovingandavoidrepetitivesequences.wethereforeproposepenalizingsemanticsimilaritybe-tweenconsecutiveturnsfromthesameagent.lethpiandhpi+1denoterepresentationsobtainedfromtheencoderfortwoconsecutiveturnspiandpi+1.therewardisgivenbythenegativelogofthecosinesimilaritybetweenthem:r2=   logcos(hpi,hpi+1)=   loghpi  hpi+1khpikkhpi+1k(2)semanticcoherencewealsoneedtomeasuretheadequacyofresponsestoavoidsituationsinwhichthegeneratedrepliesarehighlyrewardedbutareuid165maticalornotcoherent.wethereforeconsiderthemutualinformationbetweentheactionaandpre-viousturnsinthehistorytoensurethegeneratedresponsesarecoherentandappropriate:r3=1nalogpid195(a|qi,pi)+1nqilogpbackwardid195(qi|a)(3)pid195(a|pi,qi)denotestheid203ofgenerat-ingresponseagiventhepreviousdialogueutterances[pi,qi].pbackwardid195(qi|a)denotesthebackwardproba-bilityofgeneratingthepreviousdialogueutteranceqibasedonresponsea.pbackwardid195istrainedinasimi-larwayasstandardid195modelswithsourcesandtargetsswapped.again,tocontrolthein   u-enceoftargetlength,bothlogpid195(a|qi,pi)andlogpbackwardid195(qi|a)arescaledbythelengthoftargets.the   nalrewardforactionaisaweightedsumoftherewardsdiscussedabove:r(a,[pi,qi])=  1r1+  2r2+  3r3(4)where  1+  2+  3=1.weset  1=0.25,  2=0.25and  3=0.5.arewardisobservedaftertheagentreachestheendofeachsentence.4simulationthecentralideabehindourapproachistosimulatetheprocessoftwovirtualagentstakingturnstalkingwitheachother,throughwhichwecanexplorethe1196

state-actionspaceandlearnapolicyprl(pi+1|pi,qi)thatleadstotheoptimalexpectedreward.weadoptanalphago-stylestrategy(silveretal.,2016)byinitializingtherlsystemusingageneralresponsegenerationpolicywhichislearnedfromafullysu-pervisedsetting.4.1supervisedlearningforthe   rststageoftraining,webuildonpriorworkofpredictingageneratedtargetsequencegivendia-loguehistoryusingthesupervisedid195model(vinyalsandle,2015).resultsfromsupervisedmodelswillbelaterusedforinitialization.wetrainedaid195modelwithattention(bah-danauetal.,2015)ontheopensubtitlesdataset,whichconsistsofroughly80millionsource-targetpairs.wetreatedeachturninthedatasetasatargetandtheconcatenationoftwoprevioussentencesassourceinputs.4.2mutualinformationsamplesfromid195modelsareoftentimesdullandgeneric,e.g.,   idon   tknow   (lietal.,2016a)wethusdonotwanttoinitializethepolicymodelusingthepre-trainedid195modelsbecausethiswillleadtoalackofdiversityintherlmodels   ex-periences.lietal.(2016a)showedthatmodelingmutualinformationbetweensourcesandtargetswillsigni   cantlydecreasethechanceofgeneratingdullresponsesandimprovegeneralresponsequality.wenowshowhowwecanobtainanencoder-decodermodelwhichgeneratesmaximummutualinforma-tionresponses.asillustratedinlietal.(2016a),directdecodingfromeq3isinfeasiblesincethesecondtermrequiresthetargetsentencetobecompletelygenerated.in-spiredbyrecentworkonsequencelevellearning(ranzatoetal.,2015),wetreattheproblemofgen-eratingmaximummutualinformationresponseasareinforcementlearningprobleminwhicharewardofmutualinformationvalueisobservedwhenthemodelarrivesattheendofasequence.similartoranzatoetal.(2015),weusepolicygra-dientmethods(suttonetal.,1999;williams,1992)foroptimization.weinitializethepolicymodelprlusingapre-trainedpid195(a|pi,qi)model.givenaninputsource[pi,qi],wegenerateacandidatelista={  a|  a   prl}.foreachgeneratedcandi-date  a,wewillobtainthemutualinformationscorem(  a,[pi,qi])fromthepre-trainedpid195(a|pi,qi)andpbackwardid195(qi|a).thismutualinformationscorewillbeusedasarewardandback-propagatedtotheencoder-decodermodel,tailoringittogeneratese-quenceswithhigherrewards.wereferthereaderstozarembaandsutskever(2015)andwilliams(1992)fordetails.theexpectedrewardforasequenceisgivenby:j(  )=e[m(  a,[pi,qi])](5)thegradientisestimatedusingthelikelihoodratiotrick:   j(  )=m(  a,[pi,qi])   logprl(  a|[pi,qi])(6)weupdatetheparametersintheencoder-decodermodelusingstochasticgradientdescent.acurricu-luid113arningstrategyisadopted(bengioetal.,2009)asinranzatoetal.(2015)suchthat,foreveryse-quenceoflengthtweusetheid113lossforthe   rstltokensandthereinforcementalgorithmfortheremainingt   ltokens.wegraduallyannealthevalueofltozero.abaselinestrategyisemployedtodecreasethelearningvariance:anadditionalneuralmodeltakesasinputsthegeneratedtargetandtheinitialsourceandoutputsabaselinevalue,similartothestrategyadoptedbyzarembaandsutskever(2015).the   nalgradientisthus:   j(  )=   logprl(  a|[pi,qi])[m(  a,[pi,qi])   b](7)4.3dialoguesimulationbetweentwoagentswesimulateconversationsbetweenthetwovirtualagentsandhavethemtaketurnstalkingwitheachother.thesimulationproceedsasfollows:attheinitialstep,amessagefromthetrainingsetisfedtothe   rstagent.theagentencodestheinputmessagetoavectorrepresentationandstartsdecodingtogen-eratearesponseoutput.combiningtheimmediateoutputfromthe   rstagentwiththedialoguehistory,thesecondagentupdatesthestatebyencodingthedialoguehistoryintoarepresentationandusesthedecoderid56togenerateresponses,whicharesub-sequentlyfedbacktothe   rstagent,andtheprocessisrepeated.1197

............mhow old are you?i   m 16, why are you asking?            i   m 16inputmessage...16?i thought you were 12..........turn1p1,2p1,3turn2q11,1q11,2q21,1q21,2q31,1q31,2.........         tuid56p1n,1p1n,2p1,1p2n,1p2n,2p3n,1p3n,2encodedecodeencodedecodeencodedecodefigure1:dialoguesimulationbetweenthetwoagents.optimizationweinitializethepolicymodelprlwithparametersfromthemutualinformationmodeldescribedintheprevioussubsection.wethenusepolicygradientmethodsto   ndparametersthatleadtoalargerexpectedreward.theobjectivetomaxi-mizeistheexpectedfuturereward:jrl(  )=eprl(a1:t)[i=txi=1r(ai,[pi,qi])](8)wherer(ai,[pi,qi])denotestherewardresultingfromactionai.weusethelikelihoodratiotrick(williams,1992;glynn,1990;aleksandrovetal.,1968)forgradientupdates:   jrl(  )   xi   logp(ai|pi,qi)i=txi=1r(ai,[pi,qi])(9)wereferreaderstowilliams(1992)andglynn(1990)formoredetails.4.4curriculuid113arningacurriculuid113arningstrategyisagainemployedinwhichwebeginbysimulatingthedialoguefor2turns,andgraduallyincreasethenumberofsimulatedturns.wegenerate5turnsatmost,asthenumberofcandidatestoexaminegrowsexponentiallyinthesizeofcandidatelist.fivecandidateresponsesaregeneratedateachstepofthesimulation.5experimentalresultsinthissection,wedescribeexperimentalresultsalongwithqualitativeanalysis.weevaluatedialoguegenerationsystemsusingbothhumanjudgmentsandtwoautomaticmetrics:conversationlength(numberofturnsintheentiresession)anddiversity.5.1datasetthedialoguesimulationrequireshigh-qualityinitialinputsfedtotheagent.forexample,aninitialinputof   why?   isundesirablesinceitisunclearhowthedialoguecouldproceed.wetakeasubsetof10millionmessagesfromtheopensubtitlesdatasetandextract0.8millionsequenceswiththelowestlikelihoodofgeneratingtheresponse   idon   tknowwhatyouaretakingabout   toensureinitialinputsareeasytorespondto.5.2automaticevaluationevaluatingdialoguesystemsisdif   cult.metricssuchasid7(papinenietal.,2002)andperplexityhavebeenwidelyusedfordialoguequalityevaluation(lietal.,2016a;vinyalsandle,2015;sordonietal.,2015),butitiswidelydebatedhowwelltheseauto-maticmetricsarecorrelatedwithtrueresponsequal-ity(liuetal.,2016;galleyetal.,2015).sincethegoaloftheproposedsystemisnottopredictthehighestid203response,butratherthelong-termsuccessofthedialogue,wedonotemployid7orperplexityforevaluation2.2wefoundtherlmodelperformsworseonid7score.onarandomsampleof2,500conversationalpairs,singlereferenceid7scoresforrlmodels,mutualinformationmodelsandvanillaid195modelsarerespectively1.28,1.44and1.17.id7ishighlycorrelatedwithperplexityingenerationtasks.1198

model#ofsimulatedturnsid1952.68mutualinformation3.40rl4.48table2:theaveragenumberofsimulatedturnsfromstandardid195models,mutualinforma-tionmodelandtheproposedrlmodel.lengthofthedialoguethe   rstmetricwepro-poseisthelengthofthesimulateddialogue.wesayadialogueendswhenoneoftheagentsstartsgener-atingdullresponsessuchas   idon   tknow   3ortwoconsecutiveutterancesfromthesameuserarehighlyoverlapping4.thetestsetconsistsof1,000inputmessages.toreducetheriskofcirculardialogues,welimitthenumberofsimulatedturnstobelessthan8.resultsareshownintable2.ascanbeseen,usingmutualinformationleadstomoresustainedconversationsbetweenthetwoagents.theproposedrlmodelis   rsttrainedbasedonthemutualinformationobjec-tiveandthusbene   tsfromitinadditiontotherlmodel.weobservethattherlmodelwithdialoguesimulationachievesthebestevaluationscore.diversitywereportdegreeofdiversitybycalculat-ingthenumberofdistinctunigramsandbigramsingeneratedresponses.thevalueisscaledbythetotalnumberofgeneratedtokenstoavoidfavoringlongsentencesasdescribedinlietal.(2016a).there-sultingmetricisthusatype-tokenratioforunigramsandbigrams.forboththestandardid195modelandthepro-posedrlmodel,weusebeamsearchwithabeamsize10togeneratearesponsetoagiveninputmes-sage.forthemutualinformationmodel,we   rstgeneraten-bestlistsusingpid195(t|s)andthenlinearlyre-rankthemusingpid195(s|t).resultsarepresentedintable4.we   ndthattheproposedrlmodelgeneratesmorediverseoutputswhencom-sincetherlmodelistrainedbasedonfuturerewardratherthanid113,itisnotsurprisingthattherlbasedmodelsachievelowerid7score.3weuseasimplerulematchingmethod,withalistof8phrasesthatcountasdullresponses.althoughthiscanleadtobothfalse-positivesand-negatives,itworksprettywellinpractice.4twoutterancesareconsideredtoberepetitiveiftheysharemorethan80percentoftheirwords.paredagainstboththevanillaid195modelandthemutualinformationmodel.humanevaluationweexplorethreesettingsforhumanevaluation:the   rstsettingissimilartowhatwasdescribedinlietal.(2016a),whereweemploycrowdsourcedjudgestoevaluatearandomsampleof500items.wepresentbothaninputmessageandthegeneratedoutputsto3judgesandaskthemtodecidewhichofthetwooutputsisbetter(denotedassingle-turngeneralquality).tiesarepermitted.identicalstringsareassignedthesamescore.wemeasuretheimprovementachievedbytherlmodeloverthemutualinformationmodelbythemeandifferenceinscoresbetweenthemodels.forthesecondsetting,judgesareagainpresentedwithinputmessagesandsystemoutputs,butareaskedtodecidewhichofthetwooutputsiseasiertorespondto(denotedassingle-turneasetoanswer).againweevaluatearandomsampleof500items,eachbeingassignedto3judges.forthethirdsetting,judgesarepresentedwithsim-ulatedconversationsbetweenthetwoagents(denotedasmulti-turngeneralquality).eachconversationconsistsof5turns.weevaluate200simulatedcon-versations,eachbeingassignedto3judges,whoareaskedtodecidewhichofthesimulatedconversationsisofhigherquality.resultsforhumanevaluationareshownintable5.theproposedrlsystemdoesnotintroduceasigni   -cantboostinsingle-turnresponsequality(winning40percentoftimeandlosing36percentoftime).thisisinlinewithourexpectations,astherlmodelisnotoptimizedtopredictthenextutterance,butrathertoincreaselong-termreward.therlsys-temproducesresponsesthataresigni   cantlyeasiertoanswerthandoesthemutualinformationsystem,asdemonstratedbythesingle-turneasetoanswersetting(winning52percentoftimeandlosing23percentoftime),andalsosigni   cantlyhigherqualitymulti-turndialogues,asdemonstratedbythemulti-turngeneralqualitysetting(winning72percentoftime).qualitativeanalysisanddiscussionweshowarandomsampleofgeneratedresponsesintable3andsimulatedconversationsintable1atthebeginningofthepaper.fromtable3,wecanseethattherlbasedagentindeedgeneratesmoreinteractive1199

inputmessagemutualinformationmodelrlhowoldareyou?i   m16.i   m16,whyareyouasking?what   syourfullname?ihavenoidea.what   syours?idon   twanttogohometonight.really?why?doyouhaveanyfeelingsforme?idon   tknowwhatyouaretalkingabout.idon   twanttohurtyourfeelings.howmuchtimedoyouhavehere?notlongenough.sorry,sir.tenseconds.shallwegetstarted?ofcourse!yes.we   vegotalotofworktodohere.doyouplayfootball?no,idon   tyes.ilovefootball!we   dhavetotalktohim.imean,he   sagoodguyaboutwhat?howcomeyouneversayit?becauseidon   twanttohurtyou.idon   tthinkit   sagoodideatosayit.table3:sampledresponsesgeneratedfromthemutualinformationmodelsandtheproposedrlmodel.modelunigrambigramid1950.00620.015mutualinformation0.0110.031rl0.0170.041table4:diversityscores(type-tokenratios)forthestandardid195model,mutualinformationmodelandtheproposedrlmodel.settingrl-winrl-losetiesingle-turngeneralquality0.400.360.24single-turneasetoanswer0.520.230.25multi-turngeneralquality0.720.120.16table5:rlgainsoverthemutualinformationsys-tembasedonpairwisehumanjudgments.responsesthantheotherbaselines.wealso   ndthattherlmodelhasatendencytoendasentencewithanotherquestionandhandtheconversationovertotheuser.fromtable1,weobservethattherlmodelmanagestoproducemoreinteractiveandsustainedconversationsthanthemutualinformationmodel.duringerroranalysis,wefoundthatalthoughwepenalizerepetitiveutterancesinconsecutiveturns,thedialoguesometimesentersacyclewithlengthgreaterthanone,asshownintable6.thiscanbeascribedtothelimitedamountofconversationalhis-toryweconsider.anotherissueobservedisthatthemodelsometimesstartsalessrelevanttopicduringtheconversation.thereisatradeoffbetweenrele-vanceandlessrepetitiveness,asmanifestedintherewardfunctionwede   neineq4.thefundamentalproblem,ofcourse,isthatthemanuallyde   nedrewardfunctioncan   tpossiblycoverthecrucialaspectsthatde   neanidealconversa-tion.whiletheheuristicrewardsthatwede   nedareamenabletoautomaticcalculation,anddocapturea:what   syourname?b:daniel.a:howoldareyou?b.twelve.what   syourname?a.daniel.b:howoldareyou?a:twelve.what   syourname?b:daniel.a:howoldareyou?b...table6:ansimulateddialoguewithacyclelongerthanone.someaspectsofwhatmakesagoodconversation,ideallythesystemwouldinsteadreceiverealrewardsfromhumans.anotherproblemwiththecurrentmodelisthatwecanonlyaffordtoexploreaverysmallnumberofcandidatesandsimulatedturnssincethenumberofcasestoconsidergrowexponentially.6conclusionweintroduceareinforcementlearningframeworkforneuralresponsegenerationbysimulatingdialoguesbetweentwoagents,integratingthestrengthsofneu-ralid195systemsandreinforcementlearningfordialogue.likeearlierneuralid195models,ourframeworkcapturesthecompositionalmodelsofthemeaningofadialogueturnandgeneratesse-manticallyappropriateresponses.likereinforce-mentlearningdialoguesystems,ourframeworkisabletogenerateutterancesthatoptimizefuturere-ward,successfullycapturingglobalpropertiesofagoodconversation.despitethefactthatourmodelusesverysimple,operationableheuristicsforcaptur-ingtheseglobalproperties,theframeworkgeneratesmorediverse,interactiveresponsesthatfosteramoresustainedconversation.1200

acknowledgementwewouldliketothankchrisbrockett,billdolanandothermembersofthenlpgroupatmicrosoftre-searchforinsightfulcommentsandsuggestions.wealsowanttothankkelvinguu,percyliang,chrismanning,sidawang,ziangxieandothermembersofthestanfordnlpgroupsforusefuldiscussions.jiweiliissupportedbythefacebookfellowship,towhichwegratefullyacknowledge.thisworkispar-tiallysupportedbythensfviaawardsiis-1514268,iis-1464128,andbythedarpacommunicatingwithcomputers(cwc)programunderaroprimecontractno.w911nf-15-1-0462.anyopinions,   ndings,andconclusionsorrecommendationsex-pressedinthismaterialarethoseoftheauthorsanddonotnecessarilyre   ecttheviewsofnsf,darpa,orfacebook.referencesv.m.aleksandrov,v.i.sysoyev,andv.v.shemeneva.1968.stochasticoptimization.engineeringcybernet-ics,5:11   16.jensallwood,joakimnivre,andelisabethahls  en.1992.onthesemanticsandpragmaticsoflinguisticfeedback.journalofsemantics,9:1   26.dzmitrybahdanau,kyunghyuncho,andyoshuabengio.2015.neuralmachinetranslationbyjointlylearningtoalignandtranslate.inproc.oficlr.rafaelebanchsandhaizhouli.2012.iris:achat-orienteddialoguesystembasedonthevectorspacemodel.inproceedingsoftheacl2012systemdemon-strations,pages37   42.yoshuabengio,j  er  omelouradour,ronancollobert,andjasonweston.2009.curriculuid113arning.inpro-ceedingsofthe26thannualinternationalconferenceonmachinelearning,pages41   48.acm.srkbranavan,davidsilver,andreginabarzilay.2011.learningtowinbyreadingmanualsinamonte-carloframework.inproceedingsofthe49thannualmeetingoftheassociationforcomputationallinguistics:hu-manlanguagetechnologies-volume1,pages268   277.michelgalley,chrisbrockett,alessandrosordoni,yangfengji,michaelauli,chrisquirk,margaretmitchell,jianfenggao,andbilldolan.2015.deltaid7:adiscriminativemetricforgenerationtaskswithintrinsicallydiversetargets.inproc.ofacl-ijcnlp,pages445   450,beijing,china,july.milicaga  sic,catherinebreslin,matthewhenderson,donghokim,martinszummer,blaisethomson,pir-rostsiakoulis,andsteveyoung.2013a.pomdp-baseddialoguemanageradaptationtoextendeddomains.inproceedingsofsigdial.milicagasic,catherinebreslin,mikehenderson,dongkyukim,martinszummer,blaisethomson,pir-rostsiakoulis,andsteveyoung.2013b.on-linepolicyoptimisationofbayesianspokendialoguesystemsviahumaninteraction.inproceedingsoficassp2013,pages8367   8371.ieee.milicaga  sic,donghokim,pirrostsiakoulis,catherinebreslin,matthewhenderson,martinszummer,blaisethomson,andsteveyoung.2014.incrementalon-lineadaptationofpomdp-baseddialoguemanagerstoextendeddomains.inproceedingsoninterspeech.peterwglynn.1990.likelihoodratiogradientestima-tionforstochasticsystems.communicationsoftheacm,33(10):75   84.jihe,jianshuchen,xiaodonghe,jianfenggao,lihongli,lideng,andmariostendorf.2016.deeprein-forcementlearningwithanaturallanguageactionspace.inproceedingsofthe54thannualmeetingoftheasso-ciationforcomputationallinguistics(volume1:longpapers),pages1621   1630,berlin,germany,august.estherlevin,robertopieraccini,andwielandeckert.1997.learningdialoguestrategieswithinthemarkovdecisionprocessframework.inautomaticspeechrecognitionandunderstanding,1997.proceedings.,1997ieeeworkshopon,pages72   79.ieee.estherlevin,robertopieraccini,andwielandeckert.2000.astochasticmodelofhuman-machineinterac-tionforlearningdialogstrategies.ieeetransactionsonspeechandaudioprocessing,8(1):11   23.jiweili,michelgalley,chrisbrockett,jianfenggao,andbilldolan.2016a.adiversity-promotingobjectivefunctionforneuralconversationmodels.inproc.ofnaacl-hlt.jiweili,michelgalley,chrisbrockett,georgiossp-ithourakis,jianfenggao,andbilldolan.2016b.apersona-basedneuralconversationmodel.inproceed-ingsofthe54thannualmeetingoftheassociationforcomputationallinguistics(volume1:longpapers),pages994   1003,berlin,germany,august.chia-weiliu,ryanlowe,iulianvserban,michaelnose-worthy,laurentcharlin,andjoellepineau.2016.hownottoevaluateyourdialoguesystem:anempiricalstudyofunsupervisedevaluationmetricsfordialogueresponsegeneration.arxivpreprintarxiv:1603.08023.yiluan,yangfengji,andmariostendorf.2016.lstmbasedconversationmodels.arxivpreprintarxiv:1603.09457.volodymyrmnih,koraykavukcuoglu,davidsilver,alexgraves,ioannisantonoglou,daanwierstra,andmar-tinriedmiller.2013.playingatariwithdeeprein-forcementlearning.nipsdeeplearningworkshop.1201

karthiknarasimhan,tejaskulkarni,andreginabarzilay.2015.languageunderstandingfortext-basedgamesusingdeepreinforcementlearning.arxivpreprintarxiv:1506.08941.lasguidonio,sakrianisakti,grahamneubig,tomokitoda,mirnaadriani,andsatoshinakamura.2014.developingnon-goaldialogsystembasedonexamplesofdramatelevision.innaturalinteractionwithrobots,knowbotsandsmartphones,pages355   361.springer.alicehohandalexanderirudnicky.2000.stochasticlanguagegenerationforspokendialoguesystems.inproceedingsofthe2000anlp/naaclworkshoponconversationalsystems-volume3,pages27   32.kishorepapineni,salimroukos,toddward,andwei-jingzhu.2002.id7:amethodforautomaticeval-uationofmachinetranslation.inproceedingsofthe40thannualmeetingonassociationforcomputationallinguistics,pages311   318.robertopieraccini,davidsuendermann,krishnadayanidhi,andjacksonliscombe.2009.arewethereyet?researchincommercialspokendialogsystems.intext,speechanddialogue,pages3   13.springer.marc   aurelioranzato,sumitchopra,michaelauli,andwojciechzaremba.2015.sequenceleveltrain-ingwithrecurrentneuralnetworks.arxivpreprintarxiv:1511.06732.adwaitratnaparkhi.2002.trainableapproachestosur-facenaturallanguagegenerationandtheirapplicationtoconversationaldialogsystems.computerspeech&language,16(3):435   455.alanritter,colincherry,andwilliambdolan.2011.data-drivenresponsegenerationinsocialmedia.inproceedingsofemnlp2011,pages583   593.jostschatzmann,karlweilhammer,mattstuttle,andsteveyoung.2006.asurveyofstatisticalusersimula-tiontechniquesforreinforcement-learningofdialoguemanagementstrategies.theknowledgeengineeringreview,21(02):97   126.emanuela.schegloffandharveysacks.1973.openingupclosings.semiotica,8(4):289   327.iulianvserban,alessandrosordoni,yoshuabengio,aaroncourville,andjoellepineau.2016.buildingend-to-enddialoguesystemsusinggenerativehierar-chicalneuralnetworkmodels.inproceedingsofaaai,february.lifengshang,zhengdonglu,andhangli.2015.neuralrespondingmachineforshort-textconversation.inproceedingsofacl-ijcnlp,pages1577   1586.davidsilver,ajahuang,chrisjmaddison,arthurguez,laurentsifre,georgevandendriessche,julianschrit-twieser,ioannisantonoglou,vedapanneershelvam,marclanctot,etal.2016.masteringthegameofgowithdeepneuralnetworksandtreesearch.nature,529(7587):484   489.satinderpsingh,michaeljkearns,dianejlitman,andmarilynawalker.1999.reinforcementlearningforspokendialoguesystems.innips,pages956   962.satindersingh,michaelkearns,dianejlitman,mar-ilynawalker,etal.2000.empiricalevaluationofareinforcementlearningspokendialoguesystem.inaaai/iaai,pages645   651.satindersingh,dianelitman,michaelkearns,andmari-lynwalker.2002.optimizingdialoguemanagementwithreinforcementlearning:experimentswiththenj-funsystem.journalofarti   cialintelligenceresearch,pages105   133.alessandrosordoni,michelgalley,michaelauli,chrisbrockett,yangfengji,megmitchell,jian-yunnie,jianfenggao,andbilldolan.2015.aneuralnetworkapproachtocontext-sensitivegenerationofconversa-tionalresponses.inproceedingsofnaacl-hlt.pei-haosu,milicagasic,nikolamrksic,linarojas-barahona,stefanultes,davidvandyke,tsung-hsienwen,andsteveyoung.2016.continuouslylearningneuraldialoguemanagement.arxiv.ilyasutskever,oriolvinyals,andquocvle.2014.sequencetosequencelearningwithneuralnetworks.inadvancesinneuralinformationprocessingsystems,pages3104   3112.richardssutton,davidamcallester,satinderpsingh,yishaymansour,etal.1999.policygradientmethodsforreinforcementlearningwithfunctionapproximation.innips,volume99,pages1057   1063.oriolvinyalsandquocle.2015.aneuralconversa-tionalmodel.inproceedingsoficmldeeplearningworkshop.adamvogelanddanjurafsky.2010.learningtofollownavigationaldirections.inproceedingsofacl2010,pages806   814.marilynawalker,rashmiprasad,andamandastent.2003.atrainablegeneratorforrecommendationsinmultimodaldialog.inproceeedingsofinterspeech2003.marilyna.walker.2000.anapplicationofreinforce-mentlearningtodialoguestrategyselectioninaspokendialoguesystemforemail.journalofarti   cialintelli-genceresearch,pages387   416.tsung-hsienwen,milicagasic,nikolamrk  si  c,pei-haosu,davidvandyke,andsteveyoung.2015.semanti-callyconditionedlstm-basednaturallanguagegener-ationforspokendialoguesystems.inproceedingsofemnlp,pages1711   1721,lisbon,portugal.tsung-hsienwen,milicagasic,nikolamrksic,linamrojas-barahona,pei-haosu,stefanultes,davidvandyke,andsteveyoung.2016.anetwork-basedend-to-endtrainabletask-orienteddialoguesystem.arxivpreprintarxiv:1604.04562.1202

ronaldjwilliams.1992.simplestatisticalgradient-followingalgorithmsforconnectionistreinforcementlearning.machinelearning,8(3-4):229   256.zhenxu,bingquanliu,baoxunwang,chengjiesun,andxiaolongwang.2016.incorporatingloose-structuredknowledgeintolstmwithrecallgateforconversationmodeling.arxivpreprintarxiv:1605.05110.kaishengyao,geoffreyzweig,andbaolinpeng.2015.attentionwithintentionforaneuralnetworkconversa-tionmodel.innipsworkshoponmachinelearningforspokenlanguageunderstandingandinteraction.steveyoung,milicaga  si  c,simonkeizer,franc  oismairesse,jostschatzmann,blaisethomson,andkaiyu.2010.thehiddeninformationstatemodel:aprac-ticalframeworkforpomdp-basedspokendialogueman-agement.computerspeech&language,24(2):150   174.steveyoung,milicagasic,blaisethomson,andjasondwilliams.2013.pomdp-basedstatisticalspokendi-alogsystems:areview.proceedingsoftheieee,101(5):1160   1179.wojciechzarembaandilyasutskever.2015.reinforce-mentlearningneuralturingmachines.arxivpreprintarxiv:1505.00521.