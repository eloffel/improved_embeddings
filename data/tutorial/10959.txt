a context-aware natural language generator for dialogue systems

ond  rej du  sek and filip jur  c      cek

charles university in prague, faculty of mathematics and physics

institute of formal and applied linguistics

malostransk  e n  am  est     25, cz-11800 prague, czech republic

{odusek,jurcicek}@ufal.mff.cuni.cz

6
1
0
2

 

g
u
a
5
2

 

 
 
]
l
c
.
s
c
[
 
 

1
v
6
7
0
7
0

.

8
0
6
1
:
v
i
x
r
a

abstract

we present a novel natural language gen-
eration system for spoken dialogue sys-
tems capable of entraining (adapting) to
users    way of speaking, providing contex-
tually appropriate responses. the genera-
tor is based on recurrent neural networks
and the sequence-to-sequence approach.
it is fully trainable from data which in-
clude preceding context along with re-
sponses to be generated. we show that
the context-aware generator yields signif-
icant improvements over the baseline in
both automatic metrics and a human pair-
wise preference test.

introduction

1
in a conversation, speakers are in   uenced by pre-
vious utterances of their counterparts and tend to
adapt (align, entrain) their way of speaking to each
other, reusing lexical items as well as syntactic
structure (reitter et al., 2006). entrainment occurs
naturally and subconsciously, facilitates success-
ful conversations (friedberg et al., 2012; nenkova
et al., 2008), and forms a natural source of vari-
ation in dialogues.
in spoken dialogue systems
(sds), users were reported to entrain to system
prompts (parent and eskenazi, 2010).

the function of id86
(id86) components in task-oriented sds typically
is to produce a natural language sentence from a
dialogue act (da) (young et al., 2010) represent-
ing an action, such as inform or request, along
with one or more attributes (slots) and their val-
ues (see fig. 1). id86 is an important component
of sds which has a great impact on the perceived
naturalness of the system; its quality can also in-
   uence the overall task success (stoyanchev and
stent, 2009; lopes et al., 2013). however, typical

figure 1: an example of id86 input and output,
with context-aware additions.

id86 systems in sds only take the input da into
account and have no way of adapting to the user   s
way of speaking. to avoid repetition and add vari-
ation into the outputs, they typically alternate be-
tween a handful of preset variants (jur  c      cek et al.,
2014) or use overgeneration and random sampling
from a k-best list of outputs (wen et al., 2015b).
there have been several attempts at introducing
entrainment into id86 in sds, but they are lim-
ited to rule-based systems (see section 4).

we present a novel, fully trainable context-
aware id86 system for sds that is able to entrain
to the user and provides naturally variable outputs
because generation is conditioned not only on the
input da, but also on the preceding user utter-
ance (see fig. 1). our system is an extension of
du  sek and jur  c      cek (2016b)   s generator based on
sequence-to-sequence (id195) models with at-
tention (bahdanau et al., 2015). it is, to our knowl-
edge, the    rst fully trainable entrainment-enabled
id86 system for sds. we also present our    rst re-
sults on the dataset of du  sek and jur  c      cek (2016a),
which includes the preceding user utterance along
with each data instance (i.e., pair of input mean-
ing representation and output sentence), and we
show that our context-aware system outperforms
the baseline in both automatic metrics and a hu-
man pairwise preference test.

is there another optioninform(line=m102, direction=herald square,            vehicle=bus, departure_time=9:01am,            from_stop=wall street)take bus line m102 from wall street                     to herald square at 9:01am.there is a bus at 9:01am from wall street                            to herald square using line m102.typical id86context-awareadditionscontextually bound responsepreceding user utterancein the following, we    rst present the architec-
ture of our generator (see section 2), then give an
account of our experiments in section 3. we in-
clude a brief survey of related work in section 4.
section 5 contains concluding remarks and plans
for future work.

2 our generator
our id195 generator is an improved version of
du  sek and jur  c      cek (2016b)   s generator, which it-
self is based on the id195 model with atten-
tion (bahdanau et al., 2015, see fig. 2) as imple-
mented in the tensorflow framework (abadi et
al., 2015).1 we    rst describe the base model in
section 2.1, then list our context-aware improve-
ments in section 2.2.

2.1 baseline id195 id86 with attention
the generation has two stages: the    rst, encoder
stage uses a recurrent neural network (id56)
composed of long-short-term memory (lstm)
cells (hochreiter and schmidhuber, 1997; graves,
2013) to encode a sequence of input tokens2 x =
{x1, . . . , xn} into a sequence of hidden states h =
{h1, . . . , hn}:

ht = lstm(xt, ht   1)

(1)

the second, decoder stage then uses the hidden
states h to generate the output sequence y =
{y1, . . . , ym}.
its main component is a second
lstm-based id56, which works over its own in-
ternal state st and the previous output token yt   1:

st = lstm((yt   1     ct)ws, st   1)

(2)

it is initialized by the last hidden encoder state
(s0 = hn) and a special starting symbol. the gen-
erated output token yt is selected from a softmax
distribution:

p(yt|yt   1 . . . , x) = softmax((st     ct)wy )

(3)

in (2) and (3), ct represents the attention model     a
sum over all encoder hidden states, weighted by a
feed-forward network with one tanh hidden layer;
ws and wy are linear projection matrices and          
denotes concatenation.

das are represented as sequences on the en-
coder input: a triple of the structure    da type, slot,
1see (du  sek and jur  c      cek, 2016b) and (bahdanau et al.,

2015) for a more formal description of the base model.

2embeddings are used (bengio et al., 2003), i.e., xt and
yt are vector representations of the input and output tokens.

value    is created for each slot in the da and the
triples are concatenated (see fig. 2). 3 the gen-
erator supports greedy decoding as well as beam
search which keeps track of top k most probable
output sequences at each time step (sutskever et
al., 2014; bahdanau et al., 2015).

the generator further features a simple con-
tent classi   cation reranker to penalize irrelevant
or missing information on the output. it uses an
lstm-based id56 to encode the generator out-
puts token-by-token into a    xed-size vector. this
is then fed to a sigmoid classi   cation layer that
outputs a 1-hot vector indicating the presence of
all possible da types, slots, and values. the vec-
tors for all k-best generator outputs are then com-
pared to the input da and the number of missing
and irrelevant elements is used to rerank them.

2.2 making the generator context-aware
we implemented three different modi   cations to
our generator that make its output dependent on
the preceding context: 4

prepending context. the preceding user utter-
ance is simply prepended to the da and fed into
the encoder (see fig. 2). the dictionary for con-
text utterances is distinct from the da tokens dic-
tionary.

context encoder. we add another, separate en-
coder for the context utterances. the hidden states
of both encoders are concatenated, and the de-
coder then works with double-sized vectors both
on the input and in the attention model (see fig. 2).

id165 match reranker. we added a second
reranker for the k-best outputs of the generator
that promotes outputs that have a word or phrase
overlap with the context utterance. we use geo-
metric mean of modi   ed id165 precisions (with
n     {1, 2}) as a measure of context overlap, i.e.,
id7-2 (papineni et al., 2002) without brevity
penalty. the log id203 l of an output se-
quence on the generator k-best list is updated as
follows:

l = l + w       

p1p2

(4)

3while the sequence encoding may not necessarily be
the best way to obtain a vector representation of da, it was
shown to work well (du  sek and jur  c      cek, 2016b).

4for simplicity, we kept close to the basic id195 archi-
tecture of the generator; other possibilities for encoding the
context, such as convolution and/or max-pooling, are possi-
ble.

figure 2: the base id195 generator (black) with our improvements: prepending context (green) and
separate context encoder (blue).
setup
baseline (context not used)
id165 match reranker
prepending context

id7 nist
7.037
66.41
7.577
68.68
63.87
6.456
7.772
69.26
6.818
63.08
69.17
7.596

+ id165 match reranker

context encoder

+ id165 match reranker

table 1: id7 and nist scores of different gen-
erator setups on the test data.

in (4), p1 and p2 are modi   ed unigram and bigram
precisions of the output sequence against the con-
text, and w is a preset weight. we believe that any
reasonable measure of contextual match would be
viable here, and we opted for modi   ed id165
precisions because of simple computation, well-
de   ned range, and the relation to the de facto stan-
dard id7 metric.5 we only use unigrams and
bigrams to promote especially the reuse of single
words or short phrases.

in addition, we combine the id165 match
reranker with both of the two former approaches.
we used gold-standard transcriptions of the im-
mediately preceding user utterance in our experi-
ments in order to test the context-aware capabil-
ities of our system in a stand-alone setting; in a
live sds, 1-best id103 hypotheses
and longer user utterance history can be used with
no modi   cations to the architecture.

3 experiments

we experiment on the publicly available dataset of
du  sek and jur  c      cek (2016a)6 for id86 in the pub-

5we do not use brevity penalty as we do not want to de-
mote shorter output sequences. however, adding it to the for-
mula in our preliminary experiments yielded similar results
to the ones presented here.

6the dataset

released at http://hdl.handle.
net/11234/1-1675; we used a more recent version
from github (https://github.com/ufal-dsg/alex

is

lic transport information domain, which includes
preceding context along with each pair of input
da and target natural language sentence. it con-
tains over 5,500 utterances, i.e., three paraphrases
for each of the over 1,800 combinations of input
da and context user utterance. the data con-
cern bus and subway connections on manhattan,
and comprise four da types (icon   rm, inform, in-
form no match, request). they are delexicalized
for generation to avoid sparsity, i.e., stop names,
vehicles, times, etc., are replaced by placeholders
(wen et al., 2015a). we applied a 3:1:1 split of the
set into training, development, and test data. we
use the three paraphrases as separate instances in
training data, but they serve as three references for
a single generated output in validation and evalua-
tion.

we test

the three context-aware setups de-
scribed in section 2.2 and their combinations,
and we compare them against the baseline non-
context-aware id195 generator. same as du  sek
and jur  c      cek (2016b), we train the id195 mod-
els by minimizing cross-id178 on the training set
using the adam optimizer (kingma and ba, 2015),
and we measure id7 on the development set af-
ter each pass over the training data, selecting the
best-performing parameters.7 the content clas-
si   cation reranker is trained in a similar fashion,
measuring misclassi   cation on both training and
development set after each pass.8 we use 5 dif-
ferent random initializations of the networks and

context id86 dataset), which contains several small    xes.
7based on our preliminary experiments on development
data, we use embedding size 50, lstm cell size 128, learning
rate 0.0005, and batch size 20. training is run for at least
50 and up to 1000 passes, with early stopping if the top 10
validation id7 scores do not change for 100 passes.

8we use the same settings except for the number of passes
over the training data, which is at least 20 and 100 at most.
for validation, development set is given 10 times more im-
portance than the training set.

lstmlstmlstmlstmlstmlstmlstmlstmlstmlstmlstm+attattattattattlstmattinform_no_matchdeparture_timedeparture_timeampmampminform_no_matchlstmattlstmattschedulefordeparture_timeampmnotfound.<go>schedulefordeparture_timeampmnotfound.<stop>schedulefornumberampmistherealstmlstmlstmlstmlstmlstmlstmschedulefornumberampmistherealstmlstmlstmlstmlstmlstmlstm+++base modelprepending contextcontext encoderdecoder with attentionda encoderaverage the results.

decoding is run with a beam size of 20 and the
penalty weight for content classi   cation reranker
set to 100. we set the id165 match reranker
weight based on experiments on development
data.9

3.1 evaluation using automatic metrics
table 1 lists our results on the test data in terms
of the id7 and nist metrics (papineni et al.,
2002; doddington, 2002). we can see that while
the id165 match reranker brings a id7 score
improvement, using context prepending or sepa-
rate encoder results in scores lower than the base-
line.10 however, using the id165 match reranker
together with context prepending or separate en-
coder brings signi   cant improvements of about
2.8 id7 points in both cases, better than using
the id165 match reranker alone.11 we believe
that adding the context information into the de-
coder does increase the chances of contextually
appropriate outputs appearing on the decoder k-
best lists, but it also introduces a lot more uncer-
tainty and therefore, the appropriate outputs may
not end on top of the list based on decoder scores
alone. the id165 match reranker is then able
to promote the relevant outputs to the top of the
k-best list. however, if the generator itself does
not have access to context information, the id165
match reranker has a smaller effect as contextually
appropriate outputs may not appear on the k-best
lists at all. a closer look at the generated outputs
con   rms that entrainment is present in sentences
generated by the context-aware setups (see fig. 2).
in addition to id7 and nist scores, we mea-
sured the slot error rate err (wen et al., 2015b),
i.e., the proportion of missing or super   uous slot
placeholders in the delexicalized generated out-
puts. for all our setups, err stayed around 3%.

3.2 human evaluation
we evaluated the best-performing setting based
on id7/nist scores, i.e., prepending context
with id165 match reranker, in a blind pairwise
preference test with untrained judges recruited on

9w is set to 5 when the id165 match reranker is run by
itself or combined with the separate encoder, 10 if combined
with prepending context.

10in our experiments on development data, all three meth-

ods brought a mild id7 improvement.

11statistical signi   cance at 99% level has been assessed

using pairwise bootstrap resampling (koehn, 2004).

the crowdflower id104 platform.12 the
judges were given the context and the system out-
put for the baseline and the context-aware system,
and they were asked to pick the variant that sounds
more natural. we used a random sample of 1,000
pairs of different system outputs over all 5 ran-
dom initializations of the networks, and collected
3 judgments for each of them. the judges pre-
ferred the context-aware system output in 52.5%
cases, signi   cantly more than the baseline.13

we examined the judgments in more detail and
found three probable causes for the rather small
difference between the setups. first, both setups   
outputs    t the context relatively well in many cases
and the judges tend to prefer the overall more fre-
quent variant (e.g., for the context    starting from
park place   , the output    where do you want to
go?    is preferred over    where are you going to?   ).
second, the context-aware setup often selects a
shorter response that    ts the context well (e.g.,    is
there an option at 10:00 am?    is con   rmed sim-
ply with    at 10:00 am.   ), but the judges seem
to prefer the more eloquent variant. and third,
both setups occasionally produce non-   uent out-
puts, which introduces a certain amount of noise.

4 related work

our system is an evolutionary improvement over
the lstm id195 system of du  sek and jur  c      cek
(2016b) and as such, it is most related in terms
of architecture to other recent id56-based ap-
proaches to id86, which are not context-aware:
id56 generation with a convolutional reranker by
wen et al. (2015a) and an improved lstm-based
version (wen et al., 2015b), as well as the lstm
encoder-aligner-decoder id86 system of mei et al.
(2015). the recent end-to-end trainable sds of
wen et al. (2016) does have an implicit access to
previous context, but the authors do not focus on
its in   uence on the generated responses.

there have been several attempts at modelling
entrainment in dialogue (brockmann et al., 2005;
reitter et al., 2006; buschmeier et al., 2010) and
even successful implementations of entrainment
models in id86 systems for sds, where entrain-
ment caused an increase in perceived naturalness
of the system responses (hu et al., 2014) or in-
creased naturalness and task success (lopes et al.,

12http://crowdflower.com
13the result is statistically signi   cant at 99% level accord-

ing to the pairwise bootstrap resampling test.

context
input da
baseline
id165 match reranker
prepending context + id165 match reranker
context encoder + id165 match reranker
context
input da
baseline
id165 match reranker
prepending context + id165 match reranker
context encoder + id165 match reranker
context
input da
baseline
id165 match reranker
prepending context + id165 match reranker
context encoder + id165 match reranker

is there a later option
icon   rm(alternative=next)
next connection.
you want a later connection.
you want a later connection.
you want a later option.
i need to    nd a bus connection
inform no match(vehicle=bus)
no bus found, sorry.
i did not    nd a bus route.
i   m sorry, i cannot    nd a bus connection.
i   m sorry, i cannot    nd a bus connection.
i rather take the bus
inform(vehicle=bus, departure time=8:01am, direction=cathedral parkway, from stop=bowling green, line=m15)
at 8:01am by bus line m15 from bowling green to cathedral parkway.
at 8:01am by bus line m15 from bowling green to cathedral parkway.
you can take the m15 bus from bowling green to cathedral parkway at 8:01am.
at 8:01am by bus line m15 from bowling green to cathedral parkway.

table 2: example outputs of the different setups of our generator (with entrainment highlighted)

2013; lopes et al., 2015). however, all of the pre-
vious approaches are completely or partially rule-
based. most of them attempt to model entrainment
explicitly, focus on speci   c entrainment phenom-
ena only, and/or require manually selected lists
of variant expressions, while our system learns
synonyms and entrainment rules implicitly from
the corpus. a direct comparison with previous
entrainment-capable id86 systems for sds is not
possible in our stand-alone setting since their rules
involve the history of the whole dialogue whereas
we focus on the preceding utterance in our experi-
ments.

5 conclusions and further work

we presented an improvement
to our natural
language generator based on the sequence-to-
sequence approach (du  sek and jur  c      cek, 2016b),
allowing it to exploit preceding context user utter-
ances to adapt (entrain) to the user   s way of speak-
ing and provide more contextually accurate and
less repetitive responses. we used two different
ways of feeding previous context into the genera-
tor and a reranker based on id165 match against
the context. evaluation on our context-aware
dataset (du  sek and jur  c      cek, 2016a) showed a sig-
ni   cant id7 score improvement for the com-
bination of the two approaches, which was con-
   rmed in a subsequent human pairwise preference
test. our generator is available on github at the
following url:

https://github.com/ufal-dsg/tgen
in future work, we plan on improving the n-
gram matching metric to allow fuzzy matching
(e.g., capturing different forms of the same word),
experimenting with more ways of incorporating
context into the generator, controlling the output

eloquence and    uency, and most importantly, eval-
uating our generator in a live dialogue system.
we also intend to evaluate the generator with au-
tomatic id103 hypotheses as context
and modify it to allow n-best hypotheses as con-
texts. using our system in a live sds will also
allow a comparison against previous handcrafted
entrainment-capable id86 systems.

acknowledgments

this work was funded by the ministry of edu-
cation, youth and sports of the czech republic
under the grant agreement lk11221 and core re-
search funding, svv project 260 333, and gauk
grant 2058214 of charles university in prague. it
used language resources stored and distributed by
the lindat/clarin project of the ministry of
education, youth and sports of the czech repub-
lic (project lm2015071). the authors would like
to thank ond  rej pl  atek and miroslav vodol  an for
helpful comments.

references
m. abadi, a. agarwal, p. barham, e. brevdo,
z. chen, c. citro, g. s. corrado, a. davis,
j. dean, m. devin, s. ghemawat, i. goodfellow,
a. harp, g. irving, m. isard, y. jia, r. jozefow-
icz, l. kaiser, m. kudlur, j. levenberg, d. man  e,
r. monga, s. moore, d. murray, c. olah, m. schus-
ter, j. shlens, b. steiner, i. sutskever, k. talwar,
p. tucker, v. vanhoucke, v. vasudevan, f. vi  egas,
o. vinyals, p. warden, m. wattenberg, m. wicke,
y. yu, and x. zheng. 2015. tensorflow: large-
scale machine learning on heterogeneous systems.
software available from tensor   ow.org.

d. bahdanau, k. cho, and y. bengio. 2015. neural
machine translation by jointly learning to align and
translate. in international conference on learning
representations. arxiv:1409.0473.

y. bengio, r. ducharme, p. vincent, and c. jauvin.
2003. a neural probabilistic language model. jour-
nal of machine learning research, 3:1137   1155.

c. brockmann, a. isard, j. oberlander, and m. white.
2005. modelling alignment for affective dialogue.
in workshop on adapting the interaction style to af-
fective factors at the 10th international conference
on user modeling.

h. buschmeier, k. bergmann, and s. kopp. 2010.
modelling and evaluation of lexical and syntactic
alignment with a priming-based microplanner.
in
empirical methods in natural language genera-
tion, number 5790 in lecture notes in computer
science, pages 85   104. springer.

g. doddington.

2002.

automatic evaluation
of machine translation quality using id165 co-
in proceedings of the sec-
occurrence statistics.
ond international conference on human language
technology research, pages 138   145.

o. du  sek and f. jur  c      cek. 2016a. a context-aware
id86 dataset for dialogue sys-
in workshop on collecting and generating
tems.
resources for chatbots and conversational agents
- development and evaluation, pages 6   9.

o. du  sek and f. jur  c      cek.

sequence-to-
sequence generation for spoken dialogue via deep
syntax trees and strings. arxiv:1606.05491. to ap-
pear in proceedings of acl.

2016b.

h. friedberg, d. litman, and s. b. f. paletz. 2012.
lexical entrainment and success in student engineer-
ing groups. in proc. of slt, pages 404   409.

a. graves. 2013. generating sequences with recurrent

neural networks. arxiv:1308.0850.

s. hochreiter and j. schmidhuber. 1997. long short-
term memory. neural computation, 9(8):1735   
1780.

z. hu, g. halberg, c. jimenez, and m. walker. 2014.
entrainment in pedestrian direction giving: how
in proc. of iwsds,
many kinds of entrainment.
pages 90   101.

f. jur  c      cek, o. du  sek, o. pl  atek, and l.   zilka. 2014.
alex: a statistical dialogue systems framework. in
proc. of text, speech and dialogue, pages 587   594.

d. kingma and j. ba.

adam: a
in interna-
method for stochastic optimization.
tional conference on learning representations.
arxiv:1412.6980.

2015.

p. koehn.

2004.

machine translation evaluation.
emnlp, pages 388   395.

statistical signi   cance tests for
in proceedings of

j. lopes, m. eskenazi, and i. trancoso. 2013. auto-
mated two-way entrainment to improve spoken dia-
log system performance. in proc. of icassp, pages
8372   8376.

j. lopes, m. eskenazi, and i. trancoso. 2015. from
rule-based to data-driven lexical entrainment mod-
els in spoken id71. computer speech &
language, 31(1):87   112.

h. mei, m. bansal, and m. r. walter.

2015.
what to talk about and how?
selective genera-
tion using lstms with coarse-to-   ne alignment.
arxiv:1509.00838.

a. nenkova, a. gravano, and j. hirschberg. 2008.
high frequency word entrainment in spoken dia-
logue. in proc. of acl-hlt, pages 169   172.

k. papineni, s. roukos, t. ward, and w.-j. zhu. 2002.
id7: a method for automatic evaluation of ma-
chine translation. in proc. of acl, pages 311   318.

g. parent and m. eskenazi. 2010. lexical entrainment
of real users in the let   s go spoken dialog system.
in proc. of interspeech, pages 3018   3021.

d. reitter, f. keller, and j. d. moore. 2006. compu-
tational modelling of structural priming in dialogue.
in proc. of naacl-hlt: short papers, pages 121   
124.

s. stoyanchev and a. stent. 2009. lexical and syntac-
tic priming and their impact in deployed spoken di-
alog systems. in proc. of naacl-hlt, pages 189   
192.

i. sutskever, o. vinyals, and q. vv le. 2014. se-
quence to sequence learning with neural networks.
in advances in neural information processing sys-
tems, pages 3104   3112. arxiv:1409.3215.

t.-h. wen, m. gasic, d. kim, n. mrksic, p.-h. su,
d. vandyke, and s. young. 2015a. stochastic lan-
guage generation in dialogue using recurrent neural
networks with convolutional sentence reranking. in
proc. of sigdial, pages 275   284.

t.-h. wen, m. gasic, n. mrk  si  c, p.-h. su, d. vandyke,
and s. young. 2015b. semantically conditioned
lstm-based id86 for spo-
in proc. of emnlp, pages
ken dialogue systems.
1711   1721.

t.-h. wen, m. ga  si  c, n. mrk  si  c, l. m. rojas-
barahona, p.-h. su, s. ultes, d. vandyke, and
s. young.
a network-based end-
to-end trainable task-oriented dialogue system.
arxiv:1604.04562.

2016.

s. young, m. ga  si  c, s. keizer, f. mairesse, j. schatz-
mann, b. thomson, and k. yu. 2010. the hid-
den information state model: a practical frame-
work for pomdp-based spoken dialogue manage-
ment. computer speech & language, 24(2):150   
174.

