toward controlled generation of text

zhiting hu 1 2 zichao yang 1 xiaodan liang 1 2 ruslan salakhutdinov 1 eric p. xing 1 2

8
1
0
2

 

p
e
s
3
1

 

 
 
]

g
l
.
s
c
[
 
 

4
v
5
5
9
0
0

.

3
0
7
1
:
v
i
x
r
a

abstract

generic generation and manipulation of text is
challenging and has limited success compared
to recent deep generative modeling in visual do-
main. this paper aims at generating plausible
text sentences, whose attributes are controlled by
learning disentangled latent representations with
designated semantics. we propose a new neu-
ral generative model which combines variational
auto-encoders (vaes) and holistic attribute dis-
criminators for effective imposition of semantic
structures. the model can alternatively be seen
as enhancing vaes with the wake-sleep algo-
rithm for leveraging fake samples as extra train-
ing data. with differentiable approximation to
discrete text samples, explicit constraints on in-
dependent attribute controls, and ef   cient col-
laborative learning of generator and discrimina-
tors, our model learns interpretable representa-
tions from even only word annotations, and pro-
duces sentences with desired attributes of senti-
ment and tenses. quantitative experiments using
trained classi   ers as evaluators validate the accu-
racy of short sentence and attribute generation.

1. introduction
there is a surge of research interest in deep generative
models (hu et al., 2017), such as id5
(vaes) (kingma & welling, 2013), generative adver-
sarial nets (gans) (goodfellow et al., 2014), and auto-
regressive models (van den oord et al., 2016). despite their
impressive advances in visual domain, such as image gen-
eration (radford et al., 2015), learning interpretable image
representations (chen et al., 2016), and image editing (zhu
et al., 2016), applications to id86
have been relatively less studied. even generating realis-
tic sentences is challenging as the generative models are

1carnegie mellon university 2petuum, inc.. correspondence

to: zhiting hu <zhitingh@cs.cmu.edu>.

proceedings of the 34 th international conference on machine
learning, sydney, australia, pmlr 70, 2017. copyright 2017
by the author(s).

required to capture complex semantic structures underly-
ing sentences. previous work have been mostly limited
to task-speci   c applications in supervised settings, includ-
ing machine translation (bahdanau et al., 2014) and image
captioning (vinyals et al., 2015). however, autoencoder
frameworks (sutskever et al., 2014) and recurrent neural
network language models (mikolov et al., 2010) do not ap-
ply to generic text generation from arbitrary hidden rep-
resentations due to the unsmoothness of effective hidden
codes (bowman et al., 2015). very few recent attempts of
using vaes (bowman et al., 2015; tang et al., 2016) and
gans (yu et al., 2017; zhang et al., 2016) have been made
to investigate generic text generation, while their generated
text is largely randomized and uncontrollable.
in this paper we tackle the problem of controlled generation
of text. that is, we focus on generating realistic sentences,
whose attributes can be controlled by learning disentangled
latent representations. to enable the manipulation of gen-
erated sentences, a few challenges need to be addressed.
a    rst challenge comes from the discrete nature of text
samples. the resulting non-differentiability hinders the use
of global discriminators that assess generated samples and
back-propagate gradients to guide the optimization of gen-
erators in a holistic manner, as shown to be highly effective
in continuous image generation and representation model-
ing (chen et al., 2016; larsen et al., 2016; dosovitskiy &
brox, 2016). a number of recent approaches attempt to ad-
dress the non-differentiability through policy learning (yu
et al., 2017) which tends to suffer from high variance dur-
ing training, or continuous approximations (zhang et al.,
2016; kusner & heid56dez-lobato, 2016) where only pre-
liminary qualitative results are presented. as an alterna-
tive to the discriminator based learning, semi-supervised
vaes (kingma et al., 2014) minimize element-wise recon-
struction error on observed examples and are applicable to
discrete visibles. this, however, loses the holistic view of
full sentences and can be inferior especially for modeling
global abstract attributes (e.g., sentiment).
another challenge for controllable generation relates to
learning disentangled latent representations. interpretabil-
ity expects each part of the latent representation to govern
and only focus on one aspect of the samples. prior meth-
ods (chen et al., 2016; odena et al., 2016) on structured
representation learning lack explicit enforcement of the in-

toward controlled generation of text

dependence property on the full latent representation, and
varying individual code may result in unexpected variation
of other unspeci   ed attributes besides the desired one.
in this paper, we propose a new text generative model
that addresses the above issues, permitting highly disen-
tangled representations with designated semantic structure,
and generating sentences with dynamically speci   ed at-
tributes. we base our generator on vaes in combination
with holistic discriminators of attributes for effective im-
position of structures on the latent code. end-to-end opti-
mization is enabled with differentiable softmax approxima-
tion which anneals smoothly to discrete case and helps fast
convergence. the probabilistic encoder of vae also func-
tions as an additional discriminator to capture variations
of implicitly modeled aspects, and guide the generator to
avoid entanglement during attribute code manipulation.
our model can be interpreted as enhancing vaes with
an extended wake-sleep procedure (hinton et al., 1995),
where the sleep phase enables incorporation of generated
samples for learning both the generator and discriminators
in an alternating manner. the generator and the discrim-
inators effectively provide feedback signals to each other,
resulting in an ef   cient mutual id64 framework.
we show a little supervision (e.g., 100s of annotated sen-
tences) is suf   cient to learn structured representations.
besides ef   cient representation learning and enabled semi-
supervised training, another advantage of using discrimi-
nators as learning signals for the generator, as compared
to conventional conditional reconstruction based meth-
ods (wen et al., 2015; kingma et al., 2014), is that dis-
criminators of different attributes can be trained indepen-
dently. that is, for each attribute one can use separate
labeled data for training the respective discriminator, and
the trained discriminators can be combined arbitrarily to
control a set of attributes of interest.
in contrast, recon-
struction based approaches typically require every instance
of the training data to be labeled exhaustively with all tar-
get attributes (wen et al., 2015), or to marginalize out any
missing attributes (kingma et al., 2014) which can be com-
putationally expensive.
as a showing case, we apply our model to generate sen-
tences with controlled sentiment and tenses. though to
our best knowledge there is no text corpus with both senti-
ment and tense labels, our method enables to use separate
datasets, one with annotated sentiment and the other with
tense labels. quantitative experiments demonstrate the ef-
   cacy of our method. our model improves over previous
generative models on the accuracy of generating speci   ed
attributes as well as performing classi   cation using gen-
erated samples. we show our method learns highly dis-
entangled representations from only word-level labels, and
produces plausible short sentences.

2. related work
remarkable progress has been made in deep generative
modeling. hu et al. (2017) provide a uni   ed view of a
diverse set of deep generative methods. variational au-
toencoders (vaes) (kingma & welling, 2013) consist of
encoder and generator networks which encode a data exam-
ple to a latent representation and generate samples from the
latent space, respectively. the model is trained by maxi-
mizing a variational lower bound on the data log-likelihood
under the generative model. a kl divergence loss is mini-
mized to match the posterior of the latent code with a prior,
which enables every latent code from the prior to decode
into a plausible sentence. without the kl id173,
vaes degenerate to autoencoders and become inapplicable
for the generic generation. the vanilla vaes are incom-
patible with discrete latents as they hinder differentiable
parameterization for learning the encoder. wake-sleep al-
gorithm (hinton et al., 1995) introduced for learning deep
directed id114 shares similarity with vaes by
also combining an id136 network with the generator.
the wake phase updates the generator with samples gener-
ated from the id136 network on training data, while the
sleep phase updates the id136 network based on sam-
ples from the generator. our method combines vaes with
an extended wake-sleep in which the sleep procedure up-
dates both the generator and id136 network (discrimi-
nators), enabling collaborative semi-supervised learning.
besides reconstruction in raw data space, discriminator-
based metric provides a different way for generator learn-
ing, i.e., the discriminator assesses generated samples and
feedbacks learning signals. for instance, gans (good-
fellow et al., 2014) use a discriminator to feedback the
id203 of a sample being recognized as a real exam-
ple. larsen et al. (2016) combine vaes with gans for
enhanced image generation. dosovitskiy & brox (2016);
taigman et al. (2017) use discriminators to measure high-
level perceptual similarity. applying discriminators to text
generation is hard due to the non-differentiability of dis-
crete samples (yu et al., 2017; zhang et al., 2016; kusner
& heid56dez-lobato, 2016). bowman et al. (2015); tang
et al. (2016); yang et al. (2017) instead use vaes without
discriminators. all these text generation methods do not
learn disentangled latent representations, resulting in ran-
domized and uncontrollable samples.
in contrast, disen-
tangled generation in visual domain has made impressive
progress. e.g., infogan (chen et al., 2016), which resem-
bles the extended sleep procedure of our joint vae/wake-
sleep algorithm, disentangles latent representation in an un-
supervised manner. the semantic of each dimension is
observed after training rather than designated by users in
a controlled way. siddharth et al. (2017); kingma et al.
(2014) base on vaes and obtain disentangled image rep-
resentations with semi-supervised learning. zhou & neu-

toward controlled generation of text

big (2017) extend semi-supervised vaes for text transduc-
tion. in contrast, our model combines vaes with discrim-
inators which provide a better, holistic metric compared to
element-wise reconstruction. moreover, most of these ap-
proaches have only focused on the disentanglement of the
structured part of latent representations, while ignoring po-
tential dependence of the structured code with attributes not
explicitly encoded. we address this by introducing an in-
dependency constraint, and show its effectiveness for im-
proved interpretability.

3. controlled generation of text
our model aims to generate plausible sentences condi-
tioned on representation vectors which are endowed with
designated semantic structures. for instance, to control
sentence sentiment, our model allocates one dimension of
the latent representation to encode    positive    and    nega-
tive    semantics, and generates samples with desired sen-
timent by simply specifying a particular code. bene   ting
from the disentangled structure, each such code is able to
capture a salient attribute and is independent with other fea-
tures. our deep text generative model possesses several
merits compared to prior work, as it 1) facilitates effective
imposition of latent code semantics by enabling global dis-
criminators to guide the discrete text generator learning;
2) improves model interpretability by explicitly enforcing
the constraints on independent attribute controls; 3) per-
mits ef   cient semi-supervised learning and id64
by synthesizing variational auto-encoders with a tailored
wake-sleep approach. we    rst present the overview of our
framework (  3.1), then describe the model in detail (  3.2).

3.1. model overview

we build our framework starting from variational auto-
encoders (  2) which have been used for text genera-
tion (bowman et al., 2015), where sentence   x is generated
conditioned on latent code z. the vanilla vae employs an
unstructured vector z in which the dimensions are entan-
gled. to model and control the attributes of interest in an
interpretable way, we augment the unstructured variables z
with a set of structured variables c each of which targets a
salient and independent semantic feature of sentences.
we want our sentence generator to condition on the com-
bined vector (z, c), and generate samples that ful   ll the
attributes as speci   ed in the structured code c. conditional
generation in the context of vaes (e.g., semi-supervised
vaes (kingma et al., 2014)) is often learned by recon-
structing observed examples given their feature code. how-
ever, as demonstrated in visual domain, compared to com-
puting element-wise distances in the data space, computing
distances in the feature space allows invariance to distract-
ing transformations and provides a better, holistic metric.

figure 1. the generative model, where z is unstructured latent
code and c is structured code targeting sentence attributes to con-
trol. blue dashed arrows denote the proposed independency con-
straint (section 3.2 for details), and red arrows denote gradient
propagation enabled by the differentiable approximation.

thus, for each attribute code in c, we set up an individ-
ual discriminator to measure how well the generated sam-
ples match the desired attributes, and drive the generator to
produce improved results. the dif   culty of applying dis-
criminators in our context is that text samples are discrete
and non-differentiable, which breaks down gradient prop-
agation from the discriminators to the generator. we use
a continuous approximation based on softmax with a de-
creasing temperature, which anneals to the discrete case as
training proceeds. this simple yet effective approach en-
joys low variance and fast convergence.
intuitively, having an interpretable representation would
imply that each structured code in c can independently
control its target feature, without entangling with other at-
tributes, especially those not explicitly modeled. we en-
courage the independency by enforcing those irrelevant at-
tributes to be completely captured in the unstructured code
z and thus be separated from c that we will manipulate.
to this end, we reuse the vae encoder as an additional
discriminator for recognizing the attributes modeled in z,
and train the generator so that these unstructured attributes
can be recovered from the generated samples. as a result,
varying different attribute codes will keep the unstructured
attributes invariant as long as z is unchanged.
figure 1 shows the overall model structure. our complete
model incorporates vaes and attribute discriminators, in
which the vae component trains the generator to recon-
struct real sentences for generating plausible text, while the
discriminators enforce the generator to produce attributes
coherent with the conditioned code. the attribute discrim-
inators are learned to    t labeled examples to entail desig-
nated semantics, as well as trained to explain samples from
the generator. that is, the generator and the discrimina-
tors form a pair of collaborative learners and provide feed-
back signals to each other. the collaborative optimization
resembles wake-sleep algorithm. we show the combined
vae/wake-sleep learning enables a highly ef   cient semi-
supervised framework, which requires only a little supervi-
sion to obtain interpretable representation and generation.

        generatordiscriminators    $encoder    toward controlled generation of text

3.2. model structure

we now describe our model in detail, by presenting the
learning of generator and discriminators, respectively.

generator learning
the generator g is an lstm-id56 for generating token
sequence   x = {  x1, . . . ,   xt} conditioned on the latent code
(z, c), which depicts a generative distribution:

  x     g(z, c) = pg(   x|z, c)

(cid:89)

t

=

p(  xt|   x<t, z, c),

(1)

where   x<t indicates the tokens preceding   xt. the gener-
ation thus involves a sequence of discrete decision mak-
ing which samples a token from a multinomial distribution
parametrized using softmax function at each time step t:

  xt     softmax(ot/   ),

(2)

where ot is the logit vector as the inputs to the softmax
function, and    > 0 is the temperature normally set to 1.
the unstructured part z of the representation is modeled
as continuous variables with standard gaussian prior p(z),
while the structured code c can contain both continu-
ous and discrete variables to encode different attributes
(e.g., sentiment categories, formality) with appropriate
prior p(c). given observation x, the base vae includes
a conditional probabilistic encoder e to infer the latents z:

z     e(x) = qe(z|x).

(3)

let   g and   e denote the parameters of the generator g
and the encoder e, respectively. the vae is then opti-
mized to minimize the reconstruction error of observed real
sentences, and at the same time regularize the encoder to be
close to the prior p(z):
lvae(  g,   e; x) = kl(qe(z|x)(cid:107)p(z))

    eqe (z|x)qd (c|x) [log pg(x|z, c)] ,

(4)

where kl(  (cid:107)  ) is the kl-divergence; and qd(c|x) is the
conditional distribution de   ned by the discriminator d for
each structured variable in c:

d(x) = qd(c|x).

(5)

here, for notational simplicity, we assume only one struc-
tured variable and thus one discriminator,
though our
model speci   cation can straightforwardly be applied to
many attributes. the distribution over (z, c) factors into
qe and qd as we are learning disentangled representa-
tions. note that here the discriminator d and code c are
not learned with the vae loss, but instead optimized with

the objectives described shortly. besides the reconstruc-
tion loss which drives the generator to produce realistic
sentences, the discriminator provides extra learning signals
which enforce the generator to produce coherent attribute
that matches the structured code in c. however, as it is
impossible to propagate gradients from the discriminator
through the discrete samples, we resort to a deterministic
continuous approximation. the approximation replaces the
sampled token   xt (represented as a one-hot vector) at each
step with the id203 vector in eq.(2) which is differ-
entiable w.r.t the generator   s parameters. the id203
vector is used as the output at the current step and the input
to the next step along the sequence of decision making. the

resulting    soft    generated sentence, denoted as (cid:101)g   (z, c), is

fed into the discriminator1 to measure the    tness to the tar-
get attribute, leading to the following loss for improving g:

(cid:104)

log qd(c|(cid:101)g   (z, c))

(cid:105)

.

lattr,c(  g) =    ep(z)p(c)

(6)
the temperature    (eq.2) is set to        0 as training pro-
ceeds, yielding increasingly peaked distributions that    -
nally emulate discrete case. the simple deterministic ap-
proximation effectively leads to reduced variance and fast
convergence during training, which enables ef   cient learn-
ing of the conditional generator. the diversity of genera-
tion results is guaranteed since we use the approximation
only for attribute modeling and the base sentence genera-
tion is learned through vaes.
with the objective in eq.(6), each structured attribute of
generated sentences is controlled through the correspond-
ing code in c and is independent with other variables in the
latent representation. however, it is still possible that other
attributes not explicitly modeled may also entangle with the
code in c, and thus varying a dimension of c can yield unex-
pected variation of these attributes we are not interested in.
to address this, we introduce the independency constraint
which separates these attributes with c by enforcing them
to be fully captured by the unstructured part z. therefore,
besides the attributes explicitly encoded in c, we also train
the generator so that other non-explicit attributes can be
correctly recognized from the generated samples and match
the unstructured code z. instead of building a new discrim-
inator, we reuse the variational encoder e which serves
precisely to infer the latents z in the base vae. the loss
is in the same form as with eq.(6) except replacing the dis-
criminator conditional qd with the encoder conditional qe:

lattr,z(  g) =    ep(z)p(c)

(cid:104)
(cid:105)
log qe(z|(cid:101)g   (z, c))

.

(7)

note that, as the discriminator in eq.(6), the encoder now

1the id203 vector thus functions to average over the
id27 matrix to obtain a    soft    id27 at
each step.

toward controlled generation of text

performs id136 over generated samples from the prior,
as opposed to observed examples as in vaes.
combining eqs.(4)-(7) we obtain the generator objective:

min  g lg = lvae +   clattr,c +   zlattr,z,

(8)

where   c and   z are balancing parameters. the varia-
tional encoder is trained by minimizing the vae loss, i.e.,
min  e lvae.

discriminator learning
the discriminator d is trained to accurately infer the sen-
tence attribute and evaluate the error of recovering the de-
sired feature as speci   ed in the latent code. for instance,
for categorical attribute, the discriminator can be formu-
lated as a sentence classi   er; while for continuous target
a probabilistic regressor can be used. the discriminator
is learned in a different way compared to the vae encoder,
since the target attributes can be discrete which are not sup-
ported in the vae framework. moreover, in contrast to the
unstructured code z which is learned in an unsupervised
manner, the structured variable c uses labeled examples to
entail designated semantics. we derive an ef   cient semi-
supervised learning method for the discriminator.
formally, let   d denote the parameters of the discrimina-
tor. to learn speci   ed semantic meaning, we use a set of
labeled examples xl = {(xl, cl)} to train the discrimi-
nator d with the following objective:

ls(  d) =    exl [log qd(cl|xl)] .

(9)

besides, the conditional generator g is also capable of syn-
thesizing (noisy) sentence-attribute pairs (   x, c) which can
be used to augment training data for semi-supervised learn-
ing. to alleviate the issue of noisy data and ensure ro-
bustness of model optimization, we incorporate a minimum
id178 id173 term (grandvalet et al., 2004; reed
et al., 2014). the resulting objective is thus:
lu(  d) =    epg(   x|z,c)p(z)p(c)

(cid:2) log qd(c|   x) +   h(qd(c

(cid:48)|   x))(cid:3),

(10)
where h(qd(c(cid:48)|   x)) is the empirical shannon id178 of
distribution qd evaluated on the generated sentence   x; and
   is the balancing parameter.
intuitively, the minimum
id178 id173 encourages the model to have high
con   dence in predicting labels.
the joint training objective of the discriminator using both
labeled examples and synthesized samples is then given as:

min  d ld = ls +   ulu,

(11)

where   u is the balancing parameter.

algorithm 1 controlled generation of text
input: a large corpus of unlabeled sentences x = {x}
a few sentence attribute labels xl = {(xl, cl)}
parameters:   c,   z,   u,        balancing parameters

1: initialize the base vae by minimizing eq.(4) on x with c

sampled from prior p(c)

2: repeat
3:
4:

train the discriminator d by eq.(11)
train the generator g and the encoder e by eq.(8) and
minimizing eq.(4), respectively.

5: until convergence
output: sentence generator g conditioned on disentangled rep-

resentation (z, c)

figure 2. left: the vae and wake procedure, corresponding to
eq.(4). right: the sleep procedure, corresponding to eqs.(6)-
(7) and (10). black arrows denote id136 and generation; red
dashed arrows denote gradient propagation. the two steps in the
sleep procedure, i.e., optimizing the discriminator and the gener-
ator, respectively, are performed in an alternating manner.

summarization and discussion
we have derived our model and its learning procedure. the
generator is    rst initialized by training the base vae on a
large corpus of unlabeled sentences, through the objective
of minimizing eq.(4) with the latent code c at this time
sampled from the prior distribution p(c). the full model is
then trained by alternating the optimization of the generator
and the discriminator, as summarized in algorithm 1.
our model can be viewed as combining the vae frame-
work with an extended wake-sleep method, as illustrated in
figure 2. speci   cally, in eq.(10), samples are produced
by the generator and used as targets for maximum like-
lihood training of the discriminator. this resembles the
sleep phase of wake-sleep. eqs.(6)-(7) further leverage the
generated samples to improve the generator. we can see
the above together as an extended sleep procedure based
on    dream    samples obtained by ancestral sampling from
the generative network. on the other hand, eq.(4) samples
c from the discriminator distribution qd(c|x) on observa-
tion x, to form a target for training the generator, which
corresponds to the wake phase. the effective combination
enables discrete latent code, holistic discriminator metrics,
and ef   cient mutual id64.
training of the discriminators need supervised data to im-
pose designated semantics. discriminators for different at-
tributes can be trained independently on separate labeled
sets. that is, the model does not require a sentence to be

	    #       (    ,    )    			                   -(    |    ,    )    2(    |    )    3(    |    )    			        -(    |    ,    )    3(    |    )    (    )    (    )toward controlled generation of text

annotated with all attributes, but instead needs only inde-
pendent labeled data for each individual attribute. more-
over, as the labeled data are used only for learning attribute
semantics instead of direct sentence generation, we are al-
lowed to extend the data scope beyond labeled sentences
to, e.g., labeled words or phrases. as shown in the experi-
ments (section 4), our method is able to effectively lift the
word level knowledge to sentence level and generate con-
vincing sentences. finally, with the augmented unsuper-
vised training in the sleep phrase, we show a little supervi-
sion is suf   cient for learning structured representations.

4. experiments
we apply our model to generate short sentences (length    
15) with controlled sentiment and tense. quantitative ex-
periments using trained classi   ers as evaluators show our
model gives improved generation accuracy. disentangled
representation is learned with a few labels or only word
annotations. we also validate the effect of the proposed
independency constraint for interpretable generation.

datasets
sentence corpus. we use a large imdb text corpus (diao
et al., 2014) for training the generative models. this is
a collection of 350k movie reviews. we select sentences
containing at most 15 words, and replace infrequent words
with the token    <unk>   . the resulting dataset contains
around 1.4m sentences with the vocabulary size of 16k.
sentiment. to control the sentiment (   positive    or    neg-
ative   ) of generated sentences, we test on the following la-
beled sentiment data: (1) stanford sentiment treebank-2
(sst-full) (socher et al., 2013) consists of 6920/872/1821
movie review sentences with binary sentiment annotations
in the train/dev/test sets, respectively. we use the 2837
training examples with sentence length     15, and evalu-
ate classi   cation accuracy on the original test set. (2) sst-
small. to study the size of labeled data required in the
semi-supervised learning for accurate attribute control, we
sample a small subset from sst-full, containing only 250
labeled sentences for training. (3) lexicon. we also in-
vestigate the effectiveness of our model in terms of using
word-level labels for sentence-level control. the lexicon
from (wilson et al., 2005) contains 2700 words with senti-
ment labels. we use the lexicon for training by treating the
words as sentences, and evaluate on the sst-full test set.
(4) imdb. we collect a dataset from the imdb corpus
by randomly selecting positive and negative movie reviews.
the dataset has 5k/1k/10k sentences in train/dev/test.
tense. the second attribute is the tense of the main verb
in a sentence. though no corpus with sentence tense an-
notations is readily available, our method is able to learn
from only labeled words and generate desired sentences.

model

dataset
sst-small
0.679
0.707

s-vae
ours

sst-full
0.822
0.851

lexicon
0.660
0.701
table 1. sentiment accuracy of generated sentences.
s-vae
(kingma et al., 2014) and our model are trained on the three sen-
timent datasets and generate 30k sentences, respectively.

we compile from the timebank (timeml.org) dataset and
obtain a lexicon of 5250 words and phrases labeled with
one of {   past   ,    present   ,    future   }. the lexicon mainly
consists of verbs in different tenses (e.g.,    was   ,    will be   )
as well as time expressions (e.g.,    in the future   ).
note that our method requires only separate labeled copora
for each attribute. and for the tense attribute only anno-
tated words/phrases are used.

parameter setting
the generator and encoder are set as single-layer lstm
id56s with input/hidden dimension of 300 and max sample
length of 15. discriminators are set as convnets. detailed
con   gurations are in the supplements. to avoid vanishingly
small kl term in the vae module (eq.4) (bowman et al.,
2015), we use a kl term weight linearly annealing from 0
to 1 during training. balancing parameters are set to   c =
  z =   u = 0.1, and    is selected on the dev sets. at test
time sentences are generated with eq.(1).

4.1. accuracy of generated attributes

we quantitatively measure sentence attribute control by
evaluating the accuracy of generating designated sentiment,
and the effect of using samples for training classi   ers.
we compare with semi-supervised vae (s-vae) (kingma
et al., 2014), one of the few existing deep models capable
of conditional text generation. s-vae learns to reconstruct
observed sentences given attribute code, and no discrimi-
nators are used. see   2 and 3.1 for more discussions.
we use a state-of-the-art sentiment classi   er (hu et al.,
2016a) which achieves 90% accuracy on the sst test set, to
automatically evaluate the sentiment generation accuracy.
speci   cally, we generate sentences given sentiment code c,
and use the pre-trained sentiment classi   er to assign senti-
ment labels to the generated sentences. the accuracy is
calculated as the percentage of the predictions that match
the sentiment code c. table 1 shows the results on 30k
sentences by the two models which are trained with sst-
full, sst-small, and lexicon, respectively. we see that our
method consistently outperforms s-vae on all datasets. in
particular, trained with only 250 labeled examples in sst-
small, our model achieves reasonable generation accuracy,
demonstrating the ability of learning disentangled repre-

toward controlled generation of text

table 2 compares the samples generated by models with
and without the constraint term, respectively.
in the left
column where the constraint applies, each pair of sen-
tences, conditioned on different sentiment codes, are highly
relevant in terms of, e.g., subject, tone, and wording which
are not explicitly modeled in the structured code c while in-
stead implicitly encoded in the unstructured code z. vary-
ing the sentiment code precisely changes the sentiment of
the sentences (and paraphrases slightly to ensure    uency),
while keeping other aspects unchanged.
in contrast, the
results in the right column, where the independency con-
straint is unactivated, show that varying the sentiment code
not only changes the polarity of samples, but can also
change other aspects unexpected to control, making the
generation results less interpretable and predictable.
we demonstrate the power of learned disentangled repre-
sentation by varying one attribute variable at a time. table 3
shows the generation results. we see that each attribute
variable in our model successfully controls its correspond-
ing attribute, and is disentangled with other attribute code.
the right column of the table shows meaningful variation
of sentence tense as the tense code varies. note that the
semantic of tense is learned only from a lexicon without
complete sentence examples. our model successfully cap-
tures the key ingredients (e.g., verb    was    for past tense and
   will be    for future tense) and combines with the knowl-
edge of well-formed sentences to generate realistic samples
with speci   ed tense attributes. table 4 further shows gen-
erated sentences with varying code z in different settings
of structured attribute factors. we obtain samples that are
diverse in content while consistent in sentiment and tense.
we also occasionally observed failure cases as in table 5,
such as implausible sentences, unexpected variations of
irrelevant attributes, and inaccurate attribute generations.
improved modeling is expected such as using dilated con-
volutions as decoder, and decoding with id125, etc.
better systematic quantitative evaluations are also desired.

5. discussions
we have proposed a deep generative model that learns in-
terpretable latent representations and generates sentences
with speci   ed attributes. we obtained meaningful genera-
tion with restricted sentence length, and improved accuracy
on sentiment and tense attributes. in the future we would
like to improve the modeling and training as above, and
extend to generate longer sentences/paragraphs and control
more attributes with    ne-grained structures.
our approach combines vaes with attribute discrim-
inators and imposes explicit
independency constraints
on attribute controls, enabling disentangled latent code.
semi-supervised learning within the joint vae/wake-sleep

figure 3. test-set accuracy of classi   ers trained on four sentiment
datasets augmented with different methods (see text for details).
the    rst three datasets use the sst-full test set for evaluation.

sentations with very little supervision. more importantly,
given only word-level annotations in lexicon, our model
successfully transfers the knowledge to sentence level and
generates desired sentiments reasonably well. compared to
our method that drives learning by directly assessing gen-
erated sentences, s-vae attempts to capture sentiment se-
mantics only by reconstructing labeled words, which is less
ef   cient and gives inferior performance.
we next use the generated samples to augment the sen-
timent datasets and train sentiment classi   ers. while
not aiming to build best-performing classi   ers on these
datasets, the classi   cation accuracy serves as an auxiliary
measure of the sentence generation quality. that is, higher-
quality sentences with more accurate sentiment attribute
can predictably help yield stronger sentiment classi   ers.
figure 3 shows the accuracy of classi   ers trained on the
four datasets with different augmentations.    std    is a con-
vnet trained on the standard original datasets, with the
same network structure as with the sentiment discriminator
in our model.    h-reg    additionally imposes the minimum
id178 id173 on the generated sentences.    ours   
incorporates the minimum id178 id173 and the
sentiment attribute code c of the generated sentences, as
in eq.(10). s-vae uses the same protocol as our method
to augment with the data generated by the s-vae model.
comparison in figure 3 shows that our method consistently
gives the best performance on four datasets. for instance,
on lexicon, our approach achieves 0.733 accuracy, com-
pared to 0.701 of    std   . the improvement of    h-reg   
over    std    shows positive effect of the minimum id178
id173 on generated sentences. further incorporat-
ing the conditioned sentiment code of the generated sam-
ples, as in    ours    and    s-vae   , provides additional perfor-
mance gains, indicating the advantages of conditional gen-
eration for automatic creation of labeled data. consistent
with the above experiment, our model outperforms s-vae.

4.2. disentangled representation

we study the interpretability of generation and the explicit
independency constraint (eq.7) for disentangled control.

sst-fullsst-smalllexiconimdb0.600.650.700.750.800.85accuracystdh-regourss-vaetoward controlled generation of text

w/ independency constraint
the    lm is strictly routine !
the    lm is full of imagination .

w/o independency constraint
the acting is bad .
the movie is so much fun .

after watching this movie , i felt that disappointed .
after seeing this    lm , i    m a fan .

none of this is very original .
highly recommended viewing for its courage , and ideas .

the acting is uniformly bad either .
the performances are uniformly good .

too bland
highly watchable

this is just awful .
this is pure genius .

i can analyze this movie without more than three words .
i highly recommend this    lm to anyone who appreciates music .

table 2. samples from models with or without independency constraint on attribute control (i.e., eq.7). each pair of sentences are
generated with sentiment code set to    negative    and    positive   , respectively, while    xing the unstructured code z. the sst-full dataset
is used for learning the sentiment representation.

varying the code of tense
i thought the movie was too bland and too much
i guess the movie is too bland and too much
i guess the    lm will have been too bland

this was one of the outstanding thrillers of the last decade
this is one of the outstanding thrillers of the all time
this will be one of the great thrillers of the all time

table 3. each triple of sentences is generated by varying the tense code while    xing the sentiment code and z.

varying the unstructured code z
(   negative   ,    past   )
the acting was also kind of hit or miss .
i wish i    d never seen it
by the end i was so lost i just did n   t care anymore

(   positive   ,    past   )
his acting was impeccable
this was spectacular , i saw it in theaters twice
it was a lot of fun

(   negative   ,    present   )
the movie is very close to the show in plot and characters
the era seems impossibly distant
i think by the end of the    lm , it has confused itself

(   positive   ,    present   )
this is one of the better dance    lms
i    ve always been a big fan of the smart dialogue .
i recommend you go see this, especially if you hurt

(   negative   ,    future   )
i wo n   t watch the movie
and that would be devastating !
i wo n   t get into the story because there really is n   t one

(   positive   ,    future   )
i hope he    ll make more movies in the future
i will de   nitely be buying this on dvd
you will be thinking about it afterwards, i promise you

table 4. samples by varying the unstructured code z given sentiment (   positive   /   negative   ) and tense (   past   /   present   /   future   ) code.

failure cases
the plot is not so original
the plot weaves us into <unk>

he is a horrible actor    s most part
he    s a better actor than a standup

it does n   t get any better the other dance movies
it does n   t reach them , but the stories look

i just think so
i just think !

table 5. failure cases when varying sentiment code with other codes    xed.

toward controlled generation of text

framework is effective with little or incomplete supervi-
sion. hu et al. (2017) develop a uni   ed view of a diverse
set of deep generative paradigms, including gans, vaes,
and wake-sleep algorithm. our model can be alternatively
motivated under the view as enhancing vaes with the ex-
tended sleep phase and by leveraging generated samples.
interpretability of the latent representations not only allows
dynamic control of generated attributes, but also provides
an interface that connects the end-to-end neural model with
conventional structured methods. for instance, we can en-
code structured constraints (e.g., logic rules or probabilistic
structured models) on the interpretable latent code, to in-
corporate prior knowledge or human intentions (hu et al.,
2016a;b); or plug the disentangled generation model into
id71 to generate natural language responses from
structured dialog states (young et al., 2013).
though we have focused on the generation capacity of our
model, the proposed collaborative semi-supervised learn-
ing framework also helps improve the discriminators by
generating labeled samples for data augmentation (e.g., see
figure 3). more generally, for any discriminative task, we
can build a conditional generative model to synthesize ad-
ditional labeled data. the accurate attribute generation of
our approach can offer larger performance gains compared
to previous generative methods.

implementation we have released code for an adapted
version of the proposed algorithm at:
https://github.com/asyml/texar/tree/master/
examples/text_style_transfer.
the implementation is based on texar (hu et al., 2018), a general-
purpose text generation toolkit.

acknowledgments this research is supported by nsf
iis1447676, onr n000141410684, and onr n000141712463.

references
bahdanau, dzmitry, cho, kyunghyun, and bengio,
yoshua. id4 by jointly learning
to align and translate. arxiv preprint arxiv:1409.0473,
2014.

bowman, samuel r, vilnis, luke, vinyals, oriol, dai, an-
drew m, jozefowicz, rafal, and bengio, samy. gener-
ating sentences from a continuous space. arxiv preprint
arxiv:1511.06349, 2015.

chen, xi, duan, yan, houthooft, rein, schulman, john,
sutskever, ilya, and abbeel, pieter.
infogan: inter-
pretable representation learning by information max-
in advances in
imizing generative adversarial nets.
neural information processing systems, pp. 2172   2180,
2016.

diao, qiming, qiu, minghui, wu, chao-yuan, smola,
alexander j, jiang, jing, and wang, chong. jointly mod-
eling aspects, ratings and sentiments for movie recom-
mendation (jmars). in proceedings of the 20th acm
sigkdd international conference on knowledge dis-
covery and data mining, pp. 193   202. acm, 2014.

dosovitskiy, alexey and brox, thomas. generating im-
ages with perceptual similarity metrics based on deep
networks. arxiv preprint arxiv:1602.02644, 2016.

goodfellow, ian, pouget-abadie, jean, mirza, mehdi, xu,
bing, warde-farley, david, ozair, sherjil, courville,
aaron, and bengio, yoshua. generative adversarial nets.
in advances in neural information processing systems,
pp. 2672   2680, 2014.

grandvalet, yves, bengio, yoshua, et al. semi-supervised
learning by id178 minimization. in nips, volume 17,
pp. 529   536, 2004.

hinton, geoffrey e, dayan, peter, frey, brendan j, and
neal, radford m. the    wake-sleep    algorithm for un-
supervised neural networks. science, 268(5214):1158,
1995.

hu, zhiting, ma, xuezhe, liu, zhengzhong, hovy, eduard,
and xing, eric. harnessing deep neural networks with
logic rules. in acl, 2016a.

hu, zhiting, yang, zichao, salakhutdinov, ruslan, and
xing, eric p. deep neural networks with massive learned
knowledge. in emnlp, 2016b.

hu, zhiting, yang, zichao, salakhutdinov, ruslan, and
xing, eric p. on unifying deep generative models. arxiv
preprint arxiv:1706.00550, 2017.

hu, zhiting, shi, haoran, yang, zichao, tan, bowen,
zhao, tiancheng, he, junxian, wang, wentao, yu,
xingjiang, qin, lianhui, wang, di, ma, xuezhe, liu,
hector, liang, xiaodan, zhu, wanrong, sachan, deven-
dra singh, and xing, eric. texar: a modularized, versa-
tile, and extensible toolkit for text generation. 2018.

kingma, diederik p and welling, max. auto-encoding
arxiv preprint arxiv:1312.6114,

id58.
2013.

kingma, diederik p, mohamed, shakir, rezende,
danilo jimenez, and welling, max. semi-supervised
in advances in
learning with deep generative models.
neural information processing systems, pp. 3581   3589,
2014.

kusner, matt and heid56dez-lobato, jos. gans for se-
quences of discrete elements with the gumbel-softmax
distribution. arxiv preprint arxiv:1611.04051, 2016.

toward controlled generation of text

wen, tsung-hsien, gasic, milica, mrksic, nikola, su, pei-
hao, vandyke, david, and young, steve. semantically
conditioned lstm-based id86 for
spoken dialogue systems. in emnlp, 2015.

wilson, theresa, wiebe, janyce, and hoffmann, paul. rec-
ognizing contextual polarity in phrase-level sentiment
in proceedings of the conference on human
analysis.
language technology and empirical methods in natu-
ral language processing, pp. 347   354. association for
computational linguistics, 2005.

yang, zichao, hu, zhiting, salakhutdinov, ruslan, and
berg-kirkpatrick, taylor. improved variational autoen-
coders for text modeling using dilated convolutions. in
icml, 2017.

young, steve, ga  si  c, milica, thomson, blaise, and
williams, jason d. pomdp-based statistical spoken di-
alog systems: a review. proceedings of the ieee, 101
(5):1160   1179, 2013.

yu, lantao, zhang, weinan, wang, jun, and yu, yong. se-
qgan: sequence generative adversarial nets with policy
gradient. in aaai, 2017.

zhang, yizhe, gan, zhe, and carin, lawrence. generat-
ing text via adversarial training. in nips workshop on
adversarial training, 2016.

zhou, chunting and neubig, graham. multi-space varia-
tional encoder-decoders for semi-supervised labeled se-
quence transduction. in acl, 2017.

zhu, jun-yan, kr  ahenb  uhl, philipp, shechtman, eli, and
efros, alexei a. generative visual manipulation on the
in european conference on
natural image manifold.
id161, pp. 597   613. springer, 2016.

larsen, anders boesen lindbo, s  nderby, s  ren kaae,
and winther, ole. autoencoding beyond pixels using
a learned similarity metric. in icml, 2016.

mikolov, tomas, kara     at, martin, burget, lukas, cer-
nock`y, jan, and khudanpur, sanjeev. recurrent neu-
ral network based language model. in interspeech, vol-
ume 2, pp. 3, 2010.

odena, augustus, olah, christopher, and shlens, jonathon.
conditional image synthesis with auxiliary classi   er
gans. arxiv preprint arxiv:1610.09585, 2016.

radford, alec, metz, luke, and chintala, soumith. un-
supervised representation learning with deep convolu-
tional id3. arxiv preprint
arxiv:1511.06434, 2015.

reed, scott, lee, honglak, anguelov, dragomir, szegedy,
christian, erhan, dumitru, and rabinovich, andrew.
training deep neural networks on noisy labels with boot-
strapping. arxiv preprint arxiv:1412.6596, 2014.

siddharth, n., paige, brooks, desmaison, alban, meent,
jan-willem van de, wood, frank, goodman, noah d.,
kohli, pushmeet, and torr, philip h.s. learning disen-
tangled representations in deep generative models. 2017.

socher, richard, perelygin, alex, wu, jean y, chuang,
jason, manning, christopher d, ng, andrew y, potts,
christopher, et al. recursive deep models for semantic
compositionality over a sentiment treebank. in proceed-
ings of the conference on empirical methods in natural
language processing (emnlp), volume 1631, pp. 1642.
citeseer, 2013.

sutskever, ilya, vinyals, oriol, and le, quoc v.

se-
quence to sequence learning with neural networks.
in
advances in neural information processing systems, pp.
3104   3112, 2014.

taigman, yaniv, polyak, adam, and wolf, lior. unsuper-

vised cross-domain image generation. in iclr, 2017.

tang, shuai, jin, hailin, fang, chen, and wang, zhaowen.
unsupervised sentence representation learning with ad-
versarial auto-encoder. 2016.

van

den oord, aaron, kalchbrenner, nal,

and
kavukcuoglu, koray. pixel recurrent neural networks.
in icml, 2016.

vinyals, oriol, toshev, alexander, bengio, samy, and er-
han, dumitru. show and tell: a neural image caption
in proceedings of the ieee conference on
generator.
id161 and pattern recognition, pp. 3156   
3164, 2015.

