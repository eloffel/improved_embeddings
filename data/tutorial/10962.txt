end-to-end task-completion neural dialogue systems

xiujun li    yun-nung chen(cid:63) lihong li   

jianfeng gao    asli celikyilmaz   

   microsoft research, redmond, wa, usa
(cid:63)national taiwan university, taipei, taiwan

   {xiul,lihongli,jfgao,aslicel}@microsoft.com

(cid:63)y.v.chen@ieee.org

8
1
0
2

 

b
e
f
1
1

 

 
 
]
l
c
.
s
c
[
 
 

4
v
8
0
0
1
0

.

3
0
7
1
:
v
i
x
r
a

abstract

one of the major drawbacks of modu-
larized task-completion dialogue systems
is that each module is trained individu-
ally, which presents several challenges.
for example, downstream modules are af-
fected by earlier modules, and the per-
formance of the entire system is not ro-
bust to the accumulated errors. this pa-
per presents a novel end-to-end learning
framework for task-completion dialogue
systems to tackle such issues. our neu-
ral dialogue system can directly interact
with a structured database to assist users
in accessing information and accomplish-
ing certain tasks. the reinforcement learn-
ing based dialogue manager offers robust
capabilities to handle noises caused by
other components of the dialogue system.
our experiments in a movie-ticket book-
ing domain show that our end-to-end sys-
tem not only outperforms modularized di-
alogue system baselines for both objective
and subjective evaluation, but also is ro-
bust to noises as demonstrated by several
systematic experiments with different er-
ror granularity and rates speci   c to the lan-
guage understanding module1.

introduction

1
in the past decade, goal-oriented dialogue systems
have been the most prominent component in to-
day   s virtual personal assistants, which allow users
to speak naturally in order to accomplish tasks
more ef   ciently. traditional systems have a rather
complex and modularized pipeline, consisting of a
language understanding (lu) module, a dialogue

1the source code is available at: https://github/

com/miulab/tc-bot.

manager (dm), and a id86
(id86) component (rudnicky et al., 1999; zue
et al., 2000; zue and glass, 2000).

recent advances of deep learning have inspired
many applications of neural models to dialogue
systems. wen et al. (2017) and bordes et al.
(2017) introduced a network-based end-to-end
trainable task-oriented dialogue system, which
treated dialogue system learning as the problem
of learning a mapping from dialogue histories to
system responses, and applied an encoder-decoder
model to train the whole system. however, the
system is trained in a supervised fashion: not only
does it require a lot of training data, but it may
also fail to    nd a good policy robustly due to lack
of exploration of dialogue control in the training
data. zhao and eskenazi (2016)    rst presented an
end-to-end id23 (rl) approach
to dialogue state tracking and policy learning in
the dm. this approach is shown to be promising
when applied to the task-oriented dialogue prob-
lem of guessing the famous person a user thinks
of. in the conversation, the agent asks the user a
series of yes/no questions to    nd the correct an-
swer. however, this simpli   ed task may not gen-
eralize to practical problems due to the following:
1. in   exible question types     asking request
questions is more natural and ef   cient than
yes/no questions. for example, it is more
natural and ef   cient for the system to ask
   where are you located?    instead of    are
you located in palo alto?   , when there are
a large number of possible values for the lo-
cation slot.

2. poor robustness     the user answers are too
simple to be misunderstood, so the system
lacks the robustness against noise in real user
utterances.

3. user requests during dialogues     in a task-
oriented dialogue, user may ask questions for

figure 1: illustration of the end-to-end neural dialogue system: given user utterances, reinforcement
learning is used to train all components in an end-to-end fashion.

selecting the preferred slot values. in a    ight-
booking example, user might ask    what    ight
is available tomorrow?   .

for the second issue, su et al. (2016) brie   y in-
vestigated the effect of dialogue action level se-
mantic error rates on the dialogue performance.
lemon and liu (2007) compared policy transfer
properties under different environments, showing
that policies trained in high-noise conditions have
better transfer properties than those trained in low-
noise conditions. recently, dhingra et al. (2017)
proposed an end-to-end differentiable kb-infobot
to provide the solutions to the    rst two issues, but
the last one remained unsolved.

this paper addresses all

three issues above
by rede   ning the targeted system as a task-
completion neural dialogue system. our frame-
work is more practical in that the information can
be easily accessed by the user during the conver-
sations, while the    nal goal of the system is to
complete a task, such as movie-ticket booking.
this paper is the    rst attempt of training a real-
world task-completion dialogue system in an end-
to-end fashion by leveraging supervised learning
and id23 techniques. to fur-
ther understand the robustness of reinforcement
learning based dialogue systems, we conduct ex-
tensive experiments and quantitative analysis on a
   ne-grained level of lu errors, and provide mean-
ingful insights on how the language understanding
component impacts the overall performance of the

dialogue system.

our contributions are three-fold:

    robustness     we propose a neural dialogue
system with greater robustness by automat-
ically selecting actions based on uncertainty
and confusion by id23. we
also provide the    rst systematic analysis to
investigate the impact of different types of
natural language understanding errors on di-
alogue system performance. we show that
slot-level errors have a greater impact on the
system performance than intent-level ones,
and that slot value replacement degrades the
performance most. our    ndings shed some
light on how to design multi-task natural lan-
guage understanding models (intent classi   -
cation and slot labeling) in the dialogue sys-
tems with consideration of error control.
    flexibility     the system is the    rst neural
dialogue system that allows user-initiated be-
haviors during conversations, where the users
can interact with the system with higher    ex-
ibility that is important in realistic scenarios.
    reproducibility     we demonstrate how to
evaluate rl dialogue agents using crowd-
sourced task-speci   c datasets and simulated
users in an end-to-end fashion, guaranteeing
reproducibility and consistent comparisons
of competing methods in an identical setting.

wib-typewi+1wi+2ooeos<intent>wib-typewi+1wi+2ooeos<intent>semantic framerequest_moviegenre=action, date=this weekendsystem action/ policyrequest_locationuser dialogue actioninform(location=san francisco)time t-1wi<slot>wi+1wi+2ooeos<intent>language understanding (lu)time t-2time tdialogue management (dm)w0w1w2id86 (id86)eosuser goaluser agenda modelinguser simulatorend-to-end neural dialogue systemtext inputare there any action movies to see this weekend?error model controllerbackend databaseaction

w    nd
   
o
   nd movie

s
i

   

b-genre

movies

   
o

this
   

b-date

weekend

   

i-date

figure 2: an example utterance with annotations
of semantic slots in iob format (s) and intent (i),
b-date and i-date denote the date slot.

2 proposed framework
the proposed framework2 is illustrated in fig-
ure 1. it includes a user simulator (left part) and
a neural dialogue system (right part). in the user
simulator, an agenda-based user modeling compo-
nent based at the dialogue act level is applied to
control the conversation exchange conditioned on
the generated user goal, to ensure the user behaves
in a consistent, goal-oriented manner. an id86
module is used to generate natural language texts
corresponding to the user dialogue actions. in a
neural dialogue system, an input sentence (recog-
nized utterance or text input) passes through an lu
module and becomes a corresponding semantic
frame, and an dm, which includes a state tracker
and policy learner, is to accumulate the semantics
from each utterance, robustly track the dialogue
states during the conversation, and generate the
next system action.

2.1 neural dialogue system
language understanding (lu): a major task
of lu is to automatically classify the domain of a
user query along with domain speci   c intents and
   ll in a set of slots to form a semantic frame. the
popular iob (in-out-begin) format is used for rep-
resenting the slot tags, as shown in figure 2.

(cid:126)x = w1, ..., wn, <eos>
(cid:126)y = s1, ..., sn, im

where (cid:126)x is the input word sequence and (cid:126)y contains
the associated slots, sk, and the sentence-level in-
tent im. the lu component is implemented with
a single lstm, which performs intent prediction
and slot    lling simultaneously (hakkani-t  ur et al.,
2016; chen et al., 2016):

(cid:126)y = lstm((cid:126)x) .

(1)

the lu objective is to maximize the conditional
id203 of the slots and the intent (cid:126)y given the

2the source code is available at: https://github.

com/miulab/tc-bot

word sequence (cid:126)x:

p((cid:126)y | (cid:126)x) =

(cid:32) n(cid:89)

(cid:33)

p(si | w1, . . . , wi)

p(im | (cid:126)y).

i

the weights of the lstm model are trained us-
ing id26 to maximize the conditional
likelihood of the training set labels. the predicted
tag set is a concatenated set of iob-format slot
tags and intent tags; therefore, this model can be
trained using all available dialogue actions and ut-
terance pairs in our labeled dataset in a supervised
manner.

dialogue management (dm): the symbolic
lu output is passed to the dm in the dialogue
act form (or semantic frame). the classic dm in-
cludes two stages, dialogue state tracking and pol-
icy learning.
    dialogue state tracking: given the lu sym-
bolic output, such as request(moviename;
genre=action; date=this weekend), three
major functions are performed by the state
tracker: a symbolic query is formed to inter-
act with the database to retrieve the available
results; the state tracker will be updated based
on the available results from the database and
the latest user dialogue action; and the state
tracker will prepare the state representation
st for policy learning.
    policy learning: the state representation
for the policy learning includes the lat-
est user action (e.g., request(moviename;
genre=action; date=this weekend)), the
latest agent action (request(location)), the
available database results, turn information,
and history dialogue turns, etc. conditioned
on the state representation st from the state
tracker, the policy    is to generate the next
available system action at according to   (st).
either supervised learning or reinforcement
learning can be used to optimize   . details
about rl-based policy learning can be found
in section 3.

prior work used different implementation ap-
proaches summarized below. dialogue state track-
ing is the process of constantly updating the state
of the dialogue, and lee (2014) showed that there
is a positive correlation between state tracking per-
formance and dialogue performance. most pro-
duction systems use manually designed heuris-
tics, often based on rules, to update the dialogue

states based on the highly con   dent output from
lu. williams et al. (2013) formalized the tracking
problem as a supervised sequence labeling task,
where the input is lu outputs and the output is
the true slot values, and the state tracker   s results
can be translated into a dialogue policy. zhao and
eskenazi (2016) proposed to jointly train the state
tracker and the policy learner in order to optimize
the system actions more robustly. instead of ex-
plicitly incorporating the state tracking labels, this
paper learns the system actions with implicit dia-
logue states, so that the proposed dm can be more
   exible and robust to the noise propagated from
the previous components (su et al., 2016; liu and
lane, 2017). a rule-based agent is employed to
warm-start the system, via supervised learning on
labels generated by the rules. the system is then
further trained end-to-end with rl, as explained
in section 3.

2.2 user simulation
in order to perform end-to-end training for the pro-
posed neural dialogue systems, a user simulator
is required to automatically and naturally interact
with the dialogue system. in the task-completion
dialogue setting, the user simulator    rst generates
a user goal. the agent does not know the user goal,
but tries to help the user accomplish it in the course
of conversations. hence, the entire conversation
exchange is around this goal implicitly. a user
goal generally consists of two parts: inform slots
for slot-value pairs that serve as constraints from
the user, and request slots for slots whose value
the user has no information about, but wants to get
the values from the agent during the conversation.
the user goals are generated using a set of labeled
conversational data.
user agenda modeling: during the course
of a dialogue,
the user simulator maintains a
compact, stack-like representation called user
agenda (schatzmann and young, 2009), where the
user state su is factored into an agenda a and a
goal g. the goal consists of constraints c and
request r. at each time-step t, the user simula-
tor generates the next user action au,t based on the
current state su,t and the last agent action am,t   1,
and then updates the current status s(cid:48)
id86 (id86): given
the user   s dialogue actions, the id86 module gen-
erates natural language texts. to control the qual-
ity of user simulation given limited labeled data, a

u,t.

hybrid approach including a template-based id86
and a model-based id86 is employed, where the
model-based id86 is trained on the labeled dataset
with a sequence-to-sequence model. it takes dia-
logue acts as input, and generates sentence sketch
with slot placeholders via an lstm decoder. then
a post-processing scan is performed to replace the
slot placeholders with their actual values (wen
et al., 2015). in the lstm decoder, we apply beam
search, which iteratively considers the top k best
sub-sentences when generating the next token.

in the hybrid model, if the user dialogue actions
can be found in the prede   ned sentence templates,
the template-based id86 is applied; otherwise, the
utterance is generated by the model-based id86.
this hybrid approach allows a dialogue system
developer to easily improve id86 by providing
templates for sentences that the machine-learned
model does not handle well.

2.3 error model controller
when training or testing a policy based on seman-
tic frames of user actions, an error model (schatz-
mann et al., 2007) is introduced to simulate noises
from the lu component, and noisy communica-
tion between the user and the agent in order to test
the model robustness. here, we introduce differ-
ent levels of noises in the error model: one type
of errors is at the intent level, another is at the slot
level. for each level, there are more    ne-grained
noises.

etc.

intent-level error: at the intent level, we cat-
egorize all intents into three groups:
    group 1: general greeting, thanks, closing,
    group 2: users may inform, to tell the slot
values (or constraints) to the agent, for ex-
ample, inform(moviename=   titanic   , start-
time=   7pm   ).

    group 3: users may request

information
in a movie-booking sce-
for speci   c slots.
nario, users might ask    request(starttime;
moviename=   titanic   )   .

intents, such as request starttime,

in the speci   c task of movie-booking, for in-
there exist multiple inform and re-
stance,
re-
quest
quest moviename,
inform starttime and in-
form moviename, etc. based on the above intent
categories, there are three types of intent errors:
    random error (i0): the random noisy intent
from the same category (within group error)

or other categories (between group error).

    within-group error (i1):

the noisy intent is
from the same group of the real intent, for ex-
ample, the real intent is request theater, but
the predicted intent from lu module might
be request moviename.
    between-group error (i2): the noisy intent is
from the different group, for example, a real
intent request moviename might be pre-
dicted as the intent inform moviename.

slot-level error: at the slot level, there are four
error types:
    random error (s0): to simulate the noise that
is randomly set to the following three types.
    slot deletion (s1): is to simulate the scenario
where the slot is not recognized by the lu
component.
    incorrect slot value (s2): is to simulate the
scenario where the slot name is correctly rec-
ognized, but the slot value is wrong, e.g.,
wrong id40.
    incorrect slot (s3): is to simulate the scenario
where both the slot and its value are incor-
rectly recognized.

3 end-to-end id23
to learn the interactive policy of our system, we
apply id23 to the dm training
in an end-to-end fashion, where each neural net-
work component can be    ne tuned. the policy
is represented as a deep q-network (id25) (mnih
et al., 2015), which takes the state st from the state
tracker as input, and outputs q(st, a;   ) for all ac-
tions a. two important id25 tricks, target network
usage and experience replay are applied, where the
experience replay strategy is changed for the dia-
logue setting.

during training, we use  -greedy exploration
and an experience replay buffer with dynamically
changing buffer size. at each simulation epoch,
we simulate n (n = 100) dialogues and add these
state transition tuples (st, at, rt, st+1) to the expe-
rience replay buffer for training. in one simulation
epoch, the current id25 will be updated multiple
times (depending on the batch size and the current
size of experience replay buffer). at the last sim-
ulation epoch, the target network will be replaced
by the current id25, the target id25 network is
only updated for once in one simulation epoch.

the experience replay strategy is critical for rl
training (schaul et al., 2015). in our buffer update

strategy, we accumulate all experience tuples from
the simulation and    ush the pool till the current
rl agent reaches a success rate threshold (i.e., a
threshold which is equal to the performance of a
rule-based agent), and then use the experience tu-
ples from the current rl agent to re-   ll the buffer.
the intuition is that the initial performance of the
id25 is not strong enough to generate good ex-
perience replay tuples, thus we do not    ush the
experience replay pool till the current rl agent
can reach a certain success rate (for example, the
success rate of a rule-based agent).
in the rest
of the training process, at every simulation epoch,
we estimate the success rate of the current id25
agent (by running it multiple dialogues on sim-
ulated users).
if the current id25 agent is bet-
ter than the target network, the experience replay
buffer will be    ushed.

4 experiments

we consider a task-completion dialogue system
for helping users book movie tickets. over the
course of conversation, the dialogue system gath-
ers information about the customer   s desires and
ultimately books the movie tickets. the environ-
ment then assesses a binary outcome (success or
failure) at the end of the conversation, based on
(1) whether a movie is booked, and (2) whether
the movie satis   es the users constraints.

dataset: the raw conversational data were col-
lected via amazon mechanical turk, with anno-
tations provided by domain experts. in total, we
have labeled 280 dialogues, and the average num-
ber of turns per dialogue is approximately 11. the
annotated data includes 11 dialogue acts and 29
slots, most of the slots are informable slots, which
users can use to constrain the search, and some
are requestable slots, of which users can ask val-
ues from the agent. for example, numberofpeo-
ple cannot be a requestable slot, since arguably
user knows how many tickets he or she wants to
buy. the detailed annotations can be found in ap-
pendix a.

4.1 simulated user evaluation
two sets of experiments are conducted in the dm
training, where two input formats are used for
training the rl agents:

1. frame-level semantics: when training or test-
ing a policy based on semantic frames of user

(a) frame-level semantics for training

(b) natural language for end-to-end training

figure 3: learning curves for policy training (average of 10 runs). the blue solid lines show the rule
agent performance, where we employ to initialize the experience replay buffer pool; the orange dotted
line is the optimal upper bound, which is the percentage of reachable user goals.

intent error

type

0: random

0: random
1: within group
2: between group
0: random
0: random
0: random

rate
0.00
0.10
0.20
0.10
0.10
0.10
0.00
0.10
0.20

0: random

0.10

i
s
a
b

t
n
e
t
n
i

setting
c b1
b2
b3
i0
i1
i2
i3
i4
i5
s0
s1
s2
s3
s4
s5
s6

t
o
l

s

slot error

type

0: random

rate
0.00
0.10
0.20

0: random

0.05

0: random 0.10
1: deletion
0.10
2: value
0.10
3: slot
0.10
0.00
0: random
0.10
0: random
0.20
0: random

table 1: experimental settings with different in-
tent/slot error types described in section 2.3 and
different error rates.

actions, a noise controller described in sec-
tion 2.3 is used to simulate lu errors and
noisy communications between the user and
the agent.

2. natural language: when training or testing a
policy on natural language level, in which lu
and id86 may introduce noises. in our exper-
iments, the id86 decoder uses beam size = 3
to balance speed and performance.

figure 3(a) shows a learning curve for the dia-
logue system performance trained with the frame-
level information (user semantic frames and sys-
tem actions), where the number is the average of
10 runs. figure 3(b) is a learning curve for the
system trained at the natural language level.
in

figure 4: learning curves for different lu error
rates.

both settings, the rl agents signi   cantly outper-
form the rule-based systems, showing the poten-
tial of a neural dialogue system that can perform
real-world tasks and be improved autonomously
through interactions with users. also, the end-to-
end system in figure 3(b) takes longer for the rl
agent to adapt to the noises from lu and id86,
indicating the dif   culty of maintaining the sys-
tem robustness. the consistently increasing trend
of our proposed end-to-end system also suggests
greater robustness in noisy, real-world scenarios.
to further investigate and understand the real im-
pact of the lu component to the robustness of rl
agent in the dialogue system, we conduct a series
of experiments under different error settings (in-
tent and slot errors from lu) summarized in ta-
ble 1, where the learning curves are averaged over
10 runs.


 

 

 

 

 

 

 

 

 

 


$ 2: ,9 43  54. 

 
 
 
  
$:..088 #,90#    039#: 0   039&5507  4:3/
 

 

 

 

 

 

 

 

 

 


$ 2: ,9 43  54. 

 
 
 
  
$:..088 #,90#    039#: 0   039&5507  4:3/
 

 

 

 

 

$ 2: ,9 43  54. 

 
 
 
  
$:..088 #,90    


  39039 	 8 49 07747 7,90    
 
  39039 	 8 49 07747 7,90    
 
  39039 	 8 49 07747 7,90
 
 
 
 
 
 ;07, 0 %:738(a) intent error type analysis

(b) intent error rate analysis

(c) slot error type analysis

(d) slot error rate analysis

figure 5: learning curves of the different intent and slot errors in terms of success rate (left axis) and
average turns (right axis).

4.2 basic error analysis

the group of basic experiments (from b1 to b3)
are in the settings that combine the noise from both
intent and slot: 1) for both intent and slot, the er-
ror types are random, and the error rates are in
{0.00, 0.10, 0.20}. the rule-based agent reports
41%, 21%, and 12% success rates under 0.00,
0.10, and 0.20 error rates respectively. in contrast,
the rl-based agent achieves 90%, 79%, and 76%
success rate under the same error rates, respec-
tively. we compare the performance between two
types of agents and    nd that the rl-based agent
has greater robustness and is less sensitive to noisy
inputs. therefore, the following experiments are
performed using a rl dialogue agent due to ro-
bustness consideration. from fig. 4, the dialogue
agents degrade remarkably when the error rate in-
creases (leading to lower success rates and higher
average turns).

intent error analysis

4.3
to further understand the impact of intent-level
noises to dialogue systems,
two experimental
groups are performed: the    rst group (i0   i2) fo-
cuses on the difference among all intent error
types; the second group (i3   i5) focuses on the im-
pact of intent error rates. other factors are identi-
cal for the two groups, with the random slot error
type and a 5% slot error rate.

intent error type

4.3.1
experiments with the settings of i0   i2 are under
the same slot errors and same intent error rate
(10%), but with different intent error types: i1 in-
cludes the noisy intents from the same categories,
i2 includes the noisy intents from different cate-
gories, and i0 includes both via random selection.
fig. 5(a) shows the learning curves for all intent er-
ror types, where the difference among three curves
is insigni   cant, indicating that the incorrect intents
have similar impact no matter what categories they


 

 

 

 

 

$ 2: ,9 43  54. 

 
 
 
  
$:..088 #,90 
   39039 07747 9 50 
     39039 07747 9 50       39039 07747 9 50  
 
 
 
 
 
 ;07, 0 %:738
 

 

 

 

 

$ 2: ,9 43  54. 

 
 
 
  
$:..088 #,90    


  39039 07747 7,90    
 
  39039 07747 7,90    
 
  39039 07747 7,90
 
 
 
 
 
 ;07, 0 %:738
 

 

 

 

 

$ 2: ,9 43  54. 

 
 
 
  
$:..088 #,90$
  8 49 07747 9 50 
$   8 49 07747 9 50  $   8 49 07747 9 50  $   8 49 07747 9 50  
 
 
 
 
 
 ;07, 0 %:738
 

 

 

 

 

$ 2: ,9 43  54. 

 
 
 
  
$:..088 #,90$   


 8 49 07747 7,90$   
 
 8 49 07747 7,90$   
 
 8 49 07747 7,90
 
 
 
 
 
 ;07, 0 %:738belong to.

intent error rate

4.3.2
experiments with the settings i3   i5 investigate
the difference among different intent error rates.
when the intent error rate increases, the dialogue
agent performs slightly worse, but the difference
is subtle. it suggests that the rl-based agent has
better robustness to noisy intents. as shown in
fig. 5(a,b), all rl agents can converge to a simi-
lar success rate in both intent error type and intent
error rate settings.

4.4 slot error analysis
we further conducted two groups of experiments
to investigate the impact of slot-level noises where
other factors are    xed     with the random intent
error type and a 10% intent error rate.

4.4.1 slot error type
experiments (s0     s3) investigate the impact of
different slot error types. corresponding learning
curves are given in fig. 5(c). among single error
types (s1   s3), incorrect slot value (s2) performs
worst, which means that the slot name is recog-
nized correctly, but a wrong value is extracted with
the slot (such as wrong id40); in
this case, the agent receives a wrong value for the
slot, and eventually books a wrong ticket or fails to
book it. the probable reason is that the dialogue
agent has dif   culty identifying the mistakes, and
using the incorrect slot values for the following di-
alogue actions could signi   cantly degrade the per-
formance. between slot deletion (s1) and incor-
rect slot (s3), the difference is limited, indicating
that the rl agent has similar capability of han-
dling these two kinds of slot-level noises.

4.4.2 slot error rate
experiments with the settings from s4 to s6 focus
on different slot error rates (0%, 10%, and 20%)
and report the results in fig. 5(d). it is clear from
fig. 5(d) that the dialogue agent performs worse
as the slot error rate increases (the curve of the
success rate drops and the curve of average turns
rises). comparing with fig. 5(b), the dialogue sys-
tem performance is more sensitive to the slot error
rate than the intent error rate.

4.5 human evaluation
we further evaluated the rule-based and id25
agents against real human users recruited from

(a) success rate

(b) user rating distribution

figure 6: performance of the rule-based agent ver-
sus id25 agent tested with real users: (a) success
rate, number of tested dialogues and p-values are
indicated on each bar; (b) distribution of user rat-
ings for two agents (difference in mean is signi   -
cant with p < 0.01).

the authors af   liation, where the id25 agent was
trained on the simulated user in the frame-level
with 5% random slot errors. in each dialogue ses-
sion, one of the agents was randomly picked to
converse with a user, and the user was presented
with a prede   ned user goal sampled from our cor-
pus, and was instructed to converse with the agent
to complete the presented task. at the end of each
session, the user was asked to give a rating on a
scale from 1 (worst) to 5 (best) based on both nat-
uralness and coherence of the dialogue. we col-
lected a total of 110 dialogue sessions from 8 hu-
man users. figure 6(a) presents the performance
of these agents against real users in terms of suc-
cess rate. figure 6(b) shows the subjective evalu-
ation in terms of user rating. for all the cases, the
rl agent signi   cantly outperforms the rule-based
agent for both objective (success rate) and subjec-
tive evaluation (user rating).

5 discussion and future work

this paper presents an end-to-end learning frame-
work for task-completion neural dialogue systems.
our experiments, both on simulated and real users,
show that id23 systems outper-
form rule-based agents and have better robustness
to allow natural interactions with users in real-
world task-completion scenarios. furthermore,
we conduct a series of extensive experiments to
understand the impact of natural language under-
standing errors on the performance of a reinforce-
ment learning based, task-completion neural dia-
logue system. our empirical results suggest sev-
eral interesting    ndings: 1) slot-level errors have a
greater impact than intent-level errors; a possible
explanation is related to our dialogue action rep-

#: 0 " 


 
 
 
  
$:..088 #,90
  
  5     

     #: 0 " 
      &807 #,9 3 5     

 resentation, intent(slot-value pairs).
if an intent
is predicted wrong, for example, inform was pre-
dicted incorrectly as request ticket, the dialogue
agent can handle this unreliable situation and de-
cide to make con   rmation in order to keep the cor-
rect information for the following conversation. in
contrast, if a slot moviename is predicted wrong,
or a slot value is not identi   ed correctly, this di-
alogue turn might directly pass the wrong infor-
mation to the agent, which might lead the agent
to book a wrong ticket. another reason is that the
dialogue agent can still maintain a correct intent
based on slot information even though the pre-
dicted intent is wrong. in order to verify the hy-
potheses, further experiments are needed, which
we leave as future work. 2) different slot error
types have different impacts on the rl agents.
3) rl agents are more robust to certain types of
slot-level errors     the agents can learn to double-
check or con   rm with users, at the cost of slightly
longer conversations.

finally, it should be noted that the experiments
in this paper focus on task-completion dialogues.
another type of dialogues known as chit-chats has
different optimization goals (li et al., 2016).
it
would be interesting to extend our analysis from
this paper to chit-chat dialogues to gain useful in-
sights for impacts of lu errors.

acknowledgments

we would like to thank dilek hakkani-t  ur and re-
viewers for their insightful comments on the pa-
per. yun-nung chen is supported by the ministry
of science and technology of taiwan and medi-
atek inc..

references
antoine bordes, y-lan boureau, and jason weston.
2017. learning end-to-end goal-oriented dialog. in
proceedings of iclr.

yun-nung chen, dilek hakanni-t  ur, gokhan tur, asli
celikyilmaz, jianfeng gao, and li deng. 2016.
syntax or semantics? knowledge-guided joint se-
in proceedings of the 6th
mantic frame parsing.
ieee workshop on spoken language technology.
pages 348   355.

for computational linguistics (volume 1: long pa-
pers). pages 484   495.

dilek hakkani-t  ur, gokhan tur, asli celikyilmaz,
yun-nung chen, jianfeng gao, li deng, and ye-
yi wang. 2016. multi-domain joint semantic frame
in proceed-
parsing using bi-directional id56-lstm.
ings of interspeech. pages 715   719.

sungjin lee. 2014. extrinsic evaluation of dialog state
tracking and predictive metrics for dialog policy op-
in 15th annual meeting of the special
timization.
interest group on discourse and dialogue. page
310.

oliver lemon and xingkun liu. 2007. dialogue policy
learning for combinations of noise and user simula-
tion: transfer results. in proc. sigdial.

jiwei li, will monroe, alan ritter, michel galley,
jianfeng gao, and dan jurafsky. 2016. deep rein-
forcement learning for dialogue generation .

bing liu and ian lane. 2017. an end-to-end train-
able neural network model with belief tracking for
task-oriented dialog. in proceedings of interspeech.
pages 2506   2510.

volodymyr mnih, koray kavukcuoglu, david silver,
andrei a. rusu, joel veness, marc g. bellemare,
alex graves, martin riedmiller, andreas k. fidje-
land, georg ostrovski, stig petersen, charles beat-
tie, amir sadik, ioannis antonoglou, helen king,
dharshan kumaran, daan wierstra, shane legg,
and demis hassabis. 2015. human-level con-
trol through deep id23. nature
518:529   533.

alexander i rudnicky, eric h thayer, paul c constan-
tinides, chris tchou, r shern, kevin a lenzo, wei
xu, and alice oh. 1999. creating natural dialogs in
the carnegie mellon communicator system. in eu-
rospeech.

jost schatzmann, blaise thomson, and steve young.
2007. error simulation for training statistical di-
in ieee workshop on automatic
alogue systems.
id103 & understanding.

jost schatzmann and steve young. 2009. the hid-
ieee trans-
den agenda user simulation model.
actions on audio, speech, and language processing
17(4):733   747.

tom schaul, john quan, ioannis antonoglou, and
david silver. 2015. prioritized experience replay.
arxiv:1511.05952 .

bhuwan dhingra, lihong li, xiujun li, jianfeng gao,
yun-nung chen, faisal ahmed, and li deng. 2017.
towards end-to-end id23 of dia-
in proceed-
logue agents for information access.
ings of the 55th annual meeting of the association

pei-hao su, milica gasic, nikola mrksic, lina rojas-
barahona, stefan ultes, david vandyke, tsung-
hsien wen, and steve young. 2016.
con-
tinuously learning neural dialogue management.
arxiv:1606.02689 .

tsung-hsien wen, milica gasic, nikola mrksic,
lina m rojas-barahona, pei-hao su, stefan ultes,
david vandyke, and steve young. 2017. a network-
based end-to-end trainable task-oriented dialogue
system. pages 438   449.

tsung-hsien wen, milica gasic, nikola mrksic, pei-
hao su, david vandyke, and steve young. 2015.
semantically conditioned lstm-based natural lan-
guage generation for spoken dialogue systems.
in
proceedings of the 2015 conference on empirical
methods in natural language processing. pages
1711   1721.

jason williams, antoine raux, deepak ramachan-
dran, and alan black. 2013. the dialog state track-
ing challenge. in proceedings of the sigdial 2013
conference. pages 404   413.

tiancheng zhao and maxine eskenazi. 2016. to-
wards end-to-end learning for dialog state tracking
and management using deep id23.
in proceedings of the 17th annual meeting of the
special interest group on discourse and dialogue.
pages 1   10.

victor zue, stephanie seneff, james r glass, joseph
polifroni, christine pao, timothy j hazen, and
lee hetherington. 2000.
jupiter: a telephone-
based conversational interface for weather informa-
tion. ieee transactions on speech and audio pro-
cessing 8(1):85   96.

victor w zue and james r glass. 2000. conversa-
tional interfaces: advances and challenges. pro-
ceedings of the ieee 88(8):1166   1180.

a dataset annotation
table 2 lists all annotated dialogue acts and slots
in detail.

intent

slot

annotations

request, inform, deny, con   rm question,
con   rm answer, greeting, closing, not sure,
multiple choice, thanks, welcome
actor, actress, city, closing, critic rating,
date, description, distanceconstraints,
greeting, implicit value, movie series,
moviename, mpaa rating, numberofpeople,
numberofkids, taskcomplete, other, price,
seating, starttime, state, theater,
theater chain, video format, zip, result,
ticket, mc list

table 2: the data annotation.

b sample dialogues
table 3 shows one success and one failure dia-
logue examples generated by the rule-based agent
and rl agent interacting with user simulator in
the movie-booking domain. to be informative, we
also explicitly show the user goal at the head of the
dialogue, the purpose is to help the user to accom-
plish this goal and book the right movie tickets,
but the agent knows nothing about the user goal.

new episode, user goal:
{

   request slots   : {
   ticket   :    unk   
},
   inform slots   : {
   city   :    seattle   ,
   numberofpeople   :    2   ,
   theater   :    regal meridian 16   ,
   starttime   :    9:25 pm   ,
   date   :    tomorrow   ,
   moviename   :    zoolander 2   

}

}
rule-based agent
user: can i get some tickets for zoolander 2?
agent: what movie are you interested in?
user: i want to watch zoolander 2.
agent: what time would you like to see it?
user: i want to watch at 9:25 pm.
agent: what city you would like?
user: i want to watch at seattle.
agent: what date would you like to watch it?
user: i want to set it up tomorrow.
agent: which theater would you like?
user: i want to watch at regal meridian 16.
agent: how many tickets do you need?
user: i want 2 tickets please!
agent: great - i was able to purchase 2 tickets for you to
see zoolander 2 tomorrow at regal meridian 16 theater in
seattle at 9:25 pm.
user: thank you.
agent: thank you.
success
rl agent
user: can i get tickets for zoolander 2 tomorrow?
agent: which city would you like?
user: i want to watch at seattle.
agent: how many tickets do you need?
user: i want 2 tickets please!
agent: 9:25 pm is available.
user: i want to watch at regal meridian 16.
agent: great - i was able to purchase 2 tickets for you to
see zoolander 2 tomorrow at regal meridian 16 theater in
seattle at 9:25 pm.
user: thank you.
agent: thank you.
success

new episode, user goal:
{

   request slots   : {
   ticket   :    unk   
   theater   :    unk   
   starttime   :    unk   

},
   inform slots   : {

   numberofpeople   :    3   ,
   date   :    tomorrow   ,
   moviename   :    10 clover   eld lane   

}

}

rule-based agent
user: what is the start time for 10 clover   eld lane?
agent: what movie are you interested in?
user: i want to watch at 10 clover   eld lane.
agent: what time would you like to see it?
user: what time is it playing tomorrow? i need 3 tickets.
agent: what city you would like?
user: i do not care.
agent: what date would you like to watch it?
user: i want to set it up tomorrow.
agent: which theater would you like?
user: which theater is available?
agent: how many tickets do you need?
user: i want 3 tickets please!
agent: okay, i have booked 3 tickets for 10 clover   eld
lane tomorrow.
user: thank you.
agent: thank you.
failure: agent does not answer all the questions
(starttime, theater) from user.
rl agent
user: which theater can i book 3 tickets for 10
clover   eld lane?
agent: what time would you like to see it?
user: which theater and start time are available
tomorrow?
agent: 11:45am is available.
user: which theater is available?
agent: regal la live stadium 14 is available.
user: could you help me to book the tickets?
agent: okay - i was able to book 3 tickets for you to see
10 clover   eld lane at regal la live stadium 14 at 11:45am
tomorrow.
user:thank you.
agent: thank you.
success

table 3: two sample dialogues generated by rule-based and rl agents with user simulator: left column
shows both rule and rl agents succeed; right column shows that rule-based agent fails, while rl agent
succeeds.

