on-line active reward learning for policy optimisation

in spoken dialogue systems

pei-hao su, milica ga  si  c, nikola mrk  si  c, lina rojas-barahona,
stefan ultes, david vandyke, tsung-hsien wen and steve young
department of engineering, university of cambridge, cambridge, uk

{phs26, mg436, nm480, lmr46, su259, djv27, thw28, sjy}@cam.ac.uk

6
1
0
2

 

n
u
j
 

2

 
 
]
l
c
.
s
c
[
 
 

2
v
9
6
6
7
0

.

5
0
6
1
:
v
i
x
r
a

abstract

the ability to compute an accurate re-
ward function is essential for optimising
a dialogue policy via reinforcement learn-
ing. in real-world applications, using ex-
plicit user feedback as the reward sig-
nal is often unreliable and costly to col-
lect. this problem can be mitigated if
the user   s intent is known in advance or
data is available to pre-train a task suc-
cess predictor off-line. in practice neither
of these apply for most real world applica-
tions. here we propose an on-line learn-
ing framework whereby the dialogue pol-
icy is jointly trained alongside the reward
model via active learning with a gaussian
process model. this gaussian process op-
erates on a continuous space dialogue rep-
resentation generated in an unsupervised
fashion using a recurrent neural network
encoder-decoder. the experimental results
demonstrate that the proposed framework
is able to signi   cantly reduce data annota-
tion costs and mitigate noisy user feedback
in dialogue policy learning.

introduction

1
spoken dialogue systems (sds) allow human-
computer interaction using natural speech. they
can be broadly divided into two categories: chat-
oriented systems which aim to converse with users
and provide reasonable contextually relevant re-
sponses (vinyals and le, 2015; serban et al.,
2015), and task-oriented systems designed to as-
sist users to achieve speci   c goals (e.g.    nd ho-
tels, movies or bus schedules) (daubigney et al.,
2014; young et al., 2013). the latter are typi-
cally designed according to a structured ontology
(or a database schema), which de   nes the domain

figure 1: an example of a task-oriented dialogue
with a pre-de   ned task and the evaluation results.

that the system can talk about. teaching a system
how to respond appropriately in a task-oriented
sds is non-trivial. this dialogue management
task is often formulated as a manually de   ned di-
alogue    ow that directly determines the quality of
interaction. more recently, dialogue management
has been formulated as a id23
(rl) problem which can be automatically opti-
mised (levin and pieraccini, 1997; roy et al.,
2000; williams and young, 2007; young et al.,
2013).
in this framework, the system learns by
a trial and error process governed by a poten-
tially delayed learning objective de   ned by a re-
ward function.

a typical approach to de   ning the reward func-
tion in a task-oriented dialogue system is to ap-
ply a small per-turn penalty to encourage short
dialogues and to give a large positive reward at
the end of each successful interaction. figure 1
is an example of a dialogue task which is typi-
cally set for users who are being paid to converse
with the system. when users are primed with a
speci   c task to complete, dialogue success can be
determined from subjective user ratings (subj), or

an objective measure (obj) based on whether or
not the pre-speci   ed task was completed (walker
et al., 1997; ga  si  c et al., 2013). however, prior
knowledge of the user   s goal is not normally avail-
able in real situations, making the objective reward
estimation approach impractical.

furthermore, objective ratings are in   exible and
often fail as can be seen from figure 1, if the
user does not strictly follow the task. this re-
sults in a mismatch between the obj and subj rat-
ings. however, relying on subjective ratings alone
is also problematic since crowd-sourced subjects
frequently give inaccurate responses and real users
are often unwilling to extend the interaction in or-
der to give feedback, resulting in unstable learning
(zhao et al., 2011; ga  si  c et al., 2011). in order
to    lter out incorrect user feedback, ga  si  c et al.
(2013) used only dialogues for which obj = subj.
nonetheless, this is inef   cient and not feasible
anyway in most real-world tasks where the user   s
goal is generally unknown and dif   cult to infer.

in light of the above, su et al. (2015a) pro-
posed learning a neural network-based obj esti-
mator from off-line simulated dialogue data. this
removes the need for the obj check during on-
line policy learning and the resulting policy is as
effective as one trained with dialogues using the
obj = subj check. however, a user simulator
will only provide a rough approximation of real
user statistics and developing a user simulator is a
costly process (schatzmann et al., 2006).

to deal with the above issues, this paper de-
scribes an on-line active learning method in which
users are asked to provide feedback on whether the
dialogue was successful or not. however, active
learning is used to limit requests for feedback to
only those cases where the feedback would be use-
ful, and also a noise model is introduced to com-
pensate for cases where the user feedback is inac-
curate. a gaussian process classi   cation (gpc)
model is utilised to robustly model the uncertainty
presented by the noisy user feedback. since gpc
operates on a    xed-length observation space and
dialogues are of variable-length, a recurrent neu-
ral network (id56)-based embedding function is
used to provide    xed-length dialogue representa-
tions. in essence, the proposed method learns a di-
alogue policy and a reward estimator on-line from
scratch, and is directly applicable to real-world ap-
plications.

the rest of the paper is organised as follows.

the next section gives an overview of related
work. the proposed framework is then described
in   3. this consists of the policy learning al-
gorithm, the creation of the dialogue embedding
function and the active reward model trained from
real user ratings. in   4, the proposed approach is
evaluated in the context of an application provid-
ing restaurant information in cambridge, uk. we
   rst give an in-depth analysis of the dialogue em-
bedding space. the results of the active reward
model when it is trained together with a dialogue
policy on-line with real users are then presented.
finally, our conclusions are presented in   5.

2 related work

dialogue evaluation has been an active research
area since late 90s. walker et al. (1997) proposed
the paradise framework, where a linear func-
tion of task completion and various dialogue fea-
tures such as dialogue duration were used to in-
fer user satisfaction. this measure was later used
as a reward function for learning a dialogue pol-
icy (rieser and lemon, 2011). however, as noted,
task completion is rarely available when the sys-
tem is interacting with real users and also concerns
have been raised regarding the theoretical validity
of the model (larsen, 2003).

several approaches have been adopted for learn-
ing a dialogue reward model given a corpus of an-
notated dialogues. yang et al. (2012) used col-
laborative    ltering to infer user preferences. the
use of reward shaping has also been investigated
in (el asri et al., 2014; su et al., 2015b) to en-
rich the reward function in order to speed up di-
alogue policy learning. also, ultes and minker
(2015) demonstrated that there is a strong correla-
tion between expert   s user satisfaction ratings and
dialogue success. however, all these methods as-
sume the availability of reliable dialogue annota-
tions such as expert ratings, which in practice are
hard to obtain.

one effective way to mitigate the effects of an-
notator error is to obtain multiple ratings for the
same data and several methods have been devel-
oped to guide the annotation process with uncer-
tainty models (dai et al., 2013; lin et al., 2014).
active learning is particularly useful for determin-
ing when an annotation is needed (settles, 2010;
zhang and chaudhuri, 2015). it is often utilised
using bayesian optimisation approaches (brochu
et al., 2010). based on this, daniel et al. (2014)

exploited a pool-based active learning method for
a robotics application. they queried the user for
feedback on the most informative sample collected
so far and showed the effectiveness of this method.
rather than explicitly de   ning a reward func-
tion, inverse rl (irl) aims to recover the un-
derlying reward from demonstrations of good be-
haviour and then learn a policy which maximises
the recovered reward (russell, 1998).
irl was
   rst
introduced to sds in (paek and pierac-
cini, 2008), where the reward was inferred from
human-human dialogues to mimic the behaviour
observed in a corpus. irl has also been studied
in a wizard-of-oz (woz) setting (boularias et al.,
2010; rojas barahona and cerisara, 2014), where
typically a human expert served as the dialogue
manager to select each system reply based on the
speech understanding output at different noise lev-
els. however, this approach is costly and there is
no reason to suppose that a human wizard is acting
optimally, especially at high noise levels.

since humans are better at giving relative judge-
ments than absolute scores, another related line
of research has focused on preference-based ap-
proaches to rl (cheng et al., 2011). in (sugiyama
et al., 2012), users were asked to provide rankings
between pairs of dialogues. however, this is also
costly and does not scale well in real applications.

3 proposed framework

the proposed system framework is depicted in
figure 2. it is divided into three main parts: a dia-
logue policy, a dialogue embedding function, and
an active reward model of user feedback. when
each dialogue ends, a set of turn-level features ft
is extracted and fed into an embedding function   
to obtain a    xed-dimension dialogue representa-
tion d that serves as the input space of the reward
model r. this reward is modelled as a gaussian
process which for every input point provides an es-
timate of task success along with a measure of the
estimate uncertainty. based on this uncertainty, r
decides whether to query the user for feedback or
not. it then returns a reinforcement signal to up-
date the dialogue policy   , which is trained us-
ing the gp-sarsa algorithm (ga  si  c and young,
2014). gp-sarsa also deploys gaussian process
estimation to provide an on-line sample-ef   cient
id23 algorithm capable of boot-
strapping estimates of sparse value functions from
minimal numbers of samples (dialogues). the

quality of each dialogue is de   ned by its cumu-
lative reward, where each dialogue turn incurs a
small negative reward (-1) and the    nal reward of
either 0 or 20 depending on the estimate of task
success are provided by the reward model.

note that the key contribution here is to learn
the noise robust reward model and the dialogue
policy simultaneously on-line, using the user as a
   supervisor   . active learning is not an essential
component of the framework but highly desirable
in practice to minimise the impact of the supervi-
sion burden on users. the use of a pre-trained em-
bedding function is a sub-component of the pro-
posed approach and is trained off-line on corpus
data rather than manually designed here.

3.1 unsupervised dialogue embeddings

in order to model user feedback over dialogues
of varying length, an embedding function is used
to map each dialogue into a    xed-dimensional
continuous-space. the use of embedding func-
tions has recently gained attention especially for
word representations, and has boosted perfor-
mance on several natural
language processing
tasks (mikolov et al., 2013; turian et al., 2010;
levy and goldberg, 2014). embedding has also
been successfully applied to machine translation
(mt) where it enables varying-length phrases to
be mapped to    xed-length vectors using an id56
encoder-decoder (cho et al., 2014). similar to
mt, dialogue embedding enables variable length
sequences of utterances to be mapped into an ap-
propriate    xed-length vector. although embed-
ding is used here to create a    xed-dimension input
space for the gpc-based task success classi   er, it
should be noted that it potentially facilitates a va-
riety of other downstream tasks which depend on
classi   cation or id91.

the model structure of the embedding func-
tion is described on the left of figure 2, where
the episodic turn-level features ft are extracted
from a dialogue and serve as the input features
to the encoder.
in our proposed model, the en-
coder is a bi-directional long short-term mem-
ory network (blstm) (hochreiter and schmid-
huber, 1997; graves et al., 2013). the lstm is a
recurrent neural network (id56) with gated re-
current units introduced to alleviate the vanishing
gradient problem. the blstm encoder takes into
account the sequential information from both di-
rections of the input data, computing the forward

figure 2: schematic of the system framework. the three main system components dialogue policy,
dialogue embedding creation, and reward modelling based on user feedback, are described in   3.

t(cid:88)

1
t

hidden sequences
sequences
tures ft, t = 1, ..., t :

      
h 1:t and the backward hidden
      
h t :1 while iterating over all input fea-
      
ht = lst m (ft,
      
ht = lst m (ft,

      
h t   1)
      
h t+1)

where lst m denotes the activation function.
the dialogue representation d is then calculated
as the average over all hidden sequences:

ht

d =
      
ht] is the concatenation of the

t=1

(1)

      
ht;

where ht = [
two directional hidden sequences.

given the dialogue representation d output by
the encoder, the decoder is a forward lstm that
takes d as its input for each turn t to produce the
sequence of features f(cid:48)

1:t .

the training objective of the encoder-decoder
minimises the mean-square-error (mse) between
the prediction f(cid:48)
1:t and the output f1:t (which is
also the input):

m se =

1
n

||ft     f(cid:48)

t||2

(2)

where n is the number of training dialogues and
||    ||2 denotes the l2-norm. since all the functions
used in the encoder and decoder are differentiable,
stochastic gradient decent (sgd) can be used to
train the model.

n(cid:88)

t(cid:88)

i=1

t=1

the dialogue representations generated by this
lstm-based unsupervised embedding function
are then used as the observations for the reward
model described in the next section 3.2.

3.2 active reward learning
a gaussian process is a bayesian non-parametric
model that can be used for regression or classi   -
cation (rasmussen and williams, 2006). it is par-
ticularly appealing since it can learn from a small
number of observations by exploiting the correla-
tions de   ned by a id81 and it provides a
measure of uncertainty of its estimates. in the con-
text of spoken dialogue systems it has been suc-
cessfully used for rl policy optimisation (ga  si  c
and young, 2014; casanueva et al., 2015) and irl
reward function regression (kim et al., 2014).

here we propose modelling dialogue success as
a gaussian process (gp). this involves estimating
the id203 p(y|d,d) that the task was suc-
cessful given the current dialogue representation
d and the pool d containing previously classi-
   ed dialogues. we pose this as a classi   cation
problem where the rating is a binary observation
y     {   1, 1} that de   nes failure or success. the
observations y are considered to be drawn from
a bernoulli distribution with a success probabil-
ity p(y = 1|d,d). the id203 is related to
a latent function f (d|d) : rdim(d)     r that
is mapped to a unit interval by a probit function
p(y = 1|d,d) =   (f (d|d)), where    denotes the
cumulative density function of the standard gaus-

sian distribution.
the latent function is given a gp prior: f (d)    
gp(m(d), k(d, d(cid:48))), where m(  ) is the mean
function and k(  ,  ) the covariance function (ker-
nel). here the stationary squared exponential ker-
nel kse is used. it is also combined with a white
noise kernel kw n in order to account for the
   noise    in users    ratings:

k(d, d(cid:48)) = p2 exp(   ||d     d(cid:48)||2

2l2

) +   2
n

(3)

where the    rst term denotes kse and the second
term kw n .

the hyper-parameters p, l,   n can be ade-
quately optimised by maximising the marginal
likelihood using a gradient-based method (chen et
al., 2015). since   (  ) is not gaussian, the resulting
posterior id203 p(y = 1|d,d) is analytically
intractable. so instead an approximation method,
expectation propagation (ep), was used (nickisch
and rasmussen, 2008).

querying the user for feedback is costly and
may impact negatively on the user experience.
this impact can be reduced by using active learn-
ing informed by the uncertainty estimate of the gp
model (kapoor et al., 2007). this ensures that user
feedback is only sought when the model is uncer-
tain about its current prediction. for the current
application, an on-line (stream-based) version of
active learning is required.
an illustration of a 1-dimensional example is
shown in figure 3. given the labelled data d, the
predictive posterior mean       and posterior vari-
ance   2    of the latent value f (d   ) for the current di-
alogue representation d    can be calculated. then
a threshold interval [1       ,   ] is set on the pre-
dictive success id203 p(y    = 1|d   ,d) =

  (     /(cid:112)1 +   2   ) to decide whether this dialogue

should be labelled or not. the decision bound-
ary implicitly considers both the posterior mean
as well as the variance.

when deploying this reward model in the pro-
posed framework, a gp with a zero-mean prior for
f is initialised and d = {}. after the dialogue
policy    completes each episode with the user, the
generated dialogue turns are transformed into the
dialogue representation d =   (f1:t ) using the dia-
logue embedding function   . given d, the predic-
tive mean and variance of f (d|d) are determined,
and the reward model decides whether or not it
should seek user feedback based on the threshold
   on   (f (d|d)).
if the model is uncertain, the

figure 3: 1-dimensional example of the proposed
gp active reward learning model.

user   s feedback on the current episode d is used
to update the gp model and to generate the rein-
forcement signal for training the policy   ; other-
wise the predictive success rating from the reward
model is used directly to update the policy. this
process takes place after each dialogue.

4 experimental results

the target application is a live telephone-based
spoken dialogue system providing restaurant in-
formation for the cambridge (uk) area. the do-
main consists of approximately 150 venues each
having 6 slots (attributes) of which 3 can be used
by the system to constrain the search (food-type,
area and price-range) and the remaining 3 are in-
formable properties (phone-number, address and
postcode) available once a required database en-
tity has been found.

the shared core components of the sds com-
mon to all experiments comprise a id48-based
recogniser, a confusion network (cnet) semantic
input decoder (henderson et al., 2012), the buds
belief state tracker (thomson and young, 2010)
that factorises the dialogue state using a dynamic
id110, and a template based natural
language generator to map system semantic ac-
tions into natural language responses to the user.
all policies were trained using the gp-sarsa al-
gorithm and the summary action space of the rl
policy contains 20 actions.
the reward given to each dialogue was set to
20    1success     n, where n is the dialogue turn

number and 1 is the indicator function for dia-
logue success, which is determined by different
methods as described in the following section.
these rewards constitute the reinforcement signal
used for policy learning.

4.1 dialogue representations
the lstm encoder-decoder model described in
  3.1 was used to generate an embedding d for
each dialogue. for each dialogue turn that con-
tains a user   s utterance and a system   s response, a
feature vector f of size 74 was extracted (vandyke
et al., 2015). this vector consists of the concate-
nation of the most likely user intention determined
by the semantic decoder, the distribution over each
concept of interest de   ned in the ontology, a one-
hot encoding of the system   s reply action, and the
turn number normalised by the maximum number
of turns (here 30). this feature vector was used
as the input and the target for the lstm encoder-
decoder model, where the training objective was
to minimise the mse of the reconstruction loss.

the model was implemented using the theano
library (bergstra et al., 2010; bastien et al., 2012).
a corpus consisting of 8565, 1199 and 650 real
user dialogues in the cambridge restaurant do-
main was used for training, validation and test-
ing respectively. this corpus was collected via the
amazon mechanical turk (amt) service, where
paid subjects interacted with the dialogue system.
      
ht in the encoder and the hid-
the sizes of
den layer in the decoder were all 32, resulting in
dim(ht) = dim(d) = 64. sgd per dialogue was
used during id26 to train each model.
in order to prevent over-   tting, early stopping was
applied based on the held-out validation set.

      
ht and

in order to visualise the impact of the embed-
dings, the dialogue representations of all the 650
test dialogues were transformed by the embedding
function in figure 4 and reduced to two dimen-
sions using id167 (van der maaten and hinton,
2008). for each dialogue sample, the shape indi-
cates whether or not the dialogue was successful,
and the colour indicates the length of the dialogue
(maximum 30 turns).

from the    gure we can clearly see the colour
gradient from the top left (shorter dialogues) to
the bottom right (longer dialogues) for the positive
subj labels. this shows that dialogue length was
one of the prominent features in the dialogue rep-
resentation d. it can also be seen that the longer

figure 4: id167 visualisation on the unsupervised
dialogue representation of the real user data in the
cambridge restaurant domain. labels are the sub-
jective ratings from the users.

failed dialogues (more than 15 turns) are located
close to each other, mostly at the bottom right.
on the other hand, there are other failed dialogues
which are spread throughout the cluster. we can
also see that the successful dialogues were on av-
erage shorter than 10 turns, which is consistent
with the claim that users do not engage in longer
dialogues with well-trained task-oriented systems.
this visualisation shows the potential of the un-
supervised dialogue embedding since the trans-
formed dialogue representations appear to be cor-
related with dialogue success in the majority of
cases. for the purpose of gp reward modelling,
this lstm encoder-decoder embedding function
appears therefore to be suitable for extracting an
adequate    xed-dimension dialogue representation.

4.2 dialogue policy learning
given the well-trained dialogue embedding func-
tion, the proposed gp reward model operates on
this input space. the system was implemented us-
ing the gpy library (hensman et al., 2012). given
the predictive success id203 of each newly
seen dialogue, the threshold    for the uncertainty
region was initially set to 1 to encourage label
querying and annealed to 0.85 for the    rst 50 col-
lected dialogues and then set to 0.85 thereafter.

initially, as each new dialogue was added to
the training set, the hyper-parameters that de   ned
the structure of the kernels mentioned in eqn.
3 were optimised to minimise the negative log
marginal likelihood using conjugate gradient as-

figure 5: learning curves showing subjective suc-
cess as a function of the number of training di-
alogues used during on-line policy optimisation.
the on-line gp, subj, off-line id56 and obj=subj
systems are shown as black, yellow, blue, and red
lines. the light-coloured areas are one standard
error intervals.

figure 6: the number of times each system
queries the user for feedback during on-line policy
optimisation as a function of the number of train-
ing dialogues. the orange line represents both the
obj=subj and subj systems, and the black line rep-
resents the on-line gp system.

cent (rasmussen and williams, 2006). to pre-
vent over   tting, after the    rst 40 dialogues, these
hyper-parameters were only re-optimised after ev-
ery batch of 20 dialogues.

to investigate the performance of the proposed
on-line gp policy learning, three other contrast-
ing systems were also tested. note that the hand-
crafted system is not compared since it does not
scale to larger domains and is sensitive to speech
recognition errors. in each case, the only differ-
ence was the method used to compute the reward:

    the obj=subj system which uses prior knowl-
edge of the task to only use training dialogues
for which the user   s subjective assessment of
success is consistent with the objective as-
sessment of success as in (ga  si  c et al., 2013).

    the subj system which directly optimises the
policy using only the user assessment of suc-
cess whether accurate or not.

    the off-line id56 system that uses 1k simu-
lated data and the corresponding obj labels
to train an id56 success estimator as in (su
et al., 2015a).

for the subj system rating, in order to focus
solely on the performance of the policy rather than
other aspects of the system such as the    uency of
the reply sentence, users were asked to rate dia-
logue success by answering the following ques-
tion: did you    nd all the information you were
looking for?

all four of the above systems were trained with
a total of 500 dialogues on-line by users recruited
via the amt service. figure 5 shows the on-
line learning curve of the subjective success rat-
ing when during training. for each system, the
moving average was calculated using a window of
150 dialogues.
in each case, three distinct poli-
cies were trained and the results were averaged to
reduce noise.

as can be seen, all four systems perform better
than 80 % subjective success rate after approxi-
mately 500 training dialogues. the obj=subj sys-
tem is relatively poor compared to the others. this
might be because users often report success even
though the objective evaluation indicates failure.
in such cases, the dialogue is discarded and not
used for training. as a consequence, the obj=subj
system required approximately 700 dialogues in
order to obtain 500 which were useful, whereas
all other systems made use of every dialogue.

to investigate learning behaviour over longer
spans, training for the on-line gp and the subj sys-
tems was extended to 850 dialogues. as can be
seen, performance in both cases is broadly    at.

similar to the conclusions drawn in (ga  si  c et
al., 2011), the subj system suffers from unreliable
user feedback. firstly, as in the obj=subj system,
users forget the full requirements of the task and
in particular, forget to ask for all required infor-
mation. secondly, users give inconsistent feed-
back due to a lack of proper care and attention.
from figure 5 it can be clearly seen that the on-
line gp system consistently performed better than

subj system, presumably, because its noise model
mitigates the effect of inconsistency in user feed-
back. of course, unlike crowd-sourced subjects,
real users might provide more consistent feedback,
but nevertheless, some inconsistency is inevitable
and the noise model offers the needed robustness.
the advantage of the on-line gp system in re-
ducing the number of times that the system re-
quests user feedback (i.e.
the label cost) can be
seen in figure 6. the black curve shows the num-
ber of active learning queries triggered in the on-
line gp system averaged across the three policies.
this system required only 150 user feedback re-
quests to train a robust reward model. on the other
hand, the obj=subj and subj systems require user
feedback for every training dialogue as shown by
the dashed orange line.

of course, the off-line id56 system required no
user feedback at all when training the system on-
line since it had the bene   t of prior access to a
user simulator. its performance during training af-
ter the    rst 300 dialogues was, however, inferior
to the on-line gp system.

4.3 dialogue policy evaluation
in order to compare performance, the averaged re-
sults obtained between 400-500 training dialogues
are shown in the    rst section of table 1 along
with one standard error. for the 400-500 inter-
val, the subj, off-line id56 and on-line gp sys-
tems achieved comparable results without statisti-
cal differences. the results of continuing training
on the subj and on-line gp systems from 500 to
850 training dialogues are also shown. as can be
seen, the on-line gp system was signi   cantly bet-
ter presumably because it is more robust to erro-
neous user feedback compared to the subj system.

4.4 reward model evaluation
the above results verify the effectiveness of the
proposed reward model for policy learning. here
we investigate further the accuracy of the model
in predicting the subjective success rate. an eval-
uation of the on-line gp reward model between 1
and 850 training dialogues is presented in table 2.
since three reward models were learnt each
with 850 dialogues, there were a total of 2550
training dialogues. of these, the models queried
the user for feedback a total of 454 times, leaving
2096 dialogues for which learning relied on the re-
ward model   s prediction. the results shown in the
table are thus the average over 2096 dialogues.

table 1: subjective evaluation of the obj=subj,
off-line id56, subj and on-line gp system during
different stages of on-line policy learning. sub-
jective: user binary rating on dialogue success.
statistical signi   cance was calculated using a two-
tailed students t-test with p-value of 0.05.

dialogues reward model

obj=subj
off-line id56

subj

on-line gp

subj

on-line gp

400-500

500-850

* p < 0.05

subjective (%)
85.0    2.1
89.0    1.8
90.7    1.7
91.7    1.6
87.1    1.0
90.9    0.9*

as can be seen, there was a signi   cant imbal-
ance between success and fail labels since the pol-
icy was improving along with the training dia-
logues. this lowered the recall on failed dialogue
prediction as the model was biased to data with
positive labels. nevertheless, its precision scores
well. on the other hand, the successful dialogues
were accurately predicted by the proposed model.

table 2: statistical evaluation of the prediction of
the on-line gp systems with respect to subj rating.

subj prec. recall f-measure number
fail
suc.
total

204
1892
2096

1.00
0.95
0.96

0.52
1.00
0.95

0.68
0.97
0.95

4.5 example dialogues
the key bene   ts of the on-line gp reward model
compared to other models are its robustness to
noise and ef   cient use of user supervision. since
the four systems compared above differ only in the
design of reward model (learning objective), their
on-line behaviours were broadly similar.

two example dialogues between users and the
on-line gp system are listed in table 3 to illustrate
how the system operates under different noise con-
ditions. the user   s subjective rating and the rat-
ing determined by the on-line gp reward model
are also shown. the labels    n-th asr    and    n-
th sem    indicate the n-th most likely hypotheses
from speech recogniser and semantic decoder re-
spectively.

table 3: example dialogues between on-line users and the proposed on-line gp system.

-low noise condition- (user subjective evaluation: success, on-line gp evaluation: success)
machine
human

: hello, welcome to the cambridge dialogue system, what kind of food would you like?
: [top asr] yeah i want to    nd a restaurant that serves european food.
[top sem] inform(food=european,type=restaurant)
: hotel du vin and bistro is a nice place. it serves european food.
: [top asr] uh what is the address and phone number?
[top sem] request(phone,addr)
: hotel du vin and bistro is a nice place. their address is 15-19 trumpington street and their phone is 01223 227330.
: [top asr] thank you goodbye.
[top sem] bye()
: thank you for using this system.

machine
human

machine
human

machine
human

machine
human

machine
-higher noise condition- (user subjective evaluation: success, on-line gp evaluation: success)
: hello, welcome to the cambridge dialogue system, what kind of food would you like?
machine
human
: [top asr] and i want to    nd a expensive restaurant it should be in the south part of town.
[top sem] inform(area=south,pricerange=expensive,type=restaurant)
: gourmet burger kitchen is a nice place it is in the expensive price range.
: [top asr] and do you have something else?
[top sem] null()
[2nd sem] reqalts()
: did you say you are looking for a venue in the area of south ?
: [top asr] yes.
[top sem] af   rm()
: frankie and benny   s is a nice place, it is in the south part of town and it is in the expensive price range.
: [top asr] okay can i have the uh address phone number called for any.
[top sem] request(addr)
[2nd sem] request(phone,addr)
: frankie and benny   s is a nice place. their address is clifton way and their phone number is 01223 412430.
: [top asr] what is the pub called for free you any?
...
[5th asr] what is the post code for free you any?
[top sem] request(postcode)
: frankie and benny   s is a nice place. their postcode is c.b 1, 7 d.y.
: [top asr] thank you goodbye.
[top sem] bye()
: thank you for using this system.

machine
human

machine
human

machine
human

machine

5 conclusion

in this paper we have proposed an active reward
learning model using gaussian process classi   ca-
tion and an unsupervised neural network-based di-
alogue embedding to enable truly on-line policy
learning in spoken dialogue systems. the sys-
tem enables stable policy optimisation by robustly
modelling the inherent noise in real user feedback
and uses active learning to minimise the number
of feedback requests to the user. we found that the
proposed model achieved ef   cient policy learning
and better performance compared to other state-
of-the-art methods in the cambridge restaurant
domain. a key advantage of this bayesian model
is that its uncertainty estimate allows active learn-
ing and noise handling in a natural way. the unsu-
pervised dialogue embedding function required no
labelled data to train whilst providing a compact
and useful input to the reward predictor. overall,
the techniques developed in this paper enable for
the    rst time a viable approach to on-line learning

in deployed real-world dialogue systems which
does not need a large corpus of manually anno-
tated data or the construction of a user simulator.
consistent with all of our previous work, the re-
ward function studied here is focused primarily on
task success. this may be too simplistic for many
commercial applications and further work will be
needed in conjunction with human interaction ex-
perts to identify and incorporate the extra dimen-
sions of dialogue quality that will be needed to
achieve the highest levels of user satisfaction.

the ministry

acknowledgments
pei-hao su is supported by cambridge trust
and
taiwan.
this research was partly funded by the ep-
src grant ep/m018946/1 open domain
statistical spoken dialogue systems.
the
data used in the experiments is available at
www.repository.cam.ac.uk/handle/1810/256020.

of education,

references
[bastien et al.2012] fr  ed  eric bastien, pascal lamblin,
razvan pascanu, james bergstra, ian j. goodfellow,
arnaud bergeron, nicolas bouchard, and yoshua
bengio. 2012. theano: new features and speed im-
provements. deep learning and unsupervised fea-
ture learning nips workshop.

[bergstra et al.2010] james bergstra, olivier breuleux,
fr  ed  eric bastien, pascal lamblin, razvan pascanu,
guillaume desjardins, joseph turian, david warde-
farley, and yoshua bengio. 2010. theano: a cpu
and gpu math expression compiler. in proceedings
of the python for scienti   c computing conference.

[boularias et al.2010] abdeslam boularias, hamid r
chinaei, and brahim chaib-draa. 2010. learning
the reward model of dialogue pomdps from data. in
nips workshop on machine learning for assistive
techniques.

[brochu et al.2010] eric brochu, vlad m cora, and
nando de freitas. 2010. a tutorial on bayesian
optimization of expensive cost
functions, with
application to active user modeling and hierar-
arxiv preprint
chical
arxiv:1012.2599.

reinforcement

learning.

[casanueva et al.2015] i  nigo casanueva, thomas hain,
heidi christensen, ricard marxer, and phil green.
2015. knowledge transfer between speakers for per-
sonalised dialogue management. in proc of sigdial.

[chen et al.2015] lu chen, pei-hao su, and milica
ga  sic.
2015. hyper-parameter optimisation of
gaussian process id23 for statis-
tical dialogue management. in proc of sigdial.

[cheng et al.2011] weiwei

cheng,

johannes
f  urnkranz, eyke h  ullermeier, and sang-hyeun
park.
preference-based policy iteration:
leveraging preference learning for reinforcement
in machine learning and knowledge
learning.
discovery in databases. springer.

2011.

cho,

[cho et al.2014] kyunghyun

van
merri  enboer caglar gulcehre, dzmitry bahdanau,
fethi bougares holger schwenk, and yoshua
bengio.
2014. learning phrase representations
using id56 encoder   decoder for statistical machine
translation. arxiv preprint arxiv:1406.1078.

bart

[dai et al.2013] peng dai, christopher h lin, daniel s
weld, et al. 2013. pomdp-based control of work-
arti   cial intelligence,
   ows for id104.
202.

[daniel et al.2014] christian daniel, malte viering, jan
metz, oliver kroemer, and jan peters. 2014. active
reward learning. in proc of rss.

[daubigney et al.2014] lucie daubigney, matthieu
geist, senthilkumar chandramohan, and olivier
pietquin. 2014. a comprehensive reinforcement
learning framework for dialogue management

optimisation. journal of selected topics in signal
processing, 6(8).

[el asri et al.2014] layla el asri, romain laroche,
and olivier pietquin. 2014. task completion trans-
fer learning for reward id136. in proc of mlis.

[ga  si  c and young2014] milica ga  si  c and steve young.
2014. gaussian processes for pomdp-based dia-
logue manager optimization. taslp, 22(1):28   40.

[ga  si  c et al.2011] milica ga  si  c, filip jurcicek, blaise.
thomson, kai yu, and steve young. 2011. on-
line policy optimisation of spoken dialogue systems
in ieee
via live interaction with human subjects.
asru.

[ga  si  c et al.2013] milica ga  si  c, catherine breslin,
matthew henderson, dongho kim, martin szum-
mer, blaise thomson, pirros tsiakoulis, and steve j.
young.
2013. on-line policy optimisation of
bayesian spoken dialogue systems via human inter-
action. in proc of icassp.

[graves et al.2013] alax graves, navdeep jaitly, and
2013. hybrid speech
in ieee

abdel-rahman mohamed.
recognition with deep bidirectional lstm.
asru.

[henderson et al.2012] matthew henderson, milica
ga  si  c, blaise thomson, pirros tsiakoulis, kai yu,
and steve young. 2012. discriminative spoken
language understanding using word confusion
networks. in ieee slt.

[hensman et al.2012] james hensman, nicolo fusi, ri-
cardo andrade, nicolas durrande, alan saul, max
zwiessele, and neil d. lawrence.
2012. gpy:
a gaussian process framework in python. http:
//github.com/sheffieldml/gpy.

[hochreiter and schmidhuber1997] sepp hochreiter
and j  urgen schmidhuber. 1997. long short-term
memory. neural computation, 9(8).

[kapoor et al.2007] ashish kapoor, kristen grauman,
raquel urtasun, and trevor darrell. 2007. active
learning with gaussian processes for object catego-
rization. in proc of iccv.

[kim et al.2014] dongho kim, catherine breslin, pir-
ros tsiakoulis, matthew henderson, and steve j
young. 2014.
inverse id23 for
micro-turn management. in ieee slt.

[larsen2003] l.b. larsen. 2003. issues in the evalua-
tion of spoken dialogue systems using objective and
subjective measures. in ieee asru.

[levin and pieraccini1997] esther levin and roberto
pieraccini. 1997. a stochastic model of computer-
human interaction for learning dialogue strategies.
eurospeech.

[levy and goldberg2014] omer levy and yoav gold-
berg. 2014. neural id27 as implicit ma-
trix factorization. in nips.

[lin et al.2014] christopher h lin, daniel s weld,
et al. 2014. to re (label), or not to re (label). in sec-
ond aaai conference on human computation and
id104.

[mikolov et al.2013] tomas mikolov, ilya sutskever,
kai chen, greg s corrado, and jeff dean. 2013.
distributed representations of words and phrases
and their compositionality. in nips.

[nickisch and rasmussen2008] hannes nickisch and
carl edward rasmussen. 2008. approximations for
binary gaussian process classi   cation. jmlr, 9(10).

[paek and pieraccini2008] tim paek and roberto pier-
accini. 2008. automating spoken dialogue manage-
ment design using machine learning: an industry
perspective. speech communication, 50.

[rasmussen and williams2006] carl

mussen and chris williams.
processes for machine learning.

edward ras-
2006. gaussian

[rieser and lemon2011] verena rieser and oliver
lemon. 2011. learning and evaluation of dialogue
strategies for new applications: empirical methods
for optimization from small data sets. computa-
tional linguistics, 37(1).

[rojas barahona and cerisara2014] lina maria ro-
jas barahona and christophe cerisara.
2014.
bayesian inverse id23 for
modeling conversational agents
in a virtual
in conference on intelligent text
environment.
processing and computational linguistics.

[roy et al.2000] nicholas roy, joelle pineau, and se-
bastian thrun. 2000. spoken dialogue management
using probabilistic reasoning. in proc of sigdial.

[russell1998] stuart russell. 1998. learning agents

for uncertain environments. in proc of colt.

[schatzmann et al.2006] jost schatzmann, karl weil-
hammer, matt stuttle, and steve young.
2006.
a survey of statistical user simulation techniques
for reinforcement-learning of dialogue management
the knowledge engineering review,
strategies.
21(02):97   126.

[serban et al.2015] iulian v serban, alessandro sor-
doni, yoshua bengio, aaron courville, and joelle
pineau. 2015. hierarchical neural network gener-
ative models for movie dialogues. arxiv preprint
arxiv:1507.04808.

[settles2010] burr settles. 2010. active learning liter-
ature survey. computer sciences technical report
1648.

[su et al.2015a] pei-hao su, david vandyke, milica
ga  si  c, dongho kim, nikola mrk  si  c, tsung-hsien
wen, and steve young. 2015a. learning from real
users: rating dialogue success with neural networks
for id23 in spoken dialogue sys-
tems. in proc of interspeech.

[su et al.2015b] pei-hao su, david vandyke, milica
ga  si  c, nikola mrk  si  c, tsung-hsien wen, and steve
young. 2015b. reward shaping with recurrent neu-
ral networks for speeding up on-line policy learning
in spoken dialogue systems. in proc of sigdial.

[sugiyama et al.2012] hiroaki sugiyama, toyomi me-
guro, and yasuhiro minami.
2012. preference-
learning based inverse id23 for di-
alog control. in proc of interspeech.

[thomson and young2010] blaise thomson and steve
young. 2010. bayesian update of dialogue state:
a pomdp framework for spoken dialogue systems.
computer speech and language, 24:562   588.

[turian et al.2010] joseph turian, lev ratinov, and
yoshua bengio. 2010. word representations: a sim-
ple and general method for semi-supervised learn-
ing. in proc of acl.

[ultes and minker2015] stefan ultes and wolfgang
minker. 2015. quality-adaptive spoken dialogue
initiative selection and implications on reward mod-
elling. in proc of sigdial.

[van der maaten and hinton2008] laurens van

der
maaten and geoffrey hinton. 2008. visualizing
data using id167. jmlr, 9:85.

[vandyke et al.2015] david vandyke, pei-hao su, mil-
ica ga  si  c, nikola mrk  si  c, tsung-hsien wen, and
steve young. 2015. multi-domain dialogue success
classi   ers for policy training. in ieee asru.

[vinyals and le2015] oriol vinyals and quoc le.
arxiv

2015. a neural conversational model.
preprint arxiv:1506.05869.

[walker et al.1997] marilyn a. walker, diane j. lit-
man, candace a. kamm, and alicia abella. 1997.
paradise: a framework for evaluating spoken di-
alogue agents. in proc of eacl.

[williams and young2007] jason d. williams

and
steve young. 2007. partially observable markov
decision processes for spoken id71.
computer speech and language, 21(2):393   422.

[yang et al.2012] zhaojun yang, g levow, and helen
meng. 2012. predicting user satisfaction in spoken
dialog system evaluation with collaborative    ltering.
ieee journal of selected topics in signal process-
ing, 6(99):971   981.

[young et al.2013] steve young, milica ga  sic, blaise
thomson, and jason williams. 2013. pomdp-based
statistical spoken dialogue systems: a review.
in
proc of ieee, volume 99, pages 1   20.

[zhang and chaudhuri2015] chicheng zhang and ka-
2015. active learning from

malika chaudhuri.
weak and strong labelers. corr, abs/1510.02847.

[zhao et al.2011] liyue zhao, gita sukthankar, and
rahul sukthankar. 2011.
incremental relabeling
for active learning with noisy crowdsourced anno-
tations. in proc of passat and proc of socialcom.

