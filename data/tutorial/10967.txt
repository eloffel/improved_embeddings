context-aware id86 with recurrent neural networks

jian tang1, yifan yang2, sam carton3, ming zhang2, and qiaozhu mei3
{tangjianpku,yang1fan2}@gmail.com, mzhang cs@pku.edu.cn, {scarton, qmei}@umich.edu

1 microsoft research, 2 peking university, 3 university of michigan

6
1
0
2

 

v
o
n
9
2

 

 
 
]
l
c
.
s
c
[
 
 

1
v
0
0
9
9
0

.

1
1
6
1
:
v
i
x
r
a

abstract

this paper studied generating natural languages at particu-
lar contexts or situations. we proposed two novel approaches
which encode the contexts into a continuous semantic repre-
sentation and then decode the semantic representation into
text sequences with recurrent neural networks. during de-
coding, the context information are attended through a gat-
ing mechanism, addressing the problem of long-range depen-
dency caused by lengthy sequences. we evaluate the effec-
tiveness of the proposed approaches on user review data, in
which rich contexts are available and two informative con-
texts, sentiments and products, are selected for evaluation.
experiments show that the fake reviews generated by our ap-
proaches are very natural. results of fake review detection
with human judges show that more than 50% of the fake re-
views are misclassi   ed as the real reviews, and more than
90% are misclassi   ed by existing state-of-the-art fake review
detection algorithm.

introduction

id86 is potentially useful in a va-
riety of applications such as natural language understand-
ing (graves 2013), response generation in dialogue sys-
tems (wen et al. 2015a; 2015b; sordoni et al. 2015), text
summarization (rush, chopra, and weston 2015), machine
translation (bahdanau, cho, and bengio 2014) and image
caption (xu et al. 2015). traditional approaches usually gen-
erate languages according to some rules or templates de-
signed by humans (cheyer and guzzoni 2014), which are
speci   c for some tasks and domains and dif   cult to general-
ize to other tasks and domains. besides, the languages gener-
ated according to these approaches are very similar, lacking
the large variations of human languages. therefore, it is a
long shot of the community to develop automatic approaches
that learn from data and generate languages as diverse as hu-
man languages.

recently, recurrent neural networks (id56s) have been
proved to very effective in natural
language genera-
tion (graves 2013; sutskever, martens, and hinton 2011;
bowman et al. 2015). comparing to the traditional ap-
proaches, id56s directly model the generation process of
copyright c(cid:13) 2017, association for the advancement of arti   cial
intelligence (www.aaai.org). all rights reserved.

text sequences, and the generating function can be auto-
matically learned from a large amount of text data, pro-
viding an end-to-end solution. though traditional id56s
suffer from the problem of gradient vanishing or explod-
ing, the long-short term memory (lstm) (hochreiter and
schmidhuber 1997) unit effectively addresses this prob-
lem and is able to capture the long-range dependency in
natural languages. id56s with lstm have shown very
promising results on various data sets with different struc-
tures including wikipedia articles (graves 2013),
linux
source codes (karpathy, johnson, and li 2015), scien-
ti   c papers (karpathy, johnson, and li 2015), nsf ab-
stracts (karpathy, johnson, and li 2015).

most of existing work focus on natural language genera-
tion only with their textual content while ignoring their con-
textual information. however, in reality natural languages
are usually generated at/with particular contexts, e.g., time,
locations, emotions or sentiments. therefore, in this pa-
per we study context-aware id86.
our goal is to generate not only semantically and syntac-
tically coherent sentences, but also sentences that are rea-
sonable at particular contexts. indeed, contexts have been
proved to be very useful for various natural language pro-
cessing tasks such as topic extraction (mei et al. 2006),
text classi   cation (cao et al. 2009) and id38-
mikolov2012context.

we proposed two novel approaches for context-aware nat-
ural language generation, which map a set of contexts to text
sequences. our    rst model c2s encodes a set of contexts
into a continuous representation and then decode the repre-
sentation into a text sequence through a recurrent neural net-
work. the c2s model is able to generate semantically and
syntactically very coherent sentences. however, one limita-
tion is that when the sequences become very long, the in-
formation from the contexts may not be able to propagate
to the words in distant positions. an intuitive approach to
address this is to build the direct dependency between the
contexts and the words in the sequences, allowing the infor-
mation jump from the contexts to the words. however, not
all the words may depend on the contexts, some of which
may only depend on their preceding words. to resolve this,
a gating mechanism is introduced to control when the infor-
mation from the contexts are accessed. this is our second
model: gated contexts to sequences (gc2s).

figure 1: examples of reviews generated by our approach. only the sentiment rating and product id are fed to the algorithm.

we evaluate our approaches on the user reviews from
amazon and tripadvisor, where rich contexts are available.
two informative contexts are selected: sentiment rating and
product id. fig. presents two examples of reviews generated
by gc2s, which are very dif   cult to tell from reviews writ-
ten by real users. we choose the task of fake review detection
to evaluate the effectiveness of our approach. experimental
results show that more than 50% of the fake reviews gener-
ated by our approach are misclassi   ed as real reviews with
human judges and more than 90% are misclassi   ed by the
existing state-of-the-art fake review detection algorithm.

related work

the approaches of natural
language generation can be
roughly classi   ed into two categories: the classical rule-
based or template-based approaches, and recent approaches
with recurrent neural networks, which automatically learn
the natural language generator from the data. classical ap-
proaches usually de   ne some rules or templates (cheyer and
guzzoni 2014) by humans, which are very brittle and hard
to generalize to different tasks and domains. though there
are some recent approaches aiming to learn the template
structures from large amounts of corpus (oh and rudnicky
2000), the training data is very expensive to obtain and the    -
nal generation process still requires additional human hand-
crafted features.

comparing to the traditional rule-based approaches, the
recurrent neural networks based approaches does not rely
on any human handcrafted features and provide an end-to-
end solution. our approach is also built on recurrent neural
networks. (graves 2013) studied sequence generation, in-
cluding text, using recurrent neural networks (id56) with
long-short term memory unit. (sutskever, martens, and hin-
ton 2011) proposed a multiplicative id56 (mid56) for text
prediction and generation, in which different transformation
functions between the hidden states are used for different
input characters. (bowman et al. 2015) investigated generat-
ing sentences from continuous semantic spaces with a vari-
ational auto-encoder, in which id56 is used for both the en-
coder and the encoder. these work have shown that id56s
are very effective for text generation on various data sets of
different structures. however, all these work study natural
language generation without contexts.

there are some recent work that investigate language
modeling with context information. (mikolov and zweig
2012) studied id38 by adding the topical fea-
tures of preceding words as contexts. (wang and cho 2015)
exploited the preceding words in larger windows for lan-

guage modeling. these work focus on the task of language
modeling and the preceding words are used as the contexts
information while our task focuses on natural language gen-
eration and external contexts are studied. there are also
some related work of response generation in conversation
systems (sordoni et al. 2015; wen et al. 2015b), in which
the conversation history are treated as contexts. comparing
to their work, our solutions are more general, application for
a variety of context while their solution are speci   cally de-
signed for contexts with speci   c structures.

problem de   nition

natural languages are usually associated with rich context
information, e.g., time, location, which provide clues on how
the natural languages are generated. in this paper, we study
context-aware id86. given the con-
text clues, we want to generate the corresponding natural
languages. we    rst formally de   ne the contexts as follows:
de   nition 1 (contexts.) the contexts of natural languages
refer to the situations they are generated. each context is
de   ned as a high-dimensional vector.

the contexts of natural languages can be either discrete
or continuous features. for example, the context can be a
speci   c user or location; it can also be a continuous feature
vectors generated from other sources. for discrete features,
the context is usually represented with one-hot representa-
tions. formally, we formulate our problem as follows:
de   nition 2 (context-aware natural
language genera-
tion) given a set of contexts c = {(cid:126)ci}i=1,...,k, in which
k is the total number of context types, our goal is to gener-
ate a sequence of words w1, w2, . . . , wn that are appropriate
at the given contexts

in this paper, we take the user reviews as an example,
where there exist abundant context information, e.g., user,
time, sentiment, product. however, our proposed approaches
are also general for other contextual data. next, we intro-
duce our approach for context-aware natural language gen-
eration.

model

in this section, we introduce our proposed approaches for
generating natural language at particular contexts. we    rst
introduce the recurrent neural networks (id56), which are
very effective models for text generation, and then introduce
our proposed approaches, which map a set of contexts to a
text sequence.

(a) id56 for sequence modeling

(b) c2s: contexts to sequences

(c) gc2s: gated contexts to sequences

figure 2: (a): classical id56 model for text modeling; (b): our    rst approach in which multi-layer neural networks are encoders
and id56 are decoders; (c): our second approach which adds skip-connections from contexts to words, controlled by a gating
function.

recurrent neural network
recurrent neural network (id56) models the generative pro-
cess of sequence data, which summarizes the information
into a hidden state ( a continuous representation) and then
generate a new sample according to a id203 distribu-
tion speci   ed by the hidden state. speci   cally, the hidden
state ht from the sequence x1, x2, . . . , xt is recursively up-
dated as:

ht = f (ht   1, xt),

(1)
where f (  ,  ) is usually a nonlinear transformation, e.g., ht =
tanh(u ht   1 + v xt)) (u, v are transformation matrices).
the hidden state ht summarizes the information of the entire
sequences x1, x2, . . . , xt, and the id203 of generating
next words p(xt+1|x   t) is de   ned as

p(xt+1|x   t) = p(xt+1|ht)     exp(ot

xt+1

ht),

(2)

where oxt+1 is the low-dimensional continuous representa-
tion of word xt+1.

the overall id203 of a sequence (cid:126)x = x1, x2, . . . , xt

is calculated as follows:

p((cid:126)x) =

p(xt|ht   1),

(3)

t(cid:89)

t=1

training id56 can be done through maximizing the joint
id203 p(x) de   ned by eqn. (3) and optimized through
back-propagation. however, training id56 with traditional
state transition unit ht = tanh(u xt + v ht   1) suffers from
the problem of gradient vanishing or exploding, which is
caused by the product of multiple non-linear transforming
functions. (hochreiter and schmidhuber 1997) effectively
addresses this problem through the long-short term mem-
ory (lstm) unit. the core idea of lstm is introducing the
memory state and multiple gating functions to control the
information written to the memory sate, reading from the
memory state, and removed (or forgotten) from the memory
state. speci   cally, the detailed updating equations are listed
below:

zt = tanh(wzxt + whht   1 + bz)
it =   (wixt + whht   1 + bi)
ft =   (wf xt + whht   1 + bf )
ct = ft (cid:12) ct   1 + it (cid:12) zt
ot =   (woxt + whht   1 + bo)
ht = ot (cid:12) tanh(ct),

where ct is the memory state, zt is the module that trans-
form information from input space xt to the memory space,
ht is the information read from the memory state, it, ft, ot
are the input, forget, and output gates respectively. it con-
trols the information from input zt to the memory state, ft
controls the information in the memory state to be forgotten,
ot controls the information read from the memory state. the
memory state ct is updated through a linear combination of
the input    ltered by the input gate and the previous memory
state    ltered by the forget gate.
c2s: contexts to sequences
id56 effectively the joint id203 of natural languages
p((cid:126)x). as mentioned previously, natural languages are usu-
ally generated at particular contexts.therefore, instead of
modeling the id203 of observing a text sequence p((cid:126)x),
we are more interested in the id203 of observing x
under some contexts c, i.e., the id155
p((cid:126)x|c). in this part, we introduce two generative models for
modeling the id155 p((cid:126)x|c) based on recur-
rent neural networks.
encoder. our framework is built on the encoder-decoder
framework (cho et al. 2014). the essential idea is to encode
the contexts information into a continuous semantic repre-
sentation, and then decode the semantic representation into
a text sequence. we    rst introduce how to encode the con-
texts of natural languages into a semantic representation. we
represent the contexts as a set c = {(cid:126)ci}i=1,k, where (cid:126)ci is
a type of context, k is the number of context types. take
the review as an example, each (cid:126)ci is a sentiment rating score
(ranging from 1 to 5), a product id or a user id. for discrete
contexts, each (cid:126)ci is a one hot-vector (cid:126)ci. the embedding of
each context (cid:126)ci can be obtained through:

(cid:126)(cid:126)ci,

(cid:126)(cid:126)ei = ei

(5)
where ei     rd  |ki|, ki is the number of different con-
text values of type i, and d is the dimension of context em-
bedding. once the embeddings of different contexts are ob-
tained, they are concatenated into a long vector and followed
by a non-linear transformation, formulated as follows:

(4)

hc = tanh(w [(cid:126)e1, (cid:126)e2, . . . , (cid:126)e|c|]) + b),

(6)
where w     rkd  n , n is the size of hidden state of recur-
rent neural networks in the decoder. by eqn. (5) and eqn. 6,
we are able to encoder the contexts into a semantic represen-
tation. next we introduce how to decode it a text sequence.

decoder. we introduce two types of decoders. the    rst one
is the vanilla recurrent neural networks with lstm unit,
and the initiate state of the id56 is set as the context em-
bedding hc. we call this approach as c2s, and the whole
encoder-decoder framework is presented in 2(b). the c2s
have shown very promising results in the experiments. how-
ever, one limitation of the approach is that when the se-
quences become very long, the information from the con-
texts may not be able to propagate to the distant words. to
resolve this, a natural solution would be to directly build
the dependency between the contexts hc and each word,
i.e., add the skip-connections between the contexts and the
words. by doing this, when predicting the next word xt+1, it
not only depends on the current hidden state ht, but also de-
pends on the context representation hc. to combine the two
sources of information, a simple way would be to take their
summation or concatenate them. here we use the way of
taking their summation. however, simply summing the two
representations which treats the two sources of information
equally may be problematic as some words may depend on
the context or others may not. therefore, it would be a desir-
able to    gure out an approach when the context information
are required.

we achieve this through the gating mechanism. we intro-
duce a gating function which depends on the current hidden
state ht:
(7)
where v     rn  n ,   (  ) is the sigmoid function. the prob-
ability of next word p(xt+1|x   t, c) will be calculated as
follows:

mt =   (v ht + b),

xt+1

(ht + mt (cid:12) hc)),

p(xt+1|x   t, c)     exp(ot

(8)
where (cid:12) is the elementwise product. we call this model
gc2s, and the whole framework is presented in 2(c).
generation. once the models are trained, give a set of con-
texts, we can generate natural languages based on them.
there are usually two types of approaches for natural lan-
guage generation: id125 (bahdanau, cho, and bengio
2014), which is widely used in id4,
and random sample (graves 2013). in our experiments, we
tried both approaches. we    nd that the samples generated by
the id125 are usually very trivial without much varia-
tion. therefore, we adopt the approach of random sampling.
during the sampling process, instead of using the standard
softmax function, we also tried different values of tempera-
tures. high temperatures will generate more diverse samples
but making more mistakes while small temperatures tend to
generate more conservative and con   dent samples. in the ex-
periments, we empirically set the temperatures as 0.7.

experiments

in this section, we evaluate the effectiveness of the approach
c2s and gc2s with the user review data. different tasks are
evaluated including id38, fake review detec-
tion with human judges or existing state-of-the-art fake re-
view detection algorithm, sentiment classi   cation on both
the real and fake reviews. we    rst introduce the data sets to
be used.

table 1: statistics of the data sets
train

valid

test

electronic

name
book

movie
hotel

5,800,000
3,200,000
1,500,000
230,000

293,050
180,708
85,195
12,912

293,055
180,709
85,200
12,914

median.len max len vocab.
20,000
20,000
20,000
20,000

100
100
100
300

22
35
31
87

data sets
we choose the user review data as it contains rich context
information, e.g., users, time, sentiments, products. we se-
lect the sentiment ratings (ranging from 1 to 5) and prod-
uct ids as the context information, which we believe are the
most important factors that affect the review content. we use
data from two websites: amazon1 and tripadvisor2 are used.
for the amazon data, we select three most popular domains
including book, electronic and movie; for the tripadvisor
data, it is about the hotel domain. we select the most popu-
lar 20, 000 words as the vocabulary, and reviews containing
unknown words, with length more than 100 words in the
amazon data and more than 300 words in the tripadvisor
data are all removed. the whole data are split into train, val-
idation, test data according to the ratio 18:1:1. the statistics
of the    nal data sets are summarized into table 1.
training details. all the models on trained on a single
gpu. the batch size is set as 128. the weights are randomly
initialized with the uniform distribution (   0.1, 0.1), and the
biases are initialized with 0. the initial learning rate is set
as 1, and the learning rate is halved if the perplexity of the
current epoch on the validation data is not less than the last
epoch. the gradient is clipped if the norm is larger than 5.
dropout is used from the input to hidden layer and from the
hidden layer to output layer in the recurrent neural networks.
different values of hidden size is tried, and the results show
that the larger, the better. due to the limitation of gpu mem-
ory, we use 512 by default. for the number of layers of id56,
one layer is used by default as increasing the number of lay-
ers does not yield signi   cantly better samples.

id38
we start with the task of id38. table 2 com-
pares the performance of id38 with the ap-
proach id56, c2s and gc2s. first, both the c2s and gc2s
with either the sentiment context, product context or their
combination outperform the vanilla id56 without context
information. this shows that contextual information are in-
deed helpful for natural language understanding and predic-
tion. second, the product context seems to be more infor-
mative than the sentiment context for id38,
no matter with the c2s or gc2s model. this may be that
there are many words in the reviews that are relevant to the
product information. comparing the c2s and gc2s model,
the gc2s model consistently outperforms the c2s model no
matter which contexts are used. as explained previously,
this is because in the c2s model, the context information

1available

at http://jmcauley.ucsd.edu/data/

amazon/links.html

2available

at http://www.cs.cmu.edu/  jiweil/

html/hotel-review.html

table 2: results of id38 measured by perplexity (p: product, s: sentiment).

domain
book

electronic

movie
hotel

id56 c2s(p) c2s(s) c2s(p+s)
27.5
27.4
28.8
23.6

27.1
26.2
27.2
23.2

27.2
27.3
28.2
23.4

26.6
25.8
26.9
23.1

gc2s(p)

25.2
24.4
25.3
21.3

gc2s(s)

25.8
25.6
27.1
22.4

gc2s(p+s)

24.9
24.1
24.8
21.2

figure 3: words with the largest gating values in the gc2s model on test data. most of the words are product or sentiment
related.

ments, we only use gc2s.
fake review detection
to further evaluate the effectiveness of the gc2s model for
id86, we choose the task of fake re-
view detection, which aims to classify whether the reviews
are written by real users or generated by the gc2s model.
real reviews are treated as positive, and fake reviews are
treated as negative. for the evaluation data, we randomly se-
lect some products which have at least two real reviews for
each rating score in the amazon data and one review for each
rating score in the tripadvisor data. for each real review, a
fake review is generated with gc2s according to its contexts.
table 3 summarizes the number of reviews in each domain.
table 3: summary of the evaluation data for fake review de-
tection.

#products

#reviews/rating

domain
book

electronic

movie
hotel

74
100
100
55

2
2
2
1

total
740
1,000
1,000
275

table 4: results of fake review classi   cation with human
judgments (positive: real reviews, negative: fake reviews).
in all the domains, more than 50% of the fake reviews are
misclassi   ed.

domain
book

electronic

movie
hotel
overall

tp
73.9
77.2
79.7
85.2
77.9

fn
26.1
22.8
20.3
14.8
22.1

tn
47.1
43.5
46.1
48.5
47.5

fp
52.9
56.5
53.9
51.5
52.5

tp: true positive, fp: false positive, tn: true negative, fn: false negative

human evaluation. we use the amazon mechanical turk
to evaluate whether the reviews are fake not. we divide all

figure 4: results of id38 on different lengths
of reviews. the superiority of gc2s over c2s increases as
the lengths of reviews increase.

may not be able to affect the words that are far away from the
beginning of the sequences while the gc2s effectively ad-
dresses this through adding the direct dependency between
the contexts and the words in the sequences.

to further verify this, we compare the results of c2s and
gc2s on different lengths of reviews. fig. 4 presents the re-
sults. we can see that as the lengths of the reviews increase,
the gc2s model outperforms the c2s model more signif-
icantly, showing its effectiveness for modeling lengthy re-
views.

fig. 3 presents several examples on the test data showing
that which words are affected by the contexts. we mark the
words with the largest gating values (the average of the gat-
ing vector is compared here). we can see that most of the
words strongly affected by the contexts are words related to
the products or sentiments.

overall, we can see that the gc2s model is indeed more
effective than c2s. therefore, in all the following experi-

llllperplexitylength21     4041     6061     8081     1002224262830lgc2sc2sid56the data into different batches. each batch contains twenty
reviews about the same product. we show the urls of the
products, the sentiment rating and the review content to
users to ask the turkers to judge whether the reviews are
written by real users or not. to control the quality of the
results, some    gotcha    questions are inserted in the middle
of the list of reviews. only the results judged by users who
answer the    gotcha    questions correctly are kept. the kappa
score is .... we summarize the    nal results into table 4.

we can see that in all the domains, more than 50% of the
fake reviews generated by the gc2s model are misclassi   ed
by the turkers, and around 80% of the real reviews are cor-
rectly classi   ed. this shows that the reviews generated by
the gc2s model are indeed very natural.

figure 5: results of human evaluation w.r.t. different lengths
of reviews. lengthy fake reviews are more likely to be de-
tected by real users.

as more samples are generated by the id56, more mis-
takes are likely to make by the model. therefore, we want to
see how the results of human evaluate change as the lengths
of the reviews increase. fig. 5 presents the human evaluation
results w.r.t different lengths of reviews. we can see that for
both the lengthy fake and real reviews, fewer percentages
are misclassi   ed by human judges. however, we can see that
even for the fake reviews with more than 150 words, around
40% of them are still misclassi   ed by the human judges.

table 5: results of fake review detection in tripadvisor with
the approach in (ott et al. 2011). more than 90% fake re-
views are misclassi   ed.

feature

tp
unigram+lr 88.8
bigram+lr
87.3

fn
11.2
12.7

tn
8.3
9.1

fp
91.7
90.9

lr: id28 classi   er

automatic classi   cation. another way to evaluate the ef-
fectiveness of our approach is to see how well existing state-
of-the-art fake review detection algorithm performs on our
generated fake reviews. we adopt the approach in (ott et al.
2011), which trains a classi   er with 800 real reviews from
tripadvisor and 800 fake reviews written by the amazon
mechanical turkers3. here we use the unigram and bigram
features for classi   cation, with which the results are very

3the training data is available at http://myleott.com/

op_spam/.

close to the best results according to (ott et al. 2011). ta-
ble (?) summarizes the results. we can see that more than
90% of the fake reviews generated by the gc2s model are
misclassi   ed as the real reviews by the classi   er.

table 6: fine granularity sentiment classi   cation. results on
the true and fake reviews are very close.

domain
book

electronic

movie

hotel

data
true
fake
true
fake
true
fake
true
fake

precision recall
0.452
0.501
0.451
0.461
0.443
0.472
0.504
0.361

0.533
0.574
0.450
0.450
0.536
0.593
0.562
0.575

f1
0.480
0.529
0.410
0.419
0.474
0.507
0.527
0.396

table 7: binary sentiment classi   cation. results on the true
and fake reviews are very close.

domain
book

electronic

movie

hotel

data
true
fake
true
fake
true
fake
true
fake

precision recall
0.982
0.994
0.953
0.993
0.983
0.996
0.988
1.000

0.963
0.971
0.781
0.761
0.973
0.973
0.985
0.947

f1
0.972
0.982
0.858
0.861
0.978
0.984
0.987
0.973

sentiment classi   cation
the above results show that given particular contexts, the
gc2s model is able to generate very natural reviews that
are indistinguishable from real reviews. but how well the
generated reviews re   ect the context information, e.g., how
well the generated reviews express the sentiment polarity?
therefore, in this part we compare the results of sentiment
classi   cation on both the fake and real reviews. two types of
sentiment classi   cation are conducted:    ner granularity, i.e.,
sentiments with    ve different rating scores, and binary clas-
si   cation, in which reviews with 4 and 5 ratings are treated
as positive and reviews with 1 and 2 ratings are treated as
negative.

to conduct the classi   cation, we randomly sample one
100,000 real reviews from the training data for training the
sentiment classi   er. as for the evaluation data, a fake review
is generated for each test data according to its contexts. ta-
ble 6 and 7 summarize the results of    ne granularity and
binary sentiment classi   cation respectively. we can see that
the results on the real and fake reviews are very close to each
other. on some domains (e.g., book and movie), the results
on the fake reviews are even better than the real reviews,
showing that the reviews generated by the gc2s model ac-
curately re   ect the sentiment polarity.

finally, we present some examples of fake reviews gen-
erated by the gc2s model (table 8). we can see that the
generated reviews are very natural, which are grammatically
correct, accurately re   ect the sentiment polarity and product
information.

percentagelength0   2425   3435   4445   5960   99100   3000.10.30.50.7false negativesfalse positivestable 8: examples of fake reviews generated by the gc2s model.

domain
movie

product
   frozen   

rating

1

electronic

   leather case
cover for ipad
mini   

3

5

1

3

5

review

i love disney movies but this one was not at all what i expected . the
story line was so predictable , it was dumb .
i liked the movie but it didn   t hold my attention as much as i expected .
they just don   t make movies like this anymore .
my son loves this movie and it is a good family movie . i would re-
comend it for anyone who likes to watch movies with kids .
i bought this case for my ipad 3 . it was not as pictured and it was too
small and the ipad mini was not secured inside it , so i returned it .
the case is good for the price , but seems to be of very thin plastic and
not well made . i use the stylus for reading . i would recommend it if
you have a mini ipad .
the cover is very good and it    ts the ipad mini perfectly and the color is
exactly what i was looking for .

conclusion

this paper studied context-aware natural language genera-
tion. we proposed two approaches, c2s and gc2s, which
encode the contexts into semantic representations and then
decode the representations into text sequences. the gc2s
model signi   cantly outperforms the c2s model as it adds
skip-connections between the context representations and
the words in the sequences, allowing the information from
the contexts to be able to directly affect the generation of
words. we evaluated our approaches on the user reviews
data. experimental results show that more than 50% of the
fake reviews generated by our approach are misclassi   ed by
human judges, and more than 90% of the reviews are mis-
classi   ed by existing fake review detection algorithm.

in the future, we plan to integrate more context informa-
tion, e.g., the user, the detailed descriptions of the products,
the product prices, into our approaches and also evaluate our
approaches in other scenarios, e.g., generating the titles of
scienti   c papers based on the author, venue, and time in-
formation. it may be also bene   cial to improve our model
through the attention mechanical (bahdanau, cho, and ben-
gio 2014), i.e., attending to different types of contexts when
generating words in different positions.

references

bahdanau, d.; cho, k.; and bengio, y. 2014. neural ma-
chine translation by jointly learning to align and translate.
arxiv preprint arxiv:1409.0473.
bowman, s. r.; vilnis, l.; vinyals, o.; dai, a. m.; jozefow-
icz, r.; and bengio, s. 2015. generating sentences from a
continuous space. arxiv preprint arxiv:1511.06349.
cao, h.; hu, d. h.; shen, d.; jiang, d.; sun, j.-t.; chen, e.;
and yang, q. 2009. context-aware query classi   cation. in
proceedings of the 32nd international acm sigir confer-
ence on research and development in information retrieval,
3   10. acm.
cheyer, a., and guzzoni, d. 2014. method and apparatus
for building an intelligent automated assistant. us patent
8,677,377.
cho, k.; van merri  enboer, b.; gulcehre, c.; bahdanau, d.;
bougares, f.; schwenk, h.; and bengio, y. 2014. learning

phrase representations using id56 encoder-decoder for statis-
tical machine translation. arxiv preprint arxiv:1406.1078.
graves, a. 2013. generating sequences with recurrent neu-
ral networks. arxiv preprint arxiv:1308.0850.
hochreiter, s., and schmidhuber, j. 1997. long short-term
memory. neural computation 9(8):1735   1780.
karpathy, a.; johnson, j.; and li, f.-f. 2015. visualiz-
ing and understanding recurrent networks. arxiv preprint
arxiv:1506.02078.
mei, q.; liu, c.; su, h.; and zhai, c. 2006. a probabilis-
tic approach to spatiotemporal theme pattern mining on we-
blogs. in proceedings of the 15th international conference
on world wide web, 533   542. acm.
mikolov, t., and zweig, g. 2012. context dependent recur-
rent neural network language model. in slt, 234   239.
oh, a. h., and rudnicky, a. i.
2000. stochastic lan-
guage generation for spoken dialogue systems. in proceed-
ings of the 2000 anlp/naacl workshop on conversational
systems-volume 3, 27   32. association for computational
linguistics.
ott, m.; choi, y.; cardie, c.; and hancock, j. t. 2011. find-
ing deceptive opinion spam by any stretch of the imagina-
tion. in proceedings of the 49th annual meeting of the as-
sociation for computational linguistics: human language
technologies-volume 1, 309   319. association for compu-
tational linguistics.
rush, a. m.; chopra, s.; and weston, j. 2015. a neural at-
tention model for abstractive sentence summarization. arxiv
preprint arxiv:1509.00685.
sordoni, a.; galley, m.; auli, m.; brockett, c.; ji, y.;
mitchell, m.; nie, j.-y.; gao, j.; and dolan, b. 2015. a
neural network approach to context-sensitive generation of
conversational responses. arxiv preprint arxiv:1506.06714.
sutskever, i.; martens, j.; and hinton, g. e. 2011. gener-
in proceedings
ating text with recurrent neural networks.
of the 28th international conference on machine learning
(icml-11), 1017   1024.
wang, t., and cho, k. 2015. larger-context language mod-
elling. arxiv preprint arxiv:1511.03729.
wen, t.-h.; gasic, m.; kim, d.; mrksic, n.; su, p.-h.;
vandyke, d.; and young, s.
stochastic lan-

2015a.

guage generation in dialogue using recurrent neural net-
works with convolutional sentence reranking. arxiv preprint
arxiv:1508.01755.
wen, t.-h.; gasic, m.; mrksic, n.; su, p.-h.; vandyke, d.;
and young, s. 2015b. semantically conditioned lstm-based
id86 for spoken dialogue systems.
arxiv preprint arxiv:1508.01745.
xu, k.; ba, j.; kiros, r.; courville, a.; salakhutdinov, r.;
zemel, r.; and bengio, y. 2015. show, attend and tell:
neural image id134 with visual attention. arxiv
preprint arxiv:1502.03044.
zhang, x., and lapata, m. 2014. chinese poetry generation
with recurrent neural networks. in emnlp, 670   680.

