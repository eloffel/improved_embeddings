7
1
0
2

 

b
e
f
3
1

 

 
 
]
l
c
.
s
c
[
 
 

4
v
6
3
9
4
0

.

2
1
6
1
:
v
i
x
r
a

published as a conference paper at iclr 2017

learning through dialogue interactions
by asking questions

jiwei li, alexander h. miller, sumit chopra, marc   aurelio ranzato, jason weston
facebook ai research,
new york, usa
{jiwel,ahm,spchopra,ranzato,jase}@fb.com

abstract

a good dialogue agent should have the ability to interact with users by both re-
sponding to questions and by asking questions, and importantly to learn from both
types of interaction. in this work, we explore this direction by designing a simu-
lator and a set of synthetic tasks in the movie domain that allow such interactions
between a learner and a teacher. we investigate how a learner can bene   t from
asking questions in both of   ine and online id23 settings, and
demonstrate that the learner improves when asking questions. finally, real exper-
iments with mechanical turk validate the approach. our work represents a    rst
step in developing such end-to-end learned interactive dialogue agents.

1

introduction

when a student is asked a question by a teacher, but is not con   dent about the answer, they may ask
for clari   cation or hints. a good conversational agent (a learner/bot/student) should have this ability
to interact with a dialogue partner (the teacher/user). however, recent efforts have mostly focused
on learning through    xed answers provided in the training set, rather than through interactions. in
that case, when a learner encounters a confusing situation such as an unknown surface form (phrase
or structure), a semantically complicated sentence or an unknown word, the agent will either make
a (usually poor) guess or will redirect the user to other resources (e.g., a search engine, as in siri).
humans, in contrast, can adapt to many situations by asking questions.
we identify three categories of mistakes a learner can make during dialogue1: (1) the learner has
problems understanding the surface form of the text of the dialogue partner, e.g., the phrasing of
a question; (2) the learner has a problem with reasoning, e.g. they fail to retrieve and connect the
relevant knowledge to the question at hand; (3) the learner lacks the knowledge necessary to answer
the question in the    rst place     that is, the knowledge sources the student has access to do not contain
the needed information.
all the situations above can be potentially addressed through interaction with the dialogue partner.
such interactions can be used to learn to perform better in future dialogues. if a human student has
problems understanding a teacher   s question, they might ask the teacher to clarify the question. if
the student doesn   t know where to start, they might ask the teacher to point out which known facts
are most relevant. if the student doesn   t know the information needed at all, they might ask the
teacher to tell them the knowledge they   re missing, writing it down for future use.
in this work, we try to bridge the gap between how a human and an end-to-end machine learning
dialogue agent deal with these situations: our student has to learn how to learn. we hence design
a simulator and a set of synthetic tasks in the movie id53 domain that allow a bot to
interact with a teacher to address the issues described above. using this framework, we explore how
a bot can bene   t from interaction by asking questions in both of   ine supervised settings and online
id23 settings, as well as how to choose when to ask questions in the latter setting.
in both cases, we    nd that the learning system improves through interacting with users.

1this list is not exhaustive; for example, we do not address a failure in the dialogue generation stage.

1

published as a conference paper at iclr 2017

finally, we validate our approach on real data where the teachers are humans using amazon me-
chanical turk, and observe similar results.

2 related work

learning language through interaction and feedback can be traced back to the 1950s, when wittgen-
stein argued that the meaning of words is best understood from their use within given language
games (wittgenstein, 2010). the direction of interactive language learning through language games
has been explored in the early seminal work of winograd (winograd, 1972), and in the recent shrd-
lurn system (wang et al., 2016). in a broader context, the usefulness of feedback and interactions
has been validated in the setting of multiple language learning, such as second language learning
(bassiri, 2011) and learning by students (higgins et al., 2002; latham, 1997; werts et al., 1995).
in the context of dialogue, with the recent popularity of deep learning models, many neural dialogue
systems have been proposed. these include the chit-chat type end-to-end dialogue systems (vinyals
& le, 2015; li et al., 2015; sordoni et al., 2015), which directly generate a response given the
previous history of user utterance. it also include a collection of goal-oriented dialogue systems
(wen et al., 2016; su et al., 2016; bordes & weston, 2016), which complete a certain task such
as booking a ticket or making a reservation at a restaurant. another line of research focuses on
supervised learning for id53 from dialogues (dodge et al., 2015; weston, 2016),
using either a given database of knowledge (bordes et al., 2015; miller et al., 2016) or short stories
(weston et al., 2015). as far as we know, current dialogue systems mostly focus on learning through
   xed supervised signals rather than interacting with users.
our work is closely related to the recent work of weston (2016), which explores the problem of
learning through conducting conversations, where supervision is given naturally in the response dur-
ing the conversation. their work introduced multiple learning schemes from dialogue utterances.
in particular the authors discussed imitation learning, where the agent tries to learn by imitating
the dialogue interactions between a teacher and an expert student; reward-based imitation learn-
ing, which only learns by imitating the dialogue interactions which have have correct answers; and
forward prediction, which learns by predicting the teacher   s feedback to the student   s response.
despite the fact that forward prediction does not uses human-labeled rewards, the authors show that
it yields promising results. however, their work did not fully explore the ability of an agent to learn
via questioning and interaction. our work can be viewed as a natural extension of theirs.

3 the tasks

in this section we describe the dialogue tasks we designed2. they are tailored for the three different
situations described in section 1 that motivate the bot to ask questions: (1) question clari   cation,
in which the bot has problems understanding its dialogue partner   s text; (2) knowledge operation,
in which the bot needs to ask for help to perform reasoning steps over an existing knowledge base;
and (3) knowledge acquisition, in which the bot   s knowledge is incomplete and needs to be    lled.
for our experiments we adapt the wikimovies dataset (weston et al., 2015), which consists of
roughly 100k questions over 75k entities based on questions with answers in the open movie dataset
(omdb). the training/dev/test sets respectively contain 181638 / 9702 / 9698 examples. the accu-
racy metric corresponds to the percentage of times the student gives correct answers to the teacher   s
questions.
each dialogue takes place between a teacher and a bot. in this section we describe how we gener-
ate tasks using a simulator. section 4.2 discusses how we test similar setups with real data using
mechanical turk.
the bot is    rst presented with facts from the omdb kb. this allows us to control the exact knowl-
edge the bot has access to. then, we include several teacher-bot question-answer pairs unrelated to
the question the bot needs to answer, which we call conversation histories3. in order to explore the

2 code and data are available at https://github.com/facebook/memnn/tree/master/askingquestions.
3 these history qa pairs can be viewed as distractions and are used to test the bot   s ability to separate the

wheat from the chaff. for each dialogue, we incorporate 5 extra qa pairs (10 sentences).

2

published as a conference paper at iclr 2017

bene   ts of asking clari   cation questions during a conversation, for each of the three scenarios, our
simulator generated data for two different settings, namely, question-answering (denoted by qa),
and asking-question (denoted by aq). for both qa and aq, the bot needs to give an answer to the
teacher   s original question at the end. the details of the simulator can be found in the appendix.

3.1 question clarification.

in this setting, the bot does not understand the teacher   s question. we focus on a special situation
where the bot does not understand the teacher because of typo/spelling mistakes, as shown in figure
1. we intentionally misspell some words in the questions such as replacing the word    movie    with
   movvie    or    star    with    sttar   .4 to make sure that the bot will have problems understanding the
question, we guarantee that the bot has never encountered the misspellings before   the misspelling-
introducing mechanisms in the training, dev and test sets are different, so the same word will be
misspelled in different ways in different sets. we present two aq tasks: (i) question paraphrase
where the student asks the teacher to use a paraphrase that does not contain spelling mistakes to
clarify the question by asking    what do you mean?   ; and (ii) question veri   cation where the stu-
dent asks the teacher whether the original typo-bearing question corresponds to another question
without the spelling mistakes (e.g.,    do you mean which    lm did tom hanks appear in?   ). the
teacher will give feedback by giving a paraphrase of the original question without spelling mistakes
(e.g.,    i mean which    lm did tom hanks appear in   ) in question paraphrase or positive/negative
feedback in question veri   cation. next the student will give an answer and the teacher will give
positive/negative feedback depending on whether the student   s answer is correct. positive and nega-
tive feedback are variants of    no, that   s incorrect    or    yes, that   s right   5. in these tasks, the bot has
access to all relevant entries in the kb.

3.2 knowledge operation

the bot has access to all the relevant knowledge (facts) but lacks the ability to perform necessary
reasoning operations over them; see figure 2. we focus on a special case where the bot will try
to understand what are the relevant facts. we explore two settings: ask for relevant knowledge
(task 3) where the bot directly asks the teacher to point out the relevant kb fact and knowledge
veri   cation (task 4) where the bot asks whether the teacher   s question is relevant to one particular
kb fact. the teacher will point out the relevant kb fact in the ask for relevant knowledge setting
or give a positive or negative response in the knowledge veri   cation setting. then the bot will give
an answer to the teacher   s original question and the teacher will give feedback on the answer.

3.3 knowledge acquisition

for the tasks in this subsection, the bot has an incomplete kb and there are entities important to
the dialogue missing from it, see figure 3. for example, given the question    which movie did tom
hanks star in?   , the missing part could either be the entity that the teacher is asking about (question
entity for short, which is tom hanks in this example), the relation entity (starred actors), the answer
to the question (forrest gump), or the combination of the three. in all cases, the bot has little chance
of giving the correct answer due to the missing knowledge. it needs to ask the teacher the answer
to acquire the missing knowledge. the teacher will give the answer and then move on to other
questions (captured in the conversational history). they later will come back to reask the question.
at this point, the bot needs to give an answer since the entity is not new any more.
though the correct answer has effectively been included in the earlier part of the dialogue as the
answer to the bot   s question, as we will show later, many of the tasks are not as trivial as they look
when the teacher reasks the question. this is because the bot   s model needs to memorize the missing
entity and then construct the links between the missing entities and known ones. this is akin to the
real world case where a student might make the same mistake again and again even though each
time the teacher corrects them if their answer is wrong. we now detail each task in turn.

4many reasons could lead to the bot not understanding the teacher   s question, e.g., the teacher   s question
has an unknown phrase structure, rather than unknown words. we choose to use spelling mistakes because of
the ease of dataset construction.

5in the datasets we build, there are 6 templates for positive feedback and 6 templates for negative feedback.

3

published as a conference paper at iclr 2017

task 1: question paraphrase (qa)
kb: larry crowne directed by tom hanks
kb: forrest gump starred actors tom hanks,
robin wright, gary sinise
kb: forrest gump directed by robert zemeckis
t/s : conversation history.
t : which movvie did tom hanks sttar in ?
s : larry crowne
t : wrong. (-)

task 2: question veri   cation (qa):
kb: larry crowne directed by tom hanks
kb: forrest gump starred actors tom hanks,
robin wright, gary sinise
kb: forrest gump directed by robert zemeckis
t/s : conversation history.
t : which movvie did tom hanks sttar in ?
s : forrest gump
t : that   s correct. (+)

task 1: question paraphrase (aq):
kb: larry crowne directed by tom hanks
kb: forrest gump starred actors tom hanks,
robin wright, gary sinise
kb: forrest gump directed by robert zemeckis
t/s : conversation history.
t : which movvie did tom hanks sttar in ?
s : what do you mean ?
t : i mean which    lm did tom hanks appear in.
t : which movvie did tom hanks sttar in ?
s : forrest gump
t : that   s correct. (+)
task 2: question veri   cation (aq)
kb: larry crowne directed by tom hanks
kb: forrest gump starred actors tom hanks,
robin wright, gary sinise
kb: forrest gump directed by robert zemeckis
t/s : conversation history.
t : which movvie did tom hanks sttar in ?
s : do you mean which    lm did tom hanks appear in ?
t : that   s correct. (+)
t : which movvie did tom hanks sttar in ?
s : larry crowne
t : sorry, that   s not right. (-)

figure 1: sample dialogues for the two question clari   cation tasks (rows) using both the traditional
qa setting (left column) and aq setting (right column). in each case the same example is given for
simplicity. black text pre   xed by    kb:    denotes kb knowledge that the student has access to.
blue text is spoken by the teacher, denoted by t. (+) denotes a reward of 1 (and 0 otherwise) that
the teacher assigns to the bot. red text denotes responses or questions posed by the bot, denoted
by s. brown denotes typos deliberately introduced by the authors. for the question veri   cation
setting, the student can either ask a correct (pertinent) question (as in this example) or an incorrect
(irrelevant) one. the teacher will give positive or negative feedback based on the correctness of the
student   s question. in our of   ine superised learning experiments, the id203 of asking pertinent
questions and correctly answering the original question from the teacher is set to 0.5. finally, t/s
denotes 5 pairs of questions and answers that are irrelevant to the rest of the conversation.

task 3: ask for relevant knowledge (aq):
kb: larry crowne directed by tom hanks
kb: forrest gump starred actors tom hanks,
robin wright, gary sinise
kb: forrest gump directed by robert zemeckis
t/s : conversation history.
t : which movie did tom hanks star in ?
s : can you give me a hint ?
t : it is related to the kb fact that

forrest gump starred actors tom hanks
robin wright, gary sinise

t : which movie did tom hanks star in ?
s : forrest gump
t : that   s correct. (+)

task 4: knowledge veri   cation (aq):
kb: larry crowne directed by tom hanks
kb: forrest gump starred actors tom hanks,
robin wright, gary sinise
kb: forrest gump directed by robert zemeckis
t/s : conversation history.
t : which movie did tom hanks star in ?
s : does it have something to do with

larry crowne directed by tom hanks?

t : no. that   s wrong. (-)
t : which movie did tom hanks star in ?
s : forrest gump
t : that   s correct. (+)

figure 2: sample dialogues for knowledge operation tasks.

missing question entity: the entity that the teacher is asking about is missing from the knowledge
base. all kb facts containing the question entity will be hidden from the bot. in the example for
task 5 in figure 3, since the teacher   s question contains the entity tom hanks, the kb facts that
contain tom hanks are hidden from the bot.

4

published as a conference paper at iclr 2017

figure 3: different tasks for knowledge acquisition. crossed lines correspond to entries of the
kb whose retrieval is missed.

.
task 5: missing question entity (qa)
kb: larry crowne directed by tom hanks
kb: forrest gump starred actors tom hanks
kb: forrest gump starred actors sally field
kb: forrest gump directed by robert zemeckis
t/s : conversation history.
t : which movie did tom hanks star in ?
s : forrest gump
t : that   s correct. (+)

task 6: missing answer entity (aq)
kb: larry crowne directed by tom hanks
kb: forrest gump starred actors tom hanks
kb: forrest gump starred actors sally field
kb: forrest gump directed by robert zemeckis
t : which movie did tom hanks star in ?
s : i don   t know. what   s the answer?
t : the answer is forrest gump.
t/s : conversation history.
t : which movie did tom hanks star in ?
s : forrest gump
t : that   s correct. (+)
.
task 8: missing triple (aq)
kb: larry crowne directed by tom hanks
kb: forrest gump starred actors tom hanks
kb: forrest gump starred actors sally field
kb: forrest gump directed by robert zemeckis
t : which movie did tom hanks star in ?
s : i don   t know. what   s the answer?
t : the answer is forrest gump.
t/s : conversation history.
t : which movie did tom hanks star in ?
s : forrest gump
t : that   s correct. (+)

task 5: missing question entity (aq)
kb: larry crowne directed by tom hanks
kb: forrest gump starred actors tom hanks,
kb: forrest gump starred actors sally field
kb: forrest gump directed by robert zemeckis
t : which movie did tom hanks star in ?
s : i don   t know. what   s the answer?
t : the answer is forrest gump.
t/s : conversation history.
t : which movie did tom hanks star in ?
s : forrest gump
t : that   s correct. (+)

task 7: missing relation entity (aq)
kb: larry crowne directed by tom hanks
kb: forrest gump starred actors tom hanks,
kb: forrest gump starred actors sally field
kb: forrest gump directed by robert zemeckis
t : which movie did tom hanks star in ?
s : i don   t know. what   s the answer?
t : the answer is forrest gump.
t/s : conversation history.
t : which movie did tom hanks star in ?
s : forrest gump
t : that   s correct. (+)

task 9: missing everything (aq)
kb: larry crowne directed by tom hanks
kb: forrest gump starred actors tom hanks,
kb: forrest gump starred actors sally field
kb: forrest gump directed by robert zemeckis
t : which movie did tom hanks star in ?
s : i don   t know. what   s the answer?
t : the answer is forrest gump.
t/s : conversation history.
t : which movie did tom hanks star in ?
s : forrest gump
t : that   s correct. (+)

missing answer entity: the answer entity to the question is unknown to the bot. all kb facts that
contain the answer entity will be hidden. hence, in task 6 of figure 3, all kb facts containing the
answer entity forrest gump will be hidden from the bot.
missing relation entity: the relation type is unknown to the bot. in task 7 of figure 3, all kb
facts that express the relation starred actors are hidden from the bot.
missing triples: the triple that expresses the relation between the question entity and the answer
in task 8 of figure 3, the triple    forrest gump (question entity)
entity is hidden from the bot.
starred actors tom hanks (answer entity)    will be hidden.
missing everything: the question entity, the relation entity, the answer entity are all missing from
the kb. all kb facts in task 9 of figure 3 will be removed since they either contain the relation
entity (i.e., starred actors), the question entity (i.e., forrest gump) or the answer entity tom hanks.

5

published as a conference paper at iclr 2017

4 train/test regime

we now discuss in detail the regimes we used to train and test our models, which are divided between
evaluation within our simulator and using real data collected via mechanical turk.

4.1 simulator

using our simulator, our objective was twofold. we    rst wanted to validate the usefulness of ask-
ing questions in all the settings described in section 3. second, we wanted to assess the ability of
our student bot to learn when to ask questions. in order to accomplish these two objectives we ex-
plored training our models with our simulator using two methodologies, namely, of   ine supervised
learning and online id23.

4.1.1 offline supervised learning

the motivation behind training our student models in an of   ine supervised setting was primarily
to test the usefulness of the ability to ask questions. the dialogues are generated as described in
the previous section, and the bot   s role is generated with a    xed policy. we chose a policy where
answers to the teacher   s questions are correct answers 50% of the time, and incorrect otherwise, to
add a degree of realism. similarly, in tasks where questions can be irrelevant they are only asked
correctly 50% of the time.6
the of   ine setting explores different combinations of training and testing scenarios, which mimic
different situations in the real world. the aim is to understand when and how observing interactions
between two agents can help the bot improve its performance for different tasks. as a result we
construct training and test sets in three ways across all tasks, resulting in 9 different scenarios per
task, each of which correspond to a real world scenario.
the three training sets we generated are referred to as trainqa, trainaq, and trainmix. trainqa
follows the qa setting discussed in the previous section: the bot never asks questions and only tries
to immediately answer. trainaq follows the aq setting: the student, before answering,    rst always
asks a question in response to the teacher   s original question. trainmix is a combination of the two
where 50% of time the student asks a question and 50% does not.
the three test sets we generated are referred to as testqa, testaq, and testmodelaq. testqa and
testaq are generated similarly to trainqa and trainaq, but using a perfect    xed policy (rather than
50% correct) for evaluation purposes. in the testmodelaq setting the model has to get the form of
the question correct as well. in the question veri   cation and knowledge veri   cation tasks there are
many possible ways of forming the question and some of them are correct     the model has to choose
the right question to ask. e.g. it should ask    does it have something to do with the fact that larry
crowne directed by tom hanks?   rather than    does it have something to do with the fact that forrest
gump directed by robert zemeckis?    when the latter is irrelevant (the candidate list of questions
is generated from the known knowledge base entries with respect to that question). the policy is
trained using either the trainaq or trainmix set, depending on the training scenario. the teacher
will reply to the question, giving positive feedback if the student   s question is correct and no response
and negative feedback otherwise. the student will then give the    nal answer. the difference between
testmodelaq and testaq only exists in the question veri   cation and knowledge veri   cation tasks;
in other tasks there is only one way to ask the question and testmodelaq and testaq are identical.
to summarize, for every task listed in section 3 we train one model for each of the three training
sets (trainqa, trainaq, trainmix) and test each of these models on the three test sets (testqa,
testaq, and testmodelaq), resulting in 9 combinations. for the purpose of notation the train/test
combination is denoted by    trainsetting+testsetting   . for example, trainaq+testqa denotes a
model which is trained using the trainaq dataset and tested on testqa dataset. each combination
has a real world interpretation. for instance, trainaq+testqa would refer to a scenario where
a student can ask the teacher questions during learning but cannot to do so while taking an exam.
similarly, trainqa+testqa describes a stoic teacher that never answers a student   s question at either
learning or examination time. the setting trainqa+testaq corresponds to the case where a lazy

6this only makes sense in tasks like question or knowledge veri   cation. in tasks where the question is
static such as    what do you mean?    there is no way to ask an irrelevant question, and we do not use this policy.

6

published as a conference paper at iclr 2017

student never asks question at learning time but gets anxious during the examination and always
asks a question.

4.1.2 online id23 (rl)

we also explored scenarios where the student learns the ability to decide when to ask a question. in
other words, the student learns how to learn.
although it is in the interest of the student to ask questions at every step of the conversation, since
the response to its question will contain extra information, we don   t want our model to learn this
behavior. each time a human student asks a question, there   s a cost associated with that action.
this cost is a re   ection of the patience of the teacher, or more generally of the users interacting
with the bot in the wild: users won   t    nd the bot engaging if it always asks clari   cation questions.
the student should thus be judicious about asking questions and learn when and what to ask. for
instance, if the student is con   dent about the answer, there is no need for it to ask. or, if the teacher   s
question is so hard that clari   cation is unlikely to help enough to get the answer right, then it should
also refrain from asking.
we now discuss how we model this problem under the id23 framework. the
bot is presented with kb facts (some facts might be missing depending on the task) and a question.
it needs to decide whether to ask a question or not at this point. the decision whether to ask is
made by a binary policy prlquestion. if the student chooses to ask a question, it will be penalized
by costaq. we explored different values of costaq ranging from [0, 2], which we consider as
modeling the patience of the teacher. the goal of this setting is to    nd the best policy for asking/not-
asking questions which would lead to the highest cumulative reward. the teacher will appropriately
reply if the student asks a question. the student will eventually give an answer to the teacher   s initial
question at the end using the policy prlanswer, regardless of whether it had asked a question. the
student will get a reward of +1 if its    nal answer is correct and    1 otherwise. note that the student
can ask at most one question and that the type of question is always speci   ed by the task under
consideration. the    nal reward the student gets is the cumulative reward over the current dialogue
episode. in particular the reward structure we propose is the following:

final answer correct
final answer incorrect

1-costaq
-1-costaq

1
-1

asking question not asking question

table 1: reward structure for the id23 setting.

for each of the tasks described in section 3, we consider three different rl scenarios.
good-student: the student will be presented with all relevant kb facts. there are no misspellings
or unknown words in the teacher   s question. this represents a knowledgable student in the real
world that knows as much as it needs to know (e.g., a large knowledge base, large vocabulary). this
setting is identical across all missing entity tasks (5 - 9).
poor-student: the kb facts or the questions presented to the student are    awed depending on each
task. for example, for the question clari   cation tasks, the student does not understand the question
due to spelling mistakes. for the missing question entity task the entity that the teacher asks about
is unknown by the student and all facts containing the entity will be hidden from the student. this
setting is similar to a student that is underprepared for the tasks.
medium-student: the combination of the previous two settings where for 50% of the questions,
the student has access to the full kb and there are no new words or phrases or entities in the question,
and 50% of the time the question and kb are taken from the poor-student setting.
4.2 mechanical turk data

finally, to validate our approach beyond our simulator by using real language, we collected data via
amazon mechanical turk. due to the cost of data collection, we focused on real language versions
of tasks 4 (knowledge veri   cation) and 8 (missing triple), see secs. 3.2 and 3.3 for the simulator
versions. that is, we collect dialoguess and use them in an of   ine supervised learning setup similar
to section 4.1.1. this setup allows easily reproducibile experiments.
for mechanical turk task 4, the bot is asked a question by a human teacher, but before answering
can ask the human if the question is related to one of the facts it knows about from its memory.

7

published as a conference paper at iclr 2017

figure 4: an illustration of the poor-student setting for rl task 1 (question paraphrase).

it is then required to answer the original question, after some additional dialog turns relating to
other question/answer pairs (called    conversational history   , as before). for task 8, the bot is asked
a question by a human but lacks the triple in its memory that would be needed to answer it. it
is allowed to ask for the missing information, the human responds to the question in free-form
language. the bot is then required to answer the original question, again after some    conversational
history    has transpired.
we collect around 10,000 episodes (dialogues) for training, 1000 for validation, and 2500 for testing
for each of the two tasks. in each case, we give instructions to the turkers that still follow the
original form of the task, but make the tasks contain realistic language written by humans. the
instructions given to the turkers are given in the appendix.
for both tasks, while the human turkers replace the simulator that the bot was previously conversing
with, the bot   s dialogue actions (capabilities) are essentially unchanged from before. that is, when
answering questions, now the bot is required to answer a human   s questions rather than templated
questions from the simulator. when the bot is asking questions, the bot still asks in the same form
as before, e.g. questions like    does it have something to do with x    for task 4 or    i don   t know.
what   s the answer?    for task 8. however, now its questions are answered by a human. in both
cases (asking and answering) the human data is richer with potentially more complex language and
lexical variability. examples of the collected dialogues are given in figure 5.

mturk task 4: knowledge veri   cation (aq):
kb: in their skin written by joshua close, jeremy power regimbal
kb: in their skin starred actors selma blair, james d   arcy, rachel miner, joshua close
t: what    lm was written by joshua close?
s: does it have something to do with    in their skin starred actors joshua close   ?
t: no we were looking for who wrote the    lm not who starred in it.
. . .
mturk task 8: missing triple (aq):
t :what movies does timothy scott bogart have writing credit for?
s: i don   t know. what   s the answer?
t: extreme ops or the i inside is the right answer.
. . .

figure 5: sample dialogues for mechanical turk versions of tasks 4 and 8. compared to the original
tasks (see figs 2 and 3) the teacher   s questions, and the teacher responses to the student   s questions,
are written by humans and are more complex and contain more variety.

5 models

for both of   ine supervised and online rl settings, we use the end-to-end memory network model
(memn2n) (sukhbaatar et al., 2015) as a backbone. the model takes as input the last utterance of
the dialogue history (the question from the teacher) as well as a set of memory contexts including
short-term memories (the dialogue history between the bot and the teacher) and long-term memories

8

which	movviedid	tom	hanks	sttarin?what	do	you	mean?i	mean	which	film	did	tom	hanks	appear	in.forest	gump.	that   s	correct	(+)that   s	incorrect	(--)larry	crowneaqqareward:	1-costaqreward:	-1published as a conference paper at iclr 2017

(the knowledge base facts that the bot has access to), and outputs a label. we refer readers to the
appendix for more details about memn2n.
of   ine supervised settings: the    rst learning strategy we adopt is the reward-based imitation
strategy (denoted vanilla-memn2n) described in (weston, 2016), where at training time, the model
maximizes the log likelihood id203 of the correct answers the student gave (examples with
incorrect    nal answers are discarded). candidate answers are words that appear in the memories,
which means the bot can only predict the entities that it has seen or known before.
we also use a variation of memn2n called    context memn2n    (cont-memn2n for short) where we
replace each word   s embedding with the average of its embedding (random for unseen words) and
the embeddings of the other words that appear around it. we use both the preceeding and following
words as context and the number of context words is a hyperparameter selected on the dev set.
an issue with both vanilla-memn2n and cont-memn2n is that the model only makes use of the
bot   s answers as signals and ignores the teacher   s feedback. we thus propose to use a model that
jointly predicts the bot   s answers and the teacher   s feedback (denoted as trainqa (+fp)). the bot   s
answers are predicted using a vanilla-memn2n and the teacher   s feedback is predicted using the
forward prediction (fp) model as described in (weston, 2016). we refer the readers to the appendix
for the fp model details. at training time, the models learn to jointly predict the teacher   s feedback
and the answers with positive reward. at test time, the model will only predict the bot   s answer.
for the testmodelaq setting described in section 4, the model needs to decide the question to
ask. again, we use vanilla-memn2n that takes as input the question and contexts, and outputs the
question the bot will ask.
online rl settings: a binary vanilla-memn2n (denoted as prl(question)) is used to decide
whether the bot should or should not ask a question, with the teacher replying if the bot does ask
something. a second memn2n is then used to decide the bot   s answer, denoted as prl(answer).
prl(answer) for qa and aq are two separate models, which means the bot will use different
models for    nal-answer prediction depending on whether it chooses to ask a question or not.7
to update prl(question) and
we use the reinforce algorithm (williams, 1992)
prl(answer). for each dialogue, the bot takes two sequential actions (a1, a2):
to ask or not
to ask a question (denoted as a1); and guessing the    nal answer (denoted as a2). let r(a1, a2)
denote the cumulative reward for the dialogue episode, computed using table 1. the gradient to
update the policy is given by:

p(a1, a2) = prl(question)(a1)    prl(answer)(a2)
   j(  )         log p(a1, a2)[r(a1, a2)     b]

(1)

where b is the baseline value, which is estimated using another memn2n model that takes as input
the query x and memory c, and outputs a scalar b denoting the estimation of the future reward.
the baseline model is trained by minimizing the mean squared loss between the estimated reward b
and actual cumulative reward r, ||r     b||2. we refer the readers to (ranzato et al., 2015; zaremba
& sutskever, 2015) for more details. the baseline estimator model is independent from the policy
models and the error is not backpropagated back to them.
in practice, we    nd the following training strategy yields better results:    rst
train only
prl(answer), updating gradients only for the policy that predicts the    nal answer. after the bot   s
   nal-answer policy is suf   ciently learned, train both policies in parallel8. this has a real-world anal-
ogy where the bot    rst learns the basics of the task, and then learns to improve its performance via
a question-asking policy tailored to the user   s patience (represented by costaq) and its own ability
to asnwer questions.

7an alternative is to train one single model for    nal answer prediction in both aq and qa cases, similar to
the trainmix setting in the supervised learning setting. but we    nd training aq and qa separately for the    nal
answer prediction yields a little better result than the single model setting.

8 we implement this by running 16 epochs in total, updating only the model   s policy for    nal answers in
the    rst 8 epochs while updating both policies during the second 8 epochs. we pick the model that achieves the
best reward on the dev set during the    nal 8 epochs. due to relatively large variance for rl models, we repeat
each task 5 times and keep the best model on each task.

9

published as a conference paper at iclr 2017

train \test

trainqa (context)
trainaq (context)
trainmix (context)
train \test

trainqa (context)
trainaq (context)
trainmix (context)

question clari   cation

knowledge operation

task 1: q. paraphrase
testaq
testqa
0.726
0.754
0.889
0.640
0.751
0.846

task 2: q. veri   cation
testqa
0.742
0.643
0.740

testaq
0.684
0.807
0.789

task 3: ask for relevant k.
testqa
0.883
0.716
0.870

testaq
0.947
0.985
0.985

task 4: k. veri   cation
testqa
0.888
0.852
0.875

testaq
0.959
0.987
0.985

testqa
testaq
task 5: q. entity
0.224
<0.01
0.639
<0.01
<0.01
0.632

knowledge acquisition
testqa
testaq
task 6: answer entity
<0.01
<0.01
<0.01

testqa
task 7: relation entity
0.241
0.143
0.216

0.120
0.885
0.852

0.301
0.893
0.898

testaq

testqa

testaq

task 8: triple

0.339
0.154
0.298

0.251
0.884
0.886

testqa
testaq
task 9: everything
0.058
<0.01
0.908
<0.01
<0.01
0.903

table 2: results for cont-memn2n on different tasks.

6 experiments

6.1 simulator

of   ine results: of   ine results are presented in tables 2, 7 and 8 (the latter two are in the appendix).
table 7 presents results for the vanilla-memn2n and forward prediction models. table 2 presents
results for cont-memn2n, which is better at handling unknown words. we repeat each experiment
10 times and report the best result. finally, table 8 presents results for the test scenario where the
bot itself chooses when to ask questions. observations can be summarized as as follows:
- asking questions helps at test time, which is intuitive since it provides additional evidence:

    trainaq+testaq (questions can be asked at both training and test time) performs the best

across all the settings.

    trainqa+testaq (questions can be asked at training time but not at test time) performs
worse than trainqa+testqa (questions can be asked at neither training nor test time) in
tasks question clari   cation and knowledge operation due to the discrepancy between
training and testing.

    trainqa+testaq performs better than trainqa+testqa on all knowledge acquisition
tasks, the only exception being the cont-memn2n model on the missing triple setting.
the explanation is that for most tasks in knowledge acquisition, the learner has no chance
of giving the correct answer without asking questions. the bene   t from asking is thus
large enough to compensate for the negative effect introduced by data discrepancy between
training and test time.

    trainmix offers    exibility in bridging the gap between datasets generated using qa and
aq, very slightly underperforming trainaq+testaq, but gives competitive results on both
testqa and testaq in the question clari   cation and knowledge operations tasks.

    trainaq+testqa (allowing questions at training time but forbid questions at test time) per-
forms the worst, even worse than trainqa+testqa. this has a real-world analogy where
a student becomes dependent on the teacher answering their questions, later struggling to
answer the test questions without help.

    in the missing question entity task (the student does not know about the question entity),
the missing answer entity task (the student does not know about the answer entity), and
missing everything task, the bot achieves accuracy less than 0.01 if not asking questions at
test time (i.e., testqa).

    the performance of testmodelaq, where the bot relies on its model to ask questions at
test time (and thus can ask irrelevant questions) performs similarly to asking the correct
question at test time (testaq) and better than not asking questions (testqa).

- cont-memn2n signi   cantly outperforms vanilla-memn2n. one explanation is that considering
context provides signi   cant evidence distinguishing correct answers from candidates in the dialogue
history, especially in cases where the model encounters unfamiliar words.
rl results for the rl settings, we present results for task 2 (question veri   cation) and task 6
(missing answer entities) in figure 6. task 2 represents scenarios where different types of student

10

published as a conference paper at iclr 2017

figure 6: results of online learning for task 2 and task 6

have different abilities to correctly answer questions (e.g., a poor student can still sometimes give
correct answers even when they do not fully understand the question). task 6 represents tasks where
a poor learner who lacks the knowledge necessary to answer the question can hardly give a correct
answer. all types of students including the good student will theoretically bene   t from asking
questions (asking for the correct answer) in task 6. we show the percentage of question-asking
versus the cost of aq on the test set and the accuracy of question-answering on the test set vs the
cost of aq. our main    ndings were:

    a good student does not need to ask questions in task 2 (question veri   cation), because
they already understand the question. the student will raise questions asking for the correct
answer when cost is low for task 6 (missing answer entities).
    a poor student always asks questions when the cost is low. as the cost increases, the
    as the aq cost increases gradually, good students will stop asking questions earlier than the
medium and poor students. the explanation is intuitive: poor students bene   t more from
asking questions than good students, so they continue asking even with higher penalties.
    as the id203 of question-asking declines, the accuracy for poor and medium students

frequency of question-asking declines.

drops. good students are more resilient to not asking questions.

6.2 mechanical turk

results for the mechanical turk tasks are given in table 3. we again compare vanilla-memn2n
and cont-memn2n, using the same trainaq/trainqa and testaq/testqa combinations as before,
for tasks 4 and 8 as described in section 4.2. we tune hyperparameters on the validation set and
repeat each experiment 10 times and report the best result.
while performance is lower than on the related task 4 and task 8 simulator tasks, we still arrive
at the same trends and conclusions when real data from humans is used. the performance was
expected to be lower because (i) real data has more lexical variety, complexity and noise; and (ii) the
training set was smaller due to data collection costs (10k vs. 180k). we perform an analysis of the
difference between simulated and real training data (or combining the two) in the appendix, which
shows that using real data is indeed important and measurably superior to using simulated data.

11

0.000.050.100.150.200.250.30question cost0.00.20.40.60.81.0question-asking ratetask2 question verification question-asking rate vs question costgoodmediumpoor0.000.050.100.150.200.250.30question cost0.700.720.740.760.780.800.82final accuracytask2 question verification final accuracy vs question costgoodmediumpoor0.00.51.01.52.02.53.0question cost0.00.20.40.60.81.0question-asking ratetask6 missing answer entity question-asking rate vs question costgoodmediumpoor0.00.51.01.52.02.53.0question cost0.00.20.40.60.8final accuracytask6 missing answer entity final accuracy vs question costgoodmediumpoorpublished as a conference paper at iclr 2017

train \test
trainqa
trainaq

vanilla-memn2n

task 4: k. veri   cation
testqa
0.331
0.318

testaq
0.313
0.375

task 8: triple

testqa
0.133
0.072

testaq
0.162
0.422

cont-memn2n

task 4: k. veri   cation
testqa
0.712
0.679

testaq
0.703
0.774

task 8: triple

testqa
0.308
0.137

testaq
0.234
0.797

table 3: mechanical turk task results. asking questions (aq) outperforms only answering ques-
tions without asking (qa).

more importantly, the same main conclusion is observed as before: trainaq+testaq (questions
can be asked at both training and test time) performs the best across all the settings. that is, we
show that a bot asking questions to humans learns to outperform one that only answers them.

7 conclusions

in this paper, we explored how an intelligent agent can bene   t from interacting with users by asking
questions. we developed tasks where interaction via asking questions is desired. we explore both
online and of   ine settings that mimic different real world situations and show that in most cases,
teaching a bot to interact with humans facilitates language understanding, and consequently leads to
better id53 ability.

references
mohammad amin bassiri.

noticing l2 form. english language and literature studies, 1(2):61, 2011.

interactional feedback and the impact of attitude and motivation on

antoine bordes and jason weston. learning end-to-end goal-oriented dialog. arxiv preprint

arxiv:1605.07683, 2016.

antoine bordes, nicolas usunier, sumit chopra, and jason weston. large-scale simple question

answering with memory networks. arxiv preprint arxiv:1506.02075, 2015.

jesse dodge, andreea gane, xiang zhang, antoine bordes, sumit chopra, alexander miller, arthur
szlam, and jason weston. evaluating prerequisite qualities for learning end-to-end dialog sys-
tems. arxiv preprint arxiv:1511.06931, 2015.

richard higgins, peter hartley, and alan skelton. the conscientious consumer: reconsidering the
role of assessment feedback in student learning. studies in higher education, 27(1):53   64, 2002.

andrew s latham. learning through feedback. educational leadership, 54(8):86   87, 1997.

jiwei li, michel galley, chris brockett, jianfeng gao, and bill dolan. a diversity-promoting

objective function for neural conversation models. arxiv preprint arxiv:1510.03055, 2015.

alexander miller, adam fisch, jesse dodge, amir-hossein karimi, antoine bordes, and ja-
arxiv preprint

son weston. key-value memory networks for directly reading documents.
arxiv:1606.03126, 2016.

marc   aurelio ranzato, sumit chopra, michael auli, and wojciech zaremba. sequence level train-

ing with recurrent neural networks. arxiv preprint arxiv:1511.06732, 2015.

alessandro sordoni, michel galley, michael auli, chris brockett, yangfeng ji, margaret mitchell,
jian-yun nie, jianfeng gao, and bill dolan. a neural network approach to context-sensitive
generation of conversational responses. arxiv preprint arxiv:1506.06714, 2015.

pei-hao su, milica gasic, nikola mrksic, lina rojas-barahona, stefan ultes, david vandyke,
tsung-hsien wen, and steve young. continuously learning neural dialogue management. arxiv
preprint arxiv:1606.02689, 2016.

sainbayar sukhbaatar, jason weston, rob fergus, et al. end-to-end memory networks. in advances

in neural information processing systems, pp. 2440   2448, 2015.

12

published as a conference paper at iclr 2017

oriol vinyals and quoc le. a neural conversational model. arxiv preprint arxiv:1506.05869, 2015.

sida i wang, percy liang, and christopher d manning. learning language games through interac-

tion. arxiv preprint arxiv:1606.02447, 2016.

tsung-hsien wen, milica gasic, nikola mrksic, lina m rojas-barahona, pei-hao su, stefan ultes,
david vandyke, and steve young. a network-based end-to-end trainable task-oriented dialogue
system. arxiv preprint arxiv:1604.04562, 2016.

margaret g werts, mark wolery, ariane holcombe, and david l gast. instructive feedback: review

of parameters and effects. journal of behavioral education, 5(1):55   75, 1995.

jason weston. dialog-based language learning. arxiv preprint arxiv:1604.06045, 2016.

jason weston, antoine bordes, sumit chopra, alexander m rush, bart van merri  enboer, armand
joulin, and tomas mikolov. towards ai-complete id53: a set of prerequisite toy
tasks. arxiv preprint arxiv:1502.05698, 2015.

ronald j williams. simple statistical gradient-following algorithms for connectionist reinforcement

learning. machine learning, 8(3-4):229   256, 1992.

terry winograd. understanding natural language. cognitive psychology, 3(1):1   191, 1972.

ludwig wittgenstein. philosophical investigations. john wiley & sons, 2010.

wojciech zaremba and ilya sutskever. id23 id63s. arxiv

preprint arxiv:1505.00521, 362, 2015.

appendix
end-to-end memory networks the input to an end-to-end memory network model (memn2n)
is the last utterance of the dialogue history x as well as a set of memories (context) (c=c1, c2, ...,
cn ). memory c encodes both short-term memory, e..g, dialogue histories between the bot and the
teacher and long-term memories, e.g., the knowledgebase facts that the bot has access to. given the
input x and c, the goal is to produce an output/label a.
in the    rst step, the query x is transformed to a vector representation u0 by summing up its con-
stituent id27s: u0 = ax. the input x is a bag-of-words vector and a is the d    v word
embedding matrix where d denotes the vector dimensionality and v denotes the vocabulary size.
each memory ci is similarly transformed to vector mi. the model will read information from the
memory by linking input representation q with memory vectors mi using softmax weights:

o1 =

p1
i mi

p1
i = softmax(ut

0 mi)

(2)

i

the goal is to select memories relevant to the last utterance x, i.e., the memories with large values
of p1
i . the queried memory vector o1 is the weighted sum of memory vectors. the queried memory
vector o1 will be added on top of original input, u1 = o1 + u0. u1 is then used to query the memory
vector. such a process is repeated by querying the memory n times (so called    hops   ). n is set to
three in all experiments in this paper.
in the end, un is input to a softmax function for the    nal prediction:

n yl)

n y1, ut

n y2, ..., ut

a = softmax(ut

(3)
where l denotes the number of candidate answers and y denotes the representation of the answer.
if the answer is a word, y is the corresponding id27. if the answer is a sentence, y is
the embedding for the sentence achieved in the same way as we obtain embeddings for query x and
memory c.
reward based imitation (rbi) and forward prediction (fp) rbi and fp are two dialogue learn-
ing strategies proposed in (weston, 2016) by harnessing different types of dialogue signals. rbi
handles the case where the reward or the correctness of a bot   s answer is explicitly given (for ex-
ample, +1 if the bot   s answer is correct and 0 otherwise). the model is directly trained to predict
the correct answers (with label 1) at training time, which can be done using end-to-end memory
networks (memn2n) (sukhbaatar et al., 2015) that map a dialogue input to a prediction.

13

(cid:88)

published as a conference paper at iclr 2017

fp handles the situation where a real-valued reward for a bot   s answer is not available, meaning that
there is no +1 or 0 labels paired with a student   s utterance. however, the teacher will give a response
to the bot   s answer, taking the form of a dialogue utterance. more formally, suppose that x denotes
the teacher   s question and c=c1, c2, ..., cn denotes the dialogue history. in our aq settings, the
bot will ask a question a regarding the teacher   s question, denoted as a     a, where a denotes the
student   s question pool. the teacher will provide an utterance in response to the student question
a. in fp, the model    rst maps the teacher   s initial question x and dialogue history c to vector
representation u using a memory network with multiple hops. then the model will perform another
hopof attention over all possible student   s questions in a, with an additional part that incorporates
the information of which candidate (i.e., a) was actually selected in the dialogue:

p  a = softmax(ut y  a)

o =

p  a(y  a +       1[  a = a])

(4)

(cid:88)

  a   a

where y  a denotes the vector representation for the student   s question candidate   a.    is a d-
dimensional vector to signify the actual action a that the student chooses. for tasks where the
student only has one way to ask questions (e.g.,    what do you mean   ), there is no need to perform
hops of attention over candidates since the cardinality of a is just 1. we thus directly assign a
id203 of 1 to the student   s question, making o the sum of vector representation of ya and   .
o is then combined with u to predict the teacher   s feedback t using a softmax:

(5)

1 xrn )

1 xr1 , ut

1 xr2, ..., ut

u1 = o + u t = softmax(ut
where xri denotes the embedding for the ith response.
dialogue simulator in this section we further detail the simulator and the datasets we generated
in order to realize the various scenarios discussed in section 3. we focused on the problem of
movieqa where we adapted the wikimovies dataset proposed in weston et al. (2015). the dataset
consists of roughly 100k questions with over 75k entities from the open movie dataset (omdb).
each dialogue generated by the simulator takes place between a student and a teacher. the simulator
samples a random question from the wikimovies dataset and fetches the set of all kb facts relevant
to the chosen question. this question is assumed to be the one the teacher asks its student, and
is referred to as the    original    question. the student is    rst presented with the relevant kb facts
followed by the original question. providing the kb facts to the student allows us to control the exact
knowledge the student is given access to while answering the questions. at this point, depending
on the task at hand and the student   s ability to answer, the student might choose to directly answer
it or ask a    followup    question. the nature of the followup question will depend on the scenario
under consideration. if the student answers the question, it gets a response from the teacher about its
correctness and the conversation ends. however if the student poses a followup question, the teacher
gives an appropriate response, which should give additional information to the student to answer the
original question. in order to make things more complicated, the simulator pads the conversation
with several unrelated student-teacher question-answer pairs. these question-answer pairs can be
viewed as distractions and are used to test the student   s ability to remember the additional knowledge
provided by the teacher after it was queried. for each dialogue, the simulator incorporates 5 such
pairs (10 sentences). we refer to these pairs as conversational histories.
for the qa setting (see section 3), the dialogues generated by the simulator are such that the student
never asks a clari   cation question. instead, it simply responds to the original question, even if it is
wrong. for the dialogs in the aq setting, the student always asks a clari   cation question. the nature
of the question asked is dependent on the scenario (whether it is question clari   cation, knowledge
operation, or knowledge acquisition) under consideration. in order to simulate the case where the
student sometimes choses to directly answer the original question and at other times choses to ask
question, we created training datasets, which were a combination of qa and aq (called    mixed   ).
for all these cases, the student needs to give an answer to the teacher   s original question at the end.
instructions given to turkers
these are the instructions given for the textual feedback mechanical turk task (we also constructed
a separate task to collect the questions to ask the bot with similar instructions, not described here):

task 4 (answers to bot   s questions):

14

published as a conference paper at iclr 2017

title: write brief responses to given dialogue exchanges (about 15 min)
description: write a brief response answering a provided question (25 questions per hit).
directions:
each task consists of the following triplets:
1) a question by the teacher
2) the correct answer(s) to the question (separated by    or   ), unknown to the student
3) a clarifying question asking for feedback from the teacher
consider the scenario where you are the teacher and have already asked the question, and received
the reply from the student. please compose a brief response replying to the student   s question. the
correct answers are provided so that you know whether the student   s question was relevant or not.
for example, given 1) question:    what is a color in the united states    ag?   ; 2) correct answer:
   white or blue or red   ; 3) student reply:    does this have to do with    us flag has colors
red,white,blue   ?   , your response could be something like    that   s right!   ; for 3) reply:    does this
have to do with    united states has population 320 million   , you might say    no, that fact is not
relevant    or    not really   .
please vary responses and try to minimize spelling mistakes.
if the same responses are
copied/pasted or similar responses are overused, we   ll reject the hit. avoid naming the student or
addressing    the class    directly. we will consider bonuses for higher quality responses during review.

task 8: answers to bot   s questions:
title: write brief responses to given dialogue exchanges (about 10 min)
description: write a sentence describing the answer to a question (25 questions per hit).
directions:
each task consists of the following triplets:
1) a question by the teacher
2) the correct answer(s) to the question (separated by    or   ), unknown to the student
3) a question from the student asking the teacher for the answer
consider the scenario where you are the teacher and have already asked the question, and received
the reply from the student. please compose a brief response replying to the student   s question. the
correct answers are provided so that you know which answers to provide.
for example, given 1) question:    what is a color in the united states    ag?   ; 2) correct answer:
   white or blue or red   ; 3) student reply:    i dont know. what   s the answer ?   , your response could
be something like    the color white is in the us    ag    or    blue and red both appear in it   .
please vary responses and try to minimize spelling mistakes, and do not include the capitalized
   or    in your response. if the same responses are copied/pasted or similar responses are overused,
we   ll reject the hit. you don   t need to mention every correct answer in your response. avoid
naming the student or addressing    the class    directly. we will consider bonuses for higher quality
responses during review.

additional mechanical turk experiments
here we provide additional experiments to supplement the ones described in section 6.2. in the main
paper, results were shown when training and testing on the collected mechanical turk data (around
10,000 episodes of training dialogues for training). as we collected the data in the same settings as
task 4 and 8 of our simulator, we could also consider supplementing training with simulated data
as well, of which we have a larger amount (over 100,000 episodes). note this is only for training,
we will still test on the real (mechanical turk collected) data. although the simulated data has less
lexical variety as it is built from templates, the larger size might obtain improve results.
results are given table 5 when training on the combination of real and simulator data, and testing
on real data. this should be compared to training on only the real data (table 4) and only on the
simulator data (table 6). the best results are obtained from the combination of simulator and real
data. the best real data only results (selecting over algorithm and training strategy) on both tasks
outperform the best results using simulator data, i.e. using cont-memn2n with the train aq /
testaq setting) 0.774 and 0.797 is obtained vs. 0.714 and 0.788 for tasks 4 and 8 respectively. this

15

published as a conference paper at iclr 2017

is despite there being far fewer examples of real data compared to simulator data. overall we obtain
two main conclusions from this additional experiment: (i) real data is indeed measurably superior to
simulated data for training our models; (ii) in all cases (across different algorithms, tasks and data
types     be they real data, simulated data or combinations) the bot asking questions (aq) outperforms
it only answering questions and not asking them (qa). the latter reinforces the main result of the
paper.

train \test
trainqa
trainaq

vanilla-memn2n

task 4: k. veri   cation
testqa
0.331
0.318

testaq
0.313
0.375

task 8: triple

testqa
0.133
0.072

testaq
0.162
0.422

cont-memn2n

task 4: k. veri   cation
testqa
0.712
0.679

testaq
0.703
0.774

task 8: triple

testqa
0.308
0.137

testaq
0.234
0.797

table 4: mechanical turk task results, using real data for training and testing.

train \test
trainqa
trainaq

vanilla-memn2n

task 4: k. veri   cation
testqa
0.356
0.340

testaq
0.311
0.445

task 8: triple

testqa
0.128
0.150

testaq
0.174
0.487

cont-memn2n

task 4: k. veri   cation
testqa
0.733
0.704

testaq
0.717
0.792

task 8: triple

testqa
0.368
0.251

testaq
0.352
0.825

table 5: results on mechanical turk tasks using a combination of real and simulated data for
training, testing on real data.

train \test
trainqa
trainaq

vanilla-memn2n

task 4: k. veri   cation
testqa
0.340
0.326

testaq
0.311
0.390

task 8: triple

testqa
0.120
0.067

testaq
0.165
0.405

cont-memn2n

task 4: k. veri   cation
testqa
0.665
0.642

testaq
0.648
0.714

task 8: triple

testqa
0.349
0.197

testaq
0.342
0.788

table 6: results on mechanical turk tasks using only simulated data for training, but testing on
real data.

additional of   ine supervised learning experiments

question clari   cation

knowledge operation

task 1: q. paraphrase
testaq
testqa
0.338
0.284
0.450
0.213
0.464
0.288
0.326
0.373

train \test
trainqa
trainaq

trainaq(+fp)

trainmix

train \test

trainqa (vanila) < 0.01
trainaq (vanila) < 0.01
trainaq(+fp)
< 0.01
mix (vanila)
<0.01

testqa
testaq
task 5: q. entity
0.223
0.660
0.742
0.630

testaq
0.271
0.373
0.320
0.326

testaq
0.344
0.632
0.631
0.558

task 2: q. veri   cation
testqa
0.340
0.225
0.146
0.329

task 3: ask for relevant k.
testqa
0.462
0.187
0.342
0.442
knowledge acquisition
testqa
testaq
task 6: answer entity
<0.01
<0.01
< 0.01
<0.01

testqa
task 7: relation entity
0.109
0.082
0.085
0.070

<0.01
<0.01
< 0.01
<0.01

0.129
0.156
0.188
0.152

testaq

0.201
0.124
0.064
0.180

task 4: k. veri   cation
testqa
0.482
0.283
0.311
0.476

testaq
0.322
0.540
0.524
0.491

testqa

testaq

task 8: triple

testqa
testaq
task 9: everything
<0.01
<0.01
<0.01
<0.01
<0.01
<0.01
<0.01
<0.01

0.259
0.664
0.702
0.572

table 7: results for of   ine settings using memory networks.

question clari   cation
task 2: q. veri   cation

testmodelaq

0.382
0.344
0.352

trainaq

trainaq(+fp)

trainmix

knowledge acquisition
task 4: k. veri   cation

testmodelaq

0.480
0.501
0.469

table 8: results for testmodelaq settings.

16

