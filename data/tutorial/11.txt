foundations and trends r(cid:1) in
information retrieval
vol. 1, no. 3 (2006) 233   334
c(cid:1) 2008 p. juola
doi: 10.1561/1500000005

authorship attribution

patrick juola

department of mathematics and computer science, duquesne university,
600 forbes avenue, pittsburgh, pa 15282, usa, juola@mathcs.duq.edu

abstract

authorship attribution, the science of inferring characteristics of the
author from the characteristics of documents written by that author,
is a problem with a long history and a wide range of application.
recent work in    non-traditional    authorship attribution demonstrates
the practicality of automatically analyzing documents based on autho-
rial style, but the state of the art is confusing. analyses are di   cult
to apply, little is known about type or rate of errors, and few    best
practices    are available. in part because of this confusion, the    eld has
perhaps had less uptake and general acceptance than is its due.

this review surveys the history and present state of the discipline,
presenting some comparative results when available. it shows,    rst,
that the discipline is quite successful, even in di   cult cases involving
small documents in unfamiliar and less studied languages; it further
analyzes the types of analysis and features used and tries to determine
characteristics of well-performing systems,    nally formulating these in
a set of recommendations for best practices.

1

introduction

1.1 why    authorship attribution   ?

in 2004, potomac books published imperial hubris: why the west is
losing the war on terror. drawing on the author   s extensive personal
experience, the book described the current situation of the american-
led war on terror and argued that much us policy was misguided.

or did he? the author of the book is technically    anonymous,   
although he claims (on the dust cover) to be    a senior us intelligence
o   cial with nearly two decades of experience    as well as the author of
the 2003 book through our enemies    eyes. according to the july 2,
2004 edition of the boston phoenix, the actual author was michael
scheuer, a senior cia o   cer and head of the cia   s osama bin laden
unit in the late 1990s. if true, this would lend substantial credibility to
the author   s arguments.

but on the other hand, according to some noted historians such as
hugh trevor-roper, the author of the 1983 hitler diaries was hitler
himself, despite the later discovery that they were written on modern
paper and using ink which was unavailable in 1945. is imperial hubris
another type of sophisticated forgery? why should we believe historians

234

1.2 structure of the review 235

and journalists, no matter how eminent? what kind of evidence should
we demand before we believe?

determining the author of a particular piece of text has raised
methodological questions for centuries. questions of authorship can be
of interest not only to humanities scholars, but in a much more prac-
tical sense to politicians, journalists, and lawyers as in the examples
above. investigative journalism, combined with scienti   c (e.g., chem-
ical) analysis of documents and simple close reading by experts has
traditionally given good results. but recent developments of improved
statistical techniques in conjunction with the wider availability of
computer-accessible corpora have made the automatic and objective
id136 of authorship a practical option. this    eld has seen an explo-
sion of scholarship, including several detailed book-length treatments
[39, 41, 44, 83, 98, 103, 105, 111, 112, 150]. papers on authorship
attribution routinely appear at conference ranging from linguistics and
literature through machine learning and computation, to law and foren-
sics. despite     or perhaps because of     this interest, the    eld itself
is somewhat in disarray with little overall sense of best practices and
techniques.

1.2 structure of the review

this review therefore tries to present an overview and survey of the
current state of the art. we follow the theoretical model (presented
in detail in section 3.4) of [76] in dividing the task into three major
subtasks, each treated independently.

section 2 presents a more detailed problem statement in conjunction
with a historical overview of some approaches and major developments
in the science of authorship attribution. included is a discussion of
some of the major issues and obstacles that authorship attribution
faces as a problem, without regard to any speci   c approach, and the
characteristics of a hypothetical    good    solution (unfortunately, as will
be seen in the rest of the review, we have not yet achieved such a    good   
solution).

section 3 presents some linguistic, mathematical, and algorithmic
preliminaries. section 4 describes some of the major feature sets that

236 introduction

have been applied to authorship attribution, while section 5 describes
the methods of analysis applied to these features. section 6 goes on to
present some results in empirical evaluation and comparative testing
of authorship attribution methods, focusing mainly on the results from
the 2004 ad-hoc authorship attribution competition [75], the largest-
scale comparative test to date.

section 7 presents some other applications of these methods and
technology, that, while not (strictly speaking)    authorship    attribution,
are closely related. examples of this include gender attribution or the
determination of personality and mental state of the author. section 8
discusses the speci   c problems of using authorship attribution in court,
in a forensic setting. finally, for those practical souls who want only
to solve problems, section 9 presents some recommendations about the
current state of the art and the best practices available today.

2

background and history

2.1 problem statement

authorship attribution, broadly de   ned, is one of the oldest and one
of the newest problems in information retrieval. disputes about the
ownership of words have been around for as long as words themselves
could be owned. questions of authenticating documents have existed
as long as the documents themselves have. and the question    what can
i tell about a person from the language he uses    has been around since,
literally, the days of the old testament:

judges 12:5 and the gileadites took the passages of
jordan before the ephraimites: and it was so, that when
those ephraimites which were escaped said, let me go
over; that the men of gilead said unto him, art thou
an ephraimite? if he said, nay;

6 then said they unto him, say now shibboleth: and
he said sibboleth: for he could not frame to pronounce it
right. then they took him, and slew him at the passages
of jordan: and there fell at that time of the ephraimites
forty and two thousand.

237

238 background and history

as will be discussed in depth, this barbaric episode illustrates in a
nutshell the stylistic process. the gileadites identi   ed (correctly, one
hopes) a single salient feature of ephraimite speech     the inability
to pronounce a particular sound     that can be used to divide the
speech of one group from another. it is not clear from this passage
what the success rate of this analysis was, nor how other groups such
as zebulonites were handled. but this technique, broadly interpreted,
continues to be used today;    voice activated locks    are cheaply available
from a wide variety of sources.

the arrival of modern statistics made it possible to investigate ques-
tions of authorship in a more sophisticated fashion, and the develop-
ment of modern computers and large corpora have made it practical
to investigate these questions algorithmically via information retrieval
techniques. with the advent of corpus linguistics, authorship attri-
bution (or to use another near-synonymous term,    stylometry   ) has
become a popular and productive area of research. the principles, how-
ever, remain the same.

we can thus de   ne    authorship attribution    broadly as any attempt
to infer the characteristics of the creator of a piece of
linguistic
data. this is a deliberately broad de   nition; for example, it would
include most of the research done into id103. on the
other hand, many of the techniques of authorship attribution could
be applied to id103, and may in fact be more useful
in some circumstances, such as when the writer of the speech is
the person of interest and di   erent from the actual speaker. in gen-
eral, most of the focus is on written text (or on aspects of spo-
ken text that are shared with written text, such as lexical choice or
sentence structure) instead of on speech-only aspects like accent or
id144.

in broad terms, there are three main problems in authorship attri-
bution. the    rst is, given a particular sample of text known to be by one
of a set of authors, determine which one. this,    closed class,    version
of the problem is closely related to the second,    open class,    version of
the problem: given a particular sample of text believed to be by one of a
set of authors, determine which one, if any, or even here is a document,
tell me who wrote it. the open-class version is of course much harder to

2.2 theoretical background

239

solve, especially if one is required to distinguish among people outside
of a small candidate set.

the third problem     for which some researchers prefer to reserve
the word    stylometry    or    pro   ling,    reserving    authorship attribu-
tion    only for the    rst two     is that of determining any of the proper-
ties of the author(s) of a sample of text. for example, was the document
singly or multiply authored [47]? was the author a native speaker of
english? of us or british english? a man or a woman [5, 89]?

2.2 theoretical background

as will be discussed later, most researchers approach this problem as an
issue of feature extraction and data mining. at an intuitive and informal
level, nearly everyone is familiar with this process. many subgroups of
people have readily identi   able stereotypical habits of speech and/or
writing. for example, the spelling of the word    colour    is typical in
the united kingdom and most of the british commonwealth, but very
uncommon in the united states. these dialectological variations can
(of course) automatically provide information about the person who
wrote a given document. more abstractly, the syntactic constructions
a person uses can indicate something about them, and in some cases
[17] can even be used to help make (medical) diagnoses.

the assumption of most researchers, then, is that people have a
characteristic pattern of language use, a sort of    authorial    ngerprint   
that can be detected in their writings. van halteren [145] has gone so
far as to term this a    human stylome,    a speci   c set of measurable
traits that can be used to uniquely identify a given author. there are
good theoretical reasons for assuming that such a trait might exist.
since every person has to learn    language    by themselves, and their
experiences as language learners di   er, so will the    language    they learn
di   er in micro-aspects. on the other hand, there are also good practical
reasons to believe that such    ngerprints may be very complex, certainly
more complex than simple univariate statistics such as average word
length or vocabulary size.

a key new development in recent research has been the development
of multivariate techniques tuned to distributional features instead of the

240 background and history

mere presence or absence of features. for example, instead of looking
at speci   c words, one might focus on properties such as average word
length or vocabulary richness. unlike speci   c words, these properties
are always available. similarly, use of multiple features may produce an
improvement over univariate features because they provide more infor-
mation overall. furthermore, these improved features may be more reli-
able (as discussed in section 8.4) because they are less susceptible to
direct manipulation.

2.3 historical overview

2.3.1 classical approaches

an excellent survey of the history of stylometry is that of [56] (see also
[57]). the earliest statistical publication that holmes identi   es is that
of [106], who    proposed that word-length might be a distinguishing
characteristic of writers.    other scholars trace the origins to a letter
by de morgan to a clergyman on the subject of gospel authorship,
suggesting that the clergyman   try to balance in your own mind the
question whether the latter [text] does not deal in longer words than
the former [text]. it has always run in my head that a little expenditure
of money would settle questions of authorship this way. . . . some of these
days spurious writings will be detected by this test   [32]. this idea is
at least super   cially plausible, in that authors with large vocabularies
may typically use longer words. unfortunately, studies, including those
of [133] have shown that average word length is neither stable within
a single author, nor does it distinguish between authors. in smith   s
words (cited by holmes),    mendenhall   s method now appears to be so
unreliable that any serious student of authorship should discard it.   

since that time, many other statistics have been proposed and
largely discarded, including average sentence length [153], average word
length [93], average number of syllables per word [45], distribution of
parts of speech [134], type/token ratios [139], or other measures of
   vocabulary richness    such as simpson   s d index [131] or yule   s    char-
acteristic k    [154]. none of these methods have been demonstrated
to be su   ciently distinguishing or su   ciently accurate to be reliable
(see [61] for a speci   c discussion and strongly negative assessment).

2.3 historical overview 241

however, failure of one speci   c instantiation, of course, does not inval-
idate the entire approach.

the underlying theory behind these approaches is that an authorial
      ngerprint    can be extracted as a summary statistic     a single or
small set of features     from a given text, and that di   erent authors
will vary, noticeably and consistently, along this statistic. a slightly
di   erent approach to mine is several di   erent texts to be compared,
looking for features along which they di   er in a reliable and noticeable
way. an example of this approach is that of ule [143], who proposed
using    relative vocabulary overlap    (rvo) as a measure of the degree
to which two texts draw from the same vocabulary. the advantage of
this type of approach is that it can be sensitive to di   erences that the
summary statistics hide; a major disadvantage is that the calculations
of    di   erence    cannot be done on a per-document basis, but instead on
each pair of documents, approximately squaring the amount of e   ort
required for analysis.

a more serious problem with the ule [143] approach in particular
is that it may permit topic to dominate over authorship. any two re-
tellings of little red riding hood will probably incorporate the words
   basket    and    wolf   ; any two newspaper articles describing the same
football game will mention the same teams and star players; indeed, any
two reports of football games will probably mention    score    and    win-
ner,    and not    basket.    therefore, any measure of vocabulary overlap
will    nd a greater match between documents of similar topic.

2.3.2 the federalist analyses

one approach that has been suggested [34] is to focus on synonym
pairs. an author has, for example, almost complete freedom to choose
between the words    big    and    large   ; neither the structure of english
grammar nor the meanings of the words place any constraints. by
observing that one author consistently makes one choice and another
the opposite, one has a noticeable, topic-free, and consistent way to
di   erentiate.

mosteller and wallace attempted to apply this technique to the
federalist papers, but found that there were not enough synonym pairs

242 background and history

to make this practical. instead, they focused on so-called function
words, words like conjunctions, prepositions, and articles that carry
little meaning by themselves (think about what    of    means), but that
de   ne relationships of syntactic or semantic functions between other
(   content   ) words in the sentence. these words are therefore largely
topic-independent and may serve as useful indicators of an author   s
preferred way to express broad concepts such as    ownership.   

mosteller and wallace therefore analyzed [112] the distribution of
30 function words extracted from the text of the various federalist
papers. because this analysis has become arguably the most famous
and widely cited statistical analysis of authorship, and because the fed-
eralist papers themselves have become a touchstone for new methods
of authorship attribution, (although later scholars have sharply criti-
cized both the study itself and its many followups and replications) it
is worth discussing the problem itself.

the federalist papers are a set of newspaper essays published
between 1787 and 1788 by an anonymous author named    publius,   
in favor of the rati   cation of the newly proposed constitution of
the united states. it has since become known that    publius    was a
pseudonym for a group of three authors: john jay, alexander hamil-
ton, and james madison. it has since become generally accepted that
of the 85 essays, jay wrote    ve, madison wrote 14, and hamilton wrote
51, with three more essays written jointly by madison and hamilton.
the other 12 essays, the famous    disputed essays,    have been claimed
by both madison and hamilton.

modern scholarship is almost unanimous in assigning authorship of
the disputed essays to madison on the basis of traditional historical
methods. mosteller and wallace were able to make this determination
purely on the basis of statistically inferred probabilities and bayesian
analysis.

because of the circumstances of this problem, the federalist papers
are almost a perfect test-bed for new methods of authorship attribu-
tion. first, the documents themselves are widely available (albeit with
many potential corruptions, as will be discussed later), including over
the internet through sources such as project gutenberg. second, the
candidate set for authorship is well-de   ned; the author of the disputed

2.3 historical overview 243

papers is known to be either hamilton or madison. third, the undis-
puted papers provide excellent samples of undisputed text written by
the same authors, at the same time, on the same topic, in the same
genre, for publication via the same media. a more representative train-
ing set would be hard to imagine.

for this reason, it has become almost traditional to test a new
method on this problem. studies include [58, 99, 122, 142]; rudman
[127] lists no less than nineteen studies of this particular corpus and
is hardly complete. perhaps needless to say, almost all of these studies
con   rm this particular assignment of authorship and the correctness
of mosteller and wallace   s results. as a result (as will be seen in later
sections) the idea of mining function words for cues to authorship has
become a dominant theme in modern research.

2.3.3 controversies: cusum and the elegy

if mosteller and wallace are stylometry   s best known success, it is
equally important to discuss the best known failures. the cusum or
qsum technique, an abbreviation for    cumulative sum,    [10, 11, 39, 111]
is a visual method for observing similarity between sequences of mea-
sures. as applied to sequences in general, one    rst takes the sequence,
e.g., { 8, 6, 7, 5, 3, 0, 9, 2 . . .} and calculates the mean (in this case, 5).
one then calculates the di   erences from the mean { 3, 1, 2, 0,    2,    5,
4,    3 . . .} and plots their    cumulative sum    { 3, 4, 6, 6, 4,    1, 3, 0 . . .}.
this plot measures the homogeneity or stability of a feature     in the
case of cusum, the feature is traditionally something like    percentage
of words with two or three letters.    (but see section 4).

this technique was rapidly adopted and used in several english
court cases [including the queen vs. thomas mccrossen (court of
appeal, london 1991), the queen vs. frank beck (leicester crown
court 1992), and the queen vs. joseph nelson-wilson (london
1992)] as a forensic technique. unfortunately, the accuracy of the
technique almost immediately came under question; reports such as
[24, 52, 53, 55, 60] suggested that the theory was not well-grounded
and that the results were not accurate enough to be relied upon (espe-
cially given that the case mentioned were criminal cases). however, the

244 background and history

ultimate downfall happened when    he was challenged on live british
television to attribute texts that he had never seen. the result was dis-
astrous; despite his impressive statistics and his fancy computer graph-
ics, morton could not distinguish between the writings of a convicted
felon and the chief justice of england    [50].

despite this failure variations on cusum such as wqsum (   weighted
cusum   ) continue to be used [15, 135, 136] and have some evidence
for their validity. as will be seen, frequency analysis continues to be
a major component of many successful algorithms. but the negative
publicity of such a failure has cast a substantial shadow over the    eld
as a whole.

2.3.4 foster and the elegy

another noted episode (and noted failure) in the history of authorship
attribution is the work in the late 1990s, of don foster [41, 44]. grieve
[50] provides a good overview of the controversy, but in summary, foster
found, by applying a battery of stylometric tests, that the relatively
obscure poem    a funeral elegy    by    w.s.    was, in fact, the work
of william shakespeare. to suggest that this was controversial is to
understate the case; the combination of a relatively well-known scholar
and an iconic author propelled the    nding to the front page of the
new york times. other work by foster, most notably in the 1995
identi   cation of joe klein as the author of primary colors, was to
follow, and by the mid 1990s, foster was established as arguably the
world   s best known    literary detective    [43].

traditional shakespearean scholars reacted with scorn and disbe-
lief, often on the basis of traditional objections such as writing style
and content (for example among many,    that the supreme master of
language, at the close of his career, could have written this work of
unrelieved banality of thought and expression, lacking a single mem-
orable phrase in its 578 lines, is to me unthinkable    [140]). as foster
himself pointed out [42], this is an insistence on    a radically aestheti-
cist ideology whereby the scholar   s literary sensibilities must overrule
bibliographies and empirical evidence,    and that apparently    the elegy
is not good enough to re   ect the genius of a poet who never wrote a

2.4 methodological issues in authorship attribution

245

blottable line.       the shakespeare attribution now rests on a broad and
substantial foundation. what   s required to dislodge it is not just the
overthrow of a few minor points [. . .] but a systematic rebuttal.   

however, such a rebuttal was in the works. other applications of
stylometry by other scholars, among them elliot and valenza [35, 36,
37] mcdonald jackson [68] and brian vickers [148] have uncovered
other evidence refuting the shakespearean attribution, and (perhaps
more seriously) suggesting substantial mishandling of the elegy in the
course of foster   s analysis. their analysis and debate, conducted for
several years in the pages of computers and the humanities, eventually
was able to establish to the satisfaction of all parties [108, 148] that
the elegy had not been written by shakespeare, but was much more
likely to be from the pen of john ford. by 2002, even foster himself
accepted that.

from a purely scienti   c perspective, this cut-and-thrust debate can
be regarded as a good (if somewhat bitter) result of the standard schol-
arly process of criticism. unfortunately, for many non-specialists, this
well-publicized failure was their only exposure to the discipline of sty-
lometry, and the greater the hype that foster managed to create, the
greater and more notable the eventual fall. this public collapse of a
well-known stylometric attribution may have unfortunately created a
public perception of inaccuracy, hindering uptake of the results of attri-
bution research by mainstream scholars. indeed, given the overall accu-
racy of foster   s studies, the perception is probably greater than the
reality, and much of the hindrance is unjust.

2.4 methodological issues in authorship attribution

2.4.1 does it work?

as can be seen from the previous section, a lot of work has been done on
authorship attribution. the key question, however, is does it actually
work? in some areas (for example, in the application of authorship
attribution to legal issues), the issue of accuracy is crucial. without a
well-established (and documented) reason to believe the analysis to be
accurate, it is not even admissible evidence (see section 8.3.1 for more
details). a detective desperate for leads may be willing to grasp at any

246 background and history

straw available, but few scholars would like to rely on techniques akin
to guessing wildly in pursuing their studies. in light of the many public
failures described above, questions of accuracy are probably the most
important issue facing stylometry today.

2.4.2 technical accuracy

there are three major, interrelated, aspects of the accuracy question.
the    rst is the inherent accuracy of the techniques themselves. as was
touched on earlier and as will be discussed at length in later sections,
there are an almost limitless number of techniques that have been pro-
posed for doing authorship attribution. most of these techniques have
been shown to work in small-scale experiments, but there is little cross-
validation or comparison. given the tremendous number of techniques,
which ones are reliable? given the tremendous numbers of analytic
conditions, which techniques are most reliable in any particular cir-
cumstance?

issues of genre, representativeness, and corpus size combine to make
these technical issues more critical. for literary studies, it may be rea-
sonable to expect to have several hundred thousand words by the same
author available in order to construct a training sample, but the sam-
ple sizes available in a typical forensic investigation may only be a few
hundreds or thousands of words [26]. perhaps more crucially, even for
authors whose published works extend to millions of words, the doc-
ument of interest may be in a completely di   erent and incompatible
genre. given the well-understood [13] di   erence in the statistics, and
therefore    ngerprints, across di   erent genres, identifying an accurate
cross-genre technique is an even more di   cult task.

the ease of writing computer programs to implement whatever
techniques are found provides a seductive path to misuse and to
unwarranted claims of accuracy. any skilled programmer could write
a program, for example, to calculate average word length [106] and
to compare a variety of documents on the basis of that calculation.
this almost invites abuse by application outside of its area of validity
(a document in french was written by the same author as a document
in english, because they are both far di   erent from a set of control

2.4 methodological issues in authorship attribution

247

documents in chinese?), but the end user may either not be aware of
these limitations or may deliberately choose to ignore them. the well-
known tendency to trust the output of a computer analysis as reliable    
garbage in, gospel out     adds to this problem. for this reasons, some
authors, most notably rudman, have argued against the creation of
general-purpose authorship attribution programs at all, believing that
their risk of misuse exceeds the bene   ts from their use.

even when the techniques themselves are accurate, that still may
not be adequate for a research study.    because the computer said so   
is not really a satisfactory explanation for many purposes (this is also
one of the major weaknesses with arti   cial neural networks as deci-
sion makers, and one of the reasons that id109 are usually
required to provide explanations of the reasons underlying their deci-
sions). a jury may be reluctant to decide based only on    the com-
puter,    while a researcher interested in gender di   erences in writing
may be more interested in the reasons for the di   erences instead of the
di   erences themselves.

2.4.3 textual considerations

it should be uncontroversial that a corrupt sample makes a bad basis
for analysis; this is no less true for sample texts. however, determin-
ing a clean text for authorship analysis can be a very di   cult, or even
impossible task. in theory, only the features for which the author is
directly responsible should be used for analysis. in practice, it can
be very di   cult to determine which features are the author   s. most
published works are a product of many hands, including the author   s,
the editor   s, the type-setter   s, and possibly the printer   s as well. when
using machine-readable texts (such as from google scholar, jstor,
or project gutenberg, for example), the scanning or retyping process
may have introduced corruptions of its own.

even with    pure    texts, the presence of extraneous (non-authorial)
material can be a problem. running heads, section headings and num-
bers, page numbers, and so forth can shift statistics substantially.
a more subtle issue can arise from quotations. although an author
selects quotations to incorporate, and they can provide clues to author-

248 background and history

ship (for example, a protestant writing a religious tract is unlikely to
cite from the catholic apocrypha), the words used are not the author   s
own and will skew the statistics. in extreme cases, the author may not
even mark quotations or be aware that his phrases are borrowed from a
favorite author. for the cleanest possible samples, all extraneous mate-
rial that did not come from the author   s pen (or keyboard) should be
eliminated, a task requiring extreme care and knowledge on the part
of the researcher (to be discussed further in the next subsection).

finally, the selection of materials is critical. in the words of rudman
[125],    do not include any dubitanda     a certain and stylistically pure
defoe sample must be established     all decisions must err on the side
of exclusion. if there can be no certain defoe touchstone, there can be
no . . . authorship attribution studies on his canon, and no wide ranging
stylistic studies.    of course, this degree of conservatism carries with
it its own risks and problems; by excluding questionable documents,
the    range    of defoe   s style is (possibly) reduced, and in particular,
documents outside of the core genres in which defoe has written may
be less accurately analyzed. selection and preparation of control texts
carry with them similar problems     which authors, which documents,
and which parts of documents are to be included?

forensic analysts such as chaski have replicated these warnings [27]:
   known writing samples should be authenticated independently and
reliably. if samples cannot be authenticated or if there is a possibility
that a suspect or attorney may not be telling the truth about the
authorship of a known writing sample, it is important to seek out other
known samples that can be authenticated.    in the event that such
samples cannot be found, it is almost certainly better not to analyze
at all.

2.4.4 analyst understanding

rudman [126] further points out that many of these problems are conse-
quences of the fact that    most non-traditional authorship attribution
researchers do not understand what constitutes a valid study.    the
question of analyst competence is the second major aspect of the accu-
racy question. in light of the pitfalls discussed above,    does someone

2.5 what would an ideal authorship attribution system look like?

249

trained in physics know enough about plato and all that is involved
with the study of the classics to do a valid authorship attribution study
of a questioned plato work   ? rudman [127] gives the example elsewhere
of the methodological problems of the mosteller/wallace study of    the
federalist papers.    in his words

in the mosteller and wallace study, a    little book of
decisions    is mentioned. this    book,    originally con-
structed by williams and mosteller, contained an exten-
sive list of items that mosteller and wallace unedited,
de-edited, and edited before beginning the statisti-
cal analysis of the texts     items such as quotations
and numerals. unfortunately, neither williams and
mosteller nor mosteller and wallace published the con-
tents of this    little book of decisions    and only mention
   ve of their many decisions in the published work. the
little book has been lost and cannot be recovered or
even reconstructed.

rudman argues that the existence (and loss) of this book may
hide critical and invalidating evidence. certainly, without this book,
no direct replication of the mosteller/wallace study is possible. what
   decisions    have they made that might have biased their results? and
perhaps most importantly, upon what basis did they make their deci-
sions     and are their decisions supportable in light of other scholarship
by historical specialists?

2.5 what would an ideal authorship attribution

system look like?

in light of these di   culties, what should be done? some researchers,
most notably rudman, argue that authorship attribution simply should
not be automated, that the di   culties and potential pitfalls outweigh
the value of any possible product. this review takes a more optimistic
view, while hopefully not downplaying the di   culties.

the starting point is the observation that authorship attribution
is a problem of wide and cross-disciplinary interest. the people who

250 background and history

are interested in the results of authorship attribution are not necessar-
ily either statistics professionals or literature experts, but may include
teachers looking for signs of plagiarism, journalists con   rming the valid-
ity of a document, investigators looking at a crime, or lawyers arguing
over a disputed will. it is therefore necessary for the people who are
statistics and language experts to make the fruits of their research as
widely available as possible.

accuracy of results is of course the primary consideration; a pro-
gram that gives results that are too wrong too often is simply unac-
ceptable. on the other hand, perfection is probably not necessary to
make the program useful. what is more important to the causal user
is probably a built-in con   dence or reliability assessment, and to the
greatest extent possible, a built-in safety net to manage and if neces-
sary enforce the concerns and issues detailed in the previous section. for
example, the ability to select the algorithms and features to use dynam-
ically, based on the language, genre, size, of the available documents
would be a nice feature to have. the ability automatically to detect
areas of suspect validity (quotations, editorial insertions, phrases in for-
eign languages, footnotes, chapter headings and footings, etc.) would
also be a nice, if almost certainly impractical, feature with current
technologies.

in support of this accuracy, a good system would also have a track
record; a well-validated set of results in a variety of areas and problems
to help de   ne both the expected levels of accuracy and demarcate areas
where levels might be expected to drop. implicit in this is a standard-
ized and useful way to de   ne the accuracy of such a program, whether
by standard ir measures like precision/recall, roc curves [138] or by
application-speci   c measures that have yet to fully emerge.

an explanation facility     detailing the key features that allowed the
determination of authorship to be made     would help make the results
of the program more useful to a wide but unsophisticated audience. the
standard issues of interfacing and ease of use would apply to this as to
any other software package.

at least at the current stage of technology, modularity and extensi-
bility are key features. since the ideal feature set has not yet emerged,
it must be possible to add, remove, combine, and test new proposals.

2.5 what would an ideal authorship attribution system look like?

251

finally, an ideal system would be theoretically well-founded. the
e   ectiveness of the best features would be explainable in terms of
linguistic, psychological, and/or neurological features of the writing
process.

few of these features are now available, and some may turn out to
be impossible. however, unrealistic this wish list eventually becomes,
it is nevertheless important to have a goal and to bear it in mind as
one explores the current state-of-the-art.

3

linguistic and mathematical background

3.1 mathematical linguistics

3.1.1 language models

human language can be a very di   cult system to study, because it
combines a maddening degree of variability with surprisingly subtle
regularities. in order to analyze language with computers, it is usually
necessary to make simpli   ed models of language for analysis.

in general, we can observe that a text is structured as an ordered
stream of separate    events    drawn from a population of potential
events. these events may be sounds, letters, words, or perhaps even
phrases or sentences. of course, language is not the only system that
can be so described; researchers have studied other    paralinguistic    [76]
systems (such as music). furthermore, the relationship between di   er-
ent events in the same stream is not purely random, but governed by
high-order regularities.

there are three major models in use to treat these regularities.
the most sophisticated (and psychologically plausible) are context-free
grammars (id18s) and their extensions. in general a context-free gram-
mar [65] is a set of rewrite rules that permit abstract symbols (typi-

252

3.1 mathematical linguistics

253

cally representing grammatical categories) to be re-written as strings of
other categories and speci   c words. for example, (in english) a preposi-
tional phrase (usually symbolized/abbreviated as p p ) might be rewrit-
ten (   ) as a preposition (p rep ) followed by a noun phrase (n p ).
a noun phrase, in turn could be rewritten as a common noun, an arti-
cle followed by noun, or an article followed by one or more adjectives
followed by a noun. thus we have the following grammar in partial
description of english:

    p p     p rep n p
    n p     n ou n
    n p     art n ou n
    n p     art adj-list n ou n
    adj-list     adj adj-list
this model is often used for describing computer languages and
gracefully captures some long-distance structural dependencies (such
as the idea that for every opening brace, a function must also have a
closing brace, or the idea that a prepositional phrase must end with a
noun, no matter how many adjectives intervene). unfortunately, this
model can be computationally intensive, and fails to capture many
other types of dependencies such as lexical or semantic dependencies
(e.g., prepositional phrase attachment, the di   erence between    open
the door with a key    and    open the door with a window   ) more psy-
cholinguistically plausible models, such as context-sensitive grammars,
are available but are still more computationally intensive yet, to the
point that they are not often used.

an intermediate level of complexity is to use a model such as
markov chains, where language is modeled by each word being a
probabilistic function of a    xed window of several adjacent words
in context. this captures short-range dependency structures, but
does not handle long-distance (beyond the    xed window) syntactic
patterns.

more often, language is treated as a simple    bag of words,    a col-
lection of every word that appears in the document without regard
to order (more accurately, a bag of words is a collection of every word
token that appears; in a million-word sample, the word type    the    might

254 linguistic and mathematical background

appear sixty thousand times, and therefore show up in the bag as sixty
thousand separate tokens).

3.1.2 distributions

another key property of language is that the distribution of words
and phrases tends to be highly nonuniform and contextually irreg-
ular. some words are much more common than others, and of
course some contexts may raise or lower the id203 of a given
word dramatically (the overall id203 of any given english
word being    the    is approximately 7%, but the id203 of an
english word being    the    immediately following another    the    is close
to 0%).

the most commonly accepted (if somewhat inaccurate) approxi-
mation to the distribution of words is the so-called zipf distribution
[158], a variation on the zeta distribution. in this distribution, the fre-
quency of an item is inversely proportional to its rank in the distribu-
tion. for example, in the brown corpus [18], the word    the    appears
69836 times out of slightly above one million words. the second most
common word,    of,    appears about half as often (36365 tokens), while
third-place    and    appears about a third as often (28286 tokens). the
e   ect is thus a small number of extremely frequent words (only 8 types
appear with frequency greater than 1%, only 95 appear with frequency
greater than 0.1%), and an extremely long tail of words that appear
only once (43% of the total number of types) or twice (13% of types)
(of course, this is only an approximation; for example, token counts
are integers. more seriously, if the vocabulary of a language is assumed
to be in   nite, then zipf   s formulation fails to yield a distribution as it
fails to converge).

in addition to simple frequency diversity, the properties of words
vary dramatically with their frequencies [158]. for example, common
words tend to be shorter, to be less speci   c in meaning, to be less
decomposable into meaningful units, and so forth. perhaps the most
interesting characteristic is that di   erent parts of speech tend to inhabit
di   erent parts of the frequency spectrum. table 3.1 shows some samples
from the brown corpus.

table 3.1 samples of high frequency, medium frequency, and low frequency words from the
brown corpus.

3.1 mathematical linguistics

255

high frequency
rank
type
the
1
of
2
and
3
to
4
5
a
in
6
7
that
8
9
10

is
was
he

confused
collected
climbed
changing
burden

medium frequency
rank
type
2496
2497
2498
2499
2500
2501
2502
2503
2504
2505

arranged
answers
amounts
admitted

asia

low frequency

type

farnworth
farnum
farneses
farmwife
farmlands
farmland
farmington
farmhouses
farmer-type

farmer-in-the-dell

rank
39996
39997
39998
39999
40000
40001
40002
40003
40004
40005

in this table, the    rst ten words have token frequencies varying
from about 60,000 (out of a million-token sample) to about 10,000.
the second ten are taken from about the 2500th rank, and all have
identical token frequency of 44. the last ten are taken from about rank
40,000 and appear only once (in this two cases, the rankings are of
course somewhat misleading, as ranks 2496   2505, among others, are
all tied). more strikingly, the    rst list is dominated by function words
such as articles, prepositions, conjunctions, the second by verbs, and the
third by nouns. this is a typical    nding across corpora and languages,
although of course the exact numbers vary.

a key feature of these common words, then, is that they are so-
called    closed-class    or    function    words with relatively little meaning
of their own. these words instead serve to relate other words and to
provide cues [49, 110] to the grammatical and semantic structure of the
rest of the sentence.

a related linguistic feature is the idea of    markedness.    in general,
if the most common words are the shortest, the most common and pro-
totypical expressions of a given concept are equally the shortest. non-
prototypical expressions are typically    marked    by the addition of extra
words or morphemes. for example, the english (unmarked) word    car   
usually describes a gasoline-powered vehicle. non-prototypical power
sources require extra words:    hybrid cars,       diesel cars,       electric cars,   
   solar-powered cars    (by contrast, a    truck    is diesel-powered; there   s
nothing linguistically magical about gasoline itself). the marking

256 linguistic and mathematical background

can involve extra words as above, additional morphology (   conven-
tional   /   unconventional   ), separate word forms (   pope   /   ponti      ),
unusual phonology, stress, and many other possibilities. it can also
involve nonprototypical structures (   john dropped the plate   /   the
plate was dropped by john,    unusually highlighting the patient instead
of the agent) or in some cases simple variation from common norms;
putting the verb before the agent/subject (   in the clearing stands a
boxer and a    ghter by his trade   ) or the object before the subject
(   normally i don   t like spicy food, but this i like   ) illustrate some
other forms of markedness.

it is precisely these areas of distributional variation that have proven
fruitful in authorship research. both function words and marked con-
structions represent varying ways in which the author may choose to
represent a given concept, and therefore di   erent authors may (reliably)
choose di   erent ways.

3.2

id205

3.2.1 id178

authorship attribution research has borrowed heavily from the    eld
of id205 in order to measure how reliable a given cue is.
the roots of the discipline lie in [129, 130], where shannon analyzed
all communications as a series of messages along a channel between an
information source and a listener, and established information of such
a source, measured in bits, is given by the id178 equation

h(p ) =     n(cid:1)

i=1

pilog2pi,

(3.1)

where p is the distribution of messages for that source and pi is the
id203 that message i is sent.

markedness can also appear in the syntax, for    xed n, the id178
h achieves a maximum when the pi are all equal (any message is
equally probable), dropping to zero when a single id203 pk = 1
and all others = 0 (only one speci   c message is likely or even possible).
this equation lets researchers not only measure the amount of infor-
mation in a speci   c set of messages, but also compare di   erent sets (as

3.2 id205

257

discussed below). in addition, the relevant quantity for a speci   c mes-
sage (pklog2pk) provides a measure of the unlikeliness of that speci   c
message.

3.2.2 cross-id178

implicit in the above formulation is the idea that researchers have an
estimate or measurement of the distribution for a given source. in many
cases, of course, they will not have exact knowledge. the degree to
which an actual distribution p di   ers from an inferred distribution q
is given by the cross-id178 equation

h(p, q) =     n(cid:1)

pilog2qi.

i=1

(3.2)

this provides a metric for measuring how much event distributions
p and q di   er; it achieves a minimum (the actual id178 of p ) when
p and q are identical. the di   erence, the so-called    kullback   liebler
divergence,    or kl-distance, is thus de   ned as h(p, q)     h(p ).

3.2.3 kolmogorov complexity

a major weakness with the simple formulation of id178 above is
the identi   cation of the    event    space of interest. are events words,
parts of words, phrases? furthermore, the probabilities may disguise a
hidden dependency structure unknown to the analysts. for this reason,
other researchers rely on a di   erent formulation of    information    in the
form of kolmogorov complexity [97] (also called chaitin complexity).
kolmogorov complexity measures the informativeness of a given string
(not, as in shannon   s formulation, a message source) as the length
of the algorithm required to describe/generate that string. under this
formulation, a string of a thousand alternating    a   s and    b   s would be
easily (and quickly) described, while a (speci   c) random collection of a
thousand    a   s and    b   s would be very di   cult to describe. for a large
corpus of messages, this could be used as an operationalization of the
average amount of information contained per message.

this illustrates the close relationship between shannon id178 and
kolmogorov complexity: shannon   s id178 is an upper bound on (and

258 linguistic and mathematical background

under appropriate assumptions, asymptotically equal to) kolmogorov
complexity. although the mathematics required to prove this is non-
trivial, the result can be seen intuitively by observing that a decom-
pression program and a compressed    le can be used to (re)generate the
original string. a more complex string (in the kolmogorov complexity
sense) will be less compressible, and thus require a larger program +
compressed text system to reconstruct.

unfortunately, kolmogorov complexity is formally uncomputable,
in a strict technical sense related to the halting problem. although it
is possible to prove that an algorithm will output a given string (by
running it), it is not possible to prove that an algorithm will not output
a given string     some    algorithms    simply run forever without stop-
ping, and we cannot always tell beforehand which ones those will be.
so one can easily show that a given (short) program will output the
string of interest, but not that it is the shortest possible program, since
another, shorter, one might do it. despite this technical limitation,
kolmogorov complexity is of interest as an unattainable ideal. if, as
argued above, kolmogorov complexity represents the ultimate possible
   le compression, a good    le compressor can be seen as an attempt to
approximate kolmogorov complexity within a tractable formal frame-
work [73]. again, this can be extended to a measurement of similarity
between two strings in the degree to which they compress similarly
[12, 96].

3.3 matrices and vector spaces

the preceding discussion should give an idea of the huge number of
ways in which di   erences between texts can be operationalized and
measured. it is sometimes convenient to use vector representations to
capture a large number of measurements in this way. for example, one
could measure the percentage of one-, two-, three-, . . . up to ten-letter
words in a document as ten individual measurements, then capture
these measurements as a vector of ten numbers. this implicitly embeds
that document in a ten-dimensional vector space at a particular point.
this vector space model should be a familiar construct to most ir
practitioners. it also implicitly creates another metric for comparison

3.3 matrices and vector spaces 259

of texts; two texts that are close in this vector space are close in the
aspects of style the vector components measure. this particular vec-
tor space implicitly captures elements of style related to word length
distribution.

a key problem with vector space models can be the number of
dimensions and the concomitant computational burden. for this rea-
son, vector space models are often used in conjunction with one or
more methods of factor analysis to reduce the number of variables.
this analysis serves two purposes:    rst, to identify dependencies and
correlations within the di   erent vectors, and second, to identify the
most salient dimensions of variation within the vector space. two typ-
ical types of factor analysis are principal components analysis (pca)
and id156 (lda).

pca is a method of statistical analysis that simply rotates the
underlying data space in order to create a new set of axes that capture
the most variance in the data. in particular, the    rst    principle compo-
nent    is the axis on which the data has the most variance. the second
principle component is the axis orthogonal to the    rst that captures the
next greatest variance, and so forth. two dimensions in which the orig-
inal data strongly correlated would thus largely be captured by a single
new dimension that is the vector sum of the original two. algorithms
for performing pca (via calculating the singular value decomposition
of the covariance matrix) are well-known [147] and relatively e   cient.
pca provides an easy method for both reducing the dimensionality of
a dataset (by focusing on only the    rst few, often the    rst two, prin-
cipal components) and for creating diagrams for easy visualization or
cluster analysis of data.

pca is not, however, well-optimized for the task of classi   cation.
the dimensions that are the most variable are not necessarily the most
salient ones. other methods (notably id156
and support vector machines) can address this by inferring which
dimensions are the most informative (information, of course, being
task-speci   c) instead of merely the most variable. similarly, modern
machine learning methods such as support vector machines can deal
directly with high-order vector spaces and do not need to perform fea-
ture analysis or other id84.

260 linguistic and mathematical background

of course, other classi   cation techniques such as k-nearest neigh-
bor, id90, and so forth are equally well-known and can also
be applied. the key research question for authorship attribution thus
becomes one of feature identi   cation and the proper sort of classi   ca-
tion technique to use.

3.4 a theoretical framework

in light of these di   erences among models and analytic frameworks,
comparison of two di   erent methods for authorship attribution may
be problematic. furthermore, it may be di   cult to merge or hybridize
two di   erent methods. to mitigate this, juola [76, 81] has proposed a
theoretical and computational framework in which the di   erent meth-
ods could be uni   ed, cross-compared, cross-fertilized, and evaluated to
achieve a well-de   ned    best of breed.   

the proposed framework postulates a three-phase division of the
authorship attribution task, each of which can be independently per-
formed. these phases are:

    canonicization     no two physical realizations of events will
ever be exactly identical. one can choose to treat similar real-
izations as identical to restrict the event space to a    nite set.
    determination of the event set     the input stream is parti-
tioned into individual non-overlapping    events.    at the same
time, uninformative events can be eliminated from the event
stream.
    statistical
id136     the remaining events can be
subjected to a variety of inferential statistics, ranging from
simple analysis of event distributions through complex
pattern-based analysis. the results of this id136 deter-
mine the results (and con   dence) in the    nal report.

as an example of how this procedure works, we consider a method
for identifying the language in which a document is written. we    rst
canonicize the document by identifying each letter (an italic e, a
boldface e, or a capital e should be treated identically) and produc-
ing a transcription. we then identify each letter as a separate event,

3.4 a theoretical framework

261

eliminating all non-letter characters such as numbers or punctuation.
finally, by compiling an event histogram and comparing it with the
well-known distribution of english letters, we can determine a probabil-
ity that the document was written in english. a similar process would
treat each word as a separate event (eliminating words not found in a
standard lexicon) and comparing event histograms with a standardized
set such as the brown histogram [94]. the question of the comparative
accuracy of these methods can be judged empirically. this framework
allows researchers both to focus on the important di   erences between
methods and to mix and match techniques to achieve the best practical
results (in this case, are letter frequencies or word frequencies a better
indicator of language?     a question of possible importance to both
cryptographers and to designers of web search engines).

it should be apparent how classical methods mentioned in sec-
tion can be described in this framework. for example, yule   s average-
sentence-length [153] can be captured by canonicizing (removing page
breaks and such), treating each sentence as an individual event, and
determining the length of each event and taking a document-level aver-
age. we will use this framework and model to discuss the more modern
approaches in the following sections.

4

linguistic features

4.1 vocabulary as a feature

the simplest way to con   rm or refute authorship is simply to look for
something that completely settles the authorship question. as a good
example of this, the vocabulary of the beale cipher [92, 93, 132] would
seem to be conclusive proof of a hoax. the story is simple. in 1822,
a man named beale purportedly buried some treasure, leaving behind
him a letter and a coded set of directions to    nd the treasure. this
story was published in the 1860s, when the man to whom the letter
was entrusted decided to go public. but did it happen that way?

one sentence of the letter of 1822 reads    keeping well together they
followed their trail for two weeks or more, securing many, and stam-
peding the rest.    unfortunately, the word    stampede    is    rst attested
by the oxford english dictionary as appearing in 1844; the earliest
variant spelling (   stompado   ) dates back to 1826, four years after the
date on the letter. similarly, the 1822 letter speaks of    improvised   
tools, a meaning that the word    improvise    would not take until the
1850s. the word    appliance,    also appearing in the letter, had been
used by shakespeare, but was considered obsolete and outdated until

262

4.1 vocabulary as a feature

263

it became popular again in the early 1860s [4]. the conclusion is almost
inescapable that the    beale letters    are in fact late 19th century forg-
eries. one would be equally suspicious of a hitler diary that mentioned
the hiroshima bombing or the construction of the berlin wall.

it is clear, then, that the individual words an author uses can be
strong cues to his or her identity. the vocabulary labels the document
as written at the time when the vocabulary existed, and most likely
when it was current. speci   c words can label the author by group
identity     as hinted at in an earlier section, an author who writes of
   colour,       honour,    and    ironmongers    is likely to be british. similarly,
one who writes of sitting on a    chester   eld    is not only presumptively
canadian, but an older canadian at that [33]. studies such as [70]
have found many examples of words that vary strongly in their usage
across space and time. idiosyncratic misspellings can even identify peo-
ple as individuals. wellman [149] gives an example of using a speci   c
misspelling (the word    *toutch   ) to validate the authorship of a dis-
puted document in court. he had noted this particular misspelling, and
(under the guise of eliciting handwriting samples) was able to persuade
the witness to write a phrase containing    touch    while on the witness
stand. showing that she had in fact written    *toutch,    wellman was
able to satisfy the court that was how she spelled that particular word,
and hence that the document with that misspelling had been written
by the witness.

there are two major problems with this kind of analysis. the    rst
is that it is relatively easy to fool, because the data can be faked easily.
as an american writer, i nevertheless know all about    ironmongers   
and    colours   ; in fact, journals will often make demands like    standard
british spelling should be used throughout,    and expect their yankee
authors to be able to adjust their spelling appropriately. editors will
also routinely adjust spelling of individual words prior to publication.
thus the presence or absence of any individual word, especially in a
published document, may not be compelling.

a more serious problem is that the words themselves may not be
present. idiosyncratic spelling or not, the word    touch    is rather rare.
the million word brown corpus [94] lists only 87 tokens (the same as
the word    battle   ) or approximately one instance per 30 pages of text.

264 linguistic features

   honor    is even rarer (66 tokens), and    stampede    gets only four. an
attribution method requiring, on average, a quarter of a million words
before it becomes useful is not going to see widespread application.

4.2 vocabulary properties as features

a more reliable method, then, would be able to take into account a
large fraction of the words in the document. one way to do this is
to look at large-scale overall statistics. as discussed above, each word
in the document has an age that can be used to date the document.
similarly, every word has other properties such as length, number of
syllables, language of origin, part of speech, and so forth. many clas-
sical approaches (as discussed in section 2.3.1) have tried using this
sort of super   cial feature with simple summary statistics (e.g., taking
the mean of the data). for example, kruh [92, 93], following [111, 150],
calculated the average sentence length in the beale manuscripts as fur-
ther evidence that they were forgeries. one particularly compelling
measure is    vocabulary richness,    a measure of the estimated size of
the author   s vocabulary. grieve [50] cites not less than    fteen di   er-
ent ways of estimating this, ranging in dates from the 1940s to the
1980s. in a pre-computer age, this probably describes the limit of the
feasible. unfortunately, these methods have not been su   ciently accu-
rate to rely upon. it remains intuitively plausible that di   erent people
have di   erent preferred vocabulary (and di   erent vocabulary sizes); it
is not clear at this writing whether other, more sophisticated methods
of id136 could achieve decent accuracy based on data such as word
length, and further work is obviously required.

another approach is to limit the    features    to a reasonably sized
subset of the document vocabulary. the synonym pairs of mosteller
and wallace [112] have already been mentioned, as has the idea of
   function words.    in both of these cases, the intuitive idea is that the
words in the chosen subset are both more common than any individ-
ual words, but more likely to vary in an interesting and informative
way. another advantage that function words have is that they are rela-
tively easy to spot from their high frequency alone     many researchers
[8, 20, 63, 145] have thus used    the most frequent n words in the

4.3 syntactic properties as features

265

corpus    as a stand-in for a more principled de   nition of such function
words. given the current state-of-the-art, it is probably fair to say that
the accuracy baseline as currently established, as well as the most well-
known techniques, derive from simple statistics such as pca, applied
to the top 50 or so words of the corpus [20, 22].

4.3 syntactic properties as features

one reason that function words perform well
is because they are
topic-independent. but the reason for this topic-independence is itself
interesting. function words tend to be semantically    bleached,    and rel-
atively meaningless, as one can see by trying to de   ne    the    or    of    in a
satisfactory and non-circular manner. instead, function words describe
relationships between content words; i.e., syntax. for instance, the main
   function    of the word    of    is simply to establish a (fairly vague) rela-
tionship between two nouns. to the extent that other prepositions do
the same thing, there is a large degree of synonymity (is so-and-so a
student    at    a particular university,    in    a particular university, or
   of    that university?) and hence free variation and personal choice in
the statistics of the speci   c words. they also may re   ect larger-scale
synonymity such as between active and passive constructions, the use
of rhetorical questions against simple declaratives, or the use of con-
junctions instead of a series of individual assertions.

in other words, a person   s preferred syntactic constructions can
also be cues to his authorship. one simple way to capture this is to
tag the relevant documents for part of speech (pos) or other syn-
tactic constructions [137] using any of the standard taggers [16, 31].
punctuation can be another easily accessible source of such informa-
tion [25]. the downside of such processing, especially for pos tag-
gers, is the introduction of errors in the processing itself; a system
that cannot distinguish between contraction apostrophes and clos-
ing single quotes or that can only tag with 95% accuracy will con-
   ate entirely di   erent syntactic constructs, muddying the inferential
waters. an alternative approach that combines lexical and syntactic
information is the use of word id165s (bigrams, trigrams, etc.) to
capture words in context. in this scheme, the bigram    to dance    is

266 linguistic features

clearly di   erent than    a dance   ; distinguishing between people for
whom    dance    is primarily a noun or a verb. this allows the infer-
ence mechanism to take advantage of both vocabulary and syntactic
information.

of course, nothing prevents one from combining these features, for
example, by analyzing pos id165s [7, 9, 89] as a more-informative
alternative to individual pos tags. this list is of course illustrative,
not exhaustive.

4.4 miscellaneous features

4.4.1 orthographic properties as features

one weakness of vocabulary-based approaches is that they do not
take advantage of morphologically related words. a person who writes
of    dance    is also likely to write of    dancing,       dances,       dancer,   
and so forth. some researchers [71, 74, 117] have proposed instead
to analyze documents as sequences of character. for example, the
character 4-gram    danc    is shared in the examples above, something
that a pure vocabulary analysis would miss (although of course this
could be addressed via id30 as a pre-processing step). this infor-
mation could also be obtained via a more in-depth morphological
analysis.

4.4.2 structure and layout as features

for many kinds of text, web pages, presentations, and wysiwyg   ed
documents among them, aspects of layout can be important clues to
authorship [1]. these tools give authors control over text formatting
and layout, including speci   c aspects such as font choice, font sizing,
placement of    gures and clip art, or the use of color. similar features
for typewritten documents could include the use of justi   cation, the
number of spaces that follow punctuation such as periods/full stops,
the location of tab stops, and so forth.

computer code, in particular, may lend itself to this kind of anal-
ysis, due to the importance many authors place on    style    in their
programming. the di   erence between

4.4 miscellaneous features

267

int foo(int i, int j) {

// comment here

and

int
foo(int i, int j)
{

/* comment here */

or even

// comment here
int foo(int i, int j) {

is almost entirely irrelevant to anyone except the author of the code,
and yet strongly supported by most development environments.

there are, of course, two chief weaknesses of this particular feature
set. the    rst is that they are very strongly register-bound, perhaps
more so than other features. at least to a limited extent, the vocabulary
of a given author is register-independent (if you do not even know the
word    zymurgy,    you will not ever use it, regardless of register). the
features used to lay out a web page are not necessarily the same as one
would use to lay out a powerpoint presentations; the coding standards
one follows writing c++ are not at all the same as one would use to
write lisp, let alone english. the second is that layout is particularly
vulnerable, much more so than vocabulary, morphology, or syntax, to
editorial tinkering such as pagination, standardization of fonts, and
so forth. we can be con   dent that the author of a scienti   c journal
planned to put    gure 1 into her article. unless she delivered camera-
ready copy, or edited the volume herself, she could not possibly have
planned to put it at the top of the second column of page 138. even
just transferring a document from one format to the other may result
in substantial reformatting.

4.4.3 anomalies

an interesting approach has been taken by [26, 43, 90], focusing specif-
ically on usage anomalies. this approach cuts across all linguistic levels

268 linguistic features

and instead analyzes linguistic idiosyncracies. the example of spelling
mistakes (   toutch   ) and cultural di   erences (   colour    vs.    color   ) have
already been mentioned, but di   erences in grammar and usage could
also qualify. to observe this, the document can be compared against
a reference standard (using spelling and grammar checking tools) and
the points of di   erence noted. chaski, in particular, argues [26] that
   markedness,    as de   ned earlier, is a key identi   er that    pervades all
levels of language, from id102 to morphology to syntax to seman-
tics to discourse,    and that markedness per se can be a useful feature
category. she supports this with a series of experiments on her own
corpus [25] (but see also [48, 104]).

4.4.3.1 metadata

another fruitful area for investigation is in the use of metadata to
determine document authorship. indeed, under the guise of    digital
forensics,    this may be one of the best-developed and most understood
kinds of authorship attribution. looking at the headers of the relevant
piece of email shows where it came from, and often other information
besides. even when deliberately masked, experts can often get some
kind of useful information from the type of masking     at least enough
to know where to ask the next set of questions.

similar metadata often exists in complex documents. microsoft
word, for example is famous for burying information in the document
such as the date of creation, the program (including location) used to
create it, the user who created it, the revision history, the original    le
name, and many other pieces of information. this information is not
ordinarily visible, but is embedded in plain-text within the    le itself.
from a practical standpoint,    nding the name    i. m. originalauthor    in
the metadata may be a clearer    smoking gun    than abstract statistical
analysis when one wants to deal with a recalcitrant student-plagiarist.
similarly, it is possible to embed speci   c metadata into a document
in order to support a later assertion of authorship. this process goes by
various names such as    digital watermarking    or    steganography    and
is another well-studied and mature discipline. by their very nature,
the analysis and evidence usually associated with these processes is

4.5 caveats

269

not of the same type as the soft statistics described elsewhere in this
section.

at the same time, the potential informativeness of metadata should
not be overlooked (and could be considered as a generalization of for-
matting/layout features described earlier) [155]. that a document was
written on microsoft word at 10 a.m. are two    features    that might
tell against a theory of authorship by a notoriously nocturnal linux
advocate, and should be weighed accordingly.

4.5 caveats

rudman [124] has estimated that over a thousand di   erent features
have been used in authorship attribution studies. obviously, space pre-
vents a full discussion of each and every one. equally obviously, no con-
sensus has emerged about what features are the best. it seems likely
that no individual feature is universally informative enough, and that
the best results will come from analysis of an extremely broad set of
features [1, 157] covering many di   erent approaches. this of course
raises the question about how to combine information from di   erent
feature types, a question best left to the analysis process.

it is also likely that, even with this broad-brushed approach, dif-
ferent languages and registers will still require di   erent features; the
discussion of    function words,    for example, has been highly english-
speci   c and may not apply directly to strongly in   ected languages such
as finnish and turkish (although see section 6.3 for an interesting
empirical take on this).

4.6 other domains

it is interesting to speculate about what other domains might come
under this kind of analysis. natural language, of course, is not the only
creative product of the human mind, nor the only thing that is pla-
giarized. cases of art forgery are well-known (hans van meergan is
probably one of the most famous), and in many cases, the    style    of
the painting is one of the key points under dispute. music is another
area ripe for plagiarism, as both joyce hatto and george harrison

270 linguistic features

illustrate, and of course copying of source code is the bane of computer
teachers everywhere. but, as the discussion above has already illus-
trated, authorship disputes can be analyzed using the same methods,
using an appropriate choice of features.

features for code have already been discussed; because of the strong
in   uence of natural languages (usually english) on their designs, many
of the same lexical and syntactic features useful in english may be
useful in analyzing code. in many cases, there are clear-cut examples
of synonymity (such as the equivalence between for and while loops,
or between if and case statements), while function and variable names
provide ample opportunity for programmers to show their individual
quirks.

juola [76] has presented a similar analysis for music; focusing on
individual bytes (analogous to characters) in the digital representation
of songs, he was able to identify the performer of the relevant songs. he
also presented several other feature-sets that could in theory be applied
to the analysis of music, such as encoding monophonic melodies as a
series of numbers representing the number of semitones between each
note and the next, encoding each note as its representation on a scale
(do, re, mi, etc.), or a parsons    encoding of whether each note was the
same, higher, or lower, than the preceding one (e.g.,    * s s d u s s d   
is the parsons    code for beethoven   s 5th symphony).

what do natural language, computer code, and music have in com-
mon? juola identi   ed three key characteristics. first, like text, both
code and music are created from a sequence of distinct events, whether
those be keypresses on a typewriter keyboard or a piano. second, these
events are drawn from a    nite (or at least    nitizable) set; there are
only 88 possible notes on a piano keyboard. compared to the number
of characters in chinese or possible and actual words in english, this is
tiny. third, these events occur in a time-structured sequence. analysis
along the line of    what event is likely to come next    is thus capable of
distinguishing usefully in all three domains.

juola used the term    paralinguistic    to describe such domains. it is
interesting to think about what other creative enterprises are paralin-
guistic. dance, for example, can certainly be described as a sequence of
body shapes; it is not clear whether or not the set of these shapes are

4.7 summary

271

   nite, but the existence of various dance notations [141] suggests that
they are at least    nitizable. a future researcher might examine di   er-
ent dancers in an e   ort to    nd key aspects of a choreographer   s    style.   
gestures, as in the game of charades, is another type of sequenced set
of body positions, and may prove to be paralinguistic. film, of course,
is a sequence of frames, but the potential set of frames may be (for
all practical purposes) in   nite and therefore too large for this sort of
analysis to be fruitful. still art, such as a painting or photograph, is
not sequenced, and would therefore probably not qualify.

4.7 summary

in the proposed framework, the    rst step in authorship analysis is the
identi   cation of the feature set of interest, as preparation for later ana-
lytic phase. the variety of proposed feature sets is potentially bewil-
dering, and little consensus has yet emerged about the best features,
or even the best types of features, to use. proposals have covered all
levels of language and even included salient non-linguistic features such
as document layout.

the end result of this is to reduce a set of documents to a set of
ordered feature vectors, which are then analyzed to produce authorship
judgments. at the very grossest level, the vectors that are    most sim-
ilar    are probably by the same author, and the question is simply the
best and most e   cient way to identify this similarity. unfortunately,
as will be seen in the next section, little consensus exists there, either.

5

attributional analysis

once the relevant features have been extracted from the documents
of interest, the next task is to determine by analysis of the features
which document was written by which author. again, the number of
methods proposed and used is staggeringly large, and space precludes
a full discussion of more than a few representative methods.

a key factor involved in the selection of an analysis method is
an understanding of the requirements of the    nal answer. a forensic
authorship report to be presented at a jury trial, for example, needs to
be understandable to the general public (the jury) in a way that some
specialists may not need. graphics and visualizations may be important
as a way to make authorship distinctions clear. it may be important not
just to present the results, but to explain them in terms of underlying
aspects of the documents; to some extent, this simply replicates the age-
old debate between id109 and neural networks        because
the computer says so    is not a particularly satisfactory explanation,
even if the computer is usually right.

other important factors involve questions such as the amount
and type of training material available. for example, the distinction
between    supervised    and    unsupervised    techniques applies here as

272

5.1 unsupervised analysis

273

elsewhere in machine learning. supervised techniques require a priori
knowledge of class labels, often in the form of sample documents of
undisputed authorship. unsupervised techniques are more appropriate
for data exploration, with no prior information needed. these analytic
methods will be addressed separately.

5.1 unsupervised analysis

unsupervised analysis acts without regard to the presumptive attribu-
tions already made, and instead looks for super   cial patterns in the
data. at its simplest level, just making a data scatterplot and look-
ing for groupings is a type of unsupervised analysis. and, still at this
simple level, much authorship analysis can be done this way.

5.1.1 vector spaces and pca

with the feature structure de   ned in the previous section, it should
be apparent how documents can be described in terms of collections
of features; quantifying the features, in turn, will implicitly create a
high-dimensional    document space    with each document   s feature set
de   ning a vector or a point in that space. for example, the token fre-
quency of    fty well-chosen words de   nes a    fty-place vector for each
document (some id172 would probably be necessary). simple
visual inspection of this high-dimensional space may reveal interesting
clusters; in particular, if two documents are    close,    they may have
similar authors.

there are two major problem with this. the    rst is just the di   culty
of visualizing    fty-dimensional space, while the second is the problem
of independence of the various dimensions. in general, the correlation
between any two features (across many documents) will not be zero.
for example, documents written in the    rst person will often have a
high frequency of    rst person pronouns such as    i    or    me.    they are
also likely to have a high frequency of    rst person verbs such as    am.   
therefore, these frequency measurements are not independent, since
they were really just all measurements of the degree of    rst-person
exposition in the writing (in a more technical phrasing, the correlation,
or covariance, among all those frequencies will be positive). this will

274 attributional analysis

act to weight the single feature    personness    three times as heavily,
since it is represented by three separate features.

to address this, researchers [2, 14, 20, 48, 59] often use principal
component analysis (pca), as described brie   y in an earlier section.
technically, pca is simply the eigenvectors of the covariance matrix
among a set of features. informally, pca determines a smaller set of
(orthogonal, independent) basis vectors that describe as much of the
variation in the initial data set as possible. in particular, the two prin-
cipal components (technically, the eigenvectors with the largest eigen-
values) describe the original data set in an easily visualizable two-
dimensional space while preserving as much as possible the similarity
and    closeness    between individual data items.

the most popular feature set for this kind of analysis, following
the work of burrows [20], is the token frequency of the top    fty or so
most common word types in the document set. applying this analysis
method and plotting the results will often produce clear visual separa-
tion between di   erent authors, as in figure 5.1.

fig. 5.1 pca separation of various authors based on function word frequencies (data and
image courtesy of david hoover).

5.1 unsupervised analysis

275

in this    gure, glasgow can be clearly seen to be in the upper left,
wharton in the center, lewis above dreiser on the right, and james
in the bottom center. learning that an unknown manuscript was also
in the bottom center would be a strong evidence for its authorship by
james instead of lewis or glasgow.

5.1.2 multidimensional scaling (mds)

an alternative approach for unsupervised data exploration involves the
calculation of intertextual di   erences as distances. a full discussion of
such distance calculations would be mathematically dense, but it should
be intuitively plausible that any similarity measure can be extended to
a    distance.    the actual mathematics are slightly more complicated,
since the de   nition of    distance    (or more accurately    metric   ) involves
some properties such as symmetry, the triangle inequality, and the idea
that the distance between any entity and itself should be zero. for true
distances, however, it is then possible to embed the distance structure
in a high-dimensional abstract space while preserving (as much as pos-
sible) the distances themselves.

the primary process by which this is done is called multidimensional
scaling (mds). essentially, mds operates by placing objects in a space
of a previously de   ned dimension (usually two or three dimensions)
such that the total error introduced is minimized. once this space is
created, it can be examined for apparent patterns.

two examples of this should su   ce. figure 5.2 shows a two-
dimensional mds plot of juola   s    cross-id178    [71] distance as
applied to a set of documents (kostic, p.c., see also [75]) by three
(four, counting the unknown, unrelated, and uninteresting introduc-
tion) di   erent authors. the visual separation here corresponds largely
to a separation in time; danilo and author a are both medieval ser-
bians (author a, known to history as    student,    identi   es himself as
a student of danilo   s in this work), while b is a later interpolator;
the sections written by b are clearly separable, showing that a   s style
is almost indistinguishable (by this test) from his mentor   s, but b is
clearly di   erent (and the writing of sample 2, attributed to danilo
himself, is atypical and may stand re-examination).

276 attributional analysis

4

3

2

1

0

1
   

i

 

2
n
o
s
n
e
m
d

i

unknown t01.txt

danilo t02.txt

danilo t04.txt

danilo t03.txtdanilo t05.txt

danilo t06txt

a 118.txt
a 117.txt

danilo t12.txt

b t15.txt

b t18.txt
b t13.txt
b t14.txt

b t10.txt
b t17.txt
b t16.txt
b t19.txt

b t09.txt

b t11.txt

   4

   2

0

2

4

6

dimension 1

fig. 5.2 authorship of lives of kings and archbishops.

figure 5.3 shows two dimensions out of a three-dimensional mds
projection of some novel-length works by jack london [79]. the num-
bers indicate date of publication. here there is no di   erence in    author-
ship,    but the pattern clearly indicates a visual separation (at the
indicated vertical line) between early and late works, happening at
about 1912. this, in turn, may indicate a signi   cant stylistic change of
unknown type happening at this time     a possible line of inquiry for
interested literature scholars.

in both cases, the visual separation provides a strong indication of

substantive di   erences in style.

5.1.3 cluster analysis

a third form of unsupervised analysis is (hierarchical) cluster analysis.
as with mds, cluster analysis presumes the existence of a distance
measure between document pairs, either measured directly or inferred

1914

1915

5.2 supervised analysis

277

1918

1911

1904

1917

1907

1905

1907

1911

1907

1903 1908

1904

i

2
 
n
o
s
n
e
m
d

i

1913

1913

1917

dimension 1

fig. 5.3 chronological development of style in the writings of jack london (data and    gure
adapted from [79]).

from a metric applied to a vector space. in both cases, cluster analysis
proceeds by grouping the closest pair of items into a cluster and then
replacing that pair of items by a new item representing the cluster
itself. at each step, the number of items analyzed is reduced by one,
until    nally all items have been joined into a single cluster.

the result of such analysis is usually displayed as a cluster dia-
gram (sometimes also called a dendrogram), a rooted tree with binary
branching. the height of each internal node represents the distance at
which the closest pair was found, and the children of that node (either
individual documents or subtrees) represent the two items joined at
that step. an example dendrogram is provided in figure 5.4. this,
again, provides a visual indication about what kind of structure and
similarities are present in the document set.

5.2 supervised analysis

in contrast to unsupervised analysis, supervised analysis requires that
documents be categorized prior to analysis. it is perhaps unsurprising

278 attributional analysis

fig. 5.4 dendrogram of authorship for    ve novelists (data and    gure courtesy of david
hoover).

that this additional information can be helpful in arriving at methods
of categorization.

5.2.1 simple statistics

the simplest form of supervised analysis, used since the 1800s, is sim-
ple descriptive statistics. for example, given a set of documents from
two di   erent authors, we can easily calculate word lengths [106] and
(handwaving a few statistical assumptions) apply t-tests to determine
whether the two authors have di   erent means. once we have done
that, we can apply id28 to estimate the authorship (and
our con   dence in that authorship) of a novel document (with more
than two authors, we could use anova to similar purposes). a sim-
ilar analysis, using di   erent features, could analyze the mean number
of syllables per word, number of words per sentence, percentage of
passive constructions, etc. for those unwilling to make the necessary
assumptions of independence and normal distribution, non-parametric
statistics such as the wilcoxon test can be used. the mathematics of
such statistics is well-known and needs no detailed explanation.

5.2 supervised analysis

279

unfortunately, these simple methods produce equally simple fail-
ures. more accurately, no single feature has been found that robustly
separates di   erent authors in a large number of cases. but these simple
statistics can be and have been combined successfully.

the most notable example of this is burrows       delta    method
[21, 64, 138]. in its original form, burrows analyzed the frequency of
the 150 most frequent words in a collection of restoration poets. for
each of these word variables, he calculated an appropriate z-distribution
(essentially, an estimate of the mean frequency of the word as well as an
estimate of the variance in that frequency). individual documents were
scored on each of the 150 variables regarding how far their frequencies
were above/below the norm. a positive z-score indicates a word more
common than average, a negative one indicates a word less common
than average, and of course a zero z-score indicates a word appearing
with exactly average frequency. burrows then de   ned the delta mea-
sure as    the mean of the absolute di   erences between the z-scores for
a set of word-variables in a given text-group and the z-scores for the
same set of word-variables in a target text    [21]. the smallest delta,
representing the greatest similarity to the training text-group, is the
category to which authorship of the test document is assigned.

performance of burrows    delta has generally been considered to be
very good among attribution specialists, and it has in many cases come
to represent the baseline against which new methods are compared.

intuitively, delta can be viewed as creating a 150-dimensional vec-
tor space of word frequencies, scaling each dimension by the frequency
variation to normalize the degree of dispersion, and embedding indi-
vidual documents in this space (the analysis to this point is of course
unsupervised). he then applies a simple metric to the space (the mean
in this case is a simple re-scaling of the l1-metric) to obtain average
distances between the test document and the training documents (by
category), selecting the appropriate category based on a variation of
the nearest neighbor algorithm (choosing the nearest category instead
of document) (see [138] for further discussion of this analysis). once
this general framework has been observed, variations can easily be sug-
gested. some simple variations would include adjusting the number
of word-variables and therefore dimensions or the basis on which the

280 attributional analysis

word-variables are selected, adjusting the metric imposed upon vector
space, or adjusting the basis on which the judgment is made.

david hoover [63, 64] has made extensive study of such variations.
examples of the variations that he has studied include changing the
number of words studied (ranging from 20 to 800 and beyond), elim-
inating contractions and/or personal pronouns from the set of word-
variables, and    culling    the list of word-variables by eliminating words
for which a single training document supplied most (70% [64]) of the
words. he found the greatest accuracy occurred in a 700-dimensional
space, eliminating personal pronouns but not contractions, and apply-
ing culling at the 70% level. by contrast, eliminating contractions    gen-
erally reduces the accuracy of an analysis overall,    indicating perhaps
that the use of contractions is an important indicator of authorship    
while personal pronouns are more about the subject of the document
than the author.

he also tested [63] variation in the calculation of the delta score
itself, such as replacing the z-score by percentage di   erence from the
mean (and thus ignoring the degree of variability); this avoids the nor-
malization step above. as might be expected this reduces performance
slightly. he has experimented with di   erent methods of treating delta,
for example, by limiting the calculations to words with relatively large
z-scores (where the frequency di   erence is more likely to be meaning-
ful), or words where the z-scores have opposite signs in the test and
training documents (following the intuition that the di   erence between
   more    frequent and    less    frequent may be more meaningful than
the di   erence between    more    frequency and    a lot more    frequent).
he has experimented with    alternate formulas    for delta, for example,
including words with positive di   erence only (words more frequent in
the test document than the training set), words with negative di   er-
ence only, and weighting words with positive di   erence more heavily
than words with negative di   erence. the somewhat ad hoc modi   ca-
tions change the topology imposed upon the vector space, although the
exact change is not clear, and in some cases the topology imposed may
no longer be a true    distance    [for example, by violating symmetry
conditions     delta(a,b) may not be equal to delta(b,a)]. in no case
did he obtain substantial improvements over those cited above.

5.2 supervised analysis

281

a more theoretical investigation of the underlying mathematics of
delta was given by argamon [138], who observed that delta (in its orig-
inal version) was mathematically equivalent to the above vector space
representation, and further that it e   ectively ranked candidate authors
by their id203 under the assumption that word frequencies have
a laplace distribution. he noted that the method for estimating the
parameters of this distribution was nonstandard, and that an equivalent
system could be built using standard maximum-likelihood estimators
(id113s) for the parameters. he further noted that this formalization
leads directly to other, less ad hoc variations, such as assuming that
word frequencies are distributed as a gaussian, possibly a multivari-
ate gaussian with the possibility of covariance, or a beta distribution
(argamon, p.c.).

5.2.2 id156

of course, once a feature-based vector space has created and documents
embedded in that space, many other methods can be applied to the task
of assigning documents to authors. another major approach that has
been used is id156 (lda). lda is closely related
to pca in that it tries to    nd linear combinations of the underlying
basis of the vector space that are maximally informative, and thus
performs a type of id84. unlike pca, lda works
on category-labeled data (and hence can only be used as a supervised
algorithm) to infer the di   erences among the labeled categories, and
more speci   cally, to identify lines or (hyper)planes that best separate
the categories. one advantage of lda over pca is that the method is
free to select the most relevant dimensions (the dimensions along which
the category variation is most salient), instead of the ones on which the
variation is simply the largest (the principal components).

a good example of this is the dutch experiment of [8]. this study is
somewhat unusual in two respects;    rst, the texts studied were rather
small (averaging only 908 words), and second, they were in dutch, not
a common language for study. the documents studied had been speci   -
cally elicited for this study from students at u. nijmegen, and consisted
of nine controlled-topic essays (three    ction, including a re-telling of

282 attributional analysis

little red riding hood among other topics; three argumentative, and
three descriptive) for each of eight students, four in their    rst year,
four in their fourth. this thus provided a tightly controlled corpus that
should produce strongly similar essays, and a very di   cult test set.

despite the relatively strong performance of burrows    function word
pca in other studies, here analysis of    the most frequent function
words in the texts shows no authorial structure.    some group struc-
ture was revealed, for example, the second principal component served
to separate    rst year from fourth year students, and the    rst com-
ponent provided some separation by genre (it is unsurprising that
genre is more salient than authorship, and hence on the    rst princi-
pal component). on the other hand, lda provided evidence of some
separability, allowing about 60% of the pairwise comparisons to be
made correctly, compared to a chance id203 of 50%. adjust-
ing the weightings of the word frequencies to take into account their
by-text id178 raised the accuracy to 81.5%, and adding punctua-
tion marks to the feature set raised it to 88.1%. baayen et al. took
this to be evidence, both that authorship attribution is practical in
this tightly controlled circumstances, but also that    discriminant anal-
ysis is a more appropriate technique to use than principal compo-
nent analysis,    as it is capable of detecting more subtle changes than
simple visual
inspection. this point, the appropriateness of di   er-
ent techniques, is of course an important one and one to which we
shall return.

5.2.3 distance-based methods

alternatively, the authorship decision can be made directly, without
prior embedding in a vector space (in point of fact, the vector space
is really just a convenient representation, since it allows many values
to be grouped under one easy-to-understand handle). for example, the
distribution of words (or more generally events) can be treated as a sim-
ple id203 distribution and used to de   ne a set of pairwise inter-
document distances using any of the standard, well-known id203
di   erence, such as histogram intersection, kullback   liebler divergence,
kolmogorov   smirno    distance, and so forth. this can be the basis for

5.2 supervised analysis

283

a simple k-nearest neighbor attribution algorithm. in its simplest form,
this method is not likely to work well because it assumes independence
among the event set.

a more sophisticated application of kullback   liebler divergence
was proposed by juola [71, 72, 74] as    linguistic cross-id178    based
on the    match length within a database    method of estimating id178
[38, 151]. this avoids (to some extent) the problem of independence as
it calculates the id203 of an event occurring in the context of its
immediately preceding context. using this method, he was able to out-
perform [80] the lda-based results given above on the same test data.
kukushkina et al. [96] used markov chains to similar e   ect, calculating
a    rst-order markov model based on character bigrams for each train-
ing author, and then assigning the authorship of a test document to the
chain with the highest id203 of producing it. in a series of experi-
ments, they reported between 70% and 100% accuracy in leave-one-out
cross-validation. one potential weakness of both of these approaches
is the combinatoric explosion as the event vocabulary increases;
it is much easier to compute and estimate transition probabilities
with bigrams from a 27-character alphabet than from a 50,000-word
dictionary.

one of the most interesting proposed distance measures involves
relative kolmogorov complexity. in theory, the kolmogorov complex-
ity of a string is de   ned as the length of the smallest computer program
whose output is that string [97]. relative kolmogorov complexity can
be de   ned as the length of the smallest computer program that con-
verts one string into another. under this framework, authorship can be
assigned to the training document that would require the least    work   
to convert to the test document. unfortunately, kolmogorov complex-
ity is formally uncomputable, but it can be estimated by the use of data
compression techniques [73, 77]. khmelev [96] applied this by de   ning
relative complexity c(a|b) as |ba|     |b|, where |x| is the length of
document x after compression by some standard method. using a vari-
ety of di   erent compression algorithms, they found that performance
was generally poor, but that in some cases, they could outperform a
markov chain analysis similar to [87].

284 attributional analysis

5.2.4 general machine learning techniques

given the nature of the feature sets, it is unsurprising that other
general-purpose machine learning algorithms would have been deployed
to solve these problems. three speci   c such algorithms are neural net-
works (parallel distributed processing) [101], id90 [121], and
naive bayes classi   ers [156].

neural networks are designed using the human brain as a controlling
metaphor; they consist of a large number of cooperative simple arith-
metic processors. in usual practice, they are arranged in a set of three
or more    layers    and trained using a procedure called    backpropaga-
tion of error    [128] to minimize the error at the output layer between
the desired and produced outputs. the mathematics of such networks
has been well-studied [54], and, in general, is very similar to the math-
ematics underlying lda; the middle layer performs a dimensionality
reduction to combine the most relevant underlying dimensions of the
input space, while the output layer identi   es separating hyperplanes.
three examples of this application are [100, 107, 142]. one key    aw in
neural networks is that, although they will often produce very accurate
classi   cations, it is not clear on what basis they are classifying.

in contrast, id90 [121] are a machine learning paradigm
speci   cally designed to support descriptive classi   cation and to explain
why the classi   cation happened as it did. a decision tree is a recursive
data structure containing rules for dividing feature space into a number
of smaller sub-cases, and thus implicitly de   ning a function mapping
regions of space (usually separated by hyperplanes) to category names.
a typical rule would look something like:

(rule 1) if feathers = no and warm-blooded = yes then
type is mammal else apply rule 2.

rules are inferred by splitting the training data along the maxi-
mally informative (binary) division, and the recursive splitting the two
resulting subsets until the problem has been solved. one key advan-
tage of id90 in general is that they can operate well with
non-numeric data, and thus are not con   ned to the vector space mod-
els of other methods discussed above. unfortunately, they appear to

5.2 supervised analysis

285

be outperformed [1] by other methods, most notably by support vector
machines (id166), at this writing arguably the most accurate classi   ca-
tion method known.

naive bayes classi   ers [156] perform similar classi   cation but with-
out the tree structure. instead, they rely on a simple computational
implementation of bayes    theorem to infer the id203 of a clas-
si   cation scheme to infer the most likely category given the observed
data. they are called    naive    because they use simple and unrealistic
independence assumptions (for example, they might assume that the
frequency of the word    i    is independent of the frequency of the word
   me,    a patently false assumption), but nevertheless can perform sur-
prisingly well and have the advantage of being extremely fast to develop
and to train.

5.2.5 support vector machines

id166s [19, 146] are a relatively new classi   cation method that manage
to avoid the two classic traps of machine learning,    the computational
ability to survive projecting data into a trillion dimensions and the
statistical ability to survive what at    rst sight looks like a classic over-
   tting trap    [109]. they have been applied to a huge variety of prob-
lems [19], including handwriting recognition, object recognition, face
detection, and, of course, text categorization [69].

the mathematics underlying id166 are unfortunately complex, and
space precludes a full discussion. in general, an id166 (speci   cally, an
lid166,    linear support vector machine   ) is yet another method of infer-
ring separating hyperplanes in a vector space model, but di   ers in that
it is risk-sensitive, in that the inferred vector is not just a separating
hyperplane, but the separating hyperplane with the greatest potential
margin of error (meaning the separating hyperplane could be displaced
by the greatest distance before introducing a new classi   cation error).
a more general formulation involves using nonlinear id81 to
de   ne separating spaces other than hyperplanes.

id166s have been widely used [1, 91, 157] for authorship attribution
and    anecdotally they work very well indeed    [109] on a wide variety
of problems. researchers such as abbasi [1] (see also [157]) have found

286 attributional analysis

that id166s generally outperform other methods of classi   cation such
as id90, neural networks, and lda     which in turn has been
shown to outperform simple unsupervised techniques such as pca.

does this mean that researchers should simply use id166 and ignore
the rest of this section? unfortunately, the situation is slightly more
complex than that (although a good case could be made). what has
not been discussed is the degree to which di   erent methods (and feature
sets) transfer between domains and authors. baayen et al. [8] showed,
for example, that while function word pca was adequate in some cases,
it was insu   cient to distinguish between authors as similar and as
tightly controlled as in the dutch experiment. we may have similar
reason to question hoover   s [64]    ndings that eliminating personal pro-
nouns results in a useful and substantive improvement over burrows   
delta     not that the    ndings themselves are wrong, but that they
may be more applicable to some forms of text than others. in scien-
ti   c/technical texts, and speci   cally in the typical case of a researcher
reporting her own results, the di   erence between writing    we    or    i    or
   one    may be of more stylometric interest than in the case of a novel,
where the dominant factor may simply be the narrator   s perspective. to
address this issue, what is really needed is extensive empirical testing
of the system as a whole.

6

empirical testing

from a practical standpoint, it may not be enough to know that a
given method of authorship attribution    works.    two further ques-
tions present themselves     how accurately does the method work, and
how widely/reliably? for example, as discussed in the previous section,
eliminating personal pronouns from the analysis has been shown [64] to
improve performance when applied to novels. would the same improve-
ment be seen in web pages? lda outperformed pca in an analysis
of small, tightly topic-controlled essays in dutch [8]. is this a conse-
quence of the size, the topic control, or the fact that the documents
were written in dutch?

in many applications, most notoriously in forensics, knowledge of
the expected error rate is a requirement for a technology to be useful. in
any circumstance, of course, a professional would want to use the most
accurate technology practical (and have a good idea of its limitations).
this demands a certain degree of comparative evaluation of techniques.

6.1 test corpora

the need for comparative evaluation has been recognized for some
time [25, 40, 63, 75, 80]. forsyth [40] compiled a    rst benchmark

287

288 empirical testing

collection of texts for validating authorship attribution techniques.
baayen   s corpus [8] has also been used for comparative testing. as
cited in the previous section, on this corpus, lda was shown to out-
perform pca (which performed only at chance level, as seen in the
previous section). later studies [80] have shown that cross-id178 can
perform even better than lda, and much faster. from these results
it can be concluded that under the circumstances of this test, cross-
id178, and in particular, cross-id178 using words instead of char-
acters for the event set, is a more accurate technique for assessing
authorship.

these studies raise an important followup question about the role
of the test circumstances themselves. in particular, the test data was
all in dutch, the topics were very tightly controlled, and about eight
thousand words of sample data per author were available. would
results have been substantially di   erent if the authors had writ-
ten in english? if there had been eight hundred thousand words
per author, as might be the case in a copyright dispute involving
a proli   c author? can the results of an analysis involving exposi-
tory essays be generalized across genres, for example, to personal
letters?

6.2 the ad-hoc authorship attribution competition
6.2.1 contest structure

to answer these questions, in 2004 the association for literary and
linguistic computing and the association for computers and the
humanities (allc/ach) hosted an    ad-hoc authorship attribution
competition    [75, 78] at the joint annual meeting. this remains the
largest-scale comparative evaluation of authorship attribution technol-
ogy to date. by providing a standardized test corpus for authorship
attribution, not only could the mere ability of statistical methods to
determine authors be demonstrated, but methods could further be
distinguished between the merely    successful    and    very successful.   
contest materials included 13 problems, in a variety of lengths, styles,
genres, and languages, mostly gathered from the web but including
some materials speci   cally gathered to this purpose. a dozen research

6.2 the ad-hoc authorship attribution competition

289

groups participated, some with several methods, by downloading the
(anonymized) materials and returning their attributions to be graded
and evaluated against the known correct answers.

    problem a (english) fixed-topic essays written by 13 us
university students.
    problem b (english) free-topic essays written by 13 us uni-
versity students.
    problem c (english) novels by 19th century american
authors (cooper, crane, hawthorne, irving, twain, and
   none-of-the-above   ), truncated to 100,000 characters.
    problem d (english) first act of plays by eliza-
bethan/jacobean playwrights (johnson, marlowe, shake-
speare, and    none-of-the-above   ).
    problem e (english) plays in their entirety by eliza-
bethan/jacobean playwrights (johnson, marlowe, shake-
speare, and    none-of-the-above   ).
    problem f ([middle] english) letters, speci   cally extracts
from the paston letters (by margaret paston, john pas-
ton ii, and john paston iii, and    none-of-the-above    [agnes
paston]).
    problem g (english) novels, by edgar rice burrows, divided
into    early    (pre-1914) novels, and    late    (post-1920).
    problem h (english) transcripts of unrestricted speech gath-
ered during committee meetings, taken from the corpus of
spoken professional american-english.
    problem i (french) novels by hugo and dumas (pere).
    problem j (french) training set identical to previous prob-
lem. testing set is one play by each, thus testing ability to
deal with cross-genre data.
    problem k (serbian-slavonic) short excerpts from the lives
of kings and archbishops, attributed to archbishop danilo
and two unnamed authors (a and b). (data obtained from
alexandar kostic.)
    problem l (latin) elegaic poems from classical latin authors
(catullus, ovid, propertius, and tibullus).

290 empirical testing

    problem m (dutch) fixed-topic essays written by dutch uni-
versity students. (data obtained from harald baayen, in fact,
this is the data set described in the previous section.)

this data thus represents a wide variety of languages, lengths, and

genres.

the contest (and results) were surprising at many levels; some
researchers initially refused to participate given the admittedly di   cult
tasks included among the corpora. for example, problem f consisted
of a set of letters extracted from the paston letters. aside from the very
real issue of applying methods designed/tested for the most part for
modern english on documents in middle english, the size of these doc-
uments (very few letters, today or in centuries past, exceed 1000 words)
makes statistical id136 di   cult. similarly, problem a was a realistic
exercise in the analysis of student essays (gathered in a freshman writ-
ing class during the fall of 2003)     as is typical, no essay exceeded 1200
words. from a standpoint of literary analysis, this may be regarded as
an unreasonably short sample, but from a standpoint both of a realistic
test of forensic attribution, as well as a legitimately di   cult problem
for testing the sensitivity of techniques, these are legitimate.

6.3 aaac results

participants in this contest are listed in table 6.1.

results from this competition were heartening (   unbelievable,    in
the words of one contest participant). the highest scoring participant
was the research group of moshe koppel and jonathan schler (listed as
schler, the corresponding author, in tables 6.2   6.4), with an average
success rate of approximately 71% (juola, the contest organizer, also
submitted solutions. in the interests of full disclosure, he placed fourth
averaging 65% correct). in particular, koppel and schler   s methods
achieved 53% accuracy on problem a and 100% accuracy on problem
f. ke  selj, the runner-up, achieved 85% on problem a and 90% on
problem f. both problems were acknowledged, even by the organizer,
to be di   cult and considered by many to be unsolvably so. actually,
david hoover identi   ed a weakness in the problem structure. since
much of the data was taken from the web, using a web search engine

table 6.1 aaac participants and method.

a   liation

name
andrea baronchelli et al. rome    la sapienza   
aaron coburn
hans van haltern
david l. hoover
david l. hoover
patrick juola
lana and amisano
ke  selj and cercone
ke  selj and cercone
o   brien and vogel

middlebury
nijmegen
nyu
nyu
duquesne
piedmont orientale
dalhousie
dalhousie
trinity college,

lawrence m. rudner
koppel and schler
efstathios stamatatos

dublin

gmac
bar-ilan
patras

6.3 aaac results

291

method

id178-based informatic distance
contextual network graph
   linguistic pro   ling   
cluster analysis of word frequencies
google search for distinctive phrases
match length within a database
common id165s (two variants)
cng method with weighted voting
cng-wv with reject
chi by degrees of freedom

multinomial bayesian model/betsy
id166 with linear id81
meta-classi   ers via feature selection

b

c

a

d

table 6.2 aaac results for problems a   d.
team1
baronchelli
coburn
halteren
hoover1
hoover2
juola
keselj1
keselj2
lana-amisano1
lana-amisano2
obrien
rudner
schler
stamatatos
1not all groups submitted solutions to all problems; groups for which no solutions were
received scored 0 on that problem.

3/13 (23.08%)
5/13 (38.46%)
9/13 (69.23%)
4/13 (30.77%)
4/13 (30.77%)
9/13 (69.23%)
11/13 (84.62%)
9/13 (69.23%)
0/13 (0.00%)
0/13 (0.00%)
2/13 (15.38%)
0/13 (0.00%)
7/13 (53.85%)
9/13 (69.23%)

3/13 (23.08%)
2/13 (15.38%)
3/13 (23.08%)
1/13 (7.69%)
2/13 (15.38%)
7/13 (53.85%)
7/13 (53.85%)
5/13 (38.46%)
0/13 (0.00%)
0/13 (0.00%)
3/13 (23.08%)
0/13 (0.00%)
4/13 (30.77%)
2/13 (15.38%)

8/9 (88.89%)
8/9 (88.89%)
9/9 (100.00%)
8/9 (88.89%)
9/9 (100.00%)
6/9 (66.67%)
8/9 (88.89%)
7/9 (77.78%)
3/9 (33.33%)
0/9 (0.00%)
6/9 (66.67%)
6/9 (66.67%)
9/9 (100.00%)
8/9 (88.89%)

3/4 (75.00%)
3/4 (75.00%)
3/4 (75.00%)
2/4 (50.00%)
4/4 (100.00%)
3/4 (75.00%)
3/4 (75.00%)
2/4 (50.00%)
2/4 (50.00%)
2/4 (50.00%)
3/5 (75.00%)
3/4 (75.00%)
4/4 (100.00%)
2/4 (50.00%)

such as google could identify many of the documents, and therefore the
authors. using this method, submitted as    hoover2,    hoover was able
to    cheat    and outscore koppel/schler. hoover himself admits that
this solution does not generalize and does not address the technical
questions of stylometry; for the data not available on the web, this
method of course failed spectacularly.

as part of the aaac procedure, participants were asked to submit
brief writeups of their methods to help identify characteristics of good

292 empirical testing

table 6.3 aaac results for problems e   h.

team
baronchelli
coburn
halteren
hoover1
hoover2
juola
keselj1
keselj2
lana-amisano1
lana-amisano2
obrien
rudner
schler
stamatatos

e

1/4 (25.00%)
4/4 (100.00%)
3/4 (75.00%)
2/4 (50.00%)
4/4 (100.00%)
2/4 (50.00%)
2/4 (50.00%)
1/4 (25.00%)
0/4 (0.00%)
0/4 (0.00%)
2/4 (50.00%)
1/4 (25.00%)
4/4 (100.00%)
2/4 (50.00%)

f

9/10 (90.00%)
9/10 (90.00%)
9/10 (90.00%)
9/10 (90.00%)
10/10 (100.00%)
9/10 (90.00%)
9/10 (90.00%)
9/10 (90.00%)
0/10 (0.00%)
0/10 (0.00%)
7/10 (70.00%)
0/10 (0.00%)

10/10 (100.00%)
9/10 (90.00%)

table 6.4 aaac results for problems i   m.

g

2/4 (50.00%)
1/4 (25.00%)
2/4 (50.00%)
2/4 (50.00%)
2/4 (50.00%)
2/4 (50.00%)
3/4 (75.00%)
2/4 (50.00%)
0/4 (0.00%)
0/4 (0.00%)
2/4 (50.00%)
3/4 (75.00%)
2/4 (50.00%)
2/4 (50.00%)

h

3/3 (100.00%)
2/3 (66.67%)
2/3 (66.67%)
2/3 (66.67%)
3/3 (100.00%)
3/3 (100.00%)
1/3 (33.33%)
0/3 (0.00%)
3/3 (100.00%)
0/3 (0.00%)
1/3 (33.33%)
3/3 (100.00%)
2/3 (66.67%)
1/3 (33.33%)

i

j

l

k

1/2 (50.00%)
1/2 (50.00%)
1/2 (50.00%)
1/2 (50.00%)

team
2/4 (50.00%) 4/4 (100.00%)
baronchelli 2/4 (50.00%)
2/4 (50.00%) 3/4 (75.00%)
2/4 (50.00%)
coburn
2/4 (50.00%) 2/4 (50.00%)
halteren
3/4 (75.00%)
3/4 (75.00%)
hoover1
2/4 (50.00%) 4/4 (100.00%)
4/4 (100.00%) 2/2 (100.00%) 2/4 (50.00%) 4/4 (100.00%)
hoover2
2/4 (50.00%)
juola
3/4 (75.00%)
keselj1
keselj2
2/4 (50.00%)
0/4 (0.00%)
lana-

5/24 (20.83%)
19/24 (79.17%)
21/24 (87.50%)
7/24 (29.17%)
7/24 (29.17%)
2/4 (50.00%) 4/4 (100.00%) 11/24 (45.83%)
2/4 (50.00%) 4/4 (100.00%) 17/24 (70.83%)
1/4 (25.00%) 3/4 (75.00%)
15/24 (62.50%)
0/24 (0.00%)
1/4 (25.00%)
0/4 (0.00%)

1/2 (50.00%)
1/2 (50.00%)
0/2 (0.00%)
0/2 (0.00%)

m

amisano1

lana-

0/4 (0.00%)

0/2 (0.00%)

0/4 (0.00%)

3/4 (75.00%)

0/24 (0.00%)

amisano2

1/4 (25.00%)
obrien
3/4 (75.00%)
rudner
schler
3/4 (75.00%)
stamatatos 3/4 (75.00%)

3/4 (75.00%) 4/4 (100.00%)
1/2 (50.00%)
1/2 (50.00%)
1/4 (25.00%)
0/4 (0.00%)
2/2 (100.00%) 1/4 (25.00%) 4/4 (100.00%)
1/2 (50.00%)
2/4 (50.00%) 3/4 (75.00%)

5/24 (20.83%)
0/24 (0.00%)
4/24 (16.67%)
14/24 (58.33%)

performers. unfortunately, another apparent result is that the high-
performing algorithms appear to be mathematically and statistically
(although not necessarily linguistically) sophisticated and to demand
large numbers of features. the good methods have names that may
appear fearsome to the uninitiated: id156 [8, 145],
orthographic or lexical cross-id178 [74, 80], common byte id165s
[84], id166 with a linear id81 [91]. from a practical per-
spective, this may cause di   culties down the road in explaining to a

6.3 aaac results

293

jury or to a frantic english teacher exactly what kind of analysis is
being performed, but we hope the di   culties will be no greater than
explaining dna.

the following are extracts and summaries of the individual writeups

submitted by the    ve top-scoring participants:

    moshe koppel and jonathan schler: id166 with unstable
words
we used a machine learning approach. for english texts,
our primary feature set was a set of common    unstable   
words, that is, words with commonly used substitutes (for
a full description of this feature set see [88]). when neces-
sary, we used as secondary feature sets: function words, 500
most frequent words (in training), and part-of-speech tags.
in languages other than english, we used only the 500 most
frequent words.

the learning method we used is id166 with a linear kernel
function. as id166 handles only binary problems, for cate-
gories with more than 2 categories, we applied    one vs. oth-
ers    classi   ers, and chose the authors (categories) that pass
the threshold. in case of no category passing the threshold,
the document was not assigned to any of the given authors
in the training set. in case of collision (i.e., a document
was assigned to more than one author) we used our sec-
ondary feature sets and assigned a document to the author
with the most votes. with one exception (see results), this
method resolved collisions (overall aaac performance: 918%
correct).
    vlado ke  selj and nick cercone: cng method for authorship
attribution
the common id165s (cng) classi   cation method for
authorship attribution (aatt) was described elsewhere [85].
the method is based on extracting the most frequent byte
id165s of size n from the training data. the id165s
are sorted by their normalized frequency, and the    rst l
most-frequent id165s de   ne an author pro   le. given a test

294 empirical testing

document, the test pro   le is produced in the same way, and
then the distances between the test pro   le and the author
pro   les are calculated. the test document is classi   ed using
k-nearest neighbors method with k = 1, i.e., the test docu-
ment is attributed to the author whose pro   le is closest to
the test pro   le. given two pro   les f1 and f2, which map n-
grams from sets d1 and d2 to their respective frequencies,
the distance measure between them is de   ned by [a distance
formula] (overall aaac performance: 897% correct).
    hans van halteren: linguistic pro   ling
a linguistic pro   le for each author and text is constructed as
follows:

    for each token, determine token, token pattern, and

potential word classes.

    take all possible uni-, bi- and trigrams, using either

token or class in each position.

    use the token pattern instead of the token itself for

low-frequency tokens.

    use only those id165s which occur in more than one

text.

    determine frequencies of all the selected id165s.
    express frequencies as number of standard deviations

above/below mean.

    a text pro   le is the vector containing these numbers

for all selected id165s.

    an author pro   le is the mean over the known texts

for the author.

to determine if a text conforms to an author pro   le, use
[an appropriate] formula (cf. [144]). the score is then nor-
malized to the number of standard deviations above the
mean over the negative examples. generally, a positive score
indicates conformance (overall aaac performance: 861%
correct).

6.3 aaac results

295

    patrick juola: linguistic cross-id178
   cross-id178    is a measure of the unpredictability of a
given event, given a speci   c (but not necessarily best) model
of events and expectations. this di   erence can be quanti-
   ed [71, 74, 151] and measured as a    distance    between two
samples.

to apply this measure, we treat each document (training
and test) as source of random    events,    in this case letters.
from each training document is taken a sample of up to
10,000 letters, and then for each position in the test doc-
ument, the longest substring starting at that position and
contained in the sample is found and its length computed.

the mean of these [substring lengths] has been shown
to be closely (and inversely) related to the cross-id178
between the two documents     the longer the length, the
lower the cross-id178.

to complete the attribution task, the average distance
between a single test document and all training documents
by the same author is computed. the document is assigned
to the author with the smallest average distance, or alterna-
tively, to the authors in order of increasing distance (overall
aaac performance: 851% correct).
    aaron coburn: attributing authorship using a contextual
network graph
this approach combines word-use statistics and graph the-
ory to identify the author of an unknown text. by compar-
ing stylistic attributes of a document to a corpus of know
texts, one can make a reasonable guess about its author-
ship. in the    rst step of this method, each text is reduced to
word sequences. these are typically two-word sequences, but
three- and four-word phrases work well for some very large
texts. next, the indexed collection is projected onto a graph
in which the documents and term sequences are represented
as an interconnected network.

for longer texts (typically greater than 3,000 words), the
   rst stage is simple: word pairs are extracted and counted.

296 empirical testing

if, for instance, the word pair    altogether too    appears more
prominently in two particular texts, one can begin to asso-
ciate these documents. by extracting commonly appearing
word sequences, the resulting index tends to form a    nger-
print of a document   s style rather than its content. shorter
texts are more di   cult; there are typically too few word pairs
that appear across the collection to provide any meaningful
correlations. for these texts, i apply a part-of-speech tagger,
and reduce nouns, adjectives, and verbs each to a common
token. thus the phrase    i walk through the    eld at noon   
becomes    i verb through the noun at noun.    then, the word
pair frequencies are extracted as before.

with an index of word sequence frequencies for each
document, these values are applied to the connected graph
described above. document and term nodes are connected
by edges, and the value of each edge is determined by the
frequency of a word sequence in a document. this contex-
tual network graph produces a network in which similar doc-
uments are closely connected and dissimilar texts are less
closely connected. once the graph is constructed, a measure
of similarity can easily be determined between the document
with unknown authorship and all other documents. those
documents with the highest level of similarity can then likely
be identi   ed as having the same author (overall aaac per-
formance: 804% correct).

6.4 discussion

more generally, most participants scored signi   cantly above chance on
all problems for which they submitted solutions. perhaps as should
be expected, performance on english problems tended to be higher
than on other languages. perhaps more surprisingly, the availability
of large documents was not as important to accuracy as the availabil-
ity of a large number of smaller documents, perhaps because they can
give a more representative sample of the range of an author   s writing.
finally, methods based on simple lexical statistics tended to perform

6.4 discussion

297

substantially worse than methods based on id165s or similar mea-
sures of syntax in conjunction with lexical statistics (note the relatively
poor performance of hoover   s    rst method based on word frequency
analysis).

with regard to generalization and con   dence issues, the    ndings
are very good for the    eld as a whole. in general, algorithms that
were successful under one set of conditions tended to be successful
(although not necessarily as successful numerically) under other condi-
tions. in particular, the average performance of a method on english
samples (problems a   h) correlates signi   cantly (r = 0.594, p < 0.05)
with that method   s performance on non-english samples. this may
suggest that the authorship problem is linguistically universal and
that a single    best    algorithm and feature set can be found, but it
may also suggest that some algorithms have simply been tuned to be
better on their primary set than others, and a poor algorithm for
english is unlikely to magically improve when tested on french or
serbian.

correlation between large-sample problems (problems with over
50,000 words per sample) and small-sample problems was still good,
although no longer strictly signi   cant (r = 0.3141). this suggests that
the problem of authorship attribution is at least somewhat a language-
and data-independent problem, and one to which we may be able to
expect to    nd wide-ranging technical solutions for the general case,
instead of (as, for example, in machine translation) to have to tailor
our solutions with detailed knowledge of the problem/texts/languages
at hand. in particular, juola has o   ered the following challenge to all
researchers in the process of developing a new forensic analysis method:
if you can   t get 90% correct on the paston letters (problem f), then your
algorithm is not competitively accurate. every well-performing algo-
rithm studied in this competition had no di   culty achieving this stan-
dard. statements from researchers that their methods will not work on
small training samples should be regarded with appropriate suspicion.
the aaac corpus itself also has some limitations that need to be
addressed. as a simple example, the mere fact that the data is on the
web (as well as the more serious issue that much of the data was gath-
ered from web-accessible archives such as project gutenberg) gives an

298 empirical testing

unfair advantage to any methods (such as hoover   s second method)
that rely upon searching the web to extend or leverage their data and
analysis. similarly, the multilingual coverage is unbalanced     on the
one hand, any consideration at all of languages other than english
might be regarded as a waste of time by some experts who focus only
on practical problems in the heartland of the united states, while at
the same time, many important and widely studied languages like ara-
bic and japanese are not represented. the coverage of di   erent genres
is spotty (no newspaper text, for example), and there are probably
important issues that have not been addressed at all.

research into comparative evaluation of attribution technologies
continues, including active discussion of the best sort of testbeds to
develop. in light of the problems mentioned above, for example, should
future test corpora focus on only a single category, for example, anal-
ysis of blog or web forum messages in english? this would have some
immediate and practical bene   ts, especially for communities such as
intelligence and law enforcement that need to analyze such data on a
daily basis. on the other hand, this would leave french medievalists
out in the cold. a recent (2006) nsf-sponsored working group identi-
   ed further evaluation, ideally as an ongoing periodic process, modeled
after research groups such as trec (text retrieval conferences), with
each individual evaluation focusing on a speci   c corpus and problem
type. as authorship attribution matures, we can expect this sort of
evaluation to be more common and much more detailed.

7

other applications of authorship attribution

thus far, we have been focusing on authorship attribution primar-
ily as a question of inferring the personal identity of the document   s
author. however, the de   nition as given in section 2.1 is broader    
   any attempt to infer the characteristics of the creator of a piece of
linguistic data.    as we have seen in section 4.6, this can be broadened
even further to cover other forms of data such as web pages, music,
and computer code. however, this de   nition also includes not just per-
sonal identity, but group identity such as gender, dialect/culture, edu-
cation level, or even personality. as discussed below, researchers have
found that many of the same techniques work for such evaluations
as well.

although this may seem plausible, even obvious, at least one related
discipline speci   cally abjures such application.    document analysis    is
the term used by forensic specialists in inferring authorship from hand-
writing (such as ransom notes). these examiners, often certi   ed by
professional organizations such as the american board of forensic doc-
ument examiners (www.abfde.org), try to establish    the authenticity
of the contested material as well as the detection of alterations    [3].
   in their words, forensic document examination involves the analysis

299

300 other applications of authorship attribution

and comparison of questioned documents with known material in order
to identify, whenever possible, the author or origin of the questioned
document.    they speci   cally do not infer the traits of the author-
ship, and cannot usually be used to pro   le the author (   the author
of this document was a college-educated white woman from califor-
nia   ); the discipline that covers this is usually called    graphoanalysis,   
which ostensibly claims    to provide insight into personality traits and
evaluation of a writer   s personality    [67] but is usually considered a
pseudoscience like astrology. the di   erence, simply put, is accuracy.
traditional handwriting analysis, as practiced by forensic document
examiners, is not able to infer group characteristics, even simple ones
like gender and handedness, to the accuracy demanded by their primary
clients, the court system. it is somewhat unfortunate that there is little
public documentation about the accuracy rate they are able to achieve,
as it would be surprising if they could not infer gender with rates sub-
stantially better than chance, which would still be a publishable result
in the authorship attribution community. authorship attribution, hav-
ing come late to the forensic community, has been happy to study group
as well as individual characteristics.

7.1 gender

an obvious place to start looking for group characteristics is in the
gender of the author. in some languages, of course, gender is so marked
that it is hard to avoid in    rst-person writing (consider guessing what
gender of francophone writes je suis belle). even in languages such as
english, gender distinctions in spoken language have been studied for a
long time, long enough to preclude a useful set of citations. these dis-
tinctions tend to be more obvious in spoken than in written language;
in fact, some authors have suggested that male and female styles should
not di   er in formal writing.

koppel [89] has studied this and found just the opposite, that there
is a detectable di   erence, enough to enable his team to categorize docu-
ments with about 80% accuracy. his method is familiar enough in light
of the previous discussion; documents from the british national cor-
pus (bnc) were embedded in a high-dimensional feature space, using

7.2 document dating and language change 301

features such as function words, pos id165s, and punctuation. clas-
si   cation was done by inferring a linear separator between male- and
female-authored documents using a standard machine learning method
(similar in spirit to lda or id166). they further found that non-   ction
and    ction had very separate categorization rules, and that non-   ction
was generally easier to categorize than    ction. analysis of the speci   c
features that were used to create the category both supported and
extended prior work     for example, men used pronouns signi   cantly
less frequently than women in both    ction and non-   ction, while men
used the determiners    a,       an,       the,    and    no    signi   cantly more
frequently.

other studies of this nature include [5, 29, 95] with largely simi-
lar results, although on di   erent languages, genres, and methods. for
example, [95] studied internet chat logs (e.g., irc, im, icq, and such)
in turkish using a feature set most notable for its inclusion of smileys
and a variety of classi   ers including neural networks, k-nearest neigh-
bor, and naive bayesian analysis. the results from baysian analysis
were again in the 80% range, with other methods slightly or signi   -
cantly less. an interesting variation on this type of study is that of
hota et al. [66] who studied how authors portray di   erent genders. in
particular, do the characters in a play who are supposed to be female
actually speak like females? using a corpus of character speeches from
34 shakespearean plays, the researchers were able to classify approxi-
mately 60%   75% of the speeches correctly by the gender of the speaker.
the lower accuracy may suggest that shakespeare had some intuitive
understanding of some language di   erences, but was unable to com-
pletely mask his own masculinity.

7.2 document dating and language change

another aspect of authorship category is the time of authorship, both
as expressed on the calendar and in terms of one   s personal development
as a writer. this is a surprisingly controversial area of authorship attri-
bution research, because it re   ects directly on the underlying assump-
tion of an authorial       ngerprint.    some researchers, most notably van
halteren [145], have claimed that people should have a    xed and

302 other applications of authorship attribution

unchanging set of stylistic markers, but at the same time, the fact
that new words routinely enter and leave the language means that
there must also be continuous change. perhaps these are other stylistic
markers that can be used for other purposes?

hota   s study of shakespearean gender is also an example of a study
in document dating; hota    somewhat arbitrarily    divided his cor-
pus into early and late plays (pre- and post-1600) and analyzed the
two groups separately. there was a substantial di   erence in accuracy
between the two groups, indicating    that there is a greater stylistic
di   erence between the genders in late shakespeare than early shake-
speare.    this is, of course, evidence of stylistic development over
shakespeare   s career. of course, establishing this development does
not establish how and why this development happened; the cause may
be as simple (and uninteresting) as increasing the number of female
characters, or as signi   cant as the development of a female-speci   c
vocabulary.

similar studies have found such development in other writers    as
well; examples include [23, 62, 79, 119]. figure 7.1 shows an example
of such development in the novels of henry james. unlike [66], this

fig. 7.1 chronological development of style in the writings of henry james (data and    gure
courtesy of david hoover).

7.3 other socioeconomic variables

303

   gure was produced by unsupervised analysis (pca of the 990 most
common words) and does not involve arbitrary categorization of texts;
instead they are simply scatter-plotted by date across the    rst two
principal components. the pattern that emerges is a clear picture of
stylistic development; novels known to be earlier sort themselves to the
left of the diagram. a similar picture of stylistic development can be
seen in figure 5.3, covering the work of jack london [79]. indeed, this
may be even more striking, given the apparent lack of pattern prior to
1912 and after 1912, but the clear linear separation between early and
late works; indeed, this picture argues for a non-arbitrary classi   cation
as of 1912 for the works of jack london.

there is thus substantial work looking at the development of stylis-
tic change within the career of a single writer. can such stylistic change
be observed for society as a whole? obviously, lexical items will change
relatively rapidly [33, 70], and other levels will also change, albeit more
slowly. juola [74] applied stylometric techniques     the linguistic cross-
id178 discussed in a previous section     to infer the average distance
between magazine articles covering approximately a forty-year period.
he was able to infer not only that documents more distant in time
are also more distant in style, but to approximate the rate at which
language changed. his    ndings were not only that language changed,
but the rate itself was non-uniform; as measured by samples from
the national geographic, language changed more rapidly in the 1950s
than in the 1940s or 1960s (and the rate of language change in the
1940s, although measured to be positive, was not signi   cantly di   erent
from zero).

both of these types of analysis could in theory be applied to the task
of document dating; a previously undiscovered manuscript by henry
james, for example, could be placed on the graph of figure 7.1 to
guess (perhaps via k-nearest neighbor) the approximate time period in
which it was written.

7.3 other socioeconomic variables

using similar techniques, it is possible to analyze for any group identity
for which data is available. nationality, dialect, and region are almost

304 other applications of authorship attribution

too well-studied to deserve detailed comment. yu [152] applied rule-
based feature analysis on word id165s to distinguish between native
english (usa/uk) and native chinese speakers in a study of doc-
toral dissertations; the methodology is an extension of that of oakes
[115] for distinguishing usa and uk english. the baayen corpus is
a good example for the identi   cation of more subtle di   erences; both
van halteren [8, 145] and juola [80] have con   rmed that the educa-
tional di   erences between    rst year and fourth year college students
can be detected using lda and cross-id178, respectively. a study by
keune [86] showed that di   erences in education levels could be detected
in the pronunciation of the dutch word mogelijk (   possible   ). further
analysis showed other e   ects of both sex and nationality (dutch vs.
flemish).

7.4 personality and mental health

as one of the major products of the human mind, language should
be expected to provide some sort of indicator of the mind that pro-
duced it. in addition to indicating identity, it may also be able to show
something about process. the use of linguistic indicators as cues for
medical diagnosis has a long history. a simple example is the sen-
tential complexity measures as revised by [17], where the absence of
highly complex sentences can be a sign of cognitive trouble. similarly,
other studies have shown that particular linguistic features can be a
sign of speci   c emotional attitudes     when people are depressed, for
example, they use more    rst-person singular pronouns [123]. perhaps
more hopefully, an increased use of causal words such as    because,   
as well as an increased use of cognitive words such as    realize    pre-
dicts recovery from trauma [120]. these are simple examples of lex-
ical features that can be used to categorize people by their mental
states. other examples of such studies     and examples of intense inter-
est to law enforcement, one expects     include the studies of lying
and linguistic style performed by newman et al. [113] as well as by
hancock [51].

other researchers [6, 28, 114, 116, 118] have studied to what
extent linguistic cues can identify personality type and pro   le.

7.5 section summary

305

argamon [6]
is another almost prototypical example of standard
authorship attribution technology applied to this task. as before,
the documents (student-written stream-of-consciousness essays) were
embedded in a large vector space and supervised learning in the form
of a linear separator was applied to determine the feature combination
that best separated the categories of interest. in this case, the indi-
vidual students had been tested on neuroticism (   roughly: tendency to
worry   ) and extroversion (   roughly: preference for the company of oth-
ers   ). the feature set included the by-now familiar set of common words
(675 in this set), but also indicators of    cohesion,       assessment,    and
   appraisal    measured though a systemic functional grammar (sfg)
approach. the analysis was able to identify neuroticism with substan-
tially better than chance odds (58%), but did not perform nearly as
well in detecting extroversion. this may re   ect the inherent di   culty
of the task, or it may simply re   ect the fact that a more sophisticated
feature set is necessary for analysis of psychological structures.

a similar experiment was conducted by nowson and oberlander
[114]. these researchers were able to identify an    internet meme    equiv-
alent to a personality test yielding relatively coarse scores on    ve per-
sonality factors (neuroticism, extraversion, openness, agreeableness,
and conscientiousness). applying this test to web log (blog) data, they
used word-based id165s and applied a naive bayes classi   er, typi-
cally getting above chance in classifying on each of these scales, but
not by much (e.g., between 52% and 65% on two-way classi   cation
tests). although these results are numerically disappointing, this study
is remarkable for the (poor) quality of the data from which they were
able to work, suggesting that good methods may have an almost phe-
nomenal degree of resistance to noise.

7.5 section summary

beyond simple questions of the identity of individual authors, the
underlying technology has been applied to questions of group identity
as well. examples include gender, age, date, social class, education,
nationality, and even personality type. of course, the degree of success
in these applications varies substantially.

306 other applications of authorship attribution

it is interesting to compare the limited scope of document analysis
with the far-ranging scope of authorship attribution technology. doc-
ument analysts deliberately avoid inferring personality, because they
consider that they cannot do it with su   cient accuracy. that concern
does not appear to have stopped argamon (although argamon presum-
ably has not been called upon to testify in court about his personality
pro   les). is 58% a useful level of accuracy for any purpose at all?

8

special problems of linguistic forensics

8.1 forensic issues

for many purposes, simply producing an answer is enough. a simple
authoritative statement to the e   ect that the haid113t looks like shake-
speare   s work is often enough, especially when the answer is simply a
con   rmation of existing scholarly belief. on the other hand, in cases of
genuine controversy, it is not enough to simply issue pronouncements.
before accepting the conclusions, people will usually want to see and
to evaluate the evidence supporting them.

few areas are as controversial as the law. few subjects are as con-
trolled by restrictive rules as the law. in few situations can one not
only expect, but rely upon the presence of an intelligent, articulate
person speci   cally dedicated to discrediting one   s arguments, method-
ology, and evidence. it can be said with some degree of accuracy that
forensic authorship attribution is among the most challenging. at the
same time, questions of language and authorship can be crucial in estab-
lishing justice.

chaski [26] presents three examples drawn from actual cases where
the question of a document   s authorship was key to the resolution.

307

308 special problems of linguistic forensics

in the    rst, an employee was    red for racist email sent from his (open)
cubicle, email he denied sending. in the second, a young man was found
dead by his roommate, with suicide notes typed on the man   s computer.
were these notes forgeries, perhaps created to cover the roommate   s
murder of the man? in the third, one party claimed that their on-line
journal, a crucial bit of evidence, had been tampered with by the other
party before it was able to be introduced into evidence.

in each of these cases, traditional digital forensics is more or less
helpless; all parties in the dispute had access to the relevant computer
and could have created or modi   ed the documents; tracing the doc-
uments back to the machine of origin was both trivial and useless.
how, then, does one establish evidence su   cient to convince the inves-
tigators, judge, and eventual jury of the guilt or innocence of those
involved?

there are three main problems with authorship attribution as
applied to forensics     credibility, admissibility, and active malice. we
will look brie   y at each of these in turn.

8.2 credibility

the    rst problem facing a forensic scholar, oddly enough, is not the
problem of rules of evidence. before there is a trial, there is an
investigation, and even before that, there is an allegation. the    rst
task is to make the initial    nding credible enough that the inves-
tigators will take it seriously (after all, even formally inadmissible
evidence can form the basis of an investigation; the unabomber,
for example, was    nally caught after his brother informally noted
that the manifesto was written in his style and suggested they
investigate).

in order to be credible, a statement must    rst be accurate, or per-
haps more accurately, backed up by a track record of accuracy. this is
the problem with argamon   s personality assessment with accuracy of
58%; unless an investigator were truly desperate, an assessment with
barely greater accuracy than a coin    ip is not a useful lead. on the
other hand, the 90%+ accuracy that most aaac participants man-
aged to achieve on the paston letters are probably enough to cause

8.2 credibility

309

investigators to start focusing on margaret instead of john. even a
suggestion that there is an 80% likelihood that the culprit was female
[89, 95] is probably a useful    nding.

the relevance and representativeness of the track record are also
important. if the case under discussion involves email and blog entries,
a track record of accuracy derived from the study of large-scale literary
documents is less convincing than one derived from shorter documents
such as letters, and a study based on email and blogs would be better
yet. in the absence of such a direct study,    ndings that a particular
method were accurate across a wide variety of genres would support
the credibility of that method applied to a new one.

another important consideration, however, cannot really be quanti-
   ed. to be credible, a method should also be understandable. the idea
of a shibboleth or similar direct marker of authorship is one that can
be easily understood; and may not [149] even require    expert    testi-
mony to explain. the assumption that one can infer group identity from
accent and vocabulary is easily made and easily expressed. it is slightly
more di   cult to accept the idea of systematic variation     although
everyone uses adverbs, some people use more of them than others, and
do so consistently. the key di   culty here is probably consistency, and
would probably need empirical support. but at least the basic idea, the
varying frequency of use of a particular features, is something under-
standable. quantifying and combining such variables, as with delta
and its variants, is a simple next step.

however, as shown in the previous sections, simple feature quan-
ti   cation tends not to be as accurate or as sensitive as more complex
statistics. unfortunately, these more complex statistics almost instantly
lose the direct connection with understandable features. if two docu-
ments are separated along    the    rst principal component,    what does
that mean? what is the association between the features encoded in the
study and the pca vectors? it is possible (see [14] for an example) to
plot the features themselves in terms of their representation in the prin-
cipal components, to show the degree to which certain features appear
in the same pca    space    as the documents that are attributed to
them, and to show (visually) that the di   erent authors appear to clus-
ter in this space. this permits researchers to make relatively accessible

310 special problems of linguistic forensics

statements like    thompson [has a] tendency to use words indicating
position        up,       down,       on,       over,       out,    and    back        more fre-
quently than baum. an examination of the raw data reveals that, for
these words, thompson   s average rates of usage are about twice as [sic]
baum   s    [14] less convincing are the diagrams (such as those in [8, 79])
that display id91 without explaining the underlying features that
produce that id91.

but even these methods are substantially more credible than those
that simply report percentage accuracy, whether in the form of roc
curves, graphs, or simple statements. unfortunately, most of the meth-
ods with the highest accuracy operate in too many dimensions to
make simple representations practical; most people cannot visualize
a four dimensional hypercube, let alone one with two hundred dimen-
sions and a separating non-planar hypersurface (as would be produced
by id166 with a nonlinear id81). direct distance measures
such as juola   s cross-id178 appear to be among the worst o   end-
ers in this regard, as the    features    are implicit and cannot even
be extracted easily. other o   enders include general machine learn-
ing methods such as neural networks, from which the overall method
cannot easily be extracted. there is room (and need) for much addi-
tional research in trying to    nd a method that is both transpar-
ent and accurate, or alternatively, in simply trying to    nd a way
to explain methods like pca, id166, and lda to a non-technical
audience.

8.3 admissibility

as discussed above, admissibility is less of an issue than one might
think, because, if necessary, other evidence is likely to be found. it
may also be possible to explain the methods for authorship attribution
without recourse to direct expert testimony, if the methods themselves
are simple enough and credible enough (as with wellman [149]). even
with only partial admissibility, it may still be possible for an expert
to take the stand, outline a set of features (such as thompson   s use
of    words indicating position   ) and demonstrate informally that the
defendant   s use is more in-line with the document under discussion

8.3 admissibility

311

than other candidates, while leaving the jury to draw the necessary
conclusion that the document was written by the defendant.

nevertheless, it would be better all around if the standards for
authorship attribution met the standards for admissible evidence.
these latter standards, of course, are always shifting and vary not only
from place to place, but also from trial to trial. we focus brie   y on the
evidence standards under us law.

8.3.1 us legal background

expert evidence in the united states is dominated by two major cases,
frye vs. united states (1923), and daubert vs. merrill dow (1993), each
of which establishes separate tests for whether or not scienti   c evidence
is admissible. historically speaking, the frye test established rules for
federal procedure that were later copied by most us states. in this case,
the court recognized that, while science is a progressive enterprise, the
courts have an active duty to sort out pseudoscience and avoid undue
reliance on controversial or unsupported theories. the court thus held
that

just when a scienti   c principle or discovery crosses the
line between experimental and demonstrable stages is
di   cult to de   ne. somewhere in this twilight zone the
evidential force of the principle must be recognized, and
while courts will go a long way in admitting expert tes-
timony deduced from a well-recognized scienti   c prin-
ciple or discovery, the thing from which the deduction
is made must be su   ciently established to have gained
general acceptance in the particular    eld in which it
belongs.

this    general acceptance    test de   nes acceptable science as the
uncontroversial core. in the particular case under evaluation, the pro-
posed science (later described in daubert as    a systolic blood pressure
deception test, a crude precursor to the polygraph machine   ) was not
su   ciently accepted and therefore not admissible.

312 special problems of linguistic forensics

later rewritings of the federal rules of evidence (most notably
fre 702) laid down a more sophisticated epistemological framework.
rule 702 reads:

if scienti   c, technical, or other specialized knowledge
will assist the trier of fact to understand the evidence
or to determine a fact in issue, a witness quali   ed as
an expert by knowledge, skill, experience, training, or
education, may testify thereto in the form of an opinion
or otherwise, if (1) the testimony is based upon su   -
cient facts or data, (2) the testimony is the product of
reliable principles and methods, and (3) the witness has
applied the principles and methods reliably to the facts
of the case.

(note that this rule does not even mention    general acceptance   !)
the supreme court ampli   ed and interpreted this rule in daubert,
creating a new standard. in broad terms, the court recognized that
the frye test was unduly restrictive and eliminated much cutting-
edge but legitimate science. the role of the court was substantially
reduced to evaluating whether or not a proposed bit of testimony rested
on    su   cient facts or data    and was the product of    reliable princi-
ple and methods.    novel or controversial, but still scienti   c,    ndings
were to be admitted, although their weight would still be for the jury
to decide.

the daubert decision avoided drawing a clear line or enumerating a
checklist of the features that a method must have in order to be reliable
and scienti   c, but it did note a number of factors that are ordinarily
likely to be relevant. these factors include

    whether or not the method of analysis has been tested. the
court speci   cally drew attention to the popperian theory of
falsi   ability; a    scienti   c    method is one that can be sub-
jected to empirical tests to see whether it is true or false.
    whether or not the method of analysis has been subject to
peer review and publication. science operates largely on the
basis of shared knowledge and consensus, established through

8.3 admissibility

313

the publication process. this review process also acts as a
   lter,    in part because it increases the likelihood that sub-
stantive    aws in methodology will be detected.    (daubert)
    the known or potential rate of error     and in particular,
whether the technique has a known, potential rate of error.
obviously, knowing the accuracy rate of a technique is key to
knowing how much weight to give the results of its analyses.
    the existence of standards and practices governing the use
of the technique, and by extension the existence of standards
and practice bodies.
    finally,    general acceptance    is still a consideration, as    it
does permit [. . .] explicit identi   cation of a relevant scien-
ti   c community and an express determination of a particular
degree of acceptance within that community.    in particular,
   a known technique that has been able to attract only min-
imal support within the community [. . .] may properly be
viewed with skepticism.    (daubert)

as a federal court decision (and one interpreting the federal rules
of evidence), daubert is technically only binding at the federal level,
but many states have adjusted their rules to incorporate daubert. oth-
ers have not, preferring to stay with frye or extensions to frye.

8.3.2 other jurisdictions

other countries, of course, have their own rules and procedures with
regard to expert scienti   c evidence. in much of the british common-
wealth, the role of the expert is somewhat di   erent. in england, for
example, expert witnesses (under the civil procedure rules, rule 35)
under an overriding duty, not to the person that hired them, but to the
court. as a result, they are o   cially independent, overriding    any obli-
gation to the person from whom the expert has received instructions
or by whom he is paid: rule 35.3(2).   

partly for this reason, it is (at least in theory) more di   cult under
english law than under us law to get an expert report written to order
to support just one side of the case. english procedure focuses more
on the quali   cations of experts; to be admissible, a purported expert

314 special problems of linguistic forensics

must satisfy the court that there is    recognised expertise governed by
recognised standards and rules of conduct capable of in   uencing the
court   s decision,    and further, that the expert himself    has a su   cient
familiarity with and knowledge of the expertise in question to render
his opinion potentially of value    (barings plc v coopers & lybrand
(no. 2) [2001]). this, of course, is similar in spirit to the frye test, in
that the key determination is whether or not an established body of
practice (general acceptance) exists.

the law is similar in other common law jurisdictions [46]; for exam-
ple, the wording of the australian civil procedure act 2005 (nsw) lays
down the same overriding duty of independence. other legal systems
are beyond the scope of this report; consult your local experts.

8.3.3 admissibility of authorship attribution

it should be apparent that, regardless of the actual jurisdiction, the
spirit of the daubert criteria are a standard that any practicing scholar
should be happy to try to meet. who would not want their methods
and practices to be    reliable    and their judgments to be    based on
su   cient facts and data?    it should also be apparent that the other
standards, whether    general acceptance    or    recognized expertise gov-
erned by recognized standards    can only come from a body of practices
that meet the daubert criteria for reliability and su   ciency. the ques-
tion, of course, is whether the current state of authorship attribution
meets them or not.

this has been a matter of some debate; as recently as 200l, tiersma
and solan (cited in [102]) cited disputed authorship cases as a    problem
area    in forensic linguistics, while mcmenamin disagreed. from a prin-
cipled standpoint, there is ample reason to be concerned about many
individual techniques and methods. one key problem with function
word pca, for example, is that the technique does not easily admit
to the calculation of real or expected error rates. other methods, most
notably the easily explained primitive statistics, are too inaccurate (and
too easily proved inaccurate) to be taken seriously by jurors, while
other methods have simply not been proved against a large enough set
of trials.

8.4 malice and deception

315

on the other hand, papers such as the current one should help
establish both the existence of a large research community that    gen-
erally accepts    the idea of stylometric analysis, and more importantly,
establishes generally accepted standards for accuracy across a number
of di   erent areas and corpora. the methods speci   cally recommended
by such peer-reviewed papers (see the following section) can be argued
to have established themselves as practices.

at any rate, the ultimate determiner of whether or not a particular
method is admissible is the speci   c court before whom one wishes to
testify. as chaski [27] has pointed out, in many speci   c courts, this
test has already been passed.

8.4 malice and deception

one    nal consideration in the evaluation of forensic authorship attri-
bution is the question of active malicious alteration of writing style. in
short, can i write using someone else   s style in order to deceive the ana-
lyst (and by extension, the court system)? this problem is not entirely
con   ned to the forensic realm (there are many other reasons that one
could want to disguise one   s true authorship), but it is a speci   c concern
for forensics and takes on a heightened importance.

to some extent, the question of fooling the analysis obviously
depends on how good the analysis is. in particular, it depends both
on how robust the feature set (the    stylome   ) is to deliberate manipu-
lation as well as how sensitive the analysis mechanism is. it is relatively
eazy for a good speler to pritend to be a pooor one (and therefore per-
haps to fool an anomaly-based scheme). it is harder for a poor speller
to pretend to be a good one (without the aid of a co-author or a spelling
checker). it is simple to fake the use of dialect words that are not one   s
own, but harder to use them in their proper context (e.g., *   voltmetre   
only looks like a british word; it is not. a    metre    is a unit of length,
not a measuring instrument). harder still (one hopes) is to model one   s
use of abstract concepts like function words in order to match another
person   s patterns.

an early study [136] of this found that di   erent methods (as might
be expected) di   ered in their abilities to detect    pastiche.    using

316 special problems of linguistic forensics

adair   s alice through the needle   s eye, a parody of lewis carroll,
somers and tweedie found that measures of lexical richness distin-
guish the authors, but that word frequency pca did not. a later study
[82] focused on lexical variation in the federalist papers. using deci-
sion trees and id166s, the researchers found the word tokens that were
most informative of authorship and neutralized them. with an average
of about 14 changes per thousand words of text, the researcher were
able to essentially neutralize the id166   s ability to correctly identify the
author (reducing the likelihood of a successful attribution by more than
80%). on the other hand, more sophisticated techniques were still able
to identify the original author after changes.

from a research standpoint, this suggests the strong possibility of
some kind of predator   prey or arms race situation; the best techniques
for obfuscation will depend on what sort of analysis is expected. there
is obviously great potential for further work here.

9

recommendations

9.1 discussion

as the previous sections should make clear, stylometry and authorship
attribution are an active research area encompassing a wide variety
of approaches, techniques, and sub-problems. no paper of reasonable
length can attempt to do justice to the entire    eld.

from this confusing mess, however, a few overall themes appear to
emerge. the    rst, quite simply, is that it works        non-traditional   
authorship attribution, attribution based on the mathematical and sta-
tistical analysis of text, can identify the author of a document with
id203 substantially better than chance. the second is that almost
anything can be a cue to authorship under the right circumstances,
and the primary task for researchers is not to    nd potential cues, but
to evaluate and combine them in a way that produces the greatest
overall accuracy. the third is that the way the problem is de   ned
matters greatly, both since    authorship attribution    can include other
aspects such as group identi   cation, gender attribution, and so forth,
and because the intended use of the results can have a strong in   u-
ence on the best method to use. the needs of the forensic community,

317

318 recommendations

for example, are di   erent from those of traditional english scholarship,
which in turn is di   erent from the needs of a high school teacher looking
to prevent cheating.

in discussing authorship attribution, we have made extensive use
of juola   s three-phase framework [76]. separating the purely mechan-
ical aspects of canonicization (stripping out markup, spelling regular-
ization, case id172, and so forth) from the process of iden-
tifying the event set or feature space, and separating that, in turn,
from the analysis method make it easier to present and understand
the possibly bewildering array of methods that have been proposed.
most researchers have approached this problem by embedding doc-
uments in a high-dimensional space of abstract features and then
applying some sort of statistical analysis to that space. more sophis-
ticated     but also more computationally intensive, and more di   cult
to understand     approaches apply machine learning techniques to the
analysis, treating it as a form of document categorization problem. dif-
ferent types of feature space used include vocabulary, syntax, orthogra-
phy, structure/layout, or miscellaneous anomalies, or a combination of
the above     the number of features used in any particular study can
vary from merely one or two up to hundreds or thousands. the num-
ber of analysis methods applied is almost equally huge, ranging from
simple summary statistics such as averages, through unsupervised anal-
yses like principal component analysis or hierarchical cluster analysis,
to state-of-the-art machine learning and data mining techniques such as
id156 and support vector machines. much addi-
tional research can be done simply in identifying new features and new
methods to see whether or not they work.

however, another advantage of this three-phase framework is that
it can be (and has been [81]) used to direct software development
for the systematic exploration of this problem. a simple combina-
tion of twenty-   ve di   erent feature sets with    fteen di   erent analytic
variations yields almost four hundred di   erent and potentially high-
performing systems to explore. juola and his students have begun this
process with the development of jgaap, a modular framework for
authorship attribution using the object-oriented capacities of the java
programming language. this program de   nes separate    event    and

9.2 recommendations

319

   processor    classes that can be implemented in a variety of ways; for
example, an event could be de   ned as a letter, a word, a pos tag,
a punctuation mark, or a word selected from a speci   c pre-de   ned
set (as of function words or synonym sets); each document, in turn,
can be represented as a vector of events upon which the processor
operates. a processor implementing burrows    delta could calculate
z-distributions of any sort of event and categorize documents appropri-
ately; variations on delta [63, 138] would operate similarly, but di   er in
the details.

a major theme implicit in the previous paragraph is the exis-
tence of a dataset for testing and cross-comparison. many researchers
[25, 40, 63, 75, 80] have been quite explicit in their call for such a
framework. as a simple example, re-analysis of the aaac data (juola,
forthcoming) has shown that a mixture of experts approach can outper-
form any individual aaac participant. absent this sort of standard-
ized test data, this result would not have been apparent. no corpus
has yet emerged to be the standard touchstone and testbed, although
there are a few promising candidates such as juola   s aaac corpus or
chaski   s writing sample database. depending upon the community   s
needs, further development of test corpora is almost inevitable, but
the exact form that it will take depends on what type of problem is
held to be more important. because of the amount of research oppor-
tunity available in this    eld, the next several years promise to be quite
interesting and productive.

9.2 recommendations

what, though, of the plight of a researcher who cannot a   ord to wait
   ve or ten years for an answer? the aaac and studies like it can
at least provide the basis for a preliminary set of recommendations.
for those interested in forensic applications, where documented    best
practices    are needed before the judge will even look at your report    
i can recommend the following:

    before performing a non-standard authorship analysis, con-
duct a standard one, consulting with experts as necessary.
rudman, in particular, has written often and at length about

320 recommendations

the need for such an examination. the results of the analysis
can only be as accurate as the data, including the quality of
the document to be examined and of the training data. all
data must be carefully vetted to get, as close as possible, back
to the original manuscript. the set of candidate authors must
be chosen as carefully and rationally as possible, and the set
of candidate writings must also be chosen with equal care
and rationality. for example, if the document of interest is
a novel written in third person, the distribution of pronouns
will [64] be radically di   erent than that of a novel written
in    rst person, not by virtue of an authorship di   erence, but
simply from genre.

as hoover showed in one of his submissions to the aaac,
in some cases, perhaps many, an analysis of this sort may
render the stylometry itself irrelevant. if a search for candi-
date writings turns up the original document of interest as
an entry in a google search, then the attribution problem is
trivial. if a close reading of the text reveals the name of the
author in the metadata, there is no need for statistics.
    methods using a large number of features seem to outper-
form methods using a small number of features, provided
that there is some method of weighting or sorting through
the feature set. in particular, simple statistics of a few simple
features, such as average word or sentence length, does not
usually produce accurate enough results to be used. there
appears to be general agreement that both function words
(or sometimes merely frequent words) and pos tags are good
features to include. methods that do not use syntax in one
form or another, either through the use of word id165s or
explicit syntactic coding tend to perform poorly. other fea-
ture categories are more controversial.
    the best choice of feature set is of course strongly dependent
upon the data to be analyzed     for example, the set of func-
tion words is controlled by the language of the documents.
however, the choice of analysis method appears to be largely
language independent. no method has yet emerged from any

9.2 recommendations

321

study as being particularly good within a narrow range of
language, genre, size, and so forth. this lends credence to
the theory that there is a best possible analysis method, and
we simply need to    nd it.
    simple unsupervised analysis     most notably, principal com-
ponent analysis     will sometimes produce useful and easy-
to-understand results. on the other hand, pca and similar
algorithms are often unable to uncover authorship structure
that more powerful algorithms    nd.
    understandability remains a major problem. one reason that
pca and similar algorithms remain popular, despite their
modest e   ectiveness, is that the reasons for their decisions
can easily be articulated. the same vector space that cate-
gorizes text can be used to categorize individual words (or
features); one can literally superimpose on the graphic sepa-
rating a from b the words used, and the words near a are
the ones that a uses and b does not. for researchers more
interested in the    why    than the    what,    this ease of expla-
nation is a key feature of pca. otherwise, you run the risk
of being able to tell people apart without actually knowing
anything about their styles [30].
    the real heavyweights emerging from the aaac are the
same high-performing analysis methods that have been useful
elsewhere. these high-   yers include support vector machines
(id166), id156 (lda), and k-nearest
neighbor in a suitably chosen space (either by cross-id178
or id165 distance). these methods were the top four per-
formers in the aaac and have usually been reported as the
top-scoring method in other analyses. id166, in particular,
appears to be the leading contender for    best performing
analysis method for any given feature set.   
    document all steps in the process. in a forensic context, this
is a necessity. in a less formal context, this is still important,
as the minor details of document processing will often be key
to the accuracy of your results.

322 recommendations

9.3 conclusions and future work

why care about authorship attribution? and, especially, why care
about statistical methods for doing authorship attribution? because
   style,    and the identity underlying style, has been a major focus
of humanistic inquiry since time immemorial. just as corpus studies
have produced a revolution in linguistics, both by challenging long-
held beliefs and by making new methods of study practicable,    non-
traditional    stylometry can force researchers to re-evaluate long-held
beliefs about the individualization of language. the practical bene   ts
are also immense; applications include not only literature scholarship,
but teaching, history, journalism, computer security, civil and criminal
law, and intelligence investigation. automatic authorship attribution
holds the promise both of greater ease of use and improved accuracy.
this review has discussed only a few of the approaches that have
been proposed over a longer-than-a-century history of statistical sty-
lometry. given the current research interest, it will no doubt be out of
date by the time it sees print. new researchers will produce new cor-
pora and new ways of testing to better evaluate the current state of the
art. others will invent new methods, taking advantage of advances in
other disciplines such as machine learning and classi   cation. id166, the
current front-runner, was not even dreamed of twenty years ago; twenty
years from now, the front-runner will probably be a technique currently
unknown. similarly, the variety of problems     not just identi   cation
of    authorship,    but of authorial gender, age, social class, education,
personality, and mental state     will almost certainly increase as people
   nd it useful to more accurately pro   le their correspondents.

the most serious problem facing the current discipline is disorga-
nization. there are too many mediocre solutions, and not enough that
are both good and principled. especially if authorship is to be use-
ful as a forensic discipline, there are a number of crucial issues that
need to be addressed fortunately, the seeds of most of the issues and
developments have already been planted.

better test data, for example, is something of a sine qua non. some
test corpora have already been developed, and others are on the way.
a key aspect to be addressed is the development of speci   c corpora

9.3 conclusions and future work

323

representing the speci   c needs of speci   c communities. for example,
researchers such as nyu   s david hoover have been collecting large
sets of literary text such as novels, to better aid in the literary anal-
ysis of major authors. such corpora can easily be deployed to answer
questions of literary style, such as whether or not a given (anonymous)
political pamphlet was actually written by an author of recognized
merit, and as such re   ects his/her political and social views, to the
enrichment of scholars. such a corpus, however, would not be of much
use to law enforcement; not only is 18th or 19th century text unrep-
resentative of the 21st century, but the idea of a 100,000 word ransom
note being analyzed with an eye toward criminal prosecution borders
on the ludicrous. the needs of law enforcement are much better served
by the development of corpora of web log (blog) entries, email, and so
forth     document styles that are used routinely in investigations. so
while we can expect to see much greater development of corpora to serve
community needs, we can also expect a certain degree of fragmentation
as di   erent subcommunities express (and fund) di   erent needs.

we can expect to see the current ad hoc mess of methods and algo-
rithms to be straightened out, as testing on the newly developed cor-
pora becomes more commonplace. programs such as jgaap [81] will
help support the idea of standardized testing of new algorithms on
standardized problems, and the software programs themselves can and
will be made available in standard (tested) con   gurations for use by
non-experts. just as tools like flash make multimedia publishing prac-
tical, so will the next generation of authorship attribution tools make
stylometry generally practical.

the new level of computer support will trigger new levels of under-
standing of the algorithms. although some e   orts (most notably arg-
amon [138]) have been made to explain not only that certain methods
work, but to why they work, most research to this date has been con-
tent with    nding accurate methods rather than explaining them. the
need to explain one   s conclusions, whether to a judge, jury, and oppos-
ing counsel, or simply to a non-technical phd advisor, will not doubt
spur research into the fundamental linguistic, psychological, and cogni-
tive underpinnings, possibly shedding more light on the purely mental
aspects of authorship.

324 recommendations

finally, as scholarship in these areas improves and provides new
resources, the uptake and acceptance of non-traditional authorship
attribution can be expected to improve. the communities that drive
authorship attribution do so because they need the results. the better
the results, the more badly they are needed.

why care about authorship attribution? ultimately, because other

people do.

references

[1] a. abbasi and h. chen,    applying authorship analysis to extremist-group
web forum messages,    ieee intelligent systems, vol. 20, no. 6, pp. 67   75,
2005.

[2] a. abbasi and h. chen, visualizing authorship for identi   cation, pp. 60   71.

springer, 2006.

[3] american board of forensic document examiners,    frequently asked ques-

tions,    http://www.abfde.org/faqs.html, accessed january 6, 2007.

[4] anonymous,    some anachronisms in the january 4, 1822 beale letter,   

http://www.myoutbox.net/bch2.htm, accessed may 31, 2007, 1984.

[5] s. argamon and s. levitan,    measuring the usefulness of function words for
authorship attribution,    in proceedings of ach/allc 2005, association for
computing and the humanities, victoria, bc, 2005.

[6] s. argamon, s. dhawle, m. koppel, and j. w. pennebaker,    lexical predic-
tors of personality type,    in proceedings of the classi   cation society of north
america annual meeting, 2005.

[7] a. argamon-engleson, m. koppel, and g. avneri,    style-based text catego-
rization: what newspaper am i reading,    in proceedings of the aaai work-
shop of learning for text categorization, pp. 1   4, 1998.

[8] r. h. baayen, h. van halteren, a. neijt, and f. tweedie,    an experiment in
authorship attribution,    in proceedings of jadt 2002, pp. 29   37, universit  e
de rennes, st. malo, 2002.

[9] r. h. baayen, h. van halteren, and f. tweedie,    outside the cave of shadows:
using syntactic annotation to enhance authorship attribution,    literary and
linguistic computing, vol. 11, pp. 121   131, 1996.

325

326 references

[10] r. e. bee,    some methods in the study of the masoretic text of the old
testament,    journal of the royal statistical society, vol. 134, no. 4, pp. 611   
622, 1971.

[11] r. e. bee,    a statistical study of the pinai pericope,    journal of the royal

statistical society, vol. 135, no. 3, pp. 391   402, 1972.

[12] d. benedetto, e. caglioti, and v. loreto,    language trees and zipping,    phys-

ical review letters, vol. 88, no. 4, p. 048072, 2002.

[13] d. biber, s. conrad, and r. reppen, corpus linguistics: investigating lan-

guage structure and use. cambridge: cambridge university press, 1998.

[14] j. n. g. binongo,    who wrote the 15th book of oz? an application of mul-
tivariate analysis to authorship attribution,    chance, vol. 16, no. 2, pp. 9   17,
2003.

[15] a. f. bissell,    weighted cumulative sums for text analysis using word counts,   

journal of the royal statistical society a, vol. 158, pp. 525   545, 1995.

[16] e. brill,    a corpus-based approach to language learning,    phd thesis, uni-

versity of pennsylvania, 1993.

[17] c. brown, m. a. covington, j. semple, and j. brown,    reduced idea den-
sity in speech as an indicator of schizophrenia and ketamine intoxication,    in
international congress on schizophrenia research, savannah, ga, 2005.

[18] p. f. brown, j. cocke, s. a. della pietra, v. j. della pietra, f. jelinek, j. d.
la   erty, r. l. mercer, and p. s. roossin,    a statistical approach to machine
translation,    computational linguistics, vol. 16, pp. 79   85, june 1990.

[19] c. j. c. burges,    a tutorial on support vector machines for pattern recog-
nition,    data mining and knowledge discovery, vol. 2, no. 2, pp. 955   974,
1998.

[20] j. f. burrows,       an ocean where each kind. . .    : statistical analysis and some
major determinants of literary style,    computers and the humanities, vol. 23,
no. 4   5, pp. 309   21, 1989.

[21] j. f. burrows,    delta: a measure of stylistic di   erence and a guide to likely
authorship,    literary and linguistic computing, vol. 17, pp. 267   287, 2002.
[22] j. f. burrows,    questions of authorship: attribution and beyond,    computers

and the humanities, vol. 37, no. 1, pp. 5   32, 2003.

[23] f. can and j. m. patton,    change of writing style with time,    computers

and the humanities, vol. 28, no. 4, pp. 61   82, 2004.

[24] d. canter,    an evaluation of    cusum    stylistic analysis of confessions,    expert

evidence, vol. 1, no. 2, pp. 93   99, 1992.

[25] c. e. chaski,    empirical evaluations of language-based author identi   cation,   

forensic linguistics, vol. 8, no. 1, pp. 1   65, 2001.

[26] c. e. chaski,    who   s at the keyboard: authorship attribution in digital evi-
dence invesigations,    international journal of digital evidence, vol. 4, no. 1,
p. n/a, electronic-only journal: http://www.ijde.org, accessed may 31, 2007,
2005.

[27] c. e. chaski,    the keyboard dilemma and forensic authorship attribution,   

advances in digital forensics iii, 2007.

[28] d. coniam,    concordancing oneself: constructing individual textual pro   les,   
international journal of corpus linguistics, vol. 9, no. 2, pp. 271   298, 2004.

references

327

[29] m. corney, o. de vel, a. anderson, and g. mohay,    gender-preferential text
mining of e-mail discourse,    in proceedings of computer security applications
conference, pp. 282   289, 2002.

[30] h. craig    authorial attribution and computational stylistics: if you can tell
authors apart, have you learned anything about them?    literary and linguis-
tic computing, vol. 14, no. 1, pp. 103   113, 1999.

[31] d. cutting, j. kupiec, j. pedersen, and p. sibun,    a practical part-of-speech
tagger,    in proceedings of the third conference on applied natural lanugage
processing, association for computational linguistics, trento, italy, april
1992. also available as xerox parc technical report ssl-92-01.

[32] a. de morgan,    letter to rev. heald 18/08/1851,    in memoirs of augustus
de morgan by his wife sophia elizabeth de morgan with selections from his
letters, (s. elizabeth and d. morgan, eds.), london: longman   s green and
co., 1851/1882.

[33] g. easson,    the linguistic implications of shibboleths,    in annual meeting of

the canadian linguistics association, toronto, canada, 2002.

[34] a. ellegard, a statistical method for determining authorship: the junius
leters 1769   1772. gothenburg, sweden: university of gothenburg press,
1962.

[35] w. elliot and r. j. valenza,    and then there were none: winnowing the
shakespeare claimants,    computers and the humanities, vol. 30, pp. 191   245,
1996.

[36] w. elliot and r. j. valenza,    the professor doth protest too much, methinks,   

computers and the humanities, vol. 32, pp. 425   490, 1998.

[37] w. elliot and r. j. valenza,    so many hardballs so few over the plate,   

computers and the humanities, vol. 36, pp. 455   460, 2002.

[38] m. farach, m. noordewier, s. savari, l. shepp, a. wyner, and j. ziv,    on
the id178 of dna: algorithms and measurements based on memory and
rapid convergence,    in proceedings of the sixth annual acm-siam sympo-
sium on discrete algorithms, pp. 48   57, san francisco, california, january
22   24, 1995.

[39] j. m. farringdon, analyzing for authorship: a guide to the cusum technique.

cardi   : university of wales press, 1996.

[40] r. s. forsyth,    towards a text benchmark suite,    in proceedings of 1997 joint
international conference of the association for computers and the humani-
ties and the association for literary and linguistic computing (ach/allc
1997), kingston, on, 1997.

[41] d. foster, an elegy by w.s.: a study in attribution. newark: university of

delaware press, 1989.

[42] d. foster,    attributing a funeral elegy,    pmla, vol. 112, no. 3, pp. 432   434,

1997.

[43] d. foster, author unknown: adventures of a literary detective. london: owl

books, 2000.

[44] d. foster, author unknown: on the trail of anonymous. new york: henry

holt and company, 2001.

328 references

[45] w. fucks,    on the mathematical analysis of style,    biometrika, vol. 39,

pp. 122   129, 1952.

[46] j. gibbons, forensic linguistics: an introduction to language in the justice

system. oxford: blackwell, 2003.

[47] n. graham, g. hirst, and b. marthi,    segmenting documents by stylistic

character,    natural language engineering, vol. 11, pp. 397   415, 2005.

[48] t. grant and k. baker,    identifying reliable, valid markers of authorship:

a reponse to chaski,    forensic linguistics, vol. 8, no. 1, pp. 66   79, 2001.

[49] t. r. g. green,    the necessity of syntax markers: two experiments with
arti   cial languages,    journal of verbal learning and verbal behavior, vol. 18,
pp. 481   96, 1979.

[50] j. w. grieve,    quantitative authorship attribution: a history and an eval-
uation of techniques   . master   s thesis, simon fraser university, 2005. url:
http://hdl.handle.net/1892/2055, accessed may 31, 2007.

[51] j. hancock,    digital deception: when, where and how people lie online,    in
oxford handbook of internet psychology, (k. mckenna, t. postmes, u. reips,
and a. joinson, eds.), pp. 287   301, oxford: oxford university press, 2007.

[52] r. a. hardcastle,    forensic linguistics: an assessment of the cusum method
for the determination of authorship,    journal of the forensic science society,
vol. 33, no. 2, pp. 95   106, 1993.

[53] r. a. hardcastle,    cusum: a credible method for the determination of author-

ship?,    science and justice, vol. 37, no. 2, pp. 129   138, 1997.

[54] j. hertz, a. krogh, and r. g. palmer, introduction to the theory of neural

computation. redwood city, ca: addison wesley, 1991.

[55] m. l. hilton and d. i. holmes,    an assessment of cumulative sum charts for
authorship attribution,    literary and linguistic computing, vol. 8, pp. 73   80,
1993.

[56] d. i. holmes,    authorship attribution,    computers and the humanities,

vol. 28, no. 2, pp. 87   106, 1994.

[57] d. i. holmes,    the evolution of stylometry in humanities computing,    literary

and linguistic computing, vol. 13, no. 3, pp. 111   117, 1998.

[58] d. i. holmes and r. s. forsyth,    the federalist revisited: new directions in
authorship attribution,    literary and linguistic computing, vol. 10, no. 2,
pp. 111   127, 1995.

[59] d. i. holmes,    stylometry and the civil war: the case of the pickett letters,   

chance, vol. 16, no. 2, pp. 18   26, 2003.

[60] d. i. holmes and f. j. tweedie,    forensic stylometry: a review of the cusum
controversy,    in revue informatique et statistique dans les science humaines,
pp. 19   47, university of liege, liege, belgium, 1995.

[61] d. hoover,    another perspective on vocabulary richness,    computers and the

humanities, vol. 37, no. 2, pp. 151   178, 2003.

[62] d. hoover,    stylometry, chronology, and the styles of henry james,    in pro-

ceedings of digital humanities 2006, pp. 78   80, paris, 2006.

[63] d. l. hoover,    delta prime?,    literary and linguistic computing, vol. 19,

no. 4, pp. 477   495, 2004.

[64] d. l. hoover,    testing burrows   s delta,    literary and linguistic computing,

vol. 19, no. 4, pp. 453   475, 2004.

references

329

[65] j. hopcroft and j. ullman, introduction to automata theory, languages, and

computation. reading: addison-wesley, 1979.

[66] s. r. hota, s. argamon, m. koppel, and i. zigdon,    performing gender: auto-
matic stylistic analysis of shakespeare   s characters,    in proceedings of digital
humanities 2006, pp. 100   104, paris, 2006.

[67] igas,    igas     our company,    http://www.igas.com/company.asp,

accessed may 31, 2007.

[68] m. p. jackson,    function words in the    funeral elegy   ,    the shakespeare

newsletter, vol. 45, no. 4, p. 74, 1995.

[69] t. joachims, learning to classify text using support vector machines.

kluwer, 2002.

[70] e. johnson, lexical change and variation in the southeastern united states

1930   1990. tuscaloosa, al: university of alabama press, 1996.

[71] p. juola,    what can we do with small corpora? document categorization via
cross-id178,    in proceedings of an interdisciplinary workshop on similarity
and categorization, department of arti   cial intelligence, university of edin-
burgh, edinburgh, uk, 1997.

[72] p. juola,    cross-id178 and linguistic typology,    in proceedings of new meth-
ods in language processing and computational natural language learning,
(d. m. w. powers, ed.), sydney, australia: acl, 1998.

[73] p. juola,    measuring linguistic complexity: the morphological tier,    journal

of quantitative linguistics, vol. 5, no. 3, pp. 206   213, 1998.

[74] p. juola,    the time course of language change,    computers and the human-

ities, vol. 37, no. 1, pp. 77   96, 2003.

[75] p. juola,    ad-hoc authorship attribution competition,    in proceedings of
2004 joint international conference of the association for literary and lin-
guistic computing and the association for computers and the humanities
(allc/ach 2004), g  oteborg, sweden, june 2004.

[76] p. juola,    on composership attribution,    in proceedings of 2004 joint inter-
national conference of the association for literary and linguistic computing
and the association for computers and the humanities (allc/ach 2004),
g  oteborg, sweden, june 2004.

[77] p. juola,    compression-based analysis of language complexity,    presented at

approaches to complexity in language, 2005.

[78] p. juola,    authorship attribution for electronic documents,    in advances in
digital forensics ii, (m. olivier and s. shenoi, eds.), pp. 119   130, boston:
springer, 2006.

[79] p. juola,    becoming jack london,    journal of quantitative linguistics,

vol. 14, no. 2, pp. 145   147, 2007.

[80] p. juola and h. baayen,    a controlled-corpus experiment in authorship attri-
bution by cross-id178,    literary and linguistic computing, vol. 20, pp. 59   
67, 2005.

[81] p. juola, j. sofko, and p. brennan,    a prototype for authorship attribu-
tion studies,    literary and linguistic computing, vol. 21, no. 2, pp. 169   178,
advance access published on april 12, 2006; doi: doi:10.1093/llc/fql019, 2006.

330 references

[82] g. kacmarcik and m. gamon,    obfuscating document stylometry to preserve

author anonymity,    in proceedings of acl 2006, 2006.

[83] a. kenny, the computation of style. oxford: pergamon press, 1982.
[84] v. keselj and n. cercone,    cng method with weighted voting,    in ad-hoc

authorship attribution contest, (p. juola, ed.), ach/allc 2004, 2004.

[85] v. keselj, f. peng, n. cercone, and c. thomas,    id165-based author pro-
   les for authorship attribution,    in proceedings of the conference paci   c
association for computational linguistics, pacling03, pp. 255   264, dal-
housie university, halifax, ns, august 2003.

[86] k. keune, m. ernestus, r. van hout, and h. baayen,    social, geographical,
and register variation in dutch: from written mogelijk to spoken mok,   
in proceedings of ach/allc 2005, victoria, bc, canada, 2005.

[87] d. v. khmelev and f. j. tweedie,    using markov chains for identi   cation of
writers,    literary and linguistic computing, vol. 16, no. 3, pp. 299   307, 2001.
[88] m. koppel, n. akiva, and i. dagan,    feature instability as a criterion for
selecting potential style markers,    journal of the american society for infor-
mation science and technology, vol. 57, no. 11, pp. 1519   1525, 2006.

[89] m. koppel, s. argamon, and a. r. shimoni,    automatically categorizing
written texts by author gender,    literary and linguistic computing, vol. 17,
no. 4, pp. 401   412, doi:10.1093/llc/17.4.401, 2002.

[90] m. koppel and j. schler,    exploiting stylistic idiosyncrasies for author-
ship attribution,    in proceedings of ijcai   03 workshop on computational
approaches to style analysis and synthesis, acapulco, mexico, 2003.

[91] m. koppel and j. schler,    ad-hoc authorship attribution competition
approach outline,    in ad-hoc authorship attribution contest, (p. juola, ed.),
ach/allc 2004, 2004.

[92] l. kruh,    a basic probe of the beale cipher as a bamboozlement: part i,   

cryptologia, vol. 6, no. 4, pp. 378   382, 1982.

[93] l. kruh,    the beale cipher as a bamboozlement: part ii,    cryptologia, vol. 12,

no. 4, pp. 241   246, 1988.

[94] h. kucera and w. n. francis, computational analysis of present-day amer-

ican english. providence: brown university press, 1967.

[95] t. kucukyilmaz, b. b. cambazoglu, c. aykanat, and f. can,    chat min-
ing for gender prediction,    lecture notes in computer science, vol. 4243,
p. 274283, 2006.

[96] o. v. kukushkina, a. a. polikarpov, and d. v. khmelev,    using literal and
grammatical statistics for authorship attribution,    problemy peredachi infor-
matii, vol. 37, no. 2, pp. 96   198, translated in    problems of information
transmission,    pp. 172   184, 2000.

[97] m. li and p. vit  anyi, an introduction to kolmogorov complexity and its
applications. graduate texts in computer science, new york: springer, sec-
ond ed., 1997.

[98] h. love, attributing authorship: an introduction. cambridge: cambridge uni-

versity press, 2002.

[99] c. martindale and d. mckenzie,    on the utility of content analysis in author-
ship attribution: the federalist papers,    computers and the humanities,
vol. 29, pp. 259   70, 1995.

references

331

[100] r. a. j. matthews and t. v. n. merriam,    neural computation in stylometry
i: an application to the works of shakespeare and marlowe,    literary and
linguistic computing, vol. 8, no. 4, pp. 203   209, 1993.

[101] j. l. mcclelland, d. e. rumelhart, and the pdp research group, parallel
distributed processing: explorations in the microstructure of cognition. cam-
bridge, ma: mit press, 1987.

[102] g. mcmenamin,    disputed authorship in us law,    international journal of

speech, language and the law, vol. 11, no. 1, pp. 73   82, 2004.
[103] g. r. mcmenamin, forensic stylistics. london: elsevier, 1993.
[104] g. r. mcmenamin,    style markers in authorship studies,    forensic linguis-

tics, vol. 8, no. 2, pp. 93   97, 2001.

[105] g. r. mcmenamin, forensic linguistics     advances in forensic stylistics.

boca raton, fl: crc press, 2002.

[106] t. c. mendenhall,    the characteristic curves of composition,    science, vol. ix,

pp. 237   249, 1887.

[107] t. v. n. merriam and r. a. j. matthews,    neural computation in stylometry
ii: an application to the works of shakespeare and marlowe,    literary and
linguistic computing, vol. 9, no. 1, pp. 1   6, 1994.

[108] g. monsarrat,    a funeral elegy: ford, w.s., and shakespeare,    review of

english studies, vol. 53, p. 186, 2002.

[109] a. w. moore,    support vector machines,    online tutorial: http://jmvidal.

cse.sc.edu/csce883/id16614.pdf, accessed may 31, 2007, 2001.

[110] j. l. morgan, from simple input to complex grammar. cambridge, ma:

mit press, 1986.

[111] a. q. morton, literary detection: how to prove authorship and fraud in

literature and documents. new york: scribner   s, 1978.

[112] f. mosteller and d. l. wallace, id136 and disputed authorship: the fed-

eralist. reading, ma: addison-wesley, 1964.

[113] m. newman, j. pennebaker, d. berry, and j. richards,    lying words: pre-
dicting deception from linguistic style,    personality and social psychology
bulletin, vol. 29, pp. 665   675, 2003.

[114] s. nowson and j. oberlander,    identifying more bloggers: towards large scale
personality classi   ation of personal weblogs,    in international conference on
weblogs and social media, boulder, co, 2007.

[115] m. oakes,    text categorization: automatic discrimination between us and
uk english using the chi-square text and high ratio pairs,    research in lan-
guage, vol. 1, pp. 143   156, 2003.

[116] j. oberlander and s. nowson,    whose thumb is it anyway? classifying
author personality from weblog text,    in proceedings of the 44th annual meet-
ing of the association for computational linguistics and 21st international
conference on computational linguistics, pp. 627   634, sydney, australia,
2006.

[117] f. peng, d. schuurmans, v. keselj, and s. wang,    language independent
authorship attribution using id186,    in proceedings
of the 10th conference of the european chapter of the association for com-
putational linguistics, pp. 267   274, budapest: acl, 2003.

332 references

[118] j. w. pennebaker and l. a. king,    linguistic styles: language use as an
individual di   erence,    journal of personality and social psychology, vol. 77,
pp. 1296   1312, 1999.

[119] j. w. pennebaker and l. d. stone,    words of wisdom: language use over
the life span,    journal of personality and social psychology, vol. 85, no. 2,
pp. 291   301, 2003.

[120] j. pennebaker, m. mehl, and k. niederho   er,    psychological aspects of natu-
ral language use: our words, ourselves,    annual review of psychology, vol. 54,
pp. 547   577, 2003.

[121] j. r. quinlan, c4.5: programs for machine learning. morgan kau   man, 1993.
[122] m. rockeach, r. homant, and l. penner,    a value analysis of the disputed
federalist papers,    journal of personality and social psychology, vol. 16,
pp. 245   250, 1970.

[123] s. rude, e. gortner, and j. pennebaker,    language use of depressed and
depression-vulnerable college students,    cognition and emotion, vol. 18,
pp. 1121   1133, 2004.

[124] j. rudman,    the state of authorship attribution studies: some problems and

solutions,    computers and the humanities, vol. 31, pp. 351   365, 1998.

[125] j. rudman,    non-traditional authorship attribution studies in eighteenth
century literature: stylistics, statistics and the computer,    url: http://
computerphilologie.uni-muenchen.de/jg02/rudman.html, accessed may 31,
2007.

[126] j. rudman,    the state of authorship attribution studies: (1) the history
and the scope; (2) the problems     towards credibility and validity,    panel
session from ach/allc 1997, 1997.

[127] j. rudman,    the non-traditional case for the authorship of the twelve dis-
puted federalist papers: a monument built on sand,    in proceedings of
ach/allc 2005, association for computing and the humanities, victoria,
bc, 2005.

[128] d. rumelhart, g. hinton, and r. williams,    learning internal representations
by error propagation,    in parallel distributed processing: explorations in the
microstructure of cognition, pp. 318   362, the mit press, 1986.

[129] c. e. shannon,    a mathematical theory of communication,    bell system

technical journal, vol. 27, no. 4, pp. 379   423, 1948.

[130] c. e. shannon,    prediction and id178 of printed english,    bell system

technical journal, vol. 30, no. 1, pp. 50   64, 1951.

[131] e. h. simpson,    measurement of diversity,    nature, vol. 163, p. 688, 1949.
[132] s. singh, the code book: the science of secrecy from ancient egypt to quan-

tum cryptography. anchor, 2000.

[133] m. smith,    recent experiences and new developments of methods for the
determination of authorship,    association of literary and linguistic com-
puting bulletin, vol. 11, pp. 73   82, 1983.

[134] h. h. somers,    statistical methods in literary analysis,    in the computer
and literary style, (j. leed, ed.), kent, oh: kent state university press,
1972.

references

333

[135] h. somers,    an attempt to use weighted cusums to identify sublanguages,   
in proceedings of new methods in language processing 3 and computational
natural langauge learning, (d. m. w. powers, ed.), sydney, australia: acl,
1998.

[136] h. somers and f. tweedie,    authorship attribution and pastiche,    computers

and the humanities, vol. 37, pp. 407   429, 2003.

[137] e. stamatatos, n. fakotakis, and g. kokkinakis,    computer-based authorship
attribution without lexical measures,    computers and the humanities, vol. 35,
no. 2, pp. 193   214, 2001.

[138] s. stein and s. argamon,    a mathematical explanation of burrows    delta,   

in proceedings of digital humanities 2006, paris, france, july 2006.

[139] d. r. tallentire,    towards an archive of lexical norms     a proposal,    in the

computer and literary studies, cardi   : unversity of wales press, 1976.

[140] s. thomas,    attributing a funeral elegy,    pmla, vol. 112, no. 3, p. 431, 1997.
[141] e. tufte, envisioning information. graphics press, 1990.
[142] f. j. tweedie, s. singh, and d. i. holmes,    neural network applications in
stylometry: the federalist papers,    computers and the humanities, vol. 30,
no. 1, pp. 1   10, 1996.

[143] l. ule,    recent progress in computer methods of authorship determination,   
association for literary and linguistic computing bulletin, vol. 10, pp. 73   89,
1982.

[144] h. van halteren,    author veri   cation by linguistic pro   ling: an exploration of
the parameter space,    acm transactions on speech and language processing,
vol. 4, 2007.

[145] h. van halteren, r. h. baayen, f. tweedie, m. haverkort, and a. neijt,    new
machine learning methods demonstrate the existence of a human stylome,   
journal of quantitative linguistics, vol. 12, no. 1, pp. 65   77, 2005.

[146] v. n. vapnik, the nature of statistical learning theory. berlin: springer-

verlag, 1995.

[147] w. t. vetterling and b. p. flannery, numerical recipes in c++: the art of

scienti   c computing. cambridge: cambridge university press, 2002.

[148] b. vickers, counterfeiting shakespeare. cambridge: cambridge university

press, 2002.

[149] f. l. wellman, the art of cross-examination. new york: macmillan, fourth

ed., 1936.

[150] c. b. williams, style and vocabulary: numerical studies. london: gri   n,

1970.

[151] a. j. wyner,    id178 estimation and patterns,    in proceedings of the 1996

workshop on id205, 1996.

[152] b. yu, q. mei, and c. zhai,    english usage comparison between native
and non-native english speakers in academic writing,    in proceedings of
ach/allc 2005, victoria, bc, canada, 2005.

[153] g. u. yule,    on sentence-length as a statistical characteristic of style in prose,
with application to two cases of disputed authorship,    biometrika, vol. 30,
pp. 363   90, 1938.

334 references

[154] g. u. yule, the statistical study of literary vocabulary. cambridge: cam-

bridge university press, 1944.

[155] p. m. zatko,    alternative routes for data acquisition and system compromise,   
in 3rd annual ifip working group 11.9 international conference on digital
forensics, orlando, fl, 2007.

[156] h. zhang,    the optimality of naive bayes,    in proceedings of the seven-
teenth international florida arti   cial intelligence research society confer-
ence, (v. barr and z. markov, eds.), miami beach, fl: aaai press, 2004.

[157] r. zheng, j. li, h. chen, and z. huang,    a framework for authorship identi   -
cation of online messages: writing-style features and classi   cation techniques,   
journal of the american society for information science and technology,
vol. 57, no. 3, pp. 378   393, 2006.

[158] g. k. zipf, human behavior and the principle of least e   ort. new york:

hafner publishing company, 1949. reprinted 1965.

