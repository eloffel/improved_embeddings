compositional morphology for word representations and language modelling

4
1
0
2

 

y
a
m
6
1

 

 
 
]
l
c
.
s
c
[
 
 

1
v
3
7
2
4

.

5
0
4
1
:
v
i
x
r
a

jan a. botha
phil blunsom
department of computer science, university of oxford, oxford, ox1 3qd, uk

jan.botha@cs.ox.ac.uk
phil.blunsom@cs.ox.ac.uk

abstract

this paper presents a scalable method for inte-
grating compositional morphological representa-
tions into a vector-based probabilistic language
model. our approach is evaluated in the con-
text of log-bilinear language models, rendered
suitably ef   cient for implementation inside a ma-
chine translation decoder by factoring the vocab-
ulary. we perform both intrinsic and extrinsic
evaluations, presenting results on a range of lan-
guages which demonstrate that our model learns
morphological representations that both perform
well on word similarity tasks and lead to sub-
stantial reductions in perplexity. when used for
translation into morphologically rich languages
with large vocabularies, our models obtain im-
provements of up to 1.2 id7 points relative to
a baseline system using back-off id165 models.

1 introduction

the proliferation of word forms in morphologically rich
languages presents challenges to the statistical language
models (lms) that play a key role in machine translation
and id103. conventional back-off id165
lms (chen & goodman, 1998) and the increasingly popu-
lar vector-based lms (bengio et al., 2003; schwenk et al.,
2006; mikolov et al., 2010) use parametrisations that do
not explicitly encode morphological regularities among re-
lated forms, like abstract, abstraction and abstracted. such
models suffer from data sparsity arising from morphologi-
cal processes and lack a coherent method of assigning prob-
abilities or representations to unseen word forms.
this work focuses on continuous space language models
(cslms), an umbrella term for the lms that represent
words with real-valued vectors. such word representations
have been found to capture some morphological regular-
ity (mikolov et al., 2013b), but we contend that there is
a case for building a priori morphological awareness into
proceedings of the 31 st international conference on machine
learning, beijing, china, 2014. jmlr: w&cp volume 32.
copyright 2014 by the author(s).

the language models    inductive bias. conversely, composi-
tional vector-space modelling has recently been applied to
morphology to good effect (lazaridou et al., 2013; luong
et al., 2013), but lacked the probabilistic basis necessary for
use with a machine translation decoder.
the method we propose strikes a balance between proba-
bilistic language modelling and morphology-based repre-
sentation learning. word vectors are composed as a linear
function of arbitrary sub-elements of the word, e.g. surface
form, stem, af   xes, or other latent information. the effect
is to tie together the representations of morphologically re-
lated words, directly combating data sparsity. this is exe-
cuted in the context of a log-bilinear (lbl) lm (mnih &
hinton, 2007), which is sped up suf   ciently by the use of
word classing so that we can integrate the model into an
open source machine translation decoder1 and evaluate its
impact on translation into 6 languages, including the mor-
phologically complex czech, german and russian.
in word similarity rating tasks, our morpheme vectors help
improve correlation with human ratings in multiple lan-
guages. fine-grained analysis is used to determine the ori-
gin of our perplexity reductions, while scaling experiments
demonstrate tractability on vocabularies of 900k types us-
ing 100m+ tokens.

2 additive word representations

a generic cslm associates with each word type v in the
vocabulary v a d-dimensional feature vector rv     rd.
regularities among words are captured in an opaque way
through the interaction of these feature values and a set of
transformation weights. this leverages linguistic intuitions
only in an extremely rudimentary way, in contrast to hand-
engineered linguistic features that target very speci   c phe-
nomena, as often used in supervised-learning settings.
we seek a compromise that retains the unsupervised na-
ture of cslm feature vectors, but also incorporates a priori
linguistic knowledge in a    exible and ef   cient manner. in
particular, morphologically related words should share sta-
tistical strength in spite of differences in surface form.

1our source code for language model training and integration

into cdec is available from http://bothameister.github.io

compositional morphology for word representations and language modelling

to achieve this, we de   ne a mapping    : v (cid:55)    f + of a
surface word into a variable-length sequence of factors, i.e.
  (v) = (f1, . . . , fk), where v     v and fi     f. each
factor f has an associated factor feature vector rf     rd.
we thereby factorise a word into its surface morphemes,
although the approach could also incorporate other infor-
mation, e.g. lemma, part of speech.
the vector representation   rv of a word v is computed as
a function     (v) of its factor vectors. we use addition as
f     (v) rf . the
vectors of morphologically related words become linked
through shared factor vectors (notation:             word,             factor),

composition function:   rv =     (v) = (cid:80)

                              
imperfection =       im +                perfect +       ion
                     
perfectly =                perfect +       ly .

overhang).

hangover (cid:54)=                      

furthermore, representations for out-of-vocabulary (oov)
words can be constructed using their available morpheme
vectors.
we include the surface form of a word as a factor it-
self. this accounts for noncompositional constructions
(                        
greenhouse =                         
greenhouse +             green +             house), and makes
the approach more robust to noisy morphological segmen-
tation. this strategy also overcomes the order-invariance
of additive composition (                     
the number of factors per word is free to vary over the vo-
cabulary, making the approach applicable across the spec-
trum of more fusional languages (e.g. czech, russian) to
more agglutinative languages (e.g. turkish). this is in con-
trast to factored language models (alexandrescu & kirch-
hoff, 2006), which assume a    xed number of factors per
word. their method of concatenating factor vectors to ob-
tain a single representation vector for a word can be seen
as enforcing a partition on the feature space. our method
of addition avoids such a partitioning and better re   ects the
absence of a strong intuition about what an appropriate par-
titioning might be. a limitation of our method compared to
theirs is that the deterministic mapping    currently enforces
a single factorisation per word type, which sacri   ces infor-
mation obtainable from context-disambiguated morpholog-
ical analyses.
our additive composition function can be regarded as an in-
stantiation of the weighted addition strategy that performed
well in a distributional compositional approach to deriva-
tional morphology (lazaridou et al., 2013). unlike the
recursive neural-network method of luong et al. (2013),
we do not impose a single tree structure over a word,
which would ignore the ambiguity inherent in words like
un[[lock]able] vs. [un[lock]]able. in contrast to these two
previous approaches to morphological modelling, our ad-
ditive representations are readily implementable in a prob-
abilistic language model suitable for use in a decoder.

3 log-bilinear language models

(cid:1).

i p(cid:0)wi|wi   1
(cid:81)

log-bilinear (lbl) models (mnih & hinton, 2007) are an
instance of cslms that make the same markov assump-
tion as id165 language models. the id203 of a sen-
tence w is decomposed over its words, each conditioned
on the n   1 preceding words: p (w)    
these distributions are modelled by a smooth scoring func-
tion   (  ) over vector representations of words. in contrast,
discrete id165 models are estimated by smoothing and
backing off over empirical distributions (kneser & ney,
1995; chen & goodman, 1998).
the lbl predicts the vector p for the next word as a func-
tion of the context vectors qj     rd of the preceding words,

i   n+1

p =

qjcj,

(1)

n   1(cid:88)

j=1

where cj     rd  d are position-speci   c transformations.
  (w) expresses how well the observed word w    ts that pre-
diction and is de   ned as   (w) = p    rw + bw, where bw is
a bias term encoding the prior id203 of a word type.
softmax then yields the word id203 as

p(cid:0)wi|wi   1

i   n+1

(cid:1) =

(cid:80)

exp (  (wi))
v   v exp (  (v))

.

(2)

this model is subsequently denoted as lbl with parame-
ters   lbl = (cj, q, r, b), where q, r     r|v|  d contain
the word representation vectors as rows, and b     r|v|. q
and r imply that separate representations are used for con-
ditioning and output.

3.1 additive log-bilinear model

we introduce a variant of the lbl that makes use
of additive representations (  2) by associating the com-
posed word vectors   r and   qj with the target and con-
text words,
the representation matrices
  lbl++ =(cid:0)cj, q(f ), r(f ), b(cid:1). words sharing factors are
q(f ), r(f )     r|f|  d thus contain a vector for each factor
type. this model is designated lbl++ and has parameters

respectively.

matrix m     zv  |f|

tied together, which is expected to improve performance
on rare word forms.
representing the mapping    with a sparse transformation
, where a row vector mv has some
non-zero elements to select factor vectors, establishes the
relation between word and factor representation matrices
as r = m r(f ) and q = m q(f ).
in practice, we ex-
ploit this for test-time ef   ciency   word vectors are com-
piled of   ine so that the computational cost of lbl++ prob-
ability lookups is the same as for the lbl.
we consider two obvious variations of the lbl++ to evalu-
ate the extent to which interactions between context and

+

compositional morphology for word representations and language modelling

score    (c) = p    sc + tc and word score   (w), which are
normalised separately:

(cid:80)|c|
(cid:80)

exp (   (c))
c(cid:48)=1 exp (   (c(cid:48)))
exp (  (w))
v(cid:48)   cc exp (  (v(cid:48)))

p (c|h) =

p (w|h, c) =

(4)

(5)

.

figure 1. model diagram. illustration of how a 3-gram clbl++
model treats the czech phrase pro novou   skolu (   for the new
school   ), assuming the target word   skolu is clustered into word
class 17 by the method described in   3.2.
target factors affect the model: lbl+o only factorises
output words and retains simple word vectors for the con-
text (i.e. q     q(f )), while lbl+c does the reverse, only
factorising context words.2 both reduce to the lbl when
setting    to be the identity function, such that v     f.
the factorisation permits an approach to unknown context
words that is less harsh than the standard method of replac-
ing them with a global unknown symbol   instead, a vector
can be constructed from the known factors of the word (e.g.
the observed stem of an unobserved in   ected form). a sim-
ilar scheme can be used for scoring unknown target words,
but requires changing the event space of the probabilistic
model. we use this vocabulary stretching capability in our
word similarity experiments, but leave the extensions for
test-time language model predictions as future work.

3.2 class-based model decomposition

the key obstacle to using cslms in a decoder is the ex-
pensive normalisation over the vocabulary. our approach
to reducing the computational cost of normalisation is to
use a class-based decomposition of the probabilistic model
(goodman, 2001; mikolov et al., 2011). using brown-
id91 (brown et al., 1992),3 we partition the vocabu-
lary into |c| classes, denoting as cc the set of vocabulary
items in class c, such that v = c1                c|c|.
in this model, the id203 of a word conditioned on the
history h of n     1 preceding words is decomposed as

p (w|h) = p (c|h)p (w|h, c).

(3)

this class-based model, clbl, extends over the lbl by
associating a representation vector sc and bias parameter
tc to each class c, such that   clbl = (cj, q, r, s, b, t).
the same prediction vector p is used to compute both class

2the +c, +o and ++ naming suf   xes denote these same dis-

tinctions when used with the clbl model introduced later.

3in preliminary experiments, brown clusters gave better per-

plexities than frequency-binning (mikolov et al., 2011).

we favour this    at vocabulary partitioning for its computa-
tional adequacy, simplicity and robustness. computational
adequacy is obtained by using |c|     |v|0.5, thereby reduc-
ing the o(|v|) normalisation operation of the lbl to two
o(|v|0.5) operations in the clbl.
other methods for achieving more drastic complexity re-
ductions exist in the form of frequency-based truncation,
shortlists (schwenk, 2004), or casting the vocabulary as a
full hierarchy (mnih & hinton, 2008) or partial hierarchy
(le et al., 2011). we expect these approaches could have
adverse effects in the rich morphology setting, where much
of the vocabulary is in the long tail of the word distribution.

3.3 training & initialisation

model parameters    are estimated by optimising an l2-
regularised log likelihood objective. training the clbl and
its additive variants directly against this objective is fast
because normalisation of model scores, which is required
in computing gradients, is over a small number of events.
for the classless lbls we use noise-contrastive estimation
(nce) (gutmann & hyv  arinen, 2012; mnih & teh, 2012)
to avoid normalisation during training. this leaves the ex-
pensive test-time normalisation of lbls unchanged, pre-
cluding their usage during decoding.
bias terms b (resp. t) are initialised to the log unigram
probabilities of words (resp. classes) in the training cor-
pus, with laplace smoothing, while all other parameters are
initialised randomly according to sharp, zero-mean gaus-
sians. representations are thus learnt from scratch and not
based on publicly available embeddings, meaning our ap-
proach can easily be applied to many languages.
optimisation is performed by stochastic id119
with updates after each mini-batch of l training examples.
we apply adagrad (duchi et al., 2011) and tune the step-
size    on development data.4 we halt training once the per-
plexity on the development data starts to increase.

4 experiments

the overarching aim of our evaluation is to investigate the
effect of using the proposed additive representations across
languages with a range of morphological complexity.

4l=10k   40k,   =0.05   0.08, dependent on |v| and data size.

compositional morphology for word representations and language modelling

table 1. corpus statistics. the number of sentence pairs for a
row x refers to the english   x parallel data (but row en has
czech as source language).

data-main

sent. pairs

data-1m
toks.
|v|
1m 46k
1m 36k
1m 17k
1m 27k
1m 25k
1m 62k

cs
de
en
es
fr
ru

toks.
|v|
16.8m 206k
50.9m 339k
19.5m
60k
56.2m 152k
57.4m 137k
25.1m 497k

0.7m
1.9m
0.7m
2.0m
2.0m
1.5m

our intrinsic language model evaluation has two parts. we
   rst perform a model selection experiment on small data
to consider the relative merits of using additive representa-
tions for context words, target words, or both, and to vali-
date the use of the class-based decomposition.
then we consider class-based additive models trained on
tens of millions of tokens and large vocabularies. these
larger language models are applied in two extrinsic tasks:
i) a word-similarity rating experiment on multiple lan-
guages, aiming to gauge the quality of the induced word
and morpheme representation vectors; ii) a machine trans-
lation experiment, where we are speci   cally interested in
testing the impact of an lbl lm feature when translating
into morphologically rich languages.

4.1 data & methods

we make use of data from the 2013 acl workshop on ma-
chine translation.5 we    rst describe data used for transla-
tion experiments, since the monolingual datasets used for
language model training were derived from that. the lan-
guage pairs are english   {german, french, spanish, rus-
sian} and english   czech. our parallel data comprised
the europarl-v7 and news-commentary corpora, except for
english   russian where we used news-commentary and the
yandex parallel corpus.6 pre-processing involved lower-
casing, tokenising and    ltering to exclude sentences of
more than 80 tokens or substantially different lengths.
4-gram language models were trained on the target data
in two batches: data-1m consists of the    rst million to-
kens only, while data-main is the full target-side data.
statistics are given in table 1. newstest2011 was used
as development data7 for tuning language model hyper-
parameters, while intrinsic lm evaluation was done on
newstest2012. as metric, we use model perplexity (ppl)
i=1 ln p (wi)), where n is the number of test
exp(    1
tokens. in addition to contrasting the lbl variants, we also
use modi   ed kneser-ney id165 models (mkns) (chen &
goodman, 1998) as baselines.

(cid:80)n

n

figure 2. model selection results. box-plots show the spread,
across 6 languages, of relative perplexity reductions obtained by
each type of additive model against its non-additive baseline, for
which median absolute perplexity is given in parentheses; for
mkn, that is 348. each box-plot summarises the behaviour of
a model across languages. circles give sample means, while
crosses show outliers beyond 3   the inter-quartile range.
language model vocabularies. additive representa-
tions that link morphologically related words speci   cally
aim to improve modelling of the long tail of the lexicon, so
we do not want to prune away all rare words, as is common
practice in language modelling and id27 learn-
ing. we de   ne a singleton pruning rate   , and randomly
replace that fraction of words occurring only once in the
training data with a global unk symbol.    = 1 would im-
ply a unigram count cut-off threshold of 1. instead, we use
low pruning rates8 and thus model large vocabularies.9

word factorisation   . we obtain labelled morphologi-
cal segmentations from the unsupervised segmentor mor-
fessor cat-map (creutz & lagus, 2007). the mapping   
of a word is taken as its surface form and the morphemes
identi   ed by morfessor. keeping the morpheme labels al-
lows the model to learn separate vectors for, say, instem the
preposition and inpre   x occurring as in  appropriate. by not
post-processing segmentations in a more sophisticated way,
we keep the overall method more language independent.

4.2

intrinsic language model evaluation

results on data-1m. the use of morphology-based, ad-
ditive representations for both context and output words
(models++) yielded perplexity reductions on all 6 lan-
guages when using 1m training tokens. furthermore, these
double-additive models consistently outperform the ones
that factorise only context (+c) or only output (+o) words,
indicating that context and output contribute complemen-
tary information and supporting our hypothesis that is it
bene   cial to model morphological dependencies across
words. the results are summarised in figure 2.
for lack of space we do not present numbers for individual
languages, but report that the impact of clbl++ varies by

5http://www.statmt.org/wmt13/translation-task.html
6https://translate.yandex.ru/corpus?lang=en
7for russian, some training data was held out for tuning.

8 data-1m:    = 0.2; data-main:    = 0.05
9 we also mapped digits to 0, and cleaned the russian data by

replacing tokens having <80% cyrillic characters with unk.

+c+o++   14   12   10   8   6   4   202perplexityreduction(%)lbl(308)+c+o++clbl(309)compositional morphology for word representations and language modelling

table 2. test-set perplexities on data-main using two vocab-
ulary pruning settings. percentage reductions are relative to the
preceding model, e.g. the    rst czech clbl improves over mkn
by 20.8% (rel.1); the clbl++ improves over that clbl by a
further 5.9% (rel.2).

mkn
ppl
862
463
291
219
243
390
634
379
254
195
218
347

clbl

clbl++

ppl
683
422
281
207
232
313
477
331
234
180
201
271

rel.1
ppl
-20.8% 643
-8.9% 404
-3.4% 273
-5.7% 203
-4.9% 227
-19.7% 300
-24.8% 462
-12.6% 329
-7.6% 233
-7.7% 180
-7.7% 198
-21.8% 262

rel.2
-5.9%
-4.2%
-2.8%
-1.9%
-1.9%
-4.2%
-3.1%
-0.9%
-0.7%
0.02%
-1.3%
-3.4%

  =0.05 cs
de
en
es
fr
ru
cs
de
en
es
fr
ru

  =1.0

language, correlating with vocabulary size: russian bene-
   ted most, followed by czech and german. even on en-
glish, often regarded as having simple morphology, the rel-
ative improvement is 4%.
the relative merits of the +c and +o schemes depend on
which model is used as starting point. with lbl, the
output-additive scheme (lbl+o) gives larger improvements
than the context-additive scheme (lbl+c). the reverse is
true for clbl, indicating the class decomposition damp-
ens the effectiveness of using morphological information
in output words.
the use of classes increases perplexity slightly compared
to the lbls, but this is in exchange for much faster compu-
tation of language model probabilities, allowing the clbls
to be used in a machine translation decoder (  4.4).

results on data-main. based on the outcomes of the
small-scale evaluation, we focus our main language model
evaluation on the additive class-based model clbl++ in
comparison to clbl and mkn baselines, using the larger
training dataset, with vocabularies of up to 500k types.
the overall trend that morphology-based additive represen-
tations yield lower perplexity carries over to this larger data
setting, again with the biggest impact being on czech and
russian (table 2, top). improvements are in the 2%   6%
range, slightly lower than the corresponding differences on
the small data.
our hypothesis is that the much of the improvement is due
to the additive representations being especially bene   cial
for modelling rare words. we test this by repeating the
experiment under the condition where all word types oc-
curring only once are excluded from the vocabulary (  =1).
if the additive representations were not bene   cial to rare

figure 3. perplexity reductions by token frequency, clbl++
relative to clbl. dotted bars extending further down are better.
a bin labelled with a number x contains those test tokens that
occur y     [10x, 10x+1) times in the training data. striped bars
show percentage of test-set covered by each bin.

words, the outcome should remain the same. instead, we
   nd the relative improvements become a lot smaller (ta-
ble 2, bottom) than when only excluding some singletons
(  =0.05), which supports that hypothesis.

analysis. model perplexity on a whole dataset is a con-
venient summary of its intrinsic performance, but such a
global view does not give much insight into how one model
outperforms another. we now partition the test data into
subsets of interest and measure ppl over these subsets.
we    rst partition on token frequency, as computed on the
training data. figure 3 provides further evidence that the
additive models have most impact on rare words generally,
and not only on singletons. czech, german and russian
see relative ppl reductions of 8%   21% for words occur-
ring fewer than 100 times in the training data. reductions
become negligible for the high-frequency tokens. these
tend to be punctuation and closed-class words, where any
putative relevance of morphology is overwhelmed by the
fact that the predictive uncertainty is very low to begin with
(absolute ppl<10 for the highest frequency subset). for
the morphologically simpler spanish case, ppl reductions
are generally smaller across frequency scales.
we also break down ppl reductions by part of speech tags,
focusing on german. we used the decision tree-based tag-
ger of schmid & laws (2008). aside from unseen tokens,
the biggest improvements are on nouns and adjectives (fig-
ure 4), suggesting our segmentation-based representations
help abstract over german   s productive compounding.

   20   1001020percentagecsruunk0123456de   20   1001020percentageunk0123456es34%testsetcoverageperplexityreductioncompositional morphology for word representations and language modelling

figure 4. perplexity reductions by part of speech, clbl++ rel-
ative to clbl on german. dotted bars extending further down
are better. tokens tagged as foreign words or other opaque sym-
bols resort under    rest   . striped bars as in figure 3

german noun phrases require agreement in gender, case
and number, which are marked overtly with fusional mor-
phemes, and we see large gains on such test id165s: 15%
improvement on adjective-noun sequences, and 21% when
considering the more speci   c case of adjective-adjective-
noun sequences. an example of the latter kind is der
ehemalig  e sozial  ist  isch  e bildung  s  minister (   the former
socialist minister of education   ), where the morphological
agreement surfaces in the repeated e-suf   x.
we conducted a    nal scaling experiment on czech by
training models on increasing amounts of data from the
monolingual news corpora. improvements over the mkn
baseline decrease, but remain substantial at 14% for the
largest setting when allowing the vocabulary to grow with
the data. maintaining a constant advantage over mkn re-
quires also increasing the dimensionality d of representa-
tions (mikolov et al., 2013a), but this was outside the scope
of our experiment. although gains from the additive rep-
resentations over the clbl diminish down to 2%   3% at
the scale of 128m training tokens (figure 5), these results
demonstrate the tractability of our approach on very large
vocabularies of nearly 1m types.

4.3 task 1: word similarity rating

in the previous section, we established the positive role that
morphological awareness played in building continuous-
space language models that better predict unseen text. here
we focus on the quality of the word representations learnt
in the process. we evaluate on a standard word similarity
rating task, where one measures the correlation between
cosine-similarity scores for pairs of word vectors and a
set of human similarity ratings. an important aspect of
our evaluation is to measure performance on multiple lan-
guages using a single unsupervised, model-based approach.
morpheme vectors from the clbl++ enable handling oov
test words in a more nuanced way than using the global
unknown word vector.
in general, we compose a vector
  uv = [  qv;   rv] for a word v according to a post hoc word

figure 5. scaling experiment. relative perplexity reductions ob-
tained when varying the czech training data size (16m   128m).
in the    rst setting, the vocabulary was held    xed as data size
increased(|v|); in the second it varied freely across sizes (|vvar|).
map   (cid:48) by summing and concatenating the factor vectors rf
and qf , where f       (cid:48)(v)     f. this ignores unknown mor-
phemes occurring in oov words, and uses [qunk; runk] for
  uunk only if all morphemes are unknown.
to see whether the morphological representations improve
the quality of vectors for known words, we also report the
correlations obtained when using the clbl++ word vectors
directly, resorting to   uunk for all oov words v /    v (de-
noted       compose    in the results). this is also the strategy
that the baseline clbl model is forced to follow for oovs.
we evaluate    rst using the english rare-word dataset
(rw) created by luong et al. (2013).
its 2034 word
pairs contain more morphological complexity than other
well-established word similarity datasets, e.g. crudeness   
impoliteness. we compare against their context-sensitive
morphological id56 (csmid56), using
spearman   s rank correlation coef   cient,   . table 3 shows
our model obtaining a   -value slightly below the best csm-
id56 result, but outperforming the csmid56 that used an
alternative set of embeddings for initialisation.
this is a strong result given that our vectors come from a
simple linear probabilistic model that is also suitable for
integration directly into a decoder for translation (  4.4) or
id103, which is not the case for csmid56s.
moreover, the csmid56s were initialised with high-quality,
publicly available id27s trained over weeks on
much larger corpora of 630   990m words (collobert & we-
ston, 2008; huang et al., 2012), in contrast to ours that are
trained from scratch on much less data. this renders our
method directly applicable to languages which may not yet
have those resources.
relative to the clbl baseline, our method performs well on

unkadjadvnproprpvrest   20   100102030percentagedetestsetcoverageperplexityreduction24252627trainingtokens(m)0369121518212427perplexityreduction(%)2003004005006007008009001000vocabularysize|vvar|(k)|v|=206kclbl++vs.mknclbl++vs.clbl|vvar|clbl++vs.mknclbl++vs.clblcompositional morphology for word representations and language modelling

table 3. word-pair similarity task. spearman   s     100 for the
correlation between model scores and human ratings on the en-
glish rw dataset. the csmid56s bene   t from initialisation with
high quality pre-existing id27s, while our models
used random initialisation.

(luong et al., 2013)

our models

2 clbl
hsmn
hsmn+csmid56 22 clbl++
27    compose
c&w
34
c&w+csmid56

18
30
20

figure 6. english morpheme vectors learnt by clbl++.
id84 was performed with id167 (van der
maaten & hinton, 2008), with shading added for emphasis.
datasets across four languages. for the english rw, which
was designed with morphology in mind, the gain is 64%.
but also on the standard english ws353 dataset (finkel-
stein et al., 2002), we get a 26% better correlation with the
human ratings. on german, the clbl++ obtains correla-
tions up to three times stronger than the baseline, and 39%
better for french (table 4).
a visualisation of the english morpheme vectors (figure 6)
suggests the model captured non-trivial morphological reg-
ularities: noun suf   xes relating to persons (writer, hu-
manists) lie close together, while being separated according
to number; negation pre   xes share a region (un-, in-, mis-,
dis-); and relational pre   xes are grouped (surpa-, super-,
multi-, intra-), with a potential explanation for their separa-
tion from inter- being that the latter is more strongly bound
up in lexicalisations (international, intersection).

4.4 task 2: machine translation

the    nal aspect of our evaluation focuses on the integra-
tion of class-decomposed log-bilinear models into a ma-
chine translation system. to the best of our knowledge, this

10 es ws353 (hassan & mihalcea, 2009); gur350 (gurevych,
2005); rg65 (rubenstein & goodenough, 1965) with fr
(joubarne & inkpen, 2011); zg222 (zesch & gurevych, 2006).

table 4. word-pair similarity task (multi-language), showing
spearman   s     100 and the number of word pairs in each dataset.
as benchmarks, we include the best results from luong et al.
(2013), who relied on more training data and pre-existing embed-
dings not available in all languages. in the penultimate row our
model   s ability to compose vectors for oov words is suppressed.

datasets10

ws

model / language en
hsmn
63
65
+csmid56
clbl
32
clbl++
39
40
   compose
# pairs

353

es
   
   
26
28
27

gur
de
   
   
36
56
44
350

rg

en
63
65
47
41
41

fr
   
   
33
45
41

65

zg
de
   
   
6
25
23
222

is the    rst study to investigate large vocabulary normalised
cslms inside a decoder when translating into a range of
morphologically rich languages. we consider 5 language
pairs, translating from english into czech, german, rus-
sian, spanish and french.
aside from the choice of language pairs, this evaluation
diverges from vaswani et al. (2013) by using normalised
probabilities, a process made tractable by the class-based
decomposition and caching of context-speci   c normaliser
terms. vaswani et al. (2013) relied on unnormalised model
scores for ef   ciency, but do not report on the performance
impact of this assumption.
in our preliminary experi-
ments, there was high variance in the performance of un-
normalised models. they are dif   cult to reason about as a
feature function that must help the translation model dis-
criminate between alternative hypotheses.
we use cdec (dyer et al., 2010; 2013) to build symmetric
word-alignments and extract rules for hierarchical phrase-
based translation (chiang, 2007). our baseline system uses
a standard set of features in a log-linear translation model.
this includes a baseline 4-gram mkn language model,
trained with srilm (stolcke, 2002) and queried ef   ciently
using kenlm (hea   eld, 2011). the cslms are integrated
directly into the decoder as an additional feature function,
thus exercising a stronger in   uence on the search than in
n-best list rescoring.11 translation model feature weights
are tuned with mert (och, 2003) on newstest2012.
table 5 summarises our translation results.
inclusion of
the clbl++ language model feature outperforms the mkn-
only baseline systems by 1.2 id7 points for translation
into russian, and by 1 point into czech and spanish. the
en   de system bene   ts least from the additional cslm
feature, despite the perplexity reductions achieved in the
intrinsic evaluation. in light of german   s productive com-
pounding, it is conceivable that the bilingual coverage of

11our source code for using clbl/clbl++ with cdec is re-

leased at http://bothameister.github.io.

compositional morphology for word representations and language modelling

table 5. translation results. case-insensitive id7 scores on
newstest2013, with standard deviation over 3 runs given in
parentheses. the two right-most columns use the listed cslm
as a feature in addition to the mkn feature, i.e. these mt systems
have at most 2 lms. language models are from table 2 (top).

mation such as morphology (luong et al., 2013; lazari-
dou et al., 2013) or syntax (hermann & blunsom, 2013)
has recently proved bene   cial to learning vector represen-
tations of words. our contribution is to create morphologi-
cal awareness in a probabilistic language model.

mkn

12.6 (0.2)
15.7 (0.1)
24.7 (0.4)
24.1 (0.2)
15.9 (0.2)
19.8 (0.4)

clbl

13.2 (0.1)
15.9 (0.2)
25.5 (0.5)
24.6 (0.2)
16.9 (0.3)
20.4 (0.4)

clbl++
13.6 (0.0)
15.8 (0.4)
25.7 (0.3)
24.8 (0.5)
17.1 (0.1)
20.4 (0.5)

en   cs
de
es
fr
ru
cs   en

that system is more of a limitation than the performance of
the language models.
on the other languages, the clbl adds 0.5 to 1 id7
points over the baseline, whereas additional improvement
from the additive representations lies within mert vari-
ance except for en   cs.
the impact of our morphology-aware language model is
limited by the translation system   s inability to generate un-
seen in   ections. a future task is thus to combine it with a
system that can do so (chahuneau et al., 2013).

5 related work

factored language models (flms) have been used to inte-
grate morphological information into both discrete id165
lms (bilmes & kirchhoff, 2003) and cslms (alexan-
drescu & kirchhoff, 2006) by viewing a word as a set of
factors. alexandrescu & kirchhoff (2006) demonstrated
how factorising the representations of context-words can
help deal with out-of-vocabulary words, but they did not
evaluate the effect of factorising output words and did not
conduct an extrinsic evaluation.
a variety of strategies have been explored for bring-
ing cslms to bear on machine translation. rescoring
lattices with a cslm proved to be bene   cial for asr
(schwenk, 2004) and was subsequently applied to trans-
lation (schwenk et al., 2006; schwenk & koehn, 2008),
reaching training sizes of up to 500m words (schwenk
et al., 2012). for ef   ciency, this line of work relied heav-
ily on small    shortlists    of common words, by-passing the
cslm and using a back-off id165 model for the remain-
der of the vocabulary. using unnormalised cslms during
   rst-pass decoding has generated improvements in id7
score for translation into english (vaswani et al., 2013).
recent work has moved beyond monolingual vector-space
modelling, incorporating phrase similarity ratings based
on bilingual id27s as a translation model fea-
ture (zou et al., 2013), or formulating translation purely in
terms of continuous-space models (kalchbrenner & blun-
som, 2013). accounting for linguistically derived infor-

6 conclusion

we introduced a method for integrating morphology into
probabilistic continuous-space language models. our
method has the    exibility to be used for morphologically
rich languages (mrls) across a range of linguistic ty-
pologies. our empirical evaluation focused on multiple
mrls and different tasks. the primary outcomes are
that (i) our morphology-guided cslms improve intrin-
sic language model performance when compared to base-
line cslms and id165 mkn models; (ii) word and
morpheme representations learnt in the process compare
favourably in terms of a word similarity task to a recent
more complex model that used more data, while obtain-
ing large gains on some languages; (iii) machine transla-
tion quality as measured by id7 was improved consis-
tently across six language pairs when using cslms during
decoding, although the morphology-based representations
led to further improvements beyond the level of optimiser
variance only for english   czech. by demonstrating that
the class decomposition enables full integration of a nor-
malised cslm into a decoder, we open up many other pos-
sibilities in this active modelling space.

references

alexandrescu, a. & kirchhoff, k. factored neural language

models. in proc. hlt-naacl: short papers. acl, 2006.

bengio, y., ducharme, r., vincent, p., & jauvin, c. a neural

probabilistic language model. jmlr, 3:1137   1155, 2003.

bilmes, j. a. & kirchhoff, k. factored language models and
generalized parallel backoff. in proc. naacl-hlt: short pa-
pers. acl, 2003.

brown, p. f., desouza, p. v., mercer, r. l., della pietra, v. j., &
lai, j. c. class-based id165 models of natural language.
comp. ling., 18(4):467   479, 1992.

chahuneau, v., schlinger, e., smith, n. a., & dyer, c. trans-
lating into morphologically rich languages with synthetic
phrases. in proc. emnlp, pp. 1677   1687. acl, 2013.

chen, s. f. & goodman, j. an empirical study of smoothing
techniques for id38. technical report, harvard
university, cambridge, ma, 1998.

chiang, d. hierarchical phrase-based translation. comp. ling.,

33(2):201   228, 2007.

collobert, r. & weston, j. a uni   ed architecture for natural
language processing: deep neural networks with multitask
learning. in proc. icml. acm, 2008.

compositional morphology for word representations and language modelling

creutz, m. & lagus, k. unsupervised models for morpheme
segmentation and morphology learning. acm trans. on
speech and language processing, 4(1):1   34, 2007.

luong, m.-t., socher, r., & manning, c. d. better word rep-
resentations with id56s for morphology.
in proc. of conll, 2013.

duchi, j., hazan, e., & singer, y. adaptive subgradient methods
for online learning and stochastic optimization. jmlr, 12:
2121   2159, 2011.

dyer, c., lopez, a., ganitkevitch, j., weese, j., ture, f., blun-
som, p., setiawan, h., eidelman, v., & resnik, p. cdec: a
decoder, alignment, and learning framework for finite-state
and context-free translation models. in proc. acl: demon-
stration session, pp. 7   12, 2010. acl.

dyer, c., chahuneau, v., & smith, n. a. a simple, fast, and ef-
fective reparameterization of ibm model 2. in proc. naacl,
pp. 644   648. acl, 2013.

finkelstein, l., gabrilovich, e., matias, y., rivlin, e., solan, z.,
wolfman, g., & ruppin, e. placing search in context: the
concept revisited. acm trans. on information systems, 20
(1):116   131, 2002.

mikolov, t., kara     at, m., burget, l.,   cernock  y, j., & khudanpur,
s. recurrent neural network based language model. in proc.
interspeech, pp. 1045   1048, 2010.

mikolov, t., kombrink, s., burget, l.,   cernock  y, j., & khudan-
pur, s. extensions of recurrent neural network language
model. in proc. icassp, 2011.

mikolov, t., chen, k., corrado, g., & dean, j. ef   cient estima-
tion of word representations in vector space. in proc. iclr.
arxiv:1301.3781, 2013a.

mikolov, t., yih, w.-t., & zweig, g. linguistic regularities
in proc. hlt-

in continuous space word representations.
naacl. acl, 2013b.

mnih, a. & hinton, g. three new id114 for statis-
tical language modelling. in proc. icml, pp. 641   648, 2007.
acm.

goodman, j. classes for fast maximum id178 training.

proc. icassp, pp. 561   564. ieee, 2001.

in

mnih, a. & hinton, g. a scalable hierarchical distributed lan-

guage model. in nips, pp. 1081   1088, 2008.

gurevych, i. using the structure of a conceptual network in
computing semantic relatedness. in proc. ijcnlp, pp. 767   
778, 2005.

gutmann, m. u. & hyv  arinen, a. noise-contrastive estimation
of unnormalized statistical models , with applications to nat-
ural image statistics. jmlr, 13:307   361, 2012.

hassan, s. & mihalcea, r. cross-lingual semantic relatedness
using encyclopedic knowledge. in proc. emnlp, pp. 1192   
1201. acl, 2009.

hea   eld, k. kenlm: faster and smaller language model
in proc. workshop on statistical machine transla-

queries.
tion, pp. 187   197. acl, 2011.

hermann, k. m. & blunsom, p. the role of syntax in vector
space models of id152. in proc. acl, pp.
894   904, 2013.

huang, e. h., socher, r., manning, c. d., & ng, a. y. improving
word representations via global context and multiple word
prototypes. in proc. acl, pp. 873   882. acl, 2012.

joubarne, c. & inkpen, d. comparison of semantic similarity
for different languages using the google id165 corpus and
in proc. canadian
second- order co-occurrence measures.
conference on advances in ai, pp. 216   221. springer-verlag,
2011.

kalchbrenner, n. & blunsom, p. recurrent continuous transla-

tion models. in proc. emnlp, pp. 1700   1709. acl, 2013.

kneser, r. & ney, h.

improved backing-off for m-gram lan-

guage modelling. in proc. icassp, pp. 181   184, 1995.

lazaridou, a., marelli, m., zamparelli, r., & baroni, m.
compositional-ly derived representations of morphologically
complex words in id65. in proc. acl, pp.
1517   1526, 2013. acl.

le, h.-s., oparin, i., allauzen, a., gauvain, j.-l., & yvon, f.
structured output layer neural network language model. in
proc. icassp, pp. 5524   5527, 2011. ieee.

mnih, a. & teh, y. w. a fast and simple algorithm for training

neural probabilistic language models. in proc. icml, 2012.

och, f. j. minimum error rate training in statistical machine

translation. in proc. acl, pp. 160   167, 2003.

rubenstein, h. & goodenough, j. b. contextual correlates of

synonymy. commun. acm, 8(10):627   633, october 1965.

schmid, h. & laws, f. estimation of conditional probabilities
with id90 and an application to fine-grained pos
tagging. in proc. coling, pp. 777   784, 2008. acl.

schwenk, h. ef   cient training of large neural networks for
id38. in proc. ieee joint conference on neu-
ral networks, pp. 3059   3064. ieee, 2004.

schwenk, h. & koehn, p. large and diverse language models

for id151. in proc. ijcnlp, 2008.

schwenk, h., dchelotte, d., & gauvain, j.-l. continuous space
language models for id151. in proc.
coling/acl, pp. 723   730, 2006. acl.

schwenk, h., rousseau, a., & attik, m. large, pruned or con-
tinuous space language models on a gpu for statistical ma-
chine translation. in in proc. naacl-hlt workshop: on the
future of id38 for hlt, pp. 11   19. acl, 2012.
stolcke, a. srilm     an extensible id38 toolkit.

in proc. icslp, pp. 901   904, 2002.

van der maaten, l. & hinton, g. visualizing data using id167.

jmlr, 9:2579   2605, 2008.

vaswani, a., zhao, y., fossum, v., & chiang, d. decoding with
large-scale neural language models improves translation.
in proc. emnlp, 2013. acl.

zesch, t. & gurevych, i. automatically creating datasets for mea-
sures of semantic relatedness. in proc. workshop on linguistic
distances, pp. 16   24. acl, 2006.

zou, w. y., socher, r., cer, d., & manning, c. d. bilingual word
embeddings for phrase-based machine translation. in proc.
emnlp, pp. 1393   1398, 2013. acl.

