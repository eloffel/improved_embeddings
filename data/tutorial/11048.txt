journal of arti   cial intelligence research 42 (2011) 851-886

submitted 07/11; published 12/11

dr.fill: crosswords and an implemented

solver for singly weighted csps

matthew l. ginsberg
on time systems, inc.
355 goodpasture island road, suite 200
eugene, oregon 97401

abstract

we describe dr.fill, a program that solves american-style crossword puzzles.
from a technical perspective, dr.fill works by converting crosswords to weighted
csps, and then using a variety of novel techniques to    nd a solution. these techniques
include generally applicable heuristics for variable and value selection, a variant of
limited discrepancy search, and postprocessing and partitioning ideas. branch and
bound is not used, as it was incompatible with postprocessing and was determined
experimentally to be of little practical value. dr.fill   s performance on crosswords
from the american crossword puzzle tournament suggests that it ranks among the top
   fty or so crossword solvers in the world.

1. introduction

in recent years, there has been interest in solving constraint-satisfaction problems, or csps,
where some of the constraints are    soft    in that while their satisfaction is desirable, it is
not strictly required in a solution. as an example, if a construction problem is modeled as
a csp, it may be possible to overutilize a particular labor resource by paying the associated
workers overtime. while not the cheapest way to construct the artifact in question, the
corresponding solution is certainly viable in practice.

soft constraints can be modeled by assigning a cost to violating any such constraint,
and then looking for that solution to the original csp for which the accumulated cost is
minimized.

by and large, work on these systems has been primarily theoretical as various tech-
niques for solving these    weighted    csps (wcsps) are considered and evaluated without the
experimental support of an underlying implementation on a real-world problem. theoret-
ical complexity results have been obtained, and the general consensus appears to be that
some sort of branch-and-bound method should be used by the solver, where the cost of one
potential solution is used to bound and thereby restrict the subsequent search for possible
improvements.

our goal in this paper is to evaluate possible wcsp algorithms in a more    practical   
setting, to wit, the development of a program (dr.fill) designed to solve american-style
crossword puzzles. based on the search engine underlying dr.fill, our basic conclusions
are as follows:

c(cid:13)2011 ai access foundation. all rights reserved.

ginsberg

1. we present speci   c variable- and value-selection heuristics that improve the e   ective-

ness of the search enormously.

2. the most e   ective search technique appears to be a modi   cation of limited discrepancy

search (lds) (harvey & ginsberg, 1995).

3. branch-and-bound appears not to be a terribly e   ective solution technique for at least

some problems of this sort.

4. postprocessing complete candidate solutions improves the e   ectiveness of the search.

a more complete description of the crossword domain can be found in section 2.2;
example crosswords appear in figures 1 and 2. the overall view we will take is that, given
both a speci   c crossword clue c and possible solution word or       ll    f , there is an associated
score p(f|c) that gives the id203 that the    ll is correct, given the clue. assuming that
these probabilities are independent for di   erent clues, the id203 that a collection of

   lls solves a puzzle correctly is then simply(cid:89)

p(fi|ci)

(1)

i

where fi is the    ll entered in response to clue ci. dr.fill   s goal is to    nd a set of    lls
that is legal (in that intersecting words share a letter at the square of intersection) while
maximizing (1).
for human solvers, p(f|c) will in general be zero except for a handful of candidate    lls
that conform to full domain knowledge. thus a    1973 non   ction best seller about a woman
with multiple personalities    must be    sybil   ; a 3-letter    black halloween animal    might
be    bat    or    cat   , and so on. for dr.fill, complete domain knowledge is impractical
and much greater use is made of the crossing words, as the csp solver exploits the hard
constraints in the problem to restrict the set of candidate solutions.

dr.fill   s performance as a solver is comparable to (but signi   cantly faster than) all
but the very best human solvers. in solving new york times crosswords (which increase in
di   culty from monday to saturday, with the large sunday puzzles comparable to thursdays
in di   culty), dr.fill generally solves monday to wednesday puzzles fairly easily, does well
on friday and saturday puzzles, but often struggles with thursday and sunday puzzles.
these puzzles frequently involve some sort of a    gimmick    where the clues or    ll have been
modi   ed in some nonstandard way in order to make the puzzle more challenging. when run
on puzzles from the american crossword puzzle tournament, the annual national gathering
of top solvers in the new york city area, dr.fill   s performance puts it in the top    fty or
so of the approximately six hundred solvers who typically attend.

the outline of this paper is as follows. preliminaries are contained in the next section,
both formal preliminaries regarding csps in section 2.1 and a discussion of crosswords in
section 2.2. section 2.3 discusses crosswords as csps speci   cally, including a description of
the variety of ways in which crosswords di   er from the problems typically considered by
the id124 community.

the heuristics used by dr.fill are described in section 3, with value-selection heuristics
the topic of section 3.1 and variable-selection heuristics the topic of section 3.2. the

852

dr.fill: a crossword solver

techniques for both value and variable selection can be applied to wcsps generally, although
it is not clear how dependent their usefulness is on the crossword-speci   c features described
in section 2.3.

our modi   cation of lds is described in section 4, and is followed in section 5 with our
   rst discussion of the experimental performance of our methods. algorithmic extensions
involving postprocessing are discussed in section 6, which also discusses the reasons that
branch-and-bound techniques are not likely to work well in this domain. branch-and-
bound and postprocessing are not compatible but the arguments against branch-and-bound
are deeper than that. section 7 describes the utility of splitting a crossword into smaller
problems when the associated constraint graph disconnects, an idea dating back to work of
freuder and quinn (1985) but somewhat di   erent in the setting provided by lds.

section 8 concludes by describing related and future work, including the earlier crossword
solvers proverb (littman, keim, & shzaeer, 2002) and webcrow (ernandes, angelini,
& gori, 2005), and the jeopardy-playing program watson (ferrucci, brown, chu-carroll,
fan, gondek, kalyanpur, lally, murdock, nyberg, prager, schlaefer, & welty, 2010).

2. preliminaries

in this section, we give a brief overview of id124, crossword puzzles, and
the relationship between the two.

2.1 id124
in a conventional constraint-satisfaction problem, or csp, the goal is to assign values to
variables while satisfying a set of constraints. the constraints indicate that certain values
for one variable, say v1, are inconsistent with other speci   c values for a di   erent variable v2.
map coloring is a typical example. if a particular country is colored red, then neighboring

countries are not permitted to be the same color.

we will formulate crossword solving by associating a variable to each word in the cross-
word, with the value of the variable being the associated    ll. the fact that the    rst letter
of the word at 1-across has to match the    rst letter of the word at 1-down corresponds to
a constraint between the two variables in question.

a basic csp, then, consists of a set v of variables, a set d of domains, one for each

variable, from which the variables    values are to be taken, and a set of constraints.

de   nition 2.1 given a set of domains d and set v of variables, an n-ary constraint    is
a pair (t, u ) where t     v is of size n and u is a subset of the allowed sets of values for
the variables in v . an assignment s is a mapping from each variable v     v to an element
of v   s domain. for t     v , the restriction of s to t , to be denoted s|t , is the restriction of
the mapping s to the set t . we will say that s satis   es the constraint (t, u ) if s|t     u .

the constraint simply speci   es the sets of values that are allowed for the various variables
involved.
as an example, imagine coloring a map of europe using the four colors red, green, blue
and yellow. now t might be the set {france, spain} (which share a border), d would assign
the domain {red, green, blue, yellow} to each variable, and u would be the twelve ordered

853

ginsberg

pairs of distinct colors from the four colors available. the associated constraint indicates
that france and spain cannot be colored the same color.

as we have remarked, we will take the view that the variables in a crossword correspond
to the various slots into which words must be entered, and the values to be all of the
words in some putative dictionary from which the    lls are taken (but see the comments in
section 2.3). if two words intersect (e.g., 1-across and 1-down, typically), there is a binary
constraint excluding all pairs of words for which shared letters di   er.

de   nition 2.2 a constraint-satisfaction problem, or csp, is a triple (v, d,   ) where v is
a set of variables, d gives a domain for each variable in v , and    is a set of constraints.
|v | will be called the size of the csp. if every constraint in    is either unary or binary, the
csp is called a binary csp.

for a csp c, we will denote the set of variables in c by vc, the domains by dc, and

the constraints by   c.

a solution to a csp is an assignment that satis   es every constraint in   .

both map coloring and crossword solving as described above are binary csps.
there is an extensive literature on csps, describing both their applicability to a wide
range of problems and various techniques that are e   ective in solving them.
it is not
practical for me to repeat that literature here, but there are two points that are particularly
salient.

first, csps are generally solved using some sort of backtracking technique. values are
assigned to variables; when a con   ict is discovered, a backtrack occurs that is designed to
correct the source of the problem and allow the search to proceed. as with other chronologi-
cally based backtracking schemes, there are well-known heuristics for selecting the variables
to be valued and the values to be used, and the most e   ective backtracking techniques use
some kind of nogood reasoning (doyle, 1979; ginsberg, frank, halpin, & torrance, 1990,
and many others) to ensure that the backtrack will be able to make progress.

we will need to formalize this slightly.

de   nition 2.3 let c be a csp, and suppose that v is a variable in vc and x a value in
the associated domain in dc. by c|v=x we will denote the csp obtained by setting v to x.
in other words, c|v=x = (vc     v, dc,   ) where    consists of all constraints    such that for
some constraint (t, u )       c

   =

(t     v,{u     u|u(v) = x}|t   v),

if v (cid:54)    t ;
if v     t .

(2)

(cid:26) (t, u ),

c|v=x will be called the restriction of c to v = x.

the notation may be intimidating but the idea is simple: values permitted by constraints
in the new problem are just those permitted in the old problem, given that we have decided
to set v to x. so if an original constraint doesn   t mention v (the top line in (2)), the new
constraint is unchanged. if v is mentioned, we see which values for the other variables are
allowed, given that v itself is known to take the value x.

854

dr.fill: a crossword solver

de   nition 2.4 let s be a partial solution to a csp c, in that s maps some of the variables
in vc to elements of dc. the restriction of c to s, to be denoted c|s, is the csp obtained
by successively restricting c to each of the assignments in s as in de   nition 2.3.

this de   nition is well-de   ned because we obviously have:

lemma 2.5 the restriction de   ned in de   nition 2.4 is independent of the order in which
the individual restrictions are taken.

2

backtracking csp solvers work by selecting variables, trying various values for the vari-
ables so selected, and then recursively solving the restricted problems generated by setting
the variable in question to the value chosen.

the second general point we would like to make regarding csp solvers is that most
implementations use some kind of forward checking to help maintain consistency as the
search proceeds. as an example, suppose that we are about to assign a value x to a
variable v, but that if we do this, then every possible value for some other variable v(cid:48) will
be eliminated by the constraints. in this case, we can obviously eliminate x as a value for v.
it is worth formalizing this a bit, although we do so only in the most general of terms.

de   nition 2.6 a propagation mechanism    is a mapping from csps to csps that does not
change the variables, so that   (v, d,   ) = (v, d(cid:48),   (cid:48)) for any (v, d,   ). we also require
that for any variable in v , the associated domain in d(cid:48) is a subset of the associated domain
in d, and if    = (t, u )       , there must be a   (cid:48) = (t (cid:48), u(cid:48))       (cid:48) with u(cid:48)     u . we will say
that    is sound if, for any csp c and solution s to c, s is also a solution to   (c).

the propagation mechanism strengthens the constraints in the problem, and may reduce
some of the variable domains as well. it is sound if it never discards a solution to the original
csp.

a wide range of propagation mechanisms has been discussed in the literature. simplest,
of course, is to simply eliminate variable values that can be shown to violate one of the
constraints in the problem.
iterating this idea recursively until quiescence (mackworth,
1977) leads to the well known ac-3 algorithm, which preserves arc consistency as the csp
is solved.

returning to the map of europe, germany borders france, holland, poland, and austria
(among other countries). if holland is red, poland is blue, and austria is yellow, this is
su   cient to cause germany   s live set to be just green (assuming that these are the four
colors available), which will in turn cause france   s live set to exclude green, even though
france does not share a direct constraint with holland, poland or austria.

alternatively, consider the crossword showed in figure 1; this is the new york times
crossword (with solution) from march 10, 2011. once we decide to put reading at 1-
across [poet   s performance], the live set for words at 1-down consists only of six-letter words
beginning with r. if we had also entered asami at 21-across [   me, too   ] and redondo
at 27-across, then the live set for 1-down would be words of the form r...ar.

weighted csps sometimes it is desirable for a csp to include    soft    constraints. the
notion here is that while a soft constraint indicates a set of variable values that are preferred
in a solution, the requirement is like the pirate   s code,    more what you   d call    guidelines   

855

ginsberg

figure 1: a thursday new york times crossword. c(cid:13)2011 the new york times. reprinted

with permission.

856

ny times, thu, mar 10, 2011matt ginsberg / will shortz1r14a16w18b21a27r38s41c47r52a58w62l64s2eloise39padthai3abroad36demarest4dad22mo32bile53ants5ins23inafla48t59cob6nip19s28dif42so54many7gary29ole40a49ripe17ob24i33ern50isi15snit34yeg43g55st63i65r8coole30r37neo56mama9emu25ma35ntle51s60lai10len20d31polish61inn11loca26lity44ba57nzai12anemone45ariege13redeyes46deader   2011, the new york timesacross1. *poet'sperformance8. frequent floodingsite14. country withwhich the u.s.goes to war in"wag the dog"15. who "saved mylife tonight" in a1975 elton johnhit16. with 36- and 58-across, what theanswers to thestarred clues are18. jacket material,for short?19. 1973 nonfictionbest seller abouta woman withmultiplepersonalities20. lady of theknight?21. "me, too"24. line ___26. "the thin man"actress27. ___ beach, calif.30. plunder32. big name incircuses35. b, a, d, g and e,e.g.36. see 16-across38. say "b-a-d-g-e,"e.g.40. figures on theceiling of lacappella sistina41. impersonated at acostume party43. spoils47. nutritional amt.48. doughnuts, butnot danishes51. piece of theaction52. gillette offering54. bette's "divine"stage persona57. actress vardalos58. see 16-across62. "i'm done afterthis"63. "somehoweverything getsdone"64. does nothing65. *like seattle vis-  -vis phoenixdown1. seafood lover'shangout2. nancy drew'saunt3. one way to travelor study4. pop5. connections6. cheese ___7. player of golf8. clink9. prey of wild dogsand crocodiles10. furnish11. neighborhood12. flower thatshares its namewith a tentacledsea creature13. they might departat midnight15. huff17. japanese band22. *not fixed23. like elgar'ssymphony no. 125. cloaks28. "what's the ___?"29. pharmaceuticaloils31. *shine33. old world eagle34. burglar indetective stories36. william whoplayed unclecharley on "mythree sons"37. prefix withpaganism38. many signatures39. noodle dish42. lots and lots of44. battle cry45. frenchdepartment in thepyrenees46. less lively49. opportune50. "whatever it ___don't care!"53. drones, maybe55. excitement56. ___ bear59. inner ear?60. medieval frenchlove poem61. what a keepermay keepdr.fill: a crossword solver

than actual rules.    if no solution can be found without violating one of the soft constraints,
it is acceptable to return a solution that does violate a soft constraint. the    hard    con-
straints are required to be satis   ed in any case.

there are a variety of ways to formalize this. one of the simplest is to simply associate
a cost with each soft constraint and to then search for the overall assignment of values to
variables for which the total cost is minimized. we will take the view that the total cost
of an assignment is the sum of the costs of the soft constraints that have been violated,
although other accumulation functions (e.g., maximum) are certainly possible (bistarelli,
montanari, rossi, schiex, verfaillie, & fargier, 1999).
a soft k-ary constraint thus consists of a mapping c : dk     ir giving the cost associated
with various selections for the variables being valued. the cost of a complete assignment of
values to variables is the sum of the costs incurred for each soft constraint. a csp including
costs of this form is called a weighted id124 problem, or wcsp (larrosa &
schiex, 2004, and many others).

de   nition 2.7 a weighted csp, or wcsp, is a quadruple c = (v, d,   , w ) where (v, d,   )
is a csp and w is a set of pairs (u, c) where u     v is a set of variables and c is a cost
function assigning a cost to each assignment of the variables in u . each element w     w
will be called a weighted constraint. where no ambiguity can arise, we will abuse notation
and also denote by w the associated cost function c.

given a partial solution s, the associated cost of the weighted constraint w = (u, c), to
be denoted by c(s, w), is the minimum cost associated by c to any valuation for the variables
in u that extends the partial solution s. the cost of the partial solution is de   ned to be

c(s) =

c(s, w)

(3)

(cid:88)

w   w

informally, c(s, w) is the minimum cost that will be charged by w to any solution to c that
is an extension of s. we therefore have:

lemma 2.8 given a wcsp c and partial solution s, every solution to c that extends s
has cost at least c(s).

2

note that our de   nition 2.7 is slightly nonstandard in that we explicitly split the hard
constraints in    from the soft constraints in w . we do this because in the crossword
domain, there is a further condition that is met: the soft constraints are always unary
(although the hard constraints are not). there is simply a cost associated with setting the
variable v to some speci   c value x. we will refer to such problems as singly weighted csps,
or swcsps. while the algorithmic ideas that we will present in this paper can be applied
reasonably easily to wcsps that are not in fact swcsps, the experimental work underlying
dr.fill clearly re   ects performance on swcsps speci   cally.1

as it is possible to use propagation to reduce the sizes of the domains in any particular
csp, it is also possible to use a variety of polynomial time algorithms to compute lower

1. and in fact, larrosa and dechter (2000) have shown that all weighted csps can be recast similarly, into

a form with only hard binary constraints and soft unary constraints.

857

ginsberg

bounds on c(s) for any partial solution s. the techniques used here have become increas-
ingly sophisticated in recent years, ranging from algorithms that    move    costs around the
constraint graph to better compute the minimum (givry & zytnicki, 2005; zytnicki, gaspin,
de givry, & schiex, 2009) to more sophisticated approaches that solve id135
problems to compute more accurate bounds (cooper, de givry, sanchez, schiex, zytnicki,
& werner, 2010).

finally, we note in passing the every csp has a    dual    version where the roles of the
variables and constraints are exchanged. we view crosswords as csps where the variables
are word slots and the values are the words that    ll them, with constraints that require
that the letters match where the words cross. but we could also view crosswords as csps
where the variables are individual letters, the values are the usual a through z, and the
constraints indicate that every collection of letters needs to make up a legal word. we
will discuss the likely relative merits of these two approaches in section 2.3, after we have
described crosswords themselves.

2.2 crosswords

since the introduction of the    rst    word cross    in the sunday new york world almost a
century ago (december 21, 1913), crosswords have become one of the world   s most popular
mental pastimes. will shortz, editor of the crossword for the new york times, estimates
that some    ve million people solve the puzzle each day, including syndication.2

2.2.1 features of crosswords

a typical new york times crossword appears in figure 1. we assume that the reader is
familiar with the basic format, but there are many speci   c features that are worth men-
tioning.
crosswords are symmetric. the black squares, or blocks, are preserved under a 180   
rotation.3 in addition, crosswords are almost always square in shape, with the times daily
puzzles being of size 15    15 and the sundays 21    21.4

multiple words are permitted as    ll.
in the puzzle of figure 1, we have [seafood
lover   s hangout] cluing raw bar at 1-down and [   somehow everything gets done   ] cluing
i manage at 63-across. there is no indication in the clue that a multiword answer is
expected.

without the clues, crossword solutions are not unique. there are many ways to
   t words into any particular crossword grid; it is the clues that determine which legal    ll
is the puzzle   s solution. this is what makes solving so challenging from a computational
perspective: failing to understand the clues (at least at some level) leaves the problem
underconstrained.

2. personal communication.
3. in rare cases, horizontal symmetry is present instead of rotational symmetry. in rarer cases still, the
4. the sunday puzzles used to be 23    23 on occasion, but a reduction in the size of the times    printed

symmetry requirement is not honored.

magazine section made these larger puzzles impractical.

858

dr.fill: a crossword solver

puzzles can be themed or themeless. a themeless puzzle contains a collection of
generally unrelated words and clues. a themed puzzle has some shared element that con-
nects many of the answers; when this happens, the shared answers are generally located
symmetrically in the grid.

the puzzle in figure 1 is themed. the (symmetric) entries at 1-across, 65-across,
22-down and 31-down are marked with asterisks and are all words that are pronounced
di   erently when capitalized. this description also appears in the puzzle itself using the
(also symmetric) entries at 16-across, 36-across and 58-across.5

the presence of a theme has a profound impact on the solving experience. in this par-
ticular example (which is relatively straightforward), there are two entries (wordspro-
nounced and whencapitalized) that surely do not appear in any    dictionary    that
the solver is using to    ll the grid. there are also complex relationships among many of the
entries     the phrase in 16-across, 36-across and 58-across, but also the relationship of that
phrase to the entries marked with asterisks.

other themes present other challenges. some of the more popular themes are quip
puzzles, where a famous saying is split into symmetric pieces and inserted into the grid,
and rebus puzzles, where more than one letter must be put in a single square. arguably
the most famous themed times puzzle appeared on election day in 1996. the clue for
39-across was [lead story in tomorrow   s newspaper (!), with 43-across]. 43-across was
elected and, depending on the choices for the down words, 39-across could be either
clinton or bobdole. for example, 39-down, a three-letter [black halloween animal]
could be cat (with the c in clinton) or bat (with the b in bobdole). so here, not
only are there multiple ways to insert words legally into the grid, there are multiple ways
for those words to match the clues provided. (until the winner of the election was decided,
of course.) the two legal solutions were identical except for the clinton/bobdole
ambiguity and the associated crossing words; it is not known whether there is a puzzle that
admits multiple solutions without shared words, while conforming to the usual restrictions
on symmetry, number of black squares, and so on.

a more extreme example of a themed crossword appears in figure 2. each clue has a
particular letter replaced with asterisks (so in 1-a, for example, the e in [twinkle] has been
replaced). the letter that has been replaced is dropped in the    ll, but the result is still
a word. so 1-a, which would normally be gleam, becomes the david bowie rock genre
glam.

every word in this puzzle is missing letters in this fashion. a computer (or human)
solver will be unable to get a foothold of any kind if it fails to understand the    gimmick   ,
and dr.fill fails spectacularly on this puzzle.

puzzles have structural restrictions. for the times, a daily unthemed puzzle will
have at most 72 words; a themed puzzle will generally have at most 78. for a sunday,
140 words is the limit. at most 1
6 of the squares in the grid can be black squares.6 this
information is potentially of value to a solver because the word count can often be used to

5. when submitted, this puzzle also contained an asterisk for the entry at 36-across, which did not break
the symmetry of the pronounced-di   erently-when-capitalized entries. shortz decided that too many
solvers wouldn   t get the joke, though, and the asterisk was removed when the puzzle was published.

6. the times puzzle with the fewest words (52) appeared on january 21, 2005 and was by frank longo.
the puzzle with the fewest blocks (18) appeared on august 22, 2008 and was by kevin der, a feat

859

ginsberg

figure 2: a di   cult themed crossword.

c(cid:13)2009 the new york times. reprinted with

permission.

860

ny times, sun, may 17, 2009 takeaway crossword (see notepad)matt ginsberg / will shortz113161926313843525763677022753328544233948203244141740496468715334558624295565721305659825415022344691518425166697210354760113661123762   2009, the new york timesacross1. twinkl*5. ou*look9. "dani*l boon*"actor13. gung-*o [lat.]14. sp*tted cats15. male chauvini*t,*ay16. playin* the slots,e.*.17. minia*uredesser*s18. admonis* [suffix]19. *iding, as a sword21. neither young norol*23. diat*ibe delive*e*s25. *hief26. sergea*t of old tv29. denta* devices31. still feelin* sleepy32. *over up34. mentalist inspiredby "mandra*e themagician"38. struc* with thefoot40. mail origi*ator42. absolutely ama*e43. *emoves pencilma*ks45. big pi*kles?47. cat*' warning*48. apo*tle known a*"the zealot"50. dise*setr*nsmitted bycont*min*tedw*ter52. italian po*t?55. cerea* protein[ger.]57. p*blic sales59. im*roved one's lot[hyph.]63. can*ne restra*nt64. sou*hwes*german ci*y66. rea*y to be ri**en67. roun*e* up68. lessons fro*fables69. 1960* prote*t [2wds.]70. s*ut up71. places *o pick upchicks?72. likel* to rise?down1. brib*, informally[fr.]2. bit of *ire3. asla*'s realm4. lite*ally, "sac*edutte*ances"5. nothing speci*l6. opposite of *on't7. m*nholeem*n*tion8. the *eatles' paulmccartney, e.g.9. made smalle*10. *ostco*prehensive11. *mall amount,briefly12. device thatre*oves stalks fro*fruit14. chin*s* cuisin*20. *atchy *ony22. *oting boothfeature24. big name in ba**s26. pulit*er, e.g. [fr.]27. *egis*ative routine[sp.]28. b*ont   family30. speaks gi**erish33. first mo*th, tojua*35. lesley of "60minu*es"36. waiti*g o*e's tur*37. di*dainfulexpre**ion39. left themet*oline*41. core cont*iners44. national park insout*westtennessee46. turt*es andbu**ets havethem49. *mpos*ng house51. biase*52. qi*g dy*astypeople53. a**ow shoote*s54. o*erdo the diet56. street art, *aybe58. stron* *rowth60. not homo*eneous[lat.]61. "lo*e me tender"star62. b*rth cert., for one65. rank*esdr.fill: a crossword solver

determine whether or not a puzzle has a theme. (the themed puzzle of figure 1 has only
70 words, however, so all that can really be concluded is that a 15    15 puzzle with more
than 72 words is likely themed in some way.)

there are other restrictions as well. two-letter words are not permitted, and every
square must have two words passing through it (alternatively, one-letter words are not
permitted, either). the puzzle must be connected (in that the underlying csp graph is as
well). for a puzzle to be singly connected, in that converting a single empty square to a
block disconnects it, is viewed as a    aw but an acceptable one.

if the    ll bat appears
fill words may not be repeated elsewhere in the puzzle.
in one location in the puzzle, then it cannot be used elsewhere (including in multiword    ll).
in addition, bat cannot be used in a clue. this means that words appearing in clues do
not (or at least, should not) appear in the    ll as well.

crossword clues must pass the    substitution test   . this is arguably the most
important requirement from the solver   s perspective. it must be possible to construct a
sentence in which the clue appears, and so that the meaning of the sentence is essentially
unchanged if the clue is replaced with the    ll. so for the puzzle in figure 1, one might
say,    i   ve seen a video of e.e. cummings giving a reading,    or the equivalent (although
stilted)    i   ve seen a video of e.e. cummings giving a [poet   s performance].    one might say,
   we don   t keep food in our house   s cellar because we don   t want it to get wet,    or the
equivalent    we don   t keep food in our house   s [frequent    ooding site] because we don   t want
it to get wet.   

the fact that the clues and associated    ll must pass the substitution test means that
it is generally possible to determine the part of speech, number (singular vs. plural) and
tense (present, past, future, etc.) of the    ll from the clue. so in figure 1, the    ll for 1-a
is a singular noun, and so on. this restricts the number of possible values for each word
considerably.

there are other conventions regarding cluing.
if a clue contains an abbreviation,
then the answer is an abbreviation as well. the puzzle in figure 1 has no abbreviations
clued in this way (a rarity), but 62-d in figure 2 is [b*rth cert., for one]. the solution is
ident, which is an abbreviation for    identi   cation.    of course, the i gets dropped, so it is
dent that is entered into the grid. abbreviations can also be indicated by a phrase like
   for short    in the clue.

clues ending in a ? generally indicate that some sort of wordplay is involved. in figure 1,
for example, we have 18-a: [jacket material, for short?] the solution is bio because    jacket   
in the clue refers to a book   s jacket, not clothing.

the wordplay often exploits the fact that the    rst letter of the clue is capitalized. so
in figure 1, 7-d is [player of golf], referring to gary player. if the clue had been [golf   s
player], the capitalization (not to mention the phrasing) would have made it obvious that
a proper name was involved. as the clue was written, the solver might easily be misled.

crossword features and dr.fill many of the features that we have described (e.g.,
symmetry) do not bear directly on the solving experience, and dr.fill is therefore unaware

repeated on august 7, 2010 by joe krozel. it is not known whether a 17-block puzzle of reasonable
quality exists.

861

ginsberg

of them. the program does look for multiple-word    ll and has a module that is designed to
identify rebus puzzles. it does not check to see if    ll words are repeated elsewhere, since this
is so rare as to o   er little value in the search. it uses fairly straightforward part-of-speech
analysis to help with the substitution test, and checks clues for abbreviations. dr.fill has
no knowledge of puns.

2.3 crossword puzzles as swcsps
given all of this, how are we to cast crossword solving as a csp?

the view we will take, roughly speaking, is that we start with some large dictionary of
possible    lls, and the goal is to enter words into the grid that cross consistently and so that
each word entered is a match for the associated clue. if d is our dictionary, we will de   ne dn
to be the subset of d consisting of words of exactly n letters, so that bat is in d3, and so
on. we also assume that we have a scoring function p that scores a particular word relative
to a given clue, which we will interpret probabilistically. given a clue like [black halloween
animal]3 for a 3-letter word and potential    ll bat, p(bat|[black halloween animal]3) is the
id203 that bat is the correct answer for the word in question. our goal is to now    nd
the overall    ll with maximum id203 of being correct.

in other words, if ci is the ith clue and fi is the value entered into the grid, we want to
   nd fi that satisfy the constraints of the problem (crossing words must agree on the letter

   lled in the crossing square) and for which(cid:89)

p(fi|ci)

(4)

i

(cid:88)

is maximized. as mentioned in the introduction, this assumes that the probabilities of
various words being correct are uncorrelated, which is probably reasonably accurate but
not completely correct in a themed puzzle.

if we de   ne   (fi, ci) =     log p(fi|ci), maximizing (4) is equivalent to minimizing

  (fi, ci)

(5)

i

this is exactly the swcsp framework that we have described.

the dictionary di must include not just words of length i, but also all sequences of words
that are collectively of length i. (in other words, d15 needs to include whencapitalized.)
in actuality, however, even this is not enough. there are many instances of crossword    ll
that are not even word sequences.

this may be because a    word    does not appear in any particular dictionary. a puzzle
in the 2010 american crossword puzzle tournament (acpt) clued mmyy as [credit card
exp. date format] although mmyy itself is not a    word    in any normal sense. a further
example is the appearance of snoissiwnoow in the times puzzle of 11/11/10, clued as
   apollo 11 and 12 [180 degrees]   . rotating the entire puzzle by 180 degrees and reading
snoissiwnoow upside down produces moonmissions.

given these examples and similar ones, virtually any letter sequence can, in fact, appear
in any particular puzzle. so the domain d should in fact consist of all strings of the
appropriate length, with the cost function used to encourage use of letter strings that are

862

dr.fill: a crossword solver

dictionary words (or sequences of words) where possible. this means that the variable
domains are so large that both they and the associated    functions must be represented
functionally; computing either the dictionaries or the scores in their entirety is simply
impractical.

this is the fundamental di   erence between the problem being solved by dr.fill and
the problems generally considered in the ai literature. the number of variables is modest,
on the order of 100, but the domain size for each variable is immense, 265 or approximately
12 million for a word of length    ve, and some 1.7    1018 for a word of length    fteen.

one immediate consequence of this is that dr.fill can do only a limited amount of
forward propagation when it solves a particular puzzle. when a letter is entered into a
particular square of the puzzle, it is e   ective to see the way in which that letter constrains
the choices for the crossing words. but it appears not to be e   ective to propagate the
restriction further. so if, in the puzzle in figure 1, we restrict 1-down to be a word of
the form r...ar and the only word of that form in the dictionary is raw bar, we could
conceivably then propagate forward from the b in bar to see the impact on 18-across. in
actuality, however, the possibility that 1-down be non-dictionary    ll causes propagation
beyond a simple one-level lookahead to be of negative practical value. the sophisticated
propagation techniques mentioned in section 2.1 appear not to be suitable in this domain.
a second consequence of the unrestricted domain sizes is that it is always possible to
extend a partial solution in a way that honors the hard constraints in the problem. we can
do this by simply entering random letters into each square of the puzzle (but only one letter
per square, so that the horizontal and vertical choices agree). each such random string is
legal, and may even be correct. the reason such    lls are in general avoided is that random
strings are assigned very high cost by the soft constraints in our formulation.

the fact that partial solutions can always be extended to satisfy the hard constraints is a
di   erence between the problem being solved by dr.fill and those considered elsewhere in
the csp literature. here, however, there is an exception. much of the work on probabilistic
analysis using markov random    elds focuses on a probabilistic maximization similar to ours,
once again in an environment where solving the hard constraints is easy but maximizing
the score of the result is hard.

a popular id136 technique in the markov setting is id209, where the
roles of the variables and values are switched, with lagrange multipliers introduced cor-
responding to the variable values and their values then optimized to bound the quality of
the solution of the original problem (sontag, globerson, & jaakkola, 2011, for example).
this is similar to the csp notion of duality, where the roles of variables and values are also
exchanged.

it is not clear how to apply this idea in our setting. in the probabilistic case, the variable
values are probabilities selected from a continuous set of real numbers. in the crossword
case, the domain is still impracticably large but there does not appear to be any natural
ordering or notion of continuity between one string value and the next.

there is one further di   erence between the crossword domain and more standard ones
that is also important to understand. consider the themed crossword called    heads of
state    from the 2010 acpt. the theme entries in this puzzle were common phrases with
two-letter state abbreviations appended to the beginning. thus [film about boastful jerks?]
clues vain glourious basterds, which is the movie title inglourious basterds

863

ginsberg

together with the two-letter state abbreviation va. [origami?] clues paper forming,
which is pa adjoined to performing, and so on.

these multiword    lls that do not appear explicitly in the dictionary score fairly badly.
in most conventional csps, it is reasonable to respond to this by    lling the associated words
earlier in the search. this allows    better    values to be assigned to these apparently di   cult
variables. this general idea underlies joslin and clements    (1999)    squeaky wheel opti-
mization    and virtually every more recent variable selection heuristic, such as boussemart
et. al   s (2004) notion of constraint weighting, and the dom/wdeg heuristic (lecoutre, sa    s,
tabary, & vidal, 2009, and others).

in the crossword domain, however, words that score badly in this way should arguably
be    lled later in the search, as opposed to earlier. there is obviously no way for a program
such as dr.fill, with extremely limited domain knowledge. to    gure out that a 21-letter
   word    for a [film about boastful jerks?] should be vain glourious basterds. the
hints suggested by the crossing words are essential (as they are for humans as well). so none
of the classic variable selection heuristics can be applied here, and something else entirely
is needed. the heuristics that we use are presented in section 3.

before moving on, there are two    nal points that we should make. first, our goal is
to    nd    ll for which (4) is maximized; in other words, to maximize our chances of solving
the entire puzzle correctly. this is potentially distinct from the goal of entering as many
correct words as possible, in keeping with the scoring metric of the acpt as described in
section 5. the best human solvers generally solve the acpt puzzles perfectly, however, so
if the goal is to win the acpt, maximizing the chances of solving the puzzles perfectly is
appropriate.

second, we designed dr.fill so that it could truly exploit the power of its underlying
search algorithm. in constructing the    function of (5), for example, we don   t require that
the    correct    solution for a clue ci be the speci   c    ll fi for which (5) is minimized, but only
hope that the correct fi be vaguely near the top of the list. the intention is that the hard
constraints corresponding to the requirement that the    lling words    mesh    do the work
for us. as with so many other automated game players (campbell, hoane, & hsu, 2002;
ginsberg, 2001; schae   er, treloar, lu, & lake, 1993), we will rely on search to replace
understanding.

2.4 data resources used by dr.fill
one of the most important resources available to dr.fill is its access to a variety of
databases constructed from online information. we brie   y describe each of the data sources
here; a summary is in table 1. the table also includes information on the size of the
analogous data source used by littman   s crossword solving program proverb (littman
et al., 2002).

2.4.1 puzzles

dr.fill has access to a library of over 47,000 published crosswords. these include virtually
all of the major published sources, including the new york times, the (now defunct) new
york sun, the los angeles times, usa today, the washington post, and many others.
most puzzles from the early 1990   s on are included.

864

dr.fill: a crossword solver

data type

puzzles
clues

unique clues

small dictionary
big dictionary

word roots
synonyms

wikipedia titles
wikipedia pairs

source
various

http://www.otsys.com/clue
http://www.otsys.com/clue

quantity

47,693

3,819,799
1,891,699

http://www.crossword-compiler.com

8,452

various

http://id138.princeton.edu
id138 and other online
http://www.wikipedia.org
http://www.wikipedia.org

6,063,664
154,036
1,231,910
8,472,583
76,886,514

quantity (proverb)

5,142
350,000
250,000
655,000
2,100,000
154,036 (?)
unknown

   
   

table 1: data used by dr.fill

collectively, these puzzles provide a database of just over 3.8 million clues, of which
approximately half are unique. this is to be contrasted with the corresponding database in
proverb, which contains some 5,000 puzzles and 250,000 unique clues.

the clue database is available from http://www.otsys.com/clue, a public-domain clue
database used by many crossword constructors. the underlying data is compressed, but
the source code is available as well and should enable interested parties to decompress the
data in question.

2.4.2 dictionaries

as with proverb, dr.fill uses two dictionaries. a small dictionary is intended to contain
   common    words, and a larger one is intended to contain    everything   . the larger dictio-
nary is an amalgamation from many sources, including moby7 and other online dictionaries,
wikipedia titles, all words that have ever been used in crosswords, and so on.

the small dictionary is the    basic english    dictionary that is supplied with crossword
compiler, an automated tool that can be used to assist in the construction of crosswords.
the large dictionary is much more extensive. every entry in the large dictionary is
also marked with a score that is intended to re   ect its crossword    merit   . some words are
generally viewed as good    ll, while others are bad. as an example, buzz lightyear is
excellent    ll. it is    lively    and has positive connotations. the letters are interesting (high
scrabble score, basically), and the combination zzl is unusual. tern is acceptable    ll;
the letters are mundane and the word is overused in crosswords, but the word itself is at
least well known. elis (yale graduates) is poor    ll. the letters are common, the word
is obscure, and it   s an awkward plural to boot. crossword merit for the large dictionary
was evaluated by hand scoring approximately 50,000 words (100 volunteers, all crossword
constructors, scored 500 words each). the words were then evaluated against many criteria
(length, scrabble score, number of google hits,8 appearances in online corpora, etc.) and a
linear model was built that best matched the 50,000 hand-scored entries. this model was
used to score the remaining words.

7. http://icon.shef.ac.uk/moby/mwords.html
8. i would like to thank google in general and mark lucovsky in particular for allowing me to run the

approximately three million google queries involved here.

865

ginsberg

note that the scores here re   ect the crossword    value    of the words in isolation, ignoring
the clues. thus we cannot use the dictionaries alone to solve crosswords; indeed, for any
particular crossword, there will be many legal    lls and the actual solution is unlikely to be
anywhere near the    best       ll in terms of word merit alone.

2.4.3 grammatical and synonym information

grammatical information is collected from the data provided as part of the id138
project (fellbaum, 1998; miller, 1995). this includes a list of 154,000 words along with
their parts of speech and roots (e.g., walked has walk as a root). proverb also cites
id138 as a source. in addition, a list of 1.2 million synonyms was constructed from an
online thesaurus.

2.4.4 wikipedia

finally, a limited amount of information was collected from wikipedia speci   cally. dr.fill
uses a list of all of the titles of wikipedia entries as a source of useful names and phrases, and
uses a list of every pair of consecutive words in wikipedia to help with phrase development
and    ll-in-the-blank type clues. there are approximately 8.5 million wikipedia titles, and
wikipedia itself contains 77 million distinct word pairs.

3. heuristics

at a high level, most csps are solved using some sort of depth-   rst search. values are
assigned to variables and the procedure is then called recursively. in pseudocode, we might
have:

procedure 3.1 to compute solve(c, s), a solution to a csp c that extends a partial
solution s:

1
2
3
4
5
6
7
8
9

if s assigns a value to every variable in vc, return s
v     a variable in vc unassigned by s
for each d     dv(c|s)

do s(cid:48)     s     (v = d)

c(cid:48)     propagate(c|s(cid:48))
if c(cid:48) (cid:54)=   

then q     solve(c(cid:48), s(cid:48))
if q (cid:54)=   , return q

return   

we select an unassigned variable, and try each possible value. for each value, we set
the variable to the given value and propagate in some unspeci   ed way. we assume that
this propagation returns the empty set as a failure marker if a contradiction is discovered,
in which case we try the next value for v. if the propagation succeeds, we try to solve the
residual problem and, if we manage to do so, we return the result.

866

dr.fill: a crossword solver

proposition 3.2 let c be a csp of size n. then if the propagate function is sound, the
value solve(c,   ) computed by procedure 3.1 is    if c has no solutions, and a solution to
c otherwise.

proof. the proof is by induction on n. for a csp of size 1, each live domain value is tried
for the variable in question; when one survives the propagate construction, the solution is
returned by the recursive call in line 1 and then from line 8 as well.

for larger n, if the csp is not solvable, then every recursive call will fail as well so that
we eventually return    on line 9. if the csp is solvable, we will eventually set any particular
variable v to the right value d     d so that the recursive call succeeds and a solution is
returned.

2

for weighted csps, the algorithmic situation is more complex because we want to return
the best solution, as opposed to any solution. we can augment procedure 3.1 to also accept
an additional argument that, if nonempty, is the currently best known solution b. we need
the following easy lemma:
lemma 3.3 in a wcsp where the costs are non-negative, if s1     s2, then c(s1)     c(s2).

proof. this is immediate; more costs will be incurred by the larger set of assignments.
note that this is true for wcsps generally, not just singly weighted csps.
for convenience, we introduce an    inconsistent    assignment     and assume that c(   ) is

2

in   nite. now we can modify procedure 3.1 as follows:
procedure 3.4 to compute solve(c, s,   b), the best solution to a wcsp c that extends a
partial solution s given a currently best solution b:

if c(s)     c(b), return b
if s assigns a value to every variable in vc, return s
v     a variable in vc unassigned by s
for each d     dv(c|s)

do s(cid:48)     s     (v = d)

c(cid:48)     propagate(c|s(cid:48))
if c(cid:48) (cid:54)=   , b     solve(c(cid:48), s(cid:48), b)

return b

1
2
3
4
5
6
7
8

we use the    b notation at the beginning of the procedure to indicate that b is passed
by reference, so that when b is changed on line 7, the value of b used in other recursive
calls is changed as well.

in the loop through variable values, we can no longer return a solution as soon as we
   nd one; instead, all we can do is update the best known solution if appropriate. this
will, of course, dramatically increase the number of nodes that are expanded by the search.
there is some o   setting saving in the comparison on line 1; if the cost of a partial solution
is higher than the total cost of the best known solution, lemma 3.3 ensures that we need
not expand this partial solution further. note that the conditions of the lemma are satis   ed
in dr.fill, since the costs are negated logarithms of probabilities, and the probabilities
can be assumed not to exceed one.

867

ginsberg

proposition 3.5 let c be a csp. then the value solve(c,   ,   ) computed by proce-
dure 3.4 is     if c has no solutions, and the least cost solution to c otherwise.

proof. suppose    rst that we drop line 1 and replace line 7 with

if c(cid:48) (cid:54)=        c(solve(c(cid:48), s(cid:48), b)) < c(b) then b     solve(c(cid:48), s(cid:48), b)

(6)

now the result follows easily by an inductive argument similar to the proof of proposi-
tion 3.1. every possible solution will be considered, and we will gradually    nd the least
cost one to return.

consider now procedure 3.4 as written. if we return a set s on line 2, we must have
c(s) < c(b) by virtue of the test on line 1. thus the new requirement in (6), namely that
c(solve(c(cid:48), s(cid:48), b)) < c(b), will always be satis   ed and the proof will be complete if we can
show simply that the test on line 1 will never discard the best solution. in other words,
we need to show that for any solution s(cid:48) discarded as a result of the test on line 1, we
will have c(s(cid:48))     c(b). but this follows directly from lemma 3.3, since c(s(cid:48))     c(s) and
c(s)     c(b).

2

procedure 3.4 is the historical method of choice on wcsps. it is generally referred to as
branch and bound because the cost of the best solution b found in one branch is used to
bound the searches in other branches.

to implement the procedure, we need to specify mechanisms by which variables are
selected on line 3 and the domain is ordered on line 4. we discuss value selection    rst and
then variable selection. as described in section 2.3, the propagation mechanism most useful
in crossword solving considers only the direct impact of a word selection on the crossing
words.

3.1 value selection

the performance of procedure 3.4 depends critically on the order in which values are selected
from the domain d. the sooner we    nd good solutions, the earlier we can use the test in
line 1 to prune the subsequent search. this kind of argument will remain valid even after we
replace procedure 3.4 with other algorithms that are more e   ective in practice; it is always
advantageous to order the search so that the    nal solution is found earlier, as opposed to
later.

there are a variety of elements to this. first, note that all we really need for line 4
of procedure 3.4 is a function fill(v, n) that returns the nth element of v   s live domain
dv. on each pass through the loop, we call fill while gradually increasing the value
of n. faltings and macho-gonzalez (2005) take a similar approach in their work on    open   
constraint programming.

as in the work on open constraint programming, this observation allows us to deal with
the fact that not all crossword    lls appear explicitly in the dictionary. our scoring function
allows non-dictionary words, but assumes that an apparently unrelated string of words (or
of letters) is less likely to be correct than a word or phrase that actually appears in the
dictionary. this means that the fill function can evaluate all of the dictionary possibilities
before generating any    multiwords   . the multiwords are generated only as needed; by the

868

dr.fill: a crossword solver

time they are needed, most of the letters in the word are generally    lled. this narrows the
search for possible multiwords substantially.9

the implementation begins by scoring every word of the appropriate length and storing
the results with the domain for word n. when multiwords are needed and generated, they
are added to the end of the domain sets as appropriate.

this approach reduces the value selection problem to two subproblems. first, we need
the scoring function   (fi, ci) that evaluates    ll fi given clue ci. second, we need to use this
scoring function to produce an actual ordering on possible words to enter; the lowest cost
word may or may not be the one we wish to try    rst.

we will not spend a great deal of time describing our scoring function; the details are
predictably fairly intricate but the ideas are simple. fundamentally, we will take the view
that has proven so successful elsewhere in computer game players: it is more important
that the system be able to search e   ectively than that it actually have a terribly good idea
what it is doing. the power is always more in the search than in the heuristics. this
overall search-based approach underlies virtually all of the best computer game players
(campbell et al., 2002; ginsberg, 2001; schae   er et al., 1993) and search-based algorithms
have easily outperformed their knowledge-based counterparts (smith, nau, & throop, 1996,
for example) in games where direct comparisons can be made.

we implement this idea with a scoring system that is in principle quite simplistic. words

are analyzed based on essentially    ve criteria:10

1. a match for the clue itself. if a clue has been used before, the associated answer is
preferred. if a new clue shares a word or subphrase with an existing one, that answer
scores well also.

2. part of speech analysis. if it is possible to parse the clue to determine the likely part
of speech of the answer,    ll matching the desired part of speech is preferred. the part
of speech analysis is based on the id138 dictionary (fellbaum, 1998; miller, 1995),
which is then used to search for parse patterns in the clue database. no external
syntax or other grammatical mechanisms are used.

3. crossword    merit    as discussed in section 2.4.2.

4. abbreviation. abbreviations in the dictionary are identi   ed by assuming that words
that are generally clued using abbreviations are themselves abbreviations, as described
previously. this information is then used in scoring a possible answer to a new
clue. what exactly constitutes an    abbreviation    clue is determined by recursively
analyzing the clue database.

5. fill-in-the-blank. some clues are       ll in the blank   s. these generally refer to a
] clues

common phrase with a word missing, as in 24-a in figure 1, where [line

9. and, as remarked earlier, means that we need to value badly scoring variables late in the search as

opposed to early.

10. proverb has some thirty individual scoring modules (littman et al., 2002), although littman has
suggested (personal communication) that most of the value comes from modules that are analogous to
those used by dr.fill. proverb does not analyze the clues to determine the part of speech of the
desired    ll.

869

ginsberg

item. these clues are analyzed by looking for phrases that appear in the body of
wikipedia.

these    ve criteria are then combined linearly. to determine the weights for the various
criteria, a speci   c set of weights (w1, . . . , wn) is selected and then used to solve each of a
   xed testbed of puzzles (the    rst 100 new york times puzzles from 2010). for each puzzle
in the testbed, we count the number of words entered by the search procedure before a
mistake is made in that the heuristically chosen word is not the one that appears in the
known solution to the puzzle. the average number of words entered correctly is then the
   score    of (w1, . . . , wn) and the weights are varied to maximize the score.

given the scoring function   , how are we to order the values for any particular word?
we don   t necessarily want to put the best values    rst, since the value that is best on this
word may force us to use extremely suboptimal choices for all of the crossing words.
more precisely, suppose that we assign value d to variable v, and that propagation now
reduces the variable domains to new values di(c|s   {v=d}). an argument similar to that
underlying lemma 2.8 now produces:
proposition 3.6 let c be an swcsp and s a partial solution, so that du(  (c|s   {v=f}))
is the domain for u after v is set to f and the result propagated. then the minimum cost
of a solution to c that extends s     {v = f} is at least

min

x   du(  (c|s   {v=f}))

  (x, u).

2

(7)

(cid:88)

u

we order the variable values in order of increasing total cost as measured by (7), preferring
choices that not only work well for the word slot in question, but also minimally increase
the cost of the associated crossing words.

this notion is fairly general. in any wcsp, whenever we choose a value for a variable,
the choice    damages    the solution to the problem at large; the amount of damage can
be determined by propagating the choice made using whatever mechanism is desired (a
simplistic approach such as ours, full arc consistency, cooper   s linear relaxation, etc). cost
is incurred not only by the choice just made, but as implied on other variables by the
propagation mechanism. (7) says that we want to choose as value for the variable v that
value for which the total global cost is minimized, not just the local cost for the variable
being valued.

in the crossword domain, this heuristic appears to be reasonably e   ective in practice.
combined with the variable selection heuristic to be described in the next section, dr.fill
inserts an average of almost 60 words into a times puzzle before making its    rst mistake.

3.2 variable selection

as argued in the previous section, the heuristic we use in valuing a possible    ll f for a word
slot s in our puzzle is

h(f, v) =

min

x   du(  (c|s   {v=f}))

min

x   du(c|s )

  (x, u)

(8)

(cid:88)

u

  (x, u)    (cid:88)

u

870

dr.fill: a crossword solver

because the domain for variable u before setting v to f is du(c|s), the term on the right
in (8) gives a lower bound on the best possible score of a complete solution before v is set
to f (and this expression is thus independent of f ).
the value of the term on the left is a lower bound on the best possible score after v is
set to f because the domain for u after setting v to f and propagating is du(  (c|s   {v=f})).
the heuristic value of setting v to f is the di   erence between these two numbers, the total
   damage    caused by the commitment to use    ll f for variable v. given (8), which variable
should we select for valuation at any point in the search?

it might seem that we should choose to value that variable for which h(f, v) is minimized.
this would cause us to    ll words that could be    lled without having a signi   cant impact
on the projected    nal score of the entire puzzle. so we could de   ne the heuristic value of a
slot v, which we will denote by h(v), to be

h(v) = min

f

h(f, v)

(9)

this apparently attractive idea worked out poorly in practice, and a bit of investigation
revealed the reason. especially early on, when there remains a great deal of    exibility in
the choices for all of the variables, there may be multiple candidate    lls for a particular
clue, all of which appear attractive in that h(f, v) is small. in such a situation, there is
really no strong reason to prefer one of these attractive    lls to the others, but using (9) as
the variable selection heuristic will force us to value such a variable and therefore commit
to such a choice.

the solution to this problem is to choose to value not that variable for which h(f, v)
is minimized, but the variable for which the di   erence between the minimum value and
second-best value is maximal. it is this di   erence that indicates how con   dent we truly
are that we will    ll the slot correctly once we decide to branch on it. if we de   ne min2(s)
to be the second-smallest element of a set s, then the variable selection heuristic we are
proposing is

h(s) = min

f

2 h(f, v)     min

f

h(f, v)

(10)

where large values are to be preferred over smaller ones.

as mentioned previously, a combination of (10) and (8) allows dr.fill to enter, on

average, an initial 59.4 words into a times puzzle before it makes its    rst error.

it is important to realize that this metric     59.4 words inserted correctly on average     is
not because the scoring function accurately places the correct word       rst    a large fraction
of the time. instead, our methods are bene   ting even at this point from anticipation of how
the search is likely to develop; the heuristics themselves are based as much on a glimpse of
the future search as they are on the word values in isolation. indeed, if we use the variable
selection described here but switch our value selection heuristic to simply prefer the best    ll
for the word in question (without considering the impact on subsequent search), the average
number of words    lled correctly at the outset of the search drops to 25.3, well under half
of its previous value.

871

ginsberg

r

            hhhhhhhhhhhh
r
e
r

@

@

 

 

 

 

@

r

 

 
a
 
 
a
a

@

@

r
e

@
a
 
 
a
 

 

@

 

@

@

@

 

 

 

r

 
a
 
 

a
a
a

@

@
 
 
 

r
e
h

a
a

a
a
a
a

r

3

 
 
 

r

 

0

r
e

a
a
a

1

r

 
 
 

1

a
a
a
a

r
e
h

2

 
 
 

r

 

1

r
e
h

a
a

2

r

 
 
 

2

figure 3: limited discrepancy search

4. limited discrepancy search

given that dr.fill can enter nearly sixty correct words in a crossword before making an
error, one would expect it to be a strong solver when combined with the branch-and-bound
solving procedure 3.4. unfortunately, this is not the case.

the reason is that this solving procedure su   ers from what harvey (1995) has called
the    early mistakes    problem. once a mistake is made, it impacts the subsequent search
substantially and the mistake is never retracted until the entire associated subspace is
examined. an initial mistake at depth (say) sixty seems impressive but the quality of the
solution below this point is likely to be quite poor, and there is unlikely to be su   cient time
to retract the original error that led to the problem.

one way around this problem in csps with binary domains is to use limited discrepancy
search, or lds (harvey & ginsberg, 1995). the idea is that if a heuristic is present, we
de   ne the    discrepancy    count of a partial solution s to be the number of times that s
violates the heuristic.
in figure 3, we have shown a simple binary search tree of depth
three; assuming that the heuristic choice is always to the left, we have labeled each fringe
node with the number of times that the heuristic is violated in reaching it.

lds is an iterative search method that expands the tree using depth-   rst search and
in order of increasing discrepancy count. on the    rst iteration, only nodes without dis-
crepancies are examined, so the search is pruned at each node in the    gure with a single
bullseye. on the second iteration, a single discrepancy is permitted and the nodes with
double bullseyes are pruned. it is not hard to see that iteration n expands o(dn) nodes,
as the discrepancy limit forms a    barrier    against a full search of the tree. each iteration
also uses only o(d) memory, since the expansion on any individual iteration is depth    rst.
there is some work repeated from iteration to iteration, but since the bulk of the work in
iteration n involves nodes that were not expanded in iteration n     1, this rework has little

872

dr.fill: a crossword solver

impact on performance. korf (1996) presents an algorithmic improvement that addresses
this issue to some extent.

the point of lds is that it allows early mistakes to be avoided without searching large
portions of the space. in the    gure, for example, if the heuristic is wrong at the root of the
tree, the node labeled 1 will be explored at the second iteration (with discrepancy limit 1),
without the need to expand the left half of the search space in its entirety.

while it is clear that the basic intuition underling lds is a good match for the search
di   culties encountered by dr.fill, it is not clear how the idea itself can be applied. one
natural approach would be to order the values for any particular word slot, and to then say
that using the second value (as opposed to the    rst) incurred one discrepancy, using the
third value incurred two discrepancies, and so on.

this doesn   t work. assuming that the    rst word in the list is wrong, subsequent words
may all score quite similarly. just because we believed strongly (and wrongly, apparently)
that the    rst word was the best    ll does not mean that we have a strong opinion about
what to use as a backup choice. the net result of this is that the best solution often uses
words quite late in the ordered list; these correspond to a very high discrepancy count and
are therefore unlikely to be discovered using this sort of an algorithmic approach.

an alternative idea is to say that a discrepancy is incurred when a variable is selected for
branching other than the variable suggested by the variable-selection heuristic (10). this
avoids the problem described in the previous paragraph, since we now will pick a    ll for a
completely di   erent word slot. unfortunately, it su   ers from two other di   culties.

the    rst (and the less important) is that in some cases, we won   t want to change the
variable order after all. perhaps there was a clear    rst choice and, once that choice is
eliminated, there is a clear second choice among the remaining candidate values. in such
an instance, we would want the    single discrepancy    search choice to try the second    ll
instead of the    rst.

more important is the fact that the bad choice is likely to come back on the very next
node expansion, when we once again consider the variable in question. the word that
looked good when the discrepancy was incurred may well still look good, and we will wind
up having used the discrepancy but not really having changed the area of the search space
that we are considering.

the algorithm that we actually use combines ideas from both of these approaches. as
the search proceeds, we maintain a list p of value choices that have been discarded, or
   pitched   . each element of p is a pair (v, x) indicating that the value x should not be
proposed for variable v. the pitched choices remain in the live set, but are not considered
as branch values for v until they are forced in that v   s live set becomes a singleton.
in
evaluating the heuristic expressions (8) and (10), pitched values are not considered.

we now incur a discrepancy by pitching the variable and value suggested by the heuris-
tics. assuming that we then completely recompute both the variable chosen for branching
and the value being used, the problems mentioned in the previous paragraphs are neatly
sidestepped. we continue to make choices in which we have con   dence, and since a pitched
value remains pitched as the search proceeds, we do not repeat an apparent mistake later
in the search process.

873

ginsberg

formally, we have:

procedure 4.1 let c be a wcsp. let n be a    xed discrepancy limit and suppose that s is
a partial solution, b is the best solution known thus far, and p is the set of values pitched
in the search. to compute solve(c, s,   b, n, p ), the best solution extending s with at most
n discrepancies:

if c(s)     c(b), return b
if s assigns a value to every variable in vc, return s
v     a variable in vc unassigned by s

1
2
3
4 d     an element of dv(c|s) such that (v, d) (cid:54)    p
5 s(cid:48)     s     (v = d)
6 c(cid:48)     propagate(c|s(cid:48))
7
8
9

if c(cid:48) (cid:54)=   , b     solve(c(cid:48), s(cid:48), b, n, p )
if |p| < n, b     solve(c, s, b, n, p     (v, d))
return b

proposition 4.2 let c be a csp of size k. then the value solve(c,   ,   , n,   ) computed
by procedure 4.1 is     if c has no solutions. if c has a solution, there is some n0     k(|d|   1)
such that for any n     n0, solve(c,   ,   , n,   ) is the least cost solution to c.

proof. there are essentially three separate claims in the proposition, which we address
individually.

if c has no solutions, then the test in line 2 will never succeed, so b will be     throughout

1.
and the procedure will therefore return    .

2.
it is clear that the space explored with a larger n is a superset of the space explored
with a smaller n because the test in line 8 will succeed more often. thus if there is any n for
which the best solution is returned, the best solution will also be returned for any larger n.
3. we claim that for n = k(|d|     1), every solution is considered, and prove this by
induction on k.
for k = 1, we have n = |d|     1. if we are interested in a particular choice x for the
unique variable in the problem, then after |d|     1 iterations through line 8, we will either
have selected x on line 4 or we will have pitched every other value in which case x will be
selected on the last iteration.
the argument in the inductive case is similar. for the variable v selected on line 3, we
will use up at most |d|     1 discrepancies before setting the v to the desired value, leaving
at least (n     1)(|d|     1) discrepancies to handle the search in the subproblem after v is
set.

2

proposition 4.3 let c be a csp of size k. then for any    xed n, the number of node
expansions in computing solve(c,   ,   , n,   ) is at most (k + 1)n+1.

proof. consider figure 4, which shows the top of the lds search tree and labels the nodes
with the number of unvalued variables and number of unused discrepancies at each point.

874

dr.fill: a crossword solver

s

(k, n)

           hhhhhhhhhhh
s
s

(k     1, n)
e
%%

%

%

s

%
(k     2, n)

%

e

e

e

s

ee

(k     1, n     1)

%

%

%

s

%

(k     1, n     1)

(k, n     1)
e

%%

e

e

s

ee

e
(k, n     2)

figure 4: limited discrepancy search

at the root, therefore, there are k variables left to value and n discrepancies available. if
we branch left, we assign a value to some variable. if we branch right, we pitch that choice
so that there are still k variables left to value but only n     1 discrepancies available.

it follows that if we denote by f (d, m) the size of the search tree under the point with

d variables and m discrepancies, we have

f (d, m) = 1 + f (d, m     1) + f (d     1, m)

= 1 + f (d, m     1) + 1 + f (d     1, m     1) + f (d     2, m)
= 2 + f (d, m     1) + f (d     1, m     1) + f (d     2, m)
...

= k +

= d +

d(cid:88)
d(cid:88)

i=d   k+1

i=1

d(cid:88)

f (i, m     1) + f (d     k, m)

f (i, m     1) + f (0, m)

f (i, m     1)

= d + 1 +
    d + 1 + (d     1)[f (d, m     1)     1] + f (d, m     1)
= 2 + df (d, m     1)]

i=1

(11)

(12)

(13)

(14)

(15)

(16)

(11) follows from counting the nodes as in the    gure. (12) is the result of expanding the
last term in (11), corresponding to expanding the node labeled (k     1, n) in the    gure.
(13) continues to expand the corresponding term a total of k times, and (14) is just (13)
with k = d. but f (0, m) = 1 because there are no variables left to value, producing (15).
(16) follows because f (i, m    1)     f (d, m    1)    1 for all 0 < i < d (in the    gure, every step
down the left side is at least one node smaller).

875

ginsberg

given f (d, m)     2 + df (d, m     1), we have

f (d, m)
f (d, m     1)

   

2

f (d, m     1)

+ d     1 + d

now f (d, 0) = d+1 because the search must progress directly to the fringe if no discrepancies
remain. thus f (d, m)     (1 + d)1+m. taking m = n and d = k at the root of the tree now
produces the desired result.

2

5. dr.fill as a crossword solver

at this point, we have described enough of dr.fill   s underlying architecture that it makes
sense to report on the performance of the system as described thus far.11

our overall experimental approach is as follows.
first, we tune the word scoring function   . although there are only    ve basic contri-
butions to the value of    for any particular clue and    ll, there are currently twenty-four
tuning parameters that impact both the    ve contributions themselves and the way in which
they are combined to get an overall value for   . as described in section 3, the goal is to
maximize the average number of words entered correctly when beginning to solve any of
the    rst 100 times puzzles of 2010.

this tuning process is time consuming; dr.fill spends approximately one cpu minute
analyzing the clues in any given puzzle to determine the value of    for words in its dictionary.
this analysis often needs to be repeated if the tuning parameters are changed; it follows
that a single run through the testbed of 100 puzzles takes about an hour. the clue analysis
is multithreaded and the work is done on an 8-processor machine (two 2.8ghz quad-core
xeons), which reduces wall clock time considerably, but it remains impractical to sample
the space of parameter values with other than coarse granularity, and the parameters must
in general be tuned independently of one another even though a variety of cross e   ects
undoubtedly exist.

after the tuning is complete, dr.fill is evaluated on the puzzles from the 2010 acpt
(american crossword puzzle tournament). this is a set of only seven puzzles, but algorith-
mic and heuristic progress appear to translate quite well into progress on the acpt sample.
the puzzles are scored according to the acpt rules, and dr.fill   s total score is examined
to determine where it would have ranked had it been a competitor.

the acpt scoring on any particular puzzle is as follows:

1. 10 points for each correct word in the grid,

2. 25 bonus points for each full minute of time remaining when the puzzle was completed.
this bonus is reduced by 25 points for each incorrect letter, but can never be negative.

3. 150 bonus points if the puzzle is solved correctly.

11. dr.fill is written in c++ and currently runs under macos 10.6.

it needs approximately 3.5 gb
of free memory to run, and is multithreaded. the multithreading uses posix threads and the gui
is written using wxwidgets (www.wxwidgets.org). the code and underlying data can be obtained (for
noncommercial use only) by contacting the author. the code can be expected to run virtually unchanged
under linux; windows will be more of a challenge because windows has no native support for posix
threads.

876

dr.fill: a crossword solver

version

lds

postprocess

and/or

best human

1

1280
1280
1280
1230

2
925
1185
1185
1615

3

1765
1790
1815
1930

4

1140
1165
1165
1355

5

1690
1690
1690
1565

6

2070
2070
2095
1995

7

1920
2030
2080
2515

total
10790
11210
11310
12205

rank

89 (tied)
43 (tied)

38
1

table 2: results from the 2010 acpt

since the puzzles are timed, dr.fill needs some sort of termination condition. it stops
work and declares its puzzle complete if any of the following conditions occur:

1. a full minute goes by with no improvement in the cost of the puzzle as currently    lled,

2. a full lds iteration goes by with no improvement in the cost of the puzzle as currently

   lled, or

3. the acpt time limit for the puzzle is reached.

results for this and other versions of dr.fill appear in table 2, with scores by puzzle,
total score for the tournament, and ranking had dr.fill competed. we also give scores for
the human (dan feyer) who won the event.12 the    rst and fourth puzzles are generally the
easiest, and the second and    fth puzzles are the hardest. the lds-based dr.fill scored a
total of 10,790, good enough for 89th place.

after the evaluation is complete, an attempt is generally made to improve dr.fill   s
performance. we examine puzzles from the times testbed (not the acpt puzzles, which we
try to keep as    clean    as possible) and try to understand why mistakes were made. these
mistakes can generally be classi   ed as one of three types:

1. heuristic errors, in that the words entered scored better than the correct ones even

though they were not the correct    ll,

2. search errors, where the words entered scored worse than the correct ones but dr.fill

did not    nd a better    ll because the discrepancy limit was reached, and

3. e   ciency    errors   , where points were lost because the search took a long time to

complete.

heuristic errors generally lead to a change in the scoring algorithms in some way, al-
though generally not to the introduction of new scoring modules. perhaps a di   erent
thesaurus is used, or the understanding of theme entries changes. search errors may lead
to modi   cations of the underlying search algorithm itself, as in sections 6 and 7. dr.fill
has a graphical user interface that allows the user to watch the search proceed, and this is
often invaluable in understanding why the program performed as it did. e   ciency issues
can also (sometimes) be corrected by allowing the visual search to suggest algorithmic mod-
i   cations; this convinced us that it was worthwhile to treat the overall csp as an and/or
tree as discussed in section 7.

12. feyer went on to win in 2011 as well; tyler hinman was the acpt champion from 2005   2009.

877

ginsberg

6. postprocessing

an examination of dr.fill   s completed puzzles based on the algorithms presented thus
far reveals many cases where a single letter is wrong, and the problem is with the search
instead of the heuristics. in other words, replacing the given letter with the    right    one
decreases the total cost of the puzzle   s    ll. this would presumably have been found with a
larger discrepancy limit, but was not discovered in practice.

this suggests that dr.fill would bene   t from some sort of postprocessing. the sim-
plest approach is to simply remove each word from the    ll, and replace it with the best word
for the slot in question. if this produces a change, the process is repeated until quiescence.

6.1 formalization and algorithmic integration

we can formalize this process easily as follows:

do change     false
for each v     cv
do b(cid:48)     b

procedure 6.1 given a csp c and a best solution b, to compute post(c, b), the result
of attempting to improve b with postprocessing:
1 change     true
2 while change
3
4
5
6
7
8
9
10
11 return b

unset the value of v in b(cid:48)
b(cid:48)     solve(c, b(cid:48), b(cid:48))
if c(b(cid:48)) < c(b)
then b     b(cid:48)

change     true

we work through the puzzle, erasing each word in line 6. we then re-solve the puzzle
(line 7), so that if there is a better choice for that word in isolation, it will be found. if this
leads to an improvement, we set a    ag on line 10 and repeat the entire process. note that
we only erase one word at a time, since we always begin with the currently best solution in
line 5.

as with ac-3, procedure 6.1 can be improved somewhat by realizing that on any par-
ticular iteration, we need only examine variables that share a constraint with a variable
changed on the previous iteration. in practice, so little of dr.fill   s time is spent postpro-
cessing that e   ciency here is not a concern.
lemma 6.2 for any csp c and solution b, c(post(c, b))     c(b).

2

how are we to combine procedure 6.1 with the basic search procedure 4.1 used by
dr.fill itself? we can obviously postprocess the result computed by procedure 4.1 before
returning it as our    nal answer, but if postprocessing works e   ectively, we should surely
postprocess all of the candidate solutions considered. that produces:

878

dr.fill: a crossword solver

procedure 6.3 let c be a wcsp. let n be a    xed discrepancy limit and suppose that s is
a partial solution, b is the best solution known thus far, and p is the set of values pitched
in the search. to compute solve(c, s,   b, n, p ), the best solution extending s with at most
n discrepancies:

if c(s)     c(b), return b
if s assigns a value to every variable in vc, return post(c, s)
v     a variable in vc unassigned by s

1
2
3
4 d     an element of dv(c|s) such that (v, d) (cid:54)    p
5 s(cid:48)     s     (v = d)
6 c(cid:48)     propagate(c|s(cid:48))
7
8
9

if c(cid:48) (cid:54)=   , b     solve(c(cid:48), s(cid:48), b, n, p )
if |p| < n, b     solve(c, s, b, n, p     (v, d))
return b

the only di   erence between this and procedure 4.1 is on line 2, where we postprocess the
solution before returning it.

6.2 interaction with branch and bound

further thought reveals a potential problem with this approach. suppose that our original
procedure 4.1    rst produces a solution b1 and subsequently produces an improvement b2,
with c(b2) < c(b1). suppose also that postprocessing improves both solutions comparably,
so that c(post(b2)) < c(post(b1)). and    nally, suppose that postprocessing improves the
solutions considerably, so much so, in fact, that c(post(b1)) < c(b2).

we are now in danger of missing b2, since it will be pruned by the test on line 1 of
procedure 6.3. b2 will allow us to    nd a better solution, but only after postprocessing. if
we prune b2 early, we will never postprocess it, and the improvement will not be found
until a larger discrepancy limit is used.

this suggests that we return to the earlier possibility of postprocessing only the    nal
answer returned by procedure 4.1, but that may not work, either. perhaps b1 is improved
by postprocessing and b2 is not; once again, the best solution may be lost.

the problem is that branch-and-bound and postprocessing are fundamentally inconsis-
tent; it is impossible to use both e   ectively. the very idea of branch-and-bound is that
a solution can be pruned before it is complete if its cost gets too large. the very idea
of postprocessing is that the    nal cost of a solution cannot really be evaluated until the
solution is complete and the postprocess has been run.

our    solution    to this is to remove branch and bound from dr.fill   s search algorithm,

producing:

879

ginsberg

procedure 6.4 let c be a wcsp. let n be a    xed discrepancy limit and suppose that s is
a partial solution, b is the best solution known thus far, and p is the set of values pitched
in the search. to compute solve(c, s,   b, n, p ), the best solution extending s with at most
n discrepancies:

return whichever of b and post(c, s) has lower cost

if s assigns a value to every variable in vc,
v     a variable in vc unassigned by s

1
2
3
4 d     an element of dv(c|s) such that (v, d) (cid:54)    p
5 s(cid:48)     s     (v = d)
6 c(cid:48)     propagate(c|s(cid:48))
7
8
9

if c(cid:48) (cid:54)=   , b     solve(c(cid:48), s(cid:48), b, n, p )
if |p| < n, b     solve(c, s, b, n, p     (v, d))
return b

since the test on line 2 ensures that we only change the best solution    b when an
improvement is found, all of our previous results continue to hold. but is it really practical
to abandon branch and bound as a mechanism for controlling the size of the search?

it is. one reason is that the size of the search is now being controlled by lds via
proposition 4.3. for any    xed discrepancy limit n, this guarantees that the number of
nodes expanded is polynomial in the size of the problem being solved.

more important, however, is that experimentation showed that branch-and-bound was
ine   ective in controlling dr.fill   s search. the reason is the e   ectiveness of the (search-
anticipating) heuristics used in dr.fill itself. these heuristics are designed to ensure that
the words inserted early in the search both incur little cost themselves and allow crossing
words to incur low cost as well. what happens in practice is that the costs incurred early
are extremely modest. even when a mistake is made, attention typically changes to a
di   erent part of the puzzle because    lling an additional word w near the mistake begins
to have consequences on the expected cost of the words crossing w. eventually, the rest
of the puzzle is complete and the algorithm    nally begrudgingly returns to w and the cost
increases.

thinking about this, what happens is that while the cost does eventually increase when
an error is made, the increase is deferred until the very bottom of the search tree, or nearly
so. with so much of the cost almost invariably accumulating at the bottom the search tree,
branch and bound is simply an ine   ective pruning tool in this domain. the nature of the
argument suggests that in other wcsps that are derived from real-world problems, good
heuristics may exist and branch and bound may provide little value in practical problem
solving.13

6.3 results
the results of procedure 6.4 appear in table 2. dr.fill   s score improves to 11,210, which
would have earned it a tie for 43rd place in the 2010 tournament.

13. that said, there are certainly real-world problems where branch-and-bound is useful, such as the use of

mendelsoft to solve cattle pedigree problems (sanchez, de givry, & schiex, 2008).

880

dr.fill: a crossword solver

7. and/or search

there is one further algorithmic improvement that is part of dr.fill as the system is
currently implemented.

as we watched dr.fill complete puzzles, there were many cases where it would    ll
enough of the puzzle that the residual problem would split into two disjoint subproblems.
the search would then frequently oscillate between these two subproblems, which could
clearly introduce ine   ciencies.

this general observation has been made by many others, and probably originates with
freuder and quinn (1985), who called the variables in independent subproblems stable
sets. mcallester (1993) calls a solution technique a polynomial space aggressive backtracking
procedure if it solves disjoint subproblems in time that is the sum of the times needed for
the subproblems independently. most recently, marinescu and dechter (2009) explore this
notion in the context of constraint propagation speci   cally, exploiting the structure of the
associated search spaces as and/or graphs.

none of this work is directly applicable to dr.fill because it needs to be integrated

appropriately with lds. but the integration itself is straightforward:

de   nition 7.1 let c be a csp or wcsp. we will say that c splits if there are nonempty
v1, v2     vc such that v1     v2 =   , v1     v2 = vc, and no constraint or weighted constraint
in c mentions variables in both v1 and v2. we will denote this as c = c|v1 + c|v2.
proposition 7.2 suppose that c is a csp that splits into v1 and v2. then if s1 is a
solution to c|v1 and s2 is a solution to c|v2, s1     s2 is a solution to c, and all solutions
to c can be constructed in this fashion.
cost solutions to c|v1 and c|v2.

in addition, if c is a wcsp, then the least cost solution to c is the union of the least

2

note also that we can check to see if c splits in low order polynomial time by checking
to see if the constraint graph associated with c is connected. if so, c does not split. if the
constraint graph is disconnected, c splits.

procedure 7.3 let c be a wcsp. let n be a    xed discrepancy limit and suppose that s is
a partial solution, b is the best solution known thus far, and p is the set of values pitched
in the search. to compute solve(c, s,   b, n, p ), the best solution extending s with at most
n discrepancies:

return whichever of b and post(c, s) has lower cost
return solve(c|v , s|v , b|v , n, p )     solve(c|w , s|w , b|w , n, p )

if s assigns a value to every variable in vc,

if c splits into v and w ,
v     a variable in vc unassigned by s

1
2
3
4
5
6 d     an element of dv(c|s) such that (v, d) (cid:54)    p
7 s(cid:48)     s     (v = d)
8 c(cid:48)     propagate(c|s(cid:48))
9
10 if |p| < n, b     solve(c, s, b, n, p     (v, d))
11 return b

if c(cid:48) (cid:54)=   , b     solve(c(cid:48), s(cid:48), b, n, p )

881

ginsberg

dr.fill

feyer

puzzle words letters words wrong letters wrong time

1
2
3
4
5
6
7

total

78
94
118
78
94
122
144
643

185
237
301
187
245
289
373
1817

0
8
4
4
0
0
11
27

0
11
2
2
0
0
13
28

1
2
2
2
1
1
2
11

score time
1280
1185
1815
1165
1690
2095
2080
11310

3
4
6
3
6
5
8
35

score
1230
1615
1930
1355
1565
1995
2515
12205

table 3: results from the 2010 acpt

note that in line 4, we solve each of the split subproblems with a discrepancy limit of n.
so if (for example) we currently have n = 3 with one discrepancy having been used at the
point that the split occurs, we will be allowed two additional discrepancies in solving each
subproblem, perhaps allowing    ve discrepancies in total.

in spite of this, the node count will be reduced. if there are d variables remaining when
the split is encountered, solving the unsplit problem with m remaining discrepancies might
expand (1 + d)1+m nodes (proposition 4.1), while solving the split problems will expand at
most

(1 + d1)1+m + (1 + d     d1)1+m

(17)
nodes. a small amount of calculus and algebra14 shows that (1+d1)1+m +(1 + d    d1)1+m    
(1 + d)1+m for m     1, so that the split search will be faster even though more total
discrepancies are permitted.

the change embodied in procedure 7.3 signi   cantly improves performance on later
lds iterations, and it is arguable that we should exploit this improvement by modifying
dr.fill   s current strategy of terminating the search when an increase in the lds limit
does not produce an improved solution. even without such modi   cation, the increased
speed of solution improves dr.fill   s acpt score by 100 points (one minute faster on puz-
zles 3 and 6, and two minutes faster on puzzle 7), moving it up to a notional 38th place in
the 2010 event.

detailed performance of this    nal version on the 2010 puzzles is shown in table 3. for
each puzzle, we give the number of words and letters to be    lled, and the number of errors
made by dr.fill in each area. we also give the time required by dr.fill to solve the
program (in minutes taken), along with the time taken by dan feyer, the human winner of
the contest. (feyer made no errors on any of the seven puzzles.) as can be seen, dr.fill
had 27 incorrect words (out of 643, 95.8% correct) and 28 incorrect letters (out of 1817,
98.5% correct) over the course of the event.

14. di   erentiating (17) shows that the worst case for the split is d1 = 1, so we have to compare 21+m + d1+m
and (d + 1)1+m. multiplying out (d + 1)1+m produces d1+m + (1 + m)dm +       , and 21+m < (1 + m)dm
if d     2 and m     1.

882

dr.fill: a crossword solver

8. related and future work

there is a wide variety of work on wcsps in the academic literature, and we will not repeat
any particular element of that work here. what distinguishes our contribution is the fact
that we have been driven by results on a naturally occurring problem: that of solving
crossword puzzles. this has led us to the following speci   c innovations relative to earlier
work:

    the development of a value selection heuristic based on the projected cost of assigning
a value both to the currently selected variable and to all variables with which this
variable shares a constraint,

    the development of a variable selection heuristic that compares the di   erence between
the projected cost impacts of the best and second-best values, and branches on the
variable for which this di   erence is maximized,

    a modi   cation of limited discrepancy search that appears to work well for weighted

csps with large domain sizes,

    the recognition that branch-and-bound may not be an e   ective search technique in

wcsps for which reasonably accurate heuristics exist, and

    the development and inclusion of an e   ective postprocessing algorithm for wcsps,
and the recognition that such postprocessing is inconsistent with branch-and-bound
pruning.

we do not know the extent to which these observations are general, and the extent to
which they are a consequence of the properties of the crossword csp itself. as discussed
previously, crossword csps have a relatively small number of variables but almost unlimited
domain sizes, and variables whose valuations incur signi   cant cost should in general be    lled
late as opposed to early.

the two existing projects that most closely relate to dr.fill are proverb (littman
et al., 2002), the crossword solver developed by littman et. al in 1999, and watson (fer-
rucci et al., 2010), the jeopardy-playing robot developed by ibm in 2011. all three systems
(watson, proverb, and dr.fill) respond to natural language queries in a game-like
setting. in all three cases, the programs seem to have very little idea what they are doing,
primarily combining candidate answers from a variety of data sources and attempting to
determine which answer is the best match for the query under consideration. this appears
to mesh well with the generally accepted view (manning & schuetze, 1999) that natural
language processing is far better accomplished using statistical methods than by a more
classical    parse-and-understand    approach.

the domain di   erences between jeopardy and crosswords make the problems challenging
in di   erent ways. in one sense, crosswords are more di   cult because in jeopardy, one is
always welcome to simply decline to answer any particular question.
in crosswords, the
entire grid must be    lled. on the other hand, the crossing words in a crossword restrict
the answer in a way that is obviously unavailable to jeopardy contestants. search plays
a key role in dr.fill   s performance in a way that watson cannot exploit. as a result,
dr.fill can get by with relatively limited database and computational resources. the

883

ginsberg

program runs on a 2-core notebook with 8 gb of memory and uses a database that is just
over 300 mbytes when compressed. watson needs much more: 2880 cores and 16 tb of
memory. watson, like dr.fill, stores all of its knowledge in memory to improve access
speeds     but watson relies on much more extensive knowledge than does dr.fill.

the programs are probably comparably good at their respective cognitive tasks. dr.fill
outperforms all but the very best humans in crossword    lling, both in terms of speed (where
it is easily the fastest solver in the world) and in terms of accuracy. watson, too, outper-
forms humans easily in terms of speed; its much-ballyhooed victory against human jeopardy
competitors was probably due far more to watson   s mastery of button pushing than to its
question-answering ability. in terms of the underlying cognitive task, watson appears to
not yet be a match for the best jeopardy players, who are in general capable of answering
virtually all of the questions without error.

dr.fill itself remains a work in progress. until this point, we have found heuristic and
search errors relatively easily by examining the performance of the program on a handful of
crosswords and simply seeing what went wrong. as dr.fill   s performance has improved,
this has become more di   cult. we have therefore developed automated tools that examine
the errors made on a collection of puzzles, identify them as heuristic or search issues, and
report the nature of the errors that caused mistakes in the largest sections of    ll. the results
of these tools will, we hope, guide us in improving dr.fill   s performance still further.

acknowledgments

i would like to thank my on time systems coworkers for useful technical advice and as-
sistance, and would also like to thank the crossword solving and constructing communities,
especially will shortz, for their warm support over the years. daphne koller, rich korf,
michael littman, thomas schiex, bart selman, and this paper   s anonymous reviewers pro-
vided me with invaluable comments on earlier drafts, making the paper itself substantially
stronger as a result. the work described in this paper relates to certain pending and issued
us patent applications, and the publication of these ideas is not intended to convey a li-
cense to use any patented information or processes. on time systems will in general grant
royalty-free licenses for non-commercial purposes.

references

bistarelli, s., montanari, u., rossi, f., schiex, t., verfaillie, g., & fargier, h. (1999).
semiring-based csps and valued csps: frameworks, properties, and comparison.
constraints, 4.

boussemart, f., hemery, f., lecoutre, c., & sais, l. (2004). boosting systematic search by

weighting constraints. in proceedings of ecai-2004, pp. 146   150.

campbell, m., hoane, a. j., & hsu, f. (2002). deep blue. arti   cial intelligence, 134,

57   83.

cooper, m., de givry, s., sanchez, m., schiex, t., zytnicki, m., & werner, t. (2010). soft

arc consistency revisited. arti   cial intelligence, 174, 449   478.

doyle, j. (1979). a truth maintenance system. arti   cial intelligence, 12, 231   272.

884

dr.fill: a crossword solver

ernandes, m., angelini, g., & gori, m. (2005). webcrow: a web-based system for cross-
word solving. in proceedings of the twentieth national conference on arti   cial in-
telligence, pp. 1412   1417.

faltings, b., & macho-gonzalez, s. (2005). open constraint programming. arti   cial intel-

ligence, 161, 181   208.

fellbaum, c. (ed.). (1998). id138: an electronic lexical database. mit press, cam-

bridge, ma.

ferrucci, d., brown, e., chu-carroll, j., fan, j., gondek, d., kalyanpur, a. a., lally, a.,
murdock, j. w., nyberg, e., prager, j., schlaefer, n., & welty, c. (2010). building
watson: an overview of the deepqa poject. ai magazine, 31 (3), 59   79.

freuder, e. c., & quinn, m. j. (1985). taking advantage of stable sets of variables in
id124 problems. in proceedings of the ninth international joint con-
ference on arti   cial intelligence, pp. 224   229.

ginsberg, m. l. (2001). gib: steps toward an expert-level bridge-playing program. journal

of arti   cial intelligence research, 14, 313   368.

ginsberg, m. l., frank, m., halpin, m. p., & torrance, m. c. (1990). search lessons learned
from crossword puzzles. in proceedings of the eighth national conference on arti   cial
intelligence, pp. 210   215.

givry, s. d., & zytnicki, m. (2005). existential arc consistency: getting closer to full arc
consistency in weighted csps. in proceedings of the nineteenth international joint
conference on arti   cial intelligence, pp. 84   89.

harvey, w. d. (1995). nonsystematic backtracking search. ph.d. thesis, stanford univer-

sity, stanford, ca.

harvey, w. d., & ginsberg, m. l. (1995). limited discrepancy search. in proceedings of
the fourteenth international joint conference on arti   cial intelligence, pp. 607   613.

joslin, d. e., & clements, d. p. (1999). squeaky wheel optimization. journal of arti   cial

intelligence research, 10, 353   373.

korf, r. e. (1996). improved limited discrepancy search. in proceedings of the thirteenth

national conference on arti   cial intelligence, pp. 286   291.

larrosa, j., & dechter, r. (2000). on the dual representation of non-binary semiring-based

csps. in proceedings soft-2000.

larrosa, j., & schiex, t. (2004). solving weighted csp by maintaining arc consistency.

arti   cial intelligence, 159, 1   26.

lecoutre, c., sa    s, l., tabary, s., & vidal, v. (2009). reasoning from last con   ict(s) in

constraint programming. arti   cial intelligence, 173, 1592   1614.

littman, m. l., keim, g. a., & shzaeer, n. (2002). a probabilistic approach to solving

crossword puzzles. arti   cial intelligence, 134, 23   55.

mackworth, a. k. (1977). consistency in networks of relations. arti   cial intelligence, 8,

99   118.

885

ginsberg

manning, c. d., & schuetze, h. (1999). foundations of statistical natural language pro-

cessing. mit press.

marinescu, r., & dechter, r. (2009). and/or branch-and-bound search for combinatorial

optimization in id114. arti   cial intelligence, 173, 1457   1491.

mcallester, d. a. (1993). partial order backtracking. unpublished technical report, mit.

miller, g. a. (1995). id138: a lexical database for english. communications of the

acm, 38 (11), 39   41.

sanchez, m., de givry, s., & schiex, t. (2008). mendelian error detection in complex

pedigrees using weighted id124 techniques. constraints, 13.

schae   er, j., treloar, n., lu, p., & lake, r. (1993). man versus machine for the world

checkers championship. ai magazine, 14 (2), 28   35.

smith, s. j., nau, d. s., & throop, t. (1996). total-order multi-agent task-network plan-
in proceedings of the thirteenth national conference on

ning for contract bridge.
arti   cial intelligence, stanford, california.

sontag, d., globerson, a., & jaakkola, t. (2011). introduction to id209 for
id136. in sra, s., nowozin, s., & wright, s. j. (eds.), optimization for machine
learning, pp. 219   254. mit press.

zytnicki, m., gaspin, c., de givry, s., & schiex, t. (2009). bounds arc consistency for

weighted csps. journal of arti   cial intelligence research, 35, 593   621.

886

