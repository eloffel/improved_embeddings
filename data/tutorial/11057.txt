probabilistic frame induction   

jackie chi kit cheung   

department of computer science

university of toronto

toronto, on, m5s 3g4, canada

jcheung@cs.toronto.edu

hoifung poon

one microsoft way
microsoft research

redmond, wa 98052, usa
hoifung@microsoft.com

lucy vanderwende
one microsoft way
microsoft research

redmond, wa 98052, usa

lucyv@microsoft.com

3
1
0
2

 

b
e
f
0
2

 

 
 
]
l
c
.
s
c
[
 
 

1
v
3
1
8
4

.

2
0
3
1
:
v
i
x
r
a

abstract

in natural-language discourse, related events
tend to appear near each other to describe a
larger scenario. such structures can be formal-
ized by the notion of a frame (a.k.a. template),
which comprises a set of related events and
prototypical participants and event transitions.
identifying frames is a prerequisite for infor-
mation extraction and natural language gen-
eration, and is usually done manually. meth-
ods for inducing frames have been proposed
recently, but they typically use ad hoc proce-
dures and are dif   cult to diagnose or extend.
in this paper, we propose the    rst probabilis-
tic approach to frame induction, which incor-
porates frames, events, participants as latent
topics and learns those frame and event tran-
sitions that best explain the text. the number
of frames is inferred by a novel application of
a split-merge method from syntactic parsing.
in end-to-end evaluations from text to induced
frames and extracted facts, our method pro-
duced state-of-the-art results while substan-
tially reducing engineering effort.

introduction

1
events with causal or temporal relations tend to oc-
cur near each other in text. for example, a bomb-
ing scenario in an article on terrorism might begin
with a detonation event, in which terrorists set
   this is a postprint version of a paper to appear in the pro-
ceedings of the 2013 conference of the north american chap-
ter of the association for computational linguistics: human
language technologies (naacl/hlt 2013).
   this research was undertaken during the author   s internship

at microsoft research.

off a bomb. then, a damage event might ensue
to describe the resulting destruction and any casual-
ties, followed by an investigation event cov-
ering subsequent police investigations. afterwards,
the bombing scenario may transition into a criminal-
processing scenario, which begins with police catch-
ing the terrorists, and proceeds to a trial, sentenc-
ing, etc. a common set of participants serves as
the event arguments; e.g., the agent (or subject) of
detonation is often the same as the theme (or ob-
ject) of investigation and corresponds to the
perpetrator.

such structures can be formally captured by the
notion of a frame (a.k.a. template), which consists
of a set of events with prototypical transitions, as
well as a set of slots representing the common par-
ticipants.
identifying frames is an explicit or im-
plicit prerequisite for many nlp tasks.
informa-
tion extraction, for example, stipulates the types of
events and slots that are extracted for a frame or
template. online applications such as dialogue sys-
tems and personal-assistant applications also model
users    goals and subgoals using frame-like represen-
tations, and in natural-language generation, frames
are often used to represent content to be expressed
as well as to support surface realization.

until recently, frames and related representations
have been manually constructed, which has limited
their applicability to a relatively small number of do-
mains and a few slots within a domain. furthermore,
additional manual effort is needed after the frames
are de   ned in order to extract frame components
from text (e.g., in annotating examples and design-
ing features to train a supervised learning model).

this paradigm makes it hard to generalize across
tasks and might suffer from annotator bias.

recently, there has been increasing interest in au-
tomatically inducing frames from text. a notable
example is chambers and jurafsky (2011), which
   rst clusters related verbs to form frames, and then
clusters the verbs    syntactic arguments to identify
slots. while chambers and jurafsky (2011) repre-
sents a major step forward in frame induction, it is
also limited in several aspects. the id91 used
ad hoc steps and customized similarity metrics, as
well as an additional retrieval step from a large ex-
ternal text corpus for slot generation. this makes it
hard to replicate their approach or adapt it to new
domains. lacking a coherent model, it is also dif   -
cult to incorporate additional linguistic insights and
prior knowledge.

in this paper, we present profinder (proba-
bilistic frame inducer), which is the    rst proba-
bilistic approach for frame induction. profinder
de   nes a joint distribution over the words in a
document and their frame assignments by model-
ing frame and event transition, correlations among
events and slots, and their surface realizations.
given a set of documents, profinder outputs a
set of induced frames with learned parameters, as
well as the most probable frame assignments that
can be used for event and entity extraction. the
numbers of events and slots are dynamically deter-
mined by a novel application of the split-merge ap-
proach from syntactic parsing (petrov et al., 2006).
in end-to-end evaluations from text to entity ex-
traction using the standard muc and tac datasets,
profinder achieved state-of-the-art results while sig-
ni   cantly reducing engineering effort and requiring
no external data.

2 related work

in information extraction and other semantic pro-
cessing tasks, the dominant paradigm requires two
stages of manual effort. first, the target representa-
tion is de   ned manually by domain experts. then,
manual effort is required to construct an extractor or
annotate examples to train a machine-learning sys-
tem. recently, there has been a burgeoning body
of work in alleviating such manual effort. for exam-
ple, a popular approach to reduce annotation effort is

id64 from seed examples (patwardhan and
riloff, 2007; huang and riloff, 2012). however,
this still requires prespeci   ed frames or templates,
and selecting seed words is often a challenging task
due to semantic drift (curran et al., 2007). open
ie (banko and etzioni, 2008) reduces the manual
effort to designing a few domain-independent rela-
tion patterns, which can then be applied to extract
relational triples from text. while extremely scal-
able, this approach can only extract atomic factoids
within a sentence, and the resulting triples are noisy,
non-cannonicalized text fragments.

more relevant to our approach is the recent work
in unsupervised semantic induction, such as un-
supervised id29 (poon and domingos,
2009), unsupervised semantical role labeling (swier
and stevenson, 2004) and induction (lang and lap-
ata, 2011, e.g.), and slot induction from web search
logs (cheung and li, 2012). as in profinder,
they also model distributional contexts for slot or
role induction. however, these approaches focus on
semantics in independent sentences, and do not cap-
ture discourse-level dependencies.

the modeling component for frame and event
transitions in profinder is similar to a sequen-
tial topic model (gruber et al., 2007), and is in-
spired by the successful applications of such topic
models in summarization (barzilay and lee, 2004;
daum  e iii and marcu, 2006; haghighi and vander-
wende, 2009, inter alia). there are, however, two
main differences. first, profinder contains not
a single sequential topic model, but two (for frames
and events, respectively). in addition, it also mod-
els the interdependencies among events, slots, and
surface text, which is analogous to the usp model
(poon and domingos, 2009). profinder can thus
be viewed as a novel combination of state-of-the-
art models in unsupervised semantics and discourse
modeling.

in terms of aim and capability, profinder
is most similar to chambers and jurafsky (2011),
which culminated from a series of work for iden-
tifying correlated events and arguments in narrative
(chambers and jurafsky, 2008; chambers and ju-
rafsky, 2009). by adopting a probabilistic approach,
profinder has a sound theoretical underpinning,
and is easy to modify or extend. for example, in
section 3, we show how profinder can easily be

augmented with additional linguistically-motivated
features. likewise, profinder can easily be used
as a semi-supervised system if some slot designa-
tions and labeled examples are available.

the idea of representing and capturing stereotyp-
ical knowledge has a long history in arti   cial in-
telligence and psychology, and has assumed vari-
ous names such as frames (minsky, 1974), schemata
(rumelhart, 1975), and scripts (schank and abel-
son, 1977).
in the linguistics and computational
linguistics communities, frame semantics (fillmore,
1982) uses frames as the central representation of
word meaning, culminating in the development of
framenet (baker et al., 1998), which contains over
1000 manually annotated frames. a similarly rich
lexical resource is the mindnet project (richard-
son et al., 1998). our notion of frame is related
to these representations, but there are also subtle
differences. for example, minsky   s frame empha-
sizes inheritance, which we do not model in this pa-
per. (it should be a straightforward extension: us-
ing the split-and-merge approach, profinder already
produces a hierarchy of events and slots in learning,
although currently, it simply discards the intermedi-
ate levels.) as in id14, framenet
focuses on semantic roles and does not model event
or frame transitions, so the scope of its frames is of-
ten no more than an event in our model. perhaps
the most similar to our frame is roger schank   s
scripts, which capture prototypical events and par-
ticipants in a scenario such as restaurant dining. in
their approach, however, scripts are manually de-
   ned, making it hard to generalize. in this regard,
our work may be viewed as an attempt to revive a
long tradition in ai and linguistics, by leveraging
the recent advances in computational power, nlp,
and machine learning.

3 probabilistic frame induction

in this section, we present profinder, a proba-
bilistic model for frame induction. let f be a set of
frames, where each frame f = (ef , sf ) comprises
a unique set of events ef and slots sf . given a doc-
ument d and a word w in d, zw = (f, e) represents
an assignment of w to frame f     f and frame el-
ement e     ef     sf . at the heart of profinder
is a generative model p  (d, z) that de   nes a joint

distribution over document d and the frame assign-
ment to its words z. given a set of documents d,
frame induction in profinder amounts to determin-
ing the number of frames, events and slots, as well
as learning the parameters    by summing out the la-
tent assignments z to maximize the likelihood of the
document set

(cid:89)

d   d

p  (d).

the induced frames identify the key event structures
in the document set. additionally, profinder
can also conduct event and entity extraction by
computing the most probable frame assignment z.
in the remainder of the section, we    rst present
the base model for profinder. we then intro-
duce several linguistically motivated re   nements,
and ef   cient algorithms for learning and id136
in profinder.

3.1 base model
the probabilistic formulation of profinder
makes it extremely    exible for incorporating lin-
guistic intuition and prior knowledge. in this paper,
we design our profinder model to capture three
types of dependencies.
frame transitions between clauses a sentence
contains one or more clauses, each of which is a
minimal unit expressing a proposition. a clause is
unlikely to straddle across different frames, so we
stipulate that the words in a clause be assigned to
the same frame. on the other hand, frame transitions
can happen between clauses, and we adopt the com-
mon markov assumption that the frame of a clause
only depends on the clause immediately to its left.
here, sentences are ordered sequentially as they ap-
pear in the documents. clauses are automatically
extracted from the dependency parse and further de-
composed into an event head and its syntactic argu-
ments; see the experiment section for details.
event transitions within a frame events tend to
transition into related events in the same frame, as
determined by their causal or temporal relations.
each clause is assigned an event compatible with
its frame assignment (i.e., the event is in the given
frame). as for frame transitions, we assume that the
event assignment of a clause depends only on the
event of the previous clause.

emission of event heads and slot words similar
to topics in topic models, each event determines a
multinomial from which the event head is generated.
e.g., a detonation event might use verbs such as det-
onate, set off or nouns such as denotation, bombing
as its event head. additionally, as in usp (poon and
domingos, 2009), an event also contains a multino-
mial of slots for each of its argument types1. e.g.,
the agent argument of a detonation event is generally
the perpetrator slot of the bombing frame. fi-
nally, each slot has its own multinomials for gener-
ating the argument head and dependency label, re-
gardless of the event.
formally, let d be a document and c1,       , cl be

its clauses, the profinder model is de   ned by

p  (d, z) = pf   init(f1)   (cid:89)

pf   tran(fi+1|fi)

   pe   init(e1|f1)

i

i

  (cid:89)
  (cid:89)
  (cid:89)
  (cid:89)
  (cid:89)

i,j

i,j

i

pe   tran(ei+1|ei, fi+1, fi)
pe   head(ei|ei)
pslot(si,j|ei,j, ai,j)

pa   head(ai,j|si,j)

pa   dep(depi,j|si,j)

i,j

here, fi, ei denote the frame and event assign-
ment to clause ci, respectively, and ei denotes the
event head. for the j-th argument of clause i,
si,j denotes the slot assignment, ai,j the argument
type, ai,j the head word, and depi,j the dependency
from the event head. pe   tran(ei+1|ei, fi+1, fi) =
pe   init(ei+1|fi+1) if fi+1 (cid:54)= fi.

essentially, profinder combines a frame
id48 with an event id48, where the    rst mod-
els frame transition and emits events, and the second
models event transition within a frame and emits ar-
gument slots.

3.2 model re   nements
the base model captures the main dependencies in
event narrative, but it can be easily extended to lever-
age additional linguistic intuition. profinder in-
corporates three such re   nements.

background frame event narratives often con-
tain interjections of general content common to all
frames. for example, in newswire articles, attri-
bution is commonplace to describe who said or
reported a particular quote or fact. to avoid con-
taminating frames with generic content, we intro-
duce a background frame with its own events, slots,
and emission distributions, and a binary switch vari-
able bi     {bkg, cn t} that determines whether
clause i is generated from the actual content frame
fi (cn t ) or background (bkg). we also stipu-
late that if background is chosen, the nominal frame
stays the same as the previous clause.

stickiness in frame and event transitions prior
work has demonstrated that promoting topic coher-
ence in natural-language discourse helps discourse
modeling (barzilay and lee, 2004). we extend
profinder to leverage this intuition by incorpo-
rating a    stickiness    prior (haghighi and vander-
wende, 2009) to encourage neighboring clauses to
stay in the same frame. speci   cally, along with in-
troducing the background frame, the frame transi-
tion component now becomes
pf   tran(fi+1|fi, bi+1) =

(1)

               1(fi+1 = fi),

  1(fi+1 = fi)+
(1       )pf   tran(fi+1|fi),

if bi+1 = bkg

if bi+1 = cn t

where    is the stickiness parameter, and the event
transition component correspondingly becomes
pe   tran(ei+1|ei, fi+1, fi, bi+1) =

(2)

               1(ei+1 = ei),

pe   tran(ei+1|ei),
pe   init(ei+1),

if bi+1 = bkg
if bi+1 = cn t, fi = fi+1
if bi+1 = cn t, fi (cid:54)= fi+1

1usp generates the argument types along with events from
id91. for simplicity, in profinder we simply classify
a syntactic argument into subject, object, and prepositional ob-
ject, according to its stanford dependency to the event head.

argument dependencies as caseframes as no-
ticed in previous work such as chambers and ju-
rafsky (2011), the combination of an event head

5. for each clause in each document, generate the

clause-internal structure.

the clause-internal structure at clause i is gener-

ated by the following steps:

1. generate whether this clause is background

(bi     {cn t, bkg}     pbkg(b))

2. generate the frame fi and event ei from
pf   init(f ), pe   init(e), or according to
equations 1 and 2
pe   head(ei|ei).

3. generate the observed event head ei from

slot

(a) generate

4. for each event argument:
the
pslot(s|e, a, b).
(b) generate the dependency/caseframe emis-
sion depi,j     pa   dep(dep|s) and the
lemma of the head word of the event ar-
gument ai,j     pa   head(a|s).

from

si,j

3.4 learning and id136
our generative model admits ef   cient id136 by
id145. in particular, after collaps-
ing the latent assignment of frame, event, and back-
ground into a single hidden variable for each clause,
the expectation and most probable assignment can
be computed using standard forward-backward and
viterbi algorithms.

parameter learning can be done using em by al-
ternating the computation of expected counts and the
maximization of multinomial parameters.
in par-
ticular, profinder used incremental em, which
has been shown to have better and faster con-
vergence properties than standard em (liang and
klein, 2009).

determining the optimal number of events and
slots is challenging. one solution is to adopt non-
parametric bayesian methods by incorporating a hi-
erarchical prior over the parameters (e.g., a dirich-
let process). however, this approach can impose
unrealistic restrictions on the model choice and re-
sult in intractability which requires sampling or ap-
proximate id136 to overcome. additionally, em
learning can suffer from local optima due to its non-
convex learning objective, especially when dealing
with a large number hidden states without a good
initialization.

to address these issues, we adopt a novel appli-
cation of the split-merge method previously used in

figure 1: graphical representation of our model. hyper-
parameters, the stickiness factor, and the frame and event
initial and transition distributions are not shown for clar-
ity.

and a dependency relation often gives a strong sig-
nal of the slot that is indicated.
for example,
bomb > nsubj often indicates a perpetrator.
thus, rather than simply emitting the dependency
from the event head to an event argument depi,j, our
model instead emits the pair of event head and de-
pendency relation, which we call a caseframe fol-
lowing bean and riloff (2004).

3.3 full generative story
to summarize, the distributions that are learned by
our model are the default distributions pbkg(b),
pf   init(f ), pe   init(e),
the transition distri-
butions pf   tran(fi+1|fi),
pe   tran(ei+1|ei),
and the emission distributions pslot(s|e, a, b),
pe   head(e|e, b), pa   head(a|s), pa   dep(dep|s).
we used additive smoothing with uniform dirich-
let priors for all the multinomials. the overall
generative story of our model is as follows:

1. draw a bernoulli distribution for pbkg(b)
2. draw the frame, event, and slot distributions
3. draw an event head emission distribution
pe   head(e|e, b) for each frame including the
background frame

4. draw event argument lemma and caseframe
emission distributions for each slot in each
frame including the background frame

frame event background event head     1     1     1     1     1     1             1     1                                                                                      . . .  . . .  |    | |    | |    |                             arguments                                                     syntactic parsing for inferring re   ned latent syntac-
tic categories (petrov et al., 2006). speci   cally, we
initialize our model such that each frame is associ-
ated with one event and two slots. then, after a num-
ber of iterations of em, we split each event and slot
in two along with their id203, and duplicate the
associated emission distributions. we then add some
perturbation to break symmetry. after splitting, we
merge back a proportion of the newly split events
and slots that result in the least improvement in the
likelihood of the training data. for more details on
split-merge, see (petrov et al., 2006)

by adjusting the number of split-merge cycles and
the merge parameters, our model learns the number
of events and slots in a dynamical fashion that is tai-
lored to the data. moreover, our model starts with
a small number of frame elements, which reduces
the number of local optima and make initial learn-
ing easier. after each split, the subsequent learning
starts with (a perturbed version of) the previously
learned parameters, which makes a good initializa-
tion that is crucial for em. finally, it is also compat-
ible with the hierarchical nature of events and slots.
for example, slots can    rst be coarsely split into per-
sons versus locations, and later re   ned into subcate-
gories such as perpetrators and victims.

4 muc-4 entity extraction experiments

we    rst evaluate our model on a standard entity
extraction task, using the evaluation settings from
chambers and jurafsky (2011) to enable a head-to-
head comparison. speci   cally, we use the muc-4
data set (muc, 1992), which contains 1300 training
and development documents on terrorism in south
america, with 200 additional documents for testing.
muc-4 contains four templates: attack, kidnapping,
bombing, and arson.2 all templates share the same
set of prede   ned slots, with the evaluation focusing
on the following four: perpetrator, physical target,
human target, and instrument.

for each slot in a muc template, the system    rst
identi   ed an induced slot that best maps to it by f1
on the development set. as in chambers and juraf-
sky (2011), template is ignored in    nal evaluation.
so the system merged the induced slots across all

2two other templates have negligible counts and are ignored

templates to calculate the    nal scores. correctness
is determined by matching head words, and slots
marked as optional in muc are ignored when com-
puting recall. all hyper-parameters are tuned on the
development set3.
document classi   cation the muc-4 dataset
contains many documents that contain words related
to muc slots (e.g., plane and aviation), but are not
about terrorism. to reduce precision errors, cham-
bers and jurafsky   s (2011) (henceforth, c&j)    rst
   ltered irrelevant documents based on the speci   city
of event heads to learned frames. to estimate the
speci   city, they used additional data retrieved from a
large external corpus. in profinder, however, speci-
   city can be easily estimated using the id203
distributions learned during training. in particular,
we de   ne the id203 of an event head in a frame
j:

pf (w) =

pe   head(w|e)/|f|,

(3)

(cid:88)

ef    f

and the id203 of a frame given an event head:

p (f|w) = pf (w)/

pf (cid:48)(w).

(4)

(cid:88)

f (cid:48)   f

we then follow the rest of chambers and jurafsky
(2011) to score each learned frame with each muc
document, mapping a document to a frame if the av-
erage pf (w) in the document is above a threshold
and the document contains at least one trigger word
w(cid:48) with p (f|w(cid:48)) > 0.2. the threshold and the in-
duced frame were determined on the development
set, which were then used to    lter irrelevant docu-
ments in the test set.
results compared to c&j, profinder is con-
ceptually much simpler, involving a single proba-
bilistic model, with standard learning and id136
algorithms.
in particular, it did not require multi-
ple processing steps or customized similarity met-
rics; rather, it only used the data within muc-4. in
contrast, c&j required additional text to be retrieved
from a large external corpus (gigaword (graff et al.,
2005)) for each event cluster, yet profinder nev-
ertheless was able to outperform c&j on entity ex-
traction, as shown in table 1. our system achieved
3we will make the parameter settings used in all experiments

as in chambers and jurafsky (2011).

publicly available.

unsupervised methods
profinder (this work)
chambers and jurafsky (2011)
with extra information
profinder +doc. classi   cation
c&j 2011 +granularity

p r f1
34
32
48
33

37
25

41
44

44
36

43
40

table 1: results on muc-4 entity extraction. c&j 2011
+granularity refers to their experiment in which they
mapped one of their templates to    ve learned clusters
rather than one.

(b)

frame: terrorism

event: attack

report, participate, kid-
nap, kill, release

slot: perpetrator

person/org

words: guerrilla, po-
lice,
source, person,
group
caseframes:
report>nsubj,
kidnap>nsubj,
kill>nsubj,
participate>nsubj,
release>nsubj

event: discussion

hold, meeting, talk, dis-
cuss, investigate

slot: victim
person/org

words: people, priest,
leader, member, judge

caseframes:
kill>dobj,
murder>dobj,
release>dobj,
report>dobj,
kidnap>dobj

figure 2: a partial frame learned by profinder from the
muc-4 data set, with the most probable emissions for
each event and slot. labels are assigned by the authors
for readability.

good recall but was hurt by the lower precision. we
investigated the importance of document classi   ca-
tion by only extracting from the gold-standard rele-
vant documents (+doc. classi   cation), which led to
a substantial improvement in precision, suggesting
possible further improvement by better document
classi   cation. also unlike c&j, our system does not
currently make use of coreference information.

figure 2 shows part of a frame that is learned by
profinder, including some of the standard muc
slots and events. our method also    nds events not
annotated in muc, such as the discussion event.
other interesting events and slots that we noticed
include an arrest event (call, arrest, express, meet,
charge), a peace agreement slot (agreement, rights,
law, proposal), and an authorities slot (police, gov-

(a) accidents and natural disasters:

what: what happened
when: date, time, other temporal markers
where: physical location
why: reasons for accident/disaster
who affected: casualties...
damages: ... caused by the disaster
countermeasures: rescue efforts...
(when during the night of july 17,)
(what a 23-foot <what tsunami) hit the
north coast of papua new guinea (png)>,
(why triggered by a 7.0 undersea earth-
quake in the area).

(c) when: night

what: tsunami, coast

why: earthquake

figure 3: an example of (a) a frame from the tac
guided summarization task with abbreviated slot de-
scriptions, (b) an annotated tac contributor, and (c) the
entities that are extracted for evaluation.

ernment, force, command). the background frame
was able to capture many verbs related to report-
ing, such as say, continue, add, believe, although it
missed report.

5 evaluating frame induction using
guided summarization templates

one issue with the muc-4 evaluation is the lim-
ited variety of templates and entities that are avail-
able. moreover, this data set was speci   cally de-
veloped for information extraction and questions re-
main whether our approach can generalize beyond
it. we thus conducted a novel evaluation using the
tac guided summarization data set, which contains
a wide variety of frames and topics. our evalua-
tion corresponds to a view of summarization as ex-
tracting structured information from the source text,
and highlights the connection between summariza-
tion and information extraction (white et al., 2001).

set

for our

data preparation we use the tac 2010 guided
summarization data
experiments
(owczarzak and dang, 2010). this data set pro-
vides templates as de   ned by the task organizers
and contains 46 document clusters in    ve domains,
with each cluster comprising 20 documents on a
speci   c topic. eight human-written model sum-

maries are provided for each document cluster. as
part of the pyramid evaluation method (nenkova
and passonneau, 2004),
these summaries have
been manually segmented and labeled with slots
from the corresponding template for each segment
(figure 3)4.

we    rst considered de   ning the task as extract-
ing entities from the source text, but this annotation
is not available in tac, and pilot studies suggested
that it required nontrivial effort to train average users
to conduct high-quality annotation reliably. we thus
de   ned our task as extracting entities from the model
summaries instead. as mentioned earlier, tac slot
annotation is available for summaries. furthermore,
using the summary text has the advantage that slots
that are considered important in the domain natu-
rally appear more frequently, whereas unimportant
text is    ltered out.

each span that is labeled by a slot is called a con-
tributor. we convert the contributors into a form that
is more like the previous muc evaluation, so that we
can fairly compare against previous work like c&j
that were designed to extract information into that
form. speci   cally, we extract the head lemma from
all the maximal noun phrases found in the contrib-
utor. like in muc-4, we count a system-extracted
noun phrase as a match if this head word matches
and is extracted from the same document (i.e., sum-
mary). this process can lead to noise, as the mean-
ing of some contributors depend on a larger phrasal
unit than a noun phrase, but this heuristic normal-
izes the representations of the contributors so that
they are amenable to our evaluation. we leave the
denoising of this process to future work, and believe
it should be feasible by id104.

method and experiments the induced entity
clusters are mapped to the tac slots in the tac
frames according to the best f1 achieved for each
tac slot. however, one issue is that many tac
slots are more general than the type of slots found
in muc. for example, slots like why and coun-
termeasures likely correspond to multiple slots
at the granularity of muc. thus, we map the n-best
induced slots to tac slots rather than the 1-best, for

4the

full

set

of

slots

is

available

at http:

//www.nist.gov/tac/2010/summarization/
guided-summ.2010.guidelines.html

1-to-1

5-to-1

systems
profinder 24
58
c&j

p r
25
6.1

f1 p r f1
27
24
11
20

38
12

21
50

table 2: results on tac 2010 entity extraction with n-
to-1 mapping for n = 1 and n = 5. intermediate values
of n produce intermediate results, and are not shown for
brevity.

n up to 5. we train profinder and a reimplemen-
tation of c&j on the 920 full source texts of tac
2010, and test them on the 368 model summaries.
we do not provide c&j   s model with access to ex-
ternal data, in order to create fair comparison con-
ditions to our model. we also eliminate a sentence
relevance classi   cation step from c&j, and the doc-
ument relevance classi   cation step from both mod-
els, because all sentences in the summary text are
expected to be relevant. we tune c&j   s id91
thresholds and the parameters to our model by two-
fold cross validation on the summaries, and assume
gold summary classi   cation into the    ve topic cate-
gories de   ned by tac.

results the results on tac are shown in table 2.
the overall results are poorer than for the muc-4
task, but this task is harder given the greater diversity
in frames and slots to be induced. like in the pre-
vious evaluation, our system is able to outperform
c&j in terms of recall and f1, but not precision.
c&j   s method produces many small clusters, which
makes it easy to achieve high precision. the n-to-1
mapping procedure can also be seen to favor their
method over ours, many small clusters with high
precision can be selected to greatly improve recall,
which is indeed the case. however, profinder
with 1-to-1 mapping outperforms c&j even with 5-
to-1 mapping.

6 conclusion

we have presented the    rst probabilistic approach
to frame induction and shown that it achieves state-
of-the-art results on end-to-end entity extraction in
standard muc and tac data sets. our model is in-
spired by recent advances in unsupervised seman-
tic induction and in content modeling in summariza-
tion, and is easy to extend. we would like to further

investigate frame induction evaluation, for example
to evaluate event id91 in addition to the slots
and entities.

acknowledgments

we would like to thank nate chambers for answer-
ing questions about his system. we would also like
to thank chris quirk for help with preprocessing the
muc corpus, and the other members of the nlp
group at microsoft research for useful discussions.

references
[baker et al.1998] collin f. baker, charles j. fillmore,
and john b. lowe. 1998. the berkeley framenet
project. in proceedings of the 17th international con-
ference on computational linguistics.

[banko and etzioni2008] michele banko and oren et-
zioni. 2008. the tradeoffs between open and tra-
ditional id36. proceedings of acl-08:
hlt, pages 28   36.

[barzilay and lee2004] regina barzilay and lillian lee.
2004. catching the drift: probabilistic content mod-
els, with applications to generation and summariza-
in proceedings of the human language tech-
tion.
nology conference of the north american chapter of
the association for computational linguistics: hlt-
naacl 2004.

[bean and riloff2004] david bean and ellen riloff.
2004.
unsupervised learning of contextual role
knowledge for coreference resolution. in proceedings
of the human language technology conference of the
north american chapter of the association for com-
putational linguistics: hlt-naacl 2004.

[chambers and jurafsky2008] nathanael chambers and
dan jurafsky. 2008. unsupervised learning of nar-
rative event chains. in proceedings of acl-08: hlt,
pages 789   797, columbus, ohio, june. association
for computational linguistics.

[chambers and jurafsky2009] nathanael chambers and
dan jurafsky. 2009. unsupervised learning of nar-
rative schemas and their participants. in proceedings
of the joint conference of the 47th annual meeting of
the acl and the 4th international joint conference on
natural language processing of the afnlp. associa-
tion for computational linguistics.

[chambers and jurafsky2011] nathanael chambers and
dan jurafsky. 2011. template-based information ex-
traction without the templates. in proceedings of the
49th annual meeting of the association for compu-
tational linguistics: human language technologies,

pages 976   986, portland, oregon, usa, june. associ-
ation for computational linguistics.

[cheung and li2012] jackie c. k. cheung and xiao li.
2012. sequence id91 and labeling for unsuper-
in proceedings of the
vised query intent discovery.
5th acm international conference on web search and
data mining, pages 383   392.

[curran et al.2007] james r. curran, tara murphy, and
bernhard scholz. 2007. minimising semantic drift
with mutual exclusion id64. in proceedings
of the 10th conference of the paci   c association for
computational linguistics.

[daum  e iii and marcu2006] hal daum  e iii and daniel
marcu. 2006. bayesian query-focused summariza-
tion. in proceedings of the 21st international confer-
ence on computational linguistics and 44th annual
meeting of the association for computational linguis-
tics, pages 305   312, sydney, australia, july. associa-
tion for computational linguistics.

[fillmore1982] charles j. fillmore. 1982. frame seman-
tics. linguistics in the morning calm, pages 111   137.
[graff et al.2005] david graff, junbo kong, ke chen, and
kazuaki maeda. 2005. english gigaword second edi-
tion. linguistic data consortium, philadelphia.

[gruber et al.2007] amit gruber, michael rosen-zvi,
and yair weiss. 2007. hidden topic markov models.
arti   cial intelligence and statistics (aistats).
[haghighi and vanderwende2009] aria haghighi

and
lucy vanderwende. 2009. exploring content models
in proceedings
for id57.
of human language technologies: the 2009 annual
conference of the north american chapter of the
association for computational linguistics, pages
362   370, boulder, colorado, june. association for
computational linguistics.

[huang and riloff2012] ruihong huang and ellen riloff.
2012. bootstrapped training of event extraction classi-
   ers. in proceedings of the 13th conference of the eu-
ropean chapter of the association for computational
linguistics, pages 286   295, avignon, france, april.
association for computational linguistics.

[lang and lapata2011] joel lang and mirella lapata.
2011. unsupervised semantic role induction via split-
merge id91. in proceedings of the 49th annual
meeting of the association for computational linguis-
tics: human language technologies, pages 1117   
1126, portland, oregon, usa, june. association for
computational linguistics.

[liang and klein2009] percy liang and dan klein. 2009.
online em for unsupervised models. in proceedings
of human language technologies: the 2009 annual
conference of the north american chapter of the as-
sociation for computational linguistics, pages 611   

[swier and stevenson2004] robert s. swier and suzanne
stevenson.
2004. unsupervised semantic role la-
belling. in dekang lin and dekai wu, editors, pro-
ceedings of emnlp 2004, pages 95   102, barcelona,
spain, july. association for computational linguis-
tics.

[white et al.2001] michael white, tanya korelsky, claire
cardie, vincent ng, david pierce, and kiri wagstaff.
2001. multidocument summarization via informa-
in proceedings of the first interna-
tion extraction.
tional conference on human language technology
research. association for computational linguistics.

619, boulder, colorado, june. association for com-
putational linguistics.

[minsky1974] marvin minsky.

1974. a framework
for representing knowledge. technical report, cam-
bridge, ma, usa.

[muc1992] 1992. proceedings of the fourth message
understanding conference (muc-4). morgan kauf-
mann.

[nenkova and passonneau2004] ani nenkova and re-
becca passonneau. 2004. evaluating content selection
in summarization: the pyramid method. in proceed-
ings of the human language technology conference
of the north american chapter of the association for
computational linguistics: hlt-naacl 2004, vol-
ume 2004, pages 145   152.

[owczarzak and dang2010] karolina owczarzak

and
hoa t. dang. 2010. tac 2010 guided summarization
task guidelines.

[patwardhan and riloff2007] siddharth patwardhan and
ellen riloff.
2007. effective information extrac-
tion with semantic af   nity patterns and relevant re-
in proceedings of the 2007 joint conference
gions.
on empirical methods in natural language process-
ing and computational natural language learning
(emnlp-conll), pages 717   727, prague, czech re-
public, june. association for computational linguis-
tics.

[petrov et al.2006] slav petrov, leon barrett, romain
thibaux, and dan klein. 2006. learning accurate,
in pro-
compact, and interpretable tree annotation.
ceedings of the 21st international conference on com-
putational linguistics and 44th annual meeting of the
association for computational linguistics.

[poon and domingos2009] hoifung poon and pedro
domingos. 2009. unsupervised id29.
in proceedings of the 2009 conference on empirical
methods in natural language processing, pages
1   10.

[richardson et al.1998] stephen

d.

richardson,
william b. dolan, and lucy vanderwende.
1998.
mindnet: acquiring and structuring semantic in-
in proceedings of the 36th
formation from text.
annual meeting of the association for computational
linguistics and 17th international conference on
computational linguistics, volume 2, pages 1098   
1102, montreal, quebec, canada, august. association
for computational linguistics.

[rumelhart1975] david rumelhart, 1975. notes on a
schema for stories, pages 211   236. academic press,
inc.

[schank and abelson1977] roger

and
robert p. abelson. 1977. scripts, plans, goals, and
understanding: an inquiry into human knowledge
structures. lawrence erlbaum, july.

schank

c.

