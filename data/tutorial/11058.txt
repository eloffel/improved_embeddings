3
1
0
2

 
t
c
o
8
1

 

 
 
]

g
l
.
s
c
[
 
 

1
v
2
4
0
5

.

0
1
3
1
:
v
i
x
r
a

id65 beyond words:

supervised learning of analogy and paraphrase

peter d. turney

national research council canada

information and communications technologies

ottawa, ontario, canada, k1a 0r6

peter.turney@nrc-cnrc.gc.ca

abstract

there have been several efforts to extend
id65 beyond individual
words, to measure the similarity of word pairs,
phrases, and sentences (brie   y, tuples; ordered
sets of words, contiguous or noncontiguous).
one way to extend beyond words is to com-
pare two tuples using a function that com-
bines pairwise similarities between the com-
ponent words in the tuples. a strength of
this approach is that it works with both rela-
tional similarity (analogy) and compositional
similarity (paraphrase). however, past work
required hand-coding the combination func-
tion for different tasks. the main contribution
of this paper is that combination functions are
generated by supervised learning. we achieve
state-of-the-art results in measuring relational
similarity between word pairs (sat analo-
gies and semeval 2012 task 2) and measur-
ing compositional similarity between noun-
modi   er phrases and unigrams (multiple-
choice paraphrase questions).

1 introduction

harris (1954) and firth (1957) hypothesized that
in similar contexts tend to
words that appear
have similar meanings.
this hypothesis is the
in which
foundation for id65,
words are represented by context vectors.
the
similarity of two words is calculated by com-
paring the two corresponding context vectors
(lund et al., 1995;
landauer and dumais, 1997;
turney and pantel, 2010).

id65 is highly effective for
measuring the semantic similarity between indi-
vidual words.
on a set of eighty multiple-
choice synonym questions from the test of english
as a foreign language
(toefl), a distribu-
tional approach recently achieved 100% accuracy
(bullinaria and levy, 2012). however, it has been
dif   cult to extend id65 beyond
individual words, to word pairs, phrases, and sen-
tences.

moving beyond individual words, there are vari-
ous types of semantic similarity to consider. here
we focus on paraphrase and analogy. paraphrase
is similarity in the meaning of two pieces of text
(androutsopoulos and malakasiotis, 2010). anal-
ogy is similarity in the semantic relations of two sets
of words (turney, 2008a).

it is common to study paraphrase at the sentence
level
(androutsopoulos and malakasiotis, 2010),
but we prefer to concentrate on the simplest type of
paraphrase, where a bigram paraphrases a unigram.
for example, dog house is a paraphrase of kennel. in
our experiments, we concentrate on noun-modi   er
bigrams and noun unigrams.

analogies map terms in one domain to terms in
another domain (gentner, 1983). the familiar anal-
ogy between the solar system and the rutherford-
bohr atomic model involves several terms from the
domain of the solar system and the domain of the
atomic model (turney, 2008a).

the simplest

type of analogy is proportional
analogy, which involves
two pairs of words
(turney, 2006b). for example, the pair hcook, rawi
is analogous to the pair hdecorate, plaini. if we cook

a thing, it is no longer raw; if we decorate a thing, it
is no longer plain. the semantic relations between
cook and raw are similar to the semantic relations
between decorate and plain. in the following exper-
iments, we focus on proportional analogies.

of

or

for

erk

four

extend

(2013)

sentence

semantics

a phrase

distributional

distinguished

in the    rst,

representations

a single vector

from the
words

approaches
beyond
to
space
words:
is
representation
the
computed
(mitchell and lapata, 2010;
individual
in the second,
baroni and zamparelli, 2010).
compared by
two phrases or
sentences
val-
combining multiple
ues (socher et al., 2011; turney, 2012).
third,
weighted id136 rules integrate distributional
similarity and formal logic (garrette et al., 2011).
fourth, a single space integrates formal logic and
vectors (clarke, 2012).

similarity

pairwise

are

taking the second approach, turney (2012) intro-
duced a dual-space model, with one space for mea-
suring domain similarity (similarity of topic or    eld)
and another for function similarity (similarity of role
or usage). similarities beyond individual words are
calculated by functions that combine domain and
function similarities of component words.

the dual-space model has been applied to mea-
suring compositional similarity (paraphrase recogni-
tion) and relational similarity (analogy recognition).
in experiments that tested for sensitivity to word
order, the dual-space model performed signi   cantly
better than competing approaches (turney, 2012).

a limitation of past work with the dual-space
model is that the combination functions were hand-
coded. our main contribution is to show how hand-
coding can be eliminated with supervised learning.
for ease of reference, we will call our approach
supersim (supervised similarity). with no modi   -
cation of supersim for the speci   c task (relational
similarity or compositional similarity), we achieve
better results than previous hand-coded models.

compositional similarity (paraphrase) compares
two contiguous phrases or sentences (id165s),
whereas relational similarity (analogy) does not
require contiguity. we use tuple to refer to both con-
tiguous and noncontiguous word sequences.

we approach analogy as a problem of supervised
tuple classi   cation. to measure the relational sim-

ilarity between two word pairs, we train supersim
with quadruples that are labeled as positive and neg-
ative examples of analogies. for example, the pro-
portional analogy hcook, raw, decorate, plaini is
labeled as a positive example.

a quadruple is represented by a feature vector,
composed of domain and function similarities from
the dual-space model and other features based on
corpus frequencies. supersim uses a support vector
machine (platt, 1998) to learn the id203 that a
quadruple ha, b, c, di consists of a word pair ha, bi
and an analogous word pair hc, di. the id203
can be interpreted as the degree of relational similar-
ity between the two given word pairs.

we also approach paraphrase as supervised tuple
classi   cation. to measure the compositional simi-
larity beween an m-gram and an id165, we train
the learning algorithm with (m + n)-tuples that are
positive and negative examples of paraphrases.

supersim learns to estimate the id203 that
a triple ha, b, ci consists of a compositional bigram
ab and a synonymous unigram c. for instance, the
phrase    sh tank is synonymous with aquarium; that
is,    sh tank and aquarium have high compositional
similarity. the triple h   sh, tank, aquariumi is repre-
sented using the same features that we used for anal-
ogy. the id203 of the triple can be interpreted
as the degree of compositional similarity between
the given bigram and unigram.

we review related work in section 2. the gen-
eral feature space for learning relations and compo-
sitions is presented in section 3. the experiments
with relational similarity are described in section 4,
and section 5 reports the results with compositional
similarity. section 6 discusses the implications of
the results. we consider future work in section 7
and conclude in section 8.

2 related work

in semeval 2012, task 2 was concerned with mea-
suring the degree of relational similarity between
two word pairs (jurgens et al., 2012) and task 6
(agirre et al., 2012) examined the degree of seman-
tic equivalence between two sentences. these two
areas of research have been mostly independent,
although socher et al. (2012) and turney (2012)
present uni   ed perspectives on the two tasks. we

   rst discuss some work on relational similarity, then
some work on compositional similarity, and lastly
work that uni   es the two types of similarity.

a

similarity with

2.1 relational similarity
relational analysis) measures rela-
lra (latent
pair   pattern matrix
tional
rows in the matrix corre-
(turney, 2006b).
spond to word pairs (a, b) and columns correspond
to patterns that connect the pairs (   a for the b   ) in a
large corpus. this is a holistic (noncompositional)
approach to distributional similarity, since the word
pairs are opaque wholes;
the component words
have no separate representations. a compositional
approach to analogy has a representation for each
word, and a word pair is represented by composing
the representations for each member of the pair.
given a vocabulary of n words, a compositional
approach requires n representations to handle all
possible word pairs, but a holistic approach requires
n 2 representations. holistic approaches do not
scale up (turney, 2012). lra required nine days to
run.

bollegala et al. (2008) answered the sat anal-
ogy questions with a support vector machine trained
on quadruples (proportional analogies), as we do
here. however, their feature vectors are holistic, and
hence there are scaling problems.

herda  gdelen and baroni (2009) used a support
vector machine to learn relational similarity. their
feature vectors contained a combination of holistic
and compositional features.

measuring relational similarity is closely con-
nected to classifying word pairs according to
their semantic relations (turney and littman, 2005).
semantic relation classi   cation was the focus
of semeval 2007 task 4 (girju et al., 2007) and
semeval 2010 task 8 (hendrickx et al., 2010).

extend

semantics

distributional

2.2 compositional similarity
to
beyond
words, many researchers take the    rst approach
described by erk (2013), in which a single vec-
tor space is used for individual words, phrases,
and
(landauer and dumais, 1997;
mitchell and lapata, 2008;
mitchell and lapata, 2010).
in this approach,
given the words a and b with context vectors a

sentences

and b, we construct a vector for the bigram ab by
applying vector operations to a and b.

mitchell and lapata (2010) experiment with
many different vector operations and    nd that
element-wise multiplication performs well. the
bigram ab is represented by c = a     b, where
ci = ai    bi. however, element-wise multiplica-
tion is commutative, so the bigrams ab and ba map
to the same vector c.
in experiments that test for
order sensitivity, element-wise multiplication per-
forms poorly (turney, 2012).

we can treat the bigram ab as a unit, as if it were
a single word, and construct a context vector for
ab from occurrences of ab in a large corpus. this
holistic approach to representing bigrams performs
well when a limited set of bigrams is speci   ed in
advance (before building the word   context matrix),
but it does not scale up, because there are too many
possible bigrams (turney, 2012).

although the holistic approach does not scale
up, we can generate a few holistic bigram vectors
and use them to train a supervised regression model
(guevara, 2010;
baroni and zamparelli, 2010).
given a new bigram cd, not observed in the corpus,
the regression model can predict a holistic vector
for cd, if c and d have been observed separately. we
show in section 5 that this idea can be adapted to
train supersim without manually labeled data.

socher et al. (2011) take the second approach
described by erk (2013), in which two sentences are
compared by combining multiple pairwise similar-
ity values. they construct a variable-sized similar-
ity matrix x, in which the element xij is the sim-
ilarity between the i-th phrase of one sentence and
the j-th phrase of the other. since supervised learn-
ing is simpler with    xed-sized feature vectors, the
variable-sized similarity matrix is then reduced to a
smaller    xed-sized matrix, to allow comparison of
pairs of sentences of varying lengths.

2.3 uni   ed perspectives on similarity
socher et al. (2012) represent words and phrases
with a pair, consisting of a vector and a matrix. the
vector captures the meaning of the word or phrase
and the matrix captures how a word or phrase mod-
i   es the meaning of another word or phrase when
they are combined. they apply this matrix   vector
representation to both compositions and relations.

turney (2012) represents words with two vectors,
a vector from domain space and a vector from func-
tion space. the domain vector captures the topic or
   eld of the word and the function vector captures the
functional role of the word. this dual-space model
is applied to both compositions and relations.

here we extend the dual-space model of tur-
ney (2012) in two ways: hand-coding is replaced
with supervised learning and two new sets of fea-
tures augment domain and function space. moving
to supervised learning instead of hand-coding makes
it easier to introduce new features.

in the dual-space model, parameterized similar-
ity measures provided the input values for hand-
crafted functions. each task required a different set
of hand-crafted functions. the parameters of the
similarity measures were tuned using a customized
grid search algorithm. the grid search algorithm
was not suitable for integration with a supervised
learning algorithm. the insight behind supersim is
that, given appropriate features, a supervised learn-
ing algorithm can replace the grid search algorithm
and the hand-crafted functions.

3 features for tuple classi   cation

types of

fea-
we represent a tuple with four
tures, all based on frequencies in a large corpus.
the    rst type of feature is the logarithm of the
the second type is the
frequency of a word.
positive pointwise mutual
information (ppmi)
between
(church and hanks, 1989;
bullinaria and levy, 2007). third and fourth are the
similarities of two words in domain and function
space (turney, 2012).

two words

in the following experiments, we use the ppmi
matrix from turney et al. (2011) and the domain and
function matrices from turney (2012).1 the three
matrices and the word frequency data are based on
the same corpus, a collection of web pages gath-
ered from university web sites, containing 5    1010
words.2 all three matrices are word   context matri-
ces, in which the rows correspond to terms (words

1the three matrices and the word frequency data are avail-
able on request from the author. the matrix    les range from
two to    ve gigabytes when packaged and compressed for distri-
bution.

2the corpus was collected by charles clarke at the univer-

and phrases) in id138.3 the columns correspond
to the contexts in which the terms appear; each
matrix involves a different kind of context.

let hx1, x2, . . . , xni be an n-tuple of words. the
number of features we use to represent this tuple
increases as a function of n.

the    rst set of features consists of log frequency
values for each word xi in the n-tuple. let freq(xi)
be the frequency of xi in the corpus. we de   ne
lf(xi) as log(freq(xi)+1). if xi is not in the corpus,
freq(xi) is zero, and thus lf(xi) is also zero. there
are n log frequency features, one lf(xi) feature for
each word in the n-tuple.

the second set of features consists of positive
pointwise mutual information values for each pair of
words in the n-tuple. we use the raw ppmi matrix
from turney et al. (2011). although they computed
the singular value decomposition (svd) to project
the row vectors into a lower-dimensional space, we
need the original high-dimensional columns for our
features. the raw ppmi matrix has 114,501 rows
and 139,246 columns with a density of 1.2%. for
each term in id138, there is a corresponding row
in the raw ppmi matrix. for each unigram in word-
net, there are two corresponding columns in the raw
ppmi matrix, one marked left and the other right.

suppose xi corresponds to the i-th row of the
ppmi matrix and xj corresponds the j-th column,
marked left. the value in the i-th row and j-th col-
umn of the ppmi matrix, ppmi(xi, xj , left), is the
positive pointwise mutual information of xi and xj
co-occurring in the corpus, where xj is the    rst word
to the left of xi, ignoring any intervening stop words
(that is, ignoring any words that are not in id138).
if xi (or xj) has no corresponding row (or column)
in the matrix, then the ppmi value is set to zero.

turney et al. (2011) estimated ppmi(xi, xj , left)
by sampling the corpus for phrases containing xi and
then looking for xj to the left of xi in the sampled
phrases (and likewise for right). due to this sam-
pling process, ppmi(xi, xj , left) does not necessar-
ily equal ppmi(xj , xi, right). for example, suppose
xi is a rare word and xj is a common word. with
ppmi(xi, xj , left), when we sample phrases contain-
ing xi, we are relatively likely to    nd xj in some of

3see http://id138.princeton.edu/ for infor-

sity of waterloo. it is about 280 gigabytes of plain text.

mation about id138.

these phrases. with ppmi(xj , xi, right), when we
sample phrases containing xj, we are less likely to
   nd any phrases containing xi. although, in theory,
ppmi(xi, xj , left) should equal ppmi(xj , xi, right),
they are likely to be unequal given a limited sample.
from the n-tuple, we select all of the n(n     1)
pairs, hxi, xji, such that i 6= j. we then gener-
ate two features for each pair, ppmi(xi, xj , left) and
ppmi(xi, xj , right). thus there are 2n(n     1) ppmi
values in the second set of features.

the third set of features consists of domain space
similarity values for each pair of words in the
n-tuple. domain space was designed to capture the
topic of a word. turney (2012)    rst constructed a
frequency matrix, in which the rows correspond to
terms in id138 and the columns correspond to
nearby nouns. given a term xi, the corpus was sam-
pled for phrases containing xi and the phrases were
processed with a part-of-speech tagger, to identify
nouns. if the noun xj was the closest noun to the left
or right of xi, then the frequency count for the i-th
row and j-th column was incremented. the hypoth-
esis was that the nouns near a term characterize the
topics associated with the term.

the word   context frequency matrix for domain
space has 114,297 rows (terms) and 50,000 columns
(noun contexts, topics), with a density of 2.6%. the
frequency matrix was converted to a ppmi matrix
and then smoothed with svd. the svd yields three
matrices, u,   , and v.

a term in domain space is represented by a row
vector in uk  p
k. the parameter k speci   es the
number of singular values in the truncated singu-
lar value decomposition;
that is, k is the number
of latent factors in the low-dimensional representa-
tion of the term (landauer and dumais, 1997). we
generate uk and   k by deleting the columns in u
and    corresponding to the smallest singular values.
the parameter p raises the singular values in   k to
the power p (caron, 2001). as p goes from one to
zero, factors with smaller singular values are given
more weight. this has the effect of making the sim-
ilarity measure more discriminating (turney, 2012).
the similarity of two words in domain space,
dom(xi, xj , k, p), is computed by extracting the row
vectors in uk  p
k that correspond to the words xi and
xj, and then calculating their cosine. optimal per-
formance requires tuning the parameters k and p for

the task (bullinaria and levy, 2012; turney, 2012).
in the following experiments, we avoid directly tun-
ing k and p by generating features with a variety of
values for k and p, allowing the supervised learning
algorithm to decide which features to use.

from the n-tuple, we select all 1

2 n(n     1) pairs,
hxi, xji, such that i < j. for each pair, we gen-
erate domain similarity features, dom(xi, xj , k, p),
where k varies from 100 to 1000 in steps of 100 and
p varies from 0 to 1 in steps of 0.1. the number of k
values, nk, is 10 and the number of p values, np, is
11; therefore there are 110 features, nknp, for each
pair, hxi, xji. thus there are 1
2 n(n   1)nknp domain
space similarity values in the third set of features.

the fourth set of features consists of function
space similarity values for each pair of words in the
n-tuple. function space was designed to capture the
functional role of a word.
it is similar to domain
space, except the context is based on verbal patterns,
instead of nearby nouns. the hypothesis was that
the functional role of a word is characterized by the
patterns that relate the word to nearby verbs.

the word   context frequency matrix for function
space has 114,101 rows (terms) and 50,000 columns
(verb pattern contexts, functional roles), with a den-
sity of 1.2%. the frequency matrix was converted to
a ppmi matrix and smoothed with svd.

from the n-tuple, we select all 1

2 n(n     1) pairs,
hxi, xji, such that i < j. for each pair, we generate
function similarity features, fun(xi, xj , k, p), where
k and p vary as they did with domain space. thus
2 n(n     1)nknp function space similarity
there are 1
values in the fourth set of features.

table 1 summarizes the four sets of features and
the size of each set as a function of n, the number of
words in the given tuple. the values of nk and np
(10 and 11) are considered to be constants. table 2
shows the number of elements in the feature vector,
as n varies from 1 to 6. the total number of features
is o(n2). we believe that this is acceptable growth
and will scale up to comparing sentence pairs.

the four sets of features have a hierarchical rela-
tionship. the log frequency features are based
on counting isolated occurrences of each word
in the corpus.
the ppmi features are based
on direct co-occurrences of two words;
is,
ppmi is only greater than zero if the two words
actually occur together in the corpus. domain

that

feature set
lf(xi)
ppmi(xi, xj , handedness)
dom(xi, xj , k, p)
fun(xi, xj, k, p)

size of set
n
2n(n     1)
1
2 n(n     1)nknp
2 n(n     1)nknp

1

table 1: the four sets of features and their sizes.

stem:
choices:

solution:

word:language
paint:portrait
(1)
poetry:rhythm
(2)
note:music
(3)
tale:story
(4)
(5) week:year
(3)
note:music

n-tuple lf ppmi dom fun total
1
226
675
1348
2245
3366

0
110
330
660
1100
1650

0
110
330
660
1100
1650

0
4
12
24
40
60

1
2
3
4
5
6

1
2
3
4
5
6

table 2: number of features for various tuple sizes.

and function space capture indirect or higher-
order co-occurrence, due to the truncated svd
(lemaire and denhi`ere, 2006); that is, the values of
dom(xi, xj , k, p) and fun(xi, xj , k, p) can be high
even when xi and xj do not actually co-occur in
the corpus. we conjecture that there are yet higher
orders in this hierarchy that would provide improved
similarity measures.

supersim learns to classify tuples by represent-
ing them with these features. supersim uses the
sequential minimal optimization (smo) support
vector machine (id166) as implemented in weka
(platt, 1998; witten et al., 2011).4 the kernel is a
normalized third-order polynomial. weka provides
id203 estimates for the classes by    tting the
outputs of the id166 with id28 models.

4 relational similarity

this section presents experiments with learning rela-
tional similarity using supersim.
the training
datasets consist of quadruples that are labeled as
positive and negative examples of analogies. table 2
shows that the feature vectors have 1,348 elements.
we experiment with three datasets, a collection
of 374    ve-choice questions from the sat col-
lege entrance exam (turney et al., 2003), a mod-
i   ed ten-choice variation of the sat questions

table 3: a    ve-choice sat analogy question.

(turney, 2012), and the relational similarity dataset
from semeval 2012 task 2 (jurgens et al., 2012).5

4.1 five-choice sat questions

table 3 is an example of a question from the 374
   ve-choice sat questions. each    ve-choice ques-
tion yields    ve labeled quadruples, by combining the
stem with each choice. the quadruple hword, lan-
guage, note, musici is labeled positive and the other
four quadruples are labeled negative.

examples

since learning works better with balanced train-
ing data (japkowicz and stephen, 2002), we use the
symmetries of proportional analogies to add more
(lepage and shin-ichi, 1996).
positive
for each positive quadruple,
ha, b, c, di, we
add three more positive quadruples, hb, a, d, ci,
hc, d, a, bi, and hd, c, b, ai. thus each    ve-choice
question provides four positive and four negative
quadruples.

we use ten-fold cross-validation to apply super-
sim to the sat questions. the folds are constructed
so that the eight quadruples from each sat question
are kept together in the same fold. to answer a ques-
tion in the testing fold, the learned model assigns a
id203 to each of the    ve choices and guesses
the choice with the highest id203. supersim
achieves a score of 54.8% correct (205 out of 374).
table 4 gives the rank of supersim in the list of
the top ten results with the sat analogy questions.6
the scores ranging from 51.1% to 57.0% are not sig-
ni   cantly different from supersim   s score of 54.8%,
according to fisher   s exact test at the 95% con   -
dence level. however, supersim answers the sat

5the sat questions are available on request

from
the author. the semeval 2012 task 2 dataset is available at
https://sites.google.com/site/semeval2012task2/.

4weka is available at http://www.cs.waikato.ac.nz/ml/weka/.

http://aclweb.org/aclwiki.

6see the state of

the art page on the acl wiki at

bic  ici & yuret (2006)
herda  gdelen & baroni (2009)
turney & littman (2005)

reference

algorithm
know-best veale (2004)
id116
bagpack
vsm
dual-space turney (2012)
bmi
pairclass
pert
supersim    
lra
human

bollegala et al. (2009)
turney (2008b)
turney (2006a)

turney (2006b)
average college applicant

correct
43.0
44.0
44.1
47.1
51.1
51.1
52.1
53.5
54.8
56.1
57.0

algorithm
dual-space
supersim
supersim
supersim
supersim
supersim
supersim
supersim
supersim
supersim

features

lf ppmi dom fun correct
47.9
0
52.7
1
0
52.7
52.7
1
45.7
1
41.7
1
1
5.6
32.4
0
39.6
0
0
39.3

0
1
1
0
1
1
0
1
0
0

1
1
1
1
0
1
0
0
1
0

1
1
1
1
1
0
0
0
0
1

table 4: the top ten results on    ve-choice sat questions.

table 5: feature ablation with ten-choice sat questions.

questions in a few minutes, whereas lra requires
nine days, and supersim learns its models automat-
ically, unlike the hand-coding of turney (2012).

4.2 ten-choice sat questions

in general,

in addition to symmetries, proportional analogies
have asymmetries.
if the quadruple
ha, b, c, di is positive, ha, d, c, bi is negative. for
example, hword, language, note, musici is a good
analogy, but hword, music, note, languagei is not.
words are the basic units of language and notes are
the basic units of music, but words are not necessary
for music and notes are not necessary for language.
turney (2012) used this asymmetry to convert
the 374    ve-choice sat questions into 374 ten-
choice sat questions.
each choice hc, di was
expanded with the stem ha, bi,
resulting in the
quadruple ha, b, c, di, and then the order was shuf-
   ed to ha, d, c, bi, so that each choice pair in a    ve-
choice question generated two choice quadruples in
a ten-choice question. nine of the quadruples are
negative examples and the quadruple consisting of
the stem pair followed by the solution pair is the only
positive example. the purpose of the ten-choice
questions is to test the ability of measures of rela-
tional similarity to avoid the asymmetric distractors.
we use the ten-choice questions to compare
the hand-coded dual-space approach (turney, 2012)
with supersim. we also use these questions to per-
form an ablation study of the four sets of features
in supersim. as with the    ve-choice questions,
we use the symmetries of proportional analogies to
add three more positive examples, so the training

dataset has nine negative examples and four posi-
tive examples per question. we apply ten-fold cross-
validation to the 374 ten-choice questions.

on the ten-choice questions, supersim   s score
is 52.7% (table 5), compared to 54.8% on the
   ve-choice questions (table 4), a drop of 2.1%.
the hand-coded dual-space model scores 47.9%
(table 5), compared to 51.1% on the    ve-choice
questions (table 4), a drop of 3.2%. the dif-
ference between supersim (52.7%) and the hand-
coded dual-space model (47.9%) is not signi   cant
according to fisher   s exact test at the 95% con   -
dence level. the advantage of supersim is that it
does not need hand-coding. the results show that
supersim can avoid the asymmetric distractors.

table 5 shows the impact of different subsets of
features on the percentage of correct answers to the
ten-choice sat questions.
included features are
marked 1 and ablated features are marked 0. the
results show that the log frequency (lf) and ppmi
features are not helpful (but also not harmful) for
relational similarity. we also see that domain space
and function space are both needed for good results.

4.3 semeval 2012 task 2

the semeval 2012 task 2 dataset is based on the
semantic relation classi   cation scheme of bejar et
al. (1991), consisting of ten high-level categories
of relations and seventy-nine subcategories, with
paradigmatic examples of each subcategory. for
instance,
the subcategory taxonomic in the cate-
gory class inclusion has three paradigmatic exam-
ples,    ower:tulip, emotion:rage, and poem:sonnet.

tovar et al. (2012)
pedersen (2012)
pedersen (2012)
pedersen (2012)

algorithm reference
buap
duluth-v2
duluth-v1
duluth-v0
utd-id166 rink & harabagiu (2012)
utd-nb
rink & harabagiu (2012)
id56-1600 mikolov et al. (2013)
utd-lda rink & harabagiu (2013)
com
supersim    

zhila et al. (2013)

spearman
0.014
0.038
0.039
0.050
0.116
0.229
0.275
0.334
0.353
0.408

table 6: spearman correlations for semeval 2012 task 2.

jurgens et al. (2012) used amazon   s mechanical
turk to create the semeval 2012 task 2 dataset in
two phases. in the    rst phase, turkers expanded the
paradigmatic examples for each subcategory to an
average of forty-one word pairs per subcategory, a
total of 3,218 pairs. in the second phase, each word
pair from the    rst phase was assigned a prototypical-
ity score, indicating its similarity to the paradigmatic
examples. the challenge of semeval 2012 task 2
was to guess the prototypicality scores.

supersim was trained on the    ve-choice sat
questions and evaluated on the semeval 2012 task 2
test dataset. for a given a word pair, we created
quadruples, combining the word pair with each of
the paradigmatic examples for its subcategory. we
then used supersim to compute the probabilities for
each quadruple. our guess for the prototypicality
score of the given word pair was the average of
the probabilities. spearman   s rank correlation coef-
   cient between the turkers    prototypicality scores
and supersim   s scores was 0.408, averaged over the
sixty-nine subcategories in the testing set. super-
sim has the highest spearman correlation achieved
to date on semeval 2012 task 2 (see table 6).

5 compositional similarity

this section presents experiments using supersim
to learn compositional similarity. the datasets con-
sist of triples, ha, b, ci, such that ab is a noun-
modi   er bigram and c is a noun unigram. the
triples are labeled as positive and negative exam-
ples of paraphrases. table 2 shows that the fea-
ture vectors have 675 elements. we experiment

stem:
choices:

solution:

fantasy world
fairyland
(1)
(2)
fantasy
(3) world
(4)
(5)
(6)
(7)
(1)

phantasy
universe
ranter
souring
fairyland

table 7: a noun-modi   er question based on id138.

with two datasets, seven-choice and fourteen-choice
noun-modi   er questions (turney, 2012).7

5.1 noun-modi   er questions

dataset,

the    rst dataset is a seven-choice noun-modi   er
question
from id138
(turney, 2012). the dataset contains 680 questions
for training and 1,500 for testing, a total of 2,180
questions. table 7 shows one of the questions.

constructed

the stem is a bigram and the choices are uni-
grams. the bigram is composed of a head noun
(world), modi   ed by an adjective or noun (fantasy).
the solution is the unigram (fairyland) that belongs
to the same id138 synset as the stem.

the distractors are designed to be dif   cult for cur-
rent approaches to composition. for example, if fan-
tasy world is represented by element-wise multipli-
cation of the context vectors for fantasy and world
(mitchell and lapata, 2010), the most likely guess is
fantasy or world, not fairyland (turney, 2012).

each seven-choice question yields seven labeled
triples, by combining the stem with each choice.
the triple hfantasy, world, fairylandi is labeled pos-
itive and the other six triples are labeled negative.

in general,

if ha, b, ci is a positive example,
then hb, a, ci is negative. for example, world fan-
tasy is not a paraphrase of fairyland. the sec-
ond dataset is constructed by applying this shuf-
   ing transformation to convert
the 2,180 seven-
choice questions into 2,180 fourteen-choice ques-
tions (turney, 2012). the second dataset is designed

7the

seven-choice

at
available
http://jair.org/papers/paper3640.html.
the
fourteen-choice dataset can be generated from the seven-choice
dataset.

dataset

is

algorithm
vector addition
element-wise multiplication
dual-space model
supersim
holistic model

correct

7-choices
50.1
57.5
58.3
75.9
81.6

14-choices
22.5
27.4
41.5
68.0
   

table 8: results for the two noun-modi   er datasets.

to be dif   cult for approaches that are not sensitive to
word order.

table 8 shows the percentage of the testing
questions that are answered correctly for the two
datasets. because vector addition and element-wise
multiplication are not sensitive to word order, they
perform poorly on the fourteen-choice questions.
for both datasets, supersim performs signi   cantly
better than all other approaches, except for the holis-
tic approach, according to fisher   s exact test at the
95% con   dence level.8

the holistic approach is noncompositional. the
stem bigram is represented by a single context vec-
tor, generated by treating the bigram as if it were
a unigram. a noncompositional approach cannot
scale up to realistic applications (turney, 2012). the
holistic approach cannot be applied to the fourteen-
choice questions, because the bigrams in these ques-
tions do not correspond to terms in id138, and
hence they do not correspond to row vectors in the
matrices we use (see section 3).

turney (2012) found it necessary to hand-code a
soundness check into all of the algorithms (vector
addition, element-wise multiplication, dual-space,
and holistic). given a stem ab and a choice c, the
hand-coded check assigns a minimal score to the
choice if c = a or c = b. we do not need to hand-
code any checking into supersim. it learns automat-
ically from the training data to avoid such choices.

5.2 ablation experiments
table 9 shows the effects of ablating sets of fea-
tures on the performance of supersim with the
fourteen-choice questions. ppmi features are the
most important; by themselves, they achieve 59.7%
correct, although the other features are needed to

8the results for supersim are new but the other results in

table 8 are from turney (2012).

algorithm
dual-space
supersim
supersim
supersim
supersim
supersim
supersim
supersim
supersim
supersim

features

lf ppmi dom fun correct
41.5
0
68.0
1
0
66.6
52.3
1
69.3
1
65.9
1
1
14.1
59.7
0
34.6
0
0
32.9

0
1
1
0
1
1
0
1
0
0

1
1
1
1
0
1
0
0
1
0

1
1
1
1
1
0
0
0
0
1

table 9: ablation with fourteen-choice questions.

ppmi feature subsets
ha, bi

ha, ci

1
0
1
1
1
0
0
0

1
1
0
1
0
1
0
0

hb, ci correct
68.0
59.9
65.4
67.5
62.6
58.1
55.6
52.3

1
1
1
0
0
0
1
0

table 10: ppmi subset ablation with fourteen-choices.

reach 68.0%. domain space features reach the sec-
ond highest performance when used alone (34.6%),
but they reduce performance (from 69.3% to 68.0%)
when combined with other features; however, the
drop is not signi   cant according to fisher   s exact
test at the 95% signi   cance level.

since the ppmi features play an important role in
answering the noun-modi   er questions, let us take
a closer look at them. from table 2, we see that
there are twelve ppmi features for the triple ha, b, ci,
where ab is a noun-modi   er bigram and c is a noun
unigram. we can split the twelve features into three
subsets, one subset for each pair of words, ha, bi,
ha, ci, and hb, ci. for example, the subset for ha, bi
is the four features ppmi(a, b, left), ppmi(b, a, left),
ppmi(a, b, right), and ppmi(b, a, right). table 10
shows the effects of ablating these subsets.

the results in table 10 indicate that all three
ppmi subsets contribute to the performance of
supersim, but the ha, bi subset contributes more
than the other two subsets. the ha, bi features help

stem:
choices:

solution:

search engine
search engine
search
engine
search language
search warrant
diesel engine
steam engine
search engine

(1)
(2)
(3)
(4)
(5)
(6)
(7)
(1)

table 11: a question based on holistic vectors.

to increase the sensitivity of supersim to the order
of the words in the noun-modi   er bigram; for exam-
ple, they make it easier to distinguish fantasy world
from world fantasy.

5.3 holistic training

supersim uses 680 training questions to learn to rec-
ognize when a bigram is a paraphrase of a unigram;
it learns from expert knowledge implicit in id138
synsets. it would be advantageous to be able to train
supersim with less reliance on expert knowledge.

past work with adjective-noun bigrams has shown
that we can use holistic bigram vectors to train
a supervised regression model
(guevara, 2010;
baroni and zamparelli, 2010). the output of the
regression model is a vector representation for a
bigram that approximates the holistic vector for the
bigram; that is, it approximates the vector we would
get by treating the bigram as if it were a unigram.

supersim does not generate vectors as output, but
we can still use holistic bigram vectors for training.
table 11 shows a seven-choice training question that
was generated without using id138 synsets. the
choices of the form a b are bigrams, but we repre-
sent them with holistic bigram vectors; we pretend
they are unigrams. we call a b bigrams pseudo-
unigrams. as far as supersim is concerned, there
is no difference between these pseudo-unigrams and
true unigrams. the question in table 11 is treated
the same as the question in table 7.

we generate 680 holistic training questions by
randomly selecting 680 noun-modi   er bigrams from
id138 as stems for the questions (search engine),
avoiding any bigrams that appear as stems in the
the solution (search engine)
testing questions.
is the pseudo-unigram that corresponds to the

correct

training
holistic
standard

7-choices
61.8
75.9

14-choices
54.4
68.0

table 12: results for supersim with holistic training.

in the matrices in section 3, each
stem bigram.
term in id138 corresponds to a row vector.
these corresponding row vectors enable us to treat
bigrams from id138 as if they were unigrams.
the distractors are the component unigrams in
the stem bigram (search and engine) and pseudo-
unigrams that share a component word with the stem
(search warrant, diesel engine). to construct the
holistic training questions, we used id138 as a
source of bigrams, but we ignored the rich infor-
mation that id138 provides about these bigrams,
such as their synonyms, hypernyms, hyponyms,
meronyms, and glosses.

table 12 compares holistic training to standard
training (that is, training with questions like table 11
versus training with questions like table 7). the
testing set is the standard testing set in both cases.
there is a signi   cant drop in performance with
holistic training, but the performance still surpasses
vector addition, element-wise multiplication, and
the hand-coded dual-space model (see table 8).

since holistic questions can be generated auto-
matically without human expertise, we experi-
mented with increasing the size of the holistic train-
ing dataset, growing it from 1,000 to 10,000 ques-
tions in increments of 1,000. the performance
on the fourteen-choice questions with holistic train-
ing and standard testing varied between 53.3% and
55.1% correct, with no clear trend up or down. this
is not signi   cantly different from the performance
with 680 holistic training questions (54.4%).

it seems likely that the drop in performance with
holistic training instead of standard training is due to
a difference in the nature of the standard questions
(table 7) and the holistic questions (table 11). we
are currently investigating this issue. we expect to
be able to close the performance gap in future work,
by improving the holistic questions. however, it is
possible that there are fundamental limits to holistic
training.

6 discussion

supersim performs slightly better (not statistically
signi   cant) than the hand-coded dual-space model
on relational similarity problems (section 4), but it
performs much better on compositional similarity
problems (section 5). the ablation studies suggest
this is due to the ppmi features, which have no effect
on ten-choice sat performance (table 5), but have a
large effect on fourteen-choice noun-modi   er para-
phrase performance (table 9).

one advantage of supervised learning over hand-
coding is that it facilitates adding new features. it
is not clear how to modify the hand-coded equations
for the dual-space model of noun-modi   er composi-
tion (turney, 2012) to include ppmi information.

supersim is one of the few approaches to distri-
butional semantics beyond words that has attempted
to address both relational and compositional similar-
ity (see section 2.3). it is a strength of this approach
that it works well with both kinds of similarity.

7 future work and limitations

given the promising results with holistic training for
noun-modi   er paraphrases, we plan to experiment
with holistic training for analogies. consider the
proportional analogy hard is to hard time as good
is to good time, where hard time and good time are
pseudo-unigrams. to a human, this analogy is triv-
ial, but supersim has no access to the surface form
of a term. as far as supersim is concerned, this
analogy is much the same as the analogy hard is to
dif   culty as good is to fun. this strategy automat-
ically converts simple, easily generated analogies
into more complex, challenging analogies, which
may be suited to training supersim.

this also suggests that noun-modi   er paraphrases
may be used to solve analogies.
perhaps we
can evaluate the quality of a candidate analogy
ha, b, c, di by searching for a term e such that
hb, e, ai and hd, e, ci are good paraphrases.
for
example, consider the analogy mason is to stone as
carpenter is to wood. we can paraphrase mason as
stone worker and carpenter as wood worker. this
transforms the analogy to stone worker is to stone
as wood worker is to wood, which makes it easier to
recognize the relational similarity.

another area for future work is extending super-
sim beyond noun-modi   er paraphrases to measur-
ing the similarity of sentence pairs. we plan to adapt
ideas from socher et al. (2011) for this task. they
use dynamic pooling to represent sentences of vary-
ing size with    xed-size feature vectors. using    xed-
size feature vectors avoids the problem of quadratic
growth and it enables the supervised learner to gen-
eralize over sentences of varying length.

some of the competing approaches discussed by
erk (2013) incorporate formal logic. the work of
baroni et al. (2012) suggests ways that supersim
could be developed to deal with logic.

we believe that supersim could bene   t from
more features, with greater diversity. one place to
look for these features is higher levels in the hierar-
chy that we sketch in section 3.

our ablation experiments suggest that domain and
function spaces provide the most important features
for relational similarity, but ppmi values provide the
most important features for noun-modi   er composi-
tional similarity. explaining this is another topic for
future research.

8 conclusion

in this paper, we have presented supersim, a uni   ed
approach to analogy (relational similarity) and para-
phrase (compositional similarity). supersim treats
them both as problems of supervised tuple classi   -
cation. the supervised learning algorithm is a stan-
dard support vector machine. the main contribution
of supersim is a set of four types of features for rep-
resenting tuples. the features work well with both
analogy and paraphrase, with no task-speci   c mod-
i   cations. supersim matches the state of the art on
sat analogy questions and substantially advances
the state of the art on the semeval 2012 task 2 chal-
lenge and the noun-modi   er paraphrase questions.

faster

supersim runs much

than lra
(turney, 2006b),
answering the sat questions
in minutes instead of days. unlike the dual-space
model (turney, 2012), supersim requires no hand-
coded similarity composition functions. since there
is no hand-coding, it is easy to add new features to
supersim. much work remains to be done, such
as incorporating logic and scaling up to sentence

paraphrases, but past work suggests that
problems are tractable.

these

in the four approaches described by erk (2013),
supersim is an instance of the second approach
to extending id65 beyond words,
comparing word pairs, phrases, or sentences (in gen-
eral, tuples) by combining multiple pairwise simi-
larity values. perhaps the main signi   cance of this
paper is that it provides some evidence in support of
this general approach.

references

[agirre et al.2012] eneko agirre, daniel cer, mona diab,
and aitor gonzalez-agirre.
2012. semeval-2012
task 6: a pilot on semantic textual similarity.
in
proceedings of the first joint conference on lexi-
cal and computational semantics (*sem), pages 385   
393, montr  eal, canada.

[androutsopoulos and malakasiotis2010] ion androut-
sopoulos and prodromos malakasiotis.
2010. a
survey of id141 and id123 meth-
ods.
journal of arti   cial intelligence research,
38:135   187.

[baroni and zamparelli2010] marco baroni and roberto
zamparelli. 2010. nouns are vectors, adjectives are
matrices: representing adjective-noun constructions
in semantic space. in proceedings of the 2010 confer-
ence on empirical methods in natural language pro-
cessing (emnlp 2010), pages 1183   1193.

[baroni et al.2012] marco baroni, raffaella bernardi,
2012.
ngoc-quynh do, and chung-chieh shan.
entailment above the word level
in distributional
semantics. in proceedings of the 13th conference of
the european chapter of the association for compu-
tational linguistics (eacl 2012), pages 23   32.

[bejar et al.1991] isaac i. bejar, roger chaf   n, and
susan e. embretson. 1991. cognitive and psychomet-
ric analysis of analogical problem solving. springer-
verlag.

[bic  ici and yuret2006] ergun bic  ici and deniz yuret.
2006. id91 word pairs to answer analogy ques-
tions.
in proceedings of the fifteenth turkish sym-
posium on arti   cial intelligence and neural networks
(tainn 2006), akyaka, mugla, turkey.

[bollegala et al.2008] danushka bollegala, yutaka mat-
suo, and mitsuru ishizuka. 2008. www sits the sat:
measuring relational similarity on the web.
in pro-
ceedings of the 18th european conference on arti   -
cial intelligence (ecai 2008), pages 333   337, patras,
greece.

[bollegala et al.2009] danushka bollegala, yutaka mat-
suo, and mitsuru ishizuka. 2009. measuring the sim-
ilarity between implicit semantic relations from the
web.
in proceedings of the 18th international con-
ference on world wide web (www 2009), pages 651   
660.

[bullinaria and levy2007] john bullinaria and joseph
levy. 2007. extracting semantic representations from
word co-occurrence statistics: a computational study.
behavior research methods, 39(3):510   526.

[bullinaria and levy2012] john bullinaria and joseph
levy. 2012. extracting semantic representations from
word co-occurrence statistics: stop-lists, id30,
and svd. behavior research methods, 44(3):890   
907.

[caron2001] john caron. 2001. experiments with lsa
in proceedings of
scoring: optimal rank and basis.
the siam computational information retrieval work-
shop, pages 157   169, raleigh, nc.

[church and hanks1989] kenneth church and patrick
hanks. 1989. word association norms, mutual infor-
mation, and id69. in proceedings of the 27th
annual conference of the association of computa-
tional linguistics, pages 76   83, vancouver, british
columbia.

[clarke2012] daoud clarke.

a context-
theoretic framework for compositionality in dis-
tributional semantics.
computational linguistics,
38(1):41   71.

2012.

[erk2013] katrin erk. 2013. towards a semantics for dis-
tributional representations. in proceedings of the 10th
international conference on computational semantics
(iwcs 2013), potsdam, germany.

[firth1957] john rupert firth. 1957. a synopsis of lin-
in studies in linguistic

guistic theory 1930   1955.
analysis, pages 1   32. blackwell, oxford.

[garrette et al.2011] dan garrette, katrin erk, and ray
mooney. 2011.
integrating logical representations
with probabilistic information using markov logic. in
proceedings of the 9th international conference on
computational semantics (iwcs 2011), pages 105   
114.

[gentner1983] dedre gentner. 1983. structure-mapping:
a theoretical framework for analogy. cognitive sci-
ence, 7(2):155   170.

[girju et al.2007] roxana girju, preslav nakov, vivi nas-
tase, stan szpakowicz, peter turney, and deniz yuret.
2007. semeval-2007 task 04: classi   cation of seman-
tic relations between nominals. in proceedings of the
fourth international workshop on semantic evalua-
tions (semeval 2007), pages 13   18, prague, czech
republic.

[guevara2010] emiliano guevara. 2010. a regression
model of adjective-noun compositionality in distribu-
tional semantics.
in proceedings of the 2010 work-
shop on geometrical models of natural language
semantics (gems 2010), pages 33   37.

[harris1954] zellig harris. 1954. distributional struc-

ture. word, 10(23):146   162.

[hendrickx et al.2010] iris hendrickx, su nam kim,
zornitsa kozareva, preslav nakov, diarmuid   o
s  eaghdha, sebastian pad  o, marco pennacchiotti,
lorenza romano, and stan szpakowicz.
2010.
semeval-2010 task 8: multi-way classi   cation of
semantic relations between pairs of nominals. in pro-
ceedings of the 5th international workshop on seman-
tic evaluation, pages 33   38, uppsala, sweden.

[herda  gdelen and baroni2009] amac   herda  gdelen and
marco baroni. 2009. bagpack: a general frame-
work to represent semantic relations. in proceedings
of the eacl 2009 geometrical models for natural
language semantics (gems) workshop, pages 33   40.
[japkowicz and stephen2002] nathalie japkowicz and
2002. the class imbalance prob-
intelligent data analysis,

shaju stephen.
lem: a systematic study.
6(5):429   449.

[jurgens et al.2012] david a. jurgens, saif m. moham-
mad, peter d. turney, and keith j. holyoak. 2012.
semeval-2012 task 2: measuring degrees of rela-
tional similarity.
in proceedings of the first joint
conference on lexical and computational semantics
(*sem), pages 356   364, montr  eal, canada.

[landauer and dumais1997] thomas k. landauer and
susan t. dumais. 1997. a solution to plato   s prob-
lem: the latent semantic analysis theory of the acqui-
sition,
induction, and representation of knowledge.
psychological review, 104(2):211   240.

[lemaire and denhi`ere2006] beno    t lemaire and guy
denhi`ere. 2006. effects of high-order co-occurrences
on word semantic similarity. current psychology let-
ters: behaviour, brain & cognition, 18(1).

[lepage and shin-ichi1996] yves lepage

and ando
shin-ichi. 1996. saussurian analogy: a theoretical
account and its application.
in proceedings of the
16th international conference on computational
linguistics (coling 1996), pages 717   722.

[lund et al.1995] kevin lund, curt burgess,

and
ruth ann atchley. 1995. semantic and associative
priming in high-dimensional semantic space.
in
proceedings of the 17th annual conference of the
cognitive science society, pages 660   665.

[mikolov et al.2013] tomas mikolov, wen-tau yih, and
2013. linguistic regularities in
geoffrey zweig.
continuous space word representations.
in proceed-
ings of the 2013 conference of the north american

chapter of the association for computational linguis-
tics: human language technologies (naacl 2013),
atlanta, georgia.

[mitchell and lapata2008] jeff mitchell and mirella lap-
ata. 2008. vector-based models of semantic composi-
tion. in proceedings of acl-08: hlt, pages 236   244,
columbus, ohio. association for computational lin-
guistics.

[mitchell and lapata2010] jeff mitchell and mirella lap-
ata. 2010. composition in distributional models of
semantics. cognitive science, 34(8):1388   1429.

[pedersen2012] ted pedersen. 2012. duluth: measur-
ing degrees of relational similarity with the gloss vec-
tor measure of semantic relatedness.
in first joint
conference on lexical and computational semantics
(*sem), pages 497   501, montreal, canada.

[platt1998] john c. platt. 1998. fast training of support
vector machines using sequential minimal optimiza-
tion.
in advances in kernel methods: support vec-
tor learning, pages 185   208, cambridge, ma. mit
press.

[rink and harabagiu2012] bryan rink

sanda
harabagiu. 2012. utd: determining relational simi-
larity using lexical patterns. in first joint conference
on lexical and computational semantics (*sem),
pages 413   418, montreal, canada.

and

[rink and harabagiu2013] bryan rink

sanda
harabagiu. 2013. the impact of selectional pref-
erence agreement on semantic relational similarity.
in proceedings of the 10th international conference
on computational semantics (iwcs 2013), potsdam,
germany.

and

[socher et al.2011] richard socher, eric h. huang, jef-
frey pennington, andrew y. ng, and christopher d.
2011. dynamic pooling and unfolding
manning.
recursive autoencoders for paraphrase detection.
in
advances in neural information processing systems
(nips 2011), pages 801   809.

[socher et al.2012] richard

socher,

brody huval,
christopher manning, and andrew ng. 2012. seman-
tic compositionality through recursive matrix-vector
spaces. in proceedings of the 2012 joint conference
on empirical methods in natural language process-
ing and computational natural language learning
(emnlp-conll 2012), pages 1201   1211.

[tovar et al.2012] mireya tovar,

j. alejandro reyes,
azucena montes, darnes vilari  no, david pinto, and
saul le  on.
2012. buap: a    rst approximation
to relational similarity measuring.
in first joint
conference on lexical and computational semantics
(*sem), pages 502   505, montreal, canada.

combining heterogeneous models for measuring rela-
tional similarity. in proceedings of the 2013 confer-
ence of the north american chapter of the associa-
tion for computational linguistics: human language
technologies (naacl 2013), atlanta, georgia.

[turney and littman2005] peter

and
michael l. littman.
2005. corpus-based learn-
ing of analogies and semantic relations. machine
learning, 60(1   3):251   278.

turney

d.

[turney and pantel2010] peter d. turney and patrick
pantel. 2010. from frequency to meaning: vector
space models of semantics. journal of arti   cial intel-
ligence research, 37:141   188.

[turney et al.2003] peter d. turney, michael l. littman,
jeffrey bigham, and victor shnayder. 2003. com-
bining independent modules to solve multiple-choice
synonym and analogy problems. in proceedings of the
international conference on recent advances in nat-
ural language processing (ranlp-03), pages 482   
489, borovets, bulgaria.

[turney et al.2011] peter d. turney, yair neuman, dan
assaf, and yohai cohen. 2011. literal and metaphor-
ical sense identi   cation through concrete and abstract
context.
in proceedings of the 2011 conference on
empirical methods in natural language processing,
pages 680   690.

[turney2006a] peter d. turney.

2006a.

expressing
implicit semantic relations without supervision.
in
proceedings of the 21st international conference on
computational linguistics and 44th annual meet-
ing of the association for computational linguistics
(coling/acl-06), pages 313   320, sydney, australia.
similarity
of semantic relations. computational linguistics,
32(3):379   416.

2006b.

[turney2006b] peter d. turney.

[turney2008a] peter d. turney. 2008a. the latent rela-
tion mapping engine: algorithm and experiments.
journal of arti   cial intelligence research, 33:615   
655.

[turney2008b] peter d. turney.

2008b. a uniform
approach to analogies, synonyms, antonyms, and
associations.
in proceedings of the 22nd interna-
tional conference on computational linguistics (col-
ing 2008), pages 905   912, manchester, uk.

[turney2012] peter d. turney.

2012. domain and
function: a dual-space model of semantic relations
and compositions. journal of arti   cial intelligence
research, 44:533   585.

[veale2004] tony veale. 2004. id138 sits the sat: a
knowledge-based approach to lexical analogy. in pro-
ceedings of the 16th european conference on arti   -
cial intelligence (ecai 2004), pages 606   612, valen-
cia, spain.

[witten et al.2011] ian h. witten, eibe frank,

and
mark a. hall. 2011. data mining: practical machine
learning tools and techniques, third edition. mor-
gan kaufmann, san francisco.

[zhila et al.2013] alisa zhila, wen-tau yih, christopher
meek, geoffrey zweig, and tomas mikolov. 2013.

