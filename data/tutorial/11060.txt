zero-shot learning through cross-modal transfer

richard socher, milind ganjoo, hamsa sridhar, osbert bastani, christopher d. manning, andrew y. ng

computer science department, stanford university, stanford, ca 94305, usa

richard@socher.org, {mganjoo, hsridhar, obastani, manning, ang}@stanford.edu

3
1
0
2

 
r
a

 

m
0
2

 
 
]

v
c
.
s
c
[
 
 

2
v
6
6
6
3

.

1
0
3
1
:
v
i
x
r
a

abstract

this work introduces a model that can recognize objects in images even if no
training data is available for the objects. the only necessary knowledge about the
unseen categories comes from unsupervised large text corpora. in our zero-shot
framework distributional information in language can be seen as spanning a se-
mantic basis for understanding what objects look like. most previous zero-shot
learning models can only differentiate between unseen classes. in contrast, our
model can both obtain state of the art performance on classes that have thousands
of training images and obtain reasonable performance on unseen classes. this is
achieved by    rst using outlier detection in the semantic space and then two sepa-
rate recognition models. furthermore, our model does not require any manually
de   ned semantic features for either words or images.

1

introduction

the ability to classify instances of an unseen visual class, called zero-shot learning, is useful in
many situations. there are many species, products or activities without labeled data and new visual
categories, such as the latest gadgets or car models are introduced frequently.
in this work, we
show how to make use of the vast amount of knowledge about the visual world available in natural
language to classify unseen objects. we attempt to model people   s ability to identify unseen objects
even if the only knowledge about that object came from reading about it. for instance, after reading
the description of a two-wheeled self-balancing electric vehicle, controlled by a stick, with which
you can move around while standing on top of it, many would be able to identify a segway, possibly
after being brie   y perplexed because the new object looks different to any previously observed
object class.
we introduce a zero-shot model that can predict both seen and unseen classes. for instance, without
ever seeing a cat image, it can determine whether an image shows a cat or a known category from
the training set such as a dog or a horse. the model is based on two main ideas.
first, images are mapped into a semantic space of words that is learned by a neural network model
[15]. word vectors capture distributional similarities from a large, unsupervised text corpus. by
learning an image mapping into this space, the word vectors get implicitly grounded by the visual
modality, allowing us to give prototypical instances for various words.
second, because classi   ers prefer to assign test images into classes for which they have seen training
examples, the model incorporates an outlier detection id203 which determines whether a new
image is on the manifold of known categories. if the image is of a known category, a standard
classi   er can be used. otherwise, images are assigned to a class based on the likelihood of being
an unseen category. the id203 of being an outlier or a known category is integrated into our
probabilistic model. the model is illustrated in fig 1.
unlike previous work on zero-shot learning which can only predict intermediate features or dif-
ferentiate between various zero-shot classes [26], our joint model can achieve both state of the art
accuracy on known classes as well as reasonable performance on unseen classes. furthermore, com-

1

figure 1: overview of our multi-modal zero-shot model. we    rst map each new testing image into a
lower dimensional semantic space. then, we use outlier detection to determine whether it is on the
manifold of seen images. if the image is not on the manifold, we determine its class with the help
of unsupervised semantic word vectors. in this example, the unseen classes are truck and cat.

pared to related work in knowledge transfer [19, 27] we do not require manually de   ned semantic
or visual attributes for the zero-shot classes. our language feature representations are learned from
unsupervised and unaligned corpora.
we    rst brie   y describe a selection of related work, followed by the model description and experi-
ments on cifar10.

2 related work

we brie   y outline connections and differences to    ve related lines of research. due to space con-
straints, we cannot do justice to the complete literature.
zero-shot learning. the work most similar to ours is that by palatucci et al. [26]. they map fmri
scans of people thinking about certain words into a space of manually designed features and then
classify using these features. they are able to predict semantic features even for words for which
they have not seen scans and experiment with differentiating between several zero-shot classes.
however, the do not classify new test instances into both seen and unseen classes. we extend their
approach to allow for this setup using outlier detection.
larochelle et al. [21] describe the unseen zero-shot classes by a    canonical    example or use ground
truth human labeling of attributes.
id62 id62 [17, 18] seeks to learn a visual object class by using very few
training examples. this is usually achieved by either sharing of feature representations [2], model
parameters [12] or via similar context [14]. a recent related work on id62 is that of
salakhutdinov et al. [28]. similar to their work, our model is based on using deep learning tech-
niques to learn low-level image features followed by a probabilistic model to transfer knowledge.
however, our work is able to classify object categories without any training data due to the cross-

2

manifold of known classesautohorsedogtrucknew test image from unknown classcatmodal knowledge transfer from natural language and at the same time obtain high performance on
classes with many training examples.
knowledge and visual attribute transfer. lambert et al. and farhadi et al. [19, 10] were two
of the    rst to use well-designed visual attributes of unseen classes to classify them. this is different
to our setting since we only have distributional features of words learned from unsupervised, non-
parallel corpora and can classify between categories that have thousands or zero training images. qi
et al. [27] learn when to transfer knowledge from one category to another for each instance.
id20. id20 is useful in situations in which there is a lot of training
data in one domain but little to none in another. for instance, in id31 one could train a
classi   er for movie reviews and then adapt from that domain to book reviews [4, 13]. while related,
this line of work is different since there is data for each class but the features may differ between
domains.
multimodal embeddings. multimodal embeddings relate information from multiple sources such
as sound and video [24] or images and text. socher et al. [30] project words and image regions into a
common space using kernelized canonical correlation analysis to obtain state of the art performance
in annotation and segmentation. similar to our work, they use unsupervised large text corpora to
learn semantic word representations. their model does require a small amount of training data
however for each class. among other recent work is that by srivastava and salakhutdinov [31] who
developed multimodal deep id82s. similar to their work, we use techniques from
the broad    eld of deep learning to represent images and words.
some work has been done on multimodal distributional methods [11, 22]. most recently, bruni et al.
[5] worked on perceptually grounding word meaning and showed that joint models are better able
to predict the color of concrete objects.

3 word and image representations

we begin the description of the full framework with the feature representations of words and images.
distributional approaches are very common for capturing semantic similarity between words. in
these approaches, words are represented as vectors of distributional characteristics     most often their
co-occurrences with words in context [25, 9, 1, 32]. these representations have proven very effective
in natural language processing tasks such as sense disambiguation [29], thesaurus extraction [23, 8]
and cognitive modeling [20].
we initialize all word vectors with pre-trained 50-dimensional word vectors from the unsupervised
model of huang et al. [15]. using free wikipedia text, their model learns word vectors by predict-
ing how likely it is for each word to occur in its context. their model uses both local context in
the window around each word and global document context. similar to other local co-occurrence
based vector space models, the resulting word vectors capture distributional syntactic and semantic
information. for further details and evaluations of these embeddings, see [3, 7].
we use the unsupervised method of coates et al. [6] to extract f image features from raw pixels in
an unsupervised fashion. each image is henceforth represented by a vector x     rf .

4 projecting images into semantic word spaces

in order to learn semantic relationships and class membership of images we project the image feature
vectors into the 50-dimensional word space. during training and testing, we consider a set of classes
y . some of the classes y in this set will have available training data, others will be zero-shot classes
without any training data. we de   ne the former as the seen classes ys and the latter as the unseen
classes yu. let w = ws     wu be the set of word vectors capturing distributional information for
both seen and unseen visual classes, respectively.
all training images x(i)     xy of a seen class y     ys are mapped to the word vector wy correspond-
ing to the class name. to train this mapping, we minimize the following objective function with

3

figure 2: id167 visualization of the semantic word space. word vector locations are highlighted
and mapped image locations are shown both for images for which this mapping has been trained and
unseen images. the unseen classes are cat and truck.

respect to the matrix        r50  f :

j(  ) =

(cid:88)

(cid:88)

y   ys

x(i)   xy

(cid:107)wy       x(i)(cid:107)2.

(1)

by projecting images into the word vector space, we implicitly extend the word semantics with a
visual grounding, allowing us to query the space, for instance for prototypical visual instances of a
word or the average color of concrete nouns.
fig. 2 shows a visualization of the 50-dimensional semantic space with word vectors and images
of both seen and unseen classes. the unseen classes are cat and truck. the mapping from 50 to 2
dimensions was done with id167 [33]. we can observe that most classes are tightly clustered around
their corresponding word vector while the zero-shot classes (cat and truck for this mapping) do not
have close-by vectors. however, the images of the two zero-shot classes are close to semantically
similar classes. for instance, the cat testing images are mapped most closely to dog, and horse and
are all very far away from car or ship. this motivated the idea for    rst    nding outliers and then
classifying them to the zero-shot word vectors.
now that we have covered the representations for words and images as well as the image to word
space mapping we can describe the probabilistic model for joint zero-shot learning and standard
image classi   cation.

5 zero-shot learning model

in this section we    rst give an overview of our model and then describe each of its components.
in general, we want to predict p(y|x), the id155 for both seen and unseen classes
y     ys     yu given an image x. because standard classi   ers will never predict a class that has
no training examples, we introduce a binary visibility random variable which indicates whether an

4

  airplaneautomobilebirdcatdeerdogfroghorseshiptruckcatautomobiletruckfrogshipairplanehorsebirddogdeer(cid:88)

image is in a seen or unseen class v     {s, u}. let xs be the set of all feature vectors for training
images of seen classes.
we predict the class y for a new input image x via:

p(y|x, xs, w,   ) =

p (y|v, x, xs, w,   )p (v |x, xs, w,   ).

(2)

marginal: p (x|xs, ws,   ) = (cid:80)

v    {s,u}
next, we will describe each factor in eq. 2.
the term p (v = u|x, xs, w,   ) is the id203 of an image being in an unseen class. it can
be computed by thresholding an outlier detection score. this score is computed on the manifold of
training images that were mapped to the semantic word space. we use a threshold on the marginal of
each point under a mixture of gaussians. the mapped points of seen classes are used to obtain this
n (  x|wy,   y)p (y). the gaussian
of each class is parameterized by the corresponding semantic word vector wy for its mean and a
covariance matrix   y that is estimated from all the mapped training points with that label. we
restrict the gaussians to be isometric to prevent over   tting.
for a new image x, the outlier detector then becomes the indicator function that is 1 if the marginal
id203 is below a certain threshold t :

p (x|y)p (y) = (cid:80)

y   ys

y   ys

p (v = u|x, xs, w,   ) := 1{p (x|xs, ws,   ) < t}

(3)

we provide an experimental analysis for various thresholds t below.
in the case where v = s, i.e. the point is considered to be of a known class, we can use any clas-
si   er for obtaining p (y|v = s, x, xs). we use a softmax classi   er on the original f -dimensional
features. for the zero-shot case where v = u we assume an isometric gaussian distribution around
each of the zero-shot semantic word vectors.
an alternative would be to use the method of kriegel et al. [16] to obtain an outlier id203 for
each testing point and then use the weighted combination of classi   ers for both seen and unseen
classes.

6 experiments

we run most of our experiments on the cifar10 dataset. the dataset has 10 classes, each with
5000 32    32    3 rgb images. we use the unsupervised feature extraction method of coates and
ng [6] to obtain a 12,800-dimensional feature vector for each image. in the following experiments,
we omit the training images of 2 classes for the zero-shot analysis.

6.1 zero-shot classes only

in this section we compare classi   cation between only two zero-shot classes. we observe that if
there is no seen class that is remotely similar to the zero-shot classes, the performance is close to
random. in other words, if the two zero-shot classes are the most similar classes and the seen classes
do not properly span the subspace of the zero-shot classes then performance is poor. for instance,
when cat and dog are taken out from training, the resulting zero-shot classi   cation does not work
well because none of the other 8 categories is similar enough to learn a good feature mapping. on
the other hand, if cat and truck are taken out, then the cat vectors can be mapped to the word space
thanks to transfer from dogs and trucks can be mapped thanks to car, so the performance is very
high.
fig. 3 shows the performance at various cutoffs for the outlier detection. the cutoff is de   ned on
the negative log-likelihood of the marginal of each point in the outlier detection. we can observe
that when classifying images of unseen classes into only zero-shot classes (right side of the    gure),
we can differentiate images with an accuracy of above 80%.

6.2 zero-shot and seen classes

in fig. 3 we can observe that depending on the threshold that splits images into seen or unseen
classes at test time we can obtain accuracies of trained classes of approximately 80%. at 70%

5

figure 3: visualization of the accuracy of the seen classes (lines from the top left to bottom right)
and pairs of zero-shot classes (lines from bottom left to top right) at different thresholds of the
negative log-likelihood of each mapped test image vector.

accuracy, unseen classes can be classi   ed with accuracies of between 30% to 15%. random chance
is 10%.

7 conclusion

we introduced a novel model for joint standard and zero-shot classi   cation based on deep learned
word and image representations. the two key ideas are that (i) using semantic word vector repre-
sentations can help to transfer knowledge between categories even when these representations are
learned in an unsupervised way and (ii) that our bayesian framework that    rst differentiates outliers
from points on the projected semantic manifold can help to combine both zero-shot and seen classi-
   cation into one framework. if the task was only to differentiate between various zero-shot classes
we could obtain accuracies of up to 90% with a fully unsupervised model.

references

[1] m. baroni and a. lenci. distributional memory: a general framework for corpus-based semantics.

computational linguistics, 36(4):673   721, 2010.

[2] e. bart and s. ullman. cross-generalization: learning novel classes from a single example by feature

replacement. in cvpr, 2005.

[3] y. bengio, r. ducharme, p. vincent, and c. janvin. a neural probabilistic language model. j. mach.

learn. res., 3, march 2003.

[4] j. blitzer, m. dredze, and f. pereira. biographies, bollywood, boom-boxes and blenders: domain

adaptation for sentiment classi   cation. in acl, 2007.

[5] e. bruni, g. boleda, m. baroni, and n. tran. id65 in technicolor. in acl, 2012.

6

0102030405060708000.10.20.30.40.50.60.70.80.91negative log p(x)classification accuracy  automobile   frogdeer   truckfrog   truckhorse   shiphorse   truck[6] a. coates and a. ng. the importance of encoding versus training with sparse coding and vector

quantization . in icml, 2011.

[7] r. collobert and j. weston. a uni   ed architecture for natural language processing: deep neural networks

with multitask learning. in icml, 2008.

[8] j. curran. from distributional to semantic similarity. phd thesis, university of edinburgh, 2004.
[9] k. erk and s. pad  o. a structured vector space model for word meaning in context. in emnlp, 2008.
[10] a. farhadi, i. endres, d. hoiem, and d. forsyth. describing objects by their attributes. in cvpr, 2009.
[11] y. feng and m. lapata. visual information in semantic representation. in hlt-naacl, 2010.
[12] m. fink. object classi   cation from a single example utilizing class relevance pseudo-metrics. in nips,

2004.

[13] x. glorot, a. bordes, and y. bengio. id20 for large-scale sentiment classi   cation: a deep

learning approach. in icml, 2011.

[14] d. hoiem, a.a. efros, and m. herbert. geometric context from a single image. in iccv, 2005.
[15] e. h. huang, r. socher, c. d. manning, and a. y. ng.

improving word representations via global

context and multiple word prototypes. in acl, 2012.

[16] h. kriegel, p. kr  oger, e. schubert, and a. zimek. loop: local outlier probabilities. in proceedings of

the 18th acm conference on information and knowledge management, cikm    09, 2009.
[17] r.; perona l. fei-fei; fergus. id62 of object categories. tpami, 28, 2006.
[18] b. m. lake, j. gross r. salakhutdinov, and j. b. tenenbaum. one shot learning of simple visual concepts.

2011.

[19] c. h. lampert, h. nickisch, and s. harmeling. learning to detect unseen object classes by between-

class attribute transfer. in cvpr, 2009.

[20] t. k. landauer and s. t. dumais. a solution to plato   s problem: the latent semantic analysis theory of

acquisition, induction and representation of knowledge. psychological review, 104(2):211   240, 1997.

[21] h. larochelle, d. erhan, and y. bengio. zero-data learning of new tasks. in aaai, 2008.
[22] c.w. leong and r. mihalcea. going beyond text: a hybrid image-text approach for measuring word

relatedness. in ijcnlp, 2011.

[23] d. lin. automatic retrieval and id91 of similar words.

768   774, 1998.

in proceedings of coling-acl, pages

[24] j. ngiam, a. khosla, m. kim, j. nam, h. lee, and a.y. ng. multimodal deep learning. in icml, 2011.
[25] s. pado and m. lapata. dependency-based construction of semantic space models. computational lin-

guistics, 33(2):161   199, 2007.

[26] m. palatucci, d. pomerleau, g. hinton, and t. mitchell. zero-shot learning with semantic output codes.

in nips, 2009.

[27] guo-jun qi, c. aggarwal, y. rui, q. tian, s. chang, and t. huang. towards cross-category knowledge

propagation for learning visual concepts. in cvpr, 2011.

[28] a. torralba r. salakhutdinov, j. tenenbaum. learning to learn with compound hierarchical-deep models.

in nips, 2012.

[29] h. sch  utze. automatic word sense discrimination. computational linguistics, 24:97   124, 1998.
[30] r. socher and l. fei-fei. connecting modalities: semi-supervised segmentation and annotation of images

using unaligned text corpora. in cvpr, 2010.

[31] n. srivastava and r. salakhutdinov. multimodal learning with deep id82s. in nips, 2012.
[32] p. d. turney and p. pantel. from frequency to meaning: vector space models of semantics. journal of

arti   cial intelligence research, 37:141   188, 2010.

[33] l. van der maaten and g. hinton. visualizing data using id167. journal of machine learning research,

2008.

7

