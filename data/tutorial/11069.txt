5
1
0
2

 

b
e
f
8
2

 

 
 
]
l
c
.
s
c
[
 
 

1
v
8
6
1
0
0

.

3
0
5
1
:
v
i
x
r
a

the nlp engine: a universal turing machine for nlp

jiwei li1 and eduard hovy2

1computer science department, stanford university, stanford, ca 94305

2language technology institute, carnegie mellon university, pittsburgh, pa 15213

jiweil@stanford.edu

ehovy@andrew.cmu.edu

abstract

it
is commonly accepted that machine
translation is a more complex task than
id52. but how much
more complex?
in this paper we make
an attempt to develop a general framework
and methodology for computing the in-
formational and/or processing complexity
of nlp applications and tasks. we de-
   ne a universal framework akin to a turn-
ing machine that attempts to    t (most)
nlp tasks into one paradigm. we cal-
culate the complexities of various nlp
tasks using measures of shannon id178,
and compare    simple    ones such as part
of speech tagging to    complex    ones such
as machine translation. this paper pro-
vides a    rst, though far from perfect, at-
tempt to quantify nlp tasks under a uni-
form paradigm. we point out current de-
   ciencies and suggest some avenues for
fruitful research.

1 introduction

the purpose of this paper is to suggest a uni   ed
framework in which modern nlp research can
quantitatively describe and compare nlp tasks.
even though everyone agrees that some nlp tasks
are more complex than others, e.g., machine trans-
lation is    harder    than syntactic parsing, which in
turn is    harder    than part-of-speech tagging, we
cannot compute the relative complexities of dif-
ferent nlp tasks and subtasks.

in the typical current nlp paradigm,

re-
searchers apply several machine learning algo-
rithms to a problem, report on their performance
levels, and establish the winner as setting the level
to beat in the future. we have no single overall
model of nlp that subsumes and regularizes its
various tasks. if you were to ask nlp researchers

today they would say that no such model is possi-
ble, and that nlp is a collection of several semi-
independent research directions that all focus on
language and mostly use machine learning tech-
niques. researchers will tell you that a good sum-
marization system on duc/tac dataset obtains
a id8 score of 0.40, a good french-english
translation system achieves a blue score of 37.0,
20-news classi   ers can achieve accuracy of 0.85,
and id39 systems a recall of
0.95, and these numbers are not comparable. fur-
ther, we usually pay little attention to additional
important factors such as the performance curve
with respect to the amount of training data, the
amount of preprocessing required, the size and
complexity of auxiliary information required, etc.
and even when some studies do report such num-
bers, in nlp we don   t know how to characterize
these aspects in general and across applications,
how to quantify them in relationship to each other.
we here describe our    rst attempt to develop a
single generic high-level model of nlp. we adopt
the model of a universal machine, akin to a turing
machine but speci   c to the concerns of language
processing, and show how it can be instantiated in
different ways for different applications. we em-
ploy shannon id178 within the machine to mea-
sure the complexity of each nlp task.

in his epoch-making work, shannon (1951)
demonstrated how to compute the amount of in-
formation in a message. he considered the case
in which a string of input symbols is considered
one by one, and the uncertainty of the next is mea-
sured by counting how dif   cult it is to guess. we
make the fundamental assumption that most nlp
tasks can be viewed as transformations of nota-
tion, in which a stream of input symbols is trans-
formed and/or embellished into a stream of output
symbols (for example, id52 is the task of
embellishing each symbol with its tag, and mt is
the task of outputting the appropriate translation

word(s)). under this assumption one can ask: how
much uncertainty is there in making the embellish-
ment or transformation? this clearly depends on
the precise nature of the task, on the associated
auxiliary knowledge resources, and on the actual
algorithm employed. we discuss each of these is-
sues below. we    rst describe the key challenge in-
volved in performing uncertainty comparison us-
ing the id178 measure in section 2.
in sec-
tion 3 we provide high-level comments on what
properties a framework should have to enable fair
complexity comparison.
in section 4, based on
the properties identi   ed in section 3, we consider
the theoretical nature of nlp tasks and provide
suggestions for instantiating the paradigm. the
framework is described in sections 5, 6 and our re-
sults are presented in section 7. we point out cur-
rent de   ciencies and suggest avenues for fruitful
research in section 8, followed by a conclusion.

2 the dilemma for shannon id178

2.1 review of id178 and cross id178
id178, denoted as     px p(y) log(y), illustrates
the amount of information contained in a mes-
sage, and can be characterized as the uncertainty
of a random variable of a process. for example,
shannon (1951) reported an upper bound of 1.3
bits/character symbol for english character predic-
tion and 5.9 bits/word symbol for english word
prediction, meaning that it is highly likely that en-
glish word prediction is a harder task than english
character prediction.

if the output y n = {y0, y1, ...yn} is a sequence
generated from the input, a stationary stochastic
process. then the id178 of y is given by:

h(y ) = lim
n      

h(yn|yn   1, yn   2..., )

(1)

by the shannon-mcmillan-breiman theorem
(algoet and cover, 1988) this can be written as:

h(y ) = lim
n      

   

1
n

log p (y1, y2, ..., yn)

(2)

so we can de   ne its hardness or complexity by
computing id178 from the distribution p (y ) for
tasks like shannon   s word prediction model, or ex-
tend it to a id87 (shannon, 1948):
given a sequence of inputs x, the uncertainty of
the output transformation is given by h(y |x), in-
terpreted as the amount of uncertainty remaining
about y when x is already known.

the true distribution over y is hard to estimate.
normally we estimate the upper bound of id178
    the cross id178 denoted as h(p,   p )   -to ap-
proximate the true value of id178:

h(p,   p ) = h(p ) + dkl(p ||   p )     h(p ) (3)

where dkl(p ||   p ) denotes the kl divergence
between two distributions p and   p . a good
model will closely approximate p using   p , lead-
ing to smaller value of dkl(p ||   p ), i.e., bringing
the value cross-id178 closer to that of the real
one. different models would obtain different
values of h(p,   p ). various studies since shan-
non   s
(e.g.,(kucera and francis, 1967;
cover and king, 1978;
gopinath and cover, 1987; brown et al., 1992))
have explored methods to lower the upper bound
of character prediction id178 in english by
using more sophisticated models.

work

2.2 the dilemma for id178
while id178 describes the intrinsic nature of the
problem or task, its actual value estimation has to
be determined by the speci   c model you adopt for
prediction. when shannon approached the char-
acter prediction task, his wife acted as the predic-
tor. alternatively, if shannon had used a child as
predictor, he would have obtained a much larger
estimated id178.

similarly, if one wishes to compare the id178
of two tasks, for example, to determine which lan-
guage sequence is harder to predict, english or
french, it would be problematic if one compares
the id178 computed via a linguist for english
and a child for french. one requires twins who
are mathematically and linguistically identical in
terms of english and french for a fair compari-
son (cover and thomas, 2012). however, in real
world, it is almost impossible to    nd such twins.
different models are attuned differently to dif-
ferent scenarios, tasks, datasets, evaluation met-
rics, parameter settings, or optimization strategies.
one model might not    t all tasks equally well,
e.g., id166s are not designed to predict probabil-
ities, crfs offer more insights in sequence label-
ing tasks than id166s but are hard to use straight-
forwardly for text classi   cation, etc.

in summary, though id178 provides a theo-
retical de   nition about the uncertainty of a data
source or task, the fact that its estimation must
be performed using a real speci   c model poses a

dilemma for the accurate estimation of the uncer-
tainty of tasks and hence for their fair comparison.

3 prerequisites for fair comparison

we claim that a framework should incorporate the
following elements to enable a fair complexity
comparison of disparate nlp tasks and systems:

a universal measure: complexity can be mea-
sured in terms of multiple aspects (e.g., the amount
of training data required, the amount of prepro-
cessing required, the size and complexity of aux-
iliary information, training time, memory usage,
or even lines of code). but we need a universal
and appropriate metric. in this work, we propose
shannon id178 as the universal metric, which
we believe re   ects the intrinsic randomness, pre-
dictability, and uncertainty of datasets and tasks.
all the above aspects are highly correlated with
shannon id178.

a universal engine: a id52 system
makes decisions by selecting tags with highest
id203 while a summarization system selects
the top-ranked sentences. a fair comparison of
complexity, however, requires a single general and
uni   ed engine to de   ne all (at least most of) nlp
tasks within the same framework. the abovemen-
tioned notation transformation paradigm, elabo-
rated in the following section, accommodates most
nlp tasks.

a universal model: we cannot fairly compare
the id178 obtained from a id28
model on pos tags to that produced from a large
framework of interdependent alignment, phrase
extraction, decoding algorithms for machine trans-
lation. a uni   ed model should work with predic-
tions for all (or at least most) current nlp tasks,
and should make relatively accurate predictions (a
random guess model, for example, is general but
would not be helpful). we propose in section 6 a
candidate model.

4 the nature of nlp

in order to propose a single multi-purpose uni-
   ed engine for nlp one has to adopt a very gen-
eral perspective. when constructing an nl sys-
tem one typically assembles a variety of compo-
nents. some of them are active modules that in-
stantiate algorithms and perform transformations.
others are passive resources (like lexicons, proba-
bility tables, or rulesets) that support the former.

active modules are sometimes built to produce
passive ones.
it is important to differentiate the
role of modules in a framework in order to prop-
erly estimate the overall complexity. in this sec-
tion we    rst categorize the primary roles of nlp
(sub)systems and then postulate that modern nlp
algorithms (largely) fall into three distinct com-
plexity types.

4.1 three classes of nlp system

nlp systems generally perform one of the follow-
ing three functions/roles: (i) research into aspects
of the nature of language(s), (ii) application tasks
and subtasks, and (iii) support algorithms. the
majority of nlp development today falls into the
second and third classes.

research into language includes such studies as
determining the zip   an nature and the id178 of
language, discovering changes in patterns of use
over time and across geographic regions, identify-
ing text genres by for example creating word and
constituent distribution pro   les, and so on.

application tasks include machine translation,
information retrieval, id103, natural
language interpretation (both syntactic parsing and
semantic analysis), information extraction, ques-
tion answering, dialogue processing, text summa-
rization, text (sentence and multi-sentence) gen-
eration, id31 / opinion mining, text
mining / harvesting, and others. subtasks in-
clude id52, chunking, corefer-
ence resolution, text segmentation, query analy-
sis, bitext alignment, reference generation, pro-
   ling/characterization of language producers, and
many others, as well as numerous resource cre-
ation tasks including building monolingual and
bilingual lexicons, distributional semantic word
pro   les and embeddings, word sense lists, ontolo-
gies / taxonomies, word-sentiment lists, and many
others.

support algorithms include a variety of generic
procedures that are reused in many applications.
in addition to the classic finite state / augmented
transition network technology from the 1960s
and later, modern nlp usually works with the sta-
tistical properties of large collections of words,
and modern support algorithms such as id48s
and others generally assign and use count-derived
scores to [sets of] words, such as tf.idf, pmi,
and others, or distribute id203 across sets
of labels, words or documents, such as expecta-

tion maximization, probabilistic graphical mod-
els, id96 algorithms, certain id91
algorithms, etc. some support algorithms focus
on processing human labeling (annotation) and
comparing the results of various different label-
ing agents (human and machine), such as jensen-
shannon and other distribution comparison scor-
ing, annotation optimization procedures, etc.

4.2 three levels of nlp algorithm

we postulate that (almost every) nlp task / sub-
task can be de   ned as [a combination of] one of
three basic operations, listed in order of complex-
ity:

level 1: prediction: the algorithm reads its
input, which includes principally a sequence of
units of some kind, and predicts the next item in
the sequence. example: predicting the next word
in a stream, as used by shannon to calculate the
information content of text.

level 2: labeling: the algorithm reads its in-
put and generate label(s) based on it. labeling
tasks can be divided into two subcategories:

    aligned labeling:

there is a one-to-one
correspondence between inputs and outputs.
aligned tasks include most
tagging tasks
(e.g., named entity tagging and part-of-
speech tagging).

    unaligned labeling: no aligned correspon-
dence exists between inputs and outputs. un-
aligned sequence-label tasks can be further
divided into single-label tasks such as cate-
gorization or id91, in which a single la-
bel is assigned given the input (e.g., classi   -
cation of documents each into one class), and
sequence-label tasks, in which a sequence of
labels is produced (e.g., mt, where the labels
are target language words).

level 3: scoring: the algorithm reads its in-
put and assigns a score (without loss of general-
ity, a real value between zero and unity) to some
unit(s) in it. the score may be a id203, rat-
ing, or some other score. example: tf.idf scoring
of words.

in a probabilistic paradigm, the id203 of
level 1 tasks can be characterized as p (y ) where
y denotes the sequence to predict. level 2 tasks
can be characterized as p (y |x) where x denotes
the input and y denotes the label(s) to generate.

often, one operation is used to perform another.
it is typical in modern-day (post-1990s) nlp to
perform all kinds of labeling (level 2 operations)
by scoring all relevant possible categories (a level
3 operation) and then returning the highest-scoring
one as the selected tag. this contrasts with pre-
1990s nlp that generally computed a single re-
sult, such as the desired label, as the one and only
possible answer.

a task may require several operations in se-
quence. for example, syntactic parsing requires
labeling the part of speech tag of each word, la-
beling the start and end words of syntactic con-
stituents, labeling the head of each constituent,
and labeling the syntactic role of each constituent
with regard to its immediate head. sometimes the
label is drawn from a small set of possible tags that
is prede   ned by theorists or the researcher, such as
the part of speech tags. sometimes the label is pro-
vided in the text, such as the head word of a syn-
tactic constituent. sometimes the label is a value
computed by a scoring operation, such as the pmi
score of a word pair in a corpus.

5 the universal nlp engine

the generic nlp engine contains (see figure 1):
the transformation engine e, which takes as
input one or more symbols from s and produces
zero or more labels in response.

the input stream x, which contains the text
(without loss of generality, we talk about text (a
sequence of words and punctuation), but s might
instead be a sequence of symbols from some other
vocabulary, such as part of speech tags, or a mix-
ture of several vocabularies, such as words with
their individual part of speech tags). we therefore
consider s as consisting of an essentially in   nite
stream of units, each unit being a symbol (or set
of associated symbols) for which a label (or set of
labels) is to be computed by e. let x denote the
set of source symbols.

the data resource(s) r, typically a lexicon, a
grammar, a id203 model, or the output of
some subtask, used by e to perform its transfor-
mation.

the output label(s) y, a set of prede   ned sym-
bols that e produces. let y denote the set of target
symbols (including labels). we have    y     y, y    
y and also possibly x     y.

we next describe a generic procedure for imple-
menting the machine based on the following as-

sumption.
assumption 0.1 [most] modern nlp tasks can
be viewed as predicting a (sequence of) token(s)
(i.e., y n) using a    nite-state turing machine.
such a procedure allows one to measure and com-
pare various aspects of [almost] any nlp task
and subtask in a systematic way, and to thereby
compare the computational properties of alterna-
tive approaches and implementations to any nlp
(sub)task.

the following examples, using the same input
stream x n=   dog eats apple   , illustrate how the
engine works by phrasing several modern nlp
tasks as sequential token prediction problems:

    sentiment classi   cation:

y = {   -1   ,    0   ,    1   }, respectively for nega-
tive, neutral, and positive sentiment
y n=   0    (meaning: neutral sentiment).

    id52:

y = {id32 pos tags}
yn =   nnp vbz nn   .

    syntactic parsing:

y = {   (root   ,    (s   ,    (np   ,    )   , ...}
yn=(root (s (np (nnp ) ) (vp (vbz ) (np
(nn ) ) ) ) ).

    semantic analysis:

y = {english word list, relation list...}
y n=       e . eat ( e )     agent ( e , dog )     patient
( e , apple )   .

    id51:
y = {   1   ,    2   ,    3   ,    4   , ...}
yn=   1 3 1   , correspond to the 1st, 3rd, and
1st senses for the correspondent token.

    machine translation:

y = {french words, punctuations}
yn=   chien mange pomme   .

    summarization:

y = {english words, punctuations}
yn=   dog eats apple    (the gold-standard sum-
mary is the original sentence).

progress

the proposed framework is

inspired by
sequence-to-sequence
recent
as ma-
prediction models
(sutskever et al., 2014;
chine
translation
bahdanau et al., 2014;
vinyals et al., 2014;
cho et al., 2014; graves et al., 2014) and the

of
in nlp,

such

work of andreas et al. (2013) that illustrates that
id29 can to some extent be viewed as
a machine translation problem. treating syntactic
parsing as a string prediction task is de   ned and
implemented in (vinyals et al., 2014).

6 the universal id178 model

in this section, we discuss the generic model for
computing id178 within a language engine.

6.1 requirements

we    rst identify requirements for the universal
model. the random guess model satis   es the gen-
erality property as it can be used in any predictive
model. but it of course does not constitute a good
predictive model, since it delivers estimated dis-
tributions far away from the actual distribution. in
contrast, n-order markov models (n=1,2,3...) seem
to serve the purpose well and can be easily ap-
plied in sequence-labeling tasks such as ner and
id52. however it is tricky to adapt them to
single-label predictions such as text classi   cation.
additionally, their dependence on speci   c feature
selections make fair comparison across different
implementations complicated or impossible.

this consideration leads to the    rst requirement

for a model:

requirement 0.1 the model should be able to
leverage different types of features automatically
to avoid in   nitely complicated feature engineering
procedures.

we consider p (y n) and p (y n|x n) to gain in-
sights about better predictions for id178 calcu-
lation. we use p (y n) for illustration as it can be
easily extended to p (y n|x n).
in the sequence
prediction task, let fn denote the id178 where
predictions are made given previous n tokens (an
n-order markov model). as proved by shan-
non (shannon, 1951), fn monotonically decreases
with respect to n and h(y n) is strictly bounded
by fn:

f1     f2     ...     f        h(y n)

(4)

taking as example an id165 word predic-
tion model,
theoretically estimated id178 de-
creases as predictions are made based on increas-
ingly many preceding tokens, roughly stated in
(shannon, 1951). however, issues arise when n is
too large to maintain an id165 id203 table.

based shannon   s proof, for the purpose of to
the largest extent approximating real id178 us-
ing estimated id178, we generalize the second
property of the model as follows:

sequence-to-sequence

these thoughts are inspired by recent progress
of
generation models
(sutskever et al., 2014;
bahdanau et al., 2014;
vinyals et al., 2014; cho et al., 2014).

requirement 0.2 the model should be able to
memorize earlier information as much as possible
given the computing power and storage capacity
available.

6.2 model

this line of thinking suggests using as model
recurrent neural networks (mikolov et al., 2010;
or
funahashi and nakamura, 1993)
lstm
sophisticated
like
(hochreiter and schmidhuber, 1997)
(please
see appendix for details about lstm models).

versions

recurrent networks obtain a    xed-sized vector
for each step within the processing sequence by
convoluting current information with output from
earlier step(s). such vectors can be viewed as
combining evidence obtained so far, and are used
to predict the subsequent token(s), typically using
a softmax function. for labeling tasks, recurrent
neural networks    rst map the input x n of arbi-
trary length to a    xed-sized vector, which can be
viewed as evidence, and then map that vector to
the output by convoluting feature representations
at each step.

recurrent models have the following merits:
(1) they obey requirement 1 by automatically
encoding    features    in the real-valued represen-
tation vectors without explicit feature selection
and engineering. though the models still re-
quire signi   cant parameter tuning, they provide
a relatively uni   ed procedure for comparison.
(2) by sequentially convoluting each token with
output from earlier step(s) they have the ability
to    remember    information required to approxi-
mate (to some degree) the id155
of limn       p (yt|yt   1, yt   2, ..., yt   n), which par-
tially addresses requirement 2.
(3) the model
is manageable since it uses constant memory size
and runs in linear time.

we explicitly do not claim that recurrent neural
in an
models are a perfect choice as model
nlp engine. we acknowledge their numerous
shortcomings, and discuss some pros and cons in
the concluding section. however, we believe that
they do offer advantages over other models we
have considered with regard to tradeoffs of gen-
erality, computing power, and storage capacity.

7 experiments: comparing the

uncertainty of nlp tasks

using the above framework, we now calculate the
exact id178 for a few nlp tasks. what must
never be overlooked however is the impact of the
training/testing datasets used (e.g., the complex-
ity for guessing subsequent words in novels and
newspapers can be different) and how exactly the
task is de   ned (e.g., differences of complexity in
sentiment classi   cation between a 5-class and 2-
class problem are huge).

7.1 tasks and datasets
prediction tasks: we use wikipedia 2014 cor-
pus, divided half and half for training and test-
ing. we employ the most-frequent 200,000 words
and add an    unknown    symbol to represent the
remainder, making it a 200,001-class prediction
problem. this is a simple prediction task.

analysis

sentiment
(pang et al., 2002)   s
dataset comprises sentences containing gold-
standard sentiment labels tagged at the start of
each sentence. we divide the original dataset into
training(8101)/dev(500)/testing(2000). this is an
unaligned labeling (single) task.

(umd) the

question-answering
dataset
comprises
two domains, history and liter-
ature, and contains roughly 2,000 questions
where each question is paired with an answer
(iyyer et al., 2014). since answers are selected
from a pool of roughly 100 answer candidates,
this is not an open qa problem but a multi-class
classi   cation problem; i.e., an unaligned labeling
(single) task.

machine translation we use the wmt14
english-french dataset
and the opeid412
english-chinese dataset. this is an unaligned
labeling (sequence) task.

part-of-speech tagging (id32) we
use a random sample of wiki2014 as training and
testing dataset, each of which containing 1 million
sentences. gold-standard labels are assigned us-
ing the stanford pos tagger. this is an aligned
labeling task.

name entity recognition (conll) we use
the conll-2003 english benchmark for train-
ing, which labels four entity types (person, loca-
tion, organization, miscellaneous). the models
are tested on conll-2003 testing data.. this is
an aligned labeling task.

is

data

treebank

syntactic parsing training
the
ontonotes corpus (hovy et al., 2006) and english
web
(petrov and mcdonald, 2012)
with an additional 5 million random sen-
all parsed by the stanford parser
tences,
(socher et al., 2013).
is
section 22 of the id32 plus 1000 sen-
tences from the question treebank. we followed
protocols de   ned in (vinyals et al., 2014). this is
an unaligned labeling (sequence) task.

the testing dataset

question answer (open-domain) we use the
yahoo comprehensive qa dataset. the dataset
comprises roughly 4 million qa pairs. questions
and answers are sequences of tokens. questions
are treated as inputs and models predict word se-
quences as responsive answers. this is an un-
aligned labeling (sequence) task.

is

are similar
as
de   ned

to be estimated,
language models

7.2
implementations
7.2.1 prediction task
implementations
for prediction tasks, where
to
p (y n)
recurrent
in
(mikolov et al., 2010).
let et   1 denote the
representation obtained for timestep t     1 based
on preceding information from the lstm. let
eyt denote the feature representation for
the
token to be predicted at time t. by adopting a
softmax function, the id155 for
the occurrence of the current token given earlier
evidence is given by:

p(yt|yt   1, ..., yt   n) ==

f (et   1, eyt )

py    y f (et   1, ey)

(5)

where f (et   1, eyt ) denotes the compositional
function between vectors et   1 and eyt. in this pa-
per, we adopt the form of exponential dot product
for f (  ):

f (et   1, eyt ) = exp(et   1    eyt)

(6)

7.2.2 labeling task
we refer to frameworks (sutskever et al., 2014;
bahdanau et al., 2014; vinyals et al., 2014)) by

   rst concatenating input and output {x n, y n} =
{x1, .., xn, y1, .., yn}. let et   1 denote the ltsm
timestep t     1 by convoluting all
output at
preceding tokens before t in {x n, y n},
i.e.,
{x1, ..., xn, y1, ..., yt   1}.

unaligned single labeling single-tag label-
ing corresponds to the special case where the size
of y n is 1. taking id31 as an ex-
ample, sentence-level embeddings (denoted as en,
where n denotes the length of the current sentence)
are    rst obtained recurrently from the lstm. as
it is a binary classi   cation problem, we have:

p (y|  ) =

exp(en    ey)

py      {0,1} exp(en    ey    )

(7)

question-answering (umd) is implemented in a
similar way.

sequence

labeling following
unaligned
(bahdanau et al., 2014; vinyals et al., 2014), the
id155 for predicting the current
token yt in y n is given by

p (y n|x n) = y
1   t   n

p (yt|x1, ..., xn, y1, yt   1)

= y
1   t   n

f (et   1, eyt)

py   y f (et   1, ey)

(8)

f (  ) takes the same form as in eq.6.

aligned sequence labeling in aligned se-
quence labeling tasks, there is a one-to-one corre-
spondences between output yt and input xt, which
should be captured in the model. decisions at
timestep t are made by combining lstm repre-
sentation et   1 and input representation ext:

p (y n|x n) = y
1   t   n

p (yt|x1, ..., xn, y1, ..., yi   1)

= y
1   t   n

f (et   1, eyt , exi)

py   y f (et   1, ey, exi)

(9)

f (et   1, eyt , exi) is given as below:

f (et   1, eyt , exi) = exp(u    (w    [et   1, eyt , exi]))
(10)
where [et   1, eyt , exi] denotes the concatenation of
the three vectors and w and u denote convolu-
tional matrix and vector to project the concate-
nated vector to a scalar. taking id52 as
example, for the sentence xn =   dog eats bones   

task

avg id178

word prediction (wiki)

english-chinese translation
english-french translation

qa (open-domain)
syntactic parsing

qa (umd)

text classi   cation (20 news)

sentiment (pang)

part-of-speech tagging
name entity recognition

7.12
5.17
3.92
3.87
1.18
1.08
0.70
0.58
0.42
0.31

table 1: average id178 for different nlp tasks
with correspondent dataset speci   ed.

with correspondent labels yn =   nn vbz nns   ,
we    rst concatenate xn with yn:    dog eats bones
nn vbz nns   . when making predictions at to-
ken    vbz   , let elst m denote the lstm embed-
ding computed at preceding token    nn   , evbz de-
note the embedding for token    vbz   , eeats de-
note the correspondent input embedding for token
   eats   . then the id203 for generating part-
of-speech tag vbz is given by:

p(vbz|  ) =

f (elst m , evbz, eeats)

py      y f (elst m , ey, eeats)

(11)

7.3 details

for each task, id27s are initialized
o the same pre-trained vectors for fairness. pre-
trained embeddings were obtained from id97
on a 6-billion-word corpus with dimensionality
512. lstm models are composed of one single
hidden layer. stochastic gradient decent (without
momentum) with mini-batch (cotter et al., 2011)
is adopted. for each task, we use a learning initial
learning rate of 0.5 with a linear decay. learning
stops after 4 iterations. we initialized the lstm
parameters using a uniform distribution between
[-0.1, 0.1]. referring to (sutskever et al., 2014),
the gradient is normalized if its value exceeds a
threshold to avoid exploding gradients. for un-
aligned sequence prediction tasks (i.e., syntactic
parsing, qa(open domain)), inputs are reversed,
as suggested in (sutskever et al., 2014).

7.4 results

estimated entropies for different tasks computed
in the proposed paradigm are presented in table 1.
as can be seen, mt is less complex than word

prediction tasks, which is in line with our expec-
tation: for mt, output tokens are predicated on
source tokens. the input data provides additional
information and lowers the degree of uncertainty:
h(y |x)     h(y ) for any x and y .

as discussed earlier, estimated entropies are
subjective to datasets. being signi   cantly short
in training data, a high level of id178 is ob-
served for summarization.
this phenomenon
demonstrate one key disadvantage of the pro-
posed model   the failure to consider the impact
of datasets.
in particular, we are computing the
upper bound for a speci   c task given the speci   c
dataset adopted. how to take into account the
in   uence of different datasets (e.g., amounts of
training data, quality of training data) poses a great
challenge to developing a general nlp engine.

8 de   ciencies and directions for

improvement

we have proposed a paradigm with three require-
ments, which we believe to be essential for a uni-
versal nlp engine. we are fully aware that it is
impossible to come up with instantiations that per-
fectly meet all the requirements using current al-
gorithms and frameworks. we consider the search
for optimal solutions to be a long-term task.
in
this section we identify de   ciencies involved in
the proposed framework and suggest avenues for
improvements.

the metric: we proposed to use shannon en-
tropy as uncertainty measurement to evaluate the
complexity of tasks because we believe that en-
tropy more deeply re   ects the nature of uncer-
tainty than other current measures such as accu-
racy or recall. however, if a theoretical computer
scientist were to develop a more optimal measure
that avoids the dilemma described in section 2, we
would replace id178 with that measure.

the engine:

in this paper, we are using an
end-to-end turning string prediction engine, which
says nothing substantive about complexity of re-
source and intermediate procedures. this could
be problematic. consider the following scenarios:
in case 1 we have a long table that lists each in-
put possibility and its output answer is a simple
lookup, where the work then goes into creating the
table, and in case 2 we have a small resource of
rules but a lot of feature creation and rule applica-
tion in the main engine to perform the same task.
it is then true that the id178 from input to out-

put is the same if the two systems produce exactly
the same output (though one takes perhaps a lot
more time, the other requires perhaps more space).
but is the amount of work required (and hence
the id178 effect) to create the two resources the
same? in other words, can one argue that because
the    outside    end-to-end turning prediction task is
constant in id178, therefore the inner resources
have to contain the same amount of id178 re-
ducing    power   ? this is not necessarily true. but
should the one resource contain signi   cantly more
than the other, it appears that the outside engine
doesn   t actually use that.

the model: before discussing disadvantages
of applied recurrent neural models, it is notewor-
thy that there is an alternative to a universal and
uni   ed model (we call it uni   ed model for short)
for the framework. one can instead    nd the best
informants (we call such a strategy best models)
from various places and ask them to perform the
transformation predictions. alternatively, one can
exhaust all combinations of models, algorithms,
and features, and report the best results (smallest
value of id178) as the complexity comparison.
though all these strategies have pros and cons,
we postulate that uni   ed models might be more
suitable than best models, as different informants
might have different levels of education.

to meet the two requirements described in sec-
tion 6, we adopted recurrent neural models. re-
current models are by no means perfect: they in-
evitably forget previous information and are fun-
damentally incapable of capturing long-term de-
pendencies (bengio et al., 1994). this becomes
especially problematic in tasks where long-term
dependencies play a vital role such as discourse
parsing. without trying to defend the model too
far, we note that recurrent models seem to offer
advantages over other current models that we can
think of, e.g., transition models. we are optimistic
that other and more sophisticated variations of
neural models or other models, such as ldcrfs
(long-dependency crfs) (morency et al., 2007),
memory networks (weston et al., 2015) will cope
with the aforementioned disadvantages bit by bit.
at least, one can replace recurrent models if more
suitable algorithms come up.

9 conclusion: toward a theory of nlp

almost all nlp researchers today would all agree
that there is no such thing as a theory of nlp. we

hope that in this paper we lay some groundwork
toward such a theory.

any theory addresses some complex phe-
nomenon by (i) identifying some categories (of
objects or states or events) within it, (ii) providing
some characteristics and perhaps some de   nitions
for them, (iii) if possible describing some relation-
ships between them, and (iv) if possible quantify-
ing (some) aspects of these relationships. a scien-
ti   c theory measures aspects of some phenomena
and uses rules expressing the relationships to pre-
dict the values of other phenomena under certain
conditions.

the framework outlined in this paper names as
categories the commonly used linguistic phenom-
ena of nlp such as words, part of speech tags,
syntactic classes, and any other linguistically mo-
tivated category that nlp researchers choose to
study. but it also has as categories various al-
gorithms and data structures and other aspects of
computation, including language models, the no-
tion of training data and evaluation against a gold
standard, classi   cation, scoring, etc. the general
nlp engine puts the notions together in a single
generic framework and suggests a way to mea-
sure their separate individual characteristics with
regard to a single whole, namely the performance
of tasks phrased in a very generic manner. this al-
lows one to hold all but one category constant and
vary the characteristics of either a linguistic or a
computational category and study its effect on the
overall task relative to any other variation, even if
applied to some other category.

it is of course possible to generalize the gen-
eral nlp engine to apply to many other applica-
tion areas in computer science. however the do-
main of nlp has properties that make it very at-
tractive for    eshing out the nature of the engine
and the general    theory   , among others that nlp is
a relatively mature domain within computer sci-
ence, being just over 60 years old; nlp addresses
a very large and complex subject    eld, namely nat-
ural language, nlp uses a variety of quite dif-
ferent techniques, including    nite state transfor-
mation engines, machine learning, etc., and nu-
merous types of representations, including vector
spaces, symbolic notations, and connectionist em-
beddings.

in summary, though far from perfect, this pa-
per provides a    rst attempt to quantify nlp tasks
under a uniform paradigm which might have the

potential to signi   cantly impact natural language
processing areas.

[hochreiter and schmidhuber1997] sepp hochreiter
and j  urgen schmidhuber. 1997. long short-term
memory. neural computation, 9(8):1735   1780.

references
[algoet and cover1988] paul h algoet and thomas m
cover. 1988. a sandwich proof of the shannon-
mcmillan-breiman theorem. the annals of proba-
bility, pages 899   909.

[andreas et al.2013] jacob andreas, andreas vlachos,
and stephen clark. 2013. id29 as ma-
chine translation. in acl (2), pages 47   52.

[bahdanau et al.2014] dzmitry bahdanau, kyunghyun
cho, and yoshua bengio. 2014. neural machine
translation by jointly learning to align and translate.
arxiv preprint arxiv:1409.0473.

[bengio et al.1994] yoshua bengio, patrice simard,
and paolo frasconi. 1994. learning long-term de-
pendencies with id119 is dif   cult. neural
networks, ieee transactions on, 5(2):157   166.

[brown et al.1992] peter f brown, vincent j della
pietra, robert l mercer, stephen a della pietra,
and jennifer c lai. 1992. an estimate of an up-
per bound for the id178 of english. computational
linguistics, 18(1):31   40.

[cho et al.2014] kyunghyun cho, bart van merrien-
boer, caglar gulcehre, fethi bougares, holger
schwenk, and yoshua bengio.
2014. learning
phrase representations using id56 encoder-decoder
for id151. arxiv preprint
arxiv:1406.1078.

[cotter et al.2011] andrew cotter, ohad shamir, nati
srebro, and karthik sridharan. 2011. better mini-
batch algorithms via accelerated gradient methods.
in advances in neural information processing sys-
tems, pages 1647   1655.

[cover and king1978] thomas m cover and r king.
1978. a convergent gambling estimate of the en-
tropy of english. id205, ieee trans-
actions on, 24(4):413   421.

[cover and thomas2012] thomas m cover and joy a
thomas. 2012. elements of id205.
john wiley & sons.

[funahashi and nakamura1993] ken-ichi

funahashi
and yuichi nakamura. 1993. approximation of
dynamical systems by continuous time recurrent
neural networks. neural networks, 6(6):801   806.

[gopinath and cover1987] b gopinath and thomas m
cover. 1987. open problems in communication and
computation. springer-verlag.

[graves et al.2014] alex graves, greg wayne, and ivo
danihelka. 2014. id63s. arxiv
preprint arxiv:1410.5401.

[hovy et al.2006] eduard hovy, mitchell marcus,
and ralph
martha palmer, lance ramshaw,
the 90% solu-
weischedel.
tion.
the human language
technology conference of the naacl, companion
volume: short papers, pages 57   60. association for
computational linguistics.

2006. ontonotes:

in proceedings of

richard

[iyyer et al.2014] mohit iyyer, jordan boyd-graber,
and
leonardo claudino,
2014. a neural network for
hal daum  e iii.
in
factoid id53 over paragraphs.
proceedings of
the 2014 conference on empir-
ical methods in natural language processing
(emnlp), pages 633   644.

socher,

[kucera and francis1967] henry kucera and nelson
francis. 1967. computational analysis of present-
day american english. brown university press.

[mikolov et al.2010] tomas mikolov, martin kara     at,
lukas burget, jan cernock`y, and sanjeev khudan-
pur. 2010. recurrent neural network based lan-
guage model. in interspeech 2010, 11th annual
conference of the international speech communica-
tion association, makuhari, chiba, japan, septem-
ber 26-30, 2010, pages 1045   1048.

[morency et al.2007] l morency, ariadna quattoni,
and trevor darrell. 2007. latent-dynamic discrim-
inative models for continuous gesture recognition.
in id161 and pattern recognition, 2007.
cvpr   07. ieee conference on, pages 1   8. ieee.

[pang et al.2002] bo pang, lillian lee, and shivaku-
mar vaithyanathan. 2002. thumbs up?: sentiment
classi   cation using machine learning techniques. in
proceedings of the acl-02 conference on empirical
methods in natural language processing-volume 10,
pages 79   86. association for computational lin-
guistics.

[petrov and mcdonald2012] slav petrov and ryan mc-
donald. 2012. overview of the 2012 shared task
on parsing the web. in notes of the first workshop
on syntactic analysis of non-canonical language
(sancl), volume 59. citeseer.

[shannon1948] claude elwood shannon.

1948. a
mathematical theory of communication. acm sig-
mobile mobile computing and communications
review, 5(1):3   55.

[shannon1951] claude e shannon. 1951. prediction
and id178 of printed english. bell system technical
journal, 30(1):50   64.

[socher et al.2013] richard

john bauer,
christopher d manning, and andrew y ng. 2013.
parsing with compositional vector grammars. in in
proceedings of the acl conference. citeseer.

socher,

[sutskever et al.2014] ilya sutskever, oriol vinyals,
and quoc vv le. 2014. sequence to sequence
learning with neural networks. in advances in neu-
ral information processing systems, pages 3104   
3112.

[vinyals et al.2014] oriol vinyals, lukasz kaiser,
terry koo, slav petrov, ilya sutskever, and geof-
frey hinton. 2014. grammar as a foreign language.
arxiv preprint arxiv:1412.7449.

[weston et al.2015] jason weston, antoine bordes,
sumit chopra, and tomas mikolov. 2015. towards
ai-complete id53: a set of prerequi-
site toy tasks. arxiv preprint arxiv:1502.05698.

10 appendix

long-short term memory lstm model,    rst
proposed in (hochreiter and schmidhuber, 1997),
maps an input sequence to a    xed-sized vector
by sequentially convoluting the current representa-
tion with the output representation of the previous
step. lstm associates each time epoch with an
input, control and memory gate, and tries to min-
imize the impact of unrelated information. let-
ting it, ft and ot correspond to gate states at time
t, et   1 and et denote the output representation at
time t     1, and t, ext denote the embedding as-
sociated with the token at time t, as de   ned in
(hochreiter and schmidhuber, 1997), we have

it =   (wi    ext + vi    et   1)
ft =   (wf    ext + vf    et   1)
ot =   (wo    ext + vo    et   1)
lt = tanh(wl    ext + vl    et   1)
mt = ft    mt   1 + it    lt
et = ot    mt

(12)

where    denotes the sigmoid function. it, ft and
ot are scalars within the range of [0,1].

