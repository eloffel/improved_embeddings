bayesian optimization of text representations

dani yogatama

language technologies institute

school of computer science
carnegie mellon university
pittsburgh, pa 15213, usa
dyogatama@cs.cmu.edu

noah a. smith

language technologies institute

school of computer science
carnegie mellon university
pittsburgh, pa 15213, usa
nasmith@cs.cmu.edu

5
1
0
2

 
r
a

m
2

 

 
 
]
l
c
.
s
c
[
 
 

1
v
3
9
6
0
0

.

3
0
5
1
:
v
i
x
r
a

abstract

when applying machine learning to prob-
lems in nlp, there are many choices to
make about how to represent input texts.
these choices can have a big effect on per-
formance, but they are often uninteresting
to researchers or practitioners who simply
need a module that performs well. we
propose an approach to optimizing over
this space of choices, formulating the prob-
lem as global optimization. we apply a
sequential model-based optimization tech-
nique and show that our method makes
standard linear models competitive with
more sophisticated, expensive state-of-the-
art methods based on latent variable models
or neural networks on various topic classi-
   cation and id31 problems.
our approach is a    rst step towards black-
box nlp systems that work with raw text
and do not require manual tuning.
introduction

1
nlp researchers and practitioners spend a consid-
erable amount of time comparing machine-learned
models of text that differ in relatively uninteresting
ways. for example, in categorizing texts, should
the    bag of words    include bigrams, and is tf-idf
weighting a good idea? these choices matter exper-
imentally, often leading to big differences in per-
formance, with little consistency across tasks and
datasets in which combination of choices works
best. unfortunately, these differences tell us lit-
tle about language or the problems that machine
learners are supposed to solve.

we propose that these decisions can be auto-
mated in a similar way to hyperparameter selec-
tion (e.g., choosing the strength of a ridge or lasso
regularizer). given a particular text dataset and
classi   cation task, we introduce a technique for op-
timizing over the space of representational choices,

along with other    nuisances    that interact with
these decisions, like hyperparameter selection.1
for example, using higher-order id165s means
more features and a need for stronger regulariza-
tion and more training iterations. generally, these
decisions about instance representation are made
by humans, heuristically; our work is the    rst to
automate them.

our technique instantiates sequential model-
based optimization (smbo; hutter et al., 2011).
smbo and other bayesian optimization ap-
proaches have been shown to work well for hyper-
parameter tuning (bergstra et al., 2011; hoffman
et al., 2011; snoek et al., 2012). though popular
in id161 (bergstra et al., 2013), these
techniques have received little attention in nlp.

we apply the technique to id28
on a range of topic and sentiment classi   cation
tasks. consistently, our method    nds representa-
tional choices that perform better than linear base-
lines previously reported in the literature, and that,
in some cases, are competitive with more sophisti-
cated non-linear models trained using neural net-
works.

2 problem formulation and notation
let the training data consist of a collection of pairs
dtrain = (cid:104)(cid:104)d.i1, d.o1(cid:105), . . . ,(cid:104)d.in, d.on(cid:105)(cid:105), where
each input d.i     i is a text document and each
output d.o     o, the output space. the overall
training goal is to maximize a performance func-
tion f (e.g., classi   cation accuracy, log-likelihood,
f1 score, etc.) of a machine-learned model, on a
held-out dataset, ddev     (i    o)n(cid:48)
class   cation proceeds in three steps:    rst, x :
i     rn maps each input to a vector representation.
second, a classi   er is learned from the inputs (now
transformed into vectors) and outputs: l : (rn   
o)n     (rn     o). finally, the resulting classi   er
1in   5 we argue that the technique is also applicable in

.

unsupervised settings.

c : i     o is    xed as

l(dtrain )    
o     rn

x

rn     i

(i.e., the composition of the representation function
with the learned classi   er).

here we consider linear classi   ers of the form

c(d.i) = arg max

o   o

w(cid:62)
o x(d.i)

(1)

where the coef   cients wo     rn , for each output o,
are learned using id28 on the training
data. we let w denote the concatenation of all
wo. hence the parameters can be understood as a
function of the training data and the representation
function x. the performance function f, in turn, is
a function of the held-out data ddev and x   also w
and dtrain, through x. for simplicity, we will write
   f (x)    when the rest are clear from context.

typically, x is    xed by the model designer, per-
haps after some experimentation, and learning fo-
cuses on selecting the parameters w. for logistic
regression and many other linear models, this train-
ing step reduces to id76 in n|o|
dimensions   a solvable problem that is still costly
for large datasets and/or large output spaces. in
seeking to maximize f with respect to x, we do
not wish to carry out training any more times than
necessary.

choosing x can be understood as a problem of
selecting hyperparameter values. we therefore turn
to bayesian optimization, a family of techniques
recently introduced for selecting hyperparameter
values intelligently when solving for parameters
(w) is costly.

3 bayesian optimization
our approach is based on sequential model-based
optimization (smbo; hutter et al., 2011).
it
iteratively chooses representation functions x.
on each round, it makes this choice through a
nonparametrically-estimated probabilistic model
of f, then evaluates f   we call this a    trial.    as
in any iterative search algorithm, the goal is to
balance exploration of options for x with exploita-
tion of previously-explored options, so that a good
choice is found in a small number of trials. see
algorithm 1.

more concretely, in the tth trial, xt is selected
using an acquisition function a and a    surrogate   
probabilistic model pt. second, f is evaluated

given xt   an expensive operation which involves
training to select parameters w and assessing per-
formance on the held-out data. third, the prob-
abilistic model is updated using a nonparametric
estimator.

algorithm 1 smbo algorithm

input: number of trials t , target function f
p1 = initial surrogate model
initialize y   
for t = 1 to t do
xt     arg maxx
yt     evaluate f (xt)
update y   
estimate pt given x1:t and y1:t

a(x; pt, y   )

end for

we next describe the acquisition function a and

the surrogate model pt used in our experiments.

3.1 acquisition function
a good acquisition function returns high values
for x such that either the value f (x) is predicted
to be high, or because uncertainty about f (x)   s
value is high; balancing between these is the classic
tradeoff between exploitation and exploration. we
use a criterion called expected improvement (ei;
jones, 2001), which is the expectation (under the
current surrogate model pt) that the choice y will
exceed y   :

a(x; pt, y   ) =

max(y     y   , 0)pt(y | x)dy

(cid:90)    

      

where y    is chosen depending on the surrogate
model, discussed below. (for now, think of it as a
strongly-performing    benchmark    value of f, dis-
covered in earlier iterations.) other options for the
acquisition function include maximum id203
of improvement (jones, 2001), minimum condi-
tional id178 (villemonteix et al., 2006), gaussian
process upper con   dence bound (srinivas et al.,
2010), or a combination of them (hoffman et al.,
2011). we selected ei because it is the most widely
used acquisition function that has been shown to
work well on a range of tasks.

3.2 surrogate model
as a surrogate model, we use a tree-structured
parzen estimator (tpe; bergstra et al., 2011). this
is a nonparametric approach to density estimation.
we seek to estimate pt(y | x) where y = f (x), the

(cid:32)

(cid:33)   1

performance function that is expensive to compute
exactly. the tpe approach is as follows:

(cid:40)

pt(y | x)     pt(y)    pt(x | y)
pt(x | y) =

p<
t (x),
   
p
t (x),

if y < y   
if y     y   

t and p

   
t are densities estimated using ob-
where p<
servations from previous trials that are less than
and greater than y   , respectively. in tpe, y    is
de   ned as some quantile of the observed y; we use
15-quantiles.

as shown by bergstra et al. (2011), the expected

improvement in tpe can be written as:

a(x; pt, y   )    

   +

(1       )

p<
t (x)
   
t (x)
p

,

(2)

where    = pt(y < y   ),    xed at 0.15 by de   ni-
tion of y    (above). here, we prefer x with high
   
id203 under p
t (x) and low id203 under
t (x). to maximize this quantity, we draw many
p<
   
t (x) and evaluate them
candidates according to p
   
t (x). note that p(y) does
according to p<
not need to be given an explicit form.

t (x)/p

we discuss how to compute p<

in order to evaluate eq. 2, we need to compute
   
t (x). these joint distributions de-
t (x) and p
p<
pend on the graphical model of the hyperparameter
space   which is allowed to form a tree structure.
t (x) in the fol-
   
t (x) is computed similarly, using trials
lowing. p
where y     y   . we associate each hyperparameter
with a node in the graphical model; consider the
kth dimension of x, denoted by random variable
x k.
    if x k ranges over a discrete set x, tpe uses a
reweighted categorical distribution, where the
id203 that x k = x is proportional to a
smoothing parameter plus the counts of occur-
rences of x k = x in xk
    when x k is continuous-valued, tpe constructs
a id203 distribution by placing a truncated
gaussian distribution centered at each of xk
where yt < y   , with standard deviation set to
the greater of the distances to the left and right
neighbors.

1:t with yt < y   .

k,1:t

in the simplest version, each node is independent,
so we can compute p<
t (x) by multiplying indi-
vidual probabilities at every node.
in the tree-
structured version, we only multiply probabilities
along the relevant path, excluding some nodes.

another common approach to the surrogate is
the gaussian process (rasmussen and williams,
2006; hoffman et al., 2011; snoek et al., 2012).
like bergstra et al. (2011), our preliminary exper-
iments found the tpe to perform favorably. fur-
ther tpe   s tree-structured con   guration space is
advantageous, because it allows nested de   nitions
of hyperparameters, which we exploit in our exper-
iments (e.g., only allows bigrams to be chosen if
unigrams are also chosen).

implementation details

3.3
because research on smbo is active, many im-
plementations are publicly available; we use the
hpolib library (eggensperger et al., 2013).2 the
libray takes as input a function l, which is treated
as a black box   in our case, a id28
trainer that wraps the liblinear library (fan
et al., 2008), based on the trust region newton
method (lin et al., 2008)   and a speci   cation of
hyperparameters.

4 experiments

our experiments consider representational choices
and hyperparameters for several text categorization
problems.

4.1 setup
we    x our learner l to id28. we
optimize text representation based on the types of
id165s used, the type of weighting scheme, and
the removal of stopwords. for id165s, we have
two parameters, minimum and maximum lengths
(nmin and nmax ). (all id165 lengths between
the minimum and maximum, inclusive, are used.)
for weighting scheme, we consider term frequency,
tf-idf, and binary schemes. last, we also choose
whether we should remove stopwords before con-
structing feature vectors for each document.

furthermore, the choice of representation inter-
acts with the regularizer and the training conver-
gence criterion (e.g., more id165s means slower
training time). we consider two regularizers, (cid:96)1
penalty (tibshirani, 1996) or squared (cid:96)2 penalty
(hoerl and kennard, 1970). we also have hyper-
parameters for id173 strength and training
convergence tolerance. see table 1 for a complete
list of hyperparameters in our experiments.

note that even with this limited number of
options, the number of possible combinations is

2http://www.automl.org/hpolib.html

hyperparameter
nmin
nmax
weighting scheme
remove stop words?
id173
id173 strength
convergence tolerance

values
{1, 2, 3}
{nmin , . . . , 3}
{tf, tf-idf, binary}
{true, false}
{(cid:96)1, (cid:96)2}
[10   5, 105]
[10   5, 10   3]

table 1: the set of hyperparameters considered in our ex-
periments. the top half are hyperparameters related to text
representation, while the bottom half are id28
hyperparameters, which also interact with the chosen repre-
sentation.

huge (it is actually in   nite since the id173
strength and convergence tolerance are continuous
values, although we can also use sets of possible
values), so exhaustive search is computationally
expensive. in all our experiments for all datasets,
we limit ourselves to 30 trials per dataset. the only
preprocessing we applied was downcasing (see   5
for discussion about this).

we always use a development set to evaluate
f (x) during learning and report the    nal result on
an unseen test set.

for movie reviews

4.2 datasets
we evaluate our method on    ve text categorization
tasks.
    stanford sentiment

treebank (socher et al.,
a sentence-level sentiment analy-
2013):
sis dataset
from the
rottentomatoes.com website. we use
the binary classi   cation task where the goal
is to predict whether a review is positive or
negative (no neutral reviews). we obtained
this dataset from http://nlp.stanford.
edu/sentiment.
    electronics product reviews from amazon
this dataset
(mcauley and leskovec, 2013):
consists of electronic product reviews, which is
a subset of a large amazon review dataset. fol-
lowing the setup of johnson and zhang (2014),
we only use the text section and ignore the
summary section. we also only consider pos-
itive and negative reviews. we obtained this
dataset from http://riejohnson.com/
id98_data.html.
    imdb movie reviews (maas et al., 2011): a
binary id31 dataset of highly

dataset
stanford sentiment
amazon electronics
imdb reviews
congress vote
20n all topics
20n all science
20n atheist.religion
20n x.graphics

training
6,920
20,000
20,000
1,175
9,052
1,899
686
942

dev.
872
5,000
5,000
113
2,262
474
171
235

test
1,821
25,000
25,000
411
7,532
1,579
570
784

table 2: document counts.

polar imdb movie reviews, obtained from
http://ai.stanford.edu/  amaas/
/data/sentiment.

    congressional vote (thomas et al., 2006): tran-
scripts from the u.s. congressional    oor de-
bates.
the dataset only includes debates
for controversial bills (the losing side has
at least 20% of the speeches).
similar to
previous work (thomas et al., 2006; yesse-
nalina et al., 2010), we consider the task
to predict the vote (   yea    or    nay   ) for the
speaker of each speech segment (speaker-based
speech-segment classi   cation). we obtained
it from http://www.cs.cornell.edu/
  ainur/sle-data.html.
    20 newsgroups

the 20
is a benchmark topic
newsgroups dataset
classi   cation dataset, we use the publicly
available copy at http://qwone.com/
  jason/20newsgroups. there are 20 top-
ics in this dataset. we derived four topic
classi   cation tasks from this dataset. the
   rst task is to classify documents across all
20 topics. the second task is to classify
related science documents into four science
topics (sci.crypt, sci.electronics,
sci.med, sci.med).
the third and
fourth tasks are talk.religion.misc
vs. alt.atheism and comp.graphics
vs. comp.windows.x. to consider a more
realistic setting, we removed header information
from each article since they often contain label
information.

(lang,

1995):

3

these are standard datasets for evaluating text
categorization models, where benchmark results
are available. in total, we have eight tasks, of which
four are id31 tasks and four are topic
classi   cation tasks. see table 2 for descriptive

3we were not able to    nd previous results that are compa-
rable to ours on the second task; we include them to enable
further comparisons in the future.

dataset
stanford sentiment
amazon electronics
imdb reviews
congress vote
20n all topics
20n all science
20n atheist.religion
20n x.graphics

acc.
82.43
91.56
90.85
78.59
87.84
95.82
86.32
92.09

nmin
1
1
1
2
1
1
1
1

nmax weighting
tf-idf
binary
binary
binary
binary
binary
binary
binary

2
3
2
2
2
2
2
1

stop.
f
f
f
f
f
f
t
t

reg.
(cid:96)2
(cid:96)2
(cid:96)2
(cid:96)2
(cid:96)2
(cid:96)2
(cid:96)1
(cid:96)2

strength conv.
0.098
0.022
0.019
0.012
0.008
0.007
0.011
0.014

10
120
147
121
16
142
41
91

table 3: classi   cation accuracies and the best hyperparameters for each of the dataset in our experiments.    acc    shows
accuracies for our id28 model.    min    and    max    correspond to the min id165s and max id165s respectively.
   stop.    is whether we perform stopwords removal or not.    reg.    is the id173 type,    strength    is the id173
strength, and    conv.    is the convergence tolerance. for id173 strength, we round it to the nearest integer for readability.

statistics of our datasets.

4.3 baselines
for each dataset, we select supervised, non-
ensemble classi   cation methods from previous lit-
erature as baselines. in each case, we emphasize
comparisons with the best-published linear method
(often an id166 with a linear kernel with represen-
tation selected by experts) and the best-published
method overall. in the followings,    id166    always
means    linear id166   . all methods were trained and
evaluated on the same training/testing data splits;
in cases where standard development sets were not
available, we used a random 20% of the training
data as a development set.

4.4 results
we summarize the hyperparameters selected by our
method, and the accuracies achieved (on test data)
in table 3. we discuss comparisons to baselines
for each dataset in turn.
stanford sentiment treebank (table 4). our lo-
gistic regression model outperforms the baseline
id166 reported by socher et al. (2013), who used
only unigrams but did not specify the weighting
scheme for their id166 baseline. while our result is
still below the state-of-the-art based on the the re-
cursive neural tensor networks (socher et al., 2013)
and the paragraph vector (le and mikolov, 2014),
we show that id28 is comparable
with recursive and matrix-vector neural networks
(socher et al., 2011; socher et al., 2012).
amazon electronics
(table 5). the best-
performing methods on this dataset are based
on convolutional neural networks (johnson and
zhang, 2014).4 our method is on par with the
4these are fully connected neural networks with a recti-
   er activation function, trained under (cid:96)2 id173 with
stochastic id119.

method
na    ve bayes
id166
vector average
id56s
lr (this work)
matrix-vector id56
recursive neural tensor networks
paragraph vector

acc.
81.8
79.4
80.1
82.4
82.4
82.9
85.4
87.8

table 4: comparisons on the stanford sentiment treebank
dataset. scores are as reported by socher et al. (2013) and le
and mikolov (2014).

second-best of these, outperforming all of the
reported feed-forward neural networks and id166
variants johnson and zhang used as baselines.
they varied the representations, and used log term
frequency and id172 to unit vectors as the
weighting scheme, after    nding that this outper-
formed term frequency. our method achieved the
best performance with binary weighting, which
they did not consider.

method
id166-unigrams
id166-{1, 2}-grams
id166-{1, 2, 3}-grams
nn-unigrams
nn-{1, 2}-grams
nn-{1, 2, 3}-grams
lr (this work)
bag of words id98
sequential id98

acc.
88.62
90.70
90.68
88.94
91.10
91.24
91.56
91.58
92.22

table 5: comparisons on the amazon electronics dataset.
scores are as reported by johnson and zhang (2014).

imdb reviews (table 6). the results parallel
those for amazon electronics; our method comes

close to convolutional neural networks (johnson
and zhang, 2014), which are state-of-the-art.5 it
outperforms id166s and feed-forward neural net-
works, the restricted id82 approach
presented by dahl et al. (2012), and compressive
id171 (paskov et al., 2013).6

method
id166-unigrams
id166-{1, 2}-grams
id166-{1, 2, 3}-grams
rbm
nn-unigrams
nn-{1, 2}-grams
nn-{1, 2, 3}-grams
compressive id171
lr-{1, 2, 3, 4, 5}-grams
lr (this work)
bag of words id98
sequential id98

acc.
88.69
89.83
89.62
89.23
88.95
90.08
90.31
90.40
90.60
90.85
91.03
91.26

table 6: comparisons on the imdb reviews dataset. id166 re-
sults are from wang and manning (2012), the rbm (restricted
bolzmann machine) result is from dahl et al. (2012), nn and
id98 results are from johnson and zhang (2014), and lr-
{1, 2, 3, 4, 5}-grams and compressive id171 results
are from paskov et al. (2013).

congressional vote (table 7). our method out-
performs the best reported results of yessenalina et
al. (2010), which use a multi-level structured model
based on a latent-variable id166. we show compar-
isons to two well-known but weaker baselines, as
well.

method
id166-link
min-cut
id166-sle
lr (this work)

acc.
71.28
75.00
77.67
78.59

table 7: comparisons on the u.s. congressional vote dataset.
id166-link exploits link structures (thomas et al., 2006); the
min-cut result is from bansal et al. (2008); and id166-sle
result is reported by yessenalina et al. (2010).

20 newsgroups:
all topics (table 8). our
method outperforms state-of-the-art methods in-
5as noted, semi-supervised and ensemble methods are

excluded for a fair comparison.

6this approach is based on minimum description length,
using unlabeled data to select a set of higher-order id165s
to use as features. it is technically a semi-supervised method.
the results we compare to use id28 with elastic
net id173 and heuristic id172s.

cluding the distributed structured output model
(srikumar and manning, 2014).7 the strong lo-
gistic regression baseline from paskov et al. (2013)
uses all 5-grams, heuristic id172, and elas-
tic net id173; our method found that uni-
grams and bigrams, with binary weighting and (cid:96)2
penalty, achieved far better results.

method
discriminative rbm
compressive id171
lr-{1, 2, 3, 4, 5}-grams
distributed structured output
lr (this work)

acc.
76.20
83.00
82.80
84.00
87.84

table 8: comparisons on the 20 newsgroups dataset for
classifying documents into all topics. the disriminative rbm
result is from larochelle and bengio (2008); compressive
id171 and lr-5-grams results are from paskov et
al. (2013), and the distributed structured output result is from
srikumar and manning (2014).

20 newsgroups:
talk.religion.misc
vs. alt.atheism and comp.graphics
vs. comp.windows.x wang and manning
(2012) report a bigram na    ve bayes model achiev-
ing 85.1% and 91.2% on these tasks, respectively.8
our method achieves 86.3% and 92.1% using
slightly different setups (see table 3).

5 discussion
raw text as input and other hyperparameters.
our results suggest that seemingly mundane rep-
resentation choices can raise the performance of
simple linear models to be comparable with much
more sophisticated models. achieving these re-
sults is not a matter of deep expertise about the
domain or engineering skill; the choices can be au-
tomated. our experiments only considered logistic
regression with downcased text; more choices   
id30, count thresholding, id172 of
numbers, etc.   can be offered to the optimizer, as
can additional feature options like gappy id165s.
as nlp becomes more widely used in applica-
tions, we believe that automating these choices will
be very attractive for those who need to train a
high-performance model quickly.

7this method was designed for id170, but
srikumar and manning (2014) also applied it to classi   cation.
it attempts to learn a distributed representation for features and
for labels. the authors used unigrams and did not elaborate
the weighting scheme.

8they also report a na    ve bayes/id166 ensemble achieving

87.9% and 91.2%.

figure 1: classi   cation accuracies on development data for amazon electronics (left), stanford sentiment treebank (center),
and congressional vote (right) datasets. in each plot, the green solid line indicates the best accuracy found so far, while the dotted
orange line shows accuracy at each trial. we can see that in general the model is able to obtain reasonably good representation in
30 trials.

optimized representations. for each task, the
chosen representation is different. out of all pos-
sible hyperparameter choices in our experiments
(table 1), each of them is used by at least one of
the datsets (table 3). for example, on the con-
gressional vote dataset, we only need to use bi-
grams, whereas on the amazon electronics dataset
we need to use unigrams, bigrams, and trigrams.
the binary weighting scheme works well for most
of the datasets, except the sentence-level sentence
analysis task, where the tf-idf weighting scheme
was selected. (cid:96)2 id173 was best in all cases
but one.

we do not believe that an nlp expert would
be likely to make these particular choices, except
through the same kind of trial-and-error process
our method automates ef   ciently. often, we be-
lieve, researchers in nlp make initial choices and
stick with them through all experiments (as we have
admittedly done with id28). optimiz-
ing over more of these choices will give stronger
baselines.

training time. we ran 30 trials for each dataset
in our experiments. figure 1 shows each trial accu-
racy and the best accuracy on development data as
we increase the number of trials for three datasets.
we can see that 30 trials are generally enough
for the model to obtain good results, although the
search space is large.

in the presence of unlimited computational re-
sources, bayesian optimization is slower than grid
search on all hyperparameters, since the latter is
easy to parallelize. this is not realistic in most
research and development environments, and it is
certainly impractical in increasingly widespread

instances of personalized machine learning. the
bayesian optimization approach that we use in our
experiments is performed sequentially. it attempts
to predict what set of hyperparameters we should
try next based on information from previous trials.
there has been work to parallelize bayesian opti-
mization, making it possible to leverage the power
of multicore architectures (snoek et al., 2012; de-
sautels et al., 2012; hutter et al., 2012).

id21 and multitask setting. we
treat each dataset independently and create a sep-
arate model for each of them. it is also possible
to learn from previous datasets (i.e., transfer learn-
ing) or to learn from all datasets simultaneously
(i.e., multitask learning) to improve performance.
this has the potential to reduce the number of trials
required even further. see bardenet et al. (2013),
swersky et al. (2013), and yogatama and mann
(2014) for how to perform bayesian optimization
in these settings.

beyond linear models. we use logistic regres-
sion as our classi   cation model, and our experi-
ments show how simple linear models can be com-
petitive with more sophisticated models given the
right representation. other models, can be consid-
ered, of course, as can ensembles (yogatama and
mann, 2014). increasing the number of options
may lead to a need for more trials, and evaluating
f (x) (e.g., training the neural network) will take
longer for more sophisticated models. we have
demonstrated, using one of the simplest classi   ca-
tion models (id28), that even simple
choices about text representation can matter quite
a lot.

amazon electronicstrialaccuracy051015202530707580859095stanford sentimenttrialaccuracy05101520253050607080congress votetrialaccuracy0510152025304050607080id170 problems our frame-
work could also be applied to id170
problems. for example, in part-of-speech tagging,
the set of features can include character id165s,
word shape features, and word type features. the
optimal choice for different languages is not always
the same, our approach can automate this process.
beyond supervised learning. our framework
could also be extended to unsupervised and semi-
supervised models. for example, in document clus-
tering (e.g., id116), we also need to construct
representations for documents. log-likelihood
might serve as a performance function. a range of
random initializations might be considered. inves-
tigation of this approach for nonconvex problems
like id91 is an exciting area for future work.

6 conclusion
we used a bayesian optimization approach to opti-
mize choices about text representations for various
categorization problems. our sequential model-
based optimization technique identi   es settings for
a standard linear model (id28) that
are competitive with far more sophisticated state-
of-the-art methods on topic classi   cation and senti-
ment analysis. every task and dataset has its own
optimal choices; though relatively uninteresting to
researchers and not directly linked to domain or
linguistic expertise, these choices have a big effect
on performance. we see our approach as a    rst step
towards black-box nlp systems that work with raw
text and do not require manual tuning.

acknowledgements
this work was supported by the defense ad-
vanced research projects agency through grant
fa87501420244 and computing resources pro-
vided by amazon.

references
mohit bansal, clair cardie, and lillian lee. 2008.
the power of negative thinking: exploiting label
disagreement in the min-cut classi   cation framework.
in proc. of coling.

remi bardenet, matyas brendel, balazs kegl, and
michele sebag. 2013. collaborative hyperparam-
eter tuning. in proc. of icml.

james bergstra, remi bardenet, yoshua bengio, and
balazs kegl. 2011. algorithms for hyper-parameter
optimization. in proc. of nips.

james bergstra, daniel yamins, and david cox. 2013.
making a science of model search: hyperparameter
optimization in hundreds of dimensions for vision
architectures. in proc. of icml.

george e. dahl, ryan p. adams, and hugo larochelle.
2012. training restricted id82s on
word observations. in proc. of icml.

thomas desautels, andreas krause, and joel burdick.
2012. parallelizing explorationexploitation tradeoffs
with gaussian process bandit optimization. in proc.
of icml.

katharina eggensperger, matthias feurer, frank hutter,
james bergstra, jasper snoek, holger h. hoos, and
kevin leyton-brown. 2013. towards an empirical
foundation for assessing bayesian optimization of
in proc. of nips workshop on
hyperparameters.
bayesian optimization.

rong-en fan, kai-wei chang, cho-jui hsieh, xiang-
rui wang, and chih-jen lin. 2008. liblinear:
a library for large linear classi   cation. journal of
machine learning research, (9):1871   1874.

arthur e. hoerl and robert w. kennard. 1970. ridge
regression: biased estimation for nonorthogonal
problems. technometrics, 12(1):55   67.

matthew hoffman, eric brochu, and nando de freitas.
2011. portfolio allocation for bayesian optimization.
in proc. of uai.

frank hutter, holger h. hoos, and kevin leyton-brown.
2011. sequential model-based optimization for gen-
eral algorithm con   guration. in proc. of lion-5.

frank hutter, holger h. hoos, and kevin leyton-brown.
2012. parallel algorithm con   guration. in proc. of
lion.

rie johnson and tong zhang. 2014. effective use of
word order for text categorization with convolutional
neural networks. arxiv:1412.1058.

donald r. jones. 2001. a taxonomy of global optimiza-
tion methods based on response surfaces. journal of
global optimization, 21:345   385.

ken lang. 1995. newsweeder: learning to    lter net-

news. in proc. of icml.

hugo larochelle and yoshua bengio. 2008. classi   -
cation using discriminative restricted boltzmann ma-
chines. in proc. of icml.

quoc v. le and tomas mikolov. 2014. distributed
representations of sentences and documents. in proc.
of icml.

chih-jen lin, ruby c. weng, and s. sathiya keerthi.
2008. trust region id77 for large-scale
id28. journal of machine learning
research, (9):627   650.

ainur yessenalina, yisong yue, and claire cardie.
2010. multi-level structured models for document
sentiment classi   cation. in proc. of emnlp.

dani yogatama and gideon mann. 2014. ef   cient
id21 method for automatic hyperparame-
ter tuning. in proc. of aistats.

andrew l. maas, raymond e. daly, peter t. pham,
dan huang, andrew y. ng, and christopher potts.
2011. learning word vectors for id31.
in proc. of acl.

julian mcauley and jure leskovec. 2013. hidden
factors and hidden topics: understanding rating di-
mensions with review text. in proc. of recsys.

hristo s. paskov, robert west, john c. mitchell, and
trevor j. hastie. 2013. compressive id171.
in proc of nips.

carl edward rasmussen and christopher k. i. williams.
2006. gaussian processes for machine learning.
the mit press.

jasper snoek, hugo larrochelle, and ryan p. adams.
2012. practical bayesian optimization of machine
learning algorithms. in proc. of nips.

richard socher, jeffrey pennington, eric h. huang,
andrew y. ng, and christopher d. manning. 2011.
semi-supervised recursive autoencoders for predict-
ing sentiment distributions. in proc. of emnlp.

richard socher, brody huval, christopher d. manning,
and andrew y. ng. 2012. semantic compositionality
through recursive matrix-vector spaces. in proc. of
emnlp.

richard socher, alex perelygin, jean wu, jason
chuang, chris manning, andrew ng, and chris potts.
2013. recursive deep models for semantic compo-
in proc. of
sitionality over a sentiment treebank.
emnlp.

vivek srikumar and christopher d. manning. 2014.
learning distributed representations for structured
output prediction. in proc. of nips.

niranjan srinivas, andreas krause, sham kakade, and
matthias seeger. 2010. gaussian process optimiza-
tion in the bandit setting: no regret and experimental
design. in proc. of icml.

kevin swersky, jasper snoek, and ryan p. adams.
in proc.

2013. multi-task bayesian optimization.
of nips.

matt thomas, bo pang, and lilian lee. 2006. get out
the vote: determining support or opposition from
congressional    oor-debate transcripts. in proc. of
emnlp.

robert tibshirani. 1996. regression shrinkage and
selection via the lasso. journal of royal statistical
society b, 58(1):267   288.

julien villemonteix, emmanuel vazquez, and eric
walter. 2006. an informational approach to the
global optimization of expensive-to-evaluate func-
tions. journal of global optimization.

sida wang and christopher d. manning. 2012. base-
lines and bigrams: simple, good sentiment and topic
classi   cation. in proc. of acl.

