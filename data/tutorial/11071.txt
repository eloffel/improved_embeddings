generating multi-sentence lingual descriptions of indoor scenes

dahua lin

chinese univ. of hong kong

chen kong

carnegie mellon univ.

dhlin@ie.cuhk.edu.hk

chenk@cs.cmu.edu

sanja fidler

raquel urtasun

university of toronto

{fidler,urtasun}@cs.toronto.edu

5
1
0
2

 

b
e
f
8
2

 

 
 
]

v
c
.
s
c
[
 
 

1
v
4
6
0
0
0

.

3
0
5
1
:
v
i
x
r
a

abstract

this paper proposes a novel framework for
generating lingual descriptions of indoor
scenes. whereas substantial efforts have
been made to tackle this problem, previ-
ous approaches focusing primarily on gen-
erating a single sentence for each image,
which is not suf   cient for describing com-
plex scenes. we attempt to go beyond this,
by generating coherent descriptions with
multiple sentences. our approach is dis-
tinguished from conventional ones in sev-
eral aspects: (1) a 3d visual parsing sys-
tem that jointly infers objects, attributes,
and relations; (2) a generative grammar
learned automatically from training text;
and (3) a text generation algorithm that
takes into account the coherence among
sentences. experiments on the augmented
nyu-v2 dataset show that our framework
can generate natural descriptions with sub-
stantially higher rogue scores compared
to those produced by the baseline.
introduction

1
image understanding has been the central goal of
id161. whereas a majority of work on
image understanding focuses on class-based an-
notation, we believe, however, that describing an
image using natural language is still the best way
to show one   s understanding. the task of auto-
matically generating textual descriptions for im-
ages has received increasing attention from both
the id161 and natural language process-
ing communities. this is an important problem,
as an effective solution to this problem can enable
many exciting real-world applications, such as hu-
man robot interaction, image/video synopsis, and
automatic id134.

while this task has been explored in previous
work, existing methods mostly rely on pre-de   ned

figure 1: our method visually parses an rgb-d image
to get a scene graph that represents objects, their attributes
and relations between objects, and generates a multi-sentence
description via a learned grammar.

templates (barbu et al., 2012; krishnamoorthy et
al., 2013), which often result in tedious descrip-
tions. another line of work solves the description
generation problem via retrieval, where a descrip-
tion for an image is borrowed from semantically
most similar image from the training set (ordonez
et al., 2011; farhadi et al., 2010). this setting is,
however, less applicable to complex scenes com-
posed of a large set of objects in diverse con   gu-
rations, such as for example indoor environments.
recently, the    eld has witnessed a boom in gen-
erating image descriptions via deep neural net-
works (kiros et al., 2014; karpathy and fei-
fei, 2014; chen and zitnick, 2014) which are
able to both,
learn a weak language model as
well as generalize description to unseen images.
these approaches typically represent the image
and words/sentences with vectors and reason in
a joint embedding space. the results have been
impressive, perhaps partly due to powerful rep-
resentation on the image side (krizhevsky et al.,
2012). this line of work mainly generates a single
sentence for each image, which typically focus on
one or two objects and typically contain very few
prepositional relations between objects.

in this paper, we are interested in generat-

windowsofatablegraysofagrayin the wallliving roomnext toin front ofin front of(cid:43)(cid:80)(cid:2)(cid:78)(cid:75)(cid:88)(cid:75)(cid:80)(cid:73)(cid:2)(cid:84)(cid:81)(cid:81)(cid:79)(cid:14)(cid:2)(cid:86)(cid:74)(cid:71)(cid:84)(cid:71)(cid:2)(cid:67)(cid:84)(cid:71)(cid:2)(cid:86)(cid:89)(cid:81)(cid:2)(cid:73)(cid:84)(cid:67)(cid:91)(cid:2)(cid:85)(cid:81)(cid:72)(cid:67)(cid:85)(cid:2)(cid:80)(cid:71)(cid:90)(cid:86)(cid:2)(cid:86)(cid:81)(cid:2)(cid:71)(cid:67)(cid:69)(cid:74)(cid:2)(cid:81)(cid:86)(cid:74)(cid:71)(cid:84)(cid:2)(cid:67)(cid:80)(cid:70)(cid:2)(cid:67)(cid:2)(cid:86)(cid:67)(cid:68)(cid:78)(cid:71)(cid:2)(cid:75)(cid:80)(cid:2)(cid:72)(cid:84)(cid:81)(cid:80)(cid:86)(cid:2)(cid:81)(cid:72)(cid:2)(cid:86)(cid:74)(cid:71)(cid:79)(cid:16)(cid:2)(cid:54)(cid:74)(cid:71)(cid:84)(cid:71)(cid:2)(cid:75)(cid:85)(cid:2)(cid:67)(cid:2)(cid:74)(cid:87)(cid:73)(cid:71)(cid:2)(cid:89)(cid:75)(cid:80)(cid:70)(cid:81)(cid:89)(cid:2)(cid:75)(cid:80)(cid:2)(cid:86)(cid:74)(cid:71)(cid:2)(cid:68)(cid:67)(cid:69)(cid:77)(cid:2)(cid:89)(cid:67)(cid:78)(cid:78)(cid:16)11hugeing multi-sentence descriptions of cluttered indoor
scenes, which is particularly relevant for indoor
robotics. complex, multi-sentence output requires
us to deal with challenging problems such as con-
sistent co-referrals to visual entities across sen-
tences. furthermore, the sequence of sentences
needs to be as natural as possible, mimicking how
humans describe the scene. this is particularly
important for example in the context of social
robotics to enable realistic communications.

towards this goal, we develop a framework
with three major components: (1) a holistic vi-
sual parser that couples the id136 of objects,
attributes, and relations to produce a semantic rep-
resentation of a 3d scene (fig. 1); (2) a gener-
ative grammar automatically learned from train-
ing text; and (3) a text generation algorithm that
takes into account subtle dependencies across sen-
tences, such as logical order, diversity, saliency of
objects, and co-references.

to test the effectiveness of our approach, we
construct an augmented dataset based on nyu-
rgbd (silberman et al., 2012), where each scene
is associated with up to 5 natural language de-
scriptions from human annotators. this allows
us to learn a language model to describe images
the way that humans do. experiments show that
our method produces natural descriptions, sig-
ni   cantly improving the f-measures of id8
scores over the baseline.

2 related work

a large body of existing work deals with images
and text in one form or the other. the domi-
nant sub   eld exploits text in the form of tags or
short sentences as weak labels to learn visual mod-
els (quattoni et al., 2007; li et al., 2009; socher
and fei-fei, 2010; gupta and davis, 2008), as
well as attributes (matuszek et al., 2012; sil-
berer et al., 2013). this type of approaches have
also been explored in videos to learn visual ac-
tion models from textual summaries of videos (ra-
manathan et al., 2013), or learning visual concepts
from videos described with short sentences (yu
and siskind, 2013). another direction is to ex-
ploit short sentences associated with images in or-
der to improve visual recognition tasks (fidler et
al., 2013; kong et al., 2014). just recently, an in-
terested problem domain was introduced in (mali-
nowski and fritz, 2014) with the aim to learn how
to answer questions about images from q&a ex-

amples. in (lin et al., 2014), the authors address
visual search with complex natural lingual queries.
there has been substantial work in automat-
ically generating a caption or description for a
given image. the most popular approach has
been to retrieve a sentence from a large corpus
based on similarity of visual content (ordonez et
al., 2011; farhadi et al., 2010; kuznetsova et al.,
2012; rohrbach et al., 2013; yang et al., 2011).
this line of work bypasses having to deal with lan-
guage template speci   cation or template learning.
however, typically such approaches adopt a lim-
ited image representation such as triplets action-
object-scene (farhadi et al., 2010). this makes
a restrictive setting, as neither the image repre-
sentation nor the retrieved sentence can faithfully
model a truly complex scene. in (kuznetsova et
al., 2014) the authors go further by only learning
phrases from related images.

parallel to our work, there has been a recent
boom in image description generation with deep
networks (kiros et al., 2014; karpathy and fei-
fei, 2014; vinyals et al., 2014; mao et al., 2014;
donahue et al., 2014; fang et al., 2014; chen and
zitnick, 2014). these methods transform the im-
age as well as a sentence into a vector represen-
tation and learn a joint embedding between the
two modalities. the output of these approaches
is typically a short sentence for each image.
in
contrast, our goal here is to generate multiple de-
pendent sentences that describe the salient objects
in the scene, their properties and spatial relations.
generating descriptions has also been explored
in the video domain. (barbu et al., 2012; krish-
namoorthy et al., 2013) output a video description
in the form of subject-action-object.
in (das et
al., 2013),    concept detectors    are formed, which
are detectors for combined object and action or
scene in a particular chunk of a video. via lingual
templates the concept detectors of particular types
then produce cohesive video descriptions. due to
a limited set of concepts and templates the    nal de-
scriptions do not seem very natural. (rohrbach et
al., 2013) predicts semantic representations from
low-level video features and uses machine transla-
tion techniques to generate a sentence.

the closest to our work is (kulkarni et al.,
2011; mitchell et al., 2012; kuznetsova et al.,
2014) which, like us, is able to describe objects,
their modi   ers, and prepositions between objects.
however, our paper differs from (kulkarni et al.,

figure 2: the overall framework for description generation. the task consists of the training and the testing phase. in training,
the vision models and the generative grammar are respectively learned from a set of rgb-d images and their descriptions. in
testing, given a new image, it constructs a scene graph taking into account objects, their attributes and relationships between
objects, and transforms it to a series of semantic trees. the learned grammar then generates textual descriptions for these trees.

2011; mitchell et al., 2012) in several important
ways. in our work, we reason in 3d as opposed to
2d giving us more natural physical interpretations.
we aim to describe rich indoor scenes that contain
many objects of various classes and appear in var-
ious arrangements. in such a setting, describing
every detectable object and all relations between
them as in (kulkarni et al., 2011) would generate
prohibitively long, complex and unnatural descrip-
tions. our model tries to mimic what and how peo-
ple describe such complex 3d scenes, thus taking
into account visual saliency at the level of objects,
attributes and relations, as well as the ordering and
coherence of sentences. another important aspect
that sets us apart from most past work is that in-
stead of using a few hand-crafted templates, we
learn the grammar from training text.

3 framework overview

our framework for generating descriptions for in-
door scenes is based on a key rationale: images
and their corresponding descriptions are two dif-
ferent ways to express the underlying common se-
mantics shared by both. as shown in figure 2,
given an image, it    rst recovers the underlying se-
mantics through holistic visual analysis (lin et al.,
2013), which results in a scene graph that captures
detected objects and the spatial relations between
them (e.g. on-top-of and near, etc).

the semantics embodied by a visual scene usu-
ally has multiple aspects. when describing such
a complex scene, humans often use a paragraph
comprised of multiple sentences, each focusing on
a speci   c aspect. to imitate this behavior, this
framework transforms the scene graph into a se-
quence of semantic trees, and yields multiple sen-

tences, each from a semantic tree. to make the
results as natural as possible, we adopt two strate-
gies: (1) instead of prescribing templates in ad-
vance, we learn the grammar from a training set
    a set of rgb-d scenes with descriptions pro-
vided by humans. (2) we take into account depen-
dencies among sentences, including logical order,
saliency, coreference and diversity.

4 from rgb-d images to semantics

given an rgb-d image, we extract semantics
through holistic visual parsing. particularly, we
   rst parse the image to obtain the objects of in-
terest, their attributes, and their physical relations,
and then construct a scene graph, which provides
a coherent summary of these aspects.

4.1 holistic visual parsing
to parse the visual scene we use a recently pro-
posed approach for 3d id164 in rgb-
d data (lin et al., 2013). we brie   y summarize
this approach here. first, a set of    objectness    re-
gions are generated following (carreira and smin-
chisescu, 2012), which are encouraged to respect
intensity as well as occlusion boundaries in 3d.
these regions are projected to 3d via depth and
then cuboids are    t tightly around them, under the
constraint that they are parallel to the ground    oor.
a holistic crf model is then constructed to
jointly reason about the classes of the cuboids as
well as the class of the scene (e.g., kitchen, bath-
room). the crf thus has a random variable for
each cuboid representing its class, and a variable
for the scene. to have the possibility to remove
a bad, non-object cuboid, we have an additional
background state for each cuboid. the model ex-

generativegrammarsemantictreesparsegraphstrainingdescriptionsnewimagescenegraphsemantictreesgenerateddescriptionvisionmodelstrainingimagesabove     {1} is above {2}        above {2} is {1}below - {1} is below {2}        below {2} is {1}det   - the {1}indet - a {1}...above(indet(microwave),       det(table))a microwave is above the table.ploits various geometric and semantic relations by
incorporating them into the crf formulation as
potentials, which are summarized below:

scene appearance. to incorporate global in-
formation, a unary potential over the scene label
is computed by means of a logistic on top of the
scene classi   cation score (xiao et al., 2010).

cuboid class potential. appearance-based
classi   ers, including cpmc-o2 (carreira et al.,
2012), superpixel scores (ren et al., 2012) are
used to classify cuboids into a pre-de   ned set of
object classes. in this paper, we additionally use
id98 (krizhevsky et al., 2012) features for classi-
   cation. the classi   cation scores for each cuboid
are used as different unary potentials in the crf.
object geometry. cuboids are also classi   ed
based on geometric features (e.g. height, longer
width, aspect ratio, etc) with id166, and the classi-
   cation scores used as another unary potential.

semantic context. two co-occurrence relation-
ships are used: scene-object and object-object.
the potential values are estimated from the train-
ing set by counting the co-occurence frequencies.
geometric context. two potentials are used
to exploit the spatial relations between cuboids
in 3d, encoding close-to and on-top-of relations.
the potentials are de   ned to be the empirical co-
occurrence frequencies for each type of relation.

the crf weights to combine the potentials
are learned with a primal dual learning frame-
work (hazan and urtasun, 2010), and id136
of class labels is done with an approximated algo-
rithm (schwing et al., 2011).

4.2 scene graphs
based on the extracted visual information, we con-
struct a scene graph that captures objects, their
attributes, such as color and size, and the rela-
tions between them. in particular, a scene graph
uses nodes to represent objects and their attributes,
and edges to represent relations between nodes.
here, we consider three kinds of edges: attribute
edges that link objects to their attributes, position
edges that represent the positions of objects rela-
tive to the scene, (e.g. corner-of-room), and pair-
wise edges that characterize the relative positions
between objects (e.g. on-top-of and next-to).

given an image, a set of objects (with class la-
bels) and the scene class are obtained through vi-
sual parsing as explained in the previous section.
however, to form a scene graph, we still need

further analysis to extract attributes and relations.
for each object we also compute saliency, i.e. how
likely an object will be described. we next de-
scribe how we obtain such information.
object attributes: for each object, we use rgb
histograms and c-sift, and cluster them to ob-
tain a visual word representation. we train clas-
si   ers for nine colors that are most mentioned in
the training set, as well as two material properties
(wooden and bright). we also train classi   ers for
four different sizes (wide, tall, large, and small)
using geometric features. to encode the correla-
tions between size and the object class, we aug-
ment the feature with a class indicator vector.
object saliency: the dataset of (kong et al.,
2014) contains alignment between the nouns in
a sentence and the visual objects in the scene.
we make use of this information to train a clas-
si   er predicting whether an object in the scene is
likely to be mentioned in text. we train an id166
classi   er using class-based features (classi   cation
scores for each cuboid), geometric relations (vol-
ume, distance to camera), and color features.
object relations: we consider six types of ob-
front-of-camera,
ject positions (corner-of-room,
far-away-from-camera, center-of-room,
left-of-
room, and right-of-room), and eight types of pair-
wise relations (next-to, near, top-of, above, in-
front-of, behind, to-left-of, and to-right-of ). we
manually specify a few rules that help us decide
whether a speci   c relation is present or not1.

5 generating lingual descriptions

given a scene graph, our framework generates a
descriptive paragraph in two steps. first, it trans-
forms the scene graph into a sequence of seman-
tic trees, each focusing on a certain semantic as-
pect. then, it produces sentences, one from each
semantic tree, following a generative grammar.

5.1 semantic trees
a semantic tree captures information such as what
entities are being described and what are the re-
lationships between them. speci   cally, a semantic
tree contains a set of terminal nodes correspond-
ing to individual entities or their attributes and re-
lational nodes that express relations among them.

1we tried obtaining ground-truth for relations via mturk
(which would allow us to train classi   ers instead), however,
the results of all batches were extremely noisy.

consider a sentence    a red box is on top of a ta-
ble   . the corresponding semantic tree can be ex-
pressed as
on-top-of(indet(color(box, red)),

indet(table))

this tree has three terminals:    box   ,    table   ,
and    red   . the relation node    color(box, red)   
describes the relation between    box    and    red   ,
namely,    red    specifying the color of the    box   .
the relation    indet    quali   es the cardinality of its
child; while    on-top-of    characterizes the spatial
relation between its children.

5.2 dependencies among sentences
in human descriptions, sentences are put together
in a way that makes the resultant paragraphs co-
herent. in particular, the dependencies among sen-
tences, as outlined below, play a crucial role in
preserving the coherence a descriptive paragraph:
logical order. when describing a scene, peo-
ple present things in certain orders. the lead-
ing sentence often mentions the type of the en-
tire scene and one of the most salient object,
e.g.    there is a table in the dining room.   

diversity. people generally avoid using the
same prepositional relation in multiple sentences.
also, when an object is mentioned in multiple sen-
tences, it usually plays a different role, e.g.    there
is a table near the wall. on top of the table is a mi-
crowave oven.    here,    table    respectively serves
as a source and a target in these two sentences2.

saliency. saliency in   uences the order of sen-
tences. the statistics in (kong et al., 2014) shows
that bigger objects are often mentioned earlier on
in a description and co-referred across sentences,
e.g. one would say    this room has a dining table
with a mug on top. next to the table is a chair.   
and not    there is a mug on a table. next to the
mug is a chair.    saliency also depends on context,
e.g. for bathrooms, toilets are often mentioned.

co-reference. when an object is mentioned for
the second time following its debut, a pronoun is
often used to make the sentence concise.

richness vs. conciseness. when talking about
an object for the    rst time, describing its color/size
makes the sentence interesting and informative.
however, this is generally unnecessary the next
time the object is mentioned.

2each relation is considered as an edge. for example, in
phrases    a on-top-of b    and    a near b   ,    a    is considered
as the source, while    b    considered as the target.

i = wt

i and wt

first of all, we initialize ws

5.3 from scene graphs to semantic trees
motivated by these considerations, we devise a
method below that transforms a scene graph into
a sequence of semantic trees, each for a sentence.
i = si    ci.
here, ws
i are the weights that respectively
control how likely the i-th object will be chosen
as a source or a target in the next sentence; si is
a positive value measuring the saliency of the i-th
object, while ci is given by the classi   er to indi-
cate its con   dence as to whether it makes a correct
prediction of the object   s class. these weights are
updated as the generation proceeds.

to generate the leading sentence, we    rst draw a
source i with a id203 proportional to ws
i , and
create a semantic tree by choosing a relation, say
   in   , which would lead to a sentence like    there
is a table in the dining room.    once the i-th ob-
ject is chosen to be a source, ws
i will be set to
0, precluding it from being chosen as a source
again. however, wt
i remains unchanged, as it re-
mains    ne for it to serve as a target later.

i wt

for each subsequent sentence, we draw a source
i, a target j, and a relation r between i and j, with
id203 proportional to ws
j  r, where   r is
the prior weight of the relation r. at each iteration,
one may also choose to terminate without generat-
ing a new sentence, with a id203 proportional
to a positive value   . these choices together result
in a semantic tree in the form of    r(make tree(i),
make tree(j))   . here,    make tree(i)    creates a
sub-tree describing the object i, which may be    in-
det(color(table, black))    when the color is known.
after the generation of this semantic tree, the
j, and   r will be set to zero to pre-
weights ws
vent the objects i and j from being used again for
the same role, and the relation r from being cho-
sen next time. our algorithm also takes care of
co-references     if an object is selected again in the
next sentence, it will be replaced by a pronoun.

i , wt

5.4 grammar and derivation
given a semantic tree, our framework produces a
sentence following a generative grammar, namely,
a map from each semantic relation to a set of tem-
plates (i.e. derivation rules), as illustrated below:
indet
color
on-top-of --> {1} is on top of {2}
on top of {2} is {1}
there is {1} on top of {2}

--> a {1}
--> {2} {1}

each template has a weight that is set to its fre-

inite article will be translated into an det and indet
relation node; two nouns or noun phrases    a    and
   b    linked by a prepositional link    above    will
be translated into    above(a, b)   .

with a sentence and a semantic tree constructed
thereon, we can derive the template through re-
cursive matching, where matched children are re-
placed by a placeholder, while other words are pre-
served literally in the template. figure 3 illustrates
this procedure. we collect templates respectively
for each relation, and set the weight of each tem-
plate to its frequency. empirically, we observed a
long tailed distribution     a small number of com-
mon templates occur many times, while a dom-
inant portion of templates are used sporadically.
to improve the reliability, we discard all the tem-
plates that occur less than 5 times and all relations
whose total weight is less than 20.

6 experimental evaluation

we test the proposed framework on the nyu-v2
dataset (silberman et al., 2012) augmented with
an additional set of textual descriptions, one for
each image. particularly, we focus on assessing
both the relevance and quality of the generated de-
scriptions.

6.1 data preparation

the nyu-v2 dataset has 1449 rgb-d images of
indoor scenes (e.g. dining rooms, kitchens, of-
   ces). these images are divided into a training
a testing set, following the partition used in (lin
et al., 2013). the training set contains 795 scenes,
while the testing set contains the remaining 654.
we use the descriptions from (kong et al., 2014)
which were collected by asking mturkers to de-
scribe the image to someone who does not see it
in order to provide him/her with a vidid impres-
sion of the scene. the number of sentences per
description ranges from 1 to 10 with an average of
3. there are on average 40 words in a description.
we learn the generative grammar using the al-
gorithm described in section 5.5 from the train-
ing set of descriptions. we also train the crf for
visual analysis and apply it to detect objects and
predict their attributes and relations, following the
procedure described in section 4.1. these models
are then used to produce textual descriptions for
each test scene.

figure 3: the process to derive templates by matching se-
mantic nodes to parts of the sentence. starting from the root
node, the learning algorithm identi   es the ranges of words
corresponding to the child nodes, and replaces them with a
placeholder to obtain a template. this proceeds downward
recursively until all relation nodes are processed.

quency in the training set. the generation of a
sentence from a semantic tree proceeds from the
root, and downward recursively to the terminals.
for each relation node, a template will be chosen,
with a id203 proportional to the associated
weight. below is an example showing how a sen-
tence is derived following the grammar above.
{on-top-of(indet(color(box, red)),

=> {indet(color(box, red))} is on top of

indet(table))}

{indet(table)}

=> a {color(box, red)} is on top of a table
=> a red box is on top of a table

as the choices of templates for relational nodes
are randomized, different sentences can be derived
for the same tree, with different probabilities.

5.5 learning the grammar
the grammar for generating sentences are often
speci   ed manually in previous work (barbu et al.,
2012; das et al., 2013). this way, however, is time
consuming, unreliable, and tends to oversimplify
the language. in this work, we explore a new ap-
proach, that is, to learn the grammar from data.
the basic idea is to construct a semantic tree from
each sentence through linguistic parsing, and then
derive the templates by matching nodes of the se-
mantic tree to parts of the sentence.

first, we use the stanford parser (toutanova et
al., 2003) to obtain a parse tree for each sentence,
which is then simpli   ed through a series of    l-
tering operations. for example, we merge noun
phrases (e.g.       re distinguisher   ) into a single
node and compress common prepositional phrases
(e.g.    in the left of   ) into a single link.

a semantic tree can then be derived by re-
cursively translating the simpli   ed trees. this
is straightforward. for example, a noun    box   
with an adjective    red    will be translated into
   color(box, red)   ; a noun with a de   nite or indef-

on-top-ofindetindetcolorredboxtablea red box is on top of a tablea red boxa tablered box{1} is on top of {2}a {1}{2} {1}objects

con   g

baseline

gt
gt
gt
gt
gt
gt
real
real
real
real
real
real

l0
l1
l2
l3
l4
l5
l0
l1
l2
l3
l4
l5

r

0.3000
0.3332
0.3378
0.3392
0.3770
0.3775
0.3755
0.3243
0.3347
0.3338
0.3641
0.3663
0.3675

id81

p

0.2947
0.3249
0.3294
0.3308
0.3676
0.3680
0.3658
0.3161
0.3266
0.3256
0.3541
0.3560
0.3570

f

0.2968
0.3281
0.3327
0.3340
0.3712
0.3716
0.3695
0.3192
0.3296
0.3286
0.3580
0.3600
0.3611

r

0.0667
0.0786
0.0838
0.0849
0.1092
0.1064
0.1008
0.0752
0.0814
0.0816
0.1045
0.1039
0.1021

id82

p

0.0657
0.0765
0.0816
0.0827
0.1067
0.1040
0.0984
0.0735
0.0795
0.0796
0.1019
0.1011
0.0994

f

0.0661
0.0773
0.0824
0.0835
0.1076
0.1049
0.0993
0.0742
0.0802
0.0803
0.1029
0.1022
0.1004

r

0.1026
0.1372
0.1397
0.1409
0.1629
0.1598
0.1563
0.1306
0.1362
0.1356
0.1546
0.1534
0.1526

id8s

p

0.1006
0.1334
0.1359
0.1370
0.1584
0.1554
0.1519
0.1270
0.1325
0.1319
0.1499
0.1486
0.1478

f

0.1014
0.1348
0.1373
0.1385
0.1601
0.1570
0.1536
0.1283
0.1338
0.1332
0.1517
0.1504
0.1496

table 1: rogue scores for the baseline and our approach under con   gurations at different levels. here,    gt    and    real   
respectively refer to the results obtained based on annotated objects and objects detected by the visual parsing method. for
each rogue metric, we report the recall (r), precision (p), and f-scores (f) averaged over all scenes and 10 randomized runs.

6.2 performance metrics
to evaluate our method, we look for metrics typi-
cally used in machine translation. these include
the id7 (papineni et al., 2002) and id8
metrics among others. the id7 score measures
precision on id165s, and is thus less suitable for
our goal of lingual image description, as already
noted in (mitchell et al., 2012; das et al., 2013).
on the other hand, id8 is an id165 recall
oriented measures which evaluates the information
coverage between summaries produced by the hu-
man annotators and those automatically produced
by systems. id8-1 (unigram) recall is the best
option to use for comparing descriptions based
only on predicted keywords (das et al., 2013).
id8-2 (bigram) and id8-su4 (skip-4 bi-
gram) are best to evaluate summaries with respect
to coherence and    uency. we use the id8
metrics following (das et al., 2013) who uses it
to evaluate lingual video summarization.

6.3 comparison of results
the proposed text generation method has    ve
optional switches, controlling whether the fol-
lowing features are used during generation: (1)
diversity: encourage diversity of the sen-
tences by suppressing the entities and rela-
tions that have been mentioned; (2) saliency:
draw salient objects with higher id203; (3)
scene:
leading sentence mentions the class of
the scene; (4) attributes: use colors and sizes
to describe objects when they are available; (5)
coreference: use a pronoun to refer to an
object when it is mentioned in the previous sen-
tence. our experiments test the framework un-
der six feature-levels, level-0 to level-5, where the
level-k con   guration uses the    rst k features when

generating the sentences.
in particular, level-0
uses none of the features above, and thus each sen-
tence is generated independently using the gram-
mar; while level-5 uses all of these features.

to put our performance in perspective, we com-
pare our method to an intelligent baseline which
follows a conventional approach in description
generation. the baseline describes an image by re-
trieving visually the most similar image from the
training set, and simply using its corresponding
description. to compute our baseline, we use a
battery of visual features such as spatial pyramids
of sift, hog, lbp, geometric context, etc, and
kernels with different distances. we use (xiao et
al., 2010) to compute the kernels. based on a com-
bined kernel, we simply retrieve the training image
with the highest matching score.

table 1 shows the results. we evaluate two set-
tings: using ground-truth objects (denoted with
gt) and using the results obtained via the vi-
sual parser (denoted with real). we can see that
the proposed method signi   cantly outperforms the
baseline in all three rogue measures. also, con-
   gurations above level 3 are clearly better than
level 1 and 2, which indicates that a special leading
sentence that gives an overview of the scene is im-
portant for description generation. in addition, we
observe that there are is a noticeable improvement
from level 3 to level 4 and 5. this is not surpris-
ing: whereas attributes and coreference improve
the quality of descriptions by making them richer
and less verbose, such improvement on quality
does not contribute substantially to the rogue
score that are based on id165 comparisons.

figure 4 shows descriptions generated using our
approach on a diverse set of scenes.
it can be
seen that linguistic issues such as sentence diver-

figure 4: this figure shows several examples of the descriptions generated using the proposed frame-
work. in the top two rows the method builds on the ground-truth cuboids, while the bottom row shows
the results using the visual parser. note that in the case of gt, the input to the method is the full set of
gt objects for the image, thus the method still needs to take into account the saliency of what to talk
about. we color-code object cuboids and nouns referring to them in text.

sity, using attributes to describe objects, and using
pronouns for coreferences have been properly ad-
dressed. however, there remain some problems
that need future efforts to address. for example,
since the choices of templates for different sen-
tences are independent, sometimes an unfortunate
selection of a template sequence may make the
paragraph slightly unnatural.

7 conclusion

we presented a new framework for generating nat-
ural descriptions of indoor scenes. our framework

integrates a crf model for visual parsing, a gener-
ative grammar automatically learned from training
descriptions, as well as a transformation algorithm
to derive semantic trees from scene graphs, which
takes into account the dependencies across sen-
tences. our experiments show substantially bet-
ter descriptions than those produced by a baseline.
such    ndings indicate that high quality descrip-
tion generation requires not only reliable image
understanding, but also delicate attention to lin-
guistic issues, such as diversity, coherence, and
logical order of sentences.

(cid:54)(cid:74)(cid:71)(cid:84)(cid:71)(cid:2)(cid:75)(cid:85)(cid:2)(cid:67)(cid:2)(cid:68)(cid:84)(cid:81)(cid:89)(cid:80)(cid:2)(cid:68)(cid:71)(cid:70)(cid:2)(cid:75)(cid:80)(cid:2)(cid:86)(cid:74)(cid:71)(cid:2)(cid:68)(cid:71)(cid:70)(cid:84)(cid:81)(cid:81)(cid:79)(cid:16)(cid:2)(cid:54)(cid:74)(cid:71)(cid:2)(cid:68)(cid:71)(cid:70)(cid:2)(cid:75)(cid:85)(cid:2)(cid:75)(cid:80)(cid:2)(cid:72)(cid:84)(cid:81)(cid:80)(cid:86)(cid:2)(cid:81)(cid:72)(cid:2)(cid:67)(cid:2)(cid:74)(cid:71)(cid:67)(cid:70)(cid:68)(cid:81)(cid:67)(cid:84)(cid:70)(cid:16)(cid:2)(cid:48)(cid:71)(cid:67)(cid:84)(cid:2)(cid:86)(cid:74)(cid:71)(cid:2)(cid:68)(cid:71)(cid:70)(cid:2)(cid:75)(cid:85)(cid:2)(cid:67)(cid:2)(cid:68)(cid:78)(cid:75)(cid:80)(cid:70)(cid:85)(cid:16)(cid:2)(cid:57)(cid:71)(cid:2)(cid:69)(cid:67)(cid:80)(cid:2)(cid:85)(cid:71)(cid:71)(cid:2)(cid:67)(cid:2)(cid:68)(cid:84)(cid:81)(cid:89)(cid:80)(cid:2)(cid:69)(cid:87)(cid:84)(cid:86)(cid:67)(cid:75)(cid:80)(cid:2)(cid:80)(cid:71)(cid:67)(cid:84)(cid:2)(cid:86)(cid:74)(cid:71)(cid:2)(cid:68)(cid:78)(cid:75)(cid:80)(cid:70)(cid:85)(cid:16)(cid:2)(cid:54)(cid:74)(cid:71)(cid:84)(cid:71)(cid:2)(cid:75)(cid:85)(cid:2)(cid:67)(cid:2)(cid:69)(cid:74)(cid:71)(cid:85)(cid:86)(cid:2)(cid:80)(cid:71)(cid:67)(cid:84)(cid:2)(cid:86)(cid:74)(cid:71)(cid:2)(cid:74)(cid:71)(cid:67)(cid:70)(cid:68)(cid:81)(cid:67)(cid:84)(cid:70)(cid:16)(cid:2)(cid:35)(cid:2)(cid:89)(cid:81)(cid:81)(cid:70)(cid:71)(cid:80)(cid:2)(cid:69)(cid:87)(cid:84)(cid:86)(cid:67)(cid:75)(cid:80)(cid:2)(cid:75)(cid:85)(cid:2)(cid:75)(cid:80)(cid:2)(cid:86)(cid:74)(cid:71)(cid:2)(cid:68)(cid:71)(cid:70)(cid:84)(cid:81)(cid:81)(cid:79)(cid:16)(cid:2)(cid:54)(cid:74)(cid:71)(cid:2)(cid:69)(cid:87)(cid:84)(cid:86)(cid:67)(cid:75)(cid:80)(cid:2)(cid:75)(cid:85)(cid:2)(cid:81)(cid:80)(cid:2)(cid:86)(cid:81)(cid:82)(cid:2)(cid:81)(cid:72)(cid:2)(cid:67)(cid:2)(cid:89)(cid:81)(cid:81)(cid:70)(cid:71)(cid:80)(cid:2)(cid:74)(cid:71)(cid:67)(cid:70)(cid:68)(cid:81)(cid:67)(cid:84)(cid:70)(cid:16)(cid:2)(cid:57)(cid:71)(cid:2)(cid:69)(cid:67)(cid:80)(cid:2)(cid:85)(cid:71)(cid:71)(cid:2)(cid:67)(cid:2)(cid:69)(cid:74)(cid:71)(cid:85)(cid:86)(cid:2)(cid:75)(cid:80)(cid:2)(cid:72)(cid:84)(cid:81)(cid:80)(cid:86)(cid:2)(cid:81)(cid:72)(cid:2)(cid:86)(cid:74)(cid:71)(cid:2)(cid:69)(cid:87)(cid:84)(cid:86)(cid:67)(cid:75)(cid:80)(cid:16)(cid:2)(cid:54)(cid:74)(cid:71)(cid:2)(cid:74)(cid:71)(cid:67)(cid:70)(cid:68)(cid:81)(cid:67)(cid:84)(cid:70)(cid:2)(cid:75)(cid:85)(cid:2)(cid:80)(cid:71)(cid:67)(cid:84)(cid:2)(cid:86)(cid:74)(cid:71)(cid:2)(cid:69)(cid:74)(cid:71)(cid:85)(cid:86)(cid:16)(cid:2)(cid:43)(cid:80)(cid:2)(cid:86)(cid:74)(cid:71)(cid:2)(cid:77)(cid:75)(cid:86)(cid:69)(cid:74)(cid:71)(cid:80)(cid:14)(cid:2)(cid:86)(cid:74)(cid:71)(cid:84)(cid:71)(cid:2)(cid:75)(cid:85)(cid:2)(cid:67)(cid:2)(cid:84)(cid:71)(cid:72)(cid:84)(cid:75)(cid:73)(cid:71)(cid:84)(cid:67)(cid:86)(cid:81)(cid:84)(cid:16)(cid:2)(cid:35)(cid:2)(cid:73)(cid:84)(cid:71)(cid:71)(cid:80)(cid:2)(cid:69)(cid:67)(cid:68)(cid:75)(cid:80)(cid:71)(cid:86)(cid:2)(cid:75)(cid:85)(cid:2)(cid:80)(cid:71)(cid:67)(cid:84)(cid:2)(cid:67)(cid:2)(cid:73)(cid:84)(cid:67)(cid:91)(cid:2)(cid:81)(cid:88)(cid:71)(cid:80)(cid:16)(cid:2)(cid:48)(cid:71)(cid:67)(cid:84)(cid:2)(cid:86)(cid:74)(cid:71)(cid:2)(cid:84)(cid:71)(cid:72)(cid:84)(cid:75)(cid:73)(cid:71)(cid:84)(cid:67)(cid:86)(cid:81)(cid:84)(cid:2)(cid:75)(cid:85)(cid:2)(cid:86)(cid:74)(cid:71)(cid:2)(cid:69)(cid:67)(cid:68)(cid:75)(cid:80)(cid:71)(cid:86)(cid:16)(cid:2)(cid:57)(cid:71)(cid:2)(cid:69)(cid:67)(cid:80)(cid:2)(cid:85)(cid:71)(cid:71)(cid:2)(cid:67)(cid:2)(cid:79)(cid:75)(cid:69)(cid:84)(cid:81)(cid:89)(cid:67)(cid:88)(cid:71)(cid:2)(cid:80)(cid:71)(cid:67)(cid:84)(cid:2)(cid:86)(cid:74)(cid:71)(cid:2)(cid:69)(cid:67)(cid:68)(cid:75)(cid:80)(cid:71)(cid:86)(cid:16)(cid:2)(cid:54)(cid:74)(cid:71)(cid:2)(cid:81)(cid:88)(cid:71)(cid:80)(cid:2)(cid:75)(cid:85)(cid:2)(cid:68)(cid:71)(cid:74)(cid:75)(cid:80)(cid:70)(cid:2)(cid:86)(cid:74)(cid:71)(cid:2)(cid:84)(cid:71)(cid:72)(cid:84)(cid:75)(cid:73)(cid:71)(cid:84)(cid:67)(cid:86)(cid:81)(cid:84)(cid:16)(cid:2)(cid:2)(cid:54)(cid:74)(cid:71)(cid:84)(cid:71)(cid:2)(cid:75)(cid:85)(cid:2)(cid:67)(cid:2)(cid:85)(cid:81)(cid:72)(cid:67)(cid:2)(cid:75)(cid:80)(cid:2)(cid:86)(cid:74)(cid:71)(cid:2)(cid:78)(cid:75)(cid:88)(cid:75)(cid:80)(cid:73)(cid:2)(cid:84)(cid:81)(cid:81)(cid:79)(cid:16)(cid:2)(cid:36)(cid:71)(cid:74)(cid:75)(cid:80)(cid:70)(cid:2)(cid:86)(cid:74)(cid:71)(cid:2)(cid:85)(cid:81)(cid:72)(cid:67)(cid:2)(cid:75)(cid:85)(cid:2)(cid:67)(cid:2)(cid:89)(cid:74)(cid:75)(cid:86)(cid:71)(cid:2)(cid:69)(cid:67)(cid:68)(cid:75)(cid:80)(cid:71)(cid:86)(cid:16)(cid:2)(cid:57)(cid:71)(cid:2)(cid:69)(cid:67)(cid:80)(cid:2)(cid:85)(cid:71)(cid:71)(cid:2)(cid:67)(cid:2)(cid:68)(cid:78)(cid:67)(cid:69)(cid:77)(cid:2)(cid:69)(cid:74)(cid:67)(cid:75)(cid:84)(cid:2)(cid:75)(cid:80)(cid:2)(cid:72)(cid:84)(cid:81)(cid:80)(cid:86)(cid:2)(cid:81)(cid:72)(cid:2)(cid:86)(cid:74)(cid:71)(cid:2)(cid:69)(cid:67)(cid:68)(cid:75)(cid:80)(cid:71)(cid:86)(cid:16)(cid:2)(cid:54)(cid:74)(cid:71)(cid:84)(cid:71)(cid:2)(cid:75)(cid:85)(cid:2)(cid:67)(cid:2)(cid:79)(cid:67)(cid:80)(cid:86)(cid:71)(cid:78)(cid:2)(cid:80)(cid:71)(cid:67)(cid:84)(cid:2)(cid:86)(cid:74)(cid:71)(cid:2)(cid:69)(cid:74)(cid:67)(cid:75)(cid:84)(cid:16)(cid:2)(cid:43)(cid:80)(cid:2)(cid:86)(cid:74)(cid:71)(cid:2)(cid:68)(cid:71)(cid:70)(cid:84)(cid:81)(cid:81)(cid:79)(cid:14)(cid:2)(cid:86)(cid:74)(cid:71)(cid:84)(cid:71)(cid:2)(cid:75)(cid:85)(cid:2)(cid:67)(cid:2)(cid:85)(cid:81)(cid:72)(cid:67)(cid:16)(cid:2)(cid:57)(cid:71)(cid:2)(cid:69)(cid:67)(cid:80)(cid:2)(cid:85)(cid:71)(cid:71)(cid:2)(cid:67)(cid:2)(cid:68)(cid:84)(cid:81)(cid:89)(cid:80)(cid:2)(cid:85)(cid:74)(cid:71)(cid:78)(cid:72)(cid:2)(cid:81)(cid:80)(cid:2)(cid:86)(cid:74)(cid:71)(cid:2)(cid:84)(cid:75)(cid:73)(cid:74)(cid:86)(cid:2)(cid:81)(cid:72)(cid:2)(cid:75)(cid:86)(cid:16)(cid:2)(cid:43)(cid:80)(cid:2)(cid:72)(cid:84)(cid:81)(cid:80)(cid:86)(cid:2)(cid:81)(cid:72)(cid:2)(cid:75)(cid:86)(cid:2)(cid:75)(cid:85)(cid:2)(cid:67)(cid:2)(cid:68)(cid:78)(cid:67)(cid:69)(cid:77)(cid:2)(cid:68)(cid:71)(cid:70)(cid:16)(cid:2)(cid:43)(cid:80)(cid:2)(cid:86)(cid:74)(cid:71)(cid:2)(cid:74)(cid:81)(cid:79)(cid:71)(cid:2)(cid:81)(cid:72)(cid:72)(cid:75)(cid:69)(cid:71)(cid:14)(cid:2)(cid:86)(cid:74)(cid:71)(cid:84)(cid:71)(cid:2)(cid:75)(cid:85)(cid:2)(cid:67)(cid:2)(cid:69)(cid:74)(cid:67)(cid:75)(cid:84)(cid:16)(cid:2)(cid:43)(cid:86)(cid:2)(cid:75)(cid:85)(cid:2)(cid:80)(cid:71)(cid:67)(cid:84)(cid:2)(cid:67)(cid:2)(cid:69)(cid:67)(cid:68)(cid:75)(cid:80)(cid:71)(cid:86)(cid:16)(cid:2)(cid:57)(cid:71)(cid:2)(cid:69)(cid:67)(cid:80)(cid:2)(cid:85)(cid:71)(cid:71)(cid:2)(cid:67)(cid:2)(cid:68)(cid:84)(cid:81)(cid:89)(cid:80)(cid:2)(cid:86)(cid:67)(cid:68)(cid:78)(cid:71)(cid:2)(cid:75)(cid:80)(cid:2)(cid:72)(cid:84)(cid:81)(cid:80)(cid:86)(cid:2)(cid:81)(cid:72)(cid:2)(cid:75)(cid:86)(cid:16)(cid:2)(cid:43)(cid:80)(cid:2)(cid:86)(cid:74)(cid:71)(cid:2)(cid:68)(cid:67)(cid:86)(cid:74)(cid:84)(cid:81)(cid:81)(cid:79)(cid:14)(cid:2)(cid:86)(cid:74)(cid:71)(cid:84)(cid:71)(cid:2)(cid:75)(cid:85)(cid:2)(cid:67)(cid:2)(cid:69)(cid:87)(cid:84)(cid:86)(cid:67)(cid:75)(cid:80)(cid:16)(cid:2)(cid:43)(cid:80)(cid:2)(cid:72)(cid:84)(cid:81)(cid:80)(cid:86)(cid:2)(cid:81)(cid:72)(cid:2)(cid:67)(cid:2)(cid:86)(cid:81)(cid:75)(cid:78)(cid:71)(cid:86)(cid:2)(cid:75)(cid:85)(cid:2)(cid:67)(cid:2)(cid:85)(cid:75)(cid:80)(cid:77)(cid:16)(cid:2)(cid:54)(cid:74)(cid:71)(cid:2)(cid:86)(cid:81)(cid:75)(cid:78)(cid:71)(cid:86)(cid:2)(cid:75)(cid:85)(cid:2)(cid:75)(cid:80)(cid:2)(cid:72)(cid:84)(cid:81)(cid:80)(cid:86)(cid:2)(cid:81)(cid:72)(cid:2)(cid:86)(cid:74)(cid:71)(cid:2)(cid:69)(cid:87)(cid:84)(cid:86)(cid:67)(cid:75)(cid:80)(cid:16)(cid:2)(cid:35)(cid:2)(cid:85)(cid:74)(cid:71)(cid:78)(cid:72)(cid:2)(cid:75)(cid:85)(cid:2)(cid:75)(cid:80)(cid:2)(cid:72)(cid:84)(cid:81)(cid:80)(cid:86)(cid:2)(cid:81)(cid:72)(cid:2)(cid:86)(cid:74)(cid:71)(cid:2)(cid:69)(cid:87)(cid:84)(cid:86)(cid:67)(cid:75)(cid:80)(cid:16)(cid:2)(cid:43)(cid:80)(cid:2)(cid:86)(cid:74)(cid:71)(cid:2)(cid:74)(cid:81)(cid:79)(cid:71)(cid:2)(cid:81)(cid:72)(cid:72)(cid:75)(cid:69)(cid:71)(cid:14)(cid:2)(cid:86)(cid:74)(cid:71)(cid:84)(cid:71)(cid:2)(cid:75)(cid:85)(cid:2)(cid:67)(cid:2)(cid:68)(cid:84)(cid:81)(cid:89)(cid:80)(cid:2)(cid:85)(cid:74)(cid:71)(cid:78)(cid:72)(cid:16)(cid:2)(cid:57)(cid:71)(cid:2)(cid:69)(cid:67)(cid:80)(cid:2)(cid:85)(cid:71)(cid:71)(cid:2)(cid:67)(cid:2)(cid:86)(cid:67)(cid:68)(cid:78)(cid:71)(cid:2)(cid:75)(cid:80)(cid:2)(cid:72)(cid:84)(cid:81)(cid:80)(cid:86)(cid:2)(cid:81)(cid:72)(cid:2)(cid:75)(cid:86)(cid:16)(cid:2)(cid:48)(cid:71)(cid:67)(cid:84)(cid:2)(cid:75)(cid:86)(cid:2)(cid:75)(cid:85)(cid:2)(cid:67)(cid:2)(cid:68)(cid:84)(cid:75)(cid:73)(cid:74)(cid:86)(cid:2)(cid:82)(cid:84)(cid:75)(cid:80)(cid:86)(cid:71)(cid:84)(cid:16)(cid:2)(cid:43)(cid:80)(cid:2)(cid:86)(cid:74)(cid:71)(cid:2)(cid:81)(cid:72)(cid:72)(cid:75)(cid:69)(cid:71)(cid:14)(cid:2)(cid:86)(cid:74)(cid:71)(cid:84)(cid:71)(cid:2)(cid:75)(cid:85)(cid:2)(cid:67)(cid:2)(cid:68)(cid:81)(cid:67)(cid:84)(cid:70)(cid:16)(cid:2)(cid:57)(cid:71)(cid:2)(cid:69)(cid:67)(cid:80)(cid:2)(cid:85)(cid:71)(cid:71)(cid:2)(cid:67)(cid:2)(cid:69)(cid:67)(cid:68)(cid:75)(cid:80)(cid:71)(cid:86)(cid:2)(cid:75)(cid:80)(cid:2)(cid:72)(cid:84)(cid:81)(cid:80)(cid:86)(cid:2)(cid:81)(cid:72)(cid:2)(cid:86)(cid:74)(cid:71)(cid:2)(cid:68)(cid:81)(cid:67)(cid:84)(cid:70)(cid:16)(cid:2)(cid:57)(cid:71)(cid:2)(cid:69)(cid:67)(cid:80)(cid:2)(cid:85)(cid:71)(cid:71)(cid:2)(cid:67)(cid:2)(cid:79)(cid:81)(cid:80)(cid:75)(cid:86)(cid:81)(cid:84)(cid:2)(cid:80)(cid:71)(cid:67)(cid:84)(cid:2)(cid:86)(cid:74)(cid:71)(cid:2)(cid:68)(cid:81)(cid:67)(cid:84)(cid:70)(cid:16)(cid:2)(cid:43)(cid:80)(cid:2)(cid:86)(cid:74)(cid:71)(cid:2)(cid:81)(cid:72)(cid:72)(cid:75)(cid:69)(cid:71)(cid:14)(cid:2)(cid:86)(cid:74)(cid:71)(cid:84)(cid:71)(cid:2)(cid:75)(cid:85)(cid:2)(cid:67)(cid:2)(cid:86)(cid:67)(cid:68)(cid:78)(cid:71)(cid:16)(cid:2)(cid:43)(cid:80)(cid:2)(cid:86)(cid:74)(cid:71)(cid:2)(cid:78)(cid:75)(cid:88)(cid:75)(cid:80)(cid:73)(cid:2)(cid:84)(cid:81)(cid:81)(cid:79)(cid:14)(cid:2)(cid:86)(cid:74)(cid:71)(cid:84)(cid:71)(cid:2)(cid:75)(cid:85)(cid:2)(cid:67)(cid:2)(cid:79)(cid:81)(cid:80)(cid:75)(cid:86)(cid:81)(cid:84)(cid:16)(cid:2)(cid:54)(cid:74)(cid:71)(cid:2)(cid:79)(cid:81)(cid:80)(cid:75)(cid:86)(cid:81)(cid:84)(cid:2)(cid:75)(cid:85)(cid:2)(cid:68)(cid:71)(cid:74)(cid:75)(cid:80)(cid:70)(cid:2)(cid:67)(cid:2)(cid:69)(cid:74)(cid:67)(cid:75)(cid:84)(cid:16)(cid:2)(cid:57)(cid:71)(cid:2)(cid:69)(cid:67)(cid:80)(cid:2)(cid:85)(cid:71)(cid:71)(cid:2)(cid:86)(cid:74)(cid:71)(cid:2)(cid:79)(cid:81)(cid:80)(cid:75)(cid:86)(cid:81)(cid:84)(cid:2)(cid:81)(cid:80)(cid:2)(cid:86)(cid:81)(cid:82)(cid:2)(cid:81)(cid:72)(cid:2)(cid:67)(cid:2)(cid:86)(cid:67)(cid:68)(cid:78)(cid:71)(cid:16)(cid:2)(cid:54)(cid:74)(cid:71)(cid:84)(cid:71)(cid:2)(cid:75)(cid:85)(cid:2)(cid:86)(cid:74)(cid:71)(cid:2)(cid:86)(cid:67)(cid:68)(cid:78)(cid:71)(cid:2)(cid:80)(cid:71)(cid:67)(cid:84)(cid:2)(cid:86)(cid:74)(cid:71)(cid:2)(cid:79)(cid:81)(cid:80)(cid:75)(cid:86)(cid:81)(cid:84)(cid:16)(cid:2)(cid:54)(cid:74)(cid:71)(cid:2)(cid:69)(cid:74)(cid:67)(cid:75)(cid:84)(cid:2)(cid:75)(cid:85)(cid:2)(cid:80)(cid:71)(cid:67)(cid:84)(cid:2)(cid:86)(cid:74)(cid:71)(cid:2)(cid:86)(cid:67)(cid:68)(cid:78)(cid:71)(cid:16)(cid:2)(cid:43)(cid:80)(cid:2)(cid:86)(cid:74)(cid:71)(cid:2)(cid:77)(cid:75)(cid:86)(cid:69)(cid:74)(cid:71)(cid:80)(cid:14)(cid:2)(cid:86)(cid:74)(cid:71)(cid:84)(cid:71)(cid:2)(cid:75)(cid:85)(cid:2)(cid:67)(cid:2)(cid:69)(cid:74)(cid:67)(cid:75)(cid:84)(cid:16)(cid:2)(cid:35)(cid:2)(cid:69)(cid:67)(cid:68)(cid:75)(cid:80)(cid:71)(cid:86)(cid:2)(cid:75)(cid:85)(cid:2)(cid:68)(cid:71)(cid:74)(cid:75)(cid:80)(cid:70)(cid:2)(cid:67)(cid:2)(cid:85)(cid:81)(cid:72)(cid:67)(cid:16)(cid:2)(cid:54)(cid:74)(cid:71)(cid:2)(cid:85)(cid:81)(cid:72)(cid:67)(cid:2)(cid:75)(cid:85)(cid:2)(cid:80)(cid:71)(cid:67)(cid:84)(cid:2)(cid:86)(cid:74)(cid:71)(cid:2)(cid:69)(cid:74)(cid:67)(cid:75)(cid:84)(cid:16)(cid:2)(cid:54)(cid:74)(cid:71)(cid:84)(cid:71)(cid:2)(cid:75)(cid:85)(cid:2)(cid:67)(cid:2)(cid:89)(cid:74)(cid:75)(cid:86)(cid:71)(cid:2)(cid:69)(cid:81)(cid:87)(cid:80)(cid:86)(cid:71)(cid:84)(cid:2)(cid:75)(cid:80)(cid:2)(cid:86)(cid:74)(cid:71)(cid:2)(cid:77)(cid:75)(cid:86)(cid:69)(cid:74)(cid:71)(cid:80)(cid:16)(cid:2)(cid:54)(cid:74)(cid:71)(cid:2)(cid:69)(cid:81)(cid:87)(cid:80)(cid:86)(cid:71)(cid:84)(cid:2)(cid:75)(cid:85)(cid:2)(cid:80)(cid:71)(cid:67)(cid:84)(cid:2)(cid:67)(cid:2)(cid:89)(cid:74)(cid:75)(cid:86)(cid:71)(cid:2)(cid:69)(cid:67)(cid:68)(cid:75)(cid:80)(cid:71)(cid:86)(cid:16)(cid:2)(cid:48)(cid:71)(cid:67)(cid:84)(cid:2)(cid:67)(cid:2)(cid:84)(cid:71)(cid:72)(cid:84)(cid:75)(cid:73)(cid:71)(cid:84)(cid:67)(cid:86)(cid:81)(cid:84)(cid:2)(cid:75)(cid:85)(cid:2)(cid:86)(cid:74)(cid:71)(cid:2)(cid:69)(cid:67)(cid:68)(cid:75)(cid:80)(cid:71)(cid:86)(cid:16)(cid:2)(cid:57)(cid:71)(cid:2)(cid:69)(cid:67)(cid:80)(cid:2)(cid:85)(cid:71)(cid:71)(cid:2)(cid:67)(cid:2)(cid:73)(cid:84)(cid:71)(cid:71)(cid:80)(cid:2)(cid:79)(cid:75)(cid:69)(cid:84)(cid:81)(cid:89)(cid:67)(cid:88)(cid:71)(cid:2)(cid:81)(cid:80)(cid:2)(cid:86)(cid:74)(cid:71)(cid:2)(cid:84)(cid:75)(cid:73)(cid:74)(cid:86)(cid:2)(cid:81)(cid:72)(cid:2)(cid:86)(cid:74)(cid:71)(cid:2)(cid:69)(cid:67)(cid:68)(cid:75)(cid:80)(cid:71)(cid:86)(cid:16)(cid:2)(cid:54)(cid:74)(cid:71)(cid:2)(cid:84)(cid:71)(cid:72)(cid:84)(cid:75)(cid:73)(cid:71)(cid:84)(cid:67)(cid:86)(cid:81)(cid:84)(cid:2)(cid:75)(cid:85)(cid:2)(cid:80)(cid:71)(cid:67)(cid:84)(cid:2)(cid:67)(cid:2)(cid:85)(cid:74)(cid:71)(cid:78)(cid:72)(cid:16)(cid:2)(cid:2)references
a. barbu, a. bridge, z. burchill, d. coroian,
s. dickinson, s. fidler, a. michaux, s. mussman,
s. narayanaswamy, d. salvi, l. schmidt, j. shang-
guan, j. siskind, j. waggoner, s. wang, j. wei,
y. yin, and z. zhang. 2012. video-in-sentences
out. in uai.

j. carreira and c. sminchisescu. 2012. cpmc: au-
tomatic object segmentation using constrained para-
metric min-cuts. tpami.

j. carreira, r. caseiroa, j. batista, and c. sminchis-
escu. 2012. semantic segmentation with second-
order pooling. in eccv12.

x. chen and c. l. zitnick. 2014. learning a recurrent
visual representation for image id134.
in arxiv:1411.5654.

p. das, c. xu, r. f. doell, and j. j corso. 2013. a
thousand frames in just a few words: lingual de-
scription of videos through latent topics and sparse
object stitching. in cvpr.

j. donahue, l. a. hendricks, s. guadarrama,
m. rohrbach, s. venugopalan, k. saenko, and
t. darrell. 2014. long-term recurrent convolutional
networks for visual recognition and description. in
arxiv:1411.4389.

h. fang, s. gupta, f. iandola, r. srivastava, l. deng,
p. dollar, j. gao, x. he, m. mitchell, j. c. platt,
c. l. zitnick, and g. zweig. 2014. from captions
to visual concepts and back. in arxiv:1411.4952.

a. farhadi, m. hejrati, m. sadeghi, p. young,
c. rashtchian, j. hockenmaier, and d. forsyth.
2010. every picture tells a story: generating sen-
tences for images. in eccv.

s. fidler, a. sharma, and r. urtasun. 2013. a sen-

tence is worth a thousand pixels. in cvpr.

a. gupta and l. davis. 2008. beyond nouns: ex-
ploiting prepositions and comparative adjectives for
learning visual classi   ers. in eccv.

t. hazan and r. urtasun.

2010. a primal-dual
message-passing algorithm for approximated large
scale id170. in nips.

a. karpathy and l. fei-fei.

2014. deep visual-
semantic alignments for generating image descrip-
tions. in arxiv:1412.2306.

r. kiros, r. salakhutdinov, and r. s. zemel. 2014.
unifying visual-semantic embeddings with multi-
modal neural language models. in arxiv:1411.2539.

n. krishnamoorthy, g. malkarnenkar, r. j. mooney,
k. saenko, and s. guadarrama. 2013. generat-
ing natural-language video descriptions using text-
mined knowledge. in aaai, july.

a. krizhevsky, i. sutskever, and g. e. hinton. 2012.
id163 classi   cation with deep convolutional neu-
ral networks. in nips, pages 1097   1105.

g. kulkarni, v. premraj, s. dhar, s. li, y. choi,
a. berg, and t. berg. 2011. baby talk: under-
standing and generating simple image descriptions.
in cvpr.

p. kuznetsova, v. ordonez, a. berg, t. berg, and
y. choi. 2012. collective generation of natural im-
age descriptions. in association for computational
linguistics (acl).

p. kuznetsova, v. ordonez, t. l. berg, and y. choi.
2014. treetalk: composition and compression of
trees for image descriptions. in tacl.

l. li, r. socher, and l. fei-fei.

2009. towards
total scene understanding:classi   cation, annotation
and segmentation in an automatic framework.
in
cvpr.

d. lin, s. fidler, and r. urtasun. 2013. holistic
scene understanding for 3d id164 with
rgbd cameras. in iccv.

d. lin, s. fidler, c. kong, and r. urtasun. 2014. vi-
sual semantic search: retrieving videos via complex
textual queries. in cvpr.

m. malinowski and m. fritz. 2014. a multi-world
approach to id53 about real-world
scenes based on uncertain input. in nips.

j. mao, w. xu, y. yang, j. wang, and a. l. yuille.
2014. explain images with multimodal recurrent
neural networks. in arxiv:1410.1090.

c. matuszek, n. fitzgerald, l. zettlemoyer, l. bo, and
d. fox. 2012. a joint model of language and per-
ception for grounded attribute learning. in interna-
tional conference on machine learning.

m. mitchell, j. dodge, a. goyal, kota yamaguchi,
k. sratos, x. han, a. mensch, a. c. berg, t. l.
berg, and h. daume iii. 2012. midge: generating
image descriptions from id161 detections.
in european chapter of the association for compu-
tational linguistics.

v. ordonez, g. kulkarni, and t. berg. 2011. im2text:
describing images using 1 million captioned pho-
tographs. in nips.

k. papineni, s.; roukos, t. ward, and w. j. zhu. 2002.
id7: a method for automatic evaluation of machine
translation. in acl, pages 311   318.

c. kong, d. lin, m. bansal, r. urtasun, and s. fidler.
2014. what are you talking about? text-to-image
coreference. in cvpr.

a. quattoni, m. collins, and t. darrell. 2007. learn-
ing visual representations using images with cap-
tions. in cvpr07.

v. ramanathan, p. liang, and l. fei-fei. 2013. video
event understanding using natural language descrip-
tions. in iccv.

x. ren, l. bo, and d. fox. 2012. rgb-(d) scene label-

ing: features and algorithms. in cvpr.

m. rohrbach, w. qiu, i. titov, s. thater, m. pinkal,
and b. schiele. 2013. translating video content to
natural language descriptions. in iccv.

a. schwing, t. hazan, m. pollefeys, and r. urtasun.
2011. distributed message passing for large scale
id114. in cvpr.

c. silberer, v. ferrari, and m. lapata. 2013. models
of semantic representation with visual attributes. in
association for computational linguistics (acl).

n. silberman, p. kohli, d. hoiem, and r. fergus.
indoor segmentation and support id136

2012.
from rgbd images. in eccv.

r. socher and l. fei-fei. 2010. connecting modali-
ties: semi-supervised segmentation and annotation
of images using unaligned text corpora. in cvpr.

k. toutanova, d. klein, and c. manning.

2003.
feature-rich part-of-speech tagging with a cyclic de-
pendency network. in hlt-naacl.

o. vinyals, a. toshev, s. bengio, and d. erhan. 2014.
show and tell: a neural image caption generator. in
arxiv:1411.4555.

j. xiao, j. hays, k. ehinger, a. oliva, and a. torralba.
2010. sun database: large-scale scene recognition
from abbey to zoo. in cvpr.

y. yang, c. l. teo, h. daum  e, iii, and y. aloimonos.
2011. corpus-guided sentence generation of natural
images. in emnlp, pages 444   454.

h. yu and j. m. siskind. 2013. grounded language
in

learning from video described with sentences.
association for computational linguistics (acl).

