what	
   the	
   f-     measure	
   doesn   t	
   measure   	
   

features,	
   flaws,	
   fallacies	
   and	
   fixes	
   

 

david m.w. powers, beijing university of technology, china & flinders university, australia 
technical report kit-14-001 computer science, engineering & mathematics, flinders university 

the f-measure or f-score is one of the most commonly used    single number    measures 

in information retrieval, natural language processing and machine learning, but it is 

based on a mistake, and the flawed assumptions render it unsuitable for use in most 

contexts! fortunately, there are better alternatives    

what	
   the	
   f-     measure	
   is!	
   
f-measure, sometimes known as f-score or (incorrectly) the f1 metric (the   =1 case of 
the more general measure), is a weighted harmonic mean of recall & precision (r & p).  

there are several motivations for this choice of mean. in particular, the harmonic mean is 

commonly appropriate when averaging rates or frequencies, but there is also a set-

theoretic reason we will discuss later.  the most general form, f, allows differential 

weighting of recall and precision but commonly they are given equal weight, giving rise 

to f1 but as it is so ubiquitous this is often understood when referring to f-measure. 

who   s	
   the	
   f-     measure	
   for?	
   
f-measure comes from information retrieval (ir) where recall is the frequency with 

which relevant documents are retrieved or    recalled    by a system, but it is known 

elsewhere as sensitivity or true positive rate (tpr).  precision is the frequency with 

which retrieved documents or predictions are relevant or    correct   , and is properly a form 

of accuracy, also known as positive predictive value (ppv) or true positive accuracy 

(tpa). f is intended to combine these into a single measure of search    effectiveness   . 

figure 1. medium accuracy with high precision (left) or high trueness (right) but 
not both together - stolen from wikipedia (redraw) 

more generally, precision refers to the concept of consistency, or the ability to 

group well, while accuracy refers to how close we are to the target, and trueness refers to 
how close we are to a specific target on average (figure 1)1. 

high precision and low accuracy is possible due to systematic bias. one of the 

problems with recall, precision, f-measure and accuracy as used in information 

retrieval is that they are easily biased. to better understand the relationships between 

these measures it is useful to give their formulae in two forms, one form related to the 

raw counts, and one related to normalized frequencies (equation 1 and table 1). 

these statistics are all appropriate when there is one class of items that is of 

interest or relevance out of a larger set of n items or instances. this single class of 

interest we call the positive class, or the real positives (rp). more generally we name 

multiple classes and refer to the proportion of the total each represent as its prevalence 

(for rps, prev=  =rp=rp/n). ir also assumes a retrieval mechanism that gives rise to the 

predicted positives, and also represents the bias of this    classifier    (bias=  =pp=pp/n).  

in this systematic notation, we use upper case initialisms to refer to counts of items (e.g. 

rp), and lower case equivalents to refer to the corresponding probabilities or proportions 

(rp). it is also common to use greek equivalents for probabilities in a mnemonic way (  ). 

how   s	
   the	
   f-     measure	
   defined?	
   
we can now formally define recall (r or rec) and precision (p or prec) in terms of true 

positives (tp=tp/n) and the +ve prevalence (prev) and bias, using either their count or 

id203 forms. table 1 is provided to explain the notation in our equations, where we 

also include definitions of accuracy (a or acc) and f using two different means: 

r = tp / rp = tp / rp 

p = tp / pp = tp / pp 

a = [tp+tn]/n = tp+tn 

f = tp/am(rp,pp) = hm(r,p) 

(1) 

(2) 

(3) 

(4) 

table 1. systematic notation underlying (1-4) defining f as a harmonic mean (hm) and showing how 
this corresponds to referencing to the putative distribution given by the arithmetic mean (am). 

 

+p 
   p  

 

+r 
tp 
fn 
rp 

   r  
fp 
tn 
rn 

 
pp 
pn 
1  

 
 
 
 

 

+p 
   p  

 

+r 
tp 
fn 
rp 

   r  
fp 
tn 
rn 

 
pp 
pn 
n 

note that a, r and p are themselves probabilities or proportions.  accuracy is the 

id203 that a randomly chosen instance (positive or negative, relevant or irrelevant) 

will be correct. recall is the id203 that a randomly chosen relevant instance will be 

predicted (positive).  precision is the id203 that a randomly chosen predicted 

instance (positive) will be relevant. accuracy can also shown to be the average of recall 

and inverse recall (viz. with positives and negatives inverted) weighted by prevalence. 

accuracy is also the average of precision and inverse precision weighted by bias. 

why	
   the	
   f-     measure	
   is	
   used!	
   
we can now see another, set theoretic, way of looking at the definition of f-measure, and 
this is how it was originally defined in the equally weighted version (figure 2)2. in fact, 
what was defined was e = 1-f which is a reinvention of the dice semimetric3 that was 
normalized by dividing by the average size of the compared sets. this is how we come to 

have f as a harmonic mean, as the tp intersection size is divided by the arithmetic mean 

of the rp and pp cardinalities. moreover the dice distance measure, which failed to 
satisfy the triangle inequality, was a faulty reinvention of the jaccard metric4 - jaccard 
normalizes more appropriately by the size of the union of the two sets (without double 

counting), and we note that it indeed satisfies the mathematical definition of a metric. 

figure 2. set theoretic interpretation of f-measure. stolen from van rijsbergen (1979) fig 7.11 
(redraw). f-measure corresponds to the number of items in the white intersection divided by the 
average size of the a and b groups being the real and predicted items. 

f1 is also a reinvention of the statistical measure positive specific agreement, 

which is designed to compare the agreement of two raters under the assumption that their 

ratings come from the same distribution (e.g. they are both native speakers of the same 

dialect with the same understanding of nouns and verbs). thus the average of the two 

separate sample distributions is taken as an approximation of the underlying but unknown 

distribution. this is hardly appropriate when one distribution is a real distribution based 

on human judgements, and the other comes from a system being evaluated. we will 

however meet this assumption again when we discuss chance-corrected measures. 

the f-measure was popularized by the muc and trec5 competitions based on a 

presentation in the 2nd edition of the early van rijsbergen textbook on information 
retrieval2 that recapitulated his 1974 paper. however, the function defined there was e 
for effectiveness (e=1-f) and made use of a different function f. but f has stuck, and it 

is now virtually impossible to publish work in information retrieval or natural language 

processing without including it. but that is a big mistake    

when	
   the	
   f-     measure	
   is	
   wrong!	
   
we have already noted in passing four flaws with f-measure that emerged from 

theoretical considerations, and we add three further practical issues: 
     f-measure (like accuracy, recall and precision) focuses on one class only 
     f-measure (like accuracy, recall and precision) is biased to the majority class 
     f-measure as a id203 assumes the real and prediction distributions are identical 
     e-measure (1-f) is not technically a metric as it does not satisfy a triangle inequality  
     f-measures don   t average well across real classes or predicted labels or runs  
     f-measure doesn   t in general take into account the true negatives (tn) 
     f-measure gives different optima from other approaches and tradeoffs. 

one-     class	
   
the one-class issue is one of usage. if you are only interested in one class, then f-

measure is not unreasonable.  but note that the number of true negatives (tn) can change 

arbitrarily without changing f-measure (as we discuss further below), and that there is no 

easy way to form a macro-average across class or prediction distributions     it is assessing 

performance relative to a mixture of the real and prediction distribution, and if we did 

perform the calculations to average over that fictitious distribution, we would get rand 

accuracy (3).  with recall (1), if we macro-average weighted by the prevalence of each 

class, we also get rand accuracy (3). with precision (2), if we macro-average weighted 

by the bias towards predicting each label, we also get rand accuracy (3). 

bias	
   
the bias issue is arguably the most serious, and affects both recall and precision as well 
as accuracy. in fact, van rijsbergen2 reviewed techniques that dealt with bias and chance 
(effecively roc and informedness, which we discuss below). he however started with 

the goal of finding a way of combining recall and precision into a single measure, and 

concluded that for the purposes of information retrieval the additional complexity of the 

proposals wasn   t warranted as the studies of the other methods did not conclusively show 

they were optimal. the e-measures that we know in complementary form as the f1-

measure and the more general f-measure, were in fact special cases of a far more general 

approach to fitting intuitions about effectiveness and choosing the point to optimize in 

relation to the recall-precision tradeoff. but bias impacts both recall and precision 

themselves, and no form of averaging is going to get rid of it, so it is clearly suboptimal. 

but van rijsbergen also notes that his general f-formula for combining them (not the f-

measure we know) could be useful with measures other than recall and precision.  

the damning example of bias in f-measure that brought this to our attention came 
from real life examples in natural language processing (parsing and tagging)6.  here we 
found that a common tuning approach to increase the f-score was to dumb down the 

system by getting it to guess rather than use principled statistical techniques. there is a 

real problem here as a better system can actually get a worse f-score. 

one example concerned the word    water   , which was almost always a noun 

(roughly 90% of the time) and much more rarely a verb (say 10%) and we can assume for 

simplicity that any apparent adjectival uses are in fact nominal. what the systems did was 

simply say water is always a noun. this particular form of guessing (always guessing 

noun) delivers 100% recall, and 90% precision, and 90% accuracy, while any of the 

arithmetic, geometric or harmonic means is somewhere around halfway between the 90% 

and 100% effectiveness estimates, with arithmetic being the highest, the harmonic mean 

more conservative, and the arithmetically weighted average accuracy is the minimum.  

the arithmetic, geometric and harmonic means correspond to the lp means for  

p = +1, 0 and    1. the geometric mean is actually the geometric means of the paired lp 

means for p=  p and is thus is in a sense the middle mean and is usually closest to the 

mode and least affected by outliers. in application to recall and precision, this gives rise 

to the g-measure (which also satisfies the general form of 1-e-measure). accuracy is 

based on a weighted arithmetic mean, but even though we get 0% inverse recall and 

10% inverse precision, the low inverse bias (0%) and prevalence (10%) mean that the 

weighted average doesn   t help either, and this also applies for any other weighting in 

[0,1] that might be applied to recall and precision, in the most general form of the f-

measure. furthermore the limits for p=      correspond to max and min, and min is the 
most conservative of the lp family, but all these lp means lie in the [90,100]% range, 

irrespective of weighting. thus no choice or weighting can avoid the bias problem.   

distribution	
   
the distribution problem is more subtle, but when it comes down to it the correct number 

of positives to predict is the actual number of real positives. in the closely related ppv 

and dice measures, there is an explicit assumption that two different raters    class labels 

are drawn from the same distribution without any expectation that one is more reliable.  

this does not apply here, if we truly believe our    gold standard    is ground truth, and that 

the system we are evaluating is the one that is at fault if there are discrepancies. this is 

clearly a problem for machine learning and intelligent systems in general. 

for information retrieval, the situation is a little different, as there is really only 

one class of interest. the number of irrelevant documents is sufficiently large to be 

beyond our comprehension, but the number of relevant documents may also be very 
large, and indeed not completely known. early in the trec5 competitions, the set of 
relevant documents was initially seeded with the documents returned by any of the 

systems, and only these were evaluated for relevance. later better systems being 

evaluated were marked wrong, reducing precision, for returning a relevant document that 

none of the seeding systems discovered. this has been recognized by use of alternate 

terminology such as coverage where only a subset of relevant documents is known. 

moreover the number of documents returned is often limited by practical considerations, 

often to a fixed ceiling (and search engines may allow users to chose it). furthermore, 

search engines will allow further blocks of hits and users will often decide to take another 

block two or three times, if it seems like it is worthwhile (e.g. sufficient promising leads 

to keep going but nothing good enough a match to stop). 

two alternatives to f-measure that became common in ir in recent years through 

trec5 are map (mean average precision) and r-precision. map is a kind of area 
under the recall-precision curve that averages over the different bias points 

corresponding to this    keeping going    possibility. map tends to correlate well with the 

simpler r-precision that effectively evaluates at the point where bias = prevalence or pp 

= rp. here of course prevalence or rp represents the number of documents you expect 

which in an experimental context is the number you know you have. 

interestingly, van rijsbergen   s derivation2 hinges on the assumption that the user 

will be interested in a particular tradeoff of recall vs precision     that is how much 

increase in recall will be traded off for a decrease in precision.  this is expressed in 

terms of the ratio p/r and directly leads both to the weighting factor and the choice of the 

harmonic mean in his derivation.  however, given the definitions of recall and precision 

(1&2), this corresponds to prev/bias (  /  =rp/pp). on the other hand, setting prev=bias 

corresponding to p/r=1, or any other constant value, doesn   t allow for the fact that 

different levels of precision mean you need different numbers of hits returned to ensure 

you get any desired number of relevant documents, d: d = pp/precision. but for a 

particular system and user (and set of topics searched) it may be that precision 

approximates a constant and the p/r ratio is thus meaningful, setting the desired recall. 

setting p/r can also be viewed as setting a tradeoff between false positives (fp) 

and false negatives (fn), if we expand out the denominators of (1&2). in a more general 

intelligent systems context, this corresponds to setting the relative costs of false 

positives and negatives, and this is also a feature of roc curves which trade off tpr 

(aka recall) and fpr (aka fallout = 1     inverse recall). 

setting 0<p/r<    also forces p>0 and r>0, which in turn implies and requires 

tp>0. however, setting the weighting for f-measure does not achieve this, and does not 

solve the bias problem although it does affect the distribution problem. p/r     1. a choice 

of f-measure other than f1 (  =1) does imply a bias away from the bias = prevalence 
constraint. this however considers only the mean of the distributions, and it is possible 

that the fp and fn errors have different distributions, both from each other and for 

different queries. in particular they can have different variances (as covered by van 
rijsbergen in his review of roc-related measures2), and this means that any setting of 
p/r or    other than 1 cannot be effective. 

metricity	
   
whether a measure is a metric or not may seem rather academic. f is set up as a 

similarity measure     we want it to be 1.  but e was set up as a distance or dissimilarity or 

error measure     we want it to be 0. this difference is similar to the difference between 

use of the cosine or correlation type measures we discuss later, which we want to be 1 

for maximum similarity, versus the euclidean distance measure, which we want to be 0. 

this in turn reflects a sine function versus a cosine function of the divergence of the 

vectors being compared. for purposes of graphical visualization well behaved measures 

are desirable, and the ability to convert sensibly into distances is desirable for similarity 

measures. 

in terms of our intuitions, we expect things to add together in certain ways.  the 

triangle equality is the missing element that the e and f-measures fail on.  this is the idea 

that the shortest distance between two points is the direct line between them. for the e-

measure, the sets {r} and {p}are 1/3 away from {r,p} but 1 away from each other, so it 

is closer to go from {r} to {p} via {r,p} than directly (where r and p here represent 

individual documents)! 

if such measures are used for id91 it leads to very confusing, non-monotonic 

results. 

averaging	
   
we saw earlier that averaging recall with prevalence weighting and averaging precision 

with bias weighting both give accuacy.  this means that averaging generalizes from the 

two class case we have been considering to the multiclass case.  

f-measure   s harmonic mean essentially means we should be averaging over the 

putative expected (real or predicted) population, and in practice this means that macro-

averaging in a principled way is not practical, and that macro-averaging based on either 

equal weighting or prevalence weighting, as many systems do, is not meaningful. some 

systems even do different kinds of averaging in different places and get inconsistent 

results.  

this    apples vs pears    principle applies when we are averaging over multiple runs 

or multiple queries or multiple datasets as well as multiple classes. it is important always 

to average using weights reflecting the appropriate units. if we are talking recall we are 

talking proportions relative to the actual members of a class (per real positive). if we are 

talking precision we are talking proportions relative to the predictions (per predicted 

positive). if we are talking accuracy we are talking about all instances (relative to n if n 

differs between datasets). if we are talking f-measure, we don   t know what we are 

talking about! results are relative to some fictitious intermediate distribution. 

negatives	
   
in information retrieval the negatives, the irrelevant documents, do not concern us at all. 

there are so many of them that the inverse precision and inverse recall are near enough 

to 0 to not be a significant factor, and thus f-measure has been argued to be a useful 

simplification for the sake of efficiency and comprehensibility. 

nonetheless for other intelligent systems, both (or multiple) classes tend to be 

significant for us, but neither recall nor precision take the tn cell of the contingency 

table into account. tn can be incremented from 0, to include almost all examples, 

without affecting the precision or recall, and thus without affecting f. 

with this view, based on counts, we are adding new examples to the system, 

increasing n, and it is getting them all wrong.  

it has however been claimed that the f-measure does in fact take tn into account 

if we consider rp, rn and n to be fixed, and n known. this is based on dividing by tp 

to recover rp and pp, and then subtracting from n to recover rn and pn, and thus all 

cells of the contingency table are determined. technically this does not hold in general, 

specifically in the case where tp=0, and it is thus ill-conditioned as tp   0. moreover, 

even when it is implicitly specified it is reflected only indirectly in the denominator in 

cancellation against n (which also does not appear explicitly). 

it may be thought that we can explore the f-measure both for the positive and 

negative class, reversing the labels.  but this can give very different answers (consider the 

water as noun or verb example versus the search for documents about water). in the end it 

comes back to how to average, and some systems do macro-average f-measure 

inappropriately across multiple classes (it depends on prediction as well as class). 

tradeoffs	
   
f-measure is about finding one number with which to compare systems and find a 

winner. unfortunately this often succeeds in optimizing the wrong thing when there are 

more than one class of interest, because of the issues we have already discussed: f-

measure is specifically seeking a trade off between recall and precision, but there is 

another pair of measures we commonly trade off in roc: recall (tpr) and fallout 

(fpr). there have been claims pr and roc are essentially doing the same tradeoff (see 

fig. 3). in some practical contexts, this can even be true. in particular, if bias = 

prevalence (diagonals in fig. 3) all positive measures considered become equivalent: 

recall=precision=accuracy=f, as do the roc and chance-corrected measures we will 

discuss later. 

one specific claim is that 1   precision acts as a surrogate for fallout (1    inverse 

precision), and hence recall-precision is just as useful a tradeoff to consider as recall-

fallout (roc) since rp and rn are constant. indeed, the relationship is quite clear as 
precision = tp/pp while fallout = fp/rn     pp   tp. this however makes the tradeoff 
relationship quite different, although the shannon information conveyed by precision,  

   log(precision), interestingly gives rise to the same linear form. it also means that the 

areas under the curves are different and the maxima will not correspond. we can also see 

that taking the reciprocal for the harmonic mean of the f-measure is essentially trading 

off bias (pp) against prevalence (rp) linearly. on the other hand, in roc, recall versus 

fallout is trading off tp against fp, normalized by constants (rp and rn) that imply 

differential costs for the positive and negative cases. 

where	
   the	
   f-     measure	
   can	
   be	
   improved	
   on!	
   
we consider again our list of issues: one-class, bias, distribution, metricity, averaging, 

negatives and tradeoffs. 

one-     class	
   
if we are dealing with more than one class, then the answer is simply to calculate rand 

accuracy directly (3) or via macro-averaging of recall (2), as it is complex to macro-

average (4) correctly, and the result would still be rand accuracy. weighting f-measure 

simply by the size of each class (or the number of predictions of each class) enshrines a 

bias when these are different, and means a better result can be achieved by changing the 

bias towards the more prevalent classes (and some learning algorithms do this). 

bias	
   
there are many approaches to dealing with bias.  in statistics these include regression and 

correlation techniques, as well as more ad hoc chance-correction techniques that attempt 

to subtract off the chance component and restore the statistic to the form of a id203. 

f-measure is similarly designed to retain the form of a id203, for a fictitious 

distribution. the tradeoff technique of receiver operating characteristics (roc) also 

gives rise to a method of controling for bias, and indeed there are strong relationships 

between all of these techniques, and we introduce them briefly now and in detail below. 

kappa: this is the ad hoc approach that subtracts off a chance estimate and then 

renormalizes to a [0,1] range by dividing by the expected error, that is the room for 

improvement over chance, as shown in equation (4).  it was originally designed to 

compare human raters, but has recently been applied in machine learning to rate 

classifiers (higher kappa = better classifier), and in classifier fusion as a measure of 

diversity (higher kappa = less diversity).  the use as a measure of diversity is closer to 

the original use for comparing human raters.   

distribution	
   

there are quite a few different versions of kappa7 (5), the most common being 
cohen kappa, which is the one people in machine learning tend to know and use. but 

fleiss kappa is the one that corresponds most closely with f-measure in its distributional 

assumptions, and neither reflects a id203 in or relative to a well defined distribution. 

kappax = [accx     expaccx] / experrx 

(5) 

cohen kappa (kappac) assumes that the two marginal distributions are 

independent in estimating the expected values of the contingency table due to chance     it 

multiples the marginal probabilities, the prevalences and biases in our context, on the 

assumption that they are independent distributions. fleiss kappa (kappaf) makes the 
same assumption as f-measure and assumes the margins actually derive from the same 

distribution. like f-measure, instead of using the actual bias and prevalence (and inverse 

bias and inverse prevalence) fleiss replaces them with their arithmetic means before 

performing the same calculation as cohen kappa. 

in these calculations acc and expacc (4) are the accuracy a (3) and the expected 

value by adding the expected versions of tp and tn, etp and etn, while experr = 1   a is the 

sum of the expected version of fp and fn, namely efp and efn.  for cohen kappa we have 

etp = pp*rp and fp = pp*rn. for fleiss kappa etp = epp*erp where epp = erp = (pp+rp)/2. 

these can be applied not just to the two class situation we have focussed on here, but to 

multiple classes, and indeed to multiple raters or classifiers. 

informedness and markedness8 are principled kappa forms derived from recall 

and precision and their respective inverses, but have been derived in many different 

ways, being also closely related to gini and roc.  

informedness = r + invr     1 

markedness = p + invp     1 

(6) 

(7) 

informedness corresponding to your edge when betting on races or speculating in 

the stock market, the method of marking multiple choice exams that leads to an expected 

mark of zero for guessing, the distance of the contingency table in roc space above the 

chance line, or the gini function of the area under the curve (auc) subtended by that 
single point in roc space9. under the guise of youden   s j or deltap   , informedness 
represents a regression coefficient, and markedness or deltap represents one for the 

opposite direction of prediction.  

informedness, kappai based on recall, is the id203 of an informed decision. 

markedness, kappam based on precision, is the id203 of a decision variable being 
marked by the real class. the g-measure associated with these derivatives of recall and 

precision, consistent with an information theoretic instantiation of the original f function 
of van rijsbergen, is matthews correlation7. 

metricity	
   
of these measures, the ones that have straightforward inversion from similarity measures 

(1 is best) to metric distances (0 is best) are correlation (equivalent to cosine measure), 

informedness and markedness (both corresponding to id75 coefficients and 

interpretable as cosines). 

averaging	
   

informedness is correctly averaged weighted by bias (since each component is 

the informedness of a prediction) and markedness is correctly averaged weighted by 

prevalence (since it is the markedness of a class). as with f-measure and g-measure, 

correlation is not meaningfully macroaveraged, and if required should be calculated from 

the multiclass informedness and markedness. if a single number is required against a 

gold standard, multiclass informedness is it. if you are interested in information flow in 

the other direction, then markedness gives you that, and for an unsupervised comparison 

with the same number of classes, correlation is recommended.  but the unsupervised 
case gets more complicated as the number of classes not constrained to match10. 

negatives	
   
for the dichotomous case of positive-negative, the same result is achieved whichever 

class is designated positive, unlike recall, precision and f-measure. informedness tells 

you the id203 that you have made an informed decision, as opposed to guessing     

the ability to bias based on prevalence that we exploited with f-measure, for the water 

example    it   s been eliminated!  

tradeoffs	
   

in regard to the relationship between precision-recall (pr) and receiver 

operating characteristics (roc) curves, the same constraint that tp, recall and 

precision are nonzero is necessary to show relationships (this means that thresholds or 

other parameters must be constrained to avoid this case or these points eliminated from 

the curve, although the zero points are traditionally shown). this has been studied 
comprehensively by davis and goadrich11 who show that the roc curve and pr curve 
for a given algorithm contain the same points, that one roc curve dominates another in 

roc space if and only if the corresponding pr curves display the same dominance. 

furthermore there is an analog in pr space of the well known convex hull of roc space, 

which we call an achievable pr curve. the similar linear relationships expressed by 

precision information and fallout mean that the points in the curve correspond in a deep 

way, with the result that the operating points interpolated and omitted in a roc convex 

hull correspond to the achievable and omitted points on the corresponding smoothing of 

the pr curve, although the interpolation is no longer linear in pr space (see fig. 3).  

constant f-curves are indeed curves in pr space, but isocost curves in roc space 

are linear and parallel, with the default cost equating the value of the full set of true 

positives and the full set of real negatives.  

	
   would	
   the	
   f-     measure	
   ever	
   be	
   the	
   best	
   measure?	
   
no! there is always something better, but sometimes the error in using f-measure is 

small, and at times it can even vanish     just don   t depend on this! 

under the constraint that bias = prevalence, or equivalently at the break-even 

point where we constrain recall = precision, the question of which measure to use 

becomes academic: p = r = f, and all the kappa, regression and correlation variants 

also coincide.  this constraint is thus a useful heuristic and corresponds to the default 

assumption that getting all the positives right is of equal value to getting all the negatives 

right, so you need to work equally hard on each class. any other bias upsets this, and as 

extreme cases, bias = 0 will get all the negatives right, but none of the positives (f=0), 

while bias = 1 will get all the positives right, but none of the negatives 

(precision=f=prevalence, recall=bias=1), and the inverse prevalence, iprev = 1   prev, is 

thus the room for improvement or expected error for kappai. for optimal performance we 
need to get both right, and informedness and markedness expressed in kappa form 

capture this id172 for recall and a corresponding one for precision:  

 

ki = informedness = [r   bias] / iprev 
km = markedness = [p   prev] / ibias 

(8) 

(9) 

for supervised learning or other systems where there is a    gold standard   , in the 

absence of further information about the cost or id203 distribution of cases, 
informedness8 is the appropriate measure to use, and as it is the height above the chance 
line in roc (fig. 3), receiver operating characteristics is the appropriate graphical 

representation to use for assessing tradeoff and resilience to changes in the prevalence 
conditions9. moreover, roc does also have the flexibility to explicitly manage the cost 
tradeoff just as the general form of f-measure aims to do with its    tradeoff parameter. 

for unsupervised contexts or where each side has equal status as opposed to one 

being a gold standard (which are usually rather tarnished), matthews correlation, the 

geometric mean of informedness and markedness, is in general an appropriate measure, 

providing the number of classes match. to the extent that bias tracks prevalence, 

correlation = informedness = markedness is the id203 of information flow in each 

direction.  to the extent that bias is independent of prevalence, the coefficient of 

determination, correlation   is the joint id203 of informed determination in both 

directions. 

if unsupervised techniques such as id91 do not satisfy the constraint that the 

number of categories on one side equals the number of classes on the other, then some 

heuristic, e.g. a greedy approach to equate classes, can be applied to allow the use of 
informedness or correlation8. for this case a variety of modifed or alternate techniques 
are available.10 

roc 

prf 

 

pra 

prg 

     

 

 

figure 3. comparison of receiver operating characteristics (roc) equal informedness isobars with 
precision-recall (pr) with arithmetic (a), harmonic (f) and geometric (g) mean isobars using multi-
classifier fusion for facial expression recognition on the cohn-kanade image set12. true [positive] 
rate results are shown for anger, disgust, fear, happiness, sadness and surprise based on 10-fold 
cross validation with the key showing number of images per real class. blue isobars are equal cost 
(default recall=fallout+constant in roc) or equal score (x-am(recall,fallout)=constant in prx).  
red break-even lines also shown in the prx curves, corresponding to bias=prevalence and 
informedness=kappa=correlation and precision=recall=accuracy and is an appropriate constraint 
when variance (noise) is similar for both positives and negatives near the decision boundary).  
the appropriate linear, logarithmic or reciprocal scalings are used to permit isobars to be linear: 
x-axis is fallout (fremo) for roc and precision (premo) for pra (linear scale) and prg (log scale); 
and y-axis is reciprocal of recall (tremo) and x-axis reciprocal of precision (premo) for prf. 
note that we 6 classes, so 1/6 or 16.7% is chance recall and precision, without distributional data. 
therefore for prf, the axes representing reciprocal of recall and precision truncate at 2x this level, 
and k=6f represents that we are doing kx better relative to this naive 1/6 baseline chance level. 
the average angle of transition from predicting positives to predicting negatives at the threshold 
approximates that of the default weighting, for all of the measures, across all six of the curves. 
the roc curve is smoother and thus tuning for costs seems more appropriate than for any prx. 
the sharper the elbow, the less tuning the    for f or the costs for roc will affect the optimum.  

  

                                                                    

references	
   (seminal	
   and	
   culminal)	
   
1 bs iso 5725-1: "accuracy (trueness and precision) of measurement methods and reults - part 1: general 
principles and definitions." (1994) 
2 c.j. van rijsbergen (1979) information retrieval, 2nd ed., butterworths, or the original research paper  
c.j. van rijsbergen (1974), 'foundations of evaluation', journal of documentation, 30, 365-373.  
3 l.r. dice (1945). measures of the amount of ecologic association between species. ecology 26 (3): 297   
302. 
4 p. jaccard (1901), "  tude comparative de la distribution florale dans une portion des alpes et des jura", 
bulletin de la soci  t   vaudoise des sciences naturelles 37: 547   579. or in english (1912), "the 
distribution of the flora in the alpine zone", new phytologist 11: 37   50. 
5 message understanding conference (muc), http://www-nlpir.nist.gov/related_projects/muc/ and text 
retrieval conference (trec) http://trec.nist.gov  retrieved mon 9 june 2014 
6 j. entwisle, d.m.w. powers (1998), the present use of statistics in the evaluation of nlp parsers, joint 
conferences on new methods in language processing and computational natural language learning, 
pp. 215-214. 
7 d.m.w. powers (2012), the problem with kappa, conference of the european chapter of the association 
for computational linguistics, pp.345-355 and the related paper focussed on area under the curve 
d.m.w. powers (2012), international conference on information science and technology, pp. 567-573. 
8 d.m.w. powers (2003), recall & precision versus the bookmaker, international conference on cognitive 
science, pp.529-534, or in expanded form as flinders university technical report sie-07-001 or full paper  
d.m.w. powers (2011), evaluation: from precision, recall and f-measure to roc, informedness, 
markedness & correlation, journal of machine learning technologies 2(1):37-63. 
9 n. lavrac, p.a. flach, and b. zupan (1999). rule evaluation measures: a unifying view.  
international workshop on inductive logic programming, pp. 174   185, or in a more relevant context,  
p.a. flach (2003). the geometry of roc space: understanding machine learning metrics through roc 
isometrics international conference on machine learning, pp. 226-233. 
10 d. pfitzner, r.e. leibbrandt and d.m.w. powers (2009), characterization and evaluation of similarity 
measures for pairs of id91s, knowledge and information systems 19(3):361-394 
11 j. davis and m. goadric (2006) the relationship between precision-recall and roc curves, icml. 
12 x.b. jia, y.h. zhang, d.m.w. powers and h.b. ali (2014), multi-classifier fusion based facial expression 
recognition approach, ksii transactions on internet & information systems 8(1):196-212. a b 

