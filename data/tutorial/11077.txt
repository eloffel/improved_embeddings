inferring missing entity type instances for knowledge base completion:

new dataset and methods

arvind neelakantan   

department of computer science

university of massachusetts, amherst

amherst, ma, 01003

ming-wei chang
microsoft research
1 microsoft way

redmond, wa 98052, usa

5
1
0
2

 
r
p
a
4
2

 

 
 
]
l
c
.
s
c
[
 
 

1
v
8
5
6
6
0

.

4
0
5
1
:
v
i
x
r
a

arvind@cs.umass.edu

minchang@microsoft.com

abstract

most of previous work in knowledge base
(kb) completion has focused on the problem
of id36. in this work, we focus
on the task of inferring missing entity type in-
stances in a kb, a fundamental task for kb
competition yet receives little attention.
due to the novelty of this task, we construct
a large-scale dataset and design an automatic
evaluation methodology. our knowledge base
completion method uses information within
the existing kb and external information from
wikipedia. we show that individual methods
trained with a global objective that consid-
ers unobserved cells from both the entity and
the type side gives consistently higher qual-
ity predictions compared to baseline methods.
we also perform manual evaluation on a small
subset of the data to verify the effectiveness
of our knowledge base completion methods
and the correctness of our proposed automatic
evaluation method.

1

introduction

there is now increasing interest in the construction
of knowledge bases like freebase (bollacker et al.,
2008) and nell (carlson et al., 2010) in the nat-
ural language processing community. kbs contain
facts such as tiger woods is an athlete, and barack
obama is the president of usa. however, one of the
main drawbacks in existing kbs is that they are in-
complete and are missing important facts (west et
    most of the research conducted during summer internship

at microsoft.

al., 2014), jeopardizing their usefulness in down-
stream tasks such as id53. this has
led to the task of completing the knowledge base
entries, or knowledge base completion (kbc) ex-
tremely important.

in this paper, we address an important subprob-
lem of knowledge base completion    inferring miss-
ing entity type instances. most of previous work
in kb completion has only focused on the problem
of id36 (mintz et al., 2009; nickel et
al., 2011; bordes et al., 2013; riedel et al., 2013).
entity type information is crucial in kbs and is
widely used in many nlp tasks such as relation
extraction (chang et al., 2014), coreference reso-
lution (ratinov and roth, 2012; hajishirzi et al.,
2013), entity linking (fang and chang, 2014), se-
mantic parsing (kwiatkowski et al., 2013; berant
et al., 2013) and id53 (bordes et al.,
2014; yao and durme, 2014). for example, adding
entity type information improves id36
by 3% (chang et al., 2014) and entity linking by
4.2 f1 points (guo et al., 2013). despite their im-
portance, there is surprisingly little previous work
on this problem and, there are no datasets publicly
available for evaluation.

we construct a large-scale dataset for the task of
inferring missing entity type instances in a kb. most
of previous kbc datasets (mintz et al., 2009; riedel
et al., 2013) are constructed using a single snapshot
of the kb and methods are evaluated on a subset
of facts that are hidden during training. hence, the
methods could be potentially evaluated by their abil-
ity to predict easy facts that the kb already contains.
moreover, the methods are not directly evaluated

figure 1: freebase description of jean metellus can be used to infer that the entity has the type /book/author. this
missing fact is found by our algorithm and is still missing in the latest version of freebase when the paper is written.

on their ability to predict missing facts. to over-
come these drawbacks we construct the train and
test data using two snapshots of the kb and evaluate
the methods on predicting facts that are added to the
more recent snapshot, enabling a more realistic and
challenging evaluation.

standard id74 for kbc methods are
generally type-based (mintz et al., 2009; riedel et
al., 2013), measuring the quality of the predictions
by aggregating scores computed within a type. this
is not ideal because: (1) it treats every entity type
equally not considering the distribution of types, (2)
it does not measure the ability of the methods to rank
predictions across types. therefore, we additionally
use a global evaluation metric, where the quality of
predictions is measured within and across types, and
also accounts for the high variance in type distri-
bution.
in our experiments, we show that models
trained with negative examples from the entity side
perform better on type-based metrics, while when
trained with negative examples from the type side
perform better on the global metric.

in order to design methods that can rank pre-
dictions both within and across entity (or relation)
types, we propose a global objective to train the
models. our proposed method combines the ad-
vantages of previous approaches by using nega-
tive examples from both the entity and the type
side. when considering the same number of nega-
tive examples, we    nd that the linear classi   ers and
the low-dimensional embedding models trained with
the global objective produce better quality ranking
within and across entity types when compared to
training with negatives examples only from entity or
type side. additionally compared to prior methods,
the model trained on the proposed global objective
can more reliably suggest con   dent entity-type pair
candidates that could be added into the given knowl-
edge base.

our contributions are summarized as follows:

    we develop an evaluation framework com-
prising of methods for dataset construction
to evaluate kbc
and id74
approaches
in-
stances. the dataset and evaluation scripts are
publicly
http://research.
microsoft.com/en-us/downloads/
df481862-65cc-4b05-886c-acc181ad07bb/
default.aspx.

for missing entity type

available

at

    we propose a global training objective for kbc
methods. the experimental results show that
both linear classi   ers and low-dimensional em-
bedding models achieve best overall perfor-
mance when trained with the global objective
function.

    we conduct extensive studies on models for in-
ferring missing type instances studying the im-
pact of using various features and models.

2

inferring entity types

we consider a kb    containing entity type informa-
tion of the form (e, t), where e     e (e is the set of
all entities) is an entity in the kb with type t     t (t
is the set of all types). for example, e could be tiger
woods and t could be sports athlete. as a single
entity can have multiple types, entities in freebase
often miss some of their types. the aim of this work
is to infer missing entity type instances in the kb.
given an unobserved fact (an entity-type pair) in the
training data (e, t) (cid:54)       where entity e     e and type
t     t , the task is to infer whether the kb currently
misses the fact, i.e., infer whether (e, t)       . we
consider entities in the intersection of freebase and
wikipedia in our experiments.

information resources

2.1
now, we describe the information sources used to
construct the feature representation of an entity to

infer its types. we use information in freebase and
external information from wikipedia to complete
the kb.

    entity type features: the entity types ob-
served in the training data can be a useful sig-
nal to infer missing entity type instances. for
example, in our snapshot of freebase, it is not
uncommon to    nd an entity with the type /peo-
ple/deceased person but missing the type /peo-
ple/person.

    freebase description: almost all entities in
freebase have a short one paragraph descrip-
tion of the entity. figure 1 shows the freebase
description of jean metellus that can be used
to infer the type /book/author which freebase
does not contain as the date of writing this arti-
cle.

    wikipedia: as external information, we in-
clude the wikipedia full text article of an en-
tity in its feature representation. we con-
sider entities in freebase that have a link to
their wikipedia article. the wikipedia full text
of an entity gives several clues to predict it   s
entity types. for example, figure 2 shows
a section of the wikipedia article of claire
martin which gives clues to infer the type
/award/award winner that freebase misses.

3 evaluation framework
in this section, we propose an evaluation methodol-
ogy for the task of inferring missing entity type in-
stances in a kb. while we focus on recovering entity
types, the proposed framework can be easily adapted
to id36 as well.

first, we discuss our two-snapshot dataset con-
struction strategy. then we motivate the importance
of evaluating kbc algorithms globally and describe
the id74 we employ.

3.1 two snapshots construction
in most previous work on kb completion to pre-
dict missing relation facts (mintz et al., 2009; riedel
et al., 2013), the methods are evaluated on a subset
of facts from a single kb snapshot, that are hidden
while training. however, given that the missing en-
tries are usually selected randomly, the distribution

of the selected unknown entries could be very differ-
ent from the actual missing facts distribution. also,
since any fact could be potentially used for evalua-
tion, the methods could be evaluated on their ability
to predict easy facts that are already present in the
kb.

to overcome this drawback, we construct our
train and test set by considering two snapshots of the
knowledge base. the train snapshot is taken from
an earlier time without special treatment. the test
snapshot is taken from a later period, and a kbc
algorithm is evaluated by its ability of recovering
newly added knowledge in the test snapshot. this
enables the methods to be directly evaluated on facts
that are missing in a kb snapshot. note that the
facts that are added to the test snapshot, in general,
are more subtle than the facts that they already con-
tain and predicting the newly added facts could be
harder. hence, our approach enables a more realis-
tic and challenging evaluation setting than previous
work.

we use manually constructed freebase as the kb
in our experiments. notably, chang et al. (2014) use
a two-snapshot strategy for constructing a dataset for
id36 using automatically constructed
nell as their kb. the new facts that are added to
a kb by an automatic method may not have all the
characteristics that make the two snapshot strategy
more advantageous.

we construct our train snapshot   0 by taking the
freebase snapshot on 3rd september, 2013 and con-
sider entities that have a link to their wikipedia page.
kbc algorithms are evaluated by their ability to pre-
dict facts that were added to the 1st june, 2014 snap-
shot of freebase   . to get negative data, we make
a closed world assumption treating any unobserved
instance in freebase as a negative example. un-
observed instances in the freebase snapshot on 3rd
september, 2013 and 1st june, 2014 are used as neg-
ative examples in training and testing respectively.1
the positive instances in the test data (       0) are
facts that are newly added to the test snapshot   . us-
ing the entire set of negative examples in the test data
is impractical due to the large number of negative ex-
amples. to avoid this we only add the negative types

1note that some of the negative instances used in training
could be positive instances in test but we do not remove them
during training.

figure 2: a section of the wikipedia article of claire martin which gives clues that entity has the type
/award/award winner. this currently missing fact is also found by our algorithm.

of entities that have at least one new fact in the test
data. additionally, we add a portion of the negative
examples for entities which do not have new fact in
the test data and that were unused during training.
this makes our dataset quite challenging since the
number of negative instances is much larger than the
number of positive instances in the test data.

it

is important

to note that

the goal of this
work is not to predict facts that emerged between
the time period of the train and test snapshot2.
for example, we do not aim to predict the type
/award/award winner for an entity that won an
award after 3rd september, 2013. hence, we use
the freebase description in the training data snap-
shot and wikipedia snapshot on 3rd september, 2013
to get the features for entities.

one might worry that the new snapshot might
contain a signi   cant amount of emerging facts so
it could not be an effective way to evaluate the
kbc algorithms. therefore, we examine the differ-
ence between the training snapshot and test snap-
shot manually and found that
this is likely not
the case. for example, we randomly selected 25
/award/award winner instances that were added to
the test snapshot and found that all of them had won
at least one award before 3rd september, 2013.

note that while this automatic evaluation is closer
to the real-world scenario, it is still not perfect as the
new kb snapshot is still incomplete. therefore, we
also perform human evaluation on a small dataset to
verify the effectiveness of our approach.

2in this work, we also do not aim to correct existing false

positive errors in freebase

3.2 global evaluation metric
mean average precision (map) (manning et al.,
2008) is now commonly used to evaluate kb com-
pletion methods (mintz et al., 2009; riedel et al.,
2013). map is de   ned as the mean of average pre-
cision over all entity (or relation) types. map treats
each entity type equally (not explicitly accounting
for their distribution). however, some types occur
much more frequently than others. for example,
in our large-scale experiment with 500 entity types,
there are many entity types with only 5 instances in
the test set while the most frequent entity type has
tens of thousands of missing instances. moreover,
map only measures the ability of the methods to
correctly rank predictions within a type.

to account for the high variance in the distribu-
tion of entity types and measure the ability of the
methods to correctly rank predictions across types
we use global average precision (gap) (similarly
to micro-f1) as an additional evaluation metric for
kb completion. we convert the multi-label classi-
   cation problem to a binary classi   cation problem
where the label of an entity and type pair is true if
the entity has that type in freebase and false oth-
erwise. gap is the average precision of this trans-
formed problem which can measure the ability of the
methods to rank predictions both within and across
entity types.

prior to us, bordes et al. (2013) use mean recip-
rocal rank as a global evaluation metric for a kbc
task. we use average precision instead of mean re-
ciprocal rank since mrr could be biased to the top
predictions of the method (west et al., 2014)

while gap captures global ordering, it would be

bene   cial to measure the quality of the top k pre-
dictions of the model for id64 and active
learning scenarios (lewis and gale, 1994; cucerzan
and yarowsky, 1999). we report g@k, gap mea-
sured on the top k predictions (similarly to preci-
sion@k and hits@k). this metric can be reliably
used to measure the overall quality of the top k pre-
dictions.

4 global objective for knowledge base

completion

we describe our approach for predicting missing en-
tity types in a kb in this section. while we focus
on recovering entity types in this paper, the meth-
ods we develop can be easily extended to other kb
completion tasks.

4.1 global objective framework
during training, only positive examples are ob-
served in kb completion tasks. similar to previous
work (mintz et al., 2009; bordes et al., 2013; riedel
et al., 2013), we get negative training examples by
treating the unobserved data in the kb as negative
examples. because the number of unobserved ex-
amples is much larger than the number of facts in
the kb, we follow previous methods and sample few
unobserved negative examples for every positive ex-
ample.

previous methods largely neglect the sampling
methods on unobserved negative examples. the pro-
posed global object framework allows us to system-
atically study the effect of the different sampling
methods to get negative data, as the performance of
the model for different id74 does de-
pend on the sampling method.

we consider a training snapshot of the kb   0,
containing facts of the form (e, t) where e is an en-
tity in the kb with type t. given a fact (e, t) in
the kb, we consider two types of negative examples
constructed from the following two sets: ne(e, t) is
the    negative entity set   , and nt (e, t) is the    nega-
tive type set   . more precisely,

ne(e, t)     {e(cid:48)|e(cid:48)     e, e(cid:48) (cid:54)= e, (e(cid:48), t) /      0},

and

(cid:88)
(cid:88)

le(  0,   ) =

and

lt (  0,   ) =

nt (e, t)     {t(cid:48)|t(cid:48)     t, t(cid:48) (cid:54)= t, (e, t(cid:48)) /      0}.
let    be the model parameters, m = |ne(e, t)|
and n = |nt (e, t)| be the number of negative exam-
ples and types considered for training respectively.
for each entity-type pair (e, t), we de   ne the scor-
ing function of our model as s(e, t|  ).3 we de   ne
two id168s one using negative entities and
the other using negative types:

[s(e(cid:48), t)     s(e, t) + 1]k
+,

(e,t)     0,e(cid:48)   ne (e,t)

[s(e, t(cid:48))     s(e, t) + 1]k
+,

(e,t)     0,t(cid:48)   nt (e,t)

where k is the power of the id168 (k can be 1
or 2), and the function [  ]+ is the hinge function.

the global objective function is de   ned as

reg(  ) + clt (  0,   ) + cle(  0,   ),

(1)

min

  

where reg(  ) is the id173 term of the
model, and c is the id173 parameter.
in-
tuitively, the parameters    are estimated to rank the
observed facts above the negative examples with a
margin. the total number of negative examples is
controlled by the size of the sets ne and nt . we
experiment by sampling only entities or only types
or both by    xing the total number of negative exam-
ples in section 5.

the rest of section is organized as follows: we
propose three algorithms based on the global objec-
tive in section 4.2. in section 4.3, we discuss the re-
lationship between the proposed algorithms and ex-
isting approaches. let   (e)     rde be the feature
function that maps an entity to its feature represen-
tation, and   (t)     rdt be the feature function that
maps an entity type to its feature representation.4 de
and dt represent the feature dimensionality of the en-
tity features and the type features respectively. fea-
ture representations of the entity types (  ) is only
used in the embedding model.

3we often use s(e, t) as an abbreviation of s(e, t|  ) in order

to save space.

4this gives the possibility of de   ning features for the labels
in the output space but we use a simple one-hot representation
for types right now since richer features did not give perfor-
mance gains in our initial experiments.

if wt

for e(cid:48)     ne(e, t) do
t   (e)     wt
t   (e(cid:48))     1 < 0 then
adagradupdate(wt,   (e(cid:48))       (e))

algorithm 1 the training algorithm for lin-
ear.adagrad.
1: initialize wt = 0,   t = 1 . . .|t|
2: for (e, t)       0 do
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14: end for

end for
for t(cid:48)     nt (e, t) do
t   (e)     wt
adagradupdate(wt,     (e))
adagradupdate(wt(cid:48) ,   (e)).

t(cid:48)   (e)     1 < 0 then

end for

end if

end if

if wt

4.2 algorithms
we propose three different algorithms based on the
global objective framework for predicting missing
entity types. two algorithms use the linear model
and the other one uses the embedding model.

t=1 wt

follows: r(  ) = 1/2(cid:80)

linear model the scoring function in this model
is given by s(e, t|   = {wt}) = wt
t   (e), where
wt     rde is the parameter vector for target type
t. the id173 term in eq. (1) is de   ned as
t wt. we use k = 2 in
our experiments. our    rst algorithm is obtained by
using the dual coordinate descent algorithm (hsieh
et al., 2008) to optimize eq. (1), where we modi-
   ed the original algorithm to handle multiple weight
vectors. we refer to this algorithm as linear.dcd.
while dcd algorithm ensures convergence to the
global optimum solution, its convergence can be
slow in certain cases. therefore, we adopt an on-
line algorithm, adagrad (duchi et al., 2011). we
use the hinge id168 (k = 1) with no regu-
larization (reg(  ) =    ) since it gave best results
in our initial experiments. we refer to this algo-
rithm as linear.adagrad, which is described in al-
gorithm 1. note that adagradupdate(x, g) is a pro-
cedure which updates the vector x with the respect
to the gradient g.

embedding model
in this model, vector repre-
sentations are constructed for entities and types us-
ing linear projection matrices. recall   (t)     rdt
is the feature function that maps a type to its feature
representation. the scoring function is given by

if s(e, t)     s(e(cid:48), t)     1 < 0 then

adagradupdate(ui,   [i](  (e(cid:48))       (e)))
adagradupdate(vi,   [i]  (t))

algorithm 2 the training algorithm for the embed-
ding model.
1: initialize v, u randomly.
2: for (e, t)       0 do
for e(cid:48)     ne(e, t) do
3:
4:
       vt   (t)
5:
       ut (  (e(cid:48))       (e))
6:
for i     1 . . . d do
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23: end for

       vt (  (t(cid:48))       (t))
       ut   (e)
for i     1 . . . d do

adagradupdate(ui,   [i]  (e))
adagradupdate(vi,   [i](  (t(cid:48))       (t)))

if s(e, t)     s(e, t(cid:48))     1 < 0 then

end for
for t(cid:48)     nt (e, t) do

end for

end for

end for

end if

end if

s(e, t|   = (u, v)) =   (t)t vut   (e),

where u     rde  d and v     rdt  d are projection
matrices that embed the entities and types in a d-
dimensional space. similarly to the linear classi   er
model, we use the l1-hinge id168 (k = 1)
with no id173 (reg(  ) =    ). ui and vi
denote the i-th column vector of the matrix u and
v, respectively. the algorithm is described in detail
in algorithm 2.

the embedding model has more expressive power
than the linear model, but the training unlike in the
linear model, converges only to a local optimum so-
lution since the objective function is non-convex.

4.3 relationship to existing methods
many existing methods for id36 and
entity type prediction can be cast as a special case
under the global objective framework. for exam-
ple, we can consider the work in relation extrac-
tion (mintz et al., 2009; bordes et al., 2013; riedel
et al., 2013) as models trained with nt (e, t) =    .
these models are trained only using negative entities
which we refer to as negative entity (ne) objective.
the entity type prediction model in ling and weld
(2012) is a linear model with ne(e, t) =     which

70 types

500 types

entities

2.2m

training data statistics (  0)

positive example
max #ent for a type
min #ent for a type

4.5m
1.1m
6732

test data statistics (         0)

2.2m

6.2m
1.1m
32

positive examples
negative examples
negative/positive ratio

163k
17.1m
105.22

240k
132m
554.44

table 1: statistics of our dataset.   0 is our training snap-
shot and    is our test snapshot. an example is an entity-
type pair.

we refer to as the negative type (nt) objective. the
embedding model described in weston et al. (2011)
developed for id162 is also a special case
of our model trained with the nt objective.

while the n e or n t objective functions could
be suitable for some classi   cation tasks (weston et
al., 2011), the choice of objective functions for the
kbc tasks has not been well motivated. often the
choice is made neither with theoretical foundation
nor with empirical support. to the best of our knowl-
edge, the global objective function, which includes
both ne(e, t) and nt (e, t), has not been considered
previously by kbc methods.

5 experiments

in this section, we give details about our dataset and
discuss our experimental results. finally, we per-
form manual evaluation on a small subset of the
data.

5.1 data
first, we evaluate our methods on 70 entity types
with the most observed facts in the training data.5
we also perform large-scale evaluation by testing the
methods on 500 types with the most observed facts
in the training data.

table 1 shows statistics of our dataset. the num-
ber of positive examples is much larger in the train-
ing data compared to that in the test data since the
test set contains only facts that were added to the
more recent snapshot. an additional effect of this is

5we removed few entity types that were trivial to predict in

the test data.

that most of the facts in the test data are about en-
tities that are not very well-known or famous. the
high negative to positive examples ratio in the test
data makes this dataset very challenging.

5.2 automatic evaluation results
table 2 shows automatic evaluation results where we
give results on 70 types and 500 types. we compare
different aspects of the system on 70 types empiri-
cally.
adagrad vs dcd we    rst study the linear mod-
els by comparing linear.dcd and linear.adagrad.
table 2a shows that linear.adagrad consistently
performs better for our task.
impact of features we compare the effect of
different features on the    nal performance using
linear.adagrad in table 2b.
types are repre-
sented by boolean features while freebase descrip-
tion and wikipedia full text are represented using tf-
idf weighting. the best map results are obtained by
using all the information (t+d+w) while best gap
results are obtained by using the freebase descrip-
tion and wikipedia article of the entity. note that
the features are simply concatenated when multiple
resources are used. we tried to use idf weighting
on type features and on all features, but they did not
yield improvements.
the importance of global objective table 2c
and 2d compares global training objective with ne
and nt training objective. note that all the three
methods use the same number of negative examples.
more precisely, for each (e, t)       0, |ne(e, t)| +
|nt (e, t)| = m + n = 2. the results show that
the global training objective achieves best scores
on both map and gap for classi   ers and low-
dimensional embedding models. among ne and
nt, ne performs better on the type-based metric
while nt performs better on the global metric.
linear model vs embedding model finally, we
compare the linear classi   er model with the embed-
ding model in table 2e. the linear classi   er model
performs better than the embedding model in both
map and gap.

we perform large-scale evaluation on 500 types
with the description features (as experiments are
expensive) and the results are shown in table 2f.

features

description

description
wikipedia

algorithm
linear.adagrad
linear.dcd
linear.adagrad
linear.dcd

map gap
28.17
29.17
27.76
28.40
31.97
33.28
31.92
31.36

+

(a) adagrad vs. dual coordinate descent (dcd). results are
obtained using linear models trained with global training ob-
jective (m=1, n=1) on 70 types.

features
type (t)
description (d)
wikipedia (w)
d + w
t + d + w

map gap
13.58
12.33
28.17
29.17
30.56
30.81
31.97
33.28
36.13
31.13

(b) feature comparison. results are obtained from using lin-
ear.adagrad with global training objective (m=1, n=1) on 70
types.

features

d + w

t + d + w

objective
ne (m = 2)
nt (n = 2)
global (m = 1, n = 1)
ne (m = 2)
nt (n = 2)
global (m = 1, n = 1)

map gap
33.01
23.97
29.09
31.61
31.97
33.28
21.79
34.56
34.45
31.42
36.13
31.13

features

d + w

t + d + w

objective
ne (m = 2)
nt (n = 2)
global (m = 1, n = 1)
ne (m = 2)
nt (n = 2)
global (m = 1, n = 1)

map gap
30.92
22.38
23.40
25.77
30.13
31.60
19.34
28.70
28.06
25.42
28.71
30.35

(c) global objective vs ne and nt. results are obtained us-
ing linear.adagrad on 70 types.

(d) global objective vs ne and nt. results are obtained us-
ing the embedding model on 70 types.

features

model
linear.adagrad
embedding

d + w
t + d + w linear.adagrad

embedding

map gap
31.97
33.28
31.60
30.13
36.13
31.13
30.35
28.71

g@1000 g@10000
79.63
73.40
70.02
62.61

68.08
64.69
65.09
64.30

(e) model comparison. the models were trained with the global training objective (m=1, n=1) on 70 types.

model
linear.adagrad
embedding

map gap
20.49
13.28
9.82
17.67

g@1000 g@10000
69.23
55.31

60.14
51.29

(f) results on 500 types using freebase description features. we train the models with the global training objective (m=1, n=1).

table 2: automatic evaluation results. note that m = |ne(e, t)| and n = |nt (e, t)|.

one might expect that with the increased number of
types, the embedding model would perform better
than the classi   er since they share parameters across
types. however, despite the recent popularity of em-
bedding models in nlp, linear model still performs
better in our task.

5.3 human evaluation

to verify the effectiveness of our kbc algorithms,
and the correctness of our automatic evaluation
method, we perform manual evaluation on the top
100 predictions of the output obtained from two dif-

ferent experimental setting and the results are shown
in table 3. even though the automatic evalua-
tion gives pessimistic results since the test kb is
also incomplete6, the results indicate that the auto-
matic evaluation is correlated with manual evalua-
tion. more excitingly, among the 179 unique in-
stances we manually evaluated, 17 of them are still7
missing in freebase which emphasizes the effective-
ness of our approach.

6this is true even with existing automatic evaluation meth-

ods.

7at submission time.

g@100 g@100-m accuracy-m
features
87.68
d + w
t + d + w 84.91

97.31
91.47

97
88

table 3: manual vs. automatic evaluation of top 100 pre-
dictions on 70 types. predictions are obtained by train-
ing a linear classi   er using adagrad with global training
objective (m=1, n=1). g@100-m and accuracy-m are
computed by manual evaluation.

5.4 error analysis

    effect of training data: we    nd the perfor-
mance of the models on a type is highly de-
pendent on the number of training instances for
that type. for example, the linear classi   er
model when evaluated on 70 types performs
24.86 % better on the most frequent 35 types
compared to the least frequent 35 types. this
indicates id64 or active learning tech-
niques can be pro   tably used to provide more
supervision for the methods. in this case, g@k
would be an useful metric to compare the effec-
tiveness of the different methods.

    shallow linguistic features: we found some
of the false positive predictions are caused by
the use of shallow linguistic features. for ex-
ample, an entity who has acted in a movie and
composes music only for television shows is
wrongly tagged with the type /   lm/composer
since words like    movie   ,    composer    and
   music    occur frequently in the wikipedia arti-
cle of the entity (http://en.wikipedia.
org/wiki/j._j._abrams).

6 related work
entity type prediction and wikipedia features
much of previous work (pantel et al., 2012; ling
and weld, 2012) in entity type prediction has fo-
cused on the task of predicting entity types at the
sentence level. yao et al. (2013) develop a method
based on id105 for entity type predic-
tion in a kb using information within the kb and
new york times articles. however, the method was
still evaluated only at the sentence level. toral and
munoz (2006), kazama and torisawa (2007) use the
   rst line of an entity   s wikipedia article to perform
id39 on three entity types.

knowledge base completion much of precious
work in kb completion has focused on the problem
of id36. majority of the methods infer
missing relation facts using information within the
kb (nickel et al., 2011; lao et al., 2011; socher et
al., 2013; bordes et al., 2013) while methods such
as mintz et al. (2009) use information in text doc-
uments. riedel et al. (2013) use both information
within and outside the kb to complete the kb.

linear embedding model weston et al. (2011) is
one of    rst work that developed a supervised linear
embedding model and applied it to id162.
we apply this model to entity type prediction but
we train using a different objective function which
is more suited for our task.

7 conclusion and future work

we propose an evaluation framework comprising
of methods for dataset construction and evaluation
metrics to evaluate kbc approaches for inferring
missing entity type instances. we veri   ed that our
automatic evaluation is correlated with human eval-
uation, and our dataset and evaluation scripts are
publicly available.8 experimental results show that
models trained with our proposed global training ob-
jective produces higher quality ranking within and
across types when compared to baseline methods.

in future work, we plan to use information from
entity linked documents to improve performance
and also explore active leaning, and other human-
in-the-loop methods to get more training data.

references
[berant et al.2013] jonathan berant, vivek srikumar, pei-
chun chen, abby vander linden, brittany harding,
brad huang, and christopher d. manning.
2013.
id29 on freebase from question-answer
pairs. in empirical methods in natural language pro-
cessing.

[bollacker et al.2008] kurt bollacker, colin evans,
praveen paritosh, tim sturge, and jamie taylor.
a collaboratively created graph
2008.
database for structuring human knowledge.
in

freebase:

8

http://research.

microsoft.com/en-us/downloads/
df481862-65cc-4b05-886c-acc181ad07bb/
default.aspx

proceedings of
conference on management of data.

the acm sigmod international

[bordes et al.2013] antoine bordes, nicolas usunier,
alberto garcia-duran, jason weston, and oksana
yakhnenko. 2013. translating embeddings for mod-
eling multi-relational data. in advances in neural in-
formation processing systems.

[bordes et al.2014] antoine bordes, sumit chopra, and
jason weston. 2014. id53 with sub-
graph embeddings. in empirical methods in natural
language processing.

[carlson et al.2010] andrew carlson, justin betteridge,
bryan kisiel, burr settles, estevam r. hruschka, and
a. 2010. toward an architecture for never-ending lan-
guage learning. in in aaai.

[chang et al.2014] kai-wei chang, wen tau yih, bishan
yang, and christopher meek. 2014. typed tensor de-
composition of knowledge bases for relation extrac-
tion. in proceedings of the 2014 conference on em-
pirical methods in natural language processing.

1999.

cucerzan

[cucerzan and yarowsky1999] silviu

and
david yarowsky.
language indep endent
id39 combining morphological
in oint sigdat confer-
and contextual evidence.
ence on empirical methods in natural language
processing and very large corpora.

[duchi et al.2011] john duchi, elad hazan, and yoram
singer. 2011. adaptive subgradient methods for on-
line learning and stochastic optimization. in journal
of machine learning research.

[fang and chang2014] yuan fang and ming-wei chang.
2014. entity linking on microblogs with spatial and
in transactions of the association
temporal signals.
for computational linguistics.

[guo et al.2013] stephen guo, ming-wei chang, and
emre kiciman.
a
study on end-to-end tweet entity linking. in the north
american chapter of the association for computa-
tional linguistics., june.

2013. to link or not to link?

[hajishirzi et al.2013] hannaneh hajishirzi, leila zilles,
daniel s. weld, and luke zettlemoyer. 2013. joint
coreference resolution and named-entity linking with
in empirical methods in natural
multi-pass sieves.
language processing.

[hsieh et al.2008] cho-jui hsieh, kai-wei chang, chih-
jen lin, s. sathiya keerthi, and s. sundararajan.
2008. a dual coordinate descent method for large-
scale linear id166. in international conference on ma-
chine learning.

[kazama and torisawa2007] jun   ichi kazama and ken-
taro torisawa. 2007. exploiting wikipedia as exter-
nal knowledge for id39. in joint

conference on empirical methods in natural lan-
guage processing and computational natural lan-
guage learning.

[kwiatkowski et al.2013] tom kwiatkowski,

eunsol
2013.
choi, yoav artzi, and luke. zettlemoyer.
scaling semantic parsers with on-the-   y ontology
matching. in empirical methods in natural language
processing.

[lao et al.2011] ni lao, tom mitchell, and william w.
cohen. 2011. random walk id136 and learning in
a large scale knowledge base. in conference on em-
pirical methods in natural language processing.

[lewis and gale1994] david d. lewis and william a.
gale. 1994. a sequential algorithm for training text
in acm sigir conference on research
classi   ers.
and development in information retrieval.

[ling and weld2012] xiao ling and daniel s. weld.
2012. fine-grained entity recognition. in association
for the advancement of arti   cial intelligence.

[manning et al.2008] christopher d. manning, prabhakar
raghavan, and hinrich sch  utze. 2008. introduction to
information retrieval. in cambridge university press,
cambridge, uk.

[mintz et al.2009] mike mintz, steven bills, rion snow,
and dan jurafsky. 2009. distant supervision for re-
lation extraction without labeled data. in association
for computational linguistics and international joint
conference on natural language processing.

[nickel et al.2011] maximilian nickel, volker tresp, and
hans-peter kriegel. 2011. a three-way model for col-
in interna-
lective learning on multi-relational data.
tional conference on machine learning.

[pantel et al.2012] patrick pantel, thomas lin,

and
michael gamon. 2012. mining entity types from
in association
query logs via user intent modeling.
for computational linguistics.

[ratinov and roth2012] lev ratinov and dan roth.
2012. learning-based multi-sieve co-reference reso-
in joint conference on em-
lution with knowledge.
pirical methods in natural language processing and
computational natural language learning.

[riedel et al.2013] sebastian riedel, limin yao, andrew
mccallum, and benjamin m. marlin. 2013. rela-
tion extraction with id105 and universal
schemas. in the north american chapter of the asso-
ciation for computational linguistics.

[socher et al.2013] richard socher, danqi chen, christo-
pher manning, and andrew y. ng. 2013. reasoning
with neural tensor networks for knowledge base com-
pletion. in advances in neural information process-
ing systems.

[toral and munoz2006] antonio

and rafael
2006. a proposal to automatically build

toral

munoz.

and maintain gazetteers for id39
in european chapter of the
by using wikipedia.
association for computational linguistics.

[west et al.2014] robert west, evgeniy gabrilovich,
kevin murphy, shaohua sun, rahul gupta, and
dekang lin. 2014. knowledge base completion via
in proceedings of
search-based id53.
the 23rd international conference on world wide web,
pages 515   526. international world wide web con-
ferences steering committee.

[weston et al.2011] jason weston, samy bengio, and
nicolas usunier. 2011. wsabie: scaling up to large
in international joint
vocabulary image annotation.
conference on arti   cial intelligence.

[yao and durme2014] xuchen yao and benjamin van
durme. 2014. information extraction over structured
data: id53 with freebase. in associa-
tion for computational linguistics.

[yao et al.2013] limin yao, sebastian riedel, and an-
drew mccallum. 2013. universal schema for entity
type prediction. in proceedings of the 2013 workshop
on automated knowledge base construction.

