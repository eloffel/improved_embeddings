using syntax-based machine translation to parse english into abstract

meaning representation

michael pust, ulf hermjakob, kevin knight, daniel marcu, jonathan may

information sciences institute
computer science department
university of southern california

{pust, ulf, knight, marcu, jonmay}@isi.edu

5
1
0
2

 
r
p
a
8
2

 

 
 
]
l
c
.
s
c
[
 
 

2
v
5
6
6
6
0

.

4
0
5
1
:
v
i
x
r
a

abstract

we present a parser for abstract meaning
representation (amr). we treat english-
to-amr conversion within the framework
of string-to-tree, syntax-based machine
translation (sbmt). to make this work,
we transform the amr structure into a
form suitable for the mechanics of sbmt
and useful for modeling. we introduce
an amr-speci   c language model and add
data and features drawn from semantic re-
sources. our resulting amr parser im-
proves upon state-of-the-art results by 7
smatch points.

1

introduction

id15 (amr) is a
compact, readable, whole-sentence semantic an-
notation (banarescu et al., 2013). it includes en-
tity identi   cation and typing, propbank semantic
roles (kingsbury and palmer, 2002), individual en-
tities playing multiple roles, as well as treatments
of modality, negation, etc. amr abstracts in nu-
merous ways, e.g., by assigning the same concep-
tual structure to fear (v), fear (n), and afraid (adj).
figure 1 gives an example of an amr with several
english renderings.

in this paper, we automatically learn how to
parse english into amr, by exploiting a pub-
licly available corpus of more than 10,000 en-
glish/amr pairs.1

the amr parsing problem bears a strong for-
mal resemblance to syntax-based machine transla-
tion (sbmt) of the string-to-tree variety, as shown
in figure 2. because of this, it is appealing to con-
sider whether we can apply the substantial body of
techniques already invented for sbmt.2

1ldc catalog number 2014t12
2see e.g. the related work section of huck et al. (2014).

the soldier was not afraid of dying.
the soldier was not afraid to die.
the soldier did not fear death.

figure 1: an id15
(amr) with several english renderings.

source
target

sbmt
   at string
nested structure

amr parsing
   at string
nested structure

figure 2: similarities between amr parsing and
syntax-based machine translation (sbmt).

however, there are also some major differences,
as shown in figure 3. therefore, applying sbmt
to amr parsing requires novel representations
and techniques, which we develop in this paper.
some of our key ideas include:

1. introducing an amr-equivalent representa-
tion that is suitable for string-to-tree sbmt
rule extraction and decoding.

2. proposing a target-side reordering technique
that takes unique advantage of the fact that
child nodes in amr are unordered.

3. introducing

an amr-speci   c

language

model.

4. developing tuning methods that maximize

smatch (cai and knight, 2013).

5. integrating several

semantic knowledge

sources into the amr parsing task.

these key ideas lead to state-of-the-art amr pars-
ing results. we next describe our baseline sbmt
system, and then we adapt it to amr parsing.

instfear-01die-01arg0arg1polarity-instsoldierinstarg1target
nodes
edges
alignments

children
accuracy
metric

sbmt
tree
labeled
unlabeled
words to leaves

ordered
id7 (papineni
et al., 2002)

amr parsing
graph
unlabeled
labeled
words to leaves
+ words to edges
unordered
smatch (cai and
knight, 2013)

figure 3: differences between amr parsing and
sbmt.

2 syntax-based machine translation

our baseline sbmt system proceeds as follows.
given a corpus of (source string,
tree,
source-target word alignment) sentence translation
training tuples and a corpus of (source, target,
score) sentence translation tuning tuples:

target

1. rule extraction: a grammar of string-to-
tree rules is induced from training tuples us-
ing the ghkm algorithm (galley et al., 2004;
galley et al., 2006).

2. local feature calculation: statistical and in-
dicator features, as described by chiang et al.
(2009), are calculated over the rule grammar.
3. language model calculation: a kneser-
ney-interpolated 5-gram language model
(chen and goodman, 1996) is learned from
the yield of the target training trees.

4. decoding: a beamed bottom-up chart de-
coder calculates the optimal derivations given
a source string and feature parameter set.

5. tuning: feature parameters are optimized
using the mira learning approach (chiang
et al., 2009) to maximize the objective, typi-
cally id7 (papineni et al., 2002), associated
with a tuning corpus.

we initially use this system with no modi   ca-
tions and pretend that english   amr is a language
pair indistinct from any other.

3 data and comparisons

we use english   amr data from the amr 1.0 cor-
pus, ldc catalog number 2014t12. in contrast
to narrow-domain data sources that are often used
in work related to id29 (price, 1990;
zelle, 1995; kuhlmann et al., 2004), the amr cor-
pus covers a broad range of news and web forum
data. we use the training, development, and test
splits speci   ed in the amr corpus (table 1). the
training set is used for rule extraction, language

corpus lines
training
10,313
1,368
development
test
1,371

tokens
218,021
29,484
30,263

table 1: data splits of amr 1.0, used in this work.
tokens are english, after id121.

modeling, and statistical rule feature calculation.
the development set is used both for parameter
optimization and qualitatively for hill-climbing.
the test set is held out blind for evaluation. we
preprocess the english with a simple rule-based
tokenizer and, except where noted, lowercase all
data. we obtain english   amr alignments by us-
ing the unsupervised alignment approach of pour-
damghani et al. (2014). all parsing results re-
ported in this work are obtained with the smatch
1.0 software (cai and knight, 2013). we compare
our results to those of flanigan et al. (2014) on the
amr 1.0 data splits; we run that work   s jamr
software according to the provided instructions.3

4 amr transformations
in this section we discuss various transformations
to our amr data. initially, we concern ourselves
with converting amr into a form that is amenable
to ghkm rule extraction and string to tree decod-
ing. we then turn to structural transformations
designed to improve system performance. fig-
ure 4 progressively shows all the transformations
described in this section; the example we follow is
shown in its original form in figure 4a.

4.1 massaging amrs into syntax-style trees
the relationships in amr form a directed acyclic
graph (dag), but ghkm requires a tree, so
we must begin our transformations by discarding
some information. we arbitrarily disconnect all
but a single parent from each node (see figure 4b).
this is the only lossy modi   cation we make to our
amr data. as multi-parent relationships occur
approximately once per training sentence, this is
indeed a regrettable loss. we nevertheless make
this modi   cation, since it allows us to use the rest
of our string-to-tree tools.

amr also contains labeled edges, unlike the
constituent parse trees we are used to working with
in sbmt. these labeled edges have informative
content and we would like to use the alignment

3https://github.com/jflanigan/jamr

(a) the original amr

(b) disconnecting multiple parents of
(a)

(c) edge labels of (b) pushed to leaves,
preterminals added

(d) restructuring (c) with concept la-
bels as intermediates

(e) restructuring (c) with role labels
as intermediates

(f) string preterminal relabeling of (c)

(g) original alignment of english to
(c)

(h) after reordering of (g)

(i) restructured, relabeled, and re-
ordered tree: (e), (f), and (h)

figure 4: transformation of amr into tree structure that is acceptable to ghkm (galley et al., 2004;
galley et al., 2006) rule extraction and yields good performance.

procedure of pourdamghani et al. (2014), which
aligns words to edges as well as to terminal nodes.
so that our amr trees are compatible with both
our desired alignment approach and our desired
rule extraction approach, we propagate edge labels
to terminals via the following procedure:

1. for each node n in the amr tree we create
a corresponding node m with the all-purpose
symbol    x    in the sbmt-like tree. outgoing
edges from n come in two    avors: concept
edges, labeled    inst   , which connect n to a
terminal concept such as fear-01, and role
edges, which have a variety of labels such as
arg0 and name, and connect n to another
instance or to a string.4 a node has one in-
stance edge and zero or more role edges. we
consider each type of edge separately.

2. for each outgoing role edge we insert two un-
labeled edges into the corresponding trans-
4in figure 4 the negative polarity marker    -    is a string.
disconnected referents labeled    *    are treated as amr in-
stances with no roles.

formation; the    rst is an edge from m to a
terminal bearing the original edge   s role label
(a so-called role label edge), and the second
(a role    ller edge) connects m to the trans-
formation of the original edge   s target node,
which we process recursively. string targets
of a role receive an    x    preterminal to be con-
sistent with the form of role    ller edges.

3. for the outgoing concept edge we insert an
unlabeled edge connecting m and the con-
cept. it is unambiguous to determine which
of m   s edges is the concept edge and which
edges constitute role label edges and their
corresponding role    ller edges, as long as
paired label and    ller edges are adjacent.

4. since sbmt expects trees with preterminals,
we simply replicate the label identities of
concepts and role labels, adding a marker (   p   
in figure 4) to distinguish preterminals.

the complete transformation can be seen in fig-
ure 4c. apart from multiple parent ancestry, the

instfear-01die-01arg0arg1polarity-instsoldierinstarg1instfear-01die-01arg0arg1polarity-instsoldierinstarg1*instxpfear-01parg0parg1xxpsoldierpdie-01parg1fear-01arg0soldierppolaritypolarityx-arg1die-01arg1p**xxpfear-01parg0parg1xxpsoldierpdie-01parg1fear-01arg0soldierppolaritypolarityx-arg1die-01arg1p**fear-01fear-01xxpfear-01parg0parg1xxpsoldierpdie-01parg1fear-01arg0soldierppolaritypolarityx-arg1die-01arg1p**arg1arg0xxpfear-01parg0parg1xxpsoldierpdie-01parg1fear-01arg0soldierppolaritypolarityspolarity-arg1die-01arg1p**xxpfear-01parg0parg1xxpsoldierpdie-01parg1fear-01arg0soldierppolaritypolarityx-arg1die-01arg1p**thesoldierwasnotafraidofdyingxxpfear-01parg0parg1xxpsoldierpdie-01parg1fear-01arg0soldierppolaritypolarityx-arg1die-01arg1p**thesoldierwasnotafraidofdyingxxpfear-01parg0parg1xxpsoldierpdie-01parg1fear-01arg0soldierppolaritypolarityspolarity-arg1die-01arg1p**arg0polarityxoriginal amr can be reconstructed deterministi-
cally from this sbmt-compliant rewrite.

4.2 tree restructuring
while the transformation in figure 4c is accept-
able to ghkm, and hence an entire end-to-end
amr parser may now be built with sbmt tools,
we do not believe the resulting parser will exhibit
very good performance. the trees we are learn-
ing on are exceedingly    at, and thus yield rules
that do not generalize suf   ciently. rules produced
from the top of the tree in figure 4c, such as that
in figure 5a, are only appropriate for cases where
fear-01 has exactly three roles: arg0 (agent),
arg1 (patient), and polarity.

we follow the lead of wang et al. (2010), who
in turn were in   uenced by similar approaches in
monolingual parsing (collins, 1997; charniak,
2000), and re-structure trees at nodes with more
than three children (i.e. instances with more than
one role), to allow generalization of    at structures.
however, our trees are unlike syntactic con-
stituent trees in that they do not have labeled non-
terminal nodes, so we have no natural choice of
an intermediate (   bar   ) label. we must choose a
meaningful label to characterize an instance and
its roles. we initially choose the concept label, re-
sulting in trees like that in figure 4d. however,
this attempt at re-structuring yields rules like that
in figure 5b, which are general in form but are
tied to the concept context in which they were ex-
tracted. this leads to many redundant rules and
blows up the nonterminal vocabulary size to ap-
proximately 8,000, the size of the concept vo-
cabulary. furthermore, the rules elicited by this
procedure encourage undesirable behavior such as
the immediate juxtaposition of two rules generat-
ing arg1. we next consider restructuring with
the immediately dominant role labels, resulting in
trees like that in figure 4e and rules like that in
figure 5c. this approach leads to more useful
rules with fewer undesirable properties.

4.3 tree relabeling
amr strings have an effective preterminal label of
   x,    which allows them to compete with full amr
instances at decode time. however, whether or not
a role is    lled by a string or an instance is highly
dependent on the kind of role being    lled. the
polarity and mode roles, for instance, are nearly
always    lled by strings, but arg0 and arg1 are
always    lled by instances. the quant role, which

is used for representation of numerical quantities,
can be    lled by an instance (e.g. for approximate
quantities such as    about 3   ) or a string. to capture
this behavior we relabel string preterminals of the
tree with labels indicating role identity and string
subsumption. this relabeling, replaces, for exam-
ple, one    x    preterminal in figure 4c with    spo-
larity,    as shown in figure 4f.

4.4 tree reordering
finally, let us consider the alignments between
english amr. as is known in sbmt, non-
monotone alignments can lead to large, unwieldy
rules and in general make decoding more dif   -
cult. while this is often an unavoidable fact of life
when trying to translate between two languages
with different syntactic behavior, it is an entirely
arti   cial phenomenon in english   amr. amr is
an unordered representation, yet in order to use
an sbmt infrastructure we must declare an or-
der of the amr tree. this means we are free to
choose whatever order is most convenient to us, as
long as we keep role label edges immediately adja-
cent to their corresponding role    ller edges to pre-
serve conversion back to the edge-labeled amr
form. we thus choose the order that is as close as
possible to english yet still preserves these con-
straints. we use a simple greedy bottom-up ap-
proach that permutes the children of each internal
node of the unrestructured tree so as to minimize
crossings. this leads to a 79% overall reduction in
crossings and is exempli   ed in figure 4g (before)
and figure 4h (after). we may then restructure our
trees, as described above, in an instance-outward
manner. the    nal restructured, relabeled, and re-
ordered tree is shown in figure 4i.

5 amr language models

we now turn to language models of amrs, which
help us prefer reasonable target structures over un-
reasonable ones.

our    rst

language model

is unintuitively
simple   we pretend there is a language called
amrese that consists of yields of our restruc-
tured amrs. an example amrese string from
is    arg0 soldier polarity -
figure 4i
fear-01 arg1 die-01 arg1 *.    we then
build a standard id165 model for amrese.

it also seems sensible to judge the correctness of
an amr by calculating the empirical id203
of the concepts and their relations to each other.

(a) a rule extracted from the amr tree of
figure 4c. all roles seen in training must be
used.

(b) a rule from the amr tree of
figure 4d. many nearly iden-
tical rules of this type are ex-
tracted, and this rule can be
used multiple times in a single
derivation.

(c) a rule from the amr tree of fig-
ure 4e. this rule can be used indepen-
dent of the concept context it was ex-
tracted from and multiple reuse is dis-
couraged.

figure 5: impact of restructuring on rule extraction

this is the motivation behind the following model
of an amr:5

we de   ne an amr instance i = (c, r), where
c is a concept and r is a set of roles. we de   ne an
amr role r = (l, i), where l is a role label, and i is
an amr instance labeled l. for an amr instance
i let   ci be the concept of i   s parent instance, and
  li be the label of the role that i    lls with respect
to its parent. we also de   ne the special instance
and role labels root and stop. then, we de   ne
pamr(i|  li,   ci), the id155 of amr
instance i given its ancestry as:

pamr(i = (c, r)|  li,   ci) = p (c|  li,   ci)  

prole(r|c)    p (stop|c)

(cid:89)

r   r

where

prole(r = (l, i)|c) = p (l|c)    pamr(i|l, c)

we de   ne p (c|  li,   ci), p (l|c), and p (stop|c)
as empirical conditional probabilities, witten-bell
interpolated (witten and bell, 1991) to lower-
order models by progressively discarding context
from the right.6 we model exactly one stop
event per instance. we de   ne the id203 of
a full-sentence amr i as pamr(i|root) where
root in this case serves as both parent concept
and role label.

5this model is only de   ned over amrs that can be rep-
resented as trees, and not over all amrs. since tree amrs
are a prerequisite of our system we did not yet investigate
whether this model could be suf   ciently generalized.
6that is, p (c|  li,   ci) is interpolated with p (c|  li) and then

p (c).

as an example, the instance associated with
concept die-01 in figure 4b has   li = arg1 and
  ci = fear-01, so we may score it as:

p (die-01|arg1, fear-01)  
p (arg1|die-01)  
p (stop|die-01)  
p (*|arg1, die-01)

we add the amr lm to our system as a feature

alongside the amrese id165 lm.

6 adding external semantic resources

while we are engaged in the task of semantic pars-
ing, we have not yet discussed the use of any se-
mantic resources. in this section we rectify that
omission.

6.1 rules from numerical quantities and

named entities

while the majority of string-to-tree rules in sbmt
systems are extracted from annotated data, it is
common practice to dynamically generate rules
to handle the translation of dates and numerical
quantities, as these follow common patterns and
are easily detected at decode-time. we follow
this practice here, and additionally detect person
names at decode-time using the stanford named
entity recognizer (finkel et al., 2005). we use
cased, tokenized source data to build the decode-
time rules. we add indicator features to these rules
so that our tuning methods can decide how favor-
able the resources are. we leave as future work
the incorporation of named-entity rules for other

xpfear-01parg0parg1x1:xx2:xfear-01arg0ppolaritypolarityx-arg1thex1wasnotafraidx2parg1x2:xarg1fear-01x1:fear-01x1x2parg1x2:xarg1arg1x1:arg0x1x24.1
4.2
4.2
4.3
4.4

system section tune test
   at trees
49.9
55.3
concept restructuring
58.6
role restructuring (rr)
59.7
rr + string preterminal relabeling (rl)
59.7
rr + rl + reordering (ro)
60.6
rr + rl + ro + amr lm 5
61.3
64.3
65.3
65.8
58.2

51.6
57.2
60.8
61.3
61.7
62.3
63.3
66.2
67.1
67.7
58.8

6.1
6.2
6.3
6.3
3

rr + rl + ro + amr lm + date/number/name rules (dn)
rr + rl + ro + amr lm + dn + semantic categories (sc)
rr + rl + ro + amr lm + dn + sc, rule-based alignments
rr + rl + ro + amr lm + dn + sc, rule-based + unsupervised alignments
jamr (flanigan et al., 2014)

table 2: amr parsing smatch scores for the experiments in this work. we provide a cross-reference to
the section of this paper that describes each of the evaluated systems. entries in bold are improvements
over the previous state of the art. human inter-annotator smatch performance is in the 79-83 range (cai
and knight, 2013).

classes, since most available named-entity recog-
nition beyond person names is at a granularity
level that is incompatible with amr (e.g. we can
recognize    location    but not distinguish between
   city    and    country   ).

6.2 hierarchical semantic categories
in order to further generalize our rules, we mod-
ify our training data amrs once more, this time
replacing the identity preterminals over concepts
with preterminals designed to enhance the appli-
cability of our rules in semantically similar con-
texts. for each concept c expressed in amr, we
consult id138 (fellbaum, 1998) and a curated
set of gazetteers and vocabulary lists to identify
a hierarchy of increasingly general semantic cat-
egories that describe the concept. so as not to
be overwhelmed by the many    ne-grained distinc-
tions present in id138, we pre-select around
100 salient semantic categories from the id138
ontology. when traversing the id138 hierarchy,
we propagate a smoothed count7 of the number
of examples seen per concept sense8, combining
counts when paths meet. for each selected se-
mantic category s encountered in the traversal, we
calculate a weight by dividing the propagated ex-
ample count for c at s by the frequency s was pro-
posed over all amr concepts. we then assign c
to the highest scoring semantic category s. an ex-

7we use very simple smoothing, and add 0.1 to the pro-

vided example counts.

8since id138 senses do not correspond directly to
propbank or amr senses, we simply use a lexical match and
must consider all observed senses for that match.

ample calculation for the concept computer is
shown in figure 8.

we apply semantic categories to our data as
replacements for identity preterminals of con-
cepts. this leads to more general, more widely-
applicable rules. for example, with this trans-
formation, we can parse correctly not only con-
texts in which    soldiers die   , but also contexts in
which other kinds of    skilled workers die   . figure
6 shows the addition of semantic preterminals to
the tree from figure 4i. we also incorporate se-
mantic categories into the amr lm. for concept
c, let sc be the semantic category of c. then we
reformulate pamr(i|  li,   ci) as:

pamr(i = (c, r)|  li,   ci) =
(cid:89)
p (sc|  li, s   ci,   ci)    p (c|sc,   li, s   ci,   ci)  

prole(r|c)    p (stop|sc, c)

r   r

where

prole(r = (l, i)|c) = p (l|sc, c)    pamr(i|l, c)

6.3 semantically informed rule-based

alignments

for our    nal incorporation of semantic resources
we revisit the english-to-amr alignments used to
extract rules. as an alternative to the unsupervised
approach of pourdamghani et al. (2014), we build
alignments by taking a linguistically-aware, super-
vised heuristic approach to alignment:

first, we generate a large number of potential
links between english and amr. we attempt

figure 6: final modi   cation of the amr data; se-
mantically clustered preterminal labels are added
to concepts.

to link english and amr tokens after conver-
sion through resources such as a morphological
analyzer, a list of 3,235 pertainym pairs (e.g.
adj-   gubernatorial        noun-   governor   ), a list
of 2,444 adverb/adjective pairs (e.g.
   humbly   
       humble   ), a list of 2,076 negative polarity
pairs (e.g.    illegal           legal   ), and a list of 2,794
known english-amr transformational relation-
   asleep        sleep-01,    advertiser   
ships (e.g.
    person :arg0-of advertise-01,
   greenwich mean time        gmt). these links are
then culled based on context and amr structure.
for example, in the sentence    the big    sh ate
the little    sh,    initially both english       sh    are
aligned to both amr       sh.    however, based on
the context of    big    and    little    the spurious links
are removed.

in our experiments we explore both replacing
the unsupervised alignments of pourdamghani et
al. (2014) with these alignments and concatenat-
ing the two alignment sets together, essentially
doubling the size of the training corpus. because
the different alignments yield different target-side
tree reorderings, it is necessary to build separate
5-gram amrese language models.9 when us-
ing both alignment sets together, we also use both
amrese language models simultaneously.

7 tuning
we would like to tune our feature weights to max-
imize smatch directly. however, a very con-
venient alternative is to compare the amrese
yields of candidate amr parses to those of ref-
erence amrese strings, using a id7 objective
and forest-based mira (chiang et al., 2009). fig-

9the amr lm is insensitive to reordering so we do not

need to vary it when varying alignments.

figure 7: id7 of amrese and smatch correlate
closely when tuning.

ure 7 shows that mira tuning with id7 over
amrese tracks closely with smatch. note that,
for experiments using reordered amr trees, this
requires obtaining similarly permuted reference
tuning amrese and hence requires alignments on
the development corpus. when using unsuper-
vised alignments we may simply run id136 on
the trained alignment model to obtain develop-
ment alignments.10 the rule-based aligner runs
one sentence at a time and can be employed on
the development corpus. when using both sets of
alignments, each approach   s amrese is used as
a development reference (i.e. each development
sentence has two possible reference translations).

8 results

our amr parser   s performance is shown in ta-
ble 2. we progressively show the incremental im-
provements and compare to the current state of the
art system of flanigan et al. (2014). purely trans-
forming amr data into a form that is compatible
with the sbmt pipeline yields suboptimal results,
but by adding role-based restructuring, relabeling,
and reordering, as described in section 4 we are
able to surpass flanigan et al. (2014). adding an
amr lm and semantic resources increases scores
further. rule-based alignments are an improve-
ment upon unsupervised alignments, but concate-
nating the two alignments yields the best results.

10in practice, since the corpus is not too large and the
released implementation by pourdamghani et al. (2014) is
based on the multi-threaded mgiza++ (gao and vogel,
2008), we align twice, once with just the training data,
and once with training and development data. we use the
training-only run for rule extraction and only the develop-
ment part of the training+dev for tuning.

xthinkparg0parg1xxskilled-workerdieparg1fear-01arg0soldierppolaritypolarityspolarity-arg1die-01arg1p**arg0polarityxhandle word-to-node alignment.

earlier work on using machine translation tech-
niques for id29 includes that of pa-
pineni et al. (1997). that work applies the ibm
machine translation models (brown et al., 1993)
to english paired with a non-structural formal lan-
guage of air travel queries.
in a similar vein,
macherey et al. (2001) use ibm models and their
alignment template approach to analyze a rela-
tively large corpus of german train scheduling in-
quiries. the formal language they generate is not
structural and has a vocabulary of less than 30
types; it may thus be seen as an instance of se-
mantic role labeling rather than semantic represen-
tation parsing.

our sbmt system is grounded in the theory of
tree transducers, which have also been applied to
the task of id29 by jones et al. (2011;
2012).

id29 in general and amr pars-
ing speci   cally can be considered a subsumption
of multiple semantic resolution sub-tasks, such
as id39 (nadeau and sekine,
2007), id14 (gildea and juraf-
sky, 2002), id51 (navigli,
2009) and relation    nding (bach and badaskar,
2007).

10 conclusion
by restructuring our amrs we are able to convert
a sophisticated sbmt engine into a baseline se-
mantic parser with little additional effort. by fur-
ther restructuring our data to appropriately model
the behavior we want to capture we are able to
rapidly achieve state-of-the-art results. finally, by
incorporating novel language models and external
semantic resources, we are able to increase quality
even more. this is not the last word on amr pars-
ing, as fortunately, machine translation technology
provides more low-hanging fruit to pursue.

acknowledgments
this work was supported by darpa con-
tracts fa8750-13-2-0045, hr0011-12-c-0014,
and w911nf-14-1-0364.

references
jacob andreas, andreas vlachos, and stephen clark.
2013. id29 as machine translation. in
proceedings of the 51st annual meeting of the as-
sociation for computational linguistics (volume 2:

categories

figure 8: id138 hierarchy for computer.
pre-selected salient id138
are
boxed. smoothed sense counts are propagated
up the hierarchy and re-combined at join points.
scores are calculated by dividing propagated
sense count by count of the category   s prevalence
over the set of amr concepts. the double box
indicates the selection of artefact as the category
label for computer.

9 related work

the only prior amr parsing work we are aware
of is that of flanigan et al. (2014). in that work,
multiple discriminatively trained models are used
to identify individual concept instances and then
a minimum spanning tree algorithm connects the
concepts.

several other recent works have used a machine
translation approach to id29, but all
have been applied to domain data that is much nar-
rower and an order of magnitude smaller than that
of amr, primarily the geoquery corpus (zelle
and mooney, 1996). the wasp system of wong
and mooney (2006) uses hierarchical smt tech-
niques and does not apply semantic-speci   c im-
provements. andreas et al. (2013) use phrase-
based and hierarchical smt techniques on geo-
query. like this work, they perform a transforma-
tion of the input semantic representation so that it
is amenable to use in an existing machine trans-
lation system. however, they are unable to reach
the state of the art in performance. li et al. (2013)
directly address ghkm   s word-to-terminal align-
ment requirement by extending that algorithm to

computercomputing devicemachineartefact = 6.1/1143 unitestimatorperson = 0.1/814causal-agency = 0.1/930physical-object = 6.2/2218entity = 6.2/43816.10.10.10.10.16.16.26.16.16.1short papers), pages 47   52, so   a, bulgaria, au-
gust. association for computational linguistics.

nguyen bach and sameer badaskar. 2007. a review
unpublished. http:

of id36.
//www.cs.cmu.edu/  nbach/papers/
a-survey-on-relation-extraction.
pdf.

laura banarescu, claire bonial, shu cai, madalina
georgescu, kira grif   tt, ulf hermjakob, kevin
knight, philipp koehn, martha palmer, and nathan
schneider. 2013. id15
for sembanking. in proceedings of the 7th linguis-
tic annotation workshop and interoperability with
discourse, pages 178   186, so   a, bulgaria, august.
association for computational linguistics.

peter f. brown, stephen a. della pietra, vincent
j. della pietra, and robert. l. mercer.
1993.
the mathematics of id151:
parameter estimation. computational linguistics,
19:263   311.

shu cai and kevin knight. 2013. smatch: an evalua-
tion metric for semantic feature structures. in pro-
ceedings of the 51st annual meeting of the associa-
tion for computational linguistics (volume 2: short
papers), pages 748   752, so   a, bulgaria, august.
association for computational linguistics.

2000.

a maximum-id178-
eugene charniak.
in proceedings of the 1st north
inspired parser.
american chapter of the association for computa-
tional linguistics conference, naacl 2000, pages
132   139, stroudsburg, pa, usa. association for
computational linguistics.

stanley f. chen and joshua goodman. 1996. an em-
pirical study of smoothing techniques for language
modeling. in proceedings of the 34th annual meet-
ing on association for computational linguistics,
acl    96, pages 310   318, stroudsburg, pa, usa.
association for computational linguistics.

david chiang, kevin knight, and wei wang. 2009.
11,001 new features for statistical machine trans-
in proceedings of human language tech-
lation.
nologies: the 2009 annual conference of the north
american chapter of the association for compu-
tational linguistics, pages 218   226, boulder, col-
orado, june. association for computational linguis-
tics.

michael collins. 1997. three generative, lexicalised
models for statistical parsing. in proceedings of the
35th annual meeting of the association for com-
putational linguistics, pages 16   23, madrid, spain,
july. association for computational linguistics.

christiane fellbaum. 1998. id138: an electronic

lexical database. bradford books.

jenny rose finkel, trond grenager, and christopher
manning. 2005. incorporating non-local informa-
tion into information extraction systems by gibbs

sampling. in proceedings of the 43rd annual meet-
ing of the association for computational linguis-
tics (acl   05), pages 363   370, ann arbor, michi-
gan, june. association for computational linguis-
tics.

jeffrey flanigan, sam thomson, jaime carbonell,
chris dyer, and noah a. smith. 2014. a discrim-
inative graph-based parser for the abstract mean-
ing representation. in proceedings of the 52nd an-
nual meeting of the association for computational
linguistics (volume 1: long papers), pages 1426   
1436, baltimore, maryland, june. association for
computational linguistics.

michel galley, mark hopkins, kevin knight, and
2004. what   s in a translation
daniel marcu.
rule?
in daniel marcu susan dumais and salim
roukos, editors, hlt-naacl 2004: main proceed-
ings, pages 273   280, boston, massachusetts, usa,
may 2 - may 7. association for computational lin-
guistics.

michel galley, jonathan graehl, kevin knight, daniel
marcu, steve deneefe, wei wang, and ignacio
thayer. 2006. scalable id136 and training of
in pro-
context-rich syntactic translation models.
ceedings of the 21st international conference on
computational linguistics and 44th annual meet-
ing of the association for computational linguis-
tics, pages 961   968, sydney, australia, july. asso-
ciation for computational linguistics.

qin gao and stephan vogel. 2008. parallel implemen-
in software engi-
tations of word alignment tool.
neering, testing, and quality assurance for natu-
ral language processing, pages 49   57, columbus,
ohio, june. association for computational linguis-
tics.

daniel gildea and daniel jurafsky. 2002. automatic
labeling of semantic roles. computational linguis-
tics, 28(3):245   288, september.

matthias huck, hieu hoang, and philipp koehn. 2014.
augmenting string-to-tree and tree-to-string transla-
tion with non-syntactic phrases. in proceedings of
the ninth workshop on statistical machine trans-
lation, pages 486   498, baltimore, maryland, usa,
june. association for computational linguistics.

bevan jones, mark johnson, and sharon goldwa-
ter. 2011. formalizing id29 with tree
in proceedings of the australasian
transducers.
language technology association workshop 2011,
pages 19   28, canberra, australia, december.

bevan jones, mark johnson, and sharon goldwater.
2012. id29 with bayesian tree transduc-
ers. in proceedings of the 50th annual meeting of
the association for computational linguistics (vol-
ume 1: long papers), pages 488   496, jeju island,
korea, july. association for computational linguis-
tics.

i.h. witten and t.c. bell. 1991. the zero-frequency
problem: estimating the probabilities of novel
ieee trans-
events in adaptive text compression.
actions on id205, 37(4).

yuk wah wong and raymond mooney. 2006. learn-
ing for id29 with statistical machine
translation. in proceedings of the human language
technology conference of the naacl, main con-
ference, pages 439   446, new york city, usa, june.
association for computational linguistics.

john m. zelle and raymond j. mooney. 1996. learn-
ing to parse database queries using inductive logic
programming. in proceedings of the thirteenth na-
tional conference on arti   cial intelligence - volume
2, aaai   96, pages 1050   1055. aaai press.

john marvin zelle. 1995. using inductive logic pro-
gramming to automate the construction of natural
language parsers. ph.d. thesis, university of texas
at austin, austin, tx, usa.

paul kingsbury and martha palmer. 2002. from tree-
in in language resources and

bank to propbank.
evaluation.

gregory kuhlmann, peter stone, raymond j. mooney,
and jude w. shavlik. 2004. guiding a reinforce-
ment learner with natural language advice: initial
results in robocup soccer. in the aaai-2004 work-
shop on supervisory control of learning and adap-
tive systems, july.

peng li, yang liu, and maosong sun. 2013. an ex-
tended ghkm algorithm for inducing lambda-sid18.
in marie desjardins and michael l. littman, editors,
aaai. aaai press.

klaus macherey, franz josef och, and hermann ney.
2001. natural language understanding using sta-
in in european conf.
tistical machine translation.
on speech communication and technology, pages
2205   2208.

david nadeau and satoshi sekine. 2007. a survey
of id39 and classi   cation. lin-
guisticae investigationes, 30(1):3   26, january. pub-
lisher: john benjamins publishing company.

roberto navigli. 2009. id51:
a survey. acm comput. surv., 41(2):10:1   10:69,
february.

kishore papineni, salim roukos, and todd ward.
1997. feature-based language understanding.
in
george kokkinakis, nikos fakotakis, and evange-
los dermatas, editors, fifth european conference
on speech communication and technology, eu-
rospeech 1997, rhodes, greece, september 22-
25, 1997. isca.

kishore papineni, salim roukos, todd ward, and wei-
jing zhu. 2002. id7: a method for automatic
in proceedings
evaluation of machine translation.
of the 40th annual meeting on association for com-
putational linguistics, acl    02, pages 311   318,
stroudsburg, pa, usa. association for computa-
tional linguistics.

nima pourdamghani, yang gao, ulf hermjakob, and
kevin knight. 2014. aligning english strings with
in pro-
id15 graphs.
ceedings of the 2014 conference on empirical meth-
ods in natural language processing (emnlp),
pages 425   429, doha, qatar, october. association
for computational linguistics.

p. j. price. 1990. evaluation of spoken language sys-
in proceedings of the
tems: the atis domain.
workshop on speech and natural language, hlt
   90, pages 91   95, stroudsburg, pa, usa. associa-
tion for computational linguistics.

wei wang, jonathan may, kevin knight, and daniel
marcu.
2010. re-structuring, re-labeling, and
re-aligning for syntax-based machine translation.
computational linguistics, 36(2):247   277, june.

