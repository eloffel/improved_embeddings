big data small data, in domain out-of domain, known word unknown
word: the impact of word representation on sequence labelling tasks

lizhen qu1,2, gabriela ferraro1,2, liyuan zhou1, weiwei hou1, nathan schneider3 and timothy baldwin4

1 nicta, act 2601, australia

2 the australian national university

3 the university of melbourne, vic 3010, australia

{lizhen.qu,gabriela.ferraro,liyuan.zho,weiwei.hou}@nicta.com.au

4 university of edinburgh, eh8 9ab, uk.

nschneid@cs.cmu.edu

tb@ldwin.net

abstract

5
1
0
2

 

y
a
m
0
2

 

 
 
]
l
c
.
s
c
[
 
 

2
v
9
1
3
5
0

.

4
0
5
1
:
v
i
x
r
a

id27s     distributed word rep-
resentations that can be learned from un-
labelled data     have been shown to have
high utility in many natural language pro-
cessing applications.
in this paper, we
perform an extrinsic evaluation of    ve
popular id27 methods in the
context of four sequence labelling tasks:
pos-tagging, syntactic chunking, ner
and mwe identi   cation. a particular fo-
cus of the paper is analysing the effects
of task-based updating of word represen-
tations. we show that when using word
embeddings as features, as few as sev-
eral hundred training instances are suf   -
cient to achieve competitive results, and
that id27s lead to improve-
ments over oov words and out of domain.
perhaps more surprisingly, our results in-
dicate there is little difference between the
different id27 methods, and
that simple brown clusters are often com-
petitive with id27s across all
tasks we consider.

1

introduction

recently, distributed word representations have
grown to become a mainstay of natural language
processing (nlp), and been show to have empir-
ical utility in a myriad of tasks (collobert and
weston, 2008; turian et al., 2010; baroni et al.,
2014; andreas and klein, 2014). the underly-
ing idea behind distributed word representations
is simple:
to map each word w in our vocabu-
lary v onto a continuous-valued vector of dimen-
sionality d (cid:28) |v |. words that are similar (e.g.,
with respect to syntax or lexical semantics) will
ideally be mapped to similar regions of the vec-
tor space, implicitly supporting both generalisa-

tion across in-vocabulary (iv) items, and counter-
ing the effects of data sparsity for low-frequency
and out-of-vocabulary (oov) items.

without some means of automatically deriv-
ing the vector representations without reliance on
labelled data, however, id27s would
have little practical utility. fortunately,
it has
been shown that they can be    pre-trained    from
unlabelled text data using various algorithms to
model
that
words which occur in similar contexts tend to be
semantically similar). pre-training methods have
been re   ned considerably in recent years, and
scaled up to increasingly large corpora.

the distributional hypothesis (i.e.,

as with other machine learning methods, it is
well known that the quality of the pre-trained word
embeddings depends heavily on factors including
parameter optimisation, the size of the training
data, and the    t with the target application. for
example, turian et al. (2010) showed that the op-
timal dimensionality for id27s is task-
speci   c. one factor which has received relatively
little attention in nlp is the effect of    updating   
the pre-trained id27s as part of the
task-speci   c training, based on self-taught learn-
ing (raina et al., 2007). updating leads to word
representations that are task-speci   c, but often at
the cost of over-   tting low-frequency and oov
words.

in this paper, we perform an extensive evalu-
ation of four recently proposed id27
approaches under    xed experimental conditions,
applied to four sequence labelling tasks: pos-
tagging, full-text chunking, named entity recog-
nition (ner), and multiword expression (mwe)
identi   cation. compared to previous empirical
studies (collobert et al., 2011; turian et al.,
2010; pennington et al., 2014), we    ll their gaps
by considering more id27 approaches
and evaluating them with more sequence labelling
tasks.
in addition, we explore the following re-

search questions:
rq1: are these id27s better than
baseline approaches of one-hot unigram
features and brown clusters?

rq2: do id27s require less training
data (i.e. generalise better) than one-hot un-
igram features? if so, to what degree can
id27s reduce the amount of la-
belled data?

rq3: what is the impact of updating word em-
beddings in sequence labelling tasks, both
empirically over the target task and geo-
metrically over the vectors?

rq4: what is the impact of these word embed-
dings (with and without updating) on both
oov items (relative to the training data)
and out-of-domain data?

rq5: overall, are some id27s better
than others in a sequence labelling context?

2 word representations

2.1 types of word representations

(2010)

turian et al.
identi   es three varieties
of word representations: distributional, cluster-
based, and distributed.

distributional

representation methods map
each word w to a context word vector cw,
which is constructed directly from co-occurrence
counts between w and its context words. the
learning methods either store the co-occurrence
counts between two words w and i directly
in cwi (sahlgren, 2006; turney et al., 2010;
honkela, 1997) or project
the concurrence
counts between words into a lower dimensional
space (   reh  u  rek and sojka, 2010; lund and
burgess, 1996), using id84
techniques such as svd (dumais et al., 1988) and
lda (blei et al., 2003).

cluster-based representation methods build
clusters of words by applying either soft or hard
id91 algorithms (lin and wu, 2009; li and
mccallum, 2005). some of them also rely on
a co-occurrence matrix of words (pereira et al.,
1993). the brown id91 algorithm (brown
et al., 1992) is the best-known method in this cat-
egory.

distributed

representation methods

usu-
ally map words into dense,
low-dimensional,
continuous-valued vectors, with x     rd, where d
is referred to as the word dimension.

2.2 selected word representations
over a range of sequence labelling tasks, we
evaluate    ve methods for inducing word rep-
resentations: brown id91 (brown et al.,
1992) (   brown   ),
the neural language model
of collobert & weston (   cw   ) (collobert et
al., 2011),
the continuous bag-of-words model
(   cbow   ) (mikolov et al., 2013a), the continu-
ous skip-gram model (   skip-gram   ) (mikolov et
al., 2013b), and global vectors (   glove   ) (pen-
nington et al., 2014). with the exception of cw,
all have have been shown to be at or near state-
of-the-art in recent empirical studies (turian et
al., 2010; pennington et al., 2014). cw is in-
cluded because it was highly in   uential in earlier
research, and the pre-trained embeddings are still
used to some degree in nlp. the training of these
word representations is unsupervised:
the com-
mon underlying idea is to predict occurrence of
words in the neighbouring context. their training
objectives share the same form, which is a sum of
local training factors j(w, ctx(w)),

(cid:88)

w   v

l =

j(w, ctx(w))

where v is the vocabulary of a given corpus,
and ctx(w) denotes the local context of word w.
the local context of a word can either be its previ-
ous k words, or the k words surrounding it. local
training factors are designed to capture the rela-
tionship between w and its local contexts of use,
either by predicting w based on its local context,
or using w to predict the context words. other than
brown, which utilises a cluster-based represen-
tation, all the other methods employ a distributed
representation.

the starting point for cbow and skip-gram
is to employ softmax to predict word occurrence:

j(w, ctx(w)) =     log

wvctx(w))

exp(vt
j   v exp(vt

j vctx(w))

where vctx(w) denotes the distributed representa-
tion of the local context of word w. cbow de-
rives vctx(w) based on averaging over the context
words. that is, it estimates the id203 of each
w given its local context. in contrast, skip-gram
applies softmax to each context word of a given
occurrence of word w. in this case, vctx(w) corre-
sponds to the representation of one of its context
words. this model can be characterised as predict-

(cid:32)

(cid:80)

(cid:33)

ing context words based on w. in practice, soft-
max is too expensive to compute over large cor-
pora, and thus mikolov et al. (2013b) use hierar-
chical softmax and negative sampling to scale up
the training.

cw considers the local context of a word w to
be m words to the left and m words to the right of
w. the concatenation of the embeddings of w and
all its context words are taken as input to a neural
network with one hidden layer, which produces a
higher level representation f (w)     rd. then the
learning procedure replaces the embedding of w
with that of a randomly sampled word w(cid:48) and gen-
erates a second representation f (w(cid:48))     rd with
the same neural network. the training objective is
to maximise the difference between them:

j(w, ctx(w)) = max(0, 1     f (w) + f (w(cid:48)))

this approach can be regarded as negative sam-
pling with only one negative example.

glove assumes the dot product of two word
embeddings should be similar to the logarithm of
the co-occurrence count xij of the two words. as
such, the local factor j(w, ctx(w)) becomes:
i vj + bi + bj     log(xij))2

g(xij)(vt

where bi and bj are the bias terms of words i and
j, respectively, and g(xij) is a weighting function
based on the co-occurrence count. this weight-
ing function controls the degree of agreement be-
tween the parametric function vt
i vj + bi + bj and
log(xij). frequently co-occurring word pairs will
be larger weight than infrequent pairs, up to a
threshold.

brown partitions words into a    nite set of
word classes v . the id155 of
seeing the next word is de   ned to be:

p(wk|wk   1

k   m) = p(wk|hk)p(hk|hk   1
k   m)

where hk denotes the word class of the word
wk, wk   1
k   m are the previous m words and
hk   1
k   m are their respective word classes. then
j(w, ctx(w)) =     log p(wk|wk   1
k   m). since there
is no tractable method to    nd an optimal parti-
tion of word classes, the method uses only a bi-
gram class model, and utilises hierarchical clus-
tering as an approximation method to    nd a suf   -
ciently good partition of words.

data set
umbc
one billion
english wikipedia

size words
3g
1g
3g

48.1gb
4.1gb
49.6gb

table 1: corpora used to pre-train the word em-
beddings

2.3 building word representations
for a fair comparison, we train brown, cbow,
skip-gram, and glove on a    xed corpus, com-
prised of freely available corpora, as detailed in
tab. 1. the joint corpus was preprocessed with the
stanford corenlp sentence splitter and tokeniser.
all consecutive digit substrings were replaced by
numf, where f is the length of the digit substring
(e.g., 10.20 is replaced by num2.num2. due to
the computational complexity of the pre-training,
for cw, we simply downloaded the pre-compiled
embeddings from: http://metaoptimize.
com/projects/wordreprs.

the dimensionality of the id27s
and the size of the context window are the key hy-
perparameters when learning distributed represen-
tations. we use all combinations of the following
values to train id27s on the combined
corpus:

    embedding dim. d     {25, 50, 100, 200}
    context window size m     {1, 5, 10}

brown requires only the number of clusters as a
hyperparameter. we perform id91 with b    
{250, 500, 1000, 2000, 4000} clusters.

3 sequence labelling tasks

we evaluate the different word representations
over four sequence labelling tasks: pos-tagging
(pos-tagging), full-text chunking (chunking),
ner (ner) and mwe identi   cation (mwe). for
each task, we fed features into a    rst order linear-
chain graph transformer (collobert et al., 2011)
made up of two layers: the upper layer is identi-
cal to a linear-chain crf (lafferty et al., 2001),
and the lower layer consists of word representa-
tion and hand-crafted features.
if we treat word
representations as    xed, the graph transformer is
a simple linear-chain crf. on the other hand, if
we can treat the word representations as model pa-
rameters, the model is equivalent to a neural net-
work with id27s as the input layer. we

trained all models using adagrad (duchi et al.,
2011).

as in turian et al. (2010), at each word position,
we construct word representation features from
the words in a context window of size two to either
side of the target word, based on the pre-trained
representation of each word type. for brown,
the features are the pre   x features extracted from
word clusters in the same way as turian et al.
(2010). as a baseline (and to test rq1), we in-
clude a one-hot representation (which is equiva-
lent to a linear-chain crf with only lexical con-
text features).

our hand-crafted features for pos-tagging,
chunking and mwe, are those used by collobert
et al. (2011), turian et al. (2010) and schneider
et al. (2014b), respectively. for ner, we use the
same feature space as turian et al. (2010), except
for the previous two predictions, because we want
to evaluate all word representations with the same
type of model     a    rst-order graph transformer.

in training the distributed word representations,
we consider two settings: (1) the word represen-
tations are    xed during sequence model training;
and (2) the graph transformer updated the token-
level word representations during training.

as outlined in tab. 2, for each sequence la-
belling task, we experiment over the de facto cor-
pus, based on pre-existing training   dev   test splits
where available:1
pos-tagging: the wall street journal portion
of the id32 (marcus et al. (1993):
   wsj   ) with penn pos tags

chunking: the wall street journal portion of the
id32 (   wsj   ), converted into iob-
style full-text chunks using the conll con-
version scripts for training and dev, and the
wsj-derived conll-2000 full text chunk-
ing test data for testing (tjong kim sang and
buchholz, 2000)

ner: the english portion of the conll-2003
english id39 data set,
for which the source data was taken from
reuters newswire articles (tjong kim sang
and de meulder (2003):    reuters   )

mwe: the mwe dataset of schneider et al.
(2014b), over a portion of text from the en-
glish web treebank2 (   ewt   )

1for the mwe dataset, no such split pre-existed, so we

constructed our own.

ldc2012t13

2https://catalog.ldc.upenn.edu/

for all tasks other than mwe,3 we additionally
have an out-of-domain test set, in order to eval-
uate the out-of-domain robustness of the different
word representations, with and without updating.
these datasets are as follows:
pos-tagging: the english web treebank with

penn pos tags (   ewt   )

chunking: the brown corpus portion of the
id32 (   brown   ), converted into
iob-style full-text chunks using the conll
conversion scripts

ner: the muc-7 id39 cor-

pus4 (   muc7   )

for reproducibility, we tuned the hyperparame-
ters with random search over the development data
for each task (bergstra and bengio, 2012). in this,
we randomly sampled 50 distinct hyperparame-
ter sets with the same random seed for the non-
updating models (i.e. the models that don   t update
the word representation), and sampled 100 distinct
hyperparameter sets for the updating models (i.e.
the models that do). for each set of hyperparam-
eters and task, we train a model over its training
set and choose the best one based on its perfor-
mance on development data (turian et al., 2010).
we also tune the word representation hyperparam-
eters     namely, the word vector size d and context
window size m (distributed representations), and
in the case of brown, the number of clusters.

for the updating models, we found that the re-
sults over the test data were always inferior to
those that do not update the word representations,
due to the higher number of hyperparameters and
small sample size (i.e. 100). since the two-layer
model of the graph transformer contains a distinct
set of hyperparameters for each layer, we reuse the
best-performing hyperparameter settings from the
non-updating models, and only tune the hyperpa-
rameters of adagrad for the word representation
layer. this method requires only 32 additional
runs and achieves consistently better results than
100 random draws.

in order to test the impact of the volume of
training data on the different models (rq2), we
split the training set into 10 partitions based on
a base-2 log scale (i.e., the second smallest par-
tition will be twice the size of the smallest parti-
tion), and created 10 successively larger training

3unfortunately, there is no second domain which has been
hand-tagged with mwes using the method of schneider et al.
(2014b) to use as an out-of-domain test corpus.
4https://catalog.ldc.upenn.edu/ldc2001t02

pos-tagging

chunking

ner
mwe

training

wsj sec. 0-18

wsj

development
wsj sec. 19   21

wsj (1k sentences)

in-domain test
wsj sec. 22   24

wsj (conll-00 test)

reuters (conll-03 train) reuters (conll-03 dev) reuters (conll-03 test)

ewt (500 docs)

ewt (100 docs)

ewt (123 docs)

out-of-domain test

ewt

brown
muc7

   

table 2: training, development and test (in- and out-of-domain) data for each sequence labelling task.

sets by merging these partitions from the smallest
one to the largest one, and used each of these to
train a model. from these, we construct learning
curves over each task.

for ease of comparison with previous results,
we evaluate both in- and out-of-domain using
chunk/entity/expression-level f1-measure (   f1   )
for all tasks except pos-tagging, for which we
use token-level accuracy (   acc   ). to test per-
formance over oov (unknown) tokens     i.e., the
words that do not occur in the training set     we
use token-level accuracy for all tasks (e.g. for
chunking, we evaluate whether the full iob tag
is correct or not), due to the sparsity of all-oov
chunks/nes/mwes.

4 experimental results and discussion

we structure our evaluation by stepping through
each of our    ve research questions (rq1   5) from
the start of the paper.
in this, we make refer-
ence to: (1) the best-performing method both in-
and out-of-domain vs. the state-of-the-art (tab. 3);
(2) a heat map for each task indicating the con-
vergence rate for each word representation, with
and without updating (fig. 1); (3) oov accuracy
both in-domain and out-of-domain for each task
(fig. 2); and (4) visualisation of the impact of
updating on id27s, based on id167
(fig. 3).

rq1: are the selected id27s better
than one-hot unigram features and brown clus-
ters? as shown in tab. 3, the best-performing
method for every task except in-domain chunk-
ing is a id27 method, although the
precise method varies greatly. fig. 1, on the other
hand, tells a more subtle story: the difference be-
tween unigram and the other word representa-
tions is relatively modest, esp. as the amount of
training data increases. additionally, the differ-
ence between brown and the id27
methods is modest across all tasks. so, the over-
all answer would appear to be: yes for unigrams
when there is little training data, but not really for
brown.

rq2: do id27 features require
less training data? fig. 1 shows that
for
pos-tagging and ner, with only several hun-
dred training instances, id27 fea-
tures achieve superior results to unigram. for
example, when trained with 561 instances, the
pos-tagging model using skip-gram+up em-
beddings is 5.3% above unigram; and when
trained with 932 instances, the ner model us-
ing skip-gram is 11.7% above unigram. sim-
ilar improvements are also found for other types
of id27s and brown, when the train-
ing set is small. however, all word representa-
tions perform similarly for chunking regardless
of training data size. for mwe, brown performs
slightly better than the other methods when trained
with approximately 25% of the training instances.
therefore, we conjecture that the pos-tagging
and ner tasks bene   t more from distributional
similarity than chunking and mwe.

rq3: does task-speci   c updating improve all
id27s across all tasks? based on
fig. 1, updating of word representations can
equally correct poorly-learned word representa-
tions, and harm pre-trained representations, due
to over   tting. for example, glove perform sig-
ni   cantly worse than skip-gram in both pos-
tagging and ner without updating, but with up-
dating, the gap between their results and the best
performing method becomes smaller. in contrast,
skip-gram performs worse over the test data
with updating, despite the results on the develop-
ment set improving by 1%.

to further investigate the effects of updating,
we sampled 60 words and plotted the changes
in their id27s under updating, us-
ing 2-d vector    elds generated by using mat-
plotlib and id167 (van der maaten and hinton,
2008). half of the words were chosen manu-
ally to include known word clusters such as days
of the week and names of countries;
the other
half were selected randomly. additional plots
with 100 randomly-sampled words and the top-
100 most frequent words, for all the methods and
all the tasks, can be found in the supplementary

task
pos-tagging (acc)
chunking (f1)
ner (f1)
mwe (f1)
table 3: state-of-the-art results vs. our best results for in-domain and out-of-domain test sets.

benchmark
0.972 (toutanova et al., 2003)
0.942 (sha and pereira, 2003)
0.893 (ando and zhang, 2005)
0.625 (schneider et al., 2014a)

in-domain test set
0.959 (skip-gram+up)
0.938 (brownb=2000)
0.868 (skip-gram)
0.654 (cbow+up)

out-of-domain test set
0.910 (skip-gram)
0.676 (glove)
0.736 (skip-gram)
   

(a) pos-tagging (acc)

(b) chunking (f1)

(c) ner (f1)

(d) mwe (f1)

figure 1: results for each type of word representation over pos-tagging, chunking, ner and mwe,
optionally with updating (   +up   ). the y-axis indicates the training data sizes (on a log scale). green
= high performance, and red = low performance, based on a linear scale of the best- to worst-result for
each task.

material and at https://123abc123abd.
wordpress.com/. in each plot, a single arrow
signi   es one word, pointing from the position of
the original id27 to the updated repre-
sentation.

in fig. 3, we show vector    elds plots for
chunking and ner using skip-gram embed-
dings. for chunking, most of the vectors were
changed with similar magnitude, but in very dif-
ferent directions, including within the clusters of
days of the week and country names. in contrast,
for ner, there was more homogeneous change in
word vectors belonging to the same cluster. this
greater consistency is further evidence that seman-
tic homogeneity appears to be more bene   cial for
ner than chunking.

rq4: what is the impact of id27s
cross-domain and for oov words? as shown
in tab. 3, results predictably drop when we eval-
uate out of domain. the difference is most pro-
nounced for chunking, where there is an absolute
drop in f1 of around 30% for all methods, indi-
cating that id27s and unigram features
provide similar information for chunking.

another interesting observation is that updating
often hurts out-of-domain performance because
the distribution between domains is different. this
suggests that, if the objective is to optimise per-
formance across domains, it is best not to perform
updating.

we also analyze performance on oov words
both in-domain and out-of-domain in fig. 2. as

figure 2: acc over out-of-vocabulary (oov) words for in-domain and out-of-domain test sets.

(a) chunking

(b) ner

figure 3: a id167 plot of the impact of updating on skip-gram

expected, id27s and brown excel
in out-of-domain oov performance. consistent
with our overall observations about cross-domain

generalisation, the oov results are better when
updating is not performed.

0500010000150002000025000300003500040000training size0.50.60.70.80.91.0pos accuracy for in-domain oov0500010000150002000025000300003500040000training size0.50.60.70.80.91.0pos accuracy for out-of-domain oov0200040006000800010000120001400016000training size0.50.60.70.80.91.0ner accuracy for in-domain oov0200040006000800010000120001400016000training size0.50.60.70.80.91.0ner accuracy for out-of-domain oov0200040006000800010000training size0.50.60.70.80.91.0chunking accuracy for in-domain oov0200040006000800010000training size0.50.60.70.80.91.0chunking accuracy for out-of-domain oov0500100015002000250030003500training size0.50.60.70.80.91.0mwe accuracy for in-domain oovbrown_clustercbow_noupcbow_upglove_noupglove_upskip_gram_negsam_noupskip_gram_negsam_upunigram20010001002003002001000100200names ofcountriesdays ofthe week30201001020303020100102030names ofcountriesdays ofthe weekrq5 overall, are some id27s bet-
ter than others? comparing the different word
embedding techniques over our four sequence la-
belling tasks, for the different evaluations (overall,
out-of-domain and oov), there is no clear winner
among the id27s     for pos-tagging,
skip-gram appears to have a slight advantage,
but this does not generalise to other tasks.

while the aim of this paper was not to achieve
the state of the art over the respective tasks, it is
important to concede that our best (in-domain) re-
sults for ner, pos-tagging and chunking are
slightly worse than the state of the art (tab. 3).
the 2.7% difference between our ner system
and the best performing system is due to the fact
that we use a    rst-order instead of a second-order
crf (ando and zhang, 2005), and for the other
tasks, there are similarly differences in the learner
and the complexity of the features used. another
difference is that we tuned the hyperparameters
with random search, to enable replication using
the same random seed. in contrast, the hyperpa-
rameters for the state-of-the-art methods are tuned
more extensively by experts, making them more
dif   cult to reproduce.

5 related work

collobert et al. (2011) proposed a uni   ed neural
network framework that learns id27s
and applied it for pos-tagging, chunking, ner
and semantic role labelling. when they combined
id27s with hand crafted features (e.g.,
word suf   xes for pos-tagging; gazetteers for
ner) and applied other tricks like cascading and
classi   er combination, they achieved state-of-the-
art performance. similarly, turian et al. (2010)
evaluated three different word representations on
ner and chunking, and concluded that unsu-
pervised word representations improved ner and
chunking. they also found that combining dif-
ferent word representations can further improve
performance. guo et al. (2014) also explored dif-
ferent ways of using id27s for ner.
owoputi et al. (2013) and schneider et al. (2014a)
found that brown id91 enhances twitter
id52 and mwe, respectively. compared
to previous work, we consider more word rep-
resentations including the most recent work and
evaluate them on more sequence labelling tasks,
wherein the models are trained with training sets

of varying size.

bansal et al. (2014) reported that direct use of
id27s in id33 did not
show improvement. they achieved an improve-
ment only when they performed hierarchical clus-
tering of the id27s, and used features
extracted from the cluster hierarchy.
in a simi-
lar vein, andreas and klein (2014) explored the
use of id27s for constituency pars-
ing and concluded that the information contained
in id27s might duplicate the one ac-
quired by a syntactic parser, unless the training set
is extremely small. other syntactic parsing studies
that reported improvements by using word embed-
dings include koo et al. (2008), koo et al. (2010),
haffari et al. (2011), tratz and hovy (2011) and
chen and manning (2014).

id27s have also been applied to
other (non-sequential nlp) tasks like grammar in-
duction (spitkovsky et al., 2011), and semantic
tasks such as semantic relatedness, synonymy de-
tection, concept categorisation, selectional prefer-
ence learning and analogy (baroni et al., 2014).

huang and yates (2009) demonstrated that us-
ing distributional word representations methods
(like tf-idf and lsa) as features, improves the
labelling of oov, when test for pos-tagging and
chunking. in our study, we evaluate the labelling
performance of oov words for updated vs. not
updated id27s representations, rela-
tive to the training set and with out-of-domain
data.

6 conclusions

we have performed an extensive extrinsic evalua-
tion of four id27 methods under    xed
experimental conditions, and evaluated their appli-
cability to four sequence labelling tasks: pos-
tagging, chunking, ner and mwe identi   ca-
tion. we found that id27 features re-
liably outperformed unigram features, especially
with limited training data, but that there was rela-
tively little difference over brown clusters, and no
one embedding method was consistently superior
across the different tasks and settings. word em-
beddings and brown clusters were also found to
improve out-of-domain performance and for oov
words. we expected a performance gap between
the    xed and task-updated embeddings, but the ob-
served difference was marginal. indeed, we found
that updating can result in over   tting. we also car-

ried out preliminary analysis of the impact of up-
dating on the vectors, a direction which we intend
to pursue further.

references
rie kubota ando and tong zhang. 2005. a frame-
work for learning predictive structures from multi-
ple tasks and unlabeled data. journal of machine
learning research, 6:1817   1853, december.

jacob andreas and dan klein. 2014. how much do
in pro-
id27s encode about syntax?
ceedings of the 52nd annual meeting of the associa-
tion for computational linguistics (volume 2: short
papers), pages 822   827, baltimore, usa.

mohit bansal, kevin gimpel, and karen livescu.
2014. tailoring continuous word representations for
id33. in proceedings of the 52nd an-
nual meeting of the association for computational
linguistics (volume 2: short papers), pages 809   
815, baltimore, usa.

marco baroni, georgiana dinu,

2014. don   t count, predict!

and germ  an
kruszewski.
a
systematic comparison of context-counting vs.
context-predicting semantic vectors. in proceedings
of
the association
for computational linguistics (volume 1: long
papers), pages 238   247, baltimore, usa, june.

the 52nd annual meeting of

james bergstra and yoshua bengio. 2012. random
search for hyper-parameter optimization. journal of
machine learning research, 13(1):281   305.

david m blei, andrew y ng, and michael i jordan.
2003. id44. journal of ma-
chine learning research, 3:993   1022.

peter f. brown, peter v. desouza, robert l. mer-
cer, vincent j. della pietra, and jenifer c. lai.
1992. class-based id165 models of natural lan-
guage. computational linguistics, 18:467   479.

danqi chen and christopher d manning. 2014. a fast
and accurate dependency parser using neural net-
works. in empirical methods in natural language
processing (emnlp).

ronan collobert and jason weston. 2008. a uni-
   ed architecture for natural language processing:
deep neural networks with multitask learning.
in
proceedings of the 25th international conference
on machine learning, icml    08, pages 160   167,
helsinki, finland.

ronan collobert, jason weston, l  eon bottou, michael
karlen, koray kavukcuoglu, and pavel kuksa.
2011. natural language processing (almost) from
journal of machine learning research,
scratch.
12:2493   2537.

john duchi, elad hazan, and yoram singer. 2011.
adaptive subgradient methods for online learning
journal of machine
and stochastic optimization.
learning research, 12:2121   2159.

susan t dumais, george w furnas, thomas k lan-
dauer, scott deerwester, and richard harshman.
1988. using latent semantic analysis to improve ac-
in proceedings of the
cess to textual information.
sigchi conference on human factors in computing
systems, pages 281   285. acm.

jiang guo, wanxiang che, haifeng wang, and ting
liu. 2014. revisiting embedding features for sim-
ple semi-supervised learning. in proceedings of the
2014 conference on empirical methods in natural
language processing (emnlp), pages 110   120.

gholamreza haffari, marzieh razavi, and anoop
sarkar. 2011. an ensemble model that combines
syntactic and semantic id91 for discriminative
id33. in acl 2011 (short papers),
pages 710   714.

timo honkela. 1997. self-organizing maps of words
for natural language processing applications.
in
proceedings of the international icsc symposium
on soft computing, pages 401   407.

fei huang and alexander yates. 2009. distributional
representations for handling sparsity in supervised
sequence-labeling. in proceedings of the joint con-
ference of the 47th annual meeting of the acl and
the 4th international joint conference on natural
language processing of the afnlp: volume 1 - vol-
ume 1, acl    09, pages 495   503, stroudsburg, pa,
usa. association for computational linguistics.

terry koo, xavier carreras, and michael collins.
2008. simple semi-supervised id33.
in proceedings of acl-08: hlt, pages 595   603,
columbus, usa.

terry koo, alexander m. rush, michael collins,
tommi jaakkola, and david sontag. 2010. dual
decomposition for parsing with non-projective head
automata. in proceedings of the 2010 conference on
empirical methods in natural language process-
ing, emnlp    10, pages 1288   1298, cambridge,
uk.

john lafferty, andrew mccallum, and fernando
pereira. 2001. conditional random    elds: prob-
abilistic models for segmenting and labeling se-
in proceedings of the 18th interna-
quence data.
tional conference on machine learning, pages 282   
289, williamstown, usa.

wei li and andrew mccallum.

semi-
supervised sequence modeling with syntactic topic
models. in proceedings of the national conference
on arti   cial intelligence, pittsburgh, usa.

2005.

dekang lin and xiaoyun wu. 2009. phrase id91
in proceedings of the
for discriminative learning.
joint conference of the 47th annual meeting of the

acl and the 4th international joint conference on
natural language processing of the afnlp, pages
1030   1038.

kevin lund and curt burgess.

producing
high-dimensional semantic spaces from lexical co-
occurrence. behavior research methods, instru-
ments, & computers, 28(2):203   208.

1996.

mitchell p. marcus, beatrice santorini, and mary ann
marcinkiewicz. 1993. building a large annotated
the id32. computa-
corpus of english:
tional linguistics, 19(2):313   330.

tomas mikolov, kai chen, greg corrado, and jeffrey
dean. 2013a. ef   cient estimation of word represen-
tations in vector space. corr, abs/1301.3781.

tomas mikolov, ilya sutskever, kai chen, greg s cor-
rado, and jeff dean.
2013b. distributed repre-
sentations of words and phrases and their compo-
sitionality. in c.j.c. burges, l. bottou, m. welling,
z. ghahramani, and k.q. weinberger, editors, ad-
vances in neural information processing systems
26, pages 3111   3119. curran associates, inc.

nathan schneider, spencer onuffer, nora kazour,
emily danchik, michael t. mordowanec, henrietta
conrad, and noah a. smith. 2014b. comprehen-
sive annotation of multiword expressions in a social
web corpus. in nicoletta calzolari, khalid choukri,
thierry declerck, hrafn loftsson, bente maegaard,
joseph mariani, asuncion moreno, jan odijk, and
stelios piperidis, editors, proceedings of the ninth
international conference on language resources
and evaluation, pages 455   461, reykjav    k, iceland.

fei sha and fernando pereira. 2003. id66
in proceedings of
with conditional random    elds.
the 2003 conference of the north american chap-
ter of the association for computational linguistics
on human language technology - volume 1, pages
134   141, edmonton, canada.

valentin i. spitkovsky, hiyan alshawi, angel x.
chang, and daniel jurafsky. 2011. unsupervised
id33 without gold part-of-speech
tags. in proceedings of the conference on empirical
methods in natural language processing, emnlp
   11, pages 1281   1290, edinburgh, uk.

olutobi owoputi, brendan o   connor, chris dyer,
kevin gimpel, nathan schneider, and noah a
smith. 2013. improved part-of-speech tagging for
online conversational text with word clusters.
in
hlt-naacl, pages 380   390.

erik f. tjong kim sang and sabine buchholz.
2000.
introduction to the conll-2000 shared
task: chunking. in proceedings of the 4th confer-
ence on computational natural language learning
(conll-2000), lisbon, portugal.

jeffrey pennington, richard socher, and christo-
pher d. manning. 2014. glove: global vectors for
word representation. in proceedings of emnlp.

fernando pereira, naftali tishby, and lillian lee.
1993. distributional id91 of english words. in
proceedings of the 31st annual meeting of the asso-
ciation for computational linguistics, pages 183   
190.

rajat raina, alexis battle, honglak lee, benjamin
packer, and andrew y ng.
self-taught
learning: id21 from unlabeled data. in
proceedings of the 24th international conference on
machine learning, pages 759   766. acm.

2007.

radim   reh  u  rek and petr sojka. 2010. software frame-
work for topic modelling with large corpora. in pro-
ceedings of the lrec 2010 workshop on new chal-
lenges for nlp frameworks, pages 51   55, valetta,
malta.

magnus sahlgren. 2006. the word-space model: us-
ing distributional analysis to represent syntagmatic
and paradigmatic relations between words in high-
dimensional vector spaces. ph.d. thesis, institutio-
nen f  or lingvistik.

nathan schneider, emily danchik, chris dyer, and
noah a. smith. 2014a. discriminative lexical se-
mantic segmentation with gaps: running the mwe
gamut. transactions of the association of computa-
tional linguistics, 2(1):193   206.

erik f. tjong kim sang and fien de meulder.
introduction to the conll-2003 shared task:
2003.
language-independent id39. in
proceedings of the 7th conference on natural lan-
guage learning (conll-2003), pages 142   147, ed-
monton, canada.

kristina toutanova, dan klein, christopher d. man-
ning, and yoram singer. 2003. feature-rich part-of-
speech tagging with a cyclic dependency network.
in proceedings of the 2003 conference of the north
american chapter of the association for computa-
tional linguistics on human language technology
- volume 1, pages 173   180, stroudsburg, pa, usa.

stephen tratz and eduard hovy. 2011. a fast, ac-
curate, non-projective, semantically-enriched parser.
in proceedings of
the conference on empirical
methods in natural language processing, emnlp
   11, pages 1257   1268, edinburgh, uk.

joseph turian, lev ratinov, and yoshua bengio. 2010.
word representations: a simple and general method
for semi-supervised learning. in proceedings of the
48th annual meeting of the association for compu-
tational linguistics, pages 384   394. association for
computational linguistics.

peter d turney, patrick pantel, et al. 2010. from
frequency to meaning: vector space models of se-
mantics. journal of arti   cial intelligence research,
37(1):141   188.

laurens j.p. van der maaten and geoffrey hinton.
2008. visualizing high-dimensional data using
journal of machine learning research,
id167.
9:2579   2605.

