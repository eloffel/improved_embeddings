aligning books and movies: towards story-like visual explanations by

watching movies and reading books

yukun zhu    ,1 ryan kiros*,1 richard zemel1 ruslan salakhutdinov1

raquel urtasun1 antonio torralba2

sanja fidler1

1university of toronto

2massachusetts institute of technology

{yukun,rkiros,zemel,rsalakhu,urtasun,fidler}@cs.toronto.edu, torralba@csail.mit.edu

5
1
0
2

 

n
u
j
 

2
2

 
 
]

v
c
.
s
c
[
 
 

1
v
4
2
7
6
0

.

6
0
5
1
:
v
i
x
r
a

abstract

books are a rich source of both    ne-grained information,
how a character, an object or a scene looks like, as well as
high-level semantics, what someone is thinking, feeling and
how these states evolve through a story. this paper aims to
align books to their movie releases in order to provide rich
descriptive explanations for visual content that go semanti-
cally far beyond the captions available in current datasets.
to align movies and books we exploit a neural sentence
embedding that is trained in an unsupervised way from a
large corpus of books, as well as a video-text neural em-
bedding for computing similarities between movie clips and
sentences in the book. we propose a context-aware id98
to combine information from multiple sources. we demon-
strate good quantitative performance for movie/book align-
ment and show several qualitative examples that showcase
the diversity of tasks our model can be used for.

1. introduction

a truly intelligent machine needs to not only parse the
surrounding 3d environment, but also understand why peo-
ple take certain actions, what they will do next, what they
could possibly be thinking, and even try to empathize with
them.
in this quest, language will play a crucial role in
grounding visual information to high-level semantic con-
cepts. only a few words in a sentence may convey really
rich semantic information. language also represents a natu-
ral means of interaction between a naive user and our vision
algorithms, which is particularly important for applications
such as social robotics or assistive driving.

combining images or videos with language has gotten
signi   cant attention in the past year, partly due to the cre-
ation of coco [18], microsoft   s large-scale captioned im-
age dataset. the    eld has tackled a diverse set of tasks such
as captioning [13, 11, 36, 35, 21], alignment [11, 15, 34],
q&a [20, 19], visual model learning from textual descrip-
tions [8, 26], and semantic visual search with natural multi-
sentence queries [17].

   denotes equal contribution

figure 1: shot from the movie gone girl, along with the
subtitle, aligned with the book. we reason about the visual
and dialog (text) alignment between the movie and a book.

books provide us with very rich, descriptive text that
conveys both    ne-grained visual details (how people or
scenes look like) as well as high-level semantics (what peo-
ple think and feel, and how their states evolve through a
story). this source of knowledge, however, does not come
with associated visual information that would enable us
to ground it with descriptions. grounding descriptions in
books to vision would allow us to get textual explanations
or stories behind visual information rather than simplistic
captions available in current datasets. it can also provide us
with extremely large amount of data (with tens of thousands
books available online).

in this paper, we exploit the fact that many books have
been turned into movies. books and their movie releases
have a lot of common knowledge as well as they are com-
plementary in many ways. for instance, books provide de-
tailed descriptions about the intentions and mental states of
the characters, while movies are better at capturing visual
aspects of the settings.

the    rst challenge we need to address, and the focus
of this paper, is to align books with their movie releases
in order to obtain rich descriptions for the visual content.
we aim to align the two sources with two types of in-
formation: visual, where the goal is to link a movie shot
to a book paragraph, and dialog, where we want to    nd
correspondences between sentences in the movie   s subtitle
and sentences in the book. we formulate the problem of
movie/book alignment as    nding correspondences between
shots in the movie as well as dialog sentences in the sub-
titles and sentences in the book (fig. 1). we introduce a
novel sentence similarity measure based on a neural sen-

1

tence embedding trained on millions of sentences from a
large corpus of books. on the visual side, we extend the
neural image-sentence embeddings to the video domain and
train the model on dvs descriptions of movie clips. our
approach combines different similarity measures and takes
into account contextual information contained in the nearby
shots and book sentences. our    nal alignment model is for-
mulated as an energy minimization problem that encourages
the alignment to follow a similar timeline. to evaluate the
book-movie alignment model we collected a dataset with
11 movie/book pairs annotated with 2,070 shot-to-sentence
correspondences. we demonstrate good quantitative perfor-
mance and show several qualitative examples that showcase
the diversity of tasks our model can be used for.

the alignment model can have multiple applications.
imagine an app which allows the user to browse the book
as the scenes unroll in the movie: perhaps its ending or act-
ing are ambiguous, and one would like to query the book
for answers. vice-versa, while reading the book one might
want to switch from text to video, particularly for the juicy
scenes. we also show other applications of learning from
movies and books such as book retrieval (   nding the book
that goes with a movie and    nding other similar books), and
captioning coco images with story-like descriptions.
2. related work

most effort in the domain of vision and language has
been devoted to the problem of image captioning. older
work made use of    xed visual representations and translated
them into textual descriptions [6, 16]. recently, several
approaches based on id56s emerged, generating captions
via a learned joint image-text embedding [13, 11, 36, 21].
these approaches have also been extended to generate de-
scriptions of short video clips [35]. in [24], the authors go
beyond describing what is happening in an image and pro-
vide explanations about why something is happening.

for text-to-image alignment, [15, 7]    nd correspon-
dences between nouns and pronouns in a caption and visual
objects using several visual and textual potentials. lin et
al. [17] does so for videos. in [11], the authors use id56
embeddings to    nd the correspondences.
[37] combines
neural embeddings with soft attention in order to align the
words to image regions.

early work on movie-to-text alignment include dynamic
time warping for aligning movies to scripts with the help
of subtitles [5, 4]. sankar et al. [28] further developed a
system which identi   ed sets of visual and audio features to
align movies and scripts without making use of the subtitles.
such alignment has been exploited to provide weak labels
for person naming tasks [5, 30, 25].

closest to our work is [34], which aligns plot synopses to
shots in the tv series for story-based content retrieval. this
work adopts a similarity function between sentences in plot

synopses and shots based on person identities and keywords
in subtitles. our work differs with theirs in several impor-
tant aspects. first, we tackle a more challenging problem of
movie/book alignment. unlike plot synopsis, which closely
follow the storyline of movies, books are more verbose and
might vary in the storyline from their movie release. fur-
thermore, we use learned neural embeddings to compute the
similarities rather than hand-designed similarity functions.
parallel to our work, [33] aims to align scenes in movies
to chapters in the book. however, their approach operates
on a very coarse level (chapters), while ours does so on the
sentence/paragraph level. their dataset thus evaluates on
90 scene-chapter correspondences, while our dataset draws
2,070 shot-to-sentences alignments. furthermore, the ap-
proaches are inherently different.
[33] matches the pres-
ence of characters in a scene to those in a chapter, as well
as uses hand-crafted similarity measures between sentences
in the subtitles and dialogs in the books, similarly to [34].

rohrbach et al. [27] recently released the movie de-
scription dataset which contains clips from movies, each
time-stamped with a sentence from dvs (descriptive video
service). the dataset contains clips from over a 100 movies,
and provides a great resource for the captioning techniques.
our effort here is to align movies with books in order to ob-
tain longer, richer and more high-level video descriptions.
we start by describing our new dataset, and then explain

our proposed approach.

3. the moviebook and bookcorpus datasets
we collected two large datasets, one for movie/book

alignment and one with a large number of books.
the moviebook dataset. since no prior work or data ex-
ist on the problem of movie/book alignment, we collected a
new dataset with 11 movies along with the books on which
they were based on. for each movie we also have a sub-
title    le, which we parse into a set of time-stamped sen-
tences. note that no speaker information is provided in the
subtitles. we automatically parse each book into sentences,
paragraphs (based on indentation in the book), and chapters
(we assume a chapter title has indentation, starts on a new
page, and does not end with an end symbol).

our annotators had the movie and a book opened side
by side. they were asked to iterate between browsing the
book and watching a few shots/scenes of the movie, and
trying to    nd correspondences between them. in particular,
they marked the exact time (in seconds) of correspondence
in the movie and the matching line number in the book    le,
indicating the beginning of the matched sentence. on the
video side, we assume that the match spans across a shot (a
video unit with smooth camera motion). if the match was
longer in duration, the annotator also indicated the ending
time of the match. similarly for the book, if more sentences

book
avg. # words

movie

annotation

per sent.

per sent.

max # words

# sent.
12,603
4,229
8,050
6,458
2,562
9,467
11,992
7,103
15,498

title
gone girl
fight club
no country for old men
harry potter and the sorcerers stone
shawshank redemption
the green mile
american psycho
one flew over the cuckoo nest
the firm
brokeback mountain
the road
all
table 1: statistics for our moviebook dataset with ground-truth for alignment between books and their movie releases.

# words
148,340
48,946
69,824
78,596
40,140
133,241
143,631
112,978
135,529
10,640
58,793
980,658

# shots
2,604
2,365
1,348
2,647
1,252
2,350
1,012
1,671
2,423
1,205
1,108
19,985

76
104
223
164
44
208
278
64
82
80
126
1,449

153
90
68
227
115
119
422
192
85
173
74
156

# unique
words
3,849
1,833
1,704
2,363
1,360
3,043
4,632
2,949
3,685
470
1,580
9,032

# sent. in
subtitles
2,555
1,864
889
1,227
1,879
1,846
1,311
1,553
1,775
1,228
782

# visual
align.
106
42
47
73
12
102
85
25
60
20
49
621

# para-
graphs
3,927
2,082
3,189
2,925
637
2,760
3,945
2,236
5,223
167
2,345
29,436

15
14
10
15
18
17
16
19
11
20
10
15

638
6,638
85,238

# dialog
align.

16,909

# of books

# of sentences
74,004,228

# of words
984,846,357

11,038
table 2: summary statistics of our bookcorpus dataset. we use this corpus to train the sentence embedding model.

1,316,420

13

11

# of unique words mean # of words per sentence median # of words per sentence

matched, the annotator indicated from which to which line a
match occurred. each alignment was also tagged, indicating
whether it was a visual, dialogue, or an audio match. note
that even for dialogs, the movie and book versions are se-
mantically similar but not exactly the same. thus deciding
on what de   nes a match or not is also somewhat subjective
and may slightly vary across our annotators. altogether,
the annotators spent 90 hours labeling 11 movie/book pairs,
locating 2,070 correspondences.

table 1 presents our dataset, while fig. 8 shows a few
ground-truth alignments. one can see the complexity and
diversity of the data: the number of sentences per book vary
from 638 to 15,498, even though the movies are similar in
duration. this indicates a huge diversity in descriptiveness
across literature, and presents a challenge for matching. the
sentences also vary in length, with the sentences in broke-
back mountain being twice as long as those in the road.
the longest sentence in american psycho has 422 words
and spans over a page in the book.

aligning movies with books is challenging even for hu-
mans, mostly due to the scale of the data. each movie is on
average 2h long and has 1,800 shots, while a book has on
average 7,750 sentences. books also have different styles
of writing, formatting, different and challenging language,
slang (going vs goin   , or even was vs    us), etc. as one can
see from table 1,    nding visual matches turned out to be
particularly challenging. this is because the visual descrip-
tions in books can be either very short and hidden within
longer paragraphs or even within a longer sentence, or very
verbose     in which case they get obscured with the sur-
rounding text     and are hard to spot. of course, how close
the movie follows the book is also up to the director, which
can be seen through the number of alignments that our an-
notators found across different movie/books.
the bookcorpus dataset.
in order to train our sentence
similarity model we collected a corpus of 11,038 books
from the web. these are free books written by yet unpub-

lished authors. we only included books that had more than
20k words in order to    lter out perhaps noisier shorter sto-
ries. the dataset has books in 16 different genres, e.g.,
romance (2,865 books), fantasy (1,479), science    ction
(786), teen (430), etc. table 2 highlights the summary
statistics of our book corpus.
4. aligning books and movies

our approach aims to align a movie with a book by ex-
ploiting visual information as well as dialogs. we take shots
as video units and sentences from subtitles to represent di-
alogs. our goal is to match these to the sentences in the
book. we propose several measures to compute similari-
ties between pairs of sentences as well as shots and sen-
tences. we use our novel deep neural embedding trained
on our large corpus of books to predict similarities between
sentences. note that an extended version of the sentence
embedding is described in detail in [14] showing how to
deal with million-word vocabularies, and demonstrating its
performance on a large variety of nlp benchmarks. for
comparing shots with sentences we extend the neural em-
bedding of images and text [13] to operate in the video do-
main. we next develop a novel contextual alignment model
that combines information from various similarity measures
and a larger time-scale in order to make better local align-
ment predictions. finally, we propose a simple pairwise
conditional random field (crf) that smooths the align-
ments by encouraging them to follow a linear timeline, both
in the video and book domain.

we    rst explain our sentence, followed by our joint video
to text embedding. we next propose our contextual model
that combines similarities and discuss crf in more detail.
4.1. skip-thought vectors

in order to score the similarity between two sentences,
we exploit our architecture for learning unsupervised rep-
resentations of text [14]. the model is loosely inspired by

figure 2: sentence neural embedding [14]. given a tuple (si   1, si, si+1) of consecutive sentences in text, where si is the
i-th sentence, we encode si and aim to reconstruct the previous si   1 and the following sentence si+1. unattached arrows are
connected to the encoder output. colors depict which components share parameters. (cid:104)eos(cid:105) is the end of sentence token.

he drove down the street off into the distance .

the most effective way to end the battle .

he started the car , left the parking lot and merged onto the highway a few miles down the road .

he shut the door and watched the taxi drive off .

she watched the lights    icker through the trees as the men drove toward the road .

he jogged down the stairs , through the small lobby , through the door and into the street .

a messy business to be sure , but necessary to achieve a    ne and noble end .

they saw their only goal as survival and logically planned a strategy to achieve it .

there would be far fewer casualties and far less destruction .

the outcome was the lisbon treaty .

table 3: qualitative results from the sentence embedding model. for each query sentence on the left, we retrieve the 4 nearest
neighbor sentences (by inner product) chosen from books the model has not seen before.

the skip-gram [22] architecture for learning representations
of words. in the word skip-gram model, a word wi is cho-
sen and must predict its surrounding context (e.g. wi+1 and
wi   1 for a context window of size 1). our model works in
a similar way but at the sentence level. that is, given a sen-
tence tuple (si   1, si, si+1) our model    rst encodes the sen-
tence si into a    xed vector, then conditioned on this vector
tries to reconstruct the sentences si   1 and si+1, as shown
in fig. 2. the motivation for this architecture is inspired
by the distributional hypothesis: sentences that have similar
surrounding context are likely to be both semantically and
syntactically similar. thus, two sentences that have similar
syntax and semantics are likely to be encoded to a similar
vector. once the model is trained, we can map any sentence
through the encoder to obtain vector representations, then
score their similarity through an inner product.

the learning signal of the model depends on having con-
tiguous text, where sentences follow one another in se-
quence. a natural corpus for training our model is thus
a large collection of books. given the size and diversity
of genres, our bookcorpus allows us to learn very general
representations of text. for instance, table 3 illustrates the
nearest neighbours of query sentences, taken from held out
books that the model was not trained on. these qualitative
results demonstrate that our intuition is correct, with result-
ing nearest neighbors corresponds largely to syntactically
and semantically similar sentences. note that the sentence
embedding is general and can be applied to other domains
not considered in this paper, which is explored in [14].

to construct an encoder, we use a recurrent neural net-
work, inspired by the success of encoder-decoder models
for id4 [10, 2, 1, 31]. two kinds
of id180 have recently gained traction: long
short-term memory (lstm) [9] and the gated recurrent unit
(gru) [3]. both types of activation successfully solve the

vanishing gradient problem, through the use of gates to con-
trol the    ow of information. the lstm unit explicity em-
ploys a cell that acts as a carousel with an identity weight.
the    ow of information through a cell is controlled by in-
put, output and forget gates which control what goes into
a cell, what leaves a cell and whether to reset the contents
of the cell. the gru does not use a cell but employs two
gates: an update and a reset gate. in a gru, the hidden state
is a linear combination of the previous hidden state and the
proposed hidden state, where the combination weights are
controlled by the update gate. grus have been shown to
perform just as well as lstm on several sequence predic-
tion tasks [3] while being simpler. thus, we use gru as the
activation function for our encoder and decoder id56s.

a

given

suppose

sentence

that we

i , . . . , wn

are
tuple
(si   1, si, si+1), and let wt
i denote the t-th word for si
i be its id27. we break the model
and let xt
description into three parts:
the encoder, decoder and
objective function.
encoder. let w1
i denote words in sentence si with
n the number of words in the sentence. the encoder pro-
i at each time step which forms the
duces a hidden state ht
representation of the sequence w1
i. thus, the hid-
den state hn
is the representation of the whole sentence.
i
the gru produces the next hidden state as a linear combi-
nation of the previous hidden state and the proposed state
update (we drop subscript i):

i , . . . , wt

ht = (1     zt) (cid:12) ht   1 + zt (cid:12)   ht

(1)

where   ht is the proposed state update at time t, zt is the up-
date gate and ((cid:12)) denotes a component-wise product. the
update gate takes values between zero and one. in the ex-
treme cases, if the update gate is the vector of ones, the
previous hidden state is completely forgotten and ht =   ht.
alternatively, if the update gate is the zero vector, than the

hidden state from the previous time step is simply copied
over, that is ht = ht   1. the update gate is computed as

zt =   (wzxt + uzht   1)

(2)
where wz and uz are the update gate parameters. the
proposed state update is given by

  ht = tanh(wxt + u(rt (cid:12) ht   1))
where rt is the reset gate, which is computed as

rt =   (wrxt + urht   1)

(3)

(4)

i = hi for sentence si.

if the reset gate is the zero vector, than the proposed state
update is computed only as a function of the current word.
thus after iterating this equation sequence for each word,
we obtain a sentence vector hn
decoder. the decoder computation is analogous to the en-
coder, except that the computation is conditioned on the
sentence vector hi. two separate decoders are used, one
for the previous sentence si   1 and one for the next sentence
si+1. these decoders use different parameters to compute
their hidden states but both share the same vocabulary ma-
trix v that takes a hidden state and computes a distribution
over words. thus, the decoders are analogous to an id56
language model but conditioned on the encoder sequence.
alternatively, in the context of image id134,
the encoded sentence hi plays a similar role as the image.
we describe the decoder for the next sentence si+1 (com-
putation for si   1 is identical). let ht
i+1 denote the hidden
state of the decoder at time t. the update and reset gates for
the decoder are given as follows (we drop i + 1):
zht   1 + czhi)
rht   1 + crhi)

z xt   1 + ud
r xt   1 + ud

zt =   (wd
rt =   (wd

(5)
(6)

the hidden state ht

i+1 is then computed as:

  ht = tanh(wdxt   1 + ud(rt (cid:12) ht   1) + chi) (7)
i+1 = (1     zt) (cid:12) ht   1 + zt (cid:12)   ht
(8)
ht
i+1 given the previ-
given ht
i+1, the id203 of word wt
ous t     1 words and the encoder vector is
i+1, hi)     exp(vwt

i+1|w<t
denotes the row of v corresponding to the
i+1. an analogous computation is performed for

where vwt
word of wt
the previous sentence si   1.
objective. given (si   1, si, si+1), the objective optimized
is the sum of log-probabilities for the next and previous sen-
tences conditioned on the representation of the encoder:

p (wt

i+1)

(9)

ht

i+1

i+1

logp (wt

i+1|w<t

i+1, hi) +

logp (wt

i   1|w<t

i   1, hi)

(cid:88)

t

(cid:88)

t

(10)
the total objective is the above summed over all such train-
ing tuples. adam algorithm [12] is used for optimization.

4.2. visual-semantic embeddings of clips and dvs
the model above describes how to obtain a similarity
score between two sentences, whose representations are
learned from millions of sentences in books. we now dis-
cuss how to obtain similarities between shots and sentences.
our approach closely follows the image-sentence rank-
ing model proposed by [13]. in their model, an lstm is
used for encoding a sentence into a    xed vector. a linear
mapping is applied to image features from a convolutional
network. a score is computed based on the inner product
between the normalized sentence and image vectors. cor-
rect image-sentence pairs are trained to have high score,
while incorrect pairs are assigned low scores.

in our case, we learn a visual-semantic embedding be-
tween movie clips and their dvs description. dvs (   de-
scriptive video service   ) is a service that inserts audio de-
scriptions of the movie between the dialogs in order to en-
able the visually impaired to follow the movie like anyone
else. we used the movie description dataset of [27] for
learning our embedding. this dataset has 94 movies, and
54,000 described clips. we represent each movie clip as a
vector corresponding to mean-pooled features across each
frame in the clip. we used the googlenet architecture [32]
as well as hybrid-id98 [38] for extracting frame features.
for dvs, we pre-processed the descriptions by removing
names and replacing these with a someone token.

the lstm architecture in this work is implemented us-
ing the following equations. as before, we represent a word
embedding at time t of a sentence as xt:

it =   (wxixt + whimt   1 + wcict   1)
(11)
f t =   (wxf xt + whf mt   1 + wcf ct   1) (12)
at = tanh(wxcxt + whcmt   1)
(13)
ct = f t (cid:12) ct   1 + it (cid:12) at
(14)
ot =   (wxoxt + whomt   1 + wcoct)
(15)
mt = ot (cid:12) tanh(ct)
(16)

where (  ) denotes the sigmoid activation function and
((cid:12)) indicates component-wise multiplication. the states
(it, f t, ct, ot, mt) correspond to the input, forget, cell, out-
put and memory vectors, respectively. if the sentence is of
length n, then the vector mn = m is the vector represen-
tation of the sentence.

let q denote a movie clip vector, and let v = wi q
be the embedding of the movie clip. we de   ne a scoring
function s(m, v) = m    v, where m and v are    rst scaled
to have unit norm (making s equivalent to cosine similarity).
we then optimize the following pairwise ranking loss:

max{0,        s(m, v) + s(m, vk)}

(17)

max{0,        s(v, m) + s(v, mk)},

(cid:88)
(cid:88)
(cid:88)
(cid:88)

m

k

v

k

min

  

+

with mk a contrastive (non-descriptive) sentence vector for
a clip embedding v, and vice-versa with vk. we train our
model with stochastic id119 without momentum.
4.3. context aware similarity

we employ the clip-sentence embedding to compute
similarities between each shot in the movie and each sen-
tence in the book. for dialogs, we use several similarity
measures each capturing a different level of semantic sim-
ilarity. we compute id7 [23] between each subtitle and
book sentence to identify nearly identical matches. simi-
larly to [34], we use a tf-idf measure to    nd near duplicates
but weighing down the in   uence of the less frequent words.
finally, we use our sentence embedding learned from books
to score pairs of sentences that are semantically similar but
may have a very different wording (i.e., id141).

these similarity measures indicate the alignment be-
tween the two modalities. however, at the local, sentence
level, alignment can be rather ambiguous. for example, de-
spite being a rather dark book, gone girl contains 15 occur-
rences of the sentence    i love you   . we exploit the fact that
a match is not completely isolated but that the sentences (or
shots) around it are also to some extent similar.

we design a context aware similarity measure that takes
into account all individual similarity measures as well as
a    xed context window in both, the movie and book do-
main, and predicts a new similarity score. we stack a set
of m similarity measures into a tensor s(i, j, m), where i,
j, and m are the indices of sentences in the subtitle, in the
book, and individual similarity measures, respectively. in
particular, we use m = 9 similarities: visual and sentence
embedding, id71-5, tf-idf, and a uniform prior. we want
to predict a combined score score(i, j) = f (s(i, j, m))
at each location (i, j) based on all measurements in a    xed
volume de   ned by i around i, j around j, and 1, . . . , m.
evaluating the function f (  ) at each location (i, j) on a 3-d
tensor s is very similar to applying a convolution using a
kernel of appropriate size. this motivates us to formulate
the function f (  ) as a deep convolutional neural network
(id98). in this paper, we adopt a 3-layer id98 as illustrated
in figure 3. we adopt the relu non-linearity with dropout
to regularize our model. we optimize the cross-id178 loss
over the training set using adam algorithm.
4.4. global movie/book alignment

so far, each shot/sentence was matched independently.
however, most shots in movies and passages in the books
follow a similar timeline. we would like to incorporate this
prior into our alignment. in [34], the authors use dynamic
time warping by enforcing that the shots in the movie can
only match forward in time (to plot synopses in their case).
however, the storyline of the movie and book can have
crossings in time (fig. 8), and the alignment might contain

figure 3: our id98 for context-aware similarity computa-
tion. it has 3 conv. layers and a sigmoid layer on top.

giant leaps forwards or backwards. therefore, we formu-
late a movie/book alignment problem as id136 in a con-
ditional random field that encourages nearby shots/dialog
alignments to be consistent. each node yi in our crf rep-
resents an alignment of the shot in the movie with its cor-
responding subtitle sentence to a sentence in the book. its
state space is thus the set of all sentences in the book. the
crf energy of a con   guration y is formulated as:

    log p(x,y;   ) =

  u  u(yi) +

  p  p(yi, yj)

k(cid:88)

(cid:88)

i=1

j   n (i)

k(cid:88)

i=1

where k is the number of nodes (shots), and n (i) the left
and right neighbor of yi. here,   u(  ) and   p(  ) are unary
and pairwise potentials, respectively, and    = (  u,   p). we
directly use the output of the id98 from 4.3 as the unary
potential   u(  ). for the pairwise potential, we measure the
time span ds(yi, yj) between two neighbouring sentences in
the subtitle and the distance db(yi, yj) of their state space in
the book. one pairwise potential is de   ned as:
(ds(yi, yj)     db(yi, yj))2

  p(yi, yj) =

(ds(yi, yj)     db(yi, yj))2 +   2

(18)

here   2 is a robustness parameter to avoid punishing gi-
ant leaps too harsh. both ds and db are normalized to
[0, 1]. in addition, we also employ another pairwise poten-
tial   q(yi, yj) = (db(yi,yj ))2
(db(yi,yj ))2+  2 to encourage state consis-
tency between nearby nodes. this potential is helpful when
there is a long silence (no dialog) in the movie.
id136. our crf is a chain, thus exact id136 is
possible using id145. we also prune some
states that are very far from the uniform alignment (over
1/3 length of the book) to further speed up computation.
learning. since ground-truth is only available for a
sparse set of shots, we regard the states of unobserved nodes
as hidden variables and learn the crf weights with [29].
5. experimental evaluation

we evaluate our model on our dataset of 11 movie/book
pairs. we train the parameters in our model (id98 and crf)

on gone girl, and test our performance on the remaining
10 movies. in terms of training speed, our video-text model
   watches    1,440 movies per day and our sentence model
reads 870 books per day. we also show various qualitative
results demonstrating the power of our approach. we pro-
vide more results in the appendix of the paper.
5.1. movie/book alignment

evaluating the performance of movie/book alignment is
an interesting problem on its own. this is because our
ground-truth is far from exhaustive     around 200 correspon-
dences were typically found between a movie and its book,
and likely a number of them got missed. thus, evaluating
the precision is rather tricky. we thus focus our evaluation
on recall, similar to existing work on retrieval. for each shot
that has a gt correspondence in book, we check whether
our prediction is close to the annotated one. we evaluate
recall at the paragraph level, i.e., we say that the gt para-
graph was recalled, if our match was at most 3 paragraphs
away, and the shot was at most 5 subtitle sentences away.
as a noisier measure, we also compute recall and precision
at multiple alignment thresholds and report ap (avg. prec.).
the results are presented in table 4. columns show dif-
ferent instantiations of our model: we show the leave-one-
feature-out setting (    indicates that all features were used),
compare how different depths of the context-aware id98 in-
   uence the performance, and compare it to our full model
(crf) in the last column. we get the highest boost by
adding more layers to the id98     recall improves by 14%,
and ap doubles. generally, each feature helps performance.
our sentence embedding (book) helps by 4%, while nois-
ier video-text embedding helps by 2% in recall. crf which
encourages temporal smoothness generally helps (but not
for all movies), bringing additional 2%. we also show how
a uniform timeline performs on its own. that is, for each
shot (measured in seconds) in the movie, we    nd the sen-
tence at the same location (measured in lines) in the book.
we add another baseline to evaluate the role of context
in our model. instead of using our id98 that considers con-
textual information, we build a linear id166 to combine dif-
ferent similarity measures in a single node (shot)     the    nal
similarity is used as a unary potential in our crf alignment
model. the table shows that our id98 contextual model
outperforms the id166 baseline by 30% in recall, and dou-
bles the ap. we plot alignment for a few movies in fig. 8.

running times. we show the typical running time of
each component in our model in table 5. for each movie-
book pair, calculating id7 score takes most of the time.
note that id7 does not contribute signi   cantly to the per-
formance and is of optional use. with respect to the rest,
extracting visual features vis (mean pooling googlenet
features over the shot frames) and scene features (mean
pooling hybrid-id98 features [38] over the shot frames),

movie

books

b
u
l
c

t
h
g
i

f

e
l
i

m
n
e
e
r
g

r
e
t
t
o
p
y
r
r
a
h

.

o
y
s
p
n
a
c
i
r
e
m
a

.
.
.

w
e
l

f
e
n
o

.
.
.

k
n
a
h
s
w
a
h
s

m

r
i
f
e
h
t

.
.
.

k
c
a
b
e
k
o
r
b

d
a
o
r
e
h
t

.
.
.

y
r
t
n
u
o
c
o
n

fight club

green mile

harry potter

american psy.

one flew...

shawshank ...

the firm

brokeback ...

the road

no country...

100.0

100.0

100.0

100.0

100.0

100.0

100.0

100.0

100.0

100.0

.
.
.

y
r
t
n
u
o
c
o
n

.
.
.

w
e
l

f
e
n
o

.
.
.

k
c
a
b
e
k
o
r
b

m

r
i

f
e
h
t

m

r
i

f
e
h
t

m

r
i
f
e
h
t

.
.
.

k
n
a
h
s
w
a
h
s

.
.
.

w
e
l
f
e
n
o

m

r
i
f
e
h
t

d
a
o
r
e
h
t

.
.
.

w
e
l

f
e
n
o

.

y
s
p
n
a
c
i
r
e
m
a

.

y
s
p
n
a
c
i
r
e
m
a

.
.
.

w
e
l

f
e
n
o

r
e
t
t
o
p
y
r
r
a
h

.
.
.

y
r
t
n
u
o
c
o
n

b
u
l
c

t
h
g
i
f

b
u
l
c

t
h
g
i
f

.
.
.

w
e
l
f
e
n
o

.
.
.

k
c
a
b
e
k
o
r
b

45.4

42.5

40.5

55.5

84.0

66.0

75.0

54.8

56.0

49.7

d
a
o
r
e
h
t

.
.
.

k
n
a
h
s
w
a
h
s

d
a
o
r
e
h
t

b
u
l
c

t
h
g
i

f

d
a
o
r
e
h
t

.
.
.

w
e
l
f
e
n
o

.
.
.

k
c
a
b
e
k
o
r
b

.

y
s
p
n
a
c
i
r
e
m
a

.
.
.

y
r
t
n
u
o
c
o
n

.
.
.

w
e
l
f
e
n
o

m

r
i

f
e
h
t

.
.
.

y
r
t
n
u
o
c
o
n

.
.
.

w
e
l

f
e
n
o

.
.
.

k
n
a
h
s
w
a
h
s

.
.
.

k
n
a
h
s
w
a
h
s

d
a
o
r
e
h
t

.
.
.

w
e
l
f
e
n
o

e
l
i

m
n
e
e
r
g

b
u
l
c

t
h
g
i
f

m

r
i
f
e
h
t

45.1

39.6

39.5

53.5

79.1

61.4

73.7

51.9

54.8

46.8

45.2

40.1

39.7

54.9

80.8

62.0

73.9

52.2

55.9

49.5

43.6

38.9

39.1

53.1

79.0

60.9

71.5

50.9

54.1

46.4

.

y
s
p
n
a
c
i
r
e
m
a

m

r
i

f
e
h
t

.
.
.

k
n
a
h
s
w
a
h
s

.
.
.

y
r
t
n
u
o
c
o
n

.
.
.

k
c
a
b
e
k
o
r
b

.
.
.

k
c
a
b
e
k
o
r
b

.

y
s
p
n
a
c
i
r
e
m
a

m

r
i
f
e
h
t

.
.
.

k
n
a
h
s
w
a
h
s

.
.
.

k
n
a
h
s
w
a
h
s

.
.
.

k
n
a
h
s
w
a
h
s

d
a
o
r
e
h
t

m

r
i

f
e
h
t

.
.
.

k
c
a
b
e
k
o
r
b

.
.
.

y
r
t
n
u
o
c
o
n

e
l
i

m
n
e
e
r
g

r
e
t
t
o
p
y
r
r
a
h

.
.
.

k
n
a
h
s
w
a
h
s

e
l
i

m
n
e
e
r
g

r
e
t
t
o
p
y
r
r
a
h

43.0

38.0

39.0

52.6

77.8

59.1

71.4

50.7

53.9

45.8

42.7

36.7

38.7

51.3

76.9

58.0

68.5

50.6

53.4

45.8

table 6: book    retrieval   . for a movie (left), we rank
books wrt to their alignment similarity with the movie. we
normalize similarity to be 100 for the highest scoring book.

takes most of the time (about 80% of the total time).

we also report training times for our contextual model
(id98) and the crf alignment model. note that the times
are reported for one movie/book pair since we used only
one such pair to train all our id98 and crf parameters. we
chose gone girl for training since it had the best balance
between the dialog and visual correspondences.

5.2. describing movies via the book

we next show qualitative results of our alignment.

in
particular, we run our model on each movie/book pair, and
visualize the passage in the book that a particular shot in
the movie aligns to. we show best matching paragraphs as
well as a paragraph before and after. the results are shown
in fig. 8. one can see that our model is able to retrieve a
semantically meaningful match despite large dialog devia-
tions from those in the book, and the challenge of matching
a visual representation to the verbose text in the book.

figure 4: describing movie clips via the book: we align the movie to the book, and show a shot from the movie and its
corresponding paragraph (plus one before and after) from the book.

figure 5: we can use our model to caption movies via a corpus of books. top: a shot from american pyscho is captioned
with paragraphs from the fight club, and a shot from harry potter with paragraphs from fight club. middle and bottom:
we match shots from avatar and batman begins against 300 books from our bookcorpus, and show the best matched
paragraph.

1 layer id98 w/o one feature

fight club

the green mile

harry potter and the
sorcerers stone

american psycho

one flew over the
cuckoo nest
shawshank
redemption

the firm

brokeback mountain

id98-3

tf-idf

scene

uni

id166

ap
recall
ap
recall
ap
recall
ap
recall
ap
recall
ap
recall
ap
recall
ap
recall
ap
recall
ap
recall

1.95
17.92
28.80
74.13
27.17
76.57
34.32
81.92
14.83
49.49
19.33
94.64
18.34
37.93
31.80
98.00
19.80
65.36
28.75
71.69
66.77
22.51
table 4: performance of our model for the movies in our dataset under different settings and metrics.

0.73
10.38
14.05
51.42
10.30
44.35
14.78
34.25
5.68
25.25
8.94
46.43
4.46
18.62
24.91
74.00
13.77
41.90
12.11
33.46
38.01
10.97

0.40
11.79
6.92
53.94
5.66
46.03
12.29
60.82
1.93
32.32
4.35
73.21
2.02
26.90
14.60
86.00
3.04
32.96
8.22
46.69
47.07
5.94

0.50
11.79
13.00
60.57
8.04
49.37
15.68
66.58
9.32
36.36
9.22
75.00
7.25
30.34
15.41
86.00
6.09
42.46
8.63
49.26
50.77
9.31

1.22
2.36
0.00
0.00
0.00
0.00
0.00
0.27
0.00
1.01
0.00
1.79
0.05
1.38
2.36
27.0
0.00
1.12
0.00
1.12
3.88
0.40

prior
0.48
11.79
14.42
62.78
8.20
52.72
16.54
67.67
9.04
40.40
7.86
78.57
7.26
31.03
16.21
87.00
7.00
44.13
9.40
48.53
52.46
9.64

book
0.50
11.79
10.12
57.10
7.84
48.54
14.88
64.66
8.49
36.36
7.99
73.21
6.22
23.45
15.16
86.00
5.11
38.55
9.40
47.79
48.75
8.57

id7
0.41
12.74
14.09
60.57
8.18
52.30
17.22
66.58
6.27
34.34
8.89
76.79
8.66
36.55
17.82
92.00
7.83
48.04
9.39
49.63
52.95
9.88

vis
0.64
12.74
9.83
55.52
7.95
48.54
14.95
63.56
8.51
37.37
8.91
78.57
7.15
26.90
15.58
88.00
5.47
37.99
9.35
51.10
50.03
8.83

   
0.45
12.26
14.12
62.46
8.09
51.05
16.76
67.12
8.14
41.41
8.60
78.57
7.91
33.79
16.55
88.00
6.58
43.02
9.00
48.90
52.66
9.62

ap

the road

no country for old
men

mean recall

crf
5.17
19.81
27.60
78.23
23.65
78.66
32.87
80.27
21.13
54.55
19.96
96.79
20.74
44.83
30.58
100.00
19.58
65.10
30.45
72.79
69.10
23.17

tf

book
3 min

vis
2h

10 min
table 5: running time for our model per one movie/book pair.

0.2 min

3 min

1h

scene

id98 (training)

id98 (id136)

crf (training)

crf (id136)

5h

5 min

per movie-book pair

id7

6h

5.3. book    retrieval   

in this experiment, we compute alignment between a
movie and all (test) 10 books, and check whether our model
retrieves the correct book. results are shown in table 6.
under each book we show the computed similarity. in par-
ticular, we use the energy from the crf, and scale all sim-
ilarities relative to the highest one (100). notice that our
model retrieves the correct book for each movie.

describing a movie via other books. we can also cap-
tion movies by matching shots to paragraphs in a corpus of
books. here we do not encourage a linear timeline (crf)
since the stories are unrelated, and we only match at the lo-
cal, shot-paragraph level. we show a description for amer-
ican psycho borrowed from the book fight club in fig. 5.
5.4. the cocobook: writing stories for coco

our next experiment shows that our model is able to
   generate    descriptive stories for (static) images.
in par-
ticular we used the image-text embedding from [13] and
generated a simple caption for an image. we used this cap-
tion as a query, and used our sentence embedding trained on
books to    nd top 10 nearest sentences (sampled from a few
hundred thousand from bookcorpus). we re-ranked these
based on the 1-gram precision of non-stop words. given the
best result, we return the sentence as well as the 2 sentences
before and after it in the book. the results are in fig. 6. our
sentence embedding is able to retrieve semantically mean-
ingful stories to explain the images.

6. conclusion

in this paper, we explored a new problem of aligning
a book to its movie release. we proposed an approach
that computes several similarities between shots and di-
alogs and the sentences in the book. we exploited our new
sentence embedding in order to compute similarities be-
tween sentences. we further extended the image-text neural
embeddings to video, and proposed a context-aware align-
ment model that takes into account all the available simi-
larity information. we showed results on a new dataset of
movie/book alignments as well as several quantitative re-
sults that showcase the power and potential of our approach.
acknowledgments

we acknowledge the support from nserc, cifar, samsung,
google, and onr-n00014-14-1-0232. we also thank lea jen-
sterle for helping us with elaborate annotation, and relu patrascu
for his help with numerous infrastructure related problems.

appendix

in the appendix we provide more qualitative results.

a. qualitative movie-book alignment results
we show a few qualitative examples of alignment in
fig. 8. in this experiment, we show results obtained with
our full model (crf). for a chosen shot (a node in the crf)
we show the corresponding paragraph in the book.

the club was a little emptier than i would have expected for
the late afternoon , and the bartender , in red waistcoat and
bowtie , was busy wiping down his counter , replacing peanuts
and putting out new coasters . a television with the latest la liga
news was hung in an upper corner , and behind him , rows of
bottles were re   ected in a giant bar mirror . above the stools
, a pergola-type overhead structure held rows of wine glasses
. it was a classy place , with ferns in the corner , and not the
kind of bar to which i was accustomed . my places usually had
a more ... relaxed feel .

he felt like an idiot for yelling at the child , but his
frustration and trepidation was getting the better
of him . he glanced toward the shadowed hall and
quickly nodded toward melissa before making his
way forward . he came across more children sitting
upon a couch in the living room . they watched him ,
but did n   t move and did n   t speak . his skin started to
feel like hundreds of tiny spiders were running up and
down it and he hurried on .

a few miles before tioga road reached highway 395
and the town of lee vining , smith turned onto a
narrow blacktop road . on either side were parched ,
grassy open slopes with barbed-wire fences marking
property lines . cattle and horses grazed under trees
whose black silhouettes stood stark against
the
gold-velvet mountains . marty burst into song :    
home , home on the range , where the deer and the
antelope play ! where seldom is heard a discouraging
word and the skies are not cloudy all day !    

   number seventy-three , second to last from the corner .
   
adam slowed the porsche as he approached the quaint-he could
think of no other word to use , even though    quaint    was one
he normally , manfully , avoided-townhouse , coming to a halt
beside a sleek jaguar sedan .
it was a quiet street , devoid of
traf   c at this hour on a monday night . in the bluish-tinted light
of a corner street lamp , he developed a quick visual impression
of wrought-iron railings on tidy front stoops , window boxes
full of bright chrysanthemums , beveled glass in bay windows ,
and lace curtains . townhouses around here didn   t rent cheaply ,
he could n   t help but observe .

figure 6: cocobook: we generate a caption for a coco image via [13] and retrieve its best matched sentence (+ 2 before
and after) from a large book corpus. one can see a semantic relevance of the retrieved passage to the image.

figure 7: alignment results of our model (bottom) compared to ground-truth alignment (top). in ground-truth, blue lines
indicate visual matches, and magenta are the dialog matches. yellow lines indicate predicted alignments.

we can see that some dialogs in the movies closely fol-
low the book and thus help with the alignment. this is
particularly important since the visual information is not as
strong. since the text around the dialogs typically describe
the scene, the dialogs thus help us ground the visual infor-
mation contained in the description and the video.
b. borrowing    lines    from other books

we show a few qualitative examples of top-scoring
matches for shot in a movie with a paragraph in another
book (a book that does not correspond to this movie).

10 book experiment.

in this experiment, we allow a
clip in our 10 movie dataset (excluding the training movie)
to match to paragraphs in the remaining 9 books (excluding
the corresponding book). the results are in fig. 12. note
that the top-scoring matches chosen from only a small set
of books may not be too meaningful.

200 book experiment. we scale the experiment by ran-
domly selecting 200 books from our bookcorpus. the re-
sults are in fig. 15. one can see that by using many more
books results in increasingly better    stories   .

american psycho

american psycho

harry potter

figure 8: examples of movie-book alignment. we use our model to align a movie to a book. then for a chosen shot (which
is a node in our crf) we show the corresponding paragraph, plus one before and one after, in the book inferred by our model.
on the left we show one (central) frame from the shot along with the subtitle sentence(s) that overlap with the shot. some
dialogs in the movie closely follow the book and thus help with the alignment.

one flew over the cuckoo   s nest

one flew over the cuckoo   s nest

shawshank redemption

figure 9: examples of movie-book alignment. we use our model to align a movie to a book. then for a chosen shot (which
is a node in our crf) we show the corresponding paragraph, plus one before and one after, in the book inferred by our model.
on the left we show one (central) frame from the shot along with the subtitle sentence(s) that overlap with the shot. some
dialogs in the movie closely follow the book and thus help with the alignment.

the firm

the firm

the firm

figure 10: examples of movie-book alignment. we use our model to align a movie to a book. then for a chosen shot
(which is a node in our crf) we show the corresponding paragraph, plus one before and one after, in the book inferred by
our model. on the left we show one (central) frame from the shot along with the subtitle sentence(s) that overlap with the
shot. some dialogs in the movie closely follow the book and thus help with the alignment.

the green mile

the green mile

the road

figure 11: examples of movie-book alignment. we use our model to align a movie to a book. then for a chosen shot
(which is a node in our crf) we show the corresponding paragraph, plus one before and one after, in the book inferred by
our model. on the left we show one (central) frame from the shot along with the subtitle sentence(s) that overlap with the
shot. some dialogs in the movie closely follow the book and thus help with the alignment.

figure 12: examples of of borrowing paragraphs from other books     10 book experiment. we show a few examples
of top-scoring correspondences between a shot in a movie and a paragraph in a book that does not correspond to the movie.
note that by forcing the model to choose from another book, the top-scoring correspondences may still have a relatively low
similarity. in this experiment, we did not enforce a global alignment over the full book     we use the similarity output by our
contextual id98.

figure 13: examples of of borrowing paragraphs from other books     10 book experiment. we show a few examples
of top-scoring correspondences between a shot in a movie and a paragraph in a book that does not correspond to the movie.
note that by forcing the model to choose from another book, the top-scoring correspondences may still have a relatively low
similarity. in this experiment, we did not enforce a global alignment over the full book     we use the similarity output by our
contextual id98.

figure 14: examples of of borrowing paragraphs from other books     200 book experiment. we show a few examples of
top-scoring correspondences between a shot in a movie and a paragraph in a book that does not correspond to the movie. by
scaling up the experiment (more books to choose from), our model gets increasingly more relevant    stories   .

figure 15: examples of of borrowing paragraphs from other books     200 book experiment. we show a few examples of
top-scoring correspondences between a shot in a movie and a paragraph in a book that does not correspond to the movie. by
scaling up the experiment (more books to choose from), our model gets increasingly more relevant    stories   . bottom row:
failed example.

c. the cocobook

we show more results for captioning coco images [18] with passages from the books.

if never
    somewhere you    ll never    nd it ,     owens sneered .
meant    ve seconds , his claim was true .
the little shit    s gaze
cut left , where a laptop sat on a coffee table . trey strode to it .
owens     email program was open .

seriously .
wreck . just something like that . i try to convince her .

its like a train crashing into another train . a train

everyone was allowed to rest for the next twenty-four hours . that
following evening : the elect , not their entourages , were called
to a dining hall for supper with lady dolorous . a table that curved
inward was laden with food and drink . the wall behind the table
was windows with a view of the planet . girls in pink stood about
and at attention .

he had simply ... healed . brian watched his fellow passengers come aboard .
a young woman with blonde hair was walking with a little girl in dark glasses
. the little girl    s hand was on the blonde    s elbow . the woman murmured to
her charge , the girl looked immediately toward the sound of her voice , and
brian understood she was blind - it was something in the gesture of the head .

this was a beautiful miniature reproduction of a real london town house , and when
jessamine touched it , tessa saw that the front of it swung open on tiny hinges . tessa
caught her breath . there were beautiful tiny rooms perfectly decorated with miniature
furniture , everything built to scale , from the little wooden chairs with needlepoint
cushions to the cast-iron stove in the kitchen . there were small dolls , too , with china
heads , and real little oil paintings on the walls .     this was my house .    

if he had been nearby he would have dragged her out of the room
by her hair and strangled her . during lunch break she went with a
group back to the encampment . out of view of the house , under
a stand of towering trees , several tents were sitting in a    eld of
mud . the rain the night before had washed the world , but here it
had made a mess of things . a few women    red up a camp stove
and put on rice and lentils .

then a frightened yell .     hang on !     suddenly , jake was    ying
through the air . nefertiti became airborne , too . he screamed ,
not knowing what was happening-then he splashed into a pool of
water .

grabbing his wristwatch off the bedside table he checked the time
, grimacing when he saw that it was just after two in the afternoon
. jeanne louise should n   t be up yet . sti   ing a yawn , he slid out
of bed and made his way to the en suite bathroom for a shower
twenty minutes later paul was showered , dressed , and had
.
brushed his teeth and hair .
feeling somewhat alive now , he
made his way out of his and jeanne louise    s room , pausing to
look in on livy as he passed .

she cried . quentin put a heavy , warm , calming hand on her
thigh , saying ,     he should be sober by then .     a cell phone rang
. he pulled his from his back pocket , glanced at it , then used the
remote to turn the tv to the channel that showed the feed from the
camera at the security gate .     oh , it    s rachel .    

now however she was out of his shot . he had missed it completely until he had
ended up on the ground with his shotgun . an old clock hung on the wall near the
door . the was obviously broken , the small red hand ticking the same second away
over and over again . morgan squeezed the trigger and pellets ripped out of their
package , bounced down the barrel ,    ew through the air and ripped into the old
clock tearing it in two before it smashed to the ground .

a man sat in a chair , facing the wall opposite of me . it nearly
startled me when i    rst saw him , and made a bit of a squeak , but
he did nothing . he had dark gray hair , a black suit and pants ,
and a gray and blue striped tie . s-sir ? i said .

its been years since we last played together , but as i recall , he
was rather weak at the net . or was it his serving ? all i know
is he plays tennis much better than he plays cricket . perhaps
, mr brearly , frances eventually replied , we should wait until
we actually start playing . then we can ascertain our oppositions
faults , and make a plan based on the new information .

since it was the middle of summer , there were candles in the
   replace instead of a    re . but it still cast a romantic glow over
the room . there were candles on the mantle and on a table set
up in the corner with    owers . as she looked around , her eyes
instinctively turned to    nd max who was behind a bar opening a
bottle of champagne . the doors were closed quietly behind her
and her mouth felt dry as she looked across the room at the man
who had haunted her dreams for so long .

the open doorway of another house provided a view of an ancient
game of tiles .
it wasnt the game that held reddings attention
. it was the four elderly people who sat around a table playing
the game . they were well beyond their productive years and the
canal township had probably been their whole lives . redding and
lin ming stepped away from the doorway right into the path of a
wooden pushcart .

along with the    sh , howard had given them some other picnic
treats that had spoiled ... mushrooms in cream sauce , rotted
greens . the bats and temp were only eating from the river now ,
but the remaining picnic food was running low . there were a few
loaves of stale bread , some cheese , some dried vegetables , and
a couple of cakes . gregor looked over the supplies and thought
about boots wailing for food and water in the jungle . it had been
unbearable .

he felt the    rst stirrings of fear mixing with his anger . a light
   icked on in the room and eric jerked , blinking for a minute
at the brightness before the images focused .
there was a tall
, thin man standing over a mannequin . he looked like he was
assembling it , since its leg was on the ground next to the man
and its arm was in two pieces farther away . then the mannequin
   s head turned .

references
[1] d. bahdanau, k. cho, and y. bengio. neural machine trans-
lation by jointly learning to align and translate. iclr, 2015.
4

[2] k. cho, b. van merrienboer, c. gulcehre, f. bougares,
h. schwenk, and y. bengio. learning phrase representations
using id56 encoder-decoder for id151.
emnlp, 2014. 4

[3] j. chung, c. gulcehre, k. cho, and y. bengio. empirical
evaluation of gated recurrent neural networks on sequence
modeling. arxiv preprint arxiv:1412.3555, 2014. 4

[4] t. cour, c.

jordan, e. miltsakaki,

and b. taskar.
movie/script: alignment and parsing of video and text tran-
scription. in eccv, 2008. 2

[5] m. everingham, j. sivic, and a. zisserman.    hello! my
name is... buffy        automatic naming of characters in tv
video. bmvc, pages 899   908, 2006. 2

[6] a. farhadi, m. hejrati, m. sadeghi, p. young, c. rashtchian,
j. hockenmaier, and d. forsyth. every picture tells a story:
generating sentences for images. in eccv, 2010. 2

[7] s. fidler, a. sharma, and r. urtasun. a sentence is worth a

thousand pixels. in cvpr, 2013. 2

[8] a. gupta and l. davis. beyond nouns: exploiting prepo-
sitions and comparative adjectives for learning visual classi-
   ers. in eccv, 2008. 1

[9] s. hochreiter and j. schmidhuber. long short-term memory.

neural computation, 9(8):1735   1780, 1997. 4

[10] n. kalchbrenner and p. blunsom. recurrent continuous

translation models. in emnlp, pages 1700   1709, 2013. 4

[11] a. karpathy and l. fei-fei. deep visual-semantic align-
in cvpr, 2015.

ments for generating image descriptions.
1, 2

[12] d. kingma and j. ba. adam: a method for stochastic opti-

mization. arxiv preprint arxiv:1412.6980, 2014. 5

[13] r. kiros, r. salakhutdinov, and r. s. zemel. unifying
visual-semantic embeddings with multimodal neural lan-
guage models. corr, abs/1411.2539, 2014. 1, 2, 3, 5, 9,
10

[14] r. kiros, y. zhu, r. salakhutdinov, r. s. zemel, a. torralba,
r. urtasun, and s. fidler. skip-thought vectors. in arxiv,
2015. 3, 4

[15] c. kong, d. lin, m. bansal, r. urtasun, and s. fidler. what
are you talking about? text-to-image coreference. in cvpr,
2014. 1, 2

[16] g. kulkarni, v. premraj, s. dhar, s. li, y. choi, a. berg, and
t. berg. baby talk: understanding and generating simple
image descriptions. in cvpr, 2011. 2

[17] d. lin, s. fidler, c. kong, and r. urtasun. visual seman-
tic search: retrieving videos via complex textual queries.
cvpr, pages 2657   2664, 2014. 1, 2

[18] t.-y. lin, m. maire, s. belongie, j. hays, p. perona, d. ra-
manan, p. doll  ar, and c. l. zitnick. microsoft coco: com-
mon objects in context. in eccv, pages 740   755. 2014. 1,
19

[19] x. lin and d. parikh. don   t just listen, use your imagination:
in

leveraging visual common sense for non-visual tasks.
cvpr, 2015. 1

[20] m. malinowski and m. fritz. a multi-world approach to
id53 about real-world scenes based on uncer-
tain input. in nips, 2014. 1

[21] j. mao, w. xu, y. yang, j. wang, and a. l. yuille. ex-
plain images with multimodal recurrent neural networks. in
arxiv:1410.1090, 2014. 1, 2

[22] t. mikolov, k. chen, g. corrado, and j. dean. ef   cient
estimation of word representations in vector space. arxiv
preprint arxiv:1301.3781, 2013. 4

[23] k. papineni, s. roukos, t. ward, and w. j. zhu. id7: a
method for automatic evaluation of machine translation. in
acl, pages 311   318, 2002. 6

[24] h. pirsiavash, c. vondrick, and a. torralba.

why in images. arxiv.org, jun 2014. 2

inferring the

[25] v. ramanathan, a. joulin, p. liang, and l. fei-fei. link-
ing people in videos with    their    names using coreference
resolution. in eccv, pages 95   110. 2014. 2

[26] v. ramanathan, p. liang, and l. fei-fei. video event under-
standing using natural language descriptions. in iccv, 2013.
1

[27] a. rohrbach, m. rohrbach, n. tandon, and b. schiele. a

dataset for movie description. in cvpr, 2015. 2, 5

[28] p. sankar, c. v. jawahar, and a. zisserman. subtitle-free

movie to script alignment. in bmvc, 2009. 2

[29] a. schwing, t. hazan, m. pollefeys, and r. urtasun. ef   -
cient id170 with latent variables for general
id114. in icml, 2012. 6

[30] j. sivic, m. everingham, and a. zisserman.    who are you?   
- learning person speci   c classi   ers from video. cvpr,
pages 1145   1152, 2009. 2

[31] i. sutskever, o. vinyals, and q. v. le. sequence to sequence

learning with neural networks. in nips, 2014. 4

[32] c. szegedy, w. liu, y. jia, p. sermanet, s. reed,
d. anguelov, d. erhan, v. vanhoucke, and a. rabi-
novich. going deeper with convolutions. arxiv preprint
arxiv:1409.4842, 2014. 5

[33] m. tapaswi, m. bauml, and r. stiefelhagen. book2movie:
aligning video scenes with book chapters. in cvpr, 2015.
2

[34] m. tapaswi, m. buml, and r. stiefelhagen. aligning plot
synopses to videos for story-based retrieval. ijmir, 4:3   
16, 2015. 1, 2, 6

[35] s. venugopalan, h. xu, j. donahue, m. rohrbach, r. j.
mooney, and k. saenko. translating videos to natural
language using deep recurrent neural networks. corr
abs/1312.6229, cs.cv, 2014. 1, 2

[36] o. vinyals, a. toshev, s. bengio, and d. erhan. show and
tell: a neural image caption generator. in arxiv:1411.4555,
2014. 1, 2

[37] k. xu, j. ba, r. kiros, k. cho, a. courville, r. salakhut-
dinov, r. zemel, and y. bengio. show, attend and tell:
neural image id134 with visual attention.
in
arxiv:1502.03044, 2015. 2

[38] b. zhou, a. lapedriza, j. xiao, a. torralba, and a. oliva.
learning deep features for scene recognition using places
database. in nips, 2014. 5, 7

