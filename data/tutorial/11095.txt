markov logic networks for natural language id53

tushar khot

allen institute for ai
seattle, wa 98103

niranjan balasubramanian

dept. of computer science

stony brook university
stony brook, ny 11794

eric gribkoff

computer sci. and engr.
university of washington

seattle, wa 98195

ashish sabharwal

peter clark
oren etzioni

allen institute for ai
seattle, wa 98103

5
1
0
2

 
l
u
j
 

0
1

 
 
]
i

a
.
s
c
[
 
 

1
v
5
4
0
3
0

.

7
0
5
1
:
v
i
x
r
a

abstract

our goal is to answer elementary-level science questions
using knowledge extracted automatically from science text-
books, expressed in a subset of    rst-order logic. given the
incomplete and noisy nature of these automatically extracted
rules, markov logic networks (mlns) seem a natural model
to use, but the exact way of leveraging mlns is by no means
obvious. we investigate three ways of applying mlns to our
task. in the    rst, we simply use the extracted science rules di-
rectly as mln clauses. unlike typical mln applications, our
domain has long and complex rules, leading to an unmanage-
able number of groundings. we exploit the structure present
in hard constraints to improve tractability, but the formulation
remains ineffective. in the second approach, we instead in-
terpret science rules as describing prototypical entities, thus
mapping rules directly to grounded mln assertions, whose
constants are then clustered using existing entity resolution
methods. this drastically simpli   es the network, but still suf-
fers from brittleness. finally, our third approach, called pra-
line, uses mlns to align the lexical elements as well as de   ne
and control how id136 should be performed in this task.
our experiments, demonstrating a 15% accuracy boost and a
10x reduction in runtime, suggest that the    exibility and dif-
ferent id136 semantics of praline are a better    t for the
natural language id53 task.

introduction

many id53 or qa tasks require the ability to
reason with knowledge extracted from text. we consider the
problem of answering questions in standardized science ex-
ams (clark, harrison, and balasubramanian 2013). in partic-
ular, we focus on a subset that tests students    understanding
of various kinds of general rules and principles (e.g., grav-
ity pulls objects towards the earth) and their ability to apply
these rules to reason about speci   c situations or scenarios
(e.g., which force is responsible for a ball to drop?).

this task can be viewed as a natural    rst-order reasoning
problem speci   ed over general truths expressed over classes
of events or entities. however, this knowledge is automati-
cally derived from appropriate science texts.

in order to effectively reason over knowledge derived
from text, a qa system must handle incomplete and po-
tentially noisy knowledge, and reason under uncertainty.
markov logic network (mln) is a formal probabilistic in-
ference framework that allows for robust id136 using
rules expressed in probabilistic    rst-order logic (richardson
and domingos 2006). mlns have been widely adopted for

many tasks (singla and domingos 2006a; kok and domin-
gos 2008; poon and domingos 2009). recently, beltagy et
al. (2013; 2014) have shown that mlns can be used to rea-
son with rules derived from natural language.

while mlns appear a natural    t, it is a priori unclear
how to effectively formulate the qa task. moreover, the
unique characteristics of this domain pose new challenges
in grounding and ability to control id136 under incom-
plete evidence. we investigate two standard formulations,
uncover ef   ciency and brittleness issues, and propose an
enhanced formulation more suitable for this domain. this
enhanced formulation, called praline, signi   cantly outper-
forms our other mln formulations, reducing runtime by 10x
and improving accuracy by 15%.

setup: id53 task

following clark et al. (2014), we formulate qa as a rea-
soning task over knowledge derived from textual sources.
speci   cally, a multiple choice question with k answer op-
tions is turned into k true-false questions, each of which as-
serts some known facts, referred to as the setup, and posits a
query. the reasoning task is to determine whether the query
is true given the setup and the input knowledge.

the input knowledge is derived from 4th-grade level sci-
ence texts and augmented with a web search for terms ap-
pearing in the texts. much of this knowledge is in terms of
generalities, expressed naturally as if-then rules. we use
the representation and extraction procedures of clark et al.
(2014), recapitulated brie   y here for completeness.

rule representation: the generalities in text con-
vey information about classes of entities and events. fol-
lowing the neo-davidsonian rei   ed representation (cur-
ran, clark, and bos 2007), we encode information about
falling, dropping etc.) and entities (e.g.,
events (e.g,
ball,
such as
agent, cause, function, towards, in etc., de   ne semantic
relationships between the variables. rather than committing
to a type ontology, the variables are associated with their
original string representation using an isa predicate.

stone etc.) using variables. predicates

the    if    or antecedent part of the rule is semantically in-
terpreted as being universally quanti   ed (omitted below for
conciseness) whereas every entity or event mentioned only
in the    then    or consequent part of the rule is treated as
existentially quanti   ed. both antecedent and consequent
are interpreted as conjunctions. e.g.,    growing thicker fur

in winter helps some animals to stay warm    translates into:
isa(g, grow), isa(a, some animals), isa(f, thicker fur),
isa(w, the winter), agent(g, a), object(g, f ), in(g, w)
       s, r : isa(s, stays), isa(r, warm),

enables(g, s), agent(s, a), object(s, r)

(1)
question representation: the question representation is
computed similarly except that we use    xed constants (rep-
resented as block letters) rather than variables. e.g., consider
the question:    a fox grows thick fur as the season changes.
this helps the fox to (a) hide from danger (b) attract a mate
(c)    nd food (d) keep warm?    the t/f question correspond-
ing to option (d) translates into:
setup :isa(f, fox), isa(g, grows), isa(t, thick fur),

agent(g, f ), object(g, t )

query :isa(k, keep warm), enables(g, k), agent(k, f )
lexical reasoning: since entities and event variables
hold textual values, reasoning must accommodate the lex-
ical variability and id123. for example, the sur-
face forms    thick fur    in the question and    thicker fur   
are semantically equivalent. also, the string    fox    entails
   some animal   . we use a lexical reasoning component
based on id123 to establish lexical equivalence
or entailment between variables.

most likely answer as id136: given the input kb
rules and the question, we formulate a probabilistic rea-
soning problem by adding lexical reasoning probabilities
and incorporating uncertainties in derived rules. speci   -
cally, given setup facts s and k answer options qi, we seek
|
the most likely answer option: arg maxi   {1,...,k} pr[qi
s, kb ]. this is a partial map computation, in general #p-
hard (park 2002). hence methods such as integer linear
programming are not directly applicable.
challenges
reasoning with text-derived knowledge presents, in addition
to lexical uncertainty, challenges that expose the brittleness
and rigidity inherent in pure logic-based frameworks. in par-
ticular, text-derived rules are incomplete and include lexical
items as logical elements, making rule application in a pure
logical setting extremely brittle: many relevant rules cannot
be applied because their pre-conditions are not fully satis-
   ed due to poor alignment. for example, naive application
of rule (1) on setup would not conclude query as the rule
requires    in the winter    to be true. a robust id136 mech-
anism must allow for rule application with partial evidence.
further, a single text-derived rule may be insuf   cient to
answer a question. e.g.,    animals grow thick fur in winter   
and    thick fur helps keep warm    may need to be chained.

probabilistic formulations

statistical relational learning (srl) models (getoor and
taskar 2007) are a natural    t for qa. they provide prob-
abilistic reasoning over knowledge represented in    rst-
order logic, thereby handling uncertainty in lexical reason-
ing and incomplete matching. while there are many srl
formalisms including stochastic logic programs (slps)
(muggleton 1996), problog (raedt, kimmig, and toivo-
nen 2007), prism (sato and kameya 2001), etc., we use

markov logic networks (mlns) for their ease of speci   -
cation and their ability to naturally handle potentially cyclic
rules. we explore three formulations:

a) first-order mln: given a question and relevant    rst-
order kb rules, we convert them into an mln program
and let mln id136 automatically handle rule chaining.
while a natural    rst-order formulation of the qa task, this
struggles with long conjunctions and existentials in rules,
as well as relatively few atoms and little to no symmetries.
this results in massive grounding sizes, not remedied easily
by existing solutions such as lazy, lifted, or structured infer-
ence. we exploit the structure imposed by hard constraints
to vastly simplify groundings and bring them to the realm of
feasibility, but performance remains poor.

b) entity resolution mln: instead of reasoning with
rules that express generalities over classes of individuals,
we replace the variables in the previous formulation with
prototypical constants. this reduces the number of ground-
ings, while retaining the crux of the reasoning problem de-
   ned over generalities. combining this idea with existing
entity resolution approaches substantially improves scala-
bility. however, this turns out to be too brittle in handling
lexical mismatches, e.g., different sentence parse structures.
c) praline mln: both of the above mlns rely on ex-
actly matching the relations in the kb and question repre-
sentation, making them too brittle for this task. in response,
probabilistic alignment and id136 (praline) performs
id136 using primarily the string constants but guided by
the edge structure. we relax the rigidity in rule application
by explicitly modeling the desired qa id136 behavior.
(a) first-order mln formulation
for a set r of    rst-order kb rules, arguably the most natu-
ral way to represent the qa task of computing pr[qi | s, r]
as an mln program m is to simply add r essentially ver-
batim as    rst-order rules in m. quanti   ed variables of m
are those occurring in r. constants of m are the string rep-
resentations (e.g.,    fox   ,    thicker fur   ) in qi, s, and r, as
well as the constants in the qi and s (e.g., f , t ). in addi-
tion, for all existentially quanti   ed variables, we introduce
a new domain constant. predicates of m are those in r,
along with a binary entails predicate representing the lexi-
cal entailment blackbox, which allows mto probabilistically
connect lexically related constants such as    thick fur    and
   thicker fur    or    fox    and    some animals   . entails is de-
   ned to be closed-world and is not necessarily transitive.

re   ned types: for improved semantics and reduced
grounding size, m has entities (a), events (e), and strings
as three basic types, and predicates of m are appropriately
typed (e.g., agentea, entailsee, entailsaa). further, we
avoid irrelevant groundings by using re   ned types deter-
mined dynamically: if r(x, y) appears only with constants
associated with strings t as the second argument, then m
contains r(x, y)    !isa(y, s) for all strings s with a zero en-
tailment score with all strings in t .

evidence: soft evidence for m consists of entails re-
lations between every ordered pair of entity (or event)
strings, e.g., entails(fox, some animals). hard evidence for
m comprises grounded atoms in s.

query: the query atom in m is result(), a new zero-arity
predicate result() that is made equivalent to the conjunction

of the predicates in qi that have not been included in the ev-
idence. we are interested in computing pr[result() = true].
semantic rules: in addition to kb science rules, we
add semantic rules that capture the intended meaning of
our predicates, such as every event has a unique agent,
cause(x, y)     e   ect(y, x), etc. semantic predicates also
enforce natural restrictions such as non-re   exivity, !r(x, x),
and anti-symmetry, r(x, y)    !r(y, x).

finally, to help bridge lexical gaps more, we use a simple
external lexical alignment algorithm to estimate how much
does the setup entail antecedent r of each kb rule r, and
how much does consequent r entail query. these are then
added as two additional mln rules per kb rule.

these rules have the following    rst-order logic form:
   x1, .., xk

ri(xi1 , xi2 )        xk+1, .., xk+m

rj(xj1 , xj2 )

(cid:94)

(cid:94)

i

j

existentials spanning conjunctions in the consequent of this
rule form can neither be directly fed into existing mln sys-
tems nor ef   ciently expanded naively into a standard con-
junctive normal form (cnf) without incurring an exponen-
tial blowup during the transformation. to address this, we in-
troduce a new    existential    predicate e  
j (x1, . . . , xk, xk+j)
for each existential variable xk+j in each such rule   . this
predicate becomes the consequent of   , and subsequent hard
mln rules make it equivant to the original consequent.
boosting id136 ef   ciency. a bottleneck in using
mln solvers out-of-the-box for this qa formulation is the
prohibitively large grounded network size. for instance,
31% of our runs that timed out during the mln grounding
phase after 6 minutes were dealing, on average, with 1.4   
106 ground clauses. such behavior has also been observed,
perhaps to a lesser degree, in related nlp tasks (beltagy and
mooney 2014; beltagy, erk, and mooney 2014).

large grounding size is, of course, a well-studied prob-
lem in the mln literature. however, a key difference from
previously studied mlns is that our qa encodings have
small domain sizes and, therefore, very few ground atoms
to start with. existing techniques for addressing the ground-
ing challenge were inspired by the unmanageable number
of ground atoms often seen in traditional mln applica-
tions, and work by grouping them into large classes of in-
terchangeable atoms (de salvo braz, amir, and roth 2005;
gogate and domingos 2011; venugopal and gogate 2012;
domingos and webb 2012; niepert and van den broeck
2014). similarly, memory-ef   cient lazy id136 (singla
and domingos 2006b) and frog (shavlik and natarajan
2009) focus only on relevant atoms.
these methods were ineffective on our mlns. e.g., lazy
id136 in alchemy-1 reduced    70k ground clauses to
   56k on a question, while our method, described next,
brought it down to only 951 clauses. further, lifted blocked
gibbs and probabilistic theorem proving, as implemented
in alchemy-2, were slower than basic alchemy-1.

different from heuristic approximations (e.g., modi   ed
closed-world assumption of beltagy and mooney (2014)),
we propose reducing the grounding size without altering the
semantics of the mln program. we utilize the combina-
torial structure imposed by the set h of hard constraints
present in the mln, and use it to simplify the grounding

of both hard and soft constraints. lazy id136 mentioned
above may be viewed as the very    rst step of our approach.
assuming an arbitrary ordering of the constraints in h,
let fi denote the    rst i constraints. starting with i = 1, we
generate the propositional grounding gi of fi, use a propo-
sitional satis   ability (sat) solver to identify the set bi of
backbone variables of gi (i.e., variables that take a    xed
value in all solutions to gi), freeze values of the correspond-
ing atoms in bi, increment i, and repeat until g|h| has been
processed. although the end result can be described simply
as freezing atoms corresponding to the backbone variables in
the grounding of h, the incremental process helps us keep
the intermediate grounding size under control as a proposi-
tional variable is no longer generated for an atom once its
value is frozen. once the freezing process is complete, the
full grounding of h is further simpli   ed by removing frozen
variables. finally, the soft constraints s are grounded much
more ef   ciently by taking frozen atoms into account.

this approach can be seen as an extension of a proposal
by papai, singla, and kautz (2011). they used a polynomial-
time generalized arc consistency algorithm on h to com-
pute a subset of b|h| that is ef   ciently identi   able, imple-
mented as a sequence of join and project database operations

(b) entity resolution based mln
representing generalities as quanti   ed rules de   ned over
classes of entities or events appears to be a natural formu-
lation, but is also quite inef   cient leading to large grounded
networks. we instead consider an alternative formulation
that treats generalities as relations expressed over proto-
typical entities and events. this formulation leverages the
fact that elementary level science questions can often be an-
swered using relatively simple logical reasoning over exem-
plar objects and homogeneous classes of objects, if given
perfect information as input. the only uncertainty present in
our system is what   s introduced by lexical variations and ex-
traction errors, which we handle with probabilistic equality.
kb rules and question: we create rules de   ned over
prototypical entity/event constants, rather than    rst-order
variables. these constants are tied to their respective string
representations, with the understanding that two entities be-
have similarly if they have lexically similar strings. e.g.,

isa(g, grow), isa(a, some animals), isa(f, thicker fur),
isa(w, the winter), agent(g, a), object(g, f ), in(g, w )
    isa(s, stays), isa(r, warm),

enables(g, s), agent(s, a), object(s, r)

what was a    rst-order rule in fo-mln is now already fully
grounded! it has no variables. entities/events mentioned in
the question are also similarly represented by constants.

equivalence or resolution rules: using a simple prob-
abilistic variant of existing entity/event resolution frame-
works (singla and domingos 2006a; kok and domingos
2008), we ensure that (a) two entities/events are considered
similar when they are tied to lexically similar strings and (b)
similar entities/events participate in similar relations w.r.t.
other entities/events. this de   nes soft clusters or equiva-
lence classes of entities/events. to this end, we use a proba-
bilistic sameas predicate which is re   exive, symmetric, and

transitive, and interacts with the rest of the mln as follows:

w(s, s(cid:48)) : entails(s, s(cid:48))
isa(x, s), entails(s, s(cid:48))     isa(x, s(cid:48)).

isa(x, s), isa(y, s)     sameas(x, y).
w : isa(x, s), !isa(y, s)     !sameas(x, y)
r(x, y), sameas(y, z)     r(x, z).

r in the last rule refers to any of the mln predicates other
than entails and isa. the sameas predicate, as before, is
implemented in a typed fashion, separately for entities and
events. we will refer to this formulation as er-mln.

partial match rules due to lexical variability, often not
all conjuncts in a rule   s antecedent are present in the ques-
tion   s setup. to handle incomplete matches, for each kb
derived mln rule of the form (   k
i=1li)     r, we also add
k soft rules of the form li     r. this adds    exibility, by
helping       re    the rule in a soft manner.
comparison with fo-mln: long kb rules and ques-
tion representation now no longer have quanti   ed variables,
only the binary or ternary rules above do. these mention at
most 3 variables each and thus have relatively manageable
groundings. on the other hand, as discussed in the next sec-
tion, er-mln can fail on questions that have distinct enti-
ties with similar string representations. further, it is brittle to
syntactic differences such as agent(fall, things) generated
by    things fall due to gravity    and object(dropped, ball) for
   a student dropped a ball   . although    drop    entails    fall   
and    ball    entails    object   , er-mln cannot reliably bridge
the structural difference involving object and agent, as these
two relationships typically aren   t equivalent. despite these
limitations, er-mln provides a substantial scalability ad-
vantage over fo-mln on a vast majority of the questions
that remain within its scope.

(c) probabilistic alignment and id136
er-mln handles some of the word-level lexical variation
via resolution and soft partial match rules that break long
antecedents. however, it is still rigid in two respects:
1. id136 primarily relies on the predicates (also referred
to as links or edges) for id136. as a result, even if
the words in the antecedent and setup have high entail-
ment scores, the rule will still not       re    if the edges do
not match. to enable effective rule application under such
circumstances, we require (at a minimum) some match on
the string constants and allow edge matches (if any) to in-
crease the con   dence in rule application.

2. as id91 forces entities bound to lexically equivalent
strings to    behave    identically, it fails on questions that
involve two different entities that are bound to equivalent
string representations. to avoid this issue, we do not force
the entailment-based clusters of constants to behave sim-
ilarly. instead, as we discuss below, we use the clusters to
guide id136 in a softer manner.
to introduce such    exibility, we convert the problem of
uncertainty over links between string constants to the prob-
lem of uncertainty over the existence of these constants. to
this end, we introduce a unary predicate over string constants
to capture what is known to be true (i.e., the setup) or can

be proven to be true (via id136 using the kb rules) in the
world. we then de   ne an mln to directly control how new
facts are inferred given the kb rules. the    exibility to con-
trol id136 helps address two additional qa challenges:
acyclic id136: while knowledge is extracted from
text as a set of directed rules each with an antecedent and
a consequent, there is no guarantee that the rules taken to-
gether are acyclic. e.g., a rule stating    living things     de-
pend on the sun    and    sun     source of energy for living
things    may exist side-by-side. successful id136 for qa
must avoid feedback loops.

false unless proven: while mlns assume atoms not
mentioned in any rule to be true with id203 0.5, ele-
mentary level science reasoning is better re   ected in a sys-
tem that assumes all atoms to be false unless stated in the
question or proven through the application of a rule. this
is similar to the semantics of problog (raedt, kimmig, and
toivonen 2007) and prism (sato and kameya 2001).

mln speci   cation we introduce a unary predicate called
holds over string constants to capture the id203 of a
string constant being true given the setup is true (   x    
setup, holds(x) = true) and the kb rules hold. instead of
using edges for id136, we use them as factors in   uenc-
ing alignment: similar constants have similar local neighbor-
hoods. this reduces the number of unobserved groundings
from o(n2) edges in the er-mln to o(n) existence pred-
icates, where n is the number of string constants. for the
example rule (1), praline can be viewed as using the follow-
ing rule for id136:

holds(grow ), holds(animals), holds(fur ),
holds(winter )     holds(stays), holds(warm)

predicates

if we view kb rules and the question as a labeled graph g
shown in figure 1, alignment between string constants cor-
responds to alignment between the nodes in g. the nodes
and edges of g are the input to the mln, and the holds pred-
icate on each node captures the id203 of it being true
given the setup. we now use mlns (as described below) to
de   ne the id136 procedure for any such input graph g.
the graph struc-
ture
and
edge(nodeid , nodeid , label ). we use setup(nodeid ) and
query(nodeid ) to represent the question   s setup and query,
resp. similarly, we use inlhs(nodeid ) and inrhs(nodeid )
to represent rules    antecedent and consequent, resp.

input predicates: we represent

of g using

node(nodeid)

graph alignment rules: similar to the previous ap-
proaches, we use entailment scores between words and short
phrases to compute the alignment. in addition, we also ex-
pect aligned nodes to have similar edge structures:
aligns(x, y), edge(x, u, r), edge(y, v, s)     aligns(u, v)
aligns(u, v), edge(x, u, r), edge(y, v, s)     aligns(x, y)
that is, if node x aligns with y then their children/ancestors
should also align. we create copies of these rules for edges
with the same label, r = s, with a higher weight and for
edges with different labels, r (cid:54)= s, with a lower weight.

id136 rules: we use mlns to de   ne the id136
procedure to prove the query using the alignments from
aligns. we assume that any node y that aligns with a node
x that holds, also holds:

holds(x), aligns(x, y)     holds(y)

(2)

figure 1: kb rule and ques-
tion as a graph. setup is blue,
query is green, antecedent
is
orange, and consequent is pur-
ple. alignments are shown with
dotted lines. lhsholds combines
the individual probabilities of
antecedent nodes.

empirical evaluation

we used tuffy 0.43 (niu et al. 2011) as the base mln
solver4 and extended it to incorporate the hard-constraint
based grounding reduction technique discussed earlier, im-
plemented using the sat solver glucose 3.05 (audemard
and simon 2009) exploiting its    solving under assumptions   
capability for ef   ciency. we used a 10 minute timelimit, in-
cluding a max of 6 minutes for grounding. marginal infer-
ence was performed using mc-sat (poon and domingos
2006), with default parameters and 5000    ips per sample to
generate 500 samples for marginal estimation.

we used a 2-core 2.5 ghz amazon ec2 linux machine
with 16 gb ram. we selected 108 elementary-level science
questions (non-diagram, multiple-choice) from 4th grade
new york regents exam as our benchmark (dev-108) and
used another 68 questions as a blind test set (unseen-68).6

the kb, representing roughly 47,000 sentences, was gen-
erated in advance by processing the new york regents 4th
grade science exam syllabus, the corresponding barron   s
study guide, and documents obtained by querying the in-
ternet for relevant terms. given a question, we use a simple
word-overlap based matching algorithm, referred to as the
rule selector, to retrieve the top 30 matching sentences to be
considered for the question. id123 scores be-
tween words and short phrases were computed using word-
net (miller 1995), and converted to    desired    probabilities
for the corresponding soft entails evidence. the accuracy
reported for each approach is computed as the number of
multiple-choice questions it answers correctly, with a par-
tial credit of 1/k in case of a k-way tie between the highest
scoring options if they include the correct answer.

mln formulation comparison
table 1 compares the effectiveness of our three mln formu-
lations, named fo-mln, er-mln, and praline. for each
question and each approach, an mln program was gener-
ated for each of the answer options using the most promising
kb rule for that answer option.
in the case of fo-mln, tuffy exceeded the 6 minute
time limit when generating groundings for 34 of the 108   4
mlns for the dev-108 question set, quitting after working
with 1.4   106 clauses on average, despite starting with only

3http://i.stanford.edu/hazy/tuffy
4we also tried alchemy 1.0, which gave similar results.
5http://www.labri.fr/perso/lsimon/glucose
6question sets, mlns, and our modi   ed tuffy solver are avail-

able at http://allenai.org/software.html

for example, if the setup mentions    fox   , all nodes that
entail    fox    also hold. as we also use the edge structure dur-
ing alignment, we would have a lower id203 of    fox   
in    fox    nds food    to align with    animal    in    animal grows
fur    as compared to    animal    in    animal    nds food   .

we use kb rules to further infer new facts that should
hold based on the rule structure. we compute lhsholds, the
id203 of the rule   s antecedent holding, and use it to
infer rhsholds, the id203 of the consequent. similar
to er-mln, we break the rule into multiple small rules.1

w :holds(x), inlhs(x, r)     lhsholds(r)
w :!holds(x), inlhs(x, r)    !lhsholds(r)

lhsholds(r)     rhsholds(r).
rhsholds(r), inrhs(r, x)     holds(x).
id136: we

two

use

acyclic
predicates,
to
proves(nodeid , nodeid ) and ruleproves(rule, rule)
capture the id136 chain between nodes and rules, resp.
we can now ensure acyclicity in id136 by introducing
transitive clauses over these predicates and disallowing
re   exivity, i.e., !proves(x, x). we update rule (2) to:

w :proves(x, y), holds(x)     holds(y)
w :aligns(x, y)     proves(x, y)

we capture the direction of id136 between rules by

checking consequent and antecedent alignments:
proves(x, y), inrhs(x, r1), inlhs(y, r2)     ruleproves(r1, r2).
false unless proven: to ensure that nodes hold only if they
can be proven from setup, we add bidirectional implications
to our rules. an alternative is to introduce a strong negative
prior on holds and have a higher positive weight on all other
clauses that conclude holds. however, the performance of
our mlns was very sensitive to the choice of the weight.
we instead model this constraint explicitly. figure 1 shows
a sample id136 chain using dotted lines.

praline de   nes a meta-id136 procedure that is easily
modi   able to enforce desired qa id136 behavior, e.g.
w : aligns(x, y), setup(x)    !query(y) would prevent a
term from the setup to align with the query. further, by
representing the input kb and question as evidence, we can
de   ne a single static    rst-order mln for all the questions in-
stead of a compiled mln for every question. this opens up
the possibility of learning weights of this static mln, which
would be challenging for the previous two approaches.2
1an intuitive alternative for the 2nd clause doesn   t capture the
intending meaning,    w :!holds(x), inlhs(x, r)     lhsholds(r)

2in this work, we have set the weights manually.

living thingsdependenergythe sunagentonfromlifedependsenergythe sunagentonfromearthonrhs_holdslhs_holdstable 1: qa performance of various mln formulations. the number of mln rules, number of ground clauses, and runtime per
multiple-choice question are averages over the corresponding dataset. #answered column indicates questions where at least one
answer option didn   t time out (left) and where no answer option timed out (right). #atoms and #groundclauses for fo-mln
are averaged over the 398 mlns where grounding    nished; 34 remaining mlns timed out after processing 1.4m clauses.

question

set

dev-108

unseen-68

mln

formulation
fo-mln
er-mln
praline
fo-mln
er-mln
praline

#answered
(some / all)

106 / 82
107 / 107

108
66
68
68

exam
score
33.6%
34.5%
48.8%
33.8%
31.3%
46.3%

#mln
rules

35
41
51
-
-
-

#atoms

384   
284
182
-
-
-

#ground
clauses
524   
2,308
219
-
-
-

runtime

(all)
280 s
188 s
17 s
288 s
226 s
17 s

table 2: qa performance of praline mln variations.

mln

praline
no acyclic
no fup
no fup, acyclic

one rule

chain=2

dev-108 unseen dev-108 unseen
50.3% 52.7%
48.8%
44.7%
43.6%
30.9%
29.4%
42.1%
35.0%
37.3%
36.6%
24.3%

46.3%
36.0%
30.9%
34.2%

around 35    rst-order mln rules. in the remaining mlns,
where our clause reduction technique successfully    nished,
there is a dramatic reduction in the ground network size:
only 524 clauses and 384 atoms on average.

tuffy    nished id136 for all 4 answer options for 82
of the 108 questions; for other questions, it chose the most
promising answer option among the ones it    nished process-
ing. overall, this resulted in a score of 33.6% with an aver-
age of 280 seconds per multiple-choice question on dev-
108, and similar performance on unseen-68.

er-mln, as expected, did not result in any timeouts dur-
ing grounding. the number of ground clauses here, 2,308
on average, is dominated not by kb rules but by the binary
and ternary entity resolution clauses involving the sameas
predicate. er-mln was roughly 33% faster than fo-mln,
but overall achieved similar exam scores as fo-mln.

praline resulted in a 10x speedup over er-mln, ex-
plained in part by much smaller ground networks with only
219 clauses on average. further, it boosted exam perfor-
mance by roughly 15%, pushing it up to 48.8% on dev-108
and 46.3% on unseen-68. this demonstrates the value that
the added    exibility and control of praline bring.

praline: improvements and ablation
we next compare the performance of praline when using
multiple kb rules as a chain or multiple id136 paths.
simply using the top two rules for id136 turns out to be
ineffective as the top two rules provided by the rule selec-
tor are often very similar. instead, we use incremental in-
ference where we add one rule, perform id136 to deter-
mine which additional facts now hold and which setup facts
haven   t yet been used, and then use this information to se-
lect the next best rule. this, as the chain=2 entries in the
   rst row of table 2 show, improves praline   s accuracy on
both datasets. the improvement comes at the cost of a mod-
est increase in runtime from 17 seconds per question to 38.

finally, we evaluate the impact praline   s rules for han-
dling acyclicity (acyclic) and the false-unless-proven (fup)
constraint. table 2 shows a drop in praline   s accuracy when
either of these constraints is removed, which highlights their
importance in our qa task. speci   cally, when we use only
one kb rule, dropping fup clauses has a larger in   uence on
the score as compared to dropping the acyclic constraint.
removing acyclic constraint still causes a small drop even
with a single rule due to the possibility of cyclic id136
within a rule. when chaining multiple rules, however, cyclic
id136 becomes more likely and we see a correspondingly
larger reduction in score when dropping acyclic constraints.

discussion

our investigation of the potential of mlns for qa resulted
in multiple formulations, the third of which is a    exible
model that outperformed other, more natural approaches. we
hope our question sets and mlns will guide further research
on improved modeling of the qa task and design of more
ef   cient id136 mechanisms for such models.

while srl methods seem a perfect    t for textual reason-
ing tasks such as rte and qa, their performance on these
tasks is still not up to par with simple textual feature-based
approaches (beltagy and mooney 2014). on our datasets
too, simple word-overlap based approaches perform quite
well, scoring around 55%. we conjecture that the increased
   exibility of complex relational models comes at the cost of
increased susceptibility to noise in the input. automatically
learning weights of these models may allow leveraging this
   exibility in order to handle noise better. weight learning in
these models, however, is challenging as we only observe the
correct answer for a question and not intermediate feedback
such as ideal alignments and desirable id136 chains.

modeling the qa task with mlns, an undirected model,
gives the    exibility to de   ne a joint model that allows align-
ment to in   uence id136 and vice versa. at the same time,
id136 chains themselves need to be acyclic, suggesting
that models such as problog and slp would be a better    t for
this sub-task. exploring hybrid formulation and designing
more ef   cient and accurate mlns or other srl models for
the qa task remains an exciting avenue of future research.
acknowledgments
the authors would like thank pedro domingos and dan weld for
invaluable discussions and the aristo team at ai2, especially jesse
kinkead, for help with prototype development and evaluation.

[poon and domingos 2006] poon, h., and domingos, p.
2006. sound and ef   cient id136 with probabilistic and
deterministic dependencies. in 21st aaai, 458   463.
[poon and domingos 2009] poon, h., and domingos, p.
2009. unsupervised id29. in emnlp, 1   10.
[raedt, kimmig, and toivonen 2007] raedt, l. d.; kimmig,
a.; and toivonen, h. 2007. problog: a probabilistic prolog
and its application in link discovery. in international joint
conference on arti   cial intelligence.
[richardson and domingos 2006] richardson, m.,
and
domingos, p. 2006. markov logic networks. machine
learning 62(1   2):107   136.
[sato and kameya 2001] sato, t., and kameya, y. 2001. pa-
rameter learning of logic programs for symbolic-statistical
modeling. jair 391   454.
[shavlik and natarajan 2009] shavlik, j. w., and natarajan,
s. 2009. speeding up id136 in markov logic networks
by preprocessing to reduce the size of the resulting grounded
network. in 21st ijcai, 1951   1956.
[singla and domingos 2006a] singla, p., and domingos, p.
2006a. entity resolution with markov logic. in 6th icdm,
572   582.
[singla and domingos 2006b] singla, p., and domingos, p.
2006b. memory-ef   cient id136 in relational domains.
in 21st aaai, 488   493.
[venugopal and gogate 2012] venugopal, d., and gogate,
v. 2012. on lifting the id150 algorithm. in 26th
nips, 1664   1672.

references

[audemard and simon 2009] audemard, g., and simon, l.
2009.
predicting learnt clauses quality in modern sat
solvers. in 21st ijcai.
[beltagy and mooney 2014] beltagy, i., and mooney, r. j.
2014. ef   cient markov logic id136 for natural language
semantics. in starai.
[beltagy et al. 2013] beltagy, i.; chau, c.; boleda, g.; gar-
rette, d.; erk, k.; and mooney, r. 2013. montague meets
markov: deep semantics with probabilistic logical form.
2nd sem.
[beltagy, erk, and mooney 2014] beltagy, i.; erk, k.; and
mooney, r. j. 2014. probabilistic soft logic for semantic
textual similarity. in 52nd acl, 1210   1219.
[clark et al. 2014] clark, p.; balasubramanian, n.; bhaktha-
vatsalam, s.; humphreys, k.; kinkead, j.; sabharwal, a.;
and tafjord, o. 2014. automatic construction of id136-
supporting knowledge bases. in 4th akbc.
[clark, harrison, and balasubramanian 2013] clark, p.; har-
rison, p.; and balasubramanian, n. 2013. a study of the
akbc/requirements for passing an elementary science test.
in akbc-wekex workshop.
[curran, clark, and bos 2007] curran, j.; clark, s.; and bos,
j. 2007. linguistically motivated large-scale nlp with c&c
and boxer. in 45th acl, 33   36.
[de salvo braz, amir, and roth 2005] de salvo braz, r.;
amir, e.; and roth, d. 2005. lifted    rst-order probabilistic
id136. in 19th ijcai, 1319   1325.
[domingos and webb 2012] domingos, p., and webb, w. a.
in 26th
2012. a tractable    rst-order probabilistic logic.
aaai.
[getoor and taskar 2007] getoor, l., and taskar, b., eds.
2007. introduction to statistical relational learning. mit
press.
[gogate and domingos 2011] gogate, v., and domingos, p.
2011. probabilistic theorem proving. in 27th uai, 256   265.
[kok and domingos 2008] kok, s., and domingos, p. 2008.
extracting semantic networks from text via relational clus-
tering. in 19th ecml, 624   639.
1995. id138: a lexi-
[miller 1995] miller, g. a.
cal database for english. communications of the acm
38(11):39   41.
[muggleton 1996] muggleton, s. 1996. stochastic logic pro-
grams. in ilp.
and
[niepert and van den broeck 2014] niepert,
van den broeck, g.
tractability through ex-
changeability: a new perspective on ef   cient probabilistic
id136. in 28th aaai.
[niu et al. 2011] niu, f.; r  e, c.; doan, a.; and shavlik, j. w.
2011. tuffy: scaling up statistical id136 in markov
logic networks using an rdbms. in 37th vldb, 373   384.
[papai, singla, and kautz 2011] papai, t.; singla, p.; and
kautz, h. 2011. constraint propagation for ef   cient in-
ference in markov logic. in 17th cp. 691   705.
[park 2002] park, j. d. 2002. map complexity results and
approximation methods. 388   396.

2014.

m.,

