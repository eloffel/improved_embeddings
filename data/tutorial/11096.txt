id151 features with multitask tensor networks

hendra setiawan, zhongqiang huang, jacob devlin      , thomas lamar,

rabih zbib, richard schwartz and john makhoul

raytheon bbn technologies, 10 moulton st, cambridge, ma 02138, usa

   microsoft research, one microsoft way, redmond, wa 98052, usa

5
1
0
2

 

n
u
j
 

1

 
 
]
l
c
.
s
c
[
 
 

1
v
8
9
6
0
0

.

6
0
5
1
:
v
i
x
r
a

{hsetiawa,zhuang,tlamar,rzbib,schwartz,makhoul}@bbn.com

jdevlin@microsoft.com

abstract

we present a three-pronged approach to
improving id151
(smt), building on recent success in the
application of neural networks to smt.
first, we propose new features based on
neural networks to model various non-
local translation phenomena. second, we
augment the architecture of the neural net-
work with tensor layers that capture im-
portant higher-order interaction among the
network units. third, we apply multitask
learning to estimate the neural network
parameters jointly. each of our proposed
methods results in signi   cant
improve-
ments that are complementary. the over-
all improvement is +2.7 and +1.8 id7
points for arabic-english and chinese-
english translation over a state-of-the-art
system that already includes neural net-
work features.

introduction

1
recent advances in applying neural networks to
id151 (smt) have gen-
erally taken one of two approaches. they ei-
ther develop neural network-based features that
are used to score hypotheses generated from tra-
ditional translation grammars (sundermeyer et al.,
2014; devlin et al., 2014; auli et al., 2013; le
et al., 2012; schwenk, 2012), or they implement
the whole translation process as a single neu-
ral network (bahdanau et al., 2014; sutskever et
al., 2014). the latter approach, sometimes re-
ferred to as id4, attempts
to overhaul smt, while the former capitalizes on
the strength of the current smt paradigm and
leverages the modeling power of neural networks
to improve the scoring of hypotheses generated

   * research conducted when the author was at bbn.

by phrase-based or hierarchical translation rules.
this paper adopts the former approach, as n-best
scores from state-of-the-art smt systems often
suggest that these systems can still be signi   cantly
improved with better features.

we build on (devlin et al., 2014) who proposed
a simple yet powerful feedforward neural network
model that estimates the translation id203
conditioned on the target history and a large win-
dow of source word context. we take advantage
of neural networks    ability to handle sparsity, and
to infer useful abstract representations automati-
cally. at the same time, we address the challenge
of learning the large set of neural network param-
eters. in particular,

    we develop new neural network features
to model non-local translation phenomena
related to word reordering.
large fully-
lexicalized contexts are used to model these
phenomena effectively, making the use of
neural networks essential. all of the features
are useful individually, and their combination
results in signi   cant improvements (section
2).
    we use a tensor neural network architecture
(yu et al., 2012) to automatically learn com-
plex pairwise interactions between the net-
work nodes. the introduction of the tensor
hidden layer results in more powerful fea-
tures with lower model perplexity and signif-
icantly improved mt performance for all of
neural network features (section 3).
    we apply multitask learning (mtl) (caru-
ana, 1997) to jointly train related neural net-
work features by sharing parameters. this
allows parameters learned for one feature to
bene   t the learning of the other features. this
results in better trained models and achieves
additional mt improvements (section 4).

we apply the resulting multitask tensor net-
works to the new features and to existing ones,

obtaining strong experimental results over the
strongest previous results of (devlin et al., 2014).
we obtain improvements of +2.5 id7 points
for arabic-english and +1.8 id7 points for
chinese-english on the darpa bolt web fo-
rum condition. we also obtain improvements of
+2.7 id7 point for arabic-english and +1.9
id7 points for chinese-english on the nist
open12 test sets over the best previously pub-
lished results in (devlin et al., 2014). both the
tensor architecture and multitask learning are gen-
eral techniques that are likely to bene   t other neu-
ral network features.

2 new non-local smt features

existing smt features typically focus on local in-
formation in the source sentence, in the target hy-
pothesis, or both. for example, the id165 lan-
guage model (lm) predicts the next target word
by using previously generated target words as con-
text (local on target), while the lexical translation
model (ltm) predicts the translation of a source
word by taking into account surrounding source
words as context (local on source).

in this work, we focus on non-local transla-
tion phenomena that result from non-monotone re-
ordering, where local context becomes non-local
on the other side. we propose a new set of power-
ful mt features that are motivated by this simple
idea. to facilitate the discussion, we categorize the
features into hypothesis-enumerating features that
estimates a id203 for each generated target
word (e.g., id165 language model), and source-
enumerating features that estimates a id203
for each source word (e.g., lexical translation).

more concretely, we introduce a) joint model
with offset source context (jmo), a hypothesis
enumerating feature that predicts the next target
word the source context af   liated to the previous
target words; and b) translation context model
(tcm), a source-enumerating feature that predicts
the context of the translation of a source word
rather than the translation itself. these two mod-
els extend pre-existing features:
the joint (lan-
guage and translation) model (jm) of (devlin et
al., 2014) and the ltm respectively respectively.
we use a large lexicalized context for there fea-
tures, making the choice of implementing them as
neural networks essential. we also present neural-
network implementations of pre-existing source-
enumerating features:
lexical translation, orien-

tation and fertility models. we obtain additional
gains from using tensor networks and multitask
learning in the modeling and training of all the fea-
tures.

2.1 hypothesis-enumerating features
as mentioned, hypothesis-enumerating features
score each word in the hypothesis, typically by
conditioning it on a context of n-1 previous tar-
get words as in the id165 language model. one
recent such model, the joint model of devlin et al.
(2014) achieves large improvements to the state-
of-the-art smt by using a large context window
of 11 source words and 3 target words. the joint
model with offset source context (jmo) is an
extension of the jm that uses the source words
af   liated with the id165 target history as con-
text. the source contexts of jm and jmo over-
lap highly when the translation is monotone, but
are complementary when the translation requires
word reordering.

2.1.1
joint model with offset source context
formally, jmo estimates the id203 of the tar-
get hypothesis e conditioned on the source sen-
tence f and a target-to-source af   liation a:

|e|

(cid:89)i=1

p (e|f, a)    

p (ei|ei   n+1
i   1

,cai   k = f

ai   k+m
ai   k   m )

i   1

where ei is the word being predicted; ei   n+1
is the
string of n     1 previously generated words; cai   k
to the source context of m source words around
fai   k, the source word af   liated with ei   k. we
refer to k as the offset parameter. we use the def-
inition of word af   liation introduced in devlin et
al. (2014). when no source context is used, the
model is equivalent to an id165 language model,
while an offset parameter of k = 0 reduces the
model to the jm of devlin et al. (2014).

when k > 0, the jmo captures non-local con-
text in the prediction of the next target word. more
speci   cally, ei   k and ei, which are local on the
target side, are af   liated to fai   k and fai which
may be distant from each other on the source side
due to non-monotone translation, even for k = 1.
the offset model captures reordering constraints
by encouraging the predicted target word ei to    t
well with the previous af   liated source word fai   k
and its surrounding words. we implement a sep-
arate feature for each value of k, and later train

them jointly via multitask learning. as our ex-
periments in section 5.2.1 con   rm, the history-
af   liated source context results in stronger smt
improvement than just increasing the number of
surrounding words in jm.

fig. 1 illustrates the difference between jmo
and jm. assuming n = 3 and m = 1, then jm
estimates p (e5|e4, e3,ca5 = {f6, f7, f8}). on
the other hand, for k = 1 , jmok=1 estimates
p (e5|e4, e3,ca4 = {f8, f9, f10}).

figure 1: example to illustrate features. f 9
5 is the
source segment, e7
3 is the corresponding transla-
tion and lines refer to the alignment. we show
hypothesis-enumerating features that look at f7
and source-enumerating features that look at e5.
we surround the source words af   liated with e5
and its id165 history with a bracket, and sur-
round the source words af   liated with the history
of e5 with squares.

2.2 source-enumerating features
source-enumerating features iterate over words
in the source sentence, including unaligned words,
and assign it a score depending on what as-
pect of translation they are modeling. a source-
enumerating feature can be formulated as follows:

|f|

(cid:89)j=1

p (e|f, a)    

p (yj|cj = f j+m
j   m )

is the source context (similar to the
where caj
hypothesis-enumerating features above) and yj
is the label being predicted by the feature. we
   rst describe pre-existing source-enumerating fea-
tures: the lexical translation model, the orientation
model and the fertility model, and then discuss a
new feature: translation context model (tcm),
which is an extension of the lexical translation
model.
2.2.1 pre-existing features
lexical translation model (ltm) estimates the
id203 of translating a source word fj to a tar-

get word l(fj) = ebj given a source context cj,
bj     b is the source-to-target word af   liation as
de   ned in (devlin et al., 2014). when fj is trans-
lated to more than one word, we arbitrarily keep
the left-most one. the target word vocabulary v
is extended with a n u ll token to accommodate
unaligned source words.

orientation model (ori) describes the proba-
bility of orientation of the translation of phrases
surrounding a source word fj relative to its own
translation. we follow (setiawan et al., 2013)
in modeling the orientation of the left and right
phrases of fj with maximal orientation span (the
longest neighboring phrase consistent with align-
ment), which we denote by lj and rj respec-
tively. thus, o(fj) = (cid:104)olj (fj), orj (fj)(cid:105), where
olj and orj refer to the orientation of lj and rj
respectively. for unaligned fj, we set o(fj) =
olj (rj), the orientation of rj with respect to lj.
fertility model (fm) models the id203 that
a source word fj generates   (fj) words in the
hypothesis. our implemented model only dis-
tinguishes between aligned and unaligned source
words (i.e.,   (fj)     {0, 1}). the generalization of
the model to account for multiple values of   (fi)
is straightforward.

2.2.2 translation context model

as with jmo in section 2.1.1, we aim to cap-
ture translation phenomena that appear local on
the target hypothesis but non-local on the source
side. here, we do so by extending the ltm
feature to predict not only the translated word
ebj , but also its surrounding context.
for-
mally, we model p (l(fj)|cj), where l(fj) =
ebj   d,       , ebj ,       ebj +d is the hypothesis word
in practice, we decompose
window around ebj .
p (ebj +d(cid:48)|cj) and imple-
tcm further into
mented each as a separate neural network-based
feature. note that tcm is equivalent to the ltm
when d = 0. because of word reordering, a given
hypothesis word in l(fj) might not be af   liated
with fj or even to the words in cj. tcm can model
non-local information in this way.

+d(cid:81)d(cid:48)=   d

2.2.3 combined model

since the feature label is unde   ned for unaligned
source words, we make the model hierarchical,
based on whether the source word is aligned or

f9f5...e5e6e4e7e3......c7=ca5...z}|{f6f7f8not, and thus arrive at the following formulation:

p (l(fj))    p (ori(fj))    p (  (fj)) =
p (  p(fj) = 0)    p (olj (rj))

p (  p(fj)     1)   
  p (olj (fj), orj (fj))

+d(cid:81)d(cid:48)=   d

p (ebj +d(cid:48))

                                 

we dropped the common context (cj) for readabil-
ity.
we reuse fig. 1 to illustrate the source-
enumerating features. assuming d = 1, the scores
associated with f7 are p (  (f7)     1|c7) for the
fm; p (e4|c7)   p (e5|c7)   p (e6)|c7) for the tcm;
and p (o(f7) = (cid:104)ol7(f7) = ra, or7(f7) = ra(cid:105))
for the ori(ra refers to reverse adjacent). l7
and r7 (i.e. f6 and f 9
8 respectively), the longest
neighboring phrase of f7, are translated in reverse
order and adjacent to e5.
3 tensor neural networks
the second part of this work improves smt by
improving the neural network architecture. neural
networks derive their strength from their ability to
learn a high-level representation of the input auto-
matically from data. this high-level representa-
tion is typically constructed layer by layer through
a weighted sum linear operation and a non-linear
activation function. with suf   cient training data,
neural networks often achieve state-of-the-art per-
formance on many tasks. this stands in sharp con-
trast to other algorithms that require tedious man-
ual feature engineering. for the features presented
in this paper, the context words are fed to the net-
work network with minimal engineering.

we further strengthen the network   s ability to
learn rich interactions between its units by intro-
ducing tensors in the hidden layers. the multi-
plicative property of the tensor bares a close re-
semblance to collocation of context words which
are useful in many natural language processing
tasks.

in conventional feedforward neural networks,
the output of hidden layer l is produced by mul-
tiplying the output vector from the previous layer
with a weight matrix (wl) and then applying the
activation function    to the product. tensor neu-
ral networks generalize this formulation by using
a tensor ul of order 3 for the weights. the output
of node k in layer l is computed as follows:

hl[k] =   (cid:0)hl   1    ul[k]    ht
l   1(cid:1)

where ul[k], the k-th slice of ul, is a square ma-
trix.

in our implementation, we follow (yu et al.,
2012; hutchinson et al., 2013) and use a low-rank
approximation of ul[k] = ql[k]    rl[k]t , where
ql[k], rl[k]     rn  r. the output of node k be-
comes:

hl[k] =   (cid:0)hl   1    ql[k]    rl[k]t    ht
l   1(cid:1)

in our experiments, we choose r = 1, and also
apply the non-linear activation function    distribu-
tively. we arrive at the following three equations
for computing the hidden layer outputs (0 < l <
l):

vl =    (hl   1    ql)
(cid:48)
l =    (hl   1    rl)
v
hl = vl     v

(cid:48)
l

where hl   1 is double-projected to vl and v(cid:48)
l,
and the two projections are merged using the
hadamard element-wise product operator    .
this formulation allows us to use the same in-
frastructure of the conventional neural networks
by projecting the previous layer to two different
spaces of the same dimensions,
then multiply-
ing them element-wise. the only component that
is different from conventional feedforward neural
networks is the multiplicative function, which is
trivially differentiable with respect to the learnable
parameters. figure 3(b) illustrates the tensor ar-
chitecture for two hidden layers.

the tensor network can learn collocation fea-
tures more easily. for example, it can learn a col-
location feature that is activated only if hl   1[i] col-
locates with hl   1[j] by setting ul[k][i][j] to some
positive number. this results in smt improve-
ments as we describe in section 5.

4 multitask learning
the third part of this paper addresses the challenge
of effectively learning a large number of neural
network parameters without over   tting. the chal-
lenge is even larger for tensor network since they
practically doubles the number of parameters. in
this section, we propose to apply multitask learn-
ing (mtl) to partially address this issue. we im-
plement mtl as parameter sharing among the net-
works. this effectively reduces the number of pa-
rameters, and more importantly, it takes advan-
tage of parameters learned for one feature to better

figure 2: the network architecture for (a) a conventional feedforward neural network, (b) tensor hidden
layers, and (c) multitask learning with m features that share the embedding and    rst hidden layers
(t = 1).

learn the parameters of the other features. another
way of looking at this is that mtl facilitates reg-
ularization through learning the other tasks.

mtl is suitable for smt features as they model
different but closely related aspects of the same
translation process. mtl has long been used by
the wider machine learning community (caruana,
1997) and more recently for natural language pro-
cessing (collobert and weston, 2008; collobert
et al., 2011). the application of mtl to ma-
chine translation, however, has been much less re-
stricted, which is rather surprising since smt fea-
tures arise from the same translation task and are
naturally related.

we apply mtl for the features described in
section 2. we design all the features to also share
the same neural network architecture (in this case,
the tensor architecture described in section 3) and
the same input, thus resulting in two large neural
networks: one for the hypothesis-enumerating fea-
tures and another for the source-enumerating ones.
this simpli   es the implementation of mtl. us-
ing this setup, it is possible to vary the number
of shared hidden layers t from 0 (only sharing the
embedding layer) to l     1 (sharing all the layers
except the output). note that in principle mtl is
applicable to other set of networks that have differ-
ent architecture or even different input set. with
mtl, the training procedure is the same as that of
standard neural networks.

we use the back propagation algorithm, and use
as the id168 the product of likelihood of
each feature1:

1in this and in the other parts of the paper, we add the
id172 id173 term described in (devlin et al.,
2014) to the id168 to avoid computing the normaliza-
tion constant at model query/decoding time.

loss =(cid:88)i

m(cid:88)j

log (p (yj(xi)))

where xi is the training sample and yj is one of
the m models trained. we use the sum of log like-
lihoods since we assume that the features are inde-
pendent.

fig. 3(c) illustrates mtl between m models
sharing the input embedding layer and the    rst
hidden layer (t = 1) compared to the separately-
trained conventional feedforward neural network
and tensor neural network.

5 experiments

we demonstrate the impact of our work with ex-
tensive mt experiments on arabic-english and
chinese-english translation for the darpa bolt
web forum and the nist opeid412 conditions.

5.1 baseline mt system
we run our experiments using a state-of-the-art
string-to-dependency hierarchical decoder (shen
et al., 2010). the baseline we use includes a set
of powerful features as follow:

    forward and backward rule probabilities
    contextual lexical smoothing (devlin, 2009)
    5-gram kneser-ney lm
    dependency lm (shen et al., 2010)
    length distribution (shen et al., 2010)
    trait features (devlin and matsoukas, 2012)
    factored source syntax (huang et al., 2013)
    discriminative sparse feature, totaling 50k
features (chiang et al., 2009)
    neural network joint model (nnjm) and
neural network lexical translation model

inputh1h2inputh1   v1v01h2   v2v02inputh1   v1v01h12   v12v012hm2   vm2v0m2            w1w2q1r1r2q2r1q1q12r12qm2rm2(a)(b)(c)outputoutputtask1taskm(nnltm) (devlin et al., 2014)

as shown, our baseline system already includes
neural network-based features. nnjm, nnltm
and use two hidden layers with 500 units and use
embedding of size 200 for each input.

we use the mada-arz tokenizer (habash et
al., 2013) for arabic word id121. for chi-
nese id121, we use a simple longest-match-
   rst lexicon-based approach. we align the training
data using giza++ (och and ney, 2003). for tun-
ing the weights of mt features including the new
features, we use iterative k-best optimization with
an expectedid7 objective function (rosti et al.,
2010), and decode the test sets after 5 tuning iter-
ation. we report the lower-cased id7 and ter
scores.

5.2 bolt discussion forum
the bulk of our experiments is on the bolt web
discussion forum domain, which uses data col-
lected by the ldc. the parallel training data con-
sists of all of the high-quality nist training cor-
pora, plus an additional 3 million words of trans-
lated forum data. the tuning and test sets consist
of roughly 5000 segments each, with 2 indepen-
dent references for arabic and 3 for chinese.

5.2.1 effects of new features
we    rst look at the effects of the proposed features
compared to the baseline system. table 1 summa-
rizes the primary results of the arabic-english and
chinese-english experiments for the bolt condi-
tion. we show the experimental results related to
hypothesis-enumerating features (hypen) in rows
s2-s5, those related to source-enumerating fea-
tures (srcen) in rows s6-s9, and the combination
of the two in row s10. for all the features, we set
the source context length to m = 5 (11-word win-
dow). for jm and jmo, we set the target context
length to n = 4. for the offset parameter k of
jmo, we use values 1 to 3. for tcm, we model
one word around the translation (d = 1). larger
values of d did not result in further gains. the
baseline is comparable to the best results of (de-
vlin et al., 2014).

in rows s3 to s5, we incrementally add a model
with different offset source context, from k = 1
to k = 3. for ar-en, adding jmos with differ-
ent offset source context consistently yields pos-
itive effects in id7 score, while in zh-en, it
yields positive effects in ter score. utilizing all
offset source contexts    +jmok   3    (row s5) yields

around 0.9 id7 point improvement in ar-en
and around 0.3 id7 in zh-en compared to
the baseline. the jmo consistently provides bet-
ter improvement compared to a larger jm con-
text (row s2), validating our hypothesis that using
offset source context captures important non-local
context.

rows s6 to s9 present the improvements that
result from implementing pre-existing source-
enumerating smt features as neural networks,
and highlight the contribution of our translation
context model (tcm). this set of experiments is
orthogonal to the hypen experiments (rows s2-
s5). each pre-existing model has a modest pos-
itive cumulative effect on both id7 and ter.
we see this result as further con   rming the cur-
rent trend of casting existing smt features as neu-
ral network since our baseline already contains
such features. the next row present the results
of adding the translation context model, with one
word surrounding the translation (d = 1). as
shown, tcm yields a positive effect of around
0.5 id7 and ter improvements in ar-en and
around 0.2 id7 and ter improvements in zh-
en.

separately, the set of source-enumerating fea-
tures and the set of target-enumerating features
produce around 1.1 to 1.2 points id7 gain in
ar-en and 0.3 to 0.5 points id7 gain in zh-
en. the combination of the two sets produces a
complementary gain in addition to the gains of the
individual models as row (s10) shows. the com-
bined gain improves to 1.5 id7 points in ar-
en and 0.7 id7 points in zh-en.

system

s1: baseline
s2: s1+jmlc8
s3: s1+jmok=1
s4: s3+jmok=2
s5: s4+jmok=3
s6: s1+ltm
s7: s6+ori
s8: s7+fert
s9: s8+tcm
s10: s9+jmok   3

ar-en
bl ter
43.2
45.0
45.0
43.5
44.7
43.9
44.7
43.9
44.4
44.5
44.7
43.5
44.6
43.7
43.8
44.7
44.2
44.3
44.7
44.1

zh-en
bl ter
30.2
58.3
58.5
30.2
57.8
30.8
57.8
30.7
30.5
57.5
58.0
30.3
57.8
30.4
30.5
57.8
57.5
30.7
30.9
57.3

table 1: mt results of various model combination
in id7 and in ter.

5.2.2 effects of tensor network and

multitask learning

we    rst analyze the impact of tensor architecture
and mtl intrinsically by reporting the models   
average log-likelihood on the validation sets (a
subset of the test set) in table 2. as mentioned, we
group the models to hypen (jm and jmok   3) and
srcen (ltm, ori,fert and tcm) as we perform
mtl on these two groups. likelihood of these
two groups in the previous subsection are in col-
umn    nn    (for neural network), which serves as
a baseline. the application of the tensor architec-
ture improves their likelihood as shown in column
   tensor    for both languages and models.

feat.

independent
nn tensor

r hypen
srcen
a
h hypen
srcen

z

-8.85
-8.47
-11.48
-10.77

-8.54
-8.32
-11.06
-10.66

mtl

t = 0
t = 1
l = 2 l = 3
-
-8.35
-8.10
-8.09
-
-10.87
-10.54
-10.49

table 2: sum of the average log-likelihood of the
models in hypen and srcen. t = 0 refers to mtl
that shares only the embedding layer, while t = 1
shares the    rst hidden layer as well. l refers to the
network   s depth. higher value is better.

the likelihoods of the mtl-related experi-
ments are in columns with    mtl    header. we
present two set of results.
in the    rst set (col-
umn    mtl,t=0,l=2   ), we run mtl for features
from column    tensor    by sharing the embedding
layer only (t = 0). this allows us to isolate
the impact of mtl in the presence of tensors.
column    mtl,t=1,l=3    corresponds to the exper-
iment that produces the best intrinsic result, where
each model uses tensors with three hidden lay-
ers (500x500x500, l = 3) and the models share
the embedding and the    rst hidden layers (t = 1).
mtl consistently gives further intrinsic gain com-
pared to tensors. more sharing provides an extra
gain for srcen as shown in the last column. note
that we only experiment with different l and t for
srcen and not for hypen because the models in
hypen have different input sets.
in our experi-
ments, further sharing and more hidden layers re-
sulted in no further gain. in total, we see a consis-
tent positive effect in intrinsic evaluation from the
tensor networks and multitask learning.

moving on to mt evaluation, we summarize the

experiments showing the impact of tensors and
mtl in table 3. for mtl, we use l = 3, t = 2
since it gives the best intrinsic score. employing
tensors instead of regular neural networks gives a
signi   cant and consistent positive impact for all
models and language pairs. for the system with
the baseline features, we use the tensor architec-
ture for both the joint model and the lexical trans-
lation model of devlin et al. resulting in an im-
provement of around 0.7 id7 points, and show-
ing the wide applicability of the tensor architec-
ture. on top of this improved baseline, we also ob-
serve an improvement of the same scale for other
models (column    tensor   ), except for hypen fea-
tures in ar-en experiment. moving to mtl ex-
periments, we see improvements, especially from
srcen features. mtl gives around 0.5 id7
point improvement for ar-en and around 0.4
id7 point for zh-en. when we employ both
hypen and srcen together, mtl gives around 0.4
id7 point in ar-en and 0.2 id7 point in
zh-en. in total, our work results in an improve-
ment of 2.5 id7 point for ar-en and 1.8 for
id7 point in zh-en on top of the best results in
(devlin et al., 2014).

5.3 nist opeid412

our nist system is
compatible with the
opeid412 constrained track, which consists of
10m words of high-quality parallel training for
arabic, and 25m words for chinese. the id165
lm is trained on 5b words of data from the en-
glish gigaword. for test, we use the    arabic-to-
english original progress test    (1378 segments)
and    chinese-to-english original progress test +
opeid412 current test    (2190 segments), which
consists of a mix of newswire and web data.
all test segments have 4 references. our tuning
set contains 5000 segments, and is a mix of the
mt02-05 eval set as well as additional held-out
parallel data from the training corpora.

we report the experiments for the nist con-
dition in table 4.
in particular, we investigate
the impact of deploying our new features (column
   feat   ) and demonstrate the effects of the ten-
sor architecture (column    tensor   ) and multitask
learning (column    mtl   ). as shown the results
are inline with the bolt condition where we ob-
serve additive improvements from adding our new
features, applying tensor network and multitask
learning. on arabic-english, we see a gain of 2.7

feature set

r1: baseline features
r2: r1 + hypen
r3: r1 + srcen
r4: r1 + hypen + srcen

ar-en

zh-en

nn tensor mtl nn tensor mtl
43.2
-
31.3
44.4
44.3
31.9
32.0
44.7

30.2
30.5
30.7
30.9

43.9
44.4
44.9
45.3

30.8
31.5
31.5
31.8

-
44.5
45.5
45.7

table 3: experimental results to investigate the effects of the new features, dtn and mtl. the top
part shows the bolt results, while the bottom part shows the nist results. the best results for each
conditions and each language-pair are in bold. the baselines are in italics. .

ar-en
mixed-case
zh-en
mixed-case

base. feat tensor mtl
53.7
56.4
51.8
54.1
36.6
38.5
34.4
36.1

55.4
53.1
37.8
35.5

55.9
53.7
38.2
35.9

table 4: experimental results for the nist condi-
tion. mixed-case scores are also reported. base-
lines are in italics.

id7 point and on chinese-english, we see a 1.9
id7 point gain. we also report the mixed-cased
id7 scores for comparison with previous best
published results, i.e. devlin et al. (2014) report
52.8 id7 for arabic-english and 34.7 id7 for
chinese-english. thus, our results are around 1.3-
1.4 id7 point better. note that they use addi-
tional rescoring features but we do not.

6 related work

our work is most closely related to devlin et al.
(2014). they use a simple feedforward neural
network to model two important mt features: a
joint language and translation model, and a lex-
ical translation model. they show very large
improvements on arabic-english and chinese-
english web forum and newswire baselines. we
improve on their work in 3 aspects. first, we
model more features using neural networks, in-
cluding two novel ones: a joint model with off-
set source context and a translation context model.
second, we enhance the neural network architec-
ture by using tensor layers, which allows us to
model richer interactions. lastly, we improve the
performance of the individual features by training
them using multitask learning. in the remainder
of this section, we describe previous work relat-
ing to the three aspect of our work, namely mt
modeling, neural network architecture and model
learning.

the features we propose in this paper address
the major aspects of smt modeling that have
informed much of the research since the origi-
nal ibm models (brown et al., 1993):
lexical
translation, reordering, word fertility, and lan-
guage models. of particular relevance to our work
are approaches that incorporate context-sensitivity
into the models (carpuat and wu, 2007), formu-
late reordering as orientation prediction task (till-
man, 2004) and that use neural network language
models (bengio et al., 2003; schwenk, 2010;
schwenk, 2012), and incorporate source-side con-
text into them (devlin et al., 2014; auli et al.,
2013; le et al., 2012; schwenk, 2012).

approaches to incorporating source context into
a neural network model differ mainly in how they
represent the source sentence and in how long is
the history they keep.
in terms of representa-
tion of the source sentence, we follow (devlin et
al., 2014) in using a window around the af   liated
source word. to name some other approaches,
auli et al. (2013) uses latent semantic analysis and
source sentence embeddings learned from the re-
current neural network; sundermeyer et al. (2014)
take the representation from a bidirectional lstm
recurrent neural network; and kalchbrenner and
blunsom (2013) employ a convolutional sentence
model. for target context, recent work has tried
to look beyond the classical id165 history. (auli
et al., 2013; sundermeyer et al., 2014) consider
an unbounded history, at the expense of making
their model only applicable for n-best rescoring.
another recent line of research (bahdanau et al.,
2014; sutskever et al., 2014) departs more rad-
ically from conventional feature-based smt and
implements the mt system as a single neural net-
work. these models use a representation of the
whole input sentence.

we use a feedforward neural network in this
work. besides feedforward and recurrent net-

works, other network architectures that have been
applied to smt include convolutional networks
(kalchbrenner et al., 2014) and recursive networks
(socher et al., 2011). the simplicity of feedfor-
ward networks works to our advantage. more
speci   cally, due to the absence of a feedback loop,
the feedforward architecture allows us to treat
individual decisions independently, which makes
parallelization of the training easy and the query-
ing the network at decoding time straightforward.
the use of tensors in the hidden layers strengthens
the neural network model, allowing us to model
more complex feature interactions like colloca-
tion, which has been long recognized as impor-
tant information for many nlp tasks (e.g. word
sense disambiguation (lee and ng, 2002)). the
tensor formulation we use is similar to that of
(yu et al., 2012; hutchinson et al., 2013). ten-
sor neural networks have a wide application in
other    eld, but have only been recently applied in
nlp (socher et al., 2013; pei et al., 2014). to
our knowledge, our work is the    rst to use tensor
networks in smt.

our approach to multitask learning is related to
work that is often labeled joint training or transfer
learning. to name a few of these works, finkel
and manning (2009) successfully train name en-
tity recognizers and syntactic parsers jointly, and
singh et al. (2013) train models for coreference
resolution, id39 and relation
extraction jointly. both efforts are motivated by
the minimization of cascading errors. our work
is most closely related to collobert and weston
(2008; collobert et al. (2011), who apply multi-
task learning to train neural networks for multi-
ple nlp models: part-of-speech tagging, semantic
role labeling, named-entity recognition and lan-
guage model variations.

7 conclusion

this paper argues that a relatively simple feedfor-
ward neural network can still provides signi   cant
improvement to id151
(smt). we support this argument by presenting a
multi-pronged approach that addresses modeling,
architectural and learning aspects of pre-existing
neural network-based smt features. more con-
cretely, we paper present a new set of neural
network-based smt features to capture important
translation phenomena, extend feedforward neu-
ral network with tensor layers, and apply multi-

task learning to integrate the smt features more
tightly. empirically, all our proposals successfully
produce an improvement over state-of-the-art ma-
chine translation system for arabic-to-english and
chinese-to-english and for both bolt web fo-
rum and nist conditions. building on the suc-
cess of this paper, we plan to develop other neural-
network-based features, and to also relax the lim-
iteation of current rule extraction heuristics by
generating translations word-by-word.

acknowledgement

this work was supported by darpa/i2o contract
no. hr0011-12-c-0014 under the bolt pro-
gram. the views, opinions, and/or    ndings con-
tained in this article are those of the author and
should not be interpreted as representing the of-
   cial views or policies, either expressed or im-
plied, of the defense advanced research projects
agency or the department of defense.

references
[auli et al.2013] michael auli, michel galley, chris
quirk, and geoffrey zweig. 2013. joint language
and translation modeling with recurrent neural net-
in proceedings of the 2013 conference on
works.
empirical methods in natural language process-
ing, pages 1044   1054, seattle, washington, usa,
october. association for computational linguistics.

[bahdanau et al.2014] dzmitry bahdanau, kyunghyun
cho, and yoshua bengio. 2014. neural machine
translation by jointly learning to align and translate.
technical report 1409.0473, arxiv.

[bengio et al.2003] yoshua bengio, r  ejean ducharme,
pascal vincent, and christian jauvin. 2003. a neu-
ral probabilistic language model. journal of ma-
chine learning research, 3:1137   1155.

[brown et al.1993] peter f. brown, vincent j. della
pietra, stephen a. della pietra, and robert l. mer-
cer. 1993. the mathematics of statistical machine
translation: parameter estimation. comput. lin-
guist., 19(2):263   311, june.

[carpuat and wu2007] marine carpuat and dekai wu.
2007. improving id151 us-
in proceedings
ing id51.
of the 2007 joint conference on empirical meth-
ods in natural language processing and com-
putational natural language learning (emnlp-
conll), pages 61   72, prague, czech republic,
june. association for computational linguistics.

[caruana1997] rich caruana. 1997. multitask learn-

ing. machine learning, 28(1):41   75.

[chiang et al.2009] david chiang, kevin knight, and
wei wang. 2009. 11,001 new features for statistical
in hlt-naacl, pages 218   
machine translation.
226.

[collobert and weston2008] ronan collobert and ja-
son weston. 2008. a uni   ed architecture for natu-
ral language processing: deep neural networks with
multitask learning. in proceedings of the 25th inter-
national conference on machine learning, icml
   08, pages 160   167, new york, ny, usa. acm.

[collobert et al.2011] ronan collobert, jason weston,
l  eon bottou, michael karlen, koray kavukcuoglu,
and pavel kuksa. 2011. natural language process-
j. mach. learn. res.,
ing (almost) from scratch.
12:2493   2537, november.

[devlin and matsoukas2012] jacob devlin and spyros
matsoukas. 2012. trait-based hypothesis selection
for machine translation. in proceedings of the 2012
conference of the north american chapter of the
association for computational linguistics: human
language technologies, naacl hlt    12, pages
528   532, stroudsburg, pa, usa. association for
computational linguistics.

[devlin et al.2014] jacob devlin,

rabih

zbib,
zhongqiang huang, thomas lamar, richard
schwartz, and john makhoul.
fast and
robust neural network joint models for statistical
in proceedings of the 52nd
machine translation.
annual meeting of the association for computa-
tional linguistics (volume 1: long papers), pages
1370   1380, baltimore, maryland, june. association
for computational linguistics.

2014.

[devlin2009] jacob devlin. 2009. lexical features for
id151. master   s thesis, uni-
versity of maryland.

2009.

[finkel and manning2009] jenny rose finkel

and
joint parsing
christopher d. manning.
in proceedings of
and id39.
human language technologies: the 2009 annual
conference of the north american chapter of the
association for computational linguistics, pages
326   334, boulder, colorado, june. association for
computational linguistics.

[habash et al.2013] nizar habash, ryan roth, owen
rambow, ramy eskander, and nadi tomeh. 2013.
morphological analysis and disambiguation for di-
alectal arabic. in hlt-naacl, pages 426   432.

[kalchbrenner and blunsom2013] nal kalchbrenner
and phil blunsom. 2013. recurrent continuous
translation models. in proceedings of the 2013 con-
ference on empirical methods in natural language
processing, pages 1700   1709, seattle, washington,
usa, october. association for computational
linguistics.

[kalchbrenner et al.2014] nal kalchbrenner, edward
grefenstette, and phil blunsom. 2014. a convo-
lutional neural network for modelling sentences. in
proceedings of the 52nd annual meeting of the as-
sociation for computational linguistics (volume 1:
long papers), pages 655   665, baltimore, maryland,
june. association for computational linguistics.

[le et al.2012] hai-son le, alexandre allauzen, and
franc  ois yvon. 2012. continuous space transla-
tion models with neural networks. in proceedings of
the 2012 conference of the north american chap-
ter of the association for computational linguis-
tics: human language technologies, naacl hlt
   12, pages 39   48, stroudsburg, pa, usa. associa-
tion for computational linguistics.

[lee and ng2002] yoong keok lee and hwee tou
ng. 2002. an empirical evaluation of knowledge
sources and learning algorithms for word sense dis-
in proceedings of the acl-02 con-
ambiguation.
ference on empirical methods in natural language
processing - volume 10, emnlp    02, pages 41   48,
stroudsburg, pa, usa. association for computa-
tional linguistics.

[och and ney2003] franz josef och and hermann ney.
2003. a systematic comparison of various statisti-
cal alignment models. computational linguistics,
29(1):19   51.

[pei et al.2014] wenzhe pei, tao ge, and baobao
2014. max-margin tensor neural net-
chang.
in proceed-
work for chinese id40.
ings of the 52nd annual meeting of the association
for computational linguistics (volume 1: long pa-
pers), pages 293   303, baltimore, maryland, june.
association for computational linguistics.

[rosti et al.2010] antti rosti, bing zhang, spyros mat-
soukas, and rich schwartz. 2010. bbn system de-
scription for wmt10 system combination task. in
wmt/metricsmatr, pages 321   326.

[schwenk2010] holger schwenk. 2010. continuous-
space language models for statistical machine trans-
lation. prague bull. math. linguistics, 93:137   146.

[huang et al.2013] zhongqiang huang, jacob devlin,
and rabih zbib. 2013. factored soft source syntac-
tic constraints for hierarchical machine translation.
in emnlp, pages 556   566.

[schwenk2012] holger schwenk. 2012. continuous
space translation models for phrase-based statistical
in coling (posters), pages
machine translation.
1071   1080.

[hutchinson et al.2013] brian hutchinson, li deng,
and dong yu. 2013. tensor deep stacking net-
ieee trans. pattern anal. mach. intell.,
works.
35(8):1944   1957, august.

[setiawan et al.2013] hendra setiawan, bowen zhou,
bing xiang, and libin shen. 2013. two-neighbor
orientation model with cross-boundary global con-
texts. in proceedings of the 51st annual meeting of

deep tensor neural networks.
isca.

in interspeech.

the association for computational linguistics (vol-
ume 1: long papers), pages 1264   1274, so   a, bul-
garia, august. association for computational lin-
guistics.

[shen et al.2010] libin shen, jinxi xu, and ralph
weischedel. 2010. string-to-dependency statisti-
cal machine translation. computational linguistics,
36(4):649   671, december.

[singh et al.2013] sameer singh, sebastian riedel,
brian martin, jiaping zheng, and andrew mccal-
lum. 2013.
joint id136 of entities, relations,
and coreference. in proceedings of the 2013 work-
shop on automated knowledge base construction,
akbc    13, pages 1   6, new york, ny, usa. acm.

[snover et al.2008] matthew snover, bonnie dorr, and
richard schwartz. 2008. language and transla-
tion model adaptation using comparable corpora. in
proceedings of the conference on empirical meth-
ods in natural language processing, emnlp    08,
pages 857   866, stroudsburg, pa, usa. association
for computational linguistics.

[socher et al.2011] richard socher, cliff c. lin, an-
drew y. ng, and christopher d. manning. 2011.
parsing natural scenes and natural language with
id56s. in proceedings of the
26th international conference on machine learning
(icml).

[socher et al.2013] richard socher, danqi chen,
christopher d manning, and andrew ng. 2013.
reasoning with neural tensor networks for knowl-
edge base completion. in c.j.c. burges, l. bottou,
m. welling, z. ghahramani, and k.q. weinberger,
editors, advances in neural information processing
systems 26, pages 926   934. curran associates, inc.

[sundermeyer et al.2014] martin sundermeyer, tamer
alkhouli, joern wuebker, and hermann ney. 2014.
translation modeling with bidirectional recurrent
neural networks. in proceedings of the 2014 con-
ference on empirical methods in natural language
processing (emnlp), pages 14   25, doha, qatar,
october. association for computational linguistics.

[sutskever et al.2014] ilya sutskever, oriol vinyals,
and quoc v. v le. 2014. sequence to sequence
learning with neural networks.
in z. ghahramani,
m. welling, c. cortes, n.d. lawrence, and k.q.
weinberger, editors, advances in neural informa-
tion processing systems 27, pages 3104   3112. cur-
ran associates, inc.

[tillman2004] christoph tillman. 2004. a unigram
orientation model for id151.
in daniel marcu susan dumais and salim roukos,
editors, hlt-naacl 2004: short papers, pages
101   104, boston, massachusetts, usa, may 2 -
may 7. association for computational linguistics.

[yu et al.2012] dong yu, li deng, and frank seide.
2012. large vocabulary id103 using

