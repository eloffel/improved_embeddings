a bayesian model for generative transition-based id33

jan buys1 and phil blunsom1,2

1department of computer science, university of oxford 2google deepmind

{jan.buys,phil.blunsom}@cs.ox.ac.uk

5
1
0
2

 

n
u
j
 

8
2

 
 
]
l
c
.
s
c
[
 
 

2
v
4
3
3
4
0

.

6
0
5
1
:
v
i
x
r
a

abstract

scalable,

we propose a simple,
fully
generative model for transition-based de-
pendency parsing with high accuracy.
the model, parameterized by hierarchical
pitman-yor processes, overcomes the lim-
itations of previous generative models by
allowing fast and accurate id136. we
propose an ef   cient decoding algorithm
based on particle    ltering that can adapt
the beam size to the uncertainty in the
model while jointly predicting pos tags
and parse trees. the uas of the parser is
on par with that of a greedy discriminative
baseline. as a language model, it obtains
better perplexity than a id165 model by
performing semi-supervised learning over
a large unlabelled corpus. we show that
the model is able to generate locally and
syntactically coherent sentences, opening
the door to further applications in lan-
guage generation.

1

introduction

transition-based id33 algorithms
that perform greedy local id136 have proven to
be very successful at fast and accurate discrimina-
tive parsing (nivre, 2008; zhang and nivre, 2011;
chen and manning, 2014). beam-search decoding
further improves performance (zhang and clark,
2008; huang and sagae, 2010; choi and mccal-
lum, 2013), but increases decoding time. graph-
based parsers (mcdonald et al., 2005; koo and
collins, 2010; lei et al., 2014) perform global
id136 and although they are more accurate in
some cases, id136 tends to be slower.

in this paper we aim to transfer the advantages
of transition-based parsing to generative depen-
dency parsing. while generative models have been
used widely and successfully for constituency

parsing (collins, 1997; petrov et al., 2006), their
use in id33 has been limited. gen-
erative models offer a principled approach to semi-
and unsupervised learning, and can also be applied
to id86 tasks.

dependency grammar induction models (klein
and manning, 2004; blunsom and cohn, 2010)
are generative, but not expressive enough for
high-accuracy parsing. a previous generative
transition-based dependency parser (titov and
henderson, 2007) obtains competitive accuracies,
but training and decoding is computationally very
expensive. syntactic language models have also
been shown to improve performance in speech
recognition and machine translation (chelba and
jelinek, 2000; charniak et al., 2003). however,
the main limitation of most existing generative
syntactic models is their inef   ciency.
we propose a generative model for transition-
based parsing (  2). the model, parameterized by
hierarchical pitman-yor processes (hpyps) (teh,
2006), learns a distribution over derivations of
parser transitions, words and pos tags (  3).

to enable ef   cient id136, we propose a
novel algorithm for linear-time decoding in a gen-
erative transition-based parser (  4). the algorithm
is based on particle    ltering (doucet et al., 2001),
a method for sequential monte carlo sampling.
this method enables the beam-size during decod-
ing to depend on the uncertainty of the model.
experimental results (  5) show that the model
obtains 88.5% uas on the standard wsj parsing
task, compared to 88.9% for a greedy discrimina-
tive model with similar features. the model can
accurately parse up to 200 sentences per second.
although this performance is below state-of-the-
art discriminative models, it exceeds existing gen-
erative id33 models in either accu-
racy, speed or both.

as a language model,

the transition-based
parser offers an inexpensive way to incorporate

name

vmod

nmod

nmod

ms. waleson
nnp

nnp

is

a
vbz dt

free-lance writer
nn

jj

based
vbn

figure 1: a partially-derived dependency tree for
the sentence ms. waleson is a free-lance writer
based in new york. the next word to be predicted
by the generative model is based. words in bold
are on the stack.

syntactic structure into incremental word predic-
tion. with supervised training the model   s per-
plexity is comparable to that of id165 models,
although generated examples shows greater syn-
tactic coherence. with semi-supervised learning
over a large unannotated corpus its perplexity is
considerably better than that of a id165 model.

2 generative transition-based parsing

our parsing model is based on transition-based
projective id33 with the arc-
standard parsing strategy (nivre and scholz,
2004). parsing is restricted to (labelled) projective
trees. an arc (i, l, j)     a encodes a dependency
between two words, where i is the head node, j
the dependent and l is the dependency type of j.
in our generative model a word can be represented
by its lexical (word) type and/or its pos tag. we
add a root node to the beginning of the sentence
(although it could also be added at the end of the
sentence), such that the head word of the sentence
is the dependent of the root node.

a parser con   guration (  ,   , a) for sentence s
consists of a stack    of indices in s, an index    to
the next word to be generated, and a set of arcs a.
the stack elements are referred to as   1, . . . ,   |  |,
where   1 is the top element. for any node a,
lc1(a) refers to the leftmost child of a in a, and
rc1(a) to its rightmost child.
the initial con   guration is ([], 0,   ). a terminal
con   guration is reached when    > |s|, and    con-
sists only of the root. a sentence is generated left-
to-right by performing a sequence of transitions.
as a generative model it assigns probabilities to
sentences and dependency trees: a word w (in-
cluding its pos tag) is generated when it is shifted
on to the stack, similar to the generative models
proposed by titov and henderson (2007) and co-
hen et al. (2011), and the joint tagging and parsing
model of bohnet and nivre (2012).

the types of transitions in this model are shift

(sh), left-arc (la) and right-arc (ra):
(shw) (  , i, a) (cid:96) (  |i, i + 1, a)
(lal) (  |i|j,   , a) (cid:96) (  |j,   , a     {(j, l, i)})
(ral) (  |i|j,   , a) (cid:96) (  |i,   , a     {(i, l, j)})

left-arc and right-arc (reduce) transitions add
an arc between the top two words on the stack,
and also generate an arc label l. the parsing strat-
egy adds arcs bottom-up. no arc that would make
the root node the dependent of another node may
be added. to illustrate the generative process, the
con   guration of a partially generated dependency
tree is given in figure 1.

in general parses may have multiple derivations.
in transition-based parsing it is common to de   ne
an oracle o(c, g) that maps the current con   gu-
ration c and the gold parse g to the next transi-
tion that should be performed. in our probabilistic
model we are interested in performing id136
over all latent structure, including spurious deriva-
tions. therefore we propose a non-deterministic
oracle which allows us to    nd all derivations of
g. in contrast to dynamic oracles (goldberg and
nivre, 2013), we are only interested in derivations
of the correct parse tree, so the oracle can assume
that given c there exists a derivation for g.

first, to enforce the bottom-up property our ora-
cle has to ensure that an arc (i, j) in g may only be
added once j has been attached to all its children
    we refer to these arcs as valid. most determin-
istic oracles add valid arcs greedily. second, we
note that if there exists a valid arc between   2 and
  1 and the oracle decides to shift, the same pair
will only occur on the top of the stack again after
a right dependent has been attached to   1. there-
fore right arcs have to be added greedily if they are
valid, while adding a valid left arc may be delayed
if   1 has unattached right dependents in g.

3 probabilistic generative model

our model de   nes a joint id203 distribution
over a parsed sentence with pos tags t1:n, words
w1:n and a transition sequence a1:2n as

p(t1:n, w1:n, a1:2n)

n(cid:89)

(cid:16)

i=1

=

p(ti|ht

mi)p(wi|ti, hw
mi)

(cid:17)

,

p(aj|ha
j )

mi+1(cid:89)

j=mi+1

where mi is the number of transitions that have
been performed when (ti, wi) is generated and
ht, hw and ha are sequences representing the con-
ditioning contexts for the tag, word and transition
distributions, respectively.

in the generative process a shift transition is fol-
lowed by a sequence of 0 or more reduce tran-
sitions. this is repeated until all the words have
been generated and a terminal con   guration of the
parser has been reached. we shall also consider
unlexicalised models, based only on pos tags.

3.1 hierarchical pitman-yor processes
the id203 distributions for predicting words,
tags and transitions are drawn from hierarchical
pitmar-yor process (hpyp) priors. hpyp mod-
els were originally proposed for id165 language
modelling (teh, 2006), and have been applied to
various nlp tasks. a version of approximate in-
ference in the hpyp model recovers interpolated
kneser-ney smoothing (kneser and ney, 1995),
one of the best preforming id165 language mod-
els. the pitman-yor process (pyp) is a general-
ization of the dirichlet process which de   nes a
distribution over distributions over a id203
space x, with discount parameter 0     d < 1,
strength parameter    >    d and base distribution
b. pyp priors encode the power-law distribution
found in the distribution of words.

sampling from the posterior is characterized by
the chinese restaurant process analogy, where
each variable in a sequence is represented by a
customer entering a restaurant and sitting at one of
an in   nite number of tables. let ck be the number
of customers sitting at table k and k the number
of occupied tables. the customer chooses to sit at
a table according to the id203

(cid:40) ck   d

i   1+  
kd+  
i   1+  

1     k     k
k = k + 1,

p (zi = k|z1:i   1) =

where zi is the index of the table chosen by the ith
customer and z1:i   1 is the seating arrangement of
the previous i     1 customers.

all customers at a table share the same dish,
corresponding to the value assigned to the vari-
ables they represent. when a customer sits at an
empty table, a dish is assigned to the table by
drawing from the base distribution of the pyp.

for hpyps, the pyp base distribution can it-
self be drawn from a pyp. the restaurant analogy
is extended to the chinese restaurant franchise,

where the base distribution of a pyp corresponds
to another restaurant. so when a customer sits at a
new table, the dish is chosen by letting a new cus-
tomer enter the base distribution restaurant. all
dishes can be traced back to a uniform base distri-
bution at the top of the hierarchy.

id136 over seating arrangements in the
model is performed with id150, based
on routines to add or remove a customer from a
restaurant. in our implementation we use the ef   -
cient data structures proposed by blunsom et al.
(2009).
in addition to sampling the seating ar-
rangement, the discount and strength parameters
are also sampled, using slice sampling.

in our model tht, whw and aha are hpyps
for the tag, word and transition distributions, re-
spectively. the pyps for the transition prediction
distribution, with conditioning context sequence
1:l, are de   ned hierarchically as
ha

aha

1:l

aha

1:l   1

. . .
a   

    pyp(da
    pyp(da
. . .
    pyp(da

l , aha

l,   a
l   1,   a

1:l   1
l   1, aha

)

)

1:l   2

0 ,   a

0 , uniform),

k and   a

k are the discount and strength
where da
discount parameters for pyps with conditioning
context length k. each back-off level drops one
context element. the distribution given the empty
context backs off to the uniform distribution over
all predictions. the word and tag distributions are
de   ned by similarly-structured hpyps.

the prior speci   es an ordering of the symbols
in the context from most informative to least in-
formative to the distributions being estimated. the
choice and ordering of this context is crucial in the
formulation of our model. the contexts that we
use are given in table 1.

4 decoding
in the standard approach to id125 for
transition-based parsing (zhang and clark, 2008),
the beam stores partial derivations with the same
number of transitions performed, and the lowest-
scoring ones are removed when the size of the
beam exceeds a set threshold. however, in our
model we cannot compare derivations with the
same number of transitions but which differ in the
number of words shifted. one solution is to keep n
separate beams, each containing only derivations
with i words shifted, but this approach leads to

context elements
  1.t,   2.t, rc1(  1).t, lc1(  1).t,   3.t,
rc1(  2).t,   1.w,   2.w
  1.t,   2.t, rc1(  1).t, lc1(  1).t,   3.t,
rc1(  2).t,   1.w,   2.w
  .t,   1.t, rc1(  1).t, lc1(  1).t,   1.w,   2.w

ai

tj

wj

table 1: hpyp prediction contexts for the transi-
tion, tag and word distributions. the context ele-
ments are ordered from most important to least im-
portant; the last elements in the lists are dropped
   rst in the back-off structure. the pos tag of node
s is referred to as s.t and the word type as s.w.

o(n2) decoding complexity. another option is to
prune the beam every time after the next word is
shifted in all derivations     however the number of
reduce transitions that can be performed between
shifts is bounded by the stack size, so decoding
complexity remains quadratic.

we propose a novel linear-time decoding algo-
rithm inspired by particle    ltering (see algorithm
1). instead of specifying a    xed limit on the size of
the beam, the beam size is controlled by setting the
number of particles k. every partial derivation dj
in the beam is associated with kj particles, such
j kj = k. each pass through the beam ad-

that(cid:80)

vances each dj until the next word is shifted.

input: sentence w1:n, k particles.
output: parse tree of arg maxd in beam d.  .
initialize the beam with parser con   guration d with
weight d.   = 1 and d.k = k particles;
for i     1 to n do
search step;
foreach derivation d in beam do

nshift = round(d.k    p(sh|d.ha));
nreduce = d.k     nshift;
if nreduce > 0 then

a = arg maxa(cid:54)=sh p(a|d.ha);
beam.append(dd     d);
dd.k     nreduce;
dd.       dd.      p(a|d.ha);
dd.execute(a);

end
d.k     nshift;
if nshift > 0 then

d.       d.      p(sh|d.ha)   
maxti p(ti|d.ht)p(wi|d.hw);
d.execute(sh);

end

end
selection step;
foreach derivation d in beam do

d.  (cid:48)     d.k  d.  

d(cid:48) d(cid:48).k  d(cid:48).   ;

end
foreach derivation d in beam do

(cid:80)

d.k = (cid:98)d.  (cid:48)    k(cid:99);
if d.k = 0 then

beam.remove(d);

end

end

end
algorithm 1: id125 decoder for arc-
standard generative id33.

at each step, to predict the next transition for
dj, kj is divided proportionally between taking a
shift or reduce transition, according to p(a|dj.ha).
if a non-zero number of particles are assigned
to reduce, the highest scoring left-arc and right-
arc transitions are chosen deterministically, and
derivations that execute them are added to the
beam. in practice we found that adding only the
highest scoring reduce transition (left-arc or right-
arc) gives very similar performance. the shift
transition is performed on the current derivation,
and the derivation weight is also updated with the
word generation id203.

a pos tag is also generated along with a shift
transition. up to three candidate tags are assigned
(more do not improve performance) and corre-
sponding derivations are added to the beam, with
particles distributed relative to the tag id203
(in algorithm 1 only one tag is predicted).

a pass is complete once the derivations in the
beam, including those added by reduce transitions
during the pass, have been iterated through. then
a selection step is performed to determine which

derivations are kept. the number of particles for
each derivation are reallocated based on the nor-
malised weights of the derivations, each weighted
by its current number of particles. derivations to
which zero particles are assigned are eliminated.
the selection step allows the size of the beam to
depend on the uncertainty of the model during de-
coding. the selectional branching method pro-
posed by choi and mccallum (2013) for discrim-
inative beam-search parsing has a similar goal.

after the last word in the sentence has been
shifted, reduce transitions are performed on each
derivation until it reaches a terminal con   guration.
the parse tree corresponding to the highest scor-
ing    nal derivation is returned.

the main differences between our algorithm
and particle    ltering are that we divide particles
proportionally instead of sampling with replace-
ment, and in the selection step we base the redis-
tribution on the derivation weight instead of the
importance weight (the word generation probabil-
ity). our method can be interpreted as maximizing

by sampling from a peaked version of the distribu-
tion over derivations.

5 experiments

5.1 parsing setup
we evaluate our model as a parser on the stan-
dard english id32 (marcus et al., 1993)
setup,
training on wsj sections 02-21, devel-
oping on section 22, and testing on section
23. we use the head-   nding rules of yamada
and matsumoto (2003) (ym)1 for constituency-
to-dependency conversion, to enable comparison
with previous results. we also evaluate on the
stanford dependency representation (de marneffe
and manning, 2008) (sd)2.

words that occur only once in the training
data are treated as unknown words. we clas-
sify unknown words according to capitalization,
numbers, punctuation and common suf   xes into
classes similar to those used in the implementa-
tion of generative constituency parsers such as the
stanford parser (klein and manning, 2003).

as a discriminative baseline we use malt-
parser (nivre et al., 2006), a discriminative,
greedy transition-based parser, performing arc-
standard parsing with liblinear as classi   er. al-
though the accuracy of this model is not state-of-
the-art, it does enable us to compare our model
against an optimised discriminative model with a
feature-set based on the same elements as we in-
clude in our conditioning contexts.

our hpyp dependency parser (hpyp-dp) is
trained with 20 iterations of id150, re-
sampling the hyper-parameters after every itera-
tion, except when performing id136 over la-
tent structure, in which case they are only resam-
pled every 5 iterations. training with a determin-
istic oracle takes 28 seconds per iteration (exclud-
ing resampling hyper-parameters), while a non-
deterministic oracle (sampling with 100 particles)
takes 458 seconds.

5.2 modelling choices
we consider several modelling choices in the con-
struction of our generative id33
model. development set parsing results are given
in table 2. we report unlabelled attachment score

1http://stp.ling   l.uu.se/ nivre/research/penn2malt.html
2converted with version 3.4.1 of the stanford parser,
available at http:/nlp.stanford.edu/software/lex-parser.shtml.

model
maltparser unlex
maltparser lex
unlexicalised
lexicalised, unlex context
lexicalised, tagger pos
lexicalised, predict pos
lexicalised, gold pos

uas
85.23
89.17
85.64
87.95
87.84
89.09
89.30

las
82.80
87.81
82.93
85.04
85.54
86.78
87.28

table 2: hpyp parsing accuracies on the ym de-
velopment set, for various lexicalised and unlexi-
calised setups.

context elements uas
73.25
  1.t,   2.t
80.21
+rc1(  1).t
85.18
+lc1(  1).t
87.23
+  3.t
87.95
+rc1(  2).t
88.53
+  1.w
88.93
+  2.w

las
70.14
76.64
82.03
84.26
85.04
86.11
86.57

table 3: effect of including elements in the model
conditioning contexts. results are given on the
ym development set.

(uas) and labelled attachment score (las), ex-
cluding punctuation.

hpyp priors
the    rst modelling choice is the selection and or-
dering of elements in the conditioning contexts of
the hpyp priors. table 3 shows how the devel-
opment set accuracy increases as more elements
are added to the conditioning context. the    rst
two words on the stack are the most important,
but insuf   cient     second-order dependencies and
further elements on the stack should also be in-
cluded in the contexts. the challenge is that the
back-off structure of each hpyp speci   es an or-
dering of the elements based on their importance
in the prediction. we are therefore much more re-
stricted than classi   ers with large, sparse feature-
sets which are commonly used in transition-based
parsers. due to sparsity, the word types are the
   rst elements to be dropped in the back-off struc-
ture, and elements such as third-order dependen-
cies, which have been shown to improve parsing
performance, cannot be included successfully in
our model.

sampling over parsing derivations during train-
ing further improves performance by 0.16% to

89.09 uas. adding the root symbol at the end of
the sentence rather than at the front gives very sim-
ilar parsing performance. when unknown words
are not clustered according to surface features,
performance drops to 88.60 uas.

pos tags and lexicalisation
it is standard practice in transition-based parsing
to obtain pos tags with a stand-alone tagger be-
fore parsing. however, as we have a generative
model, we can use the model to assign pos tags
in decoding, while predicting the transition se-
quence. we compare predicting tags against us-
ing gold standard pos tags and tags obtain using
the stanford pos tagger3 (toutanova et al., 2003).
even though the predicted tags are slightly less ac-
curate than the stanford tags on the development
set (95.6%), jointly predicting tags and decoding
increases the uas by 1.1%. the jointly predicted
tags are a better    t to the generative model, which
can be seen by an improvement in the likelihood of
the test data. bohnet and nivre (2012) found that
joint prediction increases both pos and parsing
accuracy. however, their model rescored a k-best
list of tags obtained with an preprocessing tagger,
while our model does not use the external tagger
at all during joint prediction.

we train lexicalised and unlexicalised versions
of our model. unlexicalised parsing gives us a
strong baseline (85.6 uas) over which to con-
sider our model   s ability to predict and condition
on words. unlexicalised parsing is also consid-
ered to be robust for applications such as cross-
lingual parsing (mcdonald et al., 2011). addi-
tionally, we consider a version of the model that
don   t include lexical elements in the condition-
ing context. this model performs only 1% uas
lower than the best lexicalised model, although it
makes much stronger independence assumptions.
the main bene   t of lexicalised conditioning con-
texts are to make incremental decoding easier.

speed vs accuracy trade-offs
we consider a number of trade-offs between speed
and accuracy in the model. we compare using
different numbers of particles during decoding, as
well as jointly predicting pos tags against using
pre-obtained tags (table 4).

3we use the ef   cient    left 3 words    model, trained on the
same data as the parsing model, excluding distributional fea-
tures. tagging accuracy is 95.9% on the development set and
96.5% on the test set.

particles

5000
1000
100
10
1000
100
10

sent/sec uas
89.04
88.93
87.99
85.27
87.59
87.46
85.86

18
27
54
104
108
198
333

table 4: speed and accuracy for different con   gu-
rations of the decoding algorithm. above the line,
pos tags are predicted by the model, below pre-
tagged pos are used.

model
eisner (1996)
wallach et al. (2008)
titov and henderson (2007)
hpyp-dp
maltparser
zhang and nivre (2011)
choi and mccallum (2013)

uas
80.7
85.7
89.36
88.47
88.88
92.9
92.96

las

-
-

87.65
86.13
87.41
91.8
91.93

table 5: parsing accuracies on the ym test
set. compared against previous published results.
titov and henderson (2007) was retrained to en-
able direct comparison.

the optimal number of particles is found to be
1000 - more particles only increase accuracy by
about 0.1 uas. although jointly predicting tags
is more accurate, using pre-obtained tags provides
a better trade-off between speed and accuracy    
87.59 against 85.27 uas at around 100 sentences
per second. in comparison, the maltparser parses
around 500 sentences per second.

we also compare our particle    lter-based al-
gorithm against a more standard beam-search al-
gorithm that prunes the beam to a    xed size af-
ter each word is shifted. this algorithm is much
slower than the particle-based algorithm     to get
similar accuracy it parses only 3 sentences per sec-
ond (against 27) when predicting tags jointly, and
29 (against 108) when using pre-obtained tags.

5.3 parsing results
test set results comparing our model against ex-
isting discriminative and generative dependency
parsers are given in table 5. our hpyp model per-
forms much better than eisner   s generative model
as well as the bayesian version of that model pro-
posed by wallach et al. (2008) (the result for eis-

ner   s model is given as reported by wallach et al.
(2008) on the wsj). the accuracy of our model is
only 0.8 uas below the generative model of titov
and henderson (2007), despite that model being
much more powerful. the titov and henderson
model takes 3 days to train, and its decoding speed
is around 1 sentence per second.

the uas of our model is very close to that
of the maltparser. however, we do note that our
model   s performance is relatively worse on las
than on uas. an explanation for this is that as we
do not include labels in the conditioning contexts,
the predicted labels are independent of words that
have not yet been generated.

we also test the model on the stanford de-
pendencies, which have a larger label set. our
model obtains 87.9/83.2 against the maltparser   s
88.9/86.2 uas/las.

despite these promising results, our model   s
performance still lags behind recent discriminative
parsers (zhang and nivre, 2011; choi and mc-
callum, 2013) with beam-search and richer fea-
ture sets than can be incorporated in our model.
in terms of speed, zhang and nivre (2011) parse
29 sentences per second, against the 110 sentences
per second of choi and mccallum (2013). re-
cently proposed neural networks for dependency
parsers have further improved performance (dyer
et al., 2015; weiss et al., 2015), reaching up to
94.0% uas with stanford dependencies.

we argue that the main weakness of the hpyp
parser is sparsity in the large conditioning con-
texts composed of tags and words. the pos tags
in the parser con   guration context already give a
very strong signal for predicting the next transi-
tion. as a result it is challenging to construct pyp
reduction lists that also include word types with-
out making the back-off contexts too sparse.

the other limitation is that our decoding algo-
rithm, although ef   cient, still prunes the search
space aggressively, while not being able to take
advantage of look-ahead features as discriminative
models can. interestingly, we note that a discrimi-
native parser cannot reach high performance with-
out look-ahead features.

5.4 language modelling
next we evaluate our model as a language model.
first we use the standard wsj language modelling
setup, training on sections 00     20, developing
on 21     22 and testing on 23     24. punctua-

tion is removed, numbers and symbols are mapped
to a single symbol and the vocabulary is limited
to 10, 000 words. second we consider a semi-
supervised setup where we train the model, in ad-
dition to the wsj, on a subset of 1 million sen-
tences (24.1 million words) from the wmt en-
glish monolingual training data4. this model is
evaluated on newstest2012.

when training our models for language mod-
elling, we    rst perform standard supervised train-
ing, as for parsing (although we don   t predict la-
bels). this is followed by a second training stage,
where we train the model only on words, regarding
the tags and parse trees as latent structure. in this
unsupervised stage we train the model with parti-
cle id150 (andrieu et al., 2010), using
a particle    lter to sample parse trees. when only
training on the wsj, we perform this step on the
same data, now allowing the model to learn parses
that are not necessarily consistent with the anno-
tated parse trees.

for

semi-supervised training, unsupervised
learning is performed on the large unannotated
corpus. however, here we    nd the highest scoring
parse trees, rather than sampling. only the word
prediction distribution is updated, not the tag and
transition distributions.

language modelling perplexity results are given
in table 6. we note that the perplexities reported
are upper bounds on the true perplexity of the
model, as it is intractable to sum over all possi-
ble parses of a sentence to compute the marginal
id203 of the words. as an approximation we
sum over the    nal beam after decoding.

the results show that on the wsj the model per-
forms slightly better than a hpyp id165 model.
one disadvantage of evaluating on this dataset is
that due to removing punctuation and restricting
the vocabulary, the model parsing accuracy drops
to 84.6 uas. also note that in contrast to many
other evaluations, we do not interpolate with a n-
gram model     this will improve perplexity further.
on the big dataset we see a larger improvement
over the id165 model. this is a promising re-
sult, as it shows that our model can successfully
generalize to larger vocabularies and unannotated
datasets.

4available at http://www.statmt.org/wmt14/translation-

task.html.

model
hpyp 5-gram
chelba and jelinek (2000)
emami and jelinek (2005)
hpyp-dp
hpyp 5-gram
hpyp-dp

perplexity

147.22
146.1
131.3
145.54
178.13
163.96

table 6: language modelling test results. above,
training and testing on wsj. below, training semi-
supervised and testing on wmt.

5.5 generation
to support our claim that our generative model is
a good model for sentences, we generate some ex-
amples. the samples given here were obtained
by generating 1000 samples, and choosing the 10
highest scoring ones with length greater or equal
to 10. the models are trained on the standard wsj
training set (including punctuation).

the examples are given in table 7. the qual-
ity of the sentences generated by the dependency
model is superior to that of the id165 model, de-
spite the models have similar test set perplexities.
the sentences generated by the dependency model
tend to have more global syntactic structure (for
examples having verbs where expected), while re-
taining the local coherence of id165 models. the
dependency model was also able to generate bal-
anced quotation marks.

6 related work

one of the earliest graph-based dependency pars-
ing models (eisner, 1996) is generative, estimating
the id203 of dependents given their head and
previously generated siblings. to counter sparsity
in the conditioning context of the distributions,
backoff and smoothing are performed. wallach et
al. (2008) proposed a bayesian hpyp parameteri-
sation of this model.

other generative models for dependency trees
have been proposed mostly in the context of unsu-
pervised parsing. the    rst successful model was
the dependency model with valence (dmv) (klein
and manning, 2004). several extensions have
been proposed for this model, for example us-
ing structural annaeling (smith and eisner, 2006),
viterbi em training (spitkovsky et al., 2010) or
richer contexts (blunsom and cohn, 2010). how-
ever, these models are not powerful enough for ei-
ther accurate parsing or language modelling with

rich contexts (they are usually restricted to    rst-
order dependencies and valency).

although any generative parsing model can be
applied to language modelling by marginalising
out the possible parses of a sentence, in prac-
tice the success of such models has been lim-
ited. lexicalised pid18s applied to language mod-
elling (roark, 2001; charniak, 2001) show im-
provements over id165 models, but decoding is
prohibitively expensive for practical integration in
language generation applications.

chelba and jelinek (2000) as well as emami
and jelinek (2005) proposed incremental syntac-
tic language models with some similarities to
our model. those models predict binarized con-
stituency trees with a transition-based model, and
are parameterized by deleted interpolation and
neural networks, respectively. rastrow et al.
(2012) applies a transition-based dependency lan-
guage model to id103, using hierar-
chical interpolation and relative id178 pruning.
however, the model perplexity only improves over
an id165 model when interpolated with one.

titov and henderson (2007) introduced a gen-
erative latent variable model for transition-based
parsing. the model is based on an incremental sig-
moid belief networks, using the arc-eager parsing
strategy. exact id136 is intractable, so neural
networks and variational mean    eld methods are
proposed to perform approximate id136. how-
ever, this is much slower and therefore less scal-
able than our model.

a generative transition-based parsing model for
non-projective parsing is proposed in (cohen et
al., 2011), along with a dynamic program for in-
ference. the parser is similar to ours, but the dy-
namic program restricts the conditioning context
to the top 2 or 3 words on the stack. no experi-
mental results are included.

le and zuidema (2014) proposed a recursive
neural network generative model over dependency
trees. however, their model can only score trees,
not perform parsing, and its perplexity (236.58 on
the ptb development set) is worse than model   s,
despite using neural networks to combat sparsity.

finally, incremental parsing with particle    l-
tering has been proposed previously (levy et al.,
2009) to model human online sentence processing.

sales rose num to num million from $ num .
estimated volume was about $ num a share , .
meanwhile , annual sales rose to num % from $ num .
mr. bush    s pro   t climbed num % , to $ num from $ num million million , or num cents a share .
treasury securities inc. is a unit of great issues .
    he is looking out their shareholders ,     says .
while he has done well , she was out .
that    s increased in the second quarter    s new conventional wisdom .
mci communications said net dropped num % for an investor .
association motorola inc. , offering of $ num and num cents a share .
otherwise , actual pro   t is compared with the 300-day estimate .
the companies are followed by at least three analysts , and had a minimum    ve-cent change in actual earnings per share .
bonds : shearson lehman hutton treasury index num , up
posted yields on num year mortgage commitments for delivery within num days .
in composite trading on the new york mercantile exchange .
the company , which has num million shares outstanding .
the num results included a one-time gain of $ num million .
however , operating pro   t fell num % to $ num billion from $ num billion .
merrill lynch ready assets trust : num % num days ; num % num to num days ; num % num to num days .
in new york stock exchange composite trading , one trader .

table 7: sentences generated, above by the generative dependency model, below by a id165 model. in
both cases, 1000 samples were generated, and the most likely sentences of length 10 or more are given.

7 conclusion

we presented a generative id33
model that, unlike previous models, retains most
of
the speed and accuracy of discriminative
parsers. our models can accurately estimate prob-
abilities conditioned on long context sequences.
the model is scalable to large training and test
sets, and even though it de   nes a full probabil-
ity distribution over sentences and parses, decod-
ing speed is ef   cient. additionally, the genera-
tive model gives strong performance as a language
model. for future work we believe that this model
can be applied successfully to natural language
generation tasks such as machine translation.

references
christophe andrieu, arnaud doucet, and roman
holenstein. 2010. particle markov chain monte
the royal statisti-
carlo methods.
cal society: series b (statistical methodology),
72(3):269   342.

journal of

phil blunsom and trevor cohn. 2010. unsupervised
induction of tree substitution grammars for depen-
dency parsing. in emnlp, pages 1204   1213.

phil blunsom, trevor cohn, sharon goldwater, and
mark johnson. 2009. a note on the implementation
of hierarchical dirichlet processes. in acl/ijcnlp
(short papers), pages 337   340.

eugene charniak, kevin knight, and kenji yamada.
2003. syntax-based language models for statistical
machine translation. in proceedings of mt summit
ix, pages 40   46.

eugene charniak. 2001. immediate-head parsing for
in proceedings of acl, pages

language models.
124   131.

ciprian chelba and frederick jelinek. 2000. struc-
tured id38. computer speech & lan-
guage, 14(4):283   332.

danqi chen and christopher d manning. 2014. a fast
and accurate dependency parser using neural net-
works. in emnlp.

jinho d. choi and andrew mccallum.

2013.
transition-based id33 with selec-
tional branching. in acl.

shay b. cohen, carlos g  omez-rodr    guez, and gior-
gio satta. 2011. exact id136 for generative
probabilistic non-projective id33. in
emnlp, pages 1234   1245.

michael collins. 1997. three generative, lexicalised
models for statistical parsing. in acl, pages 16   23.

marie-catherine de marneffe and christopher d man-
ning. 2008. the stanford typed dependencies rep-
in coling 2008: proceedings of the
resentation.
workshop on cross-framework and cross-domain
parser evaluation, pages 1   8.

arnaud doucet, nando de freitas, and neil gordon.
2001. sequential monte carlo methods in practice.
springer.

bernd bohnet and joakim nivre. 2012. a transition-
based system for joint part-of-speech tagging and
labeled non-projective id33.
in
emnlp-conll, pages 1455   1465.

chris dyer, miguel ballesteros, wang ling, austin
matthews, and noah a. smith. 2015. transition-
based id33 with stack long short-
term memory. in proceedings of acl 2015.

jason eisner. 1996. three new probabilistic models
for id33: an exploration. in col-
ing, pages 340   345.

joakim nivre. 2008. algorithms for deterministic in-
cremental id33. computational lin-
guistics, 34(4):513   553.

ahmad emami and frederick jelinek. 2005. a neural
syntactic language model. machine learning, 60(1-
3):195   227.

yoav goldberg and joakim nivre. 2013. training
deterministic parsers with non-deterministic oracles.
tacl, 1:403   414.

liang huang and kenji sagae. 2010. dynamic pro-
in

gramming for linear-time incremental parsing.
acl, pages 1077   1086.

dan klein and christopher d. manning. 2003. accu-
rate unlexicalized parsing. in acl, pages 423   430.

dan klein and christopher d manning. 2004. corpus-
based induction of syntactic structure: models of de-
pendency and constituency. in acl, pages 478   586.

reinhard kneser and hermann ney. 1995. improved
in

backing-off for m-gram id38.
icassp, volume 1, pages 181   184. ieee.

terry koo and michael collins. 2010. ef   cient third-

order dependency parsers. in acl, pages 1   11.

phong le and willem zuidema. 2014. the inside-
outside id56 model for depen-
dency parsing. in emnlp, pages 729   739.

tao lei, yu xin, yuan zhang, regina barzilay, and
tommi jaakkola. 2014. low-rank tensors for scor-
ing dependency structures. in proceedings of acl
(volume 1: long papers), pages 1381   1391.

roger p levy, florencia reali, and thomas l grif   ths.
2009. modeling the effects of memory on human
online sentence processing with particle    lters.
in
advances in neural information processing systems,
pages 937   944.

mitchell p. marcus, beatrice santorini, and mary ann
marcinkiewicz. 1993. building a large annotated
corpus of english: the id32. computa-
tional linguistics, 19(2):313   330.

ryan t. mcdonald, koby crammer, and fernando
c. n. pereira. 2005. online large-margin training
of dependency parsers. in acl.

ryan mcdonald, slav petrov, and keith hall. 2011.
multi-source transfer of delexicalized dependency
in emnlp, pages 62   72. association for
parsers.
computational linguistics.

joakim nivre and mario scholz. 2004. deterministic

id33 of english text. in coling.

joakim nivre, johan hall, and jens nilsson. 2006.
maltparser: a data-driven parser-generator for de-
in proceedings of lrec, vol-
pendency parsing.
ume 6, pages 2216   2219.

slav petrov, leon barrett, romain thibaux, and dan
klein. 2006. learning accurate, compact, and in-
terpretable tree annotation. in coling-acl, pages
433   440.

ariya rastrow, mark dredze, and sanjeev khudanpur.
2012. ef   cient structured id38 for
id103. in interspeech.

brian roark. 2001. probabilistic top-down parsing
and id38. computational linguistics,
27(2):249   276.

noah a. smith and jason eisner. 2006. annealing
structural bias in multilingual weighted grammar in-
in proceedings of coling-acl, pages
duction.
569   576.

valentin i. spitkovsky, hiyan alshawi, daniel juraf-
sky, and christopher d. manning. 2010. viterbi
training improves unsupervised id33.
in conll, pages 9   17.

yee whye teh. 2006. a hierarchical bayesian lan-
in

guage model based on pitman-yor processes.
acl.

ivan titov and james henderson. 2007. a latent vari-
able model for generative id33.
in
proceedings of the tenth international conference
on parsing technologies, pages 144   155.

kristina toutanova, dan klein, christopher d. man-
ning, and yoram singer. 2003. feature-rich part-of-
speech tagging with a cyclic dependency network.
in hlt-naacl, pages 173   180.

hanna m wallach, charles sutton, and andrew mc-
callum. 2008. bayesian modeling of dependency
trees using hierarchical pitman-yor priors. in icml
workshop on prior knowledge for text and lan-
guage processing.

david weiss, chris alberti, michael collins, and slav
petrov. 2015. structured training for neural net-
in proceedings of
work transition-based parsing.
acl 2015.

hiroyasu yamada and yuji matsumoto. 2003. statis-
tical dependency analysis with support vector ma-
chines. in proceedings of iwpt.

yue zhang and stephen clark.

2008. a tale of
two parsers:
investigating and combining graph-
based and transition-based id33. in
emnlp, pages 562   571.

yue zhang and joakim nivre. 2011. transition-based
id33 with rich non-local features. in
acl-hlt short papers-volume 2, pages 188   193.

