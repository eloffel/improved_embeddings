5
1
0
2

 

n
u
j
 

4

 
 
]

v
c
.
s
c
[
 
 

1
v
8
9
6
1
0

.

6
0
5
1
:
v
i
x
r
a

the long-short story of movie description

anna rohrbach1, marcus rohrbach2, and bernt schiele1

1 max planck institute for informatics, saarbr  ucken, germany
2 uc berkeley eecs and icsi, berkeley, ca, united states

abstract. generating descriptions for videos has many applications in-
cluding assisting blind people and id176. the recent
advances in image captioning as well as the release of large-scale movie
description datasets such as mpii-md [28] allow to study this task in
more depth. many of the proposed methods for image captioning rely
on pre-trained object classi   er id98s and long-short term memory re-
current networks (lstms) for generating descriptions. while image de-
scription focuses on objects, we argue that it is important to distinguish
verbs, objects, and places in the challenging setting of movie description.
in this work we show how to learn robust visual classi   ers from the weak
annotations of the sentence descriptions. based on these visual classi   ers
we learn how to generate a description using an lstm. we explore dif-
ferent design choices to build and train the lstm and achieve the best
performance to date on the challenging mpii-md dataset. we compare
and analyze our approach and prior work along various dimensions to
better understand the key challenges of the movie description task.

1

introduction

automatic description of visual content has lately received a lot of interest in
our community. multiple works have successfully addressed the image captioning
problem [6, 16, 17, 35]. many of the proposed methods rely on long-short term
memory networks (lstms) [13]. in the meanwhile, two large-scale movie de-
scription datasets have been proposed, namely mpii movie description (mpii-
md) [28] and montreal video annotation dataset (m-vad) [31]. both are based
on movies with associated textual descriptions and allow studying the problem
how to generate movie description for visually disabled people. works address-
ing these datasets [28, 33, 39] show that they are indeed challenging in terms of
visual recognition and automatic description. this results in a signi   cantly lower
performance then on simpler video datasets (e.g. msvd [2]), but a detailed anal-
ysis of the di   culties is missing. in this work we address this by taking a closer
look at the performance of existing methods on the movie description task.

this work contributes a) an approach to build robust visual classi   ers which
distinguish verbs, objects, and places extracted from weak sentence annotations;
b) based on the visual classi   ers we evaluate di   erent design choices to train
an lstm for generating descriptions. this outperforms related work on the
mpii-md dataset, both using automatic and human evaluation; c) we perform
a detailed analysis of prior work and our approach to understand the challenges
of the movie description task.

2

anna rohrbach, marcus rohrbach, bernt schiele.

2 related work

image captioning. automatic image description has been studied in the past
[9, 19, 20, 24], however it regained attention just recently. multiple works have
been proposed [6, 8, 16, 17, 23, 35, 37]. many of them rely on recurrent neu-
ral networks (id56s) and in particular on long-short term memory networks
(lstms). also new datasets have been released, flickr30k [40] and ms coco
captions [3], where [3] additionally presents a standardized setup for image cap-
tioning evaluation. there are also attempts to analyze the performance of recent
methods. e.g. [5] compares them with respect to the novelty of generated de-
scriptions and additionally proposes a nearest neighbor baseline that improves
over recent methods.

video description. in the past video description has been addressed in semi-
realistic settings [1, 18], on a small scale [4, 11, 30] or in constrained scenarios
like cooking [27, 29]. most works (with a few exceptions, e.g. [27]) study the task
of describing a short clip with a single sentence. [6]    rst proposed to describe
videos using an lstm, relying on precomputed crf scores from [27]. [34] ex-
tended this work to extract id98 features from frames which are max-pooled
over time. they show the bene   t of pre-training the lstm network for image
captioning and    ne-tuning it to video description. [25] proposes a framework
that consists of a 2-d and/or 3-d id98 and the lstm is trained jointly with a
visual-semantic embedding to ensure better coherence between video and text.
[38] jointly addresses the language generation and video/language retrieval tasks
by learning a joint embedding model for a deep video model and compositional
semantic language model.

movie description. recently two large-scale movie description datasets have been
proposed, mpii movie description (mpii-md) [28] and montreal video anno-
tation dataset (m-vad) [31]. given that they are based on movies, they cover
a much broader domain then previous video description datasets. consequently
they are much more varied and challenging with respect to the visual content and
the associated description. they also do not have any additional annotations,
as e.g. tacos multi-level [27], thus one has to rely on the weak annotations
of the sentence descriptions. to handle this challenging scenario [39] proposes
an attention based model which selects the most relevant temporal segments in
a video and incorporates 3-d id98 and generates a sentence using an lstm.
[33] proposes an encoder-decoder framework, where a single lstm encodes the
input video frame by frame and decodes it into a sentence, outperforming [39].
our approach for sentence generation is most similar to [6] and we also rely on
their lstm implementation based on ca   e [15]. however, we analyze di   erent
aspects and variants of this architecture for movie description. to extract labels
from sentences we rely on the semantic parser of [28], however we treat the labels
di   erently to handle the weak supervision (see section 3.1). we show that this
improves over [28] and [33].

the long-short story of movie description.

3

fig. 1: overview of our approach. we    rst train the visual classi   ers for verbs,
objects and places, using di   erent visual features: dt (dense trajectories [36]),
lsda (large scale object detector [14]) and places (places-id98 [41]). next,
we concatenate the scores from a subset of selected robust classi   ers and use
them as input to our lstm.

3 approach

in this section we present our two-step approach to video description. the    rst
step performs visual recognition, while the second step generates textual descrip-
tions. for the visual recognition we propose to use the visual classi   ers trained
according to the labels    semantics and    visuality   . for the language generation
we rely on a lstm network which has been successfully used for image and video
description [6, 33]. we discuss various design choices for building and training
the lstm. an overview of our approach is given in figure 1.

3.1 visual labels for robust visual classi   ers

for training we rely on a parallel corpus of videos and weak sentence annotations.
as in [28] we parse the sentences to obtain a set of labels (single words or short
phrases, e.g. look up) to train our visual classi   ers. however, in contrast to [28]
we do not want to keep all of these initial labels as they are noisy, but select
only visual ones which actually can be robustly recognized.

avoiding parser failure. not all sentences can be parsed successfully, as e.g.
some sentences are incomplete or grammatically incorrect. to avoid loosing the
potential labels in these sentences, we match our set of initial labels to the
sentences which the parser failed to process.

semantic groups. our labels correspond to di   erent semantic groups. in this
work we consider three most important groups: verbs (actions), objects and
places, as they are typically visual. one could also consider e.g. groups like
mood or emotions, which are naturally harder for visual recognition. we pro-
pose to treat each label group independently. first, we rely on a di   erent repre-
sentation for the each semantic groups, which is targeted to the speci   c group.
namely we use the activity recognition feature improved dense trajectories

placeslsdadtplaces class. scoresobject class. scoresverb class. scoreslstmconcatconcatconcatsomeoneentersthelstmlstmselect robust classifiersconcatroomlstm4

anna rohrbach, marcus rohrbach, bernt schiele.

fig. 2: (a-c) lstm architectures. (d) variants of placing the dropout layer.

(dt) [36] for verbs, large scale object detector responses (lsda) [14] for ob-
jects and scene classi   cation scores (places) [41] for places. second, we train
one-vs-all id166 classi   ers for each group separately. the intuition behind this
is to discard    wrong negatives    (e.g. using object    bed    as negative for place
   bedroom   ).

visual labels. now, how do we select visual labels for our semantic groups? in
order to    nd the verbs among the labels we rely on the semantic parser of [28].
next, we look up the list of    places    used in [41] and search for corresponding
words among our labels. we look up the object classes used in [14] and search for
these    objects   , as well as their base forms (e.g.    domestic cat    and    cat   ). we
discard all the labels that do not belong to any of our three groups of interest
as we assume that they are likely not visual and thus are di   cult to recognize.
finally, we discard labels which the classi   ers could not learn, as these are likely
to be noisy or not visual. for this we require the classi   ers to have have minimum
area under the roc-curve (receiver operating characteristic).

3.2 lstm for sentence generation

we rely on the basic lstm architecture proposed in [6] for video description.
as shown in figures 1 and 2(a), at each time step, an lstm generates a word
and receives the visual classi   ers (input-vis) as well as as the previous gener-
ated word (input-lang) as input. to handle natural words we encode each word
with a one-hot-vector according to their index in a dictionary and a lower di-
mensional embedding. the embedding is jointly learned during training of the
lstm. [6] compares three variants: (a) an encoder-decoder architecture, (b) a
decoder architecture with visual max predictions, and (c) a decoder architecture
with visual probabilistic predictions. in this work we rely on variant (c) which
was shown to work best as it can rely on the richest visual input. we analyze
the following aspects for this architecture:

layer structure: we compare a 1-layer architecture with a 2-layer architecture.
in the 2-layer architecture, the output of the    rst layer is used as input for the
second layer (figure 2b) and was used by [6] for video description. additionally

lstm lstm lstm input-lang input-vis input-lang input-vis input-lang input-vis 1 layer lstm lstm lstm input-lang input-vis input-lang input-vis input-lang input-vis lstm lstm lstm 2 layers unfactored lstm lstm lstm input-lang input-vis input-lang input-vis input-lang input-vis lstm lstm lstm 2 layers factored (b) (d) input-vis input-lang concat lang-drop vis-drop concat-drop lstm lstm-drop dropouts (a) (c) the long-short story of movie description.

5

we also compare to a 2-layer factored architecture [6], where the    rst layer only
gets the language as input and the second gets the output of the    rst layer as
well as the visual input.

dropout placement: to learn a more robust network which is less likely to over   t
we rely on a dropout [12]. using dropout a ratio r of randomly selected units is
set to 0 during training (while all others are multiplied with 1/r). we explore
di   erent ways to place dropout in the network, i.e. either for language input
(lang-drop) or visual (vis-drop) input only, for both inputs (concat-drop) or for
the lstm output (lstm-drop), see figure 2(d). while the default dropout ratio
is r = 0.5, we evaluate the e   ect of di   erent ratios.

learning strategy: by default we rely on a step-based learning strategy, where a
learning rate is halved after a certain number of steps. we    nd the best learning
rate and step size on the validation set. additionally we compare this to a poly-
nomial learning strategy, where the learning rate is continuously decreased. the
polynomial learning strategy has been shown to give good results faster without
tweaking step size for googlenet implemented by sergio guadarrama in ca   e
[15].

4 evaluation

in this section we    rst analyze our approach on the mpii-md [28] dataset and
explore di   erent design choices. then, we compare our best system to prior work.

4.1 analysis of our approach

experimental setup. we build on the labels discovered by our semantic parser
[28] and additionally match these labels to sentences which the parser failed to
process. to be able to learn classi   ers we select the labels that appear at least
30 times, resulting in 1,263 labels. the parser additionally tells us whether the
label is a verb. we use the visual features (dt, lsda, places) provided with
the mpii-md dataset [28]. the lstm output/hidden unit as well as memory
cell have each 500 dimensions. we train the id166 classi   ers on the training set
(56,861 clips). we evaluate our method on the validation set (4,930 clips) using
the meteor [21] score, which, according to [7, 32], supersedes other popular
measures, such as id7 [26], id8 [22], in terms of agreement with human
judgments. the authors of cider [32] showed that meteor also outperforms
cider when the number of references is small and in the case of mpii-md we
have typically only a single reference.

robust visual classi   ers. in a    rst set of experiments we analyze our proposal
to consider groups of labels to learn di   erent classi   ers and also to use di   erent
visual representations for these groups (see section 3.1). table 1 we evaluate our
generated sentences using di   erent input features to the lstm. in our baseline,

6

anna rohrbach, marcus rohrbach, bernt schiele.

approach

classi   ers

labels retrieved trained

baseline: all labels treated the same way
(1) dt
(2) lsda
(3) places
(4) dt+lsda+places
visual labels
(5) verbs(dt), others(lsda)
(6) verbs(dt), places(places), others(lsda)
(7) verbs(dt), places(places), objects(lsda)
(8) + restriction to labels with roc     0.7
baseline: all labels treated the same way, labels from (8)
(9) dt+lsda+places

1328
1328
913
263

1263
1263
1263
1263

263

-
-
-
-

7.08
7.09
7.10
7.41

7.16

6.73
7.07
7.10
7.24

7.27
7.39
7.48
7.54

7.20

table 1: comparison of di   erent choices of labels and visual classi   ers. all results
reported on the validation set of mpii-md.

in the top part of table 1, we treat all labels equally, i.e. we use the same
visual descriptors for all labels. the places feature is best with 7.1 meteor.
combination by stacking all features (dt + lsda + places) improves further
to 7.24 meteor.

the second part of the table demonstrates the e   ect of introducing di   erent
semantic label groups. we    rst split the labels into    verbs    and all remaining.
given that some labels appear in both roles, the total number of labels increases
to 1328. we analyze two settings of training the classi   ers. in the case of    re-
trieved    we retrieve the classi   er scores from the general classi   ers trained in the
previous step.    trained    corresponds to training the id166s speci   cally for each
label type (e.g. for    verbs   ). next, we further divide the non-verbal labels into
   places    and    others   , and    nally into    places    and    objects   . we discard the
unused labels and end up with 913 labels. out of these labels, we select the labels
where the classi   er obtains a roc higher or equal to 0.7 (threshold selected on
the validation set). after this we obtain 263 labels and the best performance
in the    trained    setting. to support our intuition about the importance of the
label discrimination (i.e. using di   erent features for di   erent semantic groups
of labels), we propose another baseline (last line in the table). here we use the
same set of 263 labels but provide the same feature for all of them, namely the
best performing combination dt + lsda + places. as we see, this results
in an inferior performance.

we make several observations from table 1 which lead to robust visual clas-
si   ers from the weak sentence annotations. a) it is bene   cial to select features
based on the label semantics. b) training one-vs-all id166s for speci   c label
groups consistently improves the performance as it avoids    wrong    negatives. c)
focusing on more    visual    labels helps: we reduce the lstm input dimension-
ality to 263 while improving the performance.

the long-short story of movie description.

7

dropout m et eor

architecture m et eor

1 layer
2 layers unfact.
2 layers fact.

7.54
7.54
7.41

no dropout
lang-drop
vis-drop
concat-drop
lstm-drop

7.19
7.13
7.34
7.29
7.54

dropout ratio m et eor

r=0.1
r=0.25
r=0.5
r=0.75

7.22
7.42
7.54
7.46

(a) lstm architectures.

(b) dropout strategies.

(c) dropout ratios.

table 2: (a) di   erent lstm architectures, used lstm-dropout 0.5. (b) compar-
ison of di   erent dropout strategies in 1-layer lstm with dropout value=0.5.
(c) comparison of di   erent dropout ratios in 1-layer lstm with lstm-dropout.
labels and classi   ers from table 1 (8). on validation set of mpii-md.

approach

m et eor

lr=0.005, step=2000
lr=0.01, step=2000
lr=0.02, step=2000

lr=0.005, step=4000
lr=0.01, step=4000
lr=0.02, step=4000

lr=0.005, step=6000
lr=0.01, step=6000
lr=0.02, step=6000

7.30
7.54
7.51

7.49
7.59
7.28

7.40
7.40
7.32

approach

m et eor

step=2000, iter=25,000
step=4000, iter=25,000
step=6000, iter=25,000
step=8000, iter=25,000

poly, pow=0.5, maxiter=25,000
poly, pow=0.5, maxiter=10,000
poly, pow=0.7, maxiter=25,000
poly, pow=0.7, maxiter=10,000

7.54
7.59
7.40
7.32

7.36
7.45
7.43
7.43

(a) base learning rates

(b) learning strategies with lr=0.01.

table 3: (a) comparison of di   erent base learning rates, network trained for
25,000 iterations. (b) comparison of di   erent learning strategies with base
lr=0.01. all results reported on the validation set of mpii-md.

lstm architectures. now, as described in section 3.2, we look at di   erent
lstm architectures and training con   gurations. in the following we use the best
performing    visual labels    approach, table 1 (8).

we start with examining the architecture, where we explore di   erent con-
   gurations of lstm and dropout layers. table 2a shows the performance of
three di   erent networks:    1 layer   ,    2 layers unfactored    and    2 layers factored   
introduced in section 3.2. as we see, the    1 layer    and    2 layers unfactored   
perform equally well, while    2 layers factored    is inferior to them. in following
experiments we use the simplest    1 layer    network. we then compare di   erent
dropout placements as illustrated in (figure 2b). we obtain the best result when
applying dropout after the lstm layer (   lstm-drop   ), while having no dropout
or applying it only to language leads to stronger over-   tting to the visual fea-
tures. putting dropout after the lstm (and prior to a    nal prediction layer)
makes the entire system more robust. as for the best dropout ratio, we    nd that
0.5 works best with lstm-dropout table 2c.

we compare di   erent learning rates and learning strategies in tables 3a and
3b. we    nd that the best learning rate in the step-based learning is 0.01, while

8

anna rohrbach, marcus rohrbach, bernt schiele.

approach

m et eor

1 net: lr 0.01, step 2000, iter=25,000
ensemble of 3 nets

1 net: lr 0.01, step 4000, iter=25,000
ensemble of 3 nets

1 net: lr 0.01, step 4000, iter=15,000
ensemble of 3 nets

7.54
7.52

7.59
7.68

7.55
7.72

table 4: ensembles of networks with di   erent random initializations. all results
reported on the validation set of mpii-md.

step 4000 slightly improves over step 2000 (which we used in table 1). we explore
an alternative learning strategy, namely decreasing learning rate according to a
polynomial decay. we experiment with di   erent exponents (0.5 and 0.7) and
numbers of iterations (25k and 10k), using the base-learning rate 0.01. our
results show that the step-based learning is superior to the polynomial learning.
in most of experiments we trained our networks for 25,000 iterations. after
looking at the meteor performance for intermediate iterations we found that
for the step size 4000 at iteration 15,000 we achieve best performance overall.
additionally we train multiple lstms with di   erent random orderings of the
training data. in our experiments we combine three in an ensemble, averaging
the resulting word predictions. in most cases the ensemble improves over the
single networks in terms of meteor score (see table 4).

to summarize, the most important aspects that decrease over-   tting and
lead to a better sentence generation are: (a) a correct learning rate and step
size, (b) dropout after the lstm layer, (c) choosing the training iteration based
on meteor score as opposed to only looking at the lstm accuracy/loss which
can be misleading, and (d) building ensembles of multiple networks with di   erent
random initializations. in the following section we evaluate our best ensemble
(last line of table 4) on the test set of mpii-md.

4.2 comparison to related work

experimental setup. we compare the best method of [28], the recently proposed
method s2vt [33] and our proposed    visual labels   -lstm on the test set of
the mpii-md dataset (6,578 clips). we report all popular automatic evaluation
measures, cider [32], id7 [26], id8 [22] and meteor [21], computed
using the evaluation code of [3]. we also perform a human evaluation, by ran-
domly selecting 1300 video snippets and asking amt turkers to rank three
systems (the best smt of [28], s2vt [33] and ours) with respect to correctness,
grammar and relevance, similar to [28].

results. table 5 summarizes the results on the test set of mpii-md. while we
rely on identical features and similar labels as [28], we signi   cantly improve the
performance in all automatic measures, speci   cally by 1.44 meteor points.

the long-short story of movie description.

9

approach

human evaluation: rank
cider id7 @4 rou gel m et eor correct. grammar relev.

automatic score

best smt of [28]
s2vt [33]
our

8.14
9.00
9.98

0.47
0.49
0.80

13.21
15.32
16.02

5.59
6.27
7.03

nn upperbound

169.64

9.42

44.04

19.43

2.11
2.02
1.87

-

2.39
1.67
1.94

-

2.08
2.06
1.86

-

table 5: comparison of prior work and our proposed method using all popular
evaluation measures. human scores in form of ranking from 1 to 3, where lower
is better. all results reported on the test set of mpii-md.

moreover, we improve over the recent approach of [33], which also uses lstm to
generate video descriptions. exploring di   erent strategies to label selection and
classi   er training, as well as various lstm con   gurations allows to obtain best
result to date on the mpii-md dataset. human evaluation mainly agrees with
the automatic measures. we outperform both prior works in terms of correctness
and relevance, however we lose to s2vt in terms of grammar. this is due to
the fact that s2vt produces overall shorter (7.4 versus 8.7 words per sentence)
and simpler sentences, while our system generates longer sentences and therefore
has higher chances to make mistakes.

we also propose a retrieval upperbound (last line in table 5). for every test
sentence we retrieve the closest training sentence according to the meteor.
the rather low meteor score of 19.43 re   ects the di   culty of the dataset.

a closer look at the sentences produced by all three methods gives us addi-
tional insights. an interesting characteristic is the output vocabulary size, which
is 94 for [28], 86 for [33] and 605 for our method, while the test set contains
6422 unique words. this clearly shows a higher diversity of our output. among
the words generated by our system and absent in the outputs of others are such
verbs as grab, drive, sip, climb, follow, objects as suit, chair, cigarette, mirror,
bottle and places as kitchen, corridor, restaurant. we showcase some qualitative
results in figure 3. here, e.g. the verb pour, object drink and place courtyard
only appear in our output. we attribute this, on one hand, to our diverse and ro-
bust visual classi   ers. on the other hand, the architecture and parameter choices
of our lstm allow us to learn better correspondance between words and visual
classi   ers    scores.

5 analysis

despite the recent advances in the video description domain, including our pro-
posed approach, the video description performance on the movie description
datasets (mpii-md [28] and m-vad [31]) remains relatively low. in this section
we want to take a closer look at three methods, best smt of [28], s2vt [33] and
ours, in order to understand where these methods succeed and where they fail.
in the following we evaluate all three methods on the mpii-md test set.

10

anna rohrbach, marcus rohrbach, bernt schiele.

approach

sentence

someone is a man, someone is a man.

smt [28]
s2vt [33] someone looks at him, someone turns to someone.
our

someone is standing in the crowd,
a little man with a little smile.
someone, back in elf guise, is trying to calm the kids.

reference

smt [28] the car is a water of the water.
s2vt [33] on the door, opens the door opens.
our
the fellowship are in the courtyard.
reference they cross the quadrangle below and run along the cloister.

smt [28]

someone is down the door,
someone is a back of the door, and someone is a door.

s2vt [33] someone shakes his head and looks at someone.
our
reference

someone takes a drink and pours it into the water.
someone grabs a vodka bottle standing open on the counter
and liberally pours some on the hand.

fig. 3: qualitative comparison of prior work and our proposed method. examples
from the test set of mpii-md. our approach identi   es activities, objects, and
places better than related work.

(a) sentence length

(b) word frequency

fig. 4: meteor score per sentence. (a) test set sorted by sentence length (in-
creasing). (b) test set sorted by word frequency (decreasing). shown values are
smoothed with a mean    lter of size 500.

5.1 di   culty versus performance

as the    rst study we suggest to sort the reference sentences (from the test set)
by di   culty, where di   culty is de   ned in multiple ways.

sentence length and word frequency. two of the simplest sentence
di   culty measures are its length and average frequency of words. when sorting
the data by di   culty (increasing sentence length or decreasing average word
frequency), we    nd that all three methods have the same tendency to obtain
lower meteor score as the di   culty increases (figures 4a and 4b). for the

1000200030004000500060000246810121416<   short        sentences          long   >meteor  best smt of [28]s2vt [33]our1000200030004000500060000246810121416<   short        sentences          long   >meteor  best smt of [28]s2vt [33]ourthe long-short story of movie description.

11

(a) textual di   culty

(b) visual di   culty

fig. 5: meteor score per sentence. (a) test set sorted by textual nn score
(decreasing). (b) test set sorted by visual knn score, k = 10 (decreasing).
shown values are smoothed with a mean    lter of size 500.

word frequency the correlation is stronger. our method consistently outperforms
the other two, most notable as the di   culty increases.

textual and visual nearest neighbors. next, for each reference test
sentence we search for the closest training sentence (in terms of the meteor
score). we use the obtained best scores to sort the reference sentences by textual
di   culty, i.e. the    easy    sentences are more likely to be retrieved. if we consider
all training sentences, we obtain a textual nearest neighbor. we sort the test
sentences according to these scores (decreasing) and plot the performance of
three methods in figure 5a. all methods    agree    and ours is best throughout
the di   culty range, in particular in the more challenging part of the plot. we
can also use visual features to    nd the k nearest neighbors in the training set,
select the best one (in terms of the meteor score) and use this score to sort
the reference sentences. we call this a visual k nearest neighbor. the intuition
behind it is to consider a video clip as visually    easy    if the most similar training
clips also have similar descriptions (the    di   cult    clip might have no close visual
neighbours). we rely on our best visual representation (8) from table 1 and cos
similarity measure to de   ne the visual knn and sort the reference sentences
according to it with k = 10 (figure 5b). we see a clear correlation between the
visual di   culty and the performance of all methods (figure 5b).

summary. a) all methods perform better on shorter, common sentences
and our method notably wins on longer sentences. b) our method also wins
on sentences that are more di   cult to retrieve. c) visual di   culty, de   ned by
cos similarity and representation (8) from table 1, strongly correlates with the
performance of all methods. (d) when comparing all four plots (figures 4a
and 4b, figures 5a and 5b), we    nd that the strongest correlation between the
methods    performance and the di   culty is observed for the textual di   culty,
while the least correlation we observe for the sentence length.

1000200030004000500060000246810121416<   easy         sentences       difficult   >meteor  best smt of [28]s2vt [33]our1000200030004000500060000246810121416<   easy         sentences       difficult   >meteor  best smt of [28]s2vt [33]our12

anna rohrbach, marcus rohrbach, bernt schiele.

fig. 6: average meteor score for id138 verb topics. selected sentences with
single verb, number of sentences in brackets.

topic

id178 top-1 top-2

top-3 top-4

top-5

motion
7.05
contact
7.10
perception
4.83
stative
4.84
change
6.92
communication 6.73
body
5.04
social
6.11
cognition
5.21
possession
5.29
none
5.04
creation
5.69
competition
5.19
consumption
4.52
emotion
6.19
weather
3.93

shake move
hold
stand
see
watch
stop
go
emerge    ll
face
dress
do
read
have
   y
do

walk
turn
sit
open
stare
look
follow
be
reveal
start
look up nod
smile wear
watch join
look at see
give
take
throw hold
make
hit
walk over point
drive
drink
use
startle
draw
shine
blaze

eat
feel
light up drench

speak
grin
close
take
stand in
lie
walk through come up
play
take
touch

face
sip
enjoy
blow

go
pull
gaze
wait
make
talk
glare
make
leave
   nd
rush

table 6: id178 and top 5 frequent verbs of each id138 topic in the mpii-
md.

5.2 semantic analysis

id138 verb topics. we closer analyze the test sentences with respect
to di   erent verbs. for this we rely on id138 topics (high level entries in
the id138 ontology, e.g.    motion   ,    perception   ,    competition   ,    emotion   ),
de   ned for most synsets in id138 [10]. we obtain the sense information from
the semantic parser of [28], thus senses might be noisy. we showcase the 5 most
frequent verbs for each topic in table 6. we select sentences with a single verb,
group them according to the verb topic and compute an average meteor
score for each topic, see figure 6. we    nd that our method is best for all topics
except    communication   , where [28] wins. the most frequent verbs in this topic
are    look up    and    nod   , which are also frequent in the dataset and in the
sentences produced by [28]. the best performing topic,    cognition   , is highly
biased to    look at    verb. the most frequent topics,    motion    and    contact   ,

02468101214motion (960)contact (562)perception (492)stative (346)change (235)communication (197)body (192)social (139)cognition (120)possession (101)none (80)competition (54)creation (47)consumption (33)emotion (29)weather (7)meteor  best smt of [28]s2vt [33]ourthe long-short story of movie description.

13

which are also visual (e.g.    turn   ,    walk   ,    open   ,    sit   ), are nevertheless quite
challenging, which we attribute to their high diversity (see their id178 w.r.t.
di   erent verbs and their frequencies in table 6). at the same time    perception   
is far less diverse and mainly focuses on verbs like    look    or    stare   , which are
quite frequent in the dataset, resulting in better performance. topics with more
abstract verbs (e.g.    be   ,    have   ,    start   ) tend to get lower scores.

top 100 best and worst sentences. we look at 100 test sentences, where
our method obtains highest and lowest meteor scores. out of 100 best sen-
tences 44 contain the verb    look    (including verb phrases such as    look at   ).
the other frequent verbs are    walk   ,    turn   ,    smile   ,    nod   ,    shake   ,    stare   ,
   sit   , i.e. mainly visual verbs. overall the sentences are simple and common.
among the 100 lowest scoring sentences we observe more diversity: 12 sentences
contain no verb, 10 mention unusual words (speci   c to the movie), 24 contain no
subject, 29 have a non-human subject. altogether this leads to a lower perfor-
mance, in particular, as most training sentences contain    someone    as subject
and generated sentences are biased towards it.

summary. a) the test sentences that mention the verb    look    (and similar)
get higher meteor scores due to their high frequency in the dataset. b) the
sentences with more    visual    verbs tend to get higher scores. c) the sentences
without verbs (e.g. describing a scene), without subjects or with non-human
subjects get lower scores, which can be explained by a dataset bias towards
   someone    as subject.

6 conclusion

we propose an approach to automatic movie description which trains visual
classi   ers and uses the classi   er scores as input to lstm. to handle the weak
sentence annotations we rely on three main ingredients. first, we distinguish
three semantic groups of labels (verbs, objects and places), second we train them
discriminatively, removing potentially noisy negatives, and third, we select only a
small number of the most reliable classi   ers. for sentence generation we show the
bene   ts of exploring di   erent lstm architectures and learning con   gurations.
as the result we obtain the highest performance on the mpii-md dataset as
shown by all automatic evaluation measures and extensive human evaluation.

we analyze the challenges in the movie description task using our and two
prior works. we    nd that the factors which contribute to higher performance
include: presence of frequent words, sentence length and simplicity as well as
presence of    visual    verbs (e.g.    nod   ,    walk   ,    sit   ,    smile   ). textual and vi-
sual di   culties of sentences/clips strongly correlate with the performance of all
methods. we observe a high bias in the data towards humans as subjects and
verbs similar to    look   . future work has to focus on dealing with less frequent
words and handle less visual descriptions. this potentially requires to consider
external text corpora, modalities other than video, such as audio and dialog, and
to look across multiple sentences. this would allow exploiting long- and short-
range context and thus understanding and describing the story of the movie.

14

anna rohrbach, marcus rohrbach, bernt schiele.

acknowledgements. marcus rohrbach was supported by a fellowship within the
fitweltweit-program of the german academic exchange service (daad). the
authors thank niket tandon for help with the id138 topics analysis.

references

1. barbu, a., bridge, a., burchill, z., coroian, d., dickinson, s., fidler, s., michaux,
a., mussman, s., narayanaswamy, s., salvi, d., schmidt, l., shangguan, j.,
siskind, j.m., waggoner, j., wang, s., wei, j., yin, y., zhang, z.: video in
sentences out. in: proceedings of the conference on uncertainty in arti   cial in-
telligence (uai) (2012)

2. chen, d., dolan, w.: collecting highly parallel data for paraphrase evaluation. in:
proceedings of the annual meeting of the association for computational linguis-
tics (acl) (2011)

3. chen, x., fang, h., lin, t., vedantam, r., gupta, s., dollr, p., zitnick, c.l.:
microsoft coco captions: data collection and evaluation server. arxiv:1504.00325
(2015)

4. das, p., xu, c., doell, r., corso, j.: thousand frames in just a few words: lingual
description of videos through latent topics and sparse object stitching. in: pro-
ceedings of the ieee conference on id161 and pattern recognition
(cvpr) (2013)

5. devlin, j., cheng, h., fang, h., gupta, s., deng, l., he, x., zweig, g.,
mitchell, m.: language models for image captioning: the quirks and what works.
arxiv:1505.01809 (2015)

6. donahue, j., hendricks, l.a., guadarrama, s., rohrbach, m., venugopalan, s.,
saenko, k., darrell, t.: long-term recurrent convolutional networks for visual
recognition and description. in: proceedings of the ieee conference on computer
vision and pattern recognition (cvpr) (2015)

7. elliott, d., keller, f.: image description using visual dependency representations.

in: emnlp. pp. 1292   1302 (2013)

8. fang, h., gupta, s., iandola, f.n., srivastava, r., deng, l., doll  ar, p., gao, j.,
he, x., mitchell, m., platt, j.c., zitnick, c.l., zweig, g.: from captions to visual
concepts and back. in: proceedings of the ieee conference on id161
and pattern recognition (cvpr) (2015)

9. farhadi, a., hejrati, m., sadeghi, m., young, p., rashtchian, c., hockenmaier,
j., forsyth, d.: every picture tells a story: generating sentences from images. in:
proceedings of the european conference on id161 (eccv) (2010)

10. fellbaum, c.: id138: an electronical lexical database. the mit press (1998)
11. guadarrama, s., krishnamoorthy, n., malkarnenkar, g., venugopalan, s., mooney,
r., darrell, t., saenko, k.: youtube2text: recognizing and describing arbitrary
activities using semantic hierarchies and zero-shoot recognition. in: proceedings of
the ieee international conference on id161 (iccv) (2013)

12. hinton, g.e., srivastava, n., krizhevsky, a., sutskever, i., salakhutdinov, r.r.:
feature detectors.

improving neural networks by preventing co-adaptation of
arxiv:1207.0580 (2012)

13. hochreiter, s., schmidhuber, j.: long short-term memory. neural computation

(1997)

14. ho   man, j., guadarrama, s., tzeng, e., donahue, j., girshick, r., darrell, t.,
saenko, k.: lsda: large scale detection through adaptation. in: advances in
neural information processing systems (nips) (2014)

the long-short story of movie description.

15

15. jia, y., shelhamer, e., donahue, j., karayev, s., long, j., girshick, r., guadar-
rama, s., darrell, t.: ca   e: convolutional architecture for fast feature embedding.
arxiv:1408.5093 (2014)

16. karpathy, a., fei-fei, l.: deep visual-semantic alignments for generating image
descriptions. in: proceedings of the ieee conference on id161 and
pattern recognition (cvpr) (2015)

17. kiros, r., salakhutdinov, r., zemel, r.s.: unifying visual-semantic embeddings
with multimodal neural language models. transactions of the association for com-
putational linguistics (tacl) (2015)

18. kojima, a., tamura, t., fukunaga, k.: natural language description of human
activities from video images based on concept hierarchy of actions. international
journal of id161 (ijcv) (2002)

19. kulkarni, g., premraj, v., dhar, s., li, s., choi, y., berg, a.c., berg, t.l.: baby
talk: understanding and generating simple image descriptions. in: proceedings of
the ieee conference on id161 and pattern recognition (cvpr) (2011)
20. kuznetsova, p., ordonez, v., berg, t.l., hill, u.c., choi, y.: treetalk: compo-
sition and compression of trees for image descriptions. in: transactions of the
association for computational linguistics (tacl) (2014)

21. lavie, m.d.a.: meteor universal: language speci   c translation evaluation for any

target language. acl 2014 p. 376 (2014)

22. lin, c.y.: id8: a package for automatic evaluation of summaries. in: text sum-
marization branches out: proceedings of the acl-04 workshop. pp. 74   81 (2004)
23. mao, j., xu, w., yang, y., wang, j., yuille, a.l.: deep captioning with multimodal

recurrent neural networks (m-id56). arxiv:1412.6632 (2014)

24. mitchell, m., dodge, j., goyal, a., yamaguchi, k., stratos, k., han, x., mensch,
a., berg, a.c., berg, t.l., iii, h.d.: midge: generating image descriptions from
id161 detections. in: proceedings of the conference of the european
chapter of the association for computational linguistics (eacl) (2012)

25. pan, y., mei, t., yao, t., li, h., rui, y.: jointly modeling embedding and trans-

lation to bridge video and language. arxiv:1505.01861 (2015)

26. papineni, k., roukos, s., ward, t., zhu, w.j.: id7: a method for automatic
evaluation of machine translation. in: proceedings of the annual meeting of the
association for computational linguistics (acl) (2002)

27. rohrbach, a., rohrbach, m., qiu, w., friedrich, a., pinkal, m., schiele, b.: coher-
ent multi-sentence video description with variable level of detail. in: proceedings
of the german confeence on pattern recognition (gcpr) (2014)

28. rohrbach, a., rohrbach, m., tandon, n., schiele, b.: a dataset for movie descrip-
tion. in: proceedings of the ieee conference on id161 and pattern
recognition (cvpr) (2015)

29. rohrbach, m., qiu, w., titov, i., thater, s., pinkal, m., schiele, b.: translat-
ing video content to natural language descriptions. in: proceedings of the ieee
international conference on id161 (iccv) (2013)

30. thomason, j., venugopalan, s., guadarrama, s., saenko, k., mooney, r.j.: in-
tegrating language and vision to generate natural language descriptions of videos
in the wild. in: proceedings of the international conference on computational
linguistics (coling) (2014)

31. torabi, a., pal, c., larochelle, h., courville, a.: using descriptive video services
to create a large data source for video annotation research. arxiv:1503.01070v1
(2015)

16

anna rohrbach, marcus rohrbach, bernt schiele.

32. vedantam, r., zitnick, c.l., parikh, d.: cider: consensus-based image descrip-
tion evaluation. in: proceedings of the ieee conference on id161 and
pattern recognition (cvpr) (2015)

33. venugopalan, s., rohrbach, m., donahue, j., mooney, r., darrell, t., saenko, k.:

sequence to sequence     video to text. arxiv:1505.00487 (2015)

34. venugopalan, s., xu, h., donahue, j., rohrbach, m., mooney, r., saenko, k.:
translating videos to natural language using deep recurrent neural networks. in:
proceedings of the conference of the north american chapter of the association
for computational linguistics (naacl) (2015)

35. vinyals, o., toshev, a., bengio, s., erhan, d.: show and tell: a neural image
caption generator. in: proceedings of the ieee conference on id161
and pattern recognition (cvpr) (2015)

36. wang, h., schmid, c.: action recognition with improved trajectories. in: proceed-

ings of the ieee international conference on id161 (iccv) (2013)

37. xu, k., ba, j., kiros, r., courville, a., salakhutdinov, r., zemel, r., bengio,
y.: show, attend and tell: neural image id134 with visual attention.
arxiv:1502.03044 (2015)

38. xu, r., xiong, c., chen, w., corso, j.j.: jointly modeling deep video and compo-
sitional text to bridge vision and language in a uni   ed framework. in: proceedings
of the conference on arti   cial intelligence (aaai) (2015)

39. yao, l., torabi, a., cho, k., ballas, n., pal, c., larochelle, h., courville, a.:

describing videos by exploiting temporal structure. arxiv:1502.08029v4 (2015)

40. young, p., lai, a., hodosh, m., hockenmaier, j.: from image descriptions to visual
denotations: new similarity metrics for semantic id136 over event descriptions.
transactions of the association for computational linguistics (tacl) 2, 67   78
(2014)

41. zhou, b., lapedriza, a., xiao, j., torralba, a., oliva, a.: learning deep features
for scene recognition using places database. advances in neural information
processing systems (nips) (2014)

