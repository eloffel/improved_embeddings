5
1
0
2

 

y
a
m
1
3

 

 
 
]

v
c
.
s
c
[
 
 

1
v
8
7
2
0
0

.

6
0
5
1
:
v
i
x
r
a

visual madlibs: fill in the blank image generation and id53

licheng yu, eunbyung park, alexander c. berg, and tamara l. berg

department of computer science, university of north carolina, chapel hill

{licheng, eunbyung, acberg, tlberg}@cs.unc.edu

abstract

in this paper, we introduce a new dataset consisting of
360,001 focused natural language descriptions for 10,738
images. this dataset, the visual madlibs dataset, is col-
lected using automatically produced    ll-in-the-blank tem-
plates designed to gather targeted descriptions about: peo-
ple and objects, their appearances, activities, and interac-
tions, as well as id136s about the general scene or its
broader context. we provide several analyses of the vi-
sual madlibs dataset and demonstrate its applicability to
two new description generation tasks: focused description
generation, and multiple-choice question-answering for im-
ages. experiments using joint-embedding and deep learn-
ing methods show promising results on these tasks.

1. introduction

much of everyday language and discourse concerns the
visual world around us, making understanding the rela-
tionship between the physical world and language describ-
ing that world an important challenge problem for ai.
understanding this complex and subtle relationship will
have broad applicability toward inferring human-like under-
standing for images, producing natural human robot interac-
tions, and for tasks like natural language grounding in nlp.
in id161, along with improvements in deep learn-
ing based visual recognition, there has been an explosion of
recent interest in methods to automatically generate natural
language descriptions for images [5, 9, 15, 32, 16, 20] or
videos [31, 8]. however, most of these methods and exist-
ing datasets have focused on only one type of description, a
generic description for the entire image.

in this paper, we collect a new dataset of focused, tar-
geted, descriptions, the visual madlibs dataset, as illus-
trated in figure 1. to collect this dataset, we introduce au-
tomatically produced    ll-in-the-blank templates designed to
collect a range of different descriptions for visual content in
an image. for example, a user might be presented with an

figure 1: an example from the visual madlibs dataset.
this dataset collects targeted descriptions for people and
objects, denoting their appearances, affordances, activities,
and interactions.
it also provides descriptions of broader
emotional, spatial and temporal context for an image.

image and a    ll-in-the-blank template such as    the frisbee
is [blank]    and asked to    ll in the [blank] with a descrip-
tion of the appearance of frisbee. alternatively, they could
be asked to    ll in the [blank] with a description of what
the person is doing with the frisbee. fill-in-the-blank ques-
tions can be targeted to collect descriptions about people
and objects, their appearances, activities, and interactions,
as well as descriptions of the general scene or the broader
emotional, spatial, or temporal context of an image. us-
ing these templates, we collect a large collection of 360,001

1

figure 2: madlibs description. the    rst row corresponds to question types 1-5, the second row corresponds to question types
9-11, and the third row is to question types 6-8 and question type 12. all question types are listed in table 1.

targeted descriptions for 10,738 images. fig. 2 shows some
madlibs description samples.

with this new dataset, we can develop methods to gen-
erate more focused descriptions. instead of asking an algo-
rithm to    describe the image    we can now ask for more fo-
cused descriptions such as    describe the person   ,    describe
what the person is doing,    or    describe the relationship be-
tween the person and the frisbee.    we can also ask ques-
tions about aspects of an image that are somewhat beyond
the scope of the directly depicted content. for example,
   describe what might have happened just before this picture
was taken.    or    describe how this image makes you feel.   
these types of descriptions reach toward high-level goals of
producing human-like visual interpretations for images.

in addition to focused description generation, we also in-
troduce a multiple-choice question-answering task for im-
ages. in this task, the computer is provided with an image
and a partial description such as    the person is [blank]   .
a set of possible answers is also provided, one answer that
was written about the image in question, and several ad-

ditional answers written about other images. the com-
puter is evaluated on how well it can select the correct
choice.
in this way, we can evaluate performance of de-
scription generation on a concrete task, making evaluation
more straightforward. varying the dif   culty of the nega-
tive answers   adjusting how similar they are to the correct
answer   provides a nuanced measurement of performance.
for both the generation and question-answering tasks,
we study and evaluate a recent state of the art approach
for image description generation [32], as well as a simple
joint-embedding method learned on deep representations.
the evaluation also includes extensive analysis of the vi-
sual madlibs dataset and comparisons to the existing ms
coco dataset of natural language descriptions for images.
in summary, our contributions are:
1) a new description collection strategy, visual madlibs, for
constructing    ll-in-the-blank templates to collect targeted
natural language descriptions.
2) a new visual madlibs dataset consisting of 360,001 tar-
geted descriptions, spanning 12 different types of templates,

2

for 10,738 images, as well as analysis of the dataset and
comparisons to existing ms coco descriptions.
3) evaluation of a generation method and a simple joint em-
bedding method for targeted description generation.
4) de   nition and evaluation of generation and joint-
embedding methods on a new task, multiple-choice    ll-in-
the-blank id53 for images.

the rest of our paper is organized as follows. first, we
review related work (sec 2). then, we describe our strat-
egy for automatically generating    ll-in-the-blank templates
and introduce our visual madlibs dataset (sec 3). next we
outline the multiple-choice id53 and targeted
generation tasks (sec 4) and provide several analyses of our
dataset (sec 5). finally, we provide experiments evaluating
description generation and joint-embedding methods on the
proposed tasks (sec 6) and conclude (sec 7).
2. related work

description generation: recently, there has been an
explosion of interest in methods for producing natural lan-
guage descriptions for images or video. early work in this
area generally explored two complementary directions. the
   rst type of approach focused on detecting content elements
such as objects, attributes, activities, or spatial relationships
and then composing captions for images [18, 33, 26, 10]
or videos [17] using linguistically inspired templates. the
second type of approach explored methods to make use of
existing text either directly associated with an image [11, 1]
or retrieved from visually similar images [27, 19, 24].

with the advancement of deep learning for content es-
timation, there have been many exciting recent attempts to
generate image descriptions using neural network based ap-
proaches. some methods    rst detect words or phrases using
convolutional neural network (id98) features, then gen-
erate and re-rank candidate sentences [9, 20]. other ap-
proaches take a more end-to-end approach to generate out-
put descriptions directly from images. kiros et al. [16]
learn a joint image-sentence embedding using visual id98s
and long short term memory (lstm) networks. simi-
larly, several other methods have made use of id98 features
and lstm or recurrent neural networks (id56) for gener-
ation with a variety of different architectures [32, 15, 5].
these new methods have shown great promise for image
description generation under some measures (e.g. id7-
1) achieving near-human performance levels. we look at
related, but more focused description generation tasks.

description datasets: along with the development of
image captioning algorithms there have been a number of
datasets collected for this task. one of the    rst datasets col-
lected for this problem was the uiuc pascal sentence data
set [10] which contains 1,000 images with 5 sentences per
image written by workers on amazon mechanical turk. as
the description problem gained popularity larger and richer

datasets were collected, including the flickr8k [28] and
flickr30k [34] datasets, containing 8,000 and 30,000 im-
ages respectively. in an alternative approach, the sbu cap-
tioned photo dataset [27] contains 1 million images with ex-
isting captions collected from flickr. this dataset is larger,
but the text tends to contain more contextual information
since captions were written by the photo owners. most re-
cently, microsoft released the ms coco [21] dataset. ms
coco contains 120,000 images depicting 80 common ob-
ject classes, with object segmentations and 5 turker writ-
ten descriptions per image. these datasets have been one
of the driving forces in improving methods for description
generation, but are currently limited to a single description
about the general content of an image. we make use of ms
coco data, extending the types of descriptions associated
with images.

language

question-answering natural

question-
answering has been a long standing goal of nlp, with
commercial companies like ask-jeeves or google playing a
signi   cant role in developing effective methods. recently,
embedding and deep learning methods have shown great
promise for question-answering [30, 3, 4]. lin et al. [22]
take an interesting multi-modal approach to question-
answering. a multiple-choice text-based question is    rst
constructed from 3 sentences written about an image; 2 of
the sentences are used as the question, and 1 is used as the
positive answer, mixed with several negative answers from
sentences written about other images. the authors develop
ranking methods to answer these questions and show that
generating abstract images for each potential answer can
improve results. note, here the algorithms are not provided
with an image as part of the question. some recent work
has started to look at the problem of question-answering
for images. malinowski et al. [23] combine computer
vision and nlp in a bayesian framework, but restrict their
method to scene based questions. geman et al. [12] design
a visual turing test to test image understanding using a
series of binary questions about image content. we design
more general question-answering tasks that allow us to ask
a variety of different types of natural language questions
about images.
3. designing and collecting visual madlibs

the goal of visual madlibs is to study targeted natural
language descriptions of image content that go beyond
describing which objects are in the image, and beyond
generic descriptions of the whole image. the experiments
in this paper begin with a dataset of images where the
presence of some objects have already been labeled1. the
prompts for the madlibs-style    ll-in-the-blank questions
are automatically generated based on image content, in a

1more generally, acquiring such labels could be included as part of

collecting madlibs.

3

type

1. image   s scene
2. image   s emotion
3. image   s interesting
4. image   s past
5. image   s future
6. object   s attribute
7. object   s affordance
8. object   s position
9. person   s attribute
10. person   s activity
11. person   s location
12. pair   s relationship

instruction

prompt

describe the type of scene/place shown in this picture.
describe the emotional content of this picture.
describe the most interesting or unusual aspect of this picture.
describe what happened immediately before this picture was taken.
describe what happened immediately after this picture was taken.
describe the appearance of the indicated object.
describe the function of the indicated object.
describe the position of the indicated object.
describe the appearance of the indicated person/people.
describe the activity of the indicated person/people.
describe the location of the indicated person/people.
describe the relationship between the indicated person and object.

.

.

the place is a(n)
when i look at this picture, i feel
the most interesting aspect of this picture is
one or two seconds before this picture was taken,
one or two seconds after this picture was taken,
the object(s) is/are
people could
the object(s) is/are
the person/people is/are
the person/people is/are
the person/people is/are
the person/people is/are

.
.
.
the object(s).

the object(s).

.

.

.

.

.

#words
4+1.45
8+1.14
8+3.14
9+5.45
9+5.04

3.20+1.62
4.20+1.74
3.20+3.35

3+2.52
3+2.47

3.20+3.04
5.20+1.65

table 1: all 12 types of madlibs instructions and prompts. right-most column shows the average number of words for each
description (#words for prompt + #words for answer).

manner designed to elicit more detailed descriptions of the
objects, their interactions, and the broader context of the
scene shown in each image.

visual madlibs: image+instruction+prompts+blank
a single    ll-in-the-blank question consists of a prompt and
a blank, e.g., person a is [blank] the car. the implicit ques-
tion is,    what goes in the blank?    this is presented to a
person along with an image and instructions, e.g., describe
the relationship between the indicated person and object.
the same image and prompt may be used with different in-
structions to collect a variety of description types.
instantiating questions
while the general form of the questions for the visual
madlibs were chosen by hand, see table 1, most of the ques-
tions are instantiated depending on a subset of the objects
present in an image. for instance, if an image contained
two people and a dog, questions about each person (ques-
tion types 9-11 in table 1), the dog (types 6-8), relationships
between the two people and the dog (type 12), could be in-
stantiated. for each possible instantiation, the wording of
the questions might alter slightly to maintain grammatical
consistency. in addition to these types of questions that de-
pend on the objects present in the image, other questions
(types 1-5) can be instantiated for an image regardless of
the objects present.

notice in particular the questions about the temporal
context     what might have happened before or what might
happen after the image was taken. people can make in-
ferences beyond the speci   c content depicted in an image.
sometimes these id136s will be consistent between peo-
ple (e.g., when what will happen next is obvious), and other
times these descriptions may be less consistent. we can
use the variability of returned responses to select images
for which these id136s are reliable.

asking questions about every object and all pairs of ob-
jects quickly becomes unwieldy as the number of objects
increases. to combat this, we choose a subset of objects

present to use in instantiating questions. such selection
could be driven by a number of factors. the experiments
in this paper consider comparisons to existing, general, de-
scriptions of images, so we instantiate questions about the
objects mentioned in those existing natural language de-
scriptions. whether an object is mentioned in an image
description can be viewed as an indication of the object   s
importance [2].
3.1. data collection

to collect the visual madlibs dataset we use a subset of
10,738 human-centric images from ms coco, that make
up about a quarter of the validation data [21], and instanti-
ate    ll-in-the-blank templates as described above. the ms
coco images are annotated with a list of objects present in
the images, segmentations for the locations of those objects,
and 5 general natural language descriptions of the image. to
select the subset of images for collecting madlibs, we start
with the 19,338 images with a person labeled. we then look
at the    ve descriptions for each and perform a dependency
parse [7], only keeping those images where a word referring
to a person (woman, man, etc. e.g., in fig. 3, guys, men) is
the head noun for part of the parse. this leaves 14,150 im-
ages. we then    lter out the images whose descriptions do
not include a synonym for any of the 79 non-person object
categories labeled in the ms coco dataset. this leaves
10,738 human-centric images with at least one other object
from the ms coco data set mentioned in the general im-
age descriptions.

before    nal instantiation of the    ll-in-the blank tem-
plates, we need to resolve a potential ambiguity regarding
which objects are referred to in the descriptions. there
could be several different people or different instances of an
object type labeled in an image. it is not immediately obvi-
ous which ones are described in the sentences. to address
this assignment problem, we estimate the quantity of each
described person/object in the sentence by parsing the de-
terminant (two men and a frisbee in fig. 3), the conjunction
(a man and a woman), and the singular/plural form (dog,

4

dogs). we compare this number with the number of anno-
tated instances for each category, and consider two possible
cases: 1) there are fewer annotated instances than the sen-
tences describe, 2) there are more annotated instances than
the sentences describe. it is easy to address the    rst case,
just construct templates for all of the labeled instances. for
the second case, we sort the area of each segmented in-
stance, and pick the largest ones up to the parsed number
for instantiation. using this procedure, we obtain 26,148
labeled object or person instances in the 10,738 images.

each visual madlib is answered by 3 workers on ama-
zon   s mechanical turk. to date, we have collected 360,001
answers to madlib questions. some example madlibs an-
swers are shown in fig. 2,

figure 3: coco instance annotation and descriptions for
the image of fig. 1. we show how we map labeled instances
to the mentioned person and object in the sentence.

4. tasks: multiple-choice id53

and targeted generation
we design two tasks to evaluate targeted natural lan-
guage description for images. the    rst task is to automat-
ically generate natural language descriptions of images to
   ll in the blank for one of the madlibs questions. this
allows for producing targeted descriptions such as: a de-
scription speci   cally focused on the appearance of an ob-
ject, or a description about the relationship between two
objects. the input to this task is an image, instructions,
and a madlibs prompt. as has been discussed at length in
the community working on description generation for im-
ages, it can be dif   cult to evaluate free form generation.
our second task tries to address this issue by developing
a new targeted multiple-choice id53 task for
images. here the input is again an image, instruction, and
a prompt, but instead of a free form text answer, there are
a    xed set of multiple-choice answers to    ll in the blank.
the possible multiple-choice answers are sampled from the
madlibs responses, one that was written for the particular
image/instruction/prompt as the correct answer, and distrac-
tors chosen from either similar images or random images
depending on the level of dif   culty desired. this ability to
choose distractors to adjust the dif   culty of the question as
well as the relative ease of evaluating multiple choice an-
swers are attractive aspects of this new task.

5

in our experiments we randomly select 20% of the
10,738 images to use as our test set for evaluating these
tasks. for the multiple-choice questions we form two sets of
answers for each, with one set designed to be more dif   cult
than the other. we    rst establish the easy task distractor an-
swers by randomly choosing three descriptions (of the same
question type) from other images [22]. the hard task is de-
signed more delicately. instead of randomly choosing from
the other images, we now only look for those containing
the same objects as our question image, and then arbitrarily
pick three of their descriptions. sometimes, the descriptions
sampled from    similar    images could also be good answers
for our questions (later we experiment with using turkers to
select less ambiguous multiple-choice questions from this
set). for the targeted generation task, for question types
1-5, algorithms generate descriptions given the image, in-
structions, and prompt. for the other question types whose
prompts are related to some speci   c person or object, we
additionally provide the algorithm with the location of each
person/object mentioned in the prompt. we also experiment
with estimating these locations using object detectors.
5. analyzing the visual madlibs dataset

we begin by conducting quantitative analyses of the re-
sponses collected in the visual madlibs dataset in sec. 5.1.
a main goal is understanding what additional information is
provided by the targeted descriptions in the visual madlibs
dataset vs general image descriptions. the ms coco
dataset [21] collects general image descriptions following a
similar methodology to previous efforts for collecting gen-
eral image descriptions, e.g. [28, 34]. so, we provide further
analyses comparing the visual madlibs to the ms coco
descriptions collected for the same images in sec. 5.2
5.1. quantifying visual madlibs responses

we analyze the length, structure, and consistency of the
visual madlibs responses. first, the average length of each
type of description is shown in the far right column of ta-
ble 1. note that descriptions of people tend to be longer
than descriptions of other objects in the dataset2.

second, we use the phrase chunking [6] to analyze which
phrasal structures are commonly used to    ll in the blanks
for different questions. fig. 4, top row, shows relative fre-
quencies for the top-5 most frequent templates used for sev-
eral question types. object attributes are usually described
brie   y with a simple adjectival phrase. on the other hand,
people use more words and a wider variety of structure to
describe possible future events. except for future and past
descriptions, the distribution of structures is generally con-
centrated on a few likely choices for each question type.

2also note that the length of the prompts varies slightly depending on
the object names used to instantiate the madlib, hence the fractional values
in the mean length of the prompts shown in gray.

figure 4: first row shows top-5 most frequent phrase templates for image   s future, object   s attribute, object   s affordance and
person   s activity. second row shows the histograms of similarity between answers.

third, we analyze how consistent the mechanical turk
workers    answers are for each type of question. to com-
pute a measure of similarity between a pair of responses we
use the cosine similarity between representations of each
response. a response is represented by the mean of the
id97 [25] vectors for each word in the response, fol-
lowing [22, 20]. id97 is a 300 dimensional embedding
representation for words that encodes the distributional con-
text of words learned over very large word corpora. this
measure takes into account the actual words used in a re-
sponse, as opposed to the previous analyses of parse struc-
ture. each visual madlibs question is answered by three
workers, providing 3 pairs for which similarity is computed.
fig. 4, bottom row, shows a histogram of all pairwise simi-
larities for several question types. generally the similarities
have a normal-like distribution with an extra peak around 1
indicating the fraction of responses that agree almost per-
fectly. once again, descriptions of the future and past are
least likely to be (near) identical, while object attributes and
affordances are often very consistent.

5.2. visual madlibs vs general descriptions

we compare the targeted descriptions in the visual
madlibs dataset to the general image descriptions in ms
coco. first, we analyze the words used in visual madlibs
compared to ms coco descriptions of the same images.
for each image, we extract the unique set of words from all
descriptions of that image from both datasets, and compute
the coverage of each set with respect to the other. we    nd
that on average (across images) 22.45% of the madlibs   s
words are also present in mscoco descriptions, while
52.38% of the coco words are also present in madlibs.

second, we compare how madlibs and ms coco an-
swers describe the people and objects in images. we ob-

figure 5: template used for parsing person   s attributes,
activity and interaction with object, and object   s attribute.
the percentages below compares madlibs and mscoco
on how frequent these templates are used for description.

figure 6: frequency that a word in a position in the people
and object parsing template in one dataset is in the same
position for the other dataset.

serve that the madlibs questions types, table 1, cover much
of the information in ms coco descriptions [20]. as one
way to see this, we run the stanfordnlp parser3 on both
datasets. for attributes of people, we use the parsing tem-
plate shown in fig. 5(a) to analyze the structures being used.

3http://nlp.stanford.edu/software/lex-parser.

shtml

6

pr: np vp nppr: np vppr: np vp pp nppr: np vp advppr: np vp prt np020%40%60%80%100%one or two seconds after this  picture was taken, ___ .pr:= np pp np vp o ___ oimage's futurepr: vp adjppr: vp nppr: vppr: vp pp nppr: vp advp020%40%60%80%100%the object(s) is/are [blank] .pr:= np ___ oobject's attributepr: vppr: vp pppr: vp np pppr: vp np nppr: vp pp np pp020%40%60%80%100%people could ___ the object(s) .pr:= np ___ np oobject's affordancepr: vp nppr: vppr vp pp nppr: vp np pp nppr: vp advp020%40%60%80%100%the person/people is/are ___ .pr:= np ___ operson's activity00.20.40.60.81.005%10%15%20%25%image's future00.20.40.60.81.005%10%15%20%25%object's attribute00.20.40.60.81.005%10%15%20%25%object's affordance00.20.40.60.81.005%10%15%20%25%person's activitythe refer name indicates whether the person was mentioned
in the description. note that the madlibs descriptions al-
ways have one reference to a person in the prompt (the
person is [blank].). therefore, for madlibs, we report the
presence of additional references to the person (e.g., the
person is a man). the general attribute directly describes
the appearance of the person or object (e.g., old or small);
the af   liate object indicates whether additional objects are
used to describe the targeted person (e.g. with a bag, coat,
or glasses) and the af   liate attribute are appearance char-
acteristics of those secondary objects (e.g., red coat). the
templates for object   s attribute and verbs are more straight-
forward as shown in fig. 5(b)(c). the table in fig. 5 shows
the frequency of each parse component. overall, more of
the potential descriptive elements in these constructions are
used in response to the madlibs prompts than in the general
descriptions found in ms coco.

we also break down the overlap between visual madlibs
and ms coco descriptions over different parsing tem-
plates for descriptions about people and object (fig. 6).
yellow bars show how often words for each parse type in
mscoco descriptions were also found in the same parse
type in the visual madlibs answers, and green bars measure
the reverse direction. observations indicate that madlibs
provides more coverage in its descriptions than ms coco
for all templates except for person   s refer name. one possi-
ble reason is that the prompts already indicates    the person   
or    people    explicitly, so workers need not add an additional
reference to the person in their descriptions.
extrinsic comparison of visual madlibs data and gen-
eral descriptions: here we provide an extrinsic analysis of
the information available in the general descriptions com-
pared to visual madlibs. we perform this analysis by using
either: a) the ms coco descriptions for an image, or b)
visual madlibs responses from other turkers for an image,
to select answers for our multiple-choice evaluation task.
speci   cally, we use one of the human provided descrip-
tions, either from madlibs or from ms coco, and select
the multiple-choice answer that is most similar to that de-
scription. similarity is measured as cosine similarity be-
tween the mean id97 vectors for the words a descrip-
tion compared to the id97 vectors of the multiple-
choice answers.
in addition to comparing how well the
madlibs or ms coco descriptions can select the correct
multiple-choice answer, we also use the descriptions au-
tomatically produced by a recent natural language genera-
tion system (id98+lstm [32], implementation from [15])
trained on ms coco dataset. this allows us to make one
possible measurement of how close current automatically
generated image descriptions are to our madlibs descrip-
tions. fig. 7 shows the accuracies resulting from using
madlibs, mscoco, or id98+lstm [32] to select the cor-
rect multiple-choice answer.

figure 7: the accuracy of madlibs, ms coco and
id98+lstm [32](trained on ms coco) used as refer-
ences to answer the madlibs hard multiple-choice ques-
tions.

although this approach is quite simple, it allows us we
make two interesting observations. first, madlibs outper-
forms ms coco on all types of multiple-choice questions.
if madlibs and ms coco descriptions provided the same
information, we would expect their performance to be com-
parable. presumably the performance increase for madlibs
is due to the coverage of targeted descriptions compared
to ms coco   s sentences that describe the overall image
content more generally. second, the automatically gen-
erated descriptions from the pre-trained id98+lstm per-
form much worse than the actual ms coco descriptions,
despite doing quite well on general image description gen-
eration (the id7-1 score of id98+lstm, 0.67, is near
human agreement 0.69 on ms coco [32]).
6. experiments

in this section we evaluate a series of methods on the vi-
sual madlibs dataset for the targeted natural language gen-
eration and multiple-choice id53 tasks, in-
troduced in sec. 4. as methods, we evaluate simple joint-
embedding methods     canonical correlation analysis (cca)
and normalized cca (ncca) [14]     as well as a recent
deep-learning based method for image description gener-
ation     id98+lstm [32]. we train these models on 80%
of the images in the madlibs collection and evaluate their
performance on the remaining 20%.

in our experiments we extract image features using the
vgg convolutional neural network (id98) [29]. this
model has been trained on the ilsvrc-2012 dataset to rec-

7

020%40%60%80%image's sceneimage's emotionimage's interestingimage's pastimage's futureobject's attributeobject's affordanceobject's positionperson's attributeperson's activityperson's locationpair's relationshipmadlibsmscocoid98+lstm(coco)1. scene
2. emotion
3. past
4. future
5. interesting
6. obj attr
7. obj aff
8. obj pos
9. per attr
10. per act
11. per loc
12. pair rel

1. scene
2. emotion
3. past
4. future
5. interesting
6. obj attr
7. obj aff
8. obj pos
9. per attr
10. per act
11. per loc
12. pair rel

1. scene
2. emotion
3. future
4. past
5. interesting
6. obj attr
7. obj aff
8. obj pos
9. per attr
10. per act
11. per loc
12. pair rel

#q
6277
5138
4903
4658
5095
7194
7326
7290
6651
6501
6580
7595

#q
6277
5138
4903
4658
5095
7194
7326
7290
6651
6501
6580
7595

#q
4938
1936
3628
3811
4061
5313
3829
5240
4887
5707
4992
5976

cca
75.7%
41.3%
61.8%
61.2%
66.8%
44.1%
59.8%
53.0%
40.4%
70.0%
69.8%
54.3%

ncca
(bbox)
   
   
   
   
   
54.7%
72.2%
58.9%
53.1%
75.6%
73.8%
64.2%

easy task
ncca
86.8%
49.2%
77.5%
78.0%
76.5%
47.5%
73.0%
65.9%
48.0%
80.7%
82.7%
63.0%
hard task
ncca
70.1%
37.2%
52.8%
54.3%
53.7%
43.6%
63.5%
55.7%
38.6%
65.4%
63.3%
54.3%

ncca
(bbox)
   
   
   
   
   
49.8%
63.0%
50.7%
46.1%
65.1%
57.8%
56.5%

cca
63.8%
33.9%
47.9%
47.5%
51.4%
42.2%
54.5%
49.0%
33.9%
59.7%
56.8%
49.4%
filtered questions from hard
cca
70.4%
43.7%
52.0%
51.8%
56.5%
45.3%
62.7%
53.8%
36.5%
62.1%
63.2%
52.2%

ncca
(bbox)
   
   
   
   
   
54.5%
72.0%
55.5%
52.2%
68.1%
63.0%
60.0%

ncca
77.6%
49.4%
60.2%
58.0%
60.1%
47.1%
72.3%
61.2%
42.4%
68.6%
70.2%
57.6%

ncca
(all)
87.6%
42.4%
80.3%
80.2%
78.9%
50.9%
76.7%
69.7%
44.5%
82.8%
82.7%
67.2%

ncca
(all)
68.2%
33.2%
54.0%
53.3%
55.1%
39.3%
48.5%
53.4%
31.6%
66.6%
62.6%
52.0%

ncca
(all)
76.3%
44.2%
59.4%
60.1%
61.7%
43.0%
59.2%
58.6%
34.4%
69.9%
70.3%
56.5%

id98+lstm

(madlibs)

71.1%
34.0%
35.8%
40.0%
39.8%
45.4%
   
50.9%
37.3%
63.7%
59.2%
   

id98+lstm

(madlibs)

60.5%
32.7%
32.0%
34.3%
33.3%
40.3%
   
44.9%
36.1%
53.6%
49.3%
   

id98+lstm

(madlibs)

66.3%
34.5%
33.4%
31.1%
35.5%
43.4%
   
47.6%
37.1%
54.9%
51.5%
   

table 2: accuracies computed for different approaches on
the easy and hard multiple-choice answering task, and the
   ltered hard question set. cca, ncca, and id98+lstm
are trained on the whole image representation for each
type of question. ncca(box) is trained and evaluated on
ground-truth bounding-boxes from coco segmentations.
ncca(all) trains a single embedding using all question
types.

6. obj attr
9. per attr

#q
2021
4206

ncca
47.6%
50.2%

easy task

ncca
(bbox)
53.6%
55.4%

ncca
(dbox)
51.4%
51.2%

hard task

ncca
(bbox)
47.9%
47.0%

ncca
(dbox)
45.2%
43.3%

ncca
43.9%
40.0%

table 3: multiple-choice answering using automatic de-
tection for 42 object/person categories.
   bbox    denotes
ground-truth bounding box and    dbox    denotes detected
bounding box.

side, we average the id97 of all words in a sentence to
obtain a 300 dimensional representation.

cca is an approach for    nding a joint embedding be-
tween two multi-dimensional variables, in our case image
and text vector representations. in an attempt to increase the
   exibility of the feature selection and for improving com-
putational ef   ciency, gong et al. [14] proposed a scalable
approximation scheme of explicit kernel mapping followed
by dimension reduction and linear cca. in the projected
latent space, the similarity is measured by the eigenvalue-
weighted normalized correlation. this method, ncca, pro-
vides high-quality retrieval results, improving over the orig-
inal cca performance signi   cantly [14].

we train cca and ncca models for each question type
separately using the training portion of the visual madlibs
dataset. these models allow us to map from an image rep-
resentation, to the joint-embedding space, to vectors in the
id97 space, and vice versa. for targeted generation,
we map an image to the joint-embedding space and then
choose the answer from the training set text that is closest to
this embedded point. in order to answer a multiple-choice
question we embed each multiple choice answer, and then
select the answer who   s embedding is closest to image.

following the recent    show and tell    description gener-
ation technique [32] (using an implementation from [15]),
we train a id98+lstm model for each question type on
the visual madlibs training set. this approach has demon-
strated state of the art performance on generating general
natural language descriptions for images. these models
directly learn a mapping from an image to a sequence of
words which we can use to evaluate the targeted genera-
tion task. note that we input the words from the prompt,
e.g., the chair is, and then let the id98+lstm system
generate the remaining words of the description4. for the
multiple choice task, we compute cosine similarity between
id97 representations of the generated description and
each question answer and select the most similar answer.

6.1. discussion of results

table 2 shows accuracies of each algorithm on the easy
and hard versions of the multiple-choice task. fig. 8, shows
example correct and wrong answer choices. there are sev-
eral interesting observations we can make. first, train-
ing ncca on all types of question together, labeled as
ncca(all), is helpful for the easy variant of the task, how-
ever it is less useful on the       ne-grained    hard version of the
task. second, extracting visual features from the bounding
box of the relevant person/object yields higher accuracy for
predicting attributes, but not for other questions. based on
this    nding, we try answering the attribute question using
automatic detection methods. the detectors are trained on

ognize images depicting 1000 object classes, and generates
a 4,096 dimensional image representation. on the sentence

4the missing entries for questions 7 and 12 are due to this priming

failing for a fraction of the questions.

8

figure 8: some example question-answering results from ncca. first row shows correct choices. second row shows incor-
rect choices.

id98+lstm ncca
0.17

ncca
0.52
0.17
0.38
0.39
0.49
0.28
0.56
0.53
0.26
0.47
0.52
0.46

id7-1
ncca(bbox )
   
   
   
   
   
0.36
0.60
0.55
0.29
0.41
0.46
0.48

1. scene
2. emotion
3. future
4. past
5. interesting
6. obj attr
7. obj aff
8. obj pos
9. per attr
10. per act
11. per loc
12. pair rel

0.62
0.39
0.32
0.42
0.51
0.45
   
0.71
0.55
0.52
0.64
   

ncca(box)

id7-2
   
   
   
   
   
0.02
0.11
0.25
0.07
0.11
019
0.08

id98+lstm

0.19

0

0.08
0.11
0.15
0.01
   
0.50
0.25
0.22
0.39
   

0

0.12
0.12
0.14
0.02
0.10
0.24
0.06
0.14
0.22
0.07

table 4: id7-1 and id7-2 computed on madlibs testing dataset for different approaches.

id163 using r-id98 [13], covering 42 ms coco cat-
egories. we observe similar performance between ground-
truth and detected bounding boxes in table 3.

as an additional experiment we ask humans to answer
the multiple choice task, with 5 turkers answering each
question. we use their results to    lter out a subset of
the hard multiple-choice questions where at least 3 turk-
ers choose the correct answer. results of the methods on
this subset are shown in table 2 bottom set of rows. these
results show the same pattern as on the un   ltered set, with
slightly higher accuracy.

7. conclusions

we have introduced a new    ll-in-the blank strategy for
targeted natural language descriptions and used this to col-
lect a visual madlibs dataset. our analyses show that these
descriptions are usually more detailed than generic whole
image descriptions. we also introduce a targeted natu-
ral language description generation task, and a multiple-
choice id53 task, then train and evaluate
joint-embedding and generation models. data produced by
this paper will be publicly released upon acceptance.

acknowledgement

table 4 shows id7-1 and id7-2 scores for targeted
generation. although the id98+lstm models we trained
on madlibs were not quite as accurate as ncca for selecting
the correct multiple-choice answer, they did result in better,
sometimes much better, accuracy (as measured by id7
scores) for targeted generation.

we thank the vision and language community for feed-
back regarding this dataset, especially julia hockenmaier,
kate saenko, and jason corso. this research is supported
by nsf awards #1417991, 1405822, 144234, and 1452851,
and microsoft research.

9

references
[1] a. aker and r. gaizauskas. generating image descriptions

using dependency relational patterns. in acl, 2010.

[2] a. c. berg, t. l. berg, h. d. iii, j. dodge, a. goyal, x. han,
a. mensch, m. mitchell, a. sood, k. stratos, and k. yam-
aguchi. understanding and predicting importance in images.
in cvpr, 2012.

[3] a. bordes, s. chopra, and j. weston. id53
with subgraph embeddings. arxiv preprint arxiv:1406.3676,
2014.

[4] a. bordes, j. weston, and n. usunier. open question an-
swering with weakly supervised embedding models.
in
machine learning and knowledge discovery in databases.
springer, 2014.

[5] x. chen and c. l. zitnick. learning a recurrent visual rep-
arxiv preprint

resentation for image id134.
arxiv:1411.5654, 2014.

[6] r. collobert,

j. weston, l. bottou, m. karlen,
k. kavukcuoglu, and p. kuksa. natural language pro-
cessing (almost) from scratch. jmlr, 2011.

[7] m.-c. de marneffe, b. maccartney, c. d. manning, et al.
generating typed dependency parses from phrase structure
parses. in proceedings of lrec, 2006.

[8] j. donahue, l. a. hendricks, s. guadarrama, m. rohrbach,
s. venugopalan, k. saenko, and t. darrell. long-term recur-
rent convolutional networks for visual recognition and de-
scription. arxiv preprint arxiv:1411.4389, 2014.

[9] h. fang, s. gupta, f. iandola, r. srivastava, l. deng,
p. doll  ar, j. gao, x. he, m. mitchell, j. platt, et al.
from captions to visual concepts and back. arxiv preprint
arxiv:1411.4952, 2014.

[10] a. farhadi, m. hejrati, m. a. sadeghi, p. young,
c. rashtchian, j. hockenmaier, and d. forsyth. every pic-
ture tells a story: generating sentences from images.
in
eccv. 2010.

[11] y. feng and m. lapata. topic models for image annotation

and text illustration. in acl, 2010.

[12] d. geman, s. geman, n. hallonquist, and l. younes. visual
turing test for id161 systems. proceedings of the
national academy of sciences, 2015.

[13] r. girshick, j. donahue, t. darrell, and j. malik. rich fea-
ture hierarchies for accurate id164 and semantic
segmentation. in cvpr, 2014.

[14] y. gong, q. ke, m. isard, and s. lazebnik. a multi-view em-
bedding space for modeling internet images, tags, and their
semantics. ijcv, 2014.

[15] a. karpathy and l. fei-fei. deep visual-semantic align-
arxiv preprint

ments for generating image descriptions.
arxiv:1412.2306, 2014.

[16] r. kiros, r. salakhutdinov, and r. s. zemel. unifying
visual-semantic embeddings with multimodal neural lan-
guage models. arxiv preprint arxiv:1411.2539, 2014.

[17] n. krishnamoorthy, g. malkarnenkar, r. j. mooney,
k. saenko, and s. guadarrama. generating natural-language
video descriptions using text-mined knowledge. pages 10   
19, july 2013.

[18] g. kulkarni, v. premraj, s. dhar, s. li, y. choi, a. c. berg,
and t. l. berg. baby talk: understanding and generating
image descriptions. in cvpr, 2011.

[19] p. kuznetsova, v. ordonez, a. c. berg, t. l. berg, and
y. choi. collective generation of natural image descriptions.
in acl, 2012.

[20] r. lebret, p. o. pinheiro, and r. collobert. phrase-based

image captioning. corr, abs/1502.03671, 2015.

[21] t. lin, m. maire, s. belongie, j. hays, p. perona, d. ra-
manan, p. doll  ar, and c. l. zitnick. microsoft coco: com-
mon objects in context. corr, abs/1405.0312, 2014.

[22] x. lin and d. parikh. don   t just listen, use your imagination:
leveraging visual common sense for non-visual tasks. arxiv
preprint arxiv:1502.06108, 2015.

[23] m. malinowski and m. fritz. a multi-world approach to
id53 about real-world scenes based on uncer-
tain input. in nips, 2014.

[24] r. mason. domain-independent captioning of domain-

speci   c images. in hlt-naacl, 2013.

[25] t. mikolov, k. chen, g. corrado, and j. dean. ef   cient
estimation of word representations in vector space. arxiv
preprint arxiv:1301.3781, 2013.

[26] m. mitchell, x. han, j. dodge, a. mensch, a. goyal,
a. berg, k. yamaguchi, t. berg, k. stratos,
and
h. daum  e iii. midge: generating image descriptions from
id161 detections. in eacl, 2012.

[27] v. ordonez, g. kulkarni, and t. l. berg. im2text: describ-
ing images using 1 million captioned photographs. in nips,
2011.

[28] c. rashtchian, p. young, m. hodosh, and j. hockenmaier.
collecting image annotations using amazon   s mechanical
in proceedings of the naacl hlt 2010 workshop
turk.
on creating speech and language data with amazon   s me-
chanical turk, pages 139   147. association for computa-
tional linguistics, 2010.

[29] k. simonyan and a. zisserman. very deep convolutional
networks for large-scale image recognition. arxiv preprint
arxiv:1409.1556, 2014.

[30] s. sukhbaatar, a. szlam,

weakly supervised memory networks.
arxiv:1503.08895, 2015.

j. weston, and r. fergus.
arxiv preprint

[31] s. venugopalan, h. xu,

j. donahue, m. rohrbach,
r. mooney, and k. saenko. translating videos to natural lan-
guage using deep recurrent neural networks. arxiv preprint
arxiv:1412.4729, 2014.

[32] o. vinyals, a. toshev, s. bengio, and d. erhan. show
and tell: a neural image caption generator. arxiv preprint
arxiv:1411.4555, 2014.

[33] y. yang, c. l. teo, h. daum  e iii, and y. aloimonos.
in

corpus-guided sentence generation of natural images.
emnlp, 2011.

[34] p. young, a. lai, m. hodosh, and j. hockenmaier. from im-
age descriptions to visual denotations: new similarity met-
rics for semantic id136 over event descriptions. tacl,
2014.

10

