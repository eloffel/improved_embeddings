classifying tweet level judgements of rumours in social media

michal lukasik,1 trevor cohn2 and kalina bontcheva1
1computer science

2computing and information systems

the university of shef   eld

{m.lukasik,k.bontcheva}@shef.ac.uk

the university of melbourne
t.cohn@unimelb.edu.au

abstract

text

5
1
0
2

 

p
e
s
0
1

 

 
 
]
i
s
.
s
c
[
 
 

2
v
8
6
4
0
0

.

6
0
5
1
:
v
i
x
r
a

social media is a rich source of rumours
and corresponding community reactions.
rumours re   ect different characteristics,
some shared and some individual. we for-
mulate the problem of classifying tweet
level judgements of rumours as a super-
vised learning task. both supervised and
unsupervised id20 are con-
sidered, in which tweets from a rumour are
classi   ed on the basis of other annotated
rumours. we demonstrate how multi-task
learning helps achieve good results on ru-
mours from the 2011 england riots.

1 introduction

there is an increasing need to interpret and act
upon rumours spreading quickly through social
media, especially in circumstances where their ve-
racity is hard to establish. for instance, during
an earthquake in chile rumours spread through
twitter that a volcano had become active and
that there was a tsunami warning in valparaiso
(mendoza et al., 2010). other examples, from
the riots in england in 2011, were that rioters
were going to attack birmingham   s children hos-
pital and that animals had escaped from the zoo
(procter et al., 2013).

social scientists (procter et al., 2013) analysed
manually a sample of tweets expressing different
judgements towards rumours and categorised them
manually in supporting, denying or questioning.
the goal here is to carry out tweet-level judge-
ment classi   cation automatically, in order to assist
in (near) real-time rumour monitoring by journal-
ists and authorities (procter et al., 2013).
in ad-
dition, information about tweet-level judgements
has been used as a    rst step for early rumour de-
tection by (zhao et al., 2015).

the focus here is on tweet-level judgement clas-
si   cation on unseen rumours, based on a training

position

support

deny

question

birmingham children   s hospital has
been attacked.
f***ing morons.
#ukriots

girlfriend has just called her ward
in birmingham children   s hospital &
there   s no sign of any trouble #birm-
inghamriots

birmingham children   s
hospital
guarded by police? really? who
would target a childrens hospital
#disgusting #birminghamriots

table 1: tweets on a rumour about hospital being
attacked during 2011 england riots.

set of other already annotated rumours. previous
work on this problem either considered unrealis-
tic settings ignoring temporal ordering and rumour
identities (qazvinian et al., 2011) or proposed reg-
ular expressions as a solution (zhao et al., 2015).
we expect posts expressing similar opinions to ex-
hibit many similar characteristics across different
rumours. based on the assumption of a common
underlying linguistic signal, we build a transfer
learning system that labels newly emerging ru-
mours for which we have little or no annotated
data. results demonstrate that gaussian process-
based multi task learning allows for signi   cantly
improved performance.

the novel contributions of

this paper are:
1. formulating the problem of classifying judge-
ments of rumours in both supervised and unsuper-
vised id20 settings. 2. showing how
a id72 approach outperforms single-
task methods.

2 related work

in the context of rumour spread in social me-
dia, researchers have studied differences in infor-

mation    ows between content of varying credi-
bility. for instance, procter et al. (2013) grouped
source tweets and re-tweets into information    ows
(lotan et al., 2011),
then ranked these by    ow
size, as a proxy of signi   cance. information    ows
were then categorised manually. along similar
vein, mendoza et al. (2010) found that users deal
with true and false rumours differently: the former
are af   rmed more than 90% of the time, whereas
the latter are challenged (questioned or denied)
50% of the time. friggeri et al. (2014) analyzed
a set rumours from the snopes.com website that
have been matched to facebook public conver-
sations. they concluded that false rumours are
more likely to receive a comment with link to
snopes.com website. however, none of the above
attempted to automatically classify rumours.

with respect to automatic methods for detecting
misinformation and disinformation in social me-
dia, ratkiewicz et al. (2011) detect political abuse
(a kind of disinformation) spread through twit-
ter. the task is de   ned in purely information
diffusion settings and is not necessarily related
with the truthfulness of the piece of information.
castillo et al. (2013) proposed methods for identi-
fying newsworthy information cascades on twitter
and then classifying these cascades as credible and
not credible. the main difference from our task is
that credibility classi   cation is carried out over the
entire information cascade, classi   ed objects are
not necessarily rumours and no explicit judgement
classi   cation was performed in their approach.

early rumour identi   cation is the focus of
zhao et al. (2015), where id157 are
used for    nding questioning and denying tweets
as a key pre-requisite step for rumour detection.
unfortunately, when we applied these regular ex-
pressions on our dataset, they yielded only 16%
recall for questioning and 14% recall for denying
tweets. consequently, this motivated us to seek a
better approach to tweet-level classi   cation.

relevant

to ours

the work most

is due
to qazvinian et al. (2011). their method    rst car-
ries out rumour retrieval, whereby tweets are clas-
si   ed into rumour related and non-rumour re-
lated. next, rumour-related tweets are classi   ed
into supporting and not-supporting. the classi-
   er is trained by ignoring rumour identities, i.e.,
pooling together tweets from all rumours, and ig-
noring the temporal dependencies between tweets.
in contrast, we formulate the rumour classi   ca-

rumour

supporting denying questioning

army bank
hospital
london eye
mcdonald   s
miss selfridge   s
police beat girl
zoo

62
796
177
177
3150
783
616

42
487
295
0
0
4
129

73
132
160
13
7
95
99

table 2: counts of tweets with supporting, deny-
ing or questioning labels in each rumour collec-
tion.

tion problem as id21, where unseen
rumours (or rumours with few initial tweets ob-
served) are classi   ed using already known ru-
mours     a much harder and more practical setting.
moreover, unlike qazvinian et al. (2011), we con-
sider the multi-class classi   cation problem and do
not collaps questioning and denying tweets into a
single class, since they differ signi   cantly.

3 data

we evaluate our work on several rumours circu-
lating on twitter during the england riots in 2011
(see table 2). the dataset was analysed and an-
notated manually as supporting, questioning, or
denying a rumour, by a team of social scientists
studying the role of social media during the riots
(procter et al., 2013). the original dataset also in-
cluded commenting tweets, but these have been
removed from our experiments due to their small
number (they constituted only 5% of the corpus).
as can be seen from the dataset overview
in table 2, different rumours exhibit varying
proportions of supporting, denying and ques-
tioning tweets, which was also observed in
other studies of rumours (mendoza et al., 2010;
qazvinian et al., 2011). these variations in major-
ity classes across rumours underscores the mod-
eling challenge in tweet-level classi   cation of ru-
mour attitudes.

with respect to veracity, one rumour has been
con   rmed as true (miss selfridge   s being on    re),
one is unsubstantiated (police beat girl), and the
remaining    ve are known to be false. note, how-
ever, that the focus here is not on classifying truth-
fulness, but instead on identifying the attitude ex-
pressed in each tweet towards the rumour.

4 problem formulation

let r be a set of rumours, each of which consists
rn}.
of tweets discussing it,    r   r tr = {tr
t =    r   rtr is the complete set of tweets from all
rumours. each tweet is classi   ed as supporting,
denying or questioning with respect to its rumour:
y(t)     {0, 1, 2}, where 0 denotes supporting, 1
means denying and 2 denotes questioning.

1,          , tr

first, we consider the leave one out (loo)
setting, which means that for each rumour r     r,
we construct the test set equal to tr and the train-
ing set equal to t \ tr. therefore this is a very
challenging and realistic scenario, where the test
set contains an entirely unseen rumour, from those
in the training set.

1,          , tr

the second setting is leave part out (lpo).
in this formulation, a very small number of ini-
tial tweets from the target rumour is added to the
rk }. this scenario becomes
training set {tr
applicable typically soon after a rumour breaks
out and journalists have started monitoring and
analysing the related tweet stream. the experi-
ments section investigates how the number of ini-
tial training tweets in   uences classi   cation perfor-
rn},
mance on a    xed test set, namely: {tr
rl
l > k.

,          , tr

the tweet-level classi   cation problem here as-
sumes that tweets from the training set are al-
ready labelled with the rumour discussed and the
attitude expressed towards that. this information
can be acquired either via manual annotation as
part of expert analysis, as is the case with our
dataset, or automatically, e.g. using pattern-based
rumour detection (zhao et al., 2015). afterwards,
our method can be used to classify the attitudes ex-
pressed in each new tweet from outside the train-
ing set.

5 gaussian processes for classi   cation

gaussian processes are a bayesian non-parametric
machine learning framework that has been shown
to work well for a range of nlp problems,
state-of-the-art methods
often beating other
(cohn and specia, 2013;
lampos et al., 2014;
preotiuc-pietro et al., 2015).
beck et al., 2014;
we use gaussian processes as this probabilis-
tic kernelised framework avoids the need for
expensive cross-validation for hyperparameter
selection.1

1there exist frequentist kernel methods,

like id166s,
which additionally require extensive heldout parameter tun-

f

a

over

latent

function

the central concept of gaussian process classi-
   cation (gpc; (rasmussen and williams, 2005))
is
inputs
x: f (x)     gp(m(x), k(x, x   )), where m is
the mean function, assumed to be 0 and k is the
id81, specifying the degree to which
the outputs covary as a function of the inputs. we
use a linear kernel, k(x, x   ) =   2x   x   . the latent
function is then mapped by the probit function
  (f ) into the range [0, 1], such that the resulting
value can be interpreted as p(y = 1|x).

the gpc posterior is calculated as
p(f    |x, y, x   ) = z p(f    |x, x   , f )

p(y|f )p(f )

p(y|x)

df ,

n

yj=1

where p(y|f ) =

  (fj)yj (1     (fj))1   yj is the

bernoulli likelihood of class y. after calculating
the above posterior from the training data, this is
used in prediction, i.e.,
p(y    = 1|x, y, x   ) =z    (f   ) p (f   |x, y, x   ) df    .
the above integrals are intractable and approx-
imation techniques are required to solve them.
there exist various methods to deal with calculat-
ing the posterior; here we use expectation prop-
agation (ep; (minka and lafferty, 2002)). in ep,
the posterior is approximated by a fully factorised
distribution, where each component is assumed to
be an unnormalised gaussian.

likelihood, amongst

in order to conduct multi-class classi   cation,
we perform a one-vs-all classi   cation for each
label and then assign the one with the high-
est
the three (supporting,
denying, questioning). we choose this method
due to interpretability of results, similar to re-
cent work on occupational class classi   cation
(preotiuc-pietro et al., 2015).

intrinsic coregionalization model
in the lpo
setting initial labelled tweets from the target ru-
mour are observed as well. in this case, we pro-
pose to weight the importance of tweets from
the reference rumours depending on how simi-
lar their characteristics are to the tweets from the
target rumour available for training. to handle
this with gpc, we use a multiple output model
based on the intrinsic coregionalisation model
(icm; (   alvarez et al., 2012)). it has already been
applied successfully to nlp regression problems

ing.

(beck et al., 2014) and it can also be applied to
classi   cation ones. icm parametrizes the kernel
by a matrix which represents the extent of covari-
ance between pairs of tasks. the complete kernel
takes form of

k((x, d), (x   , d   )) = kdata(x, x   )bd,d    ,

where b is a square coregionalization matrix, d
and d    denote the tasks of the two inputs and kdata
is a kernel for comparing inputs x and x    (here, lin-
ear). we parametrize the coregionalization matrix
b =   i + vvt , where v speci   es the correlation
between tasks and the vector    controls extent of
task independence.

hyperparameter selection we tune hyperpa-
rameters v,    and   2 by maximizing evidence of
the model p(y|x), thus having no need for a vali-
dation set.

methods we consider gps in three different set-
tings, varying in what data the model is trained on
and what kernel it uses. the    rst setting (denoted
gp) considers only target rumour data for train-
ing. the second (gppooled) additionally consid-
ers tweets from reference rumours (i.e. other than
the target rumour). the third setting is gpicm,
where an icm kernel is used to weight in   uence
from tweets from reference rumours.

6 features

we conducted a series of preprocessing steps in or-
der to address data sparsity. all words were low-
ercased; stopwords removed; all emoticons were
replaced with words2; and id30 was per-
formed.
in addition, multiple occurrences of a
character were replaced with a double occurrence
(agarwal et al., 2011), to correct for misspellings
and lengthenings, e.g., looool. all punctuation
was also removed, except for ., ! and ?, which we
hypothesize to be important for expressing emo-
tion. lastly, usernames were removed as they tend
to be rumour-speci   c, i.e., very few users com-
ment on more than one rumour.

after preprocessing the text data, we use ei-
ther the resulting bag of words (bow) feature
representation or replace all words with their
brown cluster ids (brown), using 1000 clus-
ters acquired from a large scale twitter corpus

2we

used

the

http://bit.ly/1rx1hdk and
:o, : |, =/, :s, :s, :p.

dictionary
extended

from:
it with:

method

acc

0.68
majority
gppooled brown
0.72
gppooled bow 0.69

table 3: accuracy taken across all rumours in the
loo setting.

(owoputi et al., 2013).
in all cases, simple re-
tweets are removed from the training set to prevent
bias (llewellyn et al., 2014).

7 experiments and discussion

table 3 shows the mean accuracy in the loo
scenario following the gppooled method, which
pools all reference rumours together ignoring their
task identities. icm can not use correlations to tar-
get rumour in this case and so can not be used. the
majority baseline simply assigns the most frequent
class from the training set.

we can observe that methods perform on a level
similar to majority vote, outperforming it only
slightly. this indicates how dif   cult the loo task
is, when no annotated target rumour tweets are
available.

figure 1 shows accuracy for a range of methods
as the number of tweets about the target rumour
used for training increases. most notably, perfor-
mance increases from 70% to around 80%, after
only 10 annotated tweets from the target rumour
become available, as compared to the results on
unseen rumours from table 3. however, as the
amount of target rumour increases, performance
does not increase further, which suggests that even
only 10 human-annotated tweets are enough to
achieve signi   cant performance bene   ts. note
also how the use of reference rumours is very im-
portant, as methods using only the target rumour
obtain accuracy similar to the majority vote clas-
si   er (gp brown and gp bow).

the top performing methods are gpcim and
gppooled, where use of brown clusters consis-
tently improves results for both methods over
bow, irrespective of the number of tweets about
the target rumour annotated for training. more-
over, gpicm is better than gppooled both with
brown and bow features and gpcim with
brown is ultimately the best performing of all.

in order to analyse the importance of brown
clusters, automatic relevance determination

figure 1: accuracy measures for different methods versus the size of the target rumour used for training
in the lpo setting. the test set is    xed to all but the    rst 50 tweets of the target rumour.

supporting

denying

questioning

8 conclusions

table 4: top 5 brown clusters, each shown
with a
further
details please see the cluster de   nitions at
http://www.ark.cs.cmu.edu/tweetnlp/cluster_viewer.html.

representative word.

for

?
10001101

!
10001100

not
001000

fake
11111000001

fake
11111000001

not
001000

?
10001101

!
10001100

?
10001101

!
10001100

hope
01000111110

true
111110010110

true
111110010110

bullshit
11110101011111

searching
01111000010

(ard) is used (rasmussen and williams, 2005)
for the best performing gpicm brown in the lpo
scenario. only the case where the    rst 10 tweets
are used for training is considered, since it already
performs very well. using ard, we learn a sepa-
rate length-scale for each feature, thus establishing
their importance. the weights learnt for differ-
ent clusters are averaged over the 7 rumours and
the top 5 brown clusters for each label are shown
in table 4. we can see that clusters around the
words fake and bullshit turn out to be important
for the denying class, and true for both supporting
and questioning classes. this reinforces our hy-
pothesis that common linguistic cues can be found
across multiple rumours. note how punctuation
proves important as well, since clusters ? and !
are also very prominent.

this paper investigated the problem of classifying
judgements expressed in tweets about rumours.
first, we considered a setting where no training
data from target rumour is available (loo). with-
out access to annotated examples of the target ru-
mour the learning problem becomes very dif   cult.
we showed that in the supervised domain adapta-
tion setting (lpo) even annotating a small number
of tweets helps to achieve better results. more-
over, we demonstrated the bene   ts of a multi task
learning approach, as well as that brown cluster
features are more useful for the task than simple
bag of words.

judgement estimation is undoubtedly of great
value e.g. for marketing, politics and journalism,
helping to target widely believed topics. although
the focus here is on classifying community reac-
tions, castillo et al. (2013) showed that commu-
nity reaction is correlated with actual rumour ve-
racity. consequently our classi   cation methods
may prove useful in the broader and more chal-
lenging task of annotating veracity.

an interesting direction for future work would
be adding non-textual features.
for example,
the rumour diffusion pattern (lukasik et al., 2015)
may be a useful cue for judgement classi   cation.

acknowledgments

work partially supported by the european union
under grant agreement no. 611233 pheme. the
work was implemented using the gpy toolkit
(gpy authors, 2015).

references
[agarwal et al.2011] apoorv agarwal, boyi xie, ilia
vovsha, owen rambow, and rebecca passonneau.

2011. id31 of twitter data.
in pro-
ceedings of the workshop on languages in social
media, lsm    11, pages 30   38.

[   alvarez et al.2012] mauricio a.

  alvarez, lorenzo
rosasco, and neil d. lawrence. 2012. kernels for
vector-valued functions: a review. found. trends
mach. learn., 4(3):195   266.

[beck et al.2014] daniel beck, trevor cohn, and lu-
cia specia. 2014. joint emotion analysis via multi-
task gaussian processes. in proceedings of the con-
ference on empirical methods in natural language
processing, emnlp    14, pages 1798   1803.

[castillo et al.2013] carlos castillo, marcelo mendoza,
and barbara poblete. 2013. predicting information
credibility in time-sensitive social media.
internet
research, 23(5):560   588.

[cohn and specia2013] trevor cohn and lucia spe-
cia. 2013. modelling annotator bias with multi-
task gaussian processes: an application to machine
translation quality estimation. in 51st annual meet-
ing of the association for computational linguis-
tics, acl    13, pages 32   42.

[mendoza et al.2010] marcelo mendoza,

barbara
poblete, and carlos castillo. 2010. twitter under
crisis: can we trust what we rt? in 1st workshop
on social media analytics, soma   10, pages 71   79.

[minka and lafferty2002] thomas minka and john
lafferty. 2002. expectation-propagation for the
generative aspect model.
in proceedings of the
eighteenth conference on uncertainty in arti   cial
intelligence, uai   02, pages 352   359.

[owoputi et al.2013] olutobi owoputi, chris dyer,
kevin gimpel, nathan schneider, and noah a.
smith. 2013. improved part-of-speech tagging for
online conversational text with word clusters.
in
proceedings of naacl, pages 380   390.

[preotiuc-pietro et al.2015] daniel

preotiuc-pietro,
vasileios lampos, and nikolaos aletras.
2015.
an analysis of the user occupational class through
twitter content. in proceedings of the 53rd annual
meeting of
the association for computational
linguistics and the 7th international joint confer-
ence on natural language processing of the asian
federation of natural language processing, acl
2015, pages 1754   1764.

[friggeri et al.2014] adrien friggeri, lada adamic,
dean eckles, and justin cheng. 2014. rumor cas-
cades.
in international aaai conference on we-
blogs and social media.

[procter et al.2013] rob procter, jeremy crump, su-
sanne karstedt, alex voss, and marta cantijoch.
2013. reading the riots: what were the police doing
on twitter? policing and society, 23(4):413   436.

[qazvinian et al.2011] vahed qazvinian, emily rosen-
gren, dragomir r. radev, and qiaozhu mei. 2011.
rumor has it:
identifying misinformation in mi-
croblogs. in proceedings of the conference on em-
pirical methods in natural language processing,
emnlp    11, pages 1589   1599.

[rasmussen and williams2005] carl

edward ras-
mussen and christopher k. i. williams.
2005.
gaussian processes for machine learning (adap-
tive computation and machine learning). the mit
press.

[ratkiewicz et al.2011] jacob ratkiewicz, michael
conover, mark meiss, bruno gonalves, alessandro
flammini, and filippo menczer. 2011. detecting
and tracking political abuse in social media. in 5th
international aaai conference on weblogs and
social media, icwsm   11.

[zhao et al.2015] zhe zhao, paul resnick, and qiaozhu
mei. 2015. early detection of rumors in social me-
dia from enquiry posts. in international world wide
web conference committee (iw3c2).

[gpy authors2015] the gpy authors.

a gaussian
http://github.com/sheffieldml/gpy.

framework

process

2015. gpy:
in python.

[lampos et al.2014] vasileios lampos, nikolaos ale-
tras, daniel preotiuc-pietro, and trevor cohn. 2014.
predicting and characterising user impact on twitter.
in proceedings of the 14th conference of the euro-
pean chapter of the association for computational
linguistics, eacl   14, pages 405   413.

[llewellyn et al.2014] clare llewellyn, claire grover,
jon oberlander, and ewan klein. 2014. re-using
an argument corpus to aid in the curation of social
media collections. in proceedings of the ninth in-
ternational conference on language resources and
evaluation, lrec   14, pages 462   468.

[lotan et al.2011] gilad lotan, erhardt graeff, mike
ananny, devin gaffney, ian pearce, and danah
boyd. 2011. the arab spring    the revolutions
were tweeted: information    ows during the 2011
tunisian and egyptian revolutions.
international
journal of communication, 5(0).

[lukasik et al.2015] michal lukasik, trevor cohn, and
kalina bontcheva. 2015. point process modelling
of rumour dynamics in social media. in proceedings
of the 53rd annual meeting of the association for
computational linguistics and the 7th international
joint conference on natural language processing
of the asian federation of natural language pro-
cessing, acl 2015, pages 518   523.

