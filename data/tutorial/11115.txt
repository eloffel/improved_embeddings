5
1
0
2

 
r
p
a
3

 

 
 
]

v
c
.
s
c
[
 
 

2
v
5
2
3
0
0

.

4
0
5
1
:
v
i
x
r
a

1

microsoft coco captions: data collection and

evaluation server

xinlei chen, hao fang, tsung-yi lin, ramakrishna vedantam

saurabh gupta, piotr doll  ar, c. lawrence zitnick

abstract   in this paper we describe the microsoft coco caption dataset and evaluation server. when completed, the dataset will
contain over one and a half million captions describing over 330,000 images. for the training and validation images,    ve independent
human generated captions will be provided. to ensure consistency in evaluation of automatic id134 algorithms, an
evaluation server is used. the evaluation server receives candidate captions and scores them using several popular metrics, including
id7, meteor, id8 and cider. instructions for using the evaluation server are provided.

!

1 introduction
the automatic generation of captions for images is a
long standing and challenging problem in arti   cial in-
telligence [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11],
[12], [13], [14], [15], [16], [17], [18], [19]. research in
this area spans numerous domains, such as computer
vision, natural language processing, and machine learn-
ing. recently there has been a surprising resurgence of
interest in this area [20], [21], [22], [23], [24], [25], [26],
[27], [28], [29], [30], due to the renewed interest in neural
network learning techniques [31], [32] and increasingly
large datasets [33], [34], [35], [7], [36], [37], [38].

in this paper, we describe our process of collecting
captions for the microsoft coco caption dataset, and
the evaluation server we have set up to evaluate perfor-
mance of different algorithms. the ms coco caption
dataset contains human generated captions for images
contained in the microsoft common objects in context
(coco) dataset [38]. similar to previous datasets [7],
[36], we collect our captions using amazon   s mechanical
turk (amt). upon completion of the dataset it will
contain over a million captions.

when evaluating image id134 algo-
rithms, it is essential that a consistent evaluation protocol
is used. comparing results from different approaches can
be dif   cult since numerous id74 exist [39],
[40], [41], [42]. to further complicate matters the imple-
mentations of these metrics often differ. to help alleviate
these issues, we have built an evaluation server to enable
consistency in evaluation of different id134
approaches. using the testing data, our evaluation server
evaluates captions output by different approaches using
numerous automatic metrics: id7 [39], meteor [41],

    xinlei chen is with carnegie mellon university.
    hao fang is with the university of washington.
    t.y. lin is with cornell nyc tech.
    ramakrishna vedantam is with virginia tech.
    saurabh gupta is with the univeristy of california, berkeley.
    p. doll  ar is with facebook ai research.
    c. l. zitnick is with microsoft research, redmond.

fig. 1: example images and captions from the microsoft
coco caption dataset.

id8 [40] and cider [42]. we hope to augment these
results with human evaluations on an annual basis.

this paper is organized as follows: first we describe
the data collection process. next, we describe the caption
evaluation server and the various metrics used. human
performance using these metrics are provided. finally
the annotation format and instructions for using the eval-
uation server are described for those who wish to submit
results. we conclude by discussing future directions and
known issues.

2 data collection
in this section we describe how the data is gathered
for the ms coco captions dataset. for images, we use
the dataset collected by microsoft coco [38]. these
images are split into training, validation and testing sets.

2

the images were gathered by searching for pairs of 80
object categories and various scene types on flickr. the
goal of the ms coco image collection process was to
gather images containing multiple objects in their natural
context. given the visual complexity of most images
in the dataset, they pose an interesting and dif   cult
challenge for image captioning.

for generating a dataset of image captions, the same
training, validation and testing sets were used as in the
original ms coco dataset. two datasets were collected.
the    rst dataset ms coco c5 contains    ve reference
captions for every image in the ms coco training,
validation and testing datasets. the second dataset ms
coco c40 contains 40 reference sentences for a ran-
domly chosen 5,000 images from the ms coco testing
dataset. ms coco c40 was created since many auto-
matic id74 achieve higher correlation with
human judgement when given more reference sentences
[42]. ms coco c40 may be expanded to include the ms
coco validation dataset in the future.

our process for gathering captions received signi   cant
inspiration from the work of young etal. [36] and ho-
dosh etal. [7] that collected captions on flickr images
using amazon   s mechanical turk (amt). each of our
captions are also generated using human subjects on
amt. each subject was shown the user interface in
figure 2. the subjects were instructed to:

    describe all the important parts of the scene.
    do not start the sentences with    there is.
    do not describe unimportant details.
    do not describe things that might have happened

in the future or past.

    do not describe what a person might say.
    do not give people proper names.
    the sentences should contain at least 8 words.

the number of captions gathered is 413,915 captions for
82,783 images in training, 202,520 captions for 40,504
images in validation and 379,249 captions for 40,775
images in testing including 179,189 for ms coco c5 and
200,060 for ms coco c40. for each testing image, we
collected one additional caption to compute the scores
of human performance for comparing scores of machine
generated captions. the total number of collected cap-
tions is 1,026,459. we plan to collect captions for the ms
coco 2015 dataset when it is released, which should
approximately double the size of the caption dataset.
the amt interface may be obtained from the ms coco
website.

3 caption evaluation
in this section we describe the ms coco caption evalu-
ation server. instructions for using the evaluation server
are provided in section 5. as input the evaluation server
receives candidate captions for both the validation and
testing datasets in the format speci   ed in section 5. the
validation and test images are provided to the submit-
ter. however, the human generated reference sentences

fig. 2: example user interface for the caption gathering
task.

are only provided for the validation set. the reference
sentences for the testing set are kept private to reduce
the risk of over   tting.

numerous id74 are computed on both
ms coco c5 and ms coco c40. these include id7-
1, id7-2, id7-3, id7-4, id8-l, meteor and
cider-d. the details of the these metrics are described
next.

3.1 id121 and preprocessing
both the candidate captions and the reference captions
are pre-processed by the evaluation server. to tokenize
the captions, we use stanford ptbtokenizer in stanford
corenlp tools (version 3.4.1) [43] which mimics penn
treebank 3 id121. in addition, punctuations1 are
removed from the tokenized captions.

3.2 id74
our goal is to automatically evaluate for an image ii
the quality of a candidate caption ci given a set of
reference captions si = {si1, . . . , sim}     s. the caption
sentences are represented using sets of id165s, where
an id165   k         is a set of one or more ordered words.
in this paper we explore id165s with one to four words.
no id30 is performed on the words. the number
of times an id165   k occurs in a sentence sij is denoted
hk(sij) or hk(ci) for the candidate sentence ci     c.

3.3 id7
id7 [39] is a popular machine translation metric that
analyzes the co-occurrences of id165s between the
candidate and reference sentences. it computes a corpus-
level clipped id165 precision between sentences as
follows:

(cid:80)

(cid:80)

k min(hk(ci), max
j   m
k hk(ci)

i

hk(sij))

,

(1)

(cid:80)

(cid:80)

i

cpn(c, s) =

1. the full list of punctuations: {   ,    ,    ,    , -lrb-, -rrb-, -lcb-, -rcb-,

., ?, !, ,, :, -,    , ..., ;}.

where k indexes the set of possible id165s of length n.
the clipped precision metric limits the number of times
an id165 may be counted to the maximum number
of times it is observed in a single reference sentence.
note that cpn is a precision score and it favors short
sentences. so a brevity penalty is also used:

(cid:40)

b(c, s) =

1
e1   ls /lc

if lc > ls
if lc     ls

,

(2)

where lc is the total length of candidate sentences ci   s
and ls is the length of the corpus-level effective refer-
ence length. when there are multiple references for a
candidate sentence, we choose to use the closest reference
length for the brevity penalty.

the overall id7 score is computed using a weighted

geometric mean of the individual id165 precision:

(cid:32) n(cid:88)

(cid:33)

id7n (c, s) = b(c, s) exp

wn log cpn(c, s)

,

rl and pl are recall and precision of lcs.    is
usually set to favor recall (   = 1.2). since n-
grams are implicit in this measure due to the use
of the lcs, they need not be speci   ed.

3) id8s: the    nal id8 metric uses skip bi-
grams instead of the lcs or id165s. skip bi-grams
are pairs of ordered words in a sentence. however,
similar to the lcs, words may be skipped between
pairs of words. thus, a sentence with 4 words
would have c 4
2 = 6 skip bi-grams. precision and
recall are again incorporated to compute an f-
measure score. if fk(sij) is the skip bi-gram count
for sentence sij, id8s is computed as:

(cid:80)
(cid:80)

(cid:80)
(cid:80)

k min(fk(ci), fk(sij))

k fk(sij)

k min(fk(ci), fk(sij))

k fk(ci)

rs = max

j

ps = max

j

n=1

(3)

rou ges(ci, si) =

(1 +   2)rsps

rs +   2ps

where n = 1, 2, 3, 4 and wn is typically held constant for
all n.

id7 has shown good performance for corpus-
level comparisons over which a high number of n-
gram matches exist. however, at a sentence-level the
id165 matches for higher n rarely occur. as a result,
id7 performs poorly when comparing individual sen-
tences.

3.4 id8
id8 [40] is a set of id74 designed to
evaluate text summarization algorithms.

1) id8n : the    rst id8 metric computes a
simple id165 recall over all reference summaries
given a candidate sentence:

(cid:80)

(cid:80)

(cid:80)

(cid:80)

j

k min(hk(ci), hk(sij))

j

k hk(sij)

rou gen (ci, si) =

(4)
2) id8l: id8l uses a measure based on the
longest common subsequence (lcs). an lcs is
a set words shared by two sentences which occur
in the same order. however, unlike id165s there
may be words in between the words that create
the lcs. given the length l(ci, sij) of the lcs
between a pair of sentences, id8l is found by
computing an f-measure:

rl = max

j

pl = max

j

l(ci, sij)
|sij|
l(ci, sij)
|ci|

rou gel(ci, si) =

(1 +   2)rlpl

rl +   2pl

(5)

(6)

(7)

skip bi-grams are capable of capturing long range
sentence structure. in practice, skip bi-grams are
computed so that the component words occur at a
distance of at most 4 from each other.

3.5 meteor
meteor [41] is calculated by generating an alignment
between the words in the candidate and reference sen-
tences, with an aim of 1:1 correspondence. this align-
ment is computed while minimizing the number of
chunks, ch, of contiguous and identically ordered tokens
in the sentence pair. the alignment is based on exact
token matching, followed by id138 synonyms [44],
stemmed tokens and then paraphrases. given a set of
alignments, m, the meteor score is the harmonic mean
of precision pm and recall rm between the best scoring
reference and candidate:

fmean =

(cid:18) ch

(cid:19)  

m

p en =   

pmrm

  pm + (1       )rm
pm =

|m|(cid:80)
|m|(cid:80)

k hk(ci)

rm =

k hk(sij)
m et eor = (1     p en)fmean

(15)
thus, the    nal meteor score includes a penalty p en
based on chunkiness of resolved matches and a har-
monic mean term that gives the quality of the resolved
matches. the default parameters   ,    and    are used for
this evaluation. note that similar to id7, statistics of
precision and recall are    rst aggregated over the entire
corpus, which are then combined to give the corpus-level
meteor score.

3

(8)

(9)

(10)

(11)

(12)

(13)

(14)

4

3.6 cider

the cider metric [42] measures consensus in image
captions by performing a term frequency inverse doc-
ument frequency (tf-idf) weighting for each id165.
the number of times an id165   k occurs in a reference
sentence sij is denoted by hk(sij) or hk(ci) for the candi-
date sentence ci. cider computes the tf-idf weighting
gk(sij) for each id165   k using:

gk(sij) =

(cid:80)

hk(sij)
  l       hl(sij)

(cid:32)

log

(cid:80)
ip   i min(1,(cid:80)

|i|

q hk(spq))

(cid:33)

,

(16)

where     is the vocabulary of all id165s and i is the
set of all images in the dataset. the    rst term measures
the tf of each id165   k, and the second term measures
the rarity of   k using its idf. intuitively, tf places higher
weight on id165s that frequently occur in the reference
sentences describing an image, while idf reduces the
weight of id165s that commonly occur across all de-
scriptions. that is, the idf provides a measure of word
saliency by discounting popular words that are likely to
be less visually informative. the idf is computed using
the logarithm of the number of images in the dataset |i|
divided by the number of images for which   k occurs
in any of its reference sentences.

the cidern score for id165s of length n is com-
puted using the average cosine similarity between the
candidate sentence and the reference sentences, which
accounts for both precision and recall:

cidern(ci, si) =

1
m

gn(ci)    gn(sij)
(cid:107)gn(ci)(cid:107)(cid:107)gn(sij)(cid:107) ,

(17)

where gn(ci) is a vector formed by gk(ci) corresponding
to all id165s of length n and (cid:107)gn(ci)(cid:107) is the magnitude
of the vector gn(ci). similarly for gn(sij).

higher order (longer) id165s to are used to cap-
ture grammatical properties as well as richer semantics.
scores from id165s of varying lengths are combined as
follows:

(cid:88)

j

n(cid:88)

cider(ci, si) =

wncidern(ci, si),

(18)

n=1

uniform weights are used wn = 1/n. n = 4 is used.

cider-d is a modi   cation to cider to make it more
robust to gaming. gaming refers to the phenomenon
where a sentence that is poorly judged by humans tends
to score highly with an automated metric. to defend the
cider metric against gaming effects,
[42] add clipping
and a length based gaussian penalty to the cider metric
described above. this results in the following equations
for cider-d:

table 1: human agreement for image captioning:
various metrics when benchmarking a human generated
caption against ground truth captions.

metric name ms coco c5 ms coco c40
id7 1
id7 2
id7 3
id7 4
meteor
id8l
cider-d

0.663
0.469
0.321
0.217
0.252
0.484
0.854

0.880
0.744
0.603
0.471
0.335
0.626
0.910

(cid:88)

n(cid:88)

cider-dn(ci, si) =

e

   (l(ci)   l(sij ))2

10
m
min(gn(ci), gn(sij))    gn(sij)

   

2  2

j

(cid:107)gn(ci)(cid:107)(cid:107)gn(sij)(cid:107)

,

(19)

where l(ci) and l(sij) denote the lengths of candidate
and reference sentences respectively.    = 6 is used. a
factor of 10 is used in the numerator to make the cider-
d scores numerically similar to the other metrics.

the    nal cider-d metric is computed in a similar

manner to cider (analogous to eqn. 18):

cider-d(ci, si) =

wncider-dn(ci, si),

(20)

n=1

note that just like the id7 and id8 metrics, cider-
d does not use id30. we adopt the cider-d metric
for the evaluation server.

4 human performance
in this section, we study the human agreement among
humans at this task. we start with analyzing the inter-
human agreement for image captioning (section. 4.1) and
then analyze human agreement for the word prediction
sub-task and provide a simple model which explains
human agreement for this sub-task (section. 4.2).

4.1 human agreement for image captioning
when examining human agreement on captions, it be-
comes clear that there are many equivalent ways to
say essentially the same thing. we quantify this by
conducting the following experiment: we collect one
additional human caption for each image in the test
set and treat this caption as the prediction. using the
ms coco caption evaluation server we compute the
various metrics. the results are tabulated in table 1.

4.2 human agreement for word prediction
we can do a similar analysis for human agreement at the
sub-task of word prediction. consider the task of tagging
the image with words that occur in the captions. for this
task, we can compute the human precision and recall for

table 2: model de   ntions.

and q are:

5

o
w
n
k
q
p

object or visual concept
=
= word associated with o
total number of images
=
number of captions per image
=
=
p (o = 1)
p (w = 1|o = 1)
=

a given word w by benchmarking words used in the k+1
human caption with respect to words used in the    rst k
reference captions. note that we use weighted versions
of precision and recall, where each negative image has
a weight of 1 and each positive image has a weight
equal to the number of captions containing the word
w. human precision (hp) and human recall (hr) can be
computed from the counts of how many subjects out of
k use the word w to describe a given image over the
whole dataset.

we plot hp versus hr for a set of nouns, verbs and
adjectives, and all 1000 words considered in figure 3.
nouns referring to animals like    elephant    have a high
recall, which means that if an    elephant    exists in the
image, a subject is likely to talk about it (which makes
intuitive sense, given    elephant    images are somewhat
rare, and there are no alternative words that could
be used instead of    elephant   ). on the other hand, an
adjective like    bright    is used inconsistently and hence
has low recall. interestingly, words with high recall also
have high precision. indeed, all the points of human
agreement appear to lie on a one-dimensional curve in
the two-dimension precision-recall space.

this observation motivates us to propose a simple
model for when subjects use a particular word w for
describing an image. let o denote an object or visual
concept associated with word w, n be the total number of
images, and k be the number of reference captions. next,
let q = p (o = 1) be the id203 that object o exists in
an image. for clarity these de   nitions are summarized
in table 2. we make two simpli   cations. first, we ig-
nore image level saliency and instead focus on word level
saliency. speci   cally, we only model p = p (w = 1|o = 1),
the id203 a subject uses w given that o is in the
image, without conditioning on the image itself. second,
we assume that p (w = 1|o = 0) = 0, i.e. that a subject
does not use w unless o is in the image. as we will
show, even with these simpli   cations our model suf   ces
to explain the empirical observations in figure 3 to a
reasonable degree of accuracy.

sion (cid:102)hp and recall (cid:102)hr for a word w given only p and k.

given these assumptions, we can model human preci-

first, given k captions per image, we need to compute
the expected number of (1) captions containing w (cw),
(2) true positives (tp), and (3) false positives (f p). note
that in our de   nition there can be up to k true positives
per image (if cw = k, i.e. each of the k captions contains
word w) but at most 1 false positive (if none of the k
captions contains w). the expectations, in terms of k, p,

e[cw] =   k

i=1p (wi = 1)

=   ip (wi = 1|o = 1)p (o = 1)
+  ip (wi = 1|o = 0)p (o = 0)

= kpq + 0 = kpq

e[tp] =   k

i=1p (wi = 1     wk+1 = 1)

=   ip (wi = 1     wk+1 = 1|o = 1)p (o = 1)
+  ip (wi = 1     wk+1 = 1|o = 0)p (o = 0)

= kppq + 0 = kp2q

e[f p] = p (w1 . . . wk = 0     wk+1 = 1)

= p (o = 1     w1 . . . wk = 0     wk+1 = 1)
+p (o = 0     w1 . . . wk = 0     wk+1 = 1)

= q(1     p)kp + 0 = q(1     p)kp

in the above wi = 1 denotes that w appeared in the ith
caption. note that we are also assuming independence
between subjects conditioned on o. we can now de   ne
model precision and recall as:

(cid:102)hp
(cid:102)hr

:=

:=

ne[tp]

ne[tp] + ne[f p]
ne[tp]
ne[cw]

= p

=

pk

pk + (1     p)k

note that these expressions are independent of q and
only depend on p. interestingly, because of the use of
weighted precision and recall, the recall for a category
comes out to be exactly equal to p, the id203 a
subject uses w given that o is in the image.

we set k = 4 and vary p to plot (cid:102)hp versus (cid:102)hr,

getting the curve as shown in blue in figure 3 (bottom
left). the curve explains the observed data quite well,
closely matching the precision-recall tradeoffs of the
empirical data (although not perfectly). we can also
reduce the number of captions from four, and look at
how the empirical and predicted precision and recall
change. figure 3 (bottom right), shows this variation as
we reduce the number of reference captions per image
from four to one annotations. we see that the points of
human agreement remain at the same recall value, but
decrease in their precision, which is consistent with what
the model predicts. also, the human precision at in   nite
subjects will approach one, which is again reasonable
given that a subject will only use the word w if the
corresponding object is in the image (and in the presence
of in   nite subjects someone else will also use the word
w).

in fact, the    xed recall value can help us recover
p, the id203 that a subject will use the word w
in describing the image given the object is present.
nouns like    elephant    and    tennis    have large p, which
is reasonable. verbs and adjectives, on the other hand,
have smaller p values, which can be justi   ed from the
fact that a) subjects are less likely to describe attributes

6

fig. 3: precision-recall points for human agreement: we compute precision and recall by treating one human caption
as prediction and benchmark it against the others to obtain points on the precision recall curve. we plot these points
for example nouns (top left), adjectives (top center), and verbs (top right), and for all words (bottom left). we also
plot the    t of our model for human agreement with the empirical data (bottom left) and show how the human
agreement changes with different number of captions being used (bottom right). we see that the human agreement
point remains at the same recall value but dips in precision when using fewer captions.

of objects and b) subjects might use a different word
(synonym) to describe the same attribute.

this analysis of human agreement also motivates us-
ing a different metric for measuring performance. we
propose precision at human recall (phr) as a metric
for measuring performance of a vision system perform-
ing this task. given that human recall for a particular
word is    xed and precision varies with the number of
annotations, we can look at system precision at human
recall and compare it with human precision to report the
performance of the vision system.

5 evaluation server instructions
directions on how to use the ms coco caption evalu-
ation server can be found on the ms coco website.
the evaluation server is hosted by codalab. to par-
ticipate, a user account on codalab must be created.
the participants need to generate results on both the
validation and testing datasets. when training for the
generation of results on the test dataset, the training
and validation dataset may be used as the participant
sees    t. that is, the validation dataset may be used for
training if desired. however, when generating results on
the validation set, we ask participants to only train on
the training dataset, and only use the validation dataset

for tuning meta-parameters. two json    les should be
created corresponding to results on each dataset in the
following format:

[{
   image id   
   caption   
}]

:
:

int,
str,

the results may then be placed into a zip    le and
uploaded to the server for evaluation. code is also
provided on github to evaluate results on the validation
dataset without having to upload to the server. the
number of submissions per user is limited to a    xed
amount.

6 discussion
many challenges exist when creating an image caption
dataset. as stated in [7], [42], [45] the captions generated
by human subjects can vary signi   cantly. however even
though two captions may be very different, they may
be judged equally    good    by human subjects. designing
effective automatic id74 that are highly
correlated with human judgment remains a dif   cult
challenge [7], [42], [45], [46]. we hope that by releasing

00.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.70.80.91recallprecisionmanpersontennisbedboyroadelephantskykitesidewalkbikeskibottlerailroadrugpierapartmentnouns00.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.70.80.91recallprecisionblackredwoodenteddyhotdoublebrighthugedrypolarblurryadjectives00.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.70.80.91recallprecisionsittinglookingflyingdrivinggrazingcuttingrunningmakingworkingkneelingverbs00.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.70.80.91recallprecision00.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.70.80.91recallprecision  number of reference = 1number of reference = 2number of reference = 3number of reference = 4results on the validation data, we can help enable future
research in this area.

since automatic id74 do not always
correspond to human judgment, we hope to conduct
experiments using human subjects to judge the quality of
automatically generated captions, which are most similar
to human captions, and whether they are grammatically
correct [45], [42], [7], [4], [5]. this is essential to determin-
ing whether future algorithms are indeed improving, or
whether they are merely over    tting to a speci   c metric.
these human experiments will also allow us to evaluate
the automatic id74 themselves, and see
which ones are correlated to human judgment.

references
[1] k. barnard and d. forsyth,    learning the semantics of words and

pictures,    in iccv, vol. 2, 2001, pp. 408   415.

[2] k. barnard, p. duygulu, d. forsyth, n. de freitas, d. m. blei, and
m. i. jordan,    matching words and pictures,    jmlr, vol. 3, pp.
1107   1135, 2003.

[3] v. lavrenko, r. manmatha, and j. jeon,    a model for learning

the semantics of pictures,    in nips, 2003.

[4] g. kulkarni, v. premraj, s. dhar, s. li, y. choi, a. c. berg, and t. l.
berg,    baby talk: understanding and generating simple image
descriptions,    in cvpr, 2011.

[5] m. mitchell, x. han, j. dodge, a. mensch, a. goyal, a. berg,
k. yamaguchi, t. berg, k. stratos, and h. daum  e iii,    midge:
generating image descriptions from id161 detections,   
in eacl, 2012.

[6] a. farhadi, m. hejrati, m. a. sadeghi, p. young, c. rashtchian,
j. hockenmaier, and d. forsyth,    every picture tells a story:
generating sentences from images,    in eccv, 2010.

[7] m. hodosh, p. young, and j. hockenmaier,    framing image de-
scription as a ranking task: data, models and id74.   
jair, vol. 47, pp. 853   899, 2013.

[8] p. kuznetsova, v. ordonez, a. c. berg, t. l. berg, and y. choi,
   collective generation of natural image descriptions,    in acl,
2012.

[9] y. yang, c. l. teo, h. daum  e iii, and y. aloimonos,    corpus-
guided sentence generation of natural images,    in emnlp, 2011.
[10] a. gupta, y. verma, and c. jawahar,    choosing linguistics over

vision to describe images.    in aaai, 2012.

[11] e. bruni, g. boleda, m. baroni, and n.-k. tran,    distributional

semantics in technicolor,    in acl, 2012.

[12] y. feng and m. lapata,    automatic id134 for news

images,    tpami, vol. 35, no. 4, pp. 797   812, 2013.

[13] d. elliott and f. keller,    image description using visual depen-

dency representations.    in emnlp, 2013, pp. 1292   1302.

[14] a. karpathy, a. joulin, and f.-f. li,    deep fragment embeddings

for bidirectional image sentence mapping,    in nips, 2014.

[15] y. gong, l. wang, m. hodosh, j. hockenmaier, and s. lazebnik,
   improving image-sentence embeddings using large weakly an-
notated photo collections,    in eccv, 2014, pp. 529   545.

[16] r. mason and e. charniak,    nonparametric method for data-

driven image captioning,    in acl, 2014.

[17] p. kuznetsova, v. ordonez, t. berg, and y. choi,    treetalk: com-
position and compression of trees for image descriptions,    tacl,
vol. 2, pp. 351   362, 2014.

[18] k. ramnath, s. baker, l. vanderwende, m. el-saban, s. n.
sinha, a. kannan, n. hassan, m. galley, y. yang, d. ramanan,
a. bergamo, and l. torresani,    autocaption: automatic caption
generation for personal photos,    in wacv, 2014.

[19] a. lazaridou, e. bruni, and m. baroni,    is this a wampimuk?
cross-modal mapping between id65 and the
visual world,    in acl, 2014.

[20] r. kiros, r. salakhutdinov, and r. zemel,    multimodal neural

language models,    in icml, 2014.

[21] j. mao, w. xu, y. yang, j. wang, and a. l. yuille,    explain im-
ages with multimodal recurrent neural networks,    arxiv preprint
arxiv:1410.1090, 2014.

7

[22] o. vinyals, a. toshev, s. bengio, and d. erhan,    show and tell:
a neural image caption generator,    arxiv preprint arxiv:1411.4555,
2014.

[23] a. karpathy and l. fei-fei,    deep visual-semantic alignments
for generating image descriptions,    arxiv preprint arxiv:1412.2306,
2014.

[24] r. kiros, r. salakhutdinov, and r. s. zemel,    unifying visual-
semantic embeddings with multimodal neural language models,   
arxiv preprint arxiv:1411.2539, 2014.

[25] j. donahue, l. a. hendricks, s. guadarrama, m. rohrbach,
s. venugopalan, k. saenko, and t. darrell,    long-term recurrent
convolutional networks for visual recognition and description,   
arxiv preprint arxiv:1411.4389, 2014.

[26] h. fang, s. gupta, f. iandola, r. srivastava, l. deng, p. doll  ar,
j. gao, x. he, m. mitchell, j. platt et al.,    from captions to visual
concepts and back,    arxiv preprint arxiv:1411.4952, 2014.

[27] x. chen and c. l. zitnick,    learning a recurrent visual representa-
tion for image id134,    arxiv preprint arxiv:1411.5654,
2014.

[28] r. lebret, p. o. pinheiro, and r. collobert,    phrase-based image

captioning,    arxiv preprint arxiv:1502.03671, 2015.

[29]       ,    simple image description generator via a linear phrase-

based approach,    arxiv preprint arxiv:1412.8419, 2014.

[30] a. lazaridou, n. t. pham, and m. baroni,    combining language
and vision with a multimodal skip-gram model,    arxiv preprint
arxiv:1501.02598, 2015.

[31] a. krizhevsky, i. sutskever, and g. hinton,    id163 classi   ca-

tion with deep convolutional neural networks,    in nips, 2012.

[32] s. hochreiter and j. schmidhuber,    long short-term memory,   

neural computation, vol. 9, no. 8, pp. 1735   1780, 1997.

[33] j. deng, w. dong, r. socher, l.-j. li, k. li, and l. fei-fei,    im-
agenet: a large-scale hierarchical image database,    in cvpr,
2009.

[34] m. grubinger, p. clough, h. m   uller, and t. deselaers,    the iapr tc-
12 benchmark: a new evaluation resource for visual information
systems,    in lrec workshop on language resources for content-
based id162, 2006.

[35] v. ordonez, g. kulkarni, and t. berg,    im2text: describing images

using 1 million captioned photographs.    in nips, 2011.

[36] p. young, a. lai, m. hodosh, and j. hockenmaier,    from image
descriptions to visual denotations: new similarity metrics for
semantic id136 over event descriptions,    tacl, vol. 2, pp. 67   
78, 2014.

[37] j. chen, p. kuznetsova, d. warren, and y. choi,    d  ej  a image-
captions: a corpus of expressive image descriptions in repetition,   
in naacl, 2015.

[38] t. lin, m. maire, s. belongie, j. hays, p. perona, d. ramanan,
p. doll  ar, and c. l. zitnick,    microsoft coco: common objects
in context,    in eccv, 2014.

[39] k. papineni, s. roukos, t. ward, and w.-j. zhu,    id7: a method
for automatic evaluation of machine translation,    in acl, 2002.
[40] c.-y. lin,    id8: a package for automatic evaluation of sum-

maries,    in acl workshop, 2004.

[41] m. denkowski and a. lavie,    meteor universal: language spe-
ci   c translation evaluation for any target language,    in eacl
workshop on id151, 2014.

[42] r. vedantam, c. l. zitnick,

   cider:
consensus-based image description evaluation,    arxiv preprint
arxiv:1411.5726, 2014.

and d. parikh,

[43] c. d. manning, m. surdeanu,

j.
bethard, and d. mcclosky,    the stanford corenlp natural
language processing toolkit,    in proceedings of 52nd annual
meeting of the association for computational linguistics: system
demonstrations, 2014, pp. 55   60.
[online]. available: http:
//www.aclweb.org/anthology/p/p14/p14-5010

j. finkel, s.

j. bauer,

[44] g. a. miller,    id138: a lexical database for english,    communi-

cations of the acm, vol. 38, no. 11, pp. 39   41, 1995.

[45] d. elliott and f. keller,    comparing automatic evaluation mea-
sures for image description,    in proceedings of the 52nd annual
meeting of the association for computational linguistics, vol. 2, 2014,
pp. 452   457.

[46] c. callison-burch, m. osborne, and p. koehn,    re-evaluation the
role of id7 in machine translation research.    in eacl, vol. 6,
2006, pp. 249   256.

