approximation-aware id33 by belief propagation

matthew r. gorid113y

mark dredze

jason eisner

department of computer science

center for language and speech processing

human language technology center of excellence

johns hopkins university, baltimore, md
{mrg,mdredze,jason}@cs.jhu.edu

5
1
0
2

 

g
u
a
0
1

 

 
 
]
l
c
.
s
c
[
 
 

1
v
5
7
3
2
0

.

8
0
5
1
:
v
i
x
r
a

abstract

we show how to train the fast dependency
parser of smith and eisner (2008) for im-
proved accuracy. this parser can consider
higher-order interactions among edges while
retaining o(n3) runtime.
it outputs the
parse with maximum expected recall   but for
speed, this expectation is taken under a pos-
terior distribution that is constructed only ap-
proximately, using loopy belief propagation
through structured factors. we show how to
adjust the model parameters to compensate for
the errors introduced by this approximation,
by following the gradient of the actual loss on
training data. we    nd this gradient by back-
propagation. that is, we treat the entire parser
(approximations and all) as a differentiable
circuit, as stoyanov et al. (2011) and domke
(2010) did for loopy crfs. the resulting
trained parser obtains higher accuracy with
fewer iterations of belief propagation than one
trained by conditional log-likelihood.

1

introduction

recent improvements to id33 ac-
curacy have been driven by higher-order features.
such a feature can look beyond just the parent and
child words connected by a single edge to also con-
sider siblings, grand-parents, etc. by including in-
creasingly global information, these features pro-
vide more information for the parser   but they also
complicate id136. the resulting higher-order
parsers depend on approximate id136 and decod-
ing procedures, which may prevent them from pre-
dicting the best parse.

for example, consider the dependency parser we
will train in this paper, which is based on the work
of smith and eisner (2008). ostensibly, this parser

   nds the minimum bayes risk (mbr) parse under
a id203 distribution de   ned by a higher-order
id33 model.
in reality, however, it
achieves o(n3t ) runtime by relying on three ap-
proximations during id136: (1) variational infer-
ence by loopy belief propagation (bp) on a factor
graph, (2) early stopping of id136 after tmax it-
erations prior to convergence, and (3) a    rst-order
pruning model to limit the number of edges consid-
ered in the higher-order model. such parsers are tra-
ditionally trained as if the id136 had been exact
(smith and eisner, 2008).1

in contrast, we train the parser such that the ap-
proximate system performs well on the    nal evalua-
tion function. stoyanov and eisner (2012) call this
approach erma, for    empirical risk minimization
under approximations.    we treat the entire parsing
computation as a differentiable circuit, and back-
propagate the evaluation function through our ap-
proximate id136 and decoding methods to im-
prove its parameters by id119.

our primary contribution is the application of
stoyanov and eisner   s learning method in the pars-
ing setting, for which the graphical model involves
a global constraint. smith and eisner (2008) pre-
viously showed how to run bp in this setting (by
calling the inside-outside algorithm as a subroutine).
we must backpropagate the downstream objective
function through their algorithm so that we can fol-
low its gradient. we carefully de   ne our objec-
tive function to be smooth and differentiable, yet
equivalent to accuracy of the minimum bayes risk
(mbr) parse in the limit. further we introduce a
new simpler objective function based on the l2 dis-

1for id88 training, utilizing inexact id136 as a
drop-in replacement for exact id136 can badly mislead the
learner (kulesza and pereira, 2008).

(cid:81)

model a factor graph (frey et al., 1997; kschis-
chang et al., 2001) is a bipartite graph between fac-
tors    and variables yi, and de   nes the factorization
of a id203 distribution over a set of variables
{y1, y2, . . .}. the factor graph contains edges be-
tween each factor    and a subset of variables y  .
each factor has a local opinion about the possible
assignments to its neighboring variables. such opin-
ions are given by the factor   s potential function     ,
which assigns a nonnegative score to each con   g-
uration of a subset of variables y  . we de   ne the
id203 of a given assignment y to be propor-
tional to a product of potential functions: p(y) =
1
z

       (y  ).

smith and eisner (2008) de   ne a factor graph for
id33 of a given n-word sentence: n2
binary variables {y1, y2, . . .} indicate which of the
directed arcs are included (yi = on) or excluded
(yi = off) in the dependency parse. one of the
factors plays the role of a hard global constraint:
  ptree(y) is 1 or 0 according to whether the as-
signment encodes a projective dependency tree. an-
other o(n2) factors (one per variable) evaluate the
individual arcs given the sentence, so that p(y) de-
scribes a    rst-order dependency parser. a higher-
order parsing model is achieved by including higher-
order factors, each scoring con   gurations of two or
more arcs, such as grandparent and sibling con   gu-
rations. higher-order factors add cycles to the factor
graph. see figure 1 for an example factor graph.
we de   ne each potential function to have a log-
linear form:     (y  ) = exp(      f   (y  , x)). here
x is the vector of observed variables such as the
sentence and its pos tags; f    extracts a vector of
features; and    is our vector of model parameters.
we write the resulting id203 distribution over
parses as p  (y), to indicate that it depends on   .
loss for id33, our id168 is
the number of missing edges in the predicted parse
  y, relative to the reference (or    gold   ) parse y   :

(cid:96)(  y, y   ) =

  (  yi = off)

(1)

(cid:88)

i: y   

i =on

figure 1: factor graph for id33 of a 4-
word sentence; the special node <root> is the root of
the dependency graph. in this    gure, the boolean variable
yh,m encodes whether the edge from parent h to child
m is present. the unary factor (black) connected to this
variable scores the edge in isolation (given the sentence).
the ptree factor (red) coordinates all variables to en-
sure that the edges form a tree. the drawing shows a
few of the higher-order factors (purple factors for grand-
parents, green factors for arbitrary siblings); these are re-
sponsible for the graph being cyclic (   loopy   ).

tance between the approximate marginals and the
   true    marginals from the gold data.

the goal of this work is to account for the ap-
proximations made by a system rooted in struc-
tured belief propagation. taking such approxima-
tions into account during training enables us to
improve the speed and accuracy of id136 at
test time. to this end, we compare our training
method with the standard approach of conditional
log-likelihood. we evaluate our parser on 19 lan-
guages from the conll-2006 (buchholz and marsi,
2006) and conll-2007 (nivre et al., 2007) shared
tasks as well as the english id32 (marcus
et al., 1993). on english, the resulting parser obtains
higher accuracy with fewer iterations of bp than
standard conditional log-likelihood (cll) training.
on the conll languages, we    nd that on average
it yields higher accuracy parsers than cll training,
particularly when limited to few bp iterations.

2 id33 by belief

propagation

this section describes the parser that we will train.

because   y and y    each specify exactly one parent
for each word token, (cid:96)(  y, y   ) equals the number of
word tokens whose parent is predicted incorrectly   
that is, directed dependency error.

0 2 1 3 4 juan su abdica reino <root> y2,1 y1,2 y3,2 y2,3 y3,1 y1,3 y4,3 y3,4 y4,2 y2,4 y4,1 y1,4 y0,1 y0,3 y0,4 y0,2 decoder to obtain a single parse as output, we use
a minimum bayes risk (mbr) decoder, which at-
tempts to    nd the tree with minimum expected loss
under the model   s distribution (bickel and doksum,
1977). for our directed dependency error loss func-
tion, we obtain the following decision rule:

h  (x) = argmin

  y

= argmax

ep  (y|x)[(cid:96)(  y, y)]

(cid:88)

p  (yi = on|x)

  y

i:   yi=on

(2)

(3)

here   y ranges over well-formed parses. thus, our
parser seeks a well-formed parse h  (x) whose in-
dividual edges have a high id203 of being cor-
rect according to p  . mbr is the principled way
to take a id168 into account under a prob-
abilistic model. by contrast, maximum a posteriori
(map) decoding does not consider the id168.
it would return the single highest-id203 parse
even if that parse, and its individual edges, were un-
likely to be correct.2

all systems in this paper use mbr decoding to
consider the id168 at test time. this implies
that the ideal training procedure would be to    nd
the true p   so that its marginals can be used in (3).
our baseline system attempts this. in practice, how-
ever, we will not be able to    nd the true p   (model
misspeci   cation) nor exactly compute the marginals
of p   (computational intractability). thus, this pa-
per proposes a training procedure that compensates
for the system   s approximations, adjusting    to re-
duce the actual loss of h  (x) as measured at training
time.

to    nd the mbr parse, we    rst run id136 to
compute the marginal id203 p  (yi = on) for
each edge. then we maximize (3) by running a    rst-
order dependency parser with edge scores equal to
those probabilities.3 when our id136 algorithm
is approximate, we replace the exact marginal with
its approximation   the normalized belief from bp,
given by bi(on) in (6) below.
id136 loopy belief propagation (bp) (mur-
phy et al., 1999) computes approximations to the
2if we used a simple 0-1 id168 within (2), then mbr

decoding would reduce to map decoding.

3prior work (smith and eisner, 2008; bansal et al., 2014)
used the log-odds ratio log p   (yi=on)
p   (yi=off) as the edge scores for
decoding, but this yields a parse different from the mbr parse.

variable marginals p  (yi) and the factor marginals
p  (y  ).
the algorithm proceeds by iteratively
sending messages from variables, yi, to factors,     :

m(t)

i     (yi) =

m(t   1)
     i (yi)

(4)

(cid:89)

     n (i)\  

(cid:89)

(cid:88)

and from factors to variables:

m(t   1)
j      (yi)

m(t)

     i(yi) =

    (y  )

y     yi

j   n (  )\i

(5)
where n (i) and n (  ) denote the neighbors of yi
and      respectively, and where y       yi is standard
notation to indicate that y   ranges over all assign-
ments to the variables participating in the factor   
provided that the ith variable has value yi. note that
the messages at time t are computed from those at
time (t    1). messages at the    nal time tmax are used
to compute the beliefs at each factor and variable:

(cid:89)

     n (i)

bi(yi) =

m(tmax)

     i (yi)

(cid:89)

i   n (  )

b  (y  ) =     (y  )

m(tmax)

i      (yi)

(6)

(7)

yi

each of the functions de   ned by equations (4)   
(7) can be optionally rescaled by a constant at any
time, e.g., to prevent over   ow/under   ow. below, we
speci   cally assume that each function bi has been
bi(yi) = 1. this bi approxi-

rescaled such that(cid:80)

mates the marginal distribution over yi values.

messages continue to change inde   nitely if the
factor graph is cyclic, but in the limit, the rescaled
messages may converge. although the equations
above update all messages in parallel, convergence
is much faster if only one message is updated per
timestep, in some well-chosen serial order.4

for the ptree factor, the summation over vari-
able assignments required for m(t)
     i(yi) in eq. (5)
equates to a summation over exponentially many
projective parse trees. however, we can use an
inside-outside variant of the algorithm of eisner

4following dreyer and eisner (2009, footnote 22), we
choose an arbitrary directed spanning tree rooted at the ptree
factor. we visit the nodes in topologically sorted order (starting
at the leaves) and update any message from the node being vis-
ited to a node that is later in the order (e.g., closer to the root).
we then reverse this order and repeat, so that every message has
been passed once. this constitutes one iteration of bp.

d(cid:88)

(1996) to compute this in polynomial time (we de-
scribe this as hypergraph parsing in    3). the re-
sulting    structured bp    id136 procedure is exact
for    rst-order id33, and approximate
when high-order factors are incorporated. the ad-
vantage of bp is that it enables fast approximate in-
ference when exact id136 is too slow. see smith
and eisner (2008) for details.5

3 approximation-aware learning
we aim to    nd the parameters       that minimize a
regularized objective function over the training sam-
ple of sentence/parse pairs {(x(d), y(d))}d

d=1.

      = argmin

  

||  ||2

2 +

  
2

1
d

j(  ; x(d), y(d))

d=1

(8)

where    > 0 is the id173 coef   cient and
j(  ; x, y) is a given differentiable function, pos-
sibly nonconvex. we locally minimize this objec-
tive using (cid:96)2-regularized adagrad with composite
mirror descent (duchi et al., 2011)   a variant of
stochastic id119 that uses mini-batches,
an adaptive learning rate per dimension, and sparse
lazy updates from the regularizer.6
objective functions as in stoyanov et al. (2011),
our aim is to minimize expected loss on the true data
distribution over sentence/parse pairs (x, y ):

      = argmin  

e[(cid:96)(h  (x), y )]

(9)

since the true data distribution is unknown, we
substitute the expected loss over the training sam-
ple, and regularize our objective to reduce sam-
pling variance. speci   cally, we aim to minimize
the regularized empirical risk, given by (8) with
j(  ; x(d), y(d)) set to (cid:96)(h  (x(d)), y(d)). using our
mbr decoder h   in (3), this id168 would
5how slow is exact id136 for id33? for
certain choices of higher-order factors, polynomial time is pos-
sible via id145 (mcdonald et al., 2005; car-
reras, 2007; koo and collins, 2010). however, bp will typically
be asymptotically faster (for a    xed number of iterations) and
faster in practice. in some other settings, exact id136 is np-
hard.
in particular, non-projective parsing becomes np-hard
with even second-order factors (mcdonald and pereira, 2006).
bp can handle this case in polynomial time by replacing the
ptree factor with a tree factor that allows edges to cross.

6   is initialized to 0 when not otherwise speci   ed.

not be differentiable because of the argmax in the
de   nition of h   (3). we will address this be-
low by substituting a differentiable softmax. this
is the    erma    method of stoyanov and eisner
(2012). we will also consider simpler choices of
j(  ; x(d), y(d)) that are commonly used in training
neural networks. finally, the standard convex objec-
tive is conditional log-likelihood (   4).
gradient computation to compute the gradi-
ent      j(  ; x, y   ) of the loss on a single sentence
(x, y   ) = (x(d), y(d)), we apply automatic differ-
entiation (ad) in the reverse mode (griewank and
corliss, 1991). this yields the same type of    back-
propagation    algorithm that has long been used for
training neural networks (rumelhart et al., 1986).
in effect, we are regarding (say) 5 iterations of the
bp algorithm on sentence x, followed by (softened)
mbr decoding and comparison to the target out-
put y   , as a kind of neural network that computes
(cid:96)(h  (x), y   ).
it is important to note that the re-
sulting gradient computation algorithm is exact up
to    oating-point error, and has the same asymptotic
complexity as the original decoding algorithm, re-
quiring only about twice the computation. the ad
method applies provided that the original function is
indeed differentiable with respect to   , an issue that
we take up below.

in principle, it is possible to compute the gradi-
ent with minimal additional coding. there exists
ad software (some listed at autodiff.org) that
could be used to derive the necessary code automat-
ically. another option would be to use the pertur-
bation method of domke (2010). however, we im-
plemented the gradient computation directly, and we
describe it here.

3.1

id136, decoding, and loss as a
feedfoward circuit

the id26 algorithm is often applied to
neural networks, where the topology of a feedfor-
ward circuit is statically speci   ed and can be ap-
plied to any input. our bp algorithm, decoder, and
id168 similarly de   ne a feedfoward circuit
that computes our function j. however, the circuit   s
topology is de   ned dynamically (per sentence x(d))
by    unrolling    the computation into a graph.

figure 2 shows this topology for one choice of ob-

jective function. the high level modules consist of
(a) computing potential functions, (b) initializing
messages, (c) sending messages, (d) computing be-
liefs, and (e) decoding and computing the loss. we
zoom in on two submodules: the    rst computes mes-
sages from the ptree factor ef   ciently (c.1   c.3);
the second computes a softened version of our loss
function (e.1   e.3). both of these submodules are
made ef   cient by the inside-outside algorithm.

the remainder of this section describes additional
details of how we de   ne the function j (the forward
pass) and how we compute its gradient (the back-
ward pass). id26 computes the deriva-
tive of any given function speci   ed by an arbitrary
circuit consisting of elementary differentiable oper-
ations (e.g. +,   ,  ,  , log, exp). this is accom-
plished by repeated application of the chain rule.

backpropagating through an algorithm proceeds
by similar application of the chain rule, where the in-
termediate quantities are determined by the topology
of the circuit. doing so with the circuit from figure
2 poses several challenges. eaton and ghahramani
(2009) and stoyanov et al. (2011) showed how to
backpropagate through the basic bp algorithm, and
we reiterate the key details below (   3.3). the re-
maining challenges form the primary technical con-
tribution of this paper:
1. our true id168 (cid:96)(h  (x), y   ) by way of
the decoder (3) contains an argmax over trees
and is therefore not differentiable. we show
how to soften this decoder, making it differen-
tiable (   3.2).

2. empirically, we    nd the above objective dif   -
cult to optimize. to address this, we substitute
a simpler l2 id168 (commonly used in
neural networks). this is easier to optimize and
yields our best parsers in practice (   3.2).

3. we show how to run backprop through
the inside-outside algorithm on a hypergraph
(   3.5), and thereby on the softened decoder
and computation of messages from the ptree
factor. this allows us to go beyond stoy-
anov et al. (2011) and train structured bp in an
approximation-aware and loss-aware fashion.

(e) decode and loss

j(  ; x, y   ) =

(e.3) expected recall

(e.2) inside-outside

(e.1) anneal beliefs

(d) beliefs
  bi(xi) = . . .,   b  (x  ) = . . .

(c) messages at time t
i     (xi) = . . .,   m(t )
  m(t )
ptree   i(xi) =

  m(t )

     i(xi) = . . .

(c.3) outgoing messages

(c.2) inside-outside

(c.1) message ratios

      

(c) messages at time t
i     (xi) = . . .,   m(t)
  m(t)
ptree   i(xi) =

  m(t)

     i(xi) = . . .

(c.3) outgoing messages

(c.2) inside-outside

(c.1) message ratios

      

(c) messages at time t = 1
  m(1)

i     (xi) = . . .,   m(1)
ptree   i(xi) =

  m(1)

(c.3) outgoing messages

(c.2) inside-outside

(c.1) message ratios

     i(xi) = . . .

(e.3) expected recall

(e.2) inside-outside

(e.1) anneal beliefs

(c.3) outgoing messages

(c.2) inside-outside

(c.1) message ratios

(a) compute potentials
    (x  ) = exp(      f (x  ))

(b) initial messages

m(0)
m(0)

i     (xi) = 1
     i(xi) = 1

figure 2: feed-forward topology of id136, decoding,
and loss. (e) shows the annealed risk, one of the objec-
tive functions we consider.

3.2 differentiable objective functions
annealed risk directed
error,
dependency
(cid:96)(h  (x), y   ),
is not differentiable due to the
argmax in the decoder h  . we therefore rede   ne
j(  ; x, y   ) to be a new differentiable id168,
(x, y   ), which approaches
the annealed risk r1/t
the loss (cid:96)(h  (x), y   ) as the temperature t     0.
this is done by replacing our non-differentiable
decoder h   with a differentiable one (at training
time). as input, it still takes the marginals p  (yi =
on | x), or in practice, their bp approximations
bi(on). we de   ne a distribution over parse trees:

  

(  y)     exp

q1/t
  

p  (yi = on|x)/t

i:  yi=on

imagine that at training time, our decoder stochas-
tically returns a parse   y sampled from this distribu-
tion. our risk is the expected loss of that decoder:

r1/t

  

(x, y   ) = e

  y   q1/t

  

[(cid:96)(  y, y   )]

(11)

as t     0 (   annealing   ), the decoder almost always

       (10)

       (cid:88)

(x, y   ) =    (cid:80)

chooses the mbr parse,7 so our risk approaches the
loss of the actual mbr decoder that will be used at
test time. however, as a function of   , it remains
differentiable (though not convex) for any t > 0.

  

  

  

i =on q1/t
i:y   

to compute the annealed risk, observe that it sim-
pli   es to r1/t
(  yi =
on). this is the negated expected recall of a
parse   y     q1/t
. we obtain the required marginals
q1/t
(  yi = on) from (10) by running inside-outside
  
where the edge weight for edge i is given by
exp(p  (yi = on|x)/t ).
with the annealed risk as our j function, we can
compute      j by backpropagating through the com-
putation in the previous paragraph. the computa-
tions of the edge weights and the expected recall
are trivially differentiable. the only challenge is
computing the partials of the marginals differentiat-
ing the function computed by this call to the inside-
outside algorithm; we address this in section 3.5.
figure 2 (e.1   e.3) shows where these computations
lie within the circuit.

whether our

test-time system computes the
marginals of p   exactly or does so approximately
via bp, our new training objective approaches (as
t     0) the true empirical risk of the test-time parser
that performs mbr decoding from the computed
marginals. empirically, however, we will    nd that
it is not the most effective training objective (   5.2).
stoyanov et al. (2011) postulate that the nonconvex-
ity of empirical risk may make it a dif   cult function
to optimize (even with annealing). our next two ob-
jectives provide alternatives.
l2 distance we can view our id136, decoder,
and loss as de   ning a form of deep neural network,
whose topology is inspired by our linguistic knowl-
edge of the problem (e.g., the edge variables should
de   ne a tree). this connection to deep learning al-
lows us to consider training methods akin to super-
vised layer-wise training. we temporarily remove
the top layers of our network (i.e. the decoder and
loss module, fig. 2 (e)) so that the output layer of
our    deep network    consists of the normalized vari-

imizes the sum (cid:80)

7recall from (3) that the mbr parse is the tree   y that max-
i:  yi=on p  (yi = on|x). as t     0, the
right-hand side of (10) grows fastest for this   y, so its probabil-
ity under q1/t
approaches 1 (or 1/k if there is a k-way tie for
  
mbr parse).

able beliefs bi(yi) from bp. we can then de   ne a
supervised id168 directly on these beliefs.

we don   t have supervised data for this layer of
beliefs, but we can create it arti   cially. use the
supervised parse y    to de   ne    target beliefs    by
i )     {0, 1}. to    nd parame-
i (yi) = i(yi = y   
b   
ters    that make bp   s beliefs close to these targets,
(cid:88)
(cid:88)
we can minimize an l2 distance id168:

(bi(yi)     b   

i (y   

i ))2

j(  ; x, y   ) =

(12)

i

yi

we can use this l2 distance objective function for
training, adding the mbr decoder and loss evalua-
tion back in only at test time.

layer-wise training just as in layer-wise train-
ing of neural networks, we can take a two-stage ap-
proach to training. first, we train to minimize the
l2 distance. then, we use the resulting    as ini-
tialization to optimize the annealed risk, which does
consider the decoder and id168 (i.e. the top
layers of fig. 2). stoyanov et al. (2011) found mean
squared error (mse) to give a smoother training ob-
jective, though still non-convex, and similarly used
it to    nd an initializer for empirical risk. though
their variant of the l2 objective did not completely
dispense with the decoder as ours does, it is a similar
approach to our proposed layer-wise training.

3.3 id26 through bp
belief propagation proceeds iteratively by sending
messages. we can label each message with a times-
tamp t (e.g. m(t)
i     ) indicating the time step at which
it was computed. figure 2 (b) shows the messages
at time t = 0, denoted m(0)
i     , which are initial-
ized to the uniform distribution. figure 2 (c) depicts
the computation of all subsequent messages via eqs.
(4) and (5). messages at time t are computed from
messages at time t     1 or before and the potential
functions     . after the    nal iteration t , the beliefs
bi(yi), b  (y  ) are computed from the    nal messages
m(t )
i      using eqs. (6) and (7)   this is shown in fig-
ure 2 (d). optionally, we can normalize the mes-
sages after each step to avoid over   ow (not shown
in the    gure) as well as the beliefs.

except for the messages sent from the ptree
factor, each step of bp computes some value from

earlier values using a simple formula.
back-
propagation differentiates these simple formulas.
this lets it compute j   s partial derivatives with re-
spect to the earlier values, once its partial derivatives
have been computed with respect to later values. ex-
plicit formulas can be found in the appendix of stoy-
anov et al. (2011).

3.4 bp and id26 with ptree
the ptree factor has a special structure that we ex-
ploit for ef   ciency during bp. stoyanov et al. (2011)
assume that bp takes an explicit sum in (5). for the
ptree factor, this equates to a sum over all projec-
tive dependency trees (since   ptree(y) = 0 for any
assignment y which is not a tree). there are expo-
nentially many such trees. however, smith and eis-
ner (2008) point out that for    = ptree, the sum-
mation has a special structure that can be exploited
by id145.

to compute the factor-to-variable messages from
   = ptree, they    rst run the inside-outside algo-
rithm where the edge weights are given by the ra-
tios of the messages to ptree: m(t)
. then
m(t)
they multiply each resulting edge marginal given by
inside-outside by the product of all the off mes-
i     (off) to get the marginal factor be-
lief b  (yi). finally they divide the belief by the in-
coming message m(t)
i     (on) to get the correspond-
ing outgoing message m(t+1)

sages(cid:81)

i     (on)
i     (off)

i m(t)

     i (on).

these steps are shown in figure 2 (c.1   c.3), and
are repeated each time we send a message from the
ptree factor. the derivatives of the message ratios
and products mentioned here are trivial. though
we focus here on projective id33,
our techniques are also applicable to non-projective
parsing and the tree factor; we leave this to fu-
ture work. in the next subsection, we explain how to
backpropagate through the inside-outside algorithm.

3.5 id26 through inside-outside

on a hypergraph

both the annealed risk id168 (   3.2) and the
computation of messages from the ptree factor use
the inside-outside algorithm for dependency pars-
ing. here we describe inside-outside and the ac-
companying id26 algorithm over a hy-
pergraph. this more general treatment shows the ap-

plicability of our method to other structured factors
such as for cnf parsing, id48 forward-backward,
etc. in the case of id33, the structure
of the hypergraph is given by the dynamic program-
ming algorithm of eisner (1996).

for the forward pass of the inside-outside mod-
ule, the input variables are the hyperedge weights
we   e and the outputs are the marginal probabilities
pw(i)   i of each node i in the hypergraph. the latter
are a function of the inside   i and outside   j proba-
bilities. we initialize   root = 1.

(cid:89)

we

j   t (e)

we   h(e)

(cid:88)
(cid:88)

e   i(i)

e   o(i)

  i =

  j =

pw(i) =   i  i/  root

  j

(cid:89)

j   t (e):j(cid:54)=i

  j

(13)

(14)

(15)

for each node i, we de   ne the set of incoming edges
i(i) and outgoing edges o(i). the antecedents of
the edge are t (e), the parent of the edge is h(e),
and its weight is we.
below we use the concise notation of an adjoint
  y =    j
   y , a derivative with respect to objective j.
for the backward pass through the inside-outside
ad module, the inputs are   pw(i)   i and the out-
puts are   we   e. we also compute the adjoints of the
intermediate quantities     j,     i. we    rst compute
    i bottom-up. next     j are computed top-down.
the adjoints   we are then computed in any order.

(cid:88)

(cid:88)

j   t (e)

    j

     j
     i

    root =

    i =   pw(i)    pw(i)

+

(cid:88)

i(cid:54)=root

(cid:88)

     i

e   i(i)
  pw(i)    pw(i)

     root

(cid:88)

e   o(j)
    k

(cid:88)

+

e   o(j)

k   t (e):k(cid:54)=j

    j =   pw(j)    pw(j)

+

     j

    h(e)

     h(e)

     j

(16)

(17)

(18)

(19)

  we =     h(e)

     h(e)

   we

+

    j

     j
   we

(20)

   j (cid:54)= root

     k
     j

(cid:88)

j   t (e)

below, we show the partial derivatives required for
the adjoint computations.

   pw(i)

     i

   pw(i)

     i

=   i/  root,

=   i/  root

   pw(i)
     root

=      i  i/(  2

root),

for some edge e, let i = h(e) be the parent of the
edge and j, k     t (e) be among its antecendents.

(cid:89)
(cid:89)

     h(e)

   we

=

     i
     j

     j
     i

     k
     j

= we

= we

k   t (e):k(cid:54)=j

k   t (e):k(cid:54)=j

  k,

  k,

(cid:89)

= we  h (e)

  l

l   t (e):l(cid:54)=j,l(cid:54)=k

(cid:89)

j   t (e)

  j

(cid:89)

     j
   we

=   h(e)

  k

k   t (e):k(cid:54)=j

this id26 method is used for both fig-
ure 2 c.2 and e.2.

4 other learning settings

loss-aware training with exact
id136
backpropagating through id136, decoder, and
loss need not be restricted to approximate id136
algorithms. li and eisner (2009) optimize bayes
risk with exact
id136 on a hypergraph for
machine translation. each of our differentiable loss
functions (   3.2) can also be coupled with exact
id136. for a    rst-order parser, bp is exact. yet,
in place of modules (b), (c), and (d) in figure 2, we
can use a standard id145 algorithm
for id33, which is simply another
instance of inside-outside on a hypergraph (   3.5).
the exact marginals from inside-outside (15) are
then fed forward into the decoder/loss module (e).

conditional and surrogate log-likelihood the
standard approach to training is conditional log-
likelihood (cll) maximization (smith and eisner,
2008), which does not take inexact id136 into
account. when id136 is exact,
this baseline
computes the true gradient of cll. when infer-
ence is approximate, this baseline uses the approxi-
mate marginals from bp in place of their exact val-
ues in the gradient.
the literature refers to this
approximation-unaware training method as surro-
gate likelihood training since it returns the    wrong   
model even under the assumption of in   nite train-
ing data (wainwright, 2006). despite this, the surro-
gate likelihood objective is commonly used to train
crfs. cll and approximation-aware training are
not mutually exclusive. training a standard factor
graph with erma and a log-likelihood objective re-
covers cll exactly (stoyanov et al., 2011).

5 experiments
5.1 setup
features as the focus of this work is on a novel
approach to training, we look to prior work for
model and feature design. we add o(n3) second-
order grandparent and arbitrary sibling factors as in
riedel and smith (2010) and martins et al. (2010).
we use standard feature sets for    rst-order (mcdon-
ald et al., 2005) and second-order (carreras, 2007)
parsing. following rush and petrov (2012), we also
include a version of each part-of-speech (pos) tag
feature, with the coarse pos tags from petrov et
al. (2012). we use feature hashing (ganchev and
dredze, 2008; attenberg et al., 2009) and restrict to
at most 20 million features. we leave the incorpora-
tion of third-order features to future work.

pruning to reduce the time spent on feature ex-
traction, we enforce the type-speci   c dependency
length bounds from eisner and smith (2005) as used
by rush and petrov (2012): the maximum allowed
dependency length for each tuple (parent tag, child
tag, direction) is given by the maximum observed
length for that tuple in the training data. follow-
ing koo and collins (2010), we train an (exact)
   rst-order model and for each token prune any par-
ents for which the marginal id203 is less than
0.0001 times the maximum parent marginal for that
token.8 on a per-token basis, we further restrict to
the ten parents with highest marginal id203 as
in martins et al. (2009). the pruning model uses a
simpler feature set as in rush and petrov (2012).

data we consider 19 languages from the conll-
2006 (buchholz and marsi, 2006) and conll-2007
(nivre et al., 2007) shared tasks. we also convert
the english id32 (ptb) (marcus et al.,
1993) to dependencies using the head rules from ya-
mada and matsumoto (2003) (ptb-ym). we evalu-
ate unlabeled attachment accuracy (uas) using gold
pos tags for the conll languages, and predicted
tags from turbotagger9 for the ptb. unlike most
prior work, we hold out 10% of each conll train-
ing dataset as development data.

8we expect this to be the least impactful of our approxima-
tions: koo and collins (2010) report 99.92% oracle accuracy
for english.

9

http://www.cs.cmu.edu/  afm/turboparser

l2
l2+ar

l2
l2+ar

figure 3: speed accuracy tradeoff of uas vs. the number
of bp iterations for standard conditional likelihood train-
ing (cll) and our approximation-aware training with ei-
ther an l2 objective (l2) or a staged training of l2 fol-
lowed by annealed risk (l2+ar). note that x-axis shows
the number of iterations used for both training and test-
ing. we use a 2nd-order model with grand.+sib. factors.

some of the conll languages contain nonpro-
jective edges. with the projectivity constraint,
the
model assigns zero id203 to such trees. for
approximation-aware training this is not a problem;
however cll training cannot handle such trees. for
cll only, we projectivize the training trees follow-
ing (carreras, 2007) by    nding the maximum pro-
jective spanning tree under an oracle model which
assigns score +1 to edges in the gold tree and 0 to
the others. we always evaluate on the nonprojec-
tive trees for comparison with prior work.

learning settings we compare three learning set-
tings. the    rst, our baseline, is conditional log-
likelihood training (cll) (   4). as is common
in the literature, we con   ate two distinct learning
settings (conditional log-likelihood/surrogate log-
likelihood) under the single name    cll    allowing
the id136 method (exact/inexact) to differentiate
them. the second learning setting is approximation-
aware learning (   3) with either our l2 distance
objective (l2) or our layer-wise training method
(l2+ar) which takes the l2-trained model as an ini-
tializer for our annealed risk (   3.2). the annealed
risk objective requires an annealing schedule: over
the course of training, we linearly anneal from initial
temperature t = 0.1 to t = 0.0001, updating t at
each iteration of stochastic optimization. the third

figure 4: uas vs. the types of 2nd-order factors included
in the model for approximation-aware training and stan-
dard conditional likelihood training. all models include
1st-order factors (unary). the 2nd-order models include
grandparents (grand.), arbitrary siblings (sib.), or both
(grand.+sib.)   and use 4 iterations of bp.

uses the same two objectives, l2 and l2+ar, but
with exact id136 (   4). the (cid:96)2-regularizer weight
0.1d . each method is trained by adagrad for
is    = 1
10 epochs with early stopping (i.e. the model with
the highest score on dev data is returned). the learn-
ing rate for each training run is dynamically tuned on
a sample of the training data.

5.2 results
our goal is to demonstrate that our approximation-
aware training method leads to improved parser ac-
curacy as compared with the standard training ap-
proach of conditional log-likelihood (cll) maxi-
mization (smith and eisner, 2008), which does not
take inexact id136 into account. the two key
   ndings of our experiments are that our learning ap-
proach is more robust to (1) decreasing the number
of iterations of bp and (2) adding additional cycles
to the factor graph in the form of higher-order fac-
tors. in short: our approach leads to faster id136
and creates opportunities for more accurate parsers.

speed-accuracy tradeoff our    rst experiment is
on english dependencies. for english ptb-ym,
figure 3 shows accuracy as a function of the num-
ber of bp iterations for our second-order model with
both arbitrary sibling and grandparent factors on en-
glish. we    nd that our training methods (l2 and
l2+ar) obtain higher accuracy than standard train-
ing (cll), particularly when a small number of bp
iterations are used and the id136 is a worse ap-

88.0 89.0 90.0 91.0 92.0 93.0 1 2 3 4 5 6 7 8 uas # iterations of bp cll                 90.5 91 91.5 92 92.5 unary grand. sib. grand.+sib. uas cll                 proximation. notice that with just two iterations of
bp, the parsers trained by our approach obtain ac-
curacy equal to the cll-trained parser with four
iterations. contrasting the two objectives for our
approximation-aware training, we    nd that our sim-
ple l2 objective performs very well. in fact, in only
one case at 6 iterations, does the additional annealed
risk (l2+ar) improve performance on test data. in
our development experiments, we also evaluated ar
without using l2 for initialization and we found that
it performed worse than either of cll and l2 alone.
that ar performs only slightly better than l2 (and
not worse) in the case of l2+ar is likely due to early
stopping on dev data, which guards against selecting
a worse model.
increasingly cyclic models figure 4 contrasts
accuracy with the type of 2nd-order factors (grand-
parent, sibling, or both) included in the model for
english, for a    xed budget of 4 bp iterations. as we
add additional higher-order factors, the model has
more loops thereby making the bp approximation
more problematic for standard cll training. by
contrast, our training performs well even when the
factor graphs have many cycles.

notice that our advantage is not restricted to the
case of loopy graphs. even when we use a 1st-
order model, for which bp id136 is exact, our
approach yields higher accuracy parsers than cll
training. we postulate that this improvement comes
from our choice of the l2 objective function. note
the following subtle point: when id136 is ex-
act, the cll estimator is actually a special case
of our approximation-aware learner   that is, cll
computes the same gradient that our training by
id26 would if we used log-likelihood as
the objective. despite its appealing theoretical justi-
   cation, the ar objective that approaches empirical
risk minimization in the limit consistently provides
no improvement over our l2 objective.
exact id136 with grandparents when our
factor graph includes unary and grandparent fac-
tors, exact id136 in o(n4) time is possible us-
ing the id145 algorithm for model
0 of koo and collins (2010). table 1 compares four
parsers, by considering two training approaches and
two id136 methods. the training approaches are
cll and approximation-aware id136 with an l2

train
cll
cll
l2
l2

id136 dev uas
bp 4 iters
exact
bp 4 iters
exact

91.37
91.99
91.83
91.91

test uas

91.25
91.62
91.63
91.66

table 1: the impact of exact vs. approximate id136
on a 2nd-order model with grandparent factors only. re-
sults are for the development (   22) and test (   23) sec-
tions of ptb-ym.

objective. the id136 methods are bp with only
four iterations or exact id136 by dynamic pro-
gramming. on test uas, we    nd that both the cll
and l2 parsers with exact id136 outperform ap-
proximate id136   though the margin for cll
is much larger. surprisingly, our l2-trained parser,
which uses only 4 iterations of bp and o(n3) run-
time, does just as well as cll with exact infer-
ence. our l2 parser with exact id136 performs
the best.

other languages our    nal experiments evaluate
our approximation-aware learning approach across
19 languages from conll-2006/2007 (table 2).
we    nd that, on average, approximation-aware
training with an l2 objective obtains higher uas
than cll training. this result holds for both 1st-
and 2nd-order models with grandparent and sibling
factors with 1, 2, 4, or 8 iterations of bp. table
2 also shows the relative improvement in uas of
l2 vs cll training for each language as we vary
the maximum number of iterations of bp. we    nd
that the approximation-aware training doesn   t al-
ways outperform cll training   only in the aggre-
gate. again, we see the trend that our training ap-
proach yields more signi   cant gains when bp is re-
stricted to a small number of maximum iterations.

6 discussion

the purpose of this work was to explore erma and
related training methods for models which incorpo-
rate structured factors. we applied these methods to
a basic higher-order id33 model, be-
cause that was the simplest and    rst (smith and eis-
ner, 2008) instance of structured bp. in future work,
we hope to explore further models with structured
factors   particularly those which jointly account for
multiple linguistic strata (e.g. syntax, semantics, and

1st-order

2nd-order (with given num. bp iterations)

1

2

4

8

cll
77.63
90.38
90.47
84.69
87.15
88.55
82.43
88.31
81.49
73.69
78.79
84.75
93.54
76.96
86.31
79.89
87.22
78.53
84.93
83.98

l2-cll
-0.26
-0.76
+0.30
-0.07
-0.12
+0.81
-0.54
+0.32
-0.09
+0.11
-0.52
+0.32
+0.19
+0.53
+0.38
+0.30
+0.60
-0.30
-0.39
+0.04

cll
73.39
89.18
88.90
79.92
86.31
88.06
80.02
85.53
79.08
71.45
76.46
84.14
93.01
74.23
85.68
78.42
86.14
77.43
82.62
82.10

l2-cll
+2.21
-0.45
+0.17
+3.78
-1.07
0.00
+0.29
+1.44
-0.37
+0.85
+1.24
+0.04
+0.44
+2.08
-0.01
+1.50
-0.02
-0.64
+1.43
+0.68

cll
77.05
90.44
90.79
82.08
87.41
89.27
81.97
87.67
80.73
74.16
79.10
85.15
93.71
77.12
87.01
79.56
87.68
78.51
84.27
83.88

l2-cll
-0.17
+0.04
+0.38
+2.27
+0.03
+0.46
+0.09
+1.82
+0.14
+0.24
+0.03
+0.01
-0.10
+0.53
+0.29
+1.02
+0.74
-1.04
+0.95
+0.41

cll
77.20
90.73
91.21
83.02
87.65
89.85
82.49
88.63
81.75
74.92
79.07
85.66
93.75
78.03
87.34
80.91
88.01
78.80
84.79
84.41

l2-cll
+0.02
+0.25
+0.78
+2.94
-0.11
-0.05
-0.16
+1.14
-0.66
-0.32
+0.60
-0.51
-0.26
-0.27
+0.08
+0.03
+0.41
-1.06
+0.68
+0.19

cll
77.16
90.63
91.49
81.60
87.68
89.87
82.66
88.85
81.52
74.94
79.28
85.81
93.47
77.83
87.30
80.80
87.87
78.91
84.77
84.34

l2-cll
-0.07
-0.19
+0.66
+4.42
-0.10
-0.07
-0.04
+0.96
+0.02
-0.38
+0.31
-0.59
+0.07
-0.09
+0.17
+0.34
+0.37
-1.13
+1.14
+0.31

language

ar
bg
ca
cs
da
de
el
en
es
eu
hu
it
ja
nl
pt
sl
sv
tr
zh
avg.

table 2: results on 19 languages from conll-2006/2007. for languages appearing in both datasets, the 2006 version
was used, except for chinese (zh). evaluation follows the 2006 conventions and excludes punctuation. we report
absolute uas for the baseline (cll) and the improvement in uas for l2 over cll (l2-cll) with positive/negative
differences in blue/red. the average uas and average difference across all languages (avg.) is given.

topic). another natural extension of this work is to
explore other types of factors: here we considered
only exponential-family potential functions (com-
monly used in crfs), but any differentiable function
would be appropriate, such as a neural network.

our primary contribution is approximation-aware
training for structured bp. while our experiments
only consider id33, our approach is
applicable for any constraint factor which amounts
to running the inside-outside algorithm on a hyper-
graph. prior work has used this structured form
of bp to do id33 (smith and eis-
ner, 2008), cnf grammar parsing (naradowsky
et al., 2012), tag (auli and lopez, 2011), itg-
constraints for phrase extraction (burkett and klein,
2012), and id114 over strings (dreyer
and eisner, 2009). our training methods could be
applied to such tasks as well.

7 conclusions

we introduce a new approximation-aware learning
framework for belief propagation with structured
factors. we present differentiable objectives for both

empirical risk minimization (a la. erma) and a
novel objective based on l2 distance between the in-
ferred beliefs and the true edge indicator functions.
experiments on the english id32 and 19
languages from conll-2006/2007 shows that our
estimator is able to train more accurate dependency
parsers with fewer iterations of belief propagation
than standard conditional log-likelihood training, by
taking approximations into account. our code is
available in a general-purpose library for structured
bp, hypergraphs, and backprop.10

references

josh attenberg, a dasgupta, j langford, a smola, and
k weinberger. 2009. feature hashing for large scale
multitask learning. in icml.

michael auli and adam lopez. 2011. a comparison
of loopy belief propagation and id209
for integrated id35 id55 and parsing. in pro-
ceedings of the 49th annual meeting of the associa-
tion for computational linguistics: human language

10http://www.cs.jhu.edu/  mrg/software/

technologies. association for computational linguis-
tics.

mohit bansal, david burkett, gerard de melo, and dan
klein. 2014. structured learning for taxonomy in-
duction with belief propagation. in proceedings of the
52nd annual meeting of the association for compu-
tational linguistics (volume 1: long papers), pages
1041   1051, baltimore, maryland, june. association
for computational linguistics.
p.j. bickel and k.a. doksum.

1977. mathematical
statistics: basic ideas and selected topics. holden-
day inc., oakland, ca, usa.

sabine buchholz and erwin marsi. 2006. conll-x
in

shared task on multilingual id33.
in proc. of conll, pages 149   164.

david burkett and dan klein. 2012. fast id136
in phrase extraction models with belief propagation.
in proceedings of the 2012 conference of the north
american chapter of the association for computa-
tional linguistics: human language technologies,
pages 29   38. association for computational linguis-
tics.

xavier carreras. 2007. experiments with a higher-order
in proceedings of the
projective dependency parser.
conll shared task session of emnlp-conll 2007,
pages 957   961.
j. domke. 2010.

implicit differentiation by perturba-
in advances in neural information processing

tion.
systems, pages 523   531.

markus dreyer and jason eisner.

2009. graphical
models over multiple strings. in proceedings of the
2009 conference on empirical methods in natural
language processing, pages 101   110. association for
computational linguistics.

john duchi, elad hazan, and yoram singer.

2011.
adaptive subgradient methods for online learning and
the journal of machine
stochastic optimization.
learning research.

frederik eaton and zoubin ghahramani. 2009. choos-
ing a variable to clamp. in international conference
on arti   cial intelligence and statistics, pages 145   
152.

jason eisner and noah a. smith. 2005. parsing with
soft and hard constraints on dependency length.
in
proceedings of the international workshop on parsing
technologies (iwpt), pages 30   41, vancouver, octo-
ber.

jason eisner. 1996. three new probabilistic models
for id33: an exploration. in proceed-
ings of the 16th international conference on com-
putational linguistics (coling-96), pages 340   345,
copenhagen, august.

brendan j. frey, frank r. kschischang, hans-andrea
loeliger, and niclas wiberg. 1997. factor graphs

and algorithms. in proceedings of the annual allerton
conference on communication control and comput-
ing, volume 35, pages 666   680.

kuzman ganchev and mark dredze. 2008. small sta-
tistical models by random feature mixing. in proceed-
ings of the acl08 hlt workshop on mobile language
processing, pages 19   20.

andreas griewank and george f. corliss, editors. 1991.
automatic differentiation of algorithms: theory, im-
plementation, and application. siam, philadelphia,
pa.

terry koo and michael collins. 2010. ef   cient third-
order dependency parsers. in proceedings of the 48th
annual meeting of the association for computational
linguistics, pages 1   11.

frank r. kschischang, brendan j. frey, and h.-a.
loeliger. 2001. factor graphs and the sum-product al-
gorithm. id205, ieee transactions on,
47(2):498   519.

a. kulesza and f. pereira. 2008. structured learning
with approximate id136. in advances in neural in-
formation processing systems 20.

zhifei li and jason eisner. 2009. first- and second-order
expectation semirings with applications to minimum-
risk training on translation forests. in proceedings of
the conference on empirical methods in natural lan-
guage processing (emnlp), pages 40   51, singapore,
august.

mitchell p. marcus, mary ann marcinkiewicz, and beat-
rice santorini. 1993. building a large annotated cor-
pus of english: the id32. computational lin-
guistics, 19(2).

andre martins, noah smith, and eric xing. 2009. con-
cise integer id135 formulations for de-
pendency parsing. in proceedings of the joint confer-
ence of the 47th annual meeting of the acl and the 4th
international joint conference on natural language
processing of the afnlp. association for computa-
tional linguistics.

andre martins, noah smith, eric xing, pedro aguiar,
and mario figueiredo. 2010. turbo parsers: depen-
dency parsing by approximate variational id136.
in proceedings of the 2010 conference on empirical
methods in natural language processing, pages 34   
44. association for computational linguistics.

ryan t. mcdonald and fernando pereira. 2006. online
learning of approximate id33 algo-
rithms. in eacl.

r. mcdonald, k. crammer, and f. pereira. 2005. on-
in

line large-margin training of dependency parsers.
proceedings of acl, volume 43, page 91.

kevin p. murphy, yair weiss, and michael i. jordan.
1999. loopy belief propagation for approximate in-
in proceedings of the
ference: an empirical study.

fifteenth conference on uncertainty in arti   cial intel-
ligence, pages 467   475. morgan kaufmann publishers
inc.

hiroyasu yamada and yuji matsumoto. 2003. statistical
dependency analysis with support vector machines. in
proceedings of iwpt, volume 3.

jason naradowsky, tim vieira, and david smith. 2012.
grammarless parsing for joint id136. in proceed-
ings of coling 2012, pages 1995   2010, mumbai, in-
dia, december. the coling 2012 organizing com-
mittee.

joakim nivre, johan hall, sandra k  ubler, ryan mcdon-
ald, jens nilsson, sebastian riedel, and deniz yuret.
2007. the conll 2007 shared task on dependency
in proceedings of the conll shared task
parsing.
session of emnlp-conll 2007, pages 915   932.

slav petrov, dipanjan das, and ryan mcdonald. 2012.

a universal part-of-speech tagset. in proc. of lrec.

sebastian riedel and david a. smith. 2010. relaxed
marginal id136 and its application to dependency
parsing. in joint human language technology con-
ference/annual meeting of the north american chap-
ter of the association for computational linguistics
(hlt-naacl    10), pages 760   768. association for
computational linguistics.

d. e. rumelhart, g. e. hinton, and r. j. williams. 1986.
learning internal representations by error propagation.
in david e. rumelhart and james l. mcclelland, edi-
tors, parallel distributed processing: explorations in
the microstructure of cognition, volume 1, pages 318   
364. mit press, cambridge, ma.

alexander m. rush and slav petrov. 2012. vine pruning
in pro-
for ef   cient multi-pass id33.
ceedings of the 2012 conference of the north ameri-
can chapter of the association for computational lin-
guistics: human language technologies, pages 498   
507.

david a. smith and jason eisner. 2008. dependency
parsing by belief propagation. in proceedings of the
conference on empirical methods in natural lan-
guage processing (emnlp).

veselin stoyanov and jason eisner. 2012. minimum-risk
training of approximate crf-based nlp systems. in
proceedings of naacl-hlt.

veselin stoyanov, alexander ropson, and jason eis-
ner. 2011. empirical risk minimization of graph-
ical model parameters given approximate id136,
in proceedings of
decoding, and model structure.
the 14th international conference on arti   cial intel-
ligence and statistics (aistats), volume 15 of jmlr
workshop and conference proceedings. supplemen-
tary material (4 pages) also available.

martin j. wainwright.

2006. estimating the wrong
graphical model: bene   ts in the computation-limited
setting. the journal of machine learning research,
7:1829   1859.

