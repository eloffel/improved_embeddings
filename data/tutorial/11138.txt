information-theoretical analysis of the statistical dependencies

among three variables: applications to written language

aps/123-qed

dami  an g. hern  andez, dami  an h. zanette, and in  es samengo

centro at  omico bariloche and instituto balseiro,

(8400) san carlos de bariloche, argentina.

(dated: october 26, 2018)

abstract

we develop the information-theoretical concepts required to study the statistical dependencies

among three variables. some of such dependencies are pure triple interactions, in the sense that

they cannot be explained in terms of a combination of pairwise correlations. we derive bounds

for triple dependencies, and characterize the shape of the joint id203 distribution of three

binary variables with high triple interaction. the analysis also allows us to quantify the amount

of redundancy in the mutual information between pairs of variables, and to assess whether the

information between two variables is or is not mediated by a third variable. these concepts are

applied to the analysis of written texts. we    nd that the id203 that a given word is found

in a particular location within the text is not only modulated by the presence or absence of other

nearby words, but also, on the presence or absence of nearby pairs of words. we identify the words

enclosing the key semantic concepts of the text, the triplets of words with high pairwise and triple

interactions, and the words that mediate the pairwise interactions between other words.

pacs numbers: 89.75.fb, 02.50.cw, 02.50.sk, 89.70.-a

5
1
0
2

 
l
u
j
 

0
3

 
 
]
l
c
.
s
c
[
 
 

1
v
0
3
5
3
0

.

8
0
5
1
:
v
i
x
r
a

1

i.

introduction

imagine a game where, as you read through a piece of text, you occasionally come across

a blank space representing a removed or occluded word. your task is to guess the missing

word. this is an example sentence,        your guess. if you were able to replace the blank

space in the previous sentence with    make   , or    try   , or some other related word, you have

understood the rules of the game. the task is called the cloze test [1] and is routinely

administered to evaluate language pro   ciency, or expertise in a given subject.

the cues available to the player to solve the task can be divided into two major groups.

first, surrounding words restrict the grammatical function of the missing word, since, for

example, a conjugated verb cannot usually take the place of a noun, nor vice versa. second,

and assuming that the grammatical function of the word has already been surmised, semantic

information provided by the surrounding words is typically helpful. that is, the presence

or absence of speci   c words in the neighborhood of the blank space a   ect the id203

of each candidate missing word. for example, if the word bee is near the blank space, the

likelihood of honey is larger than when bee is absent.

in this paper we study the structure of the probabilistic links between words due to

semantic connections. in particular, we aim at deciding whether binary interactions between

words su   ce to describe the structure of dependencies, or whether triple and higher-order

interactions are also relevant: should we only care for the presence or absence of speci   c

words in the vicinity of the blank space, or does the presence or absence of speci   c pairs

(or higher-order combinations) also matter in our ability to guess the missing word? for

example, one would expect that the presence of the word cell would increase the id203

of words as cytoplasm, phone or prisoner. the word wax, in turn, is easily associated

with ear, candle or tussaud. however, the conjoint presence of cell and wax points much

more speci   cally to concepts such as bee or honey, and diminish the id203 of words

associated with other meanings of cell and wax. combinations of words, therefore, also

matter in the creation of meaning, and context. the question is how relevant this e   ect is,

and whether the e   ect of the pair (cell + wax) is more, equal or less than the sum of the two

individual contributions (e   ect of cell + e   ect of wax). here we develop the mathematical

methods to estimate these contributions quantitatively.

the problem can be framed in more general terms. in any complex system, the statistical

2

dependence between individual units cannot always be reduced to a superposition of pairwise

interactions. triplet, or even higher-order dependencies may arise either because three

or more variables are dynamically linked together, or because some hidden variables, not

accessible to measurement, are linked to the visible variables through pairwise interactions.

in 2006, schneidman and coworkers [2] demonstrated that, in the vertebrate retina, up

to pairwise correlations between neurons could account for approximately 90% of all the

statistical dependencies in the joint id203 distribution of the whole population. this

   nding brought relief to the scienti   c community, since an expansion up to the second order

was regarded su   cient to provide an adequate description of the correlation structure of the

full system. as a consequence, not much e   ort has been dedicated to the detection and the

characterization of third or higher-order interactions. to our knowledge, the present work

constitutes the    rst example o   ering an exact description of third-order dependencies. we

derive the relevant information-theoretical measures, and then apply them to actual data.

as a model system, we work with the vast collection of words found in written language,

since this system is likely to embody complex statistical dependencies between individual

words. the dependencies arise from the syntactic and semantic structures required to map

a network of interwoven thoughts into an ordered sequence of symbols, namely, words. the

projection from the high-dimensional space of ideas onto the single dimension represented

by time can only be made because language encodes meaning in word order, and word

relations.

in particular, if speci   c words appear close to each other, they are likely to

construct a context, or a topic. the context is important in disambiguating among the

several meanings that words usually have. therefore, language constitutes a model system

where individual units (words) can be expected to exhibit high-order interactions.

statistics and id205 have proved to be useful in understanding language

structures. since zipf   s empirical law [3] on the frequency of words, and the pioneering

work of shannon [4] measuring the id178 of printed english, a whole branch of science

has followed these lines [5   7]. in recent years, the discipline gained momentum with the

availability of large data sources in the internet [8   11].

in this paper we quantify the amount of double and triple interactions between words

of a given text.

in addition, by means of a careful analysis of the structure of pairwise

interactions we distinguish between pairs of variables that interact directly, and pairs of

variables that are only correlated because they both interact with a third variable. with

3

these goals in mind, we de   ne and measure dependencies between words using concepts from

id205 [12   14], and apply them in later sections to the analysis of written texts.

ii. statistical dependencies among three variables

when it comes to quantifying the amount of statistical dependence between two variables

x1 and x2 with joint probabilities p(x1, x2) and marginal probabilities p(x1) and p(x2),

shannon   s mutual information [12, 14]

i(x1; x2) = xx1,x2

p(x1, x2) log

p(x1, x2)
p(x1)p(x2)

(1)

stands out for its generality and its simplicity. throughout this paper we take all logarithms

in base 2, and therefore measure all information-theoretical quantities in bits. in fig. 1,

pairwise statistical dependencies are represented by the rods connecting two variables (inde-

pendent variables appear disconnected). since i(x1; x2) is the id181

d[p(x1, x2) : p(x1)p(x2)] [14] between the joint distribution p(x1, x2) and its independent

approximation p(x1)p(x2), the mutual information is always non-negative. moreover, x1

and x2 are independent if and only if their mutual information vanishes.

three variables, in turn, may interact in di   erent ways; fig. 1 illustrates all the possibil-

ities. in this section, we discuss several quantities that measure the strength of the di   erent

interactions. so far, no general consensus has been reached regarding the way in which

statistical dependencies between three variables should be quanti   ed [15   24]. one attempt

in the framework of id205 is the symmetric quantity i(x1; x2; x3), sometimes

called the co-information [14, 20], de   ned as

i(x1; x2; x3) = i(x1; x2)     i(x1; x2|x3)
= i(x2; x3)     i(x2; x3|x1)
= i(x3; x1)     i(x3; x1|x2),

where i(xi; xj|xk) is the conditional mutual information,

i(xi; xj|xk) = xxi,xj,xk

p(xi, xj, xk) log[

p(xi, xj|xk)

p(xi|xk)p(xj|xk)

].

(2)

(3)

the co-information measures the way one of the variables (no matter which) in   uences

the transmission of information between the other two. positive or negative values of the

4

fig. 1. di   erent ways in which three variables may interact. a: the three variables are independent.

b: only pairwise interactions exist. these may involve 1, 2 or 3 links (from left to right). c: the three

variables are connected by a single triple interaction. d: double and triple interactions may coexist. the

most general case is illustrated in the bottom-right panel.

co-information have often been associated with redundancy or synergy between the three

variables, though one should be careful to distinguish between several possible meanings of

the words synergy and redundancy (see below, and also [25, 26]).

in an attempt to provide a systematic expansion of the di   erent interaction orders, amari

[19] developed an alternative way of measuring triple and higher-order interactions. his

approach uni   es concepts from categorical data analysis and maximum id178 techniques.

the theory is based on a decomposition of the joint id203 distribution as a product

of functions, each factor accounting for the interactions of a speci   c order. the    rst term

embodies the independent approximation, the second term adds all pairwise interactions,

subsequent terms orderly accounting for triplets, quadruplets and so forth. this approach

constitutes the starting point for the present work.

given the random variables x1, . . . , xn governed by a joint id203 distribution

p(x1, . . . , xn), all the marginal distributions of order k can be calculated by summing the
values of the joint distribution over n     k of the variables. since there are n!/k!(n     k)!
ways of choosing n    k variables among the original n, the number of marginal distributions
of order k is n!/k!(n     k)! amari de   ned the id203 distribution p(k)(x1, ..., xn ) as the
one with maximum id178 h (k)
max among all those that are compatible with all the marginal

5

distributions of order k. the maximization of the id178 under such constraints has a

unique solution [27]: the distribution allowing variables to vary with maximal freedom,

inasmuch they still obey the restriction imposed by the marginals. hence, p(k)(x1, ..., xn )

contains all the statistical dependencies among groups of k variables that were present in

the original distribution, but none of the dependencies involving more than k variables.

the interactions of order k are quanti   ed by the decrease of id178 from p(k   1) to p(k),

which can be expressed as a id181

d(k) = d[p(k) : p(k   1)]

= h (k   1)

max     h (k)
max,

(4)

where h (k)

max is the id178 of pk. the last inequality of eq. (4) derives from the generalized

pythagoras theorem [19]. as increasing constraints cannot increase the id178, d(k) is

always non-negative.

the total amount of interactions within a group of n variables, the so called multi-

information    (x1, . . . , xn ) [16], is de   ned as the id181 between the

actual joint id203 distribution and the distribution corresponding to the independent

approximation. the multi-information naturally splits in the sum of the di   erent interaction

orders

   12...n = d[p(x1, ..., xn ) : p(x1)...p(xn )]

=

n

xk=2

d(k).

(5)

for two variables, there are at most pairwise interactions. their strength, measured by

d(2), coincides with shannon   s mutual information

d(2)

12 = d[p(2)(x1, x2) : p(1)(x1, x2)]

= d[p(x1, x2) : p(x1)p(x2)]

(6)

= i(x1; x2),

since the distribution with maximum id178 that is compatible with the two univariate

marginals is p(1)(x1, x2) = p(x1)p(x2). this result is easily obtained by searching for the

6

joint distribution that maximizes the id178 using lagrange multipliers for the constraints

given by the marginals [28].

when studying three variables, x1, x2 and x3, we separately quantify the amount of

pairwise and of triple interactions. in this context, d(3)

123 measures the amount of statistical

dependency that cannot be explained by pairwise interactions, and is de   ned as

d(3)

123 = d[p(x1, x2, x3) : p(2)(x1, x2, x3)]

= h (2)

max     h123,

(7)

where h123 represents the full id178 of the triplet h(x1, x2, x3) calculated with p(x1, x2, x3).

the distribution p(2)(x1, x2, x3) contains up to pairwise interactions. if the actual dis-
tribution p(x1, x2, x3) coincides with p(2)(x1, x2, x3), there are no third-order interactions.
within amari   s framework, hence, if d(3)

123 > 0, some of the statistical dependency among

triplets cannot be explained in terms of pairwise interactions.

both i(x1; x2; x3) and d(3)

123 are generalizations of the mutual information intended to

describe the interactions between three variables, and both of them can be extended to an

arbitrary number of variables [19, 29].

it is important to notice, however, that the two

quantities have di   erent meanings. a vanishing co-information (i(x1; x2; x3) = 0) implies

that the mutual information between two of the variables remains una   ected if the value

of the third variable is changed. however, this does not mean that it su   ces to measure

only pairs of variables   and thereby obtain the marginals p(x1, x2), p(x2, x3), p(x3, x1)   to

reconstruct the full id203 distribution p(x1, x2, x3). conversely, a vanishing triple in-
teraction (d(3)

123 = 0) ensures that pairwise measurements su   ce to reconstruct the full joint

distribution. yet, the value of any of the variables may still a   ect how much information is

transmitted between the other two.

we shall later need to specify the groups of variables whose marginals are used as con-

straints. we therefore introduce a new notation for the maximum id178 id203

distributions and for the maximum entropies. let v represent a set of k variables. for
example, if k = 3, we may have v = {x1, x2, x3}. when studying the dependencies of
k-th order, we shall be working with all sets v1, . . . , vr that can be formed with k variables,
where r = n!/k!(n     k)! let pv1,v2,...,vr be the id203 distribution of maximum id178

7

hv1,v2,...,vr that satis   es the marginal restrictions of v1, v2, . . . , vk. under this notation,

p(2)(x1, x2, x3) = p12,13,23

p(1)(x1, x2, x3) = p1,2,3.

(8)

respectively, the maximum entropies are h12,13,23 and h1,2,3 = h(x1) + h(x2) + h(x3).

under the present notation, the mutual information i(xi; xj) is iij, and the co-information

of three variables x1, x2, x3 is written as i123.
the amount of pairwise interactions d(2)

ij between variables i and j is known to be

bounded by [14]

d(2)
ij = iij     min(hi, hj).

(9)

we have derived an analogous bound for triple interactions (see appendix a). the resulting
inequality links the amount of triple interactions d(3)

123 with the co-information i123,

d(3)
123     min{i12, i23, i31}     i123     min{h1, h2, h3}.

(10)

these bounds imply that pure triple interactions, appearing in the absence of pairwise

interactions (see fig. 1c), may only exist if the co-information i123 is negative.

a. characterization of the joint id203 distribution of variables with high

triple interactions

two binary variables x1 and x2 can have maximal mutual information i12 = 1 bit in two
di   erent situations. for the sake of concreteness, assume that xi =   1. maximal mutual
information is obtained either when x1 = x2 or when x1 =    x2. in other words, the joint
id203 distribution must either vanish when the two variables are equal, or when the two

variables are di   erent, as illustrated in fig. 2a. if the mutual information is high, though

perhaps not maximal, then the two variables must still remain somewhat correlated, or anti-

correlated. the joint id203 distribution, hence, must drop for those states where the

variables are equal - or di   erent. in this section we develop an equivalent intuitive picture

of the joint id203 distribution of triplets with maximal (or, less ambitiously, just high)

triple interaction.

8

fig. 2. a: density plot of the two bivariate id203 distributions that have i = 1 bit. dark states have

zero id203, and white states have p(x1, x2) = 1/2. b: density plot of the two trivariate id203

distributions with d(3)

ijk = 1 bit. dark states have zero id203, and white states have p(x1, x2, x3) =

1/4. c: gradual change between a uniform distribution and a xor distribution, for di   erent values of   

(eq. (13)). d: amount of triple interactions as a function of the parameter   .

consider three binary variables x1, x2, x3 taking values   1 with joint id203 distri-

bution

p(x1, x2, x3) =

1/4 if x1x2x3 =    1

0

if x1x2x3 = 1.

(11)

as illustrated in fig. 2b, left side. for this id203 distribution, the three univariate

               
            

9

marginals p1, p2, p3 are uniform, that is, pi(1) = pi(   1) = 1/2. moreover, the three bivariate
marginals p12, p23, p31 are also uniform: pij(1, 1) = pij(1,   1) = pij(   1, 1) = pij(   1,   1) =
1/4. the full distribution, however, is far from uniform, since only half of the 8 possible

states have non-vanishing id203.

the id203 distribution of eq. (11) is henceforth called a xor distribution. the

name is inspired by the fact that two independent binary variables x1 and x2 can be com-

bined into a third dependent variable x3 = x1 xor x2, where xor represents the logical

function exclusive-or. if the two input variables have equal probabilities for the two states
  1, then eq. (11) describes the joint id203 distribution of the triplet (x1, x2, x3).

the maximum-id178 id203 compatible with uniform bivariate marginals is uni-

form, p(2)(x1, x2, x3) = 1/8. the amount of triple interactions is therefore

d(3)
123 = h12,13,23     h123

= 3bits     2bits = 1 bit,

(12)

and d(3)

123 =    123,

i.e.

all interactions are tripletwise and d(3)

123 reaches the maximum

value allowed for binary variables. of course, the same amount of triple interactions is

obtained for the complementary id203 distribution (a so-called negative-xor), for

which p(x1, x2, x3) = 1/4 when qi xi = +1 (see fig. 2b, right side).

so far we have demonstrated that xor and    xor distributions contain the maximal
amount of triple interactions. amari [19] has proved the reciprocal result: if the amount
of triple interactions is maximal, then the distribution is either xor or    xor. we now
demonstrate that if the joint distribution lies somewhere in between a uniform distribution
and a xor (or a    xor) distribution, then the amount of triple interactions lies somewhere
in between 0 and 1, and the correspondence is monotonic. to this end, we consider a family of

joint id203 distributions parametrized by a constant   , de   ned as a linear combination
of a uniform distribution pu(x1, x2, x3) = 1/8 and a   xor distribution,

p  (x1, x2, x3) =

1
8

(1 + x1x2x3 tanh   ) ,

(13)

where        (      , +   ). varying    from zero to     shifts the p(x1, x2, x3) from the uniform
distribution pu to the xor id203 of eq. (11) (see fig. 2c). negative    values, in turn,
shift the distribution to    xor. all the bivariate marginals of the distribution p  (xi, xj) are

10

uniform, and equal to 1/4. the maximum-id178 model compatible with these marginals

is the uniform distribution pu(x1, x2, x3) = 1/8. hence, the amount of triple interactions is

d(3)

123(  ) =

1
2

[(1 + tanh   ) log(1 + tanh   ) + (1     tanh   ) log(1     tanh   )] .

(14)

as shown in fig. 2d, this function is even, and varies monotonically in each of the intervals
(      , 0) and (0, +   ). therefore, there is a one to one correspondence between the similarity
between the   xor distribution and the amount of triple interactions. the same result is
obtained for arbitrary binary distributions, as argued in the last paragraph of appendix b.
as a consequence, we conclude that for binary variables, the   xor distribution is not just
one possible example distribution with triple interactions, but rather, it is the only way in

which three binary variables interact in a tripletwise manner. if bivariate marginals are kept

   xed, and triple interactions are varied, then the joint id203 distribution either gains

or loses a xor-like component, as illustrated in fig. 2c.

iii. triplet analysis of pairwise interactions

in a triplet of variables x1, x2, x3, three possible binary interactions can exist, quanti   ed

by i(x1; x2), i(x2; x3) and i(x3; x1). in this section we characterize the amount of over-

lap between these quantities, we bound their magnitude, and we learn how to distinguish

between reducible and irreducible interactions.

a. redundancy among the three mutual informations within a triplet

in the previous section, we saw that when there are only two variables x1 and x2,

d(2)

12 coincides with the mutual information i(x1; x2). when there are more than two
variables, d(2) can no longer be equated to a mutual information, since there are several

mutual informations in play, one way per pair of variables: i(x1; x2), i(x2; x3), etc. in this

section, we derive a relation between all these quantities for the case of three interacting

variables. the multi-information of eq. (5) decomposes into pairwise and triple interactions,

   123 = d(2)

123 + d(3)
123,

11

(15)

from where we arrive at

d(2)
123 =    123     d(3)

123

= i12 + i13 + i23     i123     d(3)
123.

(16)

the total amount of pairwise dependencies, hence, is in general di   erent from the sum of
the three mutual informations. that is, depending on the sign of d(3)
of pairwise interactions d(2)

123 can be larger or smaller than p iij. this range of possibilities

123 may be a useful measure of the amount of redundancy or synergy

123 + i123, the amount

suggests that p iij     d(2)

within the pairwise interactions inside the triplet, and this is the measure that we adopt in

the present paper.

this measure coincides with the co-information when there are no triple dependencies,

that is, when d(3)

123 = 0. in this case,

i123 = i12 + i13 + i23     d(2)
123.

(17)

under these circumstances, a positive value of i123 implies that the sum of the three mutual

informations is larger than the total amount of pairwise interactions. the content of the

three informations, hence, must somehow overlap. this observation supports the idea that

a positive co-information is associated with redundancy among the variables.

in turn, a

negative value of i123 implies that although the maximum id178 distribution compatible
with the pairwise marginals is not equal to p1p2p3 (that is, although d(2)
123 > 0), when taken
two at a time, variables do look independent (that is pij     pipj). the statistical dependency
between the variables of any pair, hence, only becomes evident when    xing the third variable.

this behavior supports the idea that a negative co-information is associated with synergy

among the variables.

of course when d(3)

123 > 0, the co-information is no longer so simply related to concepts of

synergy and redundancy, not at least, if the latter are understood as the di   erence between
the sum of the three informations and d(2)

123. however, below we show that in actual data,

one can often    nd a close connection between the amount of triple interactions and the

co-information.

12

b. triangular binary interactions

in a group of interacting variables, if x1 has some degree of statistical dependence with

x2, and x2 has some statistical dependence with x3, one could expect x1 and x3 to show
some kind of statistical interaction, only due to the chained dependencies x1     x2     x3,
even in the absence of a direct connection. here we demonstrate that indeed, two strong

chained interactions necessarily imply the presence of a third connection closing the triangle.

in the pictorial representation of the middle column of fig. 1, this means that if only two

connections exist (there is no link closing the triangle), then the two present interactions

cannot be strong. for example, with binary variables, it is not possible to have i12 = i23 = 1

bit, and i31 = 0. the general inequality reads (see the derivation in appendix a)

i12 + i31     h1     i23.

(18)

c.

identi   cation of pairwise interactions that are mediated through a third vari-

able

in the previous section we demonstrated that the chained dependencies x1     x2     x3
can induce some statistical dependency between x1 and x3. on the other hand, it is also

possible for x1 and x3 to interact directly, inheriting their interdependence from no other

variable. these two possible scenarios cannot be disambiguated by just measuring the mu-

tual information between pairs of variables. in appendix c, we explain how, starting from

the most general model (illustrated in the lower-right panel of fig. 1), the analysis of triple

interactions allows us to identify those links that can be explained from binary interac-

tions involving other variables, and those that cannot: the so-called irreducible interactions.

brie   y stated, we need to evaluate whether the interaction between x1 and x2 (captured

by the bivariate marginal p12) and the interaction between x2 and x3 (captured by p23)

su   ce to explain all pairwise interactions within the triplet, including also the interaction

between x1 and x3. to that end, we compute a measure of the discrepancy between the

two corresponding maximum id178 models,

   12
13,23 = d[p12,13,23 : p13,32] = h13,23     h12,13,23.

(19)

13

the amount of irreducible interaction, that is, the amount of binary interaction between x1
and x3 that remains unexplained through the chain x1     x2     x3 is de   ned as

   13 = min(cid:8)i12,    12

13,23(cid:9) .

(20)

in sect. v d, we search for pairs of variables with small irreducible interaction, by computing

   13 using all possible candidate variables x2 that may act as mediators. from them, we

keep the one giving minimal irreducible interaction, that is, the one for which the chain
x1     x2     x3 provides the best explanation for the interaction between x1 and x3.

iv. marginalization and hidden variables

imagine we have a system of n variables that are linked through just pairwise interactions.

in such a system, for any pair of variables xi, xj there is a third variable xk producing

a vanishing irreducible interaction    ij = 0. by selecting a subset of k variables, we may
calculate the k-th order marginal pk, by marginalizing over the remaining n   k variables. as
opposed to the original multivariate distribution pn , the marginal pk may well contain triple

and higher-order interactions. in other words, there may be pairs of variables xi, xj that

belong to the subset for which there is no other third variable xk in the subset producing

a vanishing irreducible interaction    ij = 0. the high-order interactions in the subset,

therefore, result from the fact that not all interacting variables are included in the analysis.

therefore, triple and higher-order statistical dependencies do not necessarily arise due to

irreducible triple and higher-order interactions: just pairwise interactions may su   ce to

induce them, whenever we marginalize over one or more of the interacting variables. an

example of this e   ect is derived in appendix d. in the same way, marginalization may

introduce spurious pairwise interactions between variables that do not interact directly, as

illustrated in fig. 3. therefore, even if, by construction, we happen to know that the system

under study can only contain pairwise statistical dependencies, it may be important to

compute triple and higher-order interactions, whenever one or a few of the relevant variables

are not measured.

virtually all scienti   c studies focus their analysis in only a subset of all the variables that

truly interact in the real system. however, as stated above, neglecting some of the variables

typically induces high-order correlations among the remaining variables. if such correlations

14

a

b

fig. 3. examples illustrating the e   ects of marginalization in a pair of variables (a) or a triplet (b). in each

case, the variable represented in black drives the other slave variables, which do not interact directly with

each other (top). however, after marginalizing over the driving variable, a statistical dependence between

the remaining variables appears. the new interaction can be pairwise (a), or pairwise and tripletwise (b).

are interpreted within the reduced framework of the variables under study, they are spurious,

at least, in the sense that there may well be no mechanistic interaction among the selected

variables that gives rise to such high-order interactions. however, if interpreted in a broader

sense (i.e., a mathematical fact, that may result as a consequence of marginalization), high-

order correlations may be viewed as a footprint of the marginalized variables, which are

often inaccessible. as such, they constitute an opportunity to characterize those parts of

the system that cannot be described by the values of the recorded variables.

below we analyze the statistics of written language. we select a group of words (each

selected word de   nes one variable), and we measure the presence or absence of each of

these words in di   erent parts of the book. for simplicity, not all the words in the book

are included in the analysis, so the discarded words constitute examples of marginalized

variables. however, marginalized variables are not always as concrete as non-analyzed words.

other non-registered factors may also in   uence the presence or absence of speci   c words,

for example, those related to the thematic topic or the style that the author intended for

each part of the book. these aspects are latent variables that we do not have access to

by simply counting words. an analysis of the high-order statistics among the subgroup

of selected words may therefore be useful to characterize such latent variables, which are

otherwise inaccessible through automated text analysis.

as an ansatz, we can imagine that each topic a   ects the statistics of a subgroup of all

15

the words. the fact that topics are not included in the analysis is equivalent to having

marginalized over topics. by doing so, we create interactions within the di   erent subgroups

of words. if the topics do not overlap too much, from the network of the resulting interac-

tions, we may be able to identify communities of words highly connected, that are related

to certain topics. variations in the topic can therefore be diagnosed from variations in the

high-order statistics.

v. occurrence of words in a book

before analyzing a book, all its words are taken in lowercase, and spaces and punctuation

marks are neglected. each word is replaced by its base unin   ected form using the worddata
function from the program mathematica r(cid:13)[30]. in this way, for instance, a word and its
plural are considered as the same, and verb conjugations are uni   ed as well.

in order to construct the network of interactions between words, we analyze the probabil-

ity that di   erent words appear near to each other. the notion of neighborhood is introduced

by segmenting each book into parts. a book containing m words is divided into p parts, so

that there are m/p words per part. we analyze the statistics of a subgroup of k selected

words w1, . . . , wk, and de   ne the variables

xi =

               
            

1

if the word wi appears in a part

(21)

   1 otherwise.

the di   erent parts of the book constitute the di   erent samples of the joint id203

p(x1, x2, . . . , xk), or of the corresponding marginals. notice that if word wi is found in a

given part of the book, in that sample xi = 1, no matter whether the word appeared one
or many times. the marginal id203 p(xi) = (hxii + 1)/2 is the average frequency with
which word wi appears in one (any) of the parts. here, we analyze up to triple dependencies,

so we work with joint distributions of at most three variables p(xi, xj, xk).

in the present work, we choose to study words that have an intermediate range of fre-

quencies. we disregard the most frequent words (which are generally stop words such as

articles, pronouns and so on) because they predominantly play a grammatical role, and

only to a lesser extent they in   uence the semantic context [31]. we also discard the very

infrequent words (those appearing only a few times in the whole book), because their rarity

16

induces statistical inaccuracies due to limited sampling [32]. discarding words implies that

only a seemingly small number of words are analyzed, allowing us to illustrate the fact that

even a small number of variables su   ces to infer important aspects of the structure of the

network of statistical dependencies among words. in other types of data, the limitation in

the number of variables may arise from unavoidable technical constraints, and not from a

matter of choice.

we analyzed two books, on the origin of species (os) by charles darwin and the

analysis of mind (am) by bertrand russell, both taken from project gutenberg website

[33]. each book was divided into p = 512 parts. in os, each part contained 295 words, and

in am, 175. parts should be big enough so that we can still see the structure of semantic

interactions, and yet, the number of parts should not be too small as to induce inaccuracies

due to limited sampling.

in both books, we analyzed k = 400 words with intermediate frequencies. for os,
the analyzed words appeared a total number of times ni, with 33     ni     112. for am,
we analyzed words with 21     ni     136. since for these words the number of samples
(parts) is much greater than the number of states (2), entropies were calculated with the

maximum likelihood estimator. we are able to detect di   erences in id178 of 0.01 bits,

with a signi   cance of    = 0.1% (see appendix e for a analysis of signi   cance). a bayesian

analysis of the estimation error due to    nite sampling was also included, allowing us to

bound errors between 0.005 bits and 0.01 bits, depending on the size of the interaction (see

appendix f).

a. statistics of single words

before studying interactions between two or more words, we characterize the statistical

properties of single words. speci   cally, we analyze the frequency of individual words, and

their predictability of its presence in one (any) part of the book. within the framework of

id205, the natural measure of (un)predictability is id178.

using the notation pi = p(xi), the id178 hi is

hi =    (1     pi) log2(1     pi)     pi log2 pi.

(22)

this quantity is maximal (h = 1 bit) when pi = 1/2, that is, when the word wi appears

17

in half of the parts. when wi appears in either most of the parts or in almost none, hi

approaches zero. for all the analyzed words, 0 < pi < 1/2. in this range, the id178 h is

a monotonic function of pi.

the value of pi, however, is not univocally determined by the number ni of times that

the word wi appears in the book. if wi appears at most once per part, then pi = ni/p . if

wi tends to appear several times per part, then pi < ni/p .

in addition, one can determine whether the fraction of parts containing the word is in

accordance with the expected fraction given the total number of times ni the word appears

in the whole book. if ni is half the number of parts (that is, ni = p/2), then pi = 1/2 implies

that the ni words are distributed as uniformly as they possibly can: half of the parts do not

contain the word, and the other half contain it just once. if, instead, ni = 100p , a value of

pi = 1/2 corresponds to a highly non-uniform distribution: the word is absent from half of

the parts, but it appears many times in the remaining half.

to formalize these ideas, we compared the id178 of each selected word with the id178

that would be expected for a word with the same id203 per part 1/p , but randomly

distributed throughout the book and sampled ni times. the binomial id203 of    nding

the word k times in one (any) part is

  pi(k) =

ni!

k!(ni     k)! (cid:18) 1

p (cid:19)k (cid:18)1    

1

p(cid:19)ni   k

.

(23)

equation (23) describes an integer variable. in order to compare with eq. (22), we de   ne yi

as the binary variable measuring the presence/absence of word wi in one (any) part, assuming

that the word is binomially distributed. that is, yi = 0 if k = 0, and yi = 1 if k > 0. the
marginal id203 of p(yi = 1) is   p(k > 0) = 1   (1   1/p )ni. this formula is also obtained
in this case   pi(k) follows a hypergeometric
when all the words in the book are shu   ed.
m    j )    = (1     1/p )ni, where

j=0 (1     m/p

m/p (cid:1)/(cid:0) m

m/p(cid:1) = qni   1

distribution, such that   pi(k = 0) = (cid:0)m    ni
the last equality holds when m     ni.

hence, the id178 of the binary variable associated with the binomial (or the shu   ed)

model is

h binomial

i

(yi) =    (1   1/p )ni log2((1   1/p )ni)   (1   (1   1/p )ni) log2(1   (1   1/p )ni). (24)

the id178 of the variable xi measured from each book is compared with the id178 of

the binomial-derived variable yi in fig. 4.

18

]
s
t
i
b
[
 
y
p
o
r
t
n
e

0.8

0.6

0.4

0.2

0

0.8

0.6

0.4

0.2

0

a

hi
hi

binomial

50

b

hi
hi

binomial

50

100

ni

ni

100

fig. 4. id178 of the 400 selected words in each book (one data point per word), compared to the

expected id178 for a binomial variable with the same total count ni (continuous line), as a function of the

total count. entropies are calculated with the maximum likelihood estimator. the analytical expression of

eq. (24) is represented with the black line, and the gray area corresponds to the percentiles 1%-99% of the

dispersion expected in the binomial model, when using a sample of ni words. data points outside the gray

area, hence, are highly unlikely under the binomial hypothesis, even when allowing for inaccuracies due to

limited sampling. a: os. b: am.

even if the process were truly binomial, the estimation of the id178 may still    uctuate,

due to limited sampling. in fig. 4, the gray region represents the area expected for 98% of

the samples under the binomial hypothesis. we expect 1% of the words to fall above this

region, and another 1%, below. however, in os, out of 400 words, none of them appears

above, and 15% appear below. in am, the percentages are 0% and 16.5%. in both cases,

the outliers with small id178 are 15 times more numerous than predicted by the binomial

model, and no outliers with high id178 were found, although 4 were expected for each book.

in both books, hence, individual word entropies were signi   cantly smaller than predicted by

the binomial approximation, implying that they are not distributed randomly: in any given

part, each word tends to appear many times, or not at all.

a list of the words with highest di   erence (h binomial

   hi) is shown in table i. interestingly,
most of these words are nouns, with the    rst exception appearing in place 10 (the adjective

i

19

   rudimentary   ) for os. as reported previously [31], words with relevant semantic content

are the ones that tend to be most unevenly distributed.

b. statistics of pairs of words

in principle, there are two possible scenarios in which the mutual information between

two variables can be high: (a) in each part of the book the two words either appear together

or are both absent, and (b) the presence of one of the words in a given part excludes the

presence of the other. in table ii we list the pairs of words with highest mutual information.

in all these cases, the two words in the pair tend to be either simultaneously present or

simultaneously absent (option (a) above).

the words listed in table ii are semantically related. in both books, there are examples of

words that participate in two pairs: cell is connected to both bee and wax (os) and mnemic

is connected to both phenomena and causation (am). these examples keep appearing if the

lists of table ii are extended further down. their structure corresponds to the double links

in the second and third columns of figs. 1b and 1d. as explained in sect. iii b, two strong

binary links imply that the third link closing the triangle should also be present. indeed,

table i. words with highest di   erence in id178    hi = h binomial

i

    hi, expressed in bits. left:

os. right: am.

word (os)

bee

cell

slave

stripe

pollen

sterility

pigeon

fertility

nest

rudimentary

word (am)

proposition

appearance

box

datum

animal

objective

star

content

emotion

consciousness

   h i

0.369

0.365

0.302

0.295

0.275

0.266

0.252

0.248

0.242

0.234

20

   hi

0.335

0.315

0.299

0.258

0.240

0.215

0.211

0.206

0.205

0.204

in os, america is linked to both south and north (rows 2 and 4 of table ii). the words

south and north are also linked to each other, but they only appear in position 32, with a

mutual information that is approximately 1/3 of the two principal links. a similar situation

is seen with bee and wax, both connected to cell, although the direct connection between

bee and wax appears sooner, in position 16. the same happens in am with phenomena and

causation, linked through mnemic, which are connected to each other in the 39th place of

the list. these examples pose the question whether the weakest link in the triangle could be

entirely explained as a consequence of the two stronger links. a triplet analysis of pairwise

interactions allows us to assess whether such is indeed the case (see sect. iii c).

we    nish the pairwise analysis with a graphical representation of the words that are

most strongly linked with pairwise connections (left panels of fig. 5). words belonging to

a common topic are displayed in di   erent grey levels (di   erent colors, online), and tend to

form clusters.

in each cluster (insets in fig. 5), triplets of words often form triangles of

pairwise interactions. in the central plot, and in the top graph of each inset, the width of

each link is proportional to the mutual information between the two connected words.

table ii. pairs of words with highest mutual information. left: os. right: am. the values are

in bits.

wi (os)

wj (os)

iij

hi

hj

wi (am)

wj (am)

iij

hi

hj

male

south

female

0.242 0.504 0.409

1

2

0.191 0.330 0.337

america

0.210 0.480 0.560

truth

falsehood

0.110 0.429 0.191

reproductive

system

0.152 0.290 0.474

response

accuracy

0.107 0.306 0.264

north

america

0.133 0.429 0.560 depend

upon

0.107 0.229 0.616

cell

bee

wax

cell

0.122 0.201 0.150 mnemic

phenomena 0.095 0.423 0.516

0.120 0.330 0.201 mnemic

causation

0.090 0.423 0.381

fertile

sterile

0.120 0.345 0.330

consciousness

conscious

0.089 0.504 0.352

deposit

bed

0.109 0.322 0.314 door

window

0.086 0.160 0.128

fertility

sterility

0.109 0.352 0.322

stimulus

response

0.085 0.474 0.306

southern

northern 0.107 0.306 0.264 pain

pleasure

0.079 0.171 0.181

21

sex

reproductive

system

male

female

sexual

sterile

fertile

sterility

fertility

system

sex

reproductive

male

female

sexual

sterile

fertile

sterility

fertility

wax

northern

southern

young

egg

lay

cell

bee

europe

south

her

wax

north

america

northern

southern

young

egg

lay

cell

bee

her

europe

south

north

america

fig. 5. central graph: network of pairwise interactions in os. width of links proportional to the mutual

information between the two connected words. insets: detail of selected subnetworks. top graph:

links

proportional to mutual information. bottom graph: links proportional to irreducible interaction.

c. statistics of triplets

in order to determine whether triple interactions provide a relevant contribution to the

overall dependencies between words, we compare d(3)
interactions within the triplet, d(2)
ijk.

ijk with the total amount of pairwise

22

fig. 6. fraction of the total interaction within a triplet    ijk that corresponds to tripletwise dependencies,

d(3)

ijk/   ijk, as a function of the total interaction. the grey level of each data point is proportional to the

(logarithm of the) number of triplets at that location (scale bars on the right).    ijk values above 0.01 bits

are signi   cant (see appendix). a: os. b: am. dashed line: averages over all triplets with the same    ijk.

figure 6 shows the fraction of the total interaction that corresponds to triple dependen-

cies, d(3)

ijk/   ijk, as a function of the total interaction    ijk. the data extends further to the
right, but the triplets with    ijk > 0.05 bits are less than 0.4%. the    rst thing to notice is

that the values of the total interaction (values in the horizontal axis) are approximately an

order of magnitude smaller than the entropies of individual words (see fig.4). individual

entropies range between 0.1 and 0.9 bits, and interactions are around 0 and 0.05. in order

to get an intuition of the meaning of such a di   erence, we notice that if we want to know

whether words wi, wj and wk appear in a given part, the number of binary questions that

we need to ask is (depending on the three chosen words) between 0.3 and 2.7 if we assume

the words are independent (hi + hj + hk), and between 0.25 and 2.2, if we make use of
their mutual dependencies (hi + hj + hk        (3)
123). although sparing     10% of the questions
may seem a meager gain, it can certainly make a di   erence when processing large amounts

of data.

the second thing to notice, is that triple interactions are by no means small as compared

to the total interactions within the triplet, since there are triplets with d(3)

ijk/   ijk of order

23

unity.
in other words, triple interactions are not negligible, when compared to pairwise
interactions. in the triplets with d(3)
ijk/   ijk     1, the departure from the independent as-
sumption resembles the xor behavior (or    xor), in the sense that the states (x1, x2, x3)
for which qi xi = 1 have a lower (higher) id203 than the states with qi xi =    1. the

   rst case corresponds to triplets where all pairs of words tend to appear together, but the

three of them are rarely seen together. in the second case, the words tend to appear either

the three together or each one on its own, but they are rarely seen in pairs.

table iii shows the words with largest triple information. these interactions are well

above the signi   cance threshold of 0.01 bits. the triplet (america, south, north) is similar

to a xor gate, so these words tend to appear in pairs but not all three together. in certain

contexts the author uses the combination south america, in other contexts, north america,

and yet in others, he discusses topics that require both south and north but no america.

most of the triplets in table iii have triple information values that are equal in magnitude
ijk        iijk. besides, for these triplets,
ijk/   123     1. to determine whether such

to the co-information but with opposite sign, that is, d(3)
most of the interaction is tripletwise, that is, d(3)

table iii. words with highest triple information d(3)

ijk. the    rst column displays a tag that allows

us to identify each triplet in fig. 7. the last column indicates whether the triplet behaves as a

xor gate (+1) or a    xor (   1). top: os. bottom: am. values in bits.
tag

k

j

i

iijk

d(3)
ijk

d(3)/   

xor

  

  

  

  

  

  

  

  

  

  

america

south

north

inherit

occasional

appearance

action

wide

europe

perhaps

branch

chapter

climate

expect

just

speak

causation

appropriate

sense

since

wish

perception

natural

actual

me

wholly

connection

consist

should

life

0.065

0.040

0.036

0.036

0.035

0.041

0.033

0.033

0.033

0.033

0.005

   0.040
   0.036
   0.036
   0.035
   0.041
   0.033
   0.033
   0.033
   0.033

0.16

0.96

0.93

0.90

0.97

0.93

0.90

0.90

0.95

0.92

+1

   1
   1
   1
   1
   1
   1
   1
   1
   1

24

a

a

b

g

d
e

0.06

0.04

0.02

k

h

q

z

0

b

0.04

0.02

0

-0.025

0

0.025

0.05

0.075

a b

g

de

l

h

k

q

z

-0.025

0

0.025

0.05

iikj (bits)

106

104

102

100

106

104

102

100

fig. 7. triple information d3

ijk as a function of the co-information iijk for all triplets. the grey level of

each data point is proportional to the (logarithm of the) number of triplets at that location (scale bars on

the right).    ijk values above 0.01 bits are signi   cant (see appendix). a: os. b: am.

tendency is preserved throughout the population, in fig. 7 we plot the triple information
d(3)
triplets are located along the diagonal d(3)

ijk as a function of the co-information iijk for all triplets. we see that the vast majority of
ijk        iijk. in order to understand why this is so,
we analyze how data points are distributed when picking a triplet of words randomly. the

cases a, b, c and d of fig. 1 are ordered in decreasing id203. that is, picking three

unrelated words (fig. 1a) has higher id203 that picking a triplet with only pairwise

interactions (b), which is still more likely than picking a case with only triple interactions

(c), leaving the case of double and triple interactions (d) as the least probable. all cases
with no triple interaction (a and b) fall on the horizontal axis d(3)

ijk = 0 in fig. 7. therefore,
in order to understand why points outside the horizontal axis cluster along the diagonal we

must analyze the triplets that do have a triple interaction (panels c and d in fig. 1). we

begin with case c, because it has a higher id203 than case d. this case corresponds to
d(3)
ijk > 0 and iij = ijk = iki     0. it is easy to see that in these circumstances, p2     pipjpk,
and hence, d(3)
ijk        iijk. we continue with the left column of case d, since having a single
pairwise interaction has higher id203 than having more. this case corresponds to
d(3)
ijk > 0, iij = ijk     0 and iki > 0, for some ordering of the indexes i, j, k.

in these

25

circumstances, p2     pijpikpjk/pipjpk, which again implies that d(3)
ijk        iijk. therefore, all
triplets containing some triple interaction and at most a single pairwise interaction fall along
the diagonal in fig. 7. the only outliers are triplets with d(3)

ijk > 0 and at least two links
with pairwise interactions, which, as derived in sect. iii b, most likely contain also the third

pairwise link. such highly connected triplets are typically few.

from eq. (16) we see that the triplets that are near the diagonal are neither synergistic nor

redundant, that is, iij + ijk + iki     d(2)
pairwise information ( iij + ijk + iki > d(2)
analyzed books, very few (    10) triplets satisfy p iij     d(2) <    0.01 bits. contrastingly,
    300 triplets have signi   cant redundant pairwise information (p iij     d(2) > 0.01 bits).

ijk. those located above the diagonal have redundant
ijk), whereas those below are synergistic. in the two

the triplets located far from the diagonal correspond, in both cases, to those with a large

total dependency (    & 0.1 bits). table iv displays the words with highest redundant
pairwise interaction, that is, iij + ijk + iki     d(2)
ijk. with the exception of data point   
(america, south, north), the triplets that have highest redundancy tend to be in the lower

right part of fig. 7, whereas the ones with highest triple interaction lie in the upper left

table iv. triplets with highest redundant pairwise information d(3)

ijk +iijk = iij +ijk +iki   d(2)
ijk.

the    rst column displays a tag that allows us to identify each triplet in fig. 7. top: os. bottom:

am. values in bits.

tag

  

  

  

  

  

  

  

  

  

  

i

bee

america

glacial

mountain

male

leave

stimulus

mnemic

truth

place

j

cell

south

southern

glacial

female

door

response

phenomena

false

2

26

k

wax

north

northern

northern

sexual

window

accuracy

causation

falsehood

1

d(3)

ijk + iijk

0.089

0.070

0.065

0.062

0.057

0.061

0.039

0.038

0.036

0.027

corner.

d.

identi   cation of irreducible binary interactions

using the tools of sect.iii c, here we identify the pairs of words that interact only because

the two of them have strong binary interactions with a third word. in the    rst place, the

pairs of words whose mutual information is larger than the signi   cance level (0.01 bits) are

selected. for those pairs, the irreducible interaction is calculated by considering all other

candidate intermediary words, and selecting the one that minimizes eq. (20). we observe

that many pairs have a low irreducible interaction, implying that their dependency can be

understood by a path that goes through a third variable xk, such as

p(xi, xj)     xxk

p(xi, xk)p(xk, xj)

p(xk)

.

(25)

in these situations, the behavior of the pair {xi, xj} can be predicted from the dependency
between {xi, xk} and the dependency between {xk, xj}.

in table v, we list the pairs (i, j) of words that have smallest irreducible interaction,

including the third word (k) that acts as a mediator. in these triplets, most of the interaction

between words wi and wj is explained in terms of wk. mediators tend to have a high semantic

content, and to provide a context in which the other two words interact. besides, the triplets

(i, j, k) in table v tend to cluster in the lower right corner of fig. 7, implying that pairs of

words share redundant mutual information.

the number of pairs with signi   cant mutual information (i.e., iij > 0.01 bits), and whose
interaction is explained at least in a 90% through a third word (i.e.,    ij/iij < 0.1) is higher

in the book os (108) than in book am (19). out of the 108 pairs of os, 16 are explained

through the word cell, 12 through america, 8 through northern, 6 through glacial, 6 through

sterility and so on. the fact that speci   c words tend to mediate the interaction between

many pairs suggests that they may act as hubs in the network.

in the right panels of fig. 5, we see the network of irreducible interactions. when com-

pared with the network of mutual informations (left panels), the irreducible network contains

weaker bonds, as expected, since by de   nition,    ij cannot be larger than iij. in the    gure,

we can identify some of the pairs of table v, whose interaction is mediated by a third word.

such pairs appear with a signi   cantly weaker bond in the right panel, as for example, bee-

27

wax (mediator = cell, os), and stimulus-accuracy, (mediator = response, am). moreover,

one can also identify the pairs whose interaction is intrinsic (that is, not mediated by a third

word) as those where the link on the right has approximately the same width as on the left.

notable examples are male-female (os), and depend -upon.

vi. conclusions

in this paper, we developed the information-theoretical tools to study triple dependencies

between variables, and applied them to the analysis of written texts. previous studies had

proposed two di   erent measures to quantify the amount of triple dependencies: the co-

information iijk and the total amount of triple interactions d(3). given that there is a

certain controversy regarding which of these measures should be used, it is important to

notice that iijk is a function of three speci   c variables x1, x2, x3, whereas d(3) is a global
measure of all triple interactions within a wider set of n variables, with n     3. therefore,
it only makes sense to compare the two measures when d(3) is calculated for the same group

of variables as iijk, which implies using n = 3.

the two measures have di   erent meanings. whereas the co-information quanti   es the

table v. pairs of words with lowest irreducible interaction. the    rst column displays a tag that

allows us to identify each triplet in fig. 7. top: os. bottom: am. values in bits.

i

bee

south

continent

lay

southern

phenomena

stimulus

place

proposition

proposition

  

  

  

  

  

  

  

  

  

  

iij

0.093

0.071

0.032

0.032

0.031

0.042

0.039

0.028

0.024

0.022

j

wax

north

south

wax

arctic

causation

accuracy

2

falsehood

door

28

   ij

0.003

0.001

0.001

0.000

0.001

0.004

0.000

0.000

0.002

0.000

kmed

cell

america

america

cell

northern

mnemic

response

1

truth

window

e   ect of one (any) variable in the information transmission between the other two, the

amount of triple interactions measures the increase in id178 that results from approxi-

mating the true distribution pijk by the maximum-id178 distribution that only contains

up to pairwise interactions. when studied with all generality, these two quantities need not

be related, that is, by    xing one of them, one cannot predict the value of the other. when

restricting the analysis to binary variables, however, a link between them arises. three

binary variables are characterized by a id203 distribution over 23 possible states. due

to the id172 restriction, the distribution is determined once the id203 of 7

states are    xed. choosing those 7 numbers is equivalent to choosing the three entropies

hi, hj, hk, the three mutual informations iij, ijk, iki, and one more parameter. this extra
parameter can be either the co-information iijk (in which case the triple interaction d(3) is
   xed), or the triple interaction d(3) (in which case the co-information iijk is    xed). hence,

although in general the co-information and the amount of triple interactions are not related

to one another, for binary variables, once the single entropies and the pairwise interactions

are determined, iijk and d(3) become linked. in this particular situation, hence, there is no

controversy between the two quantities, because they both provide the same information,

only with di   erent scales.

moreover, we have shown that when pooling together all the triplets in the system, and

now without    xating the value of individual entropies or pairwise interactions, iijk and d(3)

often add up to zero. this e   ect results from the fact that most triplets contain at most a

single pairwise interaction. hence, for most of the triplets the two measures provide roughly

the same information. the exception involves the triplets containing at least two binary

interactions, which are likely to contain all three interactions, in view of sect. iii b.

one could repeat the whole analysis presented here, but with xi = number of times the

word appeared in a given part (instead of the binary variable appeared / not appeared ).

this choice would transform the binary approach into an integer description, which could

potentially be more accurate, if enough data are available.

it should be borne in mind,

however, that the size of the space grows with the cube of the number of states, so serious

undersampling problems are likely to appear in most real applications. we choose here the

binary description to ensure good statistics. in addition, this choice allowed us to (a) relate
triple interactions with the   xor gate, and (b) related the co-information with the amount
of triple interactions.

29

in the present work we studied interactions between words in written language through

a triple analysis. this approach allowed to accomplish two goals. first, we detected pure

triple dependencies that would not be detectable by studying pairs of variables. second, we

determined whether pairwise interactions can be explained through a third word.

we found that on average, 11% and 13% of the total interaction within a group of

three words is pure tripletwise. on average, triple dependencies are weaker than pairwise

interactions. however, in 7% and 9% of the total number of triplets, triple interactions are

larger than pairwise. although this is a small fraction of all the triplets, all the 400 selected

words participate in at least one such triplet. hence, if word interactions are to be used to

improve the performance in a cloze test, triple interactions are by no means negligible.

we believe that in particular for written language the presence of triple interactions is

mainly due the marginalization over the latent topics. for example, the triplet (america,

south, north) resembles a xor gate, so variables tend to appear two at a time, but not

alone, nor the three together. imagine we include an extra variable (this time, a non-binary

variable), specifying the geographic location of the phenomena described in each part of the

book. the new variable would take one value in those parts where darwin describes events

of north america, another value for south america, and yet other values in other parts of

the globe. if these topic-like variables are included in the analysis, the amount of high order

interactions between words is likely to diminish, because complex word interactions would

be mediated by pairwise interactions between words and topics. however, since topic-like

variables are not easily amenable to automatic analysis, here we have restricted the study

to word-like variables. we conclude that high-order interactions between words is likely to

be the footprint of having ignored (marginalized) over topic-like variables.

acknowledgments

we thank agencia nacional de promoci  on cient       ca y tecnol  ogica, comisi  on nacional

de energ    a at  omica and universidad nacional de cuyo for supporting the present research.

30

appendix a: mathematical proofs

a. derivation of the bound in eq. (10)

as imposing more restrictions cannot increase the id178, h12,23,31     h12,23. using the

fact that h12,23 = h12 + h23     h2 (see appendix b), it follows from eq. (7) that

d(3)
123     h12,23     h123

d(3)
123     i13|2.

(a1)

this inequality is tight, since a id203 distribution exists for which the equality is

ful   lled: when h12,23 = h12,23,31, that is, when p12,23,31(x1, x2, x3) = p12 p23/p2.

the derivation can be done removing any of the restrictions v     {12, 13, 23}. therefore,

d(3)
123     min{i12|3, i23|1, i13|2}

d(3)
123     min{i12, i13, i23}     i123,

where i123 is the co-information. from eq. (a2), it also follows that

d(3)
123     min{h1, h2, h3}.

b. derivation of eq. (18)

inserting the upper bound of eq. (a1) in eq. (16),

123 + d(3)

123

i12 + i23 + i31 = i123 + d(2)
    i123 + d(2)
= i23        

123 + i23|1
      i23|1 + d(2)

123 +    

      i23|1 .

therefore,

i12 + i31     d(2)
123.

(a2)

(a3)

(a4)

(a5)

in addition, since reducing the number of marginal restrictions cannot diminish the id178

of the maximum id178 distribution,

d(2)
123 =    h[p12,23,31] + h1 + h2 + h3

       h[p23] + h1 + h2 + h3
= i23 + h1.

31

(a6)

combining eqs. (a5) and (a6),

therefore, if i12 and i31 are large, i23 cannot be too small.

i12 + i31     h1     i23.

appendix b: maximum id178 solution

the problem of    nding the id203 distribution that maximizes the id178 under

linear constrains, such as    xing some of the marginals, has a unique solution [27]. although

no explicit closed form is known for the case where each variable varies in an arbitrary do-

main, there are procedures, for example the iterative proportional    tting [27], that converge

to the solution.

in some special cases a closed form exists. for example, when the univariate marginals

are    xed, the solution is the product of such marginals. another case is when we look for the

maximum id178 distribution of three variables   p(x1, x2, x3) that satis   es two constraints   

for example p(x1, x2) and p(x2, x3)   out of the three bivariate marginals. posing the maxi-

mization problem through lagrange multipliers, we obtain a solution of the form

  p(x1, x2, x3) = f1(x1, x2)f2(x2, x3).

if we enforce the marginal constrains and the id172, we get

  p(x1, x2, x3) =

p(x1, x2)p(x2, x3)

p(x2)

,

which is known as the pairwise approximation. the id178 of this distribution is

h[  p] = h12,23 = h12 + h23     h2.

(b1)

(b2)

(b3)

below we derive the solution p(2)(x1, x2, x3) in the special case of three binary variables
(xi =   1). this solution has maximum id178 and satis   es the three second order marginal
constrains, p(x1, x2), p(x1, x2) and p(x2, x3). in principle, eight variables need to be deter-

mined, one for the id203 of each state. however, considering the id172 condi-

tion, the constraints on the three univariate marginals, and on the three bivariate marginals,

we are left with only a single free variable. as shown in previous studies [18, 19], the problem

reduces to    nding the root of a cubic equation. since we are interested in comparing this

32

solution with the joint id203 p(x1, x2, x3), a convenient and conceptually enlightening
way of expressing the solution p(2)(x1, x2, x3), as in the work of martignon [18], is

p(2)(x1, x2, x3) = p(x1, x2, x3)       yi

xi,

(b4)

where the value of    is such that the probabilities remain in the simplex, that is, p(2)(x)    
[0, 1]. for the marginals, we get

p(2)(xi, xj) = p(2)(xi, xj, 1) + p(2)(xi, xj,   1)

= p(xi, xj, 1) + p(xi, xj,   1)        +   
= p(xi, xj).

the value of    is obtained from

yx/ qi xi=1

p(2)(x1, x2, x3) = yx/ qi xi=   1

p(2)(x1, x2, x3),

(b5)

(b6)

condition ensuring that the coe   cient accounting for the triple interaction in the log-linear

model vanishes [19]. eq. (b6) reduces to the previously mentioned cubic equation on   .

if the solution is    = 0, then the id203 p is the one with maximum id178. other-

wise, the id203 p departs from p(2), implying that, up to a certain degree, the multi-

variate distribution resembles either the xor gate, or its opposite.

we close this section by discussing the e   ect of varying the amount of triple interactions

while keeping all bivariate marginals    xed, as discussed in sect. ii a. there we proved that

when p(x1, x2, x3) took the shape of eq. (13), then the amount of triplet interactions was a
measure of the similarity between the joint distribution and a   xor distribution. here we
extend this result to arbitrary distributions. we have demonstrated here that p(x1, x2, x3)
can always be written as p(x1, x2, x3)     p(2)(x1, x2, x3) +   x1x2x3, where p(2)(x1, x2, x3)
is the maximum id178 model compatible with the bivariate marginals of the original

distribution, and    is a certain constant. amari showed that if    = 0, there are no triple

interactions. pushing his argument further, here we notice that if the bivariate marginals

are kept    xed, the only way of changing the amount of triple interactions is to vary the

value of   . the size of    determines the degree of similarity between p(x1, x2, x3) and a
  xor distribution. therefore, once the bivariate marginals are    xed, the only parameter
that can be manipulated in order to change the amount of triple interactions is the one that
quanti   es the size of the   xor component.
33

appendix c: irreducible interactions

following the ideas from [22, 34], we wish to detect whether the statistical dependencies
among a group of variables v = {x1, . . . , xk} contain all possible interactions, or whether
some of the interactions can be derived from others. all possible interactions are de   ned by

the power set of v , that is, the set whose elements are all the possible subsets of elements of

v . if some interactions can be explained in terms of others, then some groups of variables

in v are independent from other groups, and the set that de   nes all present interactions is

smaller than the power set. to identify the subsets of variables whose dependencies su   ce
to explain all interactions, we propose di   erent structured sets     = {u1, u2, . . . , u   }, where
each ui = {xi1, . . . , xik} is itself a set of variables that may or may not belong to v . each
set     is a candidate explanation of the statistical structure in v . within the maximum

id178 approach, for each proposed     we calculate

   v

    = d[p      v : p   ]

= h        h      v ,

(c1)

where we are using the notation described in the previous section, so that p    is the

maximum id178 distribution compatible with the marginals of the groups of variables

u1, u2, . . . , u    contained in    , and p      v is the maximum id178 distribution compatible
with the marginals of u1,         , u   , v . if    v
    is zero, then p      v = p   , and the joint probabil-
ity of the variables v can be derived from    . this means that the statistical dependencies

among the groups that compose     su   ce to explain the statistical structure among the

groups that compose v , even if the former contains interactions whose order is smaller than

the number of elements in v .

in the simplest example, we want to decide whether the statistical structure in the pair-

wise marginal p12 = p(x1, x2) may or may not be explained by the univariate marginals
in this case, v = {x1, x2} and     = {u1, u2}, with
p1 = p(x1) and p2 = p(x2).
u1 = {x1}, u2 = {x2}. when calculating the union         v , we notice that here the sign
    represents a union of marginals, not a union of sets. the bivariate marginal p12 contains
the univariate marginals p1 and p2, so         v = v . hence,

   12

1,2 = d[p12 : p1,2] = i(x1; x2).

(c2)

34

if    12

1,2 = 0, the entire statistical structure within v is accounted for by the two independent

variables x1 and x2.

in a more complex example, we may wish to determine whether the statistical dependen-

cies between the variables x1, x2 and x3 can be explained by just    rst and second order
interactions. we de   ne v = {x1, x2, x3} and     = {u1, u2, u3}, with u1 = {x1, x2},
u2 = {x2, x3}, u3 = {x3, x1}. the triple marginal p123 contains all pairwise marginals
p12, p23 and p31, so again,         v = v . therefore,

   123

12,13,23 = d[p123 : p12,13,23] = d(3)
123.

(c3)

if    123

12,13,23 = 0, pairwise interactions su   ce to explain all the statistical structure in v .

a less ambitious goal would be to determine whether the statistical dependence between
x1 and x2 is mediated by a third variable x3. we hence de   ne v = {x1, x2},     = {u1, u2},
and u1 = {x1, x3}, u2 = {x3, x2}. the union of marginals is now       v = {v, u1, u2} 6= v ,
so in this case,    12

13,23 is given by eq. (19).

the set     constitutes a candidate explanatory model for the statistical dependencies

within v . the aim is to    nd the simplest set     for which    v

    = 0. the search for such    ,
however, has to be done within the power set of the set that includes all the variables in the

system, so the number of candidate     sets grows exponentially with the number of variables.

since for a large system the search becomes computationally intractable, here we restrict

the analysis to the study of pairwise dependencies, that is, sets v with just two elements.

moreover, we search for explanatory models that attempt to reproduce all the statistical

structure in v by means of pairwise interactions with a third variable, as in eq. (19). a

similar approach, but within a di   erent theoretical framework, has been proved useful in

disambiguating couplings in oscillatory systems [35]. we de   ne the amount of irreducible

interaction between the variables xi and xj as the amount of statistical dependencies that

remain unexplained by the optimal minimal model, that is,

   ij = minn   ij

i,j, min

k {   ij

ik,kj}o

= minniij, min

k {   ij

ik,kj}o ,

= minniij, min

k {hik,kj     hij,jk,ki}o .

35

(c4)

the index k ranges through all the variables that do not coincide with i or j (k 6= i, k 6= j).
by de   ning    ij as a kullback-leiber divergence, its non-negativity is ensured. besides, the

minimization in eq. (c4) ensures that    ij is upper bounded by the mutual information,
that is,    ij     iij. expanding    ij

ik,kj,

   ij
ik,kj = hik + hkj     hk     hij,jk,ki

= hik + hjk     hk     hijk + hijk     hij,jk,ki

(c5)

= iij|k     d(3)
ijk.

therefore, if there are not triple interactions within the whole set of variables, then    ij

correspond to conditioning the mutual information between i and j with every other possible

variable k, and looking for the minimum. we can rewrite eq. (c4) as

   ij = iij       (cid:16)max

k niijk + d(3)

ijko(cid:17)

= iij       (cid:16)max

k niij + ijk + iki     d(2)

ijko(cid:17)

(c6)

where   (x) is the heaviside step function. in this sense, we are looking for a triplet that

has maximal redundancy, understanding redundancy as p i     d(2).

appendix d: example of marginalization e   ects

consider four binary variables xi =   1, which can be thought of as spins, with only
pairwise interactions between x4 and each of the other three variables. the fourth variable

is in the up state with id203 (1 + e   2  )   1. here we focus in negative    values, which

favor the down state. the joint id203 can be written as a log-linear model [17, 19]

log p(x1, x2, x3, x4) =   x4 + x1x4 + x2x4 + x3x4       

(d1)

where    < 0 is the    eld acting on x4, and    is the id172 constant. marginalizing

= (   + x1 + x2 + x3)x4       

over x4, we obtain

p(x1, x2, x3) =

cosh(   + x1 + x2 + x3)
    cosh(   + x   
2 + x   
3)

1 + x   

px

36

.

(d2)

with this id203 we are able to calculate the interactions    123, d(2)

123 and d(3)

123 as a

function of   .

0.06

0.04

0.02

0
-2

]
s
t
i
b
[
 
s
n
o
i
t
c
a
r
e
t
n
i

0.6

0.5

0.4

0.3

0.2

0.1

0

   

123
d(2)
d(3)

123

123

-1.8

  

-1.6

-3

-2

  

-1

0

fig. 8. interactions    123, d(2)

123 and d(3)

123 as a function of the    eld    acting on x4.

in fig. 8 we see the multi-information    123, the amount of pairwise interactions in the

123, and the triple information d(3)

triplet d(2)
stated above,    123 = d(2)

123 + d(3)

123 as a function of the    eld    acting on x4. as

123. all of these quantities are obtained from the marginal

probabilities p(x1, x2, x3) given by eq. (d2) (see appendix b). when the    eld is strong
(             ) the total amount of interaction vanishes, as all spins align in the down state.
for small values of the    eld, the amount of interactions is large, and can be explained almost

entirely by pairwise dependencies. however for intermediate values of the    eld (see inset of

figure 8), which corresponds to the fourth spin aligned downwards most of the time, the

triple information is crucial to understand the structure of dependencies within the group of

remaining variables. in this paper we argue that in the case of written language, the topics

or latent variables that a   ect the occurrence of words are likely to present the same kind of

behavior, that is, they tend to be inactive most of the time. and when they are active, they

tend to favor the occurrence of speci   c groups of words.

appendix e: signi   cance test

we want to assess whether a id203 distribution of three variables p(x) is explained

or not by the simpler maximum id178 model p(2)(x), obtained after measuring only the

pairwise marginal probabilities. that is, taking the maximum id178 model as the null

37

hypothesis h0, and considering as the alternative hypothesis h1 the one in which there is a

triple dependency, we want to calculate the plausibility of the distribution p(x). in statistics

a usual way of comparing two models, one of which is nested within the other, is a likelihood

ratio test.

if we take n samples, then the likelihood ratio    is given by

   =

p (x1, ..., xn|h1)
p (x1, ..., xn|h0)
= qn
qn

i=1 p(xi)
i=1 p(2)(xi)

.

considering n         and using sanov   s theorem [14], it follows

log(  ) = nd[p : p(2)].

in addition, the result by wilks [36] implies that, neglecting terms of order n    1/2,

2 log(  ) =   2
d,

(e1)

(e2)

(e3)

that is, the logarithm of the likelihood tends to a chi-square distribution, where the number

of degrees of freedom d equals the di   erence in the numbers of parameters between the

models. combining these two results, we conclude that under the null hypothesis,

d[p : p(2)] =

  2
1
2n

,

(e4)

where the chi-square distribution has one degree of freedom. taking a signi   cance of    =

0.1% and n = 512, we reject the null hypothesis if d[p : p(2)] & 0.01 bits.

an analogous analysis is done when evaluating the signi   cance of d[pij,ik,jk : pik,jk], with

the same result.

appendix f: error estimation

the estimation of the error of our measures is done by a bayesian approach [32]. es-

timation problems are dominated by    nite sampling in the probabilities of the di   erent

states.

on the one side, we have the true id203 q governing the outcome of the experiment,

whose coordinates refers to the s possible states of the system (in our case to the eight states

38

for three binary variables). on the other side, there is the frequency count f = ni/n, where

ni is the number of times the state i occurs, and n is the total number of measurements.

the id203 of measuring f given that the data are governed by q is the multinomial

id203

p(f|q) = n!yi

qni
i
ni!

= n!yi

qn fi
i
(nfi)!

.

(f1)

we have no access to q, we can only measure f. we therefore need the id203 that
the true distribution be q given that f was measured, that is, the id203 density p (q|f).
through bayes    rule,

p (q|f) =

p(f|q)p (q)

p(f)

=

exp (   nd[f : q]) p (q)

z

(f2)

where p (q) is the prior id203 distribution for q, and z is the id172 over the

domain of q. for the estimation of the error, and in the limit of a large number of samples,

the result does not depend on the choice of the prior, as we show below.

if we need to estimate some function of the probabilities w (q), the variance of the

estimate is

w = hw 2i     hwi2,
  2

(f3)

where the average is over p (q|f). in our case, we are interested in the triple information
w (q) = d[q : q(2)], where q(2) is the maximum id178 id203 compatible with the

second-order marginals.

from [32] it follows that, in the limit n     s and to a    rst order in 1/n,

fi(1     fi)

n

  2

w     xi

   qi (cid:19)2(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)f
(cid:18)   w
(cid:18)    w
   2xi xj<i

   qi

+ o(n    2)

(f4)

fifj
n

   w

   qj (cid:19)(cid:12)(cid:12)(cid:12)(cid:12)f

=    qw t             qw,

39

where the covariance matrix of the probabilities    is

  ij =

                  
               

fi(1     fi)

n

if i = j

fifj
n

   

if i 6= j

(f5)

due to    nite sampling, the frequencies fi may    uctuate. from eq. (f4) we see that we only

need the covariance matrix and the gradient of w (q) evaluated in f in order to transform

the variance of the vector f along di   erent directions of the simplex into variance in w . it
is important to notice that the error in w is of order 1/   n , which means that if we want
to reduce the error by half, we need to increase the number of samples fourfold.

in our case the gradient    qw is di   cult to calculate, but we can obtain the result from
eq. (f4) numerically. given the frequency f,    rst we calculate the eigenvalues and eigen-

vectors from the covariance matrix    given by eq. (f5). one non-degenerate eigenvector

is orthogonal to the simplex, and has a zero eigenvalue. the remaining eigenvectors vk

belong to the simplex and all have positive eigenvalues   2

k, equal to the variances in the
corresponding directions. finally, making a small change    in the frequencies along these
directions, we obtain the change    wk = w (f +   vk)     w (f), so that

  2
w = (   w )2    

1
  2

where every   2

k is in the order of 1/n.

s   1

   w 2

k   2
k,

xk=1

(f6)

fig. 9. standard deviation of the triple information d3

ijk as a function of the d3

ijk, for the triplets that

satisfy d3

ijk > 0.01. a: os. b: am. the dashed line indicates the identity.

figure 9 shows the standard deviation of d3
ijk for the triplets that satisfy d3

ijk obtained by this method as a function of
ijk > 0.01, for both books. the error lies between 0.005

d3

bits and 0.01 bits.

40

%

[1] w. taylor, journalism quarterly 30, 415 (1953).

[2] e. schneidman, m. j. berry, r. segev, and w. bialek, nature 440, 1007 (2006).

[3] g. k. zipf, human behavior and the principle of least e   ort (addison-wesley press, cam-

bridge, 1949).

[4] c. e. shannon, bell syst. tech. j. 30, 50 (1951).

[5] p. grassberger, ieee trans. inf. theory 35, 669 (1989).

[6] w. ebeling and t. p  oschel, europhys. lett. 26, 241 (1994).

[7] m. a. montemurro and d. h. zanette, adv. complex syst. 13, 135 (2010).

[8] a. m. petersen, j. n. tenenbaum, s. havlin, h. e. stanley, and m. perc, sci. rep. 2, 943

(2012).

[9] m. perc, j. r. soc. interface , 3323 (2012).

[10] m. gerlach and e. g. altmann, phys. rev. x 3, 021006 (2013).

[11] m. gerlach and e. g. altmann, new j. phys. 16, 113010 (2014).

[12] c. e. shannon, bell syst. tech. j 27, 623 (1948).

[13] e. t. jaynes, phys. rev. 106, 620 (1957).

[14] t. m. cover and j. a. thomas, elements of id205 (john wiley, new york,

2012).

[15] j. n. darroch, j. r. stat. soc. , 251 (1962).

[16] w. j. mcgill, psychometrika 19, 97 (1954).

[17] a. agresti, categorical data analysis (john wiley, new york, 2014).

[18] l. martignon, g. deco, k. laskey, m. diamond, w. freiwald, and e. vaadia, neural comput.

12, 2621 (2000).

[19] s. amari, ieee trans. inf. theory 47, 1701 (2001).

[20] a. j. bell, in proceedings of the fifth international workshop on independent component

analysis and blind signal separation: ica, vol. 2003 (citeseer, 2003).

[21] e. schneidman, w. bialek, and m. j. berry, j. neurosci. 23, 11539 (2003).

[22] i. nemenman, arxiv preprint q-bio/0406015 (2004).

[23] p. m. vit  anyi, ieee trans. inf. theory 57, 2451 (2011).

41

[24] v. gri   th and c. koch, in guided self-organization: inception (springer, 2014) pp. 159   190.

[25] e. schneidman, w. bialek, and m. j. berry, j. neurosci. 23, 11539 (2003).

[26] h. g. eyherabide and i. samengo, j. physiol. (paris) 104, 147 (2010).

[27] i. csisz  ar, ann. prob. , 146 (1975).

[28] j. n. kapur, maximum id178 models in science and engineering (john wiley, new york,

1989).

[29] r. w. yeung, a    rst course in id205 (springer, new york, 2002).

[30]    worddata,    http://reference.wolfram.com/language/note/worddatasourceinformation.html.

[31] d. h. zanette and m. a. montemurro, adv. complex syst. 5, 7. (2002).

[32] i. samengo, phys. rev. e 65, 046124 (2002).

[33]    project gutenberg,    http://www.gutenberg.org.

[34] a. a. margolin, k. wang, a. califano, and i. nemenman, iet syst. biol. 4, 428 (2010).

[35] b. kralemann, a. pikovsky, and m. rosenblum, new j. phys. 16, 085013 (2014).

[36] s. s. wilks, ann. math. stat. 9, 60 (1938).

42

