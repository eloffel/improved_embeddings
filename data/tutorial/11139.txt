analyzing sparse dictionaries
for online learning with kernels

paul honeine, member ieee

1

4
1
0
2

 

p
e
s
1
2

 

 
 
]
l
m

.
t
a
t
s
[
 
 

1
v
5
4
0
6

.

9
0
4
1
:
v
i
x
r
a

abstract   many signal processing and machine learning meth-
ods share essentially the same linear-in-the-parameter model,
with as many parameters as available samples as in kernel-based
machines. sparse approximation is essential in many disciplines,
with new challenges emerging in online learning with kernels. to
this end, several sparsity measures have been proposed in the lit-
erature to quantify sparse dictionaries and constructing relevant
ones, the most proli   c ones being the distance, the approximation,
the coherence and the babel measures. in this paper, we analyze
sparse dictionaries based on these measures. by conducting an
eigenvalue analysis, we show that these sparsity measures share
many properties, including the linear independence condition and
inducing a well-posed optimization problem. furthermore, we
prove that there exists a quasi-isometry between the parameter
(i.e., dual) space and the dictionary   s induced feature space.

index terms   sparse approximation, adaptive    ltering, kernel-
based methods, gram matrix, machine learning, pattern recog-
nition.

i. introduction

s parse approximation is essential in many disciplines due

to the advent of data deluge in the era of    big data   , as
illustrated by the extensive literature of compressed sensing
(see [1] and references therein). sparsity promoting is crucial
in signal processing and machine learning, such as gaussian
processes [2], kernel-based methods [3], bayesian learning [4],
as well as neural networks [5] with pruning [6] and the more
recent dropout principle in deep learning [7].

many learning machines share essentially the same model,
in a linear or a nonlinear     kernel     form, including support
vector machines [8], gaussian processes [9] and radial-basis-
function networks such as resource-allocating networks [5]
and more recently neural networks for function approximation
[10]; see also the seminal work of poggio and smale [11]. all
these learning machines rely on the well-known    represen-
ter theorem    [12], which de   nes a linear-in-the-parameters
model with as many parameters as training samples.

a sparse approximation of this model is often required for
many interesting and desirable properties, such as enforcing
the interpretation of the results and providing a computational
tractable problem for large-scale datasets. while this issue has
been studied within the last 15 years in kernel-based machines
[13], [14], recent developments in sparse approximation and
compressed sensing open the way to new advances. moreover,
online learning brings new challenges to sparsity in signal
processing and machine learning, when a new sample is
available at each instant, thus leads to an incrementation of the

p. honeine is with the institut charles delaunay (cnrs), universit  e de
technologie de troyes, 10000, troyes, france. phone: +33(0)325715625; fax:
+33(0)325715699; e-mail: paul.honeine@utt.fr

number of parameters. therefore, one needs to control such
complexity growth, by selecting samples that take part in the
model formulation; in the literature, these contributing samples
are called atoms and are collected in a set called dictionary.
the construction from available samples of a pertinent
dictionary and the measure of its relevance have been in-
vestigated in the literature with several sparsi   cation criteria,
each being coupled with a sparsity measure that de   nes
the diversity captured by the dictionary. the oldest sparsity
criterion is the distance introduced in [5] for controlling the
complexity of the structure of radial-basis-function networks in
resource-allocating networks [15]; see also [16], [17] for recent
advances. the criterion constructs a dictionary by lower-
bounding the pairwise distance between its atoms. another
criterion, the approximation criterion, explores a more deeper
analysis of the atoms, by lower-bounding the error of approx-
imating any atom by the other atoms, as investigated in [18]
for gaussian processes, in [19] for a kernel recursive least
squares algorithm, and more recently in [20] for a kernel
principal component analysis. a third criterion takes advantage
of recent developments in the sparse approximation literature
[21] and compressed sensing [22], by upper-bounding the
coherence between any pair of atoms. initially introduced for
online learning with kernels [23], [24] and learning in sensor
networks [25], [26], it has been extensively considered for
one-class classi   cation [27], [28], for online learning with
multiple kernels [29], [30] and multiple dictionaries [31] and
for multiple-output learning [32]. the babel measure and
its criterion provide a more comprehensive analysis of the
dictionary structure, by limiting the cumulative coherence [33].
to the best of our knowledge, there is no work that studies all
these sparsity measures and criteria.

independently of the sparsi   cation criterion and the re-
sulting dictionary, many algorithms have been introduced to
update the model. as it might be expected, the wide class
of linear adaptive    lters has been extensively investigated for
online learning with kernels, by revisiting popular algorithms
such as the least mean squares (lms), the normalized lms
(nlms), the af   ne projection (ap), and the recursive least
squares (rls) algorithms; see for instance [34] for a review of
linear adaptive    lters. there exists two frameworks to develop
adaptive algorithms in online learning with kernels, thanks
to the underlying linear-in-the-parameters model: a functional
(i.e., feature) framework and a dual (i.e., parameter) one.
within the functional framework, the optimization is oper-
ated in the feature space, by estimating and updating within
the subspace spanned by the atoms of the dictionary. this
framework has been widely investigated for online learning
with kernels; see for instance [35], [29] as well as [36] for a

2

theoretical analysis and [37] for a comprehensive study. the
second framework is based on estimating the parameters of the
model, thus solving an optimization problem in the so-called
dual space. this framework has been extensively explored in
the literature due to its simplicity, with a nlms algorithm
[23], an ap algorithm [24], and a rls algorithm [19], [38].
for an overview of this framework, see [39] and references
therein. to the best of our knowledge, only yukawa pointed
out the distinction between these two frameworks in [40,
section 6.6.4]. the relationship between the two frameworks
has not been studied before, namely connecting the feature
space to the dual space.

the aim of this paper is to study all the aforementioned spar-
sity measures and sparsi   cation criteria (cf. section iii). to
this end, we provide an analysis of the eigenvalues associated
to a sparse dictionary, and provide upper and lower bounds
in terms of the sparsity measures (cf. section iv-a). we
show that the lower bounds provide conditions on the linear
independence of the atoms (cf. section iv-b). moreover, we
show that the condition number of the gram matrix associated
to a sparse dictionary is upper-bounded, illustrating the impact
of the sparsity measures on the conditioning of the optimiza-
tion problem (cf. section iv-c). a major result provided in
this paper is the connection between the dictionary   s induced
feature space and the dual space, by showing that there exists a
quasi-isometry between these spaces when dealing with sparse
dictionaries. these results allow to bridge the gaps between the
two aforementioned frameworks (cf. sections v-a and v-b).
the big picture is illustrated in table i.

ii. kernel-based learning machines

a learning problem aims to    nd the relation   (  ) between
a compact subspace of a banach space x of rd and a
compact y of r called output space, from on a set of
available samples, denoted {(x1, y1), (x2, y2), . . . , (xn, yn)}
with (xk, yk)     x    y.
a. batch learning with kernels

considering a given id168 c(  ,  ) de   ned on y    y
that measures the error between the desired output and the
estimated one with   (  ), the optimization problem consists in
minimizing a regularized empirical risk as follows

argmin
  (  )   h

n

xi=1

c(  (xi), yi) +    r(k  (  )k2
h),

(1)

where h is the space of candidate functions and    controls the
tradeoff between the    tness error (   rst term) and the regularity
of the solution (second term) where r(  ) is a monotonically in-
creasing function. examples of id168s are the quadratic
loss |  (xi)     yi|2 and the hinge loss (1       (xi)yi)+ of the
support vector machines.
by using the formalism of the reproducing kernel hilbert
space (rkhs) as the space h of candidate functions, kernel-
based machines incorporate prior knowledge by using a
kernel. let    : x    x     r be a reproducing kernel,
and (h,h  ,  ih) the induced rkhs with its inner product.
the reproducing property states that any function   (  ) of

n
o
i
t
a
m
i
x
o
r
p
p
a
[18]
[20]
[20]
[20]
x

x

x

x

e
c
n
e
r
e
h
o
c
[24]
[32]
[23]
x
[23]
x

x

x

e
c
n
a
t
s
i
d
[5]
[37]
x

x

x

x

x

x

l
e
b
a
b
[21]
[33]
x

x
[24]
x

x

x

section

iii
iii
iv-a
iv-a
iv-b
iv-c
v-a
v-b

reference: most known work
reference: more recent work
eigenvalues: lower bounds
eigenvalues: upper bounds
linear independence
condition number
isometry property: distances
isometry property: inner products

a birds eye view of the theoretical insights studied in this
paper. some of these results were previously derived for

table i

unit-norm atoms, as shown with the references given in the

table. in this work, we provide an extensive study that

completes the analysis to all sparsity measures. we derive
new theoretical insights on connecting the dual space with
the dictionary   s induced feature space. all the results are

generalized to any type of kernel, beyond the unit-norm case.

h can be evaluated at any sample xi of x using   (xi) =
h  (  ),   (xi,  )ih. this property shows that any sample xi of
x is represented with   (  , xi) in the space h, also called
feature space. moreover, the reproducing property leads to
the so-called kernel trick, that is for any pair of samples
(xi, xj), we have h  (  , xi),   (  , xj)ih =   (xi, xj). com-
monly used kernels are the linear kernel with hxi, xji, the
polynomial kernel (hxi, xji + c)p and the gaussian kernel
exp(cid:0)    1
the representer theorem is a cornerstone of kernel-based
machines [12]. it states that the solution of the optimization
problem (1) takes the form

2  2 kxi     xjk2(cid:1).

  (  ) =

n

xi=1

  i   (xi,  ).

(2)

this theorem shows that the functional optimization prob-
lem (1) is equivalent
to the estimation of n unknowns,
  1,   2, . . . ,   n in (2). by injecting the above expression into
(2), we get the the (often called) dual problem. this duality
is illustrated next for the kernel ridge regression problem.

b. kernel ridge regression algorithms

in the kernel ridge regression, the quadratic loss and regu-

larization are used in the optimization problem, namely

argmin
  (  )   h

1
2

n

xi=1

|  (xi)     yi|2 +    1

2k  (  )k2
h.

(3)

by injecting the the model (2) in the above expression, we get
the following dual optimization problem:
2kk       yk2 +    1

2      k  ,

argmin
     rn

(4)

1

where k is the gram matrix whose (i, j)-th entry is   (xi, xj),
y and    are vectors whose i-th entries are yi and   i,
respectively. in the above expression, we have used the relation

k  (  )k2

h = (cid:13)(cid:13)

n

xi=1

2
h =

n

xi,j=1

  i  (xi,  )(cid:13)(cid:13)

  i  j   (xi, xj) =      k  .

honeine: analyzing sparse dictionaries

3

the solution of this optimization problem is given by the
   normal equations   , (k   k +   k   )    = k   y, which yields1

(5)

k   y.

   = (cid:0)k   k +   k   (cid:1)   1
id173: k  (  )kh versus k  k
the id173 in the dual optimization problem (4)
is essentially a tikhonov id173 of the form k    k2
(where we have in our case         =   k). in the literature,
the tikhonov matrix    is often chosen as the identity matrix,
up to a multiplicative constant, giving preference to solutions
with smaller norms. the kernel ridge regression becomes

argmin
     rn

1

2kk       yk2 +    1

2k  k2.

(6)

with the    normal equations    (k   k +    i)    = k   y, we get

   = (cid:0)k   k +    i(cid:1)   1

k   y.

connections between the id173 in the functional
space with k  (  )kh and the id173 in the dual space
with k  k are not straightforward. the only result is based
h =      k  , and therefore we have
on the fact that k  (  )k2
from the rayleigh   s quotient and the courant-fischer minimax
theorem [41, theorem 8.1.2]:

  min     k  (  )k2

h

k  k2       max

where   min and   max are the smallest and largest eigenval-
ues of the gram matrix k. as a consequence, minimizing
h yields the upper bound on the norm of the parameter
k  (  )k2
vector with k  k2          1
h, while minimizing k  k2
yields the following upper bound on the norm in the functional
space with k  (  )k2
tighter bounds, as studied in detail in section v.

mink  (  )k2
h       maxk  k2.

it turns out that sparse dictionaries provide models with

c. online learning with kernels

the representer theorem with its linear-in-the-parameters
model (2) constitutes a bottleneck for online learning, which
is required for real-time system identi   cation, big-data pro-
cessing and distributed optimization (e.g., sensor networks).
indeed, in an online setting, the solution should be updated
recursively based on a new information available at each
instant, namely a novel (xt, yt) at instant t. thus, by including
the new pair (xt, yt) in the training set,
the representer
theorem dictates a new parameter   t to be added to the set of
unknowns. as a consequence, the order of the linear-in-the-
parameters model is continuously increasing.

to overcome this drawback, one needs to control the growth
of the model order at each instant, by keeping only a fraction
of the id81s in the expansion (2). the reduced-order
model at instant t takes the form

  t(  ) =

m

xj=1

  j,t   ( `xj,  ),

(7)

1the expression (5) is often simpli   ed to    = (k +    i)   1y. this
equivalence is granted only when the matrix k is nonsingular, an assumption
that
is unfortunately not satis   ed in general. this is due to the linear
dependence of the training samples.

for some order m,    xed or controlled, with m     t. each
is chosen from all available samples up to instant t,
`xj
namely2 { `x1, `x2, . . . , `xm}     {x1, x2, . . . , xt}. we denote by
dictionary the set d = {  ( `x1,  ),   ( `x2,  ), . . . ,   ( `xm,  )}, by
atoms its elements, and by `h the space spanned by d. in this
paper, we do not restrictive ourselves to unit-norm3 atoms. let

r2 = inf
x   x

  (x, x)

and

r2 = sup
x   x

  (x, x).

problem is

optimization
selecting

each
the
instant:
=
the
{  ( `x1,  ),   ( `x2,  ), . . . ,   ( `xm,  )}
corresponding parameters   1,   2, . . . ,   m. before studying in
detail the former in section iii, the latter is outlined next.

two-fold
at
dictionary d
estimating
and

proper

the

notation

throughout this paper, all quantities associated to the dic-
tionary have an accent (by analogy to id102, where stress
accents are associated to prominence). this is the case for
instance of the m-by-1 vector `  (  ) whose j-th entry is   ( `xj,  )
and the gram matrix `k of size m-by-m whose (i, j)-th
entry is   ( `xi, `xj). the eigenvalues of this matrix are denoted
`  1, `  2, . . . , `  m, given in non-increasing order.

d. parameter estimation for online learning

before studying in section iii the dictionary in terms of
sparsity measures and sparsi   cation criteria for constructing
a relevant dictionary, we assume for now that the dictionary
is known. from (7), the problem of determining the model
can be solved in two ways: the functional framework where
  t(  ) is updated from   t   1(  ), and the dual framework with
the update of the parameter vector   t from   t   1. these two
frameworks are summarized next, starting with the latter since
its vector-based formulation is straightforward.

we denote by et = yt       t   1(xt) the prediction error.

dual framework

this framework explores the model (7) written, for any x,

t `  (x),

  t(x) =      

(8)
where   t = [  1,t   2,t
         m,t]    is updated from the
previous estimate, i.e.,   t   1, in the dual space rm. it is easy
to see in (8) the structure of a a    nite-impulse-response    lter,
the    lter input being `  (x) and its coef   cient vector   t.

by considering the instantaneous risk 1
2|yt           `  (xt)|2 +
   1
2k  k2, where the    rst term is the quadratic instantaneous
error e2

t , we get the stochastic id119 rule
  t =   t   1 +   t(cid:0)et `  (xt)          t   1(cid:1).

2we consider that each `xj is a sample selected from available samples,
that is `xj is some x  j with   j     {1, 2, . . . , t}. by using the notation `xj in
this paper, as opposed to x  j , the elements `xj in the expansion (7) need not
be samples drawn from the distribution. this difference is investigated in[42],
[43], by updating `xj at each instant in order to minimize the prediction error.
3throughout this paper, we outline the special case of unit-norm atoms
since such setting is often considered in the literature. unit-norm atoms arise
when dealing either with the linear kernel when kxk = 1 for any x     x, or
with a unit-norm kernel, namely   (x, x) = 1 for any x     x.

(9)

4

h as
when dealing with the functional id173 k  (  )k2
in (4), this id173 is approximated with       `k  , which
yields the modi   ed version

  t =   t   1 +   t(cid:0)et `  (xt)        `k  t   1(cid:1).

the two rules (9) and (10) reduce to the lms algorithm when
   = 0. another algorithm is the nlms, which provides a scale
insensitive version with

(10)

  t =   t   1 +

  t

k `  (xt)k2 +   

et `  (xt).

see [23] for more details. an extension to an ap algorithm is
proposed in [24], while a rls algorithm is presented in [19],
[38]. a comprehensive study of adaptive    lter algorithms in
the dual framework is given in [39]. see also [29], [31], [33].

functional framework

the functional framework considers the de   nition of the

model (7) in the rkhs, with the form

  t(x) = h  t(  ),   (x,  )ih,

(11)
for any x     x. the estimation of   t(  ) from the previous esti-
mate   t   1(  ) is operated in the rkhs h, or more speci   cally
in the span of the available dictionary, i.e.,   t(  )     `h     h.
by considering the instantaneous risk 1
2|  (xt)     yi|2 +
   1
2k  (  )k2

h, the stochastic id119 in h is

  t(  ) =   t   1(  ) +   t(cid:0)et   (xt,  )          t   1(  )(cid:1).

by analogy with the dual framework, other algorithms can
also be described such as an lms, a nlms, an ap, and a
rls algorithms. see [37] for more details.

unfortunately, all these formulations assume the    niteness
of the training set, as reported in [35] and [36]. this drawback
is due to the fact that the model is fed with a new kernel
function at each instant. in order to control this growth and
restrict ourselves to the span of the dictionary, we replace4 the
current   (xt,  ) by its projection onto the subspace spanned
   1 `  (  ); see
by the dictionary, namely `  xt (  ) = `  (xt)    `k
appendix for details. this leads to the expression
  t(  ) = (1       t   )   t   1(  ) +   t et `  x(  ).

to implement this formula, one needs to provide an update
rule of the parameters, with an expression of the form

  t = (1       t   )   t   1 +   t et `  (xt)    `k

   1

.

iii. online sparsification and sparsity measures
independently of the investigated framework, online learn-
ing algorithms should be coupled with a sparsi   cation scheme.
at each instant, the dictionary is updated if necessary, or it is
left unchanged. indeed, the dictionary is augmented whenever
the novel id81   (xt,  ) increases the diversity of the
dictionary. there exists several sparsity measures to quantify
this diversity, as described in the following.

4besides the approximation with the projection which can be computation-
ally expensive, one may replace the current id81 with its most
collinear atom. this leads to a quantization strategy [44].

before detailing these sparsity measures, we outline the

online sparsi   cation scheme. two cases may arise:

    case 1: the dictionary is left unchanged.

this case arises when the novel id81   (xt,  )
does not contribute signi   cantly to the diversity of the
dictionary, and therefore it could be discarded.

    case 2: the id81 is added to the dictionary.
this case arises when the id81   (xt,  ) is
signi   cantly different from the atoms of the dictionary.
one may also use a removal process in the latter case in order
to provide a    xed-budget learning [45], [46], by discarding
the atom that has the least contribution to the diversity of the
dictionary, as investigated for instance in [47].

a. the distance measure

a simple measure to characterize a sparse dictionary is the
least distance between all pairs of its atoms. a dictionary is
said to be   -distant when

min

i,j=1      m

   k  ( `xi,  )          ( `xj,  )kh       ,
min

(12)

i6=j

where we have included a scaling factor   . this corresponds
to the reconstruction error of projecting   ( `xi,  ) onto   ( `xj,  ),
with    =   ( `xi, `xj)/  ( `xj, `xj). by substituting this value in
(12), we get for any pair ( `xi, `xj):

  ( `xi, `xi)    

  ( `xi, `xj)2
  ( `xj, `xj)       2.

(13)

a sparsi   cation criterion based on this measure constructs
a dictionary with a large distance measure, thus including the
candidate id81   (xt,  ) in the dictionary if
  ( `xj, `xj) (cid:19)       2,

j=1      m(cid:18)  (xt, xt)    

  (xt, `xj)2

(14)

min

for some threshold parameter   . this sparsi   cation criterion
is related to the novelty criterion given in [5], which is the
sparsi   cation criterion without the scaling factor followed by
a prediction error mechanism.

b. the approximation measure

the distance measure de   ned in (12)-(13) relies only on
two atoms, that is the closest pair in the dictionary. a more
comprehensive analysis of the dictionary composition is the
capacity of approximating any atom by a linear combination
of the other atoms. a dictionary is designated   -approximate
if the following is satis   ed:

min

i=1      m

  ( `xi,  )    

min

  1        m(cid:13)(cid:13)(cid:13)

m

xj=1

j6=i

  j   ( `xj,  )(cid:13)(cid:13)(cid:13)h       .

(15)

this corresponds to the reconstruction error of projecting any
id81   ( `xi,  ) onto the subspace spanned by the other
id81s. following the derivation given in appendix

   = `k

   1

\{i}

`  \{i}( `xi),

(16)

honeine: analyzing sparse dictionaries

5

where `k\{i} and `  \{i}( `xi) are obtained from `k and `  ( `xi),
respectively, by removing the entries associated to `xi. as a
consequence, expression (15) becomes

min

i=1      m

  ( `xi, `xi)     `  \{i}( `xi)    `k

   1

\{i}

`  \{i}( `xi)       2.

(17)

the (linear) approximation criterion is based on construct-
ing a dictionary with a high approximation measure, as investi-
gated for gaussian processes in [2], for a kernel-based    lter in
[19] and more recently for kernel principal component analysis
in [20]. the id81   (xt,  ) is added to the dictionary
if

min

  1        m(cid:13)(cid:13)(cid:13)

  (xt,  )    

m

xj=1

  j   ( `xj,  )(cid:13)(cid:13)(cid:13)

2

h       2,

(18)

where    is a positive threshold parameter that controls the level
of sparseness. this leads to the following condition, written
in matrix form   (xt, xt)     `  (xt)    `k

   1 `  (xt)       2.

c. the coherence measure

the coherence is a fundamental measure to characterize
a dictionary in the literature of sparse approximation. it
corresponds to the largest correlation between atoms of a given
dictionary, or mutually between atoms of two dictionaries. the
coherence measure has been investigated for the analysis of
the quality of representing a signal with a dictionary, initially
with the work [48], [21], and more recently in the abundant
publications on compressed sensing [22]. while most work
consider the use of a linear measure, we explore in the
following the coherence on id81s in order to derive
the coherence criterion, as initially proposed in [23], [24].

a dictionary d is said   -coherent if
|  ( `xi, `xj)|

max

i,j=1      m

i6=j

p  ( `xi, `xi)   ( `xj, `xj)       .

(19)

the coherence corresponds to the cosine of the angle between
the id81s, since the above quotient can be written

|h  ( `xi,  ),   ( `xj,  )ih|
k  ( `xi,  )khk  ( `xj,  )kh
for unit-norm kernels, (19) becomes max

.

i,j=1      m

|  ( `xi, `xj)|       .
the coherence criterion constructs a low-coherent dictionary
[23], [24]. it includes the candidate id81   (xt,  ) in
the dictionary if the coherence of the latter does not exceed a
given threshold        ]0 ; 1], namely

i6=j

max
j=1      m

|  (xt, `xj)|

p  (xt, xt)   ( `xj, `xj)       .

(20)

this condition enforces an upper bound on the cosine of the
angle between each pair of id81s. the threshold
   controls the level of sparseness of the dictionary, where a
null value yields an orthogonal basis. this criterion is com-
putationally ef   cient as given in expression (20), where the
denominator reduces to 1 for unit-norm atoms, thus becomes
in this case max

j=1      m|  (xt, `xj)|       .

d. the babel measure

from a norm perspective, the coherence is essentially the
   -norm when dealing with unit-norm atoms. the babel
notion explores such analogy with the norm operator, thus
providing a more complete description of the dictionary struc-
ture [49], [21]. the babel is related to the 1-norm of the gram
matrix, with the de   nition

babel = max
i=1      m

m

xj=1

j6=i

|  ( `xi, `xj)|.

(21)

it corresponds to the maximum cumulative correlation between
an atom and all the other atoms of the dictionary. it is easy to
see that, when dealing with a unit-norm atoms, the coherence
of the dictionary cannot exceed its babel measure.

kernel

the babel criterion is de   ned as follows. a candidate
function   (xt,  ) is included in the dictionary if
pm
j=1 |  (xt, `xj)|       , for a given positive threshold   . this
de   nition can be viewed as an extension of the coherence cri-
terion in the same sense as the approximation is an extension
of the distance criterion. see [33] for the use of the babel
measure for sparsi   cation.

iv. an eigenvalue analysis

since the gram matrix is fundamental in the analysis of
the dictionary, we study in the following its eigenvalues, and
provide theoretical bounds. these results provide an analysis
of the span de   ned by a sparse dictionary, given in terms of the
sparsity measure under scrutiny. lower bounds are used in the
forthcoming linear independence analysis (cf. section iv-b),
while lower and upper bounds are investigated in the forth-
coming study of the condition number (cf. section iv-c) and
in the main results derived in next section (cf. section v). let
`  1, `  2, . . . , `  m be the eigenvalues of the matrix `k, given in
non-increasing order, namely `  1     `  2     . . .     `  m.
a. bounds on the eigenvalues

j=1

before proceeding, we bring to mind the well-known
ger  sgorin discs theorem [50, chapter 6], revisited here for
the gram matrix of a sparse dictionary. it is also well known
that the trace of a matrix equals the sum of its eigenvalues.
`  j = trace( `k) =

we get for unit-norm atoms: pm
j=1   ( `xj, `xj) = m, thus 1     `  1 and `  m     1.
pm
theorem 1 (ger  sgorin discs theorem): every eigenvalue
of an m-by-m matrix `k lies in the union of the m discs,
`k with a radius given by
centered on each diagonal entry of
the sum of the absolute values of the other m    1 entries from
the same row. in other words, for each `  i, there exists at least
one j     {1, 2, . . . , m} such that
| `  i       ( `xj, `xj)|    

|  ( `xi, `xj)|.

m

xj=1

j6=i

this theorem is a cornerstone in our study, as described next
by providing upper and lower bounds on the eigenvalues of the
gram matrix associated to a sparse dictionary, by investigating
its sparsity measure.

6

distance measure

when the distance measure of a given sparse dictionary
is known, namely   , we have from (12)-(13) that any pair
( `xi, `xj) satis   es

therefore, we have

|  ( `xi, `xj)|     q  ( `xj, `xj)(cid:0)  ( `xi, `xi)       2(cid:1).
|  ( `xi, `xj)|     xj q  ( `xj, `xj)(cid:0)  ( `xi, `xi)       2(cid:1)
= p  ( `xi, `xi)       2xj q  ( `xj, `xj).

xj

by applying the ger  sgorin discs theorem (theorem 1) with
the above relation in mind, we get that, for each eigenvalue
`  k, there exists at least one i such that

| `  k       ( `xi, `xi)|    

m

xj=1

j6=i

|  ( `xi, `xj)|

    p  ( `xi, `xi)       2

q  ( `xj, `xj).

m

xj=1

j6=i

the proof of the theorem follows from the ger  sgorin discs
theorem (theorem 1), namely for any eigenvalue `  k, there
exists an i such that

| `  k       ( `xi, `xi)|    

m

xj=1

j6=i

|  ( `xi, `xj)|       ( `xi, `xi)       2.

coherence measure

when measuring the sparsity of the dictionary with the
coherence measure, we have the following theorem. only the
lower bound has been previously investigated in the literature
when dealing with unit-norm atoms; see [23].

theorem 4: the eigenvalues of the gram matrix associated
to a   -coherent dictionary of m atoms are bounded as follows:
r2     (m     1)  r2     `  m                `  1     r2 + (m     1)  r2,
where r2 = supx   (x, x) and r2 = inf x   (x, x). for unit-
norm atoms, we get

1     (m     1)        `  m                `  1     1 + (m     1)   .
proof: a   -coherent dictionary satis   es

by exploring these results, the proof of the following theorem
is straightforward.

theorem 2: the eigenvalues of the gram matrix associated

max
j=1      m

j6=i

|  ( `xi, `xj)|

p  ( `xi, `xi)   ( `xj, `xj)       ,

to a   -distant dictionary are bounded as follows:

for any i = 1, 2, . . . , m, which yields

r2     (m     1)rpr2       2     `  m           

           `  1     r2 + (m     1)rpr2       2,
where r2 = inf x   (x, x) and r2 = supx   (x, x). for unit-
norm atoms, we get

1   (m   1)p1       2     `  m                `  1     1+(m   1)p1       2.

approximation measure

max
j=1      m

|  ( `xi, `xj)|        max

j=1      m

j6=i

j6=i

q  ( `xi, `xi)   ( `xj, `xj)
q  ( `xj, `xj)

j=1      m

=   p  ( `xi, `xi) max
      rp  ( `xi, `xi).

j6=i

finally, the proof results from applying the ger  sgorin discs
theorem (theorem 1), since

presented here for completeness, the following theorem is

essential due to honeine in [20].

theorem 3: the eigenvalues of the gram matrix associated

to a   -approximate dictionary are bounded as follows:

m

xj=1

j6=i

  2     `  m                `  1     2r2       2,

|  ( `xi, `xj)|     (m     1) max

j=1      m

|  ( `xi, `xj)|

j6=i

    (m     1)  rp  ( `xi, `xi)
    (m     1)  r2.

where r2 = supx   (x, x).

proof: by injecting (16) in (17), we get min     ( `xi, `xi)   

`  \{i}( `xi)            2, for any i = 1, 2, . . . , m, or equivalently

max

  

m

xj=1

j6=i

  j   ( `xi, `xj)       ( `xi, `xi)       2.

considering the special case (which could be far from the
optimum) of   j = sign(  ( `xi, `xj)), we get

m

xj=1

j6=i

|  ( `xi, `xj)|       ( `xi, `xi)       2.

babel measure

when dealing with the babel measure as a sparsity measure,
the eigenvalues of the gram matrix associated to the dictionary
are bounded as given in the following theorem.

theorem 5: the eigenvalues of the gram matrix associated

to a   -babel dictionary are bounded as follows:
r2            `  m                `  1     r2 +   ,

where r2 = supx   (x, x) and r2 = inf x   (x, x). for unit-
norm atoms, we get 1            `  m                `  1     1 +   .

honeine: analyzing sparse dictionaries

7

proof: the proof follows from the ger  sgorin discs the-
orem (theorem 1) since, for any eigenvalue `  k, there exists
an i     {1, 2, . . . , m} with

| `  k       ( `xi, `xi)|    

m

|  ( `xi, `xj)|       .

xj=1

j6=i

b. linear independence

it is relevant to construct a dictionary with linearly indepen-
dent atoms, a condition that allows to represent any feature
of dh in a unique linear way. for a dictionary of m kernel
functions, the atoms are linearly independent if the following is
satis   ed: any linear combination pm
j=1   j   ( `xj,  ) is the zero
element if and only if all the weighting coef   cients   j are null.
it is trivial that a dictionary with an nonzero approximation

measure has linear independent atoms, since we have

m

(cid:13)(cid:13)(cid:13)
xj=1

  j   ( `xj,  )(cid:13)(cid:13)(cid:13)h
= (cid:13)(cid:13)(cid:13)
  i  ( `xi,  )    
= |  i|(cid:13)(cid:13)(cid:13)

  ( `xi,  )    

(for any i)

m

xj=1

j6=i

m

  j
  i

  j   ( `xj,  )(cid:13)(cid:13)(cid:13)h
  ( `xj,  )(cid:13)(cid:13)(cid:13)h
xj=1
  j   ( `xj,  )(cid:13)(cid:13)(cid:13)h
xj=1

j6=i

j6=i

m

    |  i| min

  1        m(cid:13)(cid:13)(cid:13)

  ( `xi,  )    

    |  i|   ,

for any decomposition, i.e., i     {1, 2, . . . , m}. thus, the linear
combination is the zero element only when all coef   cients   i
are null or when the threshold    is null.

in the following, we show that all the sparsity measures
provide suf   cient conditions for linear independence of the
dictionary   s atoms. to this end, we investigate the duality
between linear independence and the non singularity of the
associated gram matrix, which is essentially considered in [48]
for the coherence of a linear dictionary with unit-norm atoms
and extended in [24] for kernel-based dictionaries. indeed, we
have

m

(cid:13)(cid:13)(cid:13)
xj=1

  j   ( `xj,  )(cid:13)(cid:13)(cid:13)

2

h

=       `k       `  mk  k2,

where the courant-fischer minimax theorem is used [41,
theorem 8.1.2]. as a consequence, we prove the linear in-
dependence of a atoms by providing a lower bound on the
eigenvalues of the associated gram matrix. the following the-
orem summarizes this property for different sparsity measures.

theorem 6 (linear independence): a suf   cient condition

for the linear independence of the m atoms is:

    (m     1)r   r2       2 < r2 for a   -distant dictionary.
       > 0 for a   -approximate dictionary.
    (m     1)  r2 < r2 for a   -coherent dictionary.
       < r2 for a   -babel dictionary.

these results generalize the bounds given for only unit-norm
atoms, in [23] for the coherence measure with (m     1)   < 1
and in [24] for the babel measure with    < 1.

c. condition number

the condition number of a matrix `k, for a given matrix
norm, is de   ned by cond( `k) = k `kkk `k
k, which reduces
for the    2-norm to:

   1

cond( `k) = | `  1|
| `  m|

.

(22)

it is an important measure of the sensitivity, with respect to
variations within the matrix `k, of the resolution of a problem
of the form `k   = y,    being the unknown. it gives a bound
on how inaccurate the solution    will be after approximation.
when its value is small, i.e., close de 1, the solution is robust
to perturbations, as opposed to large values that lead to ill-
conditioned problems, if not even ill-posed.

for instant, consider a id119 procedure to solve
the linear system `k   = y. it is shown in [51] that the error
reduction at each iteration is bounded by an upper bound that
is proportional to the condition number of the matrix `k. the
condition number has been studied more recently in kernel-
based machine learning; see for instant [52]. next, we provide
an upper bound on the condition number, in terms of the
sparsity measure of the dictionary. the proof of the following
theorem is straightforward from the de   nition of the condition
number (22) and the aforementioned theorems on lower and
upper bounds on the eigenvalues.

theorem 7 (condition number): the condition number of
the gram matrix associated to a sparse dictionary is upper-
bounded by:

for a   -distant dictionary.

   

   

   

   

r2 + (m     1)r   r2       2
r2     (m     1)r   r2       2
2r2
  2     1 for a   -approximate dictionary.
r2 + (m     1)  r2
r2     (m     1)  r2
r2 +   
r2       

for a   -babel dictionary.

for a   -coherent dictionary.

the case of unit-norm atoms is obtained from the relation r =
r = 1, which yields for instance the upper bound 1+(m   1)  
1   (m   1)  
for a   -coherent dictionary. these results demonstrate how
the choice of the threshold value in the sparsi   cation criterion
impacts on the conditioning of the system, towards a well-
posed optimization problem.

v. connecting the dictionary   s induced feature

space and the dual space

in this section, we show that both feature subspace and the
dual space are intimately related in their topologies, when the
feature subspace is spanned by the atoms from a sparse dic-
tionary. to this end, we show in section v-a that the pairwise
distances in both spaces are almost preserved. this quasi-
isometry property associated to a given sparse dictionary is
quanti   ed in terms of each of the sparsity measures presented
in section iii, namely the distance, approximation, coherence,
and babel measures. these results on the isometry are ex-
tended in section v-b to the issue of preserving the pairwise
inner-products in both spaces. all these results establish the

8

structural-preserving map that connects both spaces, namely
the map   d de   ned as follows

  d : rm 7       `h     h

  

         (  ) =       `  (  )

it is worth noting that these results require that the atoms
of the dictionary are linear independent, since this condition
`h can be uniquely rep-
guarantees that any feature   (  ) of
resented by atoms of the dictionary. see section iv-b and in
particular theorem 6 which provides weak conditions in terms
of the sparsity measure of the dictionary.

a. isometry property

j=1      

j=1         

without limiting ourselves to online learning by comparing
  t(  ) with   t   1(  ), we consider here any two features from
the feature space `h, denoted      (  ) = pm
j   ( `xj,  ) and
        (  ) = pm
j   ( `xj,  ). their representations in the dual
space rm are denoted       and         , respectively. there exists
an isometry between these two spaces if the distance between
any pair of features corresponds to the distance between their
parameter vectors, namely k     (  )             (  )kh = k                  k.
while the isometry property is too restrictive, we relax it with
the following de   nition of quasi-isometry, by showing that the
quotient of these two distances is close to unity. we denote
  (  ) =      (  )           (  ), then its parameter vector is    =                 .
de   nition 8 (quasi-isometry): given a dictionary of kernel
functions {  ( `x1,  ),   ( `x2,  ), . . . ,   ( `xm,  )}, and `h the space
spanned by its atoms, we say that the spaces rm and `h
are quasi-isometric if there exists an isometry constant    (the
smallest number) such that, for any vector    of entries   j,
the feature   (  ) =       `  (  ) satis   es
1            k  (  )k2
k  k2

2     1 +   .

this means that the map   d :              `  (  ) approximately
preserves the distances in both spaces rm and `h. it is easy to
see that a dictionary with an isometry constant    = 0 provides
a    total    isometry between these spaces.

(23)

h

in the following, we show that the quasi-isometry property
is satis   ed for sparse dictionaries, by relying on the investi-
gated sparsity measure. before generalizing with theorem 10,
we restrict ourselves in theorem 9 to the case of unit-norm
atoms, which is often suf   cient in most work in the literature
of sparse approximation, e.g., when using the gaussian kernel.
theorem 9 (isometry property    unit-norm atoms   ): a dic-
tionary of unit-norm atoms has an isometry constant    de   ned
as follows:

       = (m     1)   1       2 for a   -distant dictionary.
       = 1       2 for a   -approximate dictionary.
       = (m     1)   for a   -coherent dictionary.
       =    for a   -babel dictionary.

proof: for any   (  ) with its parameter vector    we have
h =       `k  , then the quotient
h = kpm
k  (  )k2
in (23) is the rayleigh-ritz quotient of the gram matrix `k.
by applying the courant-fischer minimax theorem, we get

j=1   j   ( `xj,  )k2

`  m     k  (  )k2
k  k2

h

2     `  1,

where `  m and `  1 and the smallest and largest eigenvalues of
the matrix `k. we can easily identify from (23) the following
pair of inequalities:

1            `  m

and

`  1     1 +   .

by exploring the results derived in section iv, we can identify
the isometry constants of the dictionary in terms of its distance,
approximation, coherence and babel measures. besides the ap-
proximation measure, all these expressions are straightforward
from theorems 2, 4, 5, thanks to the bounds on the eigenvalues
that are symmetric about 1. even in the asymmetric bounds
of the approximation measure as given in theorem 3, that is
  2     `  m                `  1     2       2, one can easily identify the
expression of the isometry constant    = 1       2.
when dealing with non-unit-norm atoms, expressions are a
bit more dif   cult to derive, due to the asymmetry of the bounds
on the eigenvalues, as shown by the following theorem.
has

theorem 10 (isometry property): a dictionary

an

isometry constant    de   ned as follows:

r2     r2 + 2(k     1)r   r2       2

       =

r2 + r2

for a   -distant

dictionary.
  2
r2

       = 1    
       =

r2     r2 + 2(k     1)  r2
r2     r2 + 2  

r2 + r2

r2 + r2

for a   -approximate dictionary.

for a   -coherent dictionary.

       =

for a   -babel dictionary.

in these expressions, r2 = supx   (x, x) and r2 =
inf x   (x, x).

proof: consider the general asymmetric bounds

lk     `  m     k  (  )k2
k  k2

h

2     `  1     uk,

for some lower bound lk and upper bound uk, such that
0 < lk     uk <    . in order to get bounds that are
symmetric about 1, as in de   nition 8, we divide each term
by (uk + lk)/2. this yields the isometry constant    = (uk    
lk)/(uk + lk) for the rescaled atoms of the dictionary, where
each atom is divided by p(uk + lk)/2. finally, the proof

of the theorem follows the same steps as in the proof of
theorem 9.

it is easy to see that theorem 9 is a special case of this
theorem when dealing with unit-norm atoms, i.e., r = r = 1.

b. preserving inner products

theorems 9 and 10 show that a sparse dictionary provides a
quasi-isometry, with respect to the distances, between the dual
space and the subspace spanned by its atoms. in the following,
we show that this property of quasi-isometry extends to inner
products. it is worth noting that, when dealing with a    total   
isometry, the isometry with respect to inner products extends
naturally to the isometry with respect to distances, and vice
versa5. this is not the case when using the quasi-isometry
de   nition. we aim to bridge this gap in the following.

5for any linear operator a from an inner product space to another inner
product space, there exists an equivalence between hau, avi = hu, vi for
any (u, v) and kauk = kuk for any u.this equivalence is less obvious
when dealing with quasi-isometry.

honeine: analyzing sparse dictionaries

9

de   nition 11 (quasi-isometry w.r.t. inner products):

appendix

a

of

kernel

dictionary

given
functions
{  ( `x1,  ),   ( `x2,  ), . . . ,   ( `xm,  )}, and `h the space spanned
by its atoms, we say that the spaces rm and `h are quasi-
isometric with respect to inner products if there exists an
isometry constant    (the smallest number) such that, for any
pair of vectors (     ,         ), we have

(cid:12)(cid:12)(cid:12)dpm

j=1      

j   ( `xj,  ),pm

j=1         

j   ( `xj,  )eh                     (cid:12)(cid:12)(cid:12)

      .
(24)
it is easy to see that the    total    isometry with respect to
inner products corresponds to    = 0 in (24). this expression

k     k2 k        k2

becomes (cid:10)pm

j=1      

j   ( `xj,  ),pm

and as a consequence the condition (23) is satis   ed as a special
case where       =         .

j   ( `xj,  )(cid:11)h =                 ,

j=1         

in the general case, the quotient in (24) can be written as

the projection of any id81   (x,  ) onto the
subspace spanned by a dictionary of id81s   ( `xj,  ),
for j = 1, 2, . . . , m, takes the form

`  x(  ) =

m

xj=1

  j   ( `xj,  ),

or equivalently `  x(  ) =      `  (  ), where    is obtained by
minimizing the quadratic reconstruction error

k  (x,  )          `  (  )k2
h.

(26)
the expansion of this norm is given by   (x, x)    2      `  (x) +
      `k  . by taking its derivative with respect to    and nullifying
it, we get

`k   = `  (x).

(cid:12)(cid:12)         `k                             (cid:12)(cid:12)
k     k2 k        k2

= (cid:12)(cid:12)        ( `k     i)        (cid:12)(cid:12)
k     k2 k        k2

,

therefore, the projection is given by
`  x(  ) = `  (x)    `k

   1 `  (  ).

(25)

and therefore the inequality (24) becomes
        ( `k     i)         
k     k2 k        k2       .

          

to tackle this expression, several issues need to be addressed.
first of all, the above quotient needs to be connected to the
rayleigh-ritz quotient of the matrix `k     i, in order to apply
the courant-fischer minimax theorem. indeed, this theorem
can be also applied to study a quotient of the form

u   a v
kuk2 kvk2

,

for any pair (u, v), as shown in [41, theorem 8.6.1]; see also
[53, theorem 3] for a detailed proof. as a consequence, the
quotient in (25) is bounded by the extreme eigenvalues of the
matrix `k     i. second, it is easy to see that both matrices
`k and `k     i share the same eigenvectors, while for any
eigenvalue `  j of
`k corresponds the eigenvalue `  j     1 of
`k   i. indeed, any eigenpair ( `v, `  j) of `k satis   es(cid:0) `k   i(cid:1) `v =
`k `v     i `v = `  j `v     i `v = (cid:0) `  j     1(cid:1) `v, therefore ( `v, `  j     1) is
an eigenpair of the matrix `k     i.
as a consequence, one can take advantage of bounds on
the eigenvalues from theorems 2, 3, 4 and 5 to provide
expressions for the isometry constant w.r.t. inner products, as
detailed in theorems 9 and 10.

vi. final remarks

this paper provided a framework, based on an eigenvalue
analysis, to study sparsity measures and sparsi   cation criteria.
we proposed a uni   ed study for the well-conditioning of the
optimization problem and for the condition on the uniqueness
of the solution. we established a quasi-isometry between the
dual space and the dictionary   s induced feature space, thus
connecting the functional to the dual frameworks and illus-
trating the impact of the sparsity measures on the topologies.
as for future work, we are extending this framework to include
new insights on sparse dictionary analysis.

the quadratic reconstruction error of such approximation is
obtained by substituting this expression into (26), yielding

  (x, x)     `  (x)    `k

   1 `  (x).

references

[1] r. baraniuk, e. candes, r. nowak, and m. vetterli, eds., ieee signal
processing magazine, special issue on    sensing, sampling, and com-
pression   , vol. 25 (2). ieee signal processing society, march 2008.

[2] l. csat  o and m. opper,    sparse representation for gaussian process
models,    in advances in neural information processing systems 13,
pp. 444   450, mit press, 2001.

[3] m. wu, b. sch  olkopf, and g. bak  r,    a direct method for building sparse
kernel learning algorithms,    journal of machine learning research,
vol. 7, pp. 603   624, 2006.

[4] d. wipf, j. palmer, and b. rao,    perspectives on sparse bayesian
learning,    in advances in neural information processing systems 16
(s. thrun, l. saul, and b. sch  olkopf, eds.), mit press, 2004.

[5] j. platt,    a resource-allocating network for function interpolation,   

neural comput., vol. 3, pp. 213   225, june 1991.

[6] y. l. cun, j. s. denker, and s. a. solla,    optimal brain damage,    in
advances in neural information processing systems 2 (d. s. touret-
zky, ed.), pp. 598   605, san francisco, ca, usa: morgan kaufmann
publishers inc., 1990.

[7] n. srivastava, g. hinton, a. krizhevsky, i. sutskever, and r. salakhut-
dinov,    dropout: a simple way to prevent neural networks from over   t-
ting,    journal of machine learning research, vol. 15, pp. 1929   1958,
2014.

[8] v. n. vapnik, statistical learning theory. new york, ny, usa: wiley,

september 1998.

[9] c. e. rasmussen and c. williams, gaussian processes for machine

learning. mit press, 2006.

[10] g. bin huang, p. saratch, s. member, and n. sundararajan,    a gener-
alized growing and pruning rbf (ggap-rbf) neural network for function
approximation,    ieee transactions on neural networks, vol. 16, pp. 57   
67, 2005.

[11] t. poggio and s. smale,    the mathematics of learning: dealing with

data,    notices of the american mathematical society, 2003.

[12] b. sch  olkopf, r. herbrich, and a. j. smola,    a generalized representer
theorem,    in proc. 14th annual conference on computational learning
theory and 5th european conference on computational learning the-
ory, colt/eurocolt, (london, uk), pp. 416   426, springer-verlag,
2001.

10

[13] b. sch  olkopf, s. mika, c. j. c. burges, p. knirsch, k.-r. m  uller,
g. r  atsch, and a. j. smola,    input space versus feature space in kernel-
based methods,    ieee trans. neural networks, vol. 10, pp. 1000   1017,
1999.

[14] j. weston, a. elisseeff, b. sch  olkopf, and m. tipping,    use of the zero
norm with linear models and kernel methods,    j. mach. learn. res.,
vol. 3, pp. 1439   1461, mar. 2003.

[15] r. rosipal, m. koska, and i. farkas,    prediction of chaotic time-series
with a resource-allocating rbf network,    in neural processing letters,
pp. 185   197, 1997.

[16] y.-k. yang, t.-y. sun, c.-l. huo, y.-h. yu, c.-c. liu, and c.-h. tsai,
   a novel self-constructing radial basis function neural-fuzzy system,   
applied soft computing, vol. 13, no. 5, pp. 2390     2404, 2013.

[17] n. vukovi  c and z. miljkovi  c,    a growing and pruning sequential
learning algorithm of hyper basis function neural network for function
approximation,    neural netw., vol. 46, pp. 210   226, oct. 2013.

[18] l. csat  o and m. opper,    sparse online gaussian processes,    neural

computation, vol. 14, pp. 641   668, 2002.

[19] y. engel, s. mannor, and r. meir,    the kernel recursive least squares
algorithm,    ieee trans. signal processing, vol. 52, no. 8, pp. 2275   
2285, 2004.

[20] p. honeine,    online kernel principal component analysis: a reduced-
order model,    ieee transactions on pattern analysis and machine
intelligence, vol. 34, pp. 1814   1826, september 2012.

[21] j. a. tropp,    greed is good: algorithmic results for sparse approxima-
tion,    ieee trans. id205, vol. 50, pp. 2231   2242, 2004.
[22] m. elad, sparse and redundant representations: from theory to

applications in signal and image processing. springer, 2010.

[23] p. honeine, c. richard, and j. c. m. bermudez,    on-line nonlinear
sparse approximation of functions,    in proc. ieee international sympo-
sium on id205, (nice, france), pp. 956   960, june 2007.
[24] c. richard, j. c. m. bermudez, and p. honeine,    online prediction of
time series data with kernels,    ieee transactions on signal processing,
vol. 57, pp. 1058   1067, march 2009.

[25] p. honeine, m. essoloh, c. richard, and h. snoussi,    distributed
regression in sensor networks with a reduced-order kernel model,    in
proc. 51st ieee globecom global communications conference,
(new orleans, la, usa), pp. 1   5, 2008.

[26] p. honeine, c. richard, h. snoussi, j. c. m. bermudez, and j. chen,    a
decentralized approach for non-linear prediction of time series data in
sensor networks,    journal on wireless communications and networking,
vol. special issue on theoretical and algorithmic foundations of wireless
ad hoc and sensor networks, pp. 12:1   12:12, jan. 2010.

[27] z. noumir, p. honeine, and c. richard,    online one-class machines
based on the coherence criterion,    in proc. 20th european conference on
signal processing, (bucharest, romania), pp. 664   668, 27   31 august
2012.

[28] z. noumir, p. honeine, and c. richard,    one-class machines based on
the coherence criterion,    in proc. ieee workshop on statistical signal
processing, (ann arbor, michigan, usa), pp. 600   603, 5   8 august
2012.

[29] m. yukawa,    multikernel adaptive    ltering,    signal processing, ieee

transactions on, vol. 60, pp. 4672   4682, sept 2012.

[30] f. tobar, s.-y. kung, and d. mandic,    multikernel least mean square
algorithm,    neural networks and learning systems, ieee transactions
on, vol. 25, pp. 265   277, feb 2014.

[31] t. ishida and t. tanaka,    multikernel adaptive    lters with multiple
dictionaries and id173,    in signal and information processing
association annual summit and conference (apsipa), 2013 asia-
paci   c, pp. 1   6, oct 2013.

[32] c. said  e, r. lengell  e, p. honeine, and r. achkar,    online kernel
adaptive algorithms with dictionary adaptation for mimo models,    ieee
signal processing letters, vol. 20, pp. 535   538, may 2013.

[33] h. fan, q. song, and s. b. shrestha,    online learning with kernel
regularized least mean square algorithms,    knowledge-based systems,
vol. 59, no. 0, pp. 21     32, 2014.

[34] a. sayed, fundamentals of adaptive    ltering. ny, usa: wiley-ieee

press, june 2003.

[35] j. kivinen, a. j. smola, and r. c. williamson,    online learning with
kernels,    ieee transactions on signal processing, vol. 52, aug 2004.
[36] s. smale and y. yao,    online learning algorithms,    found. comput.

math., vol. 6, no. 2, pp. 145   170, 2006.

[37] w. liu, j. c. principe, and s. haykin, kernel adaptive filtering: a

comprehensive introduction. wiley publishing, 1st ed., 2010.

[38] p. honeine, c. richard, and j. c. m. bermudez,    mod  elisation parci-
monieuse non lin  eaire en ligne par une m  ethode `a noyau reproduisant et

un crit`ere de coh  erence,    in actes du xxi-`eme colloque gretsi sur le
traitement du signal et des images, (troyes, france), september 2007.
[39] p. honeine, m  ethodes `a noyau pour l   analyse et la d  ecision en envi-
ronnement non-stationnaire. phd thesis, m  emoire de th`ese de doctorat
en optimisation et s  uret  e des syst`emes, ecole doctoral ssto - utt,
troyes, france, 2007.

[40] m. yukawa,    adaptive    ltering based on projection method.    lecture

notes, december 2010.

[41] g. golub and c. van loan, matrix computations.

johns hopkins
studies in the mathematical sciences, johns hopkins university press,
2013.

[42] c. said  e, r. lengell  e, p. honeine, c. richard, and r. achkar,    dictio-
nary adaptation for online prediction of time series data with kernels,   
in proc. ieee workshop on statistical signal processing, (ann arbor,
michigan, usa), pp. 604   607, 5   8 august 2012.

[43] c. said  e, p. honeine, r. lengell  e, c. richard, and r. achkar,    adap-
tation en ligne d   un dictionnaire pour les m  ethodes `a noyau,    in actes
du 24-`eme colloque gretsi sur le traitement du signal et des images,
(brest, france), september 2013.

[44] b. chen, s. zhao, p. zhu, and j. principe,    quantized kernel least
mean square algorithm,    neural networks and learning systems, ieee
transactions on, vol. 23, pp. 22   32, jan 2012.

[45] s. van vaerenbergh, i. santamaria, w. liu, and j. principe,    fixed-
budget kernel recursive least-squares,    in acoustics speech and sig-
nal processing (icassp), 2010 ieee international conference on,
pp. 1882   1885, march 2010.

[46] d. rzepka,    fixed-budget kernel

least mean squares,    in emerging
technologies factory automation (etfa), ieee 17th conference on,
pp. 1   4, sept 2012.

[47] d. nguyen-tuong and j. peters,    incremental online sparsi   cation for
model learning in real-time robot control,    neurocomputing, vol. 74,
no. 11, pp. 1859     1867, 2011.

[48] a. c. gilbert, s. muthukrishnan, and m. j. strauss,    approximation
of functions over redundant dictionaries using coherence,    in proc.
14-th annual acm-siam symposium on discrete algorithms (soda),
(philadelphia, pa, usa), pp. 243   252, society for industrial and ap-
plied mathematics, 2003.

[49] a. c. gilbert, s. muthukrishnan, m. j. strauss, and j. tropp,    improved
sparse approximation over quasi-incoherent dictionaries,    in interna-
tional conference on image processing (icip), vol. 1, (barcelona,
spain), pp. 37   40, sept. 2003.

[50] r. a. horn and c. r. johnson, matrix analysis. new york, ny, usa:

cambridge university press, 2nd edition ed., december 2012.

[51] d. luenberger, introduction to linear and nonid135.

addison-wesley, second ed., 1989.

[52] v. kurkov  a and m. sanguineti,    learning with generalization capability
by kernel methods of bounded complexity,    j. complex., vol. 21, no. 3,
pp. 350   367, 2005.

[53] h. xiang,    a note on the minimax representation for the subspace
distance and singular values,    id202 and its applications,
vol. 414, no. 2   3, pp. 470     473, 2006.

place
photo
here

paul honeine (m   07) was born in beirut, lebanon,
on october 2, 1977. he received the dipl.-ing.
degree in mechanical engineering in 2002 and the
m.sc. degree in industrial control in 2003, both from
the faculty of engineering, the lebanese university,
lebanon. in 2007, he received the ph.d. degree in
systems optimisation and security from the uni-
versity of technology of troyes, france, and was
a postdoctoral research associate with the systems
modeling and dependability laboratory, from 2007
to 2008. since september 2008, he has been an
assistant professor at the university of technology of troyes, france. his
research interests include nonstationary signal analysis and classi   cation,
nonlinear and statistical signal processing, sparse representations, machine
learning. of particular interest are applications to (wireless) sensor networks,
biomedical signal processing, hyperspectral imagery and nonlinear adaptive
system identi   cation. he is the co-author (with c. richard) of the 2009
best paper award at the ieee workshop on machine learning for signal
processing. over the past 5 years, he has published more than 100 peer-
reviewed papers.

