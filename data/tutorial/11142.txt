   #[1]meta [2]meta [3]meta

   iframe: [4]https://www.googletagmanager.com/ns.html?id=gtm-tr6twrh

   iframe: [5]https://www.googletagmanager.com/ns.html?id=gtm-tr6twrh

create a new account

   email
   ____________________
   register
   [6]returning user

   can't sign in? forgot your password?

   enter your email address below and we will send you the reset
   instructions
   email
   _______________
   submit
   [7]cancel

   if the address matches an existing account you will receive an email
   with instructions to reset your password
   [8]close

   can't sign in? forgot your username?

   enter your email address below and we will send you your username
   email
   _______________
   submit
   [9]cancel

   if the address matches an existing account you will receive an email
   with instructions to retrieve your username
   [10]close

change password

   old password
   ____________________
   new password
   ____________________
   too short weak medium strong very strong too long
   submit
   [11]cancel

password changed successfully

   your password has been changed
   [12]close

login to your account
     __________________________________________________________________

   username
   _______________
   [13]forgot username?
   password
   ____________________
   [14]forgot password?
   keep me logged in
   [ ]
   login
   new user
   institutional login
   [15][mitpress-logo-main-1483476130433.svg]
   [16]the mit pressjournals

     * [17]books
     * [18]journals
     * [19]digital
     * [20]resources
          + [21]for librarians
               o [22]institutional activation
               o [23]institutional license agreement
               o [24]usage stats
               o [25]document delivery
               o [26]mit press journals vpat
               o [27]join our librarian mailing list
               o [28]librarian faq
         [29]quick guides
               o [30]2019 institutional pricing
               o [31]2019 institutional package options and pricing
               o [32]single issue price list
          + [33]for authors
               o [34]submitting publication agreements
               o [35]author posting guidelines
               o [36]copyright information
               o [37]author reprints
               o [38]publishing open access
               o [39]nih public access policy
               o [40]author discounts
               o [41]the mit press and kudos
         [42]quick guides
               o [43]guide1
               o [44]guide2
               o [45]guide3
          + [46]request permissions
         [47]give to the frank urbanowski fund
         [48]trade sales
         [49]advertising info
         [50]release schedule
         [51]faq
         [52]terminated journals
          +
     * [53]about
          + the mit press is a leading publisher of books and journals at
            the intersection of science, technology, and the arts. mit
            press books and journals are known for their intellectual
            daring, scholarly standards, and distinctive design.

[54]about mitpj statement

[55]statement of publication ethics

[56]events & conferences

[57]publishing services

[58]mit press journals staff

[59]how we use analytics
          +
     * [60]contact
          + location
            the mit press
            one rogers street
            cambridge, ma 02142-1209
            [fpo-map-1493476240117.png]
          + phone
            (800) 207-8354
            (us & canada)
            (617) 253-2889
            (outside us and canada)
            (617) 577-1545
            fax
            mit press business hours are m-f, 9:00 a.m. - 5:00 p.m.
            eastern time
            connect
               o [61]facebook
               o [62]twitter
               o [63]google +
               o [64]pinterest
               o [65]instagram
               o [66]youtube
          + [67]send us a message
         quick email links
               o [68]change your address
               o [69]advertising inquiries
               o [70]list exchange & purchasing
               o [71]rights & permissions
               o [72]payments & wire transfer
               o [73]license agreements & consortia
               o [74]subscriptions & back issues
               o [75]electronic access queries
            to submit proposals to either launch new journals or bring an
            existing journal to mit press, please contact director for
            journals and open access, [76]nick lindsay. to submit an
            article please follow the submission guidelines for the
            appropriate journal(s).
          +

   [77]sign in / register
     * [78]enter words / phrases / doi / isbn / authors / keywords / etc.
       computational lingui _______________ _______________
       _______________ _______________
       ____________________ (button)
       search [79]advanced search
     *
     * 0

     * [80]home >
     * [81]computational linguistics >
     * [82]list of issues >
     * [83]volume 41 , no. 3 >
     * codra: a novel discriminative framework for rhetorical analysis

   article navigation
   [84]previous [85]next
   publication cover

more about computational linguistics

   [86][arrow-button-1488499533633.svg]

journal resources

   [87]editorial info
   [88]abstracting and indexing
   [89]release schedule
   [90]advertising info
     __________________________________________________________________

author resources

   [91]submission guidelines
   [92]publication agreement
   [93]author reprints
     __________________________________________________________________

reader resources

   [94]rights and permissions
   [95]most read
   [96]most cited

metrics

   [97][arrow-button-1488499533633.svg]

article metrics

altmetric

   about article usage data:

   lorem ipsum dolor sit amet, consectetur adipiscing elit. aenean euismod
   bibendum laoreet. proin gravida dolor sit amet lacus accumsan et
   viverra justo commodo. proin sodales pulvinar tempor. cum sociis
   natoque penatibus et magnis dis parturient montes, nascetur ridiculus
   mus.

open access

   [98][arrow-button-1488499533633.svg]
   [open-access-1493356222797.svg]

   computational linguistics computational linguistics is open access. all
   content is freely available in electronic format (full text html, pdf,
   and pdf plus) to readers across the globe. all articles are published
   under a [99]cc by-nc-nd 4.0 license. for more information on allowed
   uses, please view the cc license.
   [100]support oa at mitp

codra: a novel discriminative framework for rhetorical analysis

   [101]shafiq joty, [102]giuseppe carenini and [103]raymond t. ng
   posted online september 08, 2015
   [104]https://doi.org/10.1162/coli_a_00226

computational linguistics

   volume 41 | issue 3 | september 2015
   p.385-435
   [105][preview-1489555631417.svg] download options
   [arrow-button-1488499533633.svg]

codra: a novel discriminative framework for rhetorical analysis

   [106]shafiq joty, [107]giuseppe carenini and [108]raymond t. ng
   [109]https://doi.org/10.1162/coli_a_00226
   received: may 11, 2014
   accepted: march 18, 2015
   published online: september 08, 2015
     * [110]full text
     * [111]authors
     * [112]pdf
     * [113]pdf plus

abstract

   section:
   [choose________________________]
   [114]next section

   clauses and sentences rarely stand on their own in an actual discourse;
   rather, the relationship between them carries important information
   that allows the discourse to express a meaning as a whole beyond the
   sum of its individual parts. rhetorical analysis seeks to uncover this
   coherence structure. in this article, we present codra    a complete
   probabilistic discriminative framework for performing rhetorical
   analysis in accordance with rhetorical structure theory, which posits a
   tree representation of a discourse.

   codra comprises a discourse segmenter and a discourse parser. first,
   the discourse segmenter, which is based on a binary classifier,
   identifies the elementary discourse units in a given text. then the
   discourse parser builds a discourse tree by applying an optimal parsing
   algorithm to probabilities inferred from two id49:
   one for intra-sentential parsing and the other for multi-sentential
   parsing. we present two approaches to combine these two stages of
   parsing effectively. by conducting a series of empirical evaluations
   over two different data sets, we demonstrate that codra significantly
   outperforms the state-of-the-art, often by a wide margin. we also show
   that a reranking of the k-best parse hypotheses generated by codra can
   potentially improve the accuracy even further.
   no rights reserved. this work was authored as part of the contributor's
   official duties as an employee of the united states government and is
   therefore a work of the united states government. in accordance with 17
   u.s.c. 105, no copyright protection is available for such works under
   u.s. law.
   1.   introduction
   section:
   [choose________________________]
   [115]previous section [116]next section

   a well-written text is not merely a sequence of independent and
   isolated sentences, but instead a sequence of structured and related
   sentences, where the meaning of a sentence relates to the previous and
   the following ones. in other words, a well-written text has a coherence
   structure (halliday and hasan [117]1976; hobbs [118]1979), which
   logically binds its clauses and sentences together to express a meaning
   as a whole. rhetorical analysis seeks to uncover this coherence
   structure underneath the text; this has been shown to be beneficial for
   many natural language processing (nlp) applications, including text
   summarization and compression (marcu [119]2000b; daum   and marcu
   [120]2002; sporleder and lapata [121]2005; louis, joshi, and nenkova
   [122]2010), text generation (prasad et al. [123]2005), machine
   translation evaluation (guzm  n et al. [124]2014a, [125]2014b; joty et
   al. [126]2014), id31 (somasundaran [127]2010; lazaridou,
   titov, and sporleder [128]2013), information extraction (teufel and
   moens [129]2002; maslennikov and chua [130]2007), and question
   answering (verberne et al. [131]2007). furthermore, rhetorical
   structures can be useful for other discourse analysis tasks, including
   co-reference resolution using veins theory (cristea, ide, and romary
   [132]1998).

   different formal theories of discourse have been proposed from
   different view-points to describe the coherence structure of a text.
   for example, martin ([133]1992) and knott and dale ([134]1994) propose
   discourse relations based on the usage of discourse connectives (e.g.,
   because, but) in the text. asher and lascarides ([135]2003) propose
   segmented discourse representation theory, which is driven by sentence
   semantics. webber ([136]2004) and danlos ([137]2009) extend sentence
   grammar to formalize discourse structure. rhetorical structure theory
   (rst), proposed by mann and thompson ([138]1988), is perhaps the most
   influential theory of discourse in computational linguistics. although
   it was initially intended to be used in text generation, later it
   became popular as a framework for parsing the structure of a text
   (taboada and mann [139]2006). rst represents texts by labeled
   hierarchical structures, called discourse trees (dts). for example,
   consider the dt shown in [140]figure 1 for the following text:

   but he added:    some people use the purchasers' index as a leading
   indicator, some use it as a coincident indicator. but the thing it's
   supposed to measure   manufacturing strength   it missed altogether last
   month.   
   [141]figure
   figure   1    discourse tree for two sentences in rst   dt. each sentence
   contains three edus. horizontal lines indicate text segments;
   satellites are connected to their nuclei by curved arrows and two
   nuclei are connected with straight lines.

   the leaves of a dt correspond to contiguous atomic text spans, called
   elementary discourse units (edus; six in the example). edus are
   clause-like units that serve as building blocks. adjacent edus are
   connected by coherence relations (e.g., elaboration, contrast), forming
   larger discourse units (represented by internal nodes), which in turn
   are also subject to this relation linking. discourse units linked by a
   rhetorical relation are further distinguished based on their relative
   importance in the text: nuclei are the core parts of the relation and
   satellites are peripheral or supportive ones. for example, in
   [142]figure 1, elaboration is a relation between a nucleus (edu 4) and
   a satellite (edu 5), and contrast is a relation between two nuclei
   (edus 2 and 3). carlson, marcu, and okurowski ([143]2002) constructed
   the first large rst-annotated corpus (rst   dt) on wall street journal
   articles from the id32. whereas mann and thompson ([144]1988)
   had suggested about 25 relations, the rst   dt uses 53 mono-nuclear and
   25 multi-nuclear relations. the relations are grouped into 16
   coarse-grained categories; see carlson and marcu ([145]2001) for a
   detailed description of the relations. conventionally, rhetorical
   analysis in rst involves two subtasks: discourse segmentation is the
   task of breaking the text into a sequence of edus, and discourse
   parsing is the task of linking the discourse units (edus and larger
   units) into a labeled tree. in this article, we use the terms discourse
   parsing and rhetorical parsing interchangeably.

   while recent advances in automatic discourse segmentation have attained
   high accuracies (an f-score of 90.5% reported by fisher and roark
   [[146]2007]), discourse parsing still poses significant challenges
   (feng and hirst [147]2012) and the performance of the existing
   discourse parsers (soricut and marcu [148]2003; subba and di-eugenio
   [149]2009; hernault et al. [150]2010) is still considerably inferior
   compared with the human gold standard. thus, the impact of rhetorical
   structure in downstream nlp applications is still very limited. the
   work we present in this article aims to reduce this performance gap and
   take discourse parsing one step further. to this end, we address three
   key limitations of existing discourse parsers.

   first, existing discourse parsers typically model the structure and the
   labels of a dt separately, and also do not take into account the
   sequential dependencies between the dt constituents. however, for
   several nlp tasks, it has recently been shown that joint models
   typically outperform independent or pipeline models (murphy [151]2012,
   page 687). this is also supported in a recent study by feng and hirst
   ([152]2012), in which the performance of a greedy bottom   up discourse
   parser improved when sequential dependencies were considered by using
   gold annotations for the neighboring (i.e., previous and next)
   discourse units as contextual features in the parsing model. to address
   this limitation of existing parsers, as the first contribution, we
   propose a novel discourse parser based on probabilistic discriminative
   parsing models, expressed as id49 (crfs) (sutton,
   mccallum, and rohanimanesh [153]2007), to infer the id203 of all
   possible dt constituents. the crf models effectively represent the
   structure and the label of a dt constituent jointly, and, whenever
   possible, capture the sequential dependencies.

   second, existing discourse parsers typically apply greedy and
   sub-optimal parsing algorithms to build a dt. to cope with this
   limitation, we use the inferred (posterior) probabilities from our crf
   parsing models in a probabilistic cky-like bottom   up parsing algorithm
   (jurafsky and martin [154]2008), which is non-greedy and optimal.
   furthermore, a simple modification of this parsing algorithm allows us
   to generate k-best (i.e., the k highest id203) parse hypotheses
   for each input text that could then be used in a reranker to improve
   over the initial ranking using additional (global) features of the
   discourse tree as evidence, a strategy that has been successfully
   explored in syntactic parsing (charniak and johnson [155]2005; collins
   and koo [156]2005).

   third, most of the existing discourse parsers do not discriminate
   between intra-sentential parsing (i.e., building the dts for the
   individual sentences) and multi-sentential parsing (i.e., building the
   dt for the whole document). however, we argue that distinguishing
   between these two parsing conditions can result in more effective
   parsing. two separate parsing models could exploit the fact that
   rhetorical relations are distributed differently intra-sententially
   versus multi-sententially. also, they could independently choose their
   own informative feature sets. as another key contribution of our work,
   we devise two different parsing components: one for intra-sentential
   parsing, the other for multi-sentential parsing. this provides for
   scalable, modular, and flexible solutions that can exploit the strong
   correlation observed between the text structure (i.e., sentence
   boundaries) and the structure of the discourse tree.

   in order to develop a complete and robust discourse parser, we combine
   our intra-sentential and multi-sentential parsing components in two
   different ways. because most sentences have a well-formed discourse
   sub-tree in the full dt (e.g., the second sentence in [157]figure 1),
   our first approach constructs a dt for every sentence using our
   intra-sentential parser, and then runs the multi-sentential parser on
   the resulting sentence-level dts to build a complete dt for the whole
   document. however, this approach would fail in those cases where
   discourse structures violate sentence boundaries, also called    leaky   
   boundaries (vliet and redeker [158]2011). for example, consider the
   first sentence in [159]figure 1. it does not have a well-formed
   discourse sub-tree because the unit containing edus 2 and 3 merges with
   the next sentence and only then is the resulting unit merged with edu
   1. our second approach, in order to deal with these leaky cases, builds
   sentence-level sub-trees by applying the intra-sentential parser on a
   sliding window covering two adjacent sentences and by then
   consolidating the results produced by overlapping windows. after that,
   the multi-sentential parser takes all these sentence-level sub-trees
   and builds a full dt for the whole document.

   our discourse parser assumes that the input text has already been
   segmented into elementary discourse units. as an additional
   contribution, we propose a novel discriminative approach to discourse
   segmentation that not only achieves state-of-the-art performance, but
   also reduces time and space complexities by using fewer features.
   notice that the combination of our segmenter with our parser forms a
   complete probabilistic discriminative framework for rhetorical analysis
   (codra).

   whereas previous systems have been tested on only one corpus, we
   evaluate our framework on texts from two very different genres: news
   articles and instructional how-to manuals. the results demonstrate that
   our approach to discourse parsing provides consistent and statistically
   significant improvements over previous methods both at the sentence
   level and at the document level. the performance of our final system
   compares very favorably to the performance of state-of-the-art
   discourse parsers. finally, the oracle accuracy computed based on the
   k-best parse hypotheses generated by our parser demonstrates that a
   reranker could potentially improve the accuracy further.

   after discussing related work in [160]section 2, we present our
   rhetorical analysis framework in [161]section 3. in [162]section 4, we
   describe our discourse parser. then, in [163]section 5 we present our
   discourse segmenter. the experiments and analysis of results are
   presented in [164]section 6. finally, we summarize our contributions
   with future directions in [165]section 7.
   2.   related work
   section:
   [choose________________________]
   [166]previous section [167]next section

   rhetorical analysis has a long history   dating back to mann and thompson
   ([168]1988), when rst was initially proposed as a useful linguistic
   method for describing natural texts, to more recent attempts to
   automatically extract the rhetorical structure of a given text
   (hernault et al. [169]2010). in this section, we provide a brief
   overview of the computational approaches that follow rst as the theory
   of discourse, and that are related to our work; see the survey by stede
   ([170]2011) for a broader overview that also includes other theories of
   discourse.
   2.1   unsupervised and rule-based approaches

   although the most effective approaches to rhetorical analysis to date
   rely on supervised machine learning methods trained on human-annotated
   data, unsupervised methods have also been proposed, as they do not
   require human-annotated data and can be more easily applied to new
   domains.

   often, discourse connectives like but, because, and although convey
   clear information on the kind of relation linking the two text
   segments. in his early work, marcu ([171]2000a) presented a shallow
   rule-based approach relying on discourse connectives (or cues) and
   surface patterns. he used hand-coded rules, derived from an extensive
   corpus study, to break the text into edus and to build dts for
   sentences first, then for paragraphs, and so on. despite the fact that
   this work pioneered the field of rhetorical analysis, it has many
   limitations. first, identifying discourse connectives is a difficult
   task on its own, because (depending on the usage), the same phrase may
   or may not signal a discourse relation (pitler and nenkova [172]2009).
   for example, but can either signal a contrast discourse relation or can
   simply perform non-discourse acts. second, discourse segmentation using
   only discourse connectives fails to attain high accuracy (soricut and
   marcu [173]2003). third, dt structures do not always correspond to
   paragraph structures; for example, sporleder and lapata ([174]2004)
   report that more than 20% of the paragraphs in the rst   dt corpus
   (carlson, marcu, and okurowski [175]2002) do not correspond to a
   discourse unit in the dt. fourth, discourse cues are sometimes
   ambiguous; for example, but can signal contrast, antithesis and
   concession, and so on.

   finally, a more serious problem with the rule-based approach is that
   often rhetorical relations are not explicitly signaled by discourse
   cues. for example, in rst   dt, marcu and echihabi ([176]2002) found that
   only 61 out of 238 contrast relations and 79 out of 307
   cause   explanation relations were explicitly signaled by cue phrases. in
   the british national corpus, sporleder and lascarides ([177]2008)
   report that half of the sentences lack a discourse cue. other studies
   (schauer and hahn [178]2001; stede [179]2004; taboada [180]2006; subba
   and di-eugenio [181]2009) report even higher figures: about 60% of
   discourse relations are not explicitly signaled. therefore, rather than
   relying on hand-coded rules based on discourse cues and surface
   patterns, recent approaches use machine learning techniques with a
   large set of informative features.

   while some rhetorical relations need to be explicitly signaled by
   discourse cues (e.g., concession) and some do not (e.g., background),
   there is a large middle ground of relations that may be signaled or
   not. for these    middle ground    relations, can we exploit features
   present in the signaled cases to automatically identify relations when
   they are not explicitly signaled? the idea is to use unambiguous
   discourse cues (e.g., although for contrast, for example for
   elaboration) to automatically label a large corpus with rhetorical
   relations that could then be used to train a supervised model.[182]^1

   a series of previous studies have explored this idea. marcu and
   echihabi ([183]2002) first attempted to identify four broad classes of
   relations: contrast, elaboration, condition, and
   cause   explanation   evidence. they used a naive bayes classifier based on
   word pairs (w[1], w[2]), where w[1] occurs in the left segment, and
   w[2]occurs in the right segment. sporleder and lascarides ([184]2005)
   included other features (e.g., words and their stems, part-of-speech
   [pos] tags, positions, segment lengths) in a boosting-based classifier
   (i.e., boostexter [schapire and singer [185]2000]) to further improve
   relation classification accuracy. however, these studies evaluated
   classification performance on the instances where rhetorical relations
   were originally signaled (i.e., the discourse cues were artificially
   removed), and did not verify how well this approach performs on the
   instances that are not originally signaled. subsequent studies
   (blair-goldensohn, mckeown, and rambow [186]2007; sporleder [187]2007;
   sporleder and lascarides [188]2008) confirm that classifiers trained on
   instances stripped of their original discourse cues do not generalize
   well to implicit cases because they are linguistically quite different.

   note that this approach to identifying discourse relations in the
   absence of manually labeled data does not fully solve the parsing
   problem (i.e., building dts); rather, it only attempts to identify a
   small subset of coarser relations between two (flat) text segments
   (i.e., a tagging problem). arguably, to perform a complete rhetorical
   analysis, one needs to use supervised machine learning techniques based
   on human-annotated data.
   2.2   supervised approaches

   marcu ([189]1999) applies supervised machine learning techniques to
   build a discourse segmenter and a shift   reduce discourse parser. both
   the segmenter and the parser rely on c4.5 decision tree classifiers
   (poole and mackworth [190]2010) to learn the rules automatically from
   the data. the discourse segmenter mainly uses discourse cues,
   shallow-syntactic (i.e., pos tags) and contextual features (i.e.,
   neighboring words and their pos tags). to learn the shift   reduce
   actions, the discourse parser encodes five types of features: lexical
   (e.g., discourse cues), shallow-syntactic, textual similarity,
   operational (previous n shift   reduce operations), and rhetorical
   sub-structural features. despite the fact that this work has pioneered
   many of today's machine learning approaches to discourse parsing, it
   has all the limitations mentioned in [191]section 1.

   the work of marcu ([192]1999) is considerably improved by soricut and
   marcu ([193]2003). they present the publicly available spade
   system,[194]^2 which comes with probabilistic models for discourse
   segmentation and sentence-level discourse parsing. their segmentation
   and parsing models are based on lexico-syntactic patterns (or features)
   extracted from the lexicalized syntactic tree of a sentence. the
   discourse parser uses an optimal parsing algorithm to find the most
   probable dt structure for a sentence. spade was trained and tested on
   the rst   dt corpus. this work, by showing empirically the connection
   between syntax and discourse structure at the sentence level, has
   greatly influenced all major contributions in this area ever since.
   however, it is limited in several ways. first, spade does not produce a
   full-text (i.e., document-level) parse. second, it applies a generative
   parsing model based on only lexico-syntactic features, whereas
   discriminative models are generally considered to be more accurate, and
   can incorporate arbitrary features more effectively (murphy [195]2012).
   third, the parsing model makes an independence assumption between the
   label and the structure of a dt constituent, and it ignores the
   sequential and the hierarchical dependencies between the dt
   constituents.

   subsequent research addresses the question of how much syntax one
   really needs in rhetorical analysis. sporleder and lapata ([196]2005)
   focus on the discourse chunking problem, comprising two subtasks:
   discourse segmentation and (flat) nuclearity assignment. they formulate
   discourse chunking in two alternative ways. first, one-step
   classification, where the discourse chunker, a multi-class classifier,
   assigns to each token one of the four labels: (1) b   nuc (beginning of a
   nucleus), (2) i   nuc (inside a nucleus), (3) b    sat (beginning of a
   satellite), and (4) i   sat (inside a satellite). therefore, this
   approach performs discourse segmentation and nuclearity assignment
   simultaneously. second, two-step classification, where in the first
   step, the discourse segmenter (a binary classifier) labels each token
   as either b (beginning of an edu) or i (inside an edu). then, in the
   second step, a nuclearity labeler (another binary classifier) assigns a
   nuclearity status to each segment. the two-step approach avoids illegal
   chunk sequences like a b   nuc followed by an i   sat or a b   sat followed
   by an i   nuc, and in this approach, it is easier to incorporate
   sentence-level properties like the constraint that a sentence must
   contain at least one nucleus. they examine whether shallow-syntactic
   features (e.g., pos and phrase tags) would be sufficient for these
   purposes. the evaluation on the rst   dt shows that the two-step approach
   outperforms the one-step approach, and its performance is comparable to
   that of spade, which requires relatively expensive full syntactic
   parses.

   in follow   up work, fisher and roark ([197]2007) demonstrate over 4%
   absolute performance gain in discourse segmentation, by combining the
   features extracted from the syntactic tree with the ones derived via
   id52 and shallow syntactic parsing (i.e., chunking). using quite
   a large number of features in a binary log-linear model, they achieve
   state-of-the-art performance in discourse segmentation on the rst   dt
   test set.

   in a different approach, regneri, egg, and koller ([198]2008) propose
   to use underspecified discourse representation (udr) as an intermediate
   representation for discourse parsing. underspecified representations
   offer a single compact representation to express possible ambiguities
   in a linguistic structure, and have been primarily used to deal with
   scope ambiguity in semantic structures (reyle [199]1993; egg, koller,
   and niehren [200]2001; althaus et al. [201]2003; koller, regneri, and
   thater [202]2008). assuming that a udr of a dt is already given in the
   form of a dominance graph (althaus et al. [203]2003), regneri, egg, and
   koller ([204]2008) convert it into a more expressive and complete udr
   representation called regular tree grammar (koller, regneri, and thater
   [205]2008), for which efficient algorithms (knight and graehl
   [206]2005) already exist to derive the best configuration (i.e., the
   best discourse tree).

   hernault et al. ([207]2010) present the publicly available hilda
   system,[208]^3 which comes with a discourse segmenter and a parser
   based on support vector machines (id166s). the discourse segmenter is a
   binary id166 classifier that uses the same lexico-syntactic features used
   in spade, but with more context (i.e., the lexico-syntactic features
   for the previous two words and the following two words). the discourse
   parser iteratively uses two id166 classifiers in a pipeline to build a
   dt. in each iteration, a binary classifier first decides which of the
   adjacent units to merge, then a multi-class classifier connects the
   selected units with an appropriate relation label. using this simple
   method, they report promising results in document-level discourse
   parsing on the rst   dt.

   for a different genre, instructional texts, subba and di-eugenio
   ([209]2009) propose a shift   reduce discourse parser that relies on a
   classifier for relation labeling. their classifier uses inductive logic
   programming (ilp) to learn id85 rules from a large set of
   features including the linguistically rich id152
   coming from a semantic parser. they demonstrate that including
   id152 with other features improves the performance of
   the classifier, thus, also improves the performance of the parser.

   both hilda and the ilp-based approach of subba and di-eugenio
   ([210]2009) are limited in several ways. first, they do not
   differentiate between intra- and multi-sentential parsing, and both
   scenarios use a single uniform parsing model. second, they take a
   greedy (i.e., sub-optimal) approach to construct a dt. third, they
   disregard sequential dependencies between dt constituents. furthermore,
   hilda considers the structure and the labels of a dt separately. our
   discourse parser codra, as described in the next section, addresses all
   these limitations.

   more recent work than ours also attempts to address some of the
   above-mentioned limitations of the existing discourse parsers. similar
   to us, feng and hirst ([211]2014) generate a document-level dt in two
   stages, where a multi-sentential parsing follows an intra-sentential
   one. at each stage, they iteratively use two separate linear-chain crfs
   (lafferty, mccallum, and pereira [212]2001) in a cascade: one for
   predicting the presence of rhetorical relations between adjacent
   discourse units in a sequence, and the other to predict the relation
   label between the two most probable adjacent units to be merged as
   selected by the previous crf. while they use crfs to take into account
   the sequential dependencies between dt constituents, they use them
   greedily during parsing to achieve efficiency. they also propose a
   greedy post-editing step based on an additional feature (i.e., depth of
   a discourse unit) to modify the initial dt, which gives them a
   significant gain in performance. in a different approach, li et al.
   ([213]2014) propose a discourse-level dependency structure to capture
   direct relationships between edus rather than deep hierarchical
   relationships. they first create a discourse dependency treebank by
   converting the deep annotations in rst   dt to shallow head-dependent
   annotations between edus. to find the dependency parse (i.e., an
   optimal spanning tree) for a given text, they apply eisner ([214]1996)
   and maximum spanning tree (mcdonald et al. [215]2005) dependency
   parsing algorithms with the margin infused relaxed algorithm online
   learning framework (mcdonald, crammer, and pereira [216]2005).

   with the successful application of deep learning to numerous nlp
   problems including syntactic parsing (socher et al. [217]2013a),
   id31 (socher et al. [218]2013b), and various tagging
   tasks (collobert et al. [219]2011), a couple of recent studies in
   discourse parsing also use deep neural networks (dnns) and related
   feature representation methods. inspired by the work of socher et al.
   ([220]2013a, [221]2013b), li, li, and hovy ([222]2014) propose a
   recursive dnn for discourse parsing. however, as in socher et al.
   ([223]2013a, [224]2013b), word vectors (i.e., embeddings) are not
   learned explicitly for the task, rather they are taken from collobert
   et al. ([225]2011). given the vectors of the words in an edu, their
   model first composes them hierarchically based on a syntactic parse
   tree to get the vector representation for the edu. adjacent discourse
   units are then merged hierarchically to get the vector representations
   for the higher order discourse units. in every step, the merging is
   done using one binary (structure) and one multi-class (relation)
   classifier, each having a three-layer neural network architecture. the
   cost function for training the model is given by these two cascaded
   classifiers applied at different levels of the dt. similar to our
   method, they use the classifier probabilities in a cky-like parsing
   algorithm to find the global optimal dt. finally, ji and eisenstein
   ([226]2014) present a feature representation learning method in a
   shift   reduce discourse parser (marcu [227]1999). unlike dnns, which
   learn non-linear feature transformations in a maximum likelihood model,
   they learn linear transformations of features in a max margin
   classification model.
   3.   overview of our rhetorical analysis framework
   section:
   [choose___________________________]
   [228]previous section [229]next section

   codra takes as input a raw text and produces a discourse tree that
   describes the text in terms of coherence relations that hold between
   adjacent discourse units (i.e., clauses, sentences) in the text. an
   example dt generated by an online demo of codra is shown in
   [230]appendix a.[231]^4 the color of a node represents its nuclearity
   status: blue denoting nucleus and yellow denoting satellite. the demo
   also allows some useful user interactions   for example, collapsing or
   expanding a node, highlighting an edu, and so on.[232]^5

   codra follows a pipeline architecture, shown in [233]figure 2. given a
   raw text, the first task in the rhetorical analysis pipeline is to
   break the text into a sequence of edus (i.e., discourse segmentation).
   because it is taken for granted that sentence boundaries are also edu
   boundaries (i.e., edus do not span across multiple sentences), the
   discourse segmentation task boils down to finding edu boundaries inside
   sentences. codra uses a maximum id178 model for discourse
   segmentation (see [234]section 5).
   [235]figure
   figure   2    codra architecture.

   once the edus are identified, the discourse parsing problem is
   determining which discourse units (edus or larger units) to relate
   (i.e., the structure), and what relations (i.e., the labels) to use in
   the process of building the dt. specifically, discourse parsing
   requires: (1) a parsing model to explore the search space of possible
   structures and labels for their nodes, and (2) a parsing algorithm for
   selecting the best parse tree(s) among the candidates. a probabilistic
   parsing model like ours assigns a id203 to every possible dt. the
   parsing algorithm then picks the most probable dts.

   the existing discourse parsers (marcu [236]1999; soricut and marcu
   [237]2003; subba and di-eugenio [238]2009; hernault et al. [239]2010)
   described in [240]section 2 use parsing models that disregard the
   structural interdependencies between the dt constituents. however, we
   hypothesize that, like syntactic parsing, discourse parsing is also a
   id170 problem, which involves predicting multiple
   variables (i.e., the structure and the relation labels) that depend on
   each other (smith [241]2011). recently, feng and hirst ([242]2012) also
   found these interdependencies to be critical for parsing performance.
   to capture the structural dependencies between the dt constituents,
   codra uses undirected conditional id114 (i.e., crfs) as its
   parsing models.

   to find the most probable dt, unlike most previous studies (marcu
   [243]1999; subba and di-eugenio [244]2009; hernault et al. [245]2010),
   which adopt a greedy solution, codra applies an optimal cky parsing
   algorithm to the inferred posterior probabilities (obtained from the
   crfs) of all possible dt constituents. furthermore, the parsing
   algorithm allows codra to generate a list of k-best parse hypotheses
   for a given text.

   note that the way crfs and cky are used in codra is quite different
   from the way they are used in syntactic parsing. for example, in the
   crf-based constituency parsing proposed by finkel, kleeman, and manning
   ([246]2008), the id155 distribution of a parse tree
   given a sentence decomposes across factors defined over productions,
   and the standard inside   outside algorithm is used for id136 on
   possible trees. in contrast, codra first uses the standard
   forward   backward algorithm in a    fat    chain structured[247]^6 crf (to
   be discussed in [248]section 4.1.1) to compute the posterior
   probabilities of all possible dt constituents for a given text (i.e.,
   edus); then it uses a cky parsing algorithm to combine those
   probabilities and find the most probable dt.

   another crucial question related to parsing models is whether to use a
   single model or two different models for parsing at the sentence-level
   (i.e., intra-sentential) and at the document-level (i.e.,
   multi-sentential). a simple and straightforward strategy would be to
   use a single unified parsing model for both intra- and multi-sentential
   parsing without distinguishing the two cases, as was previously done
   (marcu [249]1999; subba and di-eugenio [250]2009; hernault et al.
   [251]2010). that approach has the advantages of making the parsing
   process easier, and the model gets more data to learn from. however,
   for a solution like ours, which tries to capture the interdependencies
   between constituents, this would be problematic with respect to
   scalability and inappropriate because of two modeling issues.

   more specifically, for scalability note that the number of valid trees
   grows exponentially with the number of edus in a document.[252]^7
   therefore, an exhaustive search over all the valid dts is often
   infeasible, even for relatively small documents.

   for modeling, a single unified approach is inappropriate for two
   reasons. on the one hand, it appears that discourse relations are
   distributed differently intra- versus multi-sententially. for example,
   [253]figure 3 shows a comparison between the two distributions of the
   eight most frequent relations in the rst   dt training set. notice that
   same   unit is more frequent than joint in the intra-sentential case,
   whereas joint is more frequent than same   unit in the multi-sentential
   case. similarly, the relative distributions of background, contrast,
   cause, and explanation are different in the two parsing scenarios. on
   the other hand, different kinds of features are applicable and
   informative for intra- versus multi-sentential parsing. for example,
   syntactic features like dominance sets (soricut and marcu [254]2003)
   are extremely useful for parsing at the sentence-level, but are not
   even applicable in the multi-sentential case. likewise, lexical chain
   features (sporleder and lapata [255]2004), which are useful for
   multi-sentential parsing, are not applicable at the sentence level.
   [256]figure
   figure   3    distributions of the eight most frequent relations in
   intra-sentential and multi-sentential parsing scenarios on the rst   dt
   training set.

   based on these above observations, codra comprises two separate
   modules: an intra-sentential parser and a multi-sentential parser, as
   shown in [257]figure 2. first, the intra-sentential parser produces one
   or more discourse sub-trees for each sentence. then, the
   multi-sentential parser generates a full dt for the document from these
   sub-trees. both of our parsers have the same two components: a parsing
   model and a parsing algorithm. whereas the two parsing models are
   rather different, the same parsing algorithm is shared by the two
   modules. staging multi-sentential parsing on top of intra-sentential
   parsing in this way allows codra to explicitly exploit the strong
   correlation observed between the text structure and the dt structure,
   as explained in detail in [258]section 4.3.
   4.   the discourse parser
   section:
   [choose________________________]
   [259]previous section [260]next section

   before describing the parsing models and the parsing algorithm of codra
   in detail, we introduce some terminology that we will use throughout
   this article.

   a dt can be formally represented as a set of constituents of the form
   r[i, m, j], where i     m < j. this refers to a rhetorical relation r
   between the discourse unit containing edus i through m and the
   discourse unit containing edus m+1 through j. for example, the dt for
   the second sentence in [261]figure 1 can be represented as
   {elaboration   ns[4,4,5], same   unit   nn[4,5,6]}. notice that in this
   representation, a relation r also specifies the nuclearity status of
   the discourse units involved, which can be one of nucleus   satellite
   (ns), satellite   nucleus (sn), or nucleus   nucleus (nn). attaching
   nuclearity status to the relations allows us to perform the two
   subtasks of discourse parsing, relation identification and nuclearity
   assignment, simultaneously.

   a common assumption made for generating dts effectively is that they
   are binary trees (soricut and marcu [262]2003; hernault et al.
   [263]2010). that is, multi-nuclear relations (e.g., joint, same   unit)
   involving more than two discourse units are mapped to a hierarchical
   right-branching binary tree. for example, a flat joint(e[1], e[2],
   e[3], e[4]) ([264]figure 4a) is mapped to a right-branching binary tree
   joint(e[1], joint(e[2], joint(e[3], e[4]))) ([265]figure 4b).
   [266]figure
   figure   4    multi-nuclear relation and its corresponding binary tree
   representation.
   4.1   parsing models

   as mentioned before, the job of the intra- and multi-sentential parsing
   models of codra is to assign a id203 to each of the constituents
   of all possible dts at the sentence level and at the document level,
   respectively. formally, given the model parameters    at a particular
   parsing scenario (i.e., sentence-level or document-level), for each
   possible constituent r[i, m, j] in a candidate dt at that parsing
   scenario, the parsing model estimates p(r[i, m, j]|  ), which specifies
   a joint distribution over the label r and the structure [i, m, j] of
   the constituent. for example, when applied to the sentences in
   [267]figure 1 separately, the intra-sentential parsing model (with
   learned parameters   [s]) estimates p(r[1, 1, 2]|  [s]), p(r[2, 2,
   3]|  [s]), p(r[1, 2, 3]|  [s]), and p(r[1, 1, 3]|  [s]) for the first
   sentence, and p(r[4, 4, 5]|  [s]), p(r[5, 5, 6]|  [s]), p(r[4, 5,
   6]|  [s]), and p(r[4, 4, 6]|  [s]) for the second sentence, respectively,
   for all r ranging over the set of relations.
   4.1.1   intra-sentential parsing model

   [268]figure 5 shows the parsing model of codra for intra-sentential
   parsing. the observed nodes u[j] (at the bottom) in a sequence
   represent the discourse units (edus or larger units). the first layer
   of hidden nodes are the structure nodes, where s[j]     {0, 1} denotes
   whether two adjacent discourse units u[j   1] and u[j] should be
   connected or not. the second layer of hidden nodes are the relation
   nodes, with r[j]     {1     m} denoting the relation between two adjacent
   units u[j   1] and u[j], where m is the total number of relations in the
   relation set. the connections between adjacent nodes in a hidden layer
   encode sequential dependencies between the respective hidden nodes, and
   can enforce constraints such as the fact that a node must have a unique
   mother, namely, a s[j] = 1 must not follow a s[j   1] = 1. the
   connections between the two hidden layers model the structure and the
   relation of dt constituents jointly.
   [269]figure
   figure   5    the intra-sentential parsing model of codra.

   notice that the probabilistic graphical model shown in [270]figure 5 is
   a chain-structured undirected graphical model (also known as markov
   random field or mrf [murphy [271]2012]) with two hidden layers, i.e.,
   structure chain and relation chain. it becomes a dynamic conditional
   random field (dcrf) (sutton, mccallum, and rohanimanesh [272]2007) when
   we directly model the hidden (output) variables by conditioning the
   clique potentials (i.e., factors) on the observed (input) variables:

   where {  } and {  } are the factors over the edges of the relation and
   structure chains, respectively, and {  } are the factors over the edges
   connecting the relation and structure nodes (i.e., between-chain
   edges). here, x represents input features extracted from the observed
   variables,   [s] = [  [s,r],   [s,s],   [s,c]] are model parameters, and
   z(x,   [s]) is the partition function. we use the standard log-linear
   representation of the factors:

   where f (y, z, x) is a feature vector derived from the input features x
   and the local labels y and z, and   [s,y] is the corresponding weight
   vector   that is,   [s,r] and   [s,s] are the weight vectors for the
   factors over the relation edges and the structure edges, respectively,
   and   [s,c] is the weight vector for the factors over the between-chain
   edges.

   a dcrf is a generalization of linear-chain crfs (lafferty, mccallum,
   and pereira [273]2001) to represent complex interactions between output
   variables (i.e., labels), such as when performing multiple labeling
   tasks on the same sequence. recently, there has been an explosion of
   interest in crfs for solving structured output classification problems,
   with many successful applications in nlp including syntactic parsing
   (finkel, kleeman, and manning [274]2008), syntactic chunking (sha and
   pereira [275]2003), and discourse chunking (ghosh et al. [276]2011) in
   accordance with the penn discourse treebank (prasad et al. [277]2008).

   dcrfs, being a discriminative approach to sequence modeling, have
   several advantages over their generative counterparts such as hidden
   markov models (id48s) and mrfs, which first model the joint distribution
   p(y, x|  ), and then infer the conditional distribution p(y|x,   ). it
   has been advocated that discriminative models are generally more
   accurate than generative ones because they do not    waste resources   
   modeling complex distributions that are observed (i.e., p(x)); instead,
   they focus directly on modeling what we care about, namely, the
   distribution of labels given the data (murphy [278]2012).

   other key advantages include the ability to incorporate arbitrary
   overlapping local and global features, and the ability to relax strong
   independence assumptions. furthermore, crfs surmount the label bias
   problem (lafferty, mccallum, and pereira [279]2001) of the maximum
   id178 markov model (mccallum, freitag, and pereira [280]2000), which
   is considered to be a discriminative version of the id48.
   4.1.2   training and applying the intra-sentential parsing model

   in order to obtain the id203 of the constituents of all candidate
   dts for a sentence, codra applies the intra-sentential parsing model
   (with learned parameters   [s]) recursively to sequences at different
   levels of the dt, and computes the posterior marginals over the
   relation    structure pairs. it uses the standard forward   backward
   algorithm to compute the posterior marginals. to illustrate the
   process, let us assume that the sentence contains four edus, e[1],     ,
   e[4] (see [281]figure 6). at the first (i.e., bottom) level of the dt,
   when all the discourse units are edus, there is only one unit sequence
   (e[1], e[2], e[3], e[4]) to which codra applies the dcrf model.
   [282]figure 6a at the top left shows the corresponding dcrf model. for
   this sequence it computes the posterior marginals p(r[2], s[2] =
   1|e[1], e[2], e[3], e[4],   [s]), p(r[3], s[3] = 1|e[1], e[2], e[3],
   e[4],   [s]), and p(r[4], s[4] = 1| e[1], e[2], e[3], e[4],   [s]) to
   obtain the id203 of the dt constituents r[1, 1, 2], r[2, 2, 3],
   and r[3, 3, 4], respectively.
   [283]figure
   figure   6    the intra-sentential parsing model is applied to (a) the only
   possible sequence at the first level, (b) the three possible sequences
   at the second level, and (c) the three possible sequences at the third
   level.

   at the second level, there are three unit sequences: (e[1:2], e[3],
   e[4]), (e[1],e[2:3], e[4]), and (e[1],e[2],e[3:4]). [284]figure 6b
   shows their corresponding dcrf models. notice that each of these
   sequences has a discourse unit that connects two edus, and the
   id203 of this connection has already been computed at the
   previous level. codra computes the posterior marginals p(r[3], s[3] =
   1|e[1:2], e[3], e[4],   [s]), p(r[2:3] s[2:3] = 1|e[1], e[2:3], e[4],
     [s]), p(r[4], s[4] = 1|e[1], e[2:3], e[4],   [s]), and p(r[3:4], s[3:4]
   = 1|e[1], e[2], e[3:4],   [s]) from these three sequences, which
   correspond to the id203 of the constituents r[1, 2, 3], r[1, 1,
   3], r[2, 3, 4], and r[2, 2, 4], respectively. similarly, it attains the
   id203 of the constituents r[1, 1, 4], r[1, 2, 4], and r[1, 3, 4]
   by computing their respective posterior marginals from the three
   sequences at the third (i.e., top) level of the candidate dts (see
   [285]figure 6c).

   algorithm 1 describes how codra generates the unit sequences at
   different levels of the candidate dts for a given number of edus in a
   sentence. specifically, to compute the id203 of a dt constituent
   r[i, k, j], codra generates sequences like (e[1],     , e[i   1], e[i:k],
   e[k+1:j], e[j+1],     , e[n]) for 1     i     k < j     n. however, in doing
   so, it may generate some duplicate sequences. clearly, the sequence
   (e[1],     , e[i   1], e[i:i], e[i+1:j], e[j+1],     , e[n]) for 1     i     k <
   j < n is already considered for computing the id203 of the
   constituent r[i + 1, j, j + 1]. therefore, it is a duplicate sequence
   that codra excludes from the list of sequences. the algorithm has a
   complexity of o(n^3), where n is the number of edus in the sentence.

   once codra acquires the id203 of all possible intra-sentential dt
   constituents, the discourse sub-trees for the sentences are built by
   applying an optimal parsing algorithm ([286]section 4.2) using one of
   the methods described in [287]section 4.3.

   algorithm 1 is also used to generate sequences for training the model
   (i.e., learning   [s]). for example, [288]figure 7 demonstrates how we
   generate the training instances (right) from a gold dt with four edus
   (left). to find the relevant labels for the sequences generated by the
   algorithm, we consult the gold dt and see if two discourse units are
   connected by a relation r (i.e., the corresponding labels are s = 1, r
   = r) or not (i.e., the corresponding labels are s = 0, r =nr). we train
   the model by maximizing the conditional likelihood of the labels in
   each of these training examples (see [289]equation (1)).
   [290]figure
   figure   7    a gold discourse tree (left), and the 7 training instances it
   generates (right). nr = no relation.
   4.1.3   multi-sentential parsing model

   given the discourse units (sub-trees) for all the individual sentences
   in a document, a simple approach to build the dt of the document would
   be to apply a new dcrf model, similar to the one in [291]figure 5 (with
   different parameters), to all the possible sequences generated from
   these units by algorithm 1 to infer the id203 of all possible
   higher-order (multi-sentential) constituents. however, the number of
   possible sequences and their length increase with the number of
   sentences in a document. for example, assuming that each sentence has a
   well-formed dt, for a document with n sentences, algorithm 1 generates
   o(n^3) sequences, where the sequence at the bottom level has n units,
   each of the sequences at the second level has n-1 units, and so on.
   because the dcrf model in [292]figure 5 has a    fat    chain structure,
   one could use the forward   backward algorithm for exact id136 in
   this model (murphy [293]2012). forward   backward on a sequence
   containing t units costs o(tm^2) time, where m is the number of
   relations in our relation set. this makes the chain-structured dcrf
   model impractical for multi-sentential parsing of long documents, since
   learning requires running id136 on every training sequence with an
   overall time complexity of o(tm^2 n^3) = o(m^2 n^4) per document
   (sutton and mccallum [294]2012).

   to address this problem, we have developed a simplified parsing model
   for multi-sentential parsing. our model is shown in [295]figure 8. the
   two observed nodes u[t   1] and u[t] are two adjacent (multi-sentential)
   discourse units. the (hidden) structure node s     {0, 1} denotes whether
   the two discourse units should be linked or not. the other hidden node
   r     {1     m} represents the relation between the two units. notice that
   similar to the model in [296]figure 5, this is also an undirected
   graphical model and becomes a crf model if we directly model the labels
   by conditioning the clique potential    on the input features x, derived
   from the observed variables:

   where f (r[t], s[t], x) is a feature vector derived from the input
   features x and the labels r[t] and s[t], and   [d] is the corresponding
   weight vector. although this model is similar in spirit to the parsing
   model in [297]figure 5, it now breaks the chain structure, which makes
   the id136 much faster (i.e., a complexity of o(m^2)). breaking the
   chain structure also allows codra to balance the data for training (an
   equal number of instances with s=1 and s=0), which dramatically reduces
   the learning time of the model.
   [298]figure
   figure   8    the multi-sentential parsing model of codra

   codra applies this parsing model to all possible adjacent units at all
   levels in the multi-sentential case, and computes the posterior
   marginals of the relation   structure pairs p(r[t], s[t] = 1|u[t   1],
   u[t],   [d]) using the forward   backward algorithm to obtain the
   id203 of all possible dt constituents. given the sentence-level
   discourse units, algorithm 2, which is a simplified variation of
   algorithm 1, extracts all possible adjacent discourse units for
   multi-sentential parsing. similar to algorithm 1, algorithm 2 also has
   a complexity of o(n^3), where n is the number of sentence-level
   discourse units.

   both our intra- and multi-sentential parsing models are designed using
   mallet's graphical model toolkit grmm (mccallum [299]2002). in order to
   avoid overfitting, we regularize the crf models with l[2]
   id173 and learn the model parameters using the limited-memory
   bfgs (l-bfgs) fitting algorithm.
   4.1.4   features used in the parsing models

   crucial to parsing performance is the set of features used in the
   parsing models, as summarized in [300]table 1. we categorize the
   features into seven groups and specify which groups are used in what
   parsing model. notice that some of the features are used in both
   models. most of the features have been explored in previous studies
   (e.g., soricut and marcu [301]2003; sporleder and lapata [302]2005;
   hernault et al. [303]2010). however, we improve some of these as
   explained subsequently.

   [304]table
   table   1    features used in our intra- and multi-sentential parsing
   models.

   the features are extracted from two adjacent discourse units u[t   1] and
   u[t]. organizational features encode useful information about text
   organization as shown by duverle and prendinger ([305]2009). we measure
   the length of the discourse units as the number of edus and tokens in
   it. however, in order to better adjust to the length variations, rather
   than computing their absolute numbers in a unit, we choose to measure
   their relative numbers with respect to their total numbers in the two
   units. for example, if the two discourse units under consideration
   contain three edus in total, a unit containing two of the edus will
   have a relative edu number of 0.67. we also measure the distances of
   the units in terms of the number of edus from the beginning and end of
   the sentence (or text in the multi-sentential case). text structural
   features capture the correlation between text structure and rhetorical
   structure by counting the number of sentence and paragraph boundaries
   in the discourse units.

   discourse cues (e.g., because, but), when present, signal rhetorical
   relations between two text segments, and have been used as a primary
   source of information in earlier studies (knott and dale [306]1994;
   marcu [307]2000a). however, recent studies (hernault et al. [308]2010;
   biran and rambow [309]2011) suggest that an empirically acquired
   lexical id165 dictionary is more effective than a fixed list of cue
   phrases, since this approach is domain independent and capable of
   capturing non-lexical cues such as punctuation.

   in order to build a lexical id165 dictionary empirically from the
   training corpus, we extract the first and last n tokens (n   {1, 2, 3})
   of each discourse unit and rank them according to their mutual
   information with the two labels, structure (s) and relation (r). more
   specifically, given an id165 x, we compute its conditional id178 h
   with respect to s and r as follows:[310]^8

   where c(x) is the empirical count of id165 x, and c(x, s, r) is the
   joint empirical count of id165 x with the labels s and r. this is in
   contrast to hilda (hernault et al. [311]2010), which ranks the id165s
   by their frequencies in the training corpus. however, blitzer
   ([312]2008) found mutual information to be more effective than
   frequency as a method for feature selection. intuitively, the most
   informative discourse cues are not only the most frequent, but also the
   ones that are indicative of the labels in the training data. in
   addition to the lexical id165s we also encode the pos tags of the
   first and last n tokens (n   {l, 2, 3}) in a discourse unit as
   shallow-syntactic features in our models.

   lexico-syntactic features dominance sets extracted from the discourse
   segmented lexicalized syntactic tree (ds-lst) of a sentence have been
   shown to be extremely effective for intra-sentential discourse parsing
   in spade (soricut and marcu [313]2003). [314]figure 9a shows the ds-lst
   (i.e., lexicalized syntactic tree with edus identified) for a sentence
   with three edus from the rst   dt corpus, and [315]figure 9b shows the
   corresponding discourse tree. in a ds-lst, each edu except the one with
   the root node must have a head node n[h] that is attached to an
   attachment node n[a] residing in a separate edu. a dominance set d
   (shown at the bottom of [316]figure 9a) contains these attachment
   points (shown in boxes) of the edus in a ds-lst. in addition to the
   syntactic and lexical information of the head and attachment nodes,
   each element in the dominance set also includes a dominance
   relationship between the edus involved; the edu with the attachment
   node dominates (represented by    >   ) the edu with the head node.
   [317]figure
   figure   9    dominance set features for intra-sentential discourse
   parsing.

   soricut and marcu ([318]2003) hypothesize that the dominance set (i.e.,
   lexical heads, syntactic labels, and dominance relationships) carries
   the most informative clues for intra-sentential parsing. for instance,
   the dominance relationship between the edus in our example sentence is
   3 > 1 > 2, which favors the dt structure [1, 1, 2] over [2, 2, 3]. in
   order to extract dominance set features for two adjacent discourse
   units u[t   1] and u[t], containing edus e[i:j] and e[j+1:k],
   respectively, we first compute the dominance set from the ds-lst of the
   sentence. we then extract the element from the set that holds across
   the edus j and j + 1. in our example, for the two units, containing
   edus e[1] and e[2], respectively, the relevant dominance set element is
   (1, efforts/np)>(2, to/s). we encode the syntactic labels and lexical
   heads of n[h] and n[a], and the dominance relationship as features in
   our intra-sentential parsing model.

   lexical chains (morris and hirst [319]1991) are sequences of
   semantically related words that can indicate topical boundaries in a
   text (galley et al. [320]2003; joty, carenini, and ng [321]2013).
   features extracted from lexical chains are also shown to be useful for
   finding paragraph-level discourse structure (sporleder and lapata
   [322]2004). for example, consider the text with four paragraphs (p[1]
   to p[4]) in [323]figure 10a. now, let us assume that there is a lexical
   chain that spans the whole text, skipping paragraphs p[2] and p[3],
   while a second chain only spans p[2] and p[3]. this situation makes it
   more likely that p[2] and p[3] should be linked in the dt before either
   of them is linked with another paragraph. therefore, the dt structure
   in [324]figure 10b should be more likely than the structure in
   [325]figure 10c.
   [326]figure
   figure   10    correlation between lexical chains and discourse structure.
   (a) lexical chains spanning paragraphs. (b) and (c) two possible dt
   structures.

   one challenge in computing lexical chains is that words can have
   multiple senses, and semantic relationships depend on the sense rather
   than the word itself. several methods have been proposed to compute
   lexical chains (barzilay and elhadad [327]1997; hirst and st. onge
   [328]1997; silber and mccoy [329]2002; galley and mckeown [330]2003).
   we follow the state-of-the-art approach proposed by galley and mckeown
   ([331]2003), which extracts lexical chains after performing word sense
   disambiguation (wsd).

   in the preprocessing step, we extract the nouns from the document and
   lemmatize them using id138's built-in morphy function (fellbaum
   [332]1998). then, by looking up in id138 we expand each noun to all
   of its senses, and build a lexical semantic relatedness graph (lsrg)
   (galley and mckeown [333]2003; chali and joty [334]2007). in an lsrg,
   the nodes represent noun-tokens with their candidate senses, and the
   weighted edges between senses of two different tokens represent one of
   the three semantic relations: repetition, synonym, and hypernym. for
   example, [335]figure 11a shows a partial lsrg, where the token bank has
   two possible senses, namely, money bank and river bank. using the money
   bank sense, bank is connected with institution and company by hypernymy
   relations (edges marked with h), and with another bank by a repetition
   relation (edges marked with r). similarly, using the river bank sense,
   it is connected with riverside by a hypernymy relation and with bank by
   a repetition relation. nouns that are not found in id138 are
   considered as proper nouns having only one sense, and are connected by
   only repetition relations.
   [336]figure
   figure   11    extracting lexical chains. (a) a lexical semantic
   relatedness graph (lsrg) for five noun-tokens. (b) resultant graph
   after performing wsd. the box at the bottom shows the lexical chains.

   we use this lsrg first to perform wsd, then to construct lexical
   chains. for wsd, the weights of all edges leaving the nodes under their
   different senses are summed up and the one with the highest score is
   considered to be the right sense for the word-token. for example, if
   repetition and synonymy are weighted equally, and hypernymy is given
   half as much weight as either of them, the score of bank's two senses
   are: 1 + 0.5 + 0.5 = 2 for the sense money bank and 1 + 0.5 = 1.5 for
   the sense river bank. therefore, the selected sense for bank in this
   context is river bank. in case of a tie, we select the sense that is
   most frequent (i.e., the first sense in id138). note that this
   approach to wsd is different from that of sporleder and lapata
   ([337]2004), which takes a greedy approach.

   finally, we prune the graph by only keeping the links that connect
   words with the selected senses. at the end of the process, we are left
   with the edges that form the actual lexical chains. for example,
   [338]figure 11b shows the result of pruning the graph in [339]figure
   11a. the lexical chains extracted from the pruned graph are shown in
   the box at the bottom. following sporleder and lapata ([340]2004), for
   each chain element, we keep track of the location (i.e., sentence id)
   in the text where that element was found, and exclude chains containing
   only one element. given two discourse units, we count the number of
   chains that: hit the two units, exclusively hit the two units, skip
   both units, skip one of the units, start in a unit, and end in a unit.

   we also consider more contextual information by including the above
   features computed for the neighboring adjacent discourse unit pairs in
   the current feature vector. for example, the contextual features for
   units u[t   1] and u[t] include the feature vector computed from u[t   2]
   and u[t   1] and the feature vector computed from u[t] and u[t+1].

   we incorporate hierarchical dependencies between the constituents in a
   dt by rhetorical sub-structural features. for two adjacent units u[t   1]
   and u[t], we extract the roots of the two rhetorical sub-trees. for
   example, the root of the rhetorical sub-tree spanning over edus e[1:2]
   in [341]figure 9b is elaboration   ns. however, extraction of these
   features assumes the presence of labels for the sub-trees, which is not
   the case when we apply the parser to a new text (sentence or document)
   in order to build its dt in a non-greedy fashion. one way to deal with
   this is to loop twice through the parsing process using two different
   parsing models   one trained with the complete feature set, and the other
   trained without the sub-structural features. we first build an initial,
   sub-optimal dt using the parsing model that is trained without the
   sub-structural features. this intermediate dt will now provide labels
   for the sub-structures. next we can build a final, more accurate dt by
   using the complete parsing model. this idea of two-pass discourse
   parsing, where the second pass performs post-editing using additional
   features, has recently been adopted by feng and hirst ([342]2014) in
   their greedy parser.

   one could even continue doing post-editing multiple times until the dt
   converges. however, this could be very time consuming as each
   post-editing pass requires: (1) applying the parsing model to every
   possible unit sequence and computing the posterior marginals for all
   possible dt constituents, and (2) using the parsing algorithm to find
   the most probable dt. recall from our earlier discussion in
   [343]section 4.1.3 that for n discourse units and m rhetorical
   relations, the first step requires o(m^2 n^4) and o(m^2 n^3) for intra-
   and multi-sentential parsing, respectively; we will see in the next
   section that the second step requires o(mn^3). in spite of the
   computational cost, the gain we attained in the subsequent passes was
   not significant for our development set. therefore, we restrict our
   parser to only one-pass post-editing.

   note that in parsing models where the score (i.e., likelihood) of a
   parse tree decomposes across local factors (e.g., the crf-based
   syntactic parser of finkel, kleeman, and manning [[344]2008]), it is
   possible to define a semiring using the factors and the local scores
   (e.g., given by the inside algorithm). the cky algorithm could then
   give the optimal parse tree in a single post-editing pass (smith
   [345]2011). however, because our intra-sentential parsing model is
   designed to capture sequential dependencies between dt constituents,
   the score of a dt does not directly decompose across factors over
   discourse productions. therefore, designing such a semiring was not
   possible in our case.

   in addition to these features, we also experimented with other features
   including id138-based lexical semantics, subjectivity, and
   tf.idf-based cosine similarity. however, because such features did not
   improve parsing performance on our development set, they were excluded
   from our final set of features.
   4.2   parsing algorithm

   the intra- and multi-sentential parsing models of codra assign a
   id203 to every possible dt constituent in their respective
   parsing scenarios. the job of the parsing algorithm is then to find the
   k most probable dts for a given text. we implement a probabilistic
   cky-like bottom   up parsing algorithm that uses id145 to
   compute the most likely parses (jurafsky and martin [346]2008). for
   simplicity, we first describe the specific case of generating the
   single most probable dt, then we describe how to generalize this
   algorithm to produce the k most probable dts for a given text.

   formally, the search problem for finding the most probable dt can be
   written as

   where    specifies the parameters of the parsing model (intra- or
   multi-sentential). given n discourse units, our parsing algorithm uses
   the upper-triangular portion of the n  n id145 table d,
   where cell d[i, j] (for i < j) stores:

   where u[x] (0) and u[x] (1) are the start and end edu ids of discourse
   unit u[x], and

   recall that the notation r[u[i] (0), u[m] (1), u[j] (1)] in this
   expression refers to a rhetorical relation r that holds between the
   discourse unit containing edus u[i] (0) through u[m] (1) and the unit
   containing edus u[m] (1) + 1 through u[j] (1).

   in addition to d, which stores the id203 of the most probable
   constituents of a dt, the algorithm also simultaneously maintains two
   other n  n id145 tables s and r for storing the structure
   (i.e., u[m*] (1)) and the relations (i.e., r*) of the corresponding dt
   constituents, respectively. for example, given four edus e[1]     e[4],
   the s and r id145 tables at the left side in [347]figure
   12 together represent the dt shown at the right. more specifically, to
   find the dt, we first look at the top-right entries in the two tables,
   and find s[1, 4] = 2 and r[1, 4] = r[2], which specify that the two
   discourse units e[1:2] and e[3:4] should be connected by the relation
   r[2] (the root in the dt). then, we see how edus e[1] and e[2] should
   be connected by looking at the entries s[1, 2] and r[1, 2], and find
   s[1, 2] = 1 and r[1, 2] = r[1], which indicates that these two units
   should be connected by the relation r[1] (the left pre-terminal in the
   dt). finally, to see how edus e[3] and e[4] should be linked, we look
   at the entries s[3, 4] and r[3, 4], which tell us that they should be
   linked by the relation r[4] (the right pre-terminal). the algorithm
   works in polynomial time. specifically, for n discourse units and m
   number of relations, the time and space complexities are o(n^3 m) and
   o(n^2), respectively.
   [348]figure
   figure   12    the s and r id145 tables (left), and the
   corresponding discourse tree (right).

   a key advantage of using a probabilistic parsing algorithm like the one
   we use is that it allows us to generate a list of k most probable parse
   trees. it is straightforward to generalize the above algorithm to
   produce k most probable dts. specifically, when filling up the dynamic
   programming tables, rather than storing a single best parse for each
   sub-tree, we store and keep track (i.e., using back-pointers) of k-best
   candidates simultaneously. one can show that the time and space
   complexities of the k-best version of the algorithm are o(n^3 mk^2 log
   k) and o(k^2 n), respectively (huang and chiang [349]2005).

   note that, in contrast to other document-level discourse parsers (marcu
   [350]2000b; subba and di-eugenio [351]2009; hernault et al. [352]2010;
   feng and hirst [353]2012, [354]2014), which use a greedy algorithm,
   codra finds a discourse tree that is globally optimal.[355]^9 this
   approach of codra is also different from the sentence-level discourse
   parser spade (soricut and marcu [356]2003). spade first finds the tree
   structure that is globally optimal, then it assigns the most probable
   relations to the internal nodes. more specifically, the cell d[i, j] in
   spade's id145 table stores

   where . disregarding the relation label r while populating d, this
   approach may find a discourse tree that is not globally optimal.
   4.3   document-level parsing approaches

   now that we have presented our intra-sentential and multi-sentential
   parsing components, we are ready to describe how they can be
   effectively combined in a unified framework ([357]figure 2) to perform
   document-level rhetorical analysis. recall that a key motivation for a
   two-stage[358]^10 parsing is that it allows us to capture the strong
   correlation between text structure and discourse structure in a
   scalable, modular, and flexible way. in the following, we describe two
   different approaches to model this correlation.
   4.3.1   1s   1s (1 sentence   1 sub-tree)

   a key finding from previous studies on sentence-level discourse
   analysis is that most sentences have a well-formed discourse sub-tree
   in the full document-level dt (soricut and marcu [359]2003; fisher and
   roark [360]2007). for example, [361]figure 13a shows 10 edus in three
   sentences (see boxes), where the dts for the sentences obey their
   respective sentence boundaries.
   [362]figure
   figure   13    two possible dts for three sentences.

   our first approach, called 1s   1s (1 sentence   1 sub-tree), aims to
   maximally exploit this finding. it first constructs a dt for every
   sentence using our intra-sentential parser, and then it provides our
   multi-sentential parser with the sentence-level dts to build the
   rhetorical parse for the whole document.
   4.3.2   sliding window

   although the assumption made by 1s   1s clearly simplifies the parsing
   process, it completely ignores the cases where rhetorical structures
   violate sentence boundaries. for example, in the dt shown in
   [363]figure 13b, sentence s[2] does not have a well-formed sub-tree
   because some of its units attach to the left (i.e., 4   5 and 6) and some
   to the right (i.e., 7). vliet and redeker ([364]2011) call these cases
      leaky    boundaries.

   although we find fewer than 5% of the sentences in the rst   dt have
   leaky boundaries, in other corpora this can be true for a larger
   portion of the sentences. for example, we observe that over 12% of the
   sentences in the instructional corpus of subba and di-eugenio
   ([365]2009) have leaky boundaries. however, we notice that in most
   cases where dt structures violate sentence boundaries, its units are
   merged with the units of its adjacent sentences, as in [366]figure 13b.
   for example, this is true for 75% of the leaky cases in our development
   set containing 20 news articles from the rst   dt and for 79% of the
   leaky cases in our development set containing 20 how-to manuals from
   the instructional corpus. based on this observation, we propose a
   sliding window approach.

   in this approach, our intra-sentential parser works with a window of
   two consecutive sentences, and builds a dt for the two sentences. for
   example, given the three sentences in [367]figure 13, our
   intra-sentential parser constructs a dt for s[1]   s[2] and a dt for
   s[2]   s[3]. in this process, each sentence in a document except the
   boundary sentences (i.e., the first and the last) will be associated
   with two dts: one with the previous sentence (say, dt[p]) and one with
   the next (say, dt[n]). in other words, for each non-boundary sentence,
   we will have two decisions: one from dt[p] and one from dt[n]. our
   parser consolidates the two decisions and generates one or more
   sub-trees for each sentence by checking the following three mutually
   exclusive conditions one after another:
         

   same in both: if the sentence under consideration has the same (in both
   structure and labels) well-formed sub-tree in both dt[p] and dt[n], we
   take this sub-tree. for example, in [368]figure 14a, s[2] has the same
   sub-tree in the two dts (one for s[1]   s[2] and one for s[2]   s[3]). the
   two decisions agree on the dt for the sentence.
         

   different but no cross: if the sentence under consideration has a
   well-formed sub-tree in both dt[p] and dt[n], but the two sub-trees
   vary either in structure or in labels, we pick the most probable one.
   for example, consider the dt for s[1]   s[2] (at the left) in [369]figure
   14a and the dt for s[2]   s[3] in [370]figure 14b. in both cases s2 has a
   well-formed sub-tree, but they differ in structure. we pick the
   sub-tree which has the higher id203 in the two dynamic
   programming tables.
         

   cross: if either or both of dt[p] and dt[n] segment the sentence into
   multiple sub-trees, we pick the one having more sub-trees. for example,
   consider the two dts in [371]figure 14c. in the dt for s[1]   s[2] on the
   left, s[2] has three sub-trees (4   5, 6, 7), whereas in the dt for
   s[2]   s[3] on the right, it has two (4   6, 7). so, we extract the three
   sub-trees for s[2] from the first dt. if the sentence has the same
   number of sub-trees in both dt[p] and dt[n], we pick the one with
   higher id203 in the id145 tables. note that our
   choice of picking the dt with more sub-trees is intended to allow the
   parser to find more leaky cases. however, other heuristics are also
   possible. for example, another simple heuristic that one could try is:
   when both dts segment the sentence into multiple sub-trees, pick the
   one with fewer sub-trees, and when only one of the dts segment the
   sentence into multiple sub-trees, pick that one.
   [372]figure
   figure   14    extracting sub-trees for s[2].

   at the end, the multi-sentential parser takes all these sentence-level
   sub-trees for a document, and builds a full rhetorical parse for the
   whole document.
   5.   the discourse segmenter
   section:
   [choose___________________________]
   [373]previous section [374]next section

   our discourse parser assumes that the input text has been already
   segmented into a sequence of edus. however, discourse segmentation is
   also a challenging problem, and previous studies (soricut and marcu
   [375]2003; fisher and roark [376]2007) have identified it as a primary
   source of inaccuracy for discourse parsing. regardless of its
   importance in discourse parsing, discourse segmentation itself can be
   useful in several nlp applications, including sentence compression
   (sporleder and lapata [377]2005) and textual alignment in statistical
   machine translation (stede [378]2011). therefore, in codra, we have
   developed our own discourse segmenter, which not only achieves
   state-of-the-art performance as shown later, but also reduces the time
   complexity by using fewer features.
   5.1   segmentation model

   the discourse segmenter in codra implements a binary classifier to
   decide for each word   token (except the last) in a sentence, whether to
   place an edu boundary after that token. we use a maximum id178 model
   to build a discriminative classifier. more specifically, we use a
   id28 classifier with parameter   :

   where the output y     {0, 1} denotes whether to put an edu boundary
   (i.e., y = 1) or not (i.e., y = 0) after the word   token w, which is
   represented by a feature vector x. in the equation, ber(  ) and sigm(  )
   refer to the bernoulli distribution and the sigmoid (also known as
   logistic) function, respectively. the negative log-likelihood (nll) of
   the model with l[2] id173 for n data points (i.e.,
   word   tokens) is given by

   where y[i] is the gold label for word   token w[i] (represented by
   feature vector x[i]). we learn the model parameters    using the l-bfgs
   fitting algorithm, which is time- and space-efficient. to avoid
   overfitting, we use 5-fold cross validation to learn the id173
   strength parameter    from the training data. we also use a simple
   id112 technique (breiman [379]1996) to deal with the sparsity of
   boundary (i.e., y = 1) tags.

   note that our first attempt at the discourse segmentation task
   implemented a linear-chain crf model (lafferty, mccallum, and pereira
   [380]2001) to capture the sequential dependencies between the tags in a
   discriminative way. however, the binary id28 classifier,
   using the same set of features, not only outperforms the crf model, but
   also reduces time and space complexity. one possible explanation for
   the low performance of the crf model is that markov dependencies
   between tags cannot be effectively captured due to the sparsity of
   boundary tags. also, because we could not balance the data by using
   techniques like id112 in the crf model, this further degrades the
   performance.
   5.2   features used in the segmentation model

   our set of features for discourse segmentation are mostly inspired from
   previous studies but used in a novel way, as we describe here.

   our first subset of features, which we call spade features, includes
   the lexico-syntactic patterns extracted from the lexicalized syntactic
   tree of the given sentence. these features replicate the features used
   in spade's segmenter, but used in a discriminative way. in order to
   decide on an edu boundary after a word   token w[k], we search for the
   lowest constituent in the lexicalized syntactic tree that spans over
   tokens w[i]     w[j] such that i     k < j. the production that expands
   this constituent in the tree, with the potential edu boundary marked,
   forms the primary feature. for instance, to determine the existence of
   an edu boundary after the word efforts in our sample sentence shown in
   [381]figure 9, the production np(efforts)     prp$(its) nns(efforts)    
   s(to) extracted from the lexicalized syntactic tree in [382]figure 9a
   constitutes the primary feature, where     denotes the potential edu
   boundary.

   spade predicts an edu boundary if the relative frequency (i.e., maximum
   likelihood estimate) of a potential boundary given the production in
   the training data is greater than 0.5. if the production has not been
   observed frequently enough, the unlexicalized version of the production
   (e.g., np     prp$ nns     s) is used for prediction. if the unlexicalized
   version is also found to be rare, other variations of the production,
   depending on whether they include the lexical heads and how many
   non-terminals (one or two) they consider before and after the potential
   boundary, are examined one after another (see fisher and roark
   [[383]2007] for details). in contrast, we compute the maximum
   likelihood estimates for a primary production (feature) and its other
   variations, and use those directly as features with/without binarizing
   the values.

   shallow syntactic features like chunk and pos tags have been shown to
   possess valuable clues for discourse segmentation (fisher and roark
   [384]2007; sporleder and lapata [385]2005). for example, it is less
   likely that an edu boundary occurs within a chunk. we annotate the
   tokens of a sentence with chunk and pos tags using the state-of-the-art
   illinois tagger[386]^11 and encode these as features in our model. note
   that the chunker assigns each token a tag using the bio notation, where
   b stands for beginning of a particular phrase (e.g., noun or verb
   phrase), i stands for inside of a particular phrase, and o stands for
   outside of a particular phrase. the rationale for using the illinois
   chunker is that it uses a larger set of tags (23 in total); thus it is
   more informative than most of the other existing taggers, which
   typically use only five tags (b   np, i   np, b   vp, i   vp, and o).

   edus are normally multi-word strings. thus, a token near the beginning
   or end of a sentence is unlikely to be the end of a segment. therefore,
   for each token we include its relative position (i.e., absolute
   position/total number of tokens) in the sentence and distances to the
   beginning and end of the sentence as features.

   it is unlikely that two consecutive tokens are tagged with edu
   boundaries. therefore, we incorporate contextual information for a
   token into our model by including the above features computed for its
   neighboring tokens.

   we also experimented with different id165 (n     {1, 2, 3}) features
   extracted from the token sequence, pos sequence, and chunk sequence.
   however, because such features did not improve segmentation accuracy on
   the development set, they were excluded from our final set of features.
   6.   experiments
   section:
   [choose________________________]
   [387]previous section [388]next section

   in this section we present our experimental results. first, we describe
   the corpora on which the experiments were performed and the evaluation
   metrics used to measure the performance of the discourse segmenter and
   the parser. then we show the performance of our discourse segmenter,
   followed by the performance of our discourse parser.
   6.1   corpora

   whereas previous studies on discourse analysis only report their
   results on a particular corpus, to demonstrate the generality of our
   method, we experiment with texts from two very different genres: news
   articles and instructional how-to manuals.

   our first corpus is the standard rst   dt (carlson, marcu, and okurowski
   [389]2002), which contains discourse annotations for 385 wall street
   journal news articles taken from the id32 corpus (marcus,
   marcinkiewicz, and santorini [390]1994). the corpus is partitioned into
   a training set of 347 documents and a test set of 38 documents. a total
   of 53 documents selected from both training and test sets were
   annotated by two human annotators. we measure human agreements based on
   this doubly annotated data set. we used 25 documents from the training
   set as our development set. in rst   dt, the original 25 rhetorical
   relations defined by mann and thompson ([391]1988) are further divided
   into a set of 18 coarser relation classes with 78 finer-grained
   relations (see carlson and marcu [[392]2001] for details). our second
   corpus is the instructional corpus prepared by subba and di-eugenio
   ([393]2009), which contains discourse annotations for 176 how-to
   manuals on home repair. the corpus was annotated with 26 informational
   relations (e.g., preparation   act, act   goal).

   for our experiments with the intra-sentential discourse parser, we
   extracted a sentence-level dt from a document-level dt by finding the
   sub-tree that exactly spans over the sentence. in rst   dt, by our count,
   7, 321 out of 7, 673 sentences in the training set, 951 out of 991
   sentences in the test set, and 1, 114 out of 1, 208 sentences in the
   doubly-annotated set have a well-formed dt. on the other hand, 3, 032
   out of 3, 430 sentences in the instructional corpus have a well-formed
   dt. this forms the corpora for our experiments with intra-sentential
   discourse parsing. however, the existence of a well-formed dt is not a
   necessity for discourse segmentation; therefore, we do not exclude any
   sentence in our discourse segmentation experiments.
   6.2   evaluation (and agreement) metrics

   in this subsection we describe the metrics used to measure both how
   much the annotators agree with each other, and how well the systems
   perform when their outputs are compared with human annotations for the
   discourse analysis tasks.
   6.2.1   metrics for discourse segmentation

   because sentence boundaries are considered to also be the edu
   boundaries, we measure segmentation accuracy with respect to the
   intra-sentential segment boundaries, which is a standard method
   (soricut and marcu [394]2003; fisher and roark [395]2007).
   specifically, if a sentence contains n edus, which corresponds to n     1
   intra-sentential segment boundaries, we measure the segmenter's ability
   to correctly identify these n     1 boundaries. let h be the total number
   of intra-sentential segment boundaries in the human annotation, m be
   the total number of intra-sentential segment boundaries in the model
   output, and c be the total number of correct segment boundaries in the
   model output. then, we measure precision (p), recall (r), and f-score
   for segmentation performance as follows:

   6.2.2   metrics for discourse parsing

   to evaluate parsing performance, we use the standard unlabeled and
   labeled precision, recall, and f-score as proposed by marcu
   ([396]2000b). the unlabeled metric measures how accurate the discourse
   parser is in finding the right structure (i.e., the skeleton) of the
   dt, while the labeled metrics measure the parser's ability to find the
   right labels (i.e., nuclearity statuses or relation labels) in addition
   to the right structure. assume, for example, that given the two
   sentences of [397]figure 1, our system generates the dt shown in
   [398]figure 15. in [399]figure 16, we show the same gold dt shown in
   [400]figure 1 (on the left), and the same system-generated dt shown in
   [401]figure 15 (on the right), when the two trees are aligned. for the
   sake of illustration, instead of showing the real edus, we only show
   their ids. notice that the automatic segmenter made two mistakes: (1)
   it broke the edu marked 2   3 (some people use the purchasers    index as a
   leading indicator) in the human annotation into two separate edus, and
   (2) it could not identify edu 5 (but the thing it's supposed to
   measure) and edu 6 (    manufacturing strength    ) as two separate edus.
   therefore, when we align the two annotations, we obtain seven edus in
   total.
   [402]figure
   figure   15    a hypothetical system-generated dt for the two sentences in
   [403]figure 1.
   [404]figure
   figure   16    measuring the accuracy of a discourse parser. (a) the
   human-annotated discourse tree. (b) the system-generated discourse
   tree.

   in [405]table 2, we list all constituents of the two dts and their
   associated labels at the span, nuclei, and relation levels. the recall
   (r) and precision (p) figures are shown at the bottom of the table.
   notice that, following (marcu [406]2000b), the relation labels are
   assigned to the children nodes rather than to the parent nodes in the
   evaluation process to deal with non-binary trees in human annotations.
   to our knowledge, no implementation of the id74 was made
   publicly available. therefore, to help other researchers, we have made
   our source code of the id74 publicly available.[407]^12

   [408]table
   table   2    measuring parsing accuracy (p = precision, r = recall).

   given this evaluation setup, it is easy to understand that if the
   number of edus is the same in the human and system annotations (e.g.,
   when the discourse parser uses gold discourse segmentation), and the
   discourse trees are binary, then we get the same figures for precision,
   recall, and f-score.
   6.3   discourse segmentation evaluation

   in this section we present our experiments on discourse segmentation.
   6.3.1   experimental set-up for discourse segmentation

   we compare the performance of our discourse segmenter with the
   performance of the two publicly available discourse segmenters, namely,
   the discourse segmenters of the hilda (hernault et al. [409]2010) and
   spade (soricut and marcu [410]2003) systems. we also compare our
   results with the state-of-the-art results reported by fisher and roark
   ([411]2007) on the rst   dt test set. in all our experiments when
   comparing two systems, we use paired t-test on the f-scores to measure
   statistical significance and report the p-value.

   we ran hilda with its default settings. for spade, we applied the same
   modifications to its default settings as described in fisher and roark
   ([412]2007), which delivers significantly improved performance over its
   original version. specifically, in our experiments on the rst   dt
   corpus, we trained spade using the human-annotated syntactic trees
   extracted from the id32 (marcus, marcinkiewicz, and santorini
   [413]1994), and, during testing, we replaced the charniak parser
   (charniak [414]2000) with a more accurate reranking parser (charniak
   and johnson [415]2005). however, because of the lack of gold syntactic
   trees in the instructional corpus, we trained spade in this corpus
   using the syntactic trees produced by the reranking parser. to avoid
   using the gold syntactic trees, we used the reranking parser in our
   system for both training and testing purposes. this syntactic parser
   was trained on the sections of the id32 not included in our
   test set. we applied the same canonical lexical head projection rules
   (magerman [416]1995; collins [417]2003) to lexicalize the syntactic
   trees as done in hilda and spade.

   note that previous studies (fisher and roark [418]2007; soricut and
   marcu [419]2003; hernault et al. [420]2010) on discourse segmentation
   only report their performance on the rst   dt test set. to compare our
   results with them, we evaluated our model on the rst   dt test set. in
   addition, we showed a more general performance of spade and our system
   on the two corpora based on 10-fold cross validation.[421]^13 however,
   spade does not come with a training module for its segmenter. we
   reimplemented this module and verified its correctness by reproducing
   the results on the rst   dt test set.
   6.3.2   results for discourse segmentation

   [422]table 3 shows the discourse segmentation results of different
   systems in precision, recall, and f-score on the two corpora. on the
   rst    dt corpus, hilda's segmenter delivers the weakest performance,
   having an f-score of only 74.1. note that the high segmentation
   accuracy reported by hernault et al. ([423]2010) is due to a less
   stringent evaluation metric. spade performs much better than hilda with
   an absolute f-score improvement of 11.1%. our segmenter ds outperforms
   spade with an absolute f-score improvement of 4.9% (p-value < 2.4e-06),
   and also achieves comparable results to the ones of fisher and roark
   ([424]2007) (f&r), even though we use fewer features.[425]^14 notice
   that human agreement for this task is quite high   namely, an f-score of
   98.3 computed on the doubly-annotated portion of the rst   dt corpus
   mentioned in [426]section 6.1.

   [427]table
   table   3    discourse segmentation results of different models on the two
   corpora. performances significantly superior to spade are denoted by *.

   because fisher and roark ([428]2007) only report their results on the
   rst   dt test set and we did not have access to their system, we compare
   our approach only with spade when evaluating on a whole corpus based on
   10-fold cross validation. on the rst   dt corpus, our segmenter delivers
   an absolute f-score improvement of 3.8 percentage points, which
   represents a more than 25% relative error rate reduction. the
   improvement is higher on the instructional corpus with an absolute
   f-score improvement of 8.1 percentage points, which corresponds to a
   relative error reduction of 30%. the improvements for both corpora are
   statistically significant (p-value < 3.0e-06). when we compare our
   results on the two corpora, we observe a substantial decrease in
   performance on the instructional corpus. this could be because of a
   smaller amount of data in this corpus and/or to the inaccuracies in the
   syntactic parser and taggers, which are trained on news articles. a
   promising future direction would be to apply effective domain
   adaptation methods (e.g., easyadapt [daum   [429]2007]) to improve
   discourse segmentation performance in the instructional domain by
   leveraging the rich data in the news domain (i.e., rst   dt).
   6.4   discourse parsing evaluation

   in this section we present our experiments on discourse parsing. first,
   we describe the experimental set-up. then, we present the results of
   the parsers. while presenting the performance of our discourse parser,
   we show a breakdown of intra-sentential versus inter-sentential
   results, in addition to the aggregated results at the document level.
   6.4.1   experimental set-up for discourse parsing

   in our experiments on sentence-level (i.e., intra-sentential) discourse
   parsing, we compare our approach with spade (soricut and marcu
   [430]2003) on the rst   dt corpus, and with the ilp-based approach of
   subba and di-eugenio ([431]2009) on the instructional corpus, because
   they are the state of the art in their respective genres. for spade, we
   applied the same modifications to its default settings as described in
   [432]section 6.3.1, which leads to improved performance. similarly, in
   our experiments on document-level (i.e., multi-sentential) parsing, we
   compare our approach with hilda (hernault et al. [433]2010) on the
   rst   dt corpus, and with the ilp-based approach (subba and di-eugenio
   [434]2009) on the instructional corpus. the results for hilda were
   obtained by running the system with default settings on the same inputs
   we provided to our system. because we could not run the ilp-based
   system (not publicly available), we report the performance presented in
   their paper.

   our experiments on the rst   dt corpus use the same 18 coarser coherence
   relations (see [435]figure 18 later in this article), defined by
   carlson and marcu ([436]2001) and also used in spade and hilda systems.
   more specifically, the relation set consists of 16 relation categories
   and two pseudo-relations, namely, textual   organization and same   unit.
   after attaching the nuclearity statuses (ns, sn, nn) to these
   relations, we obtain 41 distinct relations.[437]^15 our experiments on
   the instructional corpus consider the same 26 primary relations (e.g.,
   goal:act, cause:effect) used by subba and di-eugenio ([438]2009) and
   also treat the reversals of non-commutative relations as separate
   relations. that is, goal   act and act   goal are considered to be two
   different coherence relations. attaching the nuclearity statuses to
   these relations provides 76 distinct relations.

   based on our experiments on the development set, the size of the
   automatically built bi-gram and tri-gram dictionaries was set to 95% of
   their total number of items, and the size of the unigram dictionary was
   set to 100%. note that the unigram dictionary contains only special
   tags denoting edu, sentence, and paragraph boundaries.
   6.4.2   evaluation of the intra-sentential discourse parser

   this section presents our experimental evaluation on intra-sentential
   discourse parsing. first, we show the performance of the sentence-level
   parsers when they are provided with manual (or gold) discourse
   segmentations. this allows us to judge the parsing performance
   independently of the segmentation task. then, we show the end-to-end
   performance of our intra-sentential framework, that is, the
   intra-sentential parsing performance based on automatic discourse
   segmentation.

   intra-sentential parsing results based on manual segmentation

   [439]table 4 presents the intra-sentential discourse parsing results
   when manual discourse segmentation is used. recall from our discussion
   on id74 in [440]section 6.2.2 that precision, recall, and
   f-score are the same when manual segmentation is used. therefore, we
   report only one of them. notice that our sentence-level discourse
   parser par-s consistently outperforms spade on the rst   dt test set in
   all three metrics, and the improvements are statistically significant
   (p-value < 0.01). especially, on the relation labeling task, which is
   the hardest among the three tasks, we achieve an absolute f-score
   improvement of 12.2 percentage points, which represents a relative
   error rate reduction of 37.7%.

   [441]table
   table   4    intra-sentential parsing results based on manual discourse
   segmentation. performances significantly superior to spade are denoted
   by *.

   to verify our claim that capturing the sequential dependencies between
   dt constituents using a dcrf model actually contributes to the
   performance gain, we also compare our results with an intra-sentential
   parser (see crf-nc in [442]table 4) that uses a simplified crf model
   similar to the one shown in [443]figure 8. although the simplified
   model has two hidden variables to model the structure and the relation
   of a dt constituent jointly, it does not have a chain structure, thus
   it ignores the sequential dependencies between dt constituents. the
   comparison in all three measures demonstrates that the improvements are
   indeed partly due to the dcrf model (p   value < 0.01).[444]^16 a
   comparison between crf-nc and spade shows that crf-nc significantly
   outperforms spade in all three measures (p-value < 0.01). this could be
   due to the fact that crf-nc is trained discriminatively with a large
   number of features, whereas spade is trained generatively with only
   lexico-syntactic features.

   notice that the scores of our parser (par-s) are close to the human
   agreement on the doubly-annotated data, and these results on the rst   dt
   test set are also consistent with the mean scores over 10-folds on the
   whole rst   dt corpus.[445]^17

   the improvements are higher on the instructional corpus, where we
   compare our mean results over 10-folds with the reported results of the
   ilp-based system of subba and di-eugenio ([446]2009), giving absolute
   f-score improvements of 5.4 percentage points, 17.6 percentage points,
   and 12.8 percentage points in span, nuclearity, and relations,
   respectively.[447]^18 our parser par-s reduces the errors by 76.1%,
   62.4%, and 34.6% in span, nuclearity and relations, respectively.

   if we compare the performance of our intra-sentential discourse parser
   on the two corpora, we notice that our parser par-s is more accurate in
   finding the right tree structure (see span row in the table) on the
   instructional corpus. this may be due to the fact that sentences in the
   instructional domain are relatively short and contain fewer edus than
   sentences in the news domain, thus making it easier to find the right
   tree structure. however, when we compare the performance on the
   relation labeling task, we observe a decrease on the instructional
   corpus. this may be due to the small amount of data available for
   training and the imbalanced distribution of a large number of discourse
   relations (i.e., 76 with nuclearity attached) in this corpus.

   intra-sentential parsing results based on automatic segmentation

   in order to evaluate the performance of the fully automatic
   sentence-level discourse analysis systems, we feed the intra-sentential
   discourse parsers the output of their respective discourse segmenters.
   [448]table 5 shows the (p)recision, (r)ecall, and (f)   score results for
   different id74. we compare our intra-sentential parser
   par-s with spade on the rst   dt test set. we achieve absolute f-score
   improvements of 5.7 percentage points, 6.4 percentage points, and 9.5
   percentage points in span, nuclearity, and relation, respectively.
   these improvements are statistically significant (p-value<0.001). our
   system, therefore, reduces the errors by 24.5%, 21.4%, and 22.6% in
   span, nuclearity, and relations, respectively. these results are also
   consistent with the mean results over 10-folds on the whole rst   dt
   corpus.

   [449]table
   table   5    intra-sentential parsing results using automatic discourse
   segmentation. performances significantly superior to spade are denoted
   by *.

   the rightmost column in the table shows our mean results over 10-folds
   on the instructional corpus. we could not compare our system with the
   ilp-based approach of subba and di-eugenio ([450]2009) because no
   results were reported using an automatic segmenter. it is interesting
   to observe how much our parser is affected by an automatic segmenter on
   the two corpora (see [451]tables 4 and [452]5). nevertheless, taking
   into account the segmentation results in [453]table 3, this is not
   surprising because previous studies (soricut and marcu [454]2003) have
   already shown that automatic segmentation is the primary impediment to
   high accuracy discourse parsing. this demonstrates the need for a more
   accurate discourse segmentation model in the instructional genre.
   6.4.3   evaluation of the complete parser

   we experiment with our full document-level discourse parser on the two
   corpora using the two parsing approaches described in [455]section 4.3,
   namely, 1s-1s and the sliding window. on rst   dt, the standard split was
   used for training and testing. on the instructional corpus, subba and
   di-eugenio ([456]2009) used 151 documents for training and 25 documents
   for testing. because we did not have access to their particular split,
   we took five random samples of 151 documents for training and 25
   documents for testing, and report the average performance over the five
   test sets.

   [457]table 6 presents results for our two-stage discourse parser (tsp)
   using approaches 1s-1s (tsp 1-1) and the sliding window (tsp sw) on
   manually segmented texts. recall that precision, recall, and f-score
   are the same when manual segmentation is used. we compare our parser
   with the state-of-the-art on the two corpora: hilda (hernault et al.
   [458]2010) on rst   dt, and the ilp-based approach (subba and di-eugenio
   [459]2009) on the instructional domain. on both corpora, our systems
   outperform existing systems by a wide margin (p-value <7.1e-05 on
   rst   dt).[460]^19 on rst   dt, our parser tsp 1-1 achieves absolute
   improvements of 7.9 percentage points, 9.3 percentage points, and 11.5
   percentage points in span, nuclearity, and relation, respectively, over
   hilda. this represents relative error reductions of 31.2%, 22.7%, and
   20.7% in span, nuclearity, and relation, respectively.

   [461]table
   table   6    parsing results of document-level parsers using manual
   segmentation. performances significantly superior to hilda (p-value
   <0.0001) are denoted by *. significant differences between tsp 1-1 and
   tsp sw (p-value <0.01) are denoted by    .

   beside hilda, we also compare our results with two baseline parsers on
   rst   dt: (1) crf-o, which uses a single unified crf-based parsing model
   shown in [462]figure 8 (the one used for multi-sentential parsing)
   without distinguishing between intra- and multi-sentential parsing, and
   (2) crf-t, which uses two different crf-based parsing models for intra-
   and multi-sentential parsing in the two-stage approach 1s-1s, both
   models having the same structure as in [463]figure 8. thus, crf-t is a
   variation of tsp 1-1, where the dcrf-based (chain-structured)
   intra-sentential parsing model is replaced with a simpler crf-based
   parsing model.[464]^20 note that although crf-o does not explicitly
   discriminate between intra- and multi-sentential parsing, it uses
   id165 features that include sentence and edu boundaries to encode this
   information into the model.

   [465]table 6 shows that both crf-o and crf-t outperform hilda by a good
   margin (p-value <0.0001). this improvement can be attributed to the
   optimal parsing algorithm and better feature selection strategy. when
   we compare crf-t with crf-o, we notice significant performance gains
   for crf-t (p-value <0.001). the absolute gains are 4.32 percentage
   points, 2.68 percentage points, and 4.55 percentage points in span,
   nuclearity, and relation, respectively. this comparison clearly
   demonstrates the benefit of using a two-stage approach with two
   different parsing models over a framework with one single unified
   parsing model. finally, when we compare our best results with the human
   agreements, we still observe room for further improvement in all three
   measures.

   on the instructional genre, our parser tsp 1-1 delivers absolute
   f-score improvements of 10.3 percentage points, 13.6 percentage points,
   and 8.1 percentage points in span, nuclearity, and relations,
   respectively, over the ilp-based approach of subba and di-eugenio
   ([466]2009). our parser, therefore, reduces errors by 34.7%, 26.9%, and
   12.5% in span, nuclearity, and relations, respectively.

   if we compare the performance of our discourse parsers on the two
   corpora, we observe lower results on the instructional corpus. there
   could be two reasons for this. first, the instructional corpus has a
   smaller amount of data with a larger set of relations (76 with
   nuclearity attached). second, some of the frequent relations are
   semantically very similar (e.g., preparation-act, step1-step2), which
   makes it difficult even for the human annotators to distinguish them
   (subba and di-eugenio [467]2009).

   comparison between our two document-level parsing approaches reveals
   that tsp sw significantly outperforms tsp 1-1 only in finding the right
   structure on both corpora (p-value <0.01). not surprisingly, the
   improvement is higher on the instructional corpus. a likely explanation
   is that the instructional corpus contains more leaky boundaries (12%),
   allowing the sliding window approach to be more effective in finding
   those, without inducing much noise for the labels. this demonstrates
   the potential of tsp sw for data sets with even more leaky boundaries,
   e.g., the dutch (vliet and redeker [468]2011) and the german potsdam
   (stede [469]2004) corpora. however, it would be interesting to see how
   other heuristics to do consolidation in the cross condition
   ([470]section 4.3.2) perform.

   to analyze errors made by tsp sw, we looked at some poorly parsed
   examples and found that although tsp sw finds more correct structures,
   a corresponding improvement in labeling relations is not present
   because in some cases, it tends to induce noise from the neighboring
   sentences for the labels. for example, when parsing is performed on the
   first sentence in [471]figure 1 in isolation using 1s-1s, our parser
   rightly identifies the contrast relation between edus 2 and 3. but,
   when it is considered with its neighboring sentences by the sliding
   window, the parser labels it as elaboration. a promising strategy to
   deal with this and similar problems would be to apply both approaches
   to each sentence and combine them by consolidating three probabilistic
   decisions, namely, the one from 1s-1s and the two from the sliding
   window.
   6.4.4   k-best parsing results based on manual segmentation

   as described in [472]section 4.2, a straight-forward modification of
   our probabilistic parsing algorithm allows us to generate a list of
   k-best parse hypotheses for a given text. we adapt our parsing
   algorithm accordingly to produce k most probable dts for each text, and
   measure the oracle accuracy based on the f-scores of the relation
   metric which gives aggregated evaluation on structure and relation
   labels (see [473]table 2). specifically, the oracle accuracy o   score
   for k-best discourse parsing is measured as follows:

   where n is the total number of texts (sentences or documents)
   evaluated, g[i] is the gold dt annotation for text i, is the j^th parse
   hypothesis generated by the parser for text i, and f-score[r] (g[i], )
   is the f-score accuracy of hypothesis on the relation metric, which
   essentially measures how similar is to g[i] in terms of its structure
   and labels.

   [474]table 7 presents the oracle scores of our intra-sentential parser
   par-s on the rst   dt test set as a function of k of k-best parsing. the
   1-best result tells that the parser has the base accuracy of 79.8%. the
   2-best shows dramatic oracle-rate improvements (i.e., 4.65% absolute),
   meaning that often our parser generates the best tree as its top two
   outputs. 3-best and 4-best also show moderate improvements (about 2%).
   things start to slow down afterwards, and we achieve oracle rates of
   90.37% and 92.57% at 10   best and 20-best, respectively. the 30-best
   parsing gives an oracle score of 93.2%.

   [475]table
   table   7    oracle scores as a function of k of k-best sentence-level
   parses on rst   dt.

   the results of our k-best intra-sentential discourse parser demonstrate
   that a k-best reranking approach like that of collins and koo
   ([476]2005) and charniak and johnson ([477]2005) used for syntactic
   parsing can potentially improve the parsing accuracy even further by
   exploiting additional global features of the candidate discourse trees
   as evidence.

   the scenario is quite different at the document-level; [478]table 8
   shows the k-best parsing results of tsp 1s-1s on the rst   dt test set.
   the improvements in oracle-rate are small at the document-level when
   compared with the sentence-level parsing. for example, the 2-best and
   the 5-best improve over the base accuracy by only 0.7 percentage points
   and 1.0 percentage points, respectively. the improvements get even
   slower after that. however, this is not surprising because generally
   document-level dts are big with many constituents, and only a very few
   of these constituents change from k-best to k + 1-best parsing. these
   small changes among the candidate dts do not contribute much to the
   overall f-score accuracy (for further clarification see how f-score is
   calculated in [479]section 6.2.2).

   [480]table
   table   8    oracle scores as a function of k of k-best document-level
   parses on rst   dt.

   the results of our k-best document-level parsing suggest that often the
   best tree is missing in the top k parses. thus, a reranking of k-best
   document-level parses may not be a suitable option for further
   improvement at the document-level. an alternative to k-best reranking
   is to use a sampling-based parsing strategy (wick et al. [481]2011) to
   explore the space of possible trees, as recently used for dependency
   parsing (zhang et al. [482]2014). however, note that the potential gain
   we may obtain by using a reranker at the sentence level will also
   improve the (combined) accuracy of the document-level parser.
   6.4.5   analysis of features

   to analyze the relative importance of different features used in our
   parsing models, [483]table 9 presents the sentence- and document-level
   parsing results on a manually segmented rst   dt test set using different
   subsets of features. the feature subsets were defined in [484]section
   4.1.4. in each parsing condition, the subsets of features are added
   incrementally, based on their availability and historical importance.
   the columns in [485]table 9 represent the inclusion order of the
   feature subsets.

   [486]table
   table   9    parsing results using different subsets of features on rst   dt
   test set.

   because spade (soricut and marcu [487]2003) achieved the previous best
   results on intra-sentential parsing using dominance set features, these
   are included as the initial set of features in our intra-sentential
   parsing model. in hilda, hernault et al. ([488]2010) demonstrate the
   importance of organizational and id165 features for full text parsing.
   we add these two feature subsets one after another in our intra- and
   multi-sentential parsing models.[489]^21 contextual features require
   other features to be computed; thus they were added after those
   features. because computation of sub-structural features requires an
   initial parse tree (i.e., when the parser is applied), they are added
   at the very end.

   notice that inclusion of every new subset of features appears to
   improve the performance over the previous set. specifically, for
   sentence-level parsing, when we add the organizational features with
   the dominance set features, we achieve about 2 percentage points
   absolute improvements in nuclearity and relations. with id165
   features, the gain is even higher: 6 percentage points in relations and
   3.5 percentage points in nuclearity for sentence-level parsing, and 3.8
   percentage points in relations and 3.1 percentage points in nuclearity
   for document-level parsing. this demonstrates the utility of the id165
   features, which is also consistent with the previous findings of
   duverle and prendinger ([490]2009) and schilder ([491]2002).

   the features extracted from lexical chains (l-ch) have also proved to
   be useful for document-level parsing. they deliver absolute
   improvements of 2.7 percentage points, 2.9 percentage points, and 2.3
   percentage points in span, nuclearity, and relations, respectively.
   including the contextual features further gives improvements of 3
   percentage points in nuclearity and 2.2 percentage points in relation
   for sentence-level parsing, and 1.3 percentage points in nuclearity,
   and 1.2 percentage points in relation for document-level parsing.
   notice that sub-structural features are more beneficial for
   document-level parsing than they are for sentence-level parsing, that
   is, an improvement of 2.2 percentage points versus an improvement of
   0.9 percentage points. this is not surprising because document-level
   dts are generally much larger than sentence-level dts, making the
   sub-structural features more effective for document-level parsing.
   6.4.6   error analysis

   we further analyze the errors made by our discourse parser. as
   described in previous sections, the parser could be wrong in finding
   the right structure as well as the right nuclearity and relation
   labels. [492]figure 17 presents an example where our parser makes
   mistakes in finding the right structure (notice the units connected by
   attribution and cause in the two example dts) and the right relation
   label (topic-comment vs. background). the comparison between intra- and
   multi-sentential parsing results presented in [493]sections 6.4.2 and
   [494]6.4.3 tells us that the errors in structure occur more frequently
   when the dt is large (e.g., at the document level) and the parsing
   model fails to capture the long-range structural dependencies between
   the dt constituents.

   to further analyze the errors made by our parser on the hardest task of
   relation labeling, in [495]figure 18 we present the confusion matrix
   for our document-level parser tsp 1-1 on the rst   dt test set. in order
   to judge independently the ability of our parser to assign the correct
   relation labels, the confusion matrix is computed based on the
   constituents (see [496]table 2), where our parser found the right span
   (i.e., structure).[497]^22 the relations in the matrix are ordered
   according to their frequency in the training set.

   in general, the errors can be explained by two different causes acting
   together: (1) imbalanced distribution of the relations in the corpus,
   and (2) semantic similarity between the relations. the most frequent
   relation elaboration tends to overshadow others, especially the ones
   that are semantically similar (e.g., explanation, background) and less
   frequent (e.g., summary, evaluation). furthermore, our models sometimes
   fail to distinguish relations that are semantically similar (e.g.,
   temporal vs. background, cause vs. explanation).

   now, let us look more closely at a few of these errors. [498]figure 19
   presents an example where our parser mistakenly labels a summary as
   elaboration. clearly, in this example the text in parentheses (i.e.,
   (cfd)) is an acronym or summary of the text to the left. however,
   parenthesized texts are also used to provide additional information
   (i.e., to elaborate), as exemplified in [499]figure 20 by two text
   snippets from the rst-dt. notice that although the structure of the
   text (widow of the ..) in the first example is quite distinguishable
   from the structure of (cfd), the text (d., maine) in the second example
   is similar to (cfd) in structure, thus it confuses our model.[500]^23
   [501]figure
   figure   17    discourse trees generated by human annotator and our system
   for the text [what's more,][e1] [he believes][e2] [seasonal swings in
   the auto industry this year aren't occurring at the same time in the
   past,][e3] [because of production and pricing differences][e4] [that
   are curbing the accuracy of seasonal adjustments][e5] ] [built into the
   employment data.][e6]
   [502]figure
   figure   18    confusion matrix for relation labels on the rst   dt test set.
   the y-axis represents true and x-axis represents predicted relations.
   the relations are topic-change (t-c), topic-comment (t-cm),
   textualorganization (t-o), manner-means (m-m), comparison (cmp),
   evaluation (ev), summary (su), condition (cnd), enablement (en), cause
   (ca), temporal (te), explanation (ex), background (ba), contrast (co),
   joint (jo), same   unit (s-u), attribution (at), and elaboration (el).
   [503]figure
   figure   19    our system mistakenly labels a summary as elaboration.
   [504]figure
   figure   20    two examples of elaboration by texts in parentheses.

   [505]figure 21 presents two examples where our parser mistakenly labels
   background and cause as elaboration. however, notice that the two
   discourse relations (i.e., background vs. elaboration and cause vs.
   elaboration) in these examples are semantically very close, and
   arguably both can be applicable.
   [506]figure
   figure   21    confusion between background/cause and elaboration.

   given these observations, we see two possible ways to improve our
   system. first, we would like to use a more robust method (e.g.,
   ensemble methods with id112) to deal with the imbalanced distribution
   of relations, along with taking advantage of richer semantic knowledge
   (e.g., id152) to cope with the errors caused by
   semantic similarity between the relations. second, to capture
   long-range dependencies between dt constituents, we would like to
   explore the idea of k-best discriminative reranking using tree kernels
   (dinarelli, moschitti, and riccardi [507]2011). because our parser
   already produces k most probable dts, developing a reranker based on
   discourse tree kernels is very much within our reach.
   7.   conclusions and future directions
   section:
   [choose___________________________]
   [508]previous section [509]next section

   in this article we have presented codra, a complete probabilistic
   discriminative framework for performing rhetorical analysis in the rst
   framework. codra comprises components for performing both discourse
   segmentation and discourse parsing. the discourse segmenter is a binary
   classifier based on a maximum id178 model, and the discourse parser
   applies an optimal parsing algorithm to probabilities inferred from two
   crf models: one for intra-sentential parsing and the other for
   multi-sentential parsing. the crf models effectively represent the
   structure and the label of discourse tree constituents jointly.
   furthermore, the dcrf model for intra-sentential parsing captures the
   sequential dependencies between the constituents. the two separate
   parsing models use their own informative feature sets and the
   distributional variations of the relation labels in their respective
   parsing conditions.

   we have also presented two approaches to effectively combine the
   intra-sentential and the multi-sentential parsing modules, which can
   exploit the strong correlation observed between the text structure and
   the structure of the discourse tree. the first approach (1s   1s) builds
   a dt for every sentence using the intra-sentential parser, and then
   runs the multi-sentential parser on the resulting sentence-level dts.
   to deal with leaky boundaries, our second approach (sliding window)
   builds sentence-level discourse sub-trees by applying the
   intra-sentential parser on a sliding window, covering two adjacent
   sentences and then consolidating the results produced by overlapping
   windows. after that, the multi-sentential parser takes all these
   sentence-level sub-trees and builds a full rhetorical parse for the
   whole document.

   finally, we have extended the parsing algorithm to generate k most
   probable parse hypotheses for each input text, which could be used in a
   reranker to improve over the initial ranking using global features like
   long-range structural dependencies.

   empirical evaluations on two different genres demonstrate that our
   approach to discourse segmentation achieves state-of-the-art
   performance more efficiently using fewer features. a series of
   experiments on the discourse parsing task shows that both our intra-
   and multi-sentential parsers significantly outperform the state of the
   art, often by a wide margin. a comparison between our combination
   strategies reveals that the sliding window approach is more robust
   across domains. furthermore, the oracle accuracy computed based on the
   k-best parse hypotheses generated by our parser demonstrates that a
   reranker could potentially improve the accuracy even further.

   our error analysis reveals that although the sliding window approach
   finds more correct tree structures, in some cases it induces noise for
   the relation labels from the neighboring sentences. with respect to the
   performance of our discourse parser on the relation labeling task we
   also found that the most frequent relations tend to mislead the
   identification of the less frequent ones, and the models sometimes fail
   to distinguish relations that are semantically similar.

   the work presented in this article leads us to several interesting
   future directions. our short-term goal is to develop a k-best
   discriminative reranking discourse parser using tree kernels applied to
   discourse trees. we also plan to investigate to what extent discourse
   segmentation and discourse parsing can be performed jointly.

   we would also like to explore how our system performs on other genres
   like conversational (e.g., blogs, e-mails) and evaluative (e.g.,
   customer reviews) texts. to address the problem of limited annotated
   data in various genres, we are planning to develop an interactive
   version of our system that will allow users to fix the output of the
   system with minimal effort and let the system learn from that feedback.

   another interesting future direction is to perform extrinsic
   evaluations of our system in downstream applications. one important
   application of rhetorical structure is text summarization, where a
   significant challenge is producing not only informative but also
   coherent summaries. a number of researchers have already investigated
   the utility of rhetorical structure for measuring text importance
   (i.e., informativeness) in summarization (marcu [510]2000b; daum   and
   marcu [511]2002; louis, joshi, and nenkova [512]2010). recently,
   christensen et al. ([513]2013, [514]2014) propose to perform sentence
   selection and ordering at the same time, and use constraints on
   discourse structure to make the summaries coherent. however, they
   represent the discourse as an unweighted directed graph, which is
   shallow and not sufficiently informative in most cases. furthermore,
   their approach does not allow compression at the sentence level, which
   is often beneficial in summarization. in the future, we would like to
   investigate the utility of our rhetorical structure for performing
   sentence compression, selection, and ordering in a joint summarization
   process.

   discourse structure can also play important roles in sentiment
   analysis. a key research problem in id31 is extracting
   fine-grained opinions about different aspects of a product. several
   recent papers (somasundaran [515]2010; lazaridou, titov, and sporleder
   [516]2013) exploited the rhetorical structure for this task. another
   challenging problem is assessing the overall opinion expressed in a
   review because not all sentences in a review contribute equally to the
   overall sentiment. for example, some sentences are subjective, whereas
   others are objective (pang and lee [517]2004); some express the main
   claims, whereas others support them (taboada et al. [518]2011); some
   express opinions about the main entity, whereas others are about the
   peripherals. discourse structure could be useful to capture the
   relative weights of the discourse units towards the overall sentiment.
   for example, the nucleus and satellite distinction along with the
   rhetorical relations could be useful to infer the relative weights of
   the connecting discourse units.

   among other applications of discourse structure, machine translation
   (mt) and its evaluation have received a resurgence of interest
   recently. a workshop dedicated to discourse in machine translation was
   arranged recently at the acl 2013 conference (webber et al. [519]2013).
   researchers believe that mt systems should consider discourse phenomena
   that go beyond the current sentence to ensure consistency in the choice
   of lexical items or referring expressions, and the fact that
   source-language coherence relations are also realized in the target
   language (i.e., translating at the document-level [hardmeier, nivre,
   and tiedemann [520]2012]). guzm  n et al. ([521]2014a, [522]2014b) and
   joty et al. ([523]2014) propose new discourse-aware automatic
   id74 for mt systems using our discourse analysis tool.
   they demonstrate that sentence-level discourse information is
   complementary to the state-of-the-art id74, and by
   combining the discourse-based metrics with the metrics from the asiya
   mt evaluation toolkit (gim  nez and m  rquez [524]2010), they won the wmt
   2014 metrics shared task challenge (mach    ek and bojar [525]2014) both
   at the segment- and at the system-level. these results suggest that
   discourse structure helps to distinguish better translations from worse
   ones. thus, it would be interesting to explore whether discourse
   information can be used to rerank alternative mt hypotheses as a
   post-processing step for the mt output.

   a longer-term goal is to extend our framework to also work with graph
   structures of discourse, as recommended by several recent discourse
   theories (wolf and gibson [526]2005). once we achieve similar
   performance on graph structures, we will perform extrinsic evaluations
   to determine their relative utility for various nlp tasks.

   finally, we hope that the online demo, the source code of codra, and
   the id74 that we made publicly available in this work
   will facilitate other researchers in extending our work and in applying
   discourse parsing to their nlp tasks.
   bibliographic note
   section:
   [choose________________________]
   [527]previous section [528]next section

   portions of this work were previously published in two conference
   proceedings (joty, carenini, and ng [529]2012; joty et al. [530]2013).
   this article significantly extends our previous work in several ways,
   most notably: (1) we extend the parsing algorithm to generate k-most
   probable parse hypotheses for each input text ([531]section 4.2); (2)
   we show the oracle accuracies for k-best discourse parsing both at the
   sentence level and at the document level ([532]section 6.4.4); (3) to
   support our claim, we compare our best results with several variations
   of our approach (see crf-nc in [533]section 6.4.2, and crf-o and crf-t
   in [534]section 6.4.3); (4) we analyze the relative importance of
   different features for intra- and multi-sentential discourse parsing
   ([535]section 6.4.5); and (5) we perform in-depth error analysis of our
   complete rhetorical analysis framework ([536]section 6.4.6).
   appendix a. sample output generated by an online demo of codra
   section:
   [choose_________________________]
   [537]previous section [538]next section

   acknowledgments
   section:
   [choose________________________]
   [539]previous section [540]next section

   the authors acknowledge the funding support of nserc canada graduate
   scholarship (cgs-d). many thanks to bonnie webber, amanda stent,
   carolyn rose, lluis marquez, samantha wray, and the anonymous reviewers
   for their insightful comments on an earlier version of this article.
   notes
   section:
   [choose________________________]
   [541]previous section [542]next section

   1    we categorize this approach as unsupervised because it does not rely
   on human-annotated data.

   2    [543]http://www.isi.edu/licensed-sw/spade/.

   3    [544]http://nlp.prendingerlab.net/hilda/.

   4    the demo of codra is available at
   [545]http://109.228.0.153/discourse_parser_demo/. the source code of
   codra is available from [546]http://alt.qcri.org/tools/.

   5    the input text in the demo in [547]appendix a is taken from
   [548]www.bbc.co.uk/news/world-asia-26106490.

   6    by the term    fat    we refer to crfs with multiple (interconnected)
   chains of output variables.

   7    for n + 1 edus, the number of valid discourse tree structures (i.e.,
   not counting possible variations in the nuclearity and relation labels)
   is the catalan number c[n].

   8    the higher the conditional id178, the lower the mutual
   information, and vice versa.

   9    we agree that with potentially sub-optimal, sub-structural features
   in the parsing model, cky may end up finding a sub-optimal dt. but that
   is a separate issue.

   10    do not confuse the term two-stage with the term two-pass.

   11    available at [549]http://cogcomp.cs.illinois.edu/page/software.

   12    available from [550]alt.qcri.org/tools/.

   13    because the two tasks   discourse segmentation and intra-sentential
   parsing   operate at the sentence level, the cross validation was
   performed over sentences for their evaluation.

   14    because we did not have access to the system or to the complete
   output/results of fisher and roark ([551]2007), we were not able to
   perform a statistical significance test.

   15    not all relations take all the possible nuclearity statuses. for
   example, elaboration and attribution are mono-nuclear relations, and
   same   unit and joint are multi-nuclear relations.

   16    the parsing performance reported in [552]table 4 for crf-nc is when
   the crf parsing model is trained on a balanced data set (an equal
   number of instances with s=1 and s=0); training on full but imbalanced
   data set gives slightly lower results.

   17    our emnlp and acl publications (joty, carenini, and ng [553]2012;
   joty et al. [554]2013) reported slightly lower parsing accuracies.
   fixing a bug in the parsing algorithm accounts for the difference.

   18    subba and di-eugenio ([555]2009) report their results based on an
   arbitrary split between training and test sets. because we did not have
   access to their particular split, we compare our model's performance
   based on 10-fold cross validation with their reported results. also,
   because we did not have access to their system/output, we could not
   perform a significance test on the instructional corpus.

   19    because we did not have access to the output or to the system of
   subba and di-eugenio ([556]2009), we were not able to perform a
   significance test on the instructional corpus.

   20    the performance of this model for intra-sentential parsing is
   reported in [557]table 4 under the name crf-nc.

   21    text structural features are included in the organizational
   features for multi-sentential parsing.

   22    therefore, the counts of the relations shown in the table may not
   match the ones in the test set.

   23    d., maine in this example refers to democrat from state maine.
   references
   section:
   [choose________________________]
   [558]previous section [559]next section
   althaus, ernst, denys duchier, alexander koller, kurt mehlhorn, joachim
   niehren, and sven thiel. 2003. an efficient graph algorithm for
   dominance constraints. journal of algorithms, 48(1):194   219.
   [560]crossref, [561]google scholar
   asher, nicholas and alex lascarides, 2003. logics of conversation.
   cambridge university press. [562]google scholar
   barzilay, regina and michael elhadad. 1997. using lexical chains for
   text summarization. in proceedings of the 35th annual meeting of the
   association for computational linguistics and the 8th european chapter
   meeting of the association for computational linguistics, workshop on
   intelligent scalable test summarization, pages 10   17, madrid.
   [563]google scholar
   biran, or and owen rambow. 2011. identifying justifications in written
   dialogs by classifying text as argumentative. international journal of
   semantic computing, 5(4):363   381. [564]crossref, [565]google scholar
   blair-goldensohn, sasha, kathleen mckeown, and owen rambow. 2007.
   building and refining rhetorical-semantic relation models. in
   proceedings of the human language technologies: the annual conference
   of the north american chapter of the association for computational
   linguistics, hlt-naacl'07, pages 428   435. rochester, ny. [566]google
   scholar
   blitzer, john. 2008. id20 of natural language processing
   systems. ph.d. thesis, university of pennsylvania. [567]google scholar
   breiman, leo. 1996. id112 predictors. machine learning,
   24(2):123   140. [568]crossref, [569]google scholar
   carlson, lynn and daniel marcu. 2001. discourse tagging reference
   manual. technical report isi-tr-545, university of southern california
   information sciences institute. [570]google scholar
   carlson, lynn, daniel marcu, and mary ellen okurowski. 2002. rst
   discourse treebank (rst   dt) ldc2002t07. linguistic data consortium,
   philadelphia. [571]google scholar
   chali, yllias and shafiq joty. 2007. id51 using
   lexical cohesion. in proceedings of semeval-2007, pages 476   479,
   prague. [572]google scholar
   charniak, eugene. 2000. a maximum-id178-inspired parser. in
   proceedings of the 1st north american chapter of the association for
   computational linguistics conference, naacl'00, pages 132   139, seattle,
   wa. [573]google scholar
   charniak, eugene and mark johnson. 2005. coarse-to-fine n-best parsing
   and maxent discriminative reranking. in proceedings of the 43rd annual
   meeting of the association for computational linguistics, acl'05, pages
   173   180, ann arbor, mi. [574]crossref, [575]google scholar
   christensen, janara, mausam, stephen soderland, and oren etzioni. 2013.
   towards coherent id57. in proceedings of the
   2013 conference of the north american chapter of the association for
   computational linguistics: human language technologies, naacl-hlt'13,
   pages 1163   1173, atlanta, ga. [576]google scholar
   christensen, janara, stephen soderland, gagan bansal, and mausam. 2014.
   hierarchical summarization: scaling up id57. in
   proceedings of the 52nd annual meeting of the association for
   computational linguistics, acl'13, pages 902   912, baltimore, md.
   [577]google scholar
   collins, michael. 2003. head-driven statistical models for natural
   language parsing. computational linguistics, 29(4):589   637.
   [578]link, [579]google scholar
   collins, michael and terry koo. 2005. discriminative reranking for
   natural language parsing. computational linguistics, 31(1):25   70.
   [580]link, [581]google scholar
   collobert, ronan, jason weston, l  on bottou, michael karlen, koray
   kavukcuoglu, and pavel kuksa. 2011. natural language processing
   (almost) from scratch. journal of machine learning research,
   12:2493   2537. [582]google scholar
   cristea, dan, nancy ide, and laurent romary. 1998. veins theory: a
   model of global discourse cohesion and coherence. in proceedings of the
   36th annual meeting of the association for computational linguistics
   and of the 17th international conference on computational linguistics
   (coling/acl'98), pages 281   285. montreal. [583]crossref, [584]google
   scholar
   danlos, laurence. 2009. d-stag: a discourse analysis formalism based on
   synchronous tags. tal, 50(1):111   143. [585]google scholar
   daum  , iii, hal. 2007. frustratingly easy id20. in
   proceedings of the 45th annual meeting of the association for
   computational linguistics, acl'07, pages 256   263, prague. [586]google
   scholar
   daum  , iii, hal and daniel marcu. 2002. a noisy-channel model for
   document compression. in proceedings of the 40th annual meeting of the
   association for computational linguistics, acl '02, pages 449   456,
   philadelphia, pa. [587]google scholar
   dinarelli, marco, alessandro moschitti, and giuseppe riccardi. 2011.
   discriminative reranking for spoken language understanding. ieee
   transactions on audio, speech and language processing (taslp),
   20:526   539. [588]google scholar
   duverle, david and helmut prendinger. 2009. a novel discourse parser
   based on support vector machine classification. in proceedings of the
   joint conference of the 47th annual meeting of the acl and the 4th
   international joint conference on natural language processing of the
   afnlp, pages 665   673, suntec. [589]google scholar
   egg, markus, alexander koller, and joachim niehren. 2001. the
   constraint language for lambda structures. journal of logic, language
   and information, 10(4):457   485. [590]crossref, [591]google scholar
   eisner, jason. 1996. three new probabilistic models for dependency
   parsing: an exploration. in proceedings of the 16th conference on
   computational linguistics - volume 1, coling '96, pages 340   345,
   copenhagen. [592]crossref, [593]google scholar
   fellbaum, christiane. 1998. id138   an electronic lexical database. mit
   press, cambridge, ma. [594]google scholar
   feng, vanessa and graeme hirst. 2012. text-level discourse parsing with
   rich linguistic features. in proceedings of the 50th annual meeting of
   the association for computational linguistics, acl '12, pages 60   68,
   jeju island. [595]google scholar
   feng, vanessa and graeme hirst. 2014. a linear-time bottom-up discourse
   parser with constraints and post-editing. in proceedings of the 52nd
   annual meeting of the association for computational linguistics, acl
   '14, pages 511   521, baltimore, md. [596]google scholar
   finkel, jenny rose, alex kleeman, and christopher manning. 2008.
   efficient, feature-based, conditional random field parsing. in
   proceedings of the 46th annual meeting of the association for
   computational linguistics, acl'08, pages 959   967, columbus, oh.
   [597]google scholar
   fisher, seeger and brian roark. 2007. the utility of parse-derived
   features for automatic discourse segmentation. in proceedings of the
   45th annual meeting of the association for computational linguistics,
   acl'07, pages 488   495, prague. [598]google scholar
   galley, michel and kathleen mckeown. 2003. improving word sense
   disambiguation in lexical chaining. in proceedings of the 18th
   international joint conference on artificial intelligence, ijcai'03,
   pages 1486   1488, acapulco. [599]google scholar
   galley, michel, kathleen mckeown, eric fosler-lussier, and hongyan
   jing. 2003. discourse segmentation of multi-party conversation. in
   proceedings of the 41st annual meeting of the association for
   computational linguistics - volume 1, acl '03, pages 562   569, sapporo.
   [600]crossref, [601]google scholar
   ghosh, sucheta, richard johansson, giuseppe riccardi, and sara tonelli.
   2011. shallow discourse parsing with id49. in
   proceedings of the 5th international joint conference on natural
   language processing, ijcnlp'11, pages 1071   1079, chiang mai.
   [602]google scholar
   gim  nez, jes  s and llu  s m  rquez. 2010. linguistic measures for
   automatic machine translation evaluation. machine translation,
   24(3   4):77   86. [603]crossref, [604]google scholar
   guzm  n, francisco, shafiq joty, llu  s m  rquez, alessandro moschitti,
   preslav nakov, and massimo nicosia. 2014a. learning to differentiate
   better from worse translations. in proceedings of the 2014 conference
   on empirical methods in natural language processing (emnlp), pages
   214   220, doha. [605]crossref, [606]google scholar
   guzm  n, francisco, shafiq joty, llu  s m  rquez, and preslav nakov.
   2014b. using discourse structure improves machine translation
   evaluation. in proceedings of the 52nd annual meeting of the
   association for computational linguistics (volume 1: long papers),
   pages 687   698, baltimore, md. [607]crossref, [608]google scholar
   halliday, michael and ruqaiya hasan. 1976. cohesion in english.
   longman, london. [609]google scholar
   hardmeier, christian, joakim nivre, and j  rg tiedemann. 2012.
   document-wide decoding for phrase-based statistical machine
   translation. in proceedings of the 2012 joint conference on empirical
   methods in natural language processing and computational natural
   language learning, emnlp-conll '12, pages 1179   1190, jeju island.
   [610]google scholar
   hernault, hugo, helmut prendinger, david duverle, and mitsuru ishizuka.
   2010. hilda: a discourse parser using support vector machine
   classification. dialogue and discourse, 1(3):1   33.
   [611]crossref, [612]google scholar
   hirst, graeme and david st-onge. 1997. lexical chains as representation
   of context for the detection and correction of malapropisms. in
   christiane fellbaum, editor, id138: an electronic lexical database
   and some of its applications. mit press, pages 305   332. [613]google
   scholar
   hobbs, jerry. 1979. coherence and coreference. cognitive science,
   3:67   90. [614]crossref, [615]google scholar
   huang, liang and david chiang. 2005. better k-best parsing. in
   proceedings of the ninth international workshop on parsing technology,
   parsing '05, pages 53   64, stroudsburg, pa. [616]crossref, [617]google
   scholar
   ji, yangfeng and jacob eisenstein. 2014. representation learning for
   text-level discourse parsing. in proceedings of the 52nd annual meeting
   of the association for computational linguistics (volume 1: long
   papers), pages 13   24, baltimore, md. [618]crossref, [619]google scholar
   joty, shafiq, giuseppe carenini, and raymond t. ng. 2012. a novel
   discriminative framework for sentence-level discourse analysis. in
   proceedings of the 2012 joint conference on empirical methods in
   natural language processing and computational natural language
   learning, emnlp-conll '12, pages 904   915, jeju island. [620]google
   scholar
   joty, shafiq, giuseppe carenini, and raymond t. ng. 2013. topic
   segmentation and labeling in asynchronous conversations. journal of
   artificial intelligence research (jair), 47:521   573. [621]google
   scholar
   joty, shafiq, giuseppe carenini, raymond t. ng, and yashar mehdad.
   2013. combining intra- and multi-sentential rhetorical parsing for
   document-level discourse analysis. in proceedings of the 51st annual
   meeting of the association for computational linguistics, acl '13,
   pages 486   496, sofia. [622]google scholar
   joty, shafiq, francisco guzm  n, llu  s m  rquez, and preslav nakov. 2014.
   discotk: using discourse structure for machine translation evaluation.
   in proceedings of the ninth workshop on statistical machine
   translation, wmt '14, pages 402   408, baltimore, md.
   [623]crossref, [624]google scholar
   jurafsky, daniel and james martin. 2008. statistical parsing. in speech
   and language processing, chapter 14. prentice hall. [625]google scholar
   knight, kevin and jonathan graehl. 2005. an overview of probabilistic
   tree transducers for natural language processing. in computational
   linguistics and intelligent text processing, volume 3406 of lecture
   notes in computer science. springer, berlin heidelberg, pages 1   24.
   [626]google scholar
   knott, alistair and robert dale. 1994. using linguistic phenomena to
   motivate a set of coherence relations. discourse processes, 18:35   62.
   [627]crossref, [628]google scholar
   koller, alexander, michaela regneri, and stefan thater. 2008. regular
   tree grammars as a formalism for scope underspecification. in
   proceedings of the 46th annual meeting of the association for
   computational linguistics on human language technologies, pages
   218   226, columbus, oh. [629]google scholar
   lafferty, john, andrew mccallum, and fernando pereira. 2001.
   id49: probabilistic models for segmenting and
   labeling sequence data. in proceedings of the eighteenth international
   conference on machine learning, pages 282   289, san francisco, ca.
   [630]google scholar
   lazaridou, angeliki, ivan titov, and caroline sporleder. 2013. a
   bayesian model for joint unsupervised induction of sentiment, aspect
   and discourse representations. in proceedings of the 51st annual
   meeting of the association for computational linguistics, acl '13,
   sofia. [631]google scholar
   li, jiwei, rumeng li, and eduard hovy. 2014. recursive deep models for
   discourse parsing. in proceedings of the 2014 conference on empirical
   methods in natural language processing (emnlp), pages 2061   2069, doha.
   [632]crossref, [633]google scholar
   li, sujian, liang wang, ziqiang cao, and wenjie li. 2014. text-level
   discourse id33. in proceedings of the 52nd annual meeting
   of the association for computational linguistics (volume 1: long
   papers), pages 25   35, baltimore, md. [634]crossref, [635]google scholar
   louis, annie, aravind joshi, and ani nenkova. 2010. discourse
   indicators for content selection in summarization. in proceedings of
   the 11th annual meeting of the special interest group on discourse and
   dialogue, sigdial '10, pages 147   156, tokyo. [636]google scholar
   mach    ek, matou   and ond  ej bojar. 2014. results of the wmt14 metrics
   shared task. in proceedings of the ninth workshop on statistical
   machine translation, baltimore, md. [637]google scholar
   magerman, david. 1995. statistical decision-tree models for parsing. in
   proceedings of the 33rd annual meeting of the association for
   computational linguistics, acl'95, pages 276   283, cambridge, ma.
   [638]crossref, [639]google scholar
   mann, william and sandra thompson. 1988. rhetorical structure theory:
   toward a functional theory of text organization. text, 8(3):243   281.
   [640]crossref, [641]google scholar
   marcu, daniel. 1999. a decision-based approach to rhetorical parsing.
   in proceedings of the 37th annual meeting of the association for
   computational linguistics on computational linguistics, acl'99, pages
   365   372, morristown, nj. [642]crossref, [643]google scholar
   marcu, daniel. 2000a. the rhetorical parsing of unrestricted texts: a
   surface-based approach. computational linguistics, 26:395   448.
   [644]link, [645]google scholar
   marcu, daniel. 2000b. the theory and practice of discourse parsing and
   summarization. mit press, cambridge, ma. [646]google scholar
   marcu, daniel and abdessamad echihabi. 2002. an unsupervised approach
   to recognizing discourse relations. in proceedings of the 40th annual
   meeting of the association for computational linguistics, acl'02, pages
   368   375. philadelphia, pa. [647]google scholar
   marcus, mitchell, mary marcinkiewicz, and beatrice santorini. 1994.
   building a large annotated corpus of english: the id32.
   computational linguistics, 19(2):313   330. [648]google scholar
   martin, james, 1992. english text: system and structure. john benjamins
   publishing company, philadelphia/amsterdam. [649]google scholar
   maslennikov, mstislav and tat-seng chua. 2007. a multi-resolution
   framework for information extraction from free text. in proceedings of
   the 45th annual meeting of the association for computational
   linguistics, pages 592   599, prague. [650]google scholar
   mccallum, andrew. 2002. mallet: a machine learning for language
   toolkit. [651]http://mallet.cs.umass.edu. [652]google scholar
   mccallum, andrew, dayne freitag, and fernando c. n. pereira. 2000.
   maximum id178 markov models for information extraction and
   segmentation. in proceedings of the seventeenth international
   conference on machine learning, icml '00, pages 591   598, san francisco,
   ca. [653]google scholar
   mcdonald, ryan, koby crammer, and fernando pereira. 2005. online
   large-margin training of dependency parsers. in proceedings of the 43rd
   annual meeting of the association for computational linguistics, acl
   '05, pages 91   98, ann arbor, mi. [654]crossref, [655]google scholar
   mcdonald, ryan, fernando pereira, kiril ribarov, and jan haji  . 2005.
   non-projective id33 using spanning tree algorithms. in
   proceedings of the conference on human language technology and
   empirical methods in natural language processing, hlt '05, pages
   523   530, stroudsburg, pa. [656]crossref, [657]google scholar
   morris, jane and graeme hirst. 1991. lexical cohesion computed by
   thesaural relations as an indicator of structure of text. computational
   linguistics, 17(1):21   48. [658]google scholar
   murphy, kevin. 2012. machine learning: a probabilistic perspective. the
   mit press. cambridge, ma. [659]google scholar
   pang, bo and lillian lee. 2004. a sentimental education: sentiment
   analysis using subjectivity summarization based on minimum cuts. in
   proceedings of the 42nd annual meeting of the association for
   computational linguistics, acl '04, pages 271   278. barcelona.
   [660]crossref, [661]google scholar
   pitler, emily and ani nenkova. 2009. using syntax to disambiguate
   explicit discourse connectives in text. in proceedings of the
   acl-ijcnlp 2009 conference short papers, aclshort '09, pages 13   16,
   suntec. [662]google scholar
   poole, david and alan mackworth, 2010. artificial intelligence:
   foundations of computational agents. cambridge university press.
   [663]google scholar
   prasad, rashmi, nikhil dinesh, alan lee, eleni miltsakaki, livio
   robaldo, aravind joshi, and bonnie webber. 2008. the penn discourse
   treebank 2.0. in proceedings of the sixth international conference on
   language resources and evaluation (lrec), pages 2961   2968, marrakech.
   [664]google scholar
   prasad, rashmi, aravind joshi, nikhil dinesh, alan lee, eleni
   miltsakaki, and bonnie webber. 2005. the penn discourse treebank as a
   resource for id86. in proceedings of the corpus
   linguistics workshop on using corpora for id86,
   pages 25   32, birmingham. [665]google scholar
   regneri, michaela, markus egg, and alexander koller. 2008. efficient
   processing of underspecified discourse representations. in proceedings
   of the 46th annual meeting of the association for computational
   linguistics on human language technologies: short papers, hlt-short
   '08, pages 245   248, columbus, oh. [666]crossref, [667]google scholar
   reyle, uwe. 1993. dealing with ambiguities by underspecification:
   construction, representation and deduction. journal of semantics,
   10(2):123   179. [668]crossref, [669]google scholar
   schapire, robert e. and yoram singer. 2000. boostexter: a
   boosting-based system for text categorization. machine learning,
   39(2   3):135   168. [670]crossref, [671]google scholar
   schauer, holger and udo hahn. 2001. anaphoric cues for coherence
   relations. in proceedings of the conference on recent advances in
   natural language processing, ranlp '01, pages 228   234, tzigov chark.
   [672]google scholar
   schilder, frank. 2002. robust discourse parsing via discourse markers,
   topicality and position. natural language engineering, 8(3):235   255.
   [673]google scholar
   sha, fei and fernando pereira. 2003. id66 with conditional
   random fields. in proceedings of the 2003 conference of the north
   american chapter of the association for computational linguistics on
   human language technology - volume 1, naacl-hlt'03, pages 134   141,
   edmonton. [674]crossref, [675]google scholar
   silber, gregory and kathleen mccoy. 2002. efficiently computed lexical
   chains as an intermediate representation for automatic text
   summarization. computational linguistics, 28(4):487   496.
   [676]link, [677]google scholar
   smith, noah a. 2011. linguistic structure prediction. synthesis
   lectures on human language technologies. morgan and claypool.
   [678]google scholar
   socher, richard, john bauer, christopher d. manning, and ng andrew y.
   2013a. parsing with compositional vector grammars. in proceedings of
   the 51st annual meeting of the association for computational
   linguistics (volume 1: long papers), pages 455   465, sofia. [679]google
   scholar
   socher, richard, alex perelygin, jean wu, jason chuang, christopher d.
   manning, andrew ng, and christopher potts. 2013b. recursive deep models
   for semantic compositionality over a sentiment treebank. in proceedings
   of the 2013 conference on empirical methods in natural language
   processing, pages 1631   1642, seattle, wa. [680]google scholar
   somasundaran, s. 2010. discourse-level relations for opinion analysis.
   ph.d. thesis, university of pittsburgh, pa. [681]google scholar
   soricut, radu and daniel marcu. 2003. sentence level discourse parsing
   using syntactic and lexical information. in proceedings of the 2003
   conference of the north american chapter of the association for
   computational linguistics on human language technology - volume 1,
   naacl'03, pages 149   156, edmonton. [682]crossref, [683]google scholar
   sporleder, caroline. 2007. manually vs. automatically labelled data in
   discourse relation classification. effects of example and feature
   selection. ldv forum, 22(1):1   20. [684]google scholar
   sporleder, caroline and mirella lapata. 2004. automatic paragraph
   identification: a study across languages and domains. in proceedings of
   the 2004 conference on empirical methods in natural language
   processing, emnlp '04, pages 72   79, barcelona. [685]google scholar
   sporleder, caroline and mirella lapata. 2005. discourse chunking and
   its application to sentence compression. in proceedings of the
   conference on human language technology and empirical methods in
   natural language processing, hlt-emnlp'05, pages 257   264, vancouver.
   [686]crossref, [687]google scholar
   sporleder, caroline and alex lascarides. 2005. exploiting linguistic
   cues to classify rhetorical relations. in proceedings of recent
   advances in natural language processing (ranlp), pages 157   166,
   bulgaria. [688]google scholar
   sporleder, caroline and alex lascarides. 2008. using automatically
   labelled examples to classify rhetorical relations: an assessment.
   natural language engineering, 14(3):369   416. [689]crossref, [690]google
   scholar
   stede, manfred. 2004. the potsdam commentary corpus. in proceedings of
   the acl-04 workshop on discourse annotation, pages 96   102, barcelona.
   [691]google scholar
   stede, manfred. 2011. discourse processing. synthesis lectures on human
   language technologies. morgan and claypool publishers. [692]google
   scholar
   subba, rajen and barbara di-eugenio. 2009. an effective discourse
   parser that uses rich linguistic information. in proceedings of human
   language technologies: the 2009 annual conference of the north american
   chapter of the association for computational linguistics, hlt-naacl'09,
   pages 566   574, boulder, co. [693]crossref, [694]google scholar
   sutton, charles and andrew mccallum. 2012. an introduction to
   id49. foundations and trends in machine learning,
   4(4):267   373. [695]crossref, [696]google scholar
   sutton, charles, andrew mccallum, and khashayar rohanimanesh. 2007.
   dynamic id49: factorized probabilistic models for
   labeling and segmenting sequence data. journal of machine learning
   research (jmlr), 8:693   723. [697]google scholar
   taboada, maite. 2006. discourse markers as signals (or not) of
   rhetorical relations. journal of pragmatics, 38(4):567   592.
   [698]crossref, [699]google scholar
   taboada, maite, julian brooke, milan tofiloski, kimberly voll, and
   manfred stede. 2011. lexicon-based methods for id31.
   computational linguistics, 37(2):267   307. [700]link, [701]google
   scholar
   taboada, maite and william c. mann. 2006. rhetorical structure theory:
   looking back and moving ahead. discourse studies, 8(3):423   459.
   [702]crossref, [703]google scholar
   teufel, simone and marc moens. 2002. summarizing scientific articles:
   experiments with relevance and rhetorical status. computational
   linguistics, 28(4):409   445. [704]link, [705]google scholar
   verberne, suzan, lou boves, nelleke oostdijk, and peter-arno coppen.
   2007. evaluating discourse-based answer extraction for why-question
   answering. in proceedings of the 30th annual international acm sigir
   conference on research and development in information retrieval,
   sigir'07, pages 735   736, amsterdam. [706]crossref, [707]google scholar
   vliet, nynke and gisela redeker. 2011. complex sentences as leaky units
   in discourse parsing. in proceedings of constraints in discourse, pages
   1   9, agay   saint raphael. [708]google scholar
   webber, b. 2004. d-ltag: extending lexicalized tag to discourse.
   cognitive science, 28(5):751   779. [709]crossref, [710]google scholar
   webber, bonnie, andrei popescu-belis, katja markert, and j  rg
   tiedemann, editors. 2013. proceedings of the workshop on discourse in
   machine translation. acl, sofia. [711]google scholar
   wick, michael, khashayar rohanimanesh, kedare bellare, aron culotta,
   and andrew mccallum. 2011. samplerank: training factor graphs with
   atomic gradients. in proceedings of the 28th international conference
   on machine learning, icml'11, pages 777   784. bellevue, wa. [712]google
   scholar
   wolf, florian and edward gibson. 2005. representing discourse
   coherence: a corpus-based study. computational linguistics, 31:249   288.
   [713]link, [714]google scholar
   zhang, yuan, tao lei, regina barzilay, tommi jaakkola, and amir
   globerson. 2014. steps to excellence: simple id136 with refined
   scoring of dependency trees. in proceedings of the 52nd annual meeting
   of the association for computational linguistics, pages 197   207,
   baltimore, md. [715]google scholar
   shafiq joty*
   qatar computing research institute
   giuseppe carenini**
   university of british columbia
   raymond t. ng   
   university of british columbia

   *arabic language technologies, qatar computing research institute,
   qatar foundation, doha, qatar. e-mail: [716][email protected].

   **computer science department, university of british columbia,
   vancouver, bc, canada, v6t 1z4. e-mail: [717][email protected].

      computer science department, university of british columbia,
   vancouver, bc, canada, v6t 1z4. e-mail: [718][email protected].
   [719]https://doi.org/10.1162/coli_a_00226
     * [720]abstract
     * [721]full text
     * [722]authors

abstract

   section:
   [choose________________________]
   [723]next section

   clauses and sentences rarely stand on their own in an actual discourse;
   rather, the relationship between them carries important information
   that allows the discourse to express a meaning as a whole beyond the
   sum of its individual parts. rhetorical analysis seeks to uncover this
   coherence structure. in this article, we present codra    a complete
   probabilistic discriminative framework for performing rhetorical
   analysis in accordance with rhetorical structure theory, which posits a
   tree representation of a discourse.

   codra comprises a discourse segmenter and a discourse parser. first,
   the discourse segmenter, which is based on a binary classifier,
   identifies the elementary discourse units in a given text. then the
   discourse parser builds a discourse tree by applying an optimal parsing
   algorithm to probabilities inferred from two id49:
   one for intra-sentential parsing and the other for multi-sentential
   parsing. we present two approaches to combine these two stages of
   parsing effectively. by conducting a series of empirical evaluations
   over two different data sets, we demonstrate that codra significantly
   outperforms the state-of-the-art, often by a wide margin. we also show
   that a reranking of the k-best parse hypotheses generated by codra can
   potentially improve the accuracy even further.
   no rights reserved. this work was authored as part of the contributor's
   official duties as an employee of the united states government and is
   therefore a work of the united states government. in accordance with 17
   u.s.c. 105, no copyright protection is available for such works under
   u.s. law.
   1.   introduction
   section:
   [choose________________________]
   [724]previous section [725]next section

   a well-written text is not merely a sequence of independent and
   isolated sentences, but instead a sequence of structured and related
   sentences, where the meaning of a sentence relates to the previous and
   the following ones. in other words, a well-written text has a coherence
   structure (halliday and hasan [726]1976; hobbs [727]1979), which
   logically binds its clauses and sentences together to express a meaning
   as a whole. rhetorical analysis seeks to uncover this coherence
   structure underneath the text; this has been shown to be beneficial for
   many natural language processing (nlp) applications, including text
   summarization and compression (marcu [728]2000b; daum   and marcu
   [729]2002; sporleder and lapata [730]2005; louis, joshi, and nenkova
   [731]2010), text generation (prasad et al. [732]2005), machine
   translation evaluation (guzm  n et al. [733]2014a, [734]2014b; joty et
   al. [735]2014), id31 (somasundaran [736]2010; lazaridou,
   titov, and sporleder [737]2013), information extraction (teufel and
   moens [738]2002; maslennikov and chua [739]2007), and question
   answering (verberne et al. [740]2007). furthermore, rhetorical
   structures can be useful for other discourse analysis tasks, including
   co-reference resolution using veins theory (cristea, ide, and romary
   [741]1998).

   different formal theories of discourse have been proposed from
   different view-points to describe the coherence structure of a text.
   for example, martin ([742]1992) and knott and dale ([743]1994) propose
   discourse relations based on the usage of discourse connectives (e.g.,
   because, but) in the text. asher and lascarides ([744]2003) propose
   segmented discourse representation theory, which is driven by sentence
   semantics. webber ([745]2004) and danlos ([746]2009) extend sentence
   grammar to formalize discourse structure. rhetorical structure theory
   (rst), proposed by mann and thompson ([747]1988), is perhaps the most
   influential theory of discourse in computational linguistics. although
   it was initially intended to be used in text generation, later it
   became popular as a framework for parsing the structure of a text
   (taboada and mann [748]2006). rst represents texts by labeled
   hierarchical structures, called discourse trees (dts). for example,
   consider the dt shown in [749]figure 1 for the following text:

   but he added:    some people use the purchasers' index as a leading
   indicator, some use it as a coincident indicator. but the thing it's
   supposed to measure   manufacturing strength   it missed altogether last
   month.   
   [750]figure
   figure   1    discourse tree for two sentences in rst   dt. each sentence
   contains three edus. horizontal lines indicate text segments;
   satellites are connected to their nuclei by curved arrows and two
   nuclei are connected with straight lines.

   the leaves of a dt correspond to contiguous atomic text spans, called
   elementary discourse units (edus; six in the example). edus are
   clause-like units that serve as building blocks. adjacent edus are
   connected by coherence relations (e.g., elaboration, contrast), forming
   larger discourse units (represented by internal nodes), which in turn
   are also subject to this relation linking. discourse units linked by a
   rhetorical relation are further distinguished based on their relative
   importance in the text: nuclei are the core parts of the relation and
   satellites are peripheral or supportive ones. for example, in
   [751]figure 1, elaboration is a relation between a nucleus (edu 4) and
   a satellite (edu 5), and contrast is a relation between two nuclei
   (edus 2 and 3). carlson, marcu, and okurowski ([752]2002) constructed
   the first large rst-annotated corpus (rst   dt) on wall street journal
   articles from the id32. whereas mann and thompson ([753]1988)
   had suggested about 25 relations, the rst   dt uses 53 mono-nuclear and
   25 multi-nuclear relations. the relations are grouped into 16
   coarse-grained categories; see carlson and marcu ([754]2001) for a
   detailed description of the relations. conventionally, rhetorical
   analysis in rst involves two subtasks: discourse segmentation is the
   task of breaking the text into a sequence of edus, and discourse
   parsing is the task of linking the discourse units (edus and larger
   units) into a labeled tree. in this article, we use the terms discourse
   parsing and rhetorical parsing interchangeably.

   while recent advances in automatic discourse segmentation have attained
   high accuracies (an f-score of 90.5% reported by fisher and roark
   [[755]2007]), discourse parsing still poses significant challenges
   (feng and hirst [756]2012) and the performance of the existing
   discourse parsers (soricut and marcu [757]2003; subba and di-eugenio
   [758]2009; hernault et al. [759]2010) is still considerably inferior
   compared with the human gold standard. thus, the impact of rhetorical
   structure in downstream nlp applications is still very limited. the
   work we present in this article aims to reduce this performance gap and
   take discourse parsing one step further. to this end, we address three
   key limitations of existing discourse parsers.

   first, existing discourse parsers typically model the structure and the
   labels of a dt separately, and also do not take into account the
   sequential dependencies between the dt constituents. however, for
   several nlp tasks, it has recently been shown that joint models
   typically outperform independent or pipeline models (murphy [760]2012,
   page 687). this is also supported in a recent study by feng and hirst
   ([761]2012), in which the performance of a greedy bottom   up discourse
   parser improved when sequential dependencies were considered by using
   gold annotations for the neighboring (i.e., previous and next)
   discourse units as contextual features in the parsing model. to address
   this limitation of existing parsers, as the first contribution, we
   propose a novel discourse parser based on probabilistic discriminative
   parsing models, expressed as id49 (crfs) (sutton,
   mccallum, and rohanimanesh [762]2007), to infer the id203 of all
   possible dt constituents. the crf models effectively represent the
   structure and the label of a dt constituent jointly, and, whenever
   possible, capture the sequential dependencies.

   second, existing discourse parsers typically apply greedy and
   sub-optimal parsing algorithms to build a dt. to cope with this
   limitation, we use the inferred (posterior) probabilities from our crf
   parsing models in a probabilistic cky-like bottom   up parsing algorithm
   (jurafsky and martin [763]2008), which is non-greedy and optimal.
   furthermore, a simple modification of this parsing algorithm allows us
   to generate k-best (i.e., the k highest id203) parse hypotheses
   for each input text that could then be used in a reranker to improve
   over the initial ranking using additional (global) features of the
   discourse tree as evidence, a strategy that has been successfully
   explored in syntactic parsing (charniak and johnson [764]2005; collins
   and koo [765]2005).

   third, most of the existing discourse parsers do not discriminate
   between intra-sentential parsing (i.e., building the dts for the
   individual sentences) and multi-sentential parsing (i.e., building the
   dt for the whole document). however, we argue that distinguishing
   between these two parsing conditions can result in more effective
   parsing. two separate parsing models could exploit the fact that
   rhetorical relations are distributed differently intra-sententially
   versus multi-sententially. also, they could independently choose their
   own informative feature sets. as another key contribution of our work,
   we devise two different parsing components: one for intra-sentential
   parsing, the other for multi-sentential parsing. this provides for
   scalable, modular, and flexible solutions that can exploit the strong
   correlation observed between the text structure (i.e., sentence
   boundaries) and the structure of the discourse tree.

   in order to develop a complete and robust discourse parser, we combine
   our intra-sentential and multi-sentential parsing components in two
   different ways. because most sentences have a well-formed discourse
   sub-tree in the full dt (e.g., the second sentence in [766]figure 1),
   our first approach constructs a dt for every sentence using our
   intra-sentential parser, and then runs the multi-sentential parser on
   the resulting sentence-level dts to build a complete dt for the whole
   document. however, this approach would fail in those cases where
   discourse structures violate sentence boundaries, also called    leaky   
   boundaries (vliet and redeker [767]2011). for example, consider the
   first sentence in [768]figure 1. it does not have a well-formed
   discourse sub-tree because the unit containing edus 2 and 3 merges with
   the next sentence and only then is the resulting unit merged with edu
   1. our second approach, in order to deal with these leaky cases, builds
   sentence-level sub-trees by applying the intra-sentential parser on a
   sliding window covering two adjacent sentences and by then
   consolidating the results produced by overlapping windows. after that,
   the multi-sentential parser takes all these sentence-level sub-trees
   and builds a full dt for the whole document.

   our discourse parser assumes that the input text has already been
   segmented into elementary discourse units. as an additional
   contribution, we propose a novel discriminative approach to discourse
   segmentation that not only achieves state-of-the-art performance, but
   also reduces time and space complexities by using fewer features.
   notice that the combination of our segmenter with our parser forms a
   complete probabilistic discriminative framework for rhetorical analysis
   (codra).

   whereas previous systems have been tested on only one corpus, we
   evaluate our framework on texts from two very different genres: news
   articles and instructional how-to manuals. the results demonstrate that
   our approach to discourse parsing provides consistent and statistically
   significant improvements over previous methods both at the sentence
   level and at the document level. the performance of our final system
   compares very favorably to the performance of state-of-the-art
   discourse parsers. finally, the oracle accuracy computed based on the
   k-best parse hypotheses generated by our parser demonstrates that a
   reranker could potentially improve the accuracy further.

   after discussing related work in [769]section 2, we present our
   rhetorical analysis framework in [770]section 3. in [771]section 4, we
   describe our discourse parser. then, in [772]section 5 we present our
   discourse segmenter. the experiments and analysis of results are
   presented in [773]section 6. finally, we summarize our contributions
   with future directions in [774]section 7.
   2.   related work
   section:
   [choose________________________]
   [775]previous section [776]next section

   rhetorical analysis has a long history   dating back to mann and thompson
   ([777]1988), when rst was initially proposed as a useful linguistic
   method for describing natural texts, to more recent attempts to
   automatically extract the rhetorical structure of a given text
   (hernault et al. [778]2010). in this section, we provide a brief
   overview of the computational approaches that follow rst as the theory
   of discourse, and that are related to our work; see the survey by stede
   ([779]2011) for a broader overview that also includes other theories of
   discourse.
   2.1   unsupervised and rule-based approaches

   although the most effective approaches to rhetorical analysis to date
   rely on supervised machine learning methods trained on human-annotated
   data, unsupervised methods have also been proposed, as they do not
   require human-annotated data and can be more easily applied to new
   domains.

   often, discourse connectives like but, because, and although convey
   clear information on the kind of relation linking the two text
   segments. in his early work, marcu ([780]2000a) presented a shallow
   rule-based approach relying on discourse connectives (or cues) and
   surface patterns. he used hand-coded rules, derived from an extensive
   corpus study, to break the text into edus and to build dts for
   sentences first, then for paragraphs, and so on. despite the fact that
   this work pioneered the field of rhetorical analysis, it has many
   limitations. first, identifying discourse connectives is a difficult
   task on its own, because (depending on the usage), the same phrase may
   or may not signal a discourse relation (pitler and nenkova [781]2009).
   for example, but can either signal a contrast discourse relation or can
   simply perform non-discourse acts. second, discourse segmentation using
   only discourse connectives fails to attain high accuracy (soricut and
   marcu [782]2003). third, dt structures do not always correspond to
   paragraph structures; for example, sporleder and lapata ([783]2004)
   report that more than 20% of the paragraphs in the rst   dt corpus
   (carlson, marcu, and okurowski [784]2002) do not correspond to a
   discourse unit in the dt. fourth, discourse cues are sometimes
   ambiguous; for example, but can signal contrast, antithesis and
   concession, and so on.

   finally, a more serious problem with the rule-based approach is that
   often rhetorical relations are not explicitly signaled by discourse
   cues. for example, in rst   dt, marcu and echihabi ([785]2002) found that
   only 61 out of 238 contrast relations and 79 out of 307
   cause   explanation relations were explicitly signaled by cue phrases. in
   the british national corpus, sporleder and lascarides ([786]2008)
   report that half of the sentences lack a discourse cue. other studies
   (schauer and hahn [787]2001; stede [788]2004; taboada [789]2006; subba
   and di-eugenio [790]2009) report even higher figures: about 60% of
   discourse relations are not explicitly signaled. therefore, rather than
   relying on hand-coded rules based on discourse cues and surface
   patterns, recent approaches use machine learning techniques with a
   large set of informative features.

   while some rhetorical relations need to be explicitly signaled by
   discourse cues (e.g., concession) and some do not (e.g., background),
   there is a large middle ground of relations that may be signaled or
   not. for these    middle ground    relations, can we exploit features
   present in the signaled cases to automatically identify relations when
   they are not explicitly signaled? the idea is to use unambiguous
   discourse cues (e.g., although for contrast, for example for
   elaboration) to automatically label a large corpus with rhetorical
   relations that could then be used to train a supervised model.[791]^1

   a series of previous studies have explored this idea. marcu and
   echihabi ([792]2002) first attempted to identify four broad classes of
   relations: contrast, elaboration, condition, and
   cause   explanation   evidence. they used a naive bayes classifier based on
   word pairs (w[1], w[2]), where w[1] occurs in the left segment, and
   w[2]occurs in the right segment. sporleder and lascarides ([793]2005)
   included other features (e.g., words and their stems, part-of-speech
   [pos] tags, positions, segment lengths) in a boosting-based classifier
   (i.e., boostexter [schapire and singer [794]2000]) to further improve
   relation classification accuracy. however, these studies evaluated
   classification performance on the instances where rhetorical relations
   were originally signaled (i.e., the discourse cues were artificially
   removed), and did not verify how well this approach performs on the
   instances that are not originally signaled. subsequent studies
   (blair-goldensohn, mckeown, and rambow [795]2007; sporleder [796]2007;
   sporleder and lascarides [797]2008) confirm that classifiers trained on
   instances stripped of their original discourse cues do not generalize
   well to implicit cases because they are linguistically quite different.

   note that this approach to identifying discourse relations in the
   absence of manually labeled data does not fully solve the parsing
   problem (i.e., building dts); rather, it only attempts to identify a
   small subset of coarser relations between two (flat) text segments
   (i.e., a tagging problem). arguably, to perform a complete rhetorical
   analysis, one needs to use supervised machine learning techniques based
   on human-annotated data.
   2.2   supervised approaches

   marcu ([798]1999) applies supervised machine learning techniques to
   build a discourse segmenter and a shift   reduce discourse parser. both
   the segmenter and the parser rely on c4.5 decision tree classifiers
   (poole and mackworth [799]2010) to learn the rules automatically from
   the data. the discourse segmenter mainly uses discourse cues,
   shallow-syntactic (i.e., pos tags) and contextual features (i.e.,
   neighboring words and their pos tags). to learn the shift   reduce
   actions, the discourse parser encodes five types of features: lexical
   (e.g., discourse cues), shallow-syntactic, textual similarity,
   operational (previous n shift   reduce operations), and rhetorical
   sub-structural features. despite the fact that this work has pioneered
   many of today's machine learning approaches to discourse parsing, it
   has all the limitations mentioned in [800]section 1.

   the work of marcu ([801]1999) is considerably improved by soricut and
   marcu ([802]2003). they present the publicly available spade
   system,[803]^2 which comes with probabilistic models for discourse
   segmentation and sentence-level discourse parsing. their segmentation
   and parsing models are based on lexico-syntactic patterns (or features)
   extracted from the lexicalized syntactic tree of a sentence. the
   discourse parser uses an optimal parsing algorithm to find the most
   probable dt structure for a sentence. spade was trained and tested on
   the rst   dt corpus. this work, by showing empirically the connection
   between syntax and discourse structure at the sentence level, has
   greatly influenced all major contributions in this area ever since.
   however, it is limited in several ways. first, spade does not produce a
   full-text (i.e., document-level) parse. second, it applies a generative
   parsing model based on only lexico-syntactic features, whereas
   discriminative models are generally considered to be more accurate, and
   can incorporate arbitrary features more effectively (murphy [804]2012).
   third, the parsing model makes an independence assumption between the
   label and the structure of a dt constituent, and it ignores the
   sequential and the hierarchical dependencies between the dt
   constituents.

   subsequent research addresses the question of how much syntax one
   really needs in rhetorical analysis. sporleder and lapata ([805]2005)
   focus on the discourse chunking problem, comprising two subtasks:
   discourse segmentation and (flat) nuclearity assignment. they formulate
   discourse chunking in two alternative ways. first, one-step
   classification, where the discourse chunker, a multi-class classifier,
   assigns to each token one of the four labels: (1) b   nuc (beginning of a
   nucleus), (2) i   nuc (inside a nucleus), (3) b    sat (beginning of a
   satellite), and (4) i   sat (inside a satellite). therefore, this
   approach performs discourse segmentation and nuclearity assignment
   simultaneously. second, two-step classification, where in the first
   step, the discourse segmenter (a binary classifier) labels each token
   as either b (beginning of an edu) or i (inside an edu). then, in the
   second step, a nuclearity labeler (another binary classifier) assigns a
   nuclearity status to each segment. the two-step approach avoids illegal
   chunk sequences like a b   nuc followed by an i   sat or a b   sat followed
   by an i   nuc, and in this approach, it is easier to incorporate
   sentence-level properties like the constraint that a sentence must
   contain at least one nucleus. they examine whether shallow-syntactic
   features (e.g., pos and phrase tags) would be sufficient for these
   purposes. the evaluation on the rst   dt shows that the two-step approach
   outperforms the one-step approach, and its performance is comparable to
   that of spade, which requires relatively expensive full syntactic
   parses.

   in follow   up work, fisher and roark ([806]2007) demonstrate over 4%
   absolute performance gain in discourse segmentation, by combining the
   features extracted from the syntactic tree with the ones derived via
   id52 and shallow syntactic parsing (i.e., chunking). using quite
   a large number of features in a binary log-linear model, they achieve
   state-of-the-art performance in discourse segmentation on the rst   dt
   test set.

   in a different approach, regneri, egg, and koller ([807]2008) propose
   to use underspecified discourse representation (udr) as an intermediate
   representation for discourse parsing. underspecified representations
   offer a single compact representation to express possible ambiguities
   in a linguistic structure, and have been primarily used to deal with
   scope ambiguity in semantic structures (reyle [808]1993; egg, koller,
   and niehren [809]2001; althaus et al. [810]2003; koller, regneri, and
   thater [811]2008). assuming that a udr of a dt is already given in the
   form of a dominance graph (althaus et al. [812]2003), regneri, egg, and
   koller ([813]2008) convert it into a more expressive and complete udr
   representation called regular tree grammar (koller, regneri, and thater
   [814]2008), for which efficient algorithms (knight and graehl
   [815]2005) already exist to derive the best configuration (i.e., the
   best discourse tree).

   hernault et al. ([816]2010) present the publicly available hilda
   system,[817]^3 which comes with a discourse segmenter and a parser
   based on support vector machines (id166s). the discourse segmenter is a
   binary id166 classifier that uses the same lexico-syntactic features used
   in spade, but with more context (i.e., the lexico-syntactic features
   for the previous two words and the following two words). the discourse
   parser iteratively uses two id166 classifiers in a pipeline to build a
   dt. in each iteration, a binary classifier first decides which of the
   adjacent units to merge, then a multi-class classifier connects the
   selected units with an appropriate relation label. using this simple
   method, they report promising results in document-level discourse
   parsing on the rst   dt.

   for a different genre, instructional texts, subba and di-eugenio
   ([818]2009) propose a shift   reduce discourse parser that relies on a
   classifier for relation labeling. their classifier uses inductive logic
   programming (ilp) to learn id85 rules from a large set of
   features including the linguistically rich id152
   coming from a semantic parser. they demonstrate that including
   id152 with other features improves the performance of
   the classifier, thus, also improves the performance of the parser.

   both hilda and the ilp-based approach of subba and di-eugenio
   ([819]2009) are limited in several ways. first, they do not
   differentiate between intra- and multi-sentential parsing, and both
   scenarios use a single uniform parsing model. second, they take a
   greedy (i.e., sub-optimal) approach to construct a dt. third, they
   disregard sequential dependencies between dt constituents. furthermore,
   hilda considers the structure and the labels of a dt separately. our
   discourse parser codra, as described in the next section, addresses all
   these limitations.

   more recent work than ours also attempts to address some of the
   above-mentioned limitations of the existing discourse parsers. similar
   to us, feng and hirst ([820]2014) generate a document-level dt in two
   stages, where a multi-sentential parsing follows an intra-sentential
   one. at each stage, they iteratively use two separate linear-chain crfs
   (lafferty, mccallum, and pereira [821]2001) in a cascade: one for
   predicting the presence of rhetorical relations between adjacent
   discourse units in a sequence, and the other to predict the relation
   label between the two most probable adjacent units to be merged as
   selected by the previous crf. while they use crfs to take into account
   the sequential dependencies between dt constituents, they use them
   greedily during parsing to achieve efficiency. they also propose a
   greedy post-editing step based on an additional feature (i.e., depth of
   a discourse unit) to modify the initial dt, which gives them a
   significant gain in performance. in a different approach, li et al.
   ([822]2014) propose a discourse-level dependency structure to capture
   direct relationships between edus rather than deep hierarchical
   relationships. they first create a discourse dependency treebank by
   converting the deep annotations in rst   dt to shallow head-dependent
   annotations between edus. to find the dependency parse (i.e., an
   optimal spanning tree) for a given text, they apply eisner ([823]1996)
   and maximum spanning tree (mcdonald et al. [824]2005) dependency
   parsing algorithms with the margin infused relaxed algorithm online
   learning framework (mcdonald, crammer, and pereira [825]2005).

   with the successful application of deep learning to numerous nlp
   problems including syntactic parsing (socher et al. [826]2013a),
   id31 (socher et al. [827]2013b), and various tagging
   tasks (collobert et al. [828]2011), a couple of recent studies in
   discourse parsing also use deep neural networks (dnns) and related
   feature representation methods. inspired by the work of socher et al.
   ([829]2013a, [830]2013b), li, li, and hovy ([831]2014) propose a
   recursive dnn for discourse parsing. however, as in socher et al.
   ([832]2013a, [833]2013b), word vectors (i.e., embeddings) are not
   learned explicitly for the task, rather they are taken from collobert
   et al. ([834]2011). given the vectors of the words in an edu, their
   model first composes them hierarchically based on a syntactic parse
   tree to get the vector representation for the edu. adjacent discourse
   units are then merged hierarchically to get the vector representations
   for the higher order discourse units. in every step, the merging is
   done using one binary (structure) and one multi-class (relation)
   classifier, each having a three-layer neural network architecture. the
   cost function for training the model is given by these two cascaded
   classifiers applied at different levels of the dt. similar to our
   method, they use the classifier probabilities in a cky-like parsing
   algorithm to find the global optimal dt. finally, ji and eisenstein
   ([835]2014) present a feature representation learning method in a
   shift   reduce discourse parser (marcu [836]1999). unlike dnns, which
   learn non-linear feature transformations in a maximum likelihood model,
   they learn linear transformations of features in a max margin
   classification model.
   3.   overview of our rhetorical analysis framework
   section:
   [choose___________________________]
   [837]previous section [838]next section

   codra takes as input a raw text and produces a discourse tree that
   describes the text in terms of coherence relations that hold between
   adjacent discourse units (i.e., clauses, sentences) in the text. an
   example dt generated by an online demo of codra is shown in
   [839]appendix a.[840]^4 the color of a node represents its nuclearity
   status: blue denoting nucleus and yellow denoting satellite. the demo
   also allows some useful user interactions   for example, collapsing or
   expanding a node, highlighting an edu, and so on.[841]^5

   codra follows a pipeline architecture, shown in [842]figure 2. given a
   raw text, the first task in the rhetorical analysis pipeline is to
   break the text into a sequence of edus (i.e., discourse segmentation).
   because it is taken for granted that sentence boundaries are also edu
   boundaries (i.e., edus do not span across multiple sentences), the
   discourse segmentation task boils down to finding edu boundaries inside
   sentences. codra uses a maximum id178 model for discourse
   segmentation (see [843]section 5).
   [844]figure
   figure   2    codra architecture.

   once the edus are identified, the discourse parsing problem is
   determining which discourse units (edus or larger units) to relate
   (i.e., the structure), and what relations (i.e., the labels) to use in
   the process of building the dt. specifically, discourse parsing
   requires: (1) a parsing model to explore the search space of possible
   structures and labels for their nodes, and (2) a parsing algorithm for
   selecting the best parse tree(s) among the candidates. a probabilistic
   parsing model like ours assigns a id203 to every possible dt. the
   parsing algorithm then picks the most probable dts.

   the existing discourse parsers (marcu [845]1999; soricut and marcu
   [846]2003; subba and di-eugenio [847]2009; hernault et al. [848]2010)
   described in [849]section 2 use parsing models that disregard the
   structural interdependencies between the dt constituents. however, we
   hypothesize that, like syntactic parsing, discourse parsing is also a
   id170 problem, which involves predicting multiple
   variables (i.e., the structure and the relation labels) that depend on
   each other (smith [850]2011). recently, feng and hirst ([851]2012) also
   found these interdependencies to be critical for parsing performance.
   to capture the structural dependencies between the dt constituents,
   codra uses undirected conditional id114 (i.e., crfs) as its
   parsing models.

   to find the most probable dt, unlike most previous studies (marcu
   [852]1999; subba and di-eugenio [853]2009; hernault et al. [854]2010),
   which adopt a greedy solution, codra applies an optimal cky parsing
   algorithm to the inferred posterior probabilities (obtained from the
   crfs) of all possible dt constituents. furthermore, the parsing
   algorithm allows codra to generate a list of k-best parse hypotheses
   for a given text.

   note that the way crfs and cky are used in codra is quite different
   from the way they are used in syntactic parsing. for example, in the
   crf-based constituency parsing proposed by finkel, kleeman, and manning
   ([855]2008), the id155 distribution of a parse tree
   given a sentence decomposes across factors defined over productions,
   and the standard inside   outside algorithm is used for id136 on
   possible trees. in contrast, codra first uses the standard
   forward   backward algorithm in a    fat    chain structured[856]^6 crf (to
   be discussed in [857]section 4.1.1) to compute the posterior
   probabilities of all possible dt constituents for a given text (i.e.,
   edus); then it uses a cky parsing algorithm to combine those
   probabilities and find the most probable dt.

   another crucial question related to parsing models is whether to use a
   single model or two different models for parsing at the sentence-level
   (i.e., intra-sentential) and at the document-level (i.e.,
   multi-sentential). a simple and straightforward strategy would be to
   use a single unified parsing model for both intra- and multi-sentential
   parsing without distinguishing the two cases, as was previously done
   (marcu [858]1999; subba and di-eugenio [859]2009; hernault et al.
   [860]2010). that approach has the advantages of making the parsing
   process easier, and the model gets more data to learn from. however,
   for a solution like ours, which tries to capture the interdependencies
   between constituents, this would be problematic with respect to
   scalability and inappropriate because of two modeling issues.

   more specifically, for scalability note that the number of valid trees
   grows exponentially with the number of edus in a document.[861]^7
   therefore, an exhaustive search over all the valid dts is often
   infeasible, even for relatively small documents.

   for modeling, a single unified approach is inappropriate for two
   reasons. on the one hand, it appears that discourse relations are
   distributed differently intra- versus multi-sententially. for example,
   [862]figure 3 shows a comparison between the two distributions of the
   eight most frequent relations in the rst   dt training set. notice that
   same   unit is more frequent than joint in the intra-sentential case,
   whereas joint is more frequent than same   unit in the multi-sentential
   case. similarly, the relative distributions of background, contrast,
   cause, and explanation are different in the two parsing scenarios. on
   the other hand, different kinds of features are applicable and
   informative for intra- versus multi-sentential parsing. for example,
   syntactic features like dominance sets (soricut and marcu [863]2003)
   are extremely useful for parsing at the sentence-level, but are not
   even applicable in the multi-sentential case. likewise, lexical chain
   features (sporleder and lapata [864]2004), which are useful for
   multi-sentential parsing, are not applicable at the sentence level.
   [865]figure
   figure   3    distributions of the eight most frequent relations in
   intra-sentential and multi-sentential parsing scenarios on the rst   dt
   training set.

   based on these above observations, codra comprises two separate
   modules: an intra-sentential parser and a multi-sentential parser, as
   shown in [866]figure 2. first, the intra-sentential parser produces one
   or more discourse sub-trees for each sentence. then, the
   multi-sentential parser generates a full dt for the document from these
   sub-trees. both of our parsers have the same two components: a parsing
   model and a parsing algorithm. whereas the two parsing models are
   rather different, the same parsing algorithm is shared by the two
   modules. staging multi-sentential parsing on top of intra-sentential
   parsing in this way allows codra to explicitly exploit the strong
   correlation observed between the text structure and the dt structure,
   as explained in detail in [867]section 4.3.
   4.   the discourse parser
   section:
   [choose________________________]
   [868]previous section [869]next section

   before describing the parsing models and the parsing algorithm of codra
   in detail, we introduce some terminology that we will use throughout
   this article.

   a dt can be formally represented as a set of constituents of the form
   r[i, m, j], where i     m < j. this refers to a rhetorical relation r
   between the discourse unit containing edus i through m and the
   discourse unit containing edus m+1 through j. for example, the dt for
   the second sentence in [870]figure 1 can be represented as
   {elaboration   ns[4,4,5], same   unit   nn[4,5,6]}. notice that in this
   representation, a relation r also specifies the nuclearity status of
   the discourse units involved, which can be one of nucleus   satellite
   (ns), satellite   nucleus (sn), or nucleus   nucleus (nn). attaching
   nuclearity status to the relations allows us to perform the two
   subtasks of discourse parsing, relation identification and nuclearity
   assignment, simultaneously.

   a common assumption made for generating dts effectively is that they
   are binary trees (soricut and marcu [871]2003; hernault et al.
   [872]2010). that is, multi-nuclear relations (e.g., joint, same   unit)
   involving more than two discourse units are mapped to a hierarchical
   right-branching binary tree. for example, a flat joint(e[1], e[2],
   e[3], e[4]) ([873]figure 4a) is mapped to a right-branching binary tree
   joint(e[1], joint(e[2], joint(e[3], e[4]))) ([874]figure 4b).
   [875]figure
   figure   4    multi-nuclear relation and its corresponding binary tree
   representation.
   4.1   parsing models

   as mentioned before, the job of the intra- and multi-sentential parsing
   models of codra is to assign a id203 to each of the constituents
   of all possible dts at the sentence level and at the document level,
   respectively. formally, given the model parameters    at a particular
   parsing scenario (i.e., sentence-level or document-level), for each
   possible constituent r[i, m, j] in a candidate dt at that parsing
   scenario, the parsing model estimates p(r[i, m, j]|  ), which specifies
   a joint distribution over the label r and the structure [i, m, j] of
   the constituent. for example, when applied to the sentences in
   [876]figure 1 separately, the intra-sentential parsing model (with
   learned parameters   [s]) estimates p(r[1, 1, 2]|  [s]), p(r[2, 2,
   3]|  [s]), p(r[1, 2, 3]|  [s]), and p(r[1, 1, 3]|  [s]) for the first
   sentence, and p(r[4, 4, 5]|  [s]), p(r[5, 5, 6]|  [s]), p(r[4, 5,
   6]|  [s]), and p(r[4, 4, 6]|  [s]) for the second sentence, respectively,
   for all r ranging over the set of relations.
   4.1.1   intra-sentential parsing model

   [877]figure 5 shows the parsing model of codra for intra-sentential
   parsing. the observed nodes u[j] (at the bottom) in a sequence
   represent the discourse units (edus or larger units). the first layer
   of hidden nodes are the structure nodes, where s[j]     {0, 1} denotes
   whether two adjacent discourse units u[j   1] and u[j] should be
   connected or not. the second layer of hidden nodes are the relation
   nodes, with r[j]     {1     m} denoting the relation between two adjacent
   units u[j   1] and u[j], where m is the total number of relations in the
   relation set. the connections between adjacent nodes in a hidden layer
   encode sequential dependencies between the respective hidden nodes, and
   can enforce constraints such as the fact that a node must have a unique
   mother, namely, a s[j] = 1 must not follow a s[j   1] = 1. the
   connections between the two hidden layers model the structure and the
   relation of dt constituents jointly.
   [878]figure
   figure   5    the intra-sentential parsing model of codra.

   notice that the probabilistic graphical model shown in [879]figure 5 is
   a chain-structured undirected graphical model (also known as markov
   random field or mrf [murphy [880]2012]) with two hidden layers, i.e.,
   structure chain and relation chain. it becomes a dynamic conditional
   random field (dcrf) (sutton, mccallum, and rohanimanesh [881]2007) when
   we directly model the hidden (output) variables by conditioning the
   clique potentials (i.e., factors) on the observed (input) variables:

   where {  } and {  } are the factors over the edges of the relation and
   structure chains, respectively, and {  } are the factors over the edges
   connecting the relation and structure nodes (i.e., between-chain
   edges). here, x represents input features extracted from the observed
   variables,   [s] = [  [s,r],   [s,s],   [s,c]] are model parameters, and
   z(x,   [s]) is the partition function. we use the standard log-linear
   representation of the factors:

   where f (y, z, x) is a feature vector derived from the input features x
   and the local labels y and z, and   [s,y] is the corresponding weight
   vector   that is,   [s,r] and   [s,s] are the weight vectors for the
   factors over the relation edges and the structure edges, respectively,
   and   [s,c] is the weight vector for the factors over the between-chain
   edges.

   a dcrf is a generalization of linear-chain crfs (lafferty, mccallum,
   and pereira [882]2001) to represent complex interactions between output
   variables (i.e., labels), such as when performing multiple labeling
   tasks on the same sequence. recently, there has been an explosion of
   interest in crfs for solving structured output classification problems,
   with many successful applications in nlp including syntactic parsing
   (finkel, kleeman, and manning [883]2008), syntactic chunking (sha and
   pereira [884]2003), and discourse chunking (ghosh et al. [885]2011) in
   accordance with the penn discourse treebank (prasad et al. [886]2008).

   dcrfs, being a discriminative approach to sequence modeling, have
   several advantages over their generative counterparts such as hidden
   markov models (id48s) and mrfs, which first model the joint distribution
   p(y, x|  ), and then infer the conditional distribution p(y|x,   ). it
   has been advocated that discriminative models are generally more
   accurate than generative ones because they do not    waste resources   
   modeling complex distributions that are observed (i.e., p(x)); instead,
   they focus directly on modeling what we care about, namely, the
   distribution of labels given the data (murphy [887]2012).

   other key advantages include the ability to incorporate arbitrary
   overlapping local and global features, and the ability to relax strong
   independence assumptions. furthermore, crfs surmount the label bias
   problem (lafferty, mccallum, and pereira [888]2001) of the maximum
   id178 markov model (mccallum, freitag, and pereira [889]2000), which
   is considered to be a discriminative version of the id48.
   4.1.2   training and applying the intra-sentential parsing model

   in order to obtain the id203 of the constituents of all candidate
   dts for a sentence, codra applies the intra-sentential parsing model
   (with learned parameters   [s]) recursively to sequences at different
   levels of the dt, and computes the posterior marginals over the
   relation    structure pairs. it uses the standard forward   backward
   algorithm to compute the posterior marginals. to illustrate the
   process, let us assume that the sentence contains four edus, e[1],     ,
   e[4] (see [890]figure 6). at the first (i.e., bottom) level of the dt,
   when all the discourse units are edus, there is only one unit sequence
   (e[1], e[2], e[3], e[4]) to which codra applies the dcrf model.
   [891]figure 6a at the top left shows the corresponding dcrf model. for
   this sequence it computes the posterior marginals p(r[2], s[2] =
   1|e[1], e[2], e[3], e[4],   [s]), p(r[3], s[3] = 1|e[1], e[2], e[3],
   e[4],   [s]), and p(r[4], s[4] = 1| e[1], e[2], e[3], e[4],   [s]) to
   obtain the id203 of the dt constituents r[1, 1, 2], r[2, 2, 3],
   and r[3, 3, 4], respectively.
   [892]figure
   figure   6    the intra-sentential parsing model is applied to (a) the only
   possible sequence at the first level, (b) the three possible sequences
   at the second level, and (c) the three possible sequences at the third
   level.

   at the second level, there are three unit sequences: (e[1:2], e[3],
   e[4]), (e[1],e[2:3], e[4]), and (e[1],e[2],e[3:4]). [893]figure 6b
   shows their corresponding dcrf models. notice that each of these
   sequences has a discourse unit that connects two edus, and the
   id203 of this connection has already been computed at the
   previous level. codra computes the posterior marginals p(r[3], s[3] =
   1|e[1:2], e[3], e[4],   [s]), p(r[2:3] s[2:3] = 1|e[1], e[2:3], e[4],
     [s]), p(r[4], s[4] = 1|e[1], e[2:3], e[4],   [s]), and p(r[3:4], s[3:4]
   = 1|e[1], e[2], e[3:4],   [s]) from these three sequences, which
   correspond to the id203 of the constituents r[1, 2, 3], r[1, 1,
   3], r[2, 3, 4], and r[2, 2, 4], respectively. similarly, it attains the
   id203 of the constituents r[1, 1, 4], r[1, 2, 4], and r[1, 3, 4]
   by computing their respective posterior marginals from the three
   sequences at the third (i.e., top) level of the candidate dts (see
   [894]figure 6c).

   algorithm 1 describes how codra generates the unit sequences at
   different levels of the candidate dts for a given number of edus in a
   sentence. specifically, to compute the id203 of a dt constituent
   r[i, k, j], codra generates sequences like (e[1],     , e[i   1], e[i:k],
   e[k+1:j], e[j+1],     , e[n]) for 1     i     k < j     n. however, in doing
   so, it may generate some duplicate sequences. clearly, the sequence
   (e[1],     , e[i   1], e[i:i], e[i+1:j], e[j+1],     , e[n]) for 1     i     k <
   j < n is already considered for computing the id203 of the
   constituent r[i + 1, j, j + 1]. therefore, it is a duplicate sequence
   that codra excludes from the list of sequences. the algorithm has a
   complexity of o(n^3), where n is the number of edus in the sentence.

   once codra acquires the id203 of all possible intra-sentential dt
   constituents, the discourse sub-trees for the sentences are built by
   applying an optimal parsing algorithm ([895]section 4.2) using one of
   the methods described in [896]section 4.3.

   algorithm 1 is also used to generate sequences for training the model
   (i.e., learning   [s]). for example, [897]figure 7 demonstrates how we
   generate the training instances (right) from a gold dt with four edus
   (left). to find the relevant labels for the sequences generated by the
   algorithm, we consult the gold dt and see if two discourse units are
   connected by a relation r (i.e., the corresponding labels are s = 1, r
   = r) or not (i.e., the corresponding labels are s = 0, r =nr). we train
   the model by maximizing the conditional likelihood of the labels in
   each of these training examples (see [898]equation (1)).
   [899]figure
   figure   7    a gold discourse tree (left), and the 7 training instances it
   generates (right). nr = no relation.
   4.1.3   multi-sentential parsing model

   given the discourse units (sub-trees) for all the individual sentences
   in a document, a simple approach to build the dt of the document would
   be to apply a new dcrf model, similar to the one in [900]figure 5 (with
   different parameters), to all the possible sequences generated from
   these units by algorithm 1 to infer the id203 of all possible
   higher-order (multi-sentential) constituents. however, the number of
   possible sequences and their length increase with the number of
   sentences in a document. for example, assuming that each sentence has a
   well-formed dt, for a document with n sentences, algorithm 1 generates
   o(n^3) sequences, where the sequence at the bottom level has n units,
   each of the sequences at the second level has n-1 units, and so on.
   because the dcrf model in [901]figure 5 has a    fat    chain structure,
   one could use the forward   backward algorithm for exact id136 in
   this model (murphy [902]2012). forward   backward on a sequence
   containing t units costs o(tm^2) time, where m is the number of
   relations in our relation set. this makes the chain-structured dcrf
   model impractical for multi-sentential parsing of long documents, since
   learning requires running id136 on every training sequence with an
   overall time complexity of o(tm^2 n^3) = o(m^2 n^4) per document
   (sutton and mccallum [903]2012).

   to address this problem, we have developed a simplified parsing model
   for multi-sentential parsing. our model is shown in [904]figure 8. the
   two observed nodes u[t   1] and u[t] are two adjacent (multi-sentential)
   discourse units. the (hidden) structure node s     {0, 1} denotes whether
   the two discourse units should be linked or not. the other hidden node
   r     {1     m} represents the relation between the two units. notice that
   similar to the model in [905]figure 5, this is also an undirected
   graphical model and becomes a crf model if we directly model the labels
   by conditioning the clique potential    on the input features x, derived
   from the observed variables:

   where f (r[t], s[t], x) is a feature vector derived from the input
   features x and the labels r[t] and s[t], and   [d] is the corresponding
   weight vector. although this model is similar in spirit to the parsing
   model in [906]figure 5, it now breaks the chain structure, which makes
   the id136 much faster (i.e., a complexity of o(m^2)). breaking the
   chain structure also allows codra to balance the data for training (an
   equal number of instances with s=1 and s=0), which dramatically reduces
   the learning time of the model.
   [907]figure
   figure   8    the multi-sentential parsing model of codra

   codra applies this parsing model to all possible adjacent units at all
   levels in the multi-sentential case, and computes the posterior
   marginals of the relation   structure pairs p(r[t], s[t] = 1|u[t   1],
   u[t],   [d]) using the forward   backward algorithm to obtain the
   id203 of all possible dt constituents. given the sentence-level
   discourse units, algorithm 2, which is a simplified variation of
   algorithm 1, extracts all possible adjacent discourse units for
   multi-sentential parsing. similar to algorithm 1, algorithm 2 also has
   a complexity of o(n^3), where n is the number of sentence-level
   discourse units.

   both our intra- and multi-sentential parsing models are designed using
   mallet's graphical model toolkit grmm (mccallum [908]2002). in order to
   avoid overfitting, we regularize the crf models with l[2]
   id173 and learn the model parameters using the limited-memory
   bfgs (l-bfgs) fitting algorithm.
   4.1.4   features used in the parsing models

   crucial to parsing performance is the set of features used in the
   parsing models, as summarized in [909]table 1. we categorize the
   features into seven groups and specify which groups are used in what
   parsing model. notice that some of the features are used in both
   models. most of the features have been explored in previous studies
   (e.g., soricut and marcu [910]2003; sporleder and lapata [911]2005;
   hernault et al. [912]2010). however, we improve some of these as
   explained subsequently.

   [913]table
   table   1    features used in our intra- and multi-sentential parsing
   models.

   the features are extracted from two adjacent discourse units u[t   1] and
   u[t]. organizational features encode useful information about text
   organization as shown by duverle and prendinger ([914]2009). we measure
   the length of the discourse units as the number of edus and tokens in
   it. however, in order to better adjust to the length variations, rather
   than computing their absolute numbers in a unit, we choose to measure
   their relative numbers with respect to their total numbers in the two
   units. for example, if the two discourse units under consideration
   contain three edus in total, a unit containing two of the edus will
   have a relative edu number of 0.67. we also measure the distances of
   the units in terms of the number of edus from the beginning and end of
   the sentence (or text in the multi-sentential case). text structural
   features capture the correlation between text structure and rhetorical
   structure by counting the number of sentence and paragraph boundaries
   in the discourse units.

   discourse cues (e.g., because, but), when present, signal rhetorical
   relations between two text segments, and have been used as a primary
   source of information in earlier studies (knott and dale [915]1994;
   marcu [916]2000a). however, recent studies (hernault et al. [917]2010;
   biran and rambow [918]2011) suggest that an empirically acquired
   lexical id165 dictionary is more effective than a fixed list of cue
   phrases, since this approach is domain independent and capable of
   capturing non-lexical cues such as punctuation.

   in order to build a lexical id165 dictionary empirically from the
   training corpus, we extract the first and last n tokens (n   {1, 2, 3})
   of each discourse unit and rank them according to their mutual
   information with the two labels, structure (s) and relation (r). more
   specifically, given an id165 x, we compute its conditional id178 h
   with respect to s and r as follows:[919]^8

   where c(x) is the empirical count of id165 x, and c(x, s, r) is the
   joint empirical count of id165 x with the labels s and r. this is in
   contrast to hilda (hernault et al. [920]2010), which ranks the id165s
   by their frequencies in the training corpus. however, blitzer
   ([921]2008) found mutual information to be more effective than
   frequency as a method for feature selection. intuitively, the most
   informative discourse cues are not only the most frequent, but also the
   ones that are indicative of the labels in the training data. in
   addition to the lexical id165s we also encode the pos tags of the
   first and last n tokens (n   {l, 2, 3}) in a discourse unit as
   shallow-syntactic features in our models.

   lexico-syntactic features dominance sets extracted from the discourse
   segmented lexicalized syntactic tree (ds-lst) of a sentence have been
   shown to be extremely effective for intra-sentential discourse parsing
   in spade (soricut and marcu [922]2003). [923]figure 9a shows the ds-lst
   (i.e., lexicalized syntactic tree with edus identified) for a sentence
   with three edus from the rst   dt corpus, and [924]figure 9b shows the
   corresponding discourse tree. in a ds-lst, each edu except the one with
   the root node must have a head node n[h] that is attached to an
   attachment node n[a] residing in a separate edu. a dominance set d
   (shown at the bottom of [925]figure 9a) contains these attachment
   points (shown in boxes) of the edus in a ds-lst. in addition to the
   syntactic and lexical information of the head and attachment nodes,
   each element in the dominance set also includes a dominance
   relationship between the edus involved; the edu with the attachment
   node dominates (represented by    >   ) the edu with the head node.
   [926]figure
   figure   9    dominance set features for intra-sentential discourse
   parsing.

   soricut and marcu ([927]2003) hypothesize that the dominance set (i.e.,
   lexical heads, syntactic labels, and dominance relationships) carries
   the most informative clues for intra-sentential parsing. for instance,
   the dominance relationship between the edus in our example sentence is
   3 > 1 > 2, which favors the dt structure [1, 1, 2] over [2, 2, 3]. in
   order to extract dominance set features for two adjacent discourse
   units u[t   1] and u[t], containing edus e[i:j] and e[j+1:k],
   respectively, we first compute the dominance set from the ds-lst of the
   sentence. we then extract the element from the set that holds across
   the edus j and j + 1. in our example, for the two units, containing
   edus e[1] and e[2], respectively, the relevant dominance set element is
   (1, efforts/np)>(2, to/s). we encode the syntactic labels and lexical
   heads of n[h] and n[a], and the dominance relationship as features in
   our intra-sentential parsing model.

   lexical chains (morris and hirst [928]1991) are sequences of
   semantically related words that can indicate topical boundaries in a
   text (galley et al. [929]2003; joty, carenini, and ng [930]2013).
   features extracted from lexical chains are also shown to be useful for
   finding paragraph-level discourse structure (sporleder and lapata
   [931]2004). for example, consider the text with four paragraphs (p[1]
   to p[4]) in [932]figure 10a. now, let us assume that there is a lexical
   chain that spans the whole text, skipping paragraphs p[2] and p[3],
   while a second chain only spans p[2] and p[3]. this situation makes it
   more likely that p[2] and p[3] should be linked in the dt before either
   of them is linked with another paragraph. therefore, the dt structure
   in [933]figure 10b should be more likely than the structure in
   [934]figure 10c.
   [935]figure
   figure   10    correlation between lexical chains and discourse structure.
   (a) lexical chains spanning paragraphs. (b) and (c) two possible dt
   structures.

   one challenge in computing lexical chains is that words can have
   multiple senses, and semantic relationships depend on the sense rather
   than the word itself. several methods have been proposed to compute
   lexical chains (barzilay and elhadad [936]1997; hirst and st. onge
   [937]1997; silber and mccoy [938]2002; galley and mckeown [939]2003).
   we follow the state-of-the-art approach proposed by galley and mckeown
   ([940]2003), which extracts lexical chains after performing word sense
   disambiguation (wsd).

   in the preprocessing step, we extract the nouns from the document and
   lemmatize them using id138's built-in morphy function (fellbaum
   [941]1998). then, by looking up in id138 we expand each noun to all
   of its senses, and build a lexical semantic relatedness graph (lsrg)
   (galley and mckeown [942]2003; chali and joty [943]2007). in an lsrg,
   the nodes represent noun-tokens with their candidate senses, and the
   weighted edges between senses of two different tokens represent one of
   the three semantic relations: repetition, synonym, and hypernym. for
   example, [944]figure 11a shows a partial lsrg, where the token bank has
   two possible senses, namely, money bank and river bank. using the money
   bank sense, bank is connected with institution and company by hypernymy
   relations (edges marked with h), and with another bank by a repetition
   relation (edges marked with r). similarly, using the river bank sense,
   it is connected with riverside by a hypernymy relation and with bank by
   a repetition relation. nouns that are not found in id138 are
   considered as proper nouns having only one sense, and are connected by
   only repetition relations.
   [945]figure
   figure   11    extracting lexical chains. (a) a lexical semantic
   relatedness graph (lsrg) for five noun-tokens. (b) resultant graph
   after performing wsd. the box at the bottom shows the lexical chains.

   we use this lsrg first to perform wsd, then to construct lexical
   chains. for wsd, the weights of all edges leaving the nodes under their
   different senses are summed up and the one with the highest score is
   considered to be the right sense for the word-token. for example, if
   repetition and synonymy are weighted equally, and hypernymy is given
   half as much weight as either of them, the score of bank's two senses
   are: 1 + 0.5 + 0.5 = 2 for the sense money bank and 1 + 0.5 = 1.5 for
   the sense river bank. therefore, the selected sense for bank in this
   context is river bank. in case of a tie, we select the sense that is
   most frequent (i.e., the first sense in id138). note that this
   approach to wsd is different from that of sporleder and lapata
   ([946]2004), which takes a greedy approach.

   finally, we prune the graph by only keeping the links that connect
   words with the selected senses. at the end of the process, we are left
   with the edges that form the actual lexical chains. for example,
   [947]figure 11b shows the result of pruning the graph in [948]figure
   11a. the lexical chains extracted from the pruned graph are shown in
   the box at the bottom. following sporleder and lapata ([949]2004), for
   each chain element, we keep track of the location (i.e., sentence id)
   in the text where that element was found, and exclude chains containing
   only one element. given two discourse units, we count the number of
   chains that: hit the two units, exclusively hit the two units, skip
   both units, skip one of the units, start in a unit, and end in a unit.

   we also consider more contextual information by including the above
   features computed for the neighboring adjacent discourse unit pairs in
   the current feature vector. for example, the contextual features for
   units u[t   1] and u[t] include the feature vector computed from u[t   2]
   and u[t   1] and the feature vector computed from u[t] and u[t+1].

   we incorporate hierarchical dependencies between the constituents in a
   dt by rhetorical sub-structural features. for two adjacent units u[t   1]
   and u[t], we extract the roots of the two rhetorical sub-trees. for
   example, the root of the rhetorical sub-tree spanning over edus e[1:2]
   in [950]figure 9b is elaboration   ns. however, extraction of these
   features assumes the presence of labels for the sub-trees, which is not
   the case when we apply the parser to a new text (sentence or document)
   in order to build its dt in a non-greedy fashion. one way to deal with
   this is to loop twice through the parsing process using two different
   parsing models   one trained with the complete feature set, and the other
   trained without the sub-structural features. we first build an initial,
   sub-optimal dt using the parsing model that is trained without the
   sub-structural features. this intermediate dt will now provide labels
   for the sub-structures. next we can build a final, more accurate dt by
   using the complete parsing model. this idea of two-pass discourse
   parsing, where the second pass performs post-editing using additional
   features, has recently been adopted by feng and hirst ([951]2014) in
   their greedy parser.

   one could even continue doing post-editing multiple times until the dt
   converges. however, this could be very time consuming as each
   post-editing pass requires: (1) applying the parsing model to every
   possible unit sequence and computing the posterior marginals for all
   possible dt constituents, and (2) using the parsing algorithm to find
   the most probable dt. recall from our earlier discussion in
   [952]section 4.1.3 that for n discourse units and m rhetorical
   relations, the first step requires o(m^2 n^4) and o(m^2 n^3) for intra-
   and multi-sentential parsing, respectively; we will see in the next
   section that the second step requires o(mn^3). in spite of the
   computational cost, the gain we attained in the subsequent passes was
   not significant for our development set. therefore, we restrict our
   parser to only one-pass post-editing.

   note that in parsing models where the score (i.e., likelihood) of a
   parse tree decomposes across local factors (e.g., the crf-based
   syntactic parser of finkel, kleeman, and manning [[953]2008]), it is
   possible to define a semiring using the factors and the local scores
   (e.g., given by the inside algorithm). the cky algorithm could then
   give the optimal parse tree in a single post-editing pass (smith
   [954]2011). however, because our intra-sentential parsing model is
   designed to capture sequential dependencies between dt constituents,
   the score of a dt does not directly decompose across factors over
   discourse productions. therefore, designing such a semiring was not
   possible in our case.

   in addition to these features, we also experimented with other features
   including id138-based lexical semantics, subjectivity, and
   tf.idf-based cosine similarity. however, because such features did not
   improve parsing performance on our development set, they were excluded
   from our final set of features.
   4.2   parsing algorithm

   the intra- and multi-sentential parsing models of codra assign a
   id203 to every possible dt constituent in their respective
   parsing scenarios. the job of the parsing algorithm is then to find the
   k most probable dts for a given text. we implement a probabilistic
   cky-like bottom   up parsing algorithm that uses id145 to
   compute the most likely parses (jurafsky and martin [955]2008). for
   simplicity, we first describe the specific case of generating the
   single most probable dt, then we describe how to generalize this
   algorithm to produce the k most probable dts for a given text.

   formally, the search problem for finding the most probable dt can be
   written as

   where    specifies the parameters of the parsing model (intra- or
   multi-sentential). given n discourse units, our parsing algorithm uses
   the upper-triangular portion of the n  n id145 table d,
   where cell d[i, j] (for i < j) stores:

   where u[x] (0) and u[x] (1) are the start and end edu ids of discourse
   unit u[x], and

   recall that the notation r[u[i] (0), u[m] (1), u[j] (1)] in this
   expression refers to a rhetorical relation r that holds between the
   discourse unit containing edus u[i] (0) through u[m] (1) and the unit
   containing edus u[m] (1) + 1 through u[j] (1).

   in addition to d, which stores the id203 of the most probable
   constituents of a dt, the algorithm also simultaneously maintains two
   other n  n id145 tables s and r for storing the structure
   (i.e., u[m*] (1)) and the relations (i.e., r*) of the corresponding dt
   constituents, respectively. for example, given four edus e[1]     e[4],
   the s and r id145 tables at the left side in [956]figure
   12 together represent the dt shown at the right. more specifically, to
   find the dt, we first look at the top-right entries in the two tables,
   and find s[1, 4] = 2 and r[1, 4] = r[2], which specify that the two
   discourse units e[1:2] and e[3:4] should be connected by the relation
   r[2] (the root in the dt). then, we see how edus e[1] and e[2] should
   be connected by looking at the entries s[1, 2] and r[1, 2], and find
   s[1, 2] = 1 and r[1, 2] = r[1], which indicates that these two units
   should be connected by the relation r[1] (the left pre-terminal in the
   dt). finally, to see how edus e[3] and e[4] should be linked, we look
   at the entries s[3, 4] and r[3, 4], which tell us that they should be
   linked by the relation r[4] (the right pre-terminal). the algorithm
   works in polynomial time. specifically, for n discourse units and m
   number of relations, the time and space complexities are o(n^3 m) and
   o(n^2), respectively.
   [957]figure
   figure   12    the s and r id145 tables (left), and the
   corresponding discourse tree (right).

   a key advantage of using a probabilistic parsing algorithm like the one
   we use is that it allows us to generate a list of k most probable parse
   trees. it is straightforward to generalize the above algorithm to
   produce k most probable dts. specifically, when filling up the dynamic
   programming tables, rather than storing a single best parse for each
   sub-tree, we store and keep track (i.e., using back-pointers) of k-best
   candidates simultaneously. one can show that the time and space
   complexities of the k-best version of the algorithm are o(n^3 mk^2 log
   k) and o(k^2 n), respectively (huang and chiang [958]2005).

   note that, in contrast to other document-level discourse parsers (marcu
   [959]2000b; subba and di-eugenio [960]2009; hernault et al. [961]2010;
   feng and hirst [962]2012, [963]2014), which use a greedy algorithm,
   codra finds a discourse tree that is globally optimal.[964]^9 this
   approach of codra is also different from the sentence-level discourse
   parser spade (soricut and marcu [965]2003). spade first finds the tree
   structure that is globally optimal, then it assigns the most probable
   relations to the internal nodes. more specifically, the cell d[i, j] in
   spade's id145 table stores

   where . disregarding the relation label r while populating d, this
   approach may find a discourse tree that is not globally optimal.
   4.3   document-level parsing approaches

   now that we have presented our intra-sentential and multi-sentential
   parsing components, we are ready to describe how they can be
   effectively combined in a unified framework ([966]figure 2) to perform
   document-level rhetorical analysis. recall that a key motivation for a
   two-stage[967]^10 parsing is that it allows us to capture the strong
   correlation between text structure and discourse structure in a
   scalable, modular, and flexible way. in the following, we describe two
   different approaches to model this correlation.
   4.3.1   1s   1s (1 sentence   1 sub-tree)

   a key finding from previous studies on sentence-level discourse
   analysis is that most sentences have a well-formed discourse sub-tree
   in the full document-level dt (soricut and marcu [968]2003; fisher and
   roark [969]2007). for example, [970]figure 13a shows 10 edus in three
   sentences (see boxes), where the dts for the sentences obey their
   respective sentence boundaries.
   [971]figure
   figure   13    two possible dts for three sentences.

   our first approach, called 1s   1s (1 sentence   1 sub-tree), aims to
   maximally exploit this finding. it first constructs a dt for every
   sentence using our intra-sentential parser, and then it provides our
   multi-sentential parser with the sentence-level dts to build the
   rhetorical parse for the whole document.
   4.3.2   sliding window

   although the assumption made by 1s   1s clearly simplifies the parsing
   process, it completely ignores the cases where rhetorical structures
   violate sentence boundaries. for example, in the dt shown in
   [972]figure 13b, sentence s[2] does not have a well-formed sub-tree
   because some of its units attach to the left (i.e., 4   5 and 6) and some
   to the right (i.e., 7). vliet and redeker ([973]2011) call these cases
      leaky    boundaries.

   although we find fewer than 5% of the sentences in the rst   dt have
   leaky boundaries, in other corpora this can be true for a larger
   portion of the sentences. for example, we observe that over 12% of the
   sentences in the instructional corpus of subba and di-eugenio
   ([974]2009) have leaky boundaries. however, we notice that in most
   cases where dt structures violate sentence boundaries, its units are
   merged with the units of its adjacent sentences, as in [975]figure 13b.
   for example, this is true for 75% of the leaky cases in our development
   set containing 20 news articles from the rst   dt and for 79% of the
   leaky cases in our development set containing 20 how-to manuals from
   the instructional corpus. based on this observation, we propose a
   sliding window approach.

   in this approach, our intra-sentential parser works with a window of
   two consecutive sentences, and builds a dt for the two sentences. for
   example, given the three sentences in [976]figure 13, our
   intra-sentential parser constructs a dt for s[1]   s[2] and a dt for
   s[2]   s[3]. in this process, each sentence in a document except the
   boundary sentences (i.e., the first and the last) will be associated
   with two dts: one with the previous sentence (say, dt[p]) and one with
   the next (say, dt[n]). in other words, for each non-boundary sentence,
   we will have two decisions: one from dt[p] and one from dt[n]. our
   parser consolidates the two decisions and generates one or more
   sub-trees for each sentence by checking the following three mutually
   exclusive conditions one after another:
         

   same in both: if the sentence under consideration has the same (in both
   structure and labels) well-formed sub-tree in both dt[p] and dt[n], we
   take this sub-tree. for example, in [977]figure 14a, s[2] has the same
   sub-tree in the two dts (one for s[1]   s[2] and one for s[2]   s[3]). the
   two decisions agree on the dt for the sentence.
         

   different but no cross: if the sentence under consideration has a
   well-formed sub-tree in both dt[p] and dt[n], but the two sub-trees
   vary either in structure or in labels, we pick the most probable one.
   for example, consider the dt for s[1]   s[2] (at the left) in [978]figure
   14a and the dt for s[2]   s[3] in [979]figure 14b. in both cases s2 has a
   well-formed sub-tree, but they differ in structure. we pick the
   sub-tree which has the higher id203 in the two dynamic
   programming tables.
         

   cross: if either or both of dt[p] and dt[n] segment the sentence into
   multiple sub-trees, we pick the one having more sub-trees. for example,
   consider the two dts in [980]figure 14c. in the dt for s[1]   s[2] on the
   left, s[2] has three sub-trees (4   5, 6, 7), whereas in the dt for
   s[2]   s[3] on the right, it has two (4   6, 7). so, we extract the three
   sub-trees for s[2] from the first dt. if the sentence has the same
   number of sub-trees in both dt[p] and dt[n], we pick the one with
   higher id203 in the id145 tables. note that our
   choice of picking the dt with more sub-trees is intended to allow the
   parser to find more leaky cases. however, other heuristics are also
   possible. for example, another simple heuristic that one could try is:
   when both dts segment the sentence into multiple sub-trees, pick the
   one with fewer sub-trees, and when only one of the dts segment the
   sentence into multiple sub-trees, pick that one.
   [981]figure
   figure   14    extracting sub-trees for s[2].

   at the end, the multi-sentential parser takes all these sentence-level
   sub-trees for a document, and builds a full rhetorical parse for the
   whole document.
   5.   the discourse segmenter
   section:
   [choose___________________________]
   [982]previous section [983]next section

   our discourse parser assumes that the input text has been already
   segmented into a sequence of edus. however, discourse segmentation is
   also a challenging problem, and previous studies (soricut and marcu
   [984]2003; fisher and roark [985]2007) have identified it as a primary
   source of inaccuracy for discourse parsing. regardless of its
   importance in discourse parsing, discourse segmentation itself can be
   useful in several nlp applications, including sentence compression
   (sporleder and lapata [986]2005) and textual alignment in statistical
   machine translation (stede [987]2011). therefore, in codra, we have
   developed our own discourse segmenter, which not only achieves
   state-of-the-art performance as shown later, but also reduces the time
   complexity by using fewer features.
   5.1   segmentation model

   the discourse segmenter in codra implements a binary classifier to
   decide for each word   token (except the last) in a sentence, whether to
   place an edu boundary after that token. we use a maximum id178 model
   to build a discriminative classifier. more specifically, we use a
   id28 classifier with parameter   :

   where the output y     {0, 1} denotes whether to put an edu boundary
   (i.e., y = 1) or not (i.e., y = 0) after the word   token w, which is
   represented by a feature vector x. in the equation, ber(  ) and sigm(  )
   refer to the bernoulli distribution and the sigmoid (also known as
   logistic) function, respectively. the negative log-likelihood (nll) of
   the model with l[2] id173 for n data points (i.e.,
   word   tokens) is given by

   where y[i] is the gold label for word   token w[i] (represented by
   feature vector x[i]). we learn the model parameters    using the l-bfgs
   fitting algorithm, which is time- and space-efficient. to avoid
   overfitting, we use 5-fold cross validation to learn the id173
   strength parameter    from the training data. we also use a simple
   id112 technique (breiman [988]1996) to deal with the sparsity of
   boundary (i.e., y = 1) tags.

   note that our first attempt at the discourse segmentation task
   implemented a linear-chain crf model (lafferty, mccallum, and pereira
   [989]2001) to capture the sequential dependencies between the tags in a
   discriminative way. however, the binary id28 classifier,
   using the same set of features, not only outperforms the crf model, but
   also reduces time and space complexity. one possible explanation for
   the low performance of the crf model is that markov dependencies
   between tags cannot be effectively captured due to the sparsity of
   boundary tags. also, because we could not balance the data by using
   techniques like id112 in the crf model, this further degrades the
   performance.
   5.2   features used in the segmentation model

   our set of features for discourse segmentation are mostly inspired from
   previous studies but used in a novel way, as we describe here.

   our first subset of features, which we call spade features, includes
   the lexico-syntactic patterns extracted from the lexicalized syntactic
   tree of the given sentence. these features replicate the features used
   in spade's segmenter, but used in a discriminative way. in order to
   decide on an edu boundary after a word   token w[k], we search for the
   lowest constituent in the lexicalized syntactic tree that spans over
   tokens w[i]     w[j] such that i     k < j. the production that expands
   this constituent in the tree, with the potential edu boundary marked,
   forms the primary feature. for instance, to determine the existence of
   an edu boundary after the word efforts in our sample sentence shown in
   [990]figure 9, the production np(efforts)     prp$(its) nns(efforts)    
   s(to) extracted from the lexicalized syntactic tree in [991]figure 9a
   constitutes the primary feature, where     denotes the potential edu
   boundary.

   spade predicts an edu boundary if the relative frequency (i.e., maximum
   likelihood estimate) of a potential boundary given the production in
   the training data is greater than 0.5. if the production has not been
   observed frequently enough, the unlexicalized version of the production
   (e.g., np     prp$ nns     s) is used for prediction. if the unlexicalized
   version is also found to be rare, other variations of the production,
   depending on whether they include the lexical heads and how many
   non-terminals (one or two) they consider before and after the potential
   boundary, are examined one after another (see fisher and roark
   [[992]2007] for details). in contrast, we compute the maximum
   likelihood estimates for a primary production (feature) and its other
   variations, and use those directly as features with/without binarizing
   the values.

   shallow syntactic features like chunk and pos tags have been shown to
   possess valuable clues for discourse segmentation (fisher and roark
   [993]2007; sporleder and lapata [994]2005). for example, it is less
   likely that an edu boundary occurs within a chunk. we annotate the
   tokens of a sentence with chunk and pos tags using the state-of-the-art
   illinois tagger[995]^11 and encode these as features in our model. note
   that the chunker assigns each token a tag using the bio notation, where
   b stands for beginning of a particular phrase (e.g., noun or verb
   phrase), i stands for inside of a particular phrase, and o stands for
   outside of a particular phrase. the rationale for using the illinois
   chunker is that it uses a larger set of tags (23 in total); thus it is
   more informative than most of the other existing taggers, which
   typically use only five tags (b   np, i   np, b   vp, i   vp, and o).

   edus are normally multi-word strings. thus, a token near the beginning
   or end of a sentence is unlikely to be the end of a segment. therefore,
   for each token we include its relative position (i.e., absolute
   position/total number of tokens) in the sentence and distances to the
   beginning and end of the sentence as features.

   it is unlikely that two consecutive tokens are tagged with edu
   boundaries. therefore, we incorporate contextual information for a
   token into our model by including the above features computed for its
   neighboring tokens.

   we also experimented with different id165 (n     {1, 2, 3}) features
   extracted from the token sequence, pos sequence, and chunk sequence.
   however, because such features did not improve segmentation accuracy on
   the development set, they were excluded from our final set of features.
   6.   experiments
   section:
   [choose________________________]
   [996]previous section [997]next section

   in this section we present our experimental results. first, we describe
   the corpora on which the experiments were performed and the evaluation
   metrics used to measure the performance of the discourse segmenter and
   the parser. then we show the performance of our discourse segmenter,
   followed by the performance of our discourse parser.
   6.1   corpora

   whereas previous studies on discourse analysis only report their
   results on a particular corpus, to demonstrate the generality of our
   method, we experiment with texts from two very different genres: news
   articles and instructional how-to manuals.

   our first corpus is the standard rst   dt (carlson, marcu, and okurowski
   [998]2002), which contains discourse annotations for 385 wall street
   journal news articles taken from the id32 corpus (marcus,
   marcinkiewicz, and santorini [999]1994). the corpus is partitioned into
   a training set of 347 documents and a test set of 38 documents. a total
   of 53 documents selected from both training and test sets were
   annotated by two human annotators. we measure human agreements based on
   this doubly annotated data set. we used 25 documents from the training
   set as our development set. in rst   dt, the original 25 rhetorical
   relations defined by mann and thompson ([1000]1988) are further divided
   into a set of 18 coarser relation classes with 78 finer-grained
   relations (see carlson and marcu [[1001]2001] for details). our second
   corpus is the instructional corpus prepared by subba and di-eugenio
   ([1002]2009), which contains discourse annotations for 176 how-to
   manuals on home repair. the corpus was annotated with 26 informational
   relations (e.g., preparation   act, act   goal).

   for our experiments with the intra-sentential discourse parser, we
   extracted a sentence-level dt from a document-level dt by finding the
   sub-tree that exactly spans over the sentence. in rst   dt, by our count,
   7, 321 out of 7, 673 sentences in the training set, 951 out of 991
   sentences in the test set, and 1, 114 out of 1, 208 sentences in the
   doubly-annotated set have a well-formed dt. on the other hand, 3, 032
   out of 3, 430 sentences in the instructional corpus have a well-formed
   dt. this forms the corpora for our experiments with intra-sentential
   discourse parsing. however, the existence of a well-formed dt is not a
   necessity for discourse segmentation; therefore, we do not exclude any
   sentence in our discourse segmentation experiments.
   6.2   evaluation (and agreement) metrics

   in this subsection we describe the metrics used to measure both how
   much the annotators agree with each other, and how well the systems
   perform when their outputs are compared with human annotations for the
   discourse analysis tasks.
   6.2.1   metrics for discourse segmentation

   because sentence boundaries are considered to also be the edu
   boundaries, we measure segmentation accuracy with respect to the
   intra-sentential segment boundaries, which is a standard method
   (soricut and marcu [1003]2003; fisher and roark [1004]2007).
   specifically, if a sentence contains n edus, which corresponds to n     1
   intra-sentential segment boundaries, we measure the segmenter's ability
   to correctly identify these n     1 boundaries. let h be the total number
   of intra-sentential segment boundaries in the human annotation, m be
   the total number of intra-sentential segment boundaries in the model
   output, and c be the total number of correct segment boundaries in the
   model output. then, we measure precision (p), recall (r), and f-score
   for segmentation performance as follows:

   6.2.2   metrics for discourse parsing

   to evaluate parsing performance, we use the standard unlabeled and
   labeled precision, recall, and f-score as proposed by marcu
   ([1005]2000b). the unlabeled metric measures how accurate the discourse
   parser is in finding the right structure (i.e., the skeleton) of the
   dt, while the labeled metrics measure the parser's ability to find the
   right labels (i.e., nuclearity statuses or relation labels) in addition
   to the right structure. assume, for example, that given the two
   sentences of [1006]figure 1, our system generates the dt shown in
   [1007]figure 15. in [1008]figure 16, we show the same gold dt shown in
   [1009]figure 1 (on the left), and the same system-generated dt shown in
   [1010]figure 15 (on the right), when the two trees are aligned. for the
   sake of illustration, instead of showing the real edus, we only show
   their ids. notice that the automatic segmenter made two mistakes: (1)
   it broke the edu marked 2   3 (some people use the purchasers    index as a
   leading indicator) in the human annotation into two separate edus, and
   (2) it could not identify edu 5 (but the thing it's supposed to
   measure) and edu 6 (    manufacturing strength    ) as two separate edus.
   therefore, when we align the two annotations, we obtain seven edus in
   total.
   [1011]figure
   figure   15    a hypothetical system-generated dt for the two sentences in
   [1012]figure 1.
   [1013]figure
   figure   16    measuring the accuracy of a discourse parser. (a) the
   human-annotated discourse tree. (b) the system-generated discourse
   tree.

   in [1014]table 2, we list all constituents of the two dts and their
   associated labels at the span, nuclei, and relation levels. the recall
   (r) and precision (p) figures are shown at the bottom of the table.
   notice that, following (marcu [1015]2000b), the relation labels are
   assigned to the children nodes rather than to the parent nodes in the
   evaluation process to deal with non-binary trees in human annotations.
   to our knowledge, no implementation of the id74 was made
   publicly available. therefore, to help other researchers, we have made
   our source code of the id74 publicly available.[1016]^12

   [1017]table
   table   2    measuring parsing accuracy (p = precision, r = recall).

   given this evaluation setup, it is easy to understand that if the
   number of edus is the same in the human and system annotations (e.g.,
   when the discourse parser uses gold discourse segmentation), and the
   discourse trees are binary, then we get the same figures for precision,
   recall, and f-score.
   6.3   discourse segmentation evaluation

   in this section we present our experiments on discourse segmentation.
   6.3.1   experimental set-up for discourse segmentation

   we compare the performance of our discourse segmenter with the
   performance of the two publicly available discourse segmenters, namely,
   the discourse segmenters of the hilda (hernault et al. [1018]2010) and
   spade (soricut and marcu [1019]2003) systems. we also compare our
   results with the state-of-the-art results reported by fisher and roark
   ([1020]2007) on the rst   dt test set. in all our experiments when
   comparing two systems, we use paired t-test on the f-scores to measure
   statistical significance and report the p-value.

   we ran hilda with its default settings. for spade, we applied the same
   modifications to its default settings as described in fisher and roark
   ([1021]2007), which delivers significantly improved performance over
   its original version. specifically, in our experiments on the rst   dt
   corpus, we trained spade using the human-annotated syntactic trees
   extracted from the id32 (marcus, marcinkiewicz, and santorini
   [1022]1994), and, during testing, we replaced the charniak parser
   (charniak [1023]2000) with a more accurate reranking parser (charniak
   and johnson [1024]2005). however, because of the lack of gold syntactic
   trees in the instructional corpus, we trained spade in this corpus
   using the syntactic trees produced by the reranking parser. to avoid
   using the gold syntactic trees, we used the reranking parser in our
   system for both training and testing purposes. this syntactic parser
   was trained on the sections of the id32 not included in our
   test set. we applied the same canonical lexical head projection rules
   (magerman [1025]1995; collins [1026]2003) to lexicalize the syntactic
   trees as done in hilda and spade.

   note that previous studies (fisher and roark [1027]2007; soricut and
   marcu [1028]2003; hernault et al. [1029]2010) on discourse segmentation
   only report their performance on the rst   dt test set. to compare our
   results with them, we evaluated our model on the rst   dt test set. in
   addition, we showed a more general performance of spade and our system
   on the two corpora based on 10-fold cross validation.[1030]^13 however,
   spade does not come with a training module for its segmenter. we
   reimplemented this module and verified its correctness by reproducing
   the results on the rst   dt test set.
   6.3.2   results for discourse segmentation

   [1031]table 3 shows the discourse segmentation results of different
   systems in precision, recall, and f-score on the two corpora. on the
   rst    dt corpus, hilda's segmenter delivers the weakest performance,
   having an f-score of only 74.1. note that the high segmentation
   accuracy reported by hernault et al. ([1032]2010) is due to a less
   stringent evaluation metric. spade performs much better than hilda with
   an absolute f-score improvement of 11.1%. our segmenter ds outperforms
   spade with an absolute f-score improvement of 4.9% (p-value < 2.4e-06),
   and also achieves comparable results to the ones of fisher and roark
   ([1033]2007) (f&r), even though we use fewer features.[1034]^14 notice
   that human agreement for this task is quite high   namely, an f-score of
   98.3 computed on the doubly-annotated portion of the rst   dt corpus
   mentioned in [1035]section 6.1.

   [1036]table
   table   3    discourse segmentation results of different models on the two
   corpora. performances significantly superior to spade are denoted by *.

   because fisher and roark ([1037]2007) only report their results on the
   rst   dt test set and we did not have access to their system, we compare
   our approach only with spade when evaluating on a whole corpus based on
   10-fold cross validation. on the rst   dt corpus, our segmenter delivers
   an absolute f-score improvement of 3.8 percentage points, which
   represents a more than 25% relative error rate reduction. the
   improvement is higher on the instructional corpus with an absolute
   f-score improvement of 8.1 percentage points, which corresponds to a
   relative error reduction of 30%. the improvements for both corpora are
   statistically significant (p-value < 3.0e-06). when we compare our
   results on the two corpora, we observe a substantial decrease in
   performance on the instructional corpus. this could be because of a
   smaller amount of data in this corpus and/or to the inaccuracies in the
   syntactic parser and taggers, which are trained on news articles. a
   promising future direction would be to apply effective domain
   adaptation methods (e.g., easyadapt [daum   [1038]2007]) to improve
   discourse segmentation performance in the instructional domain by
   leveraging the rich data in the news domain (i.e., rst   dt).
   6.4   discourse parsing evaluation

   in this section we present our experiments on discourse parsing. first,
   we describe the experimental set-up. then, we present the results of
   the parsers. while presenting the performance of our discourse parser,
   we show a breakdown of intra-sentential versus inter-sentential
   results, in addition to the aggregated results at the document level.
   6.4.1   experimental set-up for discourse parsing

   in our experiments on sentence-level (i.e., intra-sentential) discourse
   parsing, we compare our approach with spade (soricut and marcu
   [1039]2003) on the rst   dt corpus, and with the ilp-based approach of
   subba and di-eugenio ([1040]2009) on the instructional corpus, because
   they are the state of the art in their respective genres. for spade, we
   applied the same modifications to its default settings as described in
   [1041]section 6.3.1, which leads to improved performance. similarly, in
   our experiments on document-level (i.e., multi-sentential) parsing, we
   compare our approach with hilda (hernault et al. [1042]2010) on the
   rst   dt corpus, and with the ilp-based approach (subba and di-eugenio
   [1043]2009) on the instructional corpus. the results for hilda were
   obtained by running the system with default settings on the same inputs
   we provided to our system. because we could not run the ilp-based
   system (not publicly available), we report the performance presented in
   their paper.

   our experiments on the rst   dt corpus use the same 18 coarser coherence
   relations (see [1044]figure 18 later in this article), defined by
   carlson and marcu ([1045]2001) and also used in spade and hilda
   systems. more specifically, the relation set consists of 16 relation
   categories and two pseudo-relations, namely, textual   organization and
   same   unit. after attaching the nuclearity statuses (ns, sn, nn) to
   these relations, we obtain 41 distinct relations.[1046]^15 our
   experiments on the instructional corpus consider the same 26 primary
   relations (e.g., goal:act, cause:effect) used by subba and di-eugenio
   ([1047]2009) and also treat the reversals of non-commutative relations
   as separate relations. that is, goal   act and act   goal are considered to
   be two different coherence relations. attaching the nuclearity statuses
   to these relations provides 76 distinct relations.

   based on our experiments on the development set, the size of the
   automatically built bi-gram and tri-gram dictionaries was set to 95% of
   their total number of items, and the size of the unigram dictionary was
   set to 100%. note that the unigram dictionary contains only special
   tags denoting edu, sentence, and paragraph boundaries.
   6.4.2   evaluation of the intra-sentential discourse parser

   this section presents our experimental evaluation on intra-sentential
   discourse parsing. first, we show the performance of the sentence-level
   parsers when they are provided with manual (or gold) discourse
   segmentations. this allows us to judge the parsing performance
   independently of the segmentation task. then, we show the end-to-end
   performance of our intra-sentential framework, that is, the
   intra-sentential parsing performance based on automatic discourse
   segmentation.

   intra-sentential parsing results based on manual segmentation

   [1048]table 4 presents the intra-sentential discourse parsing results
   when manual discourse segmentation is used. recall from our discussion
   on id74 in [1049]section 6.2.2 that precision, recall,
   and f-score are the same when manual segmentation is used. therefore,
   we report only one of them. notice that our sentence-level discourse
   parser par-s consistently outperforms spade on the rst   dt test set in
   all three metrics, and the improvements are statistically significant
   (p-value < 0.01). especially, on the relation labeling task, which is
   the hardest among the three tasks, we achieve an absolute f-score
   improvement of 12.2 percentage points, which represents a relative
   error rate reduction of 37.7%.

   [1050]table
   table   4    intra-sentential parsing results based on manual discourse
   segmentation. performances significantly superior to spade are denoted
   by *.

   to verify our claim that capturing the sequential dependencies between
   dt constituents using a dcrf model actually contributes to the
   performance gain, we also compare our results with an intra-sentential
   parser (see crf-nc in [1051]table 4) that uses a simplified crf model
   similar to the one shown in [1052]figure 8. although the simplified
   model has two hidden variables to model the structure and the relation
   of a dt constituent jointly, it does not have a chain structure, thus
   it ignores the sequential dependencies between dt constituents. the
   comparison in all three measures demonstrates that the improvements are
   indeed partly due to the dcrf model (p   value < 0.01).[1053]^16 a
   comparison between crf-nc and spade shows that crf-nc significantly
   outperforms spade in all three measures (p-value < 0.01). this could be
   due to the fact that crf-nc is trained discriminatively with a large
   number of features, whereas spade is trained generatively with only
   lexico-syntactic features.

   notice that the scores of our parser (par-s) are close to the human
   agreement on the doubly-annotated data, and these results on the rst   dt
   test set are also consistent with the mean scores over 10-folds on the
   whole rst   dt corpus.[1054]^17

   the improvements are higher on the instructional corpus, where we
   compare our mean results over 10-folds with the reported results of the
   ilp-based system of subba and di-eugenio ([1055]2009), giving absolute
   f-score improvements of 5.4 percentage points, 17.6 percentage points,
   and 12.8 percentage points in span, nuclearity, and relations,
   respectively.[1056]^18 our parser par-s reduces the errors by 76.1%,
   62.4%, and 34.6% in span, nuclearity and relations, respectively.

   if we compare the performance of our intra-sentential discourse parser
   on the two corpora, we notice that our parser par-s is more accurate in
   finding the right tree structure (see span row in the table) on the
   instructional corpus. this may be due to the fact that sentences in the
   instructional domain are relatively short and contain fewer edus than
   sentences in the news domain, thus making it easier to find the right
   tree structure. however, when we compare the performance on the
   relation labeling task, we observe a decrease on the instructional
   corpus. this may be due to the small amount of data available for
   training and the imbalanced distribution of a large number of discourse
   relations (i.e., 76 with nuclearity attached) in this corpus.

   intra-sentential parsing results based on automatic segmentation

   in order to evaluate the performance of the fully automatic
   sentence-level discourse analysis systems, we feed the intra-sentential
   discourse parsers the output of their respective discourse segmenters.
   [1057]table 5 shows the (p)recision, (r)ecall, and (f)   score results
   for different id74. we compare our intra-sentential
   parser par-s with spade on the rst   dt test set. we achieve absolute
   f-score improvements of 5.7 percentage points, 6.4 percentage points,
   and 9.5 percentage points in span, nuclearity, and relation,
   respectively. these improvements are statistically significant
   (p-value<0.001). our system, therefore, reduces the errors by 24.5%,
   21.4%, and 22.6% in span, nuclearity, and relations, respectively.
   these results are also consistent with the mean results over 10-folds
   on the whole rst   dt corpus.

   [1058]table
   table   5    intra-sentential parsing results using automatic discourse
   segmentation. performances significantly superior to spade are denoted
   by *.

   the rightmost column in the table shows our mean results over 10-folds
   on the instructional corpus. we could not compare our system with the
   ilp-based approach of subba and di-eugenio ([1059]2009) because no
   results were reported using an automatic segmenter. it is interesting
   to observe how much our parser is affected by an automatic segmenter on
   the two corpora (see [1060]tables 4 and [1061]5). nevertheless, taking
   into account the segmentation results in [1062]table 3, this is not
   surprising because previous studies (soricut and marcu [1063]2003) have
   already shown that automatic segmentation is the primary impediment to
   high accuracy discourse parsing. this demonstrates the need for a more
   accurate discourse segmentation model in the instructional genre.
   6.4.3   evaluation of the complete parser

   we experiment with our full document-level discourse parser on the two
   corpora using the two parsing approaches described in [1064]section
   4.3, namely, 1s-1s and the sliding window. on rst   dt, the standard
   split was used for training and testing. on the instructional corpus,
   subba and di-eugenio ([1065]2009) used 151 documents for training and
   25 documents for testing. because we did not have access to their
   particular split, we took five random samples of 151 documents for
   training and 25 documents for testing, and report the average
   performance over the five test sets.

   [1066]table 6 presents results for our two-stage discourse parser (tsp)
   using approaches 1s-1s (tsp 1-1) and the sliding window (tsp sw) on
   manually segmented texts. recall that precision, recall, and f-score
   are the same when manual segmentation is used. we compare our parser
   with the state-of-the-art on the two corpora: hilda (hernault et al.
   [1067]2010) on rst   dt, and the ilp-based approach (subba and di-eugenio
   [1068]2009) on the instructional domain. on both corpora, our systems
   outperform existing systems by a wide margin (p-value <7.1e-05 on
   rst   dt).[1069]^19 on rst   dt, our parser tsp 1-1 achieves absolute
   improvements of 7.9 percentage points, 9.3 percentage points, and 11.5
   percentage points in span, nuclearity, and relation, respectively, over
   hilda. this represents relative error reductions of 31.2%, 22.7%, and
   20.7% in span, nuclearity, and relation, respectively.

   [1070]table
   table   6    parsing results of document-level parsers using manual
   segmentation. performances significantly superior to hilda (p-value
   <0.0001) are denoted by *. significant differences between tsp 1-1 and
   tsp sw (p-value <0.01) are denoted by    .

   beside hilda, we also compare our results with two baseline parsers on
   rst   dt: (1) crf-o, which uses a single unified crf-based parsing model
   shown in [1071]figure 8 (the one used for multi-sentential parsing)
   without distinguishing between intra- and multi-sentential parsing, and
   (2) crf-t, which uses two different crf-based parsing models for intra-
   and multi-sentential parsing in the two-stage approach 1s-1s, both
   models having the same structure as in [1072]figure 8. thus, crf-t is a
   variation of tsp 1-1, where the dcrf-based (chain-structured)
   intra-sentential parsing model is replaced with a simpler crf-based
   parsing model.[1073]^20 note that although crf-o does not explicitly
   discriminate between intra- and multi-sentential parsing, it uses
   id165 features that include sentence and edu boundaries to encode this
   information into the model.

   [1074]table 6 shows that both crf-o and crf-t outperform hilda by a
   good margin (p-value <0.0001). this improvement can be attributed to
   the optimal parsing algorithm and better feature selection strategy.
   when we compare crf-t with crf-o, we notice significant performance
   gains for crf-t (p-value <0.001). the absolute gains are 4.32
   percentage points, 2.68 percentage points, and 4.55 percentage points
   in span, nuclearity, and relation, respectively. this comparison
   clearly demonstrates the benefit of using a two-stage approach with two
   different parsing models over a framework with one single unified
   parsing model. finally, when we compare our best results with the human
   agreements, we still observe room for further improvement in all three
   measures.

   on the instructional genre, our parser tsp 1-1 delivers absolute
   f-score improvements of 10.3 percentage points, 13.6 percentage points,
   and 8.1 percentage points in span, nuclearity, and relations,
   respectively, over the ilp-based approach of subba and di-eugenio
   ([1075]2009). our parser, therefore, reduces errors by 34.7%, 26.9%,
   and 12.5% in span, nuclearity, and relations, respectively.

   if we compare the performance of our discourse parsers on the two
   corpora, we observe lower results on the instructional corpus. there
   could be two reasons for this. first, the instructional corpus has a
   smaller amount of data with a larger set of relations (76 with
   nuclearity attached). second, some of the frequent relations are
   semantically very similar (e.g., preparation-act, step1-step2), which
   makes it difficult even for the human annotators to distinguish them
   (subba and di-eugenio [1076]2009).

   comparison between our two document-level parsing approaches reveals
   that tsp sw significantly outperforms tsp 1-1 only in finding the right
   structure on both corpora (p-value <0.01). not surprisingly, the
   improvement is higher on the instructional corpus. a likely explanation
   is that the instructional corpus contains more leaky boundaries (12%),
   allowing the sliding window approach to be more effective in finding
   those, without inducing much noise for the labels. this demonstrates
   the potential of tsp sw for data sets with even more leaky boundaries,
   e.g., the dutch (vliet and redeker [1077]2011) and the german potsdam
   (stede [1078]2004) corpora. however, it would be interesting to see how
   other heuristics to do consolidation in the cross condition
   ([1079]section 4.3.2) perform.

   to analyze errors made by tsp sw, we looked at some poorly parsed
   examples and found that although tsp sw finds more correct structures,
   a corresponding improvement in labeling relations is not present
   because in some cases, it tends to induce noise from the neighboring
   sentences for the labels. for example, when parsing is performed on the
   first sentence in [1080]figure 1 in isolation using 1s-1s, our parser
   rightly identifies the contrast relation between edus 2 and 3. but,
   when it is considered with its neighboring sentences by the sliding
   window, the parser labels it as elaboration. a promising strategy to
   deal with this and similar problems would be to apply both approaches
   to each sentence and combine them by consolidating three probabilistic
   decisions, namely, the one from 1s-1s and the two from the sliding
   window.
   6.4.4   k-best parsing results based on manual segmentation

   as described in [1081]section 4.2, a straight-forward modification of
   our probabilistic parsing algorithm allows us to generate a list of
   k-best parse hypotheses for a given text. we adapt our parsing
   algorithm accordingly to produce k most probable dts for each text, and
   measure the oracle accuracy based on the f-scores of the relation
   metric which gives aggregated evaluation on structure and relation
   labels (see [1082]table 2). specifically, the oracle accuracy o   score
   for k-best discourse parsing is measured as follows:

   where n is the total number of texts (sentences or documents)
   evaluated, g[i] is the gold dt annotation for text i, is the j^th parse
   hypothesis generated by the parser for text i, and f-score[r] (g[i], )
   is the f-score accuracy of hypothesis on the relation metric, which
   essentially measures how similar is to g[i] in terms of its structure
   and labels.

   [1083]table 7 presents the oracle scores of our intra-sentential parser
   par-s on the rst   dt test set as a function of k of k-best parsing. the
   1-best result tells that the parser has the base accuracy of 79.8%. the
   2-best shows dramatic oracle-rate improvements (i.e., 4.65% absolute),
   meaning that often our parser generates the best tree as its top two
   outputs. 3-best and 4-best also show moderate improvements (about 2%).
   things start to slow down afterwards, and we achieve oracle rates of
   90.37% and 92.57% at 10   best and 20-best, respectively. the 30-best
   parsing gives an oracle score of 93.2%.

   [1084]table
   table   7    oracle scores as a function of k of k-best sentence-level
   parses on rst   dt.

   the results of our k-best intra-sentential discourse parser demonstrate
   that a k-best reranking approach like that of collins and koo
   ([1085]2005) and charniak and johnson ([1086]2005) used for syntactic
   parsing can potentially improve the parsing accuracy even further by
   exploiting additional global features of the candidate discourse trees
   as evidence.

   the scenario is quite different at the document-level; [1087]table 8
   shows the k-best parsing results of tsp 1s-1s on the rst   dt test set.
   the improvements in oracle-rate are small at the document-level when
   compared with the sentence-level parsing. for example, the 2-best and
   the 5-best improve over the base accuracy by only 0.7 percentage points
   and 1.0 percentage points, respectively. the improvements get even
   slower after that. however, this is not surprising because generally
   document-level dts are big with many constituents, and only a very few
   of these constituents change from k-best to k + 1-best parsing. these
   small changes among the candidate dts do not contribute much to the
   overall f-score accuracy (for further clarification see how f-score is
   calculated in [1088]section 6.2.2).

   [1089]table
   table   8    oracle scores as a function of k of k-best document-level
   parses on rst   dt.

   the results of our k-best document-level parsing suggest that often the
   best tree is missing in the top k parses. thus, a reranking of k-best
   document-level parses may not be a suitable option for further
   improvement at the document-level. an alternative to k-best reranking
   is to use a sampling-based parsing strategy (wick et al. [1090]2011) to
   explore the space of possible trees, as recently used for dependency
   parsing (zhang et al. [1091]2014). however, note that the potential
   gain we may obtain by using a reranker at the sentence level will also
   improve the (combined) accuracy of the document-level parser.
   6.4.5   analysis of features

   to analyze the relative importance of different features used in our
   parsing models, [1092]table 9 presents the sentence- and document-level
   parsing results on a manually segmented rst   dt test set using different
   subsets of features. the feature subsets were defined in [1093]section
   4.1.4. in each parsing condition, the subsets of features are added
   incrementally, based on their availability and historical importance.
   the columns in [1094]table 9 represent the inclusion order of the
   feature subsets.

   [1095]table
   table   9    parsing results using different subsets of features on rst   dt
   test set.

   because spade (soricut and marcu [1096]2003) achieved the previous best
   results on intra-sentential parsing using dominance set features, these
   are included as the initial set of features in our intra-sentential
   parsing model. in hilda, hernault et al. ([1097]2010) demonstrate the
   importance of organizational and id165 features for full text parsing.
   we add these two feature subsets one after another in our intra- and
   multi-sentential parsing models.[1098]^21 contextual features require
   other features to be computed; thus they were added after those
   features. because computation of sub-structural features requires an
   initial parse tree (i.e., when the parser is applied), they are added
   at the very end.

   notice that inclusion of every new subset of features appears to
   improve the performance over the previous set. specifically, for
   sentence-level parsing, when we add the organizational features with
   the dominance set features, we achieve about 2 percentage points
   absolute improvements in nuclearity and relations. with id165
   features, the gain is even higher: 6 percentage points in relations and
   3.5 percentage points in nuclearity for sentence-level parsing, and 3.8
   percentage points in relations and 3.1 percentage points in nuclearity
   for document-level parsing. this demonstrates the utility of the id165
   features, which is also consistent with the previous findings of
   duverle and prendinger ([1099]2009) and schilder ([1100]2002).

   the features extracted from lexical chains (l-ch) have also proved to
   be useful for document-level parsing. they deliver absolute
   improvements of 2.7 percentage points, 2.9 percentage points, and 2.3
   percentage points in span, nuclearity, and relations, respectively.
   including the contextual features further gives improvements of 3
   percentage points in nuclearity and 2.2 percentage points in relation
   for sentence-level parsing, and 1.3 percentage points in nuclearity,
   and 1.2 percentage points in relation for document-level parsing.
   notice that sub-structural features are more beneficial for
   document-level parsing than they are for sentence-level parsing, that
   is, an improvement of 2.2 percentage points versus an improvement of
   0.9 percentage points. this is not surprising because document-level
   dts are generally much larger than sentence-level dts, making the
   sub-structural features more effective for document-level parsing.
   6.4.6   error analysis

   we further analyze the errors made by our discourse parser. as
   described in previous sections, the parser could be wrong in finding
   the right structure as well as the right nuclearity and relation
   labels. [1101]figure 17 presents an example where our parser makes
   mistakes in finding the right structure (notice the units connected by
   attribution and cause in the two example dts) and the right relation
   label (topic-comment vs. background). the comparison between intra- and
   multi-sentential parsing results presented in [1102]sections 6.4.2 and
   [1103]6.4.3 tells us that the errors in structure occur more frequently
   when the dt is large (e.g., at the document level) and the parsing
   model fails to capture the long-range structural dependencies between
   the dt constituents.

   to further analyze the errors made by our parser on the hardest task of
   relation labeling, in [1104]figure 18 we present the confusion matrix
   for our document-level parser tsp 1-1 on the rst   dt test set. in order
   to judge independently the ability of our parser to assign the correct
   relation labels, the confusion matrix is computed based on the
   constituents (see [1105]table 2), where our parser found the right span
   (i.e., structure).[1106]^22 the relations in the matrix are ordered
   according to their frequency in the training set.

   in general, the errors can be explained by two different causes acting
   together: (1) imbalanced distribution of the relations in the corpus,
   and (2) semantic similarity between the relations. the most frequent
   relation elaboration tends to overshadow others, especially the ones
   that are semantically similar (e.g., explanation, background) and less
   frequent (e.g., summary, evaluation). furthermore, our models sometimes
   fail to distinguish relations that are semantically similar (e.g.,
   temporal vs. background, cause vs. explanation).

   now, let us look more closely at a few of these errors. [1107]figure 19
   presents an example where our parser mistakenly labels a summary as
   elaboration. clearly, in this example the text in parentheses (i.e.,
   (cfd)) is an acronym or summary of the text to the left. however,
   parenthesized texts are also used to provide additional information
   (i.e., to elaborate), as exemplified in [1108]figure 20 by two text
   snippets from the rst-dt. notice that although the structure of the
   text (widow of the ..) in the first example is quite distinguishable
   from the structure of (cfd), the text (d., maine) in the second example
   is similar to (cfd) in structure, thus it confuses our model.[1109]^23
   [1110]figure
   figure   17    discourse trees generated by human annotator and our system
   for the text [what's more,][e1] [he believes][e2] [seasonal swings in
   the auto industry this year aren't occurring at the same time in the
   past,][e3] [because of production and pricing differences][e4] [that
   are curbing the accuracy of seasonal adjustments][e5] ] [built into the
   employment data.][e6]
   [1111]figure
   figure   18    confusion matrix for relation labels on the rst   dt test set.
   the y-axis represents true and x-axis represents predicted relations.
   the relations are topic-change (t-c), topic-comment (t-cm),
   textualorganization (t-o), manner-means (m-m), comparison (cmp),
   evaluation (ev), summary (su), condition (cnd), enablement (en), cause
   (ca), temporal (te), explanation (ex), background (ba), contrast (co),
   joint (jo), same   unit (s-u), attribution (at), and elaboration (el).
   [1112]figure
   figure   19    our system mistakenly labels a summary as elaboration.
   [1113]figure
   figure   20    two examples of elaboration by texts in parentheses.

   [1114]figure 21 presents two examples where our parser mistakenly
   labels background and cause as elaboration. however, notice that the
   two discourse relations (i.e., background vs. elaboration and cause vs.
   elaboration) in these examples are semantically very close, and
   arguably both can be applicable.
   [1115]figure
   figure   21    confusion between background/cause and elaboration.

   given these observations, we see two possible ways to improve our
   system. first, we would like to use a more robust method (e.g.,
   ensemble methods with id112) to deal with the imbalanced distribution
   of relations, along with taking advantage of richer semantic knowledge
   (e.g., id152) to cope with the errors caused by
   semantic similarity between the relations. second, to capture
   long-range dependencies between dt constituents, we would like to
   explore the idea of k-best discriminative reranking using tree kernels
   (dinarelli, moschitti, and riccardi [1116]2011). because our parser
   already produces k most probable dts, developing a reranker based on
   discourse tree kernels is very much within our reach.
   7.   conclusions and future directions
   section:
   [choose___________________________]
   [1117]previous section [1118]next section

   in this article we have presented codra, a complete probabilistic
   discriminative framework for performing rhetorical analysis in the rst
   framework. codra comprises components for performing both discourse
   segmentation and discourse parsing. the discourse segmenter is a binary
   classifier based on a maximum id178 model, and the discourse parser
   applies an optimal parsing algorithm to probabilities inferred from two
   crf models: one for intra-sentential parsing and the other for
   multi-sentential parsing. the crf models effectively represent the
   structure and the label of discourse tree constituents jointly.
   furthermore, the dcrf model for intra-sentential parsing captures the
   sequential dependencies between the constituents. the two separate
   parsing models use their own informative feature sets and the
   distributional variations of the relation labels in their respective
   parsing conditions.

   we have also presented two approaches to effectively combine the
   intra-sentential and the multi-sentential parsing modules, which can
   exploit the strong correlation observed between the text structure and
   the structure of the discourse tree. the first approach (1s   1s) builds
   a dt for every sentence using the intra-sentential parser, and then
   runs the multi-sentential parser on the resulting sentence-level dts.
   to deal with leaky boundaries, our second approach (sliding window)
   builds sentence-level discourse sub-trees by applying the
   intra-sentential parser on a sliding window, covering two adjacent
   sentences and then consolidating the results produced by overlapping
   windows. after that, the multi-sentential parser takes all these
   sentence-level sub-trees and builds a full rhetorical parse for the
   whole document.

   finally, we have extended the parsing algorithm to generate k most
   probable parse hypotheses for each input text, which could be used in a
   reranker to improve over the initial ranking using global features like
   long-range structural dependencies.

   empirical evaluations on two different genres demonstrate that our
   approach to discourse segmentation achieves state-of-the-art
   performance more efficiently using fewer features. a series of
   experiments on the discourse parsing task shows that both our intra-
   and multi-sentential parsers significantly outperform the state of the
   art, often by a wide margin. a comparison between our combination
   strategies reveals that the sliding window approach is more robust
   across domains. furthermore, the oracle accuracy computed based on the
   k-best parse hypotheses generated by our parser demonstrates that a
   reranker could potentially improve the accuracy even further.

   our error analysis reveals that although the sliding window approach
   finds more correct tree structures, in some cases it induces noise for
   the relation labels from the neighboring sentences. with respect to the
   performance of our discourse parser on the relation labeling task we
   also found that the most frequent relations tend to mislead the
   identification of the less frequent ones, and the models sometimes fail
   to distinguish relations that are semantically similar.

   the work presented in this article leads us to several interesting
   future directions. our short-term goal is to develop a k-best
   discriminative reranking discourse parser using tree kernels applied to
   discourse trees. we also plan to investigate to what extent discourse
   segmentation and discourse parsing can be performed jointly.

   we would also like to explore how our system performs on other genres
   like conversational (e.g., blogs, e-mails) and evaluative (e.g.,
   customer reviews) texts. to address the problem of limited annotated
   data in various genres, we are planning to develop an interactive
   version of our system that will allow users to fix the output of the
   system with minimal effort and let the system learn from that feedback.

   another interesting future direction is to perform extrinsic
   evaluations of our system in downstream applications. one important
   application of rhetorical structure is text summarization, where a
   significant challenge is producing not only informative but also
   coherent summaries. a number of researchers have already investigated
   the utility of rhetorical structure for measuring text importance
   (i.e., informativeness) in summarization (marcu [1119]2000b; daum   and
   marcu [1120]2002; louis, joshi, and nenkova [1121]2010). recently,
   christensen et al. ([1122]2013, [1123]2014) propose to perform sentence
   selection and ordering at the same time, and use constraints on
   discourse structure to make the summaries coherent. however, they
   represent the discourse as an unweighted directed graph, which is
   shallow and not sufficiently informative in most cases. furthermore,
   their approach does not allow compression at the sentence level, which
   is often beneficial in summarization. in the future, we would like to
   investigate the utility of our rhetorical structure for performing
   sentence compression, selection, and ordering in a joint summarization
   process.

   discourse structure can also play important roles in sentiment
   analysis. a key research problem in id31 is extracting
   fine-grained opinions about different aspects of a product. several
   recent papers (somasundaran [1124]2010; lazaridou, titov, and sporleder
   [1125]2013) exploited the rhetorical structure for this task. another
   challenging problem is assessing the overall opinion expressed in a
   review because not all sentences in a review contribute equally to the
   overall sentiment. for example, some sentences are subjective, whereas
   others are objective (pang and lee [1126]2004); some express the main
   claims, whereas others support them (taboada et al. [1127]2011); some
   express opinions about the main entity, whereas others are about the
   peripherals. discourse structure could be useful to capture the
   relative weights of the discourse units towards the overall sentiment.
   for example, the nucleus and satellite distinction along with the
   rhetorical relations could be useful to infer the relative weights of
   the connecting discourse units.

   among other applications of discourse structure, machine translation
   (mt) and its evaluation have received a resurgence of interest
   recently. a workshop dedicated to discourse in machine translation was
   arranged recently at the acl 2013 conference (webber et al.
   [1128]2013). researchers believe that mt systems should consider
   discourse phenomena that go beyond the current sentence to ensure
   consistency in the choice of lexical items or referring expressions,
   and the fact that source-language coherence relations are also realized
   in the target language (i.e., translating at the document-level
   [hardmeier, nivre, and tiedemann [1129]2012]). guzm  n et al.
   ([1130]2014a, [1131]2014b) and joty et al. ([1132]2014) propose new
   discourse-aware automatic id74 for mt systems using our
   discourse analysis tool. they demonstrate that sentence-level discourse
   information is complementary to the state-of-the-art evaluation
   metrics, and by combining the discourse-based metrics with the metrics
   from the asiya mt evaluation toolkit (gim  nez and m  rquez [1133]2010),
   they won the wmt 2014 metrics shared task challenge (mach    ek and bojar
   [1134]2014) both at the segment- and at the system-level. these results
   suggest that discourse structure helps to distinguish better
   translations from worse ones. thus, it would be interesting to explore
   whether discourse information can be used to rerank alternative mt
   hypotheses as a post-processing step for the mt output.

   a longer-term goal is to extend our framework to also work with graph
   structures of discourse, as recommended by several recent discourse
   theories (wolf and gibson [1135]2005). once we achieve similar
   performance on graph structures, we will perform extrinsic evaluations
   to determine their relative utility for various nlp tasks.

   finally, we hope that the online demo, the source code of codra, and
   the id74 that we made publicly available in this work
   will facilitate other researchers in extending our work and in applying
   discourse parsing to their nlp tasks.
   bibliographic note
   section:
   [choose________________________]
   [1136]previous section [1137]next section

   portions of this work were previously published in two conference
   proceedings (joty, carenini, and ng [1138]2012; joty et al.
   [1139]2013). this article significantly extends our previous work in
   several ways, most notably: (1) we extend the parsing algorithm to
   generate k-most probable parse hypotheses for each input text
   ([1140]section 4.2); (2) we show the oracle accuracies for k-best
   discourse parsing both at the sentence level and at the document level
   ([1141]section 6.4.4); (3) to support our claim, we compare our best
   results with several variations of our approach (see crf-nc in
   [1142]section 6.4.2, and crf-o and crf-t in [1143]section 6.4.3); (4)
   we analyze the relative importance of different features for intra- and
   multi-sentential discourse parsing ([1144]section 6.4.5); and (5) we
   perform in-depth error analysis of our complete rhetorical analysis
   framework ([1145]section 6.4.6).
   appendix a. sample output generated by an online demo of codra
   section:
   [choose_________________________]
   [1146]previous section [1147]next section

   acknowledgments
   section:
   [choose________________________]
   [1148]previous section [1149]next section

   the authors acknowledge the funding support of nserc canada graduate
   scholarship (cgs-d). many thanks to bonnie webber, amanda stent,
   carolyn rose, lluis marquez, samantha wray, and the anonymous reviewers
   for their insightful comments on an earlier version of this article.
   notes
   section:
   [choose________________________]
   [1150]previous section [1151]next section

   1    we categorize this approach as unsupervised because it does not rely
   on human-annotated data.

   2    [1152]http://www.isi.edu/licensed-sw/spade/.

   3    [1153]http://nlp.prendingerlab.net/hilda/.

   4    the demo of codra is available at
   [1154]http://109.228.0.153/discourse_parser_demo/. the source code of
   codra is available from [1155]http://alt.qcri.org/tools/.

   5    the input text in the demo in [1156]appendix a is taken from
   [1157]www.bbc.co.uk/news/world-asia-26106490.

   6    by the term    fat    we refer to crfs with multiple (interconnected)
   chains of output variables.

   7    for n + 1 edus, the number of valid discourse tree structures (i.e.,
   not counting possible variations in the nuclearity and relation labels)
   is the catalan number c[n].

   8    the higher the conditional id178, the lower the mutual
   information, and vice versa.

   9    we agree that with potentially sub-optimal, sub-structural features
   in the parsing model, cky may end up finding a sub-optimal dt. but that
   is a separate issue.

   10    do not confuse the term two-stage with the term two-pass.

   11    available at [1158]http://cogcomp.cs.illinois.edu/page/software.

   12    available from [1159]alt.qcri.org/tools/.

   13    because the two tasks   discourse segmentation and intra-sentential
   parsing   operate at the sentence level, the cross validation was
   performed over sentences for their evaluation.

   14    because we did not have access to the system or to the complete
   output/results of fisher and roark ([1160]2007), we were not able to
   perform a statistical significance test.

   15    not all relations take all the possible nuclearity statuses. for
   example, elaboration and attribution are mono-nuclear relations, and
   same   unit and joint are multi-nuclear relations.

   16    the parsing performance reported in [1161]table 4 for crf-nc is
   when the crf parsing model is trained on a balanced data set (an equal
   number of instances with s=1 and s=0); training on full but imbalanced
   data set gives slightly lower results.

   17    our emnlp and acl publications (joty, carenini, and ng [1162]2012;
   joty et al. [1163]2013) reported slightly lower parsing accuracies.
   fixing a bug in the parsing algorithm accounts for the difference.

   18    subba and di-eugenio ([1164]2009) report their results based on an
   arbitrary split between training and test sets. because we did not have
   access to their particular split, we compare our model's performance
   based on 10-fold cross validation with their reported results. also,
   because we did not have access to their system/output, we could not
   perform a significance test on the instructional corpus.

   19    because we did not have access to the output or to the system of
   subba and di-eugenio ([1165]2009), we were not able to perform a
   significance test on the instructional corpus.

   20    the performance of this model for intra-sentential parsing is
   reported in [1166]table 4 under the name crf-nc.

   21    text structural features are included in the organizational
   features for multi-sentential parsing.

   22    therefore, the counts of the relations shown in the table may not
   match the ones in the test set.

   23    d., maine in this example refers to democrat from state maine.
   references
   section:
   [choose________________________]
   [1167]previous section [1168]next section
   althaus, ernst, denys duchier, alexander koller, kurt mehlhorn, joachim
   niehren, and sven thiel. 2003. an efficient graph algorithm for
   dominance constraints. journal of algorithms, 48(1):194   219.
   [1169]crossref, [1170]google scholar
   asher, nicholas and alex lascarides, 2003. logics of conversation.
   cambridge university press. [1171]google scholar
   barzilay, regina and michael elhadad. 1997. using lexical chains for
   text summarization. in proceedings of the 35th annual meeting of the
   association for computational linguistics and the 8th european chapter
   meeting of the association for computational linguistics, workshop on
   intelligent scalable test summarization, pages 10   17, madrid.
   [1172]google scholar
   biran, or and owen rambow. 2011. identifying justifications in written
   dialogs by classifying text as argumentative. international journal of
   semantic computing, 5(4):363   381. [1173]crossref, [1174]google scholar
   blair-goldensohn, sasha, kathleen mckeown, and owen rambow. 2007.
   building and refining rhetorical-semantic relation models. in
   proceedings of the human language technologies: the annual conference
   of the north american chapter of the association for computational
   linguistics, hlt-naacl'07, pages 428   435. rochester, ny. [1175]google
   scholar
   blitzer, john. 2008. id20 of natural language processing
   systems. ph.d. thesis, university of pennsylvania. [1176]google scholar
   breiman, leo. 1996. id112 predictors. machine learning,
   24(2):123   140. [1177]crossref, [1178]google scholar
   carlson, lynn and daniel marcu. 2001. discourse tagging reference
   manual. technical report isi-tr-545, university of southern california
   information sciences institute. [1179]google scholar
   carlson, lynn, daniel marcu, and mary ellen okurowski. 2002. rst
   discourse treebank (rst   dt) ldc2002t07. linguistic data consortium,
   philadelphia. [1180]google scholar
   chali, yllias and shafiq joty. 2007. id51 using
   lexical cohesion. in proceedings of semeval-2007, pages 476   479,
   prague. [1181]google scholar
   charniak, eugene. 2000. a maximum-id178-inspired parser. in
   proceedings of the 1st north american chapter of the association for
   computational linguistics conference, naacl'00, pages 132   139, seattle,
   wa. [1182]google scholar
   charniak, eugene and mark johnson. 2005. coarse-to-fine n-best parsing
   and maxent discriminative reranking. in proceedings of the 43rd annual
   meeting of the association for computational linguistics, acl'05, pages
   173   180, ann arbor, mi. [1183]crossref, [1184]google scholar
   christensen, janara, mausam, stephen soderland, and oren etzioni. 2013.
   towards coherent id57. in proceedings of the
   2013 conference of the north american chapter of the association for
   computational linguistics: human language technologies, naacl-hlt'13,
   pages 1163   1173, atlanta, ga. [1185]google scholar
   christensen, janara, stephen soderland, gagan bansal, and mausam. 2014.
   hierarchical summarization: scaling up id57. in
   proceedings of the 52nd annual meeting of the association for
   computational linguistics, acl'13, pages 902   912, baltimore, md.
   [1186]google scholar
   collins, michael. 2003. head-driven statistical models for natural
   language parsing. computational linguistics, 29(4):589   637.
   [1187]link, [1188]google scholar
   collins, michael and terry koo. 2005. discriminative reranking for
   natural language parsing. computational linguistics, 31(1):25   70.
   [1189]link, [1190]google scholar
   collobert, ronan, jason weston, l  on bottou, michael karlen, koray
   kavukcuoglu, and pavel kuksa. 2011. natural language processing
   (almost) from scratch. journal of machine learning research,
   12:2493   2537. [1191]google scholar
   cristea, dan, nancy ide, and laurent romary. 1998. veins theory: a
   model of global discourse cohesion and coherence. in proceedings of the
   36th annual meeting of the association for computational linguistics
   and of the 17th international conference on computational linguistics
   (coling/acl'98), pages 281   285. montreal. [1192]crossref, [1193]google
   scholar
   danlos, laurence. 2009. d-stag: a discourse analysis formalism based on
   synchronous tags. tal, 50(1):111   143. [1194]google scholar
   daum  , iii, hal. 2007. frustratingly easy id20. in
   proceedings of the 45th annual meeting of the association for
   computational linguistics, acl'07, pages 256   263, prague. [1195]google
   scholar
   daum  , iii, hal and daniel marcu. 2002. a noisy-channel model for
   document compression. in proceedings of the 40th annual meeting of the
   association for computational linguistics, acl '02, pages 449   456,
   philadelphia, pa. [1196]google scholar
   dinarelli, marco, alessandro moschitti, and giuseppe riccardi. 2011.
   discriminative reranking for spoken language understanding. ieee
   transactions on audio, speech and language processing (taslp),
   20:526   539. [1197]google scholar
   duverle, david and helmut prendinger. 2009. a novel discourse parser
   based on support vector machine classification. in proceedings of the
   joint conference of the 47th annual meeting of the acl and the 4th
   international joint conference on natural language processing of the
   afnlp, pages 665   673, suntec. [1198]google scholar
   egg, markus, alexander koller, and joachim niehren. 2001. the
   constraint language for lambda structures. journal of logic, language
   and information, 10(4):457   485. [1199]crossref, [1200]google scholar
   eisner, jason. 1996. three new probabilistic models for dependency
   parsing: an exploration. in proceedings of the 16th conference on
   computational linguistics - volume 1, coling '96, pages 340   345,
   copenhagen. [1201]crossref, [1202]google scholar
   fellbaum, christiane. 1998. id138   an electronic lexical database. mit
   press, cambridge, ma. [1203]google scholar
   feng, vanessa and graeme hirst. 2012. text-level discourse parsing with
   rich linguistic features. in proceedings of the 50th annual meeting of
   the association for computational linguistics, acl '12, pages 60   68,
   jeju island. [1204]google scholar
   feng, vanessa and graeme hirst. 2014. a linear-time bottom-up discourse
   parser with constraints and post-editing. in proceedings of the 52nd
   annual meeting of the association for computational linguistics, acl
   '14, pages 511   521, baltimore, md. [1205]google scholar
   finkel, jenny rose, alex kleeman, and christopher manning. 2008.
   efficient, feature-based, conditional random field parsing. in
   proceedings of the 46th annual meeting of the association for
   computational linguistics, acl'08, pages 959   967, columbus, oh.
   [1206]google scholar
   fisher, seeger and brian roark. 2007. the utility of parse-derived
   features for automatic discourse segmentation. in proceedings of the
   45th annual meeting of the association for computational linguistics,
   acl'07, pages 488   495, prague. [1207]google scholar
   galley, michel and kathleen mckeown. 2003. improving word sense
   disambiguation in lexical chaining. in proceedings of the 18th
   international joint conference on artificial intelligence, ijcai'03,
   pages 1486   1488, acapulco. [1208]google scholar
   galley, michel, kathleen mckeown, eric fosler-lussier, and hongyan
   jing. 2003. discourse segmentation of multi-party conversation. in
   proceedings of the 41st annual meeting of the association for
   computational linguistics - volume 1, acl '03, pages 562   569, sapporo.
   [1209]crossref, [1210]google scholar
   ghosh, sucheta, richard johansson, giuseppe riccardi, and sara tonelli.
   2011. shallow discourse parsing with id49. in
   proceedings of the 5th international joint conference on natural
   language processing, ijcnlp'11, pages 1071   1079, chiang mai.
   [1211]google scholar
   gim  nez, jes  s and llu  s m  rquez. 2010. linguistic measures for
   automatic machine translation evaluation. machine translation,
   24(3   4):77   86. [1212]crossref, [1213]google scholar
   guzm  n, francisco, shafiq joty, llu  s m  rquez, alessandro moschitti,
   preslav nakov, and massimo nicosia. 2014a. learning to differentiate
   better from worse translations. in proceedings of the 2014 conference
   on empirical methods in natural language processing (emnlp), pages
   214   220, doha. [1214]crossref, [1215]google scholar
   guzm  n, francisco, shafiq joty, llu  s m  rquez, and preslav nakov.
   2014b. using discourse structure improves machine translation
   evaluation. in proceedings of the 52nd annual meeting of the
   association for computational linguistics (volume 1: long papers),
   pages 687   698, baltimore, md. [1216]crossref, [1217]google scholar
   halliday, michael and ruqaiya hasan. 1976. cohesion in english.
   longman, london. [1218]google scholar
   hardmeier, christian, joakim nivre, and j  rg tiedemann. 2012.
   document-wide decoding for phrase-based statistical machine
   translation. in proceedings of the 2012 joint conference on empirical
   methods in natural language processing and computational natural
   language learning, emnlp-conll '12, pages 1179   1190, jeju island.
   [1219]google scholar
   hernault, hugo, helmut prendinger, david duverle, and mitsuru ishizuka.
   2010. hilda: a discourse parser using support vector machine
   classification. dialogue and discourse, 1(3):1   33.
   [1220]crossref, [1221]google scholar
   hirst, graeme and david st-onge. 1997. lexical chains as representation
   of context for the detection and correction of malapropisms. in
   christiane fellbaum, editor, id138: an electronic lexical database
   and some of its applications. mit press, pages 305   332. [1222]google
   scholar
   hobbs, jerry. 1979. coherence and coreference. cognitive science,
   3:67   90. [1223]crossref, [1224]google scholar
   huang, liang and david chiang. 2005. better k-best parsing. in
   proceedings of the ninth international workshop on parsing technology,
   parsing '05, pages 53   64, stroudsburg, pa. [1225]crossref, [1226]google
   scholar
   ji, yangfeng and jacob eisenstein. 2014. representation learning for
   text-level discourse parsing. in proceedings of the 52nd annual meeting
   of the association for computational linguistics (volume 1: long
   papers), pages 13   24, baltimore, md. [1227]crossref, [1228]google
   scholar
   joty, shafiq, giuseppe carenini, and raymond t. ng. 2012. a novel
   discriminative framework for sentence-level discourse analysis. in
   proceedings of the 2012 joint conference on empirical methods in
   natural language processing and computational natural language
   learning, emnlp-conll '12, pages 904   915, jeju island. [1229]google
   scholar
   joty, shafiq, giuseppe carenini, and raymond t. ng. 2013. topic
   segmentation and labeling in asynchronous conversations. journal of
   artificial intelligence research (jair), 47:521   573. [1230]google
   scholar
   joty, shafiq, giuseppe carenini, raymond t. ng, and yashar mehdad.
   2013. combining intra- and multi-sentential rhetorical parsing for
   document-level discourse analysis. in proceedings of the 51st annual
   meeting of the association for computational linguistics, acl '13,
   pages 486   496, sofia. [1231]google scholar
   joty, shafiq, francisco guzm  n, llu  s m  rquez, and preslav nakov. 2014.
   discotk: using discourse structure for machine translation evaluation.
   in proceedings of the ninth workshop on statistical machine
   translation, wmt '14, pages 402   408, baltimore, md.
   [1232]crossref, [1233]google scholar
   jurafsky, daniel and james martin. 2008. statistical parsing. in speech
   and language processing, chapter 14. prentice hall. [1234]google
   scholar
   knight, kevin and jonathan graehl. 2005. an overview of probabilistic
   tree transducers for natural language processing. in computational
   linguistics and intelligent text processing, volume 3406 of lecture
   notes in computer science. springer, berlin heidelberg, pages 1   24.
   [1235]google scholar
   knott, alistair and robert dale. 1994. using linguistic phenomena to
   motivate a set of coherence relations. discourse processes, 18:35   62.
   [1236]crossref, [1237]google scholar
   koller, alexander, michaela regneri, and stefan thater. 2008. regular
   tree grammars as a formalism for scope underspecification. in
   proceedings of the 46th annual meeting of the association for
   computational linguistics on human language technologies, pages
   218   226, columbus, oh. [1238]google scholar
   lafferty, john, andrew mccallum, and fernando pereira. 2001.
   id49: probabilistic models for segmenting and
   labeling sequence data. in proceedings of the eighteenth international
   conference on machine learning, pages 282   289, san francisco, ca.
   [1239]google scholar
   lazaridou, angeliki, ivan titov, and caroline sporleder. 2013. a
   bayesian model for joint unsupervised induction of sentiment, aspect
   and discourse representations. in proceedings of the 51st annual
   meeting of the association for computational linguistics, acl '13,
   sofia. [1240]google scholar
   li, jiwei, rumeng li, and eduard hovy. 2014. recursive deep models for
   discourse parsing. in proceedings of the 2014 conference on empirical
   methods in natural language processing (emnlp), pages 2061   2069, doha.
   [1241]crossref, [1242]google scholar
   li, sujian, liang wang, ziqiang cao, and wenjie li. 2014. text-level
   discourse id33. in proceedings of the 52nd annual meeting
   of the association for computational linguistics (volume 1: long
   papers), pages 25   35, baltimore, md. [1243]crossref, [1244]google
   scholar
   louis, annie, aravind joshi, and ani nenkova. 2010. discourse
   indicators for content selection in summarization. in proceedings of
   the 11th annual meeting of the special interest group on discourse and
   dialogue, sigdial '10, pages 147   156, tokyo. [1245]google scholar
   mach    ek, matou   and ond  ej bojar. 2014. results of the wmt14 metrics
   shared task. in proceedings of the ninth workshop on statistical
   machine translation, baltimore, md. [1246]google scholar
   magerman, david. 1995. statistical decision-tree models for parsing. in
   proceedings of the 33rd annual meeting of the association for
   computational linguistics, acl'95, pages 276   283, cambridge, ma.
   [1247]crossref, [1248]google scholar
   mann, william and sandra thompson. 1988. rhetorical structure theory:
   toward a functional theory of text organization. text, 8(3):243   281.
   [1249]crossref, [1250]google scholar
   marcu, daniel. 1999. a decision-based approach to rhetorical parsing.
   in proceedings of the 37th annual meeting of the association for
   computational linguistics on computational linguistics, acl'99, pages
   365   372, morristown, nj. [1251]crossref, [1252]google scholar
   marcu, daniel. 2000a. the rhetorical parsing of unrestricted texts: a
   surface-based approach. computational linguistics, 26:395   448.
   [1253]link, [1254]google scholar
   marcu, daniel. 2000b. the theory and practice of discourse parsing and
   summarization. mit press, cambridge, ma. [1255]google scholar
   marcu, daniel and abdessamad echihabi. 2002. an unsupervised approach
   to recognizing discourse relations. in proceedings of the 40th annual
   meeting of the association for computational linguistics, acl'02, pages
   368   375. philadelphia, pa. [1256]google scholar
   marcus, mitchell, mary marcinkiewicz, and beatrice santorini. 1994.
   building a large annotated corpus of english: the id32.
   computational linguistics, 19(2):313   330. [1257]google scholar
   martin, james, 1992. english text: system and structure. john benjamins
   publishing company, philadelphia/amsterdam. [1258]google scholar
   maslennikov, mstislav and tat-seng chua. 2007. a multi-resolution
   framework for information extraction from free text. in proceedings of
   the 45th annual meeting of the association for computational
   linguistics, pages 592   599, prague. [1259]google scholar
   mccallum, andrew. 2002. mallet: a machine learning for language
   toolkit. [1260]http://mallet.cs.umass.edu. [1261]google scholar
   mccallum, andrew, dayne freitag, and fernando c. n. pereira. 2000.
   maximum id178 markov models for information extraction and
   segmentation. in proceedings of the seventeenth international
   conference on machine learning, icml '00, pages 591   598, san francisco,
   ca. [1262]google scholar
   mcdonald, ryan, koby crammer, and fernando pereira. 2005. online
   large-margin training of dependency parsers. in proceedings of the 43rd
   annual meeting of the association for computational linguistics, acl
   '05, pages 91   98, ann arbor, mi. [1263]crossref, [1264]google scholar
   mcdonald, ryan, fernando pereira, kiril ribarov, and jan haji  . 2005.
   non-projective id33 using spanning tree algorithms. in
   proceedings of the conference on human language technology and
   empirical methods in natural language processing, hlt '05, pages
   523   530, stroudsburg, pa. [1265]crossref, [1266]google scholar
   morris, jane and graeme hirst. 1991. lexical cohesion computed by
   thesaural relations as an indicator of structure of text. computational
   linguistics, 17(1):21   48. [1267]google scholar
   murphy, kevin. 2012. machine learning: a probabilistic perspective. the
   mit press. cambridge, ma. [1268]google scholar
   pang, bo and lillian lee. 2004. a sentimental education: sentiment
   analysis using subjectivity summarization based on minimum cuts. in
   proceedings of the 42nd annual meeting of the association for
   computational linguistics, acl '04, pages 271   278. barcelona.
   [1269]crossref, [1270]google scholar
   pitler, emily and ani nenkova. 2009. using syntax to disambiguate
   explicit discourse connectives in text. in proceedings of the
   acl-ijcnlp 2009 conference short papers, aclshort '09, pages 13   16,
   suntec. [1271]google scholar
   poole, david and alan mackworth, 2010. artificial intelligence:
   foundations of computational agents. cambridge university press.
   [1272]google scholar
   prasad, rashmi, nikhil dinesh, alan lee, eleni miltsakaki, livio
   robaldo, aravind joshi, and bonnie webber. 2008. the penn discourse
   treebank 2.0. in proceedings of the sixth international conference on
   language resources and evaluation (lrec), pages 2961   2968, marrakech.
   [1273]google scholar
   prasad, rashmi, aravind joshi, nikhil dinesh, alan lee, eleni
   miltsakaki, and bonnie webber. 2005. the penn discourse treebank as a
   resource for id86. in proceedings of the corpus
   linguistics workshop on using corpora for id86,
   pages 25   32, birmingham. [1274]google scholar
   regneri, michaela, markus egg, and alexander koller. 2008. efficient
   processing of underspecified discourse representations. in proceedings
   of the 46th annual meeting of the association for computational
   linguistics on human language technologies: short papers, hlt-short
   '08, pages 245   248, columbus, oh. [1275]crossref, [1276]google scholar
   reyle, uwe. 1993. dealing with ambiguities by underspecification:
   construction, representation and deduction. journal of semantics,
   10(2):123   179. [1277]crossref, [1278]google scholar
   schapire, robert e. and yoram singer. 2000. boostexter: a
   boosting-based system for text categorization. machine learning,
   39(2   3):135   168. [1279]crossref, [1280]google scholar
   schauer, holger and udo hahn. 2001. anaphoric cues for coherence
   relations. in proceedings of the conference on recent advances in
   natural language processing, ranlp '01, pages 228   234, tzigov chark.
   [1281]google scholar
   schilder, frank. 2002. robust discourse parsing via discourse markers,
   topicality and position. natural language engineering, 8(3):235   255.
   [1282]google scholar
   sha, fei and fernando pereira. 2003. id66 with conditional
   random fields. in proceedings of the 2003 conference of the north
   american chapter of the association for computational linguistics on
   human language technology - volume 1, naacl-hlt'03, pages 134   141,
   edmonton. [1283]crossref, [1284]google scholar
   silber, gregory and kathleen mccoy. 2002. efficiently computed lexical
   chains as an intermediate representation for automatic text
   summarization. computational linguistics, 28(4):487   496.
   [1285]link, [1286]google scholar
   smith, noah a. 2011. linguistic structure prediction. synthesis
   lectures on human language technologies. morgan and claypool.
   [1287]google scholar
   socher, richard, john bauer, christopher d. manning, and ng andrew y.
   2013a. parsing with compositional vector grammars. in proceedings of
   the 51st annual meeting of the association for computational
   linguistics (volume 1: long papers), pages 455   465, sofia. [1288]google
   scholar
   socher, richard, alex perelygin, jean wu, jason chuang, christopher d.
   manning, andrew ng, and christopher potts. 2013b. recursive deep models
   for semantic compositionality over a sentiment treebank. in proceedings
   of the 2013 conference on empirical methods in natural language
   processing, pages 1631   1642, seattle, wa. [1289]google scholar
   somasundaran, s. 2010. discourse-level relations for opinion analysis.
   ph.d. thesis, university of pittsburgh, pa. [1290]google scholar
   soricut, radu and daniel marcu. 2003. sentence level discourse parsing
   using syntactic and lexical information. in proceedings of the 2003
   conference of the north american chapter of the association for
   computational linguistics on human language technology - volume 1,
   naacl'03, pages 149   156, edmonton. [1291]crossref, [1292]google scholar
   sporleder, caroline. 2007. manually vs. automatically labelled data in
   discourse relation classification. effects of example and feature
   selection. ldv forum, 22(1):1   20. [1293]google scholar
   sporleder, caroline and mirella lapata. 2004. automatic paragraph
   identification: a study across languages and domains. in proceedings of
   the 2004 conference on empirical methods in natural language
   processing, emnlp '04, pages 72   79, barcelona. [1294]google scholar
   sporleder, caroline and mirella lapata. 2005. discourse chunking and
   its application to sentence compression. in proceedings of the
   conference on human language technology and empirical methods in
   natural language processing, hlt-emnlp'05, pages 257   264, vancouver.
   [1295]crossref, [1296]google scholar
   sporleder, caroline and alex lascarides. 2005. exploiting linguistic
   cues to classify rhetorical relations. in proceedings of recent
   advances in natural language processing (ranlp), pages 157   166,
   bulgaria. [1297]google scholar
   sporleder, caroline and alex lascarides. 2008. using automatically
   labelled examples to classify rhetorical relations: an assessment.
   natural language engineering, 14(3):369   416.
   [1298]crossref, [1299]google scholar
   stede, manfred. 2004. the potsdam commentary corpus. in proceedings of
   the acl-04 workshop on discourse annotation, pages 96   102, barcelona.
   [1300]google scholar
   stede, manfred. 2011. discourse processing. synthesis lectures on human
   language technologies. morgan and claypool publishers. [1301]google
   scholar
   subba, rajen and barbara di-eugenio. 2009. an effective discourse
   parser that uses rich linguistic information. in proceedings of human
   language technologies: the 2009 annual conference of the north american
   chapter of the association for computational linguistics, hlt-naacl'09,
   pages 566   574, boulder, co. [1302]crossref, [1303]google scholar
   sutton, charles and andrew mccallum. 2012. an introduction to
   id49. foundations and trends in machine learning,
   4(4):267   373. [1304]crossref, [1305]google scholar
   sutton, charles, andrew mccallum, and khashayar rohanimanesh. 2007.
   dynamic id49: factorized probabilistic models for
   labeling and segmenting sequence data. journal of machine learning
   research (jmlr), 8:693   723. [1306]google scholar
   taboada, maite. 2006. discourse markers as signals (or not) of
   rhetorical relations. journal of pragmatics, 38(4):567   592.
   [1307]crossref, [1308]google scholar
   taboada, maite, julian brooke, milan tofiloski, kimberly voll, and
   manfred stede. 2011. lexicon-based methods for id31.
   computational linguistics, 37(2):267   307. [1309]link, [1310]google
   scholar
   taboada, maite and william c. mann. 2006. rhetorical structure theory:
   looking back and moving ahead. discourse studies, 8(3):423   459.
   [1311]crossref, [1312]google scholar
   teufel, simone and marc moens. 2002. summarizing scientific articles:
   experiments with relevance and rhetorical status. computational
   linguistics, 28(4):409   445. [1313]link, [1314]google scholar
   verberne, suzan, lou boves, nelleke oostdijk, and peter-arno coppen.
   2007. evaluating discourse-based answer extraction for why-question
   answering. in proceedings of the 30th annual international acm sigir
   conference on research and development in information retrieval,
   sigir'07, pages 735   736, amsterdam. [1315]crossref, [1316]google
   scholar
   vliet, nynke and gisela redeker. 2011. complex sentences as leaky units
   in discourse parsing. in proceedings of constraints in discourse, pages
   1   9, agay   saint raphael. [1317]google scholar
   webber, b. 2004. d-ltag: extending lexicalized tag to discourse.
   cognitive science, 28(5):751   779. [1318]crossref, [1319]google scholar
   webber, bonnie, andrei popescu-belis, katja markert, and j  rg
   tiedemann, editors. 2013. proceedings of the workshop on discourse in
   machine translation. acl, sofia. [1320]google scholar
   wick, michael, khashayar rohanimanesh, kedare bellare, aron culotta,
   and andrew mccallum. 2011. samplerank: training factor graphs with
   atomic gradients. in proceedings of the 28th international conference
   on machine learning, icml'11, pages 777   784. bellevue, wa. [1321]google
   scholar
   wolf, florian and edward gibson. 2005. representing discourse
   coherence: a corpus-based study. computational linguistics, 31:249   288.
   [1322]link, [1323]google scholar
   zhang, yuan, tao lei, regina barzilay, tommi jaakkola, and amir
   globerson. 2014. steps to excellence: simple id136 with refined
   scoring of dependency trees. in proceedings of the 52nd annual meeting
   of the association for computational linguistics, pages 197   207,
   baltimore, md. [1324]google scholar
   shafiq joty*
   qatar computing research institute
   giuseppe carenini**
   university of british columbia
   raymond t. ng   
   university of british columbia

   *arabic language technologies, qatar computing research institute,
   qatar foundation, doha, qatar. e-mail: [1325][email protected].

   **computer science department, university of british columbia,
   vancouver, bc, canada, v6t 1z4. e-mail: [1326][email protected].

      computer science department, university of british columbia,
   vancouver, bc, canada, v6t 1z4. e-mail: [1327][email protected].
   [1328]forthcoming
   [1329][preview-1489555631417.svg] download options
   [arrow-button-1488499533633.svg]

codra: a novel discriminative framework for rhetorical analysis

   [1330]shafiq joty, [1331]giuseppe carenini and [1332]raymond t. ng
   [1333]https://doi.org/10.1162/coli_a_00226
   received: may 11, 2014
   accepted: march 18, 2015
   published online: september 08, 2015
     * [1334]full text
     * [1335]authors
     * [1336]pdf
     * [1337]pdf plus

abstract

   section:
   [choose________________________]
   [1338]next section

   clauses and sentences rarely stand on their own in an actual discourse;
   rather, the relationship between them carries important information
   that allows the discourse to express a meaning as a whole beyond the
   sum of its individual parts. rhetorical analysis seeks to uncover this
   coherence structure. in this article, we present codra    a complete
   probabilistic discriminative framework for performing rhetorical
   analysis in accordance with rhetorical structure theory, which posits a
   tree representation of a discourse.

   codra comprises a discourse segmenter and a discourse parser. first,
   the discourse segmenter, which is based on a binary classifier,
   identifies the elementary discourse units in a given text. then the
   discourse parser builds a discourse tree by applying an optimal parsing
   algorithm to probabilities inferred from two id49:
   one for intra-sentential parsing and the other for multi-sentential
   parsing. we present two approaches to combine these two stages of
   parsing effectively. by conducting a series of empirical evaluations
   over two different data sets, we demonstrate that codra significantly
   outperforms the state-of-the-art, often by a wide margin. we also show
   that a reranking of the k-best parse hypotheses generated by codra can
   potentially improve the accuracy even further.
   no rights reserved. this work was authored as part of the contributor's
   official duties as an employee of the united states government and is
   therefore a work of the united states government. in accordance with 17
   u.s.c. 105, no copyright protection is available for such works under
   u.s. law.
   1.   introduction
   section:
   [choose________________________]
   [1339]previous section [1340]next section

   a well-written text is not merely a sequence of independent and
   isolated sentences, but instead a sequence of structured and related
   sentences, where the meaning of a sentence relates to the previous and
   the following ones. in other words, a well-written text has a coherence
   structure (halliday and hasan [1341]1976; hobbs [1342]1979), which
   logically binds its clauses and sentences together to express a meaning
   as a whole. rhetorical analysis seeks to uncover this coherence
   structure underneath the text; this has been shown to be beneficial for
   many natural language processing (nlp) applications, including text
   summarization and compression (marcu [1343]2000b; daum   and marcu
   [1344]2002; sporleder and lapata [1345]2005; louis, joshi, and nenkova
   [1346]2010), text generation (prasad et al. [1347]2005), machine
   translation evaluation (guzm  n et al. [1348]2014a, [1349]2014b; joty et
   al. [1350]2014), id31 (somasundaran [1351]2010;
   lazaridou, titov, and sporleder [1352]2013), information extraction
   (teufel and moens [1353]2002; maslennikov and chua [1354]2007), and
   id53 (verberne et al. [1355]2007). furthermore,
   rhetorical structures can be useful for other discourse analysis tasks,
   including co-reference resolution using veins theory (cristea, ide, and
   romary [1356]1998).

   different formal theories of discourse have been proposed from
   different view-points to describe the coherence structure of a text.
   for example, martin ([1357]1992) and knott and dale ([1358]1994)
   propose discourse relations based on the usage of discourse connectives
   (e.g., because, but) in the text. asher and lascarides ([1359]2003)
   propose segmented discourse representation theory, which is driven by
   sentence semantics. webber ([1360]2004) and danlos ([1361]2009) extend
   sentence grammar to formalize discourse structure. rhetorical structure
   theory (rst), proposed by mann and thompson ([1362]1988), is perhaps
   the most influential theory of discourse in computational linguistics.
   although it was initially intended to be used in text generation, later
   it became popular as a framework for parsing the structure of a text
   (taboada and mann [1363]2006). rst represents texts by labeled
   hierarchical structures, called discourse trees (dts). for example,
   consider the dt shown in [1364]figure 1 for the following text:

   but he added:    some people use the purchasers' index as a leading
   indicator, some use it as a coincident indicator. but the thing it's
   supposed to measure   manufacturing strength   it missed altogether last
   month.   
   [1365]figure
   figure   1    discourse tree for two sentences in rst   dt. each sentence
   contains three edus. horizontal lines indicate text segments;
   satellites are connected to their nuclei by curved arrows and two
   nuclei are connected with straight lines.

   the leaves of a dt correspond to contiguous atomic text spans, called
   elementary discourse units (edus; six in the example). edus are
   clause-like units that serve as building blocks. adjacent edus are
   connected by coherence relations (e.g., elaboration, contrast), forming
   larger discourse units (represented by internal nodes), which in turn
   are also subject to this relation linking. discourse units linked by a
   rhetorical relation are further distinguished based on their relative
   importance in the text: nuclei are the core parts of the relation and
   satellites are peripheral or supportive ones. for example, in
   [1366]figure 1, elaboration is a relation between a nucleus (edu 4) and
   a satellite (edu 5), and contrast is a relation between two nuclei
   (edus 2 and 3). carlson, marcu, and okurowski ([1367]2002) constructed
   the first large rst-annotated corpus (rst   dt) on wall street journal
   articles from the id32. whereas mann and thompson ([1368]1988)
   had suggested about 25 relations, the rst   dt uses 53 mono-nuclear and
   25 multi-nuclear relations. the relations are grouped into 16
   coarse-grained categories; see carlson and marcu ([1369]2001) for a
   detailed description of the relations. conventionally, rhetorical
   analysis in rst involves two subtasks: discourse segmentation is the
   task of breaking the text into a sequence of edus, and discourse
   parsing is the task of linking the discourse units (edus and larger
   units) into a labeled tree. in this article, we use the terms discourse
   parsing and rhetorical parsing interchangeably.

   while recent advances in automatic discourse segmentation have attained
   high accuracies (an f-score of 90.5% reported by fisher and roark
   [[1370]2007]), discourse parsing still poses significant challenges
   (feng and hirst [1371]2012) and the performance of the existing
   discourse parsers (soricut and marcu [1372]2003; subba and di-eugenio
   [1373]2009; hernault et al. [1374]2010) is still considerably inferior
   compared with the human gold standard. thus, the impact of rhetorical
   structure in downstream nlp applications is still very limited. the
   work we present in this article aims to reduce this performance gap and
   take discourse parsing one step further. to this end, we address three
   key limitations of existing discourse parsers.

   first, existing discourse parsers typically model the structure and the
   labels of a dt separately, and also do not take into account the
   sequential dependencies between the dt constituents. however, for
   several nlp tasks, it has recently been shown that joint models
   typically outperform independent or pipeline models (murphy [1375]2012,
   page 687). this is also supported in a recent study by feng and hirst
   ([1376]2012), in which the performance of a greedy bottom   up discourse
   parser improved when sequential dependencies were considered by using
   gold annotations for the neighboring (i.e., previous and next)
   discourse units as contextual features in the parsing model. to address
   this limitation of existing parsers, as the first contribution, we
   propose a novel discourse parser based on probabilistic discriminative
   parsing models, expressed as id49 (crfs) (sutton,
   mccallum, and rohanimanesh [1377]2007), to infer the id203 of all
   possible dt constituents. the crf models effectively represent the
   structure and the label of a dt constituent jointly, and, whenever
   possible, capture the sequential dependencies.

   second, existing discourse parsers typically apply greedy and
   sub-optimal parsing algorithms to build a dt. to cope with this
   limitation, we use the inferred (posterior) probabilities from our crf
   parsing models in a probabilistic cky-like bottom   up parsing algorithm
   (jurafsky and martin [1378]2008), which is non-greedy and optimal.
   furthermore, a simple modification of this parsing algorithm allows us
   to generate k-best (i.e., the k highest id203) parse hypotheses
   for each input text that could then be used in a reranker to improve
   over the initial ranking using additional (global) features of the
   discourse tree as evidence, a strategy that has been successfully
   explored in syntactic parsing (charniak and johnson [1379]2005; collins
   and koo [1380]2005).

   third, most of the existing discourse parsers do not discriminate
   between intra-sentential parsing (i.e., building the dts for the
   individual sentences) and multi-sentential parsing (i.e., building the
   dt for the whole document). however, we argue that distinguishing
   between these two parsing conditions can result in more effective
   parsing. two separate parsing models could exploit the fact that
   rhetorical relations are distributed differently intra-sententially
   versus multi-sententially. also, they could independently choose their
   own informative feature sets. as another key contribution of our work,
   we devise two different parsing components: one for intra-sentential
   parsing, the other for multi-sentential parsing. this provides for
   scalable, modular, and flexible solutions that can exploit the strong
   correlation observed between the text structure (i.e., sentence
   boundaries) and the structure of the discourse tree.

   in order to develop a complete and robust discourse parser, we combine
   our intra-sentential and multi-sentential parsing components in two
   different ways. because most sentences have a well-formed discourse
   sub-tree in the full dt (e.g., the second sentence in [1381]figure 1),
   our first approach constructs a dt for every sentence using our
   intra-sentential parser, and then runs the multi-sentential parser on
   the resulting sentence-level dts to build a complete dt for the whole
   document. however, this approach would fail in those cases where
   discourse structures violate sentence boundaries, also called    leaky   
   boundaries (vliet and redeker [1382]2011). for example, consider the
   first sentence in [1383]figure 1. it does not have a well-formed
   discourse sub-tree because the unit containing edus 2 and 3 merges with
   the next sentence and only then is the resulting unit merged with edu
   1. our second approach, in order to deal with these leaky cases, builds
   sentence-level sub-trees by applying the intra-sentential parser on a
   sliding window covering two adjacent sentences and by then
   consolidating the results produced by overlapping windows. after that,
   the multi-sentential parser takes all these sentence-level sub-trees
   and builds a full dt for the whole document.

   our discourse parser assumes that the input text has already been
   segmented into elementary discourse units. as an additional
   contribution, we propose a novel discriminative approach to discourse
   segmentation that not only achieves state-of-the-art performance, but
   also reduces time and space complexities by using fewer features.
   notice that the combination of our segmenter with our parser forms a
   complete probabilistic discriminative framework for rhetorical analysis
   (codra).

   whereas previous systems have been tested on only one corpus, we
   evaluate our framework on texts from two very different genres: news
   articles and instructional how-to manuals. the results demonstrate that
   our approach to discourse parsing provides consistent and statistically
   significant improvements over previous methods both at the sentence
   level and at the document level. the performance of our final system
   compares very favorably to the performance of state-of-the-art
   discourse parsers. finally, the oracle accuracy computed based on the
   k-best parse hypotheses generated by our parser demonstrates that a
   reranker could potentially improve the accuracy further.

   after discussing related work in [1384]section 2, we present our
   rhetorical analysis framework in [1385]section 3. in [1386]section 4,
   we describe our discourse parser. then, in [1387]section 5 we present
   our discourse segmenter. the experiments and analysis of results are
   presented in [1388]section 6. finally, we summarize our contributions
   with future directions in [1389]section 7.
   2.   related work
   section:
   [choose________________________]
   [1390]previous section [1391]next section

   rhetorical analysis has a long history   dating back to mann and thompson
   ([1392]1988), when rst was initially proposed as a useful linguistic
   method for describing natural texts, to more recent attempts to
   automatically extract the rhetorical structure of a given text
   (hernault et al. [1393]2010). in this section, we provide a brief
   overview of the computational approaches that follow rst as the theory
   of discourse, and that are related to our work; see the survey by stede
   ([1394]2011) for a broader overview that also includes other theories
   of discourse.
   2.1   unsupervised and rule-based approaches

   although the most effective approaches to rhetorical analysis to date
   rely on supervised machine learning methods trained on human-annotated
   data, unsupervised methods have also been proposed, as they do not
   require human-annotated data and can be more easily applied to new
   domains.

   often, discourse connectives like but, because, and although convey
   clear information on the kind of relation linking the two text
   segments. in his early work, marcu ([1395]2000a) presented a shallow
   rule-based approach relying on discourse connectives (or cues) and
   surface patterns. he used hand-coded rules, derived from an extensive
   corpus study, to break the text into edus and to build dts for
   sentences first, then for paragraphs, and so on. despite the fact that
   this work pioneered the field of rhetorical analysis, it has many
   limitations. first, identifying discourse connectives is a difficult
   task on its own, because (depending on the usage), the same phrase may
   or may not signal a discourse relation (pitler and nenkova [1396]2009).
   for example, but can either signal a contrast discourse relation or can
   simply perform non-discourse acts. second, discourse segmentation using
   only discourse connectives fails to attain high accuracy (soricut and
   marcu [1397]2003). third, dt structures do not always correspond to
   paragraph structures; for example, sporleder and lapata ([1398]2004)
   report that more than 20% of the paragraphs in the rst   dt corpus
   (carlson, marcu, and okurowski [1399]2002) do not correspond to a
   discourse unit in the dt. fourth, discourse cues are sometimes
   ambiguous; for example, but can signal contrast, antithesis and
   concession, and so on.

   finally, a more serious problem with the rule-based approach is that
   often rhetorical relations are not explicitly signaled by discourse
   cues. for example, in rst   dt, marcu and echihabi ([1400]2002) found
   that only 61 out of 238 contrast relations and 79 out of 307
   cause   explanation relations were explicitly signaled by cue phrases. in
   the british national corpus, sporleder and lascarides ([1401]2008)
   report that half of the sentences lack a discourse cue. other studies
   (schauer and hahn [1402]2001; stede [1403]2004; taboada [1404]2006;
   subba and di-eugenio [1405]2009) report even higher figures: about 60%
   of discourse relations are not explicitly signaled. therefore, rather
   than relying on hand-coded rules based on discourse cues and surface
   patterns, recent approaches use machine learning techniques with a
   large set of informative features.

   while some rhetorical relations need to be explicitly signaled by
   discourse cues (e.g., concession) and some do not (e.g., background),
   there is a large middle ground of relations that may be signaled or
   not. for these    middle ground    relations, can we exploit features
   present in the signaled cases to automatically identify relations when
   they are not explicitly signaled? the idea is to use unambiguous
   discourse cues (e.g., although for contrast, for example for
   elaboration) to automatically label a large corpus with rhetorical
   relations that could then be used to train a supervised model.[1406]^1

   a series of previous studies have explored this idea. marcu and
   echihabi ([1407]2002) first attempted to identify four broad classes of
   relations: contrast, elaboration, condition, and
   cause   explanation   evidence. they used a naive bayes classifier based on
   word pairs (w[1], w[2]), where w[1] occurs in the left segment, and
   w[2]occurs in the right segment. sporleder and lascarides ([1408]2005)
   included other features (e.g., words and their stems, part-of-speech
   [pos] tags, positions, segment lengths) in a boosting-based classifier
   (i.e., boostexter [schapire and singer [1409]2000]) to further improve
   relation classification accuracy. however, these studies evaluated
   classification performance on the instances where rhetorical relations
   were originally signaled (i.e., the discourse cues were artificially
   removed), and did not verify how well this approach performs on the
   instances that are not originally signaled. subsequent studies
   (blair-goldensohn, mckeown, and rambow [1410]2007; sporleder
   [1411]2007; sporleder and lascarides [1412]2008) confirm that
   classifiers trained on instances stripped of their original discourse
   cues do not generalize well to implicit cases because they are
   linguistically quite different.

   note that this approach to identifying discourse relations in the
   absence of manually labeled data does not fully solve the parsing
   problem (i.e., building dts); rather, it only attempts to identify a
   small subset of coarser relations between two (flat) text segments
   (i.e., a tagging problem). arguably, to perform a complete rhetorical
   analysis, one needs to use supervised machine learning techniques based
   on human-annotated data.
   2.2   supervised approaches

   marcu ([1413]1999) applies supervised machine learning techniques to
   build a discourse segmenter and a shift   reduce discourse parser. both
   the segmenter and the parser rely on c4.5 decision tree classifiers
   (poole and mackworth [1414]2010) to learn the rules automatically from
   the data. the discourse segmenter mainly uses discourse cues,
   shallow-syntactic (i.e., pos tags) and contextual features (i.e.,
   neighboring words and their pos tags). to learn the shift   reduce
   actions, the discourse parser encodes five types of features: lexical
   (e.g., discourse cues), shallow-syntactic, textual similarity,
   operational (previous n shift   reduce operations), and rhetorical
   sub-structural features. despite the fact that this work has pioneered
   many of today's machine learning approaches to discourse parsing, it
   has all the limitations mentioned in [1415]section 1.

   the work of marcu ([1416]1999) is considerably improved by soricut and
   marcu ([1417]2003). they present the publicly available spade
   system,[1418]^2 which comes with probabilistic models for discourse
   segmentation and sentence-level discourse parsing. their segmentation
   and parsing models are based on lexico-syntactic patterns (or features)
   extracted from the lexicalized syntactic tree of a sentence. the
   discourse parser uses an optimal parsing algorithm to find the most
   probable dt structure for a sentence. spade was trained and tested on
   the rst   dt corpus. this work, by showing empirically the connection
   between syntax and discourse structure at the sentence level, has
   greatly influenced all major contributions in this area ever since.
   however, it is limited in several ways. first, spade does not produce a
   full-text (i.e., document-level) parse. second, it applies a generative
   parsing model based on only lexico-syntactic features, whereas
   discriminative models are generally considered to be more accurate, and
   can incorporate arbitrary features more effectively (murphy
   [1419]2012). third, the parsing model makes an independence assumption
   between the label and the structure of a dt constituent, and it ignores
   the sequential and the hierarchical dependencies between the dt
   constituents.

   subsequent research addresses the question of how much syntax one
   really needs in rhetorical analysis. sporleder and lapata ([1420]2005)
   focus on the discourse chunking problem, comprising two subtasks:
   discourse segmentation and (flat) nuclearity assignment. they formulate
   discourse chunking in two alternative ways. first, one-step
   classification, where the discourse chunker, a multi-class classifier,
   assigns to each token one of the four labels: (1) b   nuc (beginning of a
   nucleus), (2) i   nuc (inside a nucleus), (3) b    sat (beginning of a
   satellite), and (4) i   sat (inside a satellite). therefore, this
   approach performs discourse segmentation and nuclearity assignment
   simultaneously. second, two-step classification, where in the first
   step, the discourse segmenter (a binary classifier) labels each token
   as either b (beginning of an edu) or i (inside an edu). then, in the
   second step, a nuclearity labeler (another binary classifier) assigns a
   nuclearity status to each segment. the two-step approach avoids illegal
   chunk sequences like a b   nuc followed by an i   sat or a b   sat followed
   by an i   nuc, and in this approach, it is easier to incorporate
   sentence-level properties like the constraint that a sentence must
   contain at least one nucleus. they examine whether shallow-syntactic
   features (e.g., pos and phrase tags) would be sufficient for these
   purposes. the evaluation on the rst   dt shows that the two-step approach
   outperforms the one-step approach, and its performance is comparable to
   that of spade, which requires relatively expensive full syntactic
   parses.

   in follow   up work, fisher and roark ([1421]2007) demonstrate over 4%
   absolute performance gain in discourse segmentation, by combining the
   features extracted from the syntactic tree with the ones derived via
   id52 and shallow syntactic parsing (i.e., chunking). using quite
   a large number of features in a binary log-linear model, they achieve
   state-of-the-art performance in discourse segmentation on the rst   dt
   test set.

   in a different approach, regneri, egg, and koller ([1422]2008) propose
   to use underspecified discourse representation (udr) as an intermediate
   representation for discourse parsing. underspecified representations
   offer a single compact representation to express possible ambiguities
   in a linguistic structure, and have been primarily used to deal with
   scope ambiguity in semantic structures (reyle [1423]1993; egg, koller,
   and niehren [1424]2001; althaus et al. [1425]2003; koller, regneri, and
   thater [1426]2008). assuming that a udr of a dt is already given in the
   form of a dominance graph (althaus et al. [1427]2003), regneri, egg,
   and koller ([1428]2008) convert it into a more expressive and complete
   udr representation called regular tree grammar (koller, regneri, and
   thater [1429]2008), for which efficient algorithms (knight and graehl
   [1430]2005) already exist to derive the best configuration (i.e., the
   best discourse tree).

   hernault et al. ([1431]2010) present the publicly available hilda
   system,[1432]^3 which comes with a discourse segmenter and a parser
   based on support vector machines (id166s). the discourse segmenter is a
   binary id166 classifier that uses the same lexico-syntactic features used
   in spade, but with more context (i.e., the lexico-syntactic features
   for the previous two words and the following two words). the discourse
   parser iteratively uses two id166 classifiers in a pipeline to build a
   dt. in each iteration, a binary classifier first decides which of the
   adjacent units to merge, then a multi-class classifier connects the
   selected units with an appropriate relation label. using this simple
   method, they report promising results in document-level discourse
   parsing on the rst   dt.

   for a different genre, instructional texts, subba and di-eugenio
   ([1433]2009) propose a shift   reduce discourse parser that relies on a
   classifier for relation labeling. their classifier uses inductive logic
   programming (ilp) to learn id85 rules from a large set of
   features including the linguistically rich id152
   coming from a semantic parser. they demonstrate that including
   id152 with other features improves the performance of
   the classifier, thus, also improves the performance of the parser.

   both hilda and the ilp-based approach of subba and di-eugenio
   ([1434]2009) are limited in several ways. first, they do not
   differentiate between intra- and multi-sentential parsing, and both
   scenarios use a single uniform parsing model. second, they take a
   greedy (i.e., sub-optimal) approach to construct a dt. third, they
   disregard sequential dependencies between dt constituents. furthermore,
   hilda considers the structure and the labels of a dt separately. our
   discourse parser codra, as described in the next section, addresses all
   these limitations.

   more recent work than ours also attempts to address some of the
   above-mentioned limitations of the existing discourse parsers. similar
   to us, feng and hirst ([1435]2014) generate a document-level dt in two
   stages, where a multi-sentential parsing follows an intra-sentential
   one. at each stage, they iteratively use two separate linear-chain crfs
   (lafferty, mccallum, and pereira [1436]2001) in a cascade: one for
   predicting the presence of rhetorical relations between adjacent
   discourse units in a sequence, and the other to predict the relation
   label between the two most probable adjacent units to be merged as
   selected by the previous crf. while they use crfs to take into account
   the sequential dependencies between dt constituents, they use them
   greedily during parsing to achieve efficiency. they also propose a
   greedy post-editing step based on an additional feature (i.e., depth of
   a discourse unit) to modify the initial dt, which gives them a
   significant gain in performance. in a different approach, li et al.
   ([1437]2014) propose a discourse-level dependency structure to capture
   direct relationships between edus rather than deep hierarchical
   relationships. they first create a discourse dependency treebank by
   converting the deep annotations in rst   dt to shallow head-dependent
   annotations between edus. to find the dependency parse (i.e., an
   optimal spanning tree) for a given text, they apply eisner ([1438]1996)
   and maximum spanning tree (mcdonald et al. [1439]2005) dependency
   parsing algorithms with the margin infused relaxed algorithm online
   learning framework (mcdonald, crammer, and pereira [1440]2005).

   with the successful application of deep learning to numerous nlp
   problems including syntactic parsing (socher et al. [1441]2013a),
   id31 (socher et al. [1442]2013b), and various tagging
   tasks (collobert et al. [1443]2011), a couple of recent studies in
   discourse parsing also use deep neural networks (dnns) and related
   feature representation methods. inspired by the work of socher et al.
   ([1444]2013a, [1445]2013b), li, li, and hovy ([1446]2014) propose a
   recursive dnn for discourse parsing. however, as in socher et al.
   ([1447]2013a, [1448]2013b), word vectors (i.e., embeddings) are not
   learned explicitly for the task, rather they are taken from collobert
   et al. ([1449]2011). given the vectors of the words in an edu, their
   model first composes them hierarchically based on a syntactic parse
   tree to get the vector representation for the edu. adjacent discourse
   units are then merged hierarchically to get the vector representations
   for the higher order discourse units. in every step, the merging is
   done using one binary (structure) and one multi-class (relation)
   classifier, each having a three-layer neural network architecture. the
   cost function for training the model is given by these two cascaded
   classifiers applied at different levels of the dt. similar to our
   method, they use the classifier probabilities in a cky-like parsing
   algorithm to find the global optimal dt. finally, ji and eisenstein
   ([1450]2014) present a feature representation learning method in a
   shift   reduce discourse parser (marcu [1451]1999). unlike dnns, which
   learn non-linear feature transformations in a maximum likelihood model,
   they learn linear transformations of features in a max margin
   classification model.
   3.   overview of our rhetorical analysis framework
   section:
   [choose___________________________]
   [1452]previous section [1453]next section

   codra takes as input a raw text and produces a discourse tree that
   describes the text in terms of coherence relations that hold between
   adjacent discourse units (i.e., clauses, sentences) in the text. an
   example dt generated by an online demo of codra is shown in
   [1454]appendix a.[1455]^4 the color of a node represents its nuclearity
   status: blue denoting nucleus and yellow denoting satellite. the demo
   also allows some useful user interactions   for example, collapsing or
   expanding a node, highlighting an edu, and so on.[1456]^5

   codra follows a pipeline architecture, shown in [1457]figure 2. given a
   raw text, the first task in the rhetorical analysis pipeline is to
   break the text into a sequence of edus (i.e., discourse segmentation).
   because it is taken for granted that sentence boundaries are also edu
   boundaries (i.e., edus do not span across multiple sentences), the
   discourse segmentation task boils down to finding edu boundaries inside
   sentences. codra uses a maximum id178 model for discourse
   segmentation (see [1458]section 5).
   [1459]figure
   figure   2    codra architecture.

   once the edus are identified, the discourse parsing problem is
   determining which discourse units (edus or larger units) to relate
   (i.e., the structure), and what relations (i.e., the labels) to use in
   the process of building the dt. specifically, discourse parsing
   requires: (1) a parsing model to explore the search space of possible
   structures and labels for their nodes, and (2) a parsing algorithm for
   selecting the best parse tree(s) among the candidates. a probabilistic
   parsing model like ours assigns a id203 to every possible dt. the
   parsing algorithm then picks the most probable dts.

   the existing discourse parsers (marcu [1460]1999; soricut and marcu
   [1461]2003; subba and di-eugenio [1462]2009; hernault et al.
   [1463]2010) described in [1464]section 2 use parsing models that
   disregard the structural interdependencies between the dt constituents.
   however, we hypothesize that, like syntactic parsing, discourse parsing
   is also a id170 problem, which involves predicting
   multiple variables (i.e., the structure and the relation labels) that
   depend on each other (smith [1465]2011). recently, feng and hirst
   ([1466]2012) also found these interdependencies to be critical for
   parsing performance. to capture the structural dependencies between the
   dt constituents, codra uses undirected conditional id114
   (i.e., crfs) as its parsing models.

   to find the most probable dt, unlike most previous studies (marcu
   [1467]1999; subba and di-eugenio [1468]2009; hernault et al.
   [1469]2010), which adopt a greedy solution, codra applies an optimal
   cky parsing algorithm to the inferred posterior probabilities (obtained
   from the crfs) of all possible dt constituents. furthermore, the
   parsing algorithm allows codra to generate a list of k-best parse
   hypotheses for a given text.

   note that the way crfs and cky are used in codra is quite different
   from the way they are used in syntactic parsing. for example, in the
   crf-based constituency parsing proposed by finkel, kleeman, and manning
   ([1470]2008), the id155 distribution of a parse tree
   given a sentence decomposes across factors defined over productions,
   and the standard inside   outside algorithm is used for id136 on
   possible trees. in contrast, codra first uses the standard
   forward   backward algorithm in a    fat    chain structured[1471]^6 crf (to
   be discussed in [1472]section 4.1.1) to compute the posterior
   probabilities of all possible dt constituents for a given text (i.e.,
   edus); then it uses a cky parsing algorithm to combine those
   probabilities and find the most probable dt.

   another crucial question related to parsing models is whether to use a
   single model or two different models for parsing at the sentence-level
   (i.e., intra-sentential) and at the document-level (i.e.,
   multi-sentential). a simple and straightforward strategy would be to
   use a single unified parsing model for both intra- and multi-sentential
   parsing without distinguishing the two cases, as was previously done
   (marcu [1473]1999; subba and di-eugenio [1474]2009; hernault et al.
   [1475]2010). that approach has the advantages of making the parsing
   process easier, and the model gets more data to learn from. however,
   for a solution like ours, which tries to capture the interdependencies
   between constituents, this would be problematic with respect to
   scalability and inappropriate because of two modeling issues.

   more specifically, for scalability note that the number of valid trees
   grows exponentially with the number of edus in a document.[1476]^7
   therefore, an exhaustive search over all the valid dts is often
   infeasible, even for relatively small documents.

   for modeling, a single unified approach is inappropriate for two
   reasons. on the one hand, it appears that discourse relations are
   distributed differently intra- versus multi-sententially. for example,
   [1477]figure 3 shows a comparison between the two distributions of the
   eight most frequent relations in the rst   dt training set. notice that
   same   unit is more frequent than joint in the intra-sentential case,
   whereas joint is more frequent than same   unit in the multi-sentential
   case. similarly, the relative distributions of background, contrast,
   cause, and explanation are different in the two parsing scenarios. on
   the other hand, different kinds of features are applicable and
   informative for intra- versus multi-sentential parsing. for example,
   syntactic features like dominance sets (soricut and marcu [1478]2003)
   are extremely useful for parsing at the sentence-level, but are not
   even applicable in the multi-sentential case. likewise, lexical chain
   features (sporleder and lapata [1479]2004), which are useful for
   multi-sentential parsing, are not applicable at the sentence level.
   [1480]figure
   figure   3    distributions of the eight most frequent relations in
   intra-sentential and multi-sentential parsing scenarios on the rst   dt
   training set.

   based on these above observations, codra comprises two separate
   modules: an intra-sentential parser and a multi-sentential parser, as
   shown in [1481]figure 2. first, the intra-sentential parser produces
   one or more discourse sub-trees for each sentence. then, the
   multi-sentential parser generates a full dt for the document from these
   sub-trees. both of our parsers have the same two components: a parsing
   model and a parsing algorithm. whereas the two parsing models are
   rather different, the same parsing algorithm is shared by the two
   modules. staging multi-sentential parsing on top of intra-sentential
   parsing in this way allows codra to explicitly exploit the strong
   correlation observed between the text structure and the dt structure,
   as explained in detail in [1482]section 4.3.
   4.   the discourse parser
   section:
   [choose________________________]
   [1483]previous section [1484]next section

   before describing the parsing models and the parsing algorithm of codra
   in detail, we introduce some terminology that we will use throughout
   this article.

   a dt can be formally represented as a set of constituents of the form
   r[i, m, j], where i     m < j. this refers to a rhetorical relation r
   between the discourse unit containing edus i through m and the
   discourse unit containing edus m+1 through j. for example, the dt for
   the second sentence in [1485]figure 1 can be represented as
   {elaboration   ns[4,4,5], same   unit   nn[4,5,6]}. notice that in this
   representation, a relation r also specifies the nuclearity status of
   the discourse units involved, which can be one of nucleus   satellite
   (ns), satellite   nucleus (sn), or nucleus   nucleus (nn). attaching
   nuclearity status to the relations allows us to perform the two
   subtasks of discourse parsing, relation identification and nuclearity
   assignment, simultaneously.

   a common assumption made for generating dts effectively is that they
   are binary trees (soricut and marcu [1486]2003; hernault et al.
   [1487]2010). that is, multi-nuclear relations (e.g., joint, same   unit)
   involving more than two discourse units are mapped to a hierarchical
   right-branching binary tree. for example, a flat joint(e[1], e[2],
   e[3], e[4]) ([1488]figure 4a) is mapped to a right-branching binary
   tree joint(e[1], joint(e[2], joint(e[3], e[4]))) ([1489]figure 4b).
   [1490]figure
   figure   4    multi-nuclear relation and its corresponding binary tree
   representation.
   4.1   parsing models

   as mentioned before, the job of the intra- and multi-sentential parsing
   models of codra is to assign a id203 to each of the constituents
   of all possible dts at the sentence level and at the document level,
   respectively. formally, given the model parameters    at a particular
   parsing scenario (i.e., sentence-level or document-level), for each
   possible constituent r[i, m, j] in a candidate dt at that parsing
   scenario, the parsing model estimates p(r[i, m, j]|  ), which specifies
   a joint distribution over the label r and the structure [i, m, j] of
   the constituent. for example, when applied to the sentences in
   [1491]figure 1 separately, the intra-sentential parsing model (with
   learned parameters   [s]) estimates p(r[1, 1, 2]|  [s]), p(r[2, 2,
   3]|  [s]), p(r[1, 2, 3]|  [s]), and p(r[1, 1, 3]|  [s]) for the first
   sentence, and p(r[4, 4, 5]|  [s]), p(r[5, 5, 6]|  [s]), p(r[4, 5,
   6]|  [s]), and p(r[4, 4, 6]|  [s]) for the second sentence, respectively,
   for all r ranging over the set of relations.
   4.1.1   intra-sentential parsing model

   [1492]figure 5 shows the parsing model of codra for intra-sentential
   parsing. the observed nodes u[j] (at the bottom) in a sequence
   represent the discourse units (edus or larger units). the first layer
   of hidden nodes are the structure nodes, where s[j]     {0, 1} denotes
   whether two adjacent discourse units u[j   1] and u[j] should be
   connected or not. the second layer of hidden nodes are the relation
   nodes, with r[j]     {1     m} denoting the relation between two adjacent
   units u[j   1] and u[j], where m is the total number of relations in the
   relation set. the connections between adjacent nodes in a hidden layer
   encode sequential dependencies between the respective hidden nodes, and
   can enforce constraints such as the fact that a node must have a unique
   mother, namely, a s[j] = 1 must not follow a s[j   1] = 1. the
   connections between the two hidden layers model the structure and the
   relation of dt constituents jointly.
   [1493]figure
   figure   5    the intra-sentential parsing model of codra.

   notice that the probabilistic graphical model shown in [1494]figure 5
   is a chain-structured undirected graphical model (also known as markov
   random field or mrf [murphy [1495]2012]) with two hidden layers, i.e.,
   structure chain and relation chain. it becomes a dynamic conditional
   random field (dcrf) (sutton, mccallum, and rohanimanesh [1496]2007)
   when we directly model the hidden (output) variables by conditioning
   the clique potentials (i.e., factors) on the observed (input)
   variables:

   where {  } and {  } are the factors over the edges of the relation and
   structure chains, respectively, and {  } are the factors over the edges
   connecting the relation and structure nodes (i.e., between-chain
   edges). here, x represents input features extracted from the observed
   variables,   [s] = [  [s,r],   [s,s],   [s,c]] are model parameters, and
   z(x,   [s]) is the partition function. we use the standard log-linear
   representation of the factors:

   where f (y, z, x) is a feature vector derived from the input features x
   and the local labels y and z, and   [s,y] is the corresponding weight
   vector   that is,   [s,r] and   [s,s] are the weight vectors for the
   factors over the relation edges and the structure edges, respectively,
   and   [s,c] is the weight vector for the factors over the between-chain
   edges.

   a dcrf is a generalization of linear-chain crfs (lafferty, mccallum,
   and pereira [1497]2001) to represent complex interactions between
   output variables (i.e., labels), such as when performing multiple
   labeling tasks on the same sequence. recently, there has been an
   explosion of interest in crfs for solving structured output
   classification problems, with many successful applications in nlp
   including syntactic parsing (finkel, kleeman, and manning [1498]2008),
   syntactic chunking (sha and pereira [1499]2003), and discourse chunking
   (ghosh et al. [1500]2011) in accordance with the penn discourse
   treebank (prasad et al. [1501]2008).

   dcrfs, being a discriminative approach to sequence modeling, have
   several advantages over their generative counterparts such as hidden
   markov models (id48s) and mrfs, which first model the joint distribution
   p(y, x|  ), and then infer the conditional distribution p(y|x,   ). it
   has been advocated that discriminative models are generally more
   accurate than generative ones because they do not    waste resources   
   modeling complex distributions that are observed (i.e., p(x)); instead,
   they focus directly on modeling what we care about, namely, the
   distribution of labels given the data (murphy [1502]2012).

   other key advantages include the ability to incorporate arbitrary
   overlapping local and global features, and the ability to relax strong
   independence assumptions. furthermore, crfs surmount the label bias
   problem (lafferty, mccallum, and pereira [1503]2001) of the maximum
   id178 markov model (mccallum, freitag, and pereira [1504]2000), which
   is considered to be a discriminative version of the id48.
   4.1.2   training and applying the intra-sentential parsing model

   in order to obtain the id203 of the constituents of all candidate
   dts for a sentence, codra applies the intra-sentential parsing model
   (with learned parameters   [s]) recursively to sequences at different
   levels of the dt, and computes the posterior marginals over the
   relation    structure pairs. it uses the standard forward   backward
   algorithm to compute the posterior marginals. to illustrate the
   process, let us assume that the sentence contains four edus, e[1],     ,
   e[4] (see [1505]figure 6). at the first (i.e., bottom) level of the dt,
   when all the discourse units are edus, there is only one unit sequence
   (e[1], e[2], e[3], e[4]) to which codra applies the dcrf model.
   [1506]figure 6a at the top left shows the corresponding dcrf model. for
   this sequence it computes the posterior marginals p(r[2], s[2] =
   1|e[1], e[2], e[3], e[4],   [s]), p(r[3], s[3] = 1|e[1], e[2], e[3],
   e[4],   [s]), and p(r[4], s[4] = 1| e[1], e[2], e[3], e[4],   [s]) to
   obtain the id203 of the dt constituents r[1, 1, 2], r[2, 2, 3],
   and r[3, 3, 4], respectively.
   [1507]figure
   figure   6    the intra-sentential parsing model is applied to (a) the only
   possible sequence at the first level, (b) the three possible sequences
   at the second level, and (c) the three possible sequences at the third
   level.

   at the second level, there are three unit sequences: (e[1:2], e[3],
   e[4]), (e[1],e[2:3], e[4]), and (e[1],e[2],e[3:4]). [1508]figure 6b
   shows their corresponding dcrf models. notice that each of these
   sequences has a discourse unit that connects two edus, and the
   id203 of this connection has already been computed at the
   previous level. codra computes the posterior marginals p(r[3], s[3] =
   1|e[1:2], e[3], e[4],   [s]), p(r[2:3] s[2:3] = 1|e[1], e[2:3], e[4],
     [s]), p(r[4], s[4] = 1|e[1], e[2:3], e[4],   [s]), and p(r[3:4], s[3:4]
   = 1|e[1], e[2], e[3:4],   [s]) from these three sequences, which
   correspond to the id203 of the constituents r[1, 2, 3], r[1, 1,
   3], r[2, 3, 4], and r[2, 2, 4], respectively. similarly, it attains the
   id203 of the constituents r[1, 1, 4], r[1, 2, 4], and r[1, 3, 4]
   by computing their respective posterior marginals from the three
   sequences at the third (i.e., top) level of the candidate dts (see
   [1509]figure 6c).

   algorithm 1 describes how codra generates the unit sequences at
   different levels of the candidate dts for a given number of edus in a
   sentence. specifically, to compute the id203 of a dt constituent
   r[i, k, j], codra generates sequences like (e[1],     , e[i   1], e[i:k],
   e[k+1:j], e[j+1],     , e[n]) for 1     i     k < j     n. however, in doing
   so, it may generate some duplicate sequences. clearly, the sequence
   (e[1],     , e[i   1], e[i:i], e[i+1:j], e[j+1],     , e[n]) for 1     i     k <
   j < n is already considered for computing the id203 of the
   constituent r[i + 1, j, j + 1]. therefore, it is a duplicate sequence
   that codra excludes from the list of sequences. the algorithm has a
   complexity of o(n^3), where n is the number of edus in the sentence.

   once codra acquires the id203 of all possible intra-sentential dt
   constituents, the discourse sub-trees for the sentences are built by
   applying an optimal parsing algorithm ([1510]section 4.2) using one of
   the methods described in [1511]section 4.3.

   algorithm 1 is also used to generate sequences for training the model
   (i.e., learning   [s]). for example, [1512]figure 7 demonstrates how we
   generate the training instances (right) from a gold dt with four edus
   (left). to find the relevant labels for the sequences generated by the
   algorithm, we consult the gold dt and see if two discourse units are
   connected by a relation r (i.e., the corresponding labels are s = 1, r
   = r) or not (i.e., the corresponding labels are s = 0, r =nr). we train
   the model by maximizing the conditional likelihood of the labels in
   each of these training examples (see [1513]equation (1)).
   [1514]figure
   figure   7    a gold discourse tree (left), and the 7 training instances it
   generates (right). nr = no relation.
   4.1.3   multi-sentential parsing model

   given the discourse units (sub-trees) for all the individual sentences
   in a document, a simple approach to build the dt of the document would
   be to apply a new dcrf model, similar to the one in [1515]figure 5
   (with different parameters), to all the possible sequences generated
   from these units by algorithm 1 to infer the id203 of all
   possible higher-order (multi-sentential) constituents. however, the
   number of possible sequences and their length increase with the number
   of sentences in a document. for example, assuming that each sentence
   has a well-formed dt, for a document with n sentences, algorithm 1
   generates o(n^3) sequences, where the sequence at the bottom level has
   n units, each of the sequences at the second level has n-1 units, and
   so on. because the dcrf model in [1516]figure 5 has a    fat    chain
   structure, one could use the forward   backward algorithm for exact
   id136 in this model (murphy [1517]2012). forward   backward on a
   sequence containing t units costs o(tm^2) time, where m is the number
   of relations in our relation set. this makes the chain-structured dcrf
   model impractical for multi-sentential parsing of long documents, since
   learning requires running id136 on every training sequence with an
   overall time complexity of o(tm^2 n^3) = o(m^2 n^4) per document
   (sutton and mccallum [1518]2012).

   to address this problem, we have developed a simplified parsing model
   for multi-sentential parsing. our model is shown in [1519]figure 8. the
   two observed nodes u[t   1] and u[t] are two adjacent (multi-sentential)
   discourse units. the (hidden) structure node s     {0, 1} denotes whether
   the two discourse units should be linked or not. the other hidden node
   r     {1     m} represents the relation between the two units. notice that
   similar to the model in [1520]figure 5, this is also an undirected
   graphical model and becomes a crf model if we directly model the labels
   by conditioning the clique potential    on the input features x, derived
   from the observed variables:

   where f (r[t], s[t], x) is a feature vector derived from the input
   features x and the labels r[t] and s[t], and   [d] is the corresponding
   weight vector. although this model is similar in spirit to the parsing
   model in [1521]figure 5, it now breaks the chain structure, which makes
   the id136 much faster (i.e., a complexity of o(m^2)). breaking the
   chain structure also allows codra to balance the data for training (an
   equal number of instances with s=1 and s=0), which dramatically reduces
   the learning time of the model.
   [1522]figure
   figure   8    the multi-sentential parsing model of codra

   codra applies this parsing model to all possible adjacent units at all
   levels in the multi-sentential case, and computes the posterior
   marginals of the relation   structure pairs p(r[t], s[t] = 1|u[t   1],
   u[t],   [d]) using the forward   backward algorithm to obtain the
   id203 of all possible dt constituents. given the sentence-level
   discourse units, algorithm 2, which is a simplified variation of
   algorithm 1, extracts all possible adjacent discourse units for
   multi-sentential parsing. similar to algorithm 1, algorithm 2 also has
   a complexity of o(n^3), where n is the number of sentence-level
   discourse units.

   both our intra- and multi-sentential parsing models are designed using
   mallet's graphical model toolkit grmm (mccallum [1523]2002). in order
   to avoid overfitting, we regularize the crf models with l[2]
   id173 and learn the model parameters using the limited-memory
   bfgs (l-bfgs) fitting algorithm.
   4.1.4   features used in the parsing models

   crucial to parsing performance is the set of features used in the
   parsing models, as summarized in [1524]table 1. we categorize the
   features into seven groups and specify which groups are used in what
   parsing model. notice that some of the features are used in both
   models. most of the features have been explored in previous studies
   (e.g., soricut and marcu [1525]2003; sporleder and lapata [1526]2005;
   hernault et al. [1527]2010). however, we improve some of these as
   explained subsequently.

   [1528]table
   table   1    features used in our intra- and multi-sentential parsing
   models.

   the features are extracted from two adjacent discourse units u[t   1] and
   u[t]. organizational features encode useful information about text
   organization as shown by duverle and prendinger ([1529]2009). we
   measure the length of the discourse units as the number of edus and
   tokens in it. however, in order to better adjust to the length
   variations, rather than computing their absolute numbers in a unit, we
   choose to measure their relative numbers with respect to their total
   numbers in the two units. for example, if the two discourse units under
   consideration contain three edus in total, a unit containing two of the
   edus will have a relative edu number of 0.67. we also measure the
   distances of the units in terms of the number of edus from the
   beginning and end of the sentence (or text in the multi-sentential
   case). text structural features capture the correlation between text
   structure and rhetorical structure by counting the number of sentence
   and paragraph boundaries in the discourse units.

   discourse cues (e.g., because, but), when present, signal rhetorical
   relations between two text segments, and have been used as a primary
   source of information in earlier studies (knott and dale [1530]1994;
   marcu [1531]2000a). however, recent studies (hernault et al.
   [1532]2010; biran and rambow [1533]2011) suggest that an empirically
   acquired lexical id165 dictionary is more effective than a fixed list
   of cue phrases, since this approach is domain independent and capable
   of capturing non-lexical cues such as punctuation.

   in order to build a lexical id165 dictionary empirically from the
   training corpus, we extract the first and last n tokens (n   {1, 2, 3})
   of each discourse unit and rank them according to their mutual
   information with the two labels, structure (s) and relation (r). more
   specifically, given an id165 x, we compute its conditional id178 h
   with respect to s and r as follows:[1534]^8

   where c(x) is the empirical count of id165 x, and c(x, s, r) is the
   joint empirical count of id165 x with the labels s and r. this is in
   contrast to hilda (hernault et al. [1535]2010), which ranks the id165s
   by their frequencies in the training corpus. however, blitzer
   ([1536]2008) found mutual information to be more effective than
   frequency as a method for feature selection. intuitively, the most
   informative discourse cues are not only the most frequent, but also the
   ones that are indicative of the labels in the training data. in
   addition to the lexical id165s we also encode the pos tags of the
   first and last n tokens (n   {l, 2, 3}) in a discourse unit as
   shallow-syntactic features in our models.

   lexico-syntactic features dominance sets extracted from the discourse
   segmented lexicalized syntactic tree (ds-lst) of a sentence have been
   shown to be extremely effective for intra-sentential discourse parsing
   in spade (soricut and marcu [1537]2003). [1538]figure 9a shows the
   ds-lst (i.e., lexicalized syntactic tree with edus identified) for a
   sentence with three edus from the rst   dt corpus, and [1539]figure 9b
   shows the corresponding discourse tree. in a ds-lst, each edu except
   the one with the root node must have a head node n[h] that is attached
   to an attachment node n[a] residing in a separate edu. a dominance set
   d (shown at the bottom of [1540]figure 9a) contains these attachment
   points (shown in boxes) of the edus in a ds-lst. in addition to the
   syntactic and lexical information of the head and attachment nodes,
   each element in the dominance set also includes a dominance
   relationship between the edus involved; the edu with the attachment
   node dominates (represented by    >   ) the edu with the head node.
   [1541]figure
   figure   9    dominance set features for intra-sentential discourse
   parsing.

   soricut and marcu ([1542]2003) hypothesize that the dominance set
   (i.e., lexical heads, syntactic labels, and dominance relationships)
   carries the most informative clues for intra-sentential parsing. for
   instance, the dominance relationship between the edus in our example
   sentence is 3 > 1 > 2, which favors the dt structure [1, 1, 2] over [2,
   2, 3]. in order to extract dominance set features for two adjacent
   discourse units u[t   1] and u[t], containing edus e[i:j] and e[j+1:k],
   respectively, we first compute the dominance set from the ds-lst of the
   sentence. we then extract the element from the set that holds across
   the edus j and j + 1. in our example, for the two units, containing
   edus e[1] and e[2], respectively, the relevant dominance set element is
   (1, efforts/np)>(2, to/s). we encode the syntactic labels and lexical
   heads of n[h] and n[a], and the dominance relationship as features in
   our intra-sentential parsing model.

   lexical chains (morris and hirst [1543]1991) are sequences of
   semantically related words that can indicate topical boundaries in a
   text (galley et al. [1544]2003; joty, carenini, and ng [1545]2013).
   features extracted from lexical chains are also shown to be useful for
   finding paragraph-level discourse structure (sporleder and lapata
   [1546]2004). for example, consider the text with four paragraphs (p[1]
   to p[4]) in [1547]figure 10a. now, let us assume that there is a
   lexical chain that spans the whole text, skipping paragraphs p[2] and
   p[3], while a second chain only spans p[2] and p[3]. this situation
   makes it more likely that p[2] and p[3] should be linked in the dt
   before either of them is linked with another paragraph. therefore, the
   dt structure in [1548]figure 10b should be more likely than the
   structure in [1549]figure 10c.
   [1550]figure
   figure   10    correlation between lexical chains and discourse structure.
   (a) lexical chains spanning paragraphs. (b) and (c) two possible dt
   structures.

   one challenge in computing lexical chains is that words can have
   multiple senses, and semantic relationships depend on the sense rather
   than the word itself. several methods have been proposed to compute
   lexical chains (barzilay and elhadad [1551]1997; hirst and st. onge
   [1552]1997; silber and mccoy [1553]2002; galley and mckeown
   [1554]2003). we follow the state-of-the-art approach proposed by galley
   and mckeown ([1555]2003), which extracts lexical chains after
   performing id51 (wsd).

   in the preprocessing step, we extract the nouns from the document and
   lemmatize them using id138's built-in morphy function (fellbaum
   [1556]1998). then, by looking up in id138 we expand each noun to all
   of its senses, and build a lexical semantic relatedness graph (lsrg)
   (galley and mckeown [1557]2003; chali and joty [1558]2007). in an lsrg,
   the nodes represent noun-tokens with their candidate senses, and the
   weighted edges between senses of two different tokens represent one of
   the three semantic relations: repetition, synonym, and hypernym. for
   example, [1559]figure 11a shows a partial lsrg, where the token bank
   has two possible senses, namely, money bank and river bank. using the
   money bank sense, bank is connected with institution and company by
   hypernymy relations (edges marked with h), and with another bank by a
   repetition relation (edges marked with r). similarly, using the river
   bank sense, it is connected with riverside by a hypernymy relation and
   with bank by a repetition relation. nouns that are not found in id138
   are considered as proper nouns having only one sense, and are connected
   by only repetition relations.
   [1560]figure
   figure   11    extracting lexical chains. (a) a lexical semantic
   relatedness graph (lsrg) for five noun-tokens. (b) resultant graph
   after performing wsd. the box at the bottom shows the lexical chains.

   we use this lsrg first to perform wsd, then to construct lexical
   chains. for wsd, the weights of all edges leaving the nodes under their
   different senses are summed up and the one with the highest score is
   considered to be the right sense for the word-token. for example, if
   repetition and synonymy are weighted equally, and hypernymy is given
   half as much weight as either of them, the score of bank's two senses
   are: 1 + 0.5 + 0.5 = 2 for the sense money bank and 1 + 0.5 = 1.5 for
   the sense river bank. therefore, the selected sense for bank in this
   context is river bank. in case of a tie, we select the sense that is
   most frequent (i.e., the first sense in id138). note that this
   approach to wsd is different from that of sporleder and lapata
   ([1561]2004), which takes a greedy approach.

   finally, we prune the graph by only keeping the links that connect
   words with the selected senses. at the end of the process, we are left
   with the edges that form the actual lexical chains. for example,
   [1562]figure 11b shows the result of pruning the graph in [1563]figure
   11a. the lexical chains extracted from the pruned graph are shown in
   the box at the bottom. following sporleder and lapata ([1564]2004), for
   each chain element, we keep track of the location (i.e., sentence id)
   in the text where that element was found, and exclude chains containing
   only one element. given two discourse units, we count the number of
   chains that: hit the two units, exclusively hit the two units, skip
   both units, skip one of the units, start in a unit, and end in a unit.

   we also consider more contextual information by including the above
   features computed for the neighboring adjacent discourse unit pairs in
   the current feature vector. for example, the contextual features for
   units u[t   1] and u[t] include the feature vector computed from u[t   2]
   and u[t   1] and the feature vector computed from u[t] and u[t+1].

   we incorporate hierarchical dependencies between the constituents in a
   dt by rhetorical sub-structural features. for two adjacent units u[t   1]
   and u[t], we extract the roots of the two rhetorical sub-trees. for
   example, the root of the rhetorical sub-tree spanning over edus e[1:2]
   in [1565]figure 9b is elaboration   ns. however, extraction of these
   features assumes the presence of labels for the sub-trees, which is not
   the case when we apply the parser to a new text (sentence or document)
   in order to build its dt in a non-greedy fashion. one way to deal with
   this is to loop twice through the parsing process using two different
   parsing models   one trained with the complete feature set, and the other
   trained without the sub-structural features. we first build an initial,
   sub-optimal dt using the parsing model that is trained without the
   sub-structural features. this intermediate dt will now provide labels
   for the sub-structures. next we can build a final, more accurate dt by
   using the complete parsing model. this idea of two-pass discourse
   parsing, where the second pass performs post-editing using additional
   features, has recently been adopted by feng and hirst ([1566]2014) in
   their greedy parser.

   one could even continue doing post-editing multiple times until the dt
   converges. however, this could be very time consuming as each
   post-editing pass requires: (1) applying the parsing model to every
   possible unit sequence and computing the posterior marginals for all
   possible dt constituents, and (2) using the parsing algorithm to find
   the most probable dt. recall from our earlier discussion in
   [1567]section 4.1.3 that for n discourse units and m rhetorical
   relations, the first step requires o(m^2 n^4) and o(m^2 n^3) for intra-
   and multi-sentential parsing, respectively; we will see in the next
   section that the second step requires o(mn^3). in spite of the
   computational cost, the gain we attained in the subsequent passes was
   not significant for our development set. therefore, we restrict our
   parser to only one-pass post-editing.

   note that in parsing models where the score (i.e., likelihood) of a
   parse tree decomposes across local factors (e.g., the crf-based
   syntactic parser of finkel, kleeman, and manning [[1568]2008]), it is
   possible to define a semiring using the factors and the local scores
   (e.g., given by the inside algorithm). the cky algorithm could then
   give the optimal parse tree in a single post-editing pass (smith
   [1569]2011). however, because our intra-sentential parsing model is
   designed to capture sequential dependencies between dt constituents,
   the score of a dt does not directly decompose across factors over
   discourse productions. therefore, designing such a semiring was not
   possible in our case.

   in addition to these features, we also experimented with other features
   including id138-based lexical semantics, subjectivity, and
   tf.idf-based cosine similarity. however, because such features did not
   improve parsing performance on our development set, they were excluded
   from our final set of features.
   4.2   parsing algorithm

   the intra- and multi-sentential parsing models of codra assign a
   id203 to every possible dt constituent in their respective
   parsing scenarios. the job of the parsing algorithm is then to find the
   k most probable dts for a given text. we implement a probabilistic
   cky-like bottom   up parsing algorithm that uses id145 to
   compute the most likely parses (jurafsky and martin [1570]2008). for
   simplicity, we first describe the specific case of generating the
   single most probable dt, then we describe how to generalize this
   algorithm to produce the k most probable dts for a given text.

   formally, the search problem for finding the most probable dt can be
   written as

   where    specifies the parameters of the parsing model (intra- or
   multi-sentential). given n discourse units, our parsing algorithm uses
   the upper-triangular portion of the n  n id145 table d,
   where cell d[i, j] (for i < j) stores:

   where u[x] (0) and u[x] (1) are the start and end edu ids of discourse
   unit u[x], and

   recall that the notation r[u[i] (0), u[m] (1), u[j] (1)] in this
   expression refers to a rhetorical relation r that holds between the
   discourse unit containing edus u[i] (0) through u[m] (1) and the unit
   containing edus u[m] (1) + 1 through u[j] (1).

   in addition to d, which stores the id203 of the most probable
   constituents of a dt, the algorithm also simultaneously maintains two
   other n  n id145 tables s and r for storing the structure
   (i.e., u[m*] (1)) and the relations (i.e., r*) of the corresponding dt
   constituents, respectively. for example, given four edus e[1]     e[4],
   the s and r id145 tables at the left side in [1571]figure
   12 together represent the dt shown at the right. more specifically, to
   find the dt, we first look at the top-right entries in the two tables,
   and find s[1, 4] = 2 and r[1, 4] = r[2], which specify that the two
   discourse units e[1:2] and e[3:4] should be connected by the relation
   r[2] (the root in the dt). then, we see how edus e[1] and e[2] should
   be connected by looking at the entries s[1, 2] and r[1, 2], and find
   s[1, 2] = 1 and r[1, 2] = r[1], which indicates that these two units
   should be connected by the relation r[1] (the left pre-terminal in the
   dt). finally, to see how edus e[3] and e[4] should be linked, we look
   at the entries s[3, 4] and r[3, 4], which tell us that they should be
   linked by the relation r[4] (the right pre-terminal). the algorithm
   works in polynomial time. specifically, for n discourse units and m
   number of relations, the time and space complexities are o(n^3 m) and
   o(n^2), respectively.
   [1572]figure
   figure   12    the s and r id145 tables (left), and the
   corresponding discourse tree (right).

   a key advantage of using a probabilistic parsing algorithm like the one
   we use is that it allows us to generate a list of k most probable parse
   trees. it is straightforward to generalize the above algorithm to
   produce k most probable dts. specifically, when filling up the dynamic
   programming tables, rather than storing a single best parse for each
   sub-tree, we store and keep track (i.e., using back-pointers) of k-best
   candidates simultaneously. one can show that the time and space
   complexities of the k-best version of the algorithm are o(n^3 mk^2 log
   k) and o(k^2 n), respectively (huang and chiang [1573]2005).

   note that, in contrast to other document-level discourse parsers (marcu
   [1574]2000b; subba and di-eugenio [1575]2009; hernault et al.
   [1576]2010; feng and hirst [1577]2012, [1578]2014), which use a greedy
   algorithm, codra finds a discourse tree that is globally
   optimal.[1579]^9 this approach of codra is also different from the
   sentence-level discourse parser spade (soricut and marcu [1580]2003).
   spade first finds the tree structure that is globally optimal, then it
   assigns the most probable relations to the internal nodes. more
   specifically, the cell d[i, j] in spade's id145 table
   stores

   where . disregarding the relation label r while populating d, this
   approach may find a discourse tree that is not globally optimal.
   4.3   document-level parsing approaches

   now that we have presented our intra-sentential and multi-sentential
   parsing components, we are ready to describe how they can be
   effectively combined in a unified framework ([1581]figure 2) to perform
   document-level rhetorical analysis. recall that a key motivation for a
   two-stage[1582]^10 parsing is that it allows us to capture the strong
   correlation between text structure and discourse structure in a
   scalable, modular, and flexible way. in the following, we describe two
   different approaches to model this correlation.
   4.3.1   1s   1s (1 sentence   1 sub-tree)

   a key finding from previous studies on sentence-level discourse
   analysis is that most sentences have a well-formed discourse sub-tree
   in the full document-level dt (soricut and marcu [1583]2003; fisher and
   roark [1584]2007). for example, [1585]figure 13a shows 10 edus in three
   sentences (see boxes), where the dts for the sentences obey their
   respective sentence boundaries.
   [1586]figure
   figure   13    two possible dts for three sentences.

   our first approach, called 1s   1s (1 sentence   1 sub-tree), aims to
   maximally exploit this finding. it first constructs a dt for every
   sentence using our intra-sentential parser, and then it provides our
   multi-sentential parser with the sentence-level dts to build the
   rhetorical parse for the whole document.
   4.3.2   sliding window

   although the assumption made by 1s   1s clearly simplifies the parsing
   process, it completely ignores the cases where rhetorical structures
   violate sentence boundaries. for example, in the dt shown in
   [1587]figure 13b, sentence s[2] does not have a well-formed sub-tree
   because some of its units attach to the left (i.e., 4   5 and 6) and some
   to the right (i.e., 7). vliet and redeker ([1588]2011) call these cases
      leaky    boundaries.

   although we find fewer than 5% of the sentences in the rst   dt have
   leaky boundaries, in other corpora this can be true for a larger
   portion of the sentences. for example, we observe that over 12% of the
   sentences in the instructional corpus of subba and di-eugenio
   ([1589]2009) have leaky boundaries. however, we notice that in most
   cases where dt structures violate sentence boundaries, its units are
   merged with the units of its adjacent sentences, as in [1590]figure
   13b. for example, this is true for 75% of the leaky cases in our
   development set containing 20 news articles from the rst   dt and for 79%
   of the leaky cases in our development set containing 20 how-to manuals
   from the instructional corpus. based on this observation, we propose a
   sliding window approach.

   in this approach, our intra-sentential parser works with a window of
   two consecutive sentences, and builds a dt for the two sentences. for
   example, given the three sentences in [1591]figure 13, our
   intra-sentential parser constructs a dt for s[1]   s[2] and a dt for
   s[2]   s[3]. in this process, each sentence in a document except the
   boundary sentences (i.e., the first and the last) will be associated
   with two dts: one with the previous sentence (say, dt[p]) and one with
   the next (say, dt[n]). in other words, for each non-boundary sentence,
   we will have two decisions: one from dt[p] and one from dt[n]. our
   parser consolidates the two decisions and generates one or more
   sub-trees for each sentence by checking the following three mutually
   exclusive conditions one after another:
         

   same in both: if the sentence under consideration has the same (in both
   structure and labels) well-formed sub-tree in both dt[p] and dt[n], we
   take this sub-tree. for example, in [1592]figure 14a, s[2] has the same
   sub-tree in the two dts (one for s[1]   s[2] and one for s[2]   s[3]). the
   two decisions agree on the dt for the sentence.
         

   different but no cross: if the sentence under consideration has a
   well-formed sub-tree in both dt[p] and dt[n], but the two sub-trees
   vary either in structure or in labels, we pick the most probable one.
   for example, consider the dt for s[1]   s[2] (at the left) in
   [1593]figure 14a and the dt for s[2]   s[3] in [1594]figure 14b. in both
   cases s2 has a well-formed sub-tree, but they differ in structure. we
   pick the sub-tree which has the higher id203 in the two dynamic
   programming tables.
         

   cross: if either or both of dt[p] and dt[n] segment the sentence into
   multiple sub-trees, we pick the one having more sub-trees. for example,
   consider the two dts in [1595]figure 14c. in the dt for s[1]   s[2] on
   the left, s[2] has three sub-trees (4   5, 6, 7), whereas in the dt for
   s[2]   s[3] on the right, it has two (4   6, 7). so, we extract the three
   sub-trees for s[2] from the first dt. if the sentence has the same
   number of sub-trees in both dt[p] and dt[n], we pick the one with
   higher id203 in the id145 tables. note that our
   choice of picking the dt with more sub-trees is intended to allow the
   parser to find more leaky cases. however, other heuristics are also
   possible. for example, another simple heuristic that one could try is:
   when both dts segment the sentence into multiple sub-trees, pick the
   one with fewer sub-trees, and when only one of the dts segment the
   sentence into multiple sub-trees, pick that one.
   [1596]figure
   figure   14    extracting sub-trees for s[2].

   at the end, the multi-sentential parser takes all these sentence-level
   sub-trees for a document, and builds a full rhetorical parse for the
   whole document.
   5.   the discourse segmenter
   section:
   [choose___________________________]
   [1597]previous section [1598]next section

   our discourse parser assumes that the input text has been already
   segmented into a sequence of edus. however, discourse segmentation is
   also a challenging problem, and previous studies (soricut and marcu
   [1599]2003; fisher and roark [1600]2007) have identified it as a
   primary source of inaccuracy for discourse parsing. regardless of its
   importance in discourse parsing, discourse segmentation itself can be
   useful in several nlp applications, including sentence compression
   (sporleder and lapata [1601]2005) and textual alignment in statistical
   machine translation (stede [1602]2011). therefore, in codra, we have
   developed our own discourse segmenter, which not only achieves
   state-of-the-art performance as shown later, but also reduces the time
   complexity by using fewer features.
   5.1   segmentation model

   the discourse segmenter in codra implements a binary classifier to
   decide for each word   token (except the last) in a sentence, whether to
   place an edu boundary after that token. we use a maximum id178 model
   to build a discriminative classifier. more specifically, we use a
   id28 classifier with parameter   :

   where the output y     {0, 1} denotes whether to put an edu boundary
   (i.e., y = 1) or not (i.e., y = 0) after the word   token w, which is
   represented by a feature vector x. in the equation, ber(  ) and sigm(  )
   refer to the bernoulli distribution and the sigmoid (also known as
   logistic) function, respectively. the negative log-likelihood (nll) of
   the model with l[2] id173 for n data points (i.e.,
   word   tokens) is given by

   where y[i] is the gold label for word   token w[i] (represented by
   feature vector x[i]). we learn the model parameters    using the l-bfgs
   fitting algorithm, which is time- and space-efficient. to avoid
   overfitting, we use 5-fold cross validation to learn the id173
   strength parameter    from the training data. we also use a simple
   id112 technique (breiman [1603]1996) to deal with the sparsity of
   boundary (i.e., y = 1) tags.

   note that our first attempt at the discourse segmentation task
   implemented a linear-chain crf model (lafferty, mccallum, and pereira
   [1604]2001) to capture the sequential dependencies between the tags in
   a discriminative way. however, the binary id28
   classifier, using the same set of features, not only outperforms the
   crf model, but also reduces time and space complexity. one possible
   explanation for the low performance of the crf model is that markov
   dependencies between tags cannot be effectively captured due to the
   sparsity of boundary tags. also, because we could not balance the data
   by using techniques like id112 in the crf model, this further
   degrades the performance.
   5.2   features used in the segmentation model

   our set of features for discourse segmentation are mostly inspired from
   previous studies but used in a novel way, as we describe here.

   our first subset of features, which we call spade features, includes
   the lexico-syntactic patterns extracted from the lexicalized syntactic
   tree of the given sentence. these features replicate the features used
   in spade's segmenter, but used in a discriminative way. in order to
   decide on an edu boundary after a word   token w[k], we search for the
   lowest constituent in the lexicalized syntactic tree that spans over
   tokens w[i]     w[j] such that i     k < j. the production that expands
   this constituent in the tree, with the potential edu boundary marked,
   forms the primary feature. for instance, to determine the existence of
   an edu boundary after the word efforts in our sample sentence shown in
   [1605]figure 9, the production np(efforts)     prp$(its) nns(efforts)    
   s(to) extracted from the lexicalized syntactic tree in [1606]figure 9a
   constitutes the primary feature, where     denotes the potential edu
   boundary.

   spade predicts an edu boundary if the relative frequency (i.e., maximum
   likelihood estimate) of a potential boundary given the production in
   the training data is greater than 0.5. if the production has not been
   observed frequently enough, the unlexicalized version of the production
   (e.g., np     prp$ nns     s) is used for prediction. if the unlexicalized
   version is also found to be rare, other variations of the production,
   depending on whether they include the lexical heads and how many
   non-terminals (one or two) they consider before and after the potential
   boundary, are examined one after another (see fisher and roark
   [[1607]2007] for details). in contrast, we compute the maximum
   likelihood estimates for a primary production (feature) and its other
   variations, and use those directly as features with/without binarizing
   the values.

   shallow syntactic features like chunk and pos tags have been shown to
   possess valuable clues for discourse segmentation (fisher and roark
   [1608]2007; sporleder and lapata [1609]2005). for example, it is less
   likely that an edu boundary occurs within a chunk. we annotate the
   tokens of a sentence with chunk and pos tags using the state-of-the-art
   illinois tagger[1610]^11 and encode these as features in our model.
   note that the chunker assigns each token a tag using the bio notation,
   where b stands for beginning of a particular phrase (e.g., noun or verb
   phrase), i stands for inside of a particular phrase, and o stands for
   outside of a particular phrase. the rationale for using the illinois
   chunker is that it uses a larger set of tags (23 in total); thus it is
   more informative than most of the other existing taggers, which
   typically use only five tags (b   np, i   np, b   vp, i   vp, and o).

   edus are normally multi-word strings. thus, a token near the beginning
   or end of a sentence is unlikely to be the end of a segment. therefore,
   for each token we include its relative position (i.e., absolute
   position/total number of tokens) in the sentence and distances to the
   beginning and end of the sentence as features.

   it is unlikely that two consecutive tokens are tagged with edu
   boundaries. therefore, we incorporate contextual information for a
   token into our model by including the above features computed for its
   neighboring tokens.

   we also experimented with different id165 (n     {1, 2, 3}) features
   extracted from the token sequence, pos sequence, and chunk sequence.
   however, because such features did not improve segmentation accuracy on
   the development set, they were excluded from our final set of features.
   6.   experiments
   section:
   [choose________________________]
   [1611]previous section [1612]next section

   in this section we present our experimental results. first, we describe
   the corpora on which the experiments were performed and the evaluation
   metrics used to measure the performance of the discourse segmenter and
   the parser. then we show the performance of our discourse segmenter,
   followed by the performance of our discourse parser.
   6.1   corpora

   whereas previous studies on discourse analysis only report their
   results on a particular corpus, to demonstrate the generality of our
   method, we experiment with texts from two very different genres: news
   articles and instructional how-to manuals.

   our first corpus is the standard rst   dt (carlson, marcu, and okurowski
   [1613]2002), which contains discourse annotations for 385 wall street
   journal news articles taken from the id32 corpus (marcus,
   marcinkiewicz, and santorini [1614]1994). the corpus is partitioned
   into a training set of 347 documents and a test set of 38 documents. a
   total of 53 documents selected from both training and test sets were
   annotated by two human annotators. we measure human agreements based on
   this doubly annotated data set. we used 25 documents from the training
   set as our development set. in rst   dt, the original 25 rhetorical
   relations defined by mann and thompson ([1615]1988) are further divided
   into a set of 18 coarser relation classes with 78 finer-grained
   relations (see carlson and marcu [[1616]2001] for details). our second
   corpus is the instructional corpus prepared by subba and di-eugenio
   ([1617]2009), which contains discourse annotations for 176 how-to
   manuals on home repair. the corpus was annotated with 26 informational
   relations (e.g., preparation   act, act   goal).

   for our experiments with the intra-sentential discourse parser, we
   extracted a sentence-level dt from a document-level dt by finding the
   sub-tree that exactly spans over the sentence. in rst   dt, by our count,
   7, 321 out of 7, 673 sentences in the training set, 951 out of 991
   sentences in the test set, and 1, 114 out of 1, 208 sentences in the
   doubly-annotated set have a well-formed dt. on the other hand, 3, 032
   out of 3, 430 sentences in the instructional corpus have a well-formed
   dt. this forms the corpora for our experiments with intra-sentential
   discourse parsing. however, the existence of a well-formed dt is not a
   necessity for discourse segmentation; therefore, we do not exclude any
   sentence in our discourse segmentation experiments.
   6.2   evaluation (and agreement) metrics

   in this subsection we describe the metrics used to measure both how
   much the annotators agree with each other, and how well the systems
   perform when their outputs are compared with human annotations for the
   discourse analysis tasks.
   6.2.1   metrics for discourse segmentation

   because sentence boundaries are considered to also be the edu
   boundaries, we measure segmentation accuracy with respect to the
   intra-sentential segment boundaries, which is a standard method
   (soricut and marcu [1618]2003; fisher and roark [1619]2007).
   specifically, if a sentence contains n edus, which corresponds to n     1
   intra-sentential segment boundaries, we measure the segmenter's ability
   to correctly identify these n     1 boundaries. let h be the total number
   of intra-sentential segment boundaries in the human annotation, m be
   the total number of intra-sentential segment boundaries in the model
   output, and c be the total number of correct segment boundaries in the
   model output. then, we measure precision (p), recall (r), and f-score
   for segmentation performance as follows:

   6.2.2   metrics for discourse parsing

   to evaluate parsing performance, we use the standard unlabeled and
   labeled precision, recall, and f-score as proposed by marcu
   ([1620]2000b). the unlabeled metric measures how accurate the discourse
   parser is in finding the right structure (i.e., the skeleton) of the
   dt, while the labeled metrics measure the parser's ability to find the
   right labels (i.e., nuclearity statuses or relation labels) in addition
   to the right structure. assume, for example, that given the two
   sentences of [1621]figure 1, our system generates the dt shown in
   [1622]figure 15. in [1623]figure 16, we show the same gold dt shown in
   [1624]figure 1 (on the left), and the same system-generated dt shown in
   [1625]figure 15 (on the right), when the two trees are aligned. for the
   sake of illustration, instead of showing the real edus, we only show
   their ids. notice that the automatic segmenter made two mistakes: (1)
   it broke the edu marked 2   3 (some people use the purchasers    index as a
   leading indicator) in the human annotation into two separate edus, and
   (2) it could not identify edu 5 (but the thing it's supposed to
   measure) and edu 6 (    manufacturing strength    ) as two separate edus.
   therefore, when we align the two annotations, we obtain seven edus in
   total.
   [1626]figure
   figure   15    a hypothetical system-generated dt for the two sentences in
   [1627]figure 1.
   [1628]figure
   figure   16    measuring the accuracy of a discourse parser. (a) the
   human-annotated discourse tree. (b) the system-generated discourse
   tree.

   in [1629]table 2, we list all constituents of the two dts and their
   associated labels at the span, nuclei, and relation levels. the recall
   (r) and precision (p) figures are shown at the bottom of the table.
   notice that, following (marcu [1630]2000b), the relation labels are
   assigned to the children nodes rather than to the parent nodes in the
   evaluation process to deal with non-binary trees in human annotations.
   to our knowledge, no implementation of the id74 was made
   publicly available. therefore, to help other researchers, we have made
   our source code of the id74 publicly available.[1631]^12

   [1632]table
   table   2    measuring parsing accuracy (p = precision, r = recall).

   given this evaluation setup, it is easy to understand that if the
   number of edus is the same in the human and system annotations (e.g.,
   when the discourse parser uses gold discourse segmentation), and the
   discourse trees are binary, then we get the same figures for precision,
   recall, and f-score.
   6.3   discourse segmentation evaluation

   in this section we present our experiments on discourse segmentation.
   6.3.1   experimental set-up for discourse segmentation

   we compare the performance of our discourse segmenter with the
   performance of the two publicly available discourse segmenters, namely,
   the discourse segmenters of the hilda (hernault et al. [1633]2010) and
   spade (soricut and marcu [1634]2003) systems. we also compare our
   results with the state-of-the-art results reported by fisher and roark
   ([1635]2007) on the rst   dt test set. in all our experiments when
   comparing two systems, we use paired t-test on the f-scores to measure
   statistical significance and report the p-value.

   we ran hilda with its default settings. for spade, we applied the same
   modifications to its default settings as described in fisher and roark
   ([1636]2007), which delivers significantly improved performance over
   its original version. specifically, in our experiments on the rst   dt
   corpus, we trained spade using the human-annotated syntactic trees
   extracted from the id32 (marcus, marcinkiewicz, and santorini
   [1637]1994), and, during testing, we replaced the charniak parser
   (charniak [1638]2000) with a more accurate reranking parser (charniak
   and johnson [1639]2005). however, because of the lack of gold syntactic
   trees in the instructional corpus, we trained spade in this corpus
   using the syntactic trees produced by the reranking parser. to avoid
   using the gold syntactic trees, we used the reranking parser in our
   system for both training and testing purposes. this syntactic parser
   was trained on the sections of the id32 not included in our
   test set. we applied the same canonical lexical head projection rules
   (magerman [1640]1995; collins [1641]2003) to lexicalize the syntactic
   trees as done in hilda and spade.

   note that previous studies (fisher and roark [1642]2007; soricut and
   marcu [1643]2003; hernault et al. [1644]2010) on discourse segmentation
   only report their performance on the rst   dt test set. to compare our
   results with them, we evaluated our model on the rst   dt test set. in
   addition, we showed a more general performance of spade and our system
   on the two corpora based on 10-fold cross validation.[1645]^13 however,
   spade does not come with a training module for its segmenter. we
   reimplemented this module and verified its correctness by reproducing
   the results on the rst   dt test set.
   6.3.2   results for discourse segmentation

   [1646]table 3 shows the discourse segmentation results of different
   systems in precision, recall, and f-score on the two corpora. on the
   rst    dt corpus, hilda's segmenter delivers the weakest performance,
   having an f-score of only 74.1. note that the high segmentation
   accuracy reported by hernault et al. ([1647]2010) is due to a less
   stringent evaluation metric. spade performs much better than hilda with
   an absolute f-score improvement of 11.1%. our segmenter ds outperforms
   spade with an absolute f-score improvement of 4.9% (p-value < 2.4e-06),
   and also achieves comparable results to the ones of fisher and roark
   ([1648]2007) (f&r), even though we use fewer features.[1649]^14 notice
   that human agreement for this task is quite high   namely, an f-score of
   98.3 computed on the doubly-annotated portion of the rst   dt corpus
   mentioned in [1650]section 6.1.

   [1651]table
   table   3    discourse segmentation results of different models on the two
   corpora. performances significantly superior to spade are denoted by *.

   because fisher and roark ([1652]2007) only report their results on the
   rst   dt test set and we did not have access to their system, we compare
   our approach only with spade when evaluating on a whole corpus based on
   10-fold cross validation. on the rst   dt corpus, our segmenter delivers
   an absolute f-score improvement of 3.8 percentage points, which
   represents a more than 25% relative error rate reduction. the
   improvement is higher on the instructional corpus with an absolute
   f-score improvement of 8.1 percentage points, which corresponds to a
   relative error reduction of 30%. the improvements for both corpora are
   statistically significant (p-value < 3.0e-06). when we compare our
   results on the two corpora, we observe a substantial decrease in
   performance on the instructional corpus. this could be because of a
   smaller amount of data in this corpus and/or to the inaccuracies in the
   syntactic parser and taggers, which are trained on news articles. a
   promising future direction would be to apply effective domain
   adaptation methods (e.g., easyadapt [daum   [1653]2007]) to improve
   discourse segmentation performance in the instructional domain by
   leveraging the rich data in the news domain (i.e., rst   dt).
   6.4   discourse parsing evaluation

   in this section we present our experiments on discourse parsing. first,
   we describe the experimental set-up. then, we present the results of
   the parsers. while presenting the performance of our discourse parser,
   we show a breakdown of intra-sentential versus inter-sentential
   results, in addition to the aggregated results at the document level.
   6.4.1   experimental set-up for discourse parsing

   in our experiments on sentence-level (i.e., intra-sentential) discourse
   parsing, we compare our approach with spade (soricut and marcu
   [1654]2003) on the rst   dt corpus, and with the ilp-based approach of
   subba and di-eugenio ([1655]2009) on the instructional corpus, because
   they are the state of the art in their respective genres. for spade, we
   applied the same modifications to its default settings as described in
   [1656]section 6.3.1, which leads to improved performance. similarly, in
   our experiments on document-level (i.e., multi-sentential) parsing, we
   compare our approach with hilda (hernault et al. [1657]2010) on the
   rst   dt corpus, and with the ilp-based approach (subba and di-eugenio
   [1658]2009) on the instructional corpus. the results for hilda were
   obtained by running the system with default settings on the same inputs
   we provided to our system. because we could not run the ilp-based
   system (not publicly available), we report the performance presented in
   their paper.

   our experiments on the rst   dt corpus use the same 18 coarser coherence
   relations (see [1659]figure 18 later in this article), defined by
   carlson and marcu ([1660]2001) and also used in spade and hilda
   systems. more specifically, the relation set consists of 16 relation
   categories and two pseudo-relations, namely, textual   organization and
   same   unit. after attaching the nuclearity statuses (ns, sn, nn) to
   these relations, we obtain 41 distinct relations.[1661]^15 our
   experiments on the instructional corpus consider the same 26 primary
   relations (e.g., goal:act, cause:effect) used by subba and di-eugenio
   ([1662]2009) and also treat the reversals of non-commutative relations
   as separate relations. that is, goal   act and act   goal are considered to
   be two different coherence relations. attaching the nuclearity statuses
   to these relations provides 76 distinct relations.

   based on our experiments on the development set, the size of the
   automatically built bi-gram and tri-gram dictionaries was set to 95% of
   their total number of items, and the size of the unigram dictionary was
   set to 100%. note that the unigram dictionary contains only special
   tags denoting edu, sentence, and paragraph boundaries.
   6.4.2   evaluation of the intra-sentential discourse parser

   this section presents our experimental evaluation on intra-sentential
   discourse parsing. first, we show the performance of the sentence-level
   parsers when they are provided with manual (or gold) discourse
   segmentations. this allows us to judge the parsing performance
   independently of the segmentation task. then, we show the end-to-end
   performance of our intra-sentential framework, that is, the
   intra-sentential parsing performance based on automatic discourse
   segmentation.

   intra-sentential parsing results based on manual segmentation

   [1663]table 4 presents the intra-sentential discourse parsing results
   when manual discourse segmentation is used. recall from our discussion
   on id74 in [1664]section 6.2.2 that precision, recall,
   and f-score are the same when manual segmentation is used. therefore,
   we report only one of them. notice that our sentence-level discourse
   parser par-s consistently outperforms spade on the rst   dt test set in
   all three metrics, and the improvements are statistically significant
   (p-value < 0.01). especially, on the relation labeling task, which is
   the hardest among the three tasks, we achieve an absolute f-score
   improvement of 12.2 percentage points, which represents a relative
   error rate reduction of 37.7%.

   [1665]table
   table   4    intra-sentential parsing results based on manual discourse
   segmentation. performances significantly superior to spade are denoted
   by *.

   to verify our claim that capturing the sequential dependencies between
   dt constituents using a dcrf model actually contributes to the
   performance gain, we also compare our results with an intra-sentential
   parser (see crf-nc in [1666]table 4) that uses a simplified crf model
   similar to the one shown in [1667]figure 8. although the simplified
   model has two hidden variables to model the structure and the relation
   of a dt constituent jointly, it does not have a chain structure, thus
   it ignores the sequential dependencies between dt constituents. the
   comparison in all three measures demonstrates that the improvements are
   indeed partly due to the dcrf model (p   value < 0.01).[1668]^16 a
   comparison between crf-nc and spade shows that crf-nc significantly
   outperforms spade in all three measures (p-value < 0.01). this could be
   due to the fact that crf-nc is trained discriminatively with a large
   number of features, whereas spade is trained generatively with only
   lexico-syntactic features.

   notice that the scores of our parser (par-s) are close to the human
   agreement on the doubly-annotated data, and these results on the rst   dt
   test set are also consistent with the mean scores over 10-folds on the
   whole rst   dt corpus.[1669]^17

   the improvements are higher on the instructional corpus, where we
   compare our mean results over 10-folds with the reported results of the
   ilp-based system of subba and di-eugenio ([1670]2009), giving absolute
   f-score improvements of 5.4 percentage points, 17.6 percentage points,
   and 12.8 percentage points in span, nuclearity, and relations,
   respectively.[1671]^18 our parser par-s reduces the errors by 76.1%,
   62.4%, and 34.6% in span, nuclearity and relations, respectively.

   if we compare the performance of our intra-sentential discourse parser
   on the two corpora, we notice that our parser par-s is more accurate in
   finding the right tree structure (see span row in the table) on the
   instructional corpus. this may be due to the fact that sentences in the
   instructional domain are relatively short and contain fewer edus than
   sentences in the news domain, thus making it easier to find the right
   tree structure. however, when we compare the performance on the
   relation labeling task, we observe a decrease on the instructional
   corpus. this may be due to the small amount of data available for
   training and the imbalanced distribution of a large number of discourse
   relations (i.e., 76 with nuclearity attached) in this corpus.

   intra-sentential parsing results based on automatic segmentation

   in order to evaluate the performance of the fully automatic
   sentence-level discourse analysis systems, we feed the intra-sentential
   discourse parsers the output of their respective discourse segmenters.
   [1672]table 5 shows the (p)recision, (r)ecall, and (f)   score results
   for different id74. we compare our intra-sentential
   parser par-s with spade on the rst   dt test set. we achieve absolute
   f-score improvements of 5.7 percentage points, 6.4 percentage points,
   and 9.5 percentage points in span, nuclearity, and relation,
   respectively. these improvements are statistically significant
   (p-value<0.001). our system, therefore, reduces the errors by 24.5%,
   21.4%, and 22.6% in span, nuclearity, and relations, respectively.
   these results are also consistent with the mean results over 10-folds
   on the whole rst   dt corpus.

   [1673]table
   table   5    intra-sentential parsing results using automatic discourse
   segmentation. performances significantly superior to spade are denoted
   by *.

   the rightmost column in the table shows our mean results over 10-folds
   on the instructional corpus. we could not compare our system with the
   ilp-based approach of subba and di-eugenio ([1674]2009) because no
   results were reported using an automatic segmenter. it is interesting
   to observe how much our parser is affected by an automatic segmenter on
   the two corpora (see [1675]tables 4 and [1676]5). nevertheless, taking
   into account the segmentation results in [1677]table 3, this is not
   surprising because previous studies (soricut and marcu [1678]2003) have
   already shown that automatic segmentation is the primary impediment to
   high accuracy discourse parsing. this demonstrates the need for a more
   accurate discourse segmentation model in the instructional genre.
   6.4.3   evaluation of the complete parser

   we experiment with our full document-level discourse parser on the two
   corpora using the two parsing approaches described in [1679]section
   4.3, namely, 1s-1s and the sliding window. on rst   dt, the standard
   split was used for training and testing. on the instructional corpus,
   subba and di-eugenio ([1680]2009) used 151 documents for training and
   25 documents for testing. because we did not have access to their
   particular split, we took five random samples of 151 documents for
   training and 25 documents for testing, and report the average
   performance over the five test sets.

   [1681]table 6 presents results for our two-stage discourse parser (tsp)
   using approaches 1s-1s (tsp 1-1) and the sliding window (tsp sw) on
   manually segmented texts. recall that precision, recall, and f-score
   are the same when manual segmentation is used. we compare our parser
   with the state-of-the-art on the two corpora: hilda (hernault et al.
   [1682]2010) on rst   dt, and the ilp-based approach (subba and di-eugenio
   [1683]2009) on the instructional domain. on both corpora, our systems
   outperform existing systems by a wide margin (p-value <7.1e-05 on
   rst   dt).[1684]^19 on rst   dt, our parser tsp 1-1 achieves absolute
   improvements of 7.9 percentage points, 9.3 percentage points, and 11.5
   percentage points in span, nuclearity, and relation, respectively, over
   hilda. this represents relative error reductions of 31.2%, 22.7%, and
   20.7% in span, nuclearity, and relation, respectively.

   [1685]table
   table   6    parsing results of document-level parsers using manual
   segmentation. performances significantly superior to hilda (p-value
   <0.0001) are denoted by *. significant differences between tsp 1-1 and
   tsp sw (p-value <0.01) are denoted by    .

   beside hilda, we also compare our results with two baseline parsers on
   rst   dt: (1) crf-o, which uses a single unified crf-based parsing model
   shown in [1686]figure 8 (the one used for multi-sentential parsing)
   without distinguishing between intra- and multi-sentential parsing, and
   (2) crf-t, which uses two different crf-based parsing models for intra-
   and multi-sentential parsing in the two-stage approach 1s-1s, both
   models having the same structure as in [1687]figure 8. thus, crf-t is a
   variation of tsp 1-1, where the dcrf-based (chain-structured)
   intra-sentential parsing model is replaced with a simpler crf-based
   parsing model.[1688]^20 note that although crf-o does not explicitly
   discriminate between intra- and multi-sentential parsing, it uses
   id165 features that include sentence and edu boundaries to encode this
   information into the model.

   [1689]table 6 shows that both crf-o and crf-t outperform hilda by a
   good margin (p-value <0.0001). this improvement can be attributed to
   the optimal parsing algorithm and better feature selection strategy.
   when we compare crf-t with crf-o, we notice significant performance
   gains for crf-t (p-value <0.001). the absolute gains are 4.32
   percentage points, 2.68 percentage points, and 4.55 percentage points
   in span, nuclearity, and relation, respectively. this comparison
   clearly demonstrates the benefit of using a two-stage approach with two
   different parsing models over a framework with one single unified
   parsing model. finally, when we compare our best results with the human
   agreements, we still observe room for further improvement in all three
   measures.

   on the instructional genre, our parser tsp 1-1 delivers absolute
   f-score improvements of 10.3 percentage points, 13.6 percentage points,
   and 8.1 percentage points in span, nuclearity, and relations,
   respectively, over the ilp-based approach of subba and di-eugenio
   ([1690]2009). our parser, therefore, reduces errors by 34.7%, 26.9%,
   and 12.5% in span, nuclearity, and relations, respectively.

   if we compare the performance of our discourse parsers on the two
   corpora, we observe lower results on the instructional corpus. there
   could be two reasons for this. first, the instructional corpus has a
   smaller amount of data with a larger set of relations (76 with
   nuclearity attached). second, some of the frequent relations are
   semantically very similar (e.g., preparation-act, step1-step2), which
   makes it difficult even for the human annotators to distinguish them
   (subba and di-eugenio [1691]2009).

   comparison between our two document-level parsing approaches reveals
   that tsp sw significantly outperforms tsp 1-1 only in finding the right
   structure on both corpora (p-value <0.01). not surprisingly, the
   improvement is higher on the instructional corpus. a likely explanation
   is that the instructional corpus contains more leaky boundaries (12%),
   allowing the sliding window approach to be more effective in finding
   those, without inducing much noise for the labels. this demonstrates
   the potential of tsp sw for data sets with even more leaky boundaries,
   e.g., the dutch (vliet and redeker [1692]2011) and the german potsdam
   (stede [1693]2004) corpora. however, it would be interesting to see how
   other heuristics to do consolidation in the cross condition
   ([1694]section 4.3.2) perform.

   to analyze errors made by tsp sw, we looked at some poorly parsed
   examples and found that although tsp sw finds more correct structures,
   a corresponding improvement in labeling relations is not present
   because in some cases, it tends to induce noise from the neighboring
   sentences for the labels. for example, when parsing is performed on the
   first sentence in [1695]figure 1 in isolation using 1s-1s, our parser
   rightly identifies the contrast relation between edus 2 and 3. but,
   when it is considered with its neighboring sentences by the sliding
   window, the parser labels it as elaboration. a promising strategy to
   deal with this and similar problems would be to apply both approaches
   to each sentence and combine them by consolidating three probabilistic
   decisions, namely, the one from 1s-1s and the two from the sliding
   window.
   6.4.4   k-best parsing results based on manual segmentation

   as described in [1696]section 4.2, a straight-forward modification of
   our probabilistic parsing algorithm allows us to generate a list of
   k-best parse hypotheses for a given text. we adapt our parsing
   algorithm accordingly to produce k most probable dts for each text, and
   measure the oracle accuracy based on the f-scores of the relation
   metric which gives aggregated evaluation on structure and relation
   labels (see [1697]table 2). specifically, the oracle accuracy o   score
   for k-best discourse parsing is measured as follows:

   where n is the total number of texts (sentences or documents)
   evaluated, g[i] is the gold dt annotation for text i, is the j^th parse
   hypothesis generated by the parser for text i, and f-score[r] (g[i], )
   is the f-score accuracy of hypothesis on the relation metric, which
   essentially measures how similar is to g[i] in terms of its structure
   and labels.

   [1698]table 7 presents the oracle scores of our intra-sentential parser
   par-s on the rst   dt test set as a function of k of k-best parsing. the
   1-best result tells that the parser has the base accuracy of 79.8%. the
   2-best shows dramatic oracle-rate improvements (i.e., 4.65% absolute),
   meaning that often our parser generates the best tree as its top two
   outputs. 3-best and 4-best also show moderate improvements (about 2%).
   things start to slow down afterwards, and we achieve oracle rates of
   90.37% and 92.57% at 10   best and 20-best, respectively. the 30-best
   parsing gives an oracle score of 93.2%.

   [1699]table
   table   7    oracle scores as a function of k of k-best sentence-level
   parses on rst   dt.

   the results of our k-best intra-sentential discourse parser demonstrate
   that a k-best reranking approach like that of collins and koo
   ([1700]2005) and charniak and johnson ([1701]2005) used for syntactic
   parsing can potentially improve the parsing accuracy even further by
   exploiting additional global features of the candidate discourse trees
   as evidence.

   the scenario is quite different at the document-level; [1702]table 8
   shows the k-best parsing results of tsp 1s-1s on the rst   dt test set.
   the improvements in oracle-rate are small at the document-level when
   compared with the sentence-level parsing. for example, the 2-best and
   the 5-best improve over the base accuracy by only 0.7 percentage points
   and 1.0 percentage points, respectively. the improvements get even
   slower after that. however, this is not surprising because generally
   document-level dts are big with many constituents, and only a very few
   of these constituents change from k-best to k + 1-best parsing. these
   small changes among the candidate dts do not contribute much to the
   overall f-score accuracy (for further clarification see how f-score is
   calculated in [1703]section 6.2.2).

   [1704]table
   table   8    oracle scores as a function of k of k-best document-level
   parses on rst   dt.

   the results of our k-best document-level parsing suggest that often the
   best tree is missing in the top k parses. thus, a reranking of k-best
   document-level parses may not be a suitable option for further
   improvement at the document-level. an alternative to k-best reranking
   is to use a sampling-based parsing strategy (wick et al. [1705]2011) to
   explore the space of possible trees, as recently used for dependency
   parsing (zhang et al. [1706]2014). however, note that the potential
   gain we may obtain by using a reranker at the sentence level will also
   improve the (combined) accuracy of the document-level parser.
   6.4.5   analysis of features

   to analyze the relative importance of different features used in our
   parsing models, [1707]table 9 presents the sentence- and document-level
   parsing results on a manually segmented rst   dt test set using different
   subsets of features. the feature subsets were defined in [1708]section
   4.1.4. in each parsing condition, the subsets of features are added
   incrementally, based on their availability and historical importance.
   the columns in [1709]table 9 represent the inclusion order of the
   feature subsets.

   [1710]table
   table   9    parsing results using different subsets of features on rst   dt
   test set.

   because spade (soricut and marcu [1711]2003) achieved the previous best
   results on intra-sentential parsing using dominance set features, these
   are included as the initial set of features in our intra-sentential
   parsing model. in hilda, hernault et al. ([1712]2010) demonstrate the
   importance of organizational and id165 features for full text parsing.
   we add these two feature subsets one after another in our intra- and
   multi-sentential parsing models.[1713]^21 contextual features require
   other features to be computed; thus they were added after those
   features. because computation of sub-structural features requires an
   initial parse tree (i.e., when the parser is applied), they are added
   at the very end.

   notice that inclusion of every new subset of features appears to
   improve the performance over the previous set. specifically, for
   sentence-level parsing, when we add the organizational features with
   the dominance set features, we achieve about 2 percentage points
   absolute improvements in nuclearity and relations. with id165
   features, the gain is even higher: 6 percentage points in relations and
   3.5 percentage points in nuclearity for sentence-level parsing, and 3.8
   percentage points in relations and 3.1 percentage points in nuclearity
   for document-level parsing. this demonstrates the utility of the id165
   features, which is also consistent with the previous findings of
   duverle and prendinger ([1714]2009) and schilder ([1715]2002).

   the features extracted from lexical chains (l-ch) have also proved to
   be useful for document-level parsing. they deliver absolute
   improvements of 2.7 percentage points, 2.9 percentage points, and 2.3
   percentage points in span, nuclearity, and relations, respectively.
   including the contextual features further gives improvements of 3
   percentage points in nuclearity and 2.2 percentage points in relation
   for sentence-level parsing, and 1.3 percentage points in nuclearity,
   and 1.2 percentage points in relation for document-level parsing.
   notice that sub-structural features are more beneficial for
   document-level parsing than they are for sentence-level parsing, that
   is, an improvement of 2.2 percentage points versus an improvement of
   0.9 percentage points. this is not surprising because document-level
   dts are generally much larger than sentence-level dts, making the
   sub-structural features more effective for document-level parsing.
   6.4.6   error analysis

   we further analyze the errors made by our discourse parser. as
   described in previous sections, the parser could be wrong in finding
   the right structure as well as the right nuclearity and relation
   labels. [1716]figure 17 presents an example where our parser makes
   mistakes in finding the right structure (notice the units connected by
   attribution and cause in the two example dts) and the right relation
   label (topic-comment vs. background). the comparison between intra- and
   multi-sentential parsing results presented in [1717]sections 6.4.2 and
   [1718]6.4.3 tells us that the errors in structure occur more frequently
   when the dt is large (e.g., at the document level) and the parsing
   model fails to capture the long-range structural dependencies between
   the dt constituents.

   to further analyze the errors made by our parser on the hardest task of
   relation labeling, in [1719]figure 18 we present the confusion matrix
   for our document-level parser tsp 1-1 on the rst   dt test set. in order
   to judge independently the ability of our parser to assign the correct
   relation labels, the confusion matrix is computed based on the
   constituents (see [1720]table 2), where our parser found the right span
   (i.e., structure).[1721]^22 the relations in the matrix are ordered
   according to their frequency in the training set.

   in general, the errors can be explained by two different causes acting
   together: (1) imbalanced distribution of the relations in the corpus,
   and (2) semantic similarity between the relations. the most frequent
   relation elaboration tends to overshadow others, especially the ones
   that are semantically similar (e.g., explanation, background) and less
   frequent (e.g., summary, evaluation). furthermore, our models sometimes
   fail to distinguish relations that are semantically similar (e.g.,
   temporal vs. background, cause vs. explanation).

   now, let us look more closely at a few of these errors. [1722]figure 19
   presents an example where our parser mistakenly labels a summary as
   elaboration. clearly, in this example the text in parentheses (i.e.,
   (cfd)) is an acronym or summary of the text to the left. however,
   parenthesized texts are also used to provide additional information
   (i.e., to elaborate), as exemplified in [1723]figure 20 by two text
   snippets from the rst-dt. notice that although the structure of the
   text (widow of the ..) in the first example is quite distinguishable
   from the structure of (cfd), the text (d., maine) in the second example
   is similar to (cfd) in structure, thus it confuses our model.[1724]^23
   [1725]figure
   figure   17    discourse trees generated by human annotator and our system
   for the text [what's more,][e1] [he believes][e2] [seasonal swings in
   the auto industry this year aren't occurring at the same time in the
   past,][e3] [because of production and pricing differences][e4] [that
   are curbing the accuracy of seasonal adjustments][e5] ] [built into the
   employment data.][e6]
   [1726]figure
   figure   18    confusion matrix for relation labels on the rst   dt test set.
   the y-axis represents true and x-axis represents predicted relations.
   the relations are topic-change (t-c), topic-comment (t-cm),
   textualorganization (t-o), manner-means (m-m), comparison (cmp),
   evaluation (ev), summary (su), condition (cnd), enablement (en), cause
   (ca), temporal (te), explanation (ex), background (ba), contrast (co),
   joint (jo), same   unit (s-u), attribution (at), and elaboration (el).
   [1727]figure
   figure   19    our system mistakenly labels a summary as elaboration.
   [1728]figure
   figure   20    two examples of elaboration by texts in parentheses.

   [1729]figure 21 presents two examples where our parser mistakenly
   labels background and cause as elaboration. however, notice that the
   two discourse relations (i.e., background vs. elaboration and cause vs.
   elaboration) in these examples are semantically very close, and
   arguably both can be applicable.
   [1730]figure
   figure   21    confusion between background/cause and elaboration.

   given these observations, we see two possible ways to improve our
   system. first, we would like to use a more robust method (e.g.,
   ensemble methods with id112) to deal with the imbalanced distribution
   of relations, along with taking advantage of richer semantic knowledge
   (e.g., id152) to cope with the errors caused by
   semantic similarity between the relations. second, to capture
   long-range dependencies between dt constituents, we would like to
   explore the idea of k-best discriminative reranking using tree kernels
   (dinarelli, moschitti, and riccardi [1731]2011). because our parser
   already produces k most probable dts, developing a reranker based on
   discourse tree kernels is very much within our reach.
   7.   conclusions and future directions
   section:
   [choose___________________________]
   [1732]previous section [1733]next section

   in this article we have presented codra, a complete probabilistic
   discriminative framework for performing rhetorical analysis in the rst
   framework. codra comprises components for performing both discourse
   segmentation and discourse parsing. the discourse segmenter is a binary
   classifier based on a maximum id178 model, and the discourse parser
   applies an optimal parsing algorithm to probabilities inferred from two
   crf models: one for intra-sentential parsing and the other for
   multi-sentential parsing. the crf models effectively represent the
   structure and the label of discourse tree constituents jointly.
   furthermore, the dcrf model for intra-sentential parsing captures the
   sequential dependencies between the constituents. the two separate
   parsing models use their own informative feature sets and the
   distributional variations of the relation labels in their respective
   parsing conditions.

   we have also presented two approaches to effectively combine the
   intra-sentential and the multi-sentential parsing modules, which can
   exploit the strong correlation observed between the text structure and
   the structure of the discourse tree. the first approach (1s   1s) builds
   a dt for every sentence using the intra-sentential parser, and then
   runs the multi-sentential parser on the resulting sentence-level dts.
   to deal with leaky boundaries, our second approach (sliding window)
   builds sentence-level discourse sub-trees by applying the
   intra-sentential parser on a sliding window, covering two adjacent
   sentences and then consolidating the results produced by overlapping
   windows. after that, the multi-sentential parser takes all these
   sentence-level sub-trees and builds a full rhetorical parse for the
   whole document.

   finally, we have extended the parsing algorithm to generate k most
   probable parse hypotheses for each input text, which could be used in a
   reranker to improve over the initial ranking using global features like
   long-range structural dependencies.

   empirical evaluations on two different genres demonstrate that our
   approach to discourse segmentation achieves state-of-the-art
   performance more efficiently using fewer features. a series of
   experiments on the discourse parsing task shows that both our intra-
   and multi-sentential parsers significantly outperform the state of the
   art, often by a wide margin. a comparison between our combination
   strategies reveals that the sliding window approach is more robust
   across domains. furthermore, the oracle accuracy computed based on the
   k-best parse hypotheses generated by our parser demonstrates that a
   reranker could potentially improve the accuracy even further.

   our error analysis reveals that although the sliding window approach
   finds more correct tree structures, in some cases it induces noise for
   the relation labels from the neighboring sentences. with respect to the
   performance of our discourse parser on the relation labeling task we
   also found that the most frequent relations tend to mislead the
   identification of the less frequent ones, and the models sometimes fail
   to distinguish relations that are semantically similar.

   the work presented in this article leads us to several interesting
   future directions. our short-term goal is to develop a k-best
   discriminative reranking discourse parser using tree kernels applied to
   discourse trees. we also plan to investigate to what extent discourse
   segmentation and discourse parsing can be performed jointly.

   we would also like to explore how our system performs on other genres
   like conversational (e.g., blogs, e-mails) and evaluative (e.g.,
   customer reviews) texts. to address the problem of limited annotated
   data in various genres, we are planning to develop an interactive
   version of our system that will allow users to fix the output of the
   system with minimal effort and let the system learn from that feedback.

   another interesting future direction is to perform extrinsic
   evaluations of our system in downstream applications. one important
   application of rhetorical structure is text summarization, where a
   significant challenge is producing not only informative but also
   coherent summaries. a number of researchers have already investigated
   the utility of rhetorical structure for measuring text importance
   (i.e., informativeness) in summarization (marcu [1734]2000b; daum   and
   marcu [1735]2002; louis, joshi, and nenkova [1736]2010). recently,
   christensen et al. ([1737]2013, [1738]2014) propose to perform sentence
   selection and ordering at the same time, and use constraints on
   discourse structure to make the summaries coherent. however, they
   represent the discourse as an unweighted directed graph, which is
   shallow and not sufficiently informative in most cases. furthermore,
   their approach does not allow compression at the sentence level, which
   is often beneficial in summarization. in the future, we would like to
   investigate the utility of our rhetorical structure for performing
   sentence compression, selection, and ordering in a joint summarization
   process.

   discourse structure can also play important roles in sentiment
   analysis. a key research problem in id31 is extracting
   fine-grained opinions about different aspects of a product. several
   recent papers (somasundaran [1739]2010; lazaridou, titov, and sporleder
   [1740]2013) exploited the rhetorical structure for this task. another
   challenging problem is assessing the overall opinion expressed in a
   review because not all sentences in a review contribute equally to the
   overall sentiment. for example, some sentences are subjective, whereas
   others are objective (pang and lee [1741]2004); some express the main
   claims, whereas others support them (taboada et al. [1742]2011); some
   express opinions about the main entity, whereas others are about the
   peripherals. discourse structure could be useful to capture the
   relative weights of the discourse units towards the overall sentiment.
   for example, the nucleus and satellite distinction along with the
   rhetorical relations could be useful to infer the relative weights of
   the connecting discourse units.

   among other applications of discourse structure, machine translation
   (mt) and its evaluation have received a resurgence of interest
   recently. a workshop dedicated to discourse in machine translation was
   arranged recently at the acl 2013 conference (webber et al.
   [1743]2013). researchers believe that mt systems should consider
   discourse phenomena that go beyond the current sentence to ensure
   consistency in the choice of lexical items or referring expressions,
   and the fact that source-language coherence relations are also realized
   in the target language (i.e., translating at the document-level
   [hardmeier, nivre, and tiedemann [1744]2012]). guzm  n et al.
   ([1745]2014a, [1746]2014b) and joty et al. ([1747]2014) propose new
   discourse-aware automatic id74 for mt systems using our
   discourse analysis tool. they demonstrate that sentence-level discourse
   information is complementary to the state-of-the-art evaluation
   metrics, and by combining the discourse-based metrics with the metrics
   from the asiya mt evaluation toolkit (gim  nez and m  rquez [1748]2010),
   they won the wmt 2014 metrics shared task challenge (mach    ek and bojar
   [1749]2014) both at the segment- and at the system-level. these results
   suggest that discourse structure helps to distinguish better
   translations from worse ones. thus, it would be interesting to explore
   whether discourse information can be used to rerank alternative mt
   hypotheses as a post-processing step for the mt output.

   a longer-term goal is to extend our framework to also work with graph
   structures of discourse, as recommended by several recent discourse
   theories (wolf and gibson [1750]2005). once we achieve similar
   performance on graph structures, we will perform extrinsic evaluations
   to determine their relative utility for various nlp tasks.

   finally, we hope that the online demo, the source code of codra, and
   the id74 that we made publicly available in this work
   will facilitate other researchers in extending our work and in applying
   discourse parsing to their nlp tasks.
   bibliographic note
   section:
   [choose________________________]
   [1751]previous section [1752]next section

   portions of this work were previously published in two conference
   proceedings (joty, carenini, and ng [1753]2012; joty et al.
   [1754]2013). this article significantly extends our previous work in
   several ways, most notably: (1) we extend the parsing algorithm to
   generate k-most probable parse hypotheses for each input text
   ([1755]section 4.2); (2) we show the oracle accuracies for k-best
   discourse parsing both at the sentence level and at the document level
   ([1756]section 6.4.4); (3) to support our claim, we compare our best
   results with several variations of our approach (see crf-nc in
   [1757]section 6.4.2, and crf-o and crf-t in [1758]section 6.4.3); (4)
   we analyze the relative importance of different features for intra- and
   multi-sentential discourse parsing ([1759]section 6.4.5); and (5) we
   perform in-depth error analysis of our complete rhetorical analysis
   framework ([1760]section 6.4.6).
   appendix a. sample output generated by an online demo of codra
   section:
   [choose_________________________]
   [1761]previous section [1762]next section

   acknowledgments
   section:
   [choose________________________]
   [1763]previous section [1764]next section

   the authors acknowledge the funding support of nserc canada graduate
   scholarship (cgs-d). many thanks to bonnie webber, amanda stent,
   carolyn rose, lluis marquez, samantha wray, and the anonymous reviewers
   for their insightful comments on an earlier version of this article.
   notes
   section:
   [choose________________________]
   [1765]previous section [1766]next section

   1    we categorize this approach as unsupervised because it does not rely
   on human-annotated data.

   2    [1767]http://www.isi.edu/licensed-sw/spade/.

   3    [1768]http://nlp.prendingerlab.net/hilda/.

   4    the demo of codra is available at
   [1769]http://109.228.0.153/discourse_parser_demo/. the source code of
   codra is available from [1770]http://alt.qcri.org/tools/.

   5    the input text in the demo in [1771]appendix a is taken from
   [1772]www.bbc.co.uk/news/world-asia-26106490.

   6    by the term    fat    we refer to crfs with multiple (interconnected)
   chains of output variables.

   7    for n + 1 edus, the number of valid discourse tree structures (i.e.,
   not counting possible variations in the nuclearity and relation labels)
   is the catalan number c[n].

   8    the higher the conditional id178, the lower the mutual
   information, and vice versa.

   9    we agree that with potentially sub-optimal, sub-structural features
   in the parsing model, cky may end up finding a sub-optimal dt. but that
   is a separate issue.

   10    do not confuse the term two-stage with the term two-pass.

   11    available at [1773]http://cogcomp.cs.illinois.edu/page/software.

   12    available from [1774]alt.qcri.org/tools/.

   13    because the two tasks   discourse segmentation and intra-sentential
   parsing   operate at the sentence level, the cross validation was
   performed over sentences for their evaluation.

   14    because we did not have access to the system or to the complete
   output/results of fisher and roark ([1775]2007), we were not able to
   perform a statistical significance test.

   15    not all relations take all the possible nuclearity statuses. for
   example, elaboration and attribution are mono-nuclear relations, and
   same   unit and joint are multi-nuclear relations.

   16    the parsing performance reported in [1776]table 4 for crf-nc is
   when the crf parsing model is trained on a balanced data set (an equal
   number of instances with s=1 and s=0); training on full but imbalanced
   data set gives slightly lower results.

   17    our emnlp and acl publications (joty, carenini, and ng [1777]2012;
   joty et al. [1778]2013) reported slightly lower parsing accuracies.
   fixing a bug in the parsing algorithm accounts for the difference.

   18    subba and di-eugenio ([1779]2009) report their results based on an
   arbitrary split between training and test sets. because we did not have
   access to their particular split, we compare our model's performance
   based on 10-fold cross validation with their reported results. also,
   because we did not have access to their system/output, we could not
   perform a significance test on the instructional corpus.

   19    because we did not have access to the output or to the system of
   subba and di-eugenio ([1780]2009), we were not able to perform a
   significance test on the instructional corpus.

   20    the performance of this model for intra-sentential parsing is
   reported in [1781]table 4 under the name crf-nc.

   21    text structural features are included in the organizational
   features for multi-sentential parsing.

   22    therefore, the counts of the relations shown in the table may not
   match the ones in the test set.

   23    d., maine in this example refers to democrat from state maine.
   references
   section:
   [choose________________________]
   [1782]previous section [1783]next section
   althaus, ernst, denys duchier, alexander koller, kurt mehlhorn, joachim
   niehren, and sven thiel. 2003. an efficient graph algorithm for
   dominance constraints. journal of algorithms, 48(1):194   219.
   [1784]crossref, [1785]google scholar
   asher, nicholas and alex lascarides, 2003. logics of conversation.
   cambridge university press. [1786]google scholar
   barzilay, regina and michael elhadad. 1997. using lexical chains for
   text summarization. in proceedings of the 35th annual meeting of the
   association for computational linguistics and the 8th european chapter
   meeting of the association for computational linguistics, workshop on
   intelligent scalable test summarization, pages 10   17, madrid.
   [1787]google scholar
   biran, or and owen rambow. 2011. identifying justifications in written
   dialogs by classifying text as argumentative. international journal of
   semantic computing, 5(4):363   381. [1788]crossref, [1789]google scholar
   blair-goldensohn, sasha, kathleen mckeown, and owen rambow. 2007.
   building and refining rhetorical-semantic relation models. in
   proceedings of the human language technologies: the annual conference
   of the north american chapter of the association for computational
   linguistics, hlt-naacl'07, pages 428   435. rochester, ny. [1790]google
   scholar
   blitzer, john. 2008. id20 of natural language processing
   systems. ph.d. thesis, university of pennsylvania. [1791]google scholar
   breiman, leo. 1996. id112 predictors. machine learning,
   24(2):123   140. [1792]crossref, [1793]google scholar
   carlson, lynn and daniel marcu. 2001. discourse tagging reference
   manual. technical report isi-tr-545, university of southern california
   information sciences institute. [1794]google scholar
   carlson, lynn, daniel marcu, and mary ellen okurowski. 2002. rst
   discourse treebank (rst   dt) ldc2002t07. linguistic data consortium,
   philadelphia. [1795]google scholar
   chali, yllias and shafiq joty. 2007. id51 using
   lexical cohesion. in proceedings of semeval-2007, pages 476   479,
   prague. [1796]google scholar
   charniak, eugene. 2000. a maximum-id178-inspired parser. in
   proceedings of the 1st north american chapter of the association for
   computational linguistics conference, naacl'00, pages 132   139, seattle,
   wa. [1797]google scholar
   charniak, eugene and mark johnson. 2005. coarse-to-fine n-best parsing
   and maxent discriminative reranking. in proceedings of the 43rd annual
   meeting of the association for computational linguistics, acl'05, pages
   173   180, ann arbor, mi. [1798]crossref, [1799]google scholar
   christensen, janara, mausam, stephen soderland, and oren etzioni. 2013.
   towards coherent id57. in proceedings of the
   2013 conference of the north american chapter of the association for
   computational linguistics: human language technologies, naacl-hlt'13,
   pages 1163   1173, atlanta, ga. [1800]google scholar
   christensen, janara, stephen soderland, gagan bansal, and mausam. 2014.
   hierarchical summarization: scaling up id57. in
   proceedings of the 52nd annual meeting of the association for
   computational linguistics, acl'13, pages 902   912, baltimore, md.
   [1801]google scholar
   collins, michael. 2003. head-driven statistical models for natural
   language parsing. computational linguistics, 29(4):589   637.
   [1802]link, [1803]google scholar
   collins, michael and terry koo. 2005. discriminative reranking for
   natural language parsing. computational linguistics, 31(1):25   70.
   [1804]link, [1805]google scholar
   collobert, ronan, jason weston, l  on bottou, michael karlen, koray
   kavukcuoglu, and pavel kuksa. 2011. natural language processing
   (almost) from scratch. journal of machine learning research,
   12:2493   2537. [1806]google scholar
   cristea, dan, nancy ide, and laurent romary. 1998. veins theory: a
   model of global discourse cohesion and coherence. in proceedings of the
   36th annual meeting of the association for computational linguistics
   and of the 17th international conference on computational linguistics
   (coling/acl'98), pages 281   285. montreal. [1807]crossref, [1808]google
   scholar
   danlos, laurence. 2009. d-stag: a discourse analysis formalism based on
   synchronous tags. tal, 50(1):111   143. [1809]google scholar
   daum  , iii, hal. 2007. frustratingly easy id20. in
   proceedings of the 45th annual meeting of the association for
   computational linguistics, acl'07, pages 256   263, prague. [1810]google
   scholar
   daum  , iii, hal and daniel marcu. 2002. a noisy-channel model for
   document compression. in proceedings of the 40th annual meeting of the
   association for computational linguistics, acl '02, pages 449   456,
   philadelphia, pa. [1811]google scholar
   dinarelli, marco, alessandro moschitti, and giuseppe riccardi. 2011.
   discriminative reranking for spoken language understanding. ieee
   transactions on audio, speech and language processing (taslp),
   20:526   539. [1812]google scholar
   duverle, david and helmut prendinger. 2009. a novel discourse parser
   based on support vector machine classification. in proceedings of the
   joint conference of the 47th annual meeting of the acl and the 4th
   international joint conference on natural language processing of the
   afnlp, pages 665   673, suntec. [1813]google scholar
   egg, markus, alexander koller, and joachim niehren. 2001. the
   constraint language for lambda structures. journal of logic, language
   and information, 10(4):457   485. [1814]crossref, [1815]google scholar
   eisner, jason. 1996. three new probabilistic models for dependency
   parsing: an exploration. in proceedings of the 16th conference on
   computational linguistics - volume 1, coling '96, pages 340   345,
   copenhagen. [1816]crossref, [1817]google scholar
   fellbaum, christiane. 1998. id138   an electronic lexical database. mit
   press, cambridge, ma. [1818]google scholar
   feng, vanessa and graeme hirst. 2012. text-level discourse parsing with
   rich linguistic features. in proceedings of the 50th annual meeting of
   the association for computational linguistics, acl '12, pages 60   68,
   jeju island. [1819]google scholar
   feng, vanessa and graeme hirst. 2014. a linear-time bottom-up discourse
   parser with constraints and post-editing. in proceedings of the 52nd
   annual meeting of the association for computational linguistics, acl
   '14, pages 511   521, baltimore, md. [1820]google scholar
   finkel, jenny rose, alex kleeman, and christopher manning. 2008.
   efficient, feature-based, conditional random field parsing. in
   proceedings of the 46th annual meeting of the association for
   computational linguistics, acl'08, pages 959   967, columbus, oh.
   [1821]google scholar
   fisher, seeger and brian roark. 2007. the utility of parse-derived
   features for automatic discourse segmentation. in proceedings of the
   45th annual meeting of the association for computational linguistics,
   acl'07, pages 488   495, prague. [1822]google scholar
   galley, michel and kathleen mckeown. 2003. improving word sense
   disambiguation in lexical chaining. in proceedings of the 18th
   international joint conference on artificial intelligence, ijcai'03,
   pages 1486   1488, acapulco. [1823]google scholar
   galley, michel, kathleen mckeown, eric fosler-lussier, and hongyan
   jing. 2003. discourse segmentation of multi-party conversation. in
   proceedings of the 41st annual meeting of the association for
   computational linguistics - volume 1, acl '03, pages 562   569, sapporo.
   [1824]crossref, [1825]google scholar
   ghosh, sucheta, richard johansson, giuseppe riccardi, and sara tonelli.
   2011. shallow discourse parsing with id49. in
   proceedings of the 5th international joint conference on natural
   language processing, ijcnlp'11, pages 1071   1079, chiang mai.
   [1826]google scholar
   gim  nez, jes  s and llu  s m  rquez. 2010. linguistic measures for
   automatic machine translation evaluation. machine translation,
   24(3   4):77   86. [1827]crossref, [1828]google scholar
   guzm  n, francisco, shafiq joty, llu  s m  rquez, alessandro moschitti,
   preslav nakov, and massimo nicosia. 2014a. learning to differentiate
   better from worse translations. in proceedings of the 2014 conference
   on empirical methods in natural language processing (emnlp), pages
   214   220, doha. [1829]crossref, [1830]google scholar
   guzm  n, francisco, shafiq joty, llu  s m  rquez, and preslav nakov.
   2014b. using discourse structure improves machine translation
   evaluation. in proceedings of the 52nd annual meeting of the
   association for computational linguistics (volume 1: long papers),
   pages 687   698, baltimore, md. [1831]crossref, [1832]google scholar
   halliday, michael and ruqaiya hasan. 1976. cohesion in english.
   longman, london. [1833]google scholar
   hardmeier, christian, joakim nivre, and j  rg tiedemann. 2012.
   document-wide decoding for phrase-based statistical machine
   translation. in proceedings of the 2012 joint conference on empirical
   methods in natural language processing and computational natural
   language learning, emnlp-conll '12, pages 1179   1190, jeju island.
   [1834]google scholar
   hernault, hugo, helmut prendinger, david duverle, and mitsuru ishizuka.
   2010. hilda: a discourse parser using support vector machine
   classification. dialogue and discourse, 1(3):1   33.
   [1835]crossref, [1836]google scholar
   hirst, graeme and david st-onge. 1997. lexical chains as representation
   of context for the detection and correction of malapropisms. in
   christiane fellbaum, editor, id138: an electronic lexical database
   and some of its applications. mit press, pages 305   332. [1837]google
   scholar
   hobbs, jerry. 1979. coherence and coreference. cognitive science,
   3:67   90. [1838]crossref, [1839]google scholar
   huang, liang and david chiang. 2005. better k-best parsing. in
   proceedings of the ninth international workshop on parsing technology,
   parsing '05, pages 53   64, stroudsburg, pa. [1840]crossref, [1841]google
   scholar
   ji, yangfeng and jacob eisenstein. 2014. representation learning for
   text-level discourse parsing. in proceedings of the 52nd annual meeting
   of the association for computational linguistics (volume 1: long
   papers), pages 13   24, baltimore, md. [1842]crossref, [1843]google
   scholar
   joty, shafiq, giuseppe carenini, and raymond t. ng. 2012. a novel
   discriminative framework for sentence-level discourse analysis. in
   proceedings of the 2012 joint conference on empirical methods in
   natural language processing and computational natural language
   learning, emnlp-conll '12, pages 904   915, jeju island. [1844]google
   scholar
   joty, shafiq, giuseppe carenini, and raymond t. ng. 2013. topic
   segmentation and labeling in asynchronous conversations. journal of
   artificial intelligence research (jair), 47:521   573. [1845]google
   scholar
   joty, shafiq, giuseppe carenini, raymond t. ng, and yashar mehdad.
   2013. combining intra- and multi-sentential rhetorical parsing for
   document-level discourse analysis. in proceedings of the 51st annual
   meeting of the association for computational linguistics, acl '13,
   pages 486   496, sofia. [1846]google scholar
   joty, shafiq, francisco guzm  n, llu  s m  rquez, and preslav nakov. 2014.
   discotk: using discourse structure for machine translation evaluation.
   in proceedings of the ninth workshop on statistical machine
   translation, wmt '14, pages 402   408, baltimore, md.
   [1847]crossref, [1848]google scholar
   jurafsky, daniel and james martin. 2008. statistical parsing. in speech
   and language processing, chapter 14. prentice hall. [1849]google
   scholar
   knight, kevin and jonathan graehl. 2005. an overview of probabilistic
   tree transducers for natural language processing. in computational
   linguistics and intelligent text processing, volume 3406 of lecture
   notes in computer science. springer, berlin heidelberg, pages 1   24.
   [1850]google scholar
   knott, alistair and robert dale. 1994. using linguistic phenomena to
   motivate a set of coherence relations. discourse processes, 18:35   62.
   [1851]crossref, [1852]google scholar
   koller, alexander, michaela regneri, and stefan thater. 2008. regular
   tree grammars as a formalism for scope underspecification. in
   proceedings of the 46th annual meeting of the association for
   computational linguistics on human language technologies, pages
   218   226, columbus, oh. [1853]google scholar
   lafferty, john, andrew mccallum, and fernando pereira. 2001.
   id49: probabilistic models for segmenting and
   labeling sequence data. in proceedings of the eighteenth international
   conference on machine learning, pages 282   289, san francisco, ca.
   [1854]google scholar
   lazaridou, angeliki, ivan titov, and caroline sporleder. 2013. a
   bayesian model for joint unsupervised induction of sentiment, aspect
   and discourse representations. in proceedings of the 51st annual
   meeting of the association for computational linguistics, acl '13,
   sofia. [1855]google scholar
   li, jiwei, rumeng li, and eduard hovy. 2014. recursive deep models for
   discourse parsing. in proceedings of the 2014 conference on empirical
   methods in natural language processing (emnlp), pages 2061   2069, doha.
   [1856]crossref, [1857]google scholar
   li, sujian, liang wang, ziqiang cao, and wenjie li. 2014. text-level
   discourse id33. in proceedings of the 52nd annual meeting
   of the association for computational linguistics (volume 1: long
   papers), pages 25   35, baltimore, md. [1858]crossref, [1859]google
   scholar
   louis, annie, aravind joshi, and ani nenkova. 2010. discourse
   indicators for content selection in summarization. in proceedings of
   the 11th annual meeting of the special interest group on discourse and
   dialogue, sigdial '10, pages 147   156, tokyo. [1860]google scholar
   mach    ek, matou   and ond  ej bojar. 2014. results of the wmt14 metrics
   shared task. in proceedings of the ninth workshop on statistical
   machine translation, baltimore, md. [1861]google scholar
   magerman, david. 1995. statistical decision-tree models for parsing. in
   proceedings of the 33rd annual meeting of the association for
   computational linguistics, acl'95, pages 276   283, cambridge, ma.
   [1862]crossref, [1863]google scholar
   mann, william and sandra thompson. 1988. rhetorical structure theory:
   toward a functional theory of text organization. text, 8(3):243   281.
   [1864]crossref, [1865]google scholar
   marcu, daniel. 1999. a decision-based approach to rhetorical parsing.
   in proceedings of the 37th annual meeting of the association for
   computational linguistics on computational linguistics, acl'99, pages
   365   372, morristown, nj. [1866]crossref, [1867]google scholar
   marcu, daniel. 2000a. the rhetorical parsing of unrestricted texts: a
   surface-based approach. computational linguistics, 26:395   448.
   [1868]link, [1869]google scholar
   marcu, daniel. 2000b. the theory and practice of discourse parsing and
   summarization. mit press, cambridge, ma. [1870]google scholar
   marcu, daniel and abdessamad echihabi. 2002. an unsupervised approach
   to recognizing discourse relations. in proceedings of the 40th annual
   meeting of the association for computational linguistics, acl'02, pages
   368   375. philadelphia, pa. [1871]google scholar
   marcus, mitchell, mary marcinkiewicz, and beatrice santorini. 1994.
   building a large annotated corpus of english: the id32.
   computational linguistics, 19(2):313   330. [1872]google scholar
   martin, james, 1992. english text: system and structure. john benjamins
   publishing company, philadelphia/amsterdam. [1873]google scholar
   maslennikov, mstislav and tat-seng chua. 2007. a multi-resolution
   framework for information extraction from free text. in proceedings of
   the 45th annual meeting of the association for computational
   linguistics, pages 592   599, prague. [1874]google scholar
   mccallum, andrew. 2002. mallet: a machine learning for language
   toolkit. [1875]http://mallet.cs.umass.edu. [1876]google scholar
   mccallum, andrew, dayne freitag, and fernando c. n. pereira. 2000.
   maximum id178 markov models for information extraction and
   segmentation. in proceedings of the seventeenth international
   conference on machine learning, icml '00, pages 591   598, san francisco,
   ca. [1877]google scholar
   mcdonald, ryan, koby crammer, and fernando pereira. 2005. online
   large-margin training of dependency parsers. in proceedings of the 43rd
   annual meeting of the association for computational linguistics, acl
   '05, pages 91   98, ann arbor, mi. [1878]crossref, [1879]google scholar
   mcdonald, ryan, fernando pereira, kiril ribarov, and jan haji  . 2005.
   non-projective id33 using spanning tree algorithms. in
   proceedings of the conference on human language technology and
   empirical methods in natural language processing, hlt '05, pages
   523   530, stroudsburg, pa. [1880]crossref, [1881]google scholar
   morris, jane and graeme hirst. 1991. lexical cohesion computed by
   thesaural relations as an indicator of structure of text. computational
   linguistics, 17(1):21   48. [1882]google scholar
   murphy, kevin. 2012. machine learning: a probabilistic perspective. the
   mit press. cambridge, ma. [1883]google scholar
   pang, bo and lillian lee. 2004. a sentimental education: sentiment
   analysis using subjectivity summarization based on minimum cuts. in
   proceedings of the 42nd annual meeting of the association for
   computational linguistics, acl '04, pages 271   278. barcelona.
   [1884]crossref, [1885]google scholar
   pitler, emily and ani nenkova. 2009. using syntax to disambiguate
   explicit discourse connectives in text. in proceedings of the
   acl-ijcnlp 2009 conference short papers, aclshort '09, pages 13   16,
   suntec. [1886]google scholar
   poole, david and alan mackworth, 2010. artificial intelligence:
   foundations of computational agents. cambridge university press.
   [1887]google scholar
   prasad, rashmi, nikhil dinesh, alan lee, eleni miltsakaki, livio
   robaldo, aravind joshi, and bonnie webber. 2008. the penn discourse
   treebank 2.0. in proceedings of the sixth international conference on
   language resources and evaluation (lrec), pages 2961   2968, marrakech.
   [1888]google scholar
   prasad, rashmi, aravind joshi, nikhil dinesh, alan lee, eleni
   miltsakaki, and bonnie webber. 2005. the penn discourse treebank as a
   resource for id86. in proceedings of the corpus
   linguistics workshop on using corpora for id86,
   pages 25   32, birmingham. [1889]google scholar
   regneri, michaela, markus egg, and alexander koller. 2008. efficient
   processing of underspecified discourse representations. in proceedings
   of the 46th annual meeting of the association for computational
   linguistics on human language technologies: short papers, hlt-short
   '08, pages 245   248, columbus, oh. [1890]crossref, [1891]google scholar
   reyle, uwe. 1993. dealing with ambiguities by underspecification:
   construction, representation and deduction. journal of semantics,
   10(2):123   179. [1892]crossref, [1893]google scholar
   schapire, robert e. and yoram singer. 2000. boostexter: a
   boosting-based system for text categorization. machine learning,
   39(2   3):135   168. [1894]crossref, [1895]google scholar
   schauer, holger and udo hahn. 2001. anaphoric cues for coherence
   relations. in proceedings of the conference on recent advances in
   natural language processing, ranlp '01, pages 228   234, tzigov chark.
   [1896]google scholar
   schilder, frank. 2002. robust discourse parsing via discourse markers,
   topicality and position. natural language engineering, 8(3):235   255.
   [1897]google scholar
   sha, fei and fernando pereira. 2003. id66 with conditional
   random fields. in proceedings of the 2003 conference of the north
   american chapter of the association for computational linguistics on
   human language technology - volume 1, naacl-hlt'03, pages 134   141,
   edmonton. [1898]crossref, [1899]google scholar
   silber, gregory and kathleen mccoy. 2002. efficiently computed lexical
   chains as an intermediate representation for automatic text
   summarization. computational linguistics, 28(4):487   496.
   [1900]link, [1901]google scholar
   smith, noah a. 2011. linguistic structure prediction. synthesis
   lectures on human language technologies. morgan and claypool.
   [1902]google scholar
   socher, richard, john bauer, christopher d. manning, and ng andrew y.
   2013a. parsing with compositional vector grammars. in proceedings of
   the 51st annual meeting of the association for computational
   linguistics (volume 1: long papers), pages 455   465, sofia. [1903]google
   scholar
   socher, richard, alex perelygin, jean wu, jason chuang, christopher d.
   manning, andrew ng, and christopher potts. 2013b. recursive deep models
   for semantic compositionality over a sentiment treebank. in proceedings
   of the 2013 conference on empirical methods in natural language
   processing, pages 1631   1642, seattle, wa. [1904]google scholar
   somasundaran, s. 2010. discourse-level relations for opinion analysis.
   ph.d. thesis, university of pittsburgh, pa. [1905]google scholar
   soricut, radu and daniel marcu. 2003. sentence level discourse parsing
   using syntactic and lexical information. in proceedings of the 2003
   conference of the north american chapter of the association for
   computational linguistics on human language technology - volume 1,
   naacl'03, pages 149   156, edmonton. [1906]crossref, [1907]google scholar
   sporleder, caroline. 2007. manually vs. automatically labelled data in
   discourse relation classification. effects of example and feature
   selection. ldv forum, 22(1):1   20. [1908]google scholar
   sporleder, caroline and mirella lapata. 2004. automatic paragraph
   identification: a study across languages and domains. in proceedings of
   the 2004 conference on empirical methods in natural language
   processing, emnlp '04, pages 72   79, barcelona. [1909]google scholar
   sporleder, caroline and mirella lapata. 2005. discourse chunking and
   its application to sentence compression. in proceedings of the
   conference on human language technology and empirical methods in
   natural language processing, hlt-emnlp'05, pages 257   264, vancouver.
   [1910]crossref, [1911]google scholar
   sporleder, caroline and alex lascarides. 2005. exploiting linguistic
   cues to classify rhetorical relations. in proceedings of recent
   advances in natural language processing (ranlp), pages 157   166,
   bulgaria. [1912]google scholar
   sporleder, caroline and alex lascarides. 2008. using automatically
   labelled examples to classify rhetorical relations: an assessment.
   natural language engineering, 14(3):369   416.
   [1913]crossref, [1914]google scholar
   stede, manfred. 2004. the potsdam commentary corpus. in proceedings of
   the acl-04 workshop on discourse annotation, pages 96   102, barcelona.
   [1915]google scholar
   stede, manfred. 2011. discourse processing. synthesis lectures on human
   language technologies. morgan and claypool publishers. [1916]google
   scholar
   subba, rajen and barbara di-eugenio. 2009. an effective discourse
   parser that uses rich linguistic information. in proceedings of human
   language technologies: the 2009 annual conference of the north american
   chapter of the association for computational linguistics, hlt-naacl'09,
   pages 566   574, boulder, co. [1917]crossref, [1918]google scholar
   sutton, charles and andrew mccallum. 2012. an introduction to
   id49. foundations and trends in machine learning,
   4(4):267   373. [1919]crossref, [1920]google scholar
   sutton, charles, andrew mccallum, and khashayar rohanimanesh. 2007.
   dynamic id49: factorized probabilistic models for
   labeling and segmenting sequence data. journal of machine learning
   research (jmlr), 8:693   723. [1921]google scholar
   taboada, maite. 2006. discourse markers as signals (or not) of
   rhetorical relations. journal of pragmatics, 38(4):567   592.
   [1922]crossref, [1923]google scholar
   taboada, maite, julian brooke, milan tofiloski, kimberly voll, and
   manfred stede. 2011. lexicon-based methods for id31.
   computational linguistics, 37(2):267   307. [1924]link, [1925]google
   scholar
   taboada, maite and william c. mann. 2006. rhetorical structure theory:
   looking back and moving ahead. discourse studies, 8(3):423   459.
   [1926]crossref, [1927]google scholar
   teufel, simone and marc moens. 2002. summarizing scientific articles:
   experiments with relevance and rhetorical status. computational
   linguistics, 28(4):409   445. [1928]link, [1929]google scholar
   verberne, suzan, lou boves, nelleke oostdijk, and peter-arno coppen.
   2007. evaluating discourse-based answer extraction for why-question
   answering. in proceedings of the 30th annual international acm sigir
   conference on research and development in information retrieval,
   sigir'07, pages 735   736, amsterdam. [1930]crossref, [1931]google
   scholar
   vliet, nynke and gisela redeker. 2011. complex sentences as leaky units
   in discourse parsing. in proceedings of constraints in discourse, pages
   1   9, agay   saint raphael. [1932]google scholar
   webber, b. 2004. d-ltag: extending lexicalized tag to discourse.
   cognitive science, 28(5):751   779. [1933]crossref, [1934]google scholar
   webber, bonnie, andrei popescu-belis, katja markert, and j  rg
   tiedemann, editors. 2013. proceedings of the workshop on discourse in
   machine translation. acl, sofia. [1935]google scholar
   wick, michael, khashayar rohanimanesh, kedare bellare, aron culotta,
   and andrew mccallum. 2011. samplerank: training factor graphs with
   atomic gradients. in proceedings of the 28th international conference
   on machine learning, icml'11, pages 777   784. bellevue, wa. [1936]google
   scholar
   wolf, florian and edward gibson. 2005. representing discourse
   coherence: a corpus-based study. computational linguistics, 31:249   288.
   [1937]link, [1938]google scholar
   zhang, yuan, tao lei, regina barzilay, tommi jaakkola, and amir
   globerson. 2014. steps to excellence: simple id136 with refined
   scoring of dependency trees. in proceedings of the 52nd annual meeting
   of the association for computational linguistics, pages 197   207,
   baltimore, md. [1939]google scholar
   shafiq joty*
   qatar computing research institute
   giuseppe carenini**
   university of british columbia
   raymond t. ng   
   university of british columbia

   *arabic language technologies, qatar computing research institute,
   qatar foundation, doha, qatar. e-mail: [1940][email protected].

   **computer science department, university of british columbia,
   vancouver, bc, canada, v6t 1z4. e-mail: [1941][email protected].

      computer science department, university of british columbia,
   vancouver, bc, canada, v6t 1z4. e-mail: [1942][email protected].
   [1943]favorite [favorite-1488499760477.svg]
   track citations [notify-me-alert-1488499750013.svg]
   [1944]download citation [download2-1490507427013.svg]
   [1945]rss toc [rss-1488924357683.svg]

   [1946]rss citation [rss-1488924357683.svg]
   [1947]submit your article
   [1948]support oa at mitp [open-access-1493356222797.svg]

   [1949][mitpress-logo-main-1483476130433.svg]
     * [1950]journals [1951]books
     * [1952]terms & conditions
     * [1953]privacy statement
     * [1954]contact us

     * us
          + one rogers street cambridge ma 02142-1209
     * uk
          + suite 2, 1 duchess street london, w1w 6an, uk
     * connect
          + [1955]facebook
          + [1956]twitter
          + [1957]google +
          + [1958]pinterest
          + [1959]instagram
          + [1960]youtube
     *
          +    2019 the mit press
          + technology partner:[1961] atypon systems, inc.
          + [1962]crossref member
          + [1963]counter member
          + the mit press colophon is registered in the u.s. patent and
            trademark office.
          + [1964]site help

references

   visible links
   1. https://doi.org/10.1162/coli_a_00226
   2. https://doi.org/10.1162/coli_a_00226
   3. https://doi.org/10.1162/coli_a_00226
   4. https://www.googletagmanager.com/ns.html?id=gtm-tr6twrh
   5. https://www.googletagmanager.com/ns.html?id=gtm-tr6twrh
   6. https://www.mitpressjournals.org/action/showlogin
   7. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
   8. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
   9. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
  10. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
  11. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
  12. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
  13. https://www.mitpressjournals.org/action/requestusername
  14. https://www.mitpressjournals.org/action/requestresetpassword
  15. https://www.mitpressjournals.org/
  16. http://www.mitpressjournals.org/
  17. https://mitpress.mit.edu/
  18. https://www.mitpressjournals.org/action/showpublications
  19. https://www.mitpressjournals.org/digital
  20. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
  21. https://www.mitpressjournals.org/librarians
  22. https://www.mitpressjournals.org/action/institutionadminactivation
  23. https://www.mitpressjournals.org/pb-assets/pdfs/journals institutional license-1548442853093.pdf
  24. https://www.mitpressjournals.org/action/showinstitutionusagereport
  25. https://www.mitpressjournals.org/document_delivery
  26. https://www.mitpressjournals.org/pb-assets/pdfs/vpat_mitp_journals-1507061346287.pdf
  27. https://www.mitpressjournals.org/subscribe
  28. https://www.mitpressjournals.org/inst_getting_started
  29. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
  30. https://www.mitpressjournals.org/pb-assets/pdfs/2019 mitpj institution price list-1541020584043.pdf
  31. https://www.mitpressjournals.org/pb-assets/pdfs/2019 mitpj package pricelist-1541020593767.pdf
  32. https://www.mitpressjournals.org/pb-assets/pdfs/2018 mitpj single issue price list-1513815652483.pdf
  33. https://www.mitpressjournals.org/for_authors
  34. https://www.mitpressjournals.org/for_authors#subpubagreements
  35. https://www.mitpressjournals.org/for_authors#authorposting
  36. https://www.mitpressjournals.org/for_authors#copyright
  37. https://www.mitpressjournals.org/for_authors#authorreprints
  38. https://www.mitpressjournals.org/for_authors#publishingoa
  39. https://www.mitpressjournals.org/for_authors#nihpublicaccess
  40. https://www.mitpressjournals.org/for_authors#authordiscounts
  41. https://www.mitpressjournals.org/for_authors#kudos
  42. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
  43. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
  44. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
  45. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
  46. https://www.mitpressjournals.org/rights_permission
  47. https://mitpress.mit.edu/giving/
  48. https://www.mitpressjournals.org/trade_sales
  49. https://www.mitpressjournals.org/advertising
  50. https://www.mitpressjournals.org/schedule
  51. https://www.mitpressjournals.org/faq
  52. https://www.mitpressjournals.org/terminated_journals
  53. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
  54. https://www.mitpressjournals.org/aboutmitpj
  55. https://www.mitpressjournals.org/ethics
  56. https://www.mitpressjournals.org/events
  57. https://www.mitpressjournals.org/publishing_services
  58. https://www.mitpressjournals.org/mitpj-staff
  59. https://www.mitpressjournals.org/analytics
  60. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
  61. https://www.facebook.com/mitpress
  62. http://www.twitter.com/mitpress
  63. https://plus.google.com/106848724929282487337?prsrc=3
  64. https://www.pinterest.com/mitpress/
  65. https://www.instagram.com/mitpress/
  66. https://www.youtube.com/channel/uceh0hmlpjgw2dn0ntmd0fcq
  67. https://www.mitpressjournals.org/contact_info
  68. https://mitpressjournals.mit.edu/shop/customer/account/
  69. https://www.mitpressjournals.org/cdn-cgi/l/email-protection#f09a9f85829e919c83dd919483b09d9984de959485
  70. https://www.mitpressjournals.org/cdn-cgi/l/email-protection#3c5653494e525d504f1155525a537c51554812595849
  71. https://www.mitpressjournals.org/cdn-cgi/l/email-protection#a5cfcad0d7cbc4c9d688d7ccc2cdd1d6e5c8ccd18bc0c1d0
  72. https://www.mitpressjournals.org/cdn-cgi/l/email-protection#452f2135263668322c37203605282c316b202130
  73. https://www.mitpressjournals.org/cdn-cgi/l/email-protection#dfb5bbafbcacf2b3b6bcbab1acbaac9fb2b6abf1babbaa
  74. https://www.mitpressjournals.org/cdn-cgi/l/email-protection#9df7f2e8eff3fcf1eeb0feeeddf0f4e9b3f8f9e8
  75. https://www.mitpressjournals.org/cdn-cgi/l/email-protection#274d48525549464b540a464444425454674a4e5309424352
  76. https://www.mitpressjournals.org/cdn-cgi/l/email-protection#adc3c1c4c3c9deccd4edc0c4d983c8c9d8
  77. https://www.mitpressjournals.org/action/showlogin?uri=/doi/10.1162/coli_a_00226
  78. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
  79. https://www.mitpressjournals.org/search/advanced
  80. https://www.mitpressjournals.org/
  81. https://www.mitpressjournals.org/loi/coli
  82. https://www.mitpressjournals.org/loi/coli
  83. https://www.mitpressjournals.org/toc/coli/41/3
  84. http://www.mitpressjournals.org/doi/abs/10.1162/coli_a_00225
  85. http://www.mitpressjournals.org/doi/abs/10.1162/coli_a_00227
  86. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
  87. https://www.mitpressjournals.org/journals/coli/editorial
  88. https://www.mitpressjournals.org/journals/coli/aandi
  89. https://www.mitpressjournals.org/schedule
  90. https://www.mitpressjournals.org/advertising
  91. http://cljournal.org/
  92. https://www.mitpressjournals.org/pb-assets/pdfs/coli-permission-form_sept2018-1536787440293.pdf
  93. http://www.mitpressjournals.org/for_authors#authorreprints
  94. https://www.mitpressjournals.org/rights_permission
  95. https://www.mitpressjournals.org/action/showmostreadarticles?journalcode=coli
  96. https://www.mitpressjournals.org/action/showmostcitedarticles?journalcode=coli
  97. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
  98. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
  99. https://creativecommons.org/licenses/by-nc-nd/4.0/
 100. https://giving.mit.edu/taxonomy/term/79#3920880
 101. https://www.mitpressjournals.org/author/joty,+shafiq
 102. https://www.mitpressjournals.org/author/carenini,+giuseppe
 103. https://www.mitpressjournals.org/author/ng,+raymond+t
 104. https://doi.org/10.1162/coli_a_00226
 105. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 106. https://www.mitpressjournals.org/author/joty,+shafiq
 107. https://www.mitpressjournals.org/author/carenini,+giuseppe
 108. https://www.mitpressjournals.org/author/ng,+raymond+t
 109. https://doi.org/10.1162/coli_a_00226
 110. https://www.mitpressjournals.org/doi/full/10.1162/coli_a_00226
 111. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#authors-content
 112. http://www.mitpressjournals.org/doi/pdf/10.1162/coli_a_00226
 113. http://www.mitpressjournals.org/doi/pdfplus/10.1162/coli_a_00226
 114. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i1
 115. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#abstract
 116. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i4
 117. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 118. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 119. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 120. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 121. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 122. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 123. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 124. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 125. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 126. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 127. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 128. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 129. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 130. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 131. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 132. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 133. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 134. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 135. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 136. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 137. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 138. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 139. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 140. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 141. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 142. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 143. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 144. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 145. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 146. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 147. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 148. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 149. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 150. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 151. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 152. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 153. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 154. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 155. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 156. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 157. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 158. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 159. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 160. javascript:popref('s2')
 161. javascript:popref('s3')
 162. javascript:popref('s4')
 163. javascript:popref('s5')
 164. javascript:popref('s6')
 165. javascript:popref('s7')
 166. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i1
 167. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i7
 168. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 169. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 170. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 171. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 172. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 173. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 174. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 175. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 176. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 177. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 178. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 179. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 180. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 181. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 182. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn4
 183. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 184. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 185. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 186. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 187. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 188. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 189. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 190. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 191. javascript:popref('s1')
 192. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 193. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 194. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn5
 195. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 196. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 197. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 198. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 199. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 200. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 201. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 202. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 203. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 204. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 205. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 206. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 207. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 208. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn6
 209. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 210. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 211. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 212. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 213. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 214. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 215. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 216. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 217. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 218. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 219. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 220. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 221. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 222. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 223. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 224. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 225. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 226. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 227. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 228. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i4
 229. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i12
 230. javascript:popref('s9')
 231. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn7
 232. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn8
 233. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 234. javascript:popref('s5')
 235. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 236. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 237. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 238. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 239. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 240. javascript:popref('s2')
 241. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 242. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 243. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 244. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 245. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 246. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 247. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn9
 248. javascript:popref('s4a1')
 249. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 250. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 251. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 252. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn10
 253. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 254. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 255. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 256. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 257. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 258. javascript:popref('s4c')
 259. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i7
 260. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i58
 261. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 262. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 263. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 264. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 265. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 266. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 267. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 268. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 269. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 270. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 271. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 272. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 273. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 274. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 275. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 276. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 277. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 278. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 279. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 280. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 281. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 282. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 283. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 284. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 285. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 286. javascript:popref('s4b')
 287. javascript:popref('s4c')
 288. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 289. javascript:popref('e1')
 290. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 291. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 292. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 293. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 294. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 295. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 296. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 297. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 298. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 299. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 300. javascript:popref('t1')
 301. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 302. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 303. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 304. javascript:popref('t1')
 305. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 306. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 307. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 308. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 309. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 310. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn11
 311. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 312. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 313. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 314. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 315. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 316. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 317. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 318. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 319. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 320. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 321. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 322. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 323. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 324. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 325. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 326. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 327. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 328. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 329. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 330. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 331. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 332. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 333. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 334. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 335. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 336. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 337. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 338. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 339. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 340. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 341. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 342. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 343. javascript:popref('s4a3')
 344. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 345. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 346. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 347. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 348. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 349. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 350. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 351. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 352. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 353. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 354. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 355. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn12
 356. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 357. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 358. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn13
 359. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 360. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 361. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 362. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 363. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 364. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 365. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 366. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 367. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 368. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 369. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 370. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 371. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 372. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 373. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i12
 374. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i63
 375. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 376. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 377. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 378. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 379. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 380. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 381. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 382. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 383. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 384. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 385. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 386. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn14
 387. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i58
 388. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i108
 389. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 390. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 391. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 392. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 393. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 394. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 395. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 396. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 397. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 398. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 399. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 400. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 401. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 402. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 403. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 404. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 405. javascript:popref('t2')
 406. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 407. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn15
 408. javascript:popref('t2')
 409. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 410. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 411. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 412. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 413. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 414. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 415. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 416. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 417. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 418. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 419. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 420. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 421. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn16
 422. javascript:popref('t3')
 423. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 424. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 425. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn17
 426. javascript:popref('s6a')
 427. javascript:popref('t3')
 428. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 429. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 430. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 431. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 432. javascript:popref('s6c1')
 433. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 434. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 435. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 436. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 437. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn18
 438. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 439. javascript:popref('t4')
 440. javascript:popref('s6b2')
 441. javascript:popref('t4')
 442. javascript:popref('t4')
 443. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 444. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn19
 445. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn20
 446. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 447. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn21
 448. javascript:popref('t5')
 449. javascript:popref('t5')
 450. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 451. javascript:popref('t4')
 452. javascript:popref('t5')
 453. javascript:popref('t3')
 454. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 455. javascript:popref('s4c')
 456. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 457. javascript:popref('t6')
 458. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 459. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 460. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn22
 461. javascript:popref('t6')
 462. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 463. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 464. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn23
 465. javascript:popref('t6')
 466. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 467. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 468. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 469. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 470. javascript:popref('s4c2')
 471. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 472. javascript:popref('s4b')
 473. javascript:popref('t2')
 474. javascript:popref('t7')
 475. javascript:popref('t7')
 476. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 477. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 478. javascript:popref('t8')
 479. javascript:popref('s6b2')
 480. javascript:popref('t8')
 481. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 482. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 483. javascript:popref('t9')
 484. javascript:popref('s4a4')
 485. javascript:popref('t9')
 486. javascript:popref('t9')
 487. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 488. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 489. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn24
 490. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 491. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 492. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 493. javascript:popref('s6d2')
 494. javascript:popref('s6d3')
 495. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 496. javascript:popref('t2')
 497. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn25
 498. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 499. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 500. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn26
 501. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 502. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 503. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 504. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 505. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 506. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 507. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 508. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i63
 509. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i109
 510. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 511. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 512. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 513. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 514. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 515. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 516. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 517. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 518. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 519. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 520. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 521. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 522. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 523. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 524. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 525. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 526. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 527. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i108
 528. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i110
 529. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 530. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 531. javascript:popref('s4b')
 532. javascript:popref('s6d4')
 533. javascript:popref('s6d2')
 534. javascript:popref('s6d3')
 535. javascript:popref('s6d5')
 536. javascript:popref('s6d6')
 537. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i109
 538. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i112
 539. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i110
 540. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i113
 541. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i112
 542. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i114
 543. http://www.isi.edu/licensed-sw/spade/
 544. http://nlp.prendingerlab.net/hilda/
 545. http://109.228.0.153/discourse_parser_demo/
 546. http://alt.qcri.org/tools/
 547. javascript:popref('s9')
 548. http://www.bbc.co.uk/news/world-asia-26106490
 549. http://cogcomp.cs.illinois.edu/page/software
 550. http://alt.qcri.org/tools/
 551. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 552. javascript:popref('t4')
 553. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 554. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 555. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 556. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 557. javascript:popref('t4')
 558. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i113
 559. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#citart1
 560. https://www.mitpressjournals.org/servlet/linkout?suffix=r1&dbid=16&doi=10.1162/coli_a_00226&key=10.1016/s0196-6774(03)00050-6
 561. http://scholar.google.com/scholar?hl=en&q=althaus,+ernst,+denys+duchier,+alexander+koller,+kurt+mehlhorn,+joachim+niehren,+and+sven+thiel.+2003.++an+efficient+graph+algorithm+for+dominance+constraints.+journal+of+algorithms,+48(1):194   219.
 562. http://scholar.google.com/scholar?hl=en&q=asher,+nicholas+and+alex+lascarides,+2003.+logics+of+conversation.+cambridge+university+press.
 563. http://scholar.google.com/scholar?hl=en&q=barzilay,+regina+and+michael+elhadad.+1997.++using+lexical+chains+for+text+summarization.+in+proceedings+of+the+35th+annual+meeting+of+the+association+for+computational+linguistics+and+the+8th+european+chapter+meeting+of+the+association+for+computational+linguistics,+workshop+on+intelligent+scalable+test+summarization,+pages+10   17,+madrid.
 564. https://www.mitpressjournals.org/servlet/linkout?suffix=r4&dbid=16&doi=10.1162/coli_a_00226&key=10.1142/s1793351x11001328
 565. http://scholar.google.com/scholar?hl=en&q=biran,+or+and+owen+rambow.+2011.++identifying+justifications+in+written+dialogs+by+classifying+text+as+argumentative.+international+journal+of+semantic+computing,+5(4):363   381.
 566. http://scholar.google.com/scholar?hl=en&q=blair-goldensohn,+sasha,+kathleen+mckeown,+and+owen+rambow.+2007.++building+and+refining+rhetorical-semantic+relation+models.+in+proceedings+of+the+human+language+technologies:+the+annual+conference+of+the+north+american+chapter+of+the+association+for+computational+linguistics,+hlt-naacl'07,+pages+428   435.+rochester,+ny.
 567. http://scholar.google.com/scholar?hl=en&q=blitzer,+john.+2008.+domain+adaptation+of+natural+language+processing+systems.+ph.d.+thesis,+university+of+pennsylvania.
 568. https://www.mitpressjournals.org/servlet/linkout?suffix=r7&dbid=16&doi=10.1162/coli_a_00226&key=10.1007/bf00058655
 569. http://scholar.google.com/scholar?hl=en&q=breiman,+leo.+1996.++id112+predictors.+machine+learning,+24(2):123   140.
 570. http://scholar.google.com/scholar?hl=en&q=carlson,+lynn+and+daniel+marcu.+2001.++discourse+tagging+reference+manual.+technical+report+isi-tr-545,+university+of+southern+california+information+sciences+institute.
 571. http://scholar.google.com/scholar?hl=en&q=carlson,+lynn,+daniel+marcu,+and+mary+ellen+okurowski.+2002.++rst+discourse+treebank+(rst   dt)+ldc2002t07.+linguistic+data+consortium,+philadelphia.
 572. http://scholar.google.com/scholar?hl=en&q=chali,+yllias+and+shafiq+joty.+2007.++word+sense+disambiguation+using+lexical+cohesion.+in+proceedings+of+semeval-2007,+pages+476   479,+prague.
 573. http://scholar.google.com/scholar?hl=en&q=charniak,+eugene.+2000.++a+maximum-id178-inspired+parser.+in+proceedings+of+the+1st+north+american+chapter+of+the+association+for+computational+linguistics+conference,+naacl'00,+pages+132   139,+seattle,+wa.
 574. https://www.mitpressjournals.org/servlet/linkout?suffix=r12&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/1219840.1219862
 575. http://scholar.google.com/scholar?hl=en&q=charniak,+eugene+and+mark+johnson.+2005.++coarse-to-fine+n-best+parsing+and+maxent+discriminative+reranking.+in+proceedings+of+the+43rd+annual+meeting+of+the+association+for+computational+linguistics,+acl'05,+pages+173   180,+ann+arbor,+mi.
 576. http://scholar.google.com/scholar?hl=en&q=christensen,+janara,+mausam,+stephen+soderland,+and+oren+etzioni.+2013.++towards+coherent+multi-document+summarization.+in+proceedings+of+the+2013+conference+of+the+north+american+chapter+of+the+association+for+computational+linguistics:+human+language+technologies,+naacl-hlt'13,+pages+1163   1173,+atlanta,+ga.
 577. http://scholar.google.com/scholar?hl=en&q=christensen,+janara,+stephen+soderland,+gagan+bansal,+and+mausam.+2014.++hierarchical+summarization:+scaling+up+multi-document+summarization.+in+proceedings+of+the+52nd+annual+meeting+of+the+association+for+computational+linguistics,+acl'13,+pages+902   912,+baltimore,+md.
 578. https://www.mitpressjournals.org/doi/10.1162/089120103322753356
 579. http://scholar.google.com/scholar?hl=en&q=collins,+michael.+2003.++head-driven+statistical+models+for+natural+language+parsing.+computational+linguistics,+29(4):589   637.
 580. https://www.mitpressjournals.org/doi/10.1162/0891201053630273
 581. http://scholar.google.com/scholar?hl=en&q=collins,+michael+and+terry+koo.+2005.++discriminative+reranking+for+natural+language+parsing.+computational+linguistics,+31(1):25   70.
 582. http://scholar.google.com/scholar?hl=en&q=collobert,+ronan,+jason+weston,+l  on+bottou,+michael+karlen,+koray+kavukcuoglu,+and+pavel+kuksa.+2011.++natural+language+processing+(almost)+from+scratch.+journal+of+machine+learning+research,+12:2493   2537.
 583. https://www.mitpressjournals.org/servlet/linkout?suffix=r18&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/980845.980891
 584. http://scholar.google.com/scholar?hl=en&q=cristea,+dan,+nancy+ide,+and+laurent+romary.+1998.++veins+theory:+a+model+of+global+discourse+cohesion+and+coherence.+in+proceedings+of+the+36th+annual+meeting+of+the+association+for+computational+linguistics+and+of+the+17th+international+conference+on+computational+linguistics+(coling/acl'98),+pages+281   285.+montreal.
 585. http://scholar.google.com/scholar?hl=en&q=danlos,+laurence.+2009.++d-stag:+a+discourse+analysis+formalism+based+on+synchronous+tags.+tal,+50(1):111   143.
 586. http://scholar.google.com/scholar?hl=en&q=daum  ,+iii,+hal.+2007.++frustratingly+easy+domain+adaptation.+in+proceedings+of+the+45th+annual+meeting+of+the+association+for+computational+linguistics,+acl'07,+pages+256   263,+prague.
 587. http://scholar.google.com/scholar?hl=en&q=daum  ,+iii,+hal+and+daniel+marcu.+2002.++a+noisy-channel+model+for+document+compression.+in+proceedings+of+the+40th+annual+meeting+of+the+association+for+computational+linguistics,+acl+'02,+pages+449   456,+philadelphia,+pa.
 588. http://scholar.google.com/scholar?hl=en&q=dinarelli,+marco,+alessandro+moschitti,+and+giuseppe+riccardi.+2011.++discriminative+reranking+for+spoken+language+understanding.+ieee+transactions+on+audio,+speech+and+language+processing+(taslp),+20:526   539.
 589. http://scholar.google.com/scholar?hl=en&q=duverle,+david+and+helmut+prendinger.+2009.++a+novel+discourse+parser+based+on+support+vector+machine+classification.+in+proceedings+of+the+joint+conference+of+the+47th+annual+meeting+of+the+acl+and+the+4th+international+joint+conference+on+natural+language+processing+of+the+afnlp,+pages+665   673,+suntec.
 590. https://www.mitpressjournals.org/servlet/linkout?suffix=r24&dbid=16&doi=10.1162/coli_a_00226&key=10.1023/a:1017964622902
 591. http://scholar.google.com/scholar?hl=en&q=egg,+markus,+alexander+koller,+and+joachim+niehren.+2001.++the+constraint+language+for+lambda+structures.+journal+of+logic,+language+and+information,+10(4):457   485.
 592. https://www.mitpressjournals.org/servlet/linkout?suffix=r25&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/992628.992688
 593. http://scholar.google.com/scholar?hl=en&q=eisner,+jason.+1996.++three+new+probabilistic+models+for+dependency+parsing:+an+exploration.+in+proceedings+of+the+16th+conference+on+computational+linguistics+-+volume+1,+coling+'96,+pages+340   345,+copenhagen.
 594. http://scholar.google.com/scholar?hl=en&q=fellbaum,+christiane.+1998.+id138   an+electronic+lexical+database.+mit+press,+cambridge,+ma.
 595. http://scholar.google.com/scholar?hl=en&q=feng,+vanessa+and+graeme+hirst.+2012.++text-level+discourse+parsing+with+rich+linguistic+features.+in+proceedings+of+the+50th+annual+meeting+of+the+association+for+computational+linguistics,+acl+'12,+pages+60   68,+jeju+island.
 596. http://scholar.google.com/scholar?hl=en&q=feng,+vanessa+and+graeme+hirst.+2014.++a+linear-time+bottom-up+discourse+parser+with+constraints+and+post-editing.+in+proceedings+of+the+52nd+annual+meeting+of+the+association+for+computational+linguistics,+acl+'14,+pages+511   521,+baltimore,+md.
 597. http://scholar.google.com/scholar?hl=en&q=finkel,+jenny+rose,+alex+kleeman,+and+christopher+manning.+2008.++efficient,+feature-based,+conditional+random+field+parsing.+in+proceedings+of+the+46th+annual+meeting+of+the+association+for+computational+linguistics,+acl'08,+pages+959   967,+columbus,+oh.
 598. http://scholar.google.com/scholar?hl=en&q=fisher,+seeger+and+brian+roark.+2007.++the+utility+of+parse-derived+features+for+automatic+discourse+segmentation.+in+proceedings+of+the+45th+annual+meeting+of+the+association+for+computational+linguistics,+acl'07,+pages+488   495,+prague.
 599. http://scholar.google.com/scholar?hl=en&q=galley,+michel+and+kathleen+mckeown.+2003.++improving+word+sense+disambiguation+in+lexical+chaining.+in+proceedings+of+the+18th+international+joint+conference+on+artificial+intelligence,+ijcai'03,+pages+1486   1488,+acapulco.
 600. https://www.mitpressjournals.org/servlet/linkout?suffix=r32&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/1075096.1075167
 601. http://scholar.google.com/scholar?hl=en&q=galley,+michel,+kathleen+mckeown,+eric+fosler-lussier,+and+hongyan+jing.+2003.++discourse+segmentation+of+multi-party+conversation.+in+proceedings+of+the+41st+annual+meeting+of+the+association+for+computational+linguistics+-+volume+1,+acl+'03,+pages+562   569,+sapporo.
 602. http://scholar.google.com/scholar?hl=en&q=ghosh,+sucheta,+richard+johansson,+giuseppe+riccardi,+and+sara+tonelli.+2011.++shallow+discourse+parsing+with+conditional+random+fields.+in+proceedings+of+the+5th+international+joint+conference+on+natural+language+processing,+ijcnlp'11,+pages+1071   1079,+chiang+mai.
 603. https://www.mitpressjournals.org/servlet/linkout?suffix=r34&dbid=16&doi=10.1162/coli_a_00226&key=10.1007/s10590-011-9088-7
 604. http://scholar.google.com/scholar?hl=en&q=gim  nez,+jes  s+and+llu  s+m  rquez.+2010.++linguistic+measures+for+automatic+machine+translation+evaluation.+machine+translation,+24(3   4):77   86.
 605. https://www.mitpressjournals.org/servlet/linkout?suffix=r35&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/v1/d14-1027
 606. http://scholar.google.com/scholar?hl=en&q=guzm  n,+francisco,+shafiq+joty,+llu  s+m  rquez,+alessandro+moschitti,+preslav+nakov,+and+massimo+nicosia.+2014a.++learning+to+differentiate+better+from+worse+translations.+in+proceedings+of+the+2014+conference+on+empirical+methods+in+natural+language+processing+(emnlp),+pages+214   220,+doha.
 607. https://www.mitpressjournals.org/servlet/linkout?suffix=r36&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/v1/p14-1065
 608. http://scholar.google.com/scholar?hl=en&q=guzm  n,+francisco,+shafiq+joty,+llu  s+m  rquez,+and+preslav+nakov.+2014b.++using+discourse+structure+improves+machine+translation+evaluation.+in+proceedings+of+the+52nd+annual+meeting+of+the+association+for+computational+linguistics+(volume+1:+long+papers),+pages+687   698,+baltimore,+md.
 609. http://scholar.google.com/scholar?hl=en&q=halliday,+michael+and+ruqaiya+hasan.+1976.+cohesion+in+english.+longman,+london.
 610. http://scholar.google.com/scholar?hl=en&q=hardmeier,+christian,+joakim+nivre,+and+j  rg+tiedemann.+2012.++document-wide+decoding+for+phrase-based+statistical+machine+translation.+in+proceedings+of+the+2012+joint+conference+on+empirical+methods+in+natural+language+processing+and+computational+natural+language+learning,+emnlp-conll+'12,+pages+1179   1190,+jeju+island.
 611. https://www.mitpressjournals.org/servlet/linkout?suffix=r39&dbid=16&doi=10.1162/coli_a_00226&key=10.5087/dad.2010.003
 612. http://scholar.google.com/scholar?hl=en&q=hernault,+hugo,+helmut+prendinger,+david+duverle,+and+mitsuru+ishizuka.+2010.++hilda:+a+discourse+parser+using+support+vector+machine+classification.+dialogue+and+discourse,+1(3):1   33.
 613. http://scholar.google.com/scholar?hl=en&q=hirst,+graeme+and+david+st-onge.+1997.++lexical+chains+as+representation+of+context+for+the+detection+and+correction+of+malapropisms.+in+christiane+fellbaum,+editor,+id138:+an+electronic+lexical+database+and+some+of+its+applications.+mit+press,+pages+305   332.
 614. https://www.mitpressjournals.org/servlet/linkout?suffix=r41&dbid=16&doi=10.1162/coli_a_00226&key=10.1207/s15516709cog0301_4
 615. http://scholar.google.com/scholar?hl=en&q=hobbs,+jerry.+1979.++coherence+and+coreference.+cognitive+science,+3:67   90.
 616. https://www.mitpressjournals.org/servlet/linkout?suffix=r42&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/1654494.1654500
 617. http://scholar.google.com/scholar?hl=en&q=huang,+liang+and+david+chiang.+2005.++better+k-best+parsing.+in+proceedings+of+the+ninth+international+workshop+on+parsing+technology,+parsing+'05,+pages+53   64,+stroudsburg,+pa.
 618. https://www.mitpressjournals.org/servlet/linkout?suffix=r43&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/v1/p14-1002
 619. http://scholar.google.com/scholar?hl=en&q=ji,+yangfeng+and+jacob+eisenstein.+2014.++representation+learning+for+text-level+discourse+parsing.+in+proceedings+of+the+52nd+annual+meeting+of+the+association+for+computational+linguistics+(volume+1:+long+papers),+pages+13   24,+baltimore,+md.
 620. http://scholar.google.com/scholar?hl=en&q=joty,+shafiq,+giuseppe+carenini,+and+raymond+t.+ng.+2012.++a+novel+discriminative+framework+for+sentence-level+discourse+analysis.+in+proceedings+of+the+2012+joint+conference+on+empirical+methods+in+natural+language+processing+and+computational+natural+language+learning,+emnlp-conll+'12,+pages+904   915,+jeju+island.
 621. http://scholar.google.com/scholar?hl=en&q=joty,+shafiq,+giuseppe+carenini,+and+raymond+t.+ng.+2013.++topic+segmentation+and+labeling+in+asynchronous+conversations.+journal+of+artificial+intelligence+research+(jair),+47:521   573.
 622. http://scholar.google.com/scholar?hl=en&q=joty,+shafiq,+giuseppe+carenini,+raymond+t.+ng,+and+yashar+mehdad.+2013.++combining+intra-+and+multi-sentential+rhetorical+parsing+for+document-level+discourse+analysis.+in+proceedings+of+the+51st+annual+meeting+of+the+association+for+computational+linguistics,+acl+'13,+pages+486   496,+sofia.
 623. https://www.mitpressjournals.org/servlet/linkout?suffix=r47&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/v1/w14-3352
 624. http://scholar.google.com/scholar?hl=en&q=joty,+shafiq,+francisco+guzm  n,+llu  s+m  rquez,+and+preslav+nakov.+2014.++discotk:+using+discourse+structure+for+machine+translation+evaluation.+in+proceedings+of+the+ninth+workshop+on+statistical+machine+translation,+wmt+'14,+pages+402   408,+baltimore,+md.
 625. http://scholar.google.com/scholar?hl=en&q=jurafsky,+daniel+and+james+martin.+2008.++statistical+parsing.+in+speech+and+language+processing,+chapter+14.+prentice+hall.
 626. http://scholar.google.com/scholar?hl=en&q=knight,+kevin+and+jonathan+graehl.+2005.++an+overview+of+probabilistic+tree+transducers+for+natural+language+processing.+in+computational+linguistics+and+intelligent+text+processing,+volume+3406+of+lecture+notes+in+computer+science.+springer,+berlin+heidelberg,+pages+1   24.
 627. https://www.mitpressjournals.org/servlet/linkout?suffix=r50&dbid=16&doi=10.1162/coli_a_00226&key=10.1080/01638539409544883
 628. http://scholar.google.com/scholar?hl=en&q=knott,+alistair+and+robert+dale.+1994.++using+linguistic+phenomena+to+motivate+a+set+of+coherence+relations.+discourse+processes,+18:35   62.
 629. http://scholar.google.com/scholar?hl=en&q=koller,+alexander,+michaela+regneri,+and+stefan+thater.+2008.++regular+tree+grammars+as+a+formalism+for+scope+underspecification.+in+proceedings+of+the+46th+annual+meeting+of+the+association+for+computational+linguistics+on+human+language+technologies,+pages+218   226,+columbus,+oh.
 630. http://scholar.google.com/scholar?hl=en&q=lafferty,+john,+andrew+mccallum,+and+fernando+pereira.+2001.++conditional+random+fields:+probabilistic+models+for+segmenting+and+labeling+sequence+data.+in+proceedings+of+the+eighteenth+international+conference+on+machine+learning,+pages+282   289,+san+francisco,+ca.
 631. http://scholar.google.com/scholar?hl=en&q=lazaridou,+angeliki,+ivan+titov,+and+caroline+sporleder.+2013.++a+bayesian+model+for+joint+unsupervised+induction+of+sentiment,+aspect+and+discourse+representations.+in+proceedings+of+the+51st+annual+meeting+of+the+association+for+computational+linguistics,+acl+'13,+sofia.
 632. https://www.mitpressjournals.org/servlet/linkout?suffix=r54&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/v1/d14-1220
 633. http://scholar.google.com/scholar?hl=en&q=li,+jiwei,+rumeng+li,+and+eduard+hovy.+2014.++recursive+deep+models+for+discourse+parsing.+in+proceedings+of+the+2014+conference+on+empirical+methods+in+natural+language+processing+(emnlp),+pages+2061   2069,+doha.
 634. https://www.mitpressjournals.org/servlet/linkout?suffix=r55&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/v1/p14-1003
 635. http://scholar.google.com/scholar?hl=en&q=li,+sujian,+liang+wang,+ziqiang+cao,+and+wenjie+li.+2014.++text-level+discourse+dependency+parsing.+in+proceedings+of+the+52nd+annual+meeting+of+the+association+for+computational+linguistics+(volume+1:+long+papers),+pages+25   35,+baltimore,+md.
 636. http://scholar.google.com/scholar?hl=en&q=louis,+annie,+aravind+joshi,+and+ani+nenkova.+2010.++discourse+indicators+for+content+selection+in+summarization.+in+proceedings+of+the+11th+annual+meeting+of+the+special+interest+group+on+discourse+and+dialogue,+sigdial+'10,+pages+147   156,+tokyo.
 637. http://scholar.google.com/scholar?hl=en&q=mach    ek,+matou  +and+ond  ej+bojar.+2014.+results+of+the+wmt14+metrics+shared+task.+in+proceedings+of+the+ninth+workshop+on+statistical+machine+translation,+baltimore,+md.
 638. https://www.mitpressjournals.org/servlet/linkout?suffix=r58&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/981658.981695
 639. http://scholar.google.com/scholar?hl=en&q=magerman,+david.+1995.++statistical+decision-tree+models+for+parsing.+in+proceedings+of+the+33rd+annual+meeting+of+the+association+for+computational+linguistics,+acl'95,+pages+276   283,+cambridge,+ma.
 640. https://www.mitpressjournals.org/servlet/linkout?suffix=r59&dbid=16&doi=10.1162/coli_a_00226&key=10.1515/text.1.1988.8.3.243
 641. http://scholar.google.com/scholar?hl=en&q=mann,+william+and+sandra+thompson.+1988.++rhetorical+structure+theory:+toward+a+functional+theory+of+text+organization.+text,+8(3):243   281.
 642. https://www.mitpressjournals.org/servlet/linkout?suffix=r60&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/1034678.1034736
 643. http://scholar.google.com/scholar?hl=en&q=marcu,+daniel.+1999.++a+decision-based+approach+to+rhetorical+parsing.+in+proceedings+of+the+37th+annual+meeting+of+the+association+for+computational+linguistics+on+computational+linguistics,+acl'99,+pages+365   372,+morristown,+nj.
 644. https://www.mitpressjournals.org/doi/10.1162/089120100561755
 645. http://scholar.google.com/scholar?hl=en&q=marcu,+daniel.+2000a.++the+rhetorical+parsing+of+unrestricted+texts:+a+surface-based+approach.+computational+linguistics,+26:395   448.
 646. http://scholar.google.com/scholar?hl=en&q=marcu,+daniel.+2000b.+the+theory+and+practice+of+discourse+parsing+and+summarization.+mit+press,+cambridge,+ma.
 647. http://scholar.google.com/scholar?hl=en&q=marcu,+daniel+and+abdessamad+echihabi.+2002.++an+unsupervised+approach+to+recognizing+discourse+relations.+in+proceedings+of+the+40th+annual+meeting+of+the+association+for+computational+linguistics,+acl'02,+pages+368   375.+philadelphia,+pa.
 648. http://scholar.google.com/scholar?hl=en&q=marcus,+mitchell,+mary+marcinkiewicz,+and+beatrice+santorini.+1994.++building+a+large+annotated+corpus+of+english:+the+penn+treebank.+computational+linguistics,+19(2):313   330.
 649. http://scholar.google.com/scholar?hl=en&q=martin,+james,+1992.+english+text:+system+and+structure.+john+benjamins+publishing+company,+philadelphia/amsterdam.
 650. http://scholar.google.com/scholar?hl=en&q=maslennikov,+mstislav+and+tat-seng+chua.+2007.++a+multi-resolution+framework+for+information+extraction+from+free+text.+in+proceedings+of+the+45th+annual+meeting+of+the+association+for+computational+linguistics,+pages+592   599,+prague.
 651. http://mallet.cs.umass.edu/
 652. http://scholar.google.com/scholar?hl=en&q=mccallum,+andrew.+2002.++mallet:+a+machine+learning+for+language+toolkit.+http://mallet.cs.umass.edu.
 653. http://scholar.google.com/scholar?hl=en&q=mccallum,+andrew,+dayne+freitag,+and+fernando+c.+n.+pereira.+2000.++maximum+id178+markov+models+for+information+extraction+and+segmentation.+in+proceedings+of+the+seventeenth+international+conference+on+machine+learning,+icml+'00,+pages+591   598,+san+francisco,+ca.
 654. https://www.mitpressjournals.org/servlet/linkout?suffix=r69&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/1219840.1219852
 655. http://scholar.google.com/scholar?hl=en&q=mcdonald,+ryan,+koby+crammer,+and+fernando+pereira.+2005.++online+large-margin+training+of+dependency+parsers.+in+proceedings+of+the+43rd+annual+meeting+of+the+association+for+computational+linguistics,+acl+'05,+pages+91   98,+ann+arbor,+mi.
 656. https://www.mitpressjournals.org/servlet/linkout?suffix=r70&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/1220575.1220641
 657. http://scholar.google.com/scholar?hl=en&q=mcdonald,+ryan,+fernando+pereira,+kiril+ribarov,+and+jan+haji  .+2005.++non-projective+dependency+parsing+using+spanning+tree+algorithms.+in+proceedings+of+the+conference+on+human+language+technology+and+empirical+methods+in+natural+language+processing,+hlt+'05,+pages+523   530,+stroudsburg,+pa.
 658. http://scholar.google.com/scholar?hl=en&q=morris,+jane+and+graeme+hirst.+1991.++lexical+cohesion+computed+by+thesaural+relations+as+an+indicator+of+structure+of+text.+computational+linguistics,+17(1):21   48.
 659. http://scholar.google.com/scholar?hl=en&q=murphy,+kevin.+2012.+machine+learning:+a+probabilistic+perspective.+the+mit+press.+cambridge,+ma.
 660. https://www.mitpressjournals.org/servlet/linkout?suffix=r73&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/1218955.1218990
 661. http://scholar.google.com/scholar?hl=en&q=pang,+bo+and+lillian+lee.+2004.++a+sentimental+education:+sentiment+analysis+using+subjectivity+summarization+based+on+minimum+cuts.+in+proceedings+of+the+42nd+annual+meeting+of+the+association+for+computational+linguistics,+acl+'04,+pages+271   278.+barcelona.
 662. http://scholar.google.com/scholar?hl=en&q=pitler,+emily+and+ani+nenkova.+2009.++using+syntax+to+disambiguate+explicit+discourse+connectives+in+text.+in+proceedings+of+the+acl-ijcnlp+2009+conference+short+papers,+aclshort+'09,+pages+13   16,+suntec.
 663. http://scholar.google.com/scholar?hl=en&q=poole,+david+and+alan+mackworth,+2010.+artificial+intelligence:+foundations+of+computational+agents.+cambridge+university+press.
 664. http://scholar.google.com/scholar?hl=en&q=prasad,+rashmi,+nikhil+dinesh,+alan+lee,+eleni+miltsakaki,+livio+robaldo,+aravind+joshi,+and+bonnie+webber.+2008.++the+penn+discourse+treebank+2.0.+in+proceedings+of+the+sixth+international+conference+on+language+resources+and+evaluation+(lrec),+pages+2961   2968,+marrakech.
 665. http://scholar.google.com/scholar?hl=en&q=prasad,+rashmi,+aravind+joshi,+nikhil+dinesh,+alan+lee,+eleni+miltsakaki,+and+bonnie+webber.+2005.++the+penn+discourse+treebank+as+a+resource+for+natural+language+generation.+in+proceedings+of+the+corpus+linguistics+workshop+on+using+corpora+for+natural+language+generation,+pages+25   32,+birmingham.
 666. https://www.mitpressjournals.org/servlet/linkout?suffix=r78&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/1557690.1557761
 667. http://scholar.google.com/scholar?hl=en&q=regneri,+michaela,+markus+egg,+and+alexander+koller.+2008.++efficient+processing+of+underspecified+discourse+representations.+in+proceedings+of+the+46th+annual+meeting+of+the+association+for+computational+linguistics+on+human+language+technologies:+short+papers,+hlt-short+'08,+pages+245   248,+columbus,+oh.
 668. https://www.mitpressjournals.org/servlet/linkout?suffix=r79&dbid=16&doi=10.1162/coli_a_00226&key=10.1093/jos/10.2.123
 669. http://scholar.google.com/scholar?hl=en&q=reyle,+uwe.+1993.++dealing+with+ambiguities+by+underspecification:+construction,+representation+and+deduction.+journal+of+semantics,+10(2):123   179.
 670. https://www.mitpressjournals.org/servlet/linkout?suffix=r80&dbid=16&doi=10.1162/coli_a_00226&key=10.1023/a:1007649029923
 671. http://scholar.google.com/scholar?hl=en&q=schapire,+robert+e.+and+yoram+singer.+2000.++boostexter:+a+boosting-based+system+for+text+categorization.+machine+learning,+39(2   3):135   168.
 672. http://scholar.google.com/scholar?hl=en&q=schauer,+holger+and+udo+hahn.+2001.++anaphoric+cues+for+coherence+relations.+in+proceedings+of+the+conference+on+recent+advances+in+natural+language+processing,+ranlp+'01,+pages+228   234,+tzigov+chark.
 673. http://scholar.google.com/scholar?hl=en&q=schilder,+frank.+2002.++robust+discourse+parsing+via+discourse+markers,+topicality+and+position.+natural+language+engineering,+8(3):235   255.
 674. https://www.mitpressjournals.org/servlet/linkout?suffix=r83&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/1073445.1073473
 675. http://scholar.google.com/scholar?hl=en&q=sha,+fei+and+fernando+pereira.+2003.++shallow+parsing+with+conditional+random+fields.+in+proceedings+of+the+2003+conference+of+the+north+american+chapter+of+the+association+for+computational+linguistics+on+human+language+technology+-+volume+1,+naacl-hlt'03,+pages+134   141,+edmonton.
 676. https://www.mitpressjournals.org/doi/10.1162/089120102762671954
 677. http://scholar.google.com/scholar?hl=en&q=silber,+gregory+and+kathleen+mccoy.+2002.++efficiently+computed+lexical+chains+as+an+intermediate+representation+for+automatic+text+summarization.+computational+linguistics,+28(4):487   496.
 678. http://scholar.google.com/scholar?hl=en&q=smith,+noah+a.+2011.+linguistic+structure+prediction.+synthesis+lectures+on+human+language+technologies.+morgan+and+claypool.
 679. http://scholar.google.com/scholar?hl=en&q=socher,+richard,+john+bauer,+christopher+d.+manning,+and+ng+andrew+y.+2013a.++parsing+with+compositional+vector+grammars.+in+proceedings+of+the+51st+annual+meeting+of+the+association+for+computational+linguistics+(volume+1:+long+papers),+pages+455   465,+sofia.
 680. http://scholar.google.com/scholar?hl=en&q=socher,+richard,+alex+perelygin,+jean+wu,+jason+chuang,+christopher+d.+manning,+andrew+ng,+and+christopher+potts.+2013b.++recursive+deep+models+for+semantic+compositionality+over+a+sentiment+treebank.+in+proceedings+of+the+2013+conference+on+empirical+methods+in+natural+language+processing,+pages+1631   1642,+seattle,+wa.
 681. http://scholar.google.com/scholar?hl=en&q=somasundaran,+s.+2010.+discourse-level+relations+for+opinion+analysis.+ph.d.+thesis,+university+of+pittsburgh,+pa.
 682. https://www.mitpressjournals.org/servlet/linkout?suffix=r89&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/1073445.1073475
 683. http://scholar.google.com/scholar?hl=en&q=soricut,+radu+and+daniel+marcu.+2003.++sentence+level+discourse+parsing+using+syntactic+and+lexical+information.+in+proceedings+of+the+2003+conference+of+the+north+american+chapter+of+the+association+for+computational+linguistics+on+human+language+technology+-+volume+1,+naacl'03,+pages+149   156,+edmonton.
 684. http://scholar.google.com/scholar?hl=en&q=sporleder,+caroline.+2007.++manually+vs.+automatically+labelled+data+in+discourse+relation+classification.+effects+of+example+and+feature+selection.+ldv+forum,+22(1):1   20.
 685. http://scholar.google.com/scholar?hl=en&q=sporleder,+caroline+and+mirella+lapata.+2004.++automatic+paragraph+identification:+a+study+across+languages+and+domains.+in+proceedings+of+the+2004+conference+on+empirical+methods+in+natural+language+processing,+emnlp+'04,+pages+72   79,+barcelona.
 686. https://www.mitpressjournals.org/servlet/linkout?suffix=r92&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/1220575.1220608
 687. http://scholar.google.com/scholar?hl=en&q=sporleder,+caroline+and+mirella+lapata.+2005.++discourse+chunking+and+its+application+to+sentence+compression.+in+proceedings+of+the+conference+on+human+language+technology+and+empirical+methods+in+natural+language+processing,+hlt-emnlp'05,+pages+257   264,+vancouver.
 688. http://scholar.google.com/scholar?hl=en&q=sporleder,+caroline+and+alex+lascarides.+2005.++exploiting+linguistic+cues+to+classify+rhetorical+relations.+in+proceedings+of+recent+advances+in+natural+language+processing+(ranlp),+pages+157   166,+bulgaria.
 689. https://www.mitpressjournals.org/servlet/linkout?suffix=r94&dbid=16&doi=10.1162/coli_a_00226&key=10.1017/s1351324906004451
 690. http://scholar.google.com/scholar?hl=en&q=sporleder,+caroline+and+alex+lascarides.+2008.++using+automatically+labelled+examples+to+classify+rhetorical+relations:+an+assessment.+natural+language+engineering,+14(3):369   416.
 691. http://scholar.google.com/scholar?hl=en&q=stede,+manfred.+2004.++the+potsdam+commentary+corpus.+in+proceedings+of+the+acl-04+workshop+on+discourse+annotation,+pages+96   102,+barcelona.
 692. http://scholar.google.com/scholar?hl=en&q=stede,+manfred.+2011.+discourse+processing.+synthesis+lectures+on+human+language+technologies.+morgan+and+claypool+publishers.
 693. https://www.mitpressjournals.org/servlet/linkout?suffix=r97&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/1620754.1620837
 694. http://scholar.google.com/scholar?hl=en&q=subba,+rajen+and+barbara+di-eugenio.+2009.++an+effective+discourse+parser+that+uses+rich+linguistic+information.+in+proceedings+of+human+language+technologies:+the+2009+annual+conference+of+the+north+american+chapter+of+the+association+for+computational+linguistics,+hlt-naacl'09,+pages+566   574,+boulder,+co.
 695. https://www.mitpressjournals.org/servlet/linkout?suffix=r98&dbid=16&doi=10.1162/coli_a_00226&key=10.1561/2200000013
 696. http://scholar.google.com/scholar?hl=en&q=sutton,+charles+and+andrew+mccallum.+2012.++an+introduction+to+conditional+random+fields.+foundations+and+trends+in+machine+learning,+4(4):267   373.
 697. http://scholar.google.com/scholar?hl=en&q=sutton,+charles,+andrew+mccallum,+and+khashayar+rohanimanesh.+2007.++dynamic+conditional+random+fields:+factorized+probabilistic+models+for+labeling+and+segmenting+sequence+data.+journal+of+machine+learning+research+(jmlr),+8:693   723.
 698. https://www.mitpressjournals.org/servlet/linkout?suffix=r100&dbid=16&doi=10.1162/coli_a_00226&key=10.1016/j.pragma.2005.09.010
 699. http://scholar.google.com/scholar?hl=en&q=taboada,+maite.+2006.++discourse+markers+as+signals+(or+not)+of+rhetorical+relations.+journal+of+pragmatics,+38(4):567   592.
 700. https://www.mitpressjournals.org/doi/10.1162/coli_a_00049
 701. http://scholar.google.com/scholar?hl=en&q=taboada,+maite,+julian+brooke,+milan+tofiloski,+kimberly+voll,+and+manfred+stede.+2011.++lexicon-based+methods+for+sentiment+analysis.+computational+linguistics,+37(2):267   307.
 702. https://www.mitpressjournals.org/servlet/linkout?suffix=r102&dbid=16&doi=10.1162/coli_a_00226&key=10.1177/1461445606061881
 703. http://scholar.google.com/scholar?hl=en&q=taboada,+maite+and+william+c.+mann.+2006.++rhetorical+structure+theory:+looking+back+and+moving+ahead.+discourse+studies,+8(3):423   459.
 704. https://www.mitpressjournals.org/doi/10.1162/089120102762671936
 705. http://scholar.google.com/scholar?hl=en&q=teufel,+simone+and+marc+moens.+2002.++summarizing+scientific+articles:+experiments+with+relevance+and+rhetorical+status.+computational+linguistics,+28(4):409   445.
 706. https://www.mitpressjournals.org/servlet/linkout?suffix=r104&dbid=16&doi=10.1162/coli_a_00226&key=10.1145/1277741.1277883
 707. http://scholar.google.com/scholar?hl=en&q=verberne,+suzan,+lou+boves,+nelleke+oostdijk,+and+peter-arno+coppen.+2007.++evaluating+discourse-based+answer+extraction+for+why-question+answering.+in+proceedings+of+the+30th+annual+international+acm+sigir+conference+on+research+and+development+in+information+retrieval,+sigir'07,+pages+735   736,+amsterdam.
 708. http://scholar.google.com/scholar?hl=en&q=vliet,+nynke+and+gisela+redeker.+2011.++complex+sentences+as+leaky+units+in+discourse+parsing.+in+proceedings+of+constraints+in+discourse,+pages+1   9,+agay   saint+raphael.
 709. https://www.mitpressjournals.org/servlet/linkout?suffix=r106&dbid=16&doi=10.1162/coli_a_00226&key=10.1207/s15516709cog2805_6
 710. http://scholar.google.com/scholar?hl=en&q=webber,+b.+2004.++d-ltag:+extending+lexicalized+tag+to+discourse.+cognitive+science,+28(5):751   779.
 711. http://scholar.google.com/scholar?hl=en&q=webber,+bonnie,+andrei+popescu-belis,+katja+markert,+and+j  rg+tiedemann,+editors.+2013.+proceedings+of+the+workshop+on+discourse+in+machine+translation.+acl,+sofia.
 712. http://scholar.google.com/scholar?hl=en&q=wick,+michael,+khashayar+rohanimanesh,+kedare+bellare,+aron+culotta,+and+andrew+mccallum.+2011.++samplerank:+training+factor+graphs+with+atomic+gradients.+in+proceedings+of+the+28th+international+conference+on+machine+learning,+icml'11,+pages+777   784.+bellevue,+wa.
 713. https://www.mitpressjournals.org/doi/10.1162/0891201054223977
 714. http://scholar.google.com/scholar?hl=en&q=wolf,+florian+and+edward+gibson.+2005.++representing+discourse+coherence:+a+corpus-based+study.+computational+linguistics,+31:249   288.
 715. http://scholar.google.com/scholar?hl=en&q=zhang,+yuan,+tao+lei,+regina+barzilay,+tommi+jaakkola,+and+amir+globerson.+2014.++steps+to+excellence:+simple+id136+with+refined+scoring+of+dependency+trees.+in+proceedings+of+the+52nd+annual+meeting+of+the+association+for+computational+linguistics,+pages+197   207,+baltimore,+md.
 716. https://www.mitpressjournals.org/cdn-cgi/l/email-protection#11627b7e65685160773f7e63763f6070
 717. https://www.mitpressjournals.org/cdn-cgi/l/email-protection#781b190a1d16111611381b0b560d1a1b561b19
 718. https://www.mitpressjournals.org/cdn-cgi/l/email-protection#3f4d51587f5c4c114a5d5c115c5e
 719. https://doi.org/10.1162/coli_a_00226
 720. https://www.mitpressjournals.org/doi/abs/10.1162/coli_a_00226
 721. https://www.mitpressjournals.org/doi/full/10.1162/coli_a_00226
 722. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#authors-content
 723. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i1
 724. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#abstract
 725. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i4
 726. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 727. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 728. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 729. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 730. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 731. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 732. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 733. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 734. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 735. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 736. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 737. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 738. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 739. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 740. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 741. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 742. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 743. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 744. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 745. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 746. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 747. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 748. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 749. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 750. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 751. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 752. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 753. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 754. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 755. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 756. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 757. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 758. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 759. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 760. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 761. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 762. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 763. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 764. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 765. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 766. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 767. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 768. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 769. javascript:popref('s2')
 770. javascript:popref('s3')
 771. javascript:popref('s4')
 772. javascript:popref('s5')
 773. javascript:popref('s6')
 774. javascript:popref('s7')
 775. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i1
 776. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i7
 777. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 778. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 779. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 780. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 781. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 782. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 783. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 784. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 785. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 786. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 787. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 788. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 789. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 790. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 791. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn4
 792. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 793. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 794. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 795. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 796. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 797. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 798. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 799. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 800. javascript:popref('s1')
 801. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 802. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 803. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn5
 804. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 805. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 806. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 807. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 808. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 809. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 810. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 811. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 812. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 813. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 814. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 815. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 816. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 817. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn6
 818. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 819. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 820. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 821. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 822. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 823. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 824. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 825. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 826. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 827. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 828. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 829. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 830. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 831. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 832. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 833. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 834. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 835. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 836. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 837. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i4
 838. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i12
 839. javascript:popref('s9')
 840. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn7
 841. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn8
 842. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 843. javascript:popref('s5')
 844. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 845. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 846. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 847. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 848. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 849. javascript:popref('s2')
 850. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 851. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 852. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 853. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 854. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 855. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 856. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn9
 857. javascript:popref('s4a1')
 858. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 859. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 860. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 861. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn10
 862. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 863. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 864. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 865. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 866. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 867. javascript:popref('s4c')
 868. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i7
 869. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i58
 870. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 871. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 872. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 873. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 874. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 875. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 876. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 877. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 878. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 879. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 880. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 881. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 882. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 883. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 884. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 885. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 886. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 887. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 888. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 889. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 890. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 891. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 892. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 893. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 894. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 895. javascript:popref('s4b')
 896. javascript:popref('s4c')
 897. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 898. javascript:popref('e1')
 899. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 900. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 901. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 902. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 903. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 904. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 905. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 906. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 907. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 908. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 909. javascript:popref('t1')
 910. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 911. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 912. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 913. javascript:popref('t1')
 914. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 915. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 916. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 917. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 918. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 919. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn11
 920. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 921. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 922. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 923. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 924. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 925. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 926. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 927. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 928. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 929. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 930. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 931. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 932. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 933. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 934. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 935. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 936. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 937. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 938. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 939. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 940. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 941. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 942. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 943. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 944. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 945. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 946. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 947. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 948. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 949. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 950. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 951. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 952. javascript:popref('s4a3')
 953. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 954. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 955. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 956. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 957. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 958. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 959. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 960. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 961. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 962. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 963. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 964. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn12
 965. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 966. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 967. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn13
 968. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 969. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 970. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 971. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 972. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 973. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 974. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 975. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 976. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 977. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 978. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 979. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 980. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 981. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 982. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i12
 983. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i63
 984. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 985. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 986. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 987. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 988. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 989. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 990. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 991. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 992. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 993. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 994. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 995. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn14
 996. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i58
 997. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i108
 998. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
 999. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1000. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1001. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1002. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1003. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1004. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1005. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1006. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1007. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1008. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1009. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1010. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1011. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1012. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1013. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1014. javascript:popref('t2')
1015. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1016. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn15
1017. javascript:popref('t2')
1018. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1019. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1020. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1021. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1022. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1023. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1024. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1025. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1026. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1027. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1028. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1029. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1030. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn16
1031. javascript:popref('t3')
1032. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1033. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1034. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn17
1035. javascript:popref('s6a')
1036. javascript:popref('t3')
1037. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1038. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1039. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1040. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1041. javascript:popref('s6c1')
1042. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1043. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1044. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1045. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1046. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn18
1047. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1048. javascript:popref('t4')
1049. javascript:popref('s6b2')
1050. javascript:popref('t4')
1051. javascript:popref('t4')
1052. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1053. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn19
1054. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn20
1055. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1056. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn21
1057. javascript:popref('t5')
1058. javascript:popref('t5')
1059. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1060. javascript:popref('t4')
1061. javascript:popref('t5')
1062. javascript:popref('t3')
1063. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1064. javascript:popref('s4c')
1065. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1066. javascript:popref('t6')
1067. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1068. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1069. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn22
1070. javascript:popref('t6')
1071. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1072. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1073. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn23
1074. javascript:popref('t6')
1075. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1076. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1077. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1078. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1079. javascript:popref('s4c2')
1080. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1081. javascript:popref('s4b')
1082. javascript:popref('t2')
1083. javascript:popref('t7')
1084. javascript:popref('t7')
1085. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1086. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1087. javascript:popref('t8')
1088. javascript:popref('s6b2')
1089. javascript:popref('t8')
1090. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1091. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1092. javascript:popref('t9')
1093. javascript:popref('s4a4')
1094. javascript:popref('t9')
1095. javascript:popref('t9')
1096. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1097. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1098. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn24
1099. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1100. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1101. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1102. javascript:popref('s6d2')
1103. javascript:popref('s6d3')
1104. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1105. javascript:popref('t2')
1106. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn25
1107. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1108. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1109. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn26
1110. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1111. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1112. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1113. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1114. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1115. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1116. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1117. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i63
1118. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i109
1119. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1120. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1121. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1122. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1123. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1124. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1125. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1126. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1127. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1128. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1129. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1130. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1131. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1132. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1133. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1134. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1135. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1136. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i108
1137. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i110
1138. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1139. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1140. javascript:popref('s4b')
1141. javascript:popref('s6d4')
1142. javascript:popref('s6d2')
1143. javascript:popref('s6d3')
1144. javascript:popref('s6d5')
1145. javascript:popref('s6d6')
1146. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i109
1147. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i112
1148. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i110
1149. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i113
1150. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i112
1151. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i114
1152. http://www.isi.edu/licensed-sw/spade/
1153. http://nlp.prendingerlab.net/hilda/
1154. http://109.228.0.153/discourse_parser_demo/
1155. http://alt.qcri.org/tools/
1156. javascript:popref('s9')
1157. http://www.bbc.co.uk/news/world-asia-26106490
1158. http://cogcomp.cs.illinois.edu/page/software
1159. http://alt.qcri.org/tools/
1160. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1161. javascript:popref('t4')
1162. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1163. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1164. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1165. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1166. javascript:popref('t4')
1167. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i113
1168. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#citart1
1169. https://www.mitpressjournals.org/servlet/linkout?suffix=r1&dbid=16&doi=10.1162/coli_a_00226&key=10.1016/s0196-6774(03)00050-6
1170. http://scholar.google.com/scholar?hl=en&q=althaus,+ernst,+denys+duchier,+alexander+koller,+kurt+mehlhorn,+joachim+niehren,+and+sven+thiel.+2003.++an+efficient+graph+algorithm+for+dominance+constraints.+journal+of+algorithms,+48(1):194   219.
1171. http://scholar.google.com/scholar?hl=en&q=asher,+nicholas+and+alex+lascarides,+2003.+logics+of+conversation.+cambridge+university+press.
1172. http://scholar.google.com/scholar?hl=en&q=barzilay,+regina+and+michael+elhadad.+1997.++using+lexical+chains+for+text+summarization.+in+proceedings+of+the+35th+annual+meeting+of+the+association+for+computational+linguistics+and+the+8th+european+chapter+meeting+of+the+association+for+computational+linguistics,+workshop+on+intelligent+scalable+test+summarization,+pages+10   17,+madrid.
1173. https://www.mitpressjournals.org/servlet/linkout?suffix=r4&dbid=16&doi=10.1162/coli_a_00226&key=10.1142/s1793351x11001328
1174. http://scholar.google.com/scholar?hl=en&q=biran,+or+and+owen+rambow.+2011.++identifying+justifications+in+written+dialogs+by+classifying+text+as+argumentative.+international+journal+of+semantic+computing,+5(4):363   381.
1175. http://scholar.google.com/scholar?hl=en&q=blair-goldensohn,+sasha,+kathleen+mckeown,+and+owen+rambow.+2007.++building+and+refining+rhetorical-semantic+relation+models.+in+proceedings+of+the+human+language+technologies:+the+annual+conference+of+the+north+american+chapter+of+the+association+for+computational+linguistics,+hlt-naacl'07,+pages+428   435.+rochester,+ny.
1176. http://scholar.google.com/scholar?hl=en&q=blitzer,+john.+2008.+domain+adaptation+of+natural+language+processing+systems.+ph.d.+thesis,+university+of+pennsylvania.
1177. https://www.mitpressjournals.org/servlet/linkout?suffix=r7&dbid=16&doi=10.1162/coli_a_00226&key=10.1007/bf00058655
1178. http://scholar.google.com/scholar?hl=en&q=breiman,+leo.+1996.++id112+predictors.+machine+learning,+24(2):123   140.
1179. http://scholar.google.com/scholar?hl=en&q=carlson,+lynn+and+daniel+marcu.+2001.++discourse+tagging+reference+manual.+technical+report+isi-tr-545,+university+of+southern+california+information+sciences+institute.
1180. http://scholar.google.com/scholar?hl=en&q=carlson,+lynn,+daniel+marcu,+and+mary+ellen+okurowski.+2002.++rst+discourse+treebank+(rst   dt)+ldc2002t07.+linguistic+data+consortium,+philadelphia.
1181. http://scholar.google.com/scholar?hl=en&q=chali,+yllias+and+shafiq+joty.+2007.++word+sense+disambiguation+using+lexical+cohesion.+in+proceedings+of+semeval-2007,+pages+476   479,+prague.
1182. http://scholar.google.com/scholar?hl=en&q=charniak,+eugene.+2000.++a+maximum-id178-inspired+parser.+in+proceedings+of+the+1st+north+american+chapter+of+the+association+for+computational+linguistics+conference,+naacl'00,+pages+132   139,+seattle,+wa.
1183. https://www.mitpressjournals.org/servlet/linkout?suffix=r12&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/1219840.1219862
1184. http://scholar.google.com/scholar?hl=en&q=charniak,+eugene+and+mark+johnson.+2005.++coarse-to-fine+n-best+parsing+and+maxent+discriminative+reranking.+in+proceedings+of+the+43rd+annual+meeting+of+the+association+for+computational+linguistics,+acl'05,+pages+173   180,+ann+arbor,+mi.
1185. http://scholar.google.com/scholar?hl=en&q=christensen,+janara,+mausam,+stephen+soderland,+and+oren+etzioni.+2013.++towards+coherent+multi-document+summarization.+in+proceedings+of+the+2013+conference+of+the+north+american+chapter+of+the+association+for+computational+linguistics:+human+language+technologies,+naacl-hlt'13,+pages+1163   1173,+atlanta,+ga.
1186. http://scholar.google.com/scholar?hl=en&q=christensen,+janara,+stephen+soderland,+gagan+bansal,+and+mausam.+2014.++hierarchical+summarization:+scaling+up+multi-document+summarization.+in+proceedings+of+the+52nd+annual+meeting+of+the+association+for+computational+linguistics,+acl'13,+pages+902   912,+baltimore,+md.
1187. https://www.mitpressjournals.org/doi/10.1162/089120103322753356
1188. http://scholar.google.com/scholar?hl=en&q=collins,+michael.+2003.++head-driven+statistical+models+for+natural+language+parsing.+computational+linguistics,+29(4):589   637.
1189. https://www.mitpressjournals.org/doi/10.1162/0891201053630273
1190. http://scholar.google.com/scholar?hl=en&q=collins,+michael+and+terry+koo.+2005.++discriminative+reranking+for+natural+language+parsing.+computational+linguistics,+31(1):25   70.
1191. http://scholar.google.com/scholar?hl=en&q=collobert,+ronan,+jason+weston,+l  on+bottou,+michael+karlen,+koray+kavukcuoglu,+and+pavel+kuksa.+2011.++natural+language+processing+(almost)+from+scratch.+journal+of+machine+learning+research,+12:2493   2537.
1192. https://www.mitpressjournals.org/servlet/linkout?suffix=r18&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/980845.980891
1193. http://scholar.google.com/scholar?hl=en&q=cristea,+dan,+nancy+ide,+and+laurent+romary.+1998.++veins+theory:+a+model+of+global+discourse+cohesion+and+coherence.+in+proceedings+of+the+36th+annual+meeting+of+the+association+for+computational+linguistics+and+of+the+17th+international+conference+on+computational+linguistics+(coling/acl'98),+pages+281   285.+montreal.
1194. http://scholar.google.com/scholar?hl=en&q=danlos,+laurence.+2009.++d-stag:+a+discourse+analysis+formalism+based+on+synchronous+tags.+tal,+50(1):111   143.
1195. http://scholar.google.com/scholar?hl=en&q=daum  ,+iii,+hal.+2007.++frustratingly+easy+domain+adaptation.+in+proceedings+of+the+45th+annual+meeting+of+the+association+for+computational+linguistics,+acl'07,+pages+256   263,+prague.
1196. http://scholar.google.com/scholar?hl=en&q=daum  ,+iii,+hal+and+daniel+marcu.+2002.++a+noisy-channel+model+for+document+compression.+in+proceedings+of+the+40th+annual+meeting+of+the+association+for+computational+linguistics,+acl+'02,+pages+449   456,+philadelphia,+pa.
1197. http://scholar.google.com/scholar?hl=en&q=dinarelli,+marco,+alessandro+moschitti,+and+giuseppe+riccardi.+2011.++discriminative+reranking+for+spoken+language+understanding.+ieee+transactions+on+audio,+speech+and+language+processing+(taslp),+20:526   539.
1198. http://scholar.google.com/scholar?hl=en&q=duverle,+david+and+helmut+prendinger.+2009.++a+novel+discourse+parser+based+on+support+vector+machine+classification.+in+proceedings+of+the+joint+conference+of+the+47th+annual+meeting+of+the+acl+and+the+4th+international+joint+conference+on+natural+language+processing+of+the+afnlp,+pages+665   673,+suntec.
1199. https://www.mitpressjournals.org/servlet/linkout?suffix=r24&dbid=16&doi=10.1162/coli_a_00226&key=10.1023/a:1017964622902
1200. http://scholar.google.com/scholar?hl=en&q=egg,+markus,+alexander+koller,+and+joachim+niehren.+2001.++the+constraint+language+for+lambda+structures.+journal+of+logic,+language+and+information,+10(4):457   485.
1201. https://www.mitpressjournals.org/servlet/linkout?suffix=r25&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/992628.992688
1202. http://scholar.google.com/scholar?hl=en&q=eisner,+jason.+1996.++three+new+probabilistic+models+for+dependency+parsing:+an+exploration.+in+proceedings+of+the+16th+conference+on+computational+linguistics+-+volume+1,+coling+'96,+pages+340   345,+copenhagen.
1203. http://scholar.google.com/scholar?hl=en&q=fellbaum,+christiane.+1998.+id138   an+electronic+lexical+database.+mit+press,+cambridge,+ma.
1204. http://scholar.google.com/scholar?hl=en&q=feng,+vanessa+and+graeme+hirst.+2012.++text-level+discourse+parsing+with+rich+linguistic+features.+in+proceedings+of+the+50th+annual+meeting+of+the+association+for+computational+linguistics,+acl+'12,+pages+60   68,+jeju+island.
1205. http://scholar.google.com/scholar?hl=en&q=feng,+vanessa+and+graeme+hirst.+2014.++a+linear-time+bottom-up+discourse+parser+with+constraints+and+post-editing.+in+proceedings+of+the+52nd+annual+meeting+of+the+association+for+computational+linguistics,+acl+'14,+pages+511   521,+baltimore,+md.
1206. http://scholar.google.com/scholar?hl=en&q=finkel,+jenny+rose,+alex+kleeman,+and+christopher+manning.+2008.++efficient,+feature-based,+conditional+random+field+parsing.+in+proceedings+of+the+46th+annual+meeting+of+the+association+for+computational+linguistics,+acl'08,+pages+959   967,+columbus,+oh.
1207. http://scholar.google.com/scholar?hl=en&q=fisher,+seeger+and+brian+roark.+2007.++the+utility+of+parse-derived+features+for+automatic+discourse+segmentation.+in+proceedings+of+the+45th+annual+meeting+of+the+association+for+computational+linguistics,+acl'07,+pages+488   495,+prague.
1208. http://scholar.google.com/scholar?hl=en&q=galley,+michel+and+kathleen+mckeown.+2003.++improving+word+sense+disambiguation+in+lexical+chaining.+in+proceedings+of+the+18th+international+joint+conference+on+artificial+intelligence,+ijcai'03,+pages+1486   1488,+acapulco.
1209. https://www.mitpressjournals.org/servlet/linkout?suffix=r32&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/1075096.1075167
1210. http://scholar.google.com/scholar?hl=en&q=galley,+michel,+kathleen+mckeown,+eric+fosler-lussier,+and+hongyan+jing.+2003.++discourse+segmentation+of+multi-party+conversation.+in+proceedings+of+the+41st+annual+meeting+of+the+association+for+computational+linguistics+-+volume+1,+acl+'03,+pages+562   569,+sapporo.
1211. http://scholar.google.com/scholar?hl=en&q=ghosh,+sucheta,+richard+johansson,+giuseppe+riccardi,+and+sara+tonelli.+2011.++shallow+discourse+parsing+with+conditional+random+fields.+in+proceedings+of+the+5th+international+joint+conference+on+natural+language+processing,+ijcnlp'11,+pages+1071   1079,+chiang+mai.
1212. https://www.mitpressjournals.org/servlet/linkout?suffix=r34&dbid=16&doi=10.1162/coli_a_00226&key=10.1007/s10590-011-9088-7
1213. http://scholar.google.com/scholar?hl=en&q=gim  nez,+jes  s+and+llu  s+m  rquez.+2010.++linguistic+measures+for+automatic+machine+translation+evaluation.+machine+translation,+24(3   4):77   86.
1214. https://www.mitpressjournals.org/servlet/linkout?suffix=r35&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/v1/d14-1027
1215. http://scholar.google.com/scholar?hl=en&q=guzm  n,+francisco,+shafiq+joty,+llu  s+m  rquez,+alessandro+moschitti,+preslav+nakov,+and+massimo+nicosia.+2014a.++learning+to+differentiate+better+from+worse+translations.+in+proceedings+of+the+2014+conference+on+empirical+methods+in+natural+language+processing+(emnlp),+pages+214   220,+doha.
1216. https://www.mitpressjournals.org/servlet/linkout?suffix=r36&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/v1/p14-1065
1217. http://scholar.google.com/scholar?hl=en&q=guzm  n,+francisco,+shafiq+joty,+llu  s+m  rquez,+and+preslav+nakov.+2014b.++using+discourse+structure+improves+machine+translation+evaluation.+in+proceedings+of+the+52nd+annual+meeting+of+the+association+for+computational+linguistics+(volume+1:+long+papers),+pages+687   698,+baltimore,+md.
1218. http://scholar.google.com/scholar?hl=en&q=halliday,+michael+and+ruqaiya+hasan.+1976.+cohesion+in+english.+longman,+london.
1219. http://scholar.google.com/scholar?hl=en&q=hardmeier,+christian,+joakim+nivre,+and+j  rg+tiedemann.+2012.++document-wide+decoding+for+phrase-based+statistical+machine+translation.+in+proceedings+of+the+2012+joint+conference+on+empirical+methods+in+natural+language+processing+and+computational+natural+language+learning,+emnlp-conll+'12,+pages+1179   1190,+jeju+island.
1220. https://www.mitpressjournals.org/servlet/linkout?suffix=r39&dbid=16&doi=10.1162/coli_a_00226&key=10.5087/dad.2010.003
1221. http://scholar.google.com/scholar?hl=en&q=hernault,+hugo,+helmut+prendinger,+david+duverle,+and+mitsuru+ishizuka.+2010.++hilda:+a+discourse+parser+using+support+vector+machine+classification.+dialogue+and+discourse,+1(3):1   33.
1222. http://scholar.google.com/scholar?hl=en&q=hirst,+graeme+and+david+st-onge.+1997.++lexical+chains+as+representation+of+context+for+the+detection+and+correction+of+malapropisms.+in+christiane+fellbaum,+editor,+id138:+an+electronic+lexical+database+and+some+of+its+applications.+mit+press,+pages+305   332.
1223. https://www.mitpressjournals.org/servlet/linkout?suffix=r41&dbid=16&doi=10.1162/coli_a_00226&key=10.1207/s15516709cog0301_4
1224. http://scholar.google.com/scholar?hl=en&q=hobbs,+jerry.+1979.++coherence+and+coreference.+cognitive+science,+3:67   90.
1225. https://www.mitpressjournals.org/servlet/linkout?suffix=r42&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/1654494.1654500
1226. http://scholar.google.com/scholar?hl=en&q=huang,+liang+and+david+chiang.+2005.++better+k-best+parsing.+in+proceedings+of+the+ninth+international+workshop+on+parsing+technology,+parsing+'05,+pages+53   64,+stroudsburg,+pa.
1227. https://www.mitpressjournals.org/servlet/linkout?suffix=r43&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/v1/p14-1002
1228. http://scholar.google.com/scholar?hl=en&q=ji,+yangfeng+and+jacob+eisenstein.+2014.++representation+learning+for+text-level+discourse+parsing.+in+proceedings+of+the+52nd+annual+meeting+of+the+association+for+computational+linguistics+(volume+1:+long+papers),+pages+13   24,+baltimore,+md.
1229. http://scholar.google.com/scholar?hl=en&q=joty,+shafiq,+giuseppe+carenini,+and+raymond+t.+ng.+2012.++a+novel+discriminative+framework+for+sentence-level+discourse+analysis.+in+proceedings+of+the+2012+joint+conference+on+empirical+methods+in+natural+language+processing+and+computational+natural+language+learning,+emnlp-conll+'12,+pages+904   915,+jeju+island.
1230. http://scholar.google.com/scholar?hl=en&q=joty,+shafiq,+giuseppe+carenini,+and+raymond+t.+ng.+2013.++topic+segmentation+and+labeling+in+asynchronous+conversations.+journal+of+artificial+intelligence+research+(jair),+47:521   573.
1231. http://scholar.google.com/scholar?hl=en&q=joty,+shafiq,+giuseppe+carenini,+raymond+t.+ng,+and+yashar+mehdad.+2013.++combining+intra-+and+multi-sentential+rhetorical+parsing+for+document-level+discourse+analysis.+in+proceedings+of+the+51st+annual+meeting+of+the+association+for+computational+linguistics,+acl+'13,+pages+486   496,+sofia.
1232. https://www.mitpressjournals.org/servlet/linkout?suffix=r47&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/v1/w14-3352
1233. http://scholar.google.com/scholar?hl=en&q=joty,+shafiq,+francisco+guzm  n,+llu  s+m  rquez,+and+preslav+nakov.+2014.++discotk:+using+discourse+structure+for+machine+translation+evaluation.+in+proceedings+of+the+ninth+workshop+on+statistical+machine+translation,+wmt+'14,+pages+402   408,+baltimore,+md.
1234. http://scholar.google.com/scholar?hl=en&q=jurafsky,+daniel+and+james+martin.+2008.++statistical+parsing.+in+speech+and+language+processing,+chapter+14.+prentice+hall.
1235. http://scholar.google.com/scholar?hl=en&q=knight,+kevin+and+jonathan+graehl.+2005.++an+overview+of+probabilistic+tree+transducers+for+natural+language+processing.+in+computational+linguistics+and+intelligent+text+processing,+volume+3406+of+lecture+notes+in+computer+science.+springer,+berlin+heidelberg,+pages+1   24.
1236. https://www.mitpressjournals.org/servlet/linkout?suffix=r50&dbid=16&doi=10.1162/coli_a_00226&key=10.1080/01638539409544883
1237. http://scholar.google.com/scholar?hl=en&q=knott,+alistair+and+robert+dale.+1994.++using+linguistic+phenomena+to+motivate+a+set+of+coherence+relations.+discourse+processes,+18:35   62.
1238. http://scholar.google.com/scholar?hl=en&q=koller,+alexander,+michaela+regneri,+and+stefan+thater.+2008.++regular+tree+grammars+as+a+formalism+for+scope+underspecification.+in+proceedings+of+the+46th+annual+meeting+of+the+association+for+computational+linguistics+on+human+language+technologies,+pages+218   226,+columbus,+oh.
1239. http://scholar.google.com/scholar?hl=en&q=lafferty,+john,+andrew+mccallum,+and+fernando+pereira.+2001.++conditional+random+fields:+probabilistic+models+for+segmenting+and+labeling+sequence+data.+in+proceedings+of+the+eighteenth+international+conference+on+machine+learning,+pages+282   289,+san+francisco,+ca.
1240. http://scholar.google.com/scholar?hl=en&q=lazaridou,+angeliki,+ivan+titov,+and+caroline+sporleder.+2013.++a+bayesian+model+for+joint+unsupervised+induction+of+sentiment,+aspect+and+discourse+representations.+in+proceedings+of+the+51st+annual+meeting+of+the+association+for+computational+linguistics,+acl+'13,+sofia.
1241. https://www.mitpressjournals.org/servlet/linkout?suffix=r54&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/v1/d14-1220
1242. http://scholar.google.com/scholar?hl=en&q=li,+jiwei,+rumeng+li,+and+eduard+hovy.+2014.++recursive+deep+models+for+discourse+parsing.+in+proceedings+of+the+2014+conference+on+empirical+methods+in+natural+language+processing+(emnlp),+pages+2061   2069,+doha.
1243. https://www.mitpressjournals.org/servlet/linkout?suffix=r55&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/v1/p14-1003
1244. http://scholar.google.com/scholar?hl=en&q=li,+sujian,+liang+wang,+ziqiang+cao,+and+wenjie+li.+2014.++text-level+discourse+dependency+parsing.+in+proceedings+of+the+52nd+annual+meeting+of+the+association+for+computational+linguistics+(volume+1:+long+papers),+pages+25   35,+baltimore,+md.
1245. http://scholar.google.com/scholar?hl=en&q=louis,+annie,+aravind+joshi,+and+ani+nenkova.+2010.++discourse+indicators+for+content+selection+in+summarization.+in+proceedings+of+the+11th+annual+meeting+of+the+special+interest+group+on+discourse+and+dialogue,+sigdial+'10,+pages+147   156,+tokyo.
1246. http://scholar.google.com/scholar?hl=en&q=mach    ek,+matou  +and+ond  ej+bojar.+2014.+results+of+the+wmt14+metrics+shared+task.+in+proceedings+of+the+ninth+workshop+on+statistical+machine+translation,+baltimore,+md.
1247. https://www.mitpressjournals.org/servlet/linkout?suffix=r58&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/981658.981695
1248. http://scholar.google.com/scholar?hl=en&q=magerman,+david.+1995.++statistical+decision-tree+models+for+parsing.+in+proceedings+of+the+33rd+annual+meeting+of+the+association+for+computational+linguistics,+acl'95,+pages+276   283,+cambridge,+ma.
1249. https://www.mitpressjournals.org/servlet/linkout?suffix=r59&dbid=16&doi=10.1162/coli_a_00226&key=10.1515/text.1.1988.8.3.243
1250. http://scholar.google.com/scholar?hl=en&q=mann,+william+and+sandra+thompson.+1988.++rhetorical+structure+theory:+toward+a+functional+theory+of+text+organization.+text,+8(3):243   281.
1251. https://www.mitpressjournals.org/servlet/linkout?suffix=r60&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/1034678.1034736
1252. http://scholar.google.com/scholar?hl=en&q=marcu,+daniel.+1999.++a+decision-based+approach+to+rhetorical+parsing.+in+proceedings+of+the+37th+annual+meeting+of+the+association+for+computational+linguistics+on+computational+linguistics,+acl'99,+pages+365   372,+morristown,+nj.
1253. https://www.mitpressjournals.org/doi/10.1162/089120100561755
1254. http://scholar.google.com/scholar?hl=en&q=marcu,+daniel.+2000a.++the+rhetorical+parsing+of+unrestricted+texts:+a+surface-based+approach.+computational+linguistics,+26:395   448.
1255. http://scholar.google.com/scholar?hl=en&q=marcu,+daniel.+2000b.+the+theory+and+practice+of+discourse+parsing+and+summarization.+mit+press,+cambridge,+ma.
1256. http://scholar.google.com/scholar?hl=en&q=marcu,+daniel+and+abdessamad+echihabi.+2002.++an+unsupervised+approach+to+recognizing+discourse+relations.+in+proceedings+of+the+40th+annual+meeting+of+the+association+for+computational+linguistics,+acl'02,+pages+368   375.+philadelphia,+pa.
1257. http://scholar.google.com/scholar?hl=en&q=marcus,+mitchell,+mary+marcinkiewicz,+and+beatrice+santorini.+1994.++building+a+large+annotated+corpus+of+english:+the+penn+treebank.+computational+linguistics,+19(2):313   330.
1258. http://scholar.google.com/scholar?hl=en&q=martin,+james,+1992.+english+text:+system+and+structure.+john+benjamins+publishing+company,+philadelphia/amsterdam.
1259. http://scholar.google.com/scholar?hl=en&q=maslennikov,+mstislav+and+tat-seng+chua.+2007.++a+multi-resolution+framework+for+information+extraction+from+free+text.+in+proceedings+of+the+45th+annual+meeting+of+the+association+for+computational+linguistics,+pages+592   599,+prague.
1260. http://mallet.cs.umass.edu/
1261. http://scholar.google.com/scholar?hl=en&q=mccallum,+andrew.+2002.++mallet:+a+machine+learning+for+language+toolkit.+http://mallet.cs.umass.edu.
1262. http://scholar.google.com/scholar?hl=en&q=mccallum,+andrew,+dayne+freitag,+and+fernando+c.+n.+pereira.+2000.++maximum+id178+markov+models+for+information+extraction+and+segmentation.+in+proceedings+of+the+seventeenth+international+conference+on+machine+learning,+icml+'00,+pages+591   598,+san+francisco,+ca.
1263. https://www.mitpressjournals.org/servlet/linkout?suffix=r69&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/1219840.1219852
1264. http://scholar.google.com/scholar?hl=en&q=mcdonald,+ryan,+koby+crammer,+and+fernando+pereira.+2005.++online+large-margin+training+of+dependency+parsers.+in+proceedings+of+the+43rd+annual+meeting+of+the+association+for+computational+linguistics,+acl+'05,+pages+91   98,+ann+arbor,+mi.
1265. https://www.mitpressjournals.org/servlet/linkout?suffix=r70&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/1220575.1220641
1266. http://scholar.google.com/scholar?hl=en&q=mcdonald,+ryan,+fernando+pereira,+kiril+ribarov,+and+jan+haji  .+2005.++non-projective+dependency+parsing+using+spanning+tree+algorithms.+in+proceedings+of+the+conference+on+human+language+technology+and+empirical+methods+in+natural+language+processing,+hlt+'05,+pages+523   530,+stroudsburg,+pa.
1267. http://scholar.google.com/scholar?hl=en&q=morris,+jane+and+graeme+hirst.+1991.++lexical+cohesion+computed+by+thesaural+relations+as+an+indicator+of+structure+of+text.+computational+linguistics,+17(1):21   48.
1268. http://scholar.google.com/scholar?hl=en&q=murphy,+kevin.+2012.+machine+learning:+a+probabilistic+perspective.+the+mit+press.+cambridge,+ma.
1269. https://www.mitpressjournals.org/servlet/linkout?suffix=r73&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/1218955.1218990
1270. http://scholar.google.com/scholar?hl=en&q=pang,+bo+and+lillian+lee.+2004.++a+sentimental+education:+sentiment+analysis+using+subjectivity+summarization+based+on+minimum+cuts.+in+proceedings+of+the+42nd+annual+meeting+of+the+association+for+computational+linguistics,+acl+'04,+pages+271   278.+barcelona.
1271. http://scholar.google.com/scholar?hl=en&q=pitler,+emily+and+ani+nenkova.+2009.++using+syntax+to+disambiguate+explicit+discourse+connectives+in+text.+in+proceedings+of+the+acl-ijcnlp+2009+conference+short+papers,+aclshort+'09,+pages+13   16,+suntec.
1272. http://scholar.google.com/scholar?hl=en&q=poole,+david+and+alan+mackworth,+2010.+artificial+intelligence:+foundations+of+computational+agents.+cambridge+university+press.
1273. http://scholar.google.com/scholar?hl=en&q=prasad,+rashmi,+nikhil+dinesh,+alan+lee,+eleni+miltsakaki,+livio+robaldo,+aravind+joshi,+and+bonnie+webber.+2008.++the+penn+discourse+treebank+2.0.+in+proceedings+of+the+sixth+international+conference+on+language+resources+and+evaluation+(lrec),+pages+2961   2968,+marrakech.
1274. http://scholar.google.com/scholar?hl=en&q=prasad,+rashmi,+aravind+joshi,+nikhil+dinesh,+alan+lee,+eleni+miltsakaki,+and+bonnie+webber.+2005.++the+penn+discourse+treebank+as+a+resource+for+natural+language+generation.+in+proceedings+of+the+corpus+linguistics+workshop+on+using+corpora+for+natural+language+generation,+pages+25   32,+birmingham.
1275. https://www.mitpressjournals.org/servlet/linkout?suffix=r78&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/1557690.1557761
1276. http://scholar.google.com/scholar?hl=en&q=regneri,+michaela,+markus+egg,+and+alexander+koller.+2008.++efficient+processing+of+underspecified+discourse+representations.+in+proceedings+of+the+46th+annual+meeting+of+the+association+for+computational+linguistics+on+human+language+technologies:+short+papers,+hlt-short+'08,+pages+245   248,+columbus,+oh.
1277. https://www.mitpressjournals.org/servlet/linkout?suffix=r79&dbid=16&doi=10.1162/coli_a_00226&key=10.1093/jos/10.2.123
1278. http://scholar.google.com/scholar?hl=en&q=reyle,+uwe.+1993.++dealing+with+ambiguities+by+underspecification:+construction,+representation+and+deduction.+journal+of+semantics,+10(2):123   179.
1279. https://www.mitpressjournals.org/servlet/linkout?suffix=r80&dbid=16&doi=10.1162/coli_a_00226&key=10.1023/a:1007649029923
1280. http://scholar.google.com/scholar?hl=en&q=schapire,+robert+e.+and+yoram+singer.+2000.++boostexter:+a+boosting-based+system+for+text+categorization.+machine+learning,+39(2   3):135   168.
1281. http://scholar.google.com/scholar?hl=en&q=schauer,+holger+and+udo+hahn.+2001.++anaphoric+cues+for+coherence+relations.+in+proceedings+of+the+conference+on+recent+advances+in+natural+language+processing,+ranlp+'01,+pages+228   234,+tzigov+chark.
1282. http://scholar.google.com/scholar?hl=en&q=schilder,+frank.+2002.++robust+discourse+parsing+via+discourse+markers,+topicality+and+position.+natural+language+engineering,+8(3):235   255.
1283. https://www.mitpressjournals.org/servlet/linkout?suffix=r83&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/1073445.1073473
1284. http://scholar.google.com/scholar?hl=en&q=sha,+fei+and+fernando+pereira.+2003.++shallow+parsing+with+conditional+random+fields.+in+proceedings+of+the+2003+conference+of+the+north+american+chapter+of+the+association+for+computational+linguistics+on+human+language+technology+-+volume+1,+naacl-hlt'03,+pages+134   141,+edmonton.
1285. https://www.mitpressjournals.org/doi/10.1162/089120102762671954
1286. http://scholar.google.com/scholar?hl=en&q=silber,+gregory+and+kathleen+mccoy.+2002.++efficiently+computed+lexical+chains+as+an+intermediate+representation+for+automatic+text+summarization.+computational+linguistics,+28(4):487   496.
1287. http://scholar.google.com/scholar?hl=en&q=smith,+noah+a.+2011.+linguistic+structure+prediction.+synthesis+lectures+on+human+language+technologies.+morgan+and+claypool.
1288. http://scholar.google.com/scholar?hl=en&q=socher,+richard,+john+bauer,+christopher+d.+manning,+and+ng+andrew+y.+2013a.++parsing+with+compositional+vector+grammars.+in+proceedings+of+the+51st+annual+meeting+of+the+association+for+computational+linguistics+(volume+1:+long+papers),+pages+455   465,+sofia.
1289. http://scholar.google.com/scholar?hl=en&q=socher,+richard,+alex+perelygin,+jean+wu,+jason+chuang,+christopher+d.+manning,+andrew+ng,+and+christopher+potts.+2013b.++recursive+deep+models+for+semantic+compositionality+over+a+sentiment+treebank.+in+proceedings+of+the+2013+conference+on+empirical+methods+in+natural+language+processing,+pages+1631   1642,+seattle,+wa.
1290. http://scholar.google.com/scholar?hl=en&q=somasundaran,+s.+2010.+discourse-level+relations+for+opinion+analysis.+ph.d.+thesis,+university+of+pittsburgh,+pa.
1291. https://www.mitpressjournals.org/servlet/linkout?suffix=r89&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/1073445.1073475
1292. http://scholar.google.com/scholar?hl=en&q=soricut,+radu+and+daniel+marcu.+2003.++sentence+level+discourse+parsing+using+syntactic+and+lexical+information.+in+proceedings+of+the+2003+conference+of+the+north+american+chapter+of+the+association+for+computational+linguistics+on+human+language+technology+-+volume+1,+naacl'03,+pages+149   156,+edmonton.
1293. http://scholar.google.com/scholar?hl=en&q=sporleder,+caroline.+2007.++manually+vs.+automatically+labelled+data+in+discourse+relation+classification.+effects+of+example+and+feature+selection.+ldv+forum,+22(1):1   20.
1294. http://scholar.google.com/scholar?hl=en&q=sporleder,+caroline+and+mirella+lapata.+2004.++automatic+paragraph+identification:+a+study+across+languages+and+domains.+in+proceedings+of+the+2004+conference+on+empirical+methods+in+natural+language+processing,+emnlp+'04,+pages+72   79,+barcelona.
1295. https://www.mitpressjournals.org/servlet/linkout?suffix=r92&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/1220575.1220608
1296. http://scholar.google.com/scholar?hl=en&q=sporleder,+caroline+and+mirella+lapata.+2005.++discourse+chunking+and+its+application+to+sentence+compression.+in+proceedings+of+the+conference+on+human+language+technology+and+empirical+methods+in+natural+language+processing,+hlt-emnlp'05,+pages+257   264,+vancouver.
1297. http://scholar.google.com/scholar?hl=en&q=sporleder,+caroline+and+alex+lascarides.+2005.++exploiting+linguistic+cues+to+classify+rhetorical+relations.+in+proceedings+of+recent+advances+in+natural+language+processing+(ranlp),+pages+157   166,+bulgaria.
1298. https://www.mitpressjournals.org/servlet/linkout?suffix=r94&dbid=16&doi=10.1162/coli_a_00226&key=10.1017/s1351324906004451
1299. http://scholar.google.com/scholar?hl=en&q=sporleder,+caroline+and+alex+lascarides.+2008.++using+automatically+labelled+examples+to+classify+rhetorical+relations:+an+assessment.+natural+language+engineering,+14(3):369   416.
1300. http://scholar.google.com/scholar?hl=en&q=stede,+manfred.+2004.++the+potsdam+commentary+corpus.+in+proceedings+of+the+acl-04+workshop+on+discourse+annotation,+pages+96   102,+barcelona.
1301. http://scholar.google.com/scholar?hl=en&q=stede,+manfred.+2011.+discourse+processing.+synthesis+lectures+on+human+language+technologies.+morgan+and+claypool+publishers.
1302. https://www.mitpressjournals.org/servlet/linkout?suffix=r97&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/1620754.1620837
1303. http://scholar.google.com/scholar?hl=en&q=subba,+rajen+and+barbara+di-eugenio.+2009.++an+effective+discourse+parser+that+uses+rich+linguistic+information.+in+proceedings+of+human+language+technologies:+the+2009+annual+conference+of+the+north+american+chapter+of+the+association+for+computational+linguistics,+hlt-naacl'09,+pages+566   574,+boulder,+co.
1304. https://www.mitpressjournals.org/servlet/linkout?suffix=r98&dbid=16&doi=10.1162/coli_a_00226&key=10.1561/2200000013
1305. http://scholar.google.com/scholar?hl=en&q=sutton,+charles+and+andrew+mccallum.+2012.++an+introduction+to+conditional+random+fields.+foundations+and+trends+in+machine+learning,+4(4):267   373.
1306. http://scholar.google.com/scholar?hl=en&q=sutton,+charles,+andrew+mccallum,+and+khashayar+rohanimanesh.+2007.++dynamic+conditional+random+fields:+factorized+probabilistic+models+for+labeling+and+segmenting+sequence+data.+journal+of+machine+learning+research+(jmlr),+8:693   723.
1307. https://www.mitpressjournals.org/servlet/linkout?suffix=r100&dbid=16&doi=10.1162/coli_a_00226&key=10.1016/j.pragma.2005.09.010
1308. http://scholar.google.com/scholar?hl=en&q=taboada,+maite.+2006.++discourse+markers+as+signals+(or+not)+of+rhetorical+relations.+journal+of+pragmatics,+38(4):567   592.
1309. https://www.mitpressjournals.org/doi/10.1162/coli_a_00049
1310. http://scholar.google.com/scholar?hl=en&q=taboada,+maite,+julian+brooke,+milan+tofiloski,+kimberly+voll,+and+manfred+stede.+2011.++lexicon-based+methods+for+sentiment+analysis.+computational+linguistics,+37(2):267   307.
1311. https://www.mitpressjournals.org/servlet/linkout?suffix=r102&dbid=16&doi=10.1162/coli_a_00226&key=10.1177/1461445606061881
1312. http://scholar.google.com/scholar?hl=en&q=taboada,+maite+and+william+c.+mann.+2006.++rhetorical+structure+theory:+looking+back+and+moving+ahead.+discourse+studies,+8(3):423   459.
1313. https://www.mitpressjournals.org/doi/10.1162/089120102762671936
1314. http://scholar.google.com/scholar?hl=en&q=teufel,+simone+and+marc+moens.+2002.++summarizing+scientific+articles:+experiments+with+relevance+and+rhetorical+status.+computational+linguistics,+28(4):409   445.
1315. https://www.mitpressjournals.org/servlet/linkout?suffix=r104&dbid=16&doi=10.1162/coli_a_00226&key=10.1145/1277741.1277883
1316. http://scholar.google.com/scholar?hl=en&q=verberne,+suzan,+lou+boves,+nelleke+oostdijk,+and+peter-arno+coppen.+2007.++evaluating+discourse-based+answer+extraction+for+why-question+answering.+in+proceedings+of+the+30th+annual+international+acm+sigir+conference+on+research+and+development+in+information+retrieval,+sigir'07,+pages+735   736,+amsterdam.
1317. http://scholar.google.com/scholar?hl=en&q=vliet,+nynke+and+gisela+redeker.+2011.++complex+sentences+as+leaky+units+in+discourse+parsing.+in+proceedings+of+constraints+in+discourse,+pages+1   9,+agay   saint+raphael.
1318. https://www.mitpressjournals.org/servlet/linkout?suffix=r106&dbid=16&doi=10.1162/coli_a_00226&key=10.1207/s15516709cog2805_6
1319. http://scholar.google.com/scholar?hl=en&q=webber,+b.+2004.++d-ltag:+extending+lexicalized+tag+to+discourse.+cognitive+science,+28(5):751   779.
1320. http://scholar.google.com/scholar?hl=en&q=webber,+bonnie,+andrei+popescu-belis,+katja+markert,+and+j  rg+tiedemann,+editors.+2013.+proceedings+of+the+workshop+on+discourse+in+machine+translation.+acl,+sofia.
1321. http://scholar.google.com/scholar?hl=en&q=wick,+michael,+khashayar+rohanimanesh,+kedare+bellare,+aron+culotta,+and+andrew+mccallum.+2011.++samplerank:+training+factor+graphs+with+atomic+gradients.+in+proceedings+of+the+28th+international+conference+on+machine+learning,+icml'11,+pages+777   784.+bellevue,+wa.
1322. https://www.mitpressjournals.org/doi/10.1162/0891201054223977
1323. http://scholar.google.com/scholar?hl=en&q=wolf,+florian+and+edward+gibson.+2005.++representing+discourse+coherence:+a+corpus-based+study.+computational+linguistics,+31:249   288.
1324. http://scholar.google.com/scholar?hl=en&q=zhang,+yuan,+tao+lei,+regina+barzilay,+tommi+jaakkola,+and+amir+globerson.+2014.++steps+to+excellence:+simple+id136+with+refined+scoring+of+dependency+trees.+in+proceedings+of+the+52nd+annual+meeting+of+the+association+for+computational+linguistics,+pages+197   207,+baltimore,+md.
1325. https://www.mitpressjournals.org/cdn-cgi/l/email-protection#ea9980859e93aa9b8cc485988dc49b8b
1326. https://www.mitpressjournals.org/cdn-cgi/l/email-protection#2a494b584f444344436a4959045f484904494b
1327. https://www.mitpressjournals.org/cdn-cgi/l/email-protection#43312d240320306d3621206d2022
1328. https://www.mitpressjournals.org/forthcoming/coli
1329. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1330. https://www.mitpressjournals.org/author/joty,+shafiq
1331. https://www.mitpressjournals.org/author/carenini,+giuseppe
1332. https://www.mitpressjournals.org/author/ng,+raymond+t
1333. https://doi.org/10.1162/coli_a_00226
1334. https://www.mitpressjournals.org/doi/full/10.1162/coli_a_00226
1335. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#authors-content
1336. http://www.mitpressjournals.org/doi/pdf/10.1162/coli_a_00226
1337. http://www.mitpressjournals.org/doi/pdfplus/10.1162/coli_a_00226
1338. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i1
1339. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#abstract
1340. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i4
1341. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1342. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1343. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1344. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1345. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1346. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1347. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1348. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1349. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1350. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1351. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1352. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1353. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1354. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1355. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1356. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1357. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1358. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1359. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1360. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1361. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1362. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1363. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1364. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1365. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1366. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1367. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1368. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1369. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1370. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1371. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1372. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1373. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1374. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1375. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1376. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1377. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1378. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1379. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1380. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1381. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1382. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1383. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1384. javascript:popref('s2')
1385. javascript:popref('s3')
1386. javascript:popref('s4')
1387. javascript:popref('s5')
1388. javascript:popref('s6')
1389. javascript:popref('s7')
1390. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i1
1391. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i7
1392. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1393. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1394. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1395. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1396. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1397. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1398. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1399. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1400. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1401. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1402. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1403. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1404. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1405. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1406. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn4
1407. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1408. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1409. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1410. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1411. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1412. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1413. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1414. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1415. javascript:popref('s1')
1416. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1417. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1418. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn5
1419. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1420. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1421. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1422. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1423. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1424. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1425. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1426. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1427. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1428. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1429. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1430. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1431. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1432. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn6
1433. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1434. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1435. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1436. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1437. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1438. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1439. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1440. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1441. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1442. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1443. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1444. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1445. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1446. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1447. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1448. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1449. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1450. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1451. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1452. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i4
1453. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i12
1454. javascript:popref('s9')
1455. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn7
1456. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn8
1457. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1458. javascript:popref('s5')
1459. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1460. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1461. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1462. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1463. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1464. javascript:popref('s2')
1465. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1466. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1467. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1468. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1469. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1470. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1471. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn9
1472. javascript:popref('s4a1')
1473. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1474. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1475. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1476. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn10
1477. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1478. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1479. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1480. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1481. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1482. javascript:popref('s4c')
1483. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i7
1484. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i58
1485. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1486. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1487. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1488. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1489. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1490. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1491. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1492. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1493. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1494. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1495. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1496. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1497. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1498. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1499. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1500. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1501. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1502. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1503. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1504. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1505. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1506. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1507. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1508. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1509. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1510. javascript:popref('s4b')
1511. javascript:popref('s4c')
1512. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1513. javascript:popref('e1')
1514. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1515. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1516. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1517. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1518. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1519. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1520. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1521. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1522. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1523. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1524. javascript:popref('t1')
1525. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1526. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1527. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1528. javascript:popref('t1')
1529. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1530. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1531. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1532. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1533. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1534. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn11
1535. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1536. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1537. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1538. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1539. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1540. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1541. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1542. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1543. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1544. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1545. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1546. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1547. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1548. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1549. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1550. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1551. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1552. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1553. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1554. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1555. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1556. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1557. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1558. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1559. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1560. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1561. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1562. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1563. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1564. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1565. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1566. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1567. javascript:popref('s4a3')
1568. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1569. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1570. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1571. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1572. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1573. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1574. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1575. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1576. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1577. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1578. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1579. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn12
1580. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1581. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1582. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn13
1583. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1584. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1585. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1586. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1587. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1588. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1589. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1590. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1591. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1592. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1593. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1594. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1595. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1596. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1597. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i12
1598. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i63
1599. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1600. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1601. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1602. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1603. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1604. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1605. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1606. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1607. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1608. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1609. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1610. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn14
1611. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i58
1612. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i108
1613. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1614. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1615. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1616. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1617. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1618. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1619. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1620. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1621. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1622. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1623. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1624. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1625. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1626. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1627. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1628. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1629. javascript:popref('t2')
1630. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1631. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn15
1632. javascript:popref('t2')
1633. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1634. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1635. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1636. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1637. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1638. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1639. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1640. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1641. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1642. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1643. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1644. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1645. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn16
1646. javascript:popref('t3')
1647. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1648. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1649. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn17
1650. javascript:popref('s6a')
1651. javascript:popref('t3')
1652. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1653. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1654. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1655. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1656. javascript:popref('s6c1')
1657. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1658. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1659. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1660. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1661. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn18
1662. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1663. javascript:popref('t4')
1664. javascript:popref('s6b2')
1665. javascript:popref('t4')
1666. javascript:popref('t4')
1667. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1668. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn19
1669. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn20
1670. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1671. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn21
1672. javascript:popref('t5')
1673. javascript:popref('t5')
1674. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1675. javascript:popref('t4')
1676. javascript:popref('t5')
1677. javascript:popref('t3')
1678. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1679. javascript:popref('s4c')
1680. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1681. javascript:popref('t6')
1682. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1683. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1684. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn22
1685. javascript:popref('t6')
1686. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1687. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1688. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn23
1689. javascript:popref('t6')
1690. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1691. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1692. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1693. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1694. javascript:popref('s4c2')
1695. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1696. javascript:popref('s4b')
1697. javascript:popref('t2')
1698. javascript:popref('t7')
1699. javascript:popref('t7')
1700. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1701. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1702. javascript:popref('t8')
1703. javascript:popref('s6b2')
1704. javascript:popref('t8')
1705. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1706. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1707. javascript:popref('t9')
1708. javascript:popref('s4a4')
1709. javascript:popref('t9')
1710. javascript:popref('t9')
1711. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1712. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1713. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn24
1714. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1715. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1716. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1717. javascript:popref('s6d2')
1718. javascript:popref('s6d3')
1719. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1720. javascript:popref('t2')
1721. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn25
1722. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1723. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1724. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#fn26
1725. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1726. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1727. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1728. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1729. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1730. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1731. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1732. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i63
1733. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i109
1734. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1735. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1736. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1737. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1738. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1739. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1740. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1741. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1742. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1743. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1744. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1745. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1746. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1747. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1748. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1749. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1750. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1751. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i108
1752. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i110
1753. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1754. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1755. javascript:popref('s4b')
1756. javascript:popref('s6d4')
1757. javascript:popref('s6d2')
1758. javascript:popref('s6d3')
1759. javascript:popref('s6d5')
1760. javascript:popref('s6d6')
1761. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i109
1762. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i112
1763. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i110
1764. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i113
1765. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i112
1766. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i114
1767. http://www.isi.edu/licensed-sw/spade/
1768. http://nlp.prendingerlab.net/hilda/
1769. http://109.228.0.153/discourse_parser_demo/
1770. http://alt.qcri.org/tools/
1771. javascript:popref('s9')
1772. http://www.bbc.co.uk/news/world-asia-26106490
1773. http://cogcomp.cs.illinois.edu/page/software
1774. http://alt.qcri.org/tools/
1775. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1776. javascript:popref('t4')
1777. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1778. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1779. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1780. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1781. javascript:popref('t4')
1782. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#_i113
1783. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226#citart1
1784. https://www.mitpressjournals.org/servlet/linkout?suffix=r1&dbid=16&doi=10.1162/coli_a_00226&key=10.1016/s0196-6774(03)00050-6
1785. http://scholar.google.com/scholar?hl=en&q=althaus,+ernst,+denys+duchier,+alexander+koller,+kurt+mehlhorn,+joachim+niehren,+and+sven+thiel.+2003.++an+efficient+graph+algorithm+for+dominance+constraints.+journal+of+algorithms,+48(1):194   219.
1786. http://scholar.google.com/scholar?hl=en&q=asher,+nicholas+and+alex+lascarides,+2003.+logics+of+conversation.+cambridge+university+press.
1787. http://scholar.google.com/scholar?hl=en&q=barzilay,+regina+and+michael+elhadad.+1997.++using+lexical+chains+for+text+summarization.+in+proceedings+of+the+35th+annual+meeting+of+the+association+for+computational+linguistics+and+the+8th+european+chapter+meeting+of+the+association+for+computational+linguistics,+workshop+on+intelligent+scalable+test+summarization,+pages+10   17,+madrid.
1788. https://www.mitpressjournals.org/servlet/linkout?suffix=r4&dbid=16&doi=10.1162/coli_a_00226&key=10.1142/s1793351x11001328
1789. http://scholar.google.com/scholar?hl=en&q=biran,+or+and+owen+rambow.+2011.++identifying+justifications+in+written+dialogs+by+classifying+text+as+argumentative.+international+journal+of+semantic+computing,+5(4):363   381.
1790. http://scholar.google.com/scholar?hl=en&q=blair-goldensohn,+sasha,+kathleen+mckeown,+and+owen+rambow.+2007.++building+and+refining+rhetorical-semantic+relation+models.+in+proceedings+of+the+human+language+technologies:+the+annual+conference+of+the+north+american+chapter+of+the+association+for+computational+linguistics,+hlt-naacl'07,+pages+428   435.+rochester,+ny.
1791. http://scholar.google.com/scholar?hl=en&q=blitzer,+john.+2008.+domain+adaptation+of+natural+language+processing+systems.+ph.d.+thesis,+university+of+pennsylvania.
1792. https://www.mitpressjournals.org/servlet/linkout?suffix=r7&dbid=16&doi=10.1162/coli_a_00226&key=10.1007/bf00058655
1793. http://scholar.google.com/scholar?hl=en&q=breiman,+leo.+1996.++id112+predictors.+machine+learning,+24(2):123   140.
1794. http://scholar.google.com/scholar?hl=en&q=carlson,+lynn+and+daniel+marcu.+2001.++discourse+tagging+reference+manual.+technical+report+isi-tr-545,+university+of+southern+california+information+sciences+institute.
1795. http://scholar.google.com/scholar?hl=en&q=carlson,+lynn,+daniel+marcu,+and+mary+ellen+okurowski.+2002.++rst+discourse+treebank+(rst   dt)+ldc2002t07.+linguistic+data+consortium,+philadelphia.
1796. http://scholar.google.com/scholar?hl=en&q=chali,+yllias+and+shafiq+joty.+2007.++word+sense+disambiguation+using+lexical+cohesion.+in+proceedings+of+semeval-2007,+pages+476   479,+prague.
1797. http://scholar.google.com/scholar?hl=en&q=charniak,+eugene.+2000.++a+maximum-id178-inspired+parser.+in+proceedings+of+the+1st+north+american+chapter+of+the+association+for+computational+linguistics+conference,+naacl'00,+pages+132   139,+seattle,+wa.
1798. https://www.mitpressjournals.org/servlet/linkout?suffix=r12&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/1219840.1219862
1799. http://scholar.google.com/scholar?hl=en&q=charniak,+eugene+and+mark+johnson.+2005.++coarse-to-fine+n-best+parsing+and+maxent+discriminative+reranking.+in+proceedings+of+the+43rd+annual+meeting+of+the+association+for+computational+linguistics,+acl'05,+pages+173   180,+ann+arbor,+mi.
1800. http://scholar.google.com/scholar?hl=en&q=christensen,+janara,+mausam,+stephen+soderland,+and+oren+etzioni.+2013.++towards+coherent+multi-document+summarization.+in+proceedings+of+the+2013+conference+of+the+north+american+chapter+of+the+association+for+computational+linguistics:+human+language+technologies,+naacl-hlt'13,+pages+1163   1173,+atlanta,+ga.
1801. http://scholar.google.com/scholar?hl=en&q=christensen,+janara,+stephen+soderland,+gagan+bansal,+and+mausam.+2014.++hierarchical+summarization:+scaling+up+multi-document+summarization.+in+proceedings+of+the+52nd+annual+meeting+of+the+association+for+computational+linguistics,+acl'13,+pages+902   912,+baltimore,+md.
1802. https://www.mitpressjournals.org/doi/10.1162/089120103322753356
1803. http://scholar.google.com/scholar?hl=en&q=collins,+michael.+2003.++head-driven+statistical+models+for+natural+language+parsing.+computational+linguistics,+29(4):589   637.
1804. https://www.mitpressjournals.org/doi/10.1162/0891201053630273
1805. http://scholar.google.com/scholar?hl=en&q=collins,+michael+and+terry+koo.+2005.++discriminative+reranking+for+natural+language+parsing.+computational+linguistics,+31(1):25   70.
1806. http://scholar.google.com/scholar?hl=en&q=collobert,+ronan,+jason+weston,+l  on+bottou,+michael+karlen,+koray+kavukcuoglu,+and+pavel+kuksa.+2011.++natural+language+processing+(almost)+from+scratch.+journal+of+machine+learning+research,+12:2493   2537.
1807. https://www.mitpressjournals.org/servlet/linkout?suffix=r18&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/980845.980891
1808. http://scholar.google.com/scholar?hl=en&q=cristea,+dan,+nancy+ide,+and+laurent+romary.+1998.++veins+theory:+a+model+of+global+discourse+cohesion+and+coherence.+in+proceedings+of+the+36th+annual+meeting+of+the+association+for+computational+linguistics+and+of+the+17th+international+conference+on+computational+linguistics+(coling/acl'98),+pages+281   285.+montreal.
1809. http://scholar.google.com/scholar?hl=en&q=danlos,+laurence.+2009.++d-stag:+a+discourse+analysis+formalism+based+on+synchronous+tags.+tal,+50(1):111   143.
1810. http://scholar.google.com/scholar?hl=en&q=daum  ,+iii,+hal.+2007.++frustratingly+easy+domain+adaptation.+in+proceedings+of+the+45th+annual+meeting+of+the+association+for+computational+linguistics,+acl'07,+pages+256   263,+prague.
1811. http://scholar.google.com/scholar?hl=en&q=daum  ,+iii,+hal+and+daniel+marcu.+2002.++a+noisy-channel+model+for+document+compression.+in+proceedings+of+the+40th+annual+meeting+of+the+association+for+computational+linguistics,+acl+'02,+pages+449   456,+philadelphia,+pa.
1812. http://scholar.google.com/scholar?hl=en&q=dinarelli,+marco,+alessandro+moschitti,+and+giuseppe+riccardi.+2011.++discriminative+reranking+for+spoken+language+understanding.+ieee+transactions+on+audio,+speech+and+language+processing+(taslp),+20:526   539.
1813. http://scholar.google.com/scholar?hl=en&q=duverle,+david+and+helmut+prendinger.+2009.++a+novel+discourse+parser+based+on+support+vector+machine+classification.+in+proceedings+of+the+joint+conference+of+the+47th+annual+meeting+of+the+acl+and+the+4th+international+joint+conference+on+natural+language+processing+of+the+afnlp,+pages+665   673,+suntec.
1814. https://www.mitpressjournals.org/servlet/linkout?suffix=r24&dbid=16&doi=10.1162/coli_a_00226&key=10.1023/a:1017964622902
1815. http://scholar.google.com/scholar?hl=en&q=egg,+markus,+alexander+koller,+and+joachim+niehren.+2001.++the+constraint+language+for+lambda+structures.+journal+of+logic,+language+and+information,+10(4):457   485.
1816. https://www.mitpressjournals.org/servlet/linkout?suffix=r25&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/992628.992688
1817. http://scholar.google.com/scholar?hl=en&q=eisner,+jason.+1996.++three+new+probabilistic+models+for+dependency+parsing:+an+exploration.+in+proceedings+of+the+16th+conference+on+computational+linguistics+-+volume+1,+coling+'96,+pages+340   345,+copenhagen.
1818. http://scholar.google.com/scholar?hl=en&q=fellbaum,+christiane.+1998.+id138   an+electronic+lexical+database.+mit+press,+cambridge,+ma.
1819. http://scholar.google.com/scholar?hl=en&q=feng,+vanessa+and+graeme+hirst.+2012.++text-level+discourse+parsing+with+rich+linguistic+features.+in+proceedings+of+the+50th+annual+meeting+of+the+association+for+computational+linguistics,+acl+'12,+pages+60   68,+jeju+island.
1820. http://scholar.google.com/scholar?hl=en&q=feng,+vanessa+and+graeme+hirst.+2014.++a+linear-time+bottom-up+discourse+parser+with+constraints+and+post-editing.+in+proceedings+of+the+52nd+annual+meeting+of+the+association+for+computational+linguistics,+acl+'14,+pages+511   521,+baltimore,+md.
1821. http://scholar.google.com/scholar?hl=en&q=finkel,+jenny+rose,+alex+kleeman,+and+christopher+manning.+2008.++efficient,+feature-based,+conditional+random+field+parsing.+in+proceedings+of+the+46th+annual+meeting+of+the+association+for+computational+linguistics,+acl'08,+pages+959   967,+columbus,+oh.
1822. http://scholar.google.com/scholar?hl=en&q=fisher,+seeger+and+brian+roark.+2007.++the+utility+of+parse-derived+features+for+automatic+discourse+segmentation.+in+proceedings+of+the+45th+annual+meeting+of+the+association+for+computational+linguistics,+acl'07,+pages+488   495,+prague.
1823. http://scholar.google.com/scholar?hl=en&q=galley,+michel+and+kathleen+mckeown.+2003.++improving+word+sense+disambiguation+in+lexical+chaining.+in+proceedings+of+the+18th+international+joint+conference+on+artificial+intelligence,+ijcai'03,+pages+1486   1488,+acapulco.
1824. https://www.mitpressjournals.org/servlet/linkout?suffix=r32&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/1075096.1075167
1825. http://scholar.google.com/scholar?hl=en&q=galley,+michel,+kathleen+mckeown,+eric+fosler-lussier,+and+hongyan+jing.+2003.++discourse+segmentation+of+multi-party+conversation.+in+proceedings+of+the+41st+annual+meeting+of+the+association+for+computational+linguistics+-+volume+1,+acl+'03,+pages+562   569,+sapporo.
1826. http://scholar.google.com/scholar?hl=en&q=ghosh,+sucheta,+richard+johansson,+giuseppe+riccardi,+and+sara+tonelli.+2011.++shallow+discourse+parsing+with+conditional+random+fields.+in+proceedings+of+the+5th+international+joint+conference+on+natural+language+processing,+ijcnlp'11,+pages+1071   1079,+chiang+mai.
1827. https://www.mitpressjournals.org/servlet/linkout?suffix=r34&dbid=16&doi=10.1162/coli_a_00226&key=10.1007/s10590-011-9088-7
1828. http://scholar.google.com/scholar?hl=en&q=gim  nez,+jes  s+and+llu  s+m  rquez.+2010.++linguistic+measures+for+automatic+machine+translation+evaluation.+machine+translation,+24(3   4):77   86.
1829. https://www.mitpressjournals.org/servlet/linkout?suffix=r35&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/v1/d14-1027
1830. http://scholar.google.com/scholar?hl=en&q=guzm  n,+francisco,+shafiq+joty,+llu  s+m  rquez,+alessandro+moschitti,+preslav+nakov,+and+massimo+nicosia.+2014a.++learning+to+differentiate+better+from+worse+translations.+in+proceedings+of+the+2014+conference+on+empirical+methods+in+natural+language+processing+(emnlp),+pages+214   220,+doha.
1831. https://www.mitpressjournals.org/servlet/linkout?suffix=r36&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/v1/p14-1065
1832. http://scholar.google.com/scholar?hl=en&q=guzm  n,+francisco,+shafiq+joty,+llu  s+m  rquez,+and+preslav+nakov.+2014b.++using+discourse+structure+improves+machine+translation+evaluation.+in+proceedings+of+the+52nd+annual+meeting+of+the+association+for+computational+linguistics+(volume+1:+long+papers),+pages+687   698,+baltimore,+md.
1833. http://scholar.google.com/scholar?hl=en&q=halliday,+michael+and+ruqaiya+hasan.+1976.+cohesion+in+english.+longman,+london.
1834. http://scholar.google.com/scholar?hl=en&q=hardmeier,+christian,+joakim+nivre,+and+j  rg+tiedemann.+2012.++document-wide+decoding+for+phrase-based+statistical+machine+translation.+in+proceedings+of+the+2012+joint+conference+on+empirical+methods+in+natural+language+processing+and+computational+natural+language+learning,+emnlp-conll+'12,+pages+1179   1190,+jeju+island.
1835. https://www.mitpressjournals.org/servlet/linkout?suffix=r39&dbid=16&doi=10.1162/coli_a_00226&key=10.5087/dad.2010.003
1836. http://scholar.google.com/scholar?hl=en&q=hernault,+hugo,+helmut+prendinger,+david+duverle,+and+mitsuru+ishizuka.+2010.++hilda:+a+discourse+parser+using+support+vector+machine+classification.+dialogue+and+discourse,+1(3):1   33.
1837. http://scholar.google.com/scholar?hl=en&q=hirst,+graeme+and+david+st-onge.+1997.++lexical+chains+as+representation+of+context+for+the+detection+and+correction+of+malapropisms.+in+christiane+fellbaum,+editor,+id138:+an+electronic+lexical+database+and+some+of+its+applications.+mit+press,+pages+305   332.
1838. https://www.mitpressjournals.org/servlet/linkout?suffix=r41&dbid=16&doi=10.1162/coli_a_00226&key=10.1207/s15516709cog0301_4
1839. http://scholar.google.com/scholar?hl=en&q=hobbs,+jerry.+1979.++coherence+and+coreference.+cognitive+science,+3:67   90.
1840. https://www.mitpressjournals.org/servlet/linkout?suffix=r42&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/1654494.1654500
1841. http://scholar.google.com/scholar?hl=en&q=huang,+liang+and+david+chiang.+2005.++better+k-best+parsing.+in+proceedings+of+the+ninth+international+workshop+on+parsing+technology,+parsing+'05,+pages+53   64,+stroudsburg,+pa.
1842. https://www.mitpressjournals.org/servlet/linkout?suffix=r43&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/v1/p14-1002
1843. http://scholar.google.com/scholar?hl=en&q=ji,+yangfeng+and+jacob+eisenstein.+2014.++representation+learning+for+text-level+discourse+parsing.+in+proceedings+of+the+52nd+annual+meeting+of+the+association+for+computational+linguistics+(volume+1:+long+papers),+pages+13   24,+baltimore,+md.
1844. http://scholar.google.com/scholar?hl=en&q=joty,+shafiq,+giuseppe+carenini,+and+raymond+t.+ng.+2012.++a+novel+discriminative+framework+for+sentence-level+discourse+analysis.+in+proceedings+of+the+2012+joint+conference+on+empirical+methods+in+natural+language+processing+and+computational+natural+language+learning,+emnlp-conll+'12,+pages+904   915,+jeju+island.
1845. http://scholar.google.com/scholar?hl=en&q=joty,+shafiq,+giuseppe+carenini,+and+raymond+t.+ng.+2013.++topic+segmentation+and+labeling+in+asynchronous+conversations.+journal+of+artificial+intelligence+research+(jair),+47:521   573.
1846. http://scholar.google.com/scholar?hl=en&q=joty,+shafiq,+giuseppe+carenini,+raymond+t.+ng,+and+yashar+mehdad.+2013.++combining+intra-+and+multi-sentential+rhetorical+parsing+for+document-level+discourse+analysis.+in+proceedings+of+the+51st+annual+meeting+of+the+association+for+computational+linguistics,+acl+'13,+pages+486   496,+sofia.
1847. https://www.mitpressjournals.org/servlet/linkout?suffix=r47&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/v1/w14-3352
1848. http://scholar.google.com/scholar?hl=en&q=joty,+shafiq,+francisco+guzm  n,+llu  s+m  rquez,+and+preslav+nakov.+2014.++discotk:+using+discourse+structure+for+machine+translation+evaluation.+in+proceedings+of+the+ninth+workshop+on+statistical+machine+translation,+wmt+'14,+pages+402   408,+baltimore,+md.
1849. http://scholar.google.com/scholar?hl=en&q=jurafsky,+daniel+and+james+martin.+2008.++statistical+parsing.+in+speech+and+language+processing,+chapter+14.+prentice+hall.
1850. http://scholar.google.com/scholar?hl=en&q=knight,+kevin+and+jonathan+graehl.+2005.++an+overview+of+probabilistic+tree+transducers+for+natural+language+processing.+in+computational+linguistics+and+intelligent+text+processing,+volume+3406+of+lecture+notes+in+computer+science.+springer,+berlin+heidelberg,+pages+1   24.
1851. https://www.mitpressjournals.org/servlet/linkout?suffix=r50&dbid=16&doi=10.1162/coli_a_00226&key=10.1080/01638539409544883
1852. http://scholar.google.com/scholar?hl=en&q=knott,+alistair+and+robert+dale.+1994.++using+linguistic+phenomena+to+motivate+a+set+of+coherence+relations.+discourse+processes,+18:35   62.
1853. http://scholar.google.com/scholar?hl=en&q=koller,+alexander,+michaela+regneri,+and+stefan+thater.+2008.++regular+tree+grammars+as+a+formalism+for+scope+underspecification.+in+proceedings+of+the+46th+annual+meeting+of+the+association+for+computational+linguistics+on+human+language+technologies,+pages+218   226,+columbus,+oh.
1854. http://scholar.google.com/scholar?hl=en&q=lafferty,+john,+andrew+mccallum,+and+fernando+pereira.+2001.++conditional+random+fields:+probabilistic+models+for+segmenting+and+labeling+sequence+data.+in+proceedings+of+the+eighteenth+international+conference+on+machine+learning,+pages+282   289,+san+francisco,+ca.
1855. http://scholar.google.com/scholar?hl=en&q=lazaridou,+angeliki,+ivan+titov,+and+caroline+sporleder.+2013.++a+bayesian+model+for+joint+unsupervised+induction+of+sentiment,+aspect+and+discourse+representations.+in+proceedings+of+the+51st+annual+meeting+of+the+association+for+computational+linguistics,+acl+'13,+sofia.
1856. https://www.mitpressjournals.org/servlet/linkout?suffix=r54&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/v1/d14-1220
1857. http://scholar.google.com/scholar?hl=en&q=li,+jiwei,+rumeng+li,+and+eduard+hovy.+2014.++recursive+deep+models+for+discourse+parsing.+in+proceedings+of+the+2014+conference+on+empirical+methods+in+natural+language+processing+(emnlp),+pages+2061   2069,+doha.
1858. https://www.mitpressjournals.org/servlet/linkout?suffix=r55&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/v1/p14-1003
1859. http://scholar.google.com/scholar?hl=en&q=li,+sujian,+liang+wang,+ziqiang+cao,+and+wenjie+li.+2014.++text-level+discourse+dependency+parsing.+in+proceedings+of+the+52nd+annual+meeting+of+the+association+for+computational+linguistics+(volume+1:+long+papers),+pages+25   35,+baltimore,+md.
1860. http://scholar.google.com/scholar?hl=en&q=louis,+annie,+aravind+joshi,+and+ani+nenkova.+2010.++discourse+indicators+for+content+selection+in+summarization.+in+proceedings+of+the+11th+annual+meeting+of+the+special+interest+group+on+discourse+and+dialogue,+sigdial+'10,+pages+147   156,+tokyo.
1861. http://scholar.google.com/scholar?hl=en&q=mach    ek,+matou  +and+ond  ej+bojar.+2014.+results+of+the+wmt14+metrics+shared+task.+in+proceedings+of+the+ninth+workshop+on+statistical+machine+translation,+baltimore,+md.
1862. https://www.mitpressjournals.org/servlet/linkout?suffix=r58&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/981658.981695
1863. http://scholar.google.com/scholar?hl=en&q=magerman,+david.+1995.++statistical+decision-tree+models+for+parsing.+in+proceedings+of+the+33rd+annual+meeting+of+the+association+for+computational+linguistics,+acl'95,+pages+276   283,+cambridge,+ma.
1864. https://www.mitpressjournals.org/servlet/linkout?suffix=r59&dbid=16&doi=10.1162/coli_a_00226&key=10.1515/text.1.1988.8.3.243
1865. http://scholar.google.com/scholar?hl=en&q=mann,+william+and+sandra+thompson.+1988.++rhetorical+structure+theory:+toward+a+functional+theory+of+text+organization.+text,+8(3):243   281.
1866. https://www.mitpressjournals.org/servlet/linkout?suffix=r60&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/1034678.1034736
1867. http://scholar.google.com/scholar?hl=en&q=marcu,+daniel.+1999.++a+decision-based+approach+to+rhetorical+parsing.+in+proceedings+of+the+37th+annual+meeting+of+the+association+for+computational+linguistics+on+computational+linguistics,+acl'99,+pages+365   372,+morristown,+nj.
1868. https://www.mitpressjournals.org/doi/10.1162/089120100561755
1869. http://scholar.google.com/scholar?hl=en&q=marcu,+daniel.+2000a.++the+rhetorical+parsing+of+unrestricted+texts:+a+surface-based+approach.+computational+linguistics,+26:395   448.
1870. http://scholar.google.com/scholar?hl=en&q=marcu,+daniel.+2000b.+the+theory+and+practice+of+discourse+parsing+and+summarization.+mit+press,+cambridge,+ma.
1871. http://scholar.google.com/scholar?hl=en&q=marcu,+daniel+and+abdessamad+echihabi.+2002.++an+unsupervised+approach+to+recognizing+discourse+relations.+in+proceedings+of+the+40th+annual+meeting+of+the+association+for+computational+linguistics,+acl'02,+pages+368   375.+philadelphia,+pa.
1872. http://scholar.google.com/scholar?hl=en&q=marcus,+mitchell,+mary+marcinkiewicz,+and+beatrice+santorini.+1994.++building+a+large+annotated+corpus+of+english:+the+penn+treebank.+computational+linguistics,+19(2):313   330.
1873. http://scholar.google.com/scholar?hl=en&q=martin,+james,+1992.+english+text:+system+and+structure.+john+benjamins+publishing+company,+philadelphia/amsterdam.
1874. http://scholar.google.com/scholar?hl=en&q=maslennikov,+mstislav+and+tat-seng+chua.+2007.++a+multi-resolution+framework+for+information+extraction+from+free+text.+in+proceedings+of+the+45th+annual+meeting+of+the+association+for+computational+linguistics,+pages+592   599,+prague.
1875. http://mallet.cs.umass.edu/
1876. http://scholar.google.com/scholar?hl=en&q=mccallum,+andrew.+2002.++mallet:+a+machine+learning+for+language+toolkit.+http://mallet.cs.umass.edu.
1877. http://scholar.google.com/scholar?hl=en&q=mccallum,+andrew,+dayne+freitag,+and+fernando+c.+n.+pereira.+2000.++maximum+id178+markov+models+for+information+extraction+and+segmentation.+in+proceedings+of+the+seventeenth+international+conference+on+machine+learning,+icml+'00,+pages+591   598,+san+francisco,+ca.
1878. https://www.mitpressjournals.org/servlet/linkout?suffix=r69&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/1219840.1219852
1879. http://scholar.google.com/scholar?hl=en&q=mcdonald,+ryan,+koby+crammer,+and+fernando+pereira.+2005.++online+large-margin+training+of+dependency+parsers.+in+proceedings+of+the+43rd+annual+meeting+of+the+association+for+computational+linguistics,+acl+'05,+pages+91   98,+ann+arbor,+mi.
1880. https://www.mitpressjournals.org/servlet/linkout?suffix=r70&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/1220575.1220641
1881. http://scholar.google.com/scholar?hl=en&q=mcdonald,+ryan,+fernando+pereira,+kiril+ribarov,+and+jan+haji  .+2005.++non-projective+dependency+parsing+using+spanning+tree+algorithms.+in+proceedings+of+the+conference+on+human+language+technology+and+empirical+methods+in+natural+language+processing,+hlt+'05,+pages+523   530,+stroudsburg,+pa.
1882. http://scholar.google.com/scholar?hl=en&q=morris,+jane+and+graeme+hirst.+1991.++lexical+cohesion+computed+by+thesaural+relations+as+an+indicator+of+structure+of+text.+computational+linguistics,+17(1):21   48.
1883. http://scholar.google.com/scholar?hl=en&q=murphy,+kevin.+2012.+machine+learning:+a+probabilistic+perspective.+the+mit+press.+cambridge,+ma.
1884. https://www.mitpressjournals.org/servlet/linkout?suffix=r73&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/1218955.1218990
1885. http://scholar.google.com/scholar?hl=en&q=pang,+bo+and+lillian+lee.+2004.++a+sentimental+education:+sentiment+analysis+using+subjectivity+summarization+based+on+minimum+cuts.+in+proceedings+of+the+42nd+annual+meeting+of+the+association+for+computational+linguistics,+acl+'04,+pages+271   278.+barcelona.
1886. http://scholar.google.com/scholar?hl=en&q=pitler,+emily+and+ani+nenkova.+2009.++using+syntax+to+disambiguate+explicit+discourse+connectives+in+text.+in+proceedings+of+the+acl-ijcnlp+2009+conference+short+papers,+aclshort+'09,+pages+13   16,+suntec.
1887. http://scholar.google.com/scholar?hl=en&q=poole,+david+and+alan+mackworth,+2010.+artificial+intelligence:+foundations+of+computational+agents.+cambridge+university+press.
1888. http://scholar.google.com/scholar?hl=en&q=prasad,+rashmi,+nikhil+dinesh,+alan+lee,+eleni+miltsakaki,+livio+robaldo,+aravind+joshi,+and+bonnie+webber.+2008.++the+penn+discourse+treebank+2.0.+in+proceedings+of+the+sixth+international+conference+on+language+resources+and+evaluation+(lrec),+pages+2961   2968,+marrakech.
1889. http://scholar.google.com/scholar?hl=en&q=prasad,+rashmi,+aravind+joshi,+nikhil+dinesh,+alan+lee,+eleni+miltsakaki,+and+bonnie+webber.+2005.++the+penn+discourse+treebank+as+a+resource+for+natural+language+generation.+in+proceedings+of+the+corpus+linguistics+workshop+on+using+corpora+for+natural+language+generation,+pages+25   32,+birmingham.
1890. https://www.mitpressjournals.org/servlet/linkout?suffix=r78&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/1557690.1557761
1891. http://scholar.google.com/scholar?hl=en&q=regneri,+michaela,+markus+egg,+and+alexander+koller.+2008.++efficient+processing+of+underspecified+discourse+representations.+in+proceedings+of+the+46th+annual+meeting+of+the+association+for+computational+linguistics+on+human+language+technologies:+short+papers,+hlt-short+'08,+pages+245   248,+columbus,+oh.
1892. https://www.mitpressjournals.org/servlet/linkout?suffix=r79&dbid=16&doi=10.1162/coli_a_00226&key=10.1093/jos/10.2.123
1893. http://scholar.google.com/scholar?hl=en&q=reyle,+uwe.+1993.++dealing+with+ambiguities+by+underspecification:+construction,+representation+and+deduction.+journal+of+semantics,+10(2):123   179.
1894. https://www.mitpressjournals.org/servlet/linkout?suffix=r80&dbid=16&doi=10.1162/coli_a_00226&key=10.1023/a:1007649029923
1895. http://scholar.google.com/scholar?hl=en&q=schapire,+robert+e.+and+yoram+singer.+2000.++boostexter:+a+boosting-based+system+for+text+categorization.+machine+learning,+39(2   3):135   168.
1896. http://scholar.google.com/scholar?hl=en&q=schauer,+holger+and+udo+hahn.+2001.++anaphoric+cues+for+coherence+relations.+in+proceedings+of+the+conference+on+recent+advances+in+natural+language+processing,+ranlp+'01,+pages+228   234,+tzigov+chark.
1897. http://scholar.google.com/scholar?hl=en&q=schilder,+frank.+2002.++robust+discourse+parsing+via+discourse+markers,+topicality+and+position.+natural+language+engineering,+8(3):235   255.
1898. https://www.mitpressjournals.org/servlet/linkout?suffix=r83&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/1073445.1073473
1899. http://scholar.google.com/scholar?hl=en&q=sha,+fei+and+fernando+pereira.+2003.++shallow+parsing+with+conditional+random+fields.+in+proceedings+of+the+2003+conference+of+the+north+american+chapter+of+the+association+for+computational+linguistics+on+human+language+technology+-+volume+1,+naacl-hlt'03,+pages+134   141,+edmonton.
1900. https://www.mitpressjournals.org/doi/10.1162/089120102762671954
1901. http://scholar.google.com/scholar?hl=en&q=silber,+gregory+and+kathleen+mccoy.+2002.++efficiently+computed+lexical+chains+as+an+intermediate+representation+for+automatic+text+summarization.+computational+linguistics,+28(4):487   496.
1902. http://scholar.google.com/scholar?hl=en&q=smith,+noah+a.+2011.+linguistic+structure+prediction.+synthesis+lectures+on+human+language+technologies.+morgan+and+claypool.
1903. http://scholar.google.com/scholar?hl=en&q=socher,+richard,+john+bauer,+christopher+d.+manning,+and+ng+andrew+y.+2013a.++parsing+with+compositional+vector+grammars.+in+proceedings+of+the+51st+annual+meeting+of+the+association+for+computational+linguistics+(volume+1:+long+papers),+pages+455   465,+sofia.
1904. http://scholar.google.com/scholar?hl=en&q=socher,+richard,+alex+perelygin,+jean+wu,+jason+chuang,+christopher+d.+manning,+andrew+ng,+and+christopher+potts.+2013b.++recursive+deep+models+for+semantic+compositionality+over+a+sentiment+treebank.+in+proceedings+of+the+2013+conference+on+empirical+methods+in+natural+language+processing,+pages+1631   1642,+seattle,+wa.
1905. http://scholar.google.com/scholar?hl=en&q=somasundaran,+s.+2010.+discourse-level+relations+for+opinion+analysis.+ph.d.+thesis,+university+of+pittsburgh,+pa.
1906. https://www.mitpressjournals.org/servlet/linkout?suffix=r89&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/1073445.1073475
1907. http://scholar.google.com/scholar?hl=en&q=soricut,+radu+and+daniel+marcu.+2003.++sentence+level+discourse+parsing+using+syntactic+and+lexical+information.+in+proceedings+of+the+2003+conference+of+the+north+american+chapter+of+the+association+for+computational+linguistics+on+human+language+technology+-+volume+1,+naacl'03,+pages+149   156,+edmonton.
1908. http://scholar.google.com/scholar?hl=en&q=sporleder,+caroline.+2007.++manually+vs.+automatically+labelled+data+in+discourse+relation+classification.+effects+of+example+and+feature+selection.+ldv+forum,+22(1):1   20.
1909. http://scholar.google.com/scholar?hl=en&q=sporleder,+caroline+and+mirella+lapata.+2004.++automatic+paragraph+identification:+a+study+across+languages+and+domains.+in+proceedings+of+the+2004+conference+on+empirical+methods+in+natural+language+processing,+emnlp+'04,+pages+72   79,+barcelona.
1910. https://www.mitpressjournals.org/servlet/linkout?suffix=r92&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/1220575.1220608
1911. http://scholar.google.com/scholar?hl=en&q=sporleder,+caroline+and+mirella+lapata.+2005.++discourse+chunking+and+its+application+to+sentence+compression.+in+proceedings+of+the+conference+on+human+language+technology+and+empirical+methods+in+natural+language+processing,+hlt-emnlp'05,+pages+257   264,+vancouver.
1912. http://scholar.google.com/scholar?hl=en&q=sporleder,+caroline+and+alex+lascarides.+2005.++exploiting+linguistic+cues+to+classify+rhetorical+relations.+in+proceedings+of+recent+advances+in+natural+language+processing+(ranlp),+pages+157   166,+bulgaria.
1913. https://www.mitpressjournals.org/servlet/linkout?suffix=r94&dbid=16&doi=10.1162/coli_a_00226&key=10.1017/s1351324906004451
1914. http://scholar.google.com/scholar?hl=en&q=sporleder,+caroline+and+alex+lascarides.+2008.++using+automatically+labelled+examples+to+classify+rhetorical+relations:+an+assessment.+natural+language+engineering,+14(3):369   416.
1915. http://scholar.google.com/scholar?hl=en&q=stede,+manfred.+2004.++the+potsdam+commentary+corpus.+in+proceedings+of+the+acl-04+workshop+on+discourse+annotation,+pages+96   102,+barcelona.
1916. http://scholar.google.com/scholar?hl=en&q=stede,+manfred.+2011.+discourse+processing.+synthesis+lectures+on+human+language+technologies.+morgan+and+claypool+publishers.
1917. https://www.mitpressjournals.org/servlet/linkout?suffix=r97&dbid=16&doi=10.1162/coli_a_00226&key=10.3115/1620754.1620837
1918. http://scholar.google.com/scholar?hl=en&q=subba,+rajen+and+barbara+di-eugenio.+2009.++an+effective+discourse+parser+that+uses+rich+linguistic+information.+in+proceedings+of+human+language+technologies:+the+2009+annual+conference+of+the+north+american+chapter+of+the+association+for+computational+linguistics,+hlt-naacl'09,+pages+566   574,+boulder,+co.
1919. https://www.mitpressjournals.org/servlet/linkout?suffix=r98&dbid=16&doi=10.1162/coli_a_00226&key=10.1561/2200000013
1920. http://scholar.google.com/scholar?hl=en&q=sutton,+charles+and+andrew+mccallum.+2012.++an+introduction+to+conditional+random+fields.+foundations+and+trends+in+machine+learning,+4(4):267   373.
1921. http://scholar.google.com/scholar?hl=en&q=sutton,+charles,+andrew+mccallum,+and+khashayar+rohanimanesh.+2007.++dynamic+conditional+random+fields:+factorized+probabilistic+models+for+labeling+and+segmenting+sequence+data.+journal+of+machine+learning+research+(jmlr),+8:693   723.
1922. https://www.mitpressjournals.org/servlet/linkout?suffix=r100&dbid=16&doi=10.1162/coli_a_00226&key=10.1016/j.pragma.2005.09.010
1923. http://scholar.google.com/scholar?hl=en&q=taboada,+maite.+2006.++discourse+markers+as+signals+(or+not)+of+rhetorical+relations.+journal+of+pragmatics,+38(4):567   592.
1924. https://www.mitpressjournals.org/doi/10.1162/coli_a_00049
1925. http://scholar.google.com/scholar?hl=en&q=taboada,+maite,+julian+brooke,+milan+tofiloski,+kimberly+voll,+and+manfred+stede.+2011.++lexicon-based+methods+for+sentiment+analysis.+computational+linguistics,+37(2):267   307.
1926. https://www.mitpressjournals.org/servlet/linkout?suffix=r102&dbid=16&doi=10.1162/coli_a_00226&key=10.1177/1461445606061881
1927. http://scholar.google.com/scholar?hl=en&q=taboada,+maite+and+william+c.+mann.+2006.++rhetorical+structure+theory:+looking+back+and+moving+ahead.+discourse+studies,+8(3):423   459.
1928. https://www.mitpressjournals.org/doi/10.1162/089120102762671936
1929. http://scholar.google.com/scholar?hl=en&q=teufel,+simone+and+marc+moens.+2002.++summarizing+scientific+articles:+experiments+with+relevance+and+rhetorical+status.+computational+linguistics,+28(4):409   445.
1930. https://www.mitpressjournals.org/servlet/linkout?suffix=r104&dbid=16&doi=10.1162/coli_a_00226&key=10.1145/1277741.1277883
1931. http://scholar.google.com/scholar?hl=en&q=verberne,+suzan,+lou+boves,+nelleke+oostdijk,+and+peter-arno+coppen.+2007.++evaluating+discourse-based+answer+extraction+for+why-question+answering.+in+proceedings+of+the+30th+annual+international+acm+sigir+conference+on+research+and+development+in+information+retrieval,+sigir'07,+pages+735   736,+amsterdam.
1932. http://scholar.google.com/scholar?hl=en&q=vliet,+nynke+and+gisela+redeker.+2011.++complex+sentences+as+leaky+units+in+discourse+parsing.+in+proceedings+of+constraints+in+discourse,+pages+1   9,+agay   saint+raphael.
1933. https://www.mitpressjournals.org/servlet/linkout?suffix=r106&dbid=16&doi=10.1162/coli_a_00226&key=10.1207/s15516709cog2805_6
1934. http://scholar.google.com/scholar?hl=en&q=webber,+b.+2004.++d-ltag:+extending+lexicalized+tag+to+discourse.+cognitive+science,+28(5):751   779.
1935. http://scholar.google.com/scholar?hl=en&q=webber,+bonnie,+andrei+popescu-belis,+katja+markert,+and+j  rg+tiedemann,+editors.+2013.+proceedings+of+the+workshop+on+discourse+in+machine+translation.+acl,+sofia.
1936. http://scholar.google.com/scholar?hl=en&q=wick,+michael,+khashayar+rohanimanesh,+kedare+bellare,+aron+culotta,+and+andrew+mccallum.+2011.++samplerank:+training+factor+graphs+with+atomic+gradients.+in+proceedings+of+the+28th+international+conference+on+machine+learning,+icml'11,+pages+777   784.+bellevue,+wa.
1937. https://www.mitpressjournals.org/doi/10.1162/0891201054223977
1938. http://scholar.google.com/scholar?hl=en&q=wolf,+florian+and+edward+gibson.+2005.++representing+discourse+coherence:+a+corpus-based+study.+computational+linguistics,+31:249   288.
1939. http://scholar.google.com/scholar?hl=en&q=zhang,+yuan,+tao+lei,+regina+barzilay,+tommi+jaakkola,+and+amir+globerson.+2014.++steps+to+excellence:+simple+id136+with+refined+scoring+of+dependency+trees.+in+proceedings+of+the+52nd+annual+meeting+of+the+association+for+computational+linguistics,+pages+197   207,+baltimore,+md.
1940. https://www.mitpressjournals.org/cdn-cgi/l/email-protection#9fecf5f0ebe6dfeef9b1f0edf8b1eefe
1941. https://www.mitpressjournals.org/cdn-cgi/l/email-protection#3f5c5e4d5a515651567f5c4c114a5d5c115c5e
1942. https://www.mitpressjournals.org/cdn-cgi/l/email-protection#61130f062102124f1403024f0200
1943. https://www.mitpressjournals.org/personalize/addfavoritepublication?doi=10.1162/coli_a_00226
1944. https://www.mitpressjournals.org/action/showcitformats?doi=10.1162/coli_a_00226
1945. https://www.mitpressjournals.org/action/showfeed?jc=coli&type=etoc&feed=rss
1946. https://www.mitpressjournals.org/action/showfeed?jc=coli&doi=10.1162/coli_a_00226&type=citrack&feed=rss
1947. https://www.mitpressjournals.org/journals/coli/sub
1948. https://giving.mit.edu/taxonomy/term/79#3920880
1949. https://www.mitpressjournals.org/
1950. http://www.mitpressjournals.org/action/showpublications
1951. http://mitpress.mit.edu/
1952. https://www.mitpressjournals.org/terms
1953. https://www.mitpressjournals.org/privacy
1954. https://www.mitpressjournals.org/contact_info
1955. https://www.facebook.com/mitpress
1956. http://www.twitter.com/mitpress
1957. https://plus.google.com/106848724929282487337?prsrc=3
1958. https://www.pinterest.com/mitpress/
1959. https://www.instagram.com/mitpress/
1960. https://www.youtube.com/channel/uceh0hmlpjgw2dn0ntmd0fcq
1961. https://www.atypon.com/
1962. https://www.crossref.org/
1963. https://www.projectcounter.org/
1964. https://www.mitpressjournals.org/help/main

   hidden links:
1966. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1967. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1968. https://www.mitpressjournals.org/action/oauth/linkedin?start=1&redirecturi=%2fdoi%2f10.1162%2fcoli_a_00226
1969. https://www.mitpressjournals.org/action/oauth/facebook?start=1&redirecturi=%2fdoi%2f10.1162%2fcoli_a_00226
1970. https://www.mitpressjournals.org/action/oauth/twitter?start=1&redirecturi=%2fdoi%2f10.1162%2fcoli_a_00226
1971. https://www.mitpressjournals.org/action/registration
1972. https://www.mitpressjournals.org/action/ssostart?redirecturi=%2fdoi%2f10.1162%2fcoli_a_00226
1973. https://www.mitpressjournals.org/doi/10.1162/coli_a_00226
1974. https://www.mitpressjournals.org/action/showlogin?uri=%2fdoi%2f10.1162%2fcoli_a_00226
1975. https://www.mitpressjournals.org/action/showcart?flowid=1
1976. javascript:popref('s2')
1977. javascript:popref('s3')
1978. javascript:popref('s4')
1979. javascript:popref('s4a1')
1980. javascript:popref('s4b')
1981. javascript:popref('s4c')
1982. javascript:popref('s5')
1983. javascript:popref('s6')
1984. javascript:popref('s7')
1985. javascript:popref('s9')
1986. javascript:popref('s1')
1987. javascript:popref('s2')
1988. javascript:popref('s3')
1989. javascript:popref('s4')
1990. javascript:popref('s4a1')
1991. javascript:popref('e1')
1992. javascript:popref('s4a3')
1993. javascript:popref('s4a4')
1994. javascript:popref('s4b')
1995. javascript:popref('s4c')
1996. javascript:popref('s4c2')
1997. javascript:popref('s5')
1998. javascript:popref('s6')
1999. javascript:popref('s6a')
2000. javascript:popref('s6b2')
2001. javascript:popref('s6c1')
2002. javascript:popref('s6d2')
2003. javascript:popref('s6d3')
2004. javascript:popref('s6d4')
2005. javascript:popref('s6d5')
2006. javascript:popref('s6d6')
2007. javascript:popref('s7')
2008. javascript:popref('s9')
2009. javascript:popref('s1')
2010. javascript:popref('s2')
2011. javascript:popref('s3')
2012. javascript:popref('s4')
2013. javascript:popref('s4a1')
2014. javascript:popref('e1')
2015. javascript:popref('s4a3')
2016. javascript:popref('s4a4')
2017. javascript:popref('s4b')
2018. javascript:popref('s4c')
2019. javascript:popref('s4c2')
2020. javascript:popref('s5')
2021. javascript:popref('s6')
2022. javascript:popref('s6a')
2023. javascript:popref('s6b2')
2024. javascript:popref('s6c1')
2025. javascript:popref('s6d2')
2026. javascript:popref('s6d3')
2027. javascript:popref('s6d4')
2028. javascript:popref('s6d5')
2029. javascript:popref('s6d6')
2030. javascript:popref('s7')
2031. javascript:popref('s9')
2032. https://www.mitpressjournals.org/action/addcitationalert?doi=10.1162/coli_a_00226&referrer=10.1162/coli_a_00226
