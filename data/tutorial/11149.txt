from visual attributes to adjectives

through decompositional id65

angeliki lazaridou georgiana dinu    adam liska marco baroni

center for mind/brain sciences

university of trento

{angeliki.lazaridou|georgiana.dinu|adam.liska|marco.baroni}@unitn.it

5
1
0
2

 
r
a

 

m
4
2

 
 
]
l
c
.
s
c
[
 
 

2
v
4
1
7
2
0

.

1
0
5
1
:
v
i
x
r
a

abstract

1

introduction

as automated image analysis progresses, there
is increasing interest in richer linguistic an-
notation of pictures, with attributes of ob-
jects (e.g., furry, brown. . . ) attracting most
attention. by building on the recent    zero-
shot learning    approach, and paying atten-
tion to the linguistic nature of attributes as
noun modi   ers, and speci   cally adjectives,
we show that it is possible to tag images
with attribute-denoting adjectives even when
no training data containing the relevant an-
notation are available. our approach relies
on two key observations. first, objects can
be seen as bundles of attributes, typically ex-
pressed as adjectival modi   ers (a dog is some-
thing furry, brown, etc.), and thus a function
trained to map visual representations of ob-
jects to nominal labels can implicitly learn
to map attributes to adjectives. second, ob-
jects and attributes come together in pictures
(the same thing is a dog and it is brown).
we can thus achieve better attribute (and ob-
ject) label retrieval by treating images as    vi-
sual phrases   , and decomposing their linguis-
tic representation into an attribute-denoting
adjective and an object-denoting noun. our
approach performs comparably to a method
exploiting manual attribute annotation, it out-
performs various competitive alternatives in
both attribute and object annotation, and it au-
tomatically constructs attribute-centric repre-
sentations that signi   cantly improve perfor-
mance in supervised object recognition.

    current af   liation: thomas j. watson research center,

ibm, gdinu@us.ibm.com

as the quality of image analysis algorithms im-
proves, there is increasing interest in annotating im-
ages with linguistic descriptions ranging from sin-
gle words describing the depicted objects and their
properties (farhadi et al., 2009; lampert et al.,
2009) to richer expressions such as full-   edged im-
age captions (kulkarni et al., 2011; mitchell et al.,
2012). this trend has generated wide interest in lin-
guistic annotations beyond concrete nouns, with the
role of adjectives in image descriptions receiving, in
particular, much attention.

adjectives are of special interest because of their
central role in so-called attribute-centric image rep-
resentations. this framework views objects as bun-
dles of properties, or attributes, commonly ex-
pressed by adjectives (e.g., furry, brown), and uses
the latter as features to learn higher-level, seman-
tically richer representations of objects (farhadi et
al., 2009).1 attribute-based methods achieve better
generalization of object classi   ers with less train-
ing data (lampert et al., 2009), while at the same
time producing semantic representations of visual
concepts that more accurately model human se-

1in this paper, we assume that, just like nouns are the lin-
guistic counterpart of visual objects, visual attributes are ex-
pressed by adjectives. an informal survey of the relevant litera-
ture suggests that, when attributes have linguistic labels, they
are indeed mostly expressed by adjectives. there are some
attributes, such as parts, that are more naturally expressed by
prepositional phrases (pps: with a tail). interestingly, dinu and
baroni (2014) showed that the decomposition function we will
adopt here can derive both adjective-noun and noun-pp phrases,
suggesting that our approach could be seaid113ssly extended to
visual attributes expressed by noun-modifying pps.

mantic intuition (silberer et al., 2013). moreover,
automated attribute annotation can facilitate    ner-
grained id162 (e.g., searching for a rocky
beach rather than a sandy beach) and provide the
basis for more accurate image search (for example
in cases of visual sense disambiguation (divvala et
al., 2014), where a user disambiguates their query by
searching for images of wooden cabinet as furniture
and not just cabinet, which can also mean council).
classic attribute-centric image analysis requires,
however, extensive manual and often domain-
speci   c annotation of attributes (vedaldi et al.,
2014), or, at best, complex unsupervised image-
and-text-mining procedures to learn them (berg et
al., 2010). at the same time, resources with high-
quality per-image attribute annotations are limited;
to the best of our knowledge, coverage of all pub-
licly available datasets containing non-class speci   c
attributes does not exceed 100 attributes,2 orders
of magnitude smaller than the equivalent object-
annotated datasets (deng et al., 2009). moreover,
many visual attributes currently available (e.g., 2d-
boxy, furniture leg), albeit visually meaningful, do
not have straightforward linguistic equivalents, ren-
dering them inappropriate for applications requir-
ing natural linguistic expressions, such as the search
scenarios considered above.

a promising way to limit manual attribute anno-
tation effort is to extend recently proposed zero-shot
learning methods, until now applied to object recog-
nition, to the task of labeling images with attribute-
denoting adjectives. the zero-shot approach relies
on the possibility to extract, through distributional
methods, semantically effective vector-based word
representations from text corpora, on a large scale
and without supervision (turney and pantel, 2010).
in zero-shot learning, training images labeled with
object names are also represented as vectors (of fea-
tures extracted with standard image-analysis tech-
niques), which are paired with the vectors repre-
senting the corresponding object names in language-
based distributional semantic space. given such

2the attribute datasets we are aware of are the ones of
farhadi et al. (2010), ferrari and zisserman (2007) and rus-
sakovsky and fei-fei (2010), containing annotations for 64, 7
and 25 attributes, respectively. (this count excludes the sun
attributes database (patterson et al., 2014), whose attributes
characterize scenes rather than concrete objects.)

figure 1: id167 (van der maaten and hinton, 2008) visu-
alization of 3 objects together with the 2 nearest attributes
in our visual space (left), and of the corresponding nouns
and adjectives in linguistic space (right).

paired training data, various algorithms (socher et
al., 2013; frome et al., 2013; lazaridou et al., 2014)
can be used to induce a cross-modal projection of
images onto linguistic space. this projection is then
applied to map previously unseen objects to the cor-
responding linguistic labels. the method takes ad-
vantage of the similarities in the vector space topolo-
gies of the two modalities, allowing information
propagation from the limited number of objects seen
in training to virtually any object with a vector-based
linguistic representation.

to adapt zero-shot learning to attributes, we rely
on their nature as (salient) properties of objects, and
on how this is re   ected linguistically in modi   er re-
lations between adjectives and nouns. we build on
the observation that visual and linguistic attribute-
adjective vector spaces exhibit similar structures:
the correlation    between the pairwise similari-
ties in visual and linguistic space of all attributes-
adjectives from our experiments is 0.14 (signi   cant
at p < 0.05).3 while the correlation is smaller
than for object-noun data (0.23), we conjecture it
is suf   cient for zero-shot learning of attributes. we
will con   rm this by testing a cross-modal projection
function from attributes, such as colors and shapes,
onto adjectives in linguistic semantic space, trained
on pre-existing annotated datasets covering less than
100 attributes (experiment 1).

we proceed to develop an approach achieving
equally good attribute-labeling performance without
manual attribute annotation. inspired by linguistic
and cognitive theories that characterize objects as at-
tribute bundles (murphy, 2002), we hypothesize that
when we learn to project images of objects to the
corresponding noun labels, we implicitly learn to

3in this paper, we report signi   cance at    = 0.05 threshold.

   400   300   200   1000100200300   150   100   50050100150puppycutefurrycarmetallicplasticbirdfeatheredwild   150   100   50050100   250   200   150   100   50050100150200puppycutefurrycarmetallicplasticbirdfeatheredwildadjective-noun phrase. we show that the method
yields results comparable to those obtained when us-
ing attribute-labeled training data, while only requir-
ing object-annotated data. interestingly, this decom-
positional approach also doubles the performance
of object/noun annotation over the standard zero-
shot approach (experiment 2). given the positive
results of our proposed method, we conclude with
an extrinsic evaluation (experiment 3); we show
that attribute-centric representations of images cre-
ated with the decompositional approach boost per-
formance in an object classi   cation task, supporting
claims about its practical utility.

in addition to contributions to image annotation,
our work suggests new test beds for distributional
semantic representations of nouns and associated
adjectives, and provides more in-depth evidence of
the potential of the decompositional approach.
2 general experimental setup
2.1 cross-modal mapping
our approach relies on cross-modal mapping
from a visual semantic space v, populated with
vector-based representations of
images, onto a
linguistic (distributional semantic) space w of word
vectors. the mapping is performed by    rst inducing
: rd1     rd2 from data points
a function fproj
(vi, wi), where vi     rd1 is a vector representation
of an image tagged with an object or an attribute
(such as dog or metallic), and wi     rd2 is the
linguistic vector representation of the corresponding
word. the mapping function can subsequently be
applied to any given image vi     v to obtain its
projection w(cid:48)

i     w onto linguistic space:

w(cid:48)
i = fproj(vi)

speci   cally, we consider two mapping methods. in
the ridge regression approach, we learn a linear
function fproj     rd2  d1 by solving the tikhonov-
phillips id173 problem, which minimizes
the following objective:

||w t r     fprojv t r||2

2     ||  fproj||2
2,

where w t r and v t r are obtained by stacking the
word vectors wi and corresponding image vectors
vi, from the training set.4

4the parameter    is determined through cross-validation on

the training data.

figure 2: images tagged with orange and liqueur are
mapped in linguistic space closer to the vector of the
phrase orange liqueur than to the orange or liqueur vec-
tors (id167 visualization) (the    gure also shows the near-
est neighbours of phrase, adjective and noun in linguis-
tic space). the mapping is trained using solely noun-
annotated images.

associate the visual properties/attributes of the ob-
jects to the corresponding adjectives. as an exam-
ple, figure 1 (left) displays the nearest attributes of
car, bird and puppy in the visual space and, inter-
estingly, the relative distance between the noun de-
noting objects and the adjective denoting attributes
is also preserved in the linguistic space (right).

we further observe that, as also highlighted by
recent work in object recognition, any object in an
image is, in a sense, a visual phrase (sadeghi and
farhadi, 2011; divvala et al., 2014), i.e., the object
and its attributes are mutually dependent. for exam-
ple, we cannot visually isolate the object drum from
attributes such as wooden and round. indeed, within
our data, in 80% of the cases the projected image
of an object is closer to the semantic representation
of a phrase describing it than to either the object or
attribute labels. see figure 2 for an example.

motivated by this observation, we turn to recent
work in id65 de   ning a vector
decomposition framework (dinu and baroni, 2014)
which, given a vector encoding the meaning of a
phrase, aims at decoupling its constituents, produc-
ing vectors that can then be matched to a sequence
of words best capturing the semantics of the phrase.
we adopt this framework to decompose image rep-
resentations projected onto linguistic space into an

   800   600   400   2000200400600800   1000   800   600   400   2000200400600800only fruitsparkling winesmooth pastevibrant redmobile visitherbal liqueurrich chocolateclear honeyorange liqueurmixed fruitnew orangeorangeirish creamliqueurwhite rumsecond, motivated by the success of canonical
correlations analysis (cca) (hotelling, 1936) in
several vision-and-language tasks, such as image
and caption retrieval (gong et al., 2014; hardoon et
al., 2004; hodosh et al., 2013), we adapt normalized
canonical correlations analysis (ncca) to our
setup. given two paired observation matrices x
and y , in our case w t r and v t r, cca seeks two
projection matrices a and b that maximize the
correlation between at x and bt y . this can be
solved ef   ciently by applying svd to

  c1/2
xx

  cxy

  c1/2
y y = u   v t

where   c stands for the covariance matrix. finally,
the projection matrices are de   ned as a =   c1/2
xx u
and b =   c1/2
y y v . gong et al. (2014) propose a nor-
malized variant of cca, in which the projection ma-
trices are further scaled by some power    of the sin-
gular values    returned by the svd solution. in our
experiments, we tune the choice of    on the training
data. trivially, if    = 0, ncca reduces to cca.

note that other mapping functions could also be
used. we leave a more extensive exploration of pos-
sible alternatives to further research, since the details
of how the vision-to-text conversion is conducted are
not crucial for the current study. as increasingly
more effective mapping methods are developed, we
can easily plug them into our architecture.

through the selected cross-modal mapping func-
tion, any image can be projected onto linguistic
space, where the word (possibly of the appropriate
part of speech) corresponding to the nearest vector
is returned as a candidate label for the image (fol-
lowing standard practice in id65,
we measure proximity by the cosine measure).

2.2 decomposition
dinu and baroni (2014) have recently proposed a
general decomposition framework that, given a dis-
tributional vector encoding a phrase meaning and
the syntactic structure of that phrase, decomposes
it into a set of vectors expected to express the se-
mantics of the words that composed the phrase. in
our setup, we are interested in a decomposition func-
tion fdec : rd2     r2d2 which, given a visual vec-
tor projected onto the linguistic space, assumes it
represents the meaning of an adjective-noun phrase,
and decomposes it into two vectors corresponding to

the adjective and noun constituents [wadj; wnoun] =
fdec(wan ). we take fdec to be a linear function
and, following dinu and baroni (2014), we use as
training data vectors of adjective-noun bigrams di-
rectly extracted from the corpus together with the
concatenation of the corresponding adjective and
noun word vectors. we estimate fdec by solving a
ridge regression problem minimizing the following
objective:
||[w t r

2     ||  fdec||2

an||2

2

adj; w t r
adj, w t r

noun]     fdecw t r
noun, w t r

where w t r
an are the matrices obtained
by stacking the training data vectors. the    param-
eter is tuned through generalized cross-validation
(hastie et al., 2009).

2.3 representational spaces
linguistic space we construct distributional vec-
tors from text through the method recently proposed
by mikolov et al. (2013), to which we feed a cor-
pus of 2.8 billion words obtained by concatenating
english wikipedia, ukwac and bnc.5 speci   cally,
we used the cbow algorithm, which induces vec-
tors by predicting a target word given the words sur-
rounding it. we construct vectors of 300 dimensions
considering a context window of 5 words to either
side of the target, setting the sub-sampling option to
1e-05 and the negative sampling parameter to 5.6
visual spaces following standard practice, im-
ages are represented as bags of visual words
(bovw) (sivic and zisserman, 2003).7 local low-
level image features are clustered into a set of visual
words that act as higher-level descriptors.
in our
case, we use phow-color image features, a vari-
ant of dense sift (bosch et al., 2007), and a vi-
sual vocabulary of 600 words. spatial information
is preserved with a two-level spatial pyramid rep-
resentation (lazebnik et al., 2006), achieving a    -
nal dimensionality of 12,000. the entire pipeline
is implemented using the vlfeat library (vedaldi
and fulkerson, 2010), and its setup is identical to the

5http://wacky.sslmit.unibo.it,

http:

//www.natcorp.ox.ac.uk

6the parameters are tuned on the men word similarity

dataset (bruni et al., 2014).

7in future research, we might obtain a performance boost
simply by using the more advanced visual features recently in-
troduced by krizhevsky et al. (2012).

category attributes
color

pattern
shape
texture

black, blue, brown, gray, green,
orange, pink, red, violet, white, yellow
spotted, striped
long, round, rectangular, square
furry, smooth, rough, shiny, metallic,
vegetation, wooden, wet

table 1: list of attributes in the evaluation dataset.

image

attributes object
furry
white
smooth

cat

green
shiny

cocktail

table 2: sample annotations from the evaluation dataset.

toolkit   s basic recognition sample application.8 we
apply positive pointwise mutual information (evert,
2005) to the bovw counts, and reduce the resulting
vectors to 300 dimensions using svd.

2.4 evaluation dataset
for evaluation purposes, we use the dataset consist-
ing of images annotated with adjective-noun phrases
introduced in russakovsky and fei-fei (2010),
which pertains to 384 id138/id163 synsets
with 25 images per synset. the images were manu-
ally annotated with 25 attribute-denoting adjectives
related to texture, color, pattern and shape, respect-
ing the constraints that a color must cover a signi   -
cant part of the target object, and all other attributes
must pertain to the object as a whole (as opposed
to parts). table 1 lists the 25 attributes and table 2
illustrates sample annotations.9

in order to increase annotation quality, we only
consider attributes with full annotator consensus, for
a total of 8,449 annotated images, with 2.7 attributes
per-image on average. furthermore, to make the lin-
guistic annotation more natural and avoid sparsity
problems, we renamed excessively speci   c objects
with a noun denoting a more general category, fol-
lowing recent work on entry-level categories (or-

8http://www.vlfeat.org/applications/

apps.html

training
#im.
#attr.
10,749 97
23,000
-

#obj.

-
750

exp. 1
exp. 2

evaluation

#attr.

#im.
#obj.
leave-one-attribute-out
8,449
203

25

table 3: summary of training and evaluation sets.

donez et al., 2013); e.g., colobus guereza was re-
labeled as monkey. the    nal evaluation dataset con-
tains 203 distinct objects.

3 experiment 1: zero-shot attribute

learning

in section 1, we showed that there is a signi   -
cant correlation between pairwise similarities of ad-
jectives in a language-based distributional seman-
tic space and those of visual feature vectors ex-
tracted from images labeled with the corresponding
attributes. in the    rst experiment, we test whether
this correspondence in attribute-adjective similar-
ity structure across modalities suf   ces to success-
fully apply zero-shot labeling. we learn a cross-
modal function from an annotated dataset and use
it to label images from an evaluation dataset with
attributes outside the training set. we will refer to
this approach as dira, for direct retrieval using
attribute annotation. note that this is the    rst time
that zero-shot techniques are used in the attribute
domain.
in the present evaluation, we distinguish
dira-ridge and dira-ncca, according to the
cross-modal function used to project from images to
linguistic representations (see section 2.1 above).

3.1 cross-modal training and evaluation
to gather suf   cient data to train a cross-modal
mapping function for attributes/adjectives, we com-
bine the publicly available datasets of farhadi et al.
(2009) and ferrari and zisserman (2007) with at-
tributes and associated images extracted from mir-
flickr (huiskes and lew, 2008).10 the resulting
dataset contains 72 distinct attributes and 2,300 im-
ages. each image-attribute pair represents a training
data point (v, wadj), where v is the vector represen-
tation of the image, and wadj is the linguistic vector
of the attribute (corresponding to an adjective). no
information about the depicted object is needed.

9although vegetation is a noun, we have kept it in the eval-

10we    ltered out attributes not expressed by adjectives, such

uation set, treating it as an adjective.

as wheel or leg.

figure 3: performance of zero-shot attribute classi   cation (as measured by auc) compared to the supervised method
of russakovsky and fei-fei (2010), where available. the dark-red horizontal line marks chance performance.

to further maximize the amount of training data
points, we conduct a leave-one-attribute-out evalua-
tion, in which the cross-modal mapping function is
repeatedly learned on all 72 attributes from the train-
ing set, as well as all but one attribute from the eval-
uation set (section 2.4), and the associated images.
this results in 72 + (25    1) = 96 training attributes
in total. on average, 45 images per attribute are
used. the performance is measured for the single
attribute that was excluded from training. a numeri-
cal summary of the experiment setup is presented in
the    rst row of table 3.

3.2 results and discussion
russakovsky and fei-fei (2010) trained separate
id166 classi   ers for each attribute in the evaluation
dataset in a cross-validation setting. this fully su-
pervised approach can be seen as an ambitious up-
per bound for zero-shot learning, and we directly
compare our performance to theirs using their    gure
of merit, namely area under the roc curve (auc),
which is commonly used for binary classi   cation
problems.11 a perfect classi   er achieves an auc of
1, whereas an auc of 0.5 indicates random guess-
ing. for purposes of auc computation, dira is
considered to label test images with a given adjec-
tive if the linguistic-space distance between their
mapped representation and the adjective is below
a certain threshold. auc measures the aggregated
performance over all thresholds. to get a sense of

11table 4 reports hit@k results for dira, which will be dis-

cussed below in the context of experiment 2.

what auc compares to in terms of precision and re-
call, the auc of dira for furry is 0.74, while the
precision is 71% and the corresponding recall 14%.
for the more dif   cult blue case, auc is at 0.5, pre-
cision and recall are 2% and 55%, respectively.

the auc results are presented in figure 3 (ig-
nore red bars for now). we observe    rst that, of the
two mapping functions we considered, ridge (blue
bars) clearly outperforms ncca (yellow bars). ac-
cording to a series of paired permutation tests,
ridge has a signi   cantly larger auc in 13/25
cases, ncca in only 2. this is somewhat surpris-
ing given the better performance of ncca in the
experiments of gong et al. (2014). however, our
setup is quite different from theirs: they perform
all retrieval tasks by projecting the input visual and
language data onto a common multimodal space dif-
ferent from both input spaces. ncca is a well-
suited algorithm for this. we aim instead at produc-
ing linguistic annotations of images, which is most
straightforwardly accomplished by projecting visual
representations onto linguistic space. regression-
based learning (in our case, via ridge) is a more
natural choice for this purpose.

coming now to a more general analysis of the re-
sults, as expected, and analogously to the supervised
setting, dira-ridge performance varies across at-
tributes. some achieve performance close to the
supervised model (e.g., rectangular or wooden)
and, for 18 out of 25,
the performance is well
above chance (bootstrap test). the exceptions are:
blue, square, round, vegetation, smooth, spotted and
striped.
interestingly, for the last 4 attributes in

0.40.50.60.70.80.91dir   -ridgedecdir   -nccarussakovsky and fei-fei (2010)attributesroc areathis list, russakovsky and fei-fei (2010) achieved
their lowest performance, attributing it to the lower-
quality of the corresponding image annotations.
furthermore, russakovsky and fei-fei (2010) ex-
cluded 5 attributes due to insuf   cient training data.
of these, our performance for blue, vegetation and
square is not particularly encouraging, but for violet
and pink we achieve more than 0.7 auc, at the level
of the supervised classi   ers, suggesting that the pro-
posed method can complement the latter when an-
notated data are not available.

for a different perspective on the performance
of dira, we took several objects and queried the
model for their most common attribute, based on the
average attribute rank across all images of the object
in the dataset. reassuringly, we learn that sun   ow-
ers are on average yellow (mean rank 2.3),    elds are
green (4.4), cabinets are wooden (4) and vans metal-
lic (6.6) (strawberries are, suspiciously, blue, 2.7).
overall, this experiment shows that, just like ob-
ject classi   cation, attribute classi   ers bene   t from
knowledge transfer between the visual and linguis-
tic modalities, and zero-shot learning can achieve
reasonable performance on attributes and the corre-
sponding adjectives. this conclusion is based on the
assumption that per-image annotations of attributes
are available; in the following section, we show how
equal and even better performance can be attained
using data sets annotated with objects only, there-
fore without any hand-coded attribute information.

4 experiment 2: learning attributes from

objects and visual phrases

having shown that reasonably accurate annotations
of unseen attributes can be obtained with zero-shot
learning when a small amount of manual annota-
tion is available, we now proceed to test the intu-
ition, preliminarily supported by the data in figure
1, that, since objects are bundles of attributes, at-
tributes are implicitly learned together with objects.
we thus try to induce attribute-denoting adjective la-
bels by exploiting only widely-available object-noun
data. at the same time, building on the observa-
tion illustrated in figure 2 that pictures of objects
are pictures of visual phrases, we experiment with
a vector decomposition model which treats images
as composite and derives adjective and noun anno-

tations jointly. we compare it with standard zero-
shot learning using direct label retrieval as well as
against a number of challenging alternatives that ex-
ploit gold-standard information about the depicted
objects. the second row of table 3 gives a numeri-
cal summary of the setup for this experiment.

4.1 cross-modal training
we now assume object annotations only, in the form
of training data (v, wnoun), where v is the vector
representation of an image tagged with an object and
wnoun is the linguistic vector of the corresponding
noun. to ensure high imageability and diversity, we
use as training object labels those appearing in the
cifar-100 dataset (krizhevsky, 2009), combined
with those previously used in the work of farhadi
et al. (2009), as well as the most frequent nouns in
our corpus that also exist in id163, for a total
of 750 objects-nouns. for each object label, we in-
clude at most 50 images from the corresponding im-
agenet synset, resulting in     23, 000 training data
points. images containing objects from the evalua-
tion dataset are excluded, so that both adjective and
noun retrieval adhere to the zero-shot paradigm.

4.2 object-agnostic models
diro the direct retrieval using object annota-
tion approach projects an image onto the linguistic
space and retrieves the nearest adjectives as candi-
date attribute labels. the only difference with dira
(more precisely, dira-ridge), the zero-shot ap-
proach we tested above, is that the mapping function
has been trained on object-noun data only.

dec the decomposition method uses the fdec
function inspired by dinu and baroni (2014) (see
section 2.2), to associate the image vector projected
onto linguistic space to an adjective and a noun. we
train fdec with about     50, 000 training instances,
selected based on corpus frequency. these data
are further balanced by not allowing more than 100
training samples for any adjective or noun in order
to prevent very frequent words such as other or new
from dominating the training data. no image data
are used, and there is no need for manual annota-
tion, as the adjective-noun tuples are automatically
extracted from the corpus.

at

test

time, given an image to be labeled,

we project its visual representation onto the lin-
guistic space and decompose the resulting vector
w(cid:48) into two candidate adjective and noun vectors:
[w(cid:48)
noun] = fdec(w(cid:48)). we then search the lin-
guistic space for adjectives and nouns whose vectors
are nearest to w(cid:48)

noun, respectively.

adj and w(cid:48)

adj; w(cid:48)

4.3 object-informed models
a cross-modal
function trained exclusively on
object-noun data might be able to capture only pro-
totypical characteristics of an object, as induced
from text, independently of whether they are de-
picted in an image. although the gold annotation
of our dataset should already penalize this image-
independent labeling strategy (see section 2.4), we
control for this behaviour by comparing against
three models that have access to the gold noun an-
notations of the image and favor adjectives that are
typical modi   ers of the nouns.

lm we build a bigram language model by using
the berkeley lm toolkit (pauls and klein, 2012)12
on the one-trillion-token google web1t corpus13
and smooth probabilities with the    stupid    back-
off technique (brants et al., 2007). given an
image with object-noun annotation, we score all
attributes-adjectives based on the language-model-
derived id155 p(adjective|noun).
all images of the same object produce identical
rankings. as an example, among the top attributes
of cocktail we    nd heady, creamy and fruity.

vlm lm does not exploit visual
information
about the image to be annotated. a natural way to
enhance it is to combine it with diro, our cross-
modal mapping adjective retrieval method.
in the
visually-enriched language model, we interpolate
(using equal weights) the ranks produced by the
two models. in the resulting combination, attributes
that are both linguistically sensible and likely to be
present in the given image should be ranked high-
est. we expect this approach to be challenging to
beat. mackenzie (2014) recently introduced a simi-
lar model in a supervised setting, where it improved
over standard attribute classi   ers.

12https://code.google.com/p/berkeleylm/
13https://catalog.ldc.upenn.edu/

ldc2006t13

lm sp
0
7
9
17
32
55

2
5
8
18
33
56

vlm diro
1
4
9
19
43
67

5
16
29
50
72
82

dec
10
31
44
59
81
89

dira
7
23
37
51
68
77

@1
@5
@10
@20
@50
@100

table 4: percentage hit@k attribute retrieval scores.

sp the selectional preference model robustly
captures semantic restrictions imposed by a noun on
the adjectives modifying it (erk et al., 2010). con-
cretely, for each noun denoting a target object, we
identify a set of adjectives adjnoun that co-occur
with it in a modi   er relation more that 20 times.
by averaging the linguistic vectors of these adjec-
tives, we obtain a vector wprototypical
, which should
capture the semantics of the prototypical adjectives
for that noun. adjectives that have higher similar-
ity with this prototype vector are expected to denote
typical attributes of the corresponding noun and will
be ranked as more probable attributes. similarly to
lm, all images of the same object produce identical
rankings. as an example, among the top attributes
of cocktail we    nd fantastic, delicious and perfect.

noun

4.4 results
we evaluate the performance of the models on
attribute-denoting adjective retrieval, using a search
space containing the top 5,000 most frequent ad-
jectives in our corpus. tables 4 and 5 present
respectively (k    
hit@k and recall@k results,
{1, 5, 10, 20, 50, 100}). hit@k measures the per-
centage of images for which at least one gold at-
tribute exists among the top k retrieved attributes.
recall@k measures the proportion of gold attributes
retrieved among the top k, relative to the total num-
ber of gold attributes for each image.14

first of all, we observe that lm and sp     the two
models that have access to gold object-noun annota-
tion and are entirely language-based     although well
above the random baseline (k/5,000), achieve rather
low performance. this con   rms that to model our
test set accurately, it is not suf   cient to predict typi-
cal attributes of the depicted objects.

14due to the leave-one-attribute-out approach used to train
and test dira (see section 3), it is not possible to compute
recall results for this model.

lm sp
0
3
5
10
20
34

1
2
3
9
20
35

vlm diro
0
2
4
9
22
44

2
7
15
30
49
61

dec
4
15
23
35
59
70

@1
@5
@10
@20
@50
@100

image

table 5: percentage recall@k attribute retrieval scores.

a: white, brown
n: dog

model top item
a: white
n: dog

dec
diro a: animal

n: goat
a: stray

lm
vlm a: pet

top hit (rank)
white (1)
dog (1)
white (27)
dog (25)
brown (74)
brown (17)

diro
1
3
5
9
20
33

dec
2
10
14
20
29
41

dira
0
0
1
2
6
12

@1
@5
@10
@20
@50
@100

table 6: percentage hit@k noun retrieval scores.

the diro method, which exploits visual

in-
formation, performs numerically similarly to the
object-informed models lm and sp, with better
hit and recall at high ranks. although worse than
dira, the relatively high performance of diro is
a promising result, suggesting object annotations to-
gether with linguistic knowledge extracted in an un-
supervised manner from large corpora can replace,
to some extent, manual attribute annotations. how-
ever, diro does not directly model any semantic
compatibility constraints between the retrieved ad-
jectives and the object present in the image (see ex-
amples below). hence, the object-informed model
vlm, which combines visual information wit lin-
guistic co-occurrence statistics, doubles the perfor-
mance of diro, lm and sp.

our dec model, which treats images as visual
phrases and jointly decouples their semantics, out-
performs even vlm by a large margin. it also out-
performs dira, the standard zero-shot learning ap-
proach using attribute-adjective annotated data (see
also the attribute-by-attribute auc comparison be-
tween dec, dira and the fully-supervised ap-
proach of russakovsky and fei-fei in figure 3).

interestingly, accounting for the phrasal nature of
visual information leads to substantial performance
improvement in object recognition through zero-
shot learning (i.e., tagging images with the depicted
nouns) as well. table 6 provides the hit@k results
obtained with the diro and dec methods for the
noun retrieval task in a search space of 10,000 most

a: shiny
n:    an

dec
diro a: crunchy
n: ramekin
lm
a: chocolate
vlm a: chocolate

shiny (1)
syrup (170)
shiny (15)
syrup (113)
shiny (84)
shiny (17)

a: shiny, round
n: syrup

table 7: images with gold attribute-adjective and object-
noun labels, and highest-ranked items for each model
(top item), as well as highest-ranked correct item and
rank (top hit). noun results for (v)lm are omitted since
these models have access to the gold noun label.

frequent nouns from our corpus. note that diro
represents the label retrieval technique that has been
standardly used in conjunction with zero-shot learn-
ing for objects: the cross-modal function is trained
on images annotated with nouns that denote the ob-
jects they depict, and it is then used for noun label
retrieval of unseen objects through a nearest neigh-
bor search of the mapped image representation (the
dira column shows that zero-shot noun retrieval
using the mapping function trained on adjectives
works very poorly). dec decomposes instead the
mapped image representation into two vectors de-
noting adjective and noun semantics, respectively,
and uses the latter to perform the nearest neigh-
bor search for a noun label. although not directly
comparable, the results of dec reported here are in
the same range of state-of-the-art zero-shot learning
models for object recognition (frome et al., 2013).

annotation examples table 7 presents some in-
teresting patterns we observed in the results. the
   rst example illustrates the case in which conducting
adjective and noun retrieval independently results in
mixing information, which damages the diro ap-
proach: adjectival and nominal properties are not
decoupled properly, since the animal property of the
depicted dog is re   ected in both the animal adjec-
tive and the goat noun. at the same time, the white-

ness of the object (an adjectival property) in   uences
noun selection, since goats tend to be white. instead,
dec unpacks the visual semantics in an accurate
and meaningful way, producing correct attribute and
noun annotations that form acceptable phrases. lm
and vlm are negatively affected by co-occurrence
statistics and guess stray and pet as adjectives, both
typical but generic and abstract dog properties.

in the next example, diro predicts a reason-
able noun label (ramekin), focusing on the container
rather than the liquid it contains. by ignoring the
relation between the adjective and the noun, the re-
sulting adjective annotation (crunchy) is semanti-
cally incompatible with the noun label, emphasizing
the inability of this method to account for semantic
relations between attributes-adjectives and object-
nouns. dec, on the other hand, mistakenly anno-
tates the object as    an instead of syrup. however,
having captured the right general category of the ob-
ject (   smooth gelatinous items that re   ect light   ),
it ranks a semantically appropriate and correct at-
tribute (shiny) at the top. finally, lm and vlm
choose chocolate, an attribute semantically appro-
priate for syrup but irrelevant for the target image.

semantic plausibility of phrases the examples
above suggest that one fundamental way in which
dec improves over diro is by producing seman-
tically coherent adjective-noun combinations. more
systematic evidence for this conjecture is provided
by a follow-up experiment on the linguistic qual-
ity of the generated phrases. we randomly sampled
2 images for each of the 203 objects in our data
set. for each image, we let the two models gen-
erate 9 descriptive phrases by combining their re-
spective top 3 adjective and noun predictions. from
the resulting lists of 3,654 phrases, we picked the
200 most common ones for each model, with only
1/8 of these common phrases being shared by both.
the selected phrases were presented (in random or-
der and concealing their origin) to two linguistically-
sophisticated annotators, who were asked to rate
their degree of semantic plausibility on a 1-3 scale
(the annotators were not shown the corresponding
images and had to evaluate phrases purely on lin-
guistic/semantic grounds). since the two judges
were largely in agreement (   = 0.63), we averaged
their ratings. the mean averaged plausibility score

figure 4: distributions of (per-image) concreteness
scores across different models. red line marks median
values, box edges correspond to 1st and 3rd quartiles, the
wiskers extend to the most extreme data points and out-
liers are plotted individually.

for diro phrases was 1.74 (s.d.: 0.76), for dec it
was 2.48 (s.d.: 0.64), with the difference signi   cant
according to a mann-whitney test. the two anno-
tators agreed in assigning the lowest score (   com-
pletely implausible   ) to more than 1/3 of the diro
phrases (74/200; e.g., tinned tostada, animal bird,
hollow hyrax), but they unanimously assigned the
lowest score to only 7/200 dec phrases (e.g., cylin-
drical bed-sheet, sweet ramekin, wooden meat). we
thus have solid quantitative support that the superior-
ity of dec is partially due to how it learns to jointly
account for adjective and noun semantics, producing
phrases that are linguistically more meaningful.

adjective concreteness we can gain further in-
sight into the nature of the adjectives chosen by
the models by considering the fact that phrases that
are meant to describe an object in a picture should
mostly contain concrete adjectives, and thus the de-
gree of concreteness of the adjectives produced by a
model is an indirect measure of its quality. follow-
ing hill and korhonen (2014), we de   ne the con-
creteness of an adjective as the average concreteness
score of the nouns it modi   es in our text corpus.
noun concreteness scores are taken, in turn, from
turney et al. (2011). for each test image and model,
we obtain a concreteness score by averaging the con-
creteness of the top 5 adjectives that the model se-
lected for the image. figure 4 reports the distribu-
tions of the resulting scores across models. we con-

dec lmvlmdirodira0.250.30.350.40.450.50.55sp   rm that the purely language-based models (lm,
sp) are producing generic abstract adjectives that
are not appropriate to describe images (e.g., crypto-
graphic key, homemade bread, greek salad, beaten
yolk). the image-informed vlm and diro models
produce considerably more concrete adjectives. not
surprisingly, dira, that was directly trained on con-
crete adjectives, produces the most concrete ones.
importantly, dec, despite being based on a cross-
modal function that was not explicitly exposed to
adjectives, produced adjectives that are approaching
the concreteness level of those of dira (both differ-
ences between dec and diro, dec and dira are
signi   cant as by paired mann-whitney tests).

5 using dec for attribute-based object

classi   cation

as discussed in the introduction, attributes can ef-
fectively be used for attribute-based object clas-
si   cation.
in this section, we show that clas-
si   ers trained on attribute representations created
with dec     which does not require any attribute-
annotated training data nor training a battery of at-
tribute classi   ers     outperform (and are complemen-
tary to) standard bovw features.

we use a subset of the pascal voc 2008 dataset.15
speci   cally, following farhadi et al. (2009), we use
the original voc training set for training/validation,
and the voc validation set for testing. one-vs-all
linear-id166 classi   ers are trained for all voc ob-
jects, using 3 alternative image representations.

first, we train directly on bovw features
(phow, see section 2.3), as in the classic object
recognition pipeline. we compare phow to an
attribute-centric approach with attribute labels auto-
matically generated by dec. all voc images are
projected onto the linguistic space using the cross-
modal mapping function trained with object-noun
data only (see section 4.1), from which we further
removed all images depicting a voc object. each
image projection is then decomposed through dec
into two vectors representing adjective and noun in-
formation. the    nal attribute-centric vector repre-
senting an image is created by recording the cosine
similarities of the dec-generated adjective vector

15http://pascallin.ecs.soton.ac.uk/

challenges/voc/voc2008/

image

object

aeroplane

dog

predicted
attributes
thick, wet, dry,
cylindrical,
motionless,
translucent

cuddly, wild,
cute, furry,
white, coloured

table 8: two voc images with some top attributes as-
signed by dec: these attributes, together with their co-
sine similarities to the mapped image vectors, serve as
attribute-centric representations.

with all the adjectives in our linguistic space. infor-
mally, this representation can be thought of as a vec-
tor of weights describing the appropriateness of each
adjective as an annotation for the image.16 this is
comparable to standard attribute-based classi   cation
(farhadi et al., 2009), in which images are repre-
sented as distributions over attributes estimated with
a set of ad hoc supervised attribute-speci   c classi-
   ers. table 8 show examples of top attributes auto-
matically assigned by dec. while not nearly as ac-
curate as manual annotation, many attributes are rel-
evant to the objects, both as speci   cally depicted in
the image (the aeroplane is wet), but also more pro-
totypically (aeroplanes are cylindrical in general).

we also perform feature-level fusion (fused) by
concatenating the phow and dec features, and re-
ducing the resulting vector to 100 dimensions with
svd (bruni et al., 2014) (svd dimensionality de-
termined by cross-validation on the training set).

5.1 results
there is an improvement over phow visual features
when using dec-based attribute vectors, with accu-
racy raising from 30.49% to 32.76%. the confusion
matrices in figure 5 show that phow and dec do
not only differ in quantitative performance, but make
different kinds of errors, in part pointing at the dif-
ferent modalities the two models tap into. phow,
for example, tends to confuse cats with sofas, prob-
ably because the former are often pictured lying on

16given that the resulting representations are very dense, we
sparsify them by setting to zeros all adjective dimensions with
cosine below the global mean cosine value.

6 conclusion

we extended zero-shot image labeling beyond ob-
jects, showing that it is possible to tag images with
attribute-denoting adjectives that were not seen dur-
ing training. for some attributes, performance was
comparable to that of per-attribute supervised classi-
   ers. we further showed that attributes are implicitly
induced when learning to map visual vectors of ob-
jects to their linguistic realizations as nouns, and that
improvements in both attribute and noun retrieval
are attained by treating images as visual phrases,
whose linguistic representations must be decom-
posed into a coherent word sequence. the resulting
model outperformed a set of strong rivals. while the
performance of the zero-shot decompositional ap-
proach in the adjective-noun phrase labeling alone
might still be low for practical applications, this
model can still produce attribute-based representa-
tions that signi   cantly improve performance in a
supervised object recognition task, when combined
with standard visual features.

by mapping attributes and objects to phrases in
a linguistic space, we are also likely to produce
more natural descriptions than those currently used
in id161 (   uffy kittens rather than 2-boxy
tables).
in future work, we want to delve more
into the linguistic and pragmatic naturalness of at-
tributes: can we predict not just which attributes
of a depicted object are true, but which are more
salient and thus more likely to be mentioned (red
car over metal car)? can we pick the most appro-
priate adjective to denote an attribute given the ob-
ject in the picture (moist, rather than damp lips)?
we should also address attribute dependencies: by
ignoring them, we currently get undesired results,
such as the aeroplane in table 8 being tagged as both
wet and dry. more ambitiously, inspired by karpa-
thy et al. (2014), we plan to associate image frag-
ments with phrases of arbitrary syntactic structures
(e.g., pps for backgrounds, a vps for main events),
paving the way to full-   edged id134.

acknowledgments

we thank the tacl reviewers for their feedback.
we were supported by erc 2011 starting indepen-
dent research grant n. 283554 (composes).

figure 5: confusion matrices for phow (top) and dec
(bottom). warmer-color cells correspond to higher pro-
portions of images with gold row label tagged by an algo-
rithm with the column label (e.g., the    rst cells show that
dec tags a larger proportion of aeroplanes correctly).

the latter. dec, on the other hand, tends to con-
fuse chairs with tv monitors, partially misguided
by the taxonomic information encoded in language
(both are pieces of furniture). indeed, the combined
fused approach outperforms both representations
by a large margin (35.81%), con   rming that the
linguistically-enriched information brought by dec
is to a certain extent complementary to the lower-
level visual evidence directly exploited by phow.
overall, the performance of our system is quite close
to the one obtained by farhadi et al. (2009) with en-
sembles of supervised attribute classi   ers trained on
manually annotated data (the most comparable ac-
curacy from their table 1 is at 34.3%).17

17farhadi and colleagues reduce the bias for the people cat-
egory by reporting mean per-class accuracy; we directly ex-
cluded people from our version of the data set.

references
tamara berg, alexander berg, and jonathan shih. 2010.
automatic attribute discovery and characterization
from noisy web data. in proceedings of eccv, pages
663   676, crete, greece.

anna bosch, andrew zisserman, and xavier munoz.
2007. image classi   cation using id79s and
in proceedings of iccv, pages 1   8, rio de
ferns.
janeiro, brazil.

thorsten brants, ashok popat, peng xu, franz och, and
jeffrey dean. 2007. large language models in ma-
in proceedings of emnlp, pages
chine translation.
858   867, prague, czech republic.

elia bruni, nam khanh tran, and marco baroni. 2014.
multimodal id65. journal of arti-
   cial intelligence research, 49:1   47.

jia deng, wei dong, richard socher, lia-ji li, and
li fei-fei. 2009. id163: a large-scale hierarchi-
in proceedings of cvpr, pages
cal image database.
248   255, miami beach, fl.

georgiana dinu and marco baroni. 2014. how to make
words with vectors: phrase generation in distributional
in proceedings of acl, pages 624   633,
semantics.
baltimore, md.

santosh divvala, ali farhadi, and carlos guestrin.
2014. learning everything about anything: webly-
supervised visual concept learning. in proceedings of
cvpr, columbus, oh.

katrin erk, sebastian pad  o, and ulrike pad  o. 2010. a
   exible, corpus-driven model of regular and inverse
selectional preferences. computational linguistics,
36(4):723   763.

stefan evert. 2005. the statistics of word cooccur-

rences. ph.d dissertation, stuttgart university.

ali farhadi,

ian endres, derek hoiem, and david
forsyth. 2009. describing objects by their attributes.
in proceedings of cvpr, pages 1778   1785, miami
beach, fl.

ali farhadi, mohsen hejrati, mohammad a. sadeghi,
peter young, cyrus rashtchian, julia hockenmaier,
and david forsyth. 2010. every picture tells a story:
generating sentences from images. in proceedings of
eccv, crete, greece.

vittorio ferrari and andrew zisserman. 2007. learning
visual attributes. in proceedings of nips, pages 433   
440, vancouver, canada.

andrea frome, greg corrado, jon shlens, samy ben-
gio, jeff dean, marc   aurelio ranzato, and tomas
mikolov. 2013. devise: a deep visual-semantic em-
bedding model. in proceedings of nips, pages 2121   
2129, lake tahoe, nv.

yunchao gong, liwei wang, micah hodosh, julia hock-
improving

enmaier, and svetlana lazebnik. 2014.

image-sentence embeddings using large weakly an-
in proceedings of eccv,
notated photo collections.
pages 529   545, zurich, switzerland.

david r hardoon, sandor szedmak, and john shawe-
taylor. 2004. canonical correlation analysis: an
overview with application to learning methods. neu-
ral computation, 16(12):2639   2664.

trevor hastie, robert tibshirani, and jerome friedman.
2009. the elements of statistical learning, 2nd edi-
tion. springer, new york.

felix hill and anna korhonen. 2014. concreteness and
subjectivity as dimensions of lexical meaning. in pro-
ceedings of acl, pages 725   731, baltimore, mary-
land.

micah hodosh, peter young, and julia hockenmaier.
2013. framing image description as a ranking task:
data, models and id74. journal of arti-
   cial intelligence research, 47:853   899.

harold hotelling. 1936. relations between two sets of

variates. biometrika, 28(3/4):321   377.

mark huiskes and michael lew. 2008. the mir flickr
retrieval evaluation. in proceedings of mir, pages 39   
43, new york, ny.

andrej karpathy, armand joulin, and li fei-fei. 2014.
deep fragment embeddings for bidirectional image
in proceedings of nips, pages
sentence mapping.
1097   1105, montreal, canada.

alex krizhevsky, ilya sutskever, and geoffrey hinton.
2012. id163 classi   cation with deep convolutional
neural networks. in proceedings of nips, pages 1097   
1105, lake tahoe, nevada.

alex krizhevsky. 2009. learning multiple layers of fea-

tures from tiny images. master   s thesis.

girish kulkarni, visruth premraj, sagnik dhar, siming
li, yejin choi, alexander berg, and tamara berg.
2011. baby talk: understanding and generating sim-
in proceedings of cvpr,
ple image descriptions.
pages 1601   1608, colorado springs, co.

christoph h lampert, hannes nickisch, and stefan
harmeling. 2009. learning to detect unseen object
in pro-
classes by between-class attribute transfer.
ceedings of cvpr, pages 951   958, miami beach, fl.
angeliki lazaridou, elia bruni, and marco baroni. 2014.
is this a wampimuk? cross-modal mapping between
id65 and the visual world. in pro-
ceedings of acl, pages 1403   1414, baltimore, md.

svetlana lazebnik, cordelia schmid, and jean ponce.
2006. beyond bags of features: spatial pyramid
matching for recognizing natural scene categories. in
proceedings of cvpr, pages 2169   2178, washington,
dc.

laurens van der maaten and geoffrey hinton. 2008.
journal of machine

visualizing data using id167.
learning research, 9(2579-2605).

andrea vedaldi and brian fulkerson. 2010. vlfeat    
an open and portable library of id161 al-
gorithms. in proceedings of acm multimedia, pages
1469   1472, firenze, italy.

andrea vedaldi, siddarth mahendran, stavros tsogkas,
subhransu maji, ross girshick, juho kannala, esa
rahtu, iasonas kokkinos, matthew blaschko, david
weiss, ben taskar, karen simonyan, naomi saphra,
and sammy mohamed. 2014. understanding objects
in detail with    ne-grained attributes. in proceedings
of cvpr, columbus, oh.

calvin mackenzie. 2014. integrating visual and linguis-
tic information to describe properties of objects. un-
dergraduate honors thesis, computer science depart-
ment, university of texas at austin.

tomas mikolov, kai chen, greg corrado, and jeffrey
dean. 2013. ef   cient estimation of word representa-
tions in vector space. http://arxiv.org/abs/
1301.3781/.

margaret mitchell, xufeng han, jesse dodge, alyssa
mensch, amit goyal, alex berg, kota yamaguchi,
tamara berg, karl stratos, and hal daum  e iii. 2012.
midge: generating image descriptions from computer
in proceedings of eacl, pages
vision detections.
747   756, avignon, france.

gregory murphy. 2002. the big book of concepts. mit

press, cambridge, ma.

vicente ordonez, jia deng, yejin choi, alexander berg,
and tamara berg. 2013. from large scale image cate-
gorization to entry-level categories. in proceedings of
iccv, pages 1   8, sydney, australia.

genevieve patterson, chen xu, hang su, and james
hays. 2014. the sun attribute database: beyond cat-
egories for deeper scene understanding. international
journal of id161, 108(1-2):59   81.

adam pauls and dan klein. 2012. large-scale syntactic
in proceedings of

id38 with treelets.
acl, pages 959   968, jeju island, korea.

olga russakovsky and li fei-fei. 2010. attribute learn-
ing in large-scale datasets. in proceedings of eccv,
pages 1   14.

mohammad sadeghi and ali farhadi. 2011. recognition
using visual phrases. in proceedings of cvpr, pages
1745   1752, colorado springs, co.

carina silberer, vittorio ferrari, and mirella lapata.
2013. models of semantic representation with visual
in proceedings of acl, pages 572   582,
attributes.
so   a, bulgaria.

josef sivic and andrew zisserman. 2003. video google:
a text retrieval approach to object matching in videos.
in proceedings of iccv, pages 1470   1477, nice,
france.

richard socher, milind ganjoo, christopher manning,
and andrew ng. 2013. zero-shot learning through
cross-modal transfer. in proceedings of nips, pages
935   943, lake tahoe, nv.

peter turney and patrick pantel. 2010. from frequency
to meaning: vector space models of semantics. jour-
nal of arti   cial intelligence research, 37:141   188.

peter turney, yair neuman, dan assaf, and yohai co-
hen. 2011. literal and metaphorical sense identi   -
cation through concrete and abstract context. in pro-
ceedings of emnlp, pages 680   690, edinburgh, uk.

