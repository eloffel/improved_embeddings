parser for id15 using learning to search

sudha rao1,3   , yogarshi vyas1,3   , hal daum  e iii1,3, philip resnik2,3

1computer science, 2linguistics, 3umiacs

university of maryland

raosudha@cs.umd.edu, yogarshi@cs.umd.edu, hal@cs.umd.edu, resnik@umd.edu

5
1
0
2

 
t
c
o
6
2

 

 
 
]
l
c
.
s
c
[
 
 

1
v
6
8
5
7
0

.

0
1
5
1
:
v
i
x
r
a

abstract

we develop a novel technique to parse
english sentences into abstract meaning
representation (amr) using searn, a
learning to search approach, by modeling
the concept and the relation learning in a
uni   ed framework. we evaluate our parser
on multiple datasets from varied domains
and show an absolute improvement of 2%
to 6% over the state-of-the-art. addition-
ally we show that using the most frequent
concept gives us a baseline that is stronger
than the state-of-the-art for concept pre-
diction. we plan to release our parser for
public use.

1 introduction

meaning

representation
abstract
(banarescu et al., 2013)
is a semantic repre-
sentation which is a rooted, directed, acyclic
graph where the nodes represent concepts (words,
propbank (palmer et al., 2005) framesets or spe-
cial keywords) and the edges represent relations
between these concepts.
figure 1 shows the
complete amr for a sample sentence.

the key motivation behind developing amr
was to have a comprehensive and broad-coverage
semantic formalism that puts together the best in-
sights from a variety of semantic annotations (like
named entities, co-reference, semantic relations,
discourse connectives, temporal entities, etc.) in
a way that would enable it to have the same kind
of impact that syntactic treebanks have on natu-
ral language processing tasks. currently, there are
approximately 20,000 sentences which have been
annotated with their amrs, but for such a repre-
sentation to be useful for almost any nlp task, a
larger set of annotations would be needed. algo-
rithms that can perform automatic semantic pars-

   the    rst two authors contributed equally to this work.

read-01

arg0

i

name

op1

op2

arg1

book

name

op3

stories

nature

from

topic

forest

figure 1: amr graph for the sentence    i read a
book, called stories from nature, about the forest.   

ing of sentences into amr can help alleviate the
problem of paucity of manual annotations.

automatic id29 for amr is still in
a nascent stage. there have been two published
approaches for automatically parsing english sen-
tences into amr. flanigan et al.
(2014) use a
semi-markov model to    rst identify the concepts,
and then    nd a maximum spanning connected
subgraph that de   nes the relations between these
concepts. the other approach (wang et al., 2015)
uses a transition-based algorithm to convert the de-
pendency representation of a sentence to its amr.
tech-
nique
searn
(daum  e iii et al., 2009), a learning to search
(l2s) algorithm.
searn and other l2s algo-
rithms have proven to be highly effective for tasks
like part-of-speech tagging, named entity recog-
nition (daum  e iii et al., 2014), and for even more
complex id170 tasks like corefer-
ence resolution (ma et al., 2014) and dependency
parsing (he et al., 2013). using searn allows us

in this work, we develop a novel

for amr parsing that uses

i

read

a

book

called

stories

from

nature

call-01

story

null

null

i

read-01

null

book

null

stories

from

nature

called

null

(a) concept prediction stage: shaded nodes indicate predicted concepts (current state). the middle row represents the oracle
action. other rows represents other possible actions.

r

arg0

i

r

arg0

r

arg0

r

arg0

b

i

arg1

b

i

mod

b

i

b

(b) sample current state for re-
lation prediction

(c) three possible actions given the current state for relation prediction, the last one being the
true relation i.e. no edge

figure 2: using searn for amr parsing

to model the learning of concepts and relations in
a uni   ed framework which aims to minimize the
loss over the entire predicted structure, as opposed
to minimizing the loss over concepts and relations
in two separate stages, as is done by flanigan et al
(2014).

there are three main contributions of this work.
firstly, we provide a novel algorithm based on
searn to parse sentences into amrs. addi-
tionally, our parser extracts possible    candidates   
for the right concepts and relations from the en-
tire training data, but only uses smaller sentences
to train the learning algorithm. this is impor-
tant since amr annotations are easier to obtain
for smaller sentences.
secondly, we evaluate
our parser on datasets from various domains, un-
like previous works, which have been restricted
to newswire text. we observe that our parser
performs better than the existing state-of-the-art
parser, with an absolute improvement of 2 to 6
% over the different datasets. finally, we show
that using the most frequently aligned concept for
each word in the sentence (as seen in the train-
ing data) as the predicted concept, proves to be a
strong baseline for concept prediction. this base-
line does better than existing approaches, and we
show that our parser performs as well as the base-
line at this part of the task in some datasets, and

even better in some others.

the rest of this paper is organized as follows.
in the next section, we brie   y review searn and
explain its various components with respect to our
amr parsing task. section 3 describes our main
algorithm along with the strategies we use to deal
with the large search space of the search prob-
lem. we then describe our experiments and results
(section 4).

2 using searn

the task of generating an amr given a sentence
is a id170 task where the structure
that we are trying to predict is a singly rooted,
connected directed graph with concepts (nodes)
and relations (edges). in this work, we design an
amr parser that learns to predict this structure
using searn. searn solves complex structured
prediction problems by decomposing it into clas-
si   cation problems.
it does so by decomposing
the structured output, y, into a sequence of deci-
sions y1, y2, ..., ym and then using a classi   er to
make predictions for each component in turn, al-
lowing for dependent predictions. we decompose
the amr prediction problem into the three prob-
lems of predicting the concepts of the amr, pre-
dicting the root and then predicting the relations
between the predicted concepts (explained in more

detail under section 3). below, we explain how we
use searn, with reference to a running example
in figure 2.

searn works on the notion of a policy which
can be de   ned as    what is the best next action (yi)
to take    in a search space given the current state
(s = (x, y1, y2, .., yi   1)), where x is the input.
for our problem, a state during the concept pre-
diction phase is de   ned as the concepts predicted
for a part of the input sentence. similarly, a state
during the relation prediction phase is de   ned as
the set of relations predicted for certain pairs of
concepts obtained during the concept prediction
stage. (in figure 2a (concept prediction), the cur-
rent state corresponds to the concepts {   i   ,    read-
01   ,    book   } predicted for a part of the sentence.
in figure 2c (relation prediction), the current state
corresponds to the relation    arg0    predicted be-
tween    r    and    i    )

at training time, searn operates in an iterative
fashion. it starts with some initial policy and given
an input x, makes a prediction y = y1, y2, ..., ym
using the current policy. for each prediction yi it
generates a set of cost-sensitive multi-class classi-
   cation examples each of which correspond to a
possible action (a) the algorithm can take given
the current state. each example can be de   ned
using local features and features that depend on
previous predictions. the possible set of next ac-
tions in our concept prediction phase corresponds
to the set of possible concepts the next word can
take. the possible set of next actions in our rela-
tion prediction phase corresponds to the set of pos-
sible relations the next pair of concepts can take.
(in figure 2a (concept prediction), the next action
is assigning one of {   call-01   ,    called   , null} to
the word    called   .
in figure 2c (relation predic-
tion), the next action is assigning one of {   arg1   ,
   mod   , no-edge} to the pair of concept    b    and
   i   ).

during training, searn has access to an    ora-
cle    policy which gives the true best action (a   )
given the current state . our oracle returns the cor-
rect concept and relation labels in the concept pre-
diction and relation prediction phase respectively.
(in figure 2a (concept prediction), the oracle will
return null and in figure 2c (relation prediction),
the oracle will return no-edge). searn then cal-
culates the loss between a and a    using a pre-
speci   ed id168.
it then computes a new
policy based on this loss and interpolates it with

the current policy to get an updated policy, before
moving on to the next iteration.

at test time, predictions are made greedily us-

ing the policy learned during training time.

3 methodology

3.1 learning technique

ci = predict concept(si)

algorithm 1
1: for each span si do
2:
3: end for
4: croot = predict root([c1, ..., cn])
5: for each concept ci do
6:
7:
8:
9:
10: end for

for each j < i do

r(i,j) = predict relation(ci, cj)
r(j,i) = predict relation(cj , ci)

end for

we use searn as described in section 2 to learn
a model that can successfully predict the amr y
for a sentence x. the sentence x is composed of
a sequence of spans (s1, s2, ..., sn) each of which
can be a single word or a span of words (we de-
scribe how we go from a raw sentence to a se-
quence of spans in section 4.2). given that our
input has n spans, we    rst decompose the struc-
ture into a sequence of n2 + 1 predictions d =
(c, root, r), where

c = c1, c2, ..., cn - where ci is the concept pre-

dicted for span si

root is the decision of choosing one of the
predicted concepts as the root (croot) of the amr
r = r2,   , r   ,2, r3,   , r   ,3, ..., rn,   , r   ,n - where
ri,    are the predictions for the directed relations
from ci to cj    j < i, and r   ,i are the predictions
for the directed relations from cj to ci    j < i. we
constrain our algorithm to not predict any incom-
ing relations to croot.

during training time, the possible set of ac-
tions for each prediction is given by the k-best
list, which we will describe in section 3.2. we
use hamming loss as our id168. under
hamming loss, the oracle policy is simply choos-
ing the right action for each prediction. since this
loss is de   ned on the entire predicted output, the
model learns to minimize the loss for concepts and
relations jointly.

algorithm 1 describes the sequence of pre-
dictions to be made in our problem. we learn

feature label

description

wi   2, wi   i, wi, wi+1, wi+2 words in si and context
pi   2, pi   i, pi, pi+1, pi+2

n ei

si

depi
bc

ci   2, ci   1

c

f ramei and sensei

pos tags of words in si and context
named entity tags for words in si
binary feature indicating whether wi is(are) stopword(s)
all dependency edges originating from words in wi
binary feature indicating whether c is the most frequently aligned
concept with si or not
predicted concepts for two previous spans
concept label and its conjunction with all previous features
if the label is a propbank frame (e.g.
(   see   ) and the sense(   01   ) as additional features.

   see-01   , use the frame

table 1: concept prediction features for span si and concept label ci

description
the two concepts and their conjunction

feature label
ci, cj , ci     cj
wi, wj , wi     wj words in the corresponding spans and their conjunction
pi, pj, pi     pj

pos tags of words in spans and their conjunction
all dependency edges with tail in wi and head in wj
binary feature which is true iff i < j
relation label and its conjunction with all other features

depij
dir
r

table 2: relation prediction features for concepts ci and cj and relation label r

feature label description

ci

wi
pi

is dep rooti

concept label. if the label is a propbank frame (e.g.    see-01   , use
the frame (   see   ) and the sense(   01   ) as additional features.
words in si, i.e. the span corresponding to ci
pos tags of words in si
binary feature indicating whether one of the words in si is the
root in the dependency tree of the sentence

table 3: root prediction features for concept ci

three different policies corresponding to each of
the functions predict concept, predict root and
predict relation. the learner in each stage uses
features that depend on predictions made in the
previous stages. tables 1, 2 and 3 describe the
set of features we use for the concept prediction,
relation prediction and root prediction stages re-
spectively.

all concepts that were aligned to si in the training
data. if si has not been seen in the training data,
cl-consi consists of the lemmatized span, prop-
bank frames (for verbs) obtained using the uni   ed
verb index (schuler, 2005) and the null concept.
relation candidates: the candidate list of re-
lations for a relation from concept ci to concept cj,
cl-relij, is the union of the following three sets:

3.2 selecting k-best lists

for predicting the concepts and relations using
searn, we need a candidate-list (possible set of
actions) to make predictions from.

concept candidates:

the
candidate-list of concepts, cl-consi is the set of

for a span si,

    pairwisei,j - all directed relations from ci to
cj when ci and cj occurred in the same amr,
    outgoingi - all outgoing relations from ci,

and

    incomingj - all incoming relations into cj.

in the case when both ci and cj have not been

seen in the training data, cl-relij consists of all
relations seen in the training data. in both cases,
we also provide an option no-edge which indi-
cates that there is no relation between ci and cj.

3.3 pruning the search space
to prune the search space of our learning task,
and to improve the quality of predictions, we use
two observations about the nature of the edges of
the amr of a sentence, and its dependency tree,
within our algorithm.

first, we observe that a large fraction of the
edges in the amr for a sentence are between con-
cepts whose underlying spans (more speci   cally,
the words in these underlying spans) are within
two edges of each other in the dependency tree
of the sentence. thus, we refrain from calling
the predict relation function in algorithm 1 be-
tween concepts ci and cj if each word in wi is three
or more edges away from all words in wj in the
dependency tree of the sentence under considera-
tion, and vice versa. this implies that there will be
no relation rij in the predicted amr of that sen-
tence. this doesn   t affect the number of calls to
predict relation in the worst case (n2     n, for a
sentence with n spans), but practically, the number
of calls are far fewer. also, to make sure that this
method does not    lter out too many amr edges,
we calculated the percentage of amr edges that
are more than two edges away in dependency tree.
we found this number to be only about 5% across
all our datasets.

secondly, and conversely, we observe that for a
large fraction of words which have a dependency
edge between them, there is an edge in the amr
between the concepts corresponding to those two
words. thus, when we observe two concepts ci
and cj which satisfy this property, we force our
predict relation function to assign a relation rij
that is not null.

3.4 training on smaller sentences
for a sentence containing n spans, algorithm 1
has to make n2 predictions in the worst case, and
this can be inhibitive for large values of n. to
deal with this, we use a parameter to indicate a
cut-off on the length of a sentence (c), and only
use sentences whose length (number of spans) is
less than or equal to c. this parameter can be
varied based on the size of the training data and
the distribution of the length of the sentences in
the training data. setting a higher values of c will

cause the model to use more sentences for train-
ing, but spend longer time, whereas lower values
will train quickly on fewer sentences. in our ex-
periments, a c-value between 10 and 15 gave us
the best balance between training time, and num-
ber of examples considered.

4 experiments and results

4.1 dataset and method of evaluation

we use the publicly available amr annotation
release 1.0 (ldc2014t12) corpus for our ex-
periments. this corpus consists of datasets from
varied domains such as online discussion forums,
blogs, and newswire, with about 13,000 sentence-
amr pairs. previous works have only used one of
these datasets for evaluation (proxy), but we eval-
uate our parser on all of them. additionally, we
also use the freely available amrs for the little
prince, (lp) 1 which is from a more literary do-
main. all datasets have a pre-speci   ed training
and test split (table 4).

as stated earlier (sections 3.2 and 3.4), we use
the entire training set to extract the candidate lists
for concept prediction and relation prediction, but
train our learning algorithm on only a subset of the
sentence-amr pairs in the training data, which is
obtained by selecting sentences having less than
a    xed number of spans (c, set to 10 for all our
experiments). table 4 also mentions the number of
sentences in each training dataset that are of length
    c (column training (    c)).

bolt
proxy
xinhua

dataset training training (    c) test
133
823
86
229
173

1061
6603
741
1703
1274

119
1723
115
438
584

dfa
lp

table 4: dataset statistics. all    gures represent
number of sentences.

we compare our results against those of the
jamr parser 2 of flanigan et. al (2014) 3. we run
the parser with the con   guration that is speci   ed
to give the best results.

1http://amr.isi.edu/download.html
2https://github.com/jflanigan/jamr
3the transition-based parser by wang et al. () is newer,
but the latest release of jamr performs better, hence we do
not compare against the former.

the evaluation of predicted amrs is done using
smatch (cai and knight, 2013) 4, which compares
two amrs using precision, recall and f1. addi-
tionally, we also evaluate how good we are at pre-
dicting the concepts of the amrs, by calculating
precision, recall and f1 against the gold-concepts
that are aligned to the induced spans during test
time.

4.2 preprocessing
jamr aligner: the training data for amr pars-
ing consists of sentences paired with correspond-
ing amrs. to convert a raw sentence into a se-
quence of spans (as required by our algorithm),
we obtain alignments between words in the sen-
tence and concepts in the amr using the auto-
matic aligner of jamr. the alignments obtained
can be of three types (examples refer to figure 1):

    a single word aligned to a single concept:
e.g., word    read    aligned to concept    read-
01   .

    span of words aligned to a graph fragment:
e.g., span    stories from nature    aligned to
the graph fragment rooted at    name   . this
usually happens for named entities and multi-
word expressions such as those related to date
and time.

    a word aligned to null concept: most func-
tion words like    about   ,    a   ,    the   , etc are not
aligned to any particular concept. these are
considered to be aligned to the null con-
cept.

forced alignments: the jamr aligner does
not align all concepts in a given amr to a span
in the sentence. we use a heuristic to forcibly
align these leftover concepts and improve the qual-
ity of alignments. for every unaligned concept, we
count the number of times an unaligned word oc-
curs in the same sentence with the unaligned con-
cept across all training examples. we then align
every leftover concept in every sentence with the
unaligned word in the sentence with which it has
maximally coocurred.

span identi   cation: during training time, the
aligner takes in a sentence and its amr graph and
splits each sentence into spans that can be aligned
to the concepts in the amr. however, during test
time, we do not have access to the amr graph.
hence, given a test sentence, we need to split the

sentence into spans, on which we can predict con-
cepts. we consider each word as a single span ex-
cept for two cases. first, we detect possible multi-
word spans corresponding to named entities, using
a named entity recognizer (lafferty et al., 2001).
second, we use some basic id157 to
identify time and date expressions in sentences.

4.3 experiments
to train our model, we use searn as implemented
in the vowpal wabbit machine learning library
(langford et al., 2007; daum  e iii et al., 2014).

for each dataset, we run three kinds of exper-
iments. they differ in how they get the concepts
during test time. all of them use the approach de-
scribed in section 3.1 for predicting the relations.

    oracle concept - use the true concept

aligned with each span.

    1-best concept - use the concept with which
the span was most aligned in the training
data.

    fully automatic - use the concepts predicted
using the approach described in section 3.1.

4.4 connectivity
algorithm 1 does not place explicit constraints on
the structure of the amr. hence, the predicted
output can have disconnected components. since
we want the predicted amr to be connected, we
connect the disconnected components (if any) us-
ing the following heuristic. for each component,
we    nd its roots (i.e. concepts with no incom-
ing relations). we then connect the components
together by simply adding an edge from our pre-
dicted root croot to each of the component roots.
to decide what edge to use between our predicted
root croot and the root of a component, we get the
k-best list (as described in section 3.2) between
them and choose the most frequent edge from this
list.

4.5 acyclicity
the post-processing step described in the previ-
ous section ensures that the predicted amrs are
rooted, connected, graphs. however, an amr, by
de   nition, is also acyclic. we do not model this
constraint explicitly within our learning frame-
work. despite this, we observe that only a very
small number of amrs predicted using our fully
automatic approach have cycles in them. out of
the total 1,444 amrs predicted in all test sets, less

4http://amr.isi.edu/download/smatch-v2.0.tar.gz

dataset

bolt
proxy
xinhua

dfa
lp

oracle concepts
p
f1
0.58
0.67
0.64
0.54
0.63

r
0.53
0.65
0.60
0.47
0.58

0.64
0.69
0.68
0.62
0.70

our results

1-best concepts
p
f1
0.47
0.60
0.52
0.42
0.50

r
0.43
0.59
0.49
0.37
0.45

0.52
0.61
0.55
0.48
0.57

fully automatic
p
f1
0.46
0.61
0.52
0.44
0.52

r
0.42
0.60
0.50
0.40
0.49

0.51
0.62
0.56
0.48
0.54

jamr results
fully automatic
p
f1
0.41
0.59
0.48
0.23
0.46

r
0.33
0.53
0.40
0.15
0.41

0.55
0.68
0.59
0.52
0.53

table 5: full results

dataset

our results

1-best

r
0.72
0.77
0.77
0.72
0.79

p

0.74
0.79
0.74
0.76
0.77

f1
0.73
0.78
0.74
0.74
0.78

bolt
proxy
xinhua

dfa
lp

fully automatic
p
f1
0.73
0.78
0.75
0.75
0.79

r
0.72
0.78
0.77
0.76
0.80

0.74
0.78
0.74
0.74
0.77

jamr results
fully automatic
p
f1
0.63
0.73
0.63
0.48
0.46

r
0.55
0.68
0.57
0.33
0.41

0.73
0.78
0.69
0.85
0.53

table 6: concept prediction results

than 5% have cycles in them. besides, almost all
cycles that are predicted consist of only two nodes,
i.e. both rij and rji have non-no-edge values for
concepts ci and cj. to get an acyclic graph, we
can greedily select one of rij or rji, without any
loss in parser performance.

4.6 results
table 5 shows the result of running our parser
on all    ve datasets. by running our fully auto-
matic approach, we get an absolute improvement
of about 2% to 6% on most datasets as com-
pared to jamr. surprisingly, we observe a large
improvement of 21% on the online discussion fo-
rum dataset (dfa).
in all cases, our results indi-
cate a more balanced output in terms of precision
and recall as compared to jamr, with consistently
higher recall.

it should be noted that selecting the 1-best con-
cept also gives better results than jamr. this in-
dicates that the 1-best baseline is strong, and pos-
sibly, not very easy to beat. to reinforce this, we
evaluate our concept predictions separately. the
results are shown in table 6. first, observe that
going from the fully learned concept prediction
to the 1-best concept shows only a small (or in
some cases, no) drop in performance. second,
note that we show a consistent absolute improve-
ment of 10% to 12% over the concept prediction

results of jamr. as in the full prediction case,
we observe a large performance increase (27%)
on the online discussion forum dataset.

5 related work

semantic representations and techniques for pars-
ing them have a rich and varied history. amr
itself is based on propositional
logic and neo-
davidsonian semantics (davidson, 1967). amr
is not
intended to be an interlingua, but due
to the various assumptions made while creat-
ing an amr (dropping tense, function words,
morphology, etc.), it does away with language-
speci   c idiosyncrasies and interlingual represen-
tations (dorr, 1992) are thus, important predeces-
sors to amr.
the

there
sen-
have been various
raw
tences
forms
sentences
wong and mooney, 2006).
(kate et al., 2005;
and collins
zettlemoyer
the work
by
to
attempts
(zettlemoyer and collins, 2005)
language sentences to a lambda-
map natural
calculus encoding of
they
do so by treating the problem as a structured
learning task, and use a log-linear model to learn a
probabilistic combinatory categorical grammar

task of amr parsing,

to parse
given

their semantics.

annotated

attempts

logical

form,

such

like

with

into

a

(id35) (steedman and baldridge, 2011), which is
a grammar formalism based on id198.

amr aims to combine various semantic annota-
tions to produce a uni   ed annotation, but it mainly
builds on top of propbank (palmer et al., 2005).
propbank has found extensive use in other se-
mantic tasks such as shallow id29
(giuglea and moschitti, 2006),

in our work we used searn to build an
amr parser.
searn comes from a family of
algorithms called    learning to search (l2s)   
that solves id170 problems by
decomposing the structured output
in terms
of an explicit search space and then learning
a policy that can take actions in this search
space in the optimal way.
incremental struc-
tured
(collins and roark, 2004;
huang et al., 2012), dagger (ross et al., 2011),
aggrevate
etc.
(daum  e iii and marcu, 2005; xu and fern, 2007;
ratliff et al., 2007;
xu et al., 2007;
syed and schapire, 2010;
doppa et al., 2012)
are other algorithms that also belong to this
family.

(ross and bagnell, 2014),

id88

6 conclusion and future work

we have presented a novel technique for pars-
ing english sentences into amr using a learning
to search approach. we model the concept and
the relation learning in a uni   ed framework using
searn which allows us to optimize over the loss
of the entire predicted output. we evaluate our
parser on multiple datasets from varied domains
and show that our parser performs better than the
state-of-the-art across all the datasets. we also
show that a simple technique of choosing the most
frequent concept gives us a baseline that is better
than the state-of-the-art for concept prediction.

currently we ensure various properties of
amr, such as connectedness and acyclicity using
heuristics.
in the future, we plan to incorporate
these as constraints in our learning technique.

references
[banarescu et al.2013] laura banarescu, claire bonial,
shu cai, madalina georgescu, kira grif   tt, ulf
hermjakob, kevin knight, philipp koehn, martha
palmer, and nathan schneider.
2013. abstract
meaning representation for sembanking.

[cai and knight2013] shu cai and kevin knight.
2013. smatch: an evaluation metric for semantic

feature structures.
in proceedings of the 51st an-
nual meeting of the association for computational
linguistics, acl 2013, 4-9 august 2013, so   a, bul-
garia, volume 2: short papers, pages 748   752.

[collins and roark2004] michael collins and brian
roark. 2004. incremental parsing with the percep-
tron algorithm. in proceedings of the 42nd annual
meeting on association for computational linguis-
tics, page 111. association for computational lin-
guistics.

[daum  e iii and marcu2005] hal daum  e iii and daniel
marcu. 2005. learning as search optimization: ap-
proximate large margin methods for structured pre-
diction.
in proceedings of the 22nd international
conference on machine learning, pages 169   176.
acm.

[daum  e iii et al.2009] hal daum  e iii, john langford,
and daniel marcu. 2009. search-based structured
prediction. machine learning, 75(3):297   325.

[daum  e iii et al.2014] hal daum  e iii, john langford,
and stephane ross. 2014. ef   cient programmable
learning to search. arxiv preprint arxiv:1406.1837.

[davidson1967] donald davidson. 1967. the logical

form of action sentences.

[doppa et al.2012] janardhan rao doppa, alan fern,
output space
arxiv preprint

and prasad tadepalli.
2012.
search for id170.
arxiv:1206.6460.

[dorr1992] bonnie j dorr. 1992. the use of lexical
semantics in interlingual machine translation. ma-
chine translation, 7(3):135   193.

[flanigan et al.2014] jeffrey flanigan, sam thomson,
jaime g. carbonell, chris dyer, and noah a. smith.
2014. a discriminative graph-based parser for the
id15.
in proceedings
of the 52nd annual meeting of the association for
computational linguistics, acl 2014, june 22-27,
2014, baltimore, md, usa, volume 1: long papers,
pages 1426   1436.

[giuglea and moschitti2006] ana-maria giuglea and
2006. semantic role la-
alessandro moschitti.
beling via framenet, verbnet and propbank.
in
acl 2006, 21st international conference on com-
putational linguistics and 44th annual meeting of
the association for computational linguistics, pro-
ceedings of the conference, sydney, australia, 17-21
july 2006.

[he et al.2013] he he, hal daum  e iii, and jason eis-
ner. 2013. dynamic feature selection for depen-
dency parsing.
in proceedings of the 2013 con-
ference on empirical methods in natural language
processing, emnlp 2013, 18-21 october 2013,
grand hyatt seattle, seattle, washington, usa, a
meeting of sigdat, a special interest group of the
acl, pages 1455   1464.

[wang et al.2015] chuan wang, nianwen xue, sameer
pradhan, and sameer pradhan. 2015. a transition-
based algorithm for amr parsing.

[wong and mooney2006] yuk wah wong and ray-
mond j. mooney. 2006. learning for semantic
parsing with id151. in hu-
man language technology conference of the north
american chapter of the association of compu-
tational linguistics, proceedings, june 4-9, 2006,
new york, new york, usa.

[xu and fern2007] yuehua xu and alan fern. 2007.
on learning linear ranking functions for beam
search.
in proceedings of the 24th international
conference on machine learning, pages 1047   1054.
acm.

[xu et al.2007] yuehua xu, alan fern, and sung wook
2007. discriminative learning of beam-
in ijcai, pages

yoon.
search heuristics for planning.
2041   2046.

[zettlemoyer and collins2005] luke s. zettlemoyer
and michael collins. 2005. learning to map sen-
tences to logical form: structured classi   cation with
probabilistic categorial grammars. in uai    05, pro-
ceedings of the 21st conference in uncertainty in
arti   cial intelligence, edinburgh, scotland, july 26-
29, 2005, pages 658   666.

[huang et al.2012] liang huang, suphan fayong, and
yang guo. 2012. structured id88 with inex-
act search. in proceedings of the 2012 conference of
the north american chapter of the association for
computational linguistics: human language tech-
nologies, pages 142   151. association for computa-
tional linguistics.

[kate et al.2005] rohit j. kate, yuk wah wong, and
raymond j. mooney. 2005. learning to trans-
form natural to formal languages. in proceedings,
the twentieth national conference on arti   cial in-
telligence and the seventeenth innovative applica-
tions of arti   cial intelligence conference, july 9-13,
2005, pittsburgh, pennsylvania, usa, pages 1062   
1068.

[lafferty et al.2001] john lafferty, andrew mccallum,
and fernando cn pereira. 2001. conditional ran-
dom    elds: probabilistic models for segmenting and
labeling sequence data.

[langford et al.2007] john langford, lihong li, and
2007. vowpal wabbit online

alexander strehl.
learning project.

[ma et al.2014] chao ma,

janardhan rao doppa,
j walker orr, prashanth mannem, xiaoli fern, tom
dietterich, and prasad tadepalli. 2014. prune-and-
score: learning for greedy coreference resolution.
in proceedings of conference on empirical methods
in natural language processing (emnlp).

[palmer et al.2005] martha palmer, paul kingsbury,
and daniel gildea. 2005. the proposition bank: an
annotated corpus of semantic roles. computational
linguistics, 31(1):71   106.

[ratliff et al.2007] nathan ratliff, david bradley, j an-
drew bagnell, and joel chestnutt. 2007. boost-
ing id170 for imitation learning.
robotics institute, page 54.

[ross and bagnell2014] stephane ross and j andrew
bagnell. 2014. reinforcement and imitation learn-
ing via interactive no-regret learning. arxiv preprint
arxiv:1406.5979.

[ross et al.2011] st  ephane ross, geoff j. gordon, and
j. andrew bagnell. 2011. a reduction of imitation
learning and id170 to no-regret on-
line learning.
in proceedings of the workshop on
arti   cial intelligence and statistics (aistats).

[schuler2005] karin kipper schuler. 2005. verbnet: a

broad-coverage, comprehensive verb lexicon.

[steedman and baldridge2011] mark steedman and ja-
son baldridge. 2011. combinatory categorial gram-
mar. non-transformational syntax oxford: black-
well, pages 181   224.

[syed and schapire2010] umar syed and robert e
schapire. 2010. a reduction from apprenticeship
learning to classi   cation. in advances in neural in-
formation processing systems, pages 2253   2261.

