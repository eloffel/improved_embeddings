6
1
0
2

 

p
e
s
1
2

 

 
 
]

v
c
.
s
c
[
 
 

2
v
2
0
9
2
0

.

2
1
5
1
:
v
i
x
r
a

movieqa: understanding stories in movies through question-answering

makarand tapaswi1,

antonio torralba2,

yukun zhu3,
raquel urtasun3,

rainer stiefelhagen1

sanja fidler3

1karlsruhe institute of technology, 2massachusetts institute of technology, 3university of toronto
{tapaswi,rainer.stiefelhagen}@kit.edu, torralba@csail.mit.edu, {yukun,urtasun,fidler}@cs.toronto.edu

http://movieqa.cs.toronto.edu

figure 1: our movieqa dataset contains 14,944 questions about 408 movies. it contains multiple sources of information: plots, subtitles,
video clips, scripts, and dvs transcriptions. in this    gure we show example qas from the matrix and localize them in the timeline.

abstract

we introduce the movieqa dataset which aims to eval-
uate automatic story comprehension from both video and
text. the dataset consists of 14,944 questions about 408
movies with high semantic diversity. the questions range
from simpler    who    did    what    to    whom   , to    why   
and    how    certain events occurred. each question comes
with a set of    ve possible answers; a correct one and four
deceiving answers provided by human annotators. our
dataset is unique in that it contains multiple sources of
information     video clips, plots, subtitles, scripts, and
dvs [32]. we analyze our data through various statistics
and methods. we further extend existing qa techniques
to show that question-answering with such open-ended se-
mantics is hard. we make this data set public along with an
evaluation benchmark to encourage inspiring work in this
challenging domain.

1. introduction

fast progress in deep learning as well as a large amount
of available labeled data has signi   cantly pushed forward

the performance in many visual tasks such as image tag-
ging, id164 and segmentation, action recognition,
and image/video captioning. we are steps closer to applica-
tions such as assistive solutions for the visually impaired,
or cognitive robotics, which require a holistic understand-
ing of the visual world by reasoning about all these tasks
in a common framework. however, a truly intelligent ma-
chine would ideally also infer high-level semantics underly-
ing human actions such as motivation, intent and emotion,
in order to react and, possibly, communicate appropriately.
these topics have only begun to be explored in the litera-
ture [27, 49].

a great way of showing one   s understanding about the
scene is to be able to answer any question about it [23].
this idea gave rise to several question-answering datasets
which provide a set of questions for each image along with
multi-choice answers. these datasets are either based on
rgb-d images [23] or a large collection of static photos
such as microsoft coco [1, 47]. the types of questions
typically asked are    what    is there and    where    is it, what
attributes an object has, what is its relation to other objects
in the scene, and    how many    objects of certain type are
present. while these questions verify the holistic nature of

1

time source of information 0:00 0:15 1:00 1:15 0:30 0:45 1:30 1:45 2:00 plot video subtitle scripts dvs 01:04:08 --> 01:04:09  ... you know what i realize?  01:04:17 --> 01:04:18 ignorance is bliss.   00:40:42 --> 00:40:47   it exists now only as part of a     neural-interactive simulation    00:40:47 --> 00:40:48   that we call the matrix.     the matrix is revealed to be a shared simulation of the world as it was in 1999     ... secretly betrayed morpheus to agent smith in exchange for a comfortable         morpheus and trinity exit the matrix, but smith ambushes and kills neo before he can          he ends the call and flies into the sky.     neo meets morpheus,         trinity contacts him confirming that morpheus can                    about to disconnect when an anonymous message slices onto the screen.  screen do you want to know what the matrix is, neo? neo is seen exiting the phone booth and observing the surrounding people.  he looks up and flies to the skies. a: a shared simulation of the world a: a group of robots  a: a human body  a: a set of numbers stored as a table what is the matrix? who kills neo in the matrix? why does cypher betray morpheus? how does the movie end? a: smith kills neo a: with neo flying into the sky a: in exchange for a comfortable life a: with the machines chasing after neo  a: we see mr. smith torture morpheus  a: trinity kills neo  a: morpheus kills neo after he realizes      that neo is not the one a: in exchange for money  a: because he is threatened by agent smith quiz a: his heart lights up

that he is    nally returning home?

q: how does e.t. show his happiness

q: why do joy and jack get married that    rst
q: how does patrick start winning kat over?
a: they are both vulnerable and totally drunk a: because he is upset that jenny left him a: by getting personal information about
figure 2: examples from the movieqa dataset. for illustration we show a single frame, however, all these questions/answers are time-
stamped to a much longer clip in the movie. notice that while some questions can be answered using vision or dialogs alone, most require
both. vision can be used to locate the scene set by the question, and semantics extracted from dialogs can be used to answer it.

q: why does forrest undertake a three-

night they meet in las vegas?

her likes and dislikes

year marathon?

our vision algorithms, there is an inherent limitation in what
can be asked about a static image. high-level semantics
about actions and their intent is mostly lost and can typi-
cally only be inferred from temporal, possibly life-long vi-
sual observations.

movies provide us with snapshots from people   s lives
that link into stories, allowing an experienced human viewer
to get a high-level understanding of the characters, their ac-
tions, and the motivations behind them. our goal is to create
a question-answering dataset to evaluate machine compre-
hension of both, complex videos such as movies and their
accompanying text. we believe that this data will help push
automatic semantic understanding to the next level, required
to truly understand stories of such complexity.

this paper introduces movieqa, a large-scale question-
answering dataset about movies. our dataset consists of
14,944 multiple-choice questions with    ve deceiving op-
tions, of which only one is correct, sourced from 408
movies with high semantic diversity. for 140 of these
movies (6,462 qas), we have timestamp annotations indi-
cating the location of the question and answer in the video.
the questions range from simpler    who    did    what    to
   whom    that can be solved by vision alone, to    why   
and    how    something happened, that can only be solved
by exploiting both the visual information and dialogs (see
fig. 2 for a few example    why    and    how    questions). our
dataset is unique in that it contains multiple sources of infor-
mation: video clips, subtitles, scripts, plots, and dvs [32]
as illustrated in fig. 1. we analyze the data through vari-
ous statistics and intelligent baselines that mimic how dif-
ferent    students    would approach the quiz. we further ex-
tend existing qa techniques to work with our data and
show that question-answering with such open-ended se-
mantics is hard. we have created an online benchmark
with a leaderboard (http://movieqa.cs.toronto.
edu/leaderboard), encouraging inspiring work in this
challenging domain.
2. related work

integration of language and vision is a natural step to-
wards improved understanding and is receiving increas-

ing attention from the research community. this is in
large part due to efforts in large-scale data collection such
as microsoft   s coco [22], flickr30k [46] and abstract
scenes [50] providing tens to hundreds of thousand im-
ages with natural language captions. having access to such
data enabled the community to shift from hand-crafted lan-
guage templates typically used for image description [19] or
retrieval-based approaches [11, 26, 45] to deep neural mod-
els [6, 13, 15, 42] that achieve impressive captioning results.
another way of conveying semantic understanding of both
vision and text is by retrieving semantically meaningful im-
ages given a natural language query [13]. an interesting
direction, particularly for the goals of our paper, is also the
task of learning common sense knowledge from captioned
images [40]. this has so far been demonstrated only on syn-
thetic clip-art scenes which enable perfect visual parsing.

video understanding via language. in the video do-
main, there are fewer works on integrating vision and lan-
guage, likely due to less available labeled data. in [10, 41],
the authors caption video clips using lstms,
[33] for-
mulates description as a machine translation model, while
older work uses templates [3, 8, 18]. in [21], the authors
retrieve relevant video clips for natural language queries,
while [29] exploits captioned clips to learn action and
role models. for tv series in particular, the majority of
work aims at recognizing and tracking characters in the
videos [2, 4, 28, 35]. in [7, 34], the authors aligned videos
with movie scripts in order to improve scene prediction.
[39] aligns movies with their plot synopses with the aim
to allow semantic browsing of large video content via tex-
tual queries. just recently, [38, 49] aligned movies to books
with the aim to ground temporal visual data with verbose
and detailed descriptions available in books.

question-answering. qa is a popular task in nlp with
signi   cant advances made recently with neural models such
as memory networks [36], deep lstms [12], and struc-
tured prediction [43]. in id161, [23] proposed a
bayesian approach on top of a logic-based qa system [20],
while [24, 30] encoded both an image and the question us-
ing an lstm and decoded an answer. we are not aware of
qa methods addressing the temporal domain.

#movies
#qa
q #words
ca. #words
wa. #words

total

408
14944
9.3    3.5
5.6    4.1
5.1    3.9

train

val

test

movies with plots and subtitles

269
9848
9.3
5.7
5.2

56
1958
9.3
5.4
5.0

83
3138
9.5
5.4
5.1
movies with video clips
26
1258
1288
211.4
46.6

93
4318
4385
201.0
45.6

21
886
1098
198.5
49.0

#movies
#qa
#video clips
mean clip dur. (s)
mean qa #shots

140
6462
6771
202.7    216.2
46.3    57.1
table 1: movieqa dataset stats. our dataset supports two modes
of answering: text and video. we present the split into train, val,
and test splits for the number of movies and questions. we also
present mean counts with standard deviations in the total column.

qa datasets. most available datasets focus on im-
age [17, 22, 46, 50] or video description [5, 32, 9]. par-
ticularly relevant
to our work is the moviedescription
dataset [32] which transcribed text from the described
video service (dvs), a narration service for the visually
impaired, for a collection of over 100 movies. for qa, [23]
provides questions and answers (mainly lists of objects, col-
ors, etc.) for the nyuv2 rgb-d dataset, while [1, 47] do
so for ms-coco with a dataset of a million qas. while
these datasets are unique in testing the vision algorithms in
performing various tasks such as recognition, attribute in-
duction and counting, they are inherently limited to static
images. in our work, we collect a large qa dataset sourced
from over 400 movies with challenging questions that re-
quire semantic reasoning over a long temporal domain.

our dataset is also related to purely text qa datasets
such as mctest [31] which contains 660 short stories with
4 multi-choice qas each, and [12] which converted 300k
news summaries into cloze-style questions. we go beyond
these datasets by having signi   cantly longer text, as well as
multiple sources of available information (plots, subtitles,
scripts and dvs). this makes our data one of a kind.
3. movieqa dataset

the goal of our paper is to create a challenging bench-
mark that evaluates semantic understanding over long tem-
poral data. we collect a dataset with very diverse sources
of information that can be exploited in this challenging do-
main. our data consists of quizzes about movies that the
automatic systems will have to answer. for each movie, a
quiz comprises of a set of questions, each with 5 multiple-
choice answers, only one of which is correct. the system
has access to various sources of textual and visual informa-
tion, which we describe in detail below.

we collected 408 subtitled movies, and obtained their
extended summaries in the form of plot synopses from
wikipedia. we crawled imsdb for scripts, which were avail-

figure 3: average number of words in movieqa dataset based on
the    rst word in the question. area of a bubble indicates #qa.

able for 49% (199) of our movies. a fraction of our movies
(60) come with dvs transcriptions provided by [32].

plot synopses are movie summaries that fans write af-
ter watching the movie. synopses widely vary in detail and
range from one to 20 paragraphs, but focus on describing
content that is directly relevant to the story. they rarely con-
tain detailed visual information (e.g. character appearance),
and focus more on describing the movie events and charac-
ter interactions. we exploit plots to gather our quizzes.

videos and subtitles. an average movie is about 2
hours in length and has over 198k frames and almost 2000
shots. note that video alone contains information about
e.g.,    who    did    what    to    whom   , but may be lacking in
information to explain why something happened. dialogs
play an important role, and only both modalities together
allow us to fully understand the story. note that subtitles do
not contain speaker information. in our dataset, we provide
video clips rather than full movies.

dvs is a service that narrates movie scenes to the visu-
ally impaired by inserting relevant descriptions in between
dialogs. these descriptions contain suf   cient    visual    in-
formation about the scene that they allow visually impaired
audience to follow the movie. dvs thus acts as a proxy for
a perfect vision system, and is another source for answering.
scripts. the scripts that we collected are written by
screenwriters and serve as a guideline for movie making.
they typically contain detailed descriptions of scenes, and,
unlike subtitles, contain both dialogs and speaker informa-
tion. scripts are thus similar, if not richer in content to
dvs+subtitles, however are not always entirely faithful to
the movie as the director may aspire to artistic freedom.

3.1. qa collection method

since videos are dif   cult and expensive to provide to an-
notators, we used plot synopses as a proxy for the movie.
while creating quizzes, our annotators only referred to the
story plot and were thus automatically coerced into asking
story-like questions. we split our annotation efforts into two
primary parts to ensure high quality of the collected data.

12345678910correct answer avg. #words7.07.58.08.59.09.510.010.511.0question avg. #wordswhatwhowhyhowwherewhendoeswhichinisothersimg vid goal

txt
mctest [31]
 
babi [44]
 
id98+dailymail [12]  
daquar [23]
-
-
visual madlibs [47]
-
vqa (v1) [1]
movieqa
 

-
-
-
 
 
 
 

atype
data source
children stories mc (4)
synthetic
news articles
nyu-rgbd

reading comprehension
reasoning for toy tasks
information abstraction
visual: counts, colors, objects
visual: scene, objects, person, ... coco+prompts fitb/mc (4)
visual understanding
coco+abstract open/mc (18)

-
-
-
-
-
-
  text+visual story comprehension movie stories

word
word
word/list

mc (5)

#q aw
3.40
1.0
1*
1.15
2.59
1.24
5.29

2,640
20  2,000
1,000,000*
12,468
2  75,208*
764,163
14,944

table 2: a comparison of various qa datasets. first three columns depict the modality in which the story is presented. atype: answer
type; aw: average # of words in answer(s); mc (n): multiple choice with n answers; fitb:    ll in the blanks; *estimated information.

q and correct a. our annotators were    rst asked to se-
lect a movie from a large list, and were shown its plot syn-
opsis one paragraph at a time. for each paragraph, the an-
notator had the freedom of forming any number and type of
questions. each annotator was asked to provide the correct
answer, and was additionally required to mark a minimal
set of sentences within the plot synopsis paragraph that can
be used to both frame the question and answer it. this was
treated as ground-truth for localizing the qa in the plot.

in our instructions, we asked the annotators to provide
context to each question, such that a human taking the quiz
should be able to answer it by watching the movie alone
(without having access to the synopsis). the purpose of
this was to ensure questions that are localizable in the video
and story as opposed to generic questions such as    what are
they talking?   . we trained our annotators for about one to
two hours and gave them the option to re-visit and correct
their data. the annotators were paid by the hour, a strat-
egy that allowed us to collect more thoughtful and complex
qas, rather than short questions and single-word answers.

multiple answer choices.

in the second step of data
collection, we collected multiple-choice answers for each
question. our annotators were shown a paragraph and a
question at a time, but not the correct answer. they were
then asked to answer the question correctly as well as pro-
vide 4 wrong answers. these answers were either deceiving
facts from the same paragraph or common-sense answers.
the annotator was also allowed to re-formulate or correct
the question. we used this to sanity check all the questions
received in the    rst step. all qas from the    val    and    test   
set underwent another round of clean up.

time-stamp to video. we further asked in-house anno-
tators to align each sentence in the plot synopsis to the video
by marking the beginning and end (in seconds) of the video
that the sentence describes. long and complicated plot
sentences were often aligned to multiple, non-consecutive
video clips. annotation took roughly 2 hours per movie.
since we have each qa aligned to a sentence(s) in the plot
synopsis, the video to plot alignment links qas with video
clips. we provide these clips as part of our benchmark.

figure 4: stats about movieqa questions based on answer types.
note how questions beginning with the same word may cover a
variety of answer types: causality: what happens ... ?; action:
what did x do? person name: what is the killer   s name?; etc.
3.2. dataset statistics

in the following, we present some statistics of our
movieqa dataset. table 2 presents an overview of pop-
ular and recent question-answering datasets in the    eld.
most datasets (except mctest) use very short answers and
are thus limited to covering simpler visual/textual forms of
understanding. to the best of our knowledge, our dataset
not only has long sentence-like answers, but is also the    rst
to use videos in the form of movies.

multi-choice qa. we collected a total of 14,944 qas
from 408 movies. each question comes with one correct
and four deceiving answers. table 1 presents an overview
of the dataset along with information about the train/val/test
splits, which will be used to evaluate automatically trained
qa models. on average, our questions and answers are
fairly long with about 9 and 5 words respectively unlike
most other qa datasets. the video-based answering split
for our dataset, supports 140 movies for which we aligned
plot synopses with videos. note that the qa methods needs
to look at a long video clip (   200s) to answer the question.
fig. 3 presents the number of questions (bubble area)
split based on the    rst word of the question along with infor-
mation about number of words in the question and answer.
of particular interest are    why    questions that require ver-
bose answers, justi   ed by having the largest average num-
ber of words in the correct answer, and in contrast,    who   
questions with answers being short people names.

person name (who)18.0%reasoning (why)12.4%abstract (what)9.7%reason:action (how)8.5%person type (what)7.0%location (where)6.6%action (what)6.1%object/thing (what)5.6%yes/no (is, does)5.4%causality (what happens)objective (what)event/time (when)count (how many)emotion (how feel)other9.9%text type
plot
subtitle
script
dvs

# movies

# sent. / mov.

# words in sent.

408
408
199
60

35.2
1558.3
2876.8
636.3

20.3
6.2
8.3
9.3

table 3: statistics for the various text sources used for answering.

instead of the    rst word in the question, a peculiar way
to categorize qas is based on the answer type. we present
such an analysis in fig. 4. note how reasoning based ques-
tions (why, how, abstract) are a large part of our data. in
the bottom left quadrant we see typical question types that
can likely be answered using vision alone. note however,
that even the reasoning questions typically require vision, as
the question context provides a visual description of a scene
(e.g.,    why does john run after mary?   ).

text sources for answering. in table 3, we summarize
and present some statistics about different text sources used
for answering. note how plot synopses have a large num-
ber of words per sentence, hinting towards the richness and
complexity of the source.

4. multi-choice question-answering

we now investigate a number of intelligent baselines for
qa. we also study inherent biases in the data and try to
answer the quizzes based simply on answer characteristics
such as word length or within answer diversity.

formally, let s denote the story, which can take the form
of any of the available sources of information     e.g. plots,
subtitles, or video shots. each story s has a set of questions,
and we assume that the (automatic) student reads one ques-
tion qs at a time. let {as
j=1 be the set of multiple choice
answers (only one of which is correct) corresponding to qs,
with m = 5 in our dataset.

j }m

the general problem of multi-choice question answer-
ing can be formulated by a three-way scoring function
f (s, qs, as). this function evaluates the    quality    of the
answer given the story and the question. our goal is thus to
pick the best answer as for question qs that maximizes f:

j    = arg max

j=1...m

f (s, qs, as
j )

(1)

answering schemes are thus different functions f. we drop
the superscript (  )s for simplicity of notation.
4.1. the hasty student

we    rst consider f which ignores the story and attempts
to answer the question directly based on latent biases and
similarities. we call such a baseline as the    hasty student   
since he/she is not concerned to read/watch the actual story.
the extreme case of a hasty student is to try and an-
swer the question by only looking at the answers. here,

f (s, q, aj) = gh1(aj|a), where gh1(  ) captures some
properties of the answers.

answer length. we explore using the number of words
in the multiple choices to    nd the correct answer and ex-
plore biases in the dataset. as shown in table 1, correct an-
swers are slightly longer as it is often dif   cult to frame long
deceiving answers. we choose an answer by: (i) selecting
the longest answer; (ii) selecting the shortest answer; or (iii)
selecting the answer with the most different length.

within answer similarity/difference. while still look-
ing only at the answers, we compute a distance between
all answers based on their representations (discussed in
sec. 4.4). we then select our answer as either the most sim-
ilar or most distinct among all answers.

q and a similarity. we now consider a hasty student
that looks at both the question and answer, f (s, q, aj) =
gh2(q, aj). we compute similarity between the question
and each answer and pick the highest scoring answer.
4.2. the searching student

while the hasty student ignores the story, we consider a
student that tries to answer the question by trying to locate
a subset of the story s which is most similar to both the
question and the answer. the scoring function f is

f (s, q, aj) = gi (s, q) + gi (s, aj) .

(2)

a factorization of the question and answer similarity. we
propose two similarity functions: a simple windowed co-
sine similarity, and another using a neural architecture.

cosine similarity with a sliding window. we aim to
   nd the best window of h sentences (or shots) in the story
s that maximize similarity between the story and question,
and story and answer. we de   ne our similarity function:

f (s, q, aj) = max

l

gss(sk, q) + gss(sk, aj) ,

(3)

where sk denotes a sentence (or shot) from the story s. we
use gss(s, q) = x(s)t x(q) as a dot product between the
(normalized) representations of the two sentences (shots).
we discuss these representations in detail in sec. 4.4.

searching student with a convolutional brain (sscb).
instead of factoring f (s, q, aj) as a    xed (unweighted) sum
of two similarity functions gi (s, q) and gi (s, aj), we build
a neural network that learns such a function. assuming the
story s is of length n, e.g. n plot sentences or n video shots,
gi (s, q) and gi (s, aj) can be seen as two vectors of length
n whose k-th entry is gss(sk, q). we further combine all
[gi (s, aj)]j for the 5 answers into a n  5 matrix. the vector
gi (s, q) is replicated 5-times, and we stack the question and
answer matrix together to obtain a tensor of size n    5    2.
our neural similarity model is a convnet (id98), shown
in fig. 5, that takes the above tensor, and applies couple lay-
ers of h = 10, 1    1 convolutions to approximate a family

l+h(cid:88)

k=l

figure 5: our neural similarity architecture (see text for details).
of functions   (gi (s, q), gi (s, aj)). additionally, we incor-
porate a max pooling layer with kernel size 3 to allow for
scoring the similarity within a window in the story. the
last convolutional output is a tensor with shape ( n
3 , 5), and
we apply both mean and max pooling across the storyline,
add them, and make predictions using softmax. we train
our network using cross-id178 loss and the adam opti-
mizer [14].
4.3. memory network for complex qa

memory networks were originally proposed for text qa
and model complex three-way relationships between the
story, question and answer. we brie   y describe memn2n
proposed by [36] and suggest simple extensions to make it
suitable for our data and task.

the input of the original memn2n is a story and ques-
tion. the answering is restricted to single words and is
done by picking the most likely word from the vocabulary
v of 20-40 words. note that this is not directly applica-
ble to movieqa, as our data set does not have perform
vocabulary-based answering.
a question q is encoded as a vector u     rd using a word
embedding b     rd  |v|. here, d is the embedding dimen-
sion, and u is obtained by mean-pooling the representations
of words in the question. simultaneously, the sentences of
the story sl are encoded using id27s a and c to
provide two different sentence representations ml and cl, re-
spectively. ml, the representation of sentence l in the story,
is used in conjunction with u to produce an attention-like
mechanism which selects sentences in the story most simi-
lar to the question via a softmax function:

(4)

pl = softmax(ut ml) .

bedding cl, and the output o =(cid:80)

the id203 pl is used to weight the second sentence em-
l plcl is obtained by pool-
ing the weighted sentence representations across the story.
finally, a linear projection w     r|v|  d decodes the ques-
tion u and the story representation o to provide a soft score
for each vocabulary word

a = softmax(w (o + u)) .

(5)

the top scoring word   a is picked from a as the answer. the
free parameters to train are the embeddings b, a, c, w for
different words which can be shared across different layers.

due to its    xed set of output answers, the memn2n in
the current form is not designed for multi-choice answering
with open, natural language answers. we propose two key
modi   cations to make the network suitable for our task.

memn2n for natural language answers. to allow the
memn2n to rank multiple answers written in natural lan-
guage, we add an additional embedding layer f which maps
each multi-choice answer aj to a vector gj. note that f is
similar to embeddings b, a and c, but operates on answers
instead of the question or story. to predict the correct an-
swer, we compute the similarity between the answers g, the
question embedding u and the story representation o:

a = softmax((o + u)t g)

(6)

and pick the most probable answer as correct. in our general
qa formulation, this is equivalent to

f (s, q, aj) = gm 1(s, q, aj) + gm 2(q, aj),

(7)

where gm 1 attends to parts of the story using the question,
and a second function gm 2 directly considers similarities
between the question and the answer.

weight sharing and    xed id27s. the orig-
inal memn2n learns embeddings for each word based di-
rectly on the task of question-answering. however, to scale
this to large vocabulary data sets like ours, this requires un-
reasonable amounts of training data. for example, training
a model with a vocabulary size 14,000 (obtained just from
plot synopses) and d = 100 would entail learning 1.4m pa-
rameters for each embedding. to prevent over   tting, we
   rst share all id27s b, a, c, f of the memory
network. nevertheless, even one embedding is still a large
number of parameters.

we make the following crucial modi   cation that allows
us to use the memory network for our dataset. we drop b,
a, c, f and replace them by a    xed (pre-trained) word em-
bedding z     rd1  |v| obtained from the id97 model
and learn a shared linear projection layer t     rd2  d1 to
map all sentences (stories, questions and answers) into a
common space. here, d1 is the dimension of the id97
embedding, and d2 is the projection dimension. thus, the
new encodings are

u = t    zq; ml, cl = t    zsl; and gj = t    zaj.

(8)

answer prediction is performed as before in eq. 6.

we initialize t either using an identity matrix d1    d1
or using pca to lower the dimension from d1 = 300 to
d2 = 100. training is performed using stochastic gradient
descent with a batch size of 32.
4.4. representations for text and video

tf-idf is a popular and successful feature in informa-
in our case, we treat plots (or other forms

tion retrieval.

of text) from different movies as documents and compute a
weight for each word. we set all words to lower case, use
id30, and compute the vocabulary v which consists
of words w that appear more than    times in the documents.
we represent each sentence (or question or answer) in a bag-
of-words style with an tf-idf score for each word.

id97. a disadvantage of tf-idf is that it is un-
able to capture the similarities between words. we use the
skip-gram model proposed by [25] and train it on roughly
1200 movie plots to obtain domain-speci   c, 300 dimen-
sional id27s. a sentence is then represented
by mean-pooling its id27s. we normalize the
resulting vector to have unit norm.

skipthoughts. while the sentence representation
using mean pooled id97 discards word order,
skipthoughts [16] use a recurrent neural network to cap-
ture the underlying sentence semantics. we use the pre-
trained model by [16] to compute a 4800 dimensional sen-
tence representation.

video. to answer questions from the video, we learn an
embedding between a shot and a sentence, which maps the
two modalities in a common space. in this joint space, one
can score the similarity between the two modalities via a
simple dot product. this allows us to apply all of our pro-
posed question-answering techniques in their original form.
to learn the joint embedding we follow [49] which ex-
tends [15] to video. speci   cally, we use the googlenet ar-
chitecture [37] as well as hybrid-id98 [48] to extract frame-
wise features, and mean-pool the representations over all
frames in a shot. the embedding is a linear mapping of
the shot representation and an lstm on id27s
on the sentence side, trained using the ranking loss on the
moviedescription dataset [32] as in [49].

5. qa evaluation

we present results for question-answering with the pro-
posed methods on our movieqa dataset. we study how
various sources of information in   uence the performance,
and how different levels of complexity encoded in f affects
the quality of automatic qa.

protocol. note that we have two primary tasks for eval-
uation. (i) text-based: the story takes the form of various
texts     plots, subtitles, scripts, dvs; and (ii) video-based:
story is the video, and with/without subtitles.

dataset structure. the dataset is divided into three dis-
joint splits: train, val, and test, based on unique movie titles
in each split. the splits are optimized to preserve the ratios
between #movies, #qas, and all the story sources at 10:2:3
(e.g. about 10k, 2k, and 3k qas). stats for each split are
presented in table 1. the train set is to be used for training
automatic models and tuning any hyperparameters. the val
set should not be touched during training, and may be used
to report results for several models. the test set is a held-

answer length

within answers

question-answer

longest
25.33
tf-idf
21.71
19.92
tf-idf
12.97

shortest
14.56
skipt
28.14
14.91
skipt
19.25

different
20.38
w2v
25.43
15.12
w2v
24.97

similar
distinct

similar

table 4: the question-answering accuracy for the    hasty student   
who tries to answer questions without looking at the story.
out set, and is evaluated on our movieqa server. for this
paper, all results are presented on the val set.

metrics. multiple choice qa leads to a simple and ob-
jective evaluation. we measure accuracy, the number of
correctly answered qas over the total count.
5.1. the hasty student

the    rst part of table 4 shows the performance of three
models when trying to answer questions based on the an-
swer length. notably, always choosing the longest answer
performs better (25.3%) than random (20%). the second
part of table 4 presents results when using within-answer
feature-based similarity. we see that the answer most simi-
lar to others is likely to be correct when the representations
are generic and try to capture the semantics of the sentence
(id97, skipthoughts). the most distinct answers per-
forms worse than random on all features. in the last section
of table 4 we see that computing feature-based similarity
between questions and answers is insuf   cient for answer-
ing. especially, tf-idf performs worse than random since
words in the question rarely appear in the answer.

hasty turker. to analyze the deceiving nature of our
multi-choice qas, we tested humans (via amt) on a sub-
set of 200 qas. the turkers were not shown the story in any
form and were asked to pick the best possible answer given
the question and a set of options. we asked each question to
10 turkers, and rewarded each with a bonus if their answer
agreed with the majority. we observe that without access
to the story, humans obtain an accuracy of 27.6%. we sus-
pect that the bias is due to the fact that some of the qas
reveal the movie (e.g.,    darth vader   ) and the turker may
have seen this movie. removing such questions, and re-
evaluating on a subset of 135 qas, lowers the performance
to 24.7%. this shows the genuine dif   culty of our qas.
5.2. searching student

cosine similarity with window. the    rst section of
table 5 presents results for the proposed cosine similarity
using different representations and text stories. using the
plots to answer questions outperforms other sources (sub-
titles, scripts, and dvs) as the qas were collected using
plots and annotators often reproduce words from the plot.

we show the results of using id97 or skipthought
in the following rows of table 5.

representations

method
cosine tfidf
cosine skipthought
cosine id97
sscb tfidf
sscb skipthought
sscb id97
sscb fusion
memn2n (w2v, linproj)

plot dvs
24.5
47.6
19.9
31.0
26.6
46.4
48.5
24.5
24.5
28.3
24.8
45.1
56.7
24.8
33.0
40.6

subtitle

24.5
21.3
24.5
27.6
20.8
24.8
27.7
38.0

script
24.6
21.2
23.4
26.1
21.0
25.0
28.7
42.3

table 5: accuracy for text-based qa. top: results for the search-
ing student with cosine similarity; middle: convnet sscb; and
bottom: the modi   ed memory network.

skipthoughts perform much worse than both tf-idf
and id97 which are closer. we suspect that while
skipthoughts are good at capturing the overall semantics
of a sentence, proper nouns     names, places     are often
hard to distinguish. fig. 6 presents a accuracy breakup
based on the    rst word of the questions. tf-idf and
id97 perform considerably well, however, we see a
larger difference between the two for    who    and    why   
questions.    who    questions require distinguishing between
names, and    why    answers are typically long, and mean
pooling destroys semantics.
in fact id97 performs
best on    where    questions that may use synonyms to in-
dicate places. skipthoughts perform best on    why    ques-
tions where sentence semantics help improve answering.

sscb. the middle rows of table 5 show the result of
our neural similarity model. here, we present additional re-
sults combining all text representations (sscb fusion) via
our id98. we split the train set into 90% train / 10% dev,
such that all questions and answers of the same movie are in
the same split, train our model on train and monitor perfor-
mance on dev. both val and test sets are held out. during
training, we also create several model replicas and pick the
ones with the best validation performance.

table 5 shows that the neural model outperforms the sim-
ple cosine similarity on most tasks, while the fusion method
achieves the highest performance when using plot synopses
as the story.
ignoring the case of plots, the accuracy is
capped at about 30% for most modalities showing the dif   -
culty of our dataset.
5.3. memory network

the original memn2n which trains the word embed-
dings along with the answering modules over   ts heavily
on our dataset leading to near random performance on val
(   20%). however, our modi   cations help in restraining
the learning process. table 5 (bottom) presents results for
memn2n with id97 initialization and a linear projec-
tion layer. using plot synopses, we see a performance closer
to sscb with id97 features. however, in the case of
longer stories, the attention mechanism in the network is

method
sscb all clips
memn2n all clips

video
21.6
23.1

subtitle video+subtitle

22.3
38.0

21.9
34.2

table 6: accuracy for video-based qa and late fusion of subtitle
and video scores.

figure 6: accuracy for different feature representations of plot
sentences with respect to the    rst word of the question.

able to sift through thousands of story sentences and per-
forms well on dvs, subtitles and scripts. this shows that
complex three-way scoring functions are needed to tackle
such qa sources. in terms of story sources, the memn2n
performs best with scripts which contain the most informa-
tion (descriptions, dialogs and speaker information).
5.4. video baselines

we evaluate sscb and memn2n in a setting where the
automatic models answer questions by    watching    all the
video clips that are provided for that movie. here, the story
descriptors are shot embeddings.

the results are presented in table 6. we see that learning
to answer questions using video is still a hard problem with
performance close to random. as visual information alone
is insuf   cient, we also perform and experiment combining
video and dialog (subtitles) through late fusion. we train the
sscb model with the visual-text embedding for subtitles
and see that it yields poor performance (22.3%) compared
to the fusion of all text features (27.7%). for the memory
network, we answer subtitles as before using id97.
6. conclusion

we introduced the movieqa data set which aims to
evaluate automatic story comprehension from both video
and text. our dataset is unique in that it contains several
sources of information     video clips, subtitles, scripts, plots
and dvs. we provided several intelligent baselines and ex-
tended existing qa techniques to analyze the dif   culty of
our task. our benchmark with an evaluation server is online
at http://movieqa.cs.toronto.edu.
acknowledgment. we thank the upwork annotators, lea jen-
sterle, marko boben, and so  ca fidler for data collection, and relu
patrascu for infrastructure support. mt and rs are supported by
dfg contract no. sti-598/2-1, and the work was carried out dur-
ing mt   s visit to u. of t. on a khys research travel grant.

whatwhowhyhowwhere203040506070accuracytf-idfid97skipthoughtreferences
[1] s. antol, a. agrawal, j. lu, m. mitchell, d. batra, c. l.
zitnick, and d. parikh. vqa: visual id53.
in iccv, 2015. 1, 3, 4

[2] m. baeuml, m. tapaswi, and r. stiefelhagen.

semi-
supervised learning with constraints for person identi   ca-
tion in multimedia data. in cvpr, 2013. 2

[3] a. barbu, a. bridge, z. burchill, d. coroian, s. dickin-
son, s. fidler, a. michaux, s. mussman, s. narayanaswamy,
d. salvi, l. schmidt, j. shangguan, j. siskind, j. waggoner,
s. wang, j. wei, y. yin, and z. zhang. video-in-sentences
out. in uai, 2012. 2

[4] p. bojanowski, f. bach, i. laptev, j. ponce, c. schmid, and
j. sivic. finding actors and actions in movies. iccv, pages
2280   2287, 2013. 2

[5] d. l. chen and w. b. dolan. collecting highly parallel data

for paraphrase evaluation. in acl, 2011. 3

[6] x. chen and c. l. zitnick.

sual representation for image id134.
arxiv:1411.5654, 2014. 2

learning a recurrent vi-
in

[7] t. cour, c.

jordan, e. miltsakaki,

and b. taskar.
movie/script: alignment and parsing of video and text
transcription. in eccv, 2008. 2

[8] p. das, c. xu, r. f. doell, and j. j. corso. a thousand
frames in just a few words: lingual description of videos
through latent topics and sparse object stitching. cvpr,
2013. 2

[9] p. das, c. xu, r. f. doell, and j. j. corso. a thousand frames
in just a few words: lingual description of videos through
latent topics and sparse object stitching. in cvpr, 2013. 3

[10] j. donahue, l. a. hendricks, s. guadarrama, m. rohrbach,
s. venugopalan, k. saenko, and t. darrell. long-term re-
current convolutional networks for visual recognition and
description. in arxiv:1411.4389, 2014. 2

[11] a. farhadi, m. hejrati, m. sadeghi, p. young, c. rashtchian,
j. hockenmaier, and d. forsyth. every picture tells a story:
generating sentences for images. in eccv, 2010. 2

[12] k. m. hermann, t. ko  cisky, e. grefenstette, l. espeholt,
w. kay, m. suleyman, and p. blunsom. teaching machines
to read and comprehend. in arxiv:1506.03340, 2015. 2, 3,
4

[13] a. karpathy and l. fei-fei. deep visual-semantic align-
ments for generating image descriptions. in cvpr, 2015.
2

[14] d. kingma and j. ba. adam: a method for stochastic opti-

mization. arxiv:1412.6980, 2014. 6

[15] r. kiros, r. salakhutdinov, and r. s. zemel. unifying
visual-semantic embeddings with multimodal neural lan-
guage models. tacl, 2015. 2, 7

[16] r. kiros, y. zhu, r. salakhutdinov, r. zemel, a. torralba,
r. urtasun, and s. fidler. skip-thought vectors. nips,
2015. 7

[17] c. kong, d. lin, m. bansal, r. urtasun, and s. fidler. what
are you talking about? text-to-image coreference. in cvpr,
2014. 3

[18] n. krishnamoorthy, g. malkarnenkar, r. j. mooney,
k. saenko, and s. guadarrama.
generating natural-
language video descriptions using text-mined knowl-
edge. in aaai, july 2013. 2

[19] g. kulkarni, v. premraj, s. dhar, s. li, y. choi, a. berg, and
t. berg. baby talk: understanding and generating simple
image descriptions. in cvpr, 2011. 2

[20] p. liang, m. jordan, and d. klein. learning dependency-
based id152. in computational linguis-
tics, 2013. 2

[21] d. lin, s. fidler, c. kong, and r. urtasun. visual seman-
tic search: retrieving videos via complex textual queries.
cvpr, 2014. 2

[22] t.-y. lin, m. maire, s. belongie, j. hays, p. perona, d. ra-
manan, p. doll  ar, and c. l. zitnick. microsoft coco: com-
mon objects in context. in eccv. 2014. 2, 3

[23] m. malinowski and m. fritz. a multi-world approach to
id53 about real-world scenes based on un-
certain input. in nips, 2014. 1, 2, 3, 4

[24] m. malinowski, m. rohrbach, and m. fritz. ask your neu-
rons: a neural-based approach to answering questions
about images. in iccv, 2015. 2

[25] t. mikolov, k. chen, g. corrado, and j. dean. ef   cient
estimation of word representations in vector space. arxiv
preprint arxiv:1301.3781, 2013. 7

[26] v. ordonez, g. kulkarni, and t. berg. im2text: describing
in nips,

images using 1 million captioned photographs.
2011. 2

[27] h. pirsiavash, c. vondrick, and a. torralba.

inferring the

why in images. arxiv.org, jun 2014. 1

[28] v. ramanathan, a. joulin, p. liang, and l. fei-fei. link-
ing people in videos with    their    names using coreference
resolution. in eccv. 2014. 2

[29] v. ramanathan, p. liang, and l. fei-fei. video event un-
derstanding using natural language descriptions. in iccv,
2013. 2

[30] m. ren, r. kiros, and r. zemel. exploring models and data
for image id53. arxiv:1505.02074, 2015. 2
[31] m. richardson, c. j. burges, and e. renshaw. mctest: a
challenge dataset for the open-domain machine comprehen-
sion of text. in emnlp, 2013. 3, 4

[32] a. rohrbach, m. rohrbach, n. tandon, and b. schiele. a

dataset for movie description. in cvpr, 2015. 1, 2, 3, 7

[33] m. rohrbach, w. qiu, i. titov, s. thater, m. pinkal, and
b. schiele. translating video content to natural language
descriptions. in iccv, 2013. 2

[34] p. sankar, c. v. jawahar, and a. zisserman. subtitle-free

movie to script alignment. in bmvc, 2009. 2

[35] j. sivic, m. everingham, and a. zisserman.    who are you?   
- learning person speci   c classi   ers from video. cvpr,
pages 1145   1152, 2009. 2

[36] s. sukhbaatar, a. szlam, j. weston, and r. fergus. end-
to-end memory networks. in arxiv:1503.08895, 2015. 2,
6

[37] c. szegedy, w. liu, y. jia, p. sermanet, s. reed,
d. anguelov, d. erhan, v. vanhoucke, and a. rabinovich.
going deeper with convolutions. arxiv:1409.4842, 2014. 7

[38] m. tapaswi, m. bauml, and r. stiefelhagen. book2movie:
aligning video scenes with book chapters. in cvpr, 2015.
2

[39] m. tapaswi, m. b  auml, and r. stiefelhagen. aligning plot
synopses to videos for story-based retrieval. ijmir, 4:3   
16, 2015. 2

[40] r. vedantam, x. lin, t. batra, c. l. zitnick, and d. parikh.
in

learning common sense through visual abstraction.
iccv, 2015. 2

[41] s. venugopalan, h. xu, j. donahue, m. rohrbach, r. j.
mooney, and k. saenko. translating videos to natural
language using deep recurrent neural networks. corr
abs/1312.6229, cs.cv, 2014. 2

[42] o. vinyals, a. toshev, s. bengio, and d. erhan. show
in

image caption generator.

and tell: a neural
arxiv:1411.4555, 2014. 2

[43] h. wang, m. bansal, k. gimpel, and d. mcallester. ma-
chine comprehension with syntax, frames, and semantics.
in acl, 2015. 2

[44] j. weston, a. bordes, s. chopra, and t. mikolov. towards
ai-complete id53: a set of prerequisite toy
tasks. in arxiv:1502.05698, 2014. 4

[45] y. yang, c. l. teo, h. daum  e, iii, and y. aloimonos.
corpus-guided sentence generation of natural images. in
emnlp, pages 444   454, 2011. 2

[46] p. young, a. lai, m. hodosh, and j. hockenmaier. from im-
age descriptions to visual denotations: new similarity met-
rics for semantic id136 over event descriptions. in tacl,
2014. 2, 3

[47] l. yu, e. park, a. c. berg, and t. l. berg. visual madlibs:
fill in the blank image generation and id53.
in iccv, 2015. 1, 3, 4

[48] b. zhou, a. lapedriza, j. xiao, a. torralba, and a. oliva.
learning deep features for scene recognition using places
database. in nips, 2014. 7

[49] y. zhu, r. kiros, r. zemel, r. salakhutdinov, r. urtasun,
a. torralba, and s. fidler. aligning books and movies: to-
wards story-like visual explanations by watching movies
and reading books. in iccv, 2015. 1, 2, 7

[50] c. zitnick, r. vedantam, and d. parikh. adopting abstract
images for semantic scene understanding. pami, pp, 2014.
2, 3

