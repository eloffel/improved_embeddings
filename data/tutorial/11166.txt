multilingual id36 using compositional universal schema

patrick verga, david belanger, emma strubell, benjamin roth & andrew mccallum

college of information and computer sciences

university of massachusetts amherst

6
1
0
2

 
r
a

m
3

 

 
 
]
l
c
.
s
c
[
 
 

2
v
6
9
3
6
0

.

1
1
5
1
:
v
i
x
r
a

{pat, belanger, strubell, beroth, mccallum}@cs.umass.edu

abstract

universal schema builds a knowledge base (kb) of
entities and relations by jointly embedding all rela-
tion types from input kbs as well as textual patterns
expressing relations from raw text. in most previous
applications of universal schema, each textual pat-
tern is represented as a single embedding, prevent-
ing generalization to unseen patterns. recent work
employs a neural network to capture patterns    com-
positional semantics, providing generalization to all
possible input text.
in response, this paper intro-
duces signi   cant further improvements to the cov-
erage and    exibility of universal schema relation ex-
traction: predictions for entities unseen in training
and multilingual id21 to domains with
no annotation. we evaluate our model through ex-
tensive experiments on the english and spanish tac
kbp benchmark, outperforming the top system from
tac 2013 slot-   lling using no handwritten patterns
or additional annotation. we also consider a multi-
lingual setting in which english training data entities
overlap with the seed kb, but spanish text does not.
despite having no annotation for spanish data, we
train an accurate predictor, with additional improve-
ments obtained by tying id27s across
languages. furthermore, we    nd that multilingual
training improves english id36 accu-
racy. our approach is thus suited to broad-coverage
automated knowledge base construction in a variety
of languages and domains.

1

introduction

the goal of automatic knowledge base construction
(akbc) is to build a structured knowledge base (kb)
of facts using a noisy corpus of raw text evidence, and
perhaps an initial seed kb to be augmented (carlson et
al., 2010; suchanek et al., 2007; bollacker et al., 2008).
akbc supports downstream reasoning at a high level
about extracted entities and their relations, and thus has
broad-reaching applications to a variety of domains.

one challenge in akbc is aligning knowledge from
a structured kb with a text corpus in order to perform
supervised learning through distant supervision. univer-
sal schema (riedel et al., 2013) along with its exten-
sions (yao et al., 2013; gardner et al., 2014; neelakantan
et al., 2015; rocktaschel et al., 2015), avoids alignment
by jointly embedding kb relations, entities, and surface
text patterns. this propagates information between kb
annotation and corresponding textual evidence.

the above applications of universal schema express
each text relation as a distinct item to be embedded. this
harms its ability to generalize to inputs not precisely seen
at training time. recently, toutanova et al. (2015) ad-
dressed this issue by embedding text patterns using a deep
sentence encoder, which captures the compositional se-
mantics of textual relations and allows for prediction on
inputs never seen before.

this paper further expands the coverage abilities of
universal schema id36 by introducing tech-
niques for forming predictions for new entities unseen in
training and even for new domains with no associated an-
notation. in the extreme example of id20
to a completely new language, we may have limited lin-
guistic resources or labeled data such as treebanks, and
only rarely a kb with adequate coverage. our method
performs multilingual id21, providing a pre-
dictive model for a language with no coverage in an exist-
ing kb, by leveraging common representations for shared
entities across text corpora. as depicted in figure 1, we
simply require that one language have an available kb
of seed facts. we can further improve our models by ty-
ing a small set of id27s across languages us-
ing only simple knowledge about word-level translations,
learning to embed semantically similar textual patterns
from different languages into the same latent space.

in extensive experiments on the tac knowledge base
population (kbp) slot-   lling benchmark we outperform
the top 2013 system with an f1 score of 40.7 and per-
form id36 in spanish with no labeled data
or direct overlap between the spanish training corpus and

the training kb, demonstrating that our approach is well-
suited for broad-coverage akbc in low-resource lan-
guages and domains.
interestingly, joint training with
spanish improves english accuracy.

english

low-resource

in kb

not in kb

figure 1: splitting the entities in a multilingual akbc
training set into parts. we only require that entities in the
two corpora overlap. remarkably, we can train a model
for the low-resource language even if entities in the low-
resource language do not occur in the kb.

2 background
akbc extracts unary attributes of the form (subject, at-
tribute), typed binary relations of the form (subject, rela-
tion, object), or higher-order relations. we refer to sub-
jects and objects as entities. this work focuses solely
on extracting binary relations, though many of our tech-
niques generalize naturally to unary prediction. gener-
ally, for example in freebase (bollacker et al., 2008),
higher-order relations are expressed in terms of collec-
tions of binary relations.

we now describe prior work on approaches to akbc.
they all aim to predict (s, r, o) triples, but differ in terms
of: (1) input data leveraged, (2) types of annotation re-
quired, (3) de   nition of relation label schema, and (4)
whether they are capable of predicting relations for en-
tities unseen in the training data. note that all of these
methods require pre-processing to detect entities, which
may result in additional kb construction errors.

2.1 id36 as link prediction
a knowledge base is naturally described as a graph,
in which entities are nodes and relations are labeled
edges (suchanek et al., 2007; bollacker et al., 2008).
in the case of id13 completion, the task is
akin to link prediction, assuming an initial set of (s, r,
o) triples. see nickel et al. (2015) for a review. no
accompanying text data is necessary, since links can be
predicted using properties of the graph, such as transitiv-
ity. in order to generalize well, prediction is often posed
as low-rank matrix or tensor factorization. a variety of
model variants have been suggested, where the proba-
bility of a given edge existing depends on a multi-linear
form (nickel et al., 2011; garc    a-dur  an et al., 2015; yang

et al., 2015; bordes et al., 2013; wang et al., 2014; lin
et al., 2015), or non-linear interactions between s, r, and
o (socher et al., 2013). other approaches model the com-
positionality of multi-hop paths, typically for question
answering (bordes et al., 2014; gu et al., 2015; nee-
lakantan et al., 2015).

2.2 id36 as sentence classi   cation
here, the training data consist of (1) a text corpus, and
(2) a kb of seed facts with provenance, i.e. supporting
evidence, in the corpus. given individual an individual
sentence, and pre-speci   ed entities, a classi   er predicts
whether the sentence expresses a relation from a target
schema. to train such a classi   er, kb facts need to be
aligned with supporting evidence in the text, but this is
often challenging. for example, not all sentences con-
taining barack and michelle obama state that they are
married. a variety of one-shot and iterative methods have
addressed the alignment problem (bunescu and mooney,
2007; mintz et al., 2009; riedel et al., 2010; yao et al.,
2010; hoffmann et al., 2011; surdeanu et al., 2012; min
et al., 2013; zeng et al., 2015). an additional degree
of freedom in these approaches is whether they classify
individual sentences or predicting at the corpus level by
aggregating information from all sentences containing a
given pair of entities before prediction. the former ap-
proach is often preferable in practice, due to the simplic-
ity of independently classifying individual sentences and
the ease of associating each prediction with a provenance.
prior work has applied deep learning to small-scale rela-
tion extraction problems, where functional relationships
are detected between common nouns (li et al., 2015; dos
santos et al., 2015). xu et al. (2015) apply an lstm
to a parse path, while zeng et al. (2015) use a id98 on
the raw text, with a special temporal pooling operation to
separately embed the text around each entity.

2.3 open-domain id36
in the previous two approaches, prediction is carried
out with respect to a    xed schema r of possible rela-
tions r. this may overlook salient relations that are ex-
pressed in the text but do not occur in the schema.
in
response, open-domain information extraction (openie)
lets the text speak for itself: r contains all possible pat-
terns of text occurring between entities s and o (banko et
al., 2007; etzioni et al., 2008; yates and etzioni, 2007).
these are obtained by    ltering and normalizing the raw
text. the approach offers impressive coverage, avoids
issues of distant supervision, and provides a useful ex-
ploratory tool. on the other hand, openie predictions
are dif   cult to use in downstream tasks that expect infor-
mation from a    xed schema.

table 1 provides examples of openie patterns. the ex-
amples in row two and three illustrate relational contexts

for which similarity is dif   cult to be captured by an ope-
nie approach because of their syntactically complex con-
structions. this motivates the technique in section 3.2,
which uses a deep architecture applied to raw tokens, in-
stead of rigid rules for normalizing text to obtain patterns.

*

sister

arg1 * moved with
* family to arg2

openie pattern
arg1   s
arg2

sentence (context tokens italicized)
khan    s younger sister, annapurna
devi, who later married shankar, de-
veloped into an equally accomplished
master of the surbahar, but custom pre-
vented her from performing in public.
a professor emeritus at yale, mandel-
brot was born in poland but as a child
moved with his family to paris where
he was educated.
kissel was born in provo, utah, but
her family also lived in reno.
table 1: examples of sentences expressing relations.
context tokens (italicized) consist of the text occurring
between entities (bold) in a sentence. openie patterns are
obtained by normalizing the context tokens using hand-
coded rules. the top example expresses the per:siblings
relation and the bottom two examples both express the
per:cities of residence relation.

arg1 * lived in
arg2

2.4 universal schema
when applying universal schema (riedel et al., 2013)
(uschema) to id36, we combine the ope-
nie and link-prediction perspectives. by jointly mod-
eling both openie patterns and the elements of a target
schema, the method captures broader relational structure
than multi-class classi   cation approaches that just model
the target schema. furthermore, the method avoids the
distant supervision alignment dif   culties of section 2.2.
riedel et al. (2013) augment a id13 from
a seed kb with additional edges corresponding to ope-
nie patterns observed in the corpus. even if the user does
not seek to predict these new edges, a joint model over all
edges can exploit regularities of the openie edges to im-
prove modeling of the labels from the target schema.

the data still consist of (s, r, o) triples, which can be
predicted using link-prediction techniques such as low-
rank factorization. riedel et al. (2013) explore a variety
of approximations to the 3-mode (s, r, o) tensor. one
such probabilistic model is:

p ((s, r, o)) =   (cid:0)u(cid:62)

s,ovr

(cid:1) ,

(1)

where   () is a sigmoid function, us,o is an embedding
of the entity pair (s, o), and vr is an embedding of the
relation r, which may be an openie pattern or a rela-
tion from the target schema. all of the exposition and re-
sults in this paper use this factorization, though many of
the techniques we present later could be applied easily to

the other factorizations described in riedel et al. (2013).
note that learning unique embeddings for openie rela-
tions does not guarantee that similar patterns, such as the
   nal two in table 1, will be embedded similarly.

as with most of the techniques in section 2.1, the data
only consist of positive examples of edges. the absence
of an annotated edge does not imply that the edge is false.
in fact, we seek to predict some of these missing edges as
true. riedel et al. (2013) employ the bayesian person-
alized ranking (bpr) approach of rendle et al. (2009),
which does not explicitly model unobserved edges as
negative, but instead seeks to rank the id203 of ob-
served triples above unobserved triples.

recently, toutanova et al. (2015) extended uschema
to not learn individual pattern embeddings vr, but instead
to embed text patterns using a deep architecture applied
to word tokens. this shares statistical strength between
openie patterns with similar words. we leverage this ap-
proach in section 3.2. additional work has modeled the
regularities of multi-hop paths through id13
augmented with text patterns (lao et al., 2011; lao et al.,
2012; gardner et al., 2014; neelakantan et al., 2015).

2.5 multilingual embeddings
much work has been done on id73 embed-
dings. most of this work uses aligned sentences from
the europarl dataset (koehn, 2005) to align word embed-
dings across languages (gouws et al., 2015; luong et al.,
2015; hermann and blunsom, 2014). others (mikolov
et al., 2013; faruqui et al., 2014) align separate single-
language embedding models using a word-level dictio-
nary. mikolov et al. (2013) use translation pairs to learn
a linear transform from one embedding space to another.
however, very little work exists on multilingual re-
lation extraction. faruqui and kumar (2015) perform
multilingual openie id36 by projecting all
languages to english using google translate. however,
as explained in section 2.3 the openie paradigm is not
amenable to prediction within a    xed schema. further,
their approach does not generalize to low-resource lan-
guages where translation is unavailable     while we use
translation dictionaries to improve our results, our experi-
ments demonstrate that our method is effective even with-
out this resource.

3 methods
3.1 universal schema as sentence classi   er
similar to many link prediction approaches, (riedel et al.,
2013) perform transductive learning, where a model is
learned jointly over train and test data. predictions are
made by using the model to identify edges that were un-
observed in the test data but likely to be true. the ap-
proach is vulnerable to the cold start problem in collab-

figure 2: universal schema jointly embeds kb and textual relations from spanish and english, learning dense repre-
sentations for entity pairs and relations using id105. cells with a 1 indicate triples observed during train-
ing (left). the bold score represents a test-time prediction by the model (right). using transitivity through kb/english
overlap and english/spanish overlap, our model can predict that a text pattern in spanish evidences a kb relation
despite no overlap between spanish/kb entity pairs. at train time we use bpr loss to maximize the inner product of
entity pairs with kb relations and text patterns encoded using a bidirectional lstm. at test time we score compati-
bility between embedded kb relations and encoded textual patterns using cosine similarity. in our spanish model we
treat embeddings for a small set of english/spanish translation pairs as a single word, e.g. casado and married.

orative    ltering (schein et al., 2002): it is unclear how
to form predictions for unseen entity pairs, without re-
factorizing the entire matrix or applying heuristics.

in response,

this paper re-purposes uschema as a
means to train a sentence-level relation classi   er, like
those in section 2.2. this allows us to avoid errors from
aligning distant supervision to the corpus, but is more de-
ployable for real world applications. it also provides op-
portunities in section 3.4 to improve multilingual akbc.
we produce predictions using a very simple approach:
(1) scan the corpus and extract a large quantity of
triplets (s, rtext, o), where rtext
is an openie pattern.
for each triplet, if the similarity between the embed-
ding of rtext and the embedding of a target relation
rschema is above some threshold, we predict the triplet
(s, rschema, o), and its provenance is the input sentence
containing (s, rtext, o). we refer to this technique as pat-
tern scoring. in our experiments, we use the cosine dis-
tance between the vectors (figure 2).
in section 7.3,
we discuss details for how to make this distance well-
de   ned.

3.2 using a compositional sentence encoder to

predict unseen text patterns

the pattern scoring approach is subject to an additional
cold start problem: input data may contain patterns un-
seen in training. this section describes a method for us-

ing uschema to train a relation classi   er that can take
arbitrary context tokens (section 2.3) as input.

fortunately, the cold start problem for context tokens is
more benign than that of entities since we can exploit sta-
tistical regularities of text: similar sequences of context
tokens should be embedded similarly. therefore, follow-
ing toutanova et al. (2015), we embed raw context tokens
compositionally using a deep architecture. unlike riedel
et al. (2013), this requires no manual rules to map text to
openie patterns and can embed any possible input string.
the modi   ed uschema likelihood is:

p ((s, r, o)) =   (cid:0)u(cid:62)

s,oencoder(r)(cid:1) .

(2)

here, if r is raw text, then encoder(r) is parameterized
by a deep architecture. if r is from the target schema,
encoder(r) is a produced by a lookup table (as in tradi-
tional uschema). though such an encoder increases the
computational cost of test-time prediction over straight-
forward pattern matching, evaluating a deep architecture
can be done in large batches in parallel on a gpu.

both convolutional networks (id98s) and recurrent
networks (id56s) are reasonable encoder architectures,
and we consider both in our experiments. id98s have
been useful
in a variety of nlp applications (col-
lobert et al., 2011; kalchbrenner et al., 2014; kim,
2014). unlike toutanova et al. (2015), we also consider
id56s, speci   cally long-short term memory networks

per:spouse...per:born_inarg1    s wife arg2...arg1 was born in arg 2...arg1 es la esposa de arg2...arg1 naci   en arg2...englishspanishbarack obama/ michelle obamamar  a m  nera/juan m santosbarack obama/ hawaiimar  a m  nera/colombiabernie sanders/ jane o'meara......1111.9311...1bidirectional lstmarg1      est      casado/married   con        arg2max poolinput : [per:spouse] [mar  a m  nera est   casado con juan m santos]per:spousecosine similarity.93(lstms) (hochreiter and schmidhuber, 1997). lstms
have proven successful in a variety of tasks requiring
encoding sentences as vectors (sutskever et al., 2014;
vinyals et al., 2014). in our experiments, lstms out-
perform id98s.

there are two key differences between our sentence
encoder and that of toutanova et al. (2015). first, we
use the encoder at test time, since we process the context
tokens for held-out data. on the other hand, toutanova
et al. (2015) adopt the transductive approach where the
encoder is only used to help train better representations
for the relations in the target schema; it is ignored when
forming predictions. second, we apply the encoder to the
raw text between entities, while toutanova et al. (2015)
   rst perform syntactic id33 on the data
and then apply an encoder to the path between the two
entities in the parse tree. we avoid parsing, since we seek
to perform multilingual akbc, and many languages lack
linguistic resources such as treebanks. even parsing non-
newswire english text, such as tweets, is extremely chal-
lenging.

3.3 modeling frequent text patterns
despite the coverage advantages of using a deep sen-
tence encoder, separately embedding each openie pat-
tern, as in riedel et al. (2013), has key advantages. in
practice, we have found that many high-precision pat-
terns occur quite frequently. for these, there is suf   -
cient data to model them with independent embeddings
per pattern, which imposes minimal inductive bias on the
relationship between patterns. furthermore, some dis-
criminative phrases are idiomatic, i.e..
their meaning is
not constructed compositionally from their constituents.
for these, a sentence encoder may be inappropriate.

therefore, pattern embeddings and deep token-based
encoders have very different strengths and weaknesses.
one values speci   city, and models the head of the text
distribution well, while the other has high coverage and
captures the tail. in experimental results, we demonstrate
that an ensemble of both models performs substantially
better than either in isolation.

3.4 multilingual id36 with zero

annotation

the models described in previous two sections provide
broad-coverage id36 that can generalize to
all possible input entities and text patterns, while avoid-
ing error-prone alignment of distant supervision to a cor-
pus. next, we describe techniques for an even more chal-
lenging generalization task: relation classi   cation for in-
put sentences in completely different languages.

training a sentence-level relation classi   er, either us-
ing the alignment-based techniques of section 2.2, or the
alignment-free method of section 3.1, requires an avail-

able kb of seed facts that have supporting evidence in the
corpus. unfortunately, available kbs have low overlap
with corpora in many languages, since kbs have cultural
and geographical biases. in response, we perform mul-
tilingual id36 by jointly modeling a high-
resource language, such as english, and an alternative
language with no kb annotation. this approach pro-
vides id21 of a predictive model to the al-
ternative language, and generalizes naturally to modeling
more languages.

extending the training technique of section 3.1 to cor-
pora in multiple languages can be achieved by factorizing
a matrix that mixes data from a kb and from the two cor-
pora. in figure 1 we split the entities of a multilingual
training corpus into sets depending on whether they have
annotation in a kb and what corpora they appear in. we
can perform id21 of a relation extractor to
the low-resource language if there are entity pairs occur-
ring in the two corpora, even if there is no kb annotation
for these pairs. note that we do not use the entity pair
embeddings at test time: they are used only to bridge
the languages during training. to form predictions in the
low-resource language, we can simply apply the pattern
scoring approach of section 3.1.

in section 5, we demonstrate that jointly learning mod-
els for english and spanish, with no annotation for the
spanish data, provides fairly accurate spanish akbc,
and even improves the performance of the english model.
note that we are not performing zero-shot learning of a
spanish model (larochelle et al., 2008). the relations
in the target schema are language-independent concepts,
and we have supervision for these in english.

3.5 tied sentence encoders
the sentence encoder approach of section 3.2 is com-
plementary to our multilingual modeling technique: we
simply use a separate encoder for each language. this
approach is sub-optimal, however, because each sentence
encoder will have a separate matrix of id27s
for its vocabulary, despite the fact that there may be con-
siderable shared structure between the languages. in re-
sponse, we propose a straightforward method for tying
the parameters of the sentence encoders across languages.
drawing on the dictionary-based techniques described
in section 2.5, we    rst obtain a list of word-word transla-
tion pairs between the languages using a translation dic-
tionary. the    rst layer of our deep text encoder consists
of a id27 lookup table. for the aligned word
types, we use a single cross-lingual embedding. details
of our approach are described in appendix 7.5.
4 task and system description
we focus on the tac kbp slot-   lling task. much re-
lated work on embedding knowledge bases evaluates on

the fb15k dataset (bordes et al., 2013; wang et al., 2014;
lin et al., 2015; yang et al., 2015; toutanova et al., 2015).
here, id36 is posed as link prediction on a
subset of freebase. this task does not capture the par-
ticular dif   culties we address: (1) evaluation on entities
and text unseen during training, and (2) zero-annotation
learning of a predictor for a low-resource language.

also, note both toutanova et al. (2015) and riedel et
al. (2013) explore the pros and cons of learning embed-
dings for entity pairs vs. separate embeddings for each
entity. as this is orthogonal to our contributions, we only
consider entity pair embeddings, which performed best in
both works when given suf   cient data.

4.1 tac slot-filling benchmark
the aim of the tac benchmark is to improve both cov-
erage and quality of id36 evaluation com-
pared to just checking the extracted facts against a knowl-
edge base, which can be incomplete and where the prove-
nances are not veri   ed. in the slot-   lling task, each sys-
tem is given a set of paired query entities and relations
or    slots    to    ll, and the goal is to correctly    ll as many
slots as possible along with provenance from the corpus.
for example, given the query entity/relation pair (barack
obama, per:spouse), the system should return the entity
michelle obama along with sentence(s) whose text ex-
presses that relation. the answers returned by all par-
ticipating teams, along with a human search (with time-
out), are judged manually for correctness, i.e. whether
the provenance speci   ed by the system indeed expresses
the relation in question.

in addition to verifying our models on the 2013 and
2014 english slot-   lling task, we evaluate our spanish
models on the 2012 tac spanish slot-   lling evaluation.
because this tac track was never of   cially run, the cov-
erage of facts in the available annotation is very small,
resulting in many correct predictions being marked in-
correctly as precision errors. in response, we manually
annotated all results returned by the models considered in
table 4. precision and recall are calculated with respect
to the union of the tac annotation and our new labeling1.

4.2 retrieval pipeline
our retrieval pipeline    rst generates all valid slot    ller
candidates for each query entity and slot, based on en-
tities extracted from the corpus using factorie (mc-
callum et al., 2009) to perform id121, segmenta-
tion, and entity extraction. we perform entity linking by
heuristically linking all entity mentions from our text cor-
pora to a freebase entity using anchor text in wikipedia.
making use of the fact that most freebase entries contain
a link to the corresponding wikipedia page, we link all

1following surdeanu et al. (2012) we remove facts about undiscov-

ered entities to correct for recall.

entity mentions from our text corpora to a freebase entity
by the following process: first, a set of candidate entities
is obtained by following frequent link anchor text statis-
tics. we then select that candidate entity for which the
cosine similarity between the respective wikipedia and
the sentence context of the mention is highest, and link to
that entity if a threshold is exceeded.

an entity pair quali   es as a candidate prediction if it
meets the type criteria for the slot.2 the tac 2013 en-
glish and spanish newswire corpora each contain about
1 million newswire documents from 2009   2012. the
document retrieval and entity matching components of
our id36 pipeline are based on relationfac-
tory (roth et al., 2014), the top-ranked system of the 2013
english slot-   lling task. we also use the english distantly
supervised training data from this system, which aligns
the tac 2012 corpus to freebase. more details on align-
ment are described in appendix 7.4.

as discussed in section 3.3, models using a deep sen-
tence encoder and using a pattern lookup table have com-
plementary strengths and weaknesses.
in response, we
present results where we ensemble the outputs of the two
models by simply taking the union of their individual out-
puts. slightly higher results might be obtained through
more sophisticated ensembling schemes.

4.3 model details
all models are implemented in torch (code publicly
available3). models are tuned to maximize f1 on the
2012 tac kbp slot-   lling evaluation. we additionally
tune the thresholds of our pattern scorer on a per-relation
basis to maximize f1 using 2012 tac slot-   lling for en-
glish and the 2012 spanish slot-   lling development set
for spanish. as in riedel et al. (2013), we train using
the bpr loss of rendle et al. (2009). our id98 is im-
plemented as described in toutanova et al. (2015), using
width-3 convolutions, followed by tanh and max pool lay-
ers. the lstm uses a bi-directional architecture where
the forward and backward representations of each hidden
state are averaged, followed by max pooling over time.
see section 7.2

we also report results including an alternate names
(an) heuristic, which uses automatically-extracted rules
to detect the tac    alternate name    relation. to achieve
this, we collect frequent wikipedia link anchor texts for

2due to the dif   culty of retrieval and entity detection, the maximum
recall for predictions is limited. for this reason, surdeanu et al. (2012)
restrict the evaluation to answer candidates returned by their system
and effectively rescaling recall. we do not perform such a re-scaling in
our english results in order to compare to other reported results. our
spanish numbers are rescaled. all scores re   ect the    anydoc    (relaxed)
scoring to mitigate penalizing effects for systems not included in the
evaluation pool.

3https://github.com/patverga/

torch-relation-extraction

recall precision

model
id98
lstm
uschema
uschema+lstm
uschema+lstm+es
uschema+lstm+an
uschema+lstm+es+an
roth et al. (2014)

31.6
32.2
29.4
34.4
38.1
36.7
40.2
35.8

f1
36.8 34.1
39.6 35.5
42.6 34.8
41.9 37.7
40.2 39.2
43.1 39.7
41.2 40.7
45.7 40.2

table 2: precision, recall and f1 on the english tac
2013 slot-   lling task. an refers to alternative names
heuristic and es refers to the addition of spanish text at
train time. lstm+uschema ensemble outperforms any
single model, including the highly-tuned top 2013 sys-
tem of roth et al. (2014), despite using no handwritten
patterns.

model
id98
lstm
uschema
uschema+lstm
uschema+lstm+es

recall precision
29.0
32.9
35.5
29.3
31.0

28.1
27.3
24.3
34.1
34.4

f1
28.5
29.8
28.8
31.5
32.6

table 3: precision, recall and f1 on the english tac
2014 slot-   lling task. es refers to the addition of span-
ish text at train time. the an heuristic is ineffective on
2014 adding only 0.2 to f1. our system would rank 4/18
in the of   cial tac 2014 competition behind systems that
use hand-written patterns and active learning despite our
system using neither of these additional annotations (sur-
deanu and ji., 2014).

each query entity. if a high id203 anchor text co-
occurs with the canonical name of the query in the same
document, we return the anchor text as a slot    ller.

5 experimental results

in experiments on the english and spanish tac kbc
slot-   lling tasks, we    nd that both uschema and lstm
models outperform the id98 across languages, and that
the lstm tends to perform slightly better than uschema
as the only model. ensembling the lstm and uschema
models further increases    nal f1 scores in all experi-
ments, suggesting that the two different types of model
compliment each other well. indeed, in section 5.3 we
present quantitative and qualitative analysis of our results
which further con   rms this hypothesis: the lstm and
uschema models each perform better on different pattern
lengths and are characterized by different precision-recall
tradeoffs.

model
lstm
lstm+dict
uschema
uschema+lstm
uschema+lstm+dict

recall precision
12.5
15.7
17.5
14.5
15.9

9.3
14.7
15.2
21.7
26.9

f1
10.7
15.2
16.3
17.3
20.0

table 4: zero-annotation id21 f1 scores on
2012 spanish tac kbp slot-   lling task. adding a trans-
lation dictionary improves all encoder-based models. en-
sembling lstm and uschema models performs the best.

5.1 english tac slot-   lling results

tables 2 and 3 present the performance of our models
on the 2013 and 2014 english tac slot-   lling tasks.
ensembling the lstm and uschema models improves
f1 by 2.2 points for 2013 and 1.7 points for 2014 over
the strongest single model on both evaluations, lstm.
adding the alternative names (an) heuristic described
in section 4.3 increases f1 by an additional 2 points on
2013, resulting in an f1 score that is competitive with
the state-of-the-art. we also demonstrate the effect of
jointly learning english and spanish models on english
slot-   lling performance. adding spanish data improves
our f1 scores by 1.5 points on 2013 and 1.1 on 2014 over
using english alone. this places are system higher than
the top performer at the 2013 tac slot-   lling task even
though our system uses no hand-written rules.

the state of the art systems on this task all rely on
matching handwritten patterns to    nd additional answers
while our models use only automatically generated, indi-
rect supervision; even our an heuristics (section 4.2) are
automatically generated. the top two 2014 systems were
angeli et al. (2014) and rpi blender (surdeanu and ji.,
2014) who achieved f1 scores of 39.5 and 36.4 respec-
tively. both of these systems used additional active learn-
ing annotation. the third place team (lin et al., 2014)
relied on highly tuned patterns and rules and achieved an
f1 score of 34.4.

our model performs substantially better on 2013 than
2014 for two reasons. first, our relationfactory (roth
et al., 2014) retrieval pipeline was a top retrieval pipeline
on the 2013 task, but was outperformed on the 2014 task
which introduced new challenges such as confusable en-
tities. second, improved training using active learning
gave the top 2014 systems a boost in performance. no
2013 systems, including ours, use active learning. bentor
et al. (2014), the 4th place team in the 2014 evaluation,
used the same retrieval pipeline (roth et al., 2014) as our
model and achieved an f1 score of 32.1.

ceo

dictionary

no ties

jefe (chief)
ceo
ejecutivo (executive)
cofundador (co-founder)
president (chairman)

ceo
director (principle)
directora (director)
   rma (   rm)
magnate (tycoon)

headquartered

dictionary
sede (headquarters)
situado (located)
selectivo (selective)
profesional (vocational)
bas  andose (based)

no ties

geol  ogico (geological)
treki (treki)
geof    sico(geophysical)
normand    a (normandy)
emplea (uses)

hubby

figure 3: precision-recall curves for uschema and
lstm on 2013 tac slot-   lling. uschema achieves
higher precision values whereas lstm has higher recall.

5.2 spanish tac slot-   lling results
table 4 presents 2012 spanish tac slot-   lling results for
our multilingual relation extractors trained using zero-
annotation id21. tying id27s be-
tween the two languages results in substantial improve-
ments for the lstm. we see that ensembling the non-
dictionary lstm with uschema gives a slight boost
over uschema alone, but ensembling the dictionary-tied
lstm with uschema provides a signi   cant increase of
nearly 4 f1 points over the highest-scoring single model,
uschema. clearly, grounding the spanish data using a
translation dictionary provides much better spanish word
representations. these improvements are complementary
to the baseline uschema model, and yield impressive re-
sults when ensembled.

in addition to embedding semantically similar phrases
from english and spanish to have high similarity, our
models also learn high-quality id73 embed-
dings. in table 5 we compare spanish nearest neighbors
of english query words learned by the lstm with dictio-
nary ties versus the lstm with no ties, using no unsuper-
vised pre-training for the embeddings. both approaches
jointly embed spanish and english word types, using
shared entity embeddings, but the dictionary-tied model
learns qualitatively better multilingual embeddings.

5.3 uschema vs lstm
we further analyze differences between uschema and
lstm in order to better understand why ensembling
the models results in the best performing system. fig-
ure 3 depicts precision-recall curves for the two mod-
els on the 2013 slot-   lling task. as observed in earlier
results, the lstm achieves higher recall at the loss of

dictionary

matrimonio (marriage)
casada (married)
esposa (wife)
cas  o (married)
embarazada (pregnant)

no ties

esposa (wife)
esposo (husband)
casada(married)
embarazada (pregnant)
embarazo (pregnancy)

alias

dictionary

simpli   cado (simpli   ed)
sabido (known)
seud  onimo (pseudonym)
privatizaci  on (privatization)
nombre (name)

no ties
weaver (weaver)
interrogaci  on (question)
alias
reelecto (reelected)
conocido (known)

table 5: example english query words (not in translation
dictionary) in bold with their top nearest neighbors by co-
sine similarity listed for the dictionary and no ties lstm
variants. dictionary-tied nearest neighbors are consis-
tently more relevant to the query word than untied.

figure 4: f1 achieved by uschema vs. lstm mod-
els for varying pattern token lengths on 2013 tac slot-
   lling. lstm performs better on longer patterns whereas
uschema performs better on shorter patterns.

0.10.20.30.40.5recall0.10.20.30.40.50.60.7precisionlstm+uschema:recallvs.precisionlstmuschema<3<5 5 10patternlength0.000.050.100.150.200.250.300.35f1lstm+uschemaf1:varyingpatternlengthlstmuschemasome precision, whereas uschema can make more pre-
cise predictions at a lower threshold for recall. in fig-
ure 4 we observe evidence for these different precision-
recall trade-offs: uschema scores higher in terms of f1
on shorter patterns whereas the lstm scores higher on
longer patterns. as one would expect, uschema success-
fully matches more short patterns than the lstm, mak-
ing more precise predictions at the cost of being unable
to predict on patterns unseen during training. the lstm
can predict using any text between entities observed at
test time, gaining recall at the loss of precision. combin-
ing the two models makes the most of their strengths and
weaknesses, leading to the highest overall f1.

qualitative analysis of our english models also sug-
gests that our encoder-based models (lstm) extract re-
lations based on a wide range of semantically similar
patterns that the pattern-matching model (uschema) is
unable to score due to a lack of exact string match in
the test data. for example, table 6 lists three exam-
ples of the per:children relation that the lstm    nds
which uschema does not, as well as three patterns that
uschema does    nd. though the lstm patterns are all
semantically and syntactically similar, they each contain
different speci   c noun phrases, e.g. lori, four children,
toddler daughter, lee and albert, etc. because these spe-
ci   c nouns weren   t seen during training, uschema fails
to    nd these patterns whereas the lstm learns to ignore
the speci   c nouns in favor of the overall pattern, that
of a parent-child relationship in an obituary. uschema
is limited to    nding the relations represented by pat-
terns observed during training, which limits the patterns
matched at test-time to short and common patterns; all
the uschema patterns matched at test time were similar
to those listed in table 6: variants of    s son,    .

lstm

mcgregor is survived by his wife, lori, and four children,
daughters jordan, taylor and landri, and a son, logan.
in addition to his wife, mays is survived by a toddler daugh-
ter and a son, billy mays jr., who is in his 20s.
anderson is survived by his wife carol, sons lee and albert,
daughter shirley englebrecht and nine grandchildren.

uschema

dio    s son, dan padavona, cautioned the memorial crowd
to be screened regularly by a doctor and take care of them-
selves, something he said his father did not do.
but marshall    s son, philip, told a different story.
   i   d rather have sully doing this than some stranger, or some
hotshot trying to be the next billy mays,    said the guy who
actually is the next billy mays, his son billy mays iii.
table 6: examples of the per:children relation discovered
by the lstm and universal schema. entities are bold
and patterns italicized. the lstm models a richer set of
patterns

6 conclusion
by jointly embedding english and spanish corpora along
with a kb, we can train an accurate spanish relation ex-
traction model using no direct annotation for relations in
the spanish data. this approach has the added bene   t of
providing signi   cant accuracy improvements for the en-
glish model, outperforming the top system on the 2013
tac kbc slot    lling task, without using the hand-coded
rules or additional annotations of alternative systems. by
using deep sentence encoders, we can perform prediction
for arbitrary input text and for entities unseen in train-
ing. sentence encoders also provides opportunities to im-
prove cross-lingual id21 by sharing word em-
beddings across languages. in future work we will apply
this model to many more languages and domains besides
newswire text. we would also like to avoid the entity de-
tection problem by using a deep architecture to both iden-
tify entity mentions and identify relations between them.
acknowledgments

many thanks to arvind neelakantan for good ideas
and discussions. we also appreciate a generous hard-
ware grant from nvidia.
this work was supported
in part by the center for intelligent information re-
trieval, in part by defense advanced research projects
agency (darpa) under agreement #fa8750-13-2-0020
and contract #hr0011-15-2-0036, and in part by the na-
tional science foundation (nsf) grant numbers dmr-
1534431, iis-1514053 and cns-0958392. the u.s.
government is authorized to reproduce and distribute
reprints for governmental purposes notwithstanding any
copyright notation thereon,
in part by darpa via
agreement #dfa8750-13-2-0020 and nsf grant #cns-
0958392. any opinions,    ndings and conclusions or rec-
ommendations expressed in this material are those of the
authors and do not necessarily re   ect those of the spon-
sor.

references
[angeli et al.2014] gabor angeli, sonal gupta, melvin jose,
christopher d manning, christopher r  e, julie tibshirani,
jean y wu, sen wu, and ce zhang. 2014. stanfords 2014
slot    lling systems. tac kbp.

[banko et al.2007] michele banko, michael

j cafarella,
stephen soderland, matt broadhead, and oren etzioni.
2007. id10 from the web.
in
international joint conference on arti   cial intelligence.

[bentor et al.2014] yinon bentor, vidhoon viswanathan, and
raymond mooney. 2014. university of texas at austin kbp
2014 slot    lling system: bayesian logic programs for tex-
tual id136. in proceedings of the seventh text analysis
conference: knowledge base population (tac 2014).

[bollacker et al.2008] kurt bollacker, colin evans, praveen
paritosh, tim sturge, and jamie taylor. 2008. freebase: a

collaboratively created graph database for structuring human
knowledge. in proceedings of the acm sigmod interna-
tional conference on management of data.

[bordes et al.2013] antoine bordes, nicolas usunier, alberto
garc    a-dur  an, jason weston, and oksana yakhnenko. 2013.
translating embeddings for modeling multi-relational data.
in advances in neural information processing systems.

[bordes et al.2014] antoine bordes, sumit chopra, and jason
weston. 2014. id53 with subgraph embed-
dings. arxiv preprint arxiv:1406.3676.

[bunescu and mooney2007] razvan bunescu and raymond
mooney. 2007. learning to extract relations from the web
using minimal supervision. in annual meeting-association
for computational linguistics, volume 45, page 576.

[carlson et al.2010] andrew carlson, justin betteridge, bryan
kisiel, burr settles, estevam r. hruschka, and a. 2010.
toward an architecture for never-ending language learning.
in in aaai.

[collobert et al.2011] ronan collobert, jason weston, l  eon
bottou, michael karlen, koray kavukcuoglu, and pavel
kuksa.
language processing (almost)
from scratch. the journal of machine learning research,
12:2493   2537.

natural

2011.

[dos santos et al.2015] c  cero nogueira dos santos, bing xi-
ang, and bowen zhou. 2015. classifying relations by rank-
ing with convolutional neural networks. in proceedings of
the 53rd annual meeting of the association for computa-
tional linguistics and the 7th international joint conference
on natural language processing, volume 1, pages 626   634.
[etzioni et al.2008] oren etzioni, michele banko, stephen
soderland, and daniel s weld. 2008. open information
extraction from the web. communications of the acm,
51(12):68   74.

[faruqui and kumar2015] manaal faruqui and shankar kumar.
2015. multilingual open id36 using cross-
lingual projection. arxiv preprint arxiv:1503.06450.

[faruqui et al.2014] manaal faruqui, jesse dodge, sujay k
jauhar, chris dyer, eduard hovy, and noah a smith.
2014. retro   tting word vectors to semantic lexicons. arxiv
preprint arxiv:1411.4166.

[garc    a-dur  an et al.2015] alberto garc    a-dur  an, antoine bor-
des, nicolas usunier, and yves grandvalet. 2015. combin-
ing two and three-way embeddings models for link predic-
tion in knowledge bases. corr, abs/1506.00999.

[gardner et al.2014] matt gardner, partha talukdar, jayant kr-
ishnamurthy, and tom mitchell. 2014. incorporating vector
space similarity in random walk id136 over knowledge
bases. in empirical methods in natural language process-
ing.

[gouws et al.2015] stephan gouws, yoshua bengio, and greg
corrado. 2015. b il bowa : fast bilingual distributed
representations without word alignments. icml, pages 1   
10.

[gu et al.2015] kelvin gu, john miller, and percy liang. 2015.
traversing id13s in vector space. arxiv preprint
arxiv:1506.01094.

[hermann and blunsom2014] karl moritz hermann and phil
blunsom. 2014. multilingual models for compositional dis-
tributed semantics. arxiv preprint arxiv:1404.4641.

[hochreiter and schmidhuber1997] sepp hochreiter and j  urgen
schmidhuber. 1997. long short-term memory. in neural
computation.

[hoffmann et al.2011] raphael hoffmann, congle zhang, xiao
ling, luke zettlemoyer, and daniel s weld.
2011.
knowledge-based weak supervision for information extrac-
tion of overlapping relations. in proceedings of the 49th an-
nual meeting of the association for computational linguis-
tics: human language technologies-volume 1, pages 541   
550. association for computational linguistics.

[kalchbrenner et al.2014] nal kalchbrenner, edward grefen-
stette, and phil blunsom. 2014. a convolutional neural
network for modelling sentences. proceedings of the 52nd
annual meeting of the association for computational lin-
guistics, june.

[kim2014] yoon kim. 2014. convolutional neural networks

for sentence classi   cation. emnlp.

[kingma and ba2015] diederik kingma and jimmy ba. 2015.
adam: a method for stochastic optimization. in 3rd inter-
national conference for learning representations (iclr).

[koehn2005] philipp koehn. 2005. europarl: a parallel corpus
for id151. in mt summit, volume 5,
pages 79   86. citeseer.

[lao et al.2011] ni lao, tom mitchell, and william w. cohen.
2011. random walk id136 and learning in a large scale
in conference on empirical methods in
knowledge base.
natural language processing.

[lao et al.2012] ni lao, amarnag subramanya, fernando
pereira, and william w. cohen. 2012. reading the web with
learned syntactic-semantic id136 rules. in joint confer-
ence on empirical methods in natural language processing
and computational natural language learning.

[larochelle et al.2008] hugo larochelle, dumitru erhan, and
yoshua bengio. 2008. zero-data learning of new tasks. in
national conference on arti   cial intelligence.

[li et al.2015] jiwei li, dan jurafsky, and eudard hovy. 2015.
when are tree structures necessary for deep learning of rep-
resentations? arxiv preprint arxiv:1503.00185.

[lin et al.2014] hailun lin, zeya zhao, yantao jia, yuanzhuo
wang, jinhua xiong, and xiaojing li. 2014. openkn at
tac kbp 2014.

[lin et al.2015] yankai lin, zhiyuan liu, maosong sun, yang
liu, and xuan zhu. 2015. learning entity and relation em-
beddings for id13 completion. in proceedings
of aaai.

[luong et al.2015] thang luong, hieu pham, and christo-
pher d manning. 2015. bilingual word representations with
monolingual quality in mind. in proceedings of the 1st work-
shop on vector space modeling for natural language pro-
cessing, pages 151   159.

[mccallum et al.2009] andrew mccallum, karl schultz, and
sameer singh. 2009. factorie: probabilistic program-
ming via imperatively de   ned factor graphs. in neural in-
formation processing systems (nips).

[mikolov et al.2013] tomas mikolov, quoc v le, and ilya
exploiting similarities among lan-
in arxiv preprint

sutskever.
guages for machine translation.
arxiv:1309.4168v1, pages 1   10.

2013.

[min et al.2013] bonan min, ralph grishman, li wan, chang
wang, and david gondek. 2013. distant supervision for

id36 with an incomplete knowledge base.
hlt-naacl, pages 777   782.

in

[mintz et al.2009] mike mintz, steven bills, rion snow, and
dan jurafsky. 2009. distant supervision for relation extrac-
tion without labeled data. in association for computational
linguistics and international joint conference on natural
language processing.

[neelakantan et al.2015] arvind neelakantan, benjamin roth,
and andrew mccallum. 2015. compositional vector space
models for knowledge base completion. proceedings of the
53rd annual meeting of the association for computational
linguistics.

[nickel et al.2011] maximilian nickel, volker tresp, and hans-
peter kriegel. 2011. a three-way model for collective learn-
ing on multi-relational data. in international conference on
machine learning.

[nickel et al.2015] maximilian nickel, kevin murphy, volker
tresp, and evgeniy gabrilovich. 2015. a review of rela-
tional machine learning for id13s: from multi-
relational link prediction to automated id13 con-
struction. arxiv preprint arxiv:1503.00759.

[rendle et al.2009] steffen rendle, christoph freudenthaler,
zeno gantner, and lars schmidt-thieme.
2009. bpr:
bayesian personalized ranking from implicit feedback.
in
proceedings of the twenty-fifth conference on uncertainty
in arti   cial intelligence, pages 452   461. auai press.

[riedel et al.2010] sebastian riedel, limin yao, and andrew
mccallum. 2010. modeling relations and their mentions
without labeled text. in machine learning and knowledge
discovery in databases, pages 148   163. springer.

[riedel et al.2013] sebastian riedel, limin yao, andrew mc-
callum, and benjamin m. marlin. 2013. id36
in hlt-
with id105 and universal schemas.
naacl.

[rocktaschel et al.2015] tim rocktaschel, sameer singh, and
sebastian riedel.
injecting logical background
knowledge into embeddings for id36. in an-
nual conference of the north american chapter of the asso-
ciation for computational linguistics (naacl).

2015.

[roth et al.2014] benjamin roth, tassilo barth, grzegorz
chrupa  a, martin gropp, and dietrich klakow. 2014. rela-
tionfactory: a fast, modular and effective system for knowl-
edge base population. eacl 2014, page 89.

[schein et al.2002] andrew i schein, alexandrin popescul,
lyle h ungar, and david m pennock. 2002. methods and
metrics for cold-start recommendations. in proceedings of
the 25th annual international acm sigir conference on re-
search and development in information retrieval, pages 253   
260. acm.

[socher et al.2013] richard socher, danqi chen, christopher d
manning, and andrew ng. 2013. reasoning with neural ten-
sor networks for knowledge base completion. in advances in
neural information processing systems.

[suchanek et al.2007] fabian m. suchanek, gjergji kasneci,
and gerhard weikum. 2007. yago: a core of semantic
knowledge. in proceedings of the 16th international con-
ference on world wide web.

[surdeanu and ji.2014] mihai surdeanu and heng ji.

2014.
overview of the english slot    lling track at the tac2014

knowledge base population evaluation. proc. text analysis
conference (tac2014).

[surdeanu et al.2012] mihai surdeanu, julie tibshirani, ramesh
nallapati, and christopher d manning.
2012. multi-
instance multi-label learning for id36. in pro-
ceedings of the 2012 joint conference on empirical methods
in natural language processing and computational natural
language learning, pages 455   465. association for com-
putational linguistics.

[sutskever et al.2014] ilya sutskever, oriol vinyals, and quoc
v. v le. 2014. sequence to sequence learning with neu-
ral networks. in advances in neural information processing
systems.

[toutanova et al.2015] kristina toutanova, danqi chen, patrick
pantel, hoifung poon, pallavi choudhury, and michael ga-
mon. 2015. representing text for joint embedding of text
and knowledge bases. in empirical methods in natural lan-
guage processing (emnlp).

[vinyals et al.2014] oriol vinyals, lukasz kaiser, terry koo,
slav petrov, ilya sutskever, and geoffrey hinton. 2014.
grammar as a foreign language. in corr.

[wang et al.2014] zhen wang, jianwen zhang, jianlin feng,
and zheng chen. 2014. id13 embedding by
in proceedings of the twenty-
translating on hyperplanes.
eighth aaai conference on arti   cial intelligence, pages
1112   1119. citeseer.

[xu et al.2015] yan xu, lili mou, ge li, yunchuan chen, hao
peng, and zhi jin. 2015. classifying relations via long short
term memory networks along shortest dependency paths. in
proceedings of conference on empirical methods in natural
language processing (to appear).

[yang et al.2015] bishan yang, wen-tau yih, xiaodong he,
jianfeng gao, and li deng. 2015. embedding entities and
relations for learning and id136 in knowledge bases. in-
ternational conference on learning representations 2014.
[yao et al.2010] limin yao, sebastian riedel, and andrew mc-
callum. 2010. collective cross-document relation extrac-
tion without labelled data. in proceedings of the 2010 con-
ference on empirical methods in natural language process-
ing, pages 1013   1023. association for computational lin-
guistics.

[yao et al.2013] limin yao, sebastian riedel, and andrew mc-
callum. 2013. universal schema for entity type prediction.
in proceedings of the 2013 workshop on automated knowl-
edge base construction, pages 79   84. acm.

[yates and etzioni2007] alexander yates and oren etzioni.
2007. unsupervised resolution of objects and relations on
the web. in north american chapter of the association for
computational linguistics.

[zeng et al.2015] daojian zeng, kang liu, yubo chen, and jun
zhao. 2015. distant supervision for id36 via
piecewise convolutional neural networks. emnlp.

7 appendix
7.1 additional qualitative results
qualitative analysis of our multilingual models further
suggests that they successfully embed semantically sim-
ilar relations across languages using tied entity pairs and
translation dictionary as grounding. table 7 lists three top
nearest neighbors in english for several spanish patterns
from the text. in each case, the english patterns capture
the relation represented in the spanish text.

y cuatro de sus familias, incluidos su esposa,
wu shu-chen, su hijo,
and four of his family members, including his wife,
wu shu-chen, his son,
and his son
is survived by his wife, sybil mackenzie and a son,
gave birth to a baby last week     son
(puff daddy, cuyos verdaderos nombre sea
(puff daddy, whose real name is
(usually credited as e1
(also known as gero ##, real name
and (after changing his name to
lleg  o a la alfombra roja en compa  n    a de su esposa, la

actriz suzy amis, casi una hora antes que su ex esposa,

arrived on the red carpet with his wife,

actress suzy amis, nearly an hour before his ex-wife ,

, who may or may not be having twins with husband
, aged twenty, kirk married
went to elaborate lengths to keep his wedding to former
supermodel

table 7: top english patterns for a spanish query pat-
tern encoded using the dictionary lstm: for each span-
ish query (english translation in italics), a list of english
nearest neighbors.

per:sibling
arg1, seg  un petici  on the primeros ministro,

su hermano gemelo arg2

arg1, sea the principal favorito para esto o   cina
que tambi  en ambiciona su hermano arg2
arg1, y su hermano gemelo, the primeros ministro arg2
arg1, for whose brother arg2
arg1 inherited his brother arg2
arg1 on saxophone and brother arg2
org:top members employees
arg2, presidente y director generales the arg1
arg2, presidente of the negocios especializada arg1
arg2 (cia), the director of the entidad, arg1
arg2, vice president and policy director of the arg1
arg2, president of the german soccer arg1
arg2, president of the quasi-of   cial arg1
per:alternate names
arg1 (como tambi  en son sabido para arg2
arg2-cuyos verdaderos nombre sea arg1
arg1 tambi  en sabido como arg2
arg1 aka arg2
arg1, who also creates music under the pseudonym arg2
arg1 ( of modern talking fame ) aka arg2
per:cities of residence
arg1, poblado d  onde vive arg2
arg1, una ciudadano naturalizado american

y nacido in arg2

arg1, que vive in arg2
arg1 was born jan. # , #### in arg2
arg1 was born on monday in arg2
arg1 was born at keighley in arg2
table 8: top scoring patterns for both spanish (top) and
english (bottom) given query tac relations.

our model jointly embeds kb relations together with
english and spanish text. we demonstrate that plausible
textual patterns are embedded close to the kb relations
they express. table 8 shows top scoring english and
spanish patterns given sample relations from our tac
kb.

implementation and hyperparameters

7.2
we performed a small grid search over learning rate
0.0001, 0.005, 0.001, dropout 0.0, 0.1, 0.25, 0.5, dimen-
sion 50, 100, (cid:96)2 gradient clipping 1, 10, 50, and epsilon
1e-8, 1e-6, 1e-4. all models are trained for a maximum
of 15 epochs. the id98 and lstm both use 100d em-
beddings while uschema uses 50d. the id98 and lstm
both learned 100-dimensional id27s which
were randomly initialized. using pre-trained embeddings
did not substantially affect the results. entity pair em-
beddings for the baseline uschema model are randomly

initialized. for the models with lstm and id98 text en-
coders, entity pair embeddings are initialized using vec-
tors from the baseline uschema model. this performs
better than random initialization. we perform (cid:96)2 gradi-
ent clipping to 1 on all models. universal schema uses
a batch size of 1024 while the id98 and lstm use 128.
all models are optimized using adam (kingma and ba,
2015) with   = 1e     8,   1 = 0.9, and   2 = 0.999 with
a learning rate of .001 for uschema and .0001 for id98
and lstm. the id98 and lstm also use dropout of 0.1
after the embedding layer.

7.3 details concerning cosine similarity

computation

we measure the similarity between rtext and rschema by
computing the vectors    cosine similarity. however, such
a distance is not well-de   ned, since the model was trained
using inner products between entity vectors and rela-

(ending in -ed) as we found the google translation in-
terprets these as adjectives (e.g.,    she read the borrowed
book    rather than    she borrowed the book   ) and much of
the relational structure in language we seek to model is
captured by verbs. this resulted in 6201 translation pairs
that occurred in our text corpus. though higher quality
translation dictionaries would likely improve this tech-
nique, our experimental results show that such automati-
cally generated dictionaries perform well.

7.6 open ie pattern id172
to improve us generalization, our us relations use log-
shortened patterns where the middle tokens in patterns
longer than    ve tokens are simpli   ed. for each long pat-
tern we take the    rst two tokens and last two tokens, and
replace all k remaining tokens with the number log k.
for example, the pattern barack obama is married to a
person named michelle obama would be converted to:
barack obama is married [1] person named michell
obama. this shortening performs slightly better than
whole patterns. lstm and id98 variants use the entire
sequence of tokens.

tion vectors, not between two relation vectors. the
us likelihood is invariant to invertible transformations

of the latent coordinate system, since   (cid:0)u(cid:62)
  (cid:0)(a(cid:62)us,o)(cid:62)a   1vr

(cid:1) =
(cid:1) for any invertible a. when taking

s,ovr

inner products between two v terms, however, the im-
plicit a   1 terms do not cancel out. we found that this
issue can be minimized, and high quality predictive accu-
racy can be achieved, simply by using suf   cient (cid:96)2 regu-
larization to avoid implicitly learning an a that substan-
tially stretches the space.

7.4 data pre-processing, distant supervision and

extraction pipeline

we replace tokens occurring less than 5 times in the cor-
pus with unk and normalize all digits to # (e.g. oct-
11-1988 becomes oct-##-####). for each sentence, we
then extract all entity pairs and the text between them as
surface patterns, ignoring patterns longer than 20 tokens.
this results in 48 million english    relations   .
in sec-
tion 7.6, we describe a technique for normalizing the sur-
face patterns. we    lter out entity pairs that occurred less
than 10 times in the data and extract the largest connected
component in this entity co-occurrence graph. this is
necessary for the baseline us model, as otherwise learn-
ing decouples into independent problems per connected
component. though the components are connected when
using sentence encoders, we use only a single compo-
nent to facilitate a fair comparison between modeling ap-
proaches. we add the distant supervision training facts
from the relationfactory system, i.e. 352,236 entity-
pair-relation tuples obtained from freebase and high pre-
cision seed patterns. the    nal training data contains
a set of 3,980,164 (kb and openie) facts made up of
549,760 unique entity pairs, 1,285,258 unique relations
and 62,841 unique tokens.

we perform the same preprocessing on the spanish
data, resulting in 34 million raw surface patterns between
entities. we then    lter patterns that never occur with an
entity pair found in the english data. this yields 860,502
spanish patterns. our multilingual model is trained on a
combination of these spanish patterns, the english sur-
face patterns, and the distant supervision data described
above. we learn id27s for 39,912 unique
spanish word types. after parameter tying for translation
pairs (section 3.5), there are 33,711 additional spanish
words not tied to english.

7.5 generation of cross-lingual tied word types
we follow the same procedure for generating translation
pairs as (mikolov et al., 2013). first, we select the top
6000 words occurring in the lowercased europarl dataset
for each language and obtain a google translation. we
then    lter duplicates and translations resulting in multi-
word phrases. we also remove english past participles

