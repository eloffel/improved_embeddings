5
1
0
2
 
c
e
d
5
2

 

 
 
]
l
c
.
s
c
[
 
 

2
v
9
2
7
3
0

.

1
1
5
1
:
v
i
x
r
a

under review as a conference paper at iclr 2016

larger-context language modelling
with recurrent neural network

tian wang
center for data science
new york university
t.wang@nyu.edu

kyunghyun cho
courant institute of mathematical sciences
and center for data science
new york university
kyunghyun.cho@nyu.edu

abstract

in this work, we propose a novel method to incorporate corpus-level discourse
information into language modelling. we call this larger-context language model.
we introduce a late fusion approach to a recurrent language model based on long
short-term memory units (lstm), which helps the lstm unit keep intra-sentence
dependencies and inter-sentence dependencies separate from each other. through
the evaluation on three corpora (imdb, bbc, and id32), we demon-
strate that the proposed model improves perplexity signi   cantly. in the experi-
ments, we evaluate the proposed approach while varying the number of context
sentences and observe that the proposed late fusion is superior to the usual way
of incorporating additional inputs to the lstm. by analyzing the trained larger-
context language model, we discover that content words, including nouns, adjec-
tives and verbs, bene   t most from an increasing number of context sentences. this
analysis suggests that larger-context language model improves the unconditional
language model by capturing the theme of a document better and more easily.

1

introduction

the goal of language modelling is to estimate the id203 distribution of various linguistic units,
e.g., words, sentences (rosenfeld, 2000). among the earliest techniques were count-based id165
language models which intend to assign the id203 distribution of a given word observed after a
   xed number of previous words. later bengio et al. (2003) proposed feed-forward neural language
model, which achieved substantial improvements in perplexity over count-based language models.
bengio et al. showed that this neural language model could simultaneously learn the conditional
id203 of the latest word in a sequence as well as a vector representation for each word in a
prede   ned vocabulary.
recently recurrent neural networks have become one of the most widely used models in language
modelling (mikolov et al., 2010). long short-term memory unit (lstm, hochreiter & schmidhu-
ber, 1997) is one of the most common recurrent activation function. architecturally speaking, the
memory state and output state are explicitly separated by activation gates such that the vanishing
gradient and exploding gradient problems described in bengio et al. (1994) is avoided. motivated
by such gated model, a number of variants of id56s (e.g. cho et al. (gru, 2014b), chung et al.
(gf-id56, 2015)) have been designed to easily capture long-term dependencies.
when modelling a corpus, these language models assume the mutual independence among sen-
tences, and the task is often reduced to assigning a id203 to a single sentence. in this work, we
propose a method to incorporate corpus-level discourse dependency into neural language model. we
call this larger-context language model. it models the in   uence of context by de   ning a conditional
id203 in the form of p (wn|w1:n   1, s), where w1, ..., wn are words from the same sentence,
and s represents the context which consists a number of previous sentences of arbitrary length.
we evaluated our model on three different corpora (imdb, bbc, and id32). our ex-
periments demonstrate that the proposed larger-context language model improve perplexity for sen-
tences, signi   cantly reducing per-word perplexity compared to the language models without context

1

under review as a conference paper at iclr 2016

information. further, through part-of-speech tag analysis, we discovered that content words, in-
cluding nouns, adjectives and verbs, bene   t the most from increasing number of context sentences.
such discovery led us to the conclusion that larger-context language model improves the uncondi-
tional language model by capturing the theme of a document.
to achieve such improvement, we proposed a late fusion approach, which is a modi   cation to the
lstm such that it better incorporates the discourse context from preceding sentences. in the exper-
iments, we evaluated the proposed approach against early fusion approach with various numbers of
context sentences, and demonstrated the late fusion is superior to the early fusion approach.
our model explores another aspect of context-dependent recurrent language model. it is novel in
that it also provides an insightful way to feed information into lstm unit, which could bene   t all
encoder-decoder based applications.

2 background: statistical language modelling

given a document d = (s1, s2, . . . , sl) which consists of l sentences, statistical language mod-
elling aims at computing its id203 p (d). it is often assumed that each sentence in the whole
document is mutually independent from each other:

p (d)     l(cid:89)

l=1

tl(cid:89)

t=1

p (sl).

(1)

we call this id203 (before approximation) a corpus-level id203. under this assumption of
mutual independence among sentences, the task of language modelling is often reduced to assigning
a id203 to a single sentence p (sl).
a sentence sl = (w1, w2, . . . , wtl ) is a variable-length sequence of words or tokens. by assuming
that a word at any location in a sentence is largely predictable by preceding words, we can rewrite
the sentence id203 into

p (s) =

p(wt|w<t),

(2)

p (s)     tl(cid:89)

where w<t is a shorthand notation for all the preceding words. we call this a sentence-level proba-
bility.
this rewritten id203 expression can be either directly modelled by a recurrent neural network
(mikolov et al., 2010) or further approximated as a product of id165 conditional probabilities such
that

p(wt|wt   1
t   n),

(3)

t=1

where wt   1
t   n = (wt   n, . . . , wt   1). the latter is called id165 language modelling. see, e.g.,
(kneser & ney, 1995) for detailed reviews on the most widely used techniques for id165 language
modelling.
the most widely used approach to this statistical language modelling is id165 language model in
eq. (3). this approach builds a large table of id165 statistics based on a training corpus. each
row of the table contains as its key the id165 phrase and its number of occurrences in the training
corpus. based on these statistics, one can estimate the id165 id155 (one of the
terms inside the product in eq. (3)) by
p(wt|wt   1

,

t   n) =

c(wt   n, . . . , wt   1, wt)
w(cid:48)   v c(wt   n, . . . , wt   1, w(cid:48))

where c(  ) is the count in the training corpus, and v is the vocabulary of all unique words/tokens.
as this estimate suffers severely from data sparsity (i.e., most id165s do not occur at all in the
training corpus), many smoothing/back-off techniques have been proposed over decades. one of the
most widely used smoothing technique is a modi   ed kneser-ney smoothing (kneser & ney, 1995)

(cid:80)

2

under review as a conference paper at iclr 2016

more recently, bengio et al. (2003) proposed to use a feedforward neural network to model those
id165 conditional probabilities to avoid the issue of data sparsity. this model is often referred to
as neural language model.
this id165 language modelling is however limited due to the n-th order markov assumption made
in eq. (3). hence, mikolov et al. (2010) proposed recently to use a recurrent neural network to
directly model eq. (2) without making any markov assumption. we will refer to this approach of
using a recurrent neural network for id38 as recurrent language modelling.
a recurrent language model is composed of two function   transition and output functions. the
transition function reads one word wt and updates its hidden state such that

ht =    (wt, ht   1) ,

(4)
where h0 is an all-zero vector.    is a recurrent activation function, and two most commonly used
ones are long short-term memory units (lstm, hochreiter & schmidhuber, 1997) and gated recur-
rent units (gru, cho et al., 2014b). for more details on these recurrent activation units, we refer
the reader to (jozefowicz et al., 2015; greff et al., 2015).
at each timestep, the output function computes the id203 over all possible next words in the
vocabulary v . this is done by

p(wt+1 = w(cid:48)|wt

1)     exp (gw(cid:48)(ht)) .

(5)

g is commonly implemented as an af   ne transformation:

g(ht) = woht + bo,

where wo     r|v |  d and bo     r|v |.
the whole model is trained by maximizing the log-likelihood of a training corpus often using
stochastic id119 with id26 through time (see, e.g., rumelhart et al., 1988).
these different approaches to language modelling have been extensively tested against each other
in terms of id103 and machine translation in recent years (sundermeyer et al., 2015;
baltescu & blunsom, 2014; schwenk, 2007). often the conclusion is that all three techniques tend
to have different properties and qualities dependent on many different factors, such as the size of
training corpus, available memory and target application. in many cases, it has been found that it is
bene   cial to combine all these techniques together in order to achieve the best language model.
one most important thing to note in this conventional approach to statistical language modelling is
that every sentence in a document is assumed independent from each other (see eq. (1).) this raises
a question on how strong an assumption this is, how much impact this assumption has on the    nal
language model quality and how much gain language modelling can get by making this assumption
less strong.

2.1 language modelling with long short-term memory

here let us brie   y describe a long short-term memory unit which is widely used as a recurrent
activation function    (see eq. (4)) for language modelling (see, e.g., graves, 2013).
a layer of long short-term memory (lstm) unit consists of three gates and a single memory cell.
three gates   input, output and forget    are computed by

it =   (wixt + uiht   1 + bi)
ot =   (woxt + uoht   1 + bo)
ft =   (wf xt + uf ht   1 + bf ) ,
where    is a sigmoid function. xt is the input at the t-th timestep.
the memory cell is computed by

ct = ft (cid:12) ct   1 + it (cid:12) tanh (wcx + ucht   1 + bc) ,

(9)
where (cid:12) is an element-wise multiplication. this adaptive leaky integration of the memory cell al-
lows the lstm to easily capture long-term dependencies in the input sequence, and this has recently
been widely adopted many works involving language models (see, e.g., sundermeyer et al., 2015).
the output, or the activation of this lstm layer, is then computed as

(6)
(7)
(8)

ht = ot (cid:12) tanh(ct).

3

under review as a conference paper at iclr 2016

3 larger-context language modelling

p (d)     l(cid:89)

in this paper, we aim not at improving the sentence-level id203 estimation p (s) (see eq. (2))
but at improving the corpus-level id203 p (d) from eq. (1) directly. one thing we noticed
at the beginning of this work is that it is not necessary for us to make the assumption of mutual
independence of sentences in a corpus. rather, similarly to how we model a sentence id203,
we can loosen this assumption by

p (sl|sl   1
l   n),

(10)

l=1

where sl   1
l   n = (sl   n, sl   n+1, . . . , sl   1). n decides on how many preceding sentences each condi-
tional sentence id203 conditions on, similarly to what happens with a usual id165 language
modelling.
from the statistical modelling   s perspective, estimating the corpus-level language id203 in
eq. (10) is equivalent to build a statistical model that approximates
p(wt|w<t, sl   1
l   n),

p (sl|sl   1

tl(cid:89)

l   n) =

(11)

t=1

similarly to eq. (2). one major difference from the existing approaches to statistical language mod-
elling is that now each id155 of a next word is conditioned not only on the preceding
words in the same sentence, but also on the n     1 preceding sentences.
a conventional, count-based id165 language model is not well-suited due to the issue of data
sparsity. in other words, the number of rows in the table storing id165 statistics will explode as the
number of possible sentence combinations grows exponentially with respect to both the vocabulary
size, each sentence   s length and the number of context sentences.
either neural or recurrent language modelling however does not suffer from this issue of data spar-
sity. this makes these models ideal for modelling the larger-context sentence id203 in eq. (11).
more speci   cally, we are interested in adapting the recurrent language model for this.
in doing so, we answer two questions in the following subsections. first, there is a question of how
we should represent the context sentences sl   1
l   n. we consider two possibilities in this work. second,
there is a large freedom in how we build a recurrent activation function to be conditioned on the
context sentences. we also consider two alternatives in this case.

3.1 context representation

a sequence of preceding sentences can be represented in many different ways. here, let us describe
two alternatives we test in the experiments.
the    rst representation is to simply bag all the words in the preceding sentences into a single vector
|v |. any element of s corresponding to the word that exists in one of the preceding
s     [0, 1]
sentences will be assigned the frequency of that word, and otherwise 0. this vector is multiplied
from left by a matrix p which is tuned together with all the other parameters:

p = ps.

we call this representation p a bag-of-words (bow) context.
second, we try to represent the preceding context sentences as a sequence of bag-of-words. each
bag-of-word sj is the bag-of-word representation of the j-th context sentence, and they are put into
a sequence (sl   n, . . . , sl   1). unlike the    rst bow context, this allows us to incorporate the order
of the preceding context sentences.
this sequence of bow vectors are read by a recurrent neural network which is separately from the
one used for modelling a sentence (see eq. (4).) we use lstm units as recurrent activations, and
for each context sentence in the sequence, we get

for t = l     n, . . . , l     1. we set the last hidden state zl   1 of this recurrent neural network, to which
we refer as a context recurrent neural network, as the context vector p.

zt =    (xt, zt   1) ,

4

under review as a conference paper at iclr 2016

attention-based context representation the sequence of bow vectors can be used in a bit
different way from the above. instead of a unidirectional recurrent neural network, we    rst use a
bidirectional recurrent neural network to read the sequence. the forward recurrent neural network
reads the sequence as usual in a forward direction, and the reverse recurrent neural network in the
opposite direction. the hidden states from these two networks are then concatenated for each context
sentence in order to form a sequence of annotation vectors (zl   n, . . . , zl   1).
unlike the other approaches, in this case, the context vector p differs for each word wt in the current
sentence, and we denote it by pt. the context vector pt for the t-th word is computed as the weighted
sum of the annotation vectors:

l   1(cid:88)

l(cid:48)=l   n

pt =

  t,l(cid:48)zl(cid:48),

where the attention weight   t,l(cid:48) is computed by

(cid:80)l   1

  t,l(cid:48) =

exp score (zl(cid:48), ht)

k=l   n exp score (zk, ht)

.

ht is the hidden state of the recurrent language model of the current sentence from eq. (5). the
scoring function score(zl(cid:48), ht) returns a relevance score of the l(cid:48)-th context sentence with respect to
ht.

3.2 conditional lstm

early fusion once the context vector p is computed from the n preceding sentences, we need
to feed this into the sentence-level recurrent language model. one most straightforward way is to
simply consider it as an input at every time step such that

x = e(cid:62)wt + wpp,

where e is the id27 matrix that transforms the one-hot vector of the t-th word into a
continuous word vector. this x is used by the lstm layer as the input, as described in sec. 2.1. we
call this approach an early fusion of the context into language modelling.

late fusion in addition to this approach, we propose here a modi   cation to the lstm such that it
better incorporates the context from the preceding sentences (summarized by pt.) the basic idea is
to keep dependencies within the sentence being modelled (intra-sentence dependencies) and those
between the preceding sentences and the current sent (inter-sentence dependencies) separately from
each other.
we let the memory cell ct of the lstm in eq. (9) to model intra-sentence dependencies. this simply
means that there is no change to the existing formulation of the lstm, described in eqs. (6)   (9).
the inter-sentence dependencies are re   ected on the interaction between the memory cell ct, which
models intra-sentence dependencies, and the context vector p, which summarizes the n preceding
sentences. we model this by    rst computing the amount of in   uence of the preceding context
sentences as

rt =    (wr (wpp) + wrc + br) .

this vector rt controls the strength of each of the elements in the context vector p. this amount of
in   uence from the n preceding sentences is decided based on the currently captured intra-sentence
dependency structures and the preceding sentences.
this controlled context vector rt (cid:12) (wpp) is then used to compute the output of the lstm layer
such that

ht = ot (cid:12) tanh (ct + rt (cid:12) (wpp)) .

this is illustrated in fig. 1 (b).
we call this approach a late fusion, as the effect of the preceding context is fused together with the
intra-sentence dependency structure in the later stage of the recurrent activation.

5

under review as a conference paper at iclr 2016

(a)

(b)

figure 1: graphical illustration of the proposed (a) early fusion and (b) late fusion.

late fusion is a simple, but effective way to mitigate the issue of vanishing gradient in corpus-
level language modelling. by letting the context representation    ow without having to pass through
saturating nonlinear id180, it provides a linear path through which the gradient for the
context    ows easily.

4 related work

4.1 context-dependent recurrent language model

this possibility of extending a neural or recurrent id38 to incorporate larger context
was explored earlier. especially, (mikolov & zweig, 2012) proposed an approach, called context-
dependent recurrent neural network language model, very similar to the proposed approach here.
the basic idea of their approach is to use a topic distribution, represented as a vector of probabili-
ties, of previous n words when computing the hidden state of the recurrent neural network each time.
in doing so, the words used to compute the topic distribution often went over the sentence boundary,
meaning that this distribution vector was summarizing a part of a preceding sentence. nevertheless,
their major goal was to use this topic distribution vector as a way to    convey contextual informa-
tion about the sentence being modeled.    more recently, mikolov et al. (2014) proposed a similar
approach however without relying on external topic modelling.
there are three major differences in the proposed approach from the work by mikolov & zweig
(2012). first, the goal in this work is to explicitly model preceding sentences to better approximate
the corpus-level id203 (see eq. (10)) rather than to get a better context of the current sentence.
second, mikolov & zweig (2012) use an external method, such as id44 (blei
et al., 2003) or latent semantics analysis (dumais, 2004) to extract a feature vector, where we learn
the whole model, including the context vector extraction, end-to-end. third, we propose a late
fusion approach which is well suited for the lstm units which have recently been widely adopted
many works involving language models (see, e.g., sundermeyer et al., 2015). this late fusion is
later shown to be superior to the early fusion approach.
similarly, sukhbaatar et al. (2015) proposed more recently to use a memory network for language
modelling with a very large context of a hundred to two hundreds preceding words. the major
difference to the proposed approach is in the lack of separation between the context sentences and
the current sentence being processed. there are two implications from this approach. first, each
sentence, depending on its and the preceding sentences    lengths, is conditioned on a different number
of context sentences. second, words in the beginning of the sentence being modelled tend to have
a larger context (in terms of the number of preceding sentences they are being conditioned on) than
those at the end of the sentence. these issues do not exist in the proposed approach here.
unlike these earlier works, the proposed approach here uses sentence boundaries explicitly. this
makes it easier to integrate with downstream applications, such as machine translation and speech
recognition, at the decoding level which almost always works sentence-wise.

6

under review as a conference paper at iclr 2016

it is however important to notice that these two previous works by mikolov & zweig (2012) and
sukhbaatar et al. (2015) are not in competition with the proposed larger-context recurrent language
model. rather, all these three are orthogonal to each other and can be combined.

4.2 dialogue modelling with recurrent neural networks

a more similar model to the proposed larger-context recurrent language model is a hierarchical
recurrent encoder decoder (hred) proposed recently by serban et al. (2015). the hred consists
of three recurrent neural networks to model a dialogue between two people from the perspective of
one of them, to which we refer as a speaker. if we consider the last utterance of the speaker being
modelled by the decoder of the hred, this model can be considered as a larger-context recurrent
language model with early fusion.
aside the fact that the ultimate goals differ (in their case, dialogue modelling and in our case, doc-
ument modelling), there are two technical differences. first, they only test with the early fusion
approach. we show later in the experiments that the proposed late fusion gives a better language
modelling quality than the early fusion. second, we use a sequence of bag-of-words to represent the
preceding sentences, while the hred a sequence of sequences of words. this allows the hred
to potentially better model the order of the words in each preceding sentence, but it increases com-
putational complexity (one more recurrent neural network) and decreases statistical ef   cient (more
parameters with the same amount of data.)
again, the larger-context language model proposed here is not competing against the hred. rather,
it is a variant, with differences in technical details, that is being evaluated speci   cally for document
language modelling. we believe many of the components in these two models are complementary
to each other and may improve each other. for instance, the hred may utilize the proposed late
fusion, and the larger-context recurrent language model here may represent the context sentences as
a sequence of sequences of words, instead of a bow context or a sequence of bow vectors.

4.3 skip-thought vectors

perhaps the most similar work is the skip-thought vector by kiros et al. (2015). in their work, a
recurrent neural network is trained to read a current sentence, as a sequence of words, and extract
a so-called skip-thought vector of the sentence. there are two other recurrent neural networks
which respectively model preceding and following sentences. if we only consider the prediction of
the following sentence, then this model becomes a larger-context recurrent language model which
considers a single preceding sentence as a context.
as with the other previous works we have discussed so far, the major difference is in the ultimate
goal of the model. kiros et al. (2015) fully focused on using their model to extract a good, generic
sentence vector, while in this paper we are focused on obtaining a good language model. there are
less major technical differences. first, the skip-thought vector model conditions only on the imme-
diate preceding sentence, while we extend this to multiple preceding sentences. the experiments
later will show the importance of having a larger context. second, similarly to the two other previ-
ous works by mikolov & zweig (2012) and serban et al. (2015), the skip-thought vector model only
implements early fusion.

4.4 id4: conditional language modelling

id4 is another related approach (forcada &   neco, 1997; kalchbrenner &
blunsom, 2013; cho et al., 2014b; sutskever et al., 2014; bahdanau et al., 2014). in neural machine
translation, often two recurrent neural networks are used. the    rst recurrent neural network, called
an encoder, reads a source sentence, represented as a sequence of words in a source language, to
form a context vector, or a set of context vectors. the other recurrent neural network, called a
decoder, then, models the target translation conditioned on this source context.
this is similar to the proposed larger-context recurrent language model, if we consider the source
sentence as a preceding sentence in a corpus. the major difference is in the ultimate application,
machine translation vs.
language modelling, and technically, the differences between neural ma-

7

under review as a conference paper at iclr 2016

chine translation and the proposed larger-context language model are similar to those between the
hred and the larger-context language model.
similarly to the other previous works we discussed earlier, it is possible to incorporate the proposed
larger-context language model into the existing id4 framework, and also to
incorporate advanced mechanisms from the id4 framework. attention mech-
anism was introduced by bahdanau et al. (2014) with intention to build a variable-length context
representation in source sentence. in larger-context language model, this mechanism is applied on
context sentences (see sec. 3.1,) and we present the results in the later section showing that the
attention mechanism indeed improves the quality of language modelling.

4.5 context-dependent question-answering models

context-dependent question-answering is a task in which a model is asked to answer a question
based on the facts from a natural language paragraph. the question and answer are often formulated
as    lling in a missing word in a query sentence (hermann et al., 2015; hill et al., 2015). this task is
closely related to the larger-context language model we proposed in this paper in the sense that its
goal is to build a model to learn

p(qk|q<k, q>k, d),

(12)

where qk is the missing k-th word in a query q, and q<k and q>k are the context words from the
query. d is the paragraph containing facts about this query. often, it is explicitly constructed so that
the query q does not appear in the paragraph d.
it is easy to see the similarity between eq. (12) and one of the conditional probabilities in the r.h.s. of
eq. (11). by replacing the context sentences sl   1
l   n in eq. (11) with d in eq. (12) and conditioning wt
on both the preceding and following words, we get a context-dependent question-answering model.
in other words, the proposed larger-context language model can be used for context-dependent
question-answering, however, with computational overhead. the overhead comes from the fact that
for every possible answer the id155 completed query sentence must be evaluated.

5 experimental settings

5.1 models

there are six possible combinations of the proposed methods. first, there are two ways of represent-
ing the context sentences; (1) bag-of-words (bow) and (2) a sequence of bag-of-words (seqbow),
from sec. 3.1. there are two separate ways to incorporate the seqbow; (1) with attention mecha-
nism (att) and (2) without it. then, there are two ways of feeding the context vector into the main
recurrent language model (rlm); (1) early fusion (ef) and (2) late fusion (lf), from sec. 3.2. we
will denote these six possible models by

1. rlm-bow-ef-n
2. rlm-seqbow-ef-n
3. rlm-seqbow-att-ef-n
4. rlm-bow-lf-n
5. rlm-seqbow-lf-n
6. rlm-seqbow-att-lf-n

n denotes the number of preceding sentences to have as a set of context sentences. we test four
different values of n; 1, 2, 4 and 8.
as a baseline, we also train a recurrent language model without any context information. we refer
to this model by rlm. furthermore, we also report the result with the conventional, count-based
id165 language model with the modi   ed kneser-ney smoothing with kenlm (hea   eld et al.,
2013).

8

under review as a conference paper at iclr 2016

imdb

# sentences

# words

bbc

# sentences

training
validation

test

930,139
152,987
151,987

21m
3m
3m

37,207
1,998
2,199

# words
890k
49k
53k

id32

# sentences

42,068
3,370
3,761

# words
888k
70k
79k

table 1: statistics of imdb, bbc and id32

each recurrent language model uses 1000 lstm units and is trained with adadelta (zeiler, 2012)
to maximize the log-likelihood de   ned as

k(cid:88)

k=1

l(  ) =

1
k

log p(sk|sk   1
k   n).

we early-stop training based on the validation log-likelihood and report the perplexity on the test set
using the best model according to the validation log-likelihood.
we use only those sentences of length up to 50 words when training a recurrent language model for
the computational reason. for kenlm, we used all available sentences in a training corpus.

5.2 datasets

we evaluate the proposed larger-context language model on three different corpora. for detailed
statistics, see table 1.

imdb movie reviews a set of movie reviews is an ideal dataset to evaluate many different set-
tings of the proposed larger-context language models, because each review is highly likely of a single
theme (the movie under review.) a set of words or the style of writing will be well determined based
on the preceding sentences.
we use the imdb move review corpus (imdb) prepared by maas et al. (2011).1 this corpus
has 75k training reviews and 25k test reviews. we use the 30k most frequent words for recurrent
language models.

bbc similarly to movie reviews, each new article tends to convey a single theme. we use the
bbc corpus prepared by greene & cunningham (2006).2 unlike the imdb corpus, this corpus
contains news articles which are almost always written in a formal style. by evaluating the proposed
approaches on both the imdb and bbc corpora, we can tell whether the bene   ts from larger context
exist in both informal and formal languages. we use the 10k most frequent words for recurrent
language models.
both with the imdb and bbc corpora, we did not do any preprocessing other than id121.3

id32 we evaluate a normal recurrent language model, count-based id165 language
model as well as the proposed rlm-bow-ef-n and rlm-bow-lf-n with varying n = 1, 2, 4, 8
on the id32 corpus. we preprocess the corpus according to (mikolov et al., 2011) and use
a vocabulary of 10k words.

6 results and analysis

6.1 corpus-level perplexity
we evaluated the models, including all the proposed approaches (rlm-{bow,seqbow}-{att,   }-
{ef,lf}-n), on the imdb corpus.
in fig. 2 (a), we see three major trends. first, rlm-bow,

1 http://ai.stanford.edu/  amaas/data/sentiment/
2 http://mlg.ucd.ie/datasets/bbc.html
3

https://github.com/moses-smt/mosesdecoder/blob/master/scripts/

tokenizer/tokenizer.perl

9

under review as a conference paper at iclr 2016

(a) imdb

(b) id32

(c) bbc

figure 2: corpus-level perplexity on (a) imdb, (b) id32 and (c) bbc. the count-based
5-gram language models with kneser-ney smoothing respectively resulted in the perplexities of
110.20, 148 and 127.32, and are not shown here. note that we did not show seqbow in the cases of
n = 1, as this is equivalent to bow.

either with the early fusion or late fusion, outperforms both the count-based id165 and recurrent
language model (lstm) regardless of the number of context sentences. second, the improvement
grows as the number n of context sentences increases, and this is most visible with the novel late
fusion. lastly, we see that the rlm-seqbow does not work well regardless of the fusion type
(rlm-seqbow-ef not shown), while after using attention-based model rlm-seqbow-att, the
performance is greatly improved.
because of the second observation from the imdb corpus, that the late fusion clearly outperforms
the early fusion, we evaluated only rlm-{bow,seqbow}-{att}-lf-n   s on the other two corpora.
on the other two corpora, ptb and bbc, we observed a similar trend of rlm-seqbow-att-lf-
n and rlm-bow-lf-n outperforming the two conventional language models, and that this trend
strengthened as the number n of the context sentences grew. we also observed again that the rlm-
seqbow-att-lf outperforms rlm-seqbow-lf and rlm-bow in almost all the cases.
from these experiments, the bene   t of allowing larger context to a recurrent language model is clear,
however, with the right choice of the context representation (see sec. 3.1) and the right mechanism
for feeding the context information to the recurrent language model (see sec. 3.2.) in these exper-
iments, the sequence of bag-of-words representation with attention mechanism, together with the
late fusion was found to be the best choice in all three corpora.
one possible explanation on the failure of the seqbow representation with a context recurrent neural
network is that it is simply dif   cult for the context recurrent neural network to compress multiple
sentences into a single vector. this dif   culty in training a recurrent neural network to compress
a long sequence into a single vector has been observed earlier, for instance, in neural machine
translation (cho et al., 2014a). attention mechanism, which was found to avoid this problem in
machine translation (bahdanau et al., 2014), is found to solve this problem in our task as well.

6.2 analysis: perplexity per part-of-speech tag

next, we attempted at discovering why the larger-context recurrent language model outperforms
in order to do so, we computed the perplexity per
the unconditional recurrent language model.
part-of-speech (pos) tag.
we used the stanford log-linear part-of-speech tagger (stanford pos tagger, toutanova et al., 2003)
to tag each word of each sentence in the corpora.4 we then computed the perplexity of each word
and averaged them for each tag type separately. among the 36 pos tags used by the stanford pos
tagger, we looked at the perplexities of the ten most frequent tags (nn, in, dt, jj, rb, nns, vbz,
vb, prp, cc), of which we combined nn and nns into a new tage noun and vb and vbz into a
new tag verb.
we show the results using the rlm-bow-lf and rlm-seqbow-att-lf on all three corpora   
imdb, bbc and id32    in fig. 3. we observe that the predictability, measured by the
perplexity (negatively correlated), grows most for nouns (noun) and adjectives (jj) as the number

4 http://nlp.stanford.edu/software/tagger.shtml

10

under review as a conference paper at iclr 2016

(a) imdb

(b) bbc

(i) rlm-bow-lf

(b) id32

(a) imdb

(b) bbc

(ii) rlm-seqbow-att-lf

(b) id32

figure 3: perplexity per pos tag on the (a) imdb, (b) bbc and (c) id32 corpora.

of context sentences increases. they are followed by verbs (verb). in other words, nouns, adjectives
and verbs are the ones which become more predictable by a language model given more context. we
however noticed the relative degradation of quality in coordinating conjunctions (cc), determiners
(dt) and personal pronouns (prp).
it is worthwhile to note that nouns, adjectives and verbs are open-class, content, words, and conjunc-
tions, determiners and pronouns are closed-class, function, words (see, e.g., miller, 1999). the func-
tions words often play grammatical roles, while the content words convey the content of a sentence
or discourse, as the name indicates. from this, we may carefully conclude that the larger-context
language model improves upon the conventional, unconditional language model by capturing the
theme of a document, which is re   ected by the improved perplexity on    content-heavy    open-class
words (chung & pennebaker, 2007). in our experiments, this came however at the expense of slight
degradation in the perplexity of function words, as the model   s capacity stayed same (though, it is
not necessary.)
this observation is in line with a recent    nding by hill et al. (2015). they also observed signi   -
cant gain in predicting open-class, or content, words when a question-answering model, including
humans, was allowed larger context.

7 conclusion

in this paper, we proposed a method to improve language model on corpus-level by incorporating
larger context. using this model results in the improvement in perplexity on the imdb, bbc and
id32 corpora, validating the advantage of providing larger context to a recurrent language
model.
from our experiments, we found that the sequence of bag-of-words with attention is better than bag-
of-words for representing the context sentences (see sec. 3.1), and the late fusion is better than the
early fusion for feeding the context vector into the main recurrent language model (see sec. 3.2). our
part-of-speech analysis revealed that content words, including nouns, adjectives and verbs, bene   t
most from an increasing number of context sentences (see sec. 6.2). this analysis suggests that
larger-context language model improves perplexity because it captures the theme of a document
better and more easily.

11

under review as a conference paper at iclr 2016

to explore the potential of such a model, there are several aspects in which more research needs
to be done. first, the three datasets we used in this paper are relatively small in the context of
language modelling, therefore the proposed larger-context language model should be evaluated on
larger corpora. second, more analysis, beyond the one based on part-of-speech tags, should be
conducted in order to better understand the advantage of such larger-context models. lastly, it is
important to evaluate the impact of the proposed larger-context models in downstream tasks such as
machine translation and id103.

acknowledgments

this work is done as a part of the course ds-ga 1010-001 independent study in data science at
the center for data science, new york university.

references
bahdanau, dzmitry, cho, kyunghyun, and bengio, yoshua. id4 by jointly

learning to align and translate. arxiv preprint arxiv:1409.0473, 2014.

baltescu, paul and blunsom, phil. pragmatic neural language modelling in machine translation.

arxiv preprint arxiv:1412.7119, 2014.

bengio, yoshua, simard, patrice, and frasconi, paolo. learning long-term dependencies with gra-

dient descent is dif   cult. neural networks, ieee transactions on, 5(2):157   166, 1994.

bengio, yoshua, ducharme, r  ejean, vincent, pascal, and janvin, christian. a neural probabilistic

language model. the journal of machine learning research, 3:1137   1155, 2003.

blei, david m, ng, andrew y, and jordan, michael i. id44.

machine learning research, 3:993   1022, 2003.

the journal of

cho, kyunghyun, van merrienboer, bart, bahdanau, dzmitry, and bengio, yoshua. on the proper-
ties of id4: encoder-decoder approaches. in eighth workshop on syntax,
semantics and structure in statistical translation (ssst-8), 2014a.

cho, kyunghyun, van merri  enboer, bart, gulcehre, caglar, bahdanau, dzmitry, bougares, fethi,
schwenk, holger, and bengio, yoshua. learning phrase representations using id56 encoder-
decoder for id151. arxiv preprint arxiv:1406.1078, 2014b.

chung, cindy and pennebaker, james w. the psychological functions of function words. social

communication, pp. 343   359, 2007.

chung, junyoung, gulcehre, caglar, cho, kyunghyun, and bengio, yoshua. gated feedback recur-
rent neural networks. in proceedings of the 32nd international conference on machine learning
(icml-15), pp. 2067   2075, 2015.

dumais, susan t. latent semantic analysis. annual review of information science and technology,

38(1):188   230, 2004.

forcada, mikel l and   neco, ram  on p. recursive hetero-associative memories for translation. in
biological and arti   cial computation: from neuroscience to technology, pp. 453   462. springer,
1997.

graves, alex.

generating sequences with recurrent neural networks.

arxiv:1308.0850, 2013.

arxiv preprint

greene, derek and cunningham, p  adraig. practical solutions to the problem of diagonal dominance
in proc. 23rd international conference on machine learning

in kernel document id91.
(icml   06), pp. 377   384. acm press, 2006.

greff, klaus, srivastava, rupesh kumar, koutn    k, jan, steunebrink, bas r, and schmidhuber,

j  urgen. lstm: a search space odyssey. arxiv preprint arxiv:1503.04069, 2015.

12

under review as a conference paper at iclr 2016

hea   eld, kenneth, pouzyrevsky, ivan, clark, jonathan h., and koehn, philipp. scalable modi-
   ed kneser-ney language model estimation. in proceedings of the 51st annual meeting of the
association for computational linguistics, pp. 690   696, so   a, bulgaria, august 2013. url
http://kheafield.com/professional/edinburgh/estimate_paper.pdf.

hermann, karl moritz, ko  cisk`y, tom  a  s, grefenstette, edward, espeholt, lasse, kay, will, suley-
man, mustafa, and blunsom, phil. teaching machines to read and comprehend. arxiv preprint
arxiv:1506.03340, 2015.

hill, felix, bordes, antoine, chopra, sumit, and weston, jason. the goldilocks principle: reading
children   s books with explicit memory representations. arxiv preprint arxiv:1511.02301, 2015.

hochreiter, sepp and schmidhuber, j  urgen. long short-term memory. neural computation, 9(8):

1735   1780, 1997.

jozefowicz, rafal, zaremba, wojciech, and sutskever, ilya. an empirical exploration of recurrent
network architectures. in proceedings of the 32nd international conference on machine learning
(icml-15), pp. 2342   2350, 2015.

kalchbrenner, nal and blunsom, phil. recurrent continuous translation models. in emnlp, pp.

1700   1709, 2013.

kiros, ryan, zhu, yukun, salakhutdinov, ruslan, zemel, richard s, torralba, antonio, urtasun,

raquel, and fidler, sanja. skip-thought vectors. arxiv preprint arxiv:1506.06726, 2015.

kneser, reinhard and ney, hermann. improved backing-off for m-gram id38.

in
acoustics, speech, and signal processing, 1995. icassp-95., 1995 international conference on,
volume 1, pp. 181   184. ieee, 1995.

maas, andrew l, daly, raymond e, pham, peter t, huang, dan, ng, andrew y, and potts, christo-
pher. learning word vectors for id31. in proceedings of the 49th annual meeting
of the association for computational linguistics: human language technologies-volume 1, pp.
142   150. association for computational linguistics, 2011.

mikolov, tomas and zweig, geoffrey. context dependent recurrent neural network language model.

in slt, pp. 234   239, 2012.

mikolov, tomas, kara     at, martin, burget, lukas, cernock`y, jan, and khudanpur, sanjeev. recur-
rent neural network based language model. in interspeech 2010, 11th annual conference
of the international speech communication association, makuhari, chiba, japan, september
26-30, 2010, pp. 1045   1048, 2010.

mikolov, tom  a  s, kombrink, stefan, burget, luk  a  s,   cernock`y, jan honza, and khudanpur, san-
jeev. extensions of recurrent neural network language model. in acoustics, speech and signal
processing (icassp), 2011 ieee international conference on, pp. 5528   5531. ieee, 2011.

mikolov, tomas, joulin, armand, chopra, sumit, mathieu, michael, and ranzato, marc   aurelio.

learning longer memory in recurrent neural networks. arxiv preprint arxiv:1412.7753, 2014.

miller, george a. on knowing a word. annual review of psychology, 50(1):1   19, 1999.

rosenfeld, ronald. two decades of statistical id38: where do we go from here. in

proceedings of the ieee, 2000.

rumelhart, david e, hinton, geoffrey e, and williams, ronald j. learning representations by

back-propagating errors. cognitive modeling, 5:3, 1988.

schwenk, holger. continuous space language models. computer speech & language, 21(3):492   

518, 2007.

serban, iulian v, sordoni, alessandro, bengio, yoshua, courville, aaron, and pineau, joelle. hier-
archical neural network generative models for movie dialogues. arxiv preprint arxiv:1507.04808,
2015.

13

under review as a conference paper at iclr 2016

sukhbaatar, sainbayar, szlam, arthur, weston, jason, and fergus, rob. end-to-end memory net-

works. arxiv preprint arxiv:1503.08895, 2015.

sundermeyer, martin, ney, hermann, and schluter, ralf. from feedforward to recurrent lstm neural
networks for id38. audio, speech, and language processing, ieee/acm transac-
tions on, 23(3):517   529, 2015.

sutskever, ilya, vinyals, oriol, and le, quoc vv. sequence to sequence learning with neural net-

works. in advances in neural information processing systems, pp. 3104   3112, 2014.

toutanova, kristina, klein, dan, manning, christopher d, and singer, yoram. feature-rich part-of-
speech tagging with a cyclic dependency network. in proceedings of the 2003 conference of the
north american chapter of the association for computational linguistics on human language
technology-volume 1, pp. 173   180. association for computational linguistics, 2003.

zeiler, matthew d. adadelta: an adaptive learning rate method. arxiv preprint arxiv:1212.5701,

2012.

14

