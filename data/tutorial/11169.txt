stacked attention networks for image id53

zichao yang1, xiaodong he2, jianfeng gao2, li deng2, alex smola1

1carnegie mellon university, 2microsoft research, redmond, wa 98052, usa

zichaoy@cs.cmu.edu, {xiaohe, jfgao, deng}@microsoft.com, alex@smola.org

6
1
0
2

 

n
a
j
 

6
2

 
 
]

g
l
.
s
c
[
 
 

2
v
4
7
2
2
0

.

1
1
5
1
:
v
i
x
r
a

abstract

this paper presents stacked attention networks (sans)
that learn to answer natural language questions from im-
ages. sans use semantic representation of a question as
query to search for the regions in an image that are related
to the answer. we argue that image id53
(qa) often requires multiple steps of reasoning. thus, we
develop a multiple-layer san in which we query an image
multiple times to infer the answer progressively. experi-
ments conducted on four image qa data sets demonstrate
that the proposed sans signi   cantly outperform previous
state-of-the-art approaches. the visualization of the atten-
tion layers illustrates the progress that the san locates the
relevant visual clues that lead to the answer of the question
layer-by-layer.

1. introduction

with the recent advancement in id161 and
in natural language processing (nlp), image question an-
swering (qa) becomes one of the most active research ar-
eas [7, 21, 18, 1, 19]. unlike pure language based qa sys-
tems that have been studied extensively in the nlp commu-
nity [28, 14, 4, 31, 3, 32], image qa systems are designed to
automatically answer natural language questions according
to the content of a reference image.

most of the recently proposed image qa models are
based on neural networks [7, 21, 18, 1, 19]. a commonly
used approach was to extract a global image feature vector
using a convolution neural network (id98) [15] and encode
the corresponding question as a feature vector using a long
short-term memory network (lstm) [9] and then combine
them to infer the answer. though impressive results have
been reported, these models often fail to give precise an-
swers when such answers are related to a set of    ne-grained
regions in an image.

by examining the image qa data sets, we    nd that it is
often that case that answering a question from an image re-
quires multi-step reasoning. take the question and image in
fig. 1 as an example. there are several objects in the im-
age: bicycles, window, street, baskets and

(a) stacked attention network for image qa

(b) visualization of the learned multiple attention layers. the
stacked attention network    rst focuses on all referred concepts,
e.g., bicycle, basket and objects in the basket (dogs) in
the    rst attention layer and then further narrows down the focus in
the second layer and    nds out the answer dog.

figure 1: model architecture and visualization

dogs. to answer the question what are sitting in
the basket on a bicycle, we need to    rst locate
those objects (e.g. basket, bicycle) and concepts
(e.g., sitting in) referred in the question, then gradu-
ally rule out irrelevant objects, and    nally pinpoint to the re-
gion that are most indicative to infer the answer (i.e., dogs
in the example).

in this paper, we propose stacked attention networks
(sans) that allow multi-step reasoning for image qa.
sans can be viewed as an extension of the attention mech-
anism that has been successfully applied in image caption-
ing [30] and machine translation [2]. the overall architec-
ture of san is illustrated in fig. 1a. the san consists of
three major components: (1) the image model, which uses

1

question:what are sitting in the basket on a bicycle?id98/lstmsoftmaxdogsanswer:id98+query+attention layer 1attention layer 2feature vectors of di   erentparts of imageoriginal imagefirst attention layersecond attention layera id98 to extract high level image representations, e.g. one
vector for each region of the image; (2) the question model,
which uses a id98 or a lstm to extract a semantic vector
of the question and (3) the stacked attention model, which
locates, via multi-step reasoning, the image regions that are
relevant to the question for answer prediction. as illustrated
in fig. 1a, the san    rst uses the question vector to query
the image vectors in the    rst visual attention layer, then
combine the question vector and the retrieved image vectors
to form a re   ned query vector to query the image vectors
again in the second attention layer. the higher-level atten-
tion layer gives a sharper attention distribution focusing on
the regions that are more relevant to the answer. finally, we
combine the image features from the highest attention layer
with the last query vector to predict the answer.

the main contributions of our work are three-fold. first,
we propose a stacked attention network for image qa tasks.
second, we perform comprehensive evaluations on four
image qa benchmarks, demonstrating that the proposed
multiple-layer san outperforms previous state-of-the-art
approaches by a substantial margin. third, we perform a
detailed analysis where we visualize the outputs of differ-
ent attention layers of the san and demonstrate the process
that the san takes multiple steps to progressively focus the
attention on the relevant visual clues that lead to the answer.
2. related work

image qa is closely related to image captioning [5, 30,
6, 27, 12, 10, 20]. in [27], the system    rst extracted a high
level image feature vector from googlenet and then fed it
into a lstm to generate captions. the method proposed in
[30] went one step further to use an attention mechanism in
the id134 process. different from [30, 27], the
approach proposed in [6]    rst used a id98 to detect words
given the images, then used a maximum id178 language
model to generate a list of caption candidates, and    nally
used a deep multimodal similarity model (dmsm) to re-
rank the candidates. instead of using a id56 or a lstm,
the dmsm uses a id98 to model the semantics of captions.
unlike image captioning, in image qa, the question is
given and the task is to learn the relevant visual and text rep-
resentation to infer the answer. in order to facilitate the re-
search of image qa, several data sets have been constructed
in [19, 21, 7, 1] either through automatic generation based
on image caption data or by human labeling of questions
and answers given images. among them, the image qa
data set in [21] is generated based on the coco caption
data set. given a sentence that describes an image, the au-
thors    rst used a parser to parse the sentence, then replaced
the key word in the sentence using question words and the
key word became the answer. [7] created an image qa data
set through human labeling. the initial version was in chi-
nese and then was translated to english. [1] also created an

image qa data set through human labeling. they collected
questions and answers not only for real images, but also for
abstract scenes.

several image qa models were proposed in the litera-
ture. [18] used semantic parsers and image segmentation
methods to predict answers based on images and questions.
[19, 7] both used encoder-decoder framework to generate
answers given images and questions. they    rst used a
lstm to encoder the images and questions and then used
another lstm to decode the answers. they both fed the
image feature to every lstm cell.
[21] proposed sev-
eral neural network based models, including the encoder-
decoder based models that use single direction lstms and
bi-direction lstms, respectively. however, the authors
found the concatenation of image features and bag of words
features worked the best. [1]    rst encoded questions with
lstms and then combined question vectors with image
vectors by element wise multiplication. [17] used a id98
for question modeling and used convolution operations to
combine question vectors and image feature vectors. we
compare the san with these models in sec. 4.

to the best of our knowledge, the attention mechanism,
which has been proved very successful in image captioning,
has not been explored for image qa. the san adapt the at-
tention mechanism to image qa, and can be viewed as a
signi   cant extension to previous models [30] in that multi-
ple attention layers are used to support multi-step reasoning
for the image qa task.
3. stacked attention networks (sans)

the overall architecture of the san is shown in fig. 1a.
we describe the three major components of san in this sec-
tion: the image model, the question model, and the stacked
attention model.
3.1. image model

the image model uses a id98 [13, 23, 26] to get the
representation of images. speci   cally, the vggnet [23] is
used to extract the image feature map fi from a raw image
i:

figure 2: id98 based image model

fi = id98vgg(i).

(1)

unlike previous studies [21, 17, 7] that use features from the
last inner product layer, we choose the features fi from the
last pooling layer, which retains spatial information of the
original images. we    rst rescale the images to be 448   448

image4484485121414feature mappixels, and then take the features from the last pooling layer,
which therefore have a dimension of 512  14  14, as shown
in fig. 2. 14   14 is the number of regions in the image and
512 is the dimension of the feature vector for each region.
accordingly, each feature vector in fi corresponds to a 32  
32 pixel region of the input images. we denote by fi, i    
[0, 195] the feature vector of each image region.

then for modeling convenience, we use a single layer
id88 to transform each feature vector to a new vec-
tor that has the same dimension as the question vector (de-
scribed in sec. 3.2):

vi = tanh(wi fi + bi ),

(2)

where vi is a matrix and its i-th column vi is the visual
feature vector for the region indexed by i.
3.2. question model

as [25, 22, 6] show that lstms and id98s are powerful
to capture the semantic meaning of texts, we explore both
models for question representations in this study.

3.2.1 lstm based question model

figure 3: lstm based question model

the essential structure of a lstm unit is a memory cell
ct which reserves the state of a sequence. at each step,
the lstm unit takes one input vector (word vector in our
case) xt and updates the memory cell ct, then output a hid-
den state ht. the update process uses the gate mechanism.
a forget gate ft controls how much information from past
state ct   1 is preserved. an input gate it controls how much
the current input xt updates the memory cell. an output
gate ot controls how much information of the memory is
fed to the output as hidden state. the detailed update pro-
cess is as follows:

it =  (wxixt + whiht   1 + bi),
ft =  (wxf xt + whf ht   1 + bf ),
ot =  (wxoxt + whoht   1 + bo),
ct =ftct   1 + it tanh(wxcxt + whcht   1 + bc),
ht =ot tanh(ct),

(3)
(4)
(5)
(6)
(7)

where i, f, o, c are input gate, forget gate, output gate and
memory cell, respectively. the weight matrix and bias are
parameters of the lstm and are learned on training data.

given the question q = [q1, ...qt ], where qt is the one hot
vector representation of word at position t, we    rst embed
the words to a vector space through an embedding matrix
xt = weqt. then for every time step, we feed the embed-
ding vector of words in the question to lstm:

xt =weqt, t     {1, 2, ...t},
ht =lstm(xt), t     {1, 2, ...t}.

(8)
(9)

as shown in fig. 3, the question what are sitting
in the basket on a bicycle is
fed into the
lstm. then the    nal hidden layer is taken as the repre-
sentation vector for the question, i.e., vq = ht .

3.2.2 id98 based question model

figure 4: id98 based question model

in this study, we also explore to use a id98 similar
to [11] for question representation. similar to the lstm-
based question model, we    rst embed words to vectors
xt = weqt and get the question vector by concatenating
the word vectors:

x1:t = [x1, x2, ..., xt ].

(10)

then we apply convolution operation on the word embed-
ding vectors. we use three convolution    lters, which have
the size of one (unigram), two (bigram) and three (trigram)
respectively. the t-th convolution output using window size
c is given by:

hc,t = tanh(wcxt:t+c   1 + bc).

(11)
the    lter is applied only to window t : t + c     1 of size c.
wc is the convolution weight and bc is the bias. the feature
map of the    lter with convolution size c is given by:

hc = [hc,1, hc,2, ..., hc,t   c+1].

(12)

then we apply max-pooling over the feature maps of the

convolution size c and denote it as

  hc = max

[hc,1, hc,2, ..., hc,t   c+1].

t

(13)

lstmlstmlstm   whatarebicyclewewewequestion:      unigrambigramtrigrammax pooling over timeconvolutionwhataresittingbicycle   question:embeddingthe max-pooling over these vectors is a coordinate-wise
max operation. for convolution feature maps of different
sizes c = 1, 2, 3, we concatenate them to form the feature
representation vector of the whole question sentence:

potential answer:

(cid:88)

i

pivi,

  vi =

u =  vi + vq.

(17)

(18)

h = [  h1,   h2,   h3],

(14)

hence vq = h is the id98 based question vector.

the diagram of id98 model for question is shown in
fig. 4. the convolutional and pooling layers for unigrams,
bigrams and trigrams are drawn in red, blue and orange, re-
spectively.

3.3. stacked attention networks

given the image feature matrix vi and the question fea-
ture vector vq, san predicts the answer via multi-step rea-
soning.

in many cases, an answer only related to a small region
of an image. for example, in fig. 1b, although there are
multiple objects in the image: bicycles, baskets,
window, street and dogs and the answer to the ques-
tion only relates to dogs. therefore, using the one global
image feature vector to predict the answer could lead to sub-
optimal results due to the noises introduced from regions
that are irrelevant to the potential answer. instead, reason-
ing via multiple attention layers progressively, the san are
able to gradually    lter out noises and pinpoint the regions
that are highly relevant to the answer.

given the image feature matrix vi and the question vec-
tor vq, we    rst feed them through a single layer neural net-
work and then a softmax function to generate the attention
distribution over the regions of the image:

ha = tanh(wi,avi     (wq,avq + ba)),
pi =softmax(wp ha + bp ),

(15)
(16)
where vi     rd  m, d is the image representation dimen-
sion and m is the number of image regions, vq     rd is a
d dimensional vector. suppose wi,a, wq,a     rk  d and
wp     r1  k, then pi     rm is an m dimensional vector,
which corresponds to the attention id203 of each im-
age region given vq. note that we denote by     the addition
of a matrix and a vector. since wi,avi     rk  m and both
wq,avq, ba     rk are vectors, the addition between a ma-
trix and a vector is performed by adding each column of the
matrix by the vector.

based on the attention distribution, we calculate the
weighted sum of the image vectors, each from a region,   vi
as in eq. 17. we then combine   vi with the question vec-
tor vq to form a re   ned query vector u as in eq. 18. u is
regarded as a re   ned query since it encodes both question
information and the visual information that is relevant to the

compared to models that simply combine the ques-
tion vector and the global image vector, attention mod-
els construct a more informative u since higher weights
are put on the visual regions that are more relevant to
the question. however, for complicated questions, a sin-
gle attention layer is not suf   cient to locate the correct
region for answer prediction. for example, the question
in fig. 1 what are sitting in the basket on
a bicycle refers to some subtle relationships among
multiple objects in an image. therefore, we iterate the
above query-attention process using multiple attention lay-
ers, each extracting more    ne-grained visual attention infor-
mation for answer prediction. formally, the sans take the
following formula: for the k-th attention layer, we compute:

hk
a = tanh(w k
i =softmax(w k
pk

i,avi     (w k
q,auk   1 + bk
a + bk
p ).

p hk

a)),

(19)
(20)

where u0 is initialized to be vq. then the aggregated image
feature vector is added to the previous query vector to form
a new query vector:

(cid:88)
pk
i vi,
i + uk   1.

  vk
i =

i
uk =  vk

(21)

(22)

that is, in every layer, we use the combined question
and image vector uk   1 as the query for the image. after the
image region is picked, we update the new query vector as
i + uk   1. we repeat this k times and then use the
uk =   vk
   nal uk to infer the answer:

pans =softmax(wuuk + bu).

(23)

fig. 1b illustrates the reasoning process by an exam-
ple. in the    rst attention layer, the model identi   es roughly
the area that are relevant to basket, bicycle, and
sitting in. in the second attention layer, the model fo-
cuses more sharply on the region that corresponds to the
answer dogs. more examples can be found in sec. 4.

4. experiments
4.1. data sets

we evaluate the san on four image qa data sets.
daquar-all is proposed in [18]. there are 6, 795
training questions and 5, 673 test questions. these ques-
tions are generated on 795 and 654 images respectively. the

a

images are mainly indoor scenes. the questions are catego-
rized into three types including object, color and number.
most of the answers are single words. following the setting
in [21, 17, 19], we exclude data samples that have multiple
words answers. the remaining data set covers 90% of the
original data set.

daquar-reduced is

reduced version of
daquar-all. there are 3, 876 training samples and
297 test samples. this data set is constrained to 37 object
categories and uses only 25 test images. the single word
answers data set covers 98% of the original data set.

coco-qa is proposed in [21]. based on the microsoft
coco data set, the authors    rst parse the caption of the im-
age with an off-the-shelf parser, then replace the key com-
ponents in the caption with question words for form ques-
tions. there are 78736 training samples and 38948 test sam-
ples in the data set. these questions are based on 8, 000 and
4, 000 images respectively. there are four types of ques-
tions including object, number, color, and location. each
type takes 70%, 7%, 17%, and 6% of the whole data set,
respectively. all answers in this data set are single word.

vqa is created through human labeling [1]. the data
set uses images in the coco image caption data set [16].
unlike the other data sets, for each image, there are three
questions and for each question, there are ten answers la-
beled by human annotators. there are 248, 349 training
questions and 121, 512 validation questions in the data set.
following [1], we use the top 1000 most frequent answer
as possible outputs and this set of answers covers 82.67%
of all answers. we    rst studied the performance of the pro-
posed model on the validation set. following [6], we split
the validation data set into two halves, val1 and val2. we
use training set and val1 to train and validate and val2 to
test locally. the results on the val2 set are reported in ta-
ble. 6. we also evaluated the best model, san(2, id98),
on the standard test server as provided in [1] and report the
results in table. 5.

4.2. baselines and evaluation methods

we compare our models with a set of baselines proposed
recently [21, 1, 18, 19, 17] on image qa. since the results
of these baselines are reported on different data sets in dif-
ferent literature, we present the experimental results on dif-
ferent data sets in different tables.

for all four data sets, we formulate image qa as a clas-
si   cation problem since most of answers are single words.
we evaluate the model using classi   cation accuracy as re-
ported in [1, 21, 19]. the reference models also report the
wu-palmer similarity (wups) measure [29]. the wups
measure calculates the similarity between two words based
on their longest common subsequence in the taxonomy tree.
we can set a threshold for wups, if the similarity is less
than the threshold, then it is zeroed out. following the refer-

ence models, we use wups0.9 and wups0.0 as evaluation
metrics besides the classi   cation accuracy. the evaluation
on the vqa data set is different from other three data sets,
since for each question there are ten answer labels that may
or may not be the same. we follow [1] to use the following
metric: min(# human labels that match that answer/3, 1),
which basically gives full credit to the answer when three
or more of the ten human labels match the answer and gives
partial credit if there are less matches.
4.3. model con   guration and training

for the image model, we use the vggnet to extract fea-
tures. when training the san, the parameter set of the id98
of the vggnet is    xed. we take the output from the last
pooling layer as our image feature which has a dimension
of 512    14    14 .

for daquar and coco-qa, we set the word embed-
ding dimension and lstm   s dimension to be 500 in the
question model. for the id98 based question model, we
set the unigram, bigram and trigram convolution    lter size
to be 128, 256, 256 respectively. the combination of these
   lters makes the question vector size to be 640. for vqa
dataset, since it is larger than other data sets, we double the
model size of the lstm and the id98 to accommodate the
large data set and the large number of classes. in evaluation,
we experiment with san with one and two attention layers.
we    nd that using three or more attention layers does not
further improve the performance.

in our experiments, all the models are trained using
stochastic id119 with momentum 0.9. the batch
size is    xed to be 100. the best learning rate is picked
using grid search. gradient clipping technique [8] and
dropout [24] are used.
4.4. results and analysis

the experimental results on daquar-all, daquar-
reduced, coco-qa and vqa are presented in table. 1
to 6 respectively. our model names explain their settings:
san is short for the proposed stacked attention networks,
the value 1 or 2 in the brackets refer to using one or two
attention layers, respectively. the keyword lstm or id98
refers to the question model that sans use.

the experimental results in table. 1 to 6 show that
the two-layer san gives the best results across all data
sets and the two kinds of question models in the san,
lstm and id98, give similar performance. for example,
on daquar-all (table. 1), both of the proposed two-
layer sans outperform the two best baselines, the img-
id98 in [17] and the ask-your-neuron in [19], by 5.9%
and 7.6% absolute in accuracy, respectively. similar range
of improvements are observed in metrics of wups0.9 and
wups0.0. we also observe signi   cant improvements on
daquar-reduced (table. 2), i.e., our san(2, lstm)

accuracy wups0.9 wups0.0

table 1: daquar-all results, in percentage

accuracy wups0.9 wups0.0

7.9

19.1
21.7

methods
multi-world: [18]
multi-world
ask-your-neurons: [19]
language
language + img
id98: [17]
img-id98
ours:
san(1, lstm)
san(1, id98)
san(2, lstm)
san(2, id98)
human :[18]
human

28.9
29.2
29.3
29.3

23.4

50.2

12.7

31.7
34.7

methods
multi-world: [18]
multi-world
ask-your-neurons: [19]
language
language + img
vse: [21]
guess
bow
lstm
img+bow
vis+lstm
2-vis+blstm
id98: [17]
img-id98
ours:
san(1, lstm)
san(1, id98)
san(2, lstm)
san(2, id98)
human :[18]
human

18.2
32.7
32.7
34.2
34.4
35.8

45.2
45.2
46.2
45.5

60.3

39.7

11.9

25.2
28.0

29.6

34.7
35.1
34.9
35.1

50.8

38.8

65.1
65.0

63.0

68.5
67.8
68.1
68.6

67.3

18.2

38.4
40.8

29.7
43.2
43.5
45.0
46.1
46.8

44.9

49.6
49.6
51.2
50.2

61.0

51.5

80.1
79.5

77.6
81.3
81.6
81.5
82.2
82.2

83.1

84.0
83.7
85.1
83.6

79.0

table 2: daquar-reduced results, in percentage

outperforms the img-id98 [17], the 2-vis+blstm [21],
the ask-your-neurons approach [19] and the multi-world
[18] by 6.5%, 10.4%, 11.5% and 33.5% absolute in accu-
racy, respectively. on the larger coco-qa data set, the
proposed two-layer sans signi   cantly outperform the best
baselines from [17] (img-id98) and [21] (img+bow and
2-vis+blstm) by 5.1% and 6.6% in accuracy (table. 3).

methods
vse: [21]
guess
bow
lstm
img
img+bow
vis+lstm
2-vis+blstm
id98: [17]
img-id98
id98
ours:
san(1, lstm)
san(1, id98)
san(2, lstm)
san(2, id98)

accuracy wups0.9 wups0.0

6.7
37.5
36.8
43.0
55.9
53.3
55.1

55.0
32.7

59.6
60.7
61.0
61.6

17.4
48.5
47.6
58.6
66.8
63.9
65.3

65.4
44.3

69.6
70.6
71.0
71.6

73.4
82.8
82.3
85.9
89.0
88.3
88.6

88.6
80.9

90.1
90.5
90.7
90.9

table 3: coco-qa results, in percentage

methods
vse: [21]
guess
bow
lstm
img
img+bow
vis+lstm
2-vis+blstm
ours:
san(1, lstm)
san(1, id98)
san(2, lstm)
san(2, id98)

objects number color location

2.1
37.3
35.9
40.4
58.7
56.5
58.2

62.5
63.6
63.6
64.5

35.8
43.6
45.3
29.3
44.1
46.1
44.8

49.0
48.7
49.8
48.6

13.9
34.8
36.3
42.7
52.0
45.9
49.5

54.8
56.7
57.9
57.9

8.9
40.8
38.4
44.2
49.4
45.5
47.3

51.6
52.7
52.8
54.0

table 4: coco-qa accuracy per class, in percentage

test-dev

test-std

methods
vqa: [1]
question
image
q+i
lstm q
lstm q+i
san(2, id98)

all

yes/no number other

all

48.1
28.1
52.6
48.8
53.7
58.7

75.7
64.0
75.6
78.2
78.9
79.3

36.7
0.4
33.7
35.7
35.2
36.6

27.1
3.8
37.4
26.6
36.4
46.1

-
-
-
-

54.1
58.9

table 5: vqa results on the of   cial server, in percentage

table. 5 summarizes the performance of various models on
vqa, which is the largest among the four data sets. the
overall results show that our best model, san(2, id98),

methods
san(1, lstm)
san(1, id98)
san(2, lstm)
san(2, id98)

all
56.6
56.9
57.3
57.6

yes/no
36%
78.1
78.8
78.3
78.6

number

10%
41.6
42.0
42.2
41.8

other
54%
44.8
45.0
45.9
46.4

table 6: vqa results on our partition, in percentage

outperforms the lstm q+i model, the best baseline from
[1], by 4.8% absolute. the superior performance of the
sans across all four benchmarks demonstrate the effective-
ness of using multiple layers of attention.

in order to study the strength and weakness of the san
in detail, we report performance at the question-type level
on the two large data sets, coco-qa and vqa, in ta-
ble. 4 and 5, respectively. we observe that on coco-
qa, compared to the two best baselines, img+bow and
2-vis+blstm, out best model san(2, id98) improves
7.2% in the question type of color, followed by 6.1% in
objects, 5.7% in location and 4.2% in number. we ob-
serve similar trend of improvements on vqa. as shown
in table. 5, compared to the best baseline lstm q+i, the
biggest improvement of san(2, id98) is in the other type,
9.7%, followed by the 1.4% improvement in number and
0.4% improvement in yes/no. note that the other type in
vqa refers to questions that usually have the form of    what
color, what kind, what are, what type, where    etc., which
are similar to question types of color, objects and loca-
tion in coco-qa. the vqa data set has a special yes/no
type of questions. the san only improves the performance
of this type of questions slightly. this could due to that the
answer for a yes/no question is very question dependent, so
better modeling of the visual information does not provide
much additional gains. this also con   rms the similar ob-
servation reported in [1], e.g., using additional image infor-
mation only slightly improves the performance in yes/no,
as shown in table. 5, q+i vs question, and lstm q+i vs
lstm q.

our results demonstrate clearly the positive impact of
using multiple attention layers. in all four data sets, two-
layer sans always perform better than the one-layer san.
speci   cally, on coco-qa, on average the two-layer sans
outperform the one-layer sans by 2.2% in the type of
color, followed by 1.3% and 1.0% in the location and ob-
jects categories, and then 0.4% in number. this aligns to
the order of the improvements of the san over baselines.
similar trends are observed on vqa (table. 6), e.g., the
two-layer san improve over the one-layer san by 1.4%
for the other type of question, followed by 0.2% improve-
ment for number, and    at for yes/no.

4.5. visualization of attention layers

in this section, we present analysis to demonstrate that
using multiple attention layers to perform multi-step rea-
soning leads to more    ne-grained attention layer-by-layer
in locating the regions that are relevant to the potential an-
swers. we do so by visualizing the outputs of the atten-
tion layers of a sample set of images from the coco-qa
test set. note the attention id203 distribution is of size
14    14 and the original image is 448    448, we up-sample
the attention id203 distribution and apply a gaussian
   lter to make it the same size as the original image.

fig. 5 presents six examples. more examples are pre-
sented in the appendix. they cover types as broad as object,
numbers, color and location. for each example, the three
images from left to right are the original image, the output
of the    rst attention layer and the output of the second at-
tention layer, respectively. the bright part of the image is
the detected attention. across all those examples, we see
that in the    rst attention layer, the attention is scattered on
many objects in the image, largely corresponds to the ob-
jects and concepts referred in the question, whereas in the
second layer, the attention is far more focused on the re-
gions that lead to the correct answer. for example, consider
the question what is the color of the horns,
which asks the color of the horn on the woman   s head in
fig. 5(f). in the output of the    rst attention layer, the model
   rst recognizes a woman in the image. in the output of the
second attention layer, the attention is focused on the head
of the woman, which leads to the answer of the question:
the color of the horn is red.
4.6. errors analysis

we randomly sample 100 images from the coco-qa
test set that the san make mistakes. we group the errors
into four categories: (i) the sans focus the attention on the
wrong regions (22%), e.g., the example in fig. 6(a); (ii) the
sans focus on the right region but predict a wrong answer
(42%), e.g., the examples in fig. 6(b)(c)(d); (iii) the answer
is ambiguous, the sans give answers that are different from
labels, but might be acceptable (31%). e.g., in fig. 6(e), the
answer label is pot, but out model predicts vase, which
is also visually reasonable; (iv) the labels are clearly wrong
(5%). e.g., in fig. 6(f), our model gives the correct answer
trains while the label cars is wrong.
5. conclusion

in this paper, we propose a new stacked attention net-
work (san) for image qa. san uses a multiple-layer at-
tention mechanism that queries an image multiple times to
locate the relevant visual region and to infer the answer pro-
gressively. experimental results demonstrate that the pro-
posed san signi   cantly outperforms previous state-of-the-
art approaches by a substantial margin on all four image qa

figure 5: visualization of two attention layers

figure 6: examples of mistakes

data sets. the visualization of the attention layers further il-
lustrates the process that the san focuses the attention to
the relevant visual clues that lead to the answer of the ques-
tion layer-by-layer.

references
[1] s. antol, a. agrawal, j. lu, m. mitchell, d. batra, c. l.
zitnick, and d. parikh. vqa: visual id53.
arxiv preprint arxiv:1505.00468, 2015. 1, 2, 5, 6, 7

[2] d. bahdanau, k. cho, and y. bengio. neural machine
translation by jointly learning to align and translate. arxiv
preprint arxiv:1409.0473, 2014. 1

[3] j. berant and p. liang. id29 via id141.

in proceedings of acl, volume 7, page 92, 2014. 1

[4] a. bordes, s. chopra, and j. weston. id53
with subgraph embeddings. arxiv preprint arxiv:1406.3676,
2014. 1

[5] x. chen and c. l. zitnick. learning a recurrent visual rep-
arxiv preprint

resentation for image id134.
arxiv:1411.5654, 2014. 2

[6] h. fang, s. gupta, f. iandola, r. srivastava, l. deng,
p. doll  ar, j. gao, x. he, m. mitchell, j. platt, et al.
from captions to visual concepts and back. arxiv preprint
arxiv:1411.4952, 2014. 2, 3, 5

[7] h. gao, j. mao, j. zhou, z. huang, l. wang, and w. xu.
are you talking to a machine? dataset and methods for
arxiv preprint
multilingual
arxiv:1505.05612, 2015. 1, 2

image id53.

[8] a. graves. generating sequences with recurrent neural net-

works. arxiv preprint arxiv:1308.0850, 2013. 5

[9] s. hochreiter and j. schmidhuber. long short-term memory.

neural computation, 9(8):1735   1780, 1997. 1

[10] a. karpathy and l. fei-fei. deep visual-semantic align-
arxiv preprint

ments for generating image descriptions.
arxiv:1412.2306, 2014. 2

[11] y. kim. convolutional neural networks for sentence classi   -

cation. arxiv preprint arxiv:1408.5882, 2014. 3

[12] r. kiros, r. salakhutdinov, and r. s. zemel. unifying
visual-semantic embeddings with multimodal neural lan-
guage models. arxiv preprint arxiv:1411.2539, 2014. 2

[13] a. krizhevsky, i. sutskever, and g. e. hinton.

id163
classi   cation with deep convolutional neural networks.
in
advances in neural information processing systems, pages
1097   1105, 2012. 2

[14] a. kumar, o. irsoy, j. su, j. bradbury, r. english, b. pierce,
p. ondruska, i. gulrajani, and r. socher. ask me anything:
dynamic memory networks for natural language processing.
arxiv preprint arxiv:1506.07285, 2015. 1

[15] y. lecun, l. bottou, y. bengio, and p. haffner. gradient-
based learning applied to document recognition. proceed-
ings of the ieee, 86(11):2278   2324, 1998. 1

[16] t.-y. lin, m. maire, s. belongie, j. hays, p. perona, d. ra-
manan, p. doll  ar, and c. l. zitnick. microsoft coco: com-
in id161   eccv 2014,
mon objects in context.
pages 740   755. springer, 2014. 5

[17] l. ma, z. lu, and h. li. learning to answer questions from
image using convolutional neural network. arxiv preprint
arxiv:1506.00333, 2015. 2, 5, 6

[18] m. malinowski and m. fritz. a multi-world approach to
id53 about real-world scenes based on uncer-
in advances in neural information processing
tain input.
systems, pages 1682   1690, 2014. 1, 2, 4, 5, 6

[19] m. malinowski, m. rohrbach, and m. fritz. ask your neu-
rons: a neural-based approach to answering questions about
images. arxiv preprint arxiv:1505.01121, 2015. 1, 2, 5, 6

[20] j. mao, w. xu, y. yang, j. wang, and a. yuille. deep cap-
tioning with multimodal recurrent neural networks (m-id56).
arxiv preprint arxiv:1412.6632, 2014. 2

[21] m. ren, r. kiros, and r. zemel.

and data for image id53.
arxiv:1505.02074, 2015. 1, 2, 5, 6

exploring models
arxiv preprint

[22] y. shen, x. he, j. gao, l. deng, and g. mesnil. a latent
semantic model with convolutional-pooling structure for in-
formation retrieval. in proceedings of the 23rd acm interna-
tional conference on conference on information and knowl-
edge management, pages 101   110. acm, 2014. 3

[23] k. simonyan and a. zisserman. very deep convolutional
networks for large-scale image recognition. arxiv preprint
arxiv:1409.1556, 2014. 2

[24] n. srivastava, g. hinton, a. krizhevsky, i. sutskever, and
r. salakhutdinov. dropout: a simple way to prevent neural
networks from over   tting. the journal of machine learning
research, 15(1):1929   1958, 2014. 5

[25] i. sutskever, o. vinyals, and q. v. le. sequence to sequence
learning with neural networks. in advances in neural infor-
mation processing systems, pages 3104   3112, 2014. 3

[26] c. szegedy, w. liu, y. jia, p. sermanet, s. reed,
d. anguelov, d. erhan, v. vanhoucke, and a. rabi-
novich. going deeper with convolutions. arxiv preprint
arxiv:1409.4842, 2014. 2

[27] o. vinyals, a. toshev, s. bengio, and d. erhan. show
and tell: a neural image caption generator. arxiv preprint
arxiv:1411.4555, 2014. 2

[28] j. weston, s. chopra, and a. bordes. memory networks.

arxiv preprint arxiv:1410.3916, 2014. 1

[29] z. wu and m. palmer. verbs semantics and lexical selection.
in proceedings of the 32nd annual meeting on association
for computational linguistics, pages 133   138. association
for computational linguistics, 1994. 5

[30] k. xu, j. ba, r. kiros, a. courville, r. salakhutdinov,
r. zemel, and y. bengio. show, attend and tell: neural im-
age id134 with visual attention. arxiv preprint
arxiv:1502.03044, 2015. 1, 2

[31] w.-t. yih, m.-w. chang, x. he, and j. gao. semantic pars-
ing via staged query graph generation: id53
with knowledge base. in proceedings of the joint conference
of the 53rd annual meeting of the acl and the 7th interna-
tional joint conference on natural language processing of
the afnlp, 2015. 1

[32] w.-t. yih, x. he, and c. meek. id29 for single-
relation id53. in proceedings of acl, 2014.
1

figure 7: more examples

