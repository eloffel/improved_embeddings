li yao et al.: oracle performance for visual captioning

1

oracle performance for visual captioning

1 universit   de montr  al
2 ibm t.j. watson research
3 new york university

6
1
0
2

 

p
e
s
4
1

 

 
 
]

v
c
.
s
c
[
 
 

li yao1
li.yao@umontreal.ca
nicolas ballas1
nicolas.ballas@umontreal.ca
kyunghyun cho3
kyunghyun.cho@nyu.edu
john r. smith2
jsmith@us.ibm.com
yoshua bengio1
yoshua.bengio@umontreal.ca

5
v
0
9
5
4
0

.

1
1
5
1
:
v
i
x
r
a

abstract

the task of associating images and videos with a natural language description has
attracted a great amount of attention recently. the state-of-the-art results on some of
the standard datasets have been pushed into the regime where it has become more and
more dif   cult to make signi   cant improvements. instead of proposing new models, this
work investigates performances that an oracle can obtain. in order to disentangle the
contribution from visual model from the language model, our oracle assumes that high-
quality visual concept extractor is available and focuses only on the language part. we
demonstrate the construction of such oracles on ms-coco, youtube2text and lsmdc
(a combination of m-vad and mpii-md). surprisingly, despite the simplicity of the
model and the training procedure, we show that current state-of-the-art models fall short
when being compared with the learned oracle. furthermore, it suggests the inability of
current models in capturing important visual concepts in captioning tasks.

1

introduction

with standard datasets publicly available, such as coco and flickr [15, 22, 38] in im-
age captioning, and youtube2text, mvad and mpi-md [12, 27, 30] in video caption-
ing, the    eld has been progressing in an astonishing speed. for instance, the state-of-the-
art results on coco image captioning has been improved rapidly from 0.17 to 0.31 in
id7 [3, 9, 10, 17, 18, 23, 25, 34, 36]. similarly, the benchmark on youtube2text has
been repeatedly pushed from 0.31 to 0.50 in id7 score [1, 26, 28, 32, 33, 35, 37, 39].
while obtaining encouraging results, captioning approaches involve large networks, usually
leveraging convolution network for the visual part and recurrent network for the language
side. it therefore results model with a certain complexity where the contribution of the dif-
ferent component is not clear.

instead of proposing better models, the main objective of this work is to develop a method
that offers a deeper insight of the strength and the weakness of popular visual captioning
c(cid:13) 2016. the copyright of this document resides with its authors.
it may be distributed unchanged freely in print or electronic forms.

2

li yao et al.: oracle performance for visual captioning

models. in particular, we propose a trainable oracle that disentangles the contribution of the
visual model from the language model. to obtain such oracle, we follow the assumption that
the image and video captioning task may be solved with two steps [11, 28] . consider the
model p(w|v) where v refers to usually high dimensional visual inputs, such as representa-
tions of an image or a video, and w refers to a caption, usually a sentence of natural language
description. in order to work well, p(w|v) needs to form higher level visual concept, either
explicitly or implicitly, based on v in the    rst step, denoted as p(a|v), followed by a lan-
guage model that transforms visual concept into a legitimate sentence, denoted by p(w|a). a
referes to atoms that are visually perceivable from v.

the above assumption suggests an alternative way to build an oracle. in particular, we
assume the    rst step is close to perfect in the sense that visual concept (or hints) is observed
with almost 100% accuracy. and then we train the best language model conditioned on hints
to produce captions.

using the proposed oracle, we compare the current state-of-the-art models against it,
which helps to quantify their capacity of visual modeling, a major weakness, apart from
the strong id38. in addition, when being applied on different datasets, the
oracle offers insight on the intrinsic dif   culty and blessing of them, a general guideline when
designing new algorithms and developing new models. finally, we also relax the assumption
to investigate the case where visual concept may not be realistically predicted with 100%
accuracy and demonstrate a quantity-accuracy trade-off in solving visual captioning tasks.

2 related work

visual captioning the problem of image captioning has attracted a great amount of atten-
tion lately. early work focused on constructing linguistic templates or syntactic trees based
on a set of concept from visual inputs [20, 21, 24]. another popular approach is based on
caption retrieval in the embedding space such as devlin et al. [9], kiros et al. [18]. most re-
cently, the use of language models conditioned on visual inputs have been widely studied in
the work of fang et al. [11] where a maximum id178 language model is used and in don-
ahue et al. [10], karpathy and fei-fei [17], mao et al. [23], vinyals et al. [34], xu et al. [36]
where recurrent neural network based models are built to generate natural language descrip-
tions. the work of devlin et al. [8] advocates to combine both types of language models.
furthermore, cider [31] was proposed as an alternative evaluation metric for image caption-
ing and is shown to be more advantageous compared with id7 and meteor. to further
improve the performance, bengio et al. [3] suggests a simple sampling algorithm during
training, which was one of the winning recipes for msr-coco captioning challenge 1, and
jia et al. [16] suggests the use of extra semantic information to guide the language generation
process.

similarly, video captioning has made substantial progress recently. early models such
as barbu et al. [2], kojima et al. [19], rohrbach et al. [28] tend to focus on constrained do-
mains with limited appearance of activities and objects in videos. they also rely heavily
on hand-crafted video features, followed by a template-based or shallow statistical machine
translation approaches to produce captions. borrowing success from image captioning, re-
cent models such as donahue et al. [10], rohrbach et al. [26], venugopalan et al. [32, 33], xu
et al. [35], yao et al. [37], yu et al. [39] and most recently ballas et al. [1] have adopted a

1http://mscoco.org

li yao et al.: oracle performance for visual captioning

3

more general encoder-decoder approach with end-to-end parameter tuning. videos are input
into a speci   c variant of encoding neural networks to form a higher level visual summary,
followed by a caption decoder by recurrent neural networks. training such type of models
are possible with the availability of three relatively large scale datasets, one collected from
youtube by guadarrama et al. [12], the other two constructed based on descriptive video
service (dvs) on movies by torabi et al. [30] and rohrbach et al. [27]. the latter two have
recently been combined as the of   cial dataset for large scale movie description challenge
(lsmdc) 2.

capturing higher-level visual concept the idea of using intermediate visual concept to
guide the id134 has been discussed in qi wu et al. [25] in the context of image
captioning and in rohrbach et al. [26] for video captioning. both work trained classi   ers
on a prede   ned set of visual concepts, extracted from captions using heuristics from linguis-
tics and natural language processing. our work resembles both of them in the sense that
we also extract similar constituents from captions. the purpose of this study, however, is
different. by assuming perfect classi   ers on those visual atoms, we are able to establish the
performance upper bounds for a particular dataset. note that a simple bound is suggested
by rohrbach et al. [26] where meteor is measured on all the training captions against a
particular test caption. the largest score is picked as the upper bound. as a comparison, our
approach constructs a series of oracles that are trained to generate captions given different
number of visual hints. therefore, such bounds are clear indication of models    ability of
capturing concept within images and videos when performing id134, instead of
the one suggested by rohrbach et al. [26] that performs caption retrieval.

3 oracle model
the construction of the oracle is inspired by the observation that p(w|v) =    a p   (w|a)p(a|v)
where w = {w1, . . . ,wt} denotes a caption containing a sequence of words having a length
t. v denotes the visual inputs such as an image or a video. a denotes visual concepts which
we call    atoms   . we have explicitly factorized the captioning model p(w|v) into two parts,
p(w|a), which we call the conditional language model given atoms, and p(a|v), which we
call conditional atom model given visual inputs. to establish the oracle, we assume that the
atom model is given, which amounts to treat p(a|v) as a dirac delta function that assigns all
the id203 mass to the observed atom a. in other words, p(w|v) = p   (w|a).

therefore, with the fully observed a, the task of image and video captioning reduces
to the task of id38 conditioned on atoms. this is arguably a much easier
task compared with the direct modeling of p(w|v), therefore a well-trained model could
be treated as a performance oracle of it. information contained in a directly in   uences the
dif   culty of modeling p   (w|a). for instance, if no atoms are available, p   (w|a) reduces to
unconditional id38, which could be considered as a lower bound of p(w|v).
by increasing the amount of information a carries, the modeling of p   (w|a) becomes more
and more straightforward.

2https://goo.gl/2hj4lw

li yao et al.: oracle performance for visual captioning

4
3.1 oracle parameterization
given a set of atoms a(k) that summarize the visual concept appearing in the visual in-
puts v, this section describes the detailed parameterization of the model p   (w|a(k)) with
   denoting the overall parameters.
in particular, we adopt the commonly used encoder-
decoder framework [7] to model this conditional based on the following simple factorization
p   (w|a(k)) =    t
t=1 p   (wt|w<t ,a(k)).

recurrent neural networks (id56s) are natural choices when outputs are identi   ed as
sequences. we borrow the recent success from a variant of id56s called long-short term
memory networks (lstms)    rst introduced in hochreiter and schmidhuber [14], formulated
as the following

       p(wt | w<t ,a(k)))

       =   (ht   1,ct   1,wt   1,a(k)),

(1)

ht
ct

where ht and ct represent the id56 state and memory of lstms at timestep t respectively.
combined with the atom representation, equ. (1) is implemented as following

ft =    (w f ew [wt   1] + u f ht   1 + a f   (a(k)) + b f ),
it =    (wiew [wt   1] + uiht   1 + ai  (a(k)) + bi),
ot =    (woew [wt   1] + uoht   1 + ao  (a(k)) + bo),
  ct = tanh(wcew [wt   1] + ucht   1 + ac  (a(k)) + bc),
ct = ft (cid:12) ct   1 + it (cid:12)   ct ,
ht = ot (cid:12) ct ,

where ew denotes the id27 matrix, as apposed to the atom embedding matrix
ea, w, u, a and b are parameters of the lstm. with the lstm   s state ht, the id203
of the next word in the sequence is pt = softmax(up tanh(wpht + bp) + d) with parameters
up, wp, bp and d. the overall training criterion of the oracle is

   = argmax

  

uk(   ) = log

n

   

n=1

p   (w(n)|a(n,k)) =

n

   

n=1

logp   (w(n)
t

|w(n)

<t ,a(n,k)),

(2)

t

   

t=1

given n training pairs (w(n),a(n,k)).    represents parameters in the lstm.

3.2 atoms construction
each con   guration of a may be associated with a different distribution p   (w|a), therefore a
different oracle model. we de   ne con   guration as an orderless collection of unique atoms.
that is, a(k) = {a1, . . . ,ak} where k is the size of the bag and all items in the bag are different
from each other. considering the particular problem of image and video captioning, atoms
are de   ned as words in captions that are most related to actions, entities, and attributes
of entities (in figure 1). the reason of using these three particular choices of language
components as atoms is not an arbitrary decision. it is reasonable to consider these three
types among the most visually perceivable ones when human describes visual content in
natural language. we further verify this by conducting a human evaluation procedure to

li yao et al.: oracle performance for visual captioning

5

identify    visual    atoms from this set and show that a dominant majority of them indeed match
human visual perception, detailed in section 5.1. being able to capture these important
concepts is considered as crucial in getting superior performance. therefore, comparing the
performance of existing models against this oracle reveals their ability of capturing atoms
from visual inputs when p(a|v) is unknown.

a set of atoms a(k) is treated as    a bag of words   . as with the use of id27
is used to index the atom embedding
] to obtain a vector representation of it. then the representation of the entire

matrix in neural id38 [4], the ith atom a(k)
i
matrix ea[a(k)
i
set of atoms is   (a(k)) =    k

i=1 ea[a(k)
i

].

4 contributing factors of the oracle

the formulation of section 3 is generic, only relying on the assumption the two-step visual
captioning process, independent of the parameterization in section 3.1. in practice, however,
one needs to take into account several contributing factors to the oracle.

firstly, atoms, or visual concepts, may be de   ned as 1-gram words, 2-gram phrases and
so on. arguably a mixture of id165 representations has the potential to capture more com-
plicated correlations among visual concepts. for simplicity, this work uses only 1-gram
representations, detailed in section 5.1. secondly, the procedure used to extract atoms needs
to be reliable, extracting mainly visual concepts, leaving out non-visual concepts. to en-
sure this, the procedure used in this work is veri   ed with human evaluation, detailed in 5.1.
thirdly, the modeling capacity of the conditional language p   (w|a(k)) has a direct in   uence
on the obtained oracle. section 3.1 has shown one example of many possible parameteriza-
tions. lastly, the oracle may be sensitive to the training procedure and its hyper-parameters
(see section 5.2).

while it is therefore important to keep in mind that proposed oracle conditions on the
above factors, quite surprisingly, however, with the simplest procedure and parameterization
we show in the experimental section that oracle serves their purpose reasonably well.

5 experiments

we demonstrate the procedure of learning the oracle on three standard visual captioning
datasets. ms coco [22] is the most commonly used benchmark dataset in image caption-
ing. it consists of 82,783 training and 40,504 validation images. each image accompanied
by 5 captions, all in one sentence. we follow the split used in xu et al. [36] where a sub-
set of 5,000 images are used as validation, and another subset of 5,000 images are used for
testing. youtube2text is the most commonly used benchmark dataset in video captioning.
it consists of 1,970 video clips, each accompanied with multiple captions. overall, there
are 80,000 video and caption pairs. following yao et al. [37], it is split into 1,200 clips
for training, 100 for validation and 670 for testing. another two video captioning datasets
have been recently introduced in torabi et al. [30] and rohrbach et al. [27]. compared with
youtube2text, they are both much larger in the number of video clips, most of which are
associated with one or two captions. recently they are merge together for large scale movie
description challenge (lsmdc). 3 we therefore name this particular dataset lsmdc. the

3https://goo.gl/2hj4lw

6

li yao et al.: oracle performance for visual captioning

of   cial splits contain 91,908 clips for training, 6,542 for validation and 10,053 for testing.

5.1 atom extraction

figure 1: given ground truth cap-
tions,
three categories of visual
atoms (entity, action and attribute)
are automatically extracted using
nlp parser.    na    denotes the empty
atom set.

visual concepts in images or videos are summarized as atoms that are provided to the
caption language model. they are split into three categories: actions, entities, and attributes.
to identify these three classes, we utilize stanford natural language parser 4 to automati-
cally extract them. after a caption is parsed, we apply simple heuristics based on the tags
produced by the parser, ignoring the phrase and sentence level tags 5: use words tagged
with {   nn   ,    nnp   ,    nnps    ,   nns   ,    prp   } as entity atoms. use words tagged with
{   vb   ,    vbd   ,    vbg   ,    vbn   ,    vbp   ,    vbz   } as action atoms. use words tagged with
{   jj   ,    jjr   ,    jjs   } as attribute atoms. after atoms are identi   ed, they are lemmatized with
nltk lemmatizer 6 to unify them to their original dictionary format 7. figure 1 illustrates
some results. we extracted atoms for coco, youtube2text and lsmdc. this gives 14,207
entities, 4,736 actions and 8,671 attributes for coco, 6,922 entities, 2,561 actions, 2,637
attributes for youtube2text, and 12,895 entities, 4,258 actions, 8550 attributes for lsmdc.
note that although the total number of atoms of each categories may be large, atom frequency
varies. in addition, the language parser does not guarantee the perfect tags. therefore, when
atoms are being used in training the oracle, we sort them according to their frequency and
make sure to use more frequent ones    rst to also give priority to atoms with larger coverage,
detailed in section 5.2 below.

we conducted a simple human evaluation 8 to con   rm that extracted atoms are indeed
predominantly visual. as it might be impractical to evaluate all the extracted atoms for all
three datasets, we focus on top 150 frequent atoms. this evaluation intends to match the
last column of table 2 where current state-of-the-art models have the equivalent capacity of
capturing perfectly less than 100 atoms from each of three categories. subjects are asked to
cast their vote independently. the    nal decision of an atom being visual or not is made by
majority vote. table 1 shows the ratio of atoms    agged as visual by such procedure.

5.2 training
after the atoms are extracted, they are sorted according to the frequency they appear in the
dataset, with the most frequent one leading the sorted list. taking    rst k items from this list

4http://goo.gl/lsvpr
5complete list of tags: https://goo.gl/fu8zdd
6http://www.nltk.org/
7available at https://goo.gl/t7vtfj
8details available at https://goo.gl/t7vtfj

visual'input'caption'entity'action'attribute'!a!blue!train!sitting!along!side!of!a!green!forest.!train,!!side,!!forest!sit!blue,!green!!old!train!left!out!on!the!ground!has!graffiti!all!over!it!train,!it,!ground!leave,!graffiti,!have!old!two!abandoned!blue!and!white!train!cars!next!to!trees.!car,!train,!tree!abandon!blue,!white,!next!!a!man!with!a!skateboard!under!him,!not!touching,!are!in!the!air.!air,!skateboard,!him,!man!touch!na!two!guys!are!skateboarding!and!performing!jumps!and!tricks.!jump,!trick,!guy!perform,!skateboard!na!two!men!doing!tricks!on!skateboards!at!the!skate!park!trick,!part,!men,!skateboard!do!skate!!!visual'input'caption'entity'action'attribute'!a!blue!train!sitting!along!side!of!a!green!forest.!train,!!side,!!forest!sit!blue,!green!!old!train!left!out!on!the!ground!has!graffiti!all!over!it!train,!it,!ground!leave,!graffiti,!have!old!two!abandoned!blue!and!white!train!cars!next!to!trees.!car,!train,!tree!abandon!blue,!white,!next!!a!man!with!a!skateboard!under!him,!not!touching,!are!in!the!air.!air,!skateboard,!him,!man!touch!na!two!guys!are!skateboarding!and!performing!jumps!and!tricks.!jump,!trick,!guy!perform,!skateboard!na!two!men!doing!tricks!on!skateboards!at!the!skate!park!trick,!park,!men,!skateboard!do!skate!!7
li yao et al.: oracle performance for visual captioning
table 1: human evaluation of proportion of atoms that are voted as visual. it is clear that
extracted atoms from three categories contain dominant amount of visual elements, hence
verifying the procedure described in section 3.2. another observation is that entities and
actions tend to be more visual than attributes according to human perception.

coco
youtube2text
lsmdc

entities
92%
95%
83%

actions
85%
91%
87%

attributes
81%
72%
78%

gives the top k most frequent ones, forming a bag of atoms denoted by a(k) where k is the
size of the bag. conditioned on the atom bag, the oracle is maximized as in equ (2).

to form captions, we used a vocabulary of size 20k, 13k and 25k for coco, youtube2text
and lsmdc respectively. for all three datasets, models were trained on training set with
different con   guration of (1) atom embedding size, (2) id27 size and (3) lstm
state and memory size. to avoid over   tting we also experimented weight decay and dropout
[13] to regularize the models with different size. in particular, we experimented with ran-
dom hyper-parameter search by bergstra and bengio [5] with range [128,1024] on (1), (2)
and (3). similarly we performed random search on the weight decay coef   cient with a range
of [10   6,10   2], and whether or not to use dropout. optimization was performed by sgd,
minibatch size 128, and with adadelta [40] to automatically adjust the per-parameter learn-
ing rate. model selection was done on the standard validation set, with an early stopping
patience of 2,000 (early stop if no improvement made after 2,000 minibatch updates). we
report the results on the test splits.

5.3 interpretation
all three metrics     id7, meteor and cider are computed with microsoft coco eval-
uation server [6]. figure 2 summarizes the learned oracle with an increasing number of k.

comparing oracle performance with existing models we compare the current state-of-
the-art models    performance against the established oracles in figure 2. table 2 shows the
comparison on three different datasets. with figure 2, one could easily associate a particular
performance with the equivalent number of atoms perfectly captured across all 3 atom cat-
egories, as illustrated in table 2, the oracle included in bold. it is somehow surprising that
state-of-the-art models have performance that is equivalent to capturing only a small amount
of    ent    and    all   . this experiment highlights the shortcoming of the state-of-art visual
models. by improving them, we could close the performance gap that we currently have
with the oracles.

quantify the diminishing return as the number of atoms k in a(k) increases, one would
expect the oracle to be improved accordingly. it is however not yet clear the speed of such
improvement. in other words, the gain in performance may not be proportional to the number
of atoms given when generating captions, due to atom frequencies and id38.
figure 2 quanti   es this effect. the oracle on all three datasets shows a signi   cant gain at the
beginning and diminishes quickly as more and more atoms are used.

row 2 of figure 2 also highlights the difference among actions, entities and attributes in
generating captions. for all three datasets tested, entities played much more important roles,

8

li yao et al.: oracle performance for visual captioning

figure 2: learned oracle on coco (left), youtube2text (middle) and lsmdc (right). the
number of atoms a(k) is varied on x-axis and oracles are computed on y-axis on testsets.
the    rst row shows the oracles on id7 and meteor with 3k atoms, k from each of the
three categories. the second row shows the oracles when k atoms are selected individually
for each category. cider is used for coco and youtube2text as each test example is
associated with multiple ground truth captions, argued in [31]. for lsmdc, meteor is
used, as argued by rohrbach et al. [26].

even more so than action atoms in general. this is particularly true on lsmdc where the
gain of modeling attributes is much less than the other two categories.

although visual atoms dominant the three atom categories shown in section 5.1, as they
increase in number, more and more non-visual atoms may be included, such as    living   ,
   try   ,       nd    and    free    which are relatively dif   cult to be associated with a particular part
of visual inputs. excluding non-visual atoms in the conditional language model can further
tighten the oracle bound as less hints are provided to it. the major dif   culty lies in the labor
of hand-separating visual atoms from non-visual ones as to the our best knowledge this is
dif   cult to automate with heuristics.

atom accuracy versus atom quantity we have assumed that the atoms are given, or in
other words, the prediction accuracy of atoms is 100%. in reality, one would hardly expect
to have a perfect atom classi   er. there is naturally a trade-off between number of atoms one
would like to capture and the prediction accuracy of it. figure 3 quanti   es this trade-off on
coco and lsmdc. it also indicates the upper limit of performance given different level of
atom prediction accuracy. in particular, we have replaced a(k) by   a(k)
r where r portion of a(k)
are randomly selected and replaced by other randomly picked atoms not appearing in a(k).
the case of r = 0 corresponds to those shown in figure 2. and the larger the ratio r, the worse
the assumed atom prediction is. the value of r is shown in the legend of figure 3. according
to the    gure, in order to improve the id134 score, one would have two options,
either by keeping the number of atoms    xed while improving the atom prediction accuracy

10501003007001100210031004000number of atoms0.10.20.30.40.50.60.70.80.9scoreid7-1id7-2id7-3id7-4meteor51020601003007001100130015001900number of atoms0.20.30.40.50.60.70.80.9scoreid7-1id7-2id7-3id7-4meteor10501003007001100210031004000number of atoms0.00.10.20.30.40.5scoreid7-1id7-2id7-3id7-4meteor10501003007001100210031004000number of atoms0.00.20.40.60.81.01.21.41.6cideractionsentitiesattributescombined51020601003007001100130015001900number of atoms0.00.20.40.60.81.01.21.4cideractionsentitiesattributescombined10501003007001100210031004000number of atoms0.000.050.100.150.200.25meteoractionsentitiesattributescombined9
li yao et al.: oracle performance for visual captioning
table 2: measure semantic capacity of current state-of-the-art models. using figure 2,
one could easily map the reported metric to the number of visual atoms captured. this
establishes an equivalence between a model, the proposed oracle and a model   s semantic
capacity. (   ent    for entities.    act    for actions.    att    for attributes.    all    for all three
categories combined.    b1    for id7-1,    b4    for id7-4.    m    for meteor.    c    for
cider. note that the cider is between 0 and 10 according to vedantam et al. [31]. the
learned oracle is denoted in bold.
b4
0.31
0.35
0.499
0.58
n/a
0.12

ent
all
   200    2100 >4000     50
   60
>1900     20
   10
   4000
   40

b1
0.74
0.80
0.815
0.88
n/a
0.45

m
0.26
0.30
0.326
0.40
0.07
0.22

c
0.94
1.4
0.658
1.2
n/a
n/a

   500
   50

youtube2text

coco
[25]

lsmdc

act

att

[39]

[32]

or by keeping the accuracy while increasing the number of included atoms. as state-of-art
visual model already model around 1000 atoms, we hyphotesize that we could gain more
in improving the atoms accuracy rather than increase the number of atom detected by those
models.

figure 3: learned oracles with dif-
ferent atom precision (r in red) and
atom quantity (x-axis) on coco
(left) and lsmdc (right). the num-
ber of atoms   a(k)
is varied on x-axis
r
and oracles are computed on y-axis
on testsets. cider is used for coco
and meteor for lsmdc. it shows
one could increase the score by either
improving p(a(k)|v) with a    xed k or
increase k. it also shows the maximal
error bearable for different score.

intrinsic dif   culties of particular datasets figure 2 also reveals the intrinsic properties
of each dataset.
in general, the bounds on youtube2text are much higher than coco,
with lsmdc the lowest. for instance, from the    rst column of the    gure, taking 10 atoms
respectively, blue-4 is around 0.15 for coco, 0.30 for youtube2text and less than 0.05
for lsmdc. with little visual information to condition upon, a strong language model is
required, which makes a dramatic difference across three datasets. therefore the oracle,
when compared across different datasets, offer an objective measure of dif   culties of using
them in the captioning task.

6 discussion

this work formulates oracle performance for visual captioning. the oracle is constructed
with the assumption of decomposing visual captioning into two consecutive steps. we have
assumed the perfection of the    rst step where visual atoms are recognized, followed by the

1050100300700110015002100250031004000number of atoms0.00.20.40.60.81.01.21.4cider00.050.10.20.30.40.50.61050100300700110015002100250031004000number of atoms0.000.050.100.150.200.25meteor00.050.10.20.30.40.50.610

li yao et al.: oracle performance for visual captioning

second step where language models conditioned on visual atoms are trained to maximize the
id203 of given captions. such an empirical construction requires only automatic atom
parsing and the training of conditional language models, without extra labeling or costly
human evaluation.

such an oracle enables us to gain insight on several important factors accounting for
both success and failure of the current state-of-the-art models. it further reveals model in-
dependent properties on different datasets. we furthermore relax the assumption of prefect
atom prediction. this sheds light on a trade-off between atom accuracy and atom coverage,
providing guidance to future research in this direction. importantly, our experimental re-
sults suggest that more efforts are required in step one where visual inputs are converted into
visual concepts (atoms).

despite its effectiveness shown in the experiments, the empirical oracle is constructed
with the simplest atom extraction procedure and model parameterization in mind, which
makes such a construction in a sense a    conservative    oracle.

acknowledgments

the authors would like to acknowledge the support of the following agencies for research
funding and computing support: ibm t.j. watson research, nserc, calcul qu  bec, com-
pute canada, the canada research chairs and cifar. we would also like to thank the
developers of theano [29] , for developing such a powerful tool for scienti   c computing.

references

[1] nicolas ballas, li yao, chris pal, and aaron courville. delving deeper into convolu-

tional networks for learning video representations. iclr, 2016.

[2] a. barbu, a. bridge, z. burchill, d. coroian, s. dickinson, s. fidler, a. michaux,

s. mussman, s. narayanaswamy, d. salvi, et al. video in sentences out. uai, 2012.

[3] samy bengio, oriol vinyals, navdeep jaitly, and noam shazeer.

sampling for sequence prediction with recurrent neural networks.
arxiv:1506.03099, 2015.

scheduled
arxiv preprint

[4] yoshua bengio, r  jean ducharme, pascal vincent, and christian janvin. a neural
probabilistic language model. the journal of machine learning research, 3:1137   
1155, 2003.

[5] james bergstra and yoshua bengio. random search for hyper-parameter optimization.

jmlr, 2012.

[6] xinlei chen, hao fang, tsung-yi lin, ramakrishna vedantam, saurabh gupta, piotr
dollar, and c lawrence zitnick. microsoft coco captions: data collection and evalua-
tion server. arxiv 1504.00325, 2015.

[7] kyunghyun cho, bart van merri  nboer, caglar gulcehre, dzmitry bahdanau, fethi
bougares, holger schwenk, and yoshua bengio. learning phrase representations using
id56 encoder-decoder for id151. emnlp, 2014.

li yao et al.: oracle performance for visual captioning

11

[8] jacob devlin, hao cheng, hao fang, saurabh gupta, li deng, xiaodong he, geoffrey
zweig, and margaret mitchell. language models for image captioning: the quirks and
what works. arxiv preprint arxiv:1505.01809, 2015.

[9] jacob devlin, saurabh gupta, ross girshick, margaret mitchell, and c lawrence zit-
nick. exploring nearest neighbor approaches for image captioning. arxiv preprint
arxiv:1505.04467, 2015.

[10] jeff donahue, lisa anne hendricks, sergio guadarrama, marcus rohrbach, sub-
hashini venugopalan, kate saenko, and trevor darrell. long-term recurrent convo-
lutional networks for visual recognition and description. cvpr, 2015.

[11] hao fang, saurabh gupta, forrest iandola, rupesh srivastava, li deng, piotr doll  r,
jianfeng gao, xiaodong he, margaret mitchell, john platt, et al. from captions to
visual concepts and back. cvpr, 2015.

[12] sergio guadarrama, niveda krishnamoorthy, girish malkarnenkar, subhashini venu-
gopalan, raymond mooney, trevor darrell, and kate saenko. youtube2text: rec-
ognizing and describing arbitrary activities using semantic hierarchies and zero-shot
recognition. in iccv, 2013.

[13] geoffrey e hinton, nitish srivastava, alex krizhevsky, ilya sutskever, and ruslan r
salakhutdinov. improving neural networks by preventing co-adaptation of feature de-
tectors. arxiv preprint arxiv:1207.0580, 2012.

[14] sepp hochreiter and j  rgen schmidhuber. long short-term memory. neural computa-

tion, 9(8):1735   1780, 1997.

[15] micah hodosh, peter young, and julia hockenmaier. framing image description as a
ranking task: data, models and id74. journal of arti   cial intelligence
research, 2013.

[16] xu jia, efstratios gavves, basura fernando, and tinne tuytelaars. guiding long-short

term memory for image id134. arxiv preprint arxiv:1509.04942, 2015.

[17] a karpathy and l fei-fei. deep visual-semantic alignments for generating image

descriptions. in cvpr, 2014.

[18] ryan kiros, ruslan salakhutdinov, and richard s zemel. unifying visual-semantic
embeddings with multimodal neural language models. arxiv preprint arxiv:1411.2539,
2014.

[19] atsuhiro kojima, takeshi tamura, and kunio fukunaga. natural language description
of human activities from video images based on concept hierarchy of actions. ijcv,
2002.

[20] girish kulkarni, visruth premraj, vicente ordonez, sagnik dhar, siming li, yejin
choi, alexander c berg, and tamara l berg. babytalk: understanding and generating
simple image descriptions. pami, 2013.

12

li yao et al.: oracle performance for visual captioning

[21] polina kuznetsova, vicente ordonez, alexander c berg, tamara l berg, and yejin
in proceedings of the
choi. collective generation of natural image descriptions.
50th annual meeting of the association for computational linguistics: long papers-
volume 1, pages 359   368. association for computational linguistics, 2012.

[22] tsung-yi lin, michael maire, serge belongie, james hays, pietro perona, deva ra-
manan, piotr doll  r, and c lawrence zitnick. microsoft coco: common objects in
context. in id161   eccv 2014, pages 740   755. springer, 2014.

[23] junhua mao, wei xu, yi yang, jiang wang, and alan yuille. deep captioning with

multimodal recurrent neural networks (m-id56). iclr, 2015.

[24] margaret mitchell, xufeng han, jesse dodge, alyssa mensch, amit goyal, alex berg,
kota yamaguchi, tamara berg, karl stratos, and hal daum   iii. midge: generating
image descriptions from id161 detections. in proceedings of the 13th confer-
ence of the european chapter of the association for computational linguistics, pages
747   756. association for computational linguistics, 2012.

[25] qi qi wu, chunhua shen, anton van den hengel, lingqiao liu, and anthony dick.
what value high level concepts in vision to language problems? arxiv 1506.01144,
2015.

[26] anna rohrbach, marcus rohrbach, and bernt schiele. the long-short story of movie

description. 2015.

[27] anna rohrbach, marcus rohrbach, niket tandon, and bernt schiele. a dataset for

movie description. cvpr, 2015.

[28] marcus rohrbach, wei qiu, ivan titov, stefan thater, manfred pinkal, and bernt

schiele. translating video content to natural language descriptions. in iccv, 2013.

[29] theano development team. theano: a python framework for fast computation of
mathematical expressions. arxiv e-prints, abs/1605.02688, may 2016. url http:
//arxiv.org/abs/1605.02688.

[30] atousa torabi, christopher pal, hugo larochelle, and aaron courville. using descrip-
tive video services to create a large data source for video annotation research. arxiv:
1503.01070, 2015.

[31] ramakrishna vedantam, c lawrence zitnick, and devi parikh. cider: consensus-

based image description evaluation. cvpr, 2015.

[32] subhashini venugopalan, marcus rohrbach, jeff donahue, raymond mooney, trevor

darrell, and kate saenko. sequence to sequence     video to text. in iccv, 2015.

[33] subhashini venugopalan, huijuan xu, jeff donahue, marcus rohrbach, raymond
mooney, and kate saenko. translating videos to natural language using deep recurrent
neural networks. naacl, 2015.

[34] oriol vinyals, alexander toshev, samy bengio, and dumitru erhan. show and tell: a

neural image caption generator. cvpr, 2014.

li yao et al.: oracle performance for visual captioning

13

[35] huijuan xu, subhashini venugopalan, vasili ramanishka, marcus rohrbach, and kate
saenko. a multi-scale multiple instance video description network. arxiv 1505.05914,
2015.

[36] kelvin xu, jimmy ba, ryan kiros, aaron courville, ruslan salakhutdinov, richard
zemel, and yoshua bengio. show, attend and tell: neural image id134
with visual attention. icml, 2015.

[37] li yao, atousa torabi, kyunghyun cho, nicolas ballas, christopher pal, hugo
larochelle, and aaron courville. describing videos by exploiting temporal structure.
in iccv, 2015.

[38] peter young, alice lai, micah hodosh, and julia hockenmaier. from image descrip-
tions to visual denotations: new similarity metrics for semantic id136 over event
descriptions. acl14.

[39] haonan yu, jiang wang, zhiheng huang, yi yang, and wei xu. video paragraph

captioning using hierarchical recurrent neural networks. arxiv 1510.07712, 2015.

[40] matthew d. zeiler. adadelta: an adaptive learning rate method. technical report,

2012.

