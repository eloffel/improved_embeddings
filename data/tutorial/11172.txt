   #[1]meta [2]meta [3]meta

   iframe: [4]https://www.googletagmanager.com/ns.html?id=gtm-tr6twrh

   iframe: [5]https://www.googletagmanager.com/ns.html?id=gtm-tr6twrh

create a new account

   email
   ____________________
   register
   [6]returning user

   can't sign in? forgot your password?

   enter your email address below and we will send you the reset
   instructions
   email
   _______________
   submit
   [7]cancel

   if the address matches an existing account you will receive an email
   with instructions to reset your password
   [8]close

   can't sign in? forgot your username?

   enter your email address below and we will send you your username
   email
   _______________
   submit
   [9]cancel

   if the address matches an existing account you will receive an email
   with instructions to retrieve your username
   [10]close

change password

   old password
   ____________________
   new password
   ____________________
   too short weak medium strong very strong too long
   submit
   [11]cancel

password changed successfully

   your password has been changed
   [12]close

login to your account
     __________________________________________________________________

   username
   _______________
   [13]forgot username?
   password
   ____________________
   [14]forgot password?
   keep me logged in
   [ ]
   login
   new user
   institutional login
   [15][mitpress-logo-main-1483476130433.svg]
   [16]the mit pressjournals

     * [17]books
     * [18]journals
     * [19]digital
     * [20]resources
          + [21]for librarians
               o [22]institutional activation
               o [23]institutional license agreement
               o [24]usage stats
               o [25]document delivery
               o [26]mit press journals vpat
               o [27]join our librarian mailing list
               o [28]librarian faq
         [29]quick guides
               o [30]2019 institutional pricing
               o [31]2019 institutional package options and pricing
               o [32]single issue price list
          + [33]for authors
               o [34]submitting publication agreements
               o [35]author posting guidelines
               o [36]copyright information
               o [37]author reprints
               o [38]publishing open access
               o [39]nih public access policy
               o [40]author discounts
               o [41]the mit press and kudos
         [42]quick guides
               o [43]guide1
               o [44]guide2
               o [45]guide3
          + [46]request permissions
         [47]give to the frank urbanowski fund
         [48]trade sales
         [49]advertising info
         [50]release schedule
         [51]faq
         [52]terminated journals
          +
     * [53]about
          + the mit press is a leading publisher of books and journals at
            the intersection of science, technology, and the arts. mit
            press books and journals are known for their intellectual
            daring, scholarly standards, and distinctive design.

[54]about mitpj statement

[55]statement of publication ethics

[56]events & conferences

[57]publishing services

[58]mit press journals staff

[59]how we use analytics
          +
     * [60]contact
          + location
            the mit press
            one rogers street
            cambridge, ma 02142-1209
            [fpo-map-1493476240117.png]
          + phone
            (800) 207-8354
            (us & canada)
            (617) 253-2889
            (outside us and canada)
            (617) 577-1545
            fax
            mit press business hours are m-f, 9:00 a.m. - 5:00 p.m.
            eastern time
            connect
               o [61]facebook
               o [62]twitter
               o [63]google +
               o [64]pinterest
               o [65]instagram
               o [66]youtube
          + [67]send us a message
         quick email links
               o [68]change your address
               o [69]advertising inquiries
               o [70]list exchange & purchasing
               o [71]rights & permissions
               o [72]payments & wire transfer
               o [73]license agreements & consortia
               o [74]subscriptions & back issues
               o [75]electronic access queries
            to submit proposals to either launch new journals or bring an
            existing journal to mit press, please contact director for
            journals and open access, [76]nick lindsay. to submit an
            article please follow the submission guidelines for the
            appropriate journal(s).
          +

   [77]sign in / register
     * [78]enter words / phrases / doi / isbn / authors / keywords / etc.
       computational lingui _______________ _______________
       _______________ _______________
       ____________________ (button)
       search [79]advanced search
     *
     * 0

     * [80]home >
     * [81]computational linguistics >
     * [82]list of issues >
     * [83]volume 41 , no. 3 >
     * discriminative syntax-based word ordering for text generation

   article navigation
   [84]previous [85]next
   publication cover

more about computational linguistics

   [86][arrow-button-1488499533633.svg]

journal resources

   [87]editorial info
   [88]abstracting and indexing
   [89]release schedule
   [90]advertising info
     __________________________________________________________________

author resources

   [91]submission guidelines
   [92]publication agreement
   [93]author reprints
     __________________________________________________________________

reader resources

   [94]rights and permissions
   [95]most read
   [96]most cited

metrics

   [97][arrow-button-1488499533633.svg]

article metrics

altmetric

   about article usage data:

   lorem ipsum dolor sit amet, consectetur adipiscing elit. aenean euismod
   bibendum laoreet. proin gravida dolor sit amet lacus accumsan et
   viverra justo commodo. proin sodales pulvinar tempor. cum sociis
   natoque penatibus et magnis dis parturient montes, nascetur ridiculus
   mus.

open access

   [98][arrow-button-1488499533633.svg]
   [open-access-1493356222797.svg]

   computational linguistics computational linguistics is open access. all
   content is freely available in electronic format (full text html, pdf,
   and pdf plus) to readers across the globe. all articles are published
   under a [99]cc by-nc-nd 4.0 license. for more information on allowed
   uses, please view the cc license.
   [100]support oa at mitp

discriminative syntax-based word ordering for text generation

   [101]yue zhang and [102]stephen clark
   posted online september 08, 2015
   [103]https://doi.org/10.1162/coli_a_00229

computational linguistics

   volume 41 | issue 3 | september 2015
   p.503-538
   [104][preview-1489555631417.svg] download options
   [arrow-button-1488499533633.svg]

discriminative syntax-based word ordering for text generation

   [105]yue zhang and [106]stephen clark
   [107]https://doi.org/10.1162/coli_a_00229
   received: april 17, 2013
   accepted: june 22, 2014
   published online: september 08, 2015
     * [108]full text
     * [109]authors
     * [110]pdf
     * [111]pdf plus

abstract

   section:
   [choose________________________]
   [112]next section

   word ordering is a fundamental problem in text generation. in this
   article, we study word ordering using a syntax-based approach and a
   discriminative model. two grammar formalisms are considered:
   id35 (id35) and dependency grammar. given the
   search for a likely string and syntactic analysis, the search space is
   massive, making discriminative training challenging. we develop a
   learning-guided search framework, based on best-first search, and
   investigate several alternative training algorithms.

   the framework we present is flexible in that it allows constraints to
   be imposed on output word orders. to demonstrate this flexibility, a
   variety of input conditions are considered. first, we investigate a
      pure    word-ordering task in which the input is a multi-set of words,
   and the task is to order them into a grammatical and fluent sentence.
   this task has been tackled previously, and we report improved
   performance over existing systems on a standard wall street journal
   test set. second, we tackle the same reordering problem, but with a
   variety of input conditions, from the bare case with no dependencies or
   pos tags specified, to the extreme case where all pos tags and
   unordered, unlabeled dependencies are provided as input (and various
   conditions in between). when applied to the id86 2011 shared task, our
   system gives competitive results compared with the best-performing
   systems, which provide a further demonstration of the practical utility
   of our system.
      2015 association for computational linguistics
   1.   introduction
   section:
   [choose________________________]
   [113]previous section [114]next section

   word ordering is a fundamental problem in id86
   (id86, reiter and dale [115]1997). in this article we focus on text
   generation: starting with a bag of words, or lemmas, as input, the task
   is to generate a fluent and grammatical sentence using those words.
   additional annotation may also be provided with the input   for example,
   part-of-speech (pos) tags or syntactic dependencies. applications that
   can benefit from better text generation algorithms include machine
   translation (koehn [116]2010), abstractive text summarization (barzilay
   and mckeown [117]2005), and grammar correction (lee and seneff
   [118]2006). typically, id151 (smt) systems
   (chiang [119]2007; koehn [120]2010) perform generation into the target
   language as part of an integrated system, which avoids the high
   computational complexity of independent word ordering. on the other
   hand, performing word ordering separately in a pipeline has many
   potential advantages. for smt, it offers better modularity between
   adequacy (translation) and fluency (linearization), and can potentially
   improve target grammaticality for syntactically different languages
   (e.g., chinese and english). more importantly, a stand-alone word
   ordering component can in principle be applied to a wide range of text
   generation tasks, including transfer-based machine translation (chang
   and toutanova [121]2007).

   most word ordering systems use an id165 language model, which is
   effective at controling local fluency. syntax-based language models, in
   particular dependency language models (xu, chelba, and jelinek
   [122]2002), are sometimes used in an attempt to improve global fluency
   through the capturing of long-range dependencies. in this article, we
   take a syntax-based approach and consider two grammar formalisms:
   id35 (id35) and dependency grammar. our system
   also employs a discriminative model. coupled with heuristic search, a
   strength of the model is that arbitrary features can be defined to
   capture complex syntactic patterns in output hypotheses. the
   discriminative model is trained using syntactically annotated data.

   from the perspective of search, word ordering is a computationally
   difficult problem. finding the best permutation for a set of words
   according to a bigram language model, for example, is np-hard, which
   can be proved by linear reduction from the traveling salesman problem.
   in practice, exploring the whole search space of permutations is often
   prevented by adding constraints. in phrase-based machine translation
   (koehn, och, and marcu [123]2003; koehn et al. [124]2007), a distortion
   limit is used to constrain the position of output phrases. in
   syntax-based machine translation systems such as wu ([125]1997) and
   chiang ([126]2007), synchronous grammars limit the search space so that
   polynomial-time id136 is feasible. in fluency improvement
   (blackwood, de gispert, and byrne [127]2010), parts of translation
   hypotheses identified as having high local confidence are held fixed,
   so that word ordering elsewhere is strictly local.

   in this article we begin by proposing a general system to solve the
   word ordering problem, which does not rely on constraints (which are
   typically task-specific). in particular, we treat syntax-based word
   ordering as a id170 problem, for which the input is a
   multi-set (bag) of words and the output is an ordered sentence,
   together with its syntactic analysis (either id35 derivation or
   dependency tree, depending on the grammar formalism being used). given
   an input, our system searches for the highest-scored output, according
   to a syntax-based discriminative model. one advantage of this
   formulation of the reordering problem, which can perhaps be thought of
   as a    pure    text realization task, is that systems for solving it are
   easily evaluated, because all that is required is a set of sentences
   for reordering and a standard evaluation metric such as id7 (papineni
   et al. [128]2002). however, one potential criticism of the    pure   
   problem is that it is unclear how it relates to real realization tasks,
   since in practice (e.g., in id151 systems)
   the input does provide constraints on the possible output orderings.
   our general formulation still allows task-specific contraints to be
   added if appropriate. hence as a test of the flexibility of our system,
   and a demonstration of the applicability of the system to more
   realistic text generation scenarios, we consider two further tasks for
   the dependency-based realization system.

   the first task considers a variety of input conditions for the
   dependency-based system, determined by two parameters. the first is
   whether pos information is provided for each word in the input
   multi-set. the second is whether syntactic dependencies between the
   words are provided. the extreme case is when all dependencies are
   provided, in which case the problem reduces to the tree linearization
   problem (filippova and strube [129]2009; he et al. [130]2009). however,
   the input can also lie between the two extremes of no- and
   full-dependency information.

   the second task is the id86 2011 shared task, which provides a further
   demonstration of the practical utility of our system. the shared task
   is closer to a real realization scenario, in that lemmas, rather than
   inflected words, are provided as input. hence some modifications are
   required to our system in order that it can perform some word
   inflection, as well as deciding on the ordering. the shared task data
   also uses labeled, rather than unlabeled, syntactic dependencies, and
   so the system was modified to incorporate labels. the final result is
   that our system gives competitive id7 scores, compared to the
   best-performing systems on the shared task.

   the id170 problem we solve is a very hard problem. due
   to the use of syntax, and the search for a sentence together with a
   single id35 derivation or dependency tree, the search space is
   exponentially larger than the id165 word permutation problem. no
   efficient algorithm exists for finding the optimal solution. kay
   ([131]1996) recognized the computational difficulty of chart-based
   generation, which has many similarities to the problem we address in
   his seminal paper. we tackle the high complexity by using
   learning-guided best-first search, exploring a small path in the whole
   search space, which contains the most likely structures according to
   the discriminative model. one of the contributions of this article is
   to introduce, and provide a discriminative solution to, this difficult
   id170 problem, which is an interesting machine learning
   problem in its own right.

   this article is based on, and significantly extends, three conference
   papers (zhang and clark [132]2011; zhang, blackwood, and clark
   [133]2012; zhang [134]2013). it includes a more detailed description
   and discussion of our guided-search approach to syntax-based word
   ordering, bringing together the id35- and dependency-based systems under
   one unified framework. in addition, we discuss the limitations of our
   previous work, and show that a better model can be developed through
   scaling of the feature vectors. the resulting model allows fair
   comparison of constituents of different sizes, and enables the learning
   algorithms to expand negative examples during training, which leads to
   significantly improved results over our previous work. the competitive
   results on the id86 2011 shared task data are new for this article, and
   demonstrate the applicability of our system to more realistic text
   realization scenarios.

   the contributions of this article can be summarized as follows:
         

   we address the problem of syntax-based word ordering, drawing attention
   to this challenging id38 task and offering a general
   solution that does not rely on constraints to limit the search space.
         

   we present a novel method for solving the word ordering problem that
   gives the best reported accuracies to date on the standard wall street
   journal data.
         

   we show how our system can be used with two different grammar
   formalisms: id35 and dependency grammar.
         

   we show how syntactic constraints can be easily incorporated into the
   system, presenting results for the dependency-based system with a range
   of input conditions.
         

   we demonstrate the applicability of the system to more realistic text
   realization scenarios by obtaining competitive results on the id86 2011
   shared task data, including performing some word inflection as part of
   a joint system that also performs word reordering.
         

   more generally, we propose a learning-guided, best-first search
   algorithm for application of discriminative models to extremely large
   search spaces containing structures of varying sizes. this method could
   be applied to other complex id170 tasks in nlp and
   machine learning.
   2.   overview of the search and training algorithms
   section:
   [choose___________________________]
   [135]previous section [136]next section

   in this section, the id35-based system is used to describe the search
   and training algorithms. however, the same approach can be used for the
   dependency-based system, as described in [137]section 4: instead of
   building hypotheses by applying id35 rules in a bottom-up manner, the
   dependency-based system creates dependency links between words.

   given a bag of words, the goal is to put them into an ordered sentence
   that has a plausible id35 derivation. the search space of the decoding
   problem consists of all possible id35 derivations for all possible word
   permutations, and the search goal is to find the highest-scored
   derivation in the search space. this is an np-hard problem, as
   mentioned in the introduction. we apply learning-guided search to
   address the high complexity. the intuition is that, because the whole
   search space cannot be exhausted in order to find the optimal solution,
   we choose to explore a small area in the search space. a statistical
   model is used to guide the search, so that only a small portion of the
   search space containing the most plausible hypotheses is explored.

   one natural choice for the decoding algorithm is best-first search,
   which uses an agenda to order hypotheses, and expands the
   highest-scored hypothesis on the agenda at each step. the resulting
   hypotheses after each hypothesis expansion are put back on the agenda,
   and the process repeats until a goal hypothesis (a full sentence) is
   found. this search process is guided by the current scores of the
   hypotheses, and the search path will contain the most plausible
   hypotheses if they are scored higher than implausible ones. an
   alternative to best-first search is id67, which makes use of a
   heuristic function to estimate future scores. a* can potentially be
   more efficient given an effective heuristic function; however, it is
   not straightforward to define an admissible and accurate estimate of
   future scores for our problem, and we leave this research question to
   future work.

   in our formulation of the word ordering problem, a hypothesis is a
   phrase or sentence together with its id35 derivation. hypotheses are
   constructed bottom   up: starting from single words, smaller phrases are
   combined into larger ones according to id35 rules. to allow the
   combination of hypotheses, we use an additional structure to store a
   set of hypotheses that have been expanded, which we call accepted
   hypotheses. when a hypothesis from the agenda is expanded, it is
   combined with all accepted hypotheses in all possible ways to produce
   new hypotheses. the data structure for accepted hypotheses is similar
   to that used for best-first parsing (caraballo and charniak [138]1998),
   and we adopt the term chart for this structure. however, note there are
   important differences to the parsing problem. first, the parsing
   problem has a fixed word order and is considerably simpler than the
   word ordering problem we are tackling. second, although we use the term
   chart, the structure for accepted hypotheses is not a dynamic
   programming chart in the same way as for the parsing problem. in our
   previous papers (zhang and clark [139]2011; zhang, blackwood, and clark
   [140]2012), we applied a set of beams to this structure, which makes it
   similar to the data structure used for phrase-based mt decoding (koehn
   [141]2010). however, we will show later that this structure is
   unnecessary when the model has more discriminative power, and a
   conceptually simpler single beam can be used. we will also investigate
   the possibility of applying dynamic-programming-style pruning to the
   chart.

   we now give an overview of the training algorithm, which is crucial to
   both the speed and accuracy of the resulting decoder. id35bank
   (hockenmaier and steedman [142]2007) is used to train the model. for
   each training sentence, the corresponding id35bank derivation together
   with all its sub-derivations are treated as gold-standard hypotheses.
   all other hypotheses that can be constructed from the same bag of words
   are non-gold hypotheses. from the generation perspective this
   assumption is too strong, because sentences can have multiple orderings
   (with multiple derivations) that are both grammatical and fluent.
   nevertheless, it is the most feasible choice given the training data
   available.

   the efficiency of the decoding algorithm is dependent on the training
   algorithm because the agenda is ordered according to the hypothesis
   scores. hence, a better model will lead to a goal hypothesis being
   found more quickly. in the ideal situation, all gold-standard
   hypotheses are scored higher than all non-gold hypotheses, and
   therefore only gold-standard hypotheses are expanded before the
   gold-standard goal hypothesis is found. in this case, the minimum
   number of hypotheses is expanded and the output is correct. the
   best-first search decoder is optimal not only with respect to accuracy
   but also speed. this ideal situation can hardly be met in practice, but
   it determines the goal of the training algorithm: to find a model that
   scores gold-standard hypotheses higher than non-gold ones.

   learning-guided search places more challenges on the training of a
   discriminative model than standard id170 problems, for
   example, cky parsing for id35 (clark and curran [143]2007b). if we take
   gold-standard hypotheses as positive training examples, and non-gold
   hypotheses as negative examples, then the training goal is to find a
   large separating margin between the scores of all positive examples and
   all negative examples. for cky parsing, the highest-scored negative
   example can be found via optimal viterbi decoding, according to the
   current model, and this negative example can be used in place of all
   negative examples during the updating of parameters. in contrast, our
   best-first search algorithm cannot find an output in reasonable time
   unless a good model has already been trained, and therefore we cannot
   run the decoding algorithm in the standard way during training. in our
   previous papers (zhang and clark [144]2011; zhang, blackwood, and clark
   [145]2012), we proposed an approximate online training algorithm, which
   forces positive examples to be kept in the hypothesis space without
   being discarded, and prevents the expansion of negative examples during
   the training process (so that the hypothesis space does not get too
   large). this algorithm ensures training efficiency, but greatly limits
   the space of negative examples that is explored during training (and
   hence fails to replicate the conditions experienced at test time). in
   this article, we will show that, with an improved scoring model, it is
   possible to expand negative examples, which leads to improved
   performance.

   a second and more subtle challenge for our training problem is that we
   need a stronger model for learning-guided search than for dynamic
   programming (dp)   based search, such as cky decoding. for cky decoding,
   the model is used to compare hypotheses within each chart cell, which
   cover the same input words. in contrast, for the best-first search
   decoder, the model is used to order hypotheses on the agenda, which can
   cover different numbers of words. it needs much stronger discriminating
   power, so that it can determine whether a single-word phrase is better
   than, say, a 40-word sentence. in this article we use scaling of the
   hypothesis scores by size, so that hypotheses of different sizes can be
   fairly compared. we also find that, with this new approach, negative
   examples can be expanded during training and a single beam applied to
   the chart, resulting in a conceptually simpler and more effective
   training algorithm and decoder.
   3.   id35-based word ordering
   section:
   [choose___________________________]
   [146]previous section [147]next section
   3.1   the id35 grammar

   we were motivated to use id35 as one of the grammar formalisms for our
   syntax-based realization system because of its successful application
   to a number of related tasks, such as wide-coverage parsing
   (hockenmaier [148]2003; clark and curran [149]2007b; auli and lopez
   [150]2011), id29 (zettlemoyer and collins [151]2005),
   wide-coverage semantic analysis (bos et al. [152]2004), and generation
   itself (espinosa, white, and mehay [153]2008). the grammar formalism
   has been described in detail in those papers, and so here we provide
   only a short description.

   id35 (steedman [154]2000) is a lexicalized grammar formalism that
   associates words with lexical categories. lexical categories are
   detailed grammatical labels, typically expressing subcategorization
   information. during id35 parsing, and during our search procedure,
   categories are combined using id35's combinatory rules. for example, a
   verb phrase in english (s\np) can combine with an np to its left, in
   this case using the combinatory rule of (backward) function
   application:

   in addition to binary rule instances, such as this one, there are also
   unary rules that operate on a single category in order to change its
   type. for example, forward type-raising can change a subject np into a
   complex category looking to the right for a verb phrase:

   such a type-raised category can then combine with a transitive verb
   type using the rule of forward composition:

   following fowler and penn ([155]2010), we extract the grammar by
   reading rule instances directly from the derivations in id35bank
   (hockenmaier and steedman [156]2007), rather than defining the
   combinatory rule schema manually as in clark and curran ([157]2007b).
   hence the grammar we use can be thought of as a context-free
   approximation to the mildly content sensitive grammar arising from the
   use of generalized composition rules (weir [158]1988). hockenmaier
   ([159]2003) contains a detailed description of the grammar that is
   obtained in this way, including the various unary type-changing rules,
   as well as additional rules needed to deal with naturally occurring
   text, such as punctuation rules.
   3.2   the edge data structure

   for the rest of this article, the term edge is used to refer to a
   hypothesis in the decoding algorithm. an edge corresponds to a sentence
   or phrase with a id35 derivation. edges are built bottom   up, starting
   from leaf edges, which are constructed by assigning possible lexical
   categories to input words. each leaf edge corresponds to an input word
   with a particular lexical category. two existing edges can be combined
   if there exists a id35 rule (extracted from id35bank, as described
   earlier) that combines their category labels, and if they do not
   contain the same input word more times than its total count in the
   input. the resulting edge is assigned a category label according to the
   id35 rule, and covers the concatenated surface strings of the two
   sub-edges in their order of combination. new edges can also be built by
   applying unary rules to a single existing edge. we define a goal edge
   as an edge that covers all input words.

   two edges are equivalent if they have the same surface string and
   identical id35 derivations. edge equivalence is used for comparison with
   gold-standard edges. two edges are dp-equivalent when they have the
   same dp-signature. based on the feature templates in [160]table 1, we
   define the dp-signature of an edge as the id35 category at the root of
   its derivation, the head word associated with the root category, and
   the multi-set of words it contains, together with the word and pos
   bigrams on either side of its surface string.

   [161]table
   table   1    feature template definitions, with example instances based on
   [162]figure 2.

   3.3   the scoring of edges

   edges are built bottom   up from input words or existing edges. if we
   treat the assignment of lexical categories to input words and the
   application of unary and binary id35 rules to existing edges as
   edge-building actions, the structure of an edge can be defined
   recursively as the sub-structure resulting from its top action plus the
   structure of its sub-edges (if any), as shown in [163]figure 1. here
   the top action of an edge refers to the most recent action that has
   been applied to build the edge.
   [164]figure
   figure   1    the structure of edges shown recursively.

   in our previous papers we used a global linear model to score edges,
   where the score of an edge e is defined as:

     (e) represents the feature vector of e and is the parameter vector of
   the model.

   similar to the structure of e, the feature vector   (e) can be defined
   recursively:

   in this equation, e[s]     e represents a sub-edge of e. leaf edges do
   not have any sub-edges. unary-branching edges have one sub-edge, and
   binary-branching edges have two sub-edges. represents a (strictly)
   recursive sub-edge of e. the feature vector   (e) represents the
   structure of the top action of e; it is extracted according to the
   feature templates in [165]table 1. example instances of the feature
   templates are given according to the example string and id35 derivation
   in [166]figure 2. for leaf edges,   (e) includes information about the
   lexical category label; for unary-branching edges,   (e) includes
   information from the unary rule; for binary-branching edges,   (e)
   includes information from the binary rule, and additionally the token,
   pos, and lexical category bigrams and trigrams that result from the
   surface string concatenation of its sub-edges.
   [167]figure
   figure   2    example string with its id35 derivation, used to give example
   features in [168]table 1.

   by the given definition of   (e), f(e), the score of edge e, can be
   computed recursively as e is built during the decoding process:

   when the top action is applied, the score of f(e) is computed as the
   sum of f(e[s]) (for all e[s]     e) plus .

   an important aspect of the scoring model is that it is used to compare
   edges with different sizes during decoding. the size of an edge can be
   measured in terms of the number of words it contains, or the number of
   syntax rules in its structure. we define the size of an edge as the
   number of recursive sub-edges in the edge plus one (e.g., the size of a
   leaf edge is 1), which is equivalent to the number of actions (i.e.,
   lexical category assignment for leaf edges, and rule application for
   unary/binary edges) that have been applied to construct the edge. edges
   with different sizes can have significantly different numbers of
   features, which can make the training of a discriminative linear model
   more difficult. note that it is common in id170
   problems for feature vectors to have slightly different sizes because
   of variant feature instantiation conditions. in cky parsing, for
   example, constituents with different numbers of unary rules can be kept
   in the same chart cell and compared with each other, provided that they
   cover the same span in the input. in our case, however, the sizes of
   two feature vectors under comparison can be very different indeed,
   since a leaf edge with one word can be compared with an edge over the
   entire input sentence.

   in our previous papers we observed empirical convergence of online
   learning using this linear model, and obtained competitive results.
   however, as explained in [169]section 2, only positive examples were
   expanded during training, and the expansion of negative examples led to
   non-convergence and made online training infeasible. in this article,
   in order to increase the discriminating power of the model and to make
   use of negative examples during training, we apply length id172
   to the scoring function, so that the score of an edge is independent of
   its size. to achieve this, we scale the original linear model score by
   the number of recursive sub-edges in the edge plus one. for a given
   edge e, the new score is defined as:

   in the equation, |e| represents the size of e, which is equal to the
   number of actions that have been applied when e is constructed. by
   dividing the score f(e) by the size of e, the score represents an
   averaged value of and , averaged by the number of recursive sub-edges
   plus one (i.e., the total actions), and is independent of the size of
   e. given normalized feature vectors, the training of the parameter
   vector needs to be adjusted correspondingly, which will be discussed
   subsequently.
   3.4   the decoding algorithm

   the decoding algorithm takes a multi-set of input words, turns them
   into a set of leaf edges, and searches for a goal edge by repeated
   expansion of existing edges. for best-first decoding, an agenda and a
   chart are used. the agenda is a priority queue on which edges to be
   expanded are ordered according to their current scores. the chart is a
   fixed-size beam used to record a limited number of accepted edges.
   during initialization, leaf edges are generated by assigning all
   possible lexical categories to each input word, before they are put on
   the agenda. during each step in the decoding process, the
   highest-scored edge on the agenda is popped off and expanded. if it is
   a goal edge, it is returned as the output, and the decoding finishes.
   otherwise it is extended with unary rules, and combined with existing
   edges in the chart, using binary rules to produce new edges. the
   resulting edges are scored and put on the agenda, and the original edge
   is put into the chart. the process repeats until a goal edge is found,
   or a timeout limit is reached.

   for the timeout case, a default output is produced by greedily
   combining existing edges in the chart in descending order of size. in
   particular, edges in the chart are sorted by size, and the largest is
   taken as the current default output. then the sorted list is traversed,
   with an attempt to greedily concatenate the current edges in the list
   to the right of the current default output. if the combination is not
   allowed (i.e., the two edges contain some input words more times than
   its count in the input), the current edge is discarded. otherwise, the
   current default output is updated.

   in our previous papers we used a set of beams for the chart, each
   storing a certain number of highest-scored edges that cover a
   particular number of words. this structure is similar to the chart used
   for phrase-based smt decoding. the main reason for the multiple beams
   is the non-comparability of edges in different beams, which can have
   feature vectors of significantly different sizes. in this article,
   however, our chart is a single beam structure containing the top-scored
   accepted edges. this simple data structure is enabled by the use of the
   scaled linear model, and leads to comparable accuracies to the
   multiple-beam chart. in addition to its simplicity, it also fits well
   with the use of agenda-based search, because edges of different sizes
   will ultimately be compared with each other on the agenda.

   we apply dp-style pruning to the chart, keeping only the highest-scored
   edge among those that have the same dp-signature. during decoding,
   before a newly constructued edge e is put into the chart, the chart is
   examined to check whether it contains an existing edge e[0] with the
   same dp-signature as e. if such an edge exists, it is popped off the
   chart and compared with the newly constructed edge e, with the higher
   scored edge being put into the chart and the lower scored edge e    being
   discarded. if the newly constructed edge e is not discarded, then we
   expand e to generate new edges.

   it is worth noting that, in this case, a new edge that results from the
   expansion of e can have dp-equivalent edges in the agenda or the chart,
   which had been generated by expansion of its dp-equivalent predecessor
   e    = e[0]. putting such new edges on the agenda will result in the
   system keeping multiple edges with the same signature. however, because
   applying dp-style pruning to the agenda requires updating the whole
   agenda, and is computationally expensive, we choose to tolerate such
   dp-equivalent duplications in the agenda. pseudocode for the decoder is
   shown as algorithm 1. initagenda returns an initialized agenda with all
   leaf edges. initchart returns a cleared chart. timeout returns true if
   the timeout limit has been reached, and false otherwise. popbest pops
   the top edge from the agenda and returns the edge. goaltest takes an
   edge and returns true if and only if the edge is a goal edge.
   dpchartprune takes an edge e and checks whether there exists in the
   chart an edge e[0] that is dp-equivalent to e. in case e[0] exists, it
   is popped off the chart and compared with e, with the lower scored edge
   e    being discarded, and the higher scored edge being put into the
   chart. the function returns the pair e    and . cancombine checks whether
   two edges can be combined in a given order. two edges can be combined
   if they do not contain an overlapping word (i.e., they do not contain a
   word more times than its count in the input), and their categories can
   be combined according to the id35 grammar. add inserts an edge into the
   agenda or the chart. in the former case, it is placed into the priority
   queue according to its score, and, in the latter case, the lowest
   scored edge in the beam is pruned when the chart is full.

   3.5   the learning algorithm

   we begin by introducing the training algorithm of our previous papers,
   shown in algorithm 2, which has the same fundamental structure as the
   training algorithm of this article but is simpler. the algorithm is
   based on the decoder, where an agenda is used as a priority queue of
   edges to be expanded, and a set of accepted edges is kept in a
   fixed-size chart. the functions initagenda, initchart, timeout,
   popbest, goaltest, dpchartprune, unary, cancombine, and binary are
   identical to those used in the decoding algorithm. goldstandard takes
   an edge and returns true if and only if it is a gold-standard edge.
   mingold returns the lowest scored gold-standard edge in the agenda.
   updateparameters represents the parameter update algorithm.
   recomputescore s updates the scores of edges in the agenda and chart
   after the model is updated.

   similar to the decoding algorithm, the agenda is intialized using all
   possible leaf edges. during each step, the edge e on top of the agenda
   is popped off. if it is a gold-standard edge, it is expanded in exactly
   the same way as in the decoder, with the newly generated edges being
   put on the agenda, and e being inserted into the chart. if e is not a
   gold-standard edge, we take it as a negative example e[   ], and take the
   lowest scored gold-standard edge on the agenda e[+] as a positive
   example, in order to make an update to the parameter vector . note that
   there must exist a gold-standard edge in the agenda, which can be
   proved by contradiction.[170]^1

   the two edges e[+] and e[   ] used to perform a model update can be
   radically different. for example, they may not cover the same words, or
   even the same number of words. this is different from online training
   for cky parsing, for which both positive and negative examples used to
   adjust parameter vectors reside in the same chart cell, and cover the
   same sequence of words. the training goal of a typical cky parser
   (clark and curran [171]2007a, [172]2007b) is to find a large separation
   margin between feature vectors of different derivations of the same
   sentence, which have comparable sizes. our goal is to score all
   gold-standard edges higher than all non-gold edges regardless of their
   size, which is a more challenging goal. after updating the parameters,
   the scores of the agenda edges above and including e[   ], together with
   all chart edges, are updated, and e[   ] is discarded before the start of
   the next processing step.

   one way of viewing the training process is that it pushes gold-standard
   edges towards the top of the agenda, and, crucially, pushes them above
   non-gold edges (zhang and clark [173]2011). given a positive example
   e[+] and a negative example e[   ],a id88-style update is used to
   penalize the score for   (e[   ]) and reward the score of   (e[+]):

   here and denote the parameter vectors before and after the update,
   respectively. this method proved effective empirically (zhang and clark
   [174]2011), but it did not converge well when an id165 language model
   was integrated into the system (zhang, blackwood, and clark [175]2012).

   hence we applied an alternative method for score updates that proved
   more effective than the id88 update and enabled the incorporation
   of a large-scale language model (zhang, blackwood, and clark
   [176]2012). this method treats parameter update as finding a separation
   between gold-standard and non-gold edges. given a positive example e[+]
   and a negative example e[   ], we make a minimum update to the parameters
   so that the score of e[+] is higher than that of e[   ] by a margin of 1:

   the update is similar to the parameter update of online large-margin
   learning algorithms, such as 1-best mira (crammer et al. [177]2006),
   and has a closed-form solution:

   this online learning method proved more effective than the id88
   algorithm empirically, but still has an important shortcoming in that
   it did not provide competitive results when allowing the expansion of
   negative examples during training, which can potentially improve the
   discriminative model (since expanding negative examples can result in a
   more representative sample of the search space). we address this issue
   by introducing a scaled linear model in this article, which, when
   combined with the expansion of negative examples, significantly
   improves performance. we apply the same online large-margin training
   principle; however, the parameter update has to be adjusted for the
   scaled linear model. in particular, the new goal is to find a
   separation between and instead of f[+] and f[   ], for which the
   optimization corresponding to the parameter update becomes:

   where and represent the parameter vectors before and after the update,
   respectively. the equation has a closed-form solution:

   pseudocode for the new training algorithm of this article is shown in
   algorithm 3, where maxnongold returns the highest-scored non-gold edge
   in the chart. in addition to the aforementioned difference in parameter
   updates, new code is added to perform additional updates when
   gold-standard edges are removed from the chart. in our previous work,
   parameter updates happen only when the top edge from the agenda is not
   a gold-standard edge. in this article, the expansion of negative
   training examples will lead to negative examples being put into the
   chart during training, and hence the possibility of gold-standard edges
   being removed from the chart. there are two situations when this can
   happen. first, if a non-gold edge is inserted into the chart, and there
   exists a gold-standard edge in the chart with the same dp-signature but
   a lower score, the gold-standard edge will be removed from the chart
   because of dp-style pruning (since only the highest-scored edge with
   the same dp-signature is kept in the chart).

   second, if the chart is full when a non-gold edge is put into the chart
   (recall that the chart is a fixed-size beam), then the lowest scored
   edge on the chart will be removed. this edge can be a gold-standard
   edge. in both the first and second case, a gold-standard edge is pruned
   as the result of the expansion of a negative example. on the other
   hand, in order for the gold-standard goal edge to be constructed, all
   gold-standard edges that have been expanded must remain in the chart.
   as a result, our training algorithm triggers a parameter update
   whenever a gold-standard edge is removed from the chart, the scores of
   all chart edges are updated, and the original pruned gold edge is
   returned to the chart. the original pruned gold-standard edge is
   treated as the positive example for the update. for the first
   situation, the newly inserted non-gold edge with the same dp-signature
   is taken as the negative example, and will be discarded after the
   parameter update (with a new score that is lower than the new score of
   the corresponding gold-standard). in the second situation, the
   highest-scored non-gold edge in the chart is taken as the negative
   example, and removed from the chart after the update.

   in summary, there are two main differences between algorithms 2 and 3.
   first, line 14 in algorithm 2, which skips the expansion of negative
   examples, is removed in algorithm 3. second, lines 16   20 and 42   46 are
   added in algorithm 3, which correspond to the updating of parameters
   when a gold-standard edge is removed from the chart. in addition, the
   definitions of updateparameters are different for the id88
   training algorithm (zhang and clark [178]2011), the large-margin
   training algorithm (zhang, blackwood, and clark [179]2012), and the
   large-margin algorithm of this article, as explained earlier.
   4.   dependency-based word ordering and tree linearization
   section:
   [choose___________________________]
   [180]previous section [181]next section

   as well as id35, the same approach can be applied to the word ordering
   problem using other grammar formalisms. in this section, we present a
   dependency-based word ordering system, where the input is again a
   multi-set of words with gold-standard pos, and the output is an ordered
   sentence together with its dependency parse. except for necessary
   changes to the edge data structure and edge expansion, the same
   algorithm can be applied to this task.

   in addition to abstract word ordering, our framework can be used to
   solve a more informed, dependency-based word ordering task: tree
   linearization (filippova and strube [182]2009; he et al. [183]2009), a
   task that is very similar to abstract word ordering from a
   computational perspective. both tasks involve the permutation of a set
   of input words, and are np-hard. the only difference is that, for tree
   linearization, full unordered dependency trees are given as input. as a
   result, the output word permutations are more constrained (under the
   projectivity assumption), and more information is available for search
   disambiguation.

   tree linearization can be treated as a special case of word ordering,
   where a grammar constraint is applied such that the output sentence has
   to be consistent with the input tree. there is a spectrum of grammar
   constraints between abstract word ordering (no constraints) and tree
   linearization (full tree constraints). for example, one constraint
   might consist of a set of dependency relations between input words, but
   which do not form a complete unordered spanning tree. we call this word
   ordering task the partial-tree linearization problem, a task that is
   perhaps closer to nlp applications than both the abstract word ordering
   task and the full tree linearization problem, in the sense that id86
   pipelines might provide some syntactic relations between words for the
   linearization step, but not the full spanning tree.

   the main content of this section is based on a conference paper (zhang
   [184]2013), which we extend by using the technique of expanding
   negative training examples (one of the overall contributions of this
   article).
   4.1   full- and partial-tree linearization

   given a multi-set of input words w and a set of head-dependent
   relations h between the words in w, the task is to find an ordered
   sentence consisting of all the words in w and a dependency tree that
   contains all the relations in h. if each word in w is given a pos tag
   and h covers all words in w, then the task is (full-)tree
   linearization; if not then the task is partial-tree linearization. for
   partial-tree linearization, a subset of w is given fixed pos tags. in
   all cases, a word either has exactly one (gold) pos tag, or no pos
   tags.
   4.2   the edge data structure

   similar to the id35 case, edge refers to the data structure for a
   hypothesis in the decoding algorithm. here a leaf edge refers to an
   input word with a pos tag, and a non-leaf edge refers to a phrase or
   sentence with its dependency tree. edges are constructed bottom   up, by
   recursively joining two existing edges and adding an unlabeled
   dependency link between their head words.

   as for the id35 system, edges are scored by a global linear model:

   where   (e) represents the feature vector of e and is the parameter
   vector of the model. [185]table 2 shows the feature templates we use,
   which are inspired by the rich feature templates used for dependency
   parsing (koo and collins [186]2010; zhang and nivre [187]2011). in the
   table, h, m, s, h[l], h[r], m[l], and m[r] are the indices of words in
   the newly constructed edge, where h and m refer to the head and
   dependent of the newly constructed arc, s refers to the nearest sibling
   of m (on the same side of h), and h[l], h[r], m[l], and m[r] refer to
   the left and rightmost dependents of h and m, respectively. word, pos,
   lval, and rval are maps from indices to word forms, pos, left
   valencies, and right valencies of words, respectively. example feature
   instances extracted from the sentence in [188]figure 3 are shown in the
   example column. because of the non-local nature of some of the feature
   templates we define, we do not apply dp-style pruning for
   dependency-based tree-linearization.

   [189]table
   table   2    feature templates. indices on the surface string: h = head on
   newly added arc; m = dependent on arc; s = nearest sibling of m; b =
   any index between h and m; h[l], h[r] = left/rightmost dependent of h;
   m[l], m[r] = left/rightmost dependent of m; s[2] = nearest sibling of s
   towards h; b = boundary between the conjoined phrases (index of the
   first word of the right phrase). variables: dir = direction of the arc,
   normalized by norm; dist = distance (h-m), normalized; size = number of
   words in the dependency tree. functions: word = word at index; pos =
   pos at index; norm = normalize absolute value into 1, 2, 3, 4, 5, (5,
   10], (10, 20], (20, 40], 40+.

   [190]figure
   figure   3    feature template example.
   4.3   the decoding algorithm

   the decoding algorithm is similar to that of the id35 system, where an
   agenda is a priority queue for edges to expand, and chart is a
   fixed-size beam for a list of accepted edges. during initialization,
   input words are assigned possible pos tags, resulting in a set of leaf
   edges that are put onto the agenda. for words with pos constraints,
   only the allowed pos tag is assigned. for unconstrained words, we
   assign all possible pos tags according to a tag dictionary compiled
   from the training data, following standard practice for pos-tagging
   (ratnaparkhi [191]1996).

   when an edge is expanded, it is combined with all edges in the chart in
   all possible ways to generate new edges. two edges can be combined by
   concatenation of the surface strings in both orders and, in each case,
   constructing a dependency link between their heads in two ways
   (corresponding to the two options for the head of the new link). when
   there is a head constraint on the dependent word, a dependency link can
   be constructed only if it is consistent with the constraint. this
   algorithm implements abstract word ordering, partial-tree
   linearization, and full tree linearization   all generalized word
   ordering tasks   in a unified method.

   pseudocode for the decoder is shown as algorithm 4. many of the
   functions have the same definition as for algorithm 1: initagenda,
   initchart, timeout, popbest, goaltest, add. cancombine checks whether
   two edges do not contain an overlapping word (i.e., they do not contain
   a word more times than its count in the input); unlike the id35 case,
   all pairs of words are allowed to combine according to the dependency
   model. combine creates a dependency link between two words, with the
   word order determined by the order in which the arguments are supplied
   to the function, and the head coming from either the first (headleft)
   or second (headright) argument (so there are four combinations
   considered and combine is called four times in algorithm 4).

   4.4   the learning algorithm

   as for the id35 system, an online large-margin learning algorithm based
   on the decoding process is used to train the model. at each step, the
   expanded edge e is compared with the gold standard. if it is a gold
   edge, decoding continues; otherwise e is taken as a negative example
   e[   ] and the lowest-scored gold edge in the agenda is taken as a
   positive example e[+], and a parameter update is executed (repeated
   here from [192]section 3.4):

   the training process is essentially the same as in algorithm 3, but
   with the id35 grammar and model replaced with the dependency-based
   grammar and model.

   in our conference paper describing the earlier version of the
   dependency-based system (zhang [193]2013), the decoding step is
   finished immediately after the parameter update; in this article we
   expand the negative example, as in algorithm3, putting it onto the
   chart and thereby exploring a larger part of the search space (in
   particular that part containing negative examples). our later
   experiments show that this method yields improved results, consistent
   with the id35 system.
   5.   experiments
   section:
   [choose________________________]
   [194]previous section [195]next section

   we use id35bank (hockenmaier and steedman [196]2007) and the penn
   treebank (marcus, santorini, and marcinkiewicz [197]1993) for id35 and
   dependency data, respectively. id35bank is the id35 version of the penn
   treebank. standard splits were used for both: sections 02   21 for
   training, section 00 for development, and section 23 for the final
   test. [198]table 3 gives statistics for the id32.

   [199]table
   table   3    training, development, and test data from the id32.

   for the id35 experiments, original sentences from id35bank are
   transformed into bags of words, with sequence information removed, and
   passed to our system as input data. the system outputs are compared to
   the original sentences for evaluation.

   following wan et al. ([200]2009), we use the id7 metric (papineni et
   al. [201]2002) for string comparison. although id7 is not the perfect
   measure of fluency or grammaticality, being based on id165 precision,
   it is currently widely used for automatic evaluation and allows us to
   compare directly with existing work (wan et al. [202]2009). note also
   that one criticism of id7 for evaluating machine translation systems
   (i.e., that it can only register exact matches between the same words
   in the system and reference translation), does not apply here, because
   the system output always contains the same words as the original
   reference sentence. for the dependency-based experiments, gold-standard
   dependency trees were derived from bracketed sentences in the treebank
   using the penn2malt tool.[203]^2

   for fair comparison with wan et al. ([204]2009), we keep base nps as
   atomic units when preparing the input. wan et al. used base nps from
   the id32 annotation, and we follow this practice for the
   dependency-based experiments. for the id35 experiments we extract base
   nps from id35bbank by taking as base nps those nps that do not
   recursively contain other nps. these base nps mostly correspond to the
   base nps from the id32: in the training data, there are
   242,813 id32 base nps with an average size of 1.09, and
   216,670 id35bank base nps with an average size of 1.19.
   5.1   convergence of training

   the plots in [205]figure 4 show the development test scores of three
   id35 models by the number of training iterations. the three curves
   represent the scaled model of this article, the online large-margin
   model from zhang, blackwood, and clark ([206]2012), and the id88
   model from zhang and clark ([207]2011), respectively. for each curve,
   the id7 score generally increases as the number of training iterations
   increases, until it reaches its maximum at a particular iteration. we
   use the number of training iterations that gives the best development
   test scores for the training of our model when testing on the test
   data.
   [208]figure
   figure   4    id7 scores of the id88, large-margin, and scaled
   large-margin id35 models by the number of training iterations.

   another way to observe the convergence of training is to measure the
   training times for each iteration at different numbers of iterations.
   the per-iteration training times for the large-margin and the scaled
   id35 models are shown in [209]figure 5. for each model, the training
   time for each iteration decreases as the number of training iterations
   increases, reflecting the convergence of learning-guided search. when
   the model gets better, fewer non-gold hypotheses are expanded before
   gold hypotheses, and hence it takes less time for the decoder to find
   the gold goal edge. [210]figure 6 shows the corresponding curve for
   dependency-based word ordering, with similar observations.
   [211]figure
   figure   5    training times of the large-margin model and the scaled id35
   models by the number of training iterations.
   [212]figure
   figure   6    training times of the large-margin and scaled dependency
   models by the number of training iterations.

   because of the expanding of negative examples, the systems of this
   article took more time to train than those of our previous conference
   papers. however, the convergence rate is also faster when negative
   training examples are expanded, as demonstrated by the rate of speed
   improvement as the number of training iterations increases. the
   training times of the id88 algorithm are close to those of the
   large-margin algorithm, and hence are omitted from [213]figures 5 and
   [214]6. the new model gives the best development test scores, as shown
   in [215]figure 4. the next section investigates the effects of two of
   the innovations of this article: use of negative examples during
   training and the scaling of the model by hypothesis size.
   5.2   the effect of the scaled model and negative examples

   [216]table 4 shows a set of id35 development experiments to measure the
   effect of the scaled model and the expansion of negative examples
   during training. with the standard linear model (zhang, blackwood, and
   clark [217]2012) and no expansions of negative examples, our system
   obtained a id7 score of 39.04. the scaled model improved the id7
   score by 1.41 id7 points to 40.45, and the expansion of negative
   examples gave a further improvement of 3.02 id7 points.

   [218]table
   table   4    the effect of the scaled model and expansion of negative
   examples during training for the id35 system.

   these id35 development experiments show that the expansion of negative
   examples during training is an important factor in achieving good
   performance. when no negative examples are expanded, the higher score
   of the scaled linear model demonstrates the effectiveness of fair
   comparison between edges with different sizes. however, it is a more
   important advantage of the scaled linear model that it allows the
   expansion of negative examples during training, which was not possible
   with the standard linear model. in the latter case, training failed to
   converge when negative examples were expanded, reflecting the
   limitations of the standard linear model in separating the training
   data. similar results were found for dependency-based word ordering,
   where the best development id7 score improved from 44.71 (zhang
   [219]2013) to 46.44 with the expansion of negative training examples.
   5.3   the effect of search time

   [220]figure 7 shows the id7 scores for the id35 system on the
   development data when the timeout limit for decoding a single sentence
   is set to 5 sec, 10 sec, 15 sec, 20 sec, 30 sec, 40 sec, 50 sec, and 60
   sec, respectively. the timeout was applied during decoding at test
   time. the scaled model with negative training examples was used for
   this set of experiments, and the same model was used for all timeout
   settings. the results demonstrate that better outputs can be recovered
   given more search time, which is expected for a time-constrained
   best-first search framework. recall that output is created greedily by
   combining the largest available edges, when the system times out.
   similar results were obtained with the dependency-based system of zhang
   ([221]2013), where the development id7 scores improved from 42.89 to
   43.42, 43.58, and 43.72 when the timeout limit increased from 5 sec to
   10 sec, 30 sec, and 60 sec, respectively. the scaled dependency-based
   model without expansion of negative examples was used in this set of
   experiments.
   [222]figure
   figure   7    the effect of search time for the id35 system on the
   development test data.
   5.4   example outputs

   example output for sentences in the development set is shown in
   [223]tables 5 and [224]6, grouped by sentence length. the id35 systems
   of our previous conference papers and this article are compared, all
   with the timeout value set to 5 sec. all three systems perform
   relatively better with smaller sentences. for longer sentences, the
   fluency of the output is significantly reduced. one source of errors is
   confusion between different noun phrases, and where they should be
   positioned, which becomes more severe with increased sentence length
   and adds to the difficulty in reading the outputs. the system of this
   article gave observably improved outputs compared with the two other
   systems.

   [225]table
   table   5    example development output for the id35-based systems and
   sentences with fewer than 20 words.

   [226]table
   table   6    example development output for the id35-based systems and
   sentences with more than 20 words.

   5.5   partial-tree linearization

   in the previous section, the same input settings were used for both
   training and testing, and the assumption was made that the input to the
   system would be a bag of words, with no constraints on the output
   structure. this somewhat artificial assumption allows a standardized
   evaluation but, as discussed previously, text generation applications
   are unlikely to satisfy this assumption and, in practice, the
   realization problem is likely to be easier compared with our previous
   set-up. in this section, we simulate practical situations in
   dependency-based pipelines by measuring the performance of our system
   using randomly chosen input pos tags and dependency relations. for
   maximum flexibility, so that the same system can be applied to
   different input scenarios, our system is trained without input pos tags
   or dependencies. however, if pos tags and dependencies are made
   available during testing, they will be used to provide hard constraints
   on the output (i.e., the output sentence with pos tags and dependencies
   must contain those in the input). from the perspective of search, input
   pos tags and dependencies greatly constrain the search space and lead
   to an easier search problem, with correspondingly improved outputs.

   [227]table 7 shows a set of development results with varying amounts of
   pos and dependency information in the input. for each test, we randomly
   sampled a percentage of words for which the gold-standard pos tags or
   dependencies are given in the input. as can be seen from the table,
   increased amounts of pos and dependency information in the input lead
   to higher id7 scores, and dependencies were more effective than pos
   tags in determining the word order in the output. when all pos tags and
   dependencies are given, our constraint-enabled system gave a id7 score
   of 76.28.[228]^3

   [229]table
   table   7    development id7 scores for partial-tree linearization, with
   different proportions of input pos and dependency information randomly
   selected from full gold-standard trees.

   [230]table 8 shows the output of our system for the first nine
   development test sentences with different input settings. these
   examples illustrate the positive effect of input dependencies in
   specifying the outputs. consider the second sentence as an example.
   when only input words are given, the output of the system is largely
   grammatical but nonsensical. with increasing amounts of dependency
   relations, the output begins to look more fluent, sometimes with the
   system reproducing the original sentence when all dependencies are
   given.

   [231]table
   table   8    partial-tree linearization outputs for the first nine
   development test sentences with various input information.

   5.6   final results

   [232]table 9 shows the test results of various systems. for the system
   of this article, we take the optimal setting from the development
   tests, using the scaled linear model and expansion of negative examples
   during training. for direct comparison with previous work, the timeout
   threshold was set to 5 sec. our new system of this article
   significantly outperforms all previous systems and achieves the best
   published id7 score on this task. it is worth noting that our systems
   without a language model outperform the system of our 2012 paper using
   a large-scale language model.

   [233]table
   table   9    final test results on the standard word ordering task.

   interestingly, the dependency-based systems performed better than the
   id35 systems of this article. one of the main reasons is that the id35
   systems generated shorter outputs by not finding full spanning
   derivations for a larger proportion of inputs. because of the rigidity
   in combinatory rules, not all hypotheses in the chart can be combined
   with the hypothesis being expanded, leading to an increased likelihood
   of full spanning derivations being unreachable. overall, the id35 system
   recovered 93.98% of the input words in the test set, and the dependency
   system recovered 97.71%.
   5.7   shared task evaluation

   the previous sections report evaluations on the task of word ordering,
   an abstract yet fundamental problem in text generation. one question
   that is not addressed by these experiments is how the abstract task can
   be utilized to benefit full text generation, for which more
   considerations need to be taken into account in addition to word
   ordering. we investigate this question using the 2011 generation
   challenge shared task data, which provide a common-ground for the
   evaluation of text generation systems (belz et al. [234]2011).

   the data are based on the conll 2008 shared task data (surdeanu et al.
   [235]2008), which consist of selected sections of the penn wsj
   treebank, converted to syntactic dependencies via the lth tool
   (johansson and nugues [236]2007). [237]sections 2   21 are used for
   training, section 24 for development, and section 23 for testing. a
   small number of sentences from the original wsj sections are not
   included in this set. the input format of the shared task is an
   unordered syntactic dependency tree, with nodes being lemmas, and
   dependency relations on the arcs. named entities and hyphenated words
   are broken into individual nodes, and special dependency links are used
   to mark them. information on coarse-grained pos, number, tense, and
   participle features is given to each node where applicable. the output
   is a fully ordered and inflected sentence.

   we developed a full-text generation system according to this task
   specification, with the core component being the dependency-based word
   ordering system of [238]section 4. in addition to minor engineering
   details that were required to adapt the system to this new task, one
   additional task that the generation system needs to carry out is
   morphological generation   finding the appropriate inflected form for
   each input lemma. our approach is to perform joint word ordering and
   inflection using the learning-guided search framework, letting one
   statistical model decide the best order as well as the inflections of
   ambiguous lemmas. for a lemma, we generate one or more candidate
   inflections by using a lexicon and a set of inflection rules. candidate
   inflections for an input lemma are generated according to the lemma
   itself and its input attributes, such as the number and tense. some
   lemmas are unambiguous, which are inflected before being passed to the
   word ordering system. for the other lemmas, more than one candidate's
   inflections are passed as input words to the word ordering system. to
   ensure that each lemma occurs only once in the output, a unique id is
   given to all the inflections of the same lemma, making them mutually
   exclusive.

   four types of lemmas need morphological generation, including nouns,
   verbs, adjectives, and miscellaneous cases. the last category includes
   a (a or an) and not (not or n't), for which the best inflection can be
   decided only when id165 information is available. for these lemmas, we
   pass all possible inflections to the search module. for nouns and
   adjectives, the inflection is relatively straightforward, since the
   number (e.g., singular, plural) of a lemma is given as an attribute of
   the input node, and comparative and superlative adjectives have
   specific parts of speech. for those cases where the necessary
   information is not available from the input, all possible inflections
   are handed over to the search module for further disambiguation. the
   most ambiguous lemma types are verbs, which can be further divided into
   be and other verbs. the uniqueness of be is that the inflections for
   the first and second person can be different. all verb inflections are
   disambiguated according to the tense and participle attributes of the
   input node. in addition, for verbs in the present tense, the subject
   needs to be determined in order to differentiate between third-person
   singular verbs and others. this can be straightforward when the subject
   is a noun or pronoun, but can be ambiguous when the subject is a
   wh-pronoun, in which case the real subject might not be directly
   identifiable from the dependency tree. we leave all possible
   inflections of be and other verbs to the word ordering system whenever
   the ambiguity is not directly solvable from the subject dependency
   link. overall, the pre-processing step generates 1.15 inflections for
   each lemma on average.

   for word ordering, the search procedure of algorithm 4 is applied
   directly, and the feature templates of [239]table 2 are used with
   additional labeled dependency features described subsequently. the main
   reason that the dependency-based word ordering algorithm can perform
   joint id60 is that it uses rich syntactic and
   id165 features to score candidate hypotheses, which can also
   differentiate between correct and incorrect inflections under
   particular contexts. for example, an honest person and a honest person
   can be differentiated by id165 features, while tom and sally is and
   tom and sally are can be differentiated by higher-order dependency
   features.

   in addition to lemma-formed inputs, one other difference between the
   shared task and the word ordering problem solved by algorithm 4 is that
   the former uses labeled dependencies whereas algorithm 4 constructs
   unlabeled dependency trees. we address this issue by assigning
   dependency labels in the construction of dependency links, and applying
   an extra set of features. the new features are defined by making a
   duplicate of all the features from [240]table 2 that contain dir
   information, and associating each feature in the new copy with a
   dependency label.

   the training of the word ordering system requires fully ordered
   dependency trees, while references in the shared task data are raw
   sentences. we perform a pre-processing step to obtain gold-standard
   training data by matching the input lemmas to the reference sentence in
   order to obtain their gold-standard order. more specifically, given a
   training instance, we generate all candidate inflections for each
   lemma, resulting in an exponential set of possible mappings between the
   input tree and the reference sentence. we then prune these mappings
   bottom   up, assuming that the dependency tree is projective, and
   therefore that each word dominates a continuous span in the reference.
   after such pruning, only one correct mapping is found for the majority
   of the cases. for the cases where more than one mapping is found, we
   randomly choose one as the gold-standard. there are also instances for
   which no correct ordering can be found, and these are mostly due to
   non-projectivity in the shared task data, with a few cases being due to
   conflicts between our morphological generation system and the shared
   task data, or inconsistency in the data itself. out of the 39k training
   instances, 2.8k conflicting instances are discarded, resulting in 36.2k
   gold-standard ordered dependency trees.

   [241]table 10 shows the results of our system and the top two
   participating systems of the shared task. our system outperforms the
   stumaba system by 0.5 id7 points, and the dcu system by 3.8 id7
   points. more evaluation of the system was published in song et al.
   ([242]2014).

   [243]table
   table   10    results and comparison with the top-performing systems on the
   shared task data

   6.   related work
   section:
   [choose________________________]
   [244]previous section [245]next section

   there is a recent line of research on text-to-text generation, which
   studies the linearization of dependency structures (barzilay and
   mckeown [246]2005; filippova and strube [247]2007, [248]2009; he et al.
   [249]2009; bohnet et al. [250]2010; guo, hogan, and van genabith
   [251]2011). on the other hand, wan et al. ([252]2009) study the
   ordering of a bag of words without any dependency information given. we
   generalize the word ordering problem, and formulate it as a task of
   ordering a multi-set of words, regardless of input syntactic
   constraints.

   our bottom   up, chart-based generation algorithm is inspired by the line
   of work on chart-based realization (kay [253]1996; carroll et al.
   [254]1999; white [255]2004, [256]2006; carroll and oepen [257]2005).
   kay ([258]1996) first proposed the concept of chart realization,
   drawing analogies between realization and parsing of free order
   languages. he discussed efficiency issues and provided solutions to
   specific problems. for the task of realization, efficiency improvement
   has been further investigated (carroll et al. [259]1999; carroll and
   oepen [260]2005). the inputs to these systems are logical forms, which
   form natural constraints on the interaction between edges. in our case,
   one constraint that has been leveraged in the dependency system is a
   projectivity assumption   we assume that the dependents of a word must
   all have been attached before the word is attached to its head word,
   and that spans do not cross during combination. in addition, we assume
   that the right dependents of a word must have been attached before a
   left dependent of the word is attached. this constraint avoids spurious
   ambiguities. the projectivity assumption is an important basis for the
   feasibility of the dependency system; it is similar to the chunking
   constraints of white ([261]2006) for id35-based realization.

   white ([262]2004) describes a system that performs id35 realization
   using best-first search. the search process of our algorithm is similar
   to that work, but the input is different: logical forms in the case of
   white ([263]2004) and bags of words in our case. further along this
   line, espinosa, white, and mehay ([264]2008) also describe a id35-based
   realization system, applying    hypertagging      a form of id55   to
   logical forms in order to make use of id35 lexical categories in the
   realization process. white and rajkumar ([265]2009) further use
   id88 reranking on n-best realization output to improve the
   quality.

   the use of id88 learning to improve search has been proposed in
   guided learning for easy-first search (shen, satta, and joshi
   [266]2007) and laso (daum   and marcu [267]2005). laso is a general
   framework for various search strategies. our learning algorithm is
   similar to laso with best-first id136, but the parameter updates
   are different. in particular, laso updates parameters when all correct
   hypotheses are lost, but our algorithm makes an update as soon as the
   top item from the agenda is incorrect. our algorithm updates the
   parameters using a stronger precondition, because of the large search
   space. given an incorrect hypothesis, laso finds the corresponding gold
   hypothesis for a id88 update by constructing its correct sibling.
   in contrast, our algorithm takes the lowest scored gold hypothesis
   currently in the agenda to avoid updating parameters for hypotheses
   that may have not been constructed.

   our parameter update strategy is closer to the guided learning
   mechanism for the easy-first algorithm of shen, satta, and joshi
   ([268]2007), which maintains a queue of hypotheses during search, and
   performs learning to ensure that the highest-scored hypothesis in the
   queue is correct. however, in easy-first search, hypotheses from the
   queue are ranked by the score of their next action, rather than the
   hypothesis score. moreover, shen, satta, and joshi use aggressive
   learning and regenerate the queue after each update, but we perform
   non-aggressive learning, which is faster and is more feasible for our
   large and complex search space. similar methods to shen, satta, and
   joshi ([269]2007) have also been used in shen and joshi ([270]2008) and
   goldberg and elhadad ([271]2010).

   another framework that closely integrates learning and search is searn
   (daum  , langford, and marcu [272]2009), which addresses structured
   prediction problems that can be transformed into a series of simple
   classification tasks. the transformation is akin to greedy search in
   the sense that the complex structure is constructed by sequential
   classification decisions. the key problem that searn addresses is how
   to learn the tth decision based on the previous t     1 decisions, so
   that the overall loss in the resulting structure is minimized. similar
   to our framework, searn allows arbitrary features. however, searn is
   more oriented to greedy search, optimizing local decisions. in
   contrast, our framework is oriented to best-first search, optimizing
   global structures.

   learning and search also interact with each other in a global
   discriminative learning and beam-search framework for incremental
   id170 (zhang and clark [273]2011). in this framework,
   an output is constructed incrementally by a sequence of transitions,
   while a beam is used to record the highest scored structures at each
   step. online training is performed based on the search process, with
   the objective function being the margin between correct and incorrect
   structures. the method involves an early-update strategy, which stops
   search and updates parameters immediately when the gold structure falls
   out of the beam during training. it was first proposed by collins and
   roark ([274]2004) for incremental parsing, and later gained popularity
   in the investigations of many nlp tasks, including pos-tagging (zhang
   and clark [275]2010), transition-based id33 (zhang and
   clark [276]2008; huang and sagae [277]2010), and machine translation
   (liu [278]2013). huang, fayong, and guo ([279]2012) propose a
   theoretical analysis to the early-update training strategy, pointing
   out that it is a type of training method that fixes score violations in
   inexact search. when the score of a gold-standard structure is lower
   than that of a non-gold structure, a violation exists. our parameter
   udpate strategy in this article can also be treated as a mechanism for
   violation fixing.
   7.   conclusion
   section:
   [choose________________________]
   [280]previous section [281]next section

   we investigated the general task of syntax-based word ordering, which
   is a fundamental problem for text generation, and a computationally
   very expensive search task. we provide a principled solution to this
   problem using learning-guided search, a framework that is applicable to
   other nlp problems with complex search spaces. we compared different
   methods for parameter updates, and showed that a scaled linear model
   gave the best results by allowing better comparisons between phrases of
   different sizes, increasing the separability of hypotheses and enabling
   the expansion of negative examples during training.

   we formulate abstract word ordering as a spectrum of tasks with varying
   input specificity, from    pure    word ordering without any syntactic
   information to fully-informed word ordering with a complete unordered
   dependency tree given. experiments show that our proposed method can
   effectively use available input constraints in generating output
   sentences.

   evaluation on the id86 2011 shared task data shows that our system can
   be successfully applied to a more realistic application scenario, in
   particular one where some dependency constraints are provided in the
   input and word inflection is required as well as word ordering.
   additional tasks that may be required in a practical text generation
   scenario include word selection, including the determination of content
   words and generation of function words. the joint modeling solution
   that we have proposed for word ordering and inflection could also be
   adopted for word selection, although the search space is greatly
   increased when the words themselves need deciding, particularly content
   words.
   acknowledgments
   section:
   [choose________________________]
   [282]previous section [283]next section

   this work was carried out partly while yue zhang was a postdoctoral
   research associate at the university of cambridge computer laboratory,
   where he and stephen clark were supported by the european union seventh
   framework programme (fp7-ict-2009-4) under grant agreement no. 247762,
   and partly after yue zhang joined singapore university of technology
   and design, where he was supported by the moe grant t2-moe-2013-01. we
   thank bill byrne, marcus tomalin, adri   de gispert, and graeme
   blackwood for numerous discussions; anja belz and mike white for kindly
   providing the id86 2011 shared task data; kai song for helping with
   tables and figures in the draft; yijia liu for helping with the
   bibliography; and the anonymous reviewers for the many constructive
   comments that have greatly improved this article since the first draft.
   notes
   section:
   [choose________________________]
   [284]previous section [285]next section

   1    an example proof can be based on induction, where the basis is that
   the agenda contains all gold leaf edges at first, and the induction
   step is based on edge combination.

   2    [286]http://w3.msi.vxu.se/  nivre/research/penn2malt.html.

   3    when all pos tags and dependencies are also provided during
   training, the id7 score is reduced to 74.79, showing the value in the
   system, which can adapt to varying amounts of pos and dependency
   information in the input at test time.
   references
   section:
   [choose________________________]
   [287]previous section [288]next section
   auli, michael and adam lopez. 2011. training a log-linear parser with
   id168s via softmax-margin. in proceedings of the 2011
   conference on empirical methods in natural language processing, pages
   333   343, edinburgh. [289]google scholar
   barzilay, regina and kathleen mckeown. 2005. sentence fusion for
   multidocument news summarization. computational linguistics,
   31(3):297   328. [290]link, [291]google scholar
   belz, anja, michael white, dominic espinosa, eric kow, deirdre hogan,
   and amanda stent. 2011. the first surface realisation shared task:
   overview and evaluation results. in proceedings of the 13th european
   workshop on id86, eid86 '11, pages 217   226,
   stroudsburg, pa. [292]google scholar
   blackwood, graeme, adri   de gispert, and william byrne. 2010. fluency
   constraints for minimum bayes-risk decoding of statistical machine
   translation lattices. in proceedings of the 23rd international
   conference on computational linguistics (coling 2010), pages 71   79,
   beijing. [293]google scholar
   bohnet, bernd, leo wanner, simon mill, and alicia burga. 2010. broad
   coverage multilingual deep sentence generation with a stochastic
   multi-level realizer. in proceedings of the 23rd international
   conference on computational linguistics (coling 2010), pages 98   106,
   beijing. [294]google scholar
   bos, johan, stephen clark,mark steedman, james r. curran, and julia
   hockenmaier. 2004. wide-coverage semantic representations from a id35
   parser. in proceedings of coling-04, pages 1240   1246, geneva.
   [295]crossref, [296]google scholar
   caraballo, sharon a. and eugene charniak. 1998. new figures of merit
   for best-first probabilistic chart parsing. computational linguistics,
   24:275   298. [297]google scholar
   carroll, john, ann copestake, dan flickinger, and victor poznanski.
   1999. an efficient chart generator for (semi-) lexicalist grammars. in
   proceedings of the 7th european workshop on id86
   (ewid8699), pages 86   95, toulouse. [298]google scholar
   carroll, john and stephan oepen. 2005. high efficiency realization for
   a wide-coverage unification grammar. in proceedings of the second
   international joint conference on natural language processing,
   ijcnlp'05, pages 165   176, berlin. [299]google scholar
   chang, pi-chuan and kristina toutanova. 2007. a discriminative
   syntactic word order model for machine translation. in proceedings of
   acl, pages 9   16, prague. [300]google scholar
   chiang, david. 2007. hierarchical phrase-based translation.
   computational linguistics, 33(2):201   228. [301]link, [302]google
   scholar
   clark, stephen and james r. curran. 2007a. id88 training for a
   wide-coverage lexicalized-grammar parser. in proceedings of the acl
   2007 workshop on deep linguistic processing, pages 9   16, prague.
   [303]google scholar
   clark, stephen and james r. curran. 2007b. wide-coverage efficient
   statistical parsing with id35 and id148. computational
   linguistics, 33(4):493   552. [304]link, [305]google scholar
   collins, michael and brian roark. 2004. incremental parsing with the
   id88 algorithm. in proceedings of acl, pages 111   118, barcelona.
   [306]google scholar
   crammer, koby, ofer dekel, joseph keshet, shai shalev-shwartz, and
   yoram singer. 2006. online passive-aggressive algorithms. journal of
   machine learning research, 7:551   585. [307]google scholar
   daum  , hal, john langford, and daniel marcu. 2009. search-based
   id170. machine learning, 75(3):297   325.
   [308]crossref, [309]google scholar
   daum  , hal and daniel marcu. 2005. learning as search optimization:
   approximate large margin methods for id170. in icml,
   pages 169   176, bonn. [310]google scholar
   espinosa, dominic, michael white, and dennis mehay. 2008. hypertagging:
   id55 for surface realization with id35. in proceedings of
   acl-08: hlt, pages 183   191, columbus, oh. [311]google scholar
   filippova, katja and michael strube. 2007. generating constituent order
   in german clauses. in proceedings of the 45th annual meeting of the
   association of computational linguistics, pages 320   327, prague.
   [312]google scholar
   filippova, katja and michael strube. 2009. tree linearization in
   english: improving language model based approaches. in proceedings of
   human language technologies: the 2009 annual conference of the north
   american chapter of the association for computational linguistics,
   companion volume: short papers, pages 225   228, boulder, co.
   [313]crossref, [314]google scholar
   fowler, timothy a. d. and gerald penn. 2010. accurate context-free
   parsing with id35. in proceedings of the 48th
   annual meeting of the association for computational linguistics, pages
   335   344, uppsala. [315]google scholar
   goldberg, yoav and michael elhadad. 2010. an efficient algorithm for
   easy-first non-directional id33. in human language
   technologies: the 2010 annual conference of the north american chapter
   of the association for computational linguistics, pages 742   750, los
   angeles, ca. [316]google scholar
   guo, yuqing, deirdre hogan, and josef van genabith. 2011. dcu at
   generation challenges 2011 surface realisation track. in proceedings of
   the generation challenges session at the 13th european workshop on
   id86, pages 227   229, nancy. [317]google scholar
   he, wei, haifeng wang, yuqing guo, and ting liu. 2009. dependency based
   chinese sentence realization. in proceedings of the joint conference of
   the 47th annual meeting of the acl and the 4th international joint
   conference on natural language processing of the afnlp, pages 809   816,
   suntec. [318]google scholar
   hockenmaier, julia. 2003. data and models for statistical parsing with
   id35. ph.d. thesis, school of informatics,
   university of edinburgh. [319]google scholar
   hockenmaier, julia and mark steedman. 2007. id35bank: a corpus of id35
   derivations and dependency structures extracted from the id32.
   computational linguistics, 33(3):355   396. [320]link, [321]google
   scholar
   huang, liang, suphan fayong, and yang guo. 2012. structured id88
   with inexact search. in proceedings of the 2012 conference of the north
   american chapter of the association for computational linguistics:
   human language technologies, pages 142   151, montr  al. [322]google
   scholar
   huang, liang and kenji sagae. 2010. id145 for linear-time
   incremental parsing. in proceedings of acl, pages 1077   1086, uppsala.
   [323]google scholar
   johansson, richard and pierre nugues. 2007. extended
   constituent-to-dependency conversion for english. in 16th nordic
   conference of computational linguistics, pages 105   112, tartu.
   [324]google scholar
   kay, martin. 1996. chart generation. in proceedings of acl, pages
   200   204, santa cruz, ca. [325]google scholar
   koehn, phillip. 2010. id151. cambridge
   university press. [326]google scholar
   koehn, philipp, hieu hoang, alexandra birch, chris callison-burch,
   marcello federico, nicola bertoldi, brooke cowan, wade shen, christine
   moran, richard zens, chris dyer, ondrej bojar, alexandra constantin,
   and evan herbst. 2007. moses: open source toolkit for statistical
   machine translation. in proceedings of the 45th annual meeting of the
   association for computational linguistics companion volume proceedings
   of the demo and poster sessions, pages 177   180, prague. [327]google
   scholar
   koehn, philipp, franz josef och, and daniel marcu. 2003. statistical
   phrase-based translation. in proceedings of the 2003 conference of the
   north american chapter of the association for computational linguistics
   on human language technology - volume 1, naacl '03, pages 48   54,
   edmonton, canada. [328]crossref, [329]google scholar
   koo, terry and michael collins. 2010. efficient third-order dependency
   parsers. in proceedings of acl, pages 1   11, uppsala. [330]google
   scholar
   lee, j. and s. seneff. 2006. automatic grammar correction for
   second-language learners. in proceedings of interspeech, pages
   1978   1981, pittsburgh, pa. [331]google scholar
   liu, yang. 2013. a id132 algorithm for phrase-based
   string-to-dependency translation. in proceedings of the 51st annual
   meeting of the association for computational linguistics (volume 1:
   long papers), pages 1   10, sofia. [332]google scholar
   marcus, mitchell p., beatrice santorini, and mary ann marcinkiewicz.
   1993. building a large annotated corpus of english: the id32.
   computational linguistics, 19(2):313   330. [333]google scholar
   papineni, kishore, salim roukos, todd ward, and wei-jing zhu. 2002.
   id7: a method for automatic evaluation of machine translation. in
   proceedings of the 40th annual meeting of the association for
   computational linguistics, pages 311   318, philadelphia, pa. [334]google
   scholar
   ratnaparkhi, adwait. 1996. a maximum id178 model for part-of-speech
   tagging. in proceedings of emnlp, pages 133   142, somerset, nj.
   [335]google scholar
   reiter, ehud and robert dale. 1997. building applied natural language
   generation systems. natural language engineering, 3(1):57   87.
   [336]crossref, [337]google scholar
   shen, libin and aravind joshi. 2008. ltag id33 with
   bidirectional incremental construction. in proceedings of the 2008
   conference on empirical methods in natural language processing, pages
   495   504, honolulu. hi. [338]google scholar
   shen, libin, giorgio satta, and aravind joshi. 2007. guided learning
   for bidirectional sequence classification. in proceedings of acl, pages
   760   767, prague. [339]google scholar
   song, linfeng, yue zhang, kai song, and qun liu. 2014. joint
   morphological generation and syntactic linearization. in proceedings of
   the twenty-eighth aaai conference, quebec. [340]google scholar
   steedman, mark. 2000. the syntactic process. the mit press, cambridge,
   ma. [341]google scholar
   surdeanu, mihai, richard johansson, adam meyers, llu  s m  rquez, and
   joakim nivre. 2008. the conll-2008 shared task on joint parsing of
   syntactic and semantic dependencies. in proceedings of the twelfth
   conference on computational natural language learning, pages 159   177,
   manchester. [342]crossref, [343]google scholar
   wan, stephen, mark dras, robert dale, and c  cile paris. 2009. improving
   grammaticality in statistical sentence generation: introducing a
   dependency spanning tree algorithm with an argument satisfaction model.
   in proceedings of the 12th conference of the european chapter of the
   acl (eacl 2009), pages 852   860, athens. [344]google scholar
   weir, david. 1988. characterizing mildly context-sensitive grammar
   formalisms. ph.d. thesis, university of pennsylviania. [345]google
   scholar
   white, michael. 2004. reining in id35 chart realization. in proceedings
   of iid86-04, pages 182   191, brockenhurst. [346]google scholar
   white, michael. 2006. efficient realization of coordinate structures in
   id35. research on language & computation,
   4(1):39   75. [347]crossref, [348]google scholar
   white, michael and rajakrishnan rajkumar. 2009. id88 reranking
   for id35 realization. in proceedings of the 2009 conference on empirical
   methods in natural language processing, pages 410   419, singapore.
   [349]crossref, [350]google scholar
   wu, dekai. 1997. stochastic inversion transduction grammars and
   bilingual parsing of parallel corpora. computational linguistics,
   23(3):377   403. [351]google scholar
   xu, peng, ciprian chelba, and frederick jelinek. 2002. a study on
   richer syntactic dependencies for structured id38. in
   proceedings of the 40th annual meeting of the association for
   computational linguistics, pages 191   198, philadelphia, pa. [352]google
   scholar
   zettlemoyer, luke s. and michael collins. 2005. learning to map
   sentences to logical form: structured classification with probabilistic
   categorial grammars. in proceedings of the 21st conference on
   uncertainty in artificial intelligence, pages 658   666, edinburgh.
   [353]google scholar
   zhang, yue. 2013. partial-tree linearization: generalized word ordering
   for text synthesis. in proceedings of ijcai, pages 2232   2238, beijing.
   [354]google scholar
   zhang, yue, graeme blackwood, and stephen clark. 2012. syntax-based
   word ordering incorporating a large-scale language model. in
   proceedings of the 13th conference of the european chapter of the
   association for computational linguistics, pages 736   746, avignon.
   [355]google scholar
   zhang, yue and stephen clark. 2008. joint id40 and pos
   tagging using a single id88. in proceedings of acl/hlt, pages
   888   896, columbus, oh. [356]google scholar
   zhang, yue and stephen clark. 2010. a fast decoder for joint word
   segmentation and pos-tagging using a single discriminative model. in
   proceedings of the 2010 conference on empirical methods in natural
   language processing, pages 843   852, cambridge, ma. [357]google scholar
   zhang, yue and stephen clark. 2011. syntactic processing using the
   generalized id88 and id125. computational linguistics,
   37(1):105   151. [358]link, [359]google scholar
   zhang, yue and joakim nivre. 2011. transition-based id33
   with rich non-local features. in proceedings of the 49th annual meeting
   of the association for computational linguistics: human language
   technologies, pages 188   193, portland, or. [360]google scholar
   yue zhang*
   singapore university of technology and design
   stephen clark**
   university of cambridge

   *singapore university of technology and design. 20 dover drive,
   singapore. e-mail: [361][email protected].

   **university of cambridge computer laboratory, william gates building,
   15 jj thomson avenue, cambridge, uk. e-mail: [362][email protected].
   [363]https://doi.org/10.1162/coli_a_00229
     * [364]abstract
     * [365]full text
     * [366]authors

abstract

   section:
   [choose________________________]
   [367]next section

   word ordering is a fundamental problem in text generation. in this
   article, we study word ordering using a syntax-based approach and a
   discriminative model. two grammar formalisms are considered:
   id35 (id35) and dependency grammar. given the
   search for a likely string and syntactic analysis, the search space is
   massive, making discriminative training challenging. we develop a
   learning-guided search framework, based on best-first search, and
   investigate several alternative training algorithms.

   the framework we present is flexible in that it allows constraints to
   be imposed on output word orders. to demonstrate this flexibility, a
   variety of input conditions are considered. first, we investigate a
      pure    word-ordering task in which the input is a multi-set of words,
   and the task is to order them into a grammatical and fluent sentence.
   this task has been tackled previously, and we report improved
   performance over existing systems on a standard wall street journal
   test set. second, we tackle the same reordering problem, but with a
   variety of input conditions, from the bare case with no dependencies or
   pos tags specified, to the extreme case where all pos tags and
   unordered, unlabeled dependencies are provided as input (and various
   conditions in between). when applied to the id86 2011 shared task, our
   system gives competitive results compared with the best-performing
   systems, which provide a further demonstration of the practical utility
   of our system.
      2015 association for computational linguistics
   1.   introduction
   section:
   [choose________________________]
   [368]previous section [369]next section

   word ordering is a fundamental problem in id86
   (id86, reiter and dale [370]1997). in this article we focus on text
   generation: starting with a bag of words, or lemmas, as input, the task
   is to generate a fluent and grammatical sentence using those words.
   additional annotation may also be provided with the input   for example,
   part-of-speech (pos) tags or syntactic dependencies. applications that
   can benefit from better text generation algorithms include machine
   translation (koehn [371]2010), abstractive text summarization (barzilay
   and mckeown [372]2005), and grammar correction (lee and seneff
   [373]2006). typically, id151 (smt) systems
   (chiang [374]2007; koehn [375]2010) perform generation into the target
   language as part of an integrated system, which avoids the high
   computational complexity of independent word ordering. on the other
   hand, performing word ordering separately in a pipeline has many
   potential advantages. for smt, it offers better modularity between
   adequacy (translation) and fluency (linearization), and can potentially
   improve target grammaticality for syntactically different languages
   (e.g., chinese and english). more importantly, a stand-alone word
   ordering component can in principle be applied to a wide range of text
   generation tasks, including transfer-based machine translation (chang
   and toutanova [376]2007).

   most word ordering systems use an id165 language model, which is
   effective at controling local fluency. syntax-based language models, in
   particular dependency language models (xu, chelba, and jelinek
   [377]2002), are sometimes used in an attempt to improve global fluency
   through the capturing of long-range dependencies. in this article, we
   take a syntax-based approach and consider two grammar formalisms:
   id35 (id35) and dependency grammar. our system
   also employs a discriminative model. coupled with heuristic search, a
   strength of the model is that arbitrary features can be defined to
   capture complex syntactic patterns in output hypotheses. the
   discriminative model is trained using syntactically annotated data.

   from the perspective of search, word ordering is a computationally
   difficult problem. finding the best permutation for a set of words
   according to a bigram language model, for example, is np-hard, which
   can be proved by linear reduction from the traveling salesman problem.
   in practice, exploring the whole search space of permutations is often
   prevented by adding constraints. in phrase-based machine translation
   (koehn, och, and marcu [378]2003; koehn et al. [379]2007), a distortion
   limit is used to constrain the position of output phrases. in
   syntax-based machine translation systems such as wu ([380]1997) and
   chiang ([381]2007), synchronous grammars limit the search space so that
   polynomial-time id136 is feasible. in fluency improvement
   (blackwood, de gispert, and byrne [382]2010), parts of translation
   hypotheses identified as having high local confidence are held fixed,
   so that word ordering elsewhere is strictly local.

   in this article we begin by proposing a general system to solve the
   word ordering problem, which does not rely on constraints (which are
   typically task-specific). in particular, we treat syntax-based word
   ordering as a id170 problem, for which the input is a
   multi-set (bag) of words and the output is an ordered sentence,
   together with its syntactic analysis (either id35 derivation or
   dependency tree, depending on the grammar formalism being used). given
   an input, our system searches for the highest-scored output, according
   to a syntax-based discriminative model. one advantage of this
   formulation of the reordering problem, which can perhaps be thought of
   as a    pure    text realization task, is that systems for solving it are
   easily evaluated, because all that is required is a set of sentences
   for reordering and a standard evaluation metric such as id7 (papineni
   et al. [383]2002). however, one potential criticism of the    pure   
   problem is that it is unclear how it relates to real realization tasks,
   since in practice (e.g., in id151 systems)
   the input does provide constraints on the possible output orderings.
   our general formulation still allows task-specific contraints to be
   added if appropriate. hence as a test of the flexibility of our system,
   and a demonstration of the applicability of the system to more
   realistic text generation scenarios, we consider two further tasks for
   the dependency-based realization system.

   the first task considers a variety of input conditions for the
   dependency-based system, determined by two parameters. the first is
   whether pos information is provided for each word in the input
   multi-set. the second is whether syntactic dependencies between the
   words are provided. the extreme case is when all dependencies are
   provided, in which case the problem reduces to the tree linearization
   problem (filippova and strube [384]2009; he et al. [385]2009). however,
   the input can also lie between the two extremes of no- and
   full-dependency information.

   the second task is the id86 2011 shared task, which provides a further
   demonstration of the practical utility of our system. the shared task
   is closer to a real realization scenario, in that lemmas, rather than
   inflected words, are provided as input. hence some modifications are
   required to our system in order that it can perform some word
   inflection, as well as deciding on the ordering. the shared task data
   also uses labeled, rather than unlabeled, syntactic dependencies, and
   so the system was modified to incorporate labels. the final result is
   that our system gives competitive id7 scores, compared to the
   best-performing systems on the shared task.

   the id170 problem we solve is a very hard problem. due
   to the use of syntax, and the search for a sentence together with a
   single id35 derivation or dependency tree, the search space is
   exponentially larger than the id165 word permutation problem. no
   efficient algorithm exists for finding the optimal solution. kay
   ([386]1996) recognized the computational difficulty of chart-based
   generation, which has many similarities to the problem we address in
   his seminal paper. we tackle the high complexity by using
   learning-guided best-first search, exploring a small path in the whole
   search space, which contains the most likely structures according to
   the discriminative model. one of the contributions of this article is
   to introduce, and provide a discriminative solution to, this difficult
   id170 problem, which is an interesting machine learning
   problem in its own right.

   this article is based on, and significantly extends, three conference
   papers (zhang and clark [387]2011; zhang, blackwood, and clark
   [388]2012; zhang [389]2013). it includes a more detailed description
   and discussion of our guided-search approach to syntax-based word
   ordering, bringing together the id35- and dependency-based systems under
   one unified framework. in addition, we discuss the limitations of our
   previous work, and show that a better model can be developed through
   scaling of the feature vectors. the resulting model allows fair
   comparison of constituents of different sizes, and enables the learning
   algorithms to expand negative examples during training, which leads to
   significantly improved results over our previous work. the competitive
   results on the id86 2011 shared task data are new for this article, and
   demonstrate the applicability of our system to more realistic text
   realization scenarios.

   the contributions of this article can be summarized as follows:
         

   we address the problem of syntax-based word ordering, drawing attention
   to this challenging id38 task and offering a general
   solution that does not rely on constraints to limit the search space.
         

   we present a novel method for solving the word ordering problem that
   gives the best reported accuracies to date on the standard wall street
   journal data.
         

   we show how our system can be used with two different grammar
   formalisms: id35 and dependency grammar.
         

   we show how syntactic constraints can be easily incorporated into the
   system, presenting results for the dependency-based system with a range
   of input conditions.
         

   we demonstrate the applicability of the system to more realistic text
   realization scenarios by obtaining competitive results on the id86 2011
   shared task data, including performing some word inflection as part of
   a joint system that also performs word reordering.
         

   more generally, we propose a learning-guided, best-first search
   algorithm for application of discriminative models to extremely large
   search spaces containing structures of varying sizes. this method could
   be applied to other complex id170 tasks in nlp and
   machine learning.
   2.   overview of the search and training algorithms
   section:
   [choose___________________________]
   [390]previous section [391]next section

   in this section, the id35-based system is used to describe the search
   and training algorithms. however, the same approach can be used for the
   dependency-based system, as described in [392]section 4: instead of
   building hypotheses by applying id35 rules in a bottom-up manner, the
   dependency-based system creates dependency links between words.

   given a bag of words, the goal is to put them into an ordered sentence
   that has a plausible id35 derivation. the search space of the decoding
   problem consists of all possible id35 derivations for all possible word
   permutations, and the search goal is to find the highest-scored
   derivation in the search space. this is an np-hard problem, as
   mentioned in the introduction. we apply learning-guided search to
   address the high complexity. the intuition is that, because the whole
   search space cannot be exhausted in order to find the optimal solution,
   we choose to explore a small area in the search space. a statistical
   model is used to guide the search, so that only a small portion of the
   search space containing the most plausible hypotheses is explored.

   one natural choice for the decoding algorithm is best-first search,
   which uses an agenda to order hypotheses, and expands the
   highest-scored hypothesis on the agenda at each step. the resulting
   hypotheses after each hypothesis expansion are put back on the agenda,
   and the process repeats until a goal hypothesis (a full sentence) is
   found. this search process is guided by the current scores of the
   hypotheses, and the search path will contain the most plausible
   hypotheses if they are scored higher than implausible ones. an
   alternative to best-first search is id67, which makes use of a
   heuristic function to estimate future scores. a* can potentially be
   more efficient given an effective heuristic function; however, it is
   not straightforward to define an admissible and accurate estimate of
   future scores for our problem, and we leave this research question to
   future work.

   in our formulation of the word ordering problem, a hypothesis is a
   phrase or sentence together with its id35 derivation. hypotheses are
   constructed bottom   up: starting from single words, smaller phrases are
   combined into larger ones according to id35 rules. to allow the
   combination of hypotheses, we use an additional structure to store a
   set of hypotheses that have been expanded, which we call accepted
   hypotheses. when a hypothesis from the agenda is expanded, it is
   combined with all accepted hypotheses in all possible ways to produce
   new hypotheses. the data structure for accepted hypotheses is similar
   to that used for best-first parsing (caraballo and charniak [393]1998),
   and we adopt the term chart for this structure. however, note there are
   important differences to the parsing problem. first, the parsing
   problem has a fixed word order and is considerably simpler than the
   word ordering problem we are tackling. second, although we use the term
   chart, the structure for accepted hypotheses is not a dynamic
   programming chart in the same way as for the parsing problem. in our
   previous papers (zhang and clark [394]2011; zhang, blackwood, and clark
   [395]2012), we applied a set of beams to this structure, which makes it
   similar to the data structure used for phrase-based mt decoding (koehn
   [396]2010). however, we will show later that this structure is
   unnecessary when the model has more discriminative power, and a
   conceptually simpler single beam can be used. we will also investigate
   the possibility of applying dynamic-programming-style pruning to the
   chart.

   we now give an overview of the training algorithm, which is crucial to
   both the speed and accuracy of the resulting decoder. id35bank
   (hockenmaier and steedman [397]2007) is used to train the model. for
   each training sentence, the corresponding id35bank derivation together
   with all its sub-derivations are treated as gold-standard hypotheses.
   all other hypotheses that can be constructed from the same bag of words
   are non-gold hypotheses. from the generation perspective this
   assumption is too strong, because sentences can have multiple orderings
   (with multiple derivations) that are both grammatical and fluent.
   nevertheless, it is the most feasible choice given the training data
   available.

   the efficiency of the decoding algorithm is dependent on the training
   algorithm because the agenda is ordered according to the hypothesis
   scores. hence, a better model will lead to a goal hypothesis being
   found more quickly. in the ideal situation, all gold-standard
   hypotheses are scored higher than all non-gold hypotheses, and
   therefore only gold-standard hypotheses are expanded before the
   gold-standard goal hypothesis is found. in this case, the minimum
   number of hypotheses is expanded and the output is correct. the
   best-first search decoder is optimal not only with respect to accuracy
   but also speed. this ideal situation can hardly be met in practice, but
   it determines the goal of the training algorithm: to find a model that
   scores gold-standard hypotheses higher than non-gold ones.

   learning-guided search places more challenges on the training of a
   discriminative model than standard id170 problems, for
   example, cky parsing for id35 (clark and curran [398]2007b). if we take
   gold-standard hypotheses as positive training examples, and non-gold
   hypotheses as negative examples, then the training goal is to find a
   large separating margin between the scores of all positive examples and
   all negative examples. for cky parsing, the highest-scored negative
   example can be found via optimal viterbi decoding, according to the
   current model, and this negative example can be used in place of all
   negative examples during the updating of parameters. in contrast, our
   best-first search algorithm cannot find an output in reasonable time
   unless a good model has already been trained, and therefore we cannot
   run the decoding algorithm in the standard way during training. in our
   previous papers (zhang and clark [399]2011; zhang, blackwood, and clark
   [400]2012), we proposed an approximate online training algorithm, which
   forces positive examples to be kept in the hypothesis space without
   being discarded, and prevents the expansion of negative examples during
   the training process (so that the hypothesis space does not get too
   large). this algorithm ensures training efficiency, but greatly limits
   the space of negative examples that is explored during training (and
   hence fails to replicate the conditions experienced at test time). in
   this article, we will show that, with an improved scoring model, it is
   possible to expand negative examples, which leads to improved
   performance.

   a second and more subtle challenge for our training problem is that we
   need a stronger model for learning-guided search than for dynamic
   programming (dp)   based search, such as cky decoding. for cky decoding,
   the model is used to compare hypotheses within each chart cell, which
   cover the same input words. in contrast, for the best-first search
   decoder, the model is used to order hypotheses on the agenda, which can
   cover different numbers of words. it needs much stronger discriminating
   power, so that it can determine whether a single-word phrase is better
   than, say, a 40-word sentence. in this article we use scaling of the
   hypothesis scores by size, so that hypotheses of different sizes can be
   fairly compared. we also find that, with this new approach, negative
   examples can be expanded during training and a single beam applied to
   the chart, resulting in a conceptually simpler and more effective
   training algorithm and decoder.
   3.   id35-based word ordering
   section:
   [choose___________________________]
   [401]previous section [402]next section
   3.1   the id35 grammar

   we were motivated to use id35 as one of the grammar formalisms for our
   syntax-based realization system because of its successful application
   to a number of related tasks, such as wide-coverage parsing
   (hockenmaier [403]2003; clark and curran [404]2007b; auli and lopez
   [405]2011), id29 (zettlemoyer and collins [406]2005),
   wide-coverage semantic analysis (bos et al. [407]2004), and generation
   itself (espinosa, white, and mehay [408]2008). the grammar formalism
   has been described in detail in those papers, and so here we provide
   only a short description.

   id35 (steedman [409]2000) is a lexicalized grammar formalism that
   associates words with lexical categories. lexical categories are
   detailed grammatical labels, typically expressing subcategorization
   information. during id35 parsing, and during our search procedure,
   categories are combined using id35's combinatory rules. for example, a
   verb phrase in english (s\np) can combine with an np to its left, in
   this case using the combinatory rule of (backward) function
   application:

   in addition to binary rule instances, such as this one, there are also
   unary rules that operate on a single category in order to change its
   type. for example, forward type-raising can change a subject np into a
   complex category looking to the right for a verb phrase:

   such a type-raised category can then combine with a transitive verb
   type using the rule of forward composition:

   following fowler and penn ([410]2010), we extract the grammar by
   reading rule instances directly from the derivations in id35bank
   (hockenmaier and steedman [411]2007), rather than defining the
   combinatory rule schema manually as in clark and curran ([412]2007b).
   hence the grammar we use can be thought of as a context-free
   approximation to the mildly content sensitive grammar arising from the
   use of generalized composition rules (weir [413]1988). hockenmaier
   ([414]2003) contains a detailed description of the grammar that is
   obtained in this way, including the various unary type-changing rules,
   as well as additional rules needed to deal with naturally occurring
   text, such as punctuation rules.
   3.2   the edge data structure

   for the rest of this article, the term edge is used to refer to a
   hypothesis in the decoding algorithm. an edge corresponds to a sentence
   or phrase with a id35 derivation. edges are built bottom   up, starting
   from leaf edges, which are constructed by assigning possible lexical
   categories to input words. each leaf edge corresponds to an input word
   with a particular lexical category. two existing edges can be combined
   if there exists a id35 rule (extracted from id35bank, as described
   earlier) that combines their category labels, and if they do not
   contain the same input word more times than its total count in the
   input. the resulting edge is assigned a category label according to the
   id35 rule, and covers the concatenated surface strings of the two
   sub-edges in their order of combination. new edges can also be built by
   applying unary rules to a single existing edge. we define a goal edge
   as an edge that covers all input words.

   two edges are equivalent if they have the same surface string and
   identical id35 derivations. edge equivalence is used for comparison with
   gold-standard edges. two edges are dp-equivalent when they have the
   same dp-signature. based on the feature templates in [415]table 1, we
   define the dp-signature of an edge as the id35 category at the root of
   its derivation, the head word associated with the root category, and
   the multi-set of words it contains, together with the word and pos
   bigrams on either side of its surface string.

   [416]table
   table   1    feature template definitions, with example instances based on
   [417]figure 2.

   3.3   the scoring of edges

   edges are built bottom   up from input words or existing edges. if we
   treat the assignment of lexical categories to input words and the
   application of unary and binary id35 rules to existing edges as
   edge-building actions, the structure of an edge can be defined
   recursively as the sub-structure resulting from its top action plus the
   structure of its sub-edges (if any), as shown in [418]figure 1. here
   the top action of an edge refers to the most recent action that has
   been applied to build the edge.
   [419]figure
   figure   1    the structure of edges shown recursively.

   in our previous papers we used a global linear model to score edges,
   where the score of an edge e is defined as:

     (e) represents the feature vector of e and is the parameter vector of
   the model.

   similar to the structure of e, the feature vector   (e) can be defined
   recursively:

   in this equation, e[s]     e represents a sub-edge of e. leaf edges do
   not have any sub-edges. unary-branching edges have one sub-edge, and
   binary-branching edges have two sub-edges. represents a (strictly)
   recursive sub-edge of e. the feature vector   (e) represents the
   structure of the top action of e; it is extracted according to the
   feature templates in [420]table 1. example instances of the feature
   templates are given according to the example string and id35 derivation
   in [421]figure 2. for leaf edges,   (e) includes information about the
   lexical category label; for unary-branching edges,   (e) includes
   information from the unary rule; for binary-branching edges,   (e)
   includes information from the binary rule, and additionally the token,
   pos, and lexical category bigrams and trigrams that result from the
   surface string concatenation of its sub-edges.
   [422]figure
   figure   2    example string with its id35 derivation, used to give example
   features in [423]table 1.

   by the given definition of   (e), f(e), the score of edge e, can be
   computed recursively as e is built during the decoding process:

   when the top action is applied, the score of f(e) is computed as the
   sum of f(e[s]) (for all e[s]     e) plus .

   an important aspect of the scoring model is that it is used to compare
   edges with different sizes during decoding. the size of an edge can be
   measured in terms of the number of words it contains, or the number of
   syntax rules in its structure. we define the size of an edge as the
   number of recursive sub-edges in the edge plus one (e.g., the size of a
   leaf edge is 1), which is equivalent to the number of actions (i.e.,
   lexical category assignment for leaf edges, and rule application for
   unary/binary edges) that have been applied to construct the edge. edges
   with different sizes can have significantly different numbers of
   features, which can make the training of a discriminative linear model
   more difficult. note that it is common in id170
   problems for feature vectors to have slightly different sizes because
   of variant feature instantiation conditions. in cky parsing, for
   example, constituents with different numbers of unary rules can be kept
   in the same chart cell and compared with each other, provided that they
   cover the same span in the input. in our case, however, the sizes of
   two feature vectors under comparison can be very different indeed,
   since a leaf edge with one word can be compared with an edge over the
   entire input sentence.

   in our previous papers we observed empirical convergence of online
   learning using this linear model, and obtained competitive results.
   however, as explained in [424]section 2, only positive examples were
   expanded during training, and the expansion of negative examples led to
   non-convergence and made online training infeasible. in this article,
   in order to increase the discriminating power of the model and to make
   use of negative examples during training, we apply length id172
   to the scoring function, so that the score of an edge is independent of
   its size. to achieve this, we scale the original linear model score by
   the number of recursive sub-edges in the edge plus one. for a given
   edge e, the new score is defined as:

   in the equation, |e| represents the size of e, which is equal to the
   number of actions that have been applied when e is constructed. by
   dividing the score f(e) by the size of e, the score represents an
   averaged value of and , averaged by the number of recursive sub-edges
   plus one (i.e., the total actions), and is independent of the size of
   e. given normalized feature vectors, the training of the parameter
   vector needs to be adjusted correspondingly, which will be discussed
   subsequently.
   3.4   the decoding algorithm

   the decoding algorithm takes a multi-set of input words, turns them
   into a set of leaf edges, and searches for a goal edge by repeated
   expansion of existing edges. for best-first decoding, an agenda and a
   chart are used. the agenda is a priority queue on which edges to be
   expanded are ordered according to their current scores. the chart is a
   fixed-size beam used to record a limited number of accepted edges.
   during initialization, leaf edges are generated by assigning all
   possible lexical categories to each input word, before they are put on
   the agenda. during each step in the decoding process, the
   highest-scored edge on the agenda is popped off and expanded. if it is
   a goal edge, it is returned as the output, and the decoding finishes.
   otherwise it is extended with unary rules, and combined with existing
   edges in the chart, using binary rules to produce new edges. the
   resulting edges are scored and put on the agenda, and the original edge
   is put into the chart. the process repeats until a goal edge is found,
   or a timeout limit is reached.

   for the timeout case, a default output is produced by greedily
   combining existing edges in the chart in descending order of size. in
   particular, edges in the chart are sorted by size, and the largest is
   taken as the current default output. then the sorted list is traversed,
   with an attempt to greedily concatenate the current edges in the list
   to the right of the current default output. if the combination is not
   allowed (i.e., the two edges contain some input words more times than
   its count in the input), the current edge is discarded. otherwise, the
   current default output is updated.

   in our previous papers we used a set of beams for the chart, each
   storing a certain number of highest-scored edges that cover a
   particular number of words. this structure is similar to the chart used
   for phrase-based smt decoding. the main reason for the multiple beams
   is the non-comparability of edges in different beams, which can have
   feature vectors of significantly different sizes. in this article,
   however, our chart is a single beam structure containing the top-scored
   accepted edges. this simple data structure is enabled by the use of the
   scaled linear model, and leads to comparable accuracies to the
   multiple-beam chart. in addition to its simplicity, it also fits well
   with the use of agenda-based search, because edges of different sizes
   will ultimately be compared with each other on the agenda.

   we apply dp-style pruning to the chart, keeping only the highest-scored
   edge among those that have the same dp-signature. during decoding,
   before a newly constructued edge e is put into the chart, the chart is
   examined to check whether it contains an existing edge e[0] with the
   same dp-signature as e. if such an edge exists, it is popped off the
   chart and compared with the newly constructed edge e, with the higher
   scored edge being put into the chart and the lower scored edge e    being
   discarded. if the newly constructed edge e is not discarded, then we
   expand e to generate new edges.

   it is worth noting that, in this case, a new edge that results from the
   expansion of e can have dp-equivalent edges in the agenda or the chart,
   which had been generated by expansion of its dp-equivalent predecessor
   e    = e[0]. putting such new edges on the agenda will result in the
   system keeping multiple edges with the same signature. however, because
   applying dp-style pruning to the agenda requires updating the whole
   agenda, and is computationally expensive, we choose to tolerate such
   dp-equivalent duplications in the agenda. pseudocode for the decoder is
   shown as algorithm 1. initagenda returns an initialized agenda with all
   leaf edges. initchart returns a cleared chart. timeout returns true if
   the timeout limit has been reached, and false otherwise. popbest pops
   the top edge from the agenda and returns the edge. goaltest takes an
   edge and returns true if and only if the edge is a goal edge.
   dpchartprune takes an edge e and checks whether there exists in the
   chart an edge e[0] that is dp-equivalent to e. in case e[0] exists, it
   is popped off the chart and compared with e, with the lower scored edge
   e    being discarded, and the higher scored edge being put into the
   chart. the function returns the pair e    and . cancombine checks whether
   two edges can be combined in a given order. two edges can be combined
   if they do not contain an overlapping word (i.e., they do not contain a
   word more times than its count in the input), and their categories can
   be combined according to the id35 grammar. add inserts an edge into the
   agenda or the chart. in the former case, it is placed into the priority
   queue according to its score, and, in the latter case, the lowest
   scored edge in the beam is pruned when the chart is full.

   3.5   the learning algorithm

   we begin by introducing the training algorithm of our previous papers,
   shown in algorithm 2, which has the same fundamental structure as the
   training algorithm of this article but is simpler. the algorithm is
   based on the decoder, where an agenda is used as a priority queue of
   edges to be expanded, and a set of accepted edges is kept in a
   fixed-size chart. the functions initagenda, initchart, timeout,
   popbest, goaltest, dpchartprune, unary, cancombine, and binary are
   identical to those used in the decoding algorithm. goldstandard takes
   an edge and returns true if and only if it is a gold-standard edge.
   mingold returns the lowest scored gold-standard edge in the agenda.
   updateparameters represents the parameter update algorithm.
   recomputescore s updates the scores of edges in the agenda and chart
   after the model is updated.

   similar to the decoding algorithm, the agenda is intialized using all
   possible leaf edges. during each step, the edge e on top of the agenda
   is popped off. if it is a gold-standard edge, it is expanded in exactly
   the same way as in the decoder, with the newly generated edges being
   put on the agenda, and e being inserted into the chart. if e is not a
   gold-standard edge, we take it as a negative example e[   ], and take the
   lowest scored gold-standard edge on the agenda e[+] as a positive
   example, in order to make an update to the parameter vector . note that
   there must exist a gold-standard edge in the agenda, which can be
   proved by contradiction.[425]^1

   the two edges e[+] and e[   ] used to perform a model update can be
   radically different. for example, they may not cover the same words, or
   even the same number of words. this is different from online training
   for cky parsing, for which both positive and negative examples used to
   adjust parameter vectors reside in the same chart cell, and cover the
   same sequence of words. the training goal of a typical cky parser
   (clark and curran [426]2007a, [427]2007b) is to find a large separation
   margin between feature vectors of different derivations of the same
   sentence, which have comparable sizes. our goal is to score all
   gold-standard edges higher than all non-gold edges regardless of their
   size, which is a more challenging goal. after updating the parameters,
   the scores of the agenda edges above and including e[   ], together with
   all chart edges, are updated, and e[   ] is discarded before the start of
   the next processing step.

   one way of viewing the training process is that it pushes gold-standard
   edges towards the top of the agenda, and, crucially, pushes them above
   non-gold edges (zhang and clark [428]2011). given a positive example
   e[+] and a negative example e[   ],a id88-style update is used to
   penalize the score for   (e[   ]) and reward the score of   (e[+]):

   here and denote the parameter vectors before and after the update,
   respectively. this method proved effective empirically (zhang and clark
   [429]2011), but it did not converge well when an id165 language model
   was integrated into the system (zhang, blackwood, and clark [430]2012).

   hence we applied an alternative method for score updates that proved
   more effective than the id88 update and enabled the incorporation
   of a large-scale language model (zhang, blackwood, and clark
   [431]2012). this method treats parameter update as finding a separation
   between gold-standard and non-gold edges. given a positive example e[+]
   and a negative example e[   ], we make a minimum update to the parameters
   so that the score of e[+] is higher than that of e[   ] by a margin of 1:

   the update is similar to the parameter update of online large-margin
   learning algorithms, such as 1-best mira (crammer et al. [432]2006),
   and has a closed-form solution:

   this online learning method proved more effective than the id88
   algorithm empirically, but still has an important shortcoming in that
   it did not provide competitive results when allowing the expansion of
   negative examples during training, which can potentially improve the
   discriminative model (since expanding negative examples can result in a
   more representative sample of the search space). we address this issue
   by introducing a scaled linear model in this article, which, when
   combined with the expansion of negative examples, significantly
   improves performance. we apply the same online large-margin training
   principle; however, the parameter update has to be adjusted for the
   scaled linear model. in particular, the new goal is to find a
   separation between and instead of f[+] and f[   ], for which the
   optimization corresponding to the parameter update becomes:

   where and represent the parameter vectors before and after the update,
   respectively. the equation has a closed-form solution:

   pseudocode for the new training algorithm of this article is shown in
   algorithm 3, where maxnongold returns the highest-scored non-gold edge
   in the chart. in addition to the aforementioned difference in parameter
   updates, new code is added to perform additional updates when
   gold-standard edges are removed from the chart. in our previous work,
   parameter updates happen only when the top edge from the agenda is not
   a gold-standard edge. in this article, the expansion of negative
   training examples will lead to negative examples being put into the
   chart during training, and hence the possibility of gold-standard edges
   being removed from the chart. there are two situations when this can
   happen. first, if a non-gold edge is inserted into the chart, and there
   exists a gold-standard edge in the chart with the same dp-signature but
   a lower score, the gold-standard edge will be removed from the chart
   because of dp-style pruning (since only the highest-scored edge with
   the same dp-signature is kept in the chart).

   second, if the chart is full when a non-gold edge is put into the chart
   (recall that the chart is a fixed-size beam), then the lowest scored
   edge on the chart will be removed. this edge can be a gold-standard
   edge. in both the first and second case, a gold-standard edge is pruned
   as the result of the expansion of a negative example. on the other
   hand, in order for the gold-standard goal edge to be constructed, all
   gold-standard edges that have been expanded must remain in the chart.
   as a result, our training algorithm triggers a parameter update
   whenever a gold-standard edge is removed from the chart, the scores of
   all chart edges are updated, and the original pruned gold edge is
   returned to the chart. the original pruned gold-standard edge is
   treated as the positive example for the update. for the first
   situation, the newly inserted non-gold edge with the same dp-signature
   is taken as the negative example, and will be discarded after the
   parameter update (with a new score that is lower than the new score of
   the corresponding gold-standard). in the second situation, the
   highest-scored non-gold edge in the chart is taken as the negative
   example, and removed from the chart after the update.

   in summary, there are two main differences between algorithms 2 and 3.
   first, line 14 in algorithm 2, which skips the expansion of negative
   examples, is removed in algorithm 3. second, lines 16   20 and 42   46 are
   added in algorithm 3, which correspond to the updating of parameters
   when a gold-standard edge is removed from the chart. in addition, the
   definitions of updateparameters are different for the id88
   training algorithm (zhang and clark [433]2011), the large-margin
   training algorithm (zhang, blackwood, and clark [434]2012), and the
   large-margin algorithm of this article, as explained earlier.
   4.   dependency-based word ordering and tree linearization
   section:
   [choose___________________________]
   [435]previous section [436]next section

   as well as id35, the same approach can be applied to the word ordering
   problem using other grammar formalisms. in this section, we present a
   dependency-based word ordering system, where the input is again a
   multi-set of words with gold-standard pos, and the output is an ordered
   sentence together with its dependency parse. except for necessary
   changes to the edge data structure and edge expansion, the same
   algorithm can be applied to this task.

   in addition to abstract word ordering, our framework can be used to
   solve a more informed, dependency-based word ordering task: tree
   linearization (filippova and strube [437]2009; he et al. [438]2009), a
   task that is very similar to abstract word ordering from a
   computational perspective. both tasks involve the permutation of a set
   of input words, and are np-hard. the only difference is that, for tree
   linearization, full unordered dependency trees are given as input. as a
   result, the output word permutations are more constrained (under the
   projectivity assumption), and more information is available for search
   disambiguation.

   tree linearization can be treated as a special case of word ordering,
   where a grammar constraint is applied such that the output sentence has
   to be consistent with the input tree. there is a spectrum of grammar
   constraints between abstract word ordering (no constraints) and tree
   linearization (full tree constraints). for example, one constraint
   might consist of a set of dependency relations between input words, but
   which do not form a complete unordered spanning tree. we call this word
   ordering task the partial-tree linearization problem, a task that is
   perhaps closer to nlp applications than both the abstract word ordering
   task and the full tree linearization problem, in the sense that id86
   pipelines might provide some syntactic relations between words for the
   linearization step, but not the full spanning tree.

   the main content of this section is based on a conference paper (zhang
   [439]2013), which we extend by using the technique of expanding
   negative training examples (one of the overall contributions of this
   article).
   4.1   full- and partial-tree linearization

   given a multi-set of input words w and a set of head-dependent
   relations h between the words in w, the task is to find an ordered
   sentence consisting of all the words in w and a dependency tree that
   contains all the relations in h. if each word in w is given a pos tag
   and h covers all words in w, then the task is (full-)tree
   linearization; if not then the task is partial-tree linearization. for
   partial-tree linearization, a subset of w is given fixed pos tags. in
   all cases, a word either has exactly one (gold) pos tag, or no pos
   tags.
   4.2   the edge data structure

   similar to the id35 case, edge refers to the data structure for a
   hypothesis in the decoding algorithm. here a leaf edge refers to an
   input word with a pos tag, and a non-leaf edge refers to a phrase or
   sentence with its dependency tree. edges are constructed bottom   up, by
   recursively joining two existing edges and adding an unlabeled
   dependency link between their head words.

   as for the id35 system, edges are scored by a global linear model:

   where   (e) represents the feature vector of e and is the parameter
   vector of the model. [440]table 2 shows the feature templates we use,
   which are inspired by the rich feature templates used for dependency
   parsing (koo and collins [441]2010; zhang and nivre [442]2011). in the
   table, h, m, s, h[l], h[r], m[l], and m[r] are the indices of words in
   the newly constructed edge, where h and m refer to the head and
   dependent of the newly constructed arc, s refers to the nearest sibling
   of m (on the same side of h), and h[l], h[r], m[l], and m[r] refer to
   the left and rightmost dependents of h and m, respectively. word, pos,
   lval, and rval are maps from indices to word forms, pos, left
   valencies, and right valencies of words, respectively. example feature
   instances extracted from the sentence in [443]figure 3 are shown in the
   example column. because of the non-local nature of some of the feature
   templates we define, we do not apply dp-style pruning for
   dependency-based tree-linearization.

   [444]table
   table   2    feature templates. indices on the surface string: h = head on
   newly added arc; m = dependent on arc; s = nearest sibling of m; b =
   any index between h and m; h[l], h[r] = left/rightmost dependent of h;
   m[l], m[r] = left/rightmost dependent of m; s[2] = nearest sibling of s
   towards h; b = boundary between the conjoined phrases (index of the
   first word of the right phrase). variables: dir = direction of the arc,
   normalized by norm; dist = distance (h-m), normalized; size = number of
   words in the dependency tree. functions: word = word at index; pos =
   pos at index; norm = normalize absolute value into 1, 2, 3, 4, 5, (5,
   10], (10, 20], (20, 40], 40+.

   [445]figure
   figure   3    feature template example.
   4.3   the decoding algorithm

   the decoding algorithm is similar to that of the id35 system, where an
   agenda is a priority queue for edges to expand, and chart is a
   fixed-size beam for a list of accepted edges. during initialization,
   input words are assigned possible pos tags, resulting in a set of leaf
   edges that are put onto the agenda. for words with pos constraints,
   only the allowed pos tag is assigned. for unconstrained words, we
   assign all possible pos tags according to a tag dictionary compiled
   from the training data, following standard practice for pos-tagging
   (ratnaparkhi [446]1996).

   when an edge is expanded, it is combined with all edges in the chart in
   all possible ways to generate new edges. two edges can be combined by
   concatenation of the surface strings in both orders and, in each case,
   constructing a dependency link between their heads in two ways
   (corresponding to the two options for the head of the new link). when
   there is a head constraint on the dependent word, a dependency link can
   be constructed only if it is consistent with the constraint. this
   algorithm implements abstract word ordering, partial-tree
   linearization, and full tree linearization   all generalized word
   ordering tasks   in a unified method.

   pseudocode for the decoder is shown as algorithm 4. many of the
   functions have the same definition as for algorithm 1: initagenda,
   initchart, timeout, popbest, goaltest, add. cancombine checks whether
   two edges do not contain an overlapping word (i.e., they do not contain
   a word more times than its count in the input); unlike the id35 case,
   all pairs of words are allowed to combine according to the dependency
   model. combine creates a dependency link between two words, with the
   word order determined by the order in which the arguments are supplied
   to the function, and the head coming from either the first (headleft)
   or second (headright) argument (so there are four combinations
   considered and combine is called four times in algorithm 4).

   4.4   the learning algorithm

   as for the id35 system, an online large-margin learning algorithm based
   on the decoding process is used to train the model. at each step, the
   expanded edge e is compared with the gold standard. if it is a gold
   edge, decoding continues; otherwise e is taken as a negative example
   e[   ] and the lowest-scored gold edge in the agenda is taken as a
   positive example e[+], and a parameter update is executed (repeated
   here from [447]section 3.4):

   the training process is essentially the same as in algorithm 3, but
   with the id35 grammar and model replaced with the dependency-based
   grammar and model.

   in our conference paper describing the earlier version of the
   dependency-based system (zhang [448]2013), the decoding step is
   finished immediately after the parameter update; in this article we
   expand the negative example, as in algorithm3, putting it onto the
   chart and thereby exploring a larger part of the search space (in
   particular that part containing negative examples). our later
   experiments show that this method yields improved results, consistent
   with the id35 system.
   5.   experiments
   section:
   [choose________________________]
   [449]previous section [450]next section

   we use id35bank (hockenmaier and steedman [451]2007) and the penn
   treebank (marcus, santorini, and marcinkiewicz [452]1993) for id35 and
   dependency data, respectively. id35bank is the id35 version of the penn
   treebank. standard splits were used for both: sections 02   21 for
   training, section 00 for development, and section 23 for the final
   test. [453]table 3 gives statistics for the id32.

   [454]table
   table   3    training, development, and test data from the id32.

   for the id35 experiments, original sentences from id35bank are
   transformed into bags of words, with sequence information removed, and
   passed to our system as input data. the system outputs are compared to
   the original sentences for evaluation.

   following wan et al. ([455]2009), we use the id7 metric (papineni et
   al. [456]2002) for string comparison. although id7 is not the perfect
   measure of fluency or grammaticality, being based on id165 precision,
   it is currently widely used for automatic evaluation and allows us to
   compare directly with existing work (wan et al. [457]2009). note also
   that one criticism of id7 for evaluating machine translation systems
   (i.e., that it can only register exact matches between the same words
   in the system and reference translation), does not apply here, because
   the system output always contains the same words as the original
   reference sentence. for the dependency-based experiments, gold-standard
   dependency trees were derived from bracketed sentences in the treebank
   using the penn2malt tool.[458]^2

   for fair comparison with wan et al. ([459]2009), we keep base nps as
   atomic units when preparing the input. wan et al. used base nps from
   the id32 annotation, and we follow this practice for the
   dependency-based experiments. for the id35 experiments we extract base
   nps from id35bbank by taking as base nps those nps that do not
   recursively contain other nps. these base nps mostly correspond to the
   base nps from the id32: in the training data, there are
   242,813 id32 base nps with an average size of 1.09, and
   216,670 id35bank base nps with an average size of 1.19.
   5.1   convergence of training

   the plots in [460]figure 4 show the development test scores of three
   id35 models by the number of training iterations. the three curves
   represent the scaled model of this article, the online large-margin
   model from zhang, blackwood, and clark ([461]2012), and the id88
   model from zhang and clark ([462]2011), respectively. for each curve,
   the id7 score generally increases as the number of training iterations
   increases, until it reaches its maximum at a particular iteration. we
   use the number of training iterations that gives the best development
   test scores for the training of our model when testing on the test
   data.
   [463]figure
   figure   4    id7 scores of the id88, large-margin, and scaled
   large-margin id35 models by the number of training iterations.

   another way to observe the convergence of training is to measure the
   training times for each iteration at different numbers of iterations.
   the per-iteration training times for the large-margin and the scaled
   id35 models are shown in [464]figure 5. for each model, the training
   time for each iteration decreases as the number of training iterations
   increases, reflecting the convergence of learning-guided search. when
   the model gets better, fewer non-gold hypotheses are expanded before
   gold hypotheses, and hence it takes less time for the decoder to find
   the gold goal edge. [465]figure 6 shows the corresponding curve for
   dependency-based word ordering, with similar observations.
   [466]figure
   figure   5    training times of the large-margin model and the scaled id35
   models by the number of training iterations.
   [467]figure
   figure   6    training times of the large-margin and scaled dependency
   models by the number of training iterations.

   because of the expanding of negative examples, the systems of this
   article took more time to train than those of our previous conference
   papers. however, the convergence rate is also faster when negative
   training examples are expanded, as demonstrated by the rate of speed
   improvement as the number of training iterations increases. the
   training times of the id88 algorithm are close to those of the
   large-margin algorithm, and hence are omitted from [468]figures 5 and
   [469]6. the new model gives the best development test scores, as shown
   in [470]figure 4. the next section investigates the effects of two of
   the innovations of this article: use of negative examples during
   training and the scaling of the model by hypothesis size.
   5.2   the effect of the scaled model and negative examples

   [471]table 4 shows a set of id35 development experiments to measure the
   effect of the scaled model and the expansion of negative examples
   during training. with the standard linear model (zhang, blackwood, and
   clark [472]2012) and no expansions of negative examples, our system
   obtained a id7 score of 39.04. the scaled model improved the id7
   score by 1.41 id7 points to 40.45, and the expansion of negative
   examples gave a further improvement of 3.02 id7 points.

   [473]table
   table   4    the effect of the scaled model and expansion of negative
   examples during training for the id35 system.

   these id35 development experiments show that the expansion of negative
   examples during training is an important factor in achieving good
   performance. when no negative examples are expanded, the higher score
   of the scaled linear model demonstrates the effectiveness of fair
   comparison between edges with different sizes. however, it is a more
   important advantage of the scaled linear model that it allows the
   expansion of negative examples during training, which was not possible
   with the standard linear model. in the latter case, training failed to
   converge when negative examples were expanded, reflecting the
   limitations of the standard linear model in separating the training
   data. similar results were found for dependency-based word ordering,
   where the best development id7 score improved from 44.71 (zhang
   [474]2013) to 46.44 with the expansion of negative training examples.
   5.3   the effect of search time

   [475]figure 7 shows the id7 scores for the id35 system on the
   development data when the timeout limit for decoding a single sentence
   is set to 5 sec, 10 sec, 15 sec, 20 sec, 30 sec, 40 sec, 50 sec, and 60
   sec, respectively. the timeout was applied during decoding at test
   time. the scaled model with negative training examples was used for
   this set of experiments, and the same model was used for all timeout
   settings. the results demonstrate that better outputs can be recovered
   given more search time, which is expected for a time-constrained
   best-first search framework. recall that output is created greedily by
   combining the largest available edges, when the system times out.
   similar results were obtained with the dependency-based system of zhang
   ([476]2013), where the development id7 scores improved from 42.89 to
   43.42, 43.58, and 43.72 when the timeout limit increased from 5 sec to
   10 sec, 30 sec, and 60 sec, respectively. the scaled dependency-based
   model without expansion of negative examples was used in this set of
   experiments.
   [477]figure
   figure   7    the effect of search time for the id35 system on the
   development test data.
   5.4   example outputs

   example output for sentences in the development set is shown in
   [478]tables 5 and [479]6, grouped by sentence length. the id35 systems
   of our previous conference papers and this article are compared, all
   with the timeout value set to 5 sec. all three systems perform
   relatively better with smaller sentences. for longer sentences, the
   fluency of the output is significantly reduced. one source of errors is
   confusion between different noun phrases, and where they should be
   positioned, which becomes more severe with increased sentence length
   and adds to the difficulty in reading the outputs. the system of this
   article gave observably improved outputs compared with the two other
   systems.

   [480]table
   table   5    example development output for the id35-based systems and
   sentences with fewer than 20 words.

   [481]table
   table   6    example development output for the id35-based systems and
   sentences with more than 20 words.

   5.5   partial-tree linearization

   in the previous section, the same input settings were used for both
   training and testing, and the assumption was made that the input to the
   system would be a bag of words, with no constraints on the output
   structure. this somewhat artificial assumption allows a standardized
   evaluation but, as discussed previously, text generation applications
   are unlikely to satisfy this assumption and, in practice, the
   realization problem is likely to be easier compared with our previous
   set-up. in this section, we simulate practical situations in
   dependency-based pipelines by measuring the performance of our system
   using randomly chosen input pos tags and dependency relations. for
   maximum flexibility, so that the same system can be applied to
   different input scenarios, our system is trained without input pos tags
   or dependencies. however, if pos tags and dependencies are made
   available during testing, they will be used to provide hard constraints
   on the output (i.e., the output sentence with pos tags and dependencies
   must contain those in the input). from the perspective of search, input
   pos tags and dependencies greatly constrain the search space and lead
   to an easier search problem, with correspondingly improved outputs.

   [482]table 7 shows a set of development results with varying amounts of
   pos and dependency information in the input. for each test, we randomly
   sampled a percentage of words for which the gold-standard pos tags or
   dependencies are given in the input. as can be seen from the table,
   increased amounts of pos and dependency information in the input lead
   to higher id7 scores, and dependencies were more effective than pos
   tags in determining the word order in the output. when all pos tags and
   dependencies are given, our constraint-enabled system gave a id7 score
   of 76.28.[483]^3

   [484]table
   table   7    development id7 scores for partial-tree linearization, with
   different proportions of input pos and dependency information randomly
   selected from full gold-standard trees.

   [485]table 8 shows the output of our system for the first nine
   development test sentences with different input settings. these
   examples illustrate the positive effect of input dependencies in
   specifying the outputs. consider the second sentence as an example.
   when only input words are given, the output of the system is largely
   grammatical but nonsensical. with increasing amounts of dependency
   relations, the output begins to look more fluent, sometimes with the
   system reproducing the original sentence when all dependencies are
   given.

   [486]table
   table   8    partial-tree linearization outputs for the first nine
   development test sentences with various input information.

   5.6   final results

   [487]table 9 shows the test results of various systems. for the system
   of this article, we take the optimal setting from the development
   tests, using the scaled linear model and expansion of negative examples
   during training. for direct comparison with previous work, the timeout
   threshold was set to 5 sec. our new system of this article
   significantly outperforms all previous systems and achieves the best
   published id7 score on this task. it is worth noting that our systems
   without a language model outperform the system of our 2012 paper using
   a large-scale language model.

   [488]table
   table   9    final test results on the standard word ordering task.

   interestingly, the dependency-based systems performed better than the
   id35 systems of this article. one of the main reasons is that the id35
   systems generated shorter outputs by not finding full spanning
   derivations for a larger proportion of inputs. because of the rigidity
   in combinatory rules, not all hypotheses in the chart can be combined
   with the hypothesis being expanded, leading to an increased likelihood
   of full spanning derivations being unreachable. overall, the id35 system
   recovered 93.98% of the input words in the test set, and the dependency
   system recovered 97.71%.
   5.7   shared task evaluation

   the previous sections report evaluations on the task of word ordering,
   an abstract yet fundamental problem in text generation. one question
   that is not addressed by these experiments is how the abstract task can
   be utilized to benefit full text generation, for which more
   considerations need to be taken into account in addition to word
   ordering. we investigate this question using the 2011 generation
   challenge shared task data, which provide a common-ground for the
   evaluation of text generation systems (belz et al. [489]2011).

   the data are based on the conll 2008 shared task data (surdeanu et al.
   [490]2008), which consist of selected sections of the penn wsj
   treebank, converted to syntactic dependencies via the lth tool
   (johansson and nugues [491]2007). [492]sections 2   21 are used for
   training, section 24 for development, and section 23 for testing. a
   small number of sentences from the original wsj sections are not
   included in this set. the input format of the shared task is an
   unordered syntactic dependency tree, with nodes being lemmas, and
   dependency relations on the arcs. named entities and hyphenated words
   are broken into individual nodes, and special dependency links are used
   to mark them. information on coarse-grained pos, number, tense, and
   participle features is given to each node where applicable. the output
   is a fully ordered and inflected sentence.

   we developed a full-text generation system according to this task
   specification, with the core component being the dependency-based word
   ordering system of [493]section 4. in addition to minor engineering
   details that were required to adapt the system to this new task, one
   additional task that the generation system needs to carry out is
   morphological generation   finding the appropriate inflected form for
   each input lemma. our approach is to perform joint word ordering and
   inflection using the learning-guided search framework, letting one
   statistical model decide the best order as well as the inflections of
   ambiguous lemmas. for a lemma, we generate one or more candidate
   inflections by using a lexicon and a set of inflection rules. candidate
   inflections for an input lemma are generated according to the lemma
   itself and its input attributes, such as the number and tense. some
   lemmas are unambiguous, which are inflected before being passed to the
   word ordering system. for the other lemmas, more than one candidate's
   inflections are passed as input words to the word ordering system. to
   ensure that each lemma occurs only once in the output, a unique id is
   given to all the inflections of the same lemma, making them mutually
   exclusive.

   four types of lemmas need morphological generation, including nouns,
   verbs, adjectives, and miscellaneous cases. the last category includes
   a (a or an) and not (not or n't), for which the best inflection can be
   decided only when id165 information is available. for these lemmas, we
   pass all possible inflections to the search module. for nouns and
   adjectives, the inflection is relatively straightforward, since the
   number (e.g., singular, plural) of a lemma is given as an attribute of
   the input node, and comparative and superlative adjectives have
   specific parts of speech. for those cases where the necessary
   information is not available from the input, all possible inflections
   are handed over to the search module for further disambiguation. the
   most ambiguous lemma types are verbs, which can be further divided into
   be and other verbs. the uniqueness of be is that the inflections for
   the first and second person can be different. all verb inflections are
   disambiguated according to the tense and participle attributes of the
   input node. in addition, for verbs in the present tense, the subject
   needs to be determined in order to differentiate between third-person
   singular verbs and others. this can be straightforward when the subject
   is a noun or pronoun, but can be ambiguous when the subject is a
   wh-pronoun, in which case the real subject might not be directly
   identifiable from the dependency tree. we leave all possible
   inflections of be and other verbs to the word ordering system whenever
   the ambiguity is not directly solvable from the subject dependency
   link. overall, the pre-processing step generates 1.15 inflections for
   each lemma on average.

   for word ordering, the search procedure of algorithm 4 is applied
   directly, and the feature templates of [494]table 2 are used with
   additional labeled dependency features described subsequently. the main
   reason that the dependency-based word ordering algorithm can perform
   joint id60 is that it uses rich syntactic and
   id165 features to score candidate hypotheses, which can also
   differentiate between correct and incorrect inflections under
   particular contexts. for example, an honest person and a honest person
   can be differentiated by id165 features, while tom and sally is and
   tom and sally are can be differentiated by higher-order dependency
   features.

   in addition to lemma-formed inputs, one other difference between the
   shared task and the word ordering problem solved by algorithm 4 is that
   the former uses labeled dependencies whereas algorithm 4 constructs
   unlabeled dependency trees. we address this issue by assigning
   dependency labels in the construction of dependency links, and applying
   an extra set of features. the new features are defined by making a
   duplicate of all the features from [495]table 2 that contain dir
   information, and associating each feature in the new copy with a
   dependency label.

   the training of the word ordering system requires fully ordered
   dependency trees, while references in the shared task data are raw
   sentences. we perform a pre-processing step to obtain gold-standard
   training data by matching the input lemmas to the reference sentence in
   order to obtain their gold-standard order. more specifically, given a
   training instance, we generate all candidate inflections for each
   lemma, resulting in an exponential set of possible mappings between the
   input tree and the reference sentence. we then prune these mappings
   bottom   up, assuming that the dependency tree is projective, and
   therefore that each word dominates a continuous span in the reference.
   after such pruning, only one correct mapping is found for the majority
   of the cases. for the cases where more than one mapping is found, we
   randomly choose one as the gold-standard. there are also instances for
   which no correct ordering can be found, and these are mostly due to
   non-projectivity in the shared task data, with a few cases being due to
   conflicts between our morphological generation system and the shared
   task data, or inconsistency in the data itself. out of the 39k training
   instances, 2.8k conflicting instances are discarded, resulting in 36.2k
   gold-standard ordered dependency trees.

   [496]table 10 shows the results of our system and the top two
   participating systems of the shared task. our system outperforms the
   stumaba system by 0.5 id7 points, and the dcu system by 3.8 id7
   points. more evaluation of the system was published in song et al.
   ([497]2014).

   [498]table
   table   10    results and comparison with the top-performing systems on the
   shared task data

   6.   related work
   section:
   [choose________________________]
   [499]previous section [500]next section

   there is a recent line of research on text-to-text generation, which
   studies the linearization of dependency structures (barzilay and
   mckeown [501]2005; filippova and strube [502]2007, [503]2009; he et al.
   [504]2009; bohnet et al. [505]2010; guo, hogan, and van genabith
   [506]2011). on the other hand, wan et al. ([507]2009) study the
   ordering of a bag of words without any dependency information given. we
   generalize the word ordering problem, and formulate it as a task of
   ordering a multi-set of words, regardless of input syntactic
   constraints.

   our bottom   up, chart-based generation algorithm is inspired by the line
   of work on chart-based realization (kay [508]1996; carroll et al.
   [509]1999; white [510]2004, [511]2006; carroll and oepen [512]2005).
   kay ([513]1996) first proposed the concept of chart realization,
   drawing analogies between realization and parsing of free order
   languages. he discussed efficiency issues and provided solutions to
   specific problems. for the task of realization, efficiency improvement
   has been further investigated (carroll et al. [514]1999; carroll and
   oepen [515]2005). the inputs to these systems are logical forms, which
   form natural constraints on the interaction between edges. in our case,
   one constraint that has been leveraged in the dependency system is a
   projectivity assumption   we assume that the dependents of a word must
   all have been attached before the word is attached to its head word,
   and that spans do not cross during combination. in addition, we assume
   that the right dependents of a word must have been attached before a
   left dependent of the word is attached. this constraint avoids spurious
   ambiguities. the projectivity assumption is an important basis for the
   feasibility of the dependency system; it is similar to the chunking
   constraints of white ([516]2006) for id35-based realization.

   white ([517]2004) describes a system that performs id35 realization
   using best-first search. the search process of our algorithm is similar
   to that work, but the input is different: logical forms in the case of
   white ([518]2004) and bags of words in our case. further along this
   line, espinosa, white, and mehay ([519]2008) also describe a id35-based
   realization system, applying    hypertagging      a form of id55   to
   logical forms in order to make use of id35 lexical categories in the
   realization process. white and rajkumar ([520]2009) further use
   id88 reranking on n-best realization output to improve the
   quality.

   the use of id88 learning to improve search has been proposed in
   guided learning for easy-first search (shen, satta, and joshi
   [521]2007) and laso (daum   and marcu [522]2005). laso is a general
   framework for various search strategies. our learning algorithm is
   similar to laso with best-first id136, but the parameter updates
   are different. in particular, laso updates parameters when all correct
   hypotheses are lost, but our algorithm makes an update as soon as the
   top item from the agenda is incorrect. our algorithm updates the
   parameters using a stronger precondition, because of the large search
   space. given an incorrect hypothesis, laso finds the corresponding gold
   hypothesis for a id88 update by constructing its correct sibling.
   in contrast, our algorithm takes the lowest scored gold hypothesis
   currently in the agenda to avoid updating parameters for hypotheses
   that may have not been constructed.

   our parameter update strategy is closer to the guided learning
   mechanism for the easy-first algorithm of shen, satta, and joshi
   ([523]2007), which maintains a queue of hypotheses during search, and
   performs learning to ensure that the highest-scored hypothesis in the
   queue is correct. however, in easy-first search, hypotheses from the
   queue are ranked by the score of their next action, rather than the
   hypothesis score. moreover, shen, satta, and joshi use aggressive
   learning and regenerate the queue after each update, but we perform
   non-aggressive learning, which is faster and is more feasible for our
   large and complex search space. similar methods to shen, satta, and
   joshi ([524]2007) have also been used in shen and joshi ([525]2008) and
   goldberg and elhadad ([526]2010).

   another framework that closely integrates learning and search is searn
   (daum  , langford, and marcu [527]2009), which addresses structured
   prediction problems that can be transformed into a series of simple
   classification tasks. the transformation is akin to greedy search in
   the sense that the complex structure is constructed by sequential
   classification decisions. the key problem that searn addresses is how
   to learn the tth decision based on the previous t     1 decisions, so
   that the overall loss in the resulting structure is minimized. similar
   to our framework, searn allows arbitrary features. however, searn is
   more oriented to greedy search, optimizing local decisions. in
   contrast, our framework is oriented to best-first search, optimizing
   global structures.

   learning and search also interact with each other in a global
   discriminative learning and beam-search framework for incremental
   id170 (zhang and clark [528]2011). in this framework,
   an output is constructed incrementally by a sequence of transitions,
   while a beam is used to record the highest scored structures at each
   step. online training is performed based on the search process, with
   the objective function being the margin between correct and incorrect
   structures. the method involves an early-update strategy, which stops
   search and updates parameters immediately when the gold structure falls
   out of the beam during training. it was first proposed by collins and
   roark ([529]2004) for incremental parsing, and later gained popularity
   in the investigations of many nlp tasks, including pos-tagging (zhang
   and clark [530]2010), transition-based id33 (zhang and
   clark [531]2008; huang and sagae [532]2010), and machine translation
   (liu [533]2013). huang, fayong, and guo ([534]2012) propose a
   theoretical analysis to the early-update training strategy, pointing
   out that it is a type of training method that fixes score violations in
   inexact search. when the score of a gold-standard structure is lower
   than that of a non-gold structure, a violation exists. our parameter
   udpate strategy in this article can also be treated as a mechanism for
   violation fixing.
   7.   conclusion
   section:
   [choose________________________]
   [535]previous section [536]next section

   we investigated the general task of syntax-based word ordering, which
   is a fundamental problem for text generation, and a computationally
   very expensive search task. we provide a principled solution to this
   problem using learning-guided search, a framework that is applicable to
   other nlp problems with complex search spaces. we compared different
   methods for parameter updates, and showed that a scaled linear model
   gave the best results by allowing better comparisons between phrases of
   different sizes, increasing the separability of hypotheses and enabling
   the expansion of negative examples during training.

   we formulate abstract word ordering as a spectrum of tasks with varying
   input specificity, from    pure    word ordering without any syntactic
   information to fully-informed word ordering with a complete unordered
   dependency tree given. experiments show that our proposed method can
   effectively use available input constraints in generating output
   sentences.

   evaluation on the id86 2011 shared task data shows that our system can
   be successfully applied to a more realistic application scenario, in
   particular one where some dependency constraints are provided in the
   input and word inflection is required as well as word ordering.
   additional tasks that may be required in a practical text generation
   scenario include word selection, including the determination of content
   words and generation of function words. the joint modeling solution
   that we have proposed for word ordering and inflection could also be
   adopted for word selection, although the search space is greatly
   increased when the words themselves need deciding, particularly content
   words.
   acknowledgments
   section:
   [choose________________________]
   [537]previous section [538]next section

   this work was carried out partly while yue zhang was a postdoctoral
   research associate at the university of cambridge computer laboratory,
   where he and stephen clark were supported by the european union seventh
   framework programme (fp7-ict-2009-4) under grant agreement no. 247762,
   and partly after yue zhang joined singapore university of technology
   and design, where he was supported by the moe grant t2-moe-2013-01. we
   thank bill byrne, marcus tomalin, adri   de gispert, and graeme
   blackwood for numerous discussions; anja belz and mike white for kindly
   providing the id86 2011 shared task data; kai song for helping with
   tables and figures in the draft; yijia liu for helping with the
   bibliography; and the anonymous reviewers for the many constructive
   comments that have greatly improved this article since the first draft.
   notes
   section:
   [choose________________________]
   [539]previous section [540]next section

   1    an example proof can be based on induction, where the basis is that
   the agenda contains all gold leaf edges at first, and the induction
   step is based on edge combination.

   2    [541]http://w3.msi.vxu.se/  nivre/research/penn2malt.html.

   3    when all pos tags and dependencies are also provided during
   training, the id7 score is reduced to 74.79, showing the value in the
   system, which can adapt to varying amounts of pos and dependency
   information in the input at test time.
   references
   section:
   [choose________________________]
   [542]previous section [543]next section
   auli, michael and adam lopez. 2011. training a log-linear parser with
   id168s via softmax-margin. in proceedings of the 2011
   conference on empirical methods in natural language processing, pages
   333   343, edinburgh. [544]google scholar
   barzilay, regina and kathleen mckeown. 2005. sentence fusion for
   multidocument news summarization. computational linguistics,
   31(3):297   328. [545]link, [546]google scholar
   belz, anja, michael white, dominic espinosa, eric kow, deirdre hogan,
   and amanda stent. 2011. the first surface realisation shared task:
   overview and evaluation results. in proceedings of the 13th european
   workshop on id86, eid86 '11, pages 217   226,
   stroudsburg, pa. [547]google scholar
   blackwood, graeme, adri   de gispert, and william byrne. 2010. fluency
   constraints for minimum bayes-risk decoding of statistical machine
   translation lattices. in proceedings of the 23rd international
   conference on computational linguistics (coling 2010), pages 71   79,
   beijing. [548]google scholar
   bohnet, bernd, leo wanner, simon mill, and alicia burga. 2010. broad
   coverage multilingual deep sentence generation with a stochastic
   multi-level realizer. in proceedings of the 23rd international
   conference on computational linguistics (coling 2010), pages 98   106,
   beijing. [549]google scholar
   bos, johan, stephen clark,mark steedman, james r. curran, and julia
   hockenmaier. 2004. wide-coverage semantic representations from a id35
   parser. in proceedings of coling-04, pages 1240   1246, geneva.
   [550]crossref, [551]google scholar
   caraballo, sharon a. and eugene charniak. 1998. new figures of merit
   for best-first probabilistic chart parsing. computational linguistics,
   24:275   298. [552]google scholar
   carroll, john, ann copestake, dan flickinger, and victor poznanski.
   1999. an efficient chart generator for (semi-) lexicalist grammars. in
   proceedings of the 7th european workshop on id86
   (ewid8699), pages 86   95, toulouse. [553]google scholar
   carroll, john and stephan oepen. 2005. high efficiency realization for
   a wide-coverage unification grammar. in proceedings of the second
   international joint conference on natural language processing,
   ijcnlp'05, pages 165   176, berlin. [554]google scholar
   chang, pi-chuan and kristina toutanova. 2007. a discriminative
   syntactic word order model for machine translation. in proceedings of
   acl, pages 9   16, prague. [555]google scholar
   chiang, david. 2007. hierarchical phrase-based translation.
   computational linguistics, 33(2):201   228. [556]link, [557]google
   scholar
   clark, stephen and james r. curran. 2007a. id88 training for a
   wide-coverage lexicalized-grammar parser. in proceedings of the acl
   2007 workshop on deep linguistic processing, pages 9   16, prague.
   [558]google scholar
   clark, stephen and james r. curran. 2007b. wide-coverage efficient
   statistical parsing with id35 and id148. computational
   linguistics, 33(4):493   552. [559]link, [560]google scholar
   collins, michael and brian roark. 2004. incremental parsing with the
   id88 algorithm. in proceedings of acl, pages 111   118, barcelona.
   [561]google scholar
   crammer, koby, ofer dekel, joseph keshet, shai shalev-shwartz, and
   yoram singer. 2006. online passive-aggressive algorithms. journal of
   machine learning research, 7:551   585. [562]google scholar
   daum  , hal, john langford, and daniel marcu. 2009. search-based
   id170. machine learning, 75(3):297   325.
   [563]crossref, [564]google scholar
   daum  , hal and daniel marcu. 2005. learning as search optimization:
   approximate large margin methods for id170. in icml,
   pages 169   176, bonn. [565]google scholar
   espinosa, dominic, michael white, and dennis mehay. 2008. hypertagging:
   id55 for surface realization with id35. in proceedings of
   acl-08: hlt, pages 183   191, columbus, oh. [566]google scholar
   filippova, katja and michael strube. 2007. generating constituent order
   in german clauses. in proceedings of the 45th annual meeting of the
   association of computational linguistics, pages 320   327, prague.
   [567]google scholar
   filippova, katja and michael strube. 2009. tree linearization in
   english: improving language model based approaches. in proceedings of
   human language technologies: the 2009 annual conference of the north
   american chapter of the association for computational linguistics,
   companion volume: short papers, pages 225   228, boulder, co.
   [568]crossref, [569]google scholar
   fowler, timothy a. d. and gerald penn. 2010. accurate context-free
   parsing with id35. in proceedings of the 48th
   annual meeting of the association for computational linguistics, pages
   335   344, uppsala. [570]google scholar
   goldberg, yoav and michael elhadad. 2010. an efficient algorithm for
   easy-first non-directional id33. in human language
   technologies: the 2010 annual conference of the north american chapter
   of the association for computational linguistics, pages 742   750, los
   angeles, ca. [571]google scholar
   guo, yuqing, deirdre hogan, and josef van genabith. 2011. dcu at
   generation challenges 2011 surface realisation track. in proceedings of
   the generation challenges session at the 13th european workshop on
   id86, pages 227   229, nancy. [572]google scholar
   he, wei, haifeng wang, yuqing guo, and ting liu. 2009. dependency based
   chinese sentence realization. in proceedings of the joint conference of
   the 47th annual meeting of the acl and the 4th international joint
   conference on natural language processing of the afnlp, pages 809   816,
   suntec. [573]google scholar
   hockenmaier, julia. 2003. data and models for statistical parsing with
   id35. ph.d. thesis, school of informatics,
   university of edinburgh. [574]google scholar
   hockenmaier, julia and mark steedman. 2007. id35bank: a corpus of id35
   derivations and dependency structures extracted from the id32.
   computational linguistics, 33(3):355   396. [575]link, [576]google
   scholar
   huang, liang, suphan fayong, and yang guo. 2012. structured id88
   with inexact search. in proceedings of the 2012 conference of the north
   american chapter of the association for computational linguistics:
   human language technologies, pages 142   151, montr  al. [577]google
   scholar
   huang, liang and kenji sagae. 2010. id145 for linear-time
   incremental parsing. in proceedings of acl, pages 1077   1086, uppsala.
   [578]google scholar
   johansson, richard and pierre nugues. 2007. extended
   constituent-to-dependency conversion for english. in 16th nordic
   conference of computational linguistics, pages 105   112, tartu.
   [579]google scholar
   kay, martin. 1996. chart generation. in proceedings of acl, pages
   200   204, santa cruz, ca. [580]google scholar
   koehn, phillip. 2010. id151. cambridge
   university press. [581]google scholar
   koehn, philipp, hieu hoang, alexandra birch, chris callison-burch,
   marcello federico, nicola bertoldi, brooke cowan, wade shen, christine
   moran, richard zens, chris dyer, ondrej bojar, alexandra constantin,
   and evan herbst. 2007. moses: open source toolkit for statistical
   machine translation. in proceedings of the 45th annual meeting of the
   association for computational linguistics companion volume proceedings
   of the demo and poster sessions, pages 177   180, prague. [582]google
   scholar
   koehn, philipp, franz josef och, and daniel marcu. 2003. statistical
   phrase-based translation. in proceedings of the 2003 conference of the
   north american chapter of the association for computational linguistics
   on human language technology - volume 1, naacl '03, pages 48   54,
   edmonton, canada. [583]crossref, [584]google scholar
   koo, terry and michael collins. 2010. efficient third-order dependency
   parsers. in proceedings of acl, pages 1   11, uppsala. [585]google
   scholar
   lee, j. and s. seneff. 2006. automatic grammar correction for
   second-language learners. in proceedings of interspeech, pages
   1978   1981, pittsburgh, pa. [586]google scholar
   liu, yang. 2013. a id132 algorithm for phrase-based
   string-to-dependency translation. in proceedings of the 51st annual
   meeting of the association for computational linguistics (volume 1:
   long papers), pages 1   10, sofia. [587]google scholar
   marcus, mitchell p., beatrice santorini, and mary ann marcinkiewicz.
   1993. building a large annotated corpus of english: the id32.
   computational linguistics, 19(2):313   330. [588]google scholar
   papineni, kishore, salim roukos, todd ward, and wei-jing zhu. 2002.
   id7: a method for automatic evaluation of machine translation. in
   proceedings of the 40th annual meeting of the association for
   computational linguistics, pages 311   318, philadelphia, pa. [589]google
   scholar
   ratnaparkhi, adwait. 1996. a maximum id178 model for part-of-speech
   tagging. in proceedings of emnlp, pages 133   142, somerset, nj.
   [590]google scholar
   reiter, ehud and robert dale. 1997. building applied natural language
   generation systems. natural language engineering, 3(1):57   87.
   [591]crossref, [592]google scholar
   shen, libin and aravind joshi. 2008. ltag id33 with
   bidirectional incremental construction. in proceedings of the 2008
   conference on empirical methods in natural language processing, pages
   495   504, honolulu. hi. [593]google scholar
   shen, libin, giorgio satta, and aravind joshi. 2007. guided learning
   for bidirectional sequence classification. in proceedings of acl, pages
   760   767, prague. [594]google scholar
   song, linfeng, yue zhang, kai song, and qun liu. 2014. joint
   morphological generation and syntactic linearization. in proceedings of
   the twenty-eighth aaai conference, quebec. [595]google scholar
   steedman, mark. 2000. the syntactic process. the mit press, cambridge,
   ma. [596]google scholar
   surdeanu, mihai, richard johansson, adam meyers, llu  s m  rquez, and
   joakim nivre. 2008. the conll-2008 shared task on joint parsing of
   syntactic and semantic dependencies. in proceedings of the twelfth
   conference on computational natural language learning, pages 159   177,
   manchester. [597]crossref, [598]google scholar
   wan, stephen, mark dras, robert dale, and c  cile paris. 2009. improving
   grammaticality in statistical sentence generation: introducing a
   dependency spanning tree algorithm with an argument satisfaction model.
   in proceedings of the 12th conference of the european chapter of the
   acl (eacl 2009), pages 852   860, athens. [599]google scholar
   weir, david. 1988. characterizing mildly context-sensitive grammar
   formalisms. ph.d. thesis, university of pennsylviania. [600]google
   scholar
   white, michael. 2004. reining in id35 chart realization. in proceedings
   of iid86-04, pages 182   191, brockenhurst. [601]google scholar
   white, michael. 2006. efficient realization of coordinate structures in
   id35. research on language & computation,
   4(1):39   75. [602]crossref, [603]google scholar
   white, michael and rajakrishnan rajkumar. 2009. id88 reranking
   for id35 realization. in proceedings of the 2009 conference on empirical
   methods in natural language processing, pages 410   419, singapore.
   [604]crossref, [605]google scholar
   wu, dekai. 1997. stochastic inversion transduction grammars and
   bilingual parsing of parallel corpora. computational linguistics,
   23(3):377   403. [606]google scholar
   xu, peng, ciprian chelba, and frederick jelinek. 2002. a study on
   richer syntactic dependencies for structured id38. in
   proceedings of the 40th annual meeting of the association for
   computational linguistics, pages 191   198, philadelphia, pa. [607]google
   scholar
   zettlemoyer, luke s. and michael collins. 2005. learning to map
   sentences to logical form: structured classification with probabilistic
   categorial grammars. in proceedings of the 21st conference on
   uncertainty in artificial intelligence, pages 658   666, edinburgh.
   [608]google scholar
   zhang, yue. 2013. partial-tree linearization: generalized word ordering
   for text synthesis. in proceedings of ijcai, pages 2232   2238, beijing.
   [609]google scholar
   zhang, yue, graeme blackwood, and stephen clark. 2012. syntax-based
   word ordering incorporating a large-scale language model. in
   proceedings of the 13th conference of the european chapter of the
   association for computational linguistics, pages 736   746, avignon.
   [610]google scholar
   zhang, yue and stephen clark. 2008. joint id40 and pos
   tagging using a single id88. in proceedings of acl/hlt, pages
   888   896, columbus, oh. [611]google scholar
   zhang, yue and stephen clark. 2010. a fast decoder for joint word
   segmentation and pos-tagging using a single discriminative model. in
   proceedings of the 2010 conference on empirical methods in natural
   language processing, pages 843   852, cambridge, ma. [612]google scholar
   zhang, yue and stephen clark. 2011. syntactic processing using the
   generalized id88 and id125. computational linguistics,
   37(1):105   151. [613]link, [614]google scholar
   zhang, yue and joakim nivre. 2011. transition-based id33
   with rich non-local features. in proceedings of the 49th annual meeting
   of the association for computational linguistics: human language
   technologies, pages 188   193, portland, or. [615]google scholar
   yue zhang*
   singapore university of technology and design
   stephen clark**
   university of cambridge

   *singapore university of technology and design. 20 dover drive,
   singapore. e-mail: [616][email protected].

   **university of cambridge computer laboratory, william gates building,
   15 jj thomson avenue, cambridge, uk. e-mail: [617][email protected].
   [618]forthcoming
   [619][preview-1489555631417.svg] download options
   [arrow-button-1488499533633.svg]

discriminative syntax-based word ordering for text generation

   [620]yue zhang and [621]stephen clark
   [622]https://doi.org/10.1162/coli_a_00229
   received: april 17, 2013
   accepted: june 22, 2014
   published online: september 08, 2015
     * [623]full text
     * [624]authors
     * [625]pdf
     * [626]pdf plus

abstract

   section:
   [choose________________________]
   [627]next section

   word ordering is a fundamental problem in text generation. in this
   article, we study word ordering using a syntax-based approach and a
   discriminative model. two grammar formalisms are considered:
   id35 (id35) and dependency grammar. given the
   search for a likely string and syntactic analysis, the search space is
   massive, making discriminative training challenging. we develop a
   learning-guided search framework, based on best-first search, and
   investigate several alternative training algorithms.

   the framework we present is flexible in that it allows constraints to
   be imposed on output word orders. to demonstrate this flexibility, a
   variety of input conditions are considered. first, we investigate a
      pure    word-ordering task in which the input is a multi-set of words,
   and the task is to order them into a grammatical and fluent sentence.
   this task has been tackled previously, and we report improved
   performance over existing systems on a standard wall street journal
   test set. second, we tackle the same reordering problem, but with a
   variety of input conditions, from the bare case with no dependencies or
   pos tags specified, to the extreme case where all pos tags and
   unordered, unlabeled dependencies are provided as input (and various
   conditions in between). when applied to the id86 2011 shared task, our
   system gives competitive results compared with the best-performing
   systems, which provide a further demonstration of the practical utility
   of our system.
      2015 association for computational linguistics
   1.   introduction
   section:
   [choose________________________]
   [628]previous section [629]next section

   word ordering is a fundamental problem in id86
   (id86, reiter and dale [630]1997). in this article we focus on text
   generation: starting with a bag of words, or lemmas, as input, the task
   is to generate a fluent and grammatical sentence using those words.
   additional annotation may also be provided with the input   for example,
   part-of-speech (pos) tags or syntactic dependencies. applications that
   can benefit from better text generation algorithms include machine
   translation (koehn [631]2010), abstractive text summarization (barzilay
   and mckeown [632]2005), and grammar correction (lee and seneff
   [633]2006). typically, id151 (smt) systems
   (chiang [634]2007; koehn [635]2010) perform generation into the target
   language as part of an integrated system, which avoids the high
   computational complexity of independent word ordering. on the other
   hand, performing word ordering separately in a pipeline has many
   potential advantages. for smt, it offers better modularity between
   adequacy (translation) and fluency (linearization), and can potentially
   improve target grammaticality for syntactically different languages
   (e.g., chinese and english). more importantly, a stand-alone word
   ordering component can in principle be applied to a wide range of text
   generation tasks, including transfer-based machine translation (chang
   and toutanova [636]2007).

   most word ordering systems use an id165 language model, which is
   effective at controling local fluency. syntax-based language models, in
   particular dependency language models (xu, chelba, and jelinek
   [637]2002), are sometimes used in an attempt to improve global fluency
   through the capturing of long-range dependencies. in this article, we
   take a syntax-based approach and consider two grammar formalisms:
   id35 (id35) and dependency grammar. our system
   also employs a discriminative model. coupled with heuristic search, a
   strength of the model is that arbitrary features can be defined to
   capture complex syntactic patterns in output hypotheses. the
   discriminative model is trained using syntactically annotated data.

   from the perspective of search, word ordering is a computationally
   difficult problem. finding the best permutation for a set of words
   according to a bigram language model, for example, is np-hard, which
   can be proved by linear reduction from the traveling salesman problem.
   in practice, exploring the whole search space of permutations is often
   prevented by adding constraints. in phrase-based machine translation
   (koehn, och, and marcu [638]2003; koehn et al. [639]2007), a distortion
   limit is used to constrain the position of output phrases. in
   syntax-based machine translation systems such as wu ([640]1997) and
   chiang ([641]2007), synchronous grammars limit the search space so that
   polynomial-time id136 is feasible. in fluency improvement
   (blackwood, de gispert, and byrne [642]2010), parts of translation
   hypotheses identified as having high local confidence are held fixed,
   so that word ordering elsewhere is strictly local.

   in this article we begin by proposing a general system to solve the
   word ordering problem, which does not rely on constraints (which are
   typically task-specific). in particular, we treat syntax-based word
   ordering as a id170 problem, for which the input is a
   multi-set (bag) of words and the output is an ordered sentence,
   together with its syntactic analysis (either id35 derivation or
   dependency tree, depending on the grammar formalism being used). given
   an input, our system searches for the highest-scored output, according
   to a syntax-based discriminative model. one advantage of this
   formulation of the reordering problem, which can perhaps be thought of
   as a    pure    text realization task, is that systems for solving it are
   easily evaluated, because all that is required is a set of sentences
   for reordering and a standard evaluation metric such as id7 (papineni
   et al. [643]2002). however, one potential criticism of the    pure   
   problem is that it is unclear how it relates to real realization tasks,
   since in practice (e.g., in id151 systems)
   the input does provide constraints on the possible output orderings.
   our general formulation still allows task-specific contraints to be
   added if appropriate. hence as a test of the flexibility of our system,
   and a demonstration of the applicability of the system to more
   realistic text generation scenarios, we consider two further tasks for
   the dependency-based realization system.

   the first task considers a variety of input conditions for the
   dependency-based system, determined by two parameters. the first is
   whether pos information is provided for each word in the input
   multi-set. the second is whether syntactic dependencies between the
   words are provided. the extreme case is when all dependencies are
   provided, in which case the problem reduces to the tree linearization
   problem (filippova and strube [644]2009; he et al. [645]2009). however,
   the input can also lie between the two extremes of no- and
   full-dependency information.

   the second task is the id86 2011 shared task, which provides a further
   demonstration of the practical utility of our system. the shared task
   is closer to a real realization scenario, in that lemmas, rather than
   inflected words, are provided as input. hence some modifications are
   required to our system in order that it can perform some word
   inflection, as well as deciding on the ordering. the shared task data
   also uses labeled, rather than unlabeled, syntactic dependencies, and
   so the system was modified to incorporate labels. the final result is
   that our system gives competitive id7 scores, compared to the
   best-performing systems on the shared task.

   the id170 problem we solve is a very hard problem. due
   to the use of syntax, and the search for a sentence together with a
   single id35 derivation or dependency tree, the search space is
   exponentially larger than the id165 word permutation problem. no
   efficient algorithm exists for finding the optimal solution. kay
   ([646]1996) recognized the computational difficulty of chart-based
   generation, which has many similarities to the problem we address in
   his seminal paper. we tackle the high complexity by using
   learning-guided best-first search, exploring a small path in the whole
   search space, which contains the most likely structures according to
   the discriminative model. one of the contributions of this article is
   to introduce, and provide a discriminative solution to, this difficult
   id170 problem, which is an interesting machine learning
   problem in its own right.

   this article is based on, and significantly extends, three conference
   papers (zhang and clark [647]2011; zhang, blackwood, and clark
   [648]2012; zhang [649]2013). it includes a more detailed description
   and discussion of our guided-search approach to syntax-based word
   ordering, bringing together the id35- and dependency-based systems under
   one unified framework. in addition, we discuss the limitations of our
   previous work, and show that a better model can be developed through
   scaling of the feature vectors. the resulting model allows fair
   comparison of constituents of different sizes, and enables the learning
   algorithms to expand negative examples during training, which leads to
   significantly improved results over our previous work. the competitive
   results on the id86 2011 shared task data are new for this article, and
   demonstrate the applicability of our system to more realistic text
   realization scenarios.

   the contributions of this article can be summarized as follows:
         

   we address the problem of syntax-based word ordering, drawing attention
   to this challenging id38 task and offering a general
   solution that does not rely on constraints to limit the search space.
         

   we present a novel method for solving the word ordering problem that
   gives the best reported accuracies to date on the standard wall street
   journal data.
         

   we show how our system can be used with two different grammar
   formalisms: id35 and dependency grammar.
         

   we show how syntactic constraints can be easily incorporated into the
   system, presenting results for the dependency-based system with a range
   of input conditions.
         

   we demonstrate the applicability of the system to more realistic text
   realization scenarios by obtaining competitive results on the id86 2011
   shared task data, including performing some word inflection as part of
   a joint system that also performs word reordering.
         

   more generally, we propose a learning-guided, best-first search
   algorithm for application of discriminative models to extremely large
   search spaces containing structures of varying sizes. this method could
   be applied to other complex id170 tasks in nlp and
   machine learning.
   2.   overview of the search and training algorithms
   section:
   [choose___________________________]
   [650]previous section [651]next section

   in this section, the id35-based system is used to describe the search
   and training algorithms. however, the same approach can be used for the
   dependency-based system, as described in [652]section 4: instead of
   building hypotheses by applying id35 rules in a bottom-up manner, the
   dependency-based system creates dependency links between words.

   given a bag of words, the goal is to put them into an ordered sentence
   that has a plausible id35 derivation. the search space of the decoding
   problem consists of all possible id35 derivations for all possible word
   permutations, and the search goal is to find the highest-scored
   derivation in the search space. this is an np-hard problem, as
   mentioned in the introduction. we apply learning-guided search to
   address the high complexity. the intuition is that, because the whole
   search space cannot be exhausted in order to find the optimal solution,
   we choose to explore a small area in the search space. a statistical
   model is used to guide the search, so that only a small portion of the
   search space containing the most plausible hypotheses is explored.

   one natural choice for the decoding algorithm is best-first search,
   which uses an agenda to order hypotheses, and expands the
   highest-scored hypothesis on the agenda at each step. the resulting
   hypotheses after each hypothesis expansion are put back on the agenda,
   and the process repeats until a goal hypothesis (a full sentence) is
   found. this search process is guided by the current scores of the
   hypotheses, and the search path will contain the most plausible
   hypotheses if they are scored higher than implausible ones. an
   alternative to best-first search is id67, which makes use of a
   heuristic function to estimate future scores. a* can potentially be
   more efficient given an effective heuristic function; however, it is
   not straightforward to define an admissible and accurate estimate of
   future scores for our problem, and we leave this research question to
   future work.

   in our formulation of the word ordering problem, a hypothesis is a
   phrase or sentence together with its id35 derivation. hypotheses are
   constructed bottom   up: starting from single words, smaller phrases are
   combined into larger ones according to id35 rules. to allow the
   combination of hypotheses, we use an additional structure to store a
   set of hypotheses that have been expanded, which we call accepted
   hypotheses. when a hypothesis from the agenda is expanded, it is
   combined with all accepted hypotheses in all possible ways to produce
   new hypotheses. the data structure for accepted hypotheses is similar
   to that used for best-first parsing (caraballo and charniak [653]1998),
   and we adopt the term chart for this structure. however, note there are
   important differences to the parsing problem. first, the parsing
   problem has a fixed word order and is considerably simpler than the
   word ordering problem we are tackling. second, although we use the term
   chart, the structure for accepted hypotheses is not a dynamic
   programming chart in the same way as for the parsing problem. in our
   previous papers (zhang and clark [654]2011; zhang, blackwood, and clark
   [655]2012), we applied a set of beams to this structure, which makes it
   similar to the data structure used for phrase-based mt decoding (koehn
   [656]2010). however, we will show later that this structure is
   unnecessary when the model has more discriminative power, and a
   conceptually simpler single beam can be used. we will also investigate
   the possibility of applying dynamic-programming-style pruning to the
   chart.

   we now give an overview of the training algorithm, which is crucial to
   both the speed and accuracy of the resulting decoder. id35bank
   (hockenmaier and steedman [657]2007) is used to train the model. for
   each training sentence, the corresponding id35bank derivation together
   with all its sub-derivations are treated as gold-standard hypotheses.
   all other hypotheses that can be constructed from the same bag of words
   are non-gold hypotheses. from the generation perspective this
   assumption is too strong, because sentences can have multiple orderings
   (with multiple derivations) that are both grammatical and fluent.
   nevertheless, it is the most feasible choice given the training data
   available.

   the efficiency of the decoding algorithm is dependent on the training
   algorithm because the agenda is ordered according to the hypothesis
   scores. hence, a better model will lead to a goal hypothesis being
   found more quickly. in the ideal situation, all gold-standard
   hypotheses are scored higher than all non-gold hypotheses, and
   therefore only gold-standard hypotheses are expanded before the
   gold-standard goal hypothesis is found. in this case, the minimum
   number of hypotheses is expanded and the output is correct. the
   best-first search decoder is optimal not only with respect to accuracy
   but also speed. this ideal situation can hardly be met in practice, but
   it determines the goal of the training algorithm: to find a model that
   scores gold-standard hypotheses higher than non-gold ones.

   learning-guided search places more challenges on the training of a
   discriminative model than standard id170 problems, for
   example, cky parsing for id35 (clark and curran [658]2007b). if we take
   gold-standard hypotheses as positive training examples, and non-gold
   hypotheses as negative examples, then the training goal is to find a
   large separating margin between the scores of all positive examples and
   all negative examples. for cky parsing, the highest-scored negative
   example can be found via optimal viterbi decoding, according to the
   current model, and this negative example can be used in place of all
   negative examples during the updating of parameters. in contrast, our
   best-first search algorithm cannot find an output in reasonable time
   unless a good model has already been trained, and therefore we cannot
   run the decoding algorithm in the standard way during training. in our
   previous papers (zhang and clark [659]2011; zhang, blackwood, and clark
   [660]2012), we proposed an approximate online training algorithm, which
   forces positive examples to be kept in the hypothesis space without
   being discarded, and prevents the expansion of negative examples during
   the training process (so that the hypothesis space does not get too
   large). this algorithm ensures training efficiency, but greatly limits
   the space of negative examples that is explored during training (and
   hence fails to replicate the conditions experienced at test time). in
   this article, we will show that, with an improved scoring model, it is
   possible to expand negative examples, which leads to improved
   performance.

   a second and more subtle challenge for our training problem is that we
   need a stronger model for learning-guided search than for dynamic
   programming (dp)   based search, such as cky decoding. for cky decoding,
   the model is used to compare hypotheses within each chart cell, which
   cover the same input words. in contrast, for the best-first search
   decoder, the model is used to order hypotheses on the agenda, which can
   cover different numbers of words. it needs much stronger discriminating
   power, so that it can determine whether a single-word phrase is better
   than, say, a 40-word sentence. in this article we use scaling of the
   hypothesis scores by size, so that hypotheses of different sizes can be
   fairly compared. we also find that, with this new approach, negative
   examples can be expanded during training and a single beam applied to
   the chart, resulting in a conceptually simpler and more effective
   training algorithm and decoder.
   3.   id35-based word ordering
   section:
   [choose___________________________]
   [661]previous section [662]next section
   3.1   the id35 grammar

   we were motivated to use id35 as one of the grammar formalisms for our
   syntax-based realization system because of its successful application
   to a number of related tasks, such as wide-coverage parsing
   (hockenmaier [663]2003; clark and curran [664]2007b; auli and lopez
   [665]2011), id29 (zettlemoyer and collins [666]2005),
   wide-coverage semantic analysis (bos et al. [667]2004), and generation
   itself (espinosa, white, and mehay [668]2008). the grammar formalism
   has been described in detail in those papers, and so here we provide
   only a short description.

   id35 (steedman [669]2000) is a lexicalized grammar formalism that
   associates words with lexical categories. lexical categories are
   detailed grammatical labels, typically expressing subcategorization
   information. during id35 parsing, and during our search procedure,
   categories are combined using id35's combinatory rules. for example, a
   verb phrase in english (s\np) can combine with an np to its left, in
   this case using the combinatory rule of (backward) function
   application:

   in addition to binary rule instances, such as this one, there are also
   unary rules that operate on a single category in order to change its
   type. for example, forward type-raising can change a subject np into a
   complex category looking to the right for a verb phrase:

   such a type-raised category can then combine with a transitive verb
   type using the rule of forward composition:

   following fowler and penn ([670]2010), we extract the grammar by
   reading rule instances directly from the derivations in id35bank
   (hockenmaier and steedman [671]2007), rather than defining the
   combinatory rule schema manually as in clark and curran ([672]2007b).
   hence the grammar we use can be thought of as a context-free
   approximation to the mildly content sensitive grammar arising from the
   use of generalized composition rules (weir [673]1988). hockenmaier
   ([674]2003) contains a detailed description of the grammar that is
   obtained in this way, including the various unary type-changing rules,
   as well as additional rules needed to deal with naturally occurring
   text, such as punctuation rules.
   3.2   the edge data structure

   for the rest of this article, the term edge is used to refer to a
   hypothesis in the decoding algorithm. an edge corresponds to a sentence
   or phrase with a id35 derivation. edges are built bottom   up, starting
   from leaf edges, which are constructed by assigning possible lexical
   categories to input words. each leaf edge corresponds to an input word
   with a particular lexical category. two existing edges can be combined
   if there exists a id35 rule (extracted from id35bank, as described
   earlier) that combines their category labels, and if they do not
   contain the same input word more times than its total count in the
   input. the resulting edge is assigned a category label according to the
   id35 rule, and covers the concatenated surface strings of the two
   sub-edges in their order of combination. new edges can also be built by
   applying unary rules to a single existing edge. we define a goal edge
   as an edge that covers all input words.

   two edges are equivalent if they have the same surface string and
   identical id35 derivations. edge equivalence is used for comparison with
   gold-standard edges. two edges are dp-equivalent when they have the
   same dp-signature. based on the feature templates in [675]table 1, we
   define the dp-signature of an edge as the id35 category at the root of
   its derivation, the head word associated with the root category, and
   the multi-set of words it contains, together with the word and pos
   bigrams on either side of its surface string.

   [676]table
   table   1    feature template definitions, with example instances based on
   [677]figure 2.

   3.3   the scoring of edges

   edges are built bottom   up from input words or existing edges. if we
   treat the assignment of lexical categories to input words and the
   application of unary and binary id35 rules to existing edges as
   edge-building actions, the structure of an edge can be defined
   recursively as the sub-structure resulting from its top action plus the
   structure of its sub-edges (if any), as shown in [678]figure 1. here
   the top action of an edge refers to the most recent action that has
   been applied to build the edge.
   [679]figure
   figure   1    the structure of edges shown recursively.

   in our previous papers we used a global linear model to score edges,
   where the score of an edge e is defined as:

     (e) represents the feature vector of e and is the parameter vector of
   the model.

   similar to the structure of e, the feature vector   (e) can be defined
   recursively:

   in this equation, e[s]     e represents a sub-edge of e. leaf edges do
   not have any sub-edges. unary-branching edges have one sub-edge, and
   binary-branching edges have two sub-edges. represents a (strictly)
   recursive sub-edge of e. the feature vector   (e) represents the
   structure of the top action of e; it is extracted according to the
   feature templates in [680]table 1. example instances of the feature
   templates are given according to the example string and id35 derivation
   in [681]figure 2. for leaf edges,   (e) includes information about the
   lexical category label; for unary-branching edges,   (e) includes
   information from the unary rule; for binary-branching edges,   (e)
   includes information from the binary rule, and additionally the token,
   pos, and lexical category bigrams and trigrams that result from the
   surface string concatenation of its sub-edges.
   [682]figure
   figure   2    example string with its id35 derivation, used to give example
   features in [683]table 1.

   by the given definition of   (e), f(e), the score of edge e, can be
   computed recursively as e is built during the decoding process:

   when the top action is applied, the score of f(e) is computed as the
   sum of f(e[s]) (for all e[s]     e) plus .

   an important aspect of the scoring model is that it is used to compare
   edges with different sizes during decoding. the size of an edge can be
   measured in terms of the number of words it contains, or the number of
   syntax rules in its structure. we define the size of an edge as the
   number of recursive sub-edges in the edge plus one (e.g., the size of a
   leaf edge is 1), which is equivalent to the number of actions (i.e.,
   lexical category assignment for leaf edges, and rule application for
   unary/binary edges) that have been applied to construct the edge. edges
   with different sizes can have significantly different numbers of
   features, which can make the training of a discriminative linear model
   more difficult. note that it is common in id170
   problems for feature vectors to have slightly different sizes because
   of variant feature instantiation conditions. in cky parsing, for
   example, constituents with different numbers of unary rules can be kept
   in the same chart cell and compared with each other, provided that they
   cover the same span in the input. in our case, however, the sizes of
   two feature vectors under comparison can be very different indeed,
   since a leaf edge with one word can be compared with an edge over the
   entire input sentence.

   in our previous papers we observed empirical convergence of online
   learning using this linear model, and obtained competitive results.
   however, as explained in [684]section 2, only positive examples were
   expanded during training, and the expansion of negative examples led to
   non-convergence and made online training infeasible. in this article,
   in order to increase the discriminating power of the model and to make
   use of negative examples during training, we apply length id172
   to the scoring function, so that the score of an edge is independent of
   its size. to achieve this, we scale the original linear model score by
   the number of recursive sub-edges in the edge plus one. for a given
   edge e, the new score is defined as:

   in the equation, |e| represents the size of e, which is equal to the
   number of actions that have been applied when e is constructed. by
   dividing the score f(e) by the size of e, the score represents an
   averaged value of and , averaged by the number of recursive sub-edges
   plus one (i.e., the total actions), and is independent of the size of
   e. given normalized feature vectors, the training of the parameter
   vector needs to be adjusted correspondingly, which will be discussed
   subsequently.
   3.4   the decoding algorithm

   the decoding algorithm takes a multi-set of input words, turns them
   into a set of leaf edges, and searches for a goal edge by repeated
   expansion of existing edges. for best-first decoding, an agenda and a
   chart are used. the agenda is a priority queue on which edges to be
   expanded are ordered according to their current scores. the chart is a
   fixed-size beam used to record a limited number of accepted edges.
   during initialization, leaf edges are generated by assigning all
   possible lexical categories to each input word, before they are put on
   the agenda. during each step in the decoding process, the
   highest-scored edge on the agenda is popped off and expanded. if it is
   a goal edge, it is returned as the output, and the decoding finishes.
   otherwise it is extended with unary rules, and combined with existing
   edges in the chart, using binary rules to produce new edges. the
   resulting edges are scored and put on the agenda, and the original edge
   is put into the chart. the process repeats until a goal edge is found,
   or a timeout limit is reached.

   for the timeout case, a default output is produced by greedily
   combining existing edges in the chart in descending order of size. in
   particular, edges in the chart are sorted by size, and the largest is
   taken as the current default output. then the sorted list is traversed,
   with an attempt to greedily concatenate the current edges in the list
   to the right of the current default output. if the combination is not
   allowed (i.e., the two edges contain some input words more times than
   its count in the input), the current edge is discarded. otherwise, the
   current default output is updated.

   in our previous papers we used a set of beams for the chart, each
   storing a certain number of highest-scored edges that cover a
   particular number of words. this structure is similar to the chart used
   for phrase-based smt decoding. the main reason for the multiple beams
   is the non-comparability of edges in different beams, which can have
   feature vectors of significantly different sizes. in this article,
   however, our chart is a single beam structure containing the top-scored
   accepted edges. this simple data structure is enabled by the use of the
   scaled linear model, and leads to comparable accuracies to the
   multiple-beam chart. in addition to its simplicity, it also fits well
   with the use of agenda-based search, because edges of different sizes
   will ultimately be compared with each other on the agenda.

   we apply dp-style pruning to the chart, keeping only the highest-scored
   edge among those that have the same dp-signature. during decoding,
   before a newly constructued edge e is put into the chart, the chart is
   examined to check whether it contains an existing edge e[0] with the
   same dp-signature as e. if such an edge exists, it is popped off the
   chart and compared with the newly constructed edge e, with the higher
   scored edge being put into the chart and the lower scored edge e    being
   discarded. if the newly constructed edge e is not discarded, then we
   expand e to generate new edges.

   it is worth noting that, in this case, a new edge that results from the
   expansion of e can have dp-equivalent edges in the agenda or the chart,
   which had been generated by expansion of its dp-equivalent predecessor
   e    = e[0]. putting such new edges on the agenda will result in the
   system keeping multiple edges with the same signature. however, because
   applying dp-style pruning to the agenda requires updating the whole
   agenda, and is computationally expensive, we choose to tolerate such
   dp-equivalent duplications in the agenda. pseudocode for the decoder is
   shown as algorithm 1. initagenda returns an initialized agenda with all
   leaf edges. initchart returns a cleared chart. timeout returns true if
   the timeout limit has been reached, and false otherwise. popbest pops
   the top edge from the agenda and returns the edge. goaltest takes an
   edge and returns true if and only if the edge is a goal edge.
   dpchartprune takes an edge e and checks whether there exists in the
   chart an edge e[0] that is dp-equivalent to e. in case e[0] exists, it
   is popped off the chart and compared with e, with the lower scored edge
   e    being discarded, and the higher scored edge being put into the
   chart. the function returns the pair e    and . cancombine checks whether
   two edges can be combined in a given order. two edges can be combined
   if they do not contain an overlapping word (i.e., they do not contain a
   word more times than its count in the input), and their categories can
   be combined according to the id35 grammar. add inserts an edge into the
   agenda or the chart. in the former case, it is placed into the priority
   queue according to its score, and, in the latter case, the lowest
   scored edge in the beam is pruned when the chart is full.

   3.5   the learning algorithm

   we begin by introducing the training algorithm of our previous papers,
   shown in algorithm 2, which has the same fundamental structure as the
   training algorithm of this article but is simpler. the algorithm is
   based on the decoder, where an agenda is used as a priority queue of
   edges to be expanded, and a set of accepted edges is kept in a
   fixed-size chart. the functions initagenda, initchart, timeout,
   popbest, goaltest, dpchartprune, unary, cancombine, and binary are
   identical to those used in the decoding algorithm. goldstandard takes
   an edge and returns true if and only if it is a gold-standard edge.
   mingold returns the lowest scored gold-standard edge in the agenda.
   updateparameters represents the parameter update algorithm.
   recomputescore s updates the scores of edges in the agenda and chart
   after the model is updated.

   similar to the decoding algorithm, the agenda is intialized using all
   possible leaf edges. during each step, the edge e on top of the agenda
   is popped off. if it is a gold-standard edge, it is expanded in exactly
   the same way as in the decoder, with the newly generated edges being
   put on the agenda, and e being inserted into the chart. if e is not a
   gold-standard edge, we take it as a negative example e[   ], and take the
   lowest scored gold-standard edge on the agenda e[+] as a positive
   example, in order to make an update to the parameter vector . note that
   there must exist a gold-standard edge in the agenda, which can be
   proved by contradiction.[685]^1

   the two edges e[+] and e[   ] used to perform a model update can be
   radically different. for example, they may not cover the same words, or
   even the same number of words. this is different from online training
   for cky parsing, for which both positive and negative examples used to
   adjust parameter vectors reside in the same chart cell, and cover the
   same sequence of words. the training goal of a typical cky parser
   (clark and curran [686]2007a, [687]2007b) is to find a large separation
   margin between feature vectors of different derivations of the same
   sentence, which have comparable sizes. our goal is to score all
   gold-standard edges higher than all non-gold edges regardless of their
   size, which is a more challenging goal. after updating the parameters,
   the scores of the agenda edges above and including e[   ], together with
   all chart edges, are updated, and e[   ] is discarded before the start of
   the next processing step.

   one way of viewing the training process is that it pushes gold-standard
   edges towards the top of the agenda, and, crucially, pushes them above
   non-gold edges (zhang and clark [688]2011). given a positive example
   e[+] and a negative example e[   ],a id88-style update is used to
   penalize the score for   (e[   ]) and reward the score of   (e[+]):

   here and denote the parameter vectors before and after the update,
   respectively. this method proved effective empirically (zhang and clark
   [689]2011), but it did not converge well when an id165 language model
   was integrated into the system (zhang, blackwood, and clark [690]2012).

   hence we applied an alternative method for score updates that proved
   more effective than the id88 update and enabled the incorporation
   of a large-scale language model (zhang, blackwood, and clark
   [691]2012). this method treats parameter update as finding a separation
   between gold-standard and non-gold edges. given a positive example e[+]
   and a negative example e[   ], we make a minimum update to the parameters
   so that the score of e[+] is higher than that of e[   ] by a margin of 1:

   the update is similar to the parameter update of online large-margin
   learning algorithms, such as 1-best mira (crammer et al. [692]2006),
   and has a closed-form solution:

   this online learning method proved more effective than the id88
   algorithm empirically, but still has an important shortcoming in that
   it did not provide competitive results when allowing the expansion of
   negative examples during training, which can potentially improve the
   discriminative model (since expanding negative examples can result in a
   more representative sample of the search space). we address this issue
   by introducing a scaled linear model in this article, which, when
   combined with the expansion of negative examples, significantly
   improves performance. we apply the same online large-margin training
   principle; however, the parameter update has to be adjusted for the
   scaled linear model. in particular, the new goal is to find a
   separation between and instead of f[+] and f[   ], for which the
   optimization corresponding to the parameter update becomes:

   where and represent the parameter vectors before and after the update,
   respectively. the equation has a closed-form solution:

   pseudocode for the new training algorithm of this article is shown in
   algorithm 3, where maxnongold returns the highest-scored non-gold edge
   in the chart. in addition to the aforementioned difference in parameter
   updates, new code is added to perform additional updates when
   gold-standard edges are removed from the chart. in our previous work,
   parameter updates happen only when the top edge from the agenda is not
   a gold-standard edge. in this article, the expansion of negative
   training examples will lead to negative examples being put into the
   chart during training, and hence the possibility of gold-standard edges
   being removed from the chart. there are two situations when this can
   happen. first, if a non-gold edge is inserted into the chart, and there
   exists a gold-standard edge in the chart with the same dp-signature but
   a lower score, the gold-standard edge will be removed from the chart
   because of dp-style pruning (since only the highest-scored edge with
   the same dp-signature is kept in the chart).

   second, if the chart is full when a non-gold edge is put into the chart
   (recall that the chart is a fixed-size beam), then the lowest scored
   edge on the chart will be removed. this edge can be a gold-standard
   edge. in both the first and second case, a gold-standard edge is pruned
   as the result of the expansion of a negative example. on the other
   hand, in order for the gold-standard goal edge to be constructed, all
   gold-standard edges that have been expanded must remain in the chart.
   as a result, our training algorithm triggers a parameter update
   whenever a gold-standard edge is removed from the chart, the scores of
   all chart edges are updated, and the original pruned gold edge is
   returned to the chart. the original pruned gold-standard edge is
   treated as the positive example for the update. for the first
   situation, the newly inserted non-gold edge with the same dp-signature
   is taken as the negative example, and will be discarded after the
   parameter update (with a new score that is lower than the new score of
   the corresponding gold-standard). in the second situation, the
   highest-scored non-gold edge in the chart is taken as the negative
   example, and removed from the chart after the update.

   in summary, there are two main differences between algorithms 2 and 3.
   first, line 14 in algorithm 2, which skips the expansion of negative
   examples, is removed in algorithm 3. second, lines 16   20 and 42   46 are
   added in algorithm 3, which correspond to the updating of parameters
   when a gold-standard edge is removed from the chart. in addition, the
   definitions of updateparameters are different for the id88
   training algorithm (zhang and clark [693]2011), the large-margin
   training algorithm (zhang, blackwood, and clark [694]2012), and the
   large-margin algorithm of this article, as explained earlier.
   4.   dependency-based word ordering and tree linearization
   section:
   [choose___________________________]
   [695]previous section [696]next section

   as well as id35, the same approach can be applied to the word ordering
   problem using other grammar formalisms. in this section, we present a
   dependency-based word ordering system, where the input is again a
   multi-set of words with gold-standard pos, and the output is an ordered
   sentence together with its dependency parse. except for necessary
   changes to the edge data structure and edge expansion, the same
   algorithm can be applied to this task.

   in addition to abstract word ordering, our framework can be used to
   solve a more informed, dependency-based word ordering task: tree
   linearization (filippova and strube [697]2009; he et al. [698]2009), a
   task that is very similar to abstract word ordering from a
   computational perspective. both tasks involve the permutation of a set
   of input words, and are np-hard. the only difference is that, for tree
   linearization, full unordered dependency trees are given as input. as a
   result, the output word permutations are more constrained (under the
   projectivity assumption), and more information is available for search
   disambiguation.

   tree linearization can be treated as a special case of word ordering,
   where a grammar constraint is applied such that the output sentence has
   to be consistent with the input tree. there is a spectrum of grammar
   constraints between abstract word ordering (no constraints) and tree
   linearization (full tree constraints). for example, one constraint
   might consist of a set of dependency relations between input words, but
   which do not form a complete unordered spanning tree. we call this word
   ordering task the partial-tree linearization problem, a task that is
   perhaps closer to nlp applications than both the abstract word ordering
   task and the full tree linearization problem, in the sense that id86
   pipelines might provide some syntactic relations between words for the
   linearization step, but not the full spanning tree.

   the main content of this section is based on a conference paper (zhang
   [699]2013), which we extend by using the technique of expanding
   negative training examples (one of the overall contributions of this
   article).
   4.1   full- and partial-tree linearization

   given a multi-set of input words w and a set of head-dependent
   relations h between the words in w, the task is to find an ordered
   sentence consisting of all the words in w and a dependency tree that
   contains all the relations in h. if each word in w is given a pos tag
   and h covers all words in w, then the task is (full-)tree
   linearization; if not then the task is partial-tree linearization. for
   partial-tree linearization, a subset of w is given fixed pos tags. in
   all cases, a word either has exactly one (gold) pos tag, or no pos
   tags.
   4.2   the edge data structure

   similar to the id35 case, edge refers to the data structure for a
   hypothesis in the decoding algorithm. here a leaf edge refers to an
   input word with a pos tag, and a non-leaf edge refers to a phrase or
   sentence with its dependency tree. edges are constructed bottom   up, by
   recursively joining two existing edges and adding an unlabeled
   dependency link between their head words.

   as for the id35 system, edges are scored by a global linear model:

   where   (e) represents the feature vector of e and is the parameter
   vector of the model. [700]table 2 shows the feature templates we use,
   which are inspired by the rich feature templates used for dependency
   parsing (koo and collins [701]2010; zhang and nivre [702]2011). in the
   table, h, m, s, h[l], h[r], m[l], and m[r] are the indices of words in
   the newly constructed edge, where h and m refer to the head and
   dependent of the newly constructed arc, s refers to the nearest sibling
   of m (on the same side of h), and h[l], h[r], m[l], and m[r] refer to
   the left and rightmost dependents of h and m, respectively. word, pos,
   lval, and rval are maps from indices to word forms, pos, left
   valencies, and right valencies of words, respectively. example feature
   instances extracted from the sentence in [703]figure 3 are shown in the
   example column. because of the non-local nature of some of the feature
   templates we define, we do not apply dp-style pruning for
   dependency-based tree-linearization.

   [704]table
   table   2    feature templates. indices on the surface string: h = head on
   newly added arc; m = dependent on arc; s = nearest sibling of m; b =
   any index between h and m; h[l], h[r] = left/rightmost dependent of h;
   m[l], m[r] = left/rightmost dependent of m; s[2] = nearest sibling of s
   towards h; b = boundary between the conjoined phrases (index of the
   first word of the right phrase). variables: dir = direction of the arc,
   normalized by norm; dist = distance (h-m), normalized; size = number of
   words in the dependency tree. functions: word = word at index; pos =
   pos at index; norm = normalize absolute value into 1, 2, 3, 4, 5, (5,
   10], (10, 20], (20, 40], 40+.

   [705]figure
   figure   3    feature template example.
   4.3   the decoding algorithm

   the decoding algorithm is similar to that of the id35 system, where an
   agenda is a priority queue for edges to expand, and chart is a
   fixed-size beam for a list of accepted edges. during initialization,
   input words are assigned possible pos tags, resulting in a set of leaf
   edges that are put onto the agenda. for words with pos constraints,
   only the allowed pos tag is assigned. for unconstrained words, we
   assign all possible pos tags according to a tag dictionary compiled
   from the training data, following standard practice for pos-tagging
   (ratnaparkhi [706]1996).

   when an edge is expanded, it is combined with all edges in the chart in
   all possible ways to generate new edges. two edges can be combined by
   concatenation of the surface strings in both orders and, in each case,
   constructing a dependency link between their heads in two ways
   (corresponding to the two options for the head of the new link). when
   there is a head constraint on the dependent word, a dependency link can
   be constructed only if it is consistent with the constraint. this
   algorithm implements abstract word ordering, partial-tree
   linearization, and full tree linearization   all generalized word
   ordering tasks   in a unified method.

   pseudocode for the decoder is shown as algorithm 4. many of the
   functions have the same definition as for algorithm 1: initagenda,
   initchart, timeout, popbest, goaltest, add. cancombine checks whether
   two edges do not contain an overlapping word (i.e., they do not contain
   a word more times than its count in the input); unlike the id35 case,
   all pairs of words are allowed to combine according to the dependency
   model. combine creates a dependency link between two words, with the
   word order determined by the order in which the arguments are supplied
   to the function, and the head coming from either the first (headleft)
   or second (headright) argument (so there are four combinations
   considered and combine is called four times in algorithm 4).

   4.4   the learning algorithm

   as for the id35 system, an online large-margin learning algorithm based
   on the decoding process is used to train the model. at each step, the
   expanded edge e is compared with the gold standard. if it is a gold
   edge, decoding continues; otherwise e is taken as a negative example
   e[   ] and the lowest-scored gold edge in the agenda is taken as a
   positive example e[+], and a parameter update is executed (repeated
   here from [707]section 3.4):

   the training process is essentially the same as in algorithm 3, but
   with the id35 grammar and model replaced with the dependency-based
   grammar and model.

   in our conference paper describing the earlier version of the
   dependency-based system (zhang [708]2013), the decoding step is
   finished immediately after the parameter update; in this article we
   expand the negative example, as in algorithm3, putting it onto the
   chart and thereby exploring a larger part of the search space (in
   particular that part containing negative examples). our later
   experiments show that this method yields improved results, consistent
   with the id35 system.
   5.   experiments
   section:
   [choose________________________]
   [709]previous section [710]next section

   we use id35bank (hockenmaier and steedman [711]2007) and the penn
   treebank (marcus, santorini, and marcinkiewicz [712]1993) for id35 and
   dependency data, respectively. id35bank is the id35 version of the penn
   treebank. standard splits were used for both: sections 02   21 for
   training, section 00 for development, and section 23 for the final
   test. [713]table 3 gives statistics for the id32.

   [714]table
   table   3    training, development, and test data from the id32.

   for the id35 experiments, original sentences from id35bank are
   transformed into bags of words, with sequence information removed, and
   passed to our system as input data. the system outputs are compared to
   the original sentences for evaluation.

   following wan et al. ([715]2009), we use the id7 metric (papineni et
   al. [716]2002) for string comparison. although id7 is not the perfect
   measure of fluency or grammaticality, being based on id165 precision,
   it is currently widely used for automatic evaluation and allows us to
   compare directly with existing work (wan et al. [717]2009). note also
   that one criticism of id7 for evaluating machine translation systems
   (i.e., that it can only register exact matches between the same words
   in the system and reference translation), does not apply here, because
   the system output always contains the same words as the original
   reference sentence. for the dependency-based experiments, gold-standard
   dependency trees were derived from bracketed sentences in the treebank
   using the penn2malt tool.[718]^2

   for fair comparison with wan et al. ([719]2009), we keep base nps as
   atomic units when preparing the input. wan et al. used base nps from
   the id32 annotation, and we follow this practice for the
   dependency-based experiments. for the id35 experiments we extract base
   nps from id35bbank by taking as base nps those nps that do not
   recursively contain other nps. these base nps mostly correspond to the
   base nps from the id32: in the training data, there are
   242,813 id32 base nps with an average size of 1.09, and
   216,670 id35bank base nps with an average size of 1.19.
   5.1   convergence of training

   the plots in [720]figure 4 show the development test scores of three
   id35 models by the number of training iterations. the three curves
   represent the scaled model of this article, the online large-margin
   model from zhang, blackwood, and clark ([721]2012), and the id88
   model from zhang and clark ([722]2011), respectively. for each curve,
   the id7 score generally increases as the number of training iterations
   increases, until it reaches its maximum at a particular iteration. we
   use the number of training iterations that gives the best development
   test scores for the training of our model when testing on the test
   data.
   [723]figure
   figure   4    id7 scores of the id88, large-margin, and scaled
   large-margin id35 models by the number of training iterations.

   another way to observe the convergence of training is to measure the
   training times for each iteration at different numbers of iterations.
   the per-iteration training times for the large-margin and the scaled
   id35 models are shown in [724]figure 5. for each model, the training
   time for each iteration decreases as the number of training iterations
   increases, reflecting the convergence of learning-guided search. when
   the model gets better, fewer non-gold hypotheses are expanded before
   gold hypotheses, and hence it takes less time for the decoder to find
   the gold goal edge. [725]figure 6 shows the corresponding curve for
   dependency-based word ordering, with similar observations.
   [726]figure
   figure   5    training times of the large-margin model and the scaled id35
   models by the number of training iterations.
   [727]figure
   figure   6    training times of the large-margin and scaled dependency
   models by the number of training iterations.

   because of the expanding of negative examples, the systems of this
   article took more time to train than those of our previous conference
   papers. however, the convergence rate is also faster when negative
   training examples are expanded, as demonstrated by the rate of speed
   improvement as the number of training iterations increases. the
   training times of the id88 algorithm are close to those of the
   large-margin algorithm, and hence are omitted from [728]figures 5 and
   [729]6. the new model gives the best development test scores, as shown
   in [730]figure 4. the next section investigates the effects of two of
   the innovations of this article: use of negative examples during
   training and the scaling of the model by hypothesis size.
   5.2   the effect of the scaled model and negative examples

   [731]table 4 shows a set of id35 development experiments to measure the
   effect of the scaled model and the expansion of negative examples
   during training. with the standard linear model (zhang, blackwood, and
   clark [732]2012) and no expansions of negative examples, our system
   obtained a id7 score of 39.04. the scaled model improved the id7
   score by 1.41 id7 points to 40.45, and the expansion of negative
   examples gave a further improvement of 3.02 id7 points.

   [733]table
   table   4    the effect of the scaled model and expansion of negative
   examples during training for the id35 system.

   these id35 development experiments show that the expansion of negative
   examples during training is an important factor in achieving good
   performance. when no negative examples are expanded, the higher score
   of the scaled linear model demonstrates the effectiveness of fair
   comparison between edges with different sizes. however, it is a more
   important advantage of the scaled linear model that it allows the
   expansion of negative examples during training, which was not possible
   with the standard linear model. in the latter case, training failed to
   converge when negative examples were expanded, reflecting the
   limitations of the standard linear model in separating the training
   data. similar results were found for dependency-based word ordering,
   where the best development id7 score improved from 44.71 (zhang
   [734]2013) to 46.44 with the expansion of negative training examples.
   5.3   the effect of search time

   [735]figure 7 shows the id7 scores for the id35 system on the
   development data when the timeout limit for decoding a single sentence
   is set to 5 sec, 10 sec, 15 sec, 20 sec, 30 sec, 40 sec, 50 sec, and 60
   sec, respectively. the timeout was applied during decoding at test
   time. the scaled model with negative training examples was used for
   this set of experiments, and the same model was used for all timeout
   settings. the results demonstrate that better outputs can be recovered
   given more search time, which is expected for a time-constrained
   best-first search framework. recall that output is created greedily by
   combining the largest available edges, when the system times out.
   similar results were obtained with the dependency-based system of zhang
   ([736]2013), where the development id7 scores improved from 42.89 to
   43.42, 43.58, and 43.72 when the timeout limit increased from 5 sec to
   10 sec, 30 sec, and 60 sec, respectively. the scaled dependency-based
   model without expansion of negative examples was used in this set of
   experiments.
   [737]figure
   figure   7    the effect of search time for the id35 system on the
   development test data.
   5.4   example outputs

   example output for sentences in the development set is shown in
   [738]tables 5 and [739]6, grouped by sentence length. the id35 systems
   of our previous conference papers and this article are compared, all
   with the timeout value set to 5 sec. all three systems perform
   relatively better with smaller sentences. for longer sentences, the
   fluency of the output is significantly reduced. one source of errors is
   confusion between different noun phrases, and where they should be
   positioned, which becomes more severe with increased sentence length
   and adds to the difficulty in reading the outputs. the system of this
   article gave observably improved outputs compared with the two other
   systems.

   [740]table
   table   5    example development output for the id35-based systems and
   sentences with fewer than 20 words.

   [741]table
   table   6    example development output for the id35-based systems and
   sentences with more than 20 words.

   5.5   partial-tree linearization

   in the previous section, the same input settings were used for both
   training and testing, and the assumption was made that the input to the
   system would be a bag of words, with no constraints on the output
   structure. this somewhat artificial assumption allows a standardized
   evaluation but, as discussed previously, text generation applications
   are unlikely to satisfy this assumption and, in practice, the
   realization problem is likely to be easier compared with our previous
   set-up. in this section, we simulate practical situations in
   dependency-based pipelines by measuring the performance of our system
   using randomly chosen input pos tags and dependency relations. for
   maximum flexibility, so that the same system can be applied to
   different input scenarios, our system is trained without input pos tags
   or dependencies. however, if pos tags and dependencies are made
   available during testing, they will be used to provide hard constraints
   on the output (i.e., the output sentence with pos tags and dependencies
   must contain those in the input). from the perspective of search, input
   pos tags and dependencies greatly constrain the search space and lead
   to an easier search problem, with correspondingly improved outputs.

   [742]table 7 shows a set of development results with varying amounts of
   pos and dependency information in the input. for each test, we randomly
   sampled a percentage of words for which the gold-standard pos tags or
   dependencies are given in the input. as can be seen from the table,
   increased amounts of pos and dependency information in the input lead
   to higher id7 scores, and dependencies were more effective than pos
   tags in determining the word order in the output. when all pos tags and
   dependencies are given, our constraint-enabled system gave a id7 score
   of 76.28.[743]^3

   [744]table
   table   7    development id7 scores for partial-tree linearization, with
   different proportions of input pos and dependency information randomly
   selected from full gold-standard trees.

   [745]table 8 shows the output of our system for the first nine
   development test sentences with different input settings. these
   examples illustrate the positive effect of input dependencies in
   specifying the outputs. consider the second sentence as an example.
   when only input words are given, the output of the system is largely
   grammatical but nonsensical. with increasing amounts of dependency
   relations, the output begins to look more fluent, sometimes with the
   system reproducing the original sentence when all dependencies are
   given.

   [746]table
   table   8    partial-tree linearization outputs for the first nine
   development test sentences with various input information.

   5.6   final results

   [747]table 9 shows the test results of various systems. for the system
   of this article, we take the optimal setting from the development
   tests, using the scaled linear model and expansion of negative examples
   during training. for direct comparison with previous work, the timeout
   threshold was set to 5 sec. our new system of this article
   significantly outperforms all previous systems and achieves the best
   published id7 score on this task. it is worth noting that our systems
   without a language model outperform the system of our 2012 paper using
   a large-scale language model.

   [748]table
   table   9    final test results on the standard word ordering task.

   interestingly, the dependency-based systems performed better than the
   id35 systems of this article. one of the main reasons is that the id35
   systems generated shorter outputs by not finding full spanning
   derivations for a larger proportion of inputs. because of the rigidity
   in combinatory rules, not all hypotheses in the chart can be combined
   with the hypothesis being expanded, leading to an increased likelihood
   of full spanning derivations being unreachable. overall, the id35 system
   recovered 93.98% of the input words in the test set, and the dependency
   system recovered 97.71%.
   5.7   shared task evaluation

   the previous sections report evaluations on the task of word ordering,
   an abstract yet fundamental problem in text generation. one question
   that is not addressed by these experiments is how the abstract task can
   be utilized to benefit full text generation, for which more
   considerations need to be taken into account in addition to word
   ordering. we investigate this question using the 2011 generation
   challenge shared task data, which provide a common-ground for the
   evaluation of text generation systems (belz et al. [749]2011).

   the data are based on the conll 2008 shared task data (surdeanu et al.
   [750]2008), which consist of selected sections of the penn wsj
   treebank, converted to syntactic dependencies via the lth tool
   (johansson and nugues [751]2007). [752]sections 2   21 are used for
   training, section 24 for development, and section 23 for testing. a
   small number of sentences from the original wsj sections are not
   included in this set. the input format of the shared task is an
   unordered syntactic dependency tree, with nodes being lemmas, and
   dependency relations on the arcs. named entities and hyphenated words
   are broken into individual nodes, and special dependency links are used
   to mark them. information on coarse-grained pos, number, tense, and
   participle features is given to each node where applicable. the output
   is a fully ordered and inflected sentence.

   we developed a full-text generation system according to this task
   specification, with the core component being the dependency-based word
   ordering system of [753]section 4. in addition to minor engineering
   details that were required to adapt the system to this new task, one
   additional task that the generation system needs to carry out is
   morphological generation   finding the appropriate inflected form for
   each input lemma. our approach is to perform joint word ordering and
   inflection using the learning-guided search framework, letting one
   statistical model decide the best order as well as the inflections of
   ambiguous lemmas. for a lemma, we generate one or more candidate
   inflections by using a lexicon and a set of inflection rules. candidate
   inflections for an input lemma are generated according to the lemma
   itself and its input attributes, such as the number and tense. some
   lemmas are unambiguous, which are inflected before being passed to the
   word ordering system. for the other lemmas, more than one candidate's
   inflections are passed as input words to the word ordering system. to
   ensure that each lemma occurs only once in the output, a unique id is
   given to all the inflections of the same lemma, making them mutually
   exclusive.

   four types of lemmas need morphological generation, including nouns,
   verbs, adjectives, and miscellaneous cases. the last category includes
   a (a or an) and not (not or n't), for which the best inflection can be
   decided only when id165 information is available. for these lemmas, we
   pass all possible inflections to the search module. for nouns and
   adjectives, the inflection is relatively straightforward, since the
   number (e.g., singular, plural) of a lemma is given as an attribute of
   the input node, and comparative and superlative adjectives have
   specific parts of speech. for those cases where the necessary
   information is not available from the input, all possible inflections
   are handed over to the search module for further disambiguation. the
   most ambiguous lemma types are verbs, which can be further divided into
   be and other verbs. the uniqueness of be is that the inflections for
   the first and second person can be different. all verb inflections are
   disambiguated according to the tense and participle attributes of the
   input node. in addition, for verbs in the present tense, the subject
   needs to be determined in order to differentiate between third-person
   singular verbs and others. this can be straightforward when the subject
   is a noun or pronoun, but can be ambiguous when the subject is a
   wh-pronoun, in which case the real subject might not be directly
   identifiable from the dependency tree. we leave all possible
   inflections of be and other verbs to the word ordering system whenever
   the ambiguity is not directly solvable from the subject dependency
   link. overall, the pre-processing step generates 1.15 inflections for
   each lemma on average.

   for word ordering, the search procedure of algorithm 4 is applied
   directly, and the feature templates of [754]table 2 are used with
   additional labeled dependency features described subsequently. the main
   reason that the dependency-based word ordering algorithm can perform
   joint id60 is that it uses rich syntactic and
   id165 features to score candidate hypotheses, which can also
   differentiate between correct and incorrect inflections under
   particular contexts. for example, an honest person and a honest person
   can be differentiated by id165 features, while tom and sally is and
   tom and sally are can be differentiated by higher-order dependency
   features.

   in addition to lemma-formed inputs, one other difference between the
   shared task and the word ordering problem solved by algorithm 4 is that
   the former uses labeled dependencies whereas algorithm 4 constructs
   unlabeled dependency trees. we address this issue by assigning
   dependency labels in the construction of dependency links, and applying
   an extra set of features. the new features are defined by making a
   duplicate of all the features from [755]table 2 that contain dir
   information, and associating each feature in the new copy with a
   dependency label.

   the training of the word ordering system requires fully ordered
   dependency trees, while references in the shared task data are raw
   sentences. we perform a pre-processing step to obtain gold-standard
   training data by matching the input lemmas to the reference sentence in
   order to obtain their gold-standard order. more specifically, given a
   training instance, we generate all candidate inflections for each
   lemma, resulting in an exponential set of possible mappings between the
   input tree and the reference sentence. we then prune these mappings
   bottom   up, assuming that the dependency tree is projective, and
   therefore that each word dominates a continuous span in the reference.
   after such pruning, only one correct mapping is found for the majority
   of the cases. for the cases where more than one mapping is found, we
   randomly choose one as the gold-standard. there are also instances for
   which no correct ordering can be found, and these are mostly due to
   non-projectivity in the shared task data, with a few cases being due to
   conflicts between our morphological generation system and the shared
   task data, or inconsistency in the data itself. out of the 39k training
   instances, 2.8k conflicting instances are discarded, resulting in 36.2k
   gold-standard ordered dependency trees.

   [756]table 10 shows the results of our system and the top two
   participating systems of the shared task. our system outperforms the
   stumaba system by 0.5 id7 points, and the dcu system by 3.8 id7
   points. more evaluation of the system was published in song et al.
   ([757]2014).

   [758]table
   table   10    results and comparison with the top-performing systems on the
   shared task data

   6.   related work
   section:
   [choose________________________]
   [759]previous section [760]next section

   there is a recent line of research on text-to-text generation, which
   studies the linearization of dependency structures (barzilay and
   mckeown [761]2005; filippova and strube [762]2007, [763]2009; he et al.
   [764]2009; bohnet et al. [765]2010; guo, hogan, and van genabith
   [766]2011). on the other hand, wan et al. ([767]2009) study the
   ordering of a bag of words without any dependency information given. we
   generalize the word ordering problem, and formulate it as a task of
   ordering a multi-set of words, regardless of input syntactic
   constraints.

   our bottom   up, chart-based generation algorithm is inspired by the line
   of work on chart-based realization (kay [768]1996; carroll et al.
   [769]1999; white [770]2004, [771]2006; carroll and oepen [772]2005).
   kay ([773]1996) first proposed the concept of chart realization,
   drawing analogies between realization and parsing of free order
   languages. he discussed efficiency issues and provided solutions to
   specific problems. for the task of realization, efficiency improvement
   has been further investigated (carroll et al. [774]1999; carroll and
   oepen [775]2005). the inputs to these systems are logical forms, which
   form natural constraints on the interaction between edges. in our case,
   one constraint that has been leveraged in the dependency system is a
   projectivity assumption   we assume that the dependents of a word must
   all have been attached before the word is attached to its head word,
   and that spans do not cross during combination. in addition, we assume
   that the right dependents of a word must have been attached before a
   left dependent of the word is attached. this constraint avoids spurious
   ambiguities. the projectivity assumption is an important basis for the
   feasibility of the dependency system; it is similar to the chunking
   constraints of white ([776]2006) for id35-based realization.

   white ([777]2004) describes a system that performs id35 realization
   using best-first search. the search process of our algorithm is similar
   to that work, but the input is different: logical forms in the case of
   white ([778]2004) and bags of words in our case. further along this
   line, espinosa, white, and mehay ([779]2008) also describe a id35-based
   realization system, applying    hypertagging      a form of id55   to
   logical forms in order to make use of id35 lexical categories in the
   realization process. white and rajkumar ([780]2009) further use
   id88 reranking on n-best realization output to improve the
   quality.

   the use of id88 learning to improve search has been proposed in
   guided learning for easy-first search (shen, satta, and joshi
   [781]2007) and laso (daum   and marcu [782]2005). laso is a general
   framework for various search strategies. our learning algorithm is
   similar to laso with best-first id136, but the parameter updates
   are different. in particular, laso updates parameters when all correct
   hypotheses are lost, but our algorithm makes an update as soon as the
   top item from the agenda is incorrect. our algorithm updates the
   parameters using a stronger precondition, because of the large search
   space. given an incorrect hypothesis, laso finds the corresponding gold
   hypothesis for a id88 update by constructing its correct sibling.
   in contrast, our algorithm takes the lowest scored gold hypothesis
   currently in the agenda to avoid updating parameters for hypotheses
   that may have not been constructed.

   our parameter update strategy is closer to the guided learning
   mechanism for the easy-first algorithm of shen, satta, and joshi
   ([783]2007), which maintains a queue of hypotheses during search, and
   performs learning to ensure that the highest-scored hypothesis in the
   queue is correct. however, in easy-first search, hypotheses from the
   queue are ranked by the score of their next action, rather than the
   hypothesis score. moreover, shen, satta, and joshi use aggressive
   learning and regenerate the queue after each update, but we perform
   non-aggressive learning, which is faster and is more feasible for our
   large and complex search space. similar methods to shen, satta, and
   joshi ([784]2007) have also been used in shen and joshi ([785]2008) and
   goldberg and elhadad ([786]2010).

   another framework that closely integrates learning and search is searn
   (daum  , langford, and marcu [787]2009), which addresses structured
   prediction problems that can be transformed into a series of simple
   classification tasks. the transformation is akin to greedy search in
   the sense that the complex structure is constructed by sequential
   classification decisions. the key problem that searn addresses is how
   to learn the tth decision based on the previous t     1 decisions, so
   that the overall loss in the resulting structure is minimized. similar
   to our framework, searn allows arbitrary features. however, searn is
   more oriented to greedy search, optimizing local decisions. in
   contrast, our framework is oriented to best-first search, optimizing
   global structures.

   learning and search also interact with each other in a global
   discriminative learning and beam-search framework for incremental
   id170 (zhang and clark [788]2011). in this framework,
   an output is constructed incrementally by a sequence of transitions,
   while a beam is used to record the highest scored structures at each
   step. online training is performed based on the search process, with
   the objective function being the margin between correct and incorrect
   structures. the method involves an early-update strategy, which stops
   search and updates parameters immediately when the gold structure falls
   out of the beam during training. it was first proposed by collins and
   roark ([789]2004) for incremental parsing, and later gained popularity
   in the investigations of many nlp tasks, including pos-tagging (zhang
   and clark [790]2010), transition-based id33 (zhang and
   clark [791]2008; huang and sagae [792]2010), and machine translation
   (liu [793]2013). huang, fayong, and guo ([794]2012) propose a
   theoretical analysis to the early-update training strategy, pointing
   out that it is a type of training method that fixes score violations in
   inexact search. when the score of a gold-standard structure is lower
   than that of a non-gold structure, a violation exists. our parameter
   udpate strategy in this article can also be treated as a mechanism for
   violation fixing.
   7.   conclusion
   section:
   [choose________________________]
   [795]previous section [796]next section

   we investigated the general task of syntax-based word ordering, which
   is a fundamental problem for text generation, and a computationally
   very expensive search task. we provide a principled solution to this
   problem using learning-guided search, a framework that is applicable to
   other nlp problems with complex search spaces. we compared different
   methods for parameter updates, and showed that a scaled linear model
   gave the best results by allowing better comparisons between phrases of
   different sizes, increasing the separability of hypotheses and enabling
   the expansion of negative examples during training.

   we formulate abstract word ordering as a spectrum of tasks with varying
   input specificity, from    pure    word ordering without any syntactic
   information to fully-informed word ordering with a complete unordered
   dependency tree given. experiments show that our proposed method can
   effectively use available input constraints in generating output
   sentences.

   evaluation on the id86 2011 shared task data shows that our system can
   be successfully applied to a more realistic application scenario, in
   particular one where some dependency constraints are provided in the
   input and word inflection is required as well as word ordering.
   additional tasks that may be required in a practical text generation
   scenario include word selection, including the determination of content
   words and generation of function words. the joint modeling solution
   that we have proposed for word ordering and inflection could also be
   adopted for word selection, although the search space is greatly
   increased when the words themselves need deciding, particularly content
   words.
   acknowledgments
   section:
   [choose________________________]
   [797]previous section [798]next section

   this work was carried out partly while yue zhang was a postdoctoral
   research associate at the university of cambridge computer laboratory,
   where he and stephen clark were supported by the european union seventh
   framework programme (fp7-ict-2009-4) under grant agreement no. 247762,
   and partly after yue zhang joined singapore university of technology
   and design, where he was supported by the moe grant t2-moe-2013-01. we
   thank bill byrne, marcus tomalin, adri   de gispert, and graeme
   blackwood for numerous discussions; anja belz and mike white for kindly
   providing the id86 2011 shared task data; kai song for helping with
   tables and figures in the draft; yijia liu for helping with the
   bibliography; and the anonymous reviewers for the many constructive
   comments that have greatly improved this article since the first draft.
   notes
   section:
   [choose________________________]
   [799]previous section [800]next section

   1    an example proof can be based on induction, where the basis is that
   the agenda contains all gold leaf edges at first, and the induction
   step is based on edge combination.

   2    [801]http://w3.msi.vxu.se/  nivre/research/penn2malt.html.

   3    when all pos tags and dependencies are also provided during
   training, the id7 score is reduced to 74.79, showing the value in the
   system, which can adapt to varying amounts of pos and dependency
   information in the input at test time.
   references
   section:
   [choose________________________]
   [802]previous section [803]next section
   auli, michael and adam lopez. 2011. training a log-linear parser with
   id168s via softmax-margin. in proceedings of the 2011
   conference on empirical methods in natural language processing, pages
   333   343, edinburgh. [804]google scholar
   barzilay, regina and kathleen mckeown. 2005. sentence fusion for
   multidocument news summarization. computational linguistics,
   31(3):297   328. [805]link, [806]google scholar
   belz, anja, michael white, dominic espinosa, eric kow, deirdre hogan,
   and amanda stent. 2011. the first surface realisation shared task:
   overview and evaluation results. in proceedings of the 13th european
   workshop on id86, eid86 '11, pages 217   226,
   stroudsburg, pa. [807]google scholar
   blackwood, graeme, adri   de gispert, and william byrne. 2010. fluency
   constraints for minimum bayes-risk decoding of statistical machine
   translation lattices. in proceedings of the 23rd international
   conference on computational linguistics (coling 2010), pages 71   79,
   beijing. [808]google scholar
   bohnet, bernd, leo wanner, simon mill, and alicia burga. 2010. broad
   coverage multilingual deep sentence generation with a stochastic
   multi-level realizer. in proceedings of the 23rd international
   conference on computational linguistics (coling 2010), pages 98   106,
   beijing. [809]google scholar
   bos, johan, stephen clark,mark steedman, james r. curran, and julia
   hockenmaier. 2004. wide-coverage semantic representations from a id35
   parser. in proceedings of coling-04, pages 1240   1246, geneva.
   [810]crossref, [811]google scholar
   caraballo, sharon a. and eugene charniak. 1998. new figures of merit
   for best-first probabilistic chart parsing. computational linguistics,
   24:275   298. [812]google scholar
   carroll, john, ann copestake, dan flickinger, and victor poznanski.
   1999. an efficient chart generator for (semi-) lexicalist grammars. in
   proceedings of the 7th european workshop on id86
   (ewid8699), pages 86   95, toulouse. [813]google scholar
   carroll, john and stephan oepen. 2005. high efficiency realization for
   a wide-coverage unification grammar. in proceedings of the second
   international joint conference on natural language processing,
   ijcnlp'05, pages 165   176, berlin. [814]google scholar
   chang, pi-chuan and kristina toutanova. 2007. a discriminative
   syntactic word order model for machine translation. in proceedings of
   acl, pages 9   16, prague. [815]google scholar
   chiang, david. 2007. hierarchical phrase-based translation.
   computational linguistics, 33(2):201   228. [816]link, [817]google
   scholar
   clark, stephen and james r. curran. 2007a. id88 training for a
   wide-coverage lexicalized-grammar parser. in proceedings of the acl
   2007 workshop on deep linguistic processing, pages 9   16, prague.
   [818]google scholar
   clark, stephen and james r. curran. 2007b. wide-coverage efficient
   statistical parsing with id35 and id148. computational
   linguistics, 33(4):493   552. [819]link, [820]google scholar
   collins, michael and brian roark. 2004. incremental parsing with the
   id88 algorithm. in proceedings of acl, pages 111   118, barcelona.
   [821]google scholar
   crammer, koby, ofer dekel, joseph keshet, shai shalev-shwartz, and
   yoram singer. 2006. online passive-aggressive algorithms. journal of
   machine learning research, 7:551   585. [822]google scholar
   daum  , hal, john langford, and daniel marcu. 2009. search-based
   id170. machine learning, 75(3):297   325.
   [823]crossref, [824]google scholar
   daum  , hal and daniel marcu. 2005. learning as search optimization:
   approximate large margin methods for id170. in icml,
   pages 169   176, bonn. [825]google scholar
   espinosa, dominic, michael white, and dennis mehay. 2008. hypertagging:
   id55 for surface realization with id35. in proceedings of
   acl-08: hlt, pages 183   191, columbus, oh. [826]google scholar
   filippova, katja and michael strube. 2007. generating constituent order
   in german clauses. in proceedings of the 45th annual meeting of the
   association of computational linguistics, pages 320   327, prague.
   [827]google scholar
   filippova, katja and michael strube. 2009. tree linearization in
   english: improving language model based approaches. in proceedings of
   human language technologies: the 2009 annual conference of the north
   american chapter of the association for computational linguistics,
   companion volume: short papers, pages 225   228, boulder, co.
   [828]crossref, [829]google scholar
   fowler, timothy a. d. and gerald penn. 2010. accurate context-free
   parsing with id35. in proceedings of the 48th
   annual meeting of the association for computational linguistics, pages
   335   344, uppsala. [830]google scholar
   goldberg, yoav and michael elhadad. 2010. an efficient algorithm for
   easy-first non-directional id33. in human language
   technologies: the 2010 annual conference of the north american chapter
   of the association for computational linguistics, pages 742   750, los
   angeles, ca. [831]google scholar
   guo, yuqing, deirdre hogan, and josef van genabith. 2011. dcu at
   generation challenges 2011 surface realisation track. in proceedings of
   the generation challenges session at the 13th european workshop on
   id86, pages 227   229, nancy. [832]google scholar
   he, wei, haifeng wang, yuqing guo, and ting liu. 2009. dependency based
   chinese sentence realization. in proceedings of the joint conference of
   the 47th annual meeting of the acl and the 4th international joint
   conference on natural language processing of the afnlp, pages 809   816,
   suntec. [833]google scholar
   hockenmaier, julia. 2003. data and models for statistical parsing with
   id35. ph.d. thesis, school of informatics,
   university of edinburgh. [834]google scholar
   hockenmaier, julia and mark steedman. 2007. id35bank: a corpus of id35
   derivations and dependency structures extracted from the id32.
   computational linguistics, 33(3):355   396. [835]link, [836]google
   scholar
   huang, liang, suphan fayong, and yang guo. 2012. structured id88
   with inexact search. in proceedings of the 2012 conference of the north
   american chapter of the association for computational linguistics:
   human language technologies, pages 142   151, montr  al. [837]google
   scholar
   huang, liang and kenji sagae. 2010. id145 for linear-time
   incremental parsing. in proceedings of acl, pages 1077   1086, uppsala.
   [838]google scholar
   johansson, richard and pierre nugues. 2007. extended
   constituent-to-dependency conversion for english. in 16th nordic
   conference of computational linguistics, pages 105   112, tartu.
   [839]google scholar
   kay, martin. 1996. chart generation. in proceedings of acl, pages
   200   204, santa cruz, ca. [840]google scholar
   koehn, phillip. 2010. id151. cambridge
   university press. [841]google scholar
   koehn, philipp, hieu hoang, alexandra birch, chris callison-burch,
   marcello federico, nicola bertoldi, brooke cowan, wade shen, christine
   moran, richard zens, chris dyer, ondrej bojar, alexandra constantin,
   and evan herbst. 2007. moses: open source toolkit for statistical
   machine translation. in proceedings of the 45th annual meeting of the
   association for computational linguistics companion volume proceedings
   of the demo and poster sessions, pages 177   180, prague. [842]google
   scholar
   koehn, philipp, franz josef och, and daniel marcu. 2003. statistical
   phrase-based translation. in proceedings of the 2003 conference of the
   north american chapter of the association for computational linguistics
   on human language technology - volume 1, naacl '03, pages 48   54,
   edmonton, canada. [843]crossref, [844]google scholar
   koo, terry and michael collins. 2010. efficient third-order dependency
   parsers. in proceedings of acl, pages 1   11, uppsala. [845]google
   scholar
   lee, j. and s. seneff. 2006. automatic grammar correction for
   second-language learners. in proceedings of interspeech, pages
   1978   1981, pittsburgh, pa. [846]google scholar
   liu, yang. 2013. a id132 algorithm for phrase-based
   string-to-dependency translation. in proceedings of the 51st annual
   meeting of the association for computational linguistics (volume 1:
   long papers), pages 1   10, sofia. [847]google scholar
   marcus, mitchell p., beatrice santorini, and mary ann marcinkiewicz.
   1993. building a large annotated corpus of english: the id32.
   computational linguistics, 19(2):313   330. [848]google scholar
   papineni, kishore, salim roukos, todd ward, and wei-jing zhu. 2002.
   id7: a method for automatic evaluation of machine translation. in
   proceedings of the 40th annual meeting of the association for
   computational linguistics, pages 311   318, philadelphia, pa. [849]google
   scholar
   ratnaparkhi, adwait. 1996. a maximum id178 model for part-of-speech
   tagging. in proceedings of emnlp, pages 133   142, somerset, nj.
   [850]google scholar
   reiter, ehud and robert dale. 1997. building applied natural language
   generation systems. natural language engineering, 3(1):57   87.
   [851]crossref, [852]google scholar
   shen, libin and aravind joshi. 2008. ltag id33 with
   bidirectional incremental construction. in proceedings of the 2008
   conference on empirical methods in natural language processing, pages
   495   504, honolulu. hi. [853]google scholar
   shen, libin, giorgio satta, and aravind joshi. 2007. guided learning
   for bidirectional sequence classification. in proceedings of acl, pages
   760   767, prague. [854]google scholar
   song, linfeng, yue zhang, kai song, and qun liu. 2014. joint
   morphological generation and syntactic linearization. in proceedings of
   the twenty-eighth aaai conference, quebec. [855]google scholar
   steedman, mark. 2000. the syntactic process. the mit press, cambridge,
   ma. [856]google scholar
   surdeanu, mihai, richard johansson, adam meyers, llu  s m  rquez, and
   joakim nivre. 2008. the conll-2008 shared task on joint parsing of
   syntactic and semantic dependencies. in proceedings of the twelfth
   conference on computational natural language learning, pages 159   177,
   manchester. [857]crossref, [858]google scholar
   wan, stephen, mark dras, robert dale, and c  cile paris. 2009. improving
   grammaticality in statistical sentence generation: introducing a
   dependency spanning tree algorithm with an argument satisfaction model.
   in proceedings of the 12th conference of the european chapter of the
   acl (eacl 2009), pages 852   860, athens. [859]google scholar
   weir, david. 1988. characterizing mildly context-sensitive grammar
   formalisms. ph.d. thesis, university of pennsylviania. [860]google
   scholar
   white, michael. 2004. reining in id35 chart realization. in proceedings
   of iid86-04, pages 182   191, brockenhurst. [861]google scholar
   white, michael. 2006. efficient realization of coordinate structures in
   id35. research on language & computation,
   4(1):39   75. [862]crossref, [863]google scholar
   white, michael and rajakrishnan rajkumar. 2009. id88 reranking
   for id35 realization. in proceedings of the 2009 conference on empirical
   methods in natural language processing, pages 410   419, singapore.
   [864]crossref, [865]google scholar
   wu, dekai. 1997. stochastic inversion transduction grammars and
   bilingual parsing of parallel corpora. computational linguistics,
   23(3):377   403. [866]google scholar
   xu, peng, ciprian chelba, and frederick jelinek. 2002. a study on
   richer syntactic dependencies for structured id38. in
   proceedings of the 40th annual meeting of the association for
   computational linguistics, pages 191   198, philadelphia, pa. [867]google
   scholar
   zettlemoyer, luke s. and michael collins. 2005. learning to map
   sentences to logical form: structured classification with probabilistic
   categorial grammars. in proceedings of the 21st conference on
   uncertainty in artificial intelligence, pages 658   666, edinburgh.
   [868]google scholar
   zhang, yue. 2013. partial-tree linearization: generalized word ordering
   for text synthesis. in proceedings of ijcai, pages 2232   2238, beijing.
   [869]google scholar
   zhang, yue, graeme blackwood, and stephen clark. 2012. syntax-based
   word ordering incorporating a large-scale language model. in
   proceedings of the 13th conference of the european chapter of the
   association for computational linguistics, pages 736   746, avignon.
   [870]google scholar
   zhang, yue and stephen clark. 2008. joint id40 and pos
   tagging using a single id88. in proceedings of acl/hlt, pages
   888   896, columbus, oh. [871]google scholar
   zhang, yue and stephen clark. 2010. a fast decoder for joint word
   segmentation and pos-tagging using a single discriminative model. in
   proceedings of the 2010 conference on empirical methods in natural
   language processing, pages 843   852, cambridge, ma. [872]google scholar
   zhang, yue and stephen clark. 2011. syntactic processing using the
   generalized id88 and id125. computational linguistics,
   37(1):105   151. [873]link, [874]google scholar
   zhang, yue and joakim nivre. 2011. transition-based id33
   with rich non-local features. in proceedings of the 49th annual meeting
   of the association for computational linguistics: human language
   technologies, pages 188   193, portland, or. [875]google scholar
   yue zhang*
   singapore university of technology and design
   stephen clark**
   university of cambridge

   *singapore university of technology and design. 20 dover drive,
   singapore. e-mail: [876][email protected].

   **university of cambridge computer laboratory, william gates building,
   15 jj thomson avenue, cambridge, uk. e-mail: [877][email protected].
   [878]favorite [favorite-1488499760477.svg]
   track citations [notify-me-alert-1488499750013.svg]
   [879]download citation [download2-1490507427013.svg]
   [880]rss toc [rss-1488924357683.svg]

   [881]rss citation [rss-1488924357683.svg]
   [882]submit your article
   [883]support oa at mitp [open-access-1493356222797.svg]

   [884][mitpress-logo-main-1483476130433.svg]
     * [885]journals [886]books
     * [887]terms & conditions
     * [888]privacy statement
     * [889]contact us

     * us
          + one rogers street cambridge ma 02142-1209
     * uk
          + suite 2, 1 duchess street london, w1w 6an, uk
     * connect
          + [890]facebook
          + [891]twitter
          + [892]google +
          + [893]pinterest
          + [894]instagram
          + [895]youtube
     *
          +    2019 the mit press
          + technology partner:[896] atypon systems, inc.
          + [897]crossref member
          + [898]counter member
          + the mit press colophon is registered in the u.s. patent and
            trademark office.
          + [899]site help

references

   visible links
   1. https://doi.org/10.1162/coli_a_00229
   2. https://doi.org/10.1162/coli_a_00229
   3. https://doi.org/10.1162/coli_a_00229
   4. https://www.googletagmanager.com/ns.html?id=gtm-tr6twrh
   5. https://www.googletagmanager.com/ns.html?id=gtm-tr6twrh
   6. https://www.mitpressjournals.org/action/showlogin
   7. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
   8. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
   9. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
  10. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
  11. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
  12. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
  13. https://www.mitpressjournals.org/action/requestusername
  14. https://www.mitpressjournals.org/action/requestresetpassword
  15. https://www.mitpressjournals.org/
  16. http://www.mitpressjournals.org/
  17. https://mitpress.mit.edu/
  18. https://www.mitpressjournals.org/action/showpublications
  19. https://www.mitpressjournals.org/digital
  20. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
  21. https://www.mitpressjournals.org/librarians
  22. https://www.mitpressjournals.org/action/institutionadminactivation
  23. https://www.mitpressjournals.org/pb-assets/pdfs/journals institutional license-1548442853093.pdf
  24. https://www.mitpressjournals.org/action/showinstitutionusagereport
  25. https://www.mitpressjournals.org/document_delivery
  26. https://www.mitpressjournals.org/pb-assets/pdfs/vpat_mitp_journals-1507061346287.pdf
  27. https://www.mitpressjournals.org/subscribe
  28. https://www.mitpressjournals.org/inst_getting_started
  29. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
  30. https://www.mitpressjournals.org/pb-assets/pdfs/2019 mitpj institution price list-1541020584043.pdf
  31. https://www.mitpressjournals.org/pb-assets/pdfs/2019 mitpj package pricelist-1541020593767.pdf
  32. https://www.mitpressjournals.org/pb-assets/pdfs/2018 mitpj single issue price list-1513815652483.pdf
  33. https://www.mitpressjournals.org/for_authors
  34. https://www.mitpressjournals.org/for_authors#subpubagreements
  35. https://www.mitpressjournals.org/for_authors#authorposting
  36. https://www.mitpressjournals.org/for_authors#copyright
  37. https://www.mitpressjournals.org/for_authors#authorreprints
  38. https://www.mitpressjournals.org/for_authors#publishingoa
  39. https://www.mitpressjournals.org/for_authors#nihpublicaccess
  40. https://www.mitpressjournals.org/for_authors#authordiscounts
  41. https://www.mitpressjournals.org/for_authors#kudos
  42. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
  43. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
  44. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
  45. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
  46. https://www.mitpressjournals.org/rights_permission
  47. https://mitpress.mit.edu/giving/
  48. https://www.mitpressjournals.org/trade_sales
  49. https://www.mitpressjournals.org/advertising
  50. https://www.mitpressjournals.org/schedule
  51. https://www.mitpressjournals.org/faq
  52. https://www.mitpressjournals.org/terminated_journals
  53. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
  54. https://www.mitpressjournals.org/aboutmitpj
  55. https://www.mitpressjournals.org/ethics
  56. https://www.mitpressjournals.org/events
  57. https://www.mitpressjournals.org/publishing_services
  58. https://www.mitpressjournals.org/mitpj-staff
  59. https://www.mitpressjournals.org/analytics
  60. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
  61. https://www.facebook.com/mitpress
  62. http://www.twitter.com/mitpress
  63. https://plus.google.com/106848724929282487337?prsrc=3
  64. https://www.pinterest.com/mitpress/
  65. https://www.instagram.com/mitpress/
  66. https://www.youtube.com/channel/uceh0hmlpjgw2dn0ntmd0fcq
  67. https://www.mitpressjournals.org/contact_info
  68. https://mitpressjournals.mit.edu/shop/customer/account/
  69. https://www.mitpressjournals.org/cdn-cgi/l/email-protection#b9d3d6cccbd7d8d5ca94d8ddcaf9d4d0cd97dcddcc
  70. https://www.mitpressjournals.org/cdn-cgi/l/email-protection#711b1e04031f101d025c181f171e311c18055f141504
  71. https://www.mitpressjournals.org/cdn-cgi/l/email-protection#c6aca9b3b4a8a7aab5ebb4afa1aeb2b586abafb2e8a3a2b3
  72. https://www.mitpressjournals.org/cdn-cgi/l/email-protection#076d637764742a706e756274476a6e7329626372
  73. https://www.mitpressjournals.org/cdn-cgi/l/email-protection#2a404e5a4959074643494f44594f596a47435e044f4e5f
  74. https://www.mitpressjournals.org/cdn-cgi/l/email-protection#deb4b1abacb0bfb2adf3bdad9eb3b7aaf0bbbaab
  75. https://www.mitpressjournals.org/cdn-cgi/l/email-protection#aec4c1dbdcc0cfc2dd83cfcdcdcbddddeec3c7da80cbcadb
  76. https://www.mitpressjournals.org/cdn-cgi/l/email-protection#e7898b8e898394869ea78a8e93c9828392
  77. https://www.mitpressjournals.org/action/showlogin?uri=/doi/10.1162/coli_a_00229
  78. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
  79. https://www.mitpressjournals.org/search/advanced
  80. https://www.mitpressjournals.org/
  81. https://www.mitpressjournals.org/loi/coli
  82. https://www.mitpressjournals.org/loi/coli
  83. https://www.mitpressjournals.org/toc/coli/41/3
  84. http://www.mitpressjournals.org/doi/abs/10.1162/coli_a_00228
  85. http://www.mitpressjournals.org/doi/abs/10.1162/coli_a_00230
  86. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
  87. https://www.mitpressjournals.org/journals/coli/editorial
  88. https://www.mitpressjournals.org/journals/coli/aandi
  89. https://www.mitpressjournals.org/schedule
  90. https://www.mitpressjournals.org/advertising
  91. http://cljournal.org/
  92. https://www.mitpressjournals.org/pb-assets/pdfs/coli-permission-form_sept2018-1536787440293.pdf
  93. http://www.mitpressjournals.org/for_authors#authorreprints
  94. https://www.mitpressjournals.org/rights_permission
  95. https://www.mitpressjournals.org/action/showmostreadarticles?journalcode=coli
  96. https://www.mitpressjournals.org/action/showmostcitedarticles?journalcode=coli
  97. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
  98. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
  99. https://creativecommons.org/licenses/by-nc-nd/4.0/
 100. https://giving.mit.edu/taxonomy/term/79#3920880
 101. https://www.mitpressjournals.org/author/zhang,+yue
 102. https://www.mitpressjournals.org/author/clark,+stephen
 103. https://doi.org/10.1162/coli_a_00229
 104. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 105. https://www.mitpressjournals.org/author/zhang,+yue
 106. https://www.mitpressjournals.org/author/clark,+stephen
 107. https://doi.org/10.1162/coli_a_00229
 108. https://www.mitpressjournals.org/doi/full/10.1162/coli_a_00229
 109. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#authors-content
 110. http://www.mitpressjournals.org/doi/pdf/10.1162/coli_a_00229
 111. http://www.mitpressjournals.org/doi/pdfplus/10.1162/coli_a_00229
 112. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#_i1
 113. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#abstract
 114. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#_i2
 115. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 116. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 117. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 118. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 119. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 120. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 121. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 122. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 123. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 124. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 125. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 126. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 127. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 128. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 129. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 130. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 131. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 132. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 133. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 134. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 135. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#_i1
 136. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#_i3
 137. javascript:popref('s4')
 138. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 139. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 140. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 141. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 142. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 143. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 144. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 145. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 146. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#_i2
 147. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#_i29
 148. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 149. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 150. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 151. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 152. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 153. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 154. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 155. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 156. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 157. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 158. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 159. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 160. javascript:popref('t1')
 161. javascript:popref('t1')
 162. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 163. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 164. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 165. javascript:popref('t1')
 166. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 167. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 168. javascript:popref('t1')
 169. javascript:popref('s2')
 170. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#fn3
 171. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 172. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 173. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 174. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 175. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 176. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 177. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 178. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 179. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 180. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#_i3
 181. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#_i40
 182. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 183. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 184. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 185. javascript:popref('t2')
 186. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 187. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 188. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 189. javascript:popref('t2')
 190. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 191. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 192. javascript:popref('s3d')
 193. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 194. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#_i29
 195. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#_i64
 196. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 197. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 198. javascript:popref('t3')
 199. javascript:popref('t3')
 200. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 201. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 202. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 203. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#fn4
 204. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 205. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 206. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 207. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 208. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 209. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 210. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 211. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 212. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 213. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 214. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 215. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 216. javascript:popref('t4')
 217. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 218. javascript:popref('t4')
 219. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 220. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 221. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 222. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 223. javascript:popref('t5')
 224. javascript:popref('t6')
 225. javascript:popref('t5')
 226. javascript:popref('t6')
 227. javascript:popref('t7')
 228. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#fn5
 229. javascript:popref('t7')
 230. javascript:popref('t8')
 231. javascript:popref('t8')
 232. javascript:popref('t9')
 233. javascript:popref('t9')
 234. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 235. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 236. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 237. javascript:popref('s2')
 238. javascript:popref('s4')
 239. javascript:popref('t2')
 240. javascript:popref('t2')
 241. javascript:popref('t10')
 242. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 243. javascript:popref('t10')
 244. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#_i40
 245. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#_i65
 246. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 247. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 248. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 249. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 250. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 251. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 252. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 253. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 254. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 255. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 256. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 257. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 258. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 259. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 260. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 261. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 262. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 263. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 264. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 265. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 266. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 267. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 268. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 269. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 270. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 271. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 272. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 273. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 274. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 275. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 276. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 277. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 278. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 279. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 280. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#_i64
 281. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#_i66
 282. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#_i65
 283. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#_i67
 284. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#_i66
 285. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#_i68
 286. http://w3.msi.vxu.se/  nivre/research/penn2malt.html
 287. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#_i67
 288. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#citart1
 289. http://scholar.google.com/scholar?hl=en&q=auli,+michael+and+adam+lopez.+2011.++training+a+log-linear+parser+with+loss+functions+via+softmax-margin.+in+proceedings+of+the+2011+conference+on+empirical+methods+in+natural+language+processing,+pages+333   343,+edinburgh.
 290. https://www.mitpressjournals.org/doi/10.1162/089120105774321091
 291. http://scholar.google.com/scholar?hl=en&q=barzilay,+regina+and+kathleen+mckeown.+2005.++sentence+fusion+for+multidocument+news+summarization.+computational+linguistics,+31(3):297   328.
 292. http://scholar.google.com/scholar?hl=en&q=belz,+anja,+michael+white,+dominic+espinosa,+eric+kow,+deirdre+hogan,+and+amanda+stent.+2011.++the+first+surface+realisation+shared+task:+overview+and+evaluation+results.+in+proceedings+of+the+13th+european+workshop+on+natural+language+generation,+eid86+'11,+pages+217   226,+stroudsburg,+pa.
 293. http://scholar.google.com/scholar?hl=en&q=blackwood,+graeme,+adri  +de+gispert,+and+william+byrne.+2010.++fluency+constraints+for+minimum+bayes-risk+decoding+of+statistical+machine+translation+lattices.+in+proceedings+of+the+23rd+international+conference+on+computational+linguistics+(coling+2010),+pages+71   79,+beijing.
 294. http://scholar.google.com/scholar?hl=en&q=bohnet,+bernd,+leo+wanner,+simon+mill,+and+alicia+burga.+2010.++broad+coverage+multilingual+deep+sentence+generation+with+a+stochastic+multi-level+realizer.+in+proceedings+of+the+23rd+international+conference+on+computational+linguistics+(coling+2010),+pages+98   106,+beijing.
 295. https://www.mitpressjournals.org/servlet/linkout?suffix=r6&dbid=16&doi=10.1162/coli_a_00229&key=10.3115/1220355.1220535
 296. http://scholar.google.com/scholar?hl=en&q=bos,+johan,+stephen+clark,mark+steedman,+james+r.+curran,+and+julia+hockenmaier.+2004.++wide-coverage+semantic+representations+from+a+id35+parser.+in+proceedings+of+coling-04,+pages+1240   1246,+geneva.
 297. http://scholar.google.com/scholar?hl=en&q=caraballo,+sharon+a.+and+eugene+charniak.+1998.++new+figures+of+merit+for+best-first+probabilistic+chart+parsing.+computational+linguistics,+24:275   298.
 298. http://scholar.google.com/scholar?hl=en&q=carroll,+john,+ann+copestake,+dan+flickinger,+and+victor+poznanski.+1999.++an+efficient+chart+generator+for+(semi-)+lexicalist+grammars.+in+proceedings+of+the+7th+european+workshop+on+natural+language+generation+(ewid8699),+pages+86   95,+toulouse.
 299. http://scholar.google.com/scholar?hl=en&q=carroll,+john+and+stephan+oepen.+2005.++high+efficiency+realization+for+a+wide-coverage+unification+grammar.+in+proceedings+of+the+second+international+joint+conference+on+natural+language+processing,+ijcnlp'05,+pages+165   176,+berlin.
 300. http://scholar.google.com/scholar?hl=en&q=chang,+pi-chuan+and+kristina+toutanova.+2007.++a+discriminative+syntactic+word+order+model+for+machine+translation.+in+proceedings+of+acl,+pages+9   16,+prague.
 301. https://www.mitpressjournals.org/doi/10.1162/coli.2007.33.2.201
 302. http://scholar.google.com/scholar?hl=en&q=chiang,+david.+2007.++hierarchical+phrase-based+translation.+computational+linguistics,+33(2):201   228.
 303. http://scholar.google.com/scholar?hl=en&q=clark,+stephen+and+james+r.+curran.+2007a.++id88+training+for+a+wide-coverage+lexicalized-grammar+parser.+in+proceedings+of+the+acl+2007+workshop+on+deep+linguistic+processing,+pages+9   16,+prague.
 304. https://www.mitpressjournals.org/doi/10.1162/coli.2007.33.4.493
 305. http://scholar.google.com/scholar?hl=en&q=clark,+stephen+and+james+r.+curran.+2007b.++wide-coverage+efficient+statistical+parsing+with+id35+and+log-linear+models.+computational+linguistics,+33(4):493   552.
 306. http://scholar.google.com/scholar?hl=en&q=collins,+michael+and+brian+roark.+2004.++incremental+parsing+with+the+id88+algorithm.+in+proceedings+of+acl,+pages+111   118,+barcelona.
 307. http://scholar.google.com/scholar?hl=en&q=crammer,+koby,+ofer+dekel,+joseph+keshet,+shai+shalev-shwartz,+and+yoram+singer.+2006.++online+passive-aggressive+algorithms.+journal+of+machine+learning+research,+7:551   585.
 308. https://www.mitpressjournals.org/servlet/linkout?suffix=r16&dbid=16&doi=10.1162/coli_a_00229&key=10.1007/s10994-009-5106-x
 309. http://scholar.google.com/scholar?hl=en&q=daum  ,+hal,+john+langford,+and+daniel+marcu.+2009.++search-based+structured+prediction.+machine+learning,+75(3):297   325.
 310. http://scholar.google.com/scholar?hl=en&q=daum  ,+hal+and+daniel+marcu.+2005.++learning+as+search+optimization:+approximate+large+margin+methods+for+structured+prediction.+in+icml,+pages+169   176,+bonn.
 311. http://scholar.google.com/scholar?hl=en&q=espinosa,+dominic,+michael+white,+and+dennis+mehay.+2008.++hypertagging:+id55+for+surface+realization+with+id35.+in+proceedings+of+acl-08:+hlt,+pages+183   191,+columbus,+oh.
 312. http://scholar.google.com/scholar?hl=en&q=filippova,+katja+and+michael+strube.+2007.++generating+constituent+order+in+german+clauses.+in+proceedings+of+the+45th+annual+meeting+of+the+association+of+computational+linguistics,+pages+320   327,+prague.
 313. https://www.mitpressjournals.org/servlet/linkout?suffix=r20&dbid=16&doi=10.1162/coli_a_00229&key=10.3115/1620853.1620915
 314. http://scholar.google.com/scholar?hl=en&q=filippova,+katja+and+michael+strube.+2009.++tree+linearization+in+english:+improving+language+model+based+approaches.+in+proceedings+of+human+language+technologies:+the+2009+annual+conference+of+the+north+american+chapter+of+the+association+for+computational+linguistics,+companion+volume:+short+papers,+pages+225   228,+boulder,+co.
 315. http://scholar.google.com/scholar?hl=en&q=fowler,+timothy+a.+d.+and+gerald+penn.+2010.++accurate+context-free+parsing+with+combinatory+categorial+grammar.+in+proceedings+of+the+48th+annual+meeting+of+the+association+for+computational+linguistics,+pages+335   344,+uppsala.
 316. http://scholar.google.com/scholar?hl=en&q=goldberg,+yoav+and+michael+elhadad.+2010.++an+efficient+algorithm+for+easy-first+non-directional+dependency+parsing.+in+human+language+technologies:+the+2010+annual+conference+of+the+north+american+chapter+of+the+association+for+computational+linguistics,+pages+742   750,+los+angeles,+ca.
 317. http://scholar.google.com/scholar?hl=en&q=guo,+yuqing,+deirdre+hogan,+and+josef+van+genabith.+2011.++dcu+at+generation+challenges+2011+surface+realisation+track.+in+proceedings+of+the+generation+challenges+session+at+the+13th+european+workshop+on+natural+language+generation,+pages+227   229,+nancy.
 318. http://scholar.google.com/scholar?hl=en&q=he,+wei,+haifeng+wang,+yuqing+guo,+and+ting+liu.+2009.++dependency+based+chinese+sentence+realization.+in+proceedings+of+the+joint+conference+of+the+47th+annual+meeting+of+the+acl+and+the+4th+international+joint+conference+on+natural+language+processing+of+the+afnlp,+pages+809   816,+suntec.
 319. http://scholar.google.com/scholar?hl=en&q=hockenmaier,+julia.+2003.+data+and+models+for+statistical+parsing+with+combinatory+categorial+grammar.+ph.d.+thesis,+school+of+informatics,+university+of+edinburgh.
 320. https://www.mitpressjournals.org/doi/10.1162/coli.2007.33.3.355
 321. http://scholar.google.com/scholar?hl=en&q=hockenmaier,+julia+and+mark+steedman.+2007.++id35bank:+a+corpus+of+id35+derivations+and+dependency+structures+extracted+from+the+penn+treebank.+computational+linguistics,+33(3):355   396.
 322. http://scholar.google.com/scholar?hl=en&q=huang,+liang,+suphan+fayong,+and+yang+guo.+2012.++structured+id88+with+inexact+search.+in+proceedings+of+the+2012+conference+of+the+north+american+chapter+of+the+association+for+computational+linguistics:+human+language+technologies,+pages+142   151,+montr  al.
 323. http://scholar.google.com/scholar?hl=en&q=huang,+liang+and+kenji+sagae.+2010.++dynamic+programming+for+linear-time+incremental+parsing.+in+proceedings+of+acl,+pages+1077   1086,+uppsala.
 324. http://scholar.google.com/scholar?hl=en&q=johansson,+richard+and+pierre+nugues.+2007.++extended+constituent-to-dependency+conversion+for+english.+in+16th+nordic+conference+of+computational+linguistics,+pages+105   112,+tartu.
 325. http://scholar.google.com/scholar?hl=en&q=kay,+martin.+1996.++chart+generation.+in+proceedings+of+acl,+pages+200   204,+santa+cruz,+ca.
 326. http://scholar.google.com/scholar?hl=en&q=koehn,+phillip.+2010.+statistical+machine+translation.+cambridge+university+press.
 327. http://scholar.google.com/scholar?hl=en&q=koehn,+philipp,+hieu+hoang,+alexandra+birch,+chris+callison-burch,+marcello+federico,+nicola+bertoldi,+brooke+cowan,+wade+shen,+christine+moran,+richard+zens,+chris+dyer,+ondrej+bojar,+alexandra+constantin,+and+evan+herbst.+2007.++moses:+open+source+toolkit+for+statistical+machine+translation.+in+proceedings+of+the+45th+annual+meeting+of+the+association+for+computational+linguistics+companion+volume+proceedings+of+the+demo+and+poster+sessions,+pages+177   180,+prague.
 328. https://www.mitpressjournals.org/servlet/linkout?suffix=r33&dbid=16&doi=10.1162/coli_a_00229&key=10.3115/1073445.1073462
 329. http://scholar.google.com/scholar?hl=en&q=koehn,+philipp,+franz+josef+och,+and+daniel+marcu.+2003.++statistical+phrase-based+translation.+in+proceedings+of+the+2003+conference+of+the+north+american+chapter+of+the+association+for+computational+linguistics+on+human+language+technology+-+volume+1,+naacl+'03,+pages+48   54,+edmonton,+canada.
 330. http://scholar.google.com/scholar?hl=en&q=koo,+terry+and+michael+collins.+2010.++efficient+third-order+dependency+parsers.+in+proceedings+of+acl,+pages+1   11,+uppsala.
 331. http://scholar.google.com/scholar?hl=en&q=lee,+j.+and+s.+seneff.+2006.++automatic+grammar+correction+for+second-language+learners.+in+proceedings+of+interspeech,+pages+1978   1981,+pittsburgh,+pa.
 332. http://scholar.google.com/scholar?hl=en&q=liu,+yang.+2013.++a+shift-reduce+parsing+algorithm+for+phrase-based+string-to-dependency+translation.+in+proceedings+of+the+51st+annual+meeting+of+the+association+for+computational+linguistics+(volume+1:+long+papers),+pages+1   10,+sofia.
 333. http://scholar.google.com/scholar?hl=en&q=marcus,+mitchell+p.,+beatrice+santorini,+and+mary+ann+marcinkiewicz.+1993.++building+a+large+annotated+corpus+of+english:+the+penn+treebank.+computational+linguistics,+19(2):313   330.
 334. http://scholar.google.com/scholar?hl=en&q=papineni,+kishore,+salim+roukos,+todd+ward,+and+wei-jing+zhu.+2002.++id7:+a+method+for+automatic+evaluation+of+machine+translation.+in+proceedings+of+the+40th+annual+meeting+of+the+association+for+computational+linguistics,+pages+311   318,+philadelphia,+pa.
 335. http://scholar.google.com/scholar?hl=en&q=ratnaparkhi,+adwait.+1996.++a+maximum+id178+model+for+part-of-speech+tagging.+in+proceedings+of+emnlp,+pages+133   142,+somerset,+nj.
 336. https://www.mitpressjournals.org/servlet/linkout?suffix=r40&dbid=16&doi=10.1162/coli_a_00229&key=10.1017/s1351324997001502
 337. http://scholar.google.com/scholar?hl=en&q=reiter,+ehud+and+robert+dale.+1997.++building+applied+natural+language+generation+systems.+natural+language+engineering,+3(1):57   87.
 338. http://scholar.google.com/scholar?hl=en&q=shen,+libin+and+aravind+joshi.+2008.++ltag+dependency+parsing+with+bidirectional+incremental+construction.+in+proceedings+of+the+2008+conference+on+empirical+methods+in+natural+language+processing,+pages+495   504,+honolulu.+hi.
 339. http://scholar.google.com/scholar?hl=en&q=shen,+libin,+giorgio+satta,+and+aravind+joshi.+2007.++guided+learning+for+bidirectional+sequence+classification.+in+proceedings+of+acl,+pages+760   767,+prague.
 340. http://scholar.google.com/scholar?hl=en&q=song,+linfeng,+yue+zhang,+kai+song,+and+qun+liu.+2014.++joint+morphological+generation+and+syntactic+linearization.+in+proceedings+of+the+twenty-eighth+aaai+conference,+quebec.
 341. http://scholar.google.com/scholar?hl=en&q=steedman,+mark.+2000.+the+syntactic+process.+the+mit+press,+cambridge,+ma.
 342. https://www.mitpressjournals.org/servlet/linkout?suffix=r45&dbid=16&doi=10.1162/coli_a_00229&key=10.3115/1596324.1596352
 343. http://scholar.google.com/scholar?hl=en&q=surdeanu,+mihai,+richard+johansson,+adam+meyers,+llu  s+m  rquez,+and+joakim+nivre.+2008.++the+conll-2008+shared+task+on+joint+parsing+of+syntactic+and+semantic+dependencies.+in+proceedings+of+the+twelfth+conference+on+computational+natural+language+learning,+pages+159   177,+manchester.
 344. http://scholar.google.com/scholar?hl=en&q=wan,+stephen,+mark+dras,+robert+dale,+and+c  cile+paris.+2009.++improving+grammaticality+in+statistical+sentence+generation:+introducing+a+dependency+spanning+tree+algorithm+with+an+argument+satisfaction+model.+in+proceedings+of+the+12th+conference+of+the+european+chapter+of+the+acl+(eacl+2009),+pages+852   860,+athens.
 345. http://scholar.google.com/scholar?hl=en&q=weir,+david.+1988.+characterizing+mildly+context-sensitive+grammar+formalisms.+ph.d.+thesis,+university+of+pennsylviania.
 346. http://scholar.google.com/scholar?hl=en&q=white,+michael.+2004.++reining+in+id35+chart+realization.+in+proceedings+of+iid86-04,+pages+182   191,+brockenhurst.
 347. https://www.mitpressjournals.org/servlet/linkout?suffix=r49&dbid=16&doi=10.1162/coli_a_00229&key=10.1007/s11168-006-9010-2
 348. http://scholar.google.com/scholar?hl=en&q=white,+michael.+2006.++efficient+realization+of+coordinate+structures+in+combinatory+categorial+grammar.+research+on+language+&+computation,+4(1):39   75.
 349. https://www.mitpressjournals.org/servlet/linkout?suffix=r50&dbid=16&doi=10.1162/coli_a_00229&key=10.3115/1699510.1699564
 350. http://scholar.google.com/scholar?hl=en&q=white,+michael+and+rajakrishnan+rajkumar.+2009.++id88+reranking+for+id35+realization.+in+proceedings+of+the+2009+conference+on+empirical+methods+in+natural+language+processing,+pages+410   419,+singapore.
 351. http://scholar.google.com/scholar?hl=en&q=wu,+dekai.+1997.++stochastic+inversion+transduction+grammars+and+bilingual+parsing+of+parallel+corpora.+computational+linguistics,+23(3):377   403.
 352. http://scholar.google.com/scholar?hl=en&q=xu,+peng,+ciprian+chelba,+and+frederick+jelinek.+2002.++a+study+on+richer+syntactic+dependencies+for+structured+language+modeling.+in+proceedings+of+the+40th+annual+meeting+of+the+association+for+computational+linguistics,+pages+191   198,+philadelphia,+pa.
 353. http://scholar.google.com/scholar?hl=en&q=zettlemoyer,+luke+s.+and+michael+collins.+2005.++learning+to+map+sentences+to+logical+form:+structured+classification+with+probabilistic+categorial+grammars.+in+proceedings+of+the+21st+conference+on+uncertainty+in+artificial+intelligence,+pages+658   666,+edinburgh.
 354. http://scholar.google.com/scholar?hl=en&q=zhang,+yue.+2013.++partial-tree+linearization:+generalized+word+ordering+for+text+synthesis.+in+proceedings+of+ijcai,+pages+2232   2238,+beijing.
 355. http://scholar.google.com/scholar?hl=en&q=zhang,+yue,+graeme+blackwood,+and+stephen+clark.+2012.++syntax-based+word+ordering+incorporating+a+large-scale+language+model.+in+proceedings+of+the+13th+conference+of+the+european+chapter+of+the+association+for+computational+linguistics,+pages+736   746,+avignon.
 356. http://scholar.google.com/scholar?hl=en&q=zhang,+yue+and+stephen+clark.+2008.++joint+word+segmentation+and+pos+tagging+using+a+single+id88.+in+proceedings+of+acl/hlt,+pages+888   896,+columbus,+oh.
 357. http://scholar.google.com/scholar?hl=en&q=zhang,+yue+and+stephen+clark.+2010.++a+fast+decoder+for+joint+word+segmentation+and+pos-tagging+using+a+single+discriminative+model.+in+proceedings+of+the+2010+conference+on+empirical+methods+in+natural+language+processing,+pages+843   852,+cambridge,+ma.
 358. https://www.mitpressjournals.org/doi/10.1162/coli_a_00037
 359. http://scholar.google.com/scholar?hl=en&q=zhang,+yue+and+stephen+clark.+2011.++syntactic+processing+using+the+generalized+id88+and+beam+search.+computational+linguistics,+37(1):105   151.
 360. http://scholar.google.com/scholar?hl=en&q=zhang,+yue+and+joakim+nivre.+2011.++transition-based+dependency+parsing+with+rich+non-local+features.+in+proceedings+of+the+49th+annual+meeting+of+the+association+for+computational+linguistics:+human+language+technologies,+pages+188   193,+portland,+or.
 361. https://www.mitpressjournals.org/cdn-cgi/l/email-protection#f78e8292a88d9f969990b784828393d9929382d98490
 362. https://www.mitpressjournals.org/cdn-cgi/l/email-protection#d4a7a0b1a4bcb1bafab7b8b5a6bf94b7b8fab7b5b9fab5b7faa1bf
 363. https://doi.org/10.1162/coli_a_00229
 364. https://www.mitpressjournals.org/doi/abs/10.1162/coli_a_00229
 365. https://www.mitpressjournals.org/doi/full/10.1162/coli_a_00229
 366. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#authors-content
 367. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#_i1
 368. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#abstract
 369. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#_i2
 370. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 371. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 372. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 373. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 374. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 375. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 376. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 377. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 378. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 379. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 380. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 381. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 382. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 383. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 384. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 385. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 386. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 387. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 388. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 389. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 390. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#_i1
 391. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#_i3
 392. javascript:popref('s4')
 393. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 394. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 395. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 396. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 397. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 398. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 399. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 400. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 401. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#_i2
 402. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#_i29
 403. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 404. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 405. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 406. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 407. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 408. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 409. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 410. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 411. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 412. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 413. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 414. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 415. javascript:popref('t1')
 416. javascript:popref('t1')
 417. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 418. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 419. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 420. javascript:popref('t1')
 421. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 422. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 423. javascript:popref('t1')
 424. javascript:popref('s2')
 425. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#fn3
 426. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 427. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 428. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 429. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 430. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 431. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 432. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 433. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 434. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 435. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#_i3
 436. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#_i40
 437. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 438. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 439. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 440. javascript:popref('t2')
 441. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 442. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 443. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 444. javascript:popref('t2')
 445. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 446. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 447. javascript:popref('s3d')
 448. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 449. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#_i29
 450. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#_i64
 451. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 452. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 453. javascript:popref('t3')
 454. javascript:popref('t3')
 455. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 456. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 457. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 458. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#fn4
 459. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 460. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 461. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 462. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 463. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 464. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 465. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 466. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 467. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 468. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 469. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 470. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 471. javascript:popref('t4')
 472. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 473. javascript:popref('t4')
 474. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 475. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 476. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 477. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 478. javascript:popref('t5')
 479. javascript:popref('t6')
 480. javascript:popref('t5')
 481. javascript:popref('t6')
 482. javascript:popref('t7')
 483. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#fn5
 484. javascript:popref('t7')
 485. javascript:popref('t8')
 486. javascript:popref('t8')
 487. javascript:popref('t9')
 488. javascript:popref('t9')
 489. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 490. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 491. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 492. javascript:popref('s2')
 493. javascript:popref('s4')
 494. javascript:popref('t2')
 495. javascript:popref('t2')
 496. javascript:popref('t10')
 497. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 498. javascript:popref('t10')
 499. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#_i40
 500. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#_i65
 501. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 502. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 503. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 504. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 505. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 506. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 507. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 508. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 509. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 510. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 511. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 512. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 513. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 514. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 515. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 516. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 517. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 518. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 519. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 520. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 521. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 522. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 523. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 524. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 525. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 526. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 527. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 528. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 529. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 530. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 531. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 532. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 533. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 534. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 535. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#_i64
 536. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#_i66
 537. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#_i65
 538. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#_i67
 539. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#_i66
 540. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#_i68
 541. http://w3.msi.vxu.se/  nivre/research/penn2malt.html
 542. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#_i67
 543. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#citart1
 544. http://scholar.google.com/scholar?hl=en&q=auli,+michael+and+adam+lopez.+2011.++training+a+log-linear+parser+with+loss+functions+via+softmax-margin.+in+proceedings+of+the+2011+conference+on+empirical+methods+in+natural+language+processing,+pages+333   343,+edinburgh.
 545. https://www.mitpressjournals.org/doi/10.1162/089120105774321091
 546. http://scholar.google.com/scholar?hl=en&q=barzilay,+regina+and+kathleen+mckeown.+2005.++sentence+fusion+for+multidocument+news+summarization.+computational+linguistics,+31(3):297   328.
 547. http://scholar.google.com/scholar?hl=en&q=belz,+anja,+michael+white,+dominic+espinosa,+eric+kow,+deirdre+hogan,+and+amanda+stent.+2011.++the+first+surface+realisation+shared+task:+overview+and+evaluation+results.+in+proceedings+of+the+13th+european+workshop+on+natural+language+generation,+eid86+'11,+pages+217   226,+stroudsburg,+pa.
 548. http://scholar.google.com/scholar?hl=en&q=blackwood,+graeme,+adri  +de+gispert,+and+william+byrne.+2010.++fluency+constraints+for+minimum+bayes-risk+decoding+of+statistical+machine+translation+lattices.+in+proceedings+of+the+23rd+international+conference+on+computational+linguistics+(coling+2010),+pages+71   79,+beijing.
 549. http://scholar.google.com/scholar?hl=en&q=bohnet,+bernd,+leo+wanner,+simon+mill,+and+alicia+burga.+2010.++broad+coverage+multilingual+deep+sentence+generation+with+a+stochastic+multi-level+realizer.+in+proceedings+of+the+23rd+international+conference+on+computational+linguistics+(coling+2010),+pages+98   106,+beijing.
 550. https://www.mitpressjournals.org/servlet/linkout?suffix=r6&dbid=16&doi=10.1162/coli_a_00229&key=10.3115/1220355.1220535
 551. http://scholar.google.com/scholar?hl=en&q=bos,+johan,+stephen+clark,mark+steedman,+james+r.+curran,+and+julia+hockenmaier.+2004.++wide-coverage+semantic+representations+from+a+id35+parser.+in+proceedings+of+coling-04,+pages+1240   1246,+geneva.
 552. http://scholar.google.com/scholar?hl=en&q=caraballo,+sharon+a.+and+eugene+charniak.+1998.++new+figures+of+merit+for+best-first+probabilistic+chart+parsing.+computational+linguistics,+24:275   298.
 553. http://scholar.google.com/scholar?hl=en&q=carroll,+john,+ann+copestake,+dan+flickinger,+and+victor+poznanski.+1999.++an+efficient+chart+generator+for+(semi-)+lexicalist+grammars.+in+proceedings+of+the+7th+european+workshop+on+natural+language+generation+(ewid8699),+pages+86   95,+toulouse.
 554. http://scholar.google.com/scholar?hl=en&q=carroll,+john+and+stephan+oepen.+2005.++high+efficiency+realization+for+a+wide-coverage+unification+grammar.+in+proceedings+of+the+second+international+joint+conference+on+natural+language+processing,+ijcnlp'05,+pages+165   176,+berlin.
 555. http://scholar.google.com/scholar?hl=en&q=chang,+pi-chuan+and+kristina+toutanova.+2007.++a+discriminative+syntactic+word+order+model+for+machine+translation.+in+proceedings+of+acl,+pages+9   16,+prague.
 556. https://www.mitpressjournals.org/doi/10.1162/coli.2007.33.2.201
 557. http://scholar.google.com/scholar?hl=en&q=chiang,+david.+2007.++hierarchical+phrase-based+translation.+computational+linguistics,+33(2):201   228.
 558. http://scholar.google.com/scholar?hl=en&q=clark,+stephen+and+james+r.+curran.+2007a.++id88+training+for+a+wide-coverage+lexicalized-grammar+parser.+in+proceedings+of+the+acl+2007+workshop+on+deep+linguistic+processing,+pages+9   16,+prague.
 559. https://www.mitpressjournals.org/doi/10.1162/coli.2007.33.4.493
 560. http://scholar.google.com/scholar?hl=en&q=clark,+stephen+and+james+r.+curran.+2007b.++wide-coverage+efficient+statistical+parsing+with+id35+and+log-linear+models.+computational+linguistics,+33(4):493   552.
 561. http://scholar.google.com/scholar?hl=en&q=collins,+michael+and+brian+roark.+2004.++incremental+parsing+with+the+id88+algorithm.+in+proceedings+of+acl,+pages+111   118,+barcelona.
 562. http://scholar.google.com/scholar?hl=en&q=crammer,+koby,+ofer+dekel,+joseph+keshet,+shai+shalev-shwartz,+and+yoram+singer.+2006.++online+passive-aggressive+algorithms.+journal+of+machine+learning+research,+7:551   585.
 563. https://www.mitpressjournals.org/servlet/linkout?suffix=r16&dbid=16&doi=10.1162/coli_a_00229&key=10.1007/s10994-009-5106-x
 564. http://scholar.google.com/scholar?hl=en&q=daum  ,+hal,+john+langford,+and+daniel+marcu.+2009.++search-based+structured+prediction.+machine+learning,+75(3):297   325.
 565. http://scholar.google.com/scholar?hl=en&q=daum  ,+hal+and+daniel+marcu.+2005.++learning+as+search+optimization:+approximate+large+margin+methods+for+structured+prediction.+in+icml,+pages+169   176,+bonn.
 566. http://scholar.google.com/scholar?hl=en&q=espinosa,+dominic,+michael+white,+and+dennis+mehay.+2008.++hypertagging:+id55+for+surface+realization+with+id35.+in+proceedings+of+acl-08:+hlt,+pages+183   191,+columbus,+oh.
 567. http://scholar.google.com/scholar?hl=en&q=filippova,+katja+and+michael+strube.+2007.++generating+constituent+order+in+german+clauses.+in+proceedings+of+the+45th+annual+meeting+of+the+association+of+computational+linguistics,+pages+320   327,+prague.
 568. https://www.mitpressjournals.org/servlet/linkout?suffix=r20&dbid=16&doi=10.1162/coli_a_00229&key=10.3115/1620853.1620915
 569. http://scholar.google.com/scholar?hl=en&q=filippova,+katja+and+michael+strube.+2009.++tree+linearization+in+english:+improving+language+model+based+approaches.+in+proceedings+of+human+language+technologies:+the+2009+annual+conference+of+the+north+american+chapter+of+the+association+for+computational+linguistics,+companion+volume:+short+papers,+pages+225   228,+boulder,+co.
 570. http://scholar.google.com/scholar?hl=en&q=fowler,+timothy+a.+d.+and+gerald+penn.+2010.++accurate+context-free+parsing+with+combinatory+categorial+grammar.+in+proceedings+of+the+48th+annual+meeting+of+the+association+for+computational+linguistics,+pages+335   344,+uppsala.
 571. http://scholar.google.com/scholar?hl=en&q=goldberg,+yoav+and+michael+elhadad.+2010.++an+efficient+algorithm+for+easy-first+non-directional+dependency+parsing.+in+human+language+technologies:+the+2010+annual+conference+of+the+north+american+chapter+of+the+association+for+computational+linguistics,+pages+742   750,+los+angeles,+ca.
 572. http://scholar.google.com/scholar?hl=en&q=guo,+yuqing,+deirdre+hogan,+and+josef+van+genabith.+2011.++dcu+at+generation+challenges+2011+surface+realisation+track.+in+proceedings+of+the+generation+challenges+session+at+the+13th+european+workshop+on+natural+language+generation,+pages+227   229,+nancy.
 573. http://scholar.google.com/scholar?hl=en&q=he,+wei,+haifeng+wang,+yuqing+guo,+and+ting+liu.+2009.++dependency+based+chinese+sentence+realization.+in+proceedings+of+the+joint+conference+of+the+47th+annual+meeting+of+the+acl+and+the+4th+international+joint+conference+on+natural+language+processing+of+the+afnlp,+pages+809   816,+suntec.
 574. http://scholar.google.com/scholar?hl=en&q=hockenmaier,+julia.+2003.+data+and+models+for+statistical+parsing+with+combinatory+categorial+grammar.+ph.d.+thesis,+school+of+informatics,+university+of+edinburgh.
 575. https://www.mitpressjournals.org/doi/10.1162/coli.2007.33.3.355
 576. http://scholar.google.com/scholar?hl=en&q=hockenmaier,+julia+and+mark+steedman.+2007.++id35bank:+a+corpus+of+id35+derivations+and+dependency+structures+extracted+from+the+penn+treebank.+computational+linguistics,+33(3):355   396.
 577. http://scholar.google.com/scholar?hl=en&q=huang,+liang,+suphan+fayong,+and+yang+guo.+2012.++structured+id88+with+inexact+search.+in+proceedings+of+the+2012+conference+of+the+north+american+chapter+of+the+association+for+computational+linguistics:+human+language+technologies,+pages+142   151,+montr  al.
 578. http://scholar.google.com/scholar?hl=en&q=huang,+liang+and+kenji+sagae.+2010.++dynamic+programming+for+linear-time+incremental+parsing.+in+proceedings+of+acl,+pages+1077   1086,+uppsala.
 579. http://scholar.google.com/scholar?hl=en&q=johansson,+richard+and+pierre+nugues.+2007.++extended+constituent-to-dependency+conversion+for+english.+in+16th+nordic+conference+of+computational+linguistics,+pages+105   112,+tartu.
 580. http://scholar.google.com/scholar?hl=en&q=kay,+martin.+1996.++chart+generation.+in+proceedings+of+acl,+pages+200   204,+santa+cruz,+ca.
 581. http://scholar.google.com/scholar?hl=en&q=koehn,+phillip.+2010.+statistical+machine+translation.+cambridge+university+press.
 582. http://scholar.google.com/scholar?hl=en&q=koehn,+philipp,+hieu+hoang,+alexandra+birch,+chris+callison-burch,+marcello+federico,+nicola+bertoldi,+brooke+cowan,+wade+shen,+christine+moran,+richard+zens,+chris+dyer,+ondrej+bojar,+alexandra+constantin,+and+evan+herbst.+2007.++moses:+open+source+toolkit+for+statistical+machine+translation.+in+proceedings+of+the+45th+annual+meeting+of+the+association+for+computational+linguistics+companion+volume+proceedings+of+the+demo+and+poster+sessions,+pages+177   180,+prague.
 583. https://www.mitpressjournals.org/servlet/linkout?suffix=r33&dbid=16&doi=10.1162/coli_a_00229&key=10.3115/1073445.1073462
 584. http://scholar.google.com/scholar?hl=en&q=koehn,+philipp,+franz+josef+och,+and+daniel+marcu.+2003.++statistical+phrase-based+translation.+in+proceedings+of+the+2003+conference+of+the+north+american+chapter+of+the+association+for+computational+linguistics+on+human+language+technology+-+volume+1,+naacl+'03,+pages+48   54,+edmonton,+canada.
 585. http://scholar.google.com/scholar?hl=en&q=koo,+terry+and+michael+collins.+2010.++efficient+third-order+dependency+parsers.+in+proceedings+of+acl,+pages+1   11,+uppsala.
 586. http://scholar.google.com/scholar?hl=en&q=lee,+j.+and+s.+seneff.+2006.++automatic+grammar+correction+for+second-language+learners.+in+proceedings+of+interspeech,+pages+1978   1981,+pittsburgh,+pa.
 587. http://scholar.google.com/scholar?hl=en&q=liu,+yang.+2013.++a+shift-reduce+parsing+algorithm+for+phrase-based+string-to-dependency+translation.+in+proceedings+of+the+51st+annual+meeting+of+the+association+for+computational+linguistics+(volume+1:+long+papers),+pages+1   10,+sofia.
 588. http://scholar.google.com/scholar?hl=en&q=marcus,+mitchell+p.,+beatrice+santorini,+and+mary+ann+marcinkiewicz.+1993.++building+a+large+annotated+corpus+of+english:+the+penn+treebank.+computational+linguistics,+19(2):313   330.
 589. http://scholar.google.com/scholar?hl=en&q=papineni,+kishore,+salim+roukos,+todd+ward,+and+wei-jing+zhu.+2002.++id7:+a+method+for+automatic+evaluation+of+machine+translation.+in+proceedings+of+the+40th+annual+meeting+of+the+association+for+computational+linguistics,+pages+311   318,+philadelphia,+pa.
 590. http://scholar.google.com/scholar?hl=en&q=ratnaparkhi,+adwait.+1996.++a+maximum+id178+model+for+part-of-speech+tagging.+in+proceedings+of+emnlp,+pages+133   142,+somerset,+nj.
 591. https://www.mitpressjournals.org/servlet/linkout?suffix=r40&dbid=16&doi=10.1162/coli_a_00229&key=10.1017/s1351324997001502
 592. http://scholar.google.com/scholar?hl=en&q=reiter,+ehud+and+robert+dale.+1997.++building+applied+natural+language+generation+systems.+natural+language+engineering,+3(1):57   87.
 593. http://scholar.google.com/scholar?hl=en&q=shen,+libin+and+aravind+joshi.+2008.++ltag+dependency+parsing+with+bidirectional+incremental+construction.+in+proceedings+of+the+2008+conference+on+empirical+methods+in+natural+language+processing,+pages+495   504,+honolulu.+hi.
 594. http://scholar.google.com/scholar?hl=en&q=shen,+libin,+giorgio+satta,+and+aravind+joshi.+2007.++guided+learning+for+bidirectional+sequence+classification.+in+proceedings+of+acl,+pages+760   767,+prague.
 595. http://scholar.google.com/scholar?hl=en&q=song,+linfeng,+yue+zhang,+kai+song,+and+qun+liu.+2014.++joint+morphological+generation+and+syntactic+linearization.+in+proceedings+of+the+twenty-eighth+aaai+conference,+quebec.
 596. http://scholar.google.com/scholar?hl=en&q=steedman,+mark.+2000.+the+syntactic+process.+the+mit+press,+cambridge,+ma.
 597. https://www.mitpressjournals.org/servlet/linkout?suffix=r45&dbid=16&doi=10.1162/coli_a_00229&key=10.3115/1596324.1596352
 598. http://scholar.google.com/scholar?hl=en&q=surdeanu,+mihai,+richard+johansson,+adam+meyers,+llu  s+m  rquez,+and+joakim+nivre.+2008.++the+conll-2008+shared+task+on+joint+parsing+of+syntactic+and+semantic+dependencies.+in+proceedings+of+the+twelfth+conference+on+computational+natural+language+learning,+pages+159   177,+manchester.
 599. http://scholar.google.com/scholar?hl=en&q=wan,+stephen,+mark+dras,+robert+dale,+and+c  cile+paris.+2009.++improving+grammaticality+in+statistical+sentence+generation:+introducing+a+dependency+spanning+tree+algorithm+with+an+argument+satisfaction+model.+in+proceedings+of+the+12th+conference+of+the+european+chapter+of+the+acl+(eacl+2009),+pages+852   860,+athens.
 600. http://scholar.google.com/scholar?hl=en&q=weir,+david.+1988.+characterizing+mildly+context-sensitive+grammar+formalisms.+ph.d.+thesis,+university+of+pennsylviania.
 601. http://scholar.google.com/scholar?hl=en&q=white,+michael.+2004.++reining+in+id35+chart+realization.+in+proceedings+of+iid86-04,+pages+182   191,+brockenhurst.
 602. https://www.mitpressjournals.org/servlet/linkout?suffix=r49&dbid=16&doi=10.1162/coli_a_00229&key=10.1007/s11168-006-9010-2
 603. http://scholar.google.com/scholar?hl=en&q=white,+michael.+2006.++efficient+realization+of+coordinate+structures+in+combinatory+categorial+grammar.+research+on+language+&+computation,+4(1):39   75.
 604. https://www.mitpressjournals.org/servlet/linkout?suffix=r50&dbid=16&doi=10.1162/coli_a_00229&key=10.3115/1699510.1699564
 605. http://scholar.google.com/scholar?hl=en&q=white,+michael+and+rajakrishnan+rajkumar.+2009.++id88+reranking+for+id35+realization.+in+proceedings+of+the+2009+conference+on+empirical+methods+in+natural+language+processing,+pages+410   419,+singapore.
 606. http://scholar.google.com/scholar?hl=en&q=wu,+dekai.+1997.++stochastic+inversion+transduction+grammars+and+bilingual+parsing+of+parallel+corpora.+computational+linguistics,+23(3):377   403.
 607. http://scholar.google.com/scholar?hl=en&q=xu,+peng,+ciprian+chelba,+and+frederick+jelinek.+2002.++a+study+on+richer+syntactic+dependencies+for+structured+language+modeling.+in+proceedings+of+the+40th+annual+meeting+of+the+association+for+computational+linguistics,+pages+191   198,+philadelphia,+pa.
 608. http://scholar.google.com/scholar?hl=en&q=zettlemoyer,+luke+s.+and+michael+collins.+2005.++learning+to+map+sentences+to+logical+form:+structured+classification+with+probabilistic+categorial+grammars.+in+proceedings+of+the+21st+conference+on+uncertainty+in+artificial+intelligence,+pages+658   666,+edinburgh.
 609. http://scholar.google.com/scholar?hl=en&q=zhang,+yue.+2013.++partial-tree+linearization:+generalized+word+ordering+for+text+synthesis.+in+proceedings+of+ijcai,+pages+2232   2238,+beijing.
 610. http://scholar.google.com/scholar?hl=en&q=zhang,+yue,+graeme+blackwood,+and+stephen+clark.+2012.++syntax-based+word+ordering+incorporating+a+large-scale+language+model.+in+proceedings+of+the+13th+conference+of+the+european+chapter+of+the+association+for+computational+linguistics,+pages+736   746,+avignon.
 611. http://scholar.google.com/scholar?hl=en&q=zhang,+yue+and+stephen+clark.+2008.++joint+word+segmentation+and+pos+tagging+using+a+single+id88.+in+proceedings+of+acl/hlt,+pages+888   896,+columbus,+oh.
 612. http://scholar.google.com/scholar?hl=en&q=zhang,+yue+and+stephen+clark.+2010.++a+fast+decoder+for+joint+word+segmentation+and+pos-tagging+using+a+single+discriminative+model.+in+proceedings+of+the+2010+conference+on+empirical+methods+in+natural+language+processing,+pages+843   852,+cambridge,+ma.
 613. https://www.mitpressjournals.org/doi/10.1162/coli_a_00037
 614. http://scholar.google.com/scholar?hl=en&q=zhang,+yue+and+stephen+clark.+2011.++syntactic+processing+using+the+generalized+id88+and+beam+search.+computational+linguistics,+37(1):105   151.
 615. http://scholar.google.com/scholar?hl=en&q=zhang,+yue+and+joakim+nivre.+2011.++transition-based+dependency+parsing+with+rich+non-local+features.+in+proceedings+of+the+49th+annual+meeting+of+the+association+for+computational+linguistics:+human+language+technologies,+pages+188   193,+portland,+or.
 616. https://www.mitpressjournals.org/cdn-cgi/l/email-protection#fd848898a287959c939abd8e888999d3989988d38e9a
 617. https://www.mitpressjournals.org/cdn-cgi/l/email-protection#ed9e99889d858883c38e818c9f86ad8e81c38e8c80c38c8ec39886
 618. https://www.mitpressjournals.org/forthcoming/coli
 619. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 620. https://www.mitpressjournals.org/author/zhang,+yue
 621. https://www.mitpressjournals.org/author/clark,+stephen
 622. https://doi.org/10.1162/coli_a_00229
 623. https://www.mitpressjournals.org/doi/full/10.1162/coli_a_00229
 624. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#authors-content
 625. http://www.mitpressjournals.org/doi/pdf/10.1162/coli_a_00229
 626. http://www.mitpressjournals.org/doi/pdfplus/10.1162/coli_a_00229
 627. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#_i1
 628. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#abstract
 629. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#_i2
 630. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 631. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 632. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 633. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 634. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 635. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 636. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 637. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 638. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 639. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 640. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 641. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 642. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 643. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 644. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 645. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 646. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 647. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 648. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 649. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 650. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#_i1
 651. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#_i3
 652. javascript:popref('s4')
 653. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 654. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 655. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 656. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 657. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 658. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 659. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 660. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 661. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#_i2
 662. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#_i29
 663. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 664. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 665. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 666. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 667. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 668. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 669. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 670. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 671. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 672. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 673. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 674. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 675. javascript:popref('t1')
 676. javascript:popref('t1')
 677. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 678. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 679. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 680. javascript:popref('t1')
 681. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 682. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 683. javascript:popref('t1')
 684. javascript:popref('s2')
 685. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#fn3
 686. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 687. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 688. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 689. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 690. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 691. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 692. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 693. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 694. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 695. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#_i3
 696. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#_i40
 697. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 698. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 699. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 700. javascript:popref('t2')
 701. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 702. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 703. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 704. javascript:popref('t2')
 705. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 706. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 707. javascript:popref('s3d')
 708. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 709. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#_i29
 710. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#_i64
 711. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 712. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 713. javascript:popref('t3')
 714. javascript:popref('t3')
 715. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 716. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 717. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 718. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#fn4
 719. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 720. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 721. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 722. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 723. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 724. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 725. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 726. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 727. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 728. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 729. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 730. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 731. javascript:popref('t4')
 732. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 733. javascript:popref('t4')
 734. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 735. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 736. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 737. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 738. javascript:popref('t5')
 739. javascript:popref('t6')
 740. javascript:popref('t5')
 741. javascript:popref('t6')
 742. javascript:popref('t7')
 743. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#fn5
 744. javascript:popref('t7')
 745. javascript:popref('t8')
 746. javascript:popref('t8')
 747. javascript:popref('t9')
 748. javascript:popref('t9')
 749. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 750. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 751. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 752. javascript:popref('s2')
 753. javascript:popref('s4')
 754. javascript:popref('t2')
 755. javascript:popref('t2')
 756. javascript:popref('t10')
 757. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 758. javascript:popref('t10')
 759. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#_i40
 760. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#_i65
 761. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 762. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 763. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 764. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 765. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 766. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 767. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 768. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 769. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 770. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 771. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 772. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 773. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 774. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 775. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 776. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 777. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 778. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 779. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 780. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 781. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 782. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 783. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 784. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 785. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 786. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 787. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 788. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 789. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 790. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 791. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 792. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 793. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 794. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 795. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#_i64
 796. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#_i66
 797. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#_i65
 798. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#_i67
 799. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#_i66
 800. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#_i68
 801. http://w3.msi.vxu.se/  nivre/research/penn2malt.html
 802. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#_i67
 803. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229#citart1
 804. http://scholar.google.com/scholar?hl=en&q=auli,+michael+and+adam+lopez.+2011.++training+a+log-linear+parser+with+loss+functions+via+softmax-margin.+in+proceedings+of+the+2011+conference+on+empirical+methods+in+natural+language+processing,+pages+333   343,+edinburgh.
 805. https://www.mitpressjournals.org/doi/10.1162/089120105774321091
 806. http://scholar.google.com/scholar?hl=en&q=barzilay,+regina+and+kathleen+mckeown.+2005.++sentence+fusion+for+multidocument+news+summarization.+computational+linguistics,+31(3):297   328.
 807. http://scholar.google.com/scholar?hl=en&q=belz,+anja,+michael+white,+dominic+espinosa,+eric+kow,+deirdre+hogan,+and+amanda+stent.+2011.++the+first+surface+realisation+shared+task:+overview+and+evaluation+results.+in+proceedings+of+the+13th+european+workshop+on+natural+language+generation,+eid86+'11,+pages+217   226,+stroudsburg,+pa.
 808. http://scholar.google.com/scholar?hl=en&q=blackwood,+graeme,+adri  +de+gispert,+and+william+byrne.+2010.++fluency+constraints+for+minimum+bayes-risk+decoding+of+statistical+machine+translation+lattices.+in+proceedings+of+the+23rd+international+conference+on+computational+linguistics+(coling+2010),+pages+71   79,+beijing.
 809. http://scholar.google.com/scholar?hl=en&q=bohnet,+bernd,+leo+wanner,+simon+mill,+and+alicia+burga.+2010.++broad+coverage+multilingual+deep+sentence+generation+with+a+stochastic+multi-level+realizer.+in+proceedings+of+the+23rd+international+conference+on+computational+linguistics+(coling+2010),+pages+98   106,+beijing.
 810. https://www.mitpressjournals.org/servlet/linkout?suffix=r6&dbid=16&doi=10.1162/coli_a_00229&key=10.3115/1220355.1220535
 811. http://scholar.google.com/scholar?hl=en&q=bos,+johan,+stephen+clark,mark+steedman,+james+r.+curran,+and+julia+hockenmaier.+2004.++wide-coverage+semantic+representations+from+a+id35+parser.+in+proceedings+of+coling-04,+pages+1240   1246,+geneva.
 812. http://scholar.google.com/scholar?hl=en&q=caraballo,+sharon+a.+and+eugene+charniak.+1998.++new+figures+of+merit+for+best-first+probabilistic+chart+parsing.+computational+linguistics,+24:275   298.
 813. http://scholar.google.com/scholar?hl=en&q=carroll,+john,+ann+copestake,+dan+flickinger,+and+victor+poznanski.+1999.++an+efficient+chart+generator+for+(semi-)+lexicalist+grammars.+in+proceedings+of+the+7th+european+workshop+on+natural+language+generation+(ewid8699),+pages+86   95,+toulouse.
 814. http://scholar.google.com/scholar?hl=en&q=carroll,+john+and+stephan+oepen.+2005.++high+efficiency+realization+for+a+wide-coverage+unification+grammar.+in+proceedings+of+the+second+international+joint+conference+on+natural+language+processing,+ijcnlp'05,+pages+165   176,+berlin.
 815. http://scholar.google.com/scholar?hl=en&q=chang,+pi-chuan+and+kristina+toutanova.+2007.++a+discriminative+syntactic+word+order+model+for+machine+translation.+in+proceedings+of+acl,+pages+9   16,+prague.
 816. https://www.mitpressjournals.org/doi/10.1162/coli.2007.33.2.201
 817. http://scholar.google.com/scholar?hl=en&q=chiang,+david.+2007.++hierarchical+phrase-based+translation.+computational+linguistics,+33(2):201   228.
 818. http://scholar.google.com/scholar?hl=en&q=clark,+stephen+and+james+r.+curran.+2007a.++id88+training+for+a+wide-coverage+lexicalized-grammar+parser.+in+proceedings+of+the+acl+2007+workshop+on+deep+linguistic+processing,+pages+9   16,+prague.
 819. https://www.mitpressjournals.org/doi/10.1162/coli.2007.33.4.493
 820. http://scholar.google.com/scholar?hl=en&q=clark,+stephen+and+james+r.+curran.+2007b.++wide-coverage+efficient+statistical+parsing+with+id35+and+log-linear+models.+computational+linguistics,+33(4):493   552.
 821. http://scholar.google.com/scholar?hl=en&q=collins,+michael+and+brian+roark.+2004.++incremental+parsing+with+the+id88+algorithm.+in+proceedings+of+acl,+pages+111   118,+barcelona.
 822. http://scholar.google.com/scholar?hl=en&q=crammer,+koby,+ofer+dekel,+joseph+keshet,+shai+shalev-shwartz,+and+yoram+singer.+2006.++online+passive-aggressive+algorithms.+journal+of+machine+learning+research,+7:551   585.
 823. https://www.mitpressjournals.org/servlet/linkout?suffix=r16&dbid=16&doi=10.1162/coli_a_00229&key=10.1007/s10994-009-5106-x
 824. http://scholar.google.com/scholar?hl=en&q=daum  ,+hal,+john+langford,+and+daniel+marcu.+2009.++search-based+structured+prediction.+machine+learning,+75(3):297   325.
 825. http://scholar.google.com/scholar?hl=en&q=daum  ,+hal+and+daniel+marcu.+2005.++learning+as+search+optimization:+approximate+large+margin+methods+for+structured+prediction.+in+icml,+pages+169   176,+bonn.
 826. http://scholar.google.com/scholar?hl=en&q=espinosa,+dominic,+michael+white,+and+dennis+mehay.+2008.++hypertagging:+id55+for+surface+realization+with+id35.+in+proceedings+of+acl-08:+hlt,+pages+183   191,+columbus,+oh.
 827. http://scholar.google.com/scholar?hl=en&q=filippova,+katja+and+michael+strube.+2007.++generating+constituent+order+in+german+clauses.+in+proceedings+of+the+45th+annual+meeting+of+the+association+of+computational+linguistics,+pages+320   327,+prague.
 828. https://www.mitpressjournals.org/servlet/linkout?suffix=r20&dbid=16&doi=10.1162/coli_a_00229&key=10.3115/1620853.1620915
 829. http://scholar.google.com/scholar?hl=en&q=filippova,+katja+and+michael+strube.+2009.++tree+linearization+in+english:+improving+language+model+based+approaches.+in+proceedings+of+human+language+technologies:+the+2009+annual+conference+of+the+north+american+chapter+of+the+association+for+computational+linguistics,+companion+volume:+short+papers,+pages+225   228,+boulder,+co.
 830. http://scholar.google.com/scholar?hl=en&q=fowler,+timothy+a.+d.+and+gerald+penn.+2010.++accurate+context-free+parsing+with+combinatory+categorial+grammar.+in+proceedings+of+the+48th+annual+meeting+of+the+association+for+computational+linguistics,+pages+335   344,+uppsala.
 831. http://scholar.google.com/scholar?hl=en&q=goldberg,+yoav+and+michael+elhadad.+2010.++an+efficient+algorithm+for+easy-first+non-directional+dependency+parsing.+in+human+language+technologies:+the+2010+annual+conference+of+the+north+american+chapter+of+the+association+for+computational+linguistics,+pages+742   750,+los+angeles,+ca.
 832. http://scholar.google.com/scholar?hl=en&q=guo,+yuqing,+deirdre+hogan,+and+josef+van+genabith.+2011.++dcu+at+generation+challenges+2011+surface+realisation+track.+in+proceedings+of+the+generation+challenges+session+at+the+13th+european+workshop+on+natural+language+generation,+pages+227   229,+nancy.
 833. http://scholar.google.com/scholar?hl=en&q=he,+wei,+haifeng+wang,+yuqing+guo,+and+ting+liu.+2009.++dependency+based+chinese+sentence+realization.+in+proceedings+of+the+joint+conference+of+the+47th+annual+meeting+of+the+acl+and+the+4th+international+joint+conference+on+natural+language+processing+of+the+afnlp,+pages+809   816,+suntec.
 834. http://scholar.google.com/scholar?hl=en&q=hockenmaier,+julia.+2003.+data+and+models+for+statistical+parsing+with+combinatory+categorial+grammar.+ph.d.+thesis,+school+of+informatics,+university+of+edinburgh.
 835. https://www.mitpressjournals.org/doi/10.1162/coli.2007.33.3.355
 836. http://scholar.google.com/scholar?hl=en&q=hockenmaier,+julia+and+mark+steedman.+2007.++id35bank:+a+corpus+of+id35+derivations+and+dependency+structures+extracted+from+the+penn+treebank.+computational+linguistics,+33(3):355   396.
 837. http://scholar.google.com/scholar?hl=en&q=huang,+liang,+suphan+fayong,+and+yang+guo.+2012.++structured+id88+with+inexact+search.+in+proceedings+of+the+2012+conference+of+the+north+american+chapter+of+the+association+for+computational+linguistics:+human+language+technologies,+pages+142   151,+montr  al.
 838. http://scholar.google.com/scholar?hl=en&q=huang,+liang+and+kenji+sagae.+2010.++dynamic+programming+for+linear-time+incremental+parsing.+in+proceedings+of+acl,+pages+1077   1086,+uppsala.
 839. http://scholar.google.com/scholar?hl=en&q=johansson,+richard+and+pierre+nugues.+2007.++extended+constituent-to-dependency+conversion+for+english.+in+16th+nordic+conference+of+computational+linguistics,+pages+105   112,+tartu.
 840. http://scholar.google.com/scholar?hl=en&q=kay,+martin.+1996.++chart+generation.+in+proceedings+of+acl,+pages+200   204,+santa+cruz,+ca.
 841. http://scholar.google.com/scholar?hl=en&q=koehn,+phillip.+2010.+statistical+machine+translation.+cambridge+university+press.
 842. http://scholar.google.com/scholar?hl=en&q=koehn,+philipp,+hieu+hoang,+alexandra+birch,+chris+callison-burch,+marcello+federico,+nicola+bertoldi,+brooke+cowan,+wade+shen,+christine+moran,+richard+zens,+chris+dyer,+ondrej+bojar,+alexandra+constantin,+and+evan+herbst.+2007.++moses:+open+source+toolkit+for+statistical+machine+translation.+in+proceedings+of+the+45th+annual+meeting+of+the+association+for+computational+linguistics+companion+volume+proceedings+of+the+demo+and+poster+sessions,+pages+177   180,+prague.
 843. https://www.mitpressjournals.org/servlet/linkout?suffix=r33&dbid=16&doi=10.1162/coli_a_00229&key=10.3115/1073445.1073462
 844. http://scholar.google.com/scholar?hl=en&q=koehn,+philipp,+franz+josef+och,+and+daniel+marcu.+2003.++statistical+phrase-based+translation.+in+proceedings+of+the+2003+conference+of+the+north+american+chapter+of+the+association+for+computational+linguistics+on+human+language+technology+-+volume+1,+naacl+'03,+pages+48   54,+edmonton,+canada.
 845. http://scholar.google.com/scholar?hl=en&q=koo,+terry+and+michael+collins.+2010.++efficient+third-order+dependency+parsers.+in+proceedings+of+acl,+pages+1   11,+uppsala.
 846. http://scholar.google.com/scholar?hl=en&q=lee,+j.+and+s.+seneff.+2006.++automatic+grammar+correction+for+second-language+learners.+in+proceedings+of+interspeech,+pages+1978   1981,+pittsburgh,+pa.
 847. http://scholar.google.com/scholar?hl=en&q=liu,+yang.+2013.++a+shift-reduce+parsing+algorithm+for+phrase-based+string-to-dependency+translation.+in+proceedings+of+the+51st+annual+meeting+of+the+association+for+computational+linguistics+(volume+1:+long+papers),+pages+1   10,+sofia.
 848. http://scholar.google.com/scholar?hl=en&q=marcus,+mitchell+p.,+beatrice+santorini,+and+mary+ann+marcinkiewicz.+1993.++building+a+large+annotated+corpus+of+english:+the+penn+treebank.+computational+linguistics,+19(2):313   330.
 849. http://scholar.google.com/scholar?hl=en&q=papineni,+kishore,+salim+roukos,+todd+ward,+and+wei-jing+zhu.+2002.++id7:+a+method+for+automatic+evaluation+of+machine+translation.+in+proceedings+of+the+40th+annual+meeting+of+the+association+for+computational+linguistics,+pages+311   318,+philadelphia,+pa.
 850. http://scholar.google.com/scholar?hl=en&q=ratnaparkhi,+adwait.+1996.++a+maximum+id178+model+for+part-of-speech+tagging.+in+proceedings+of+emnlp,+pages+133   142,+somerset,+nj.
 851. https://www.mitpressjournals.org/servlet/linkout?suffix=r40&dbid=16&doi=10.1162/coli_a_00229&key=10.1017/s1351324997001502
 852. http://scholar.google.com/scholar?hl=en&q=reiter,+ehud+and+robert+dale.+1997.++building+applied+natural+language+generation+systems.+natural+language+engineering,+3(1):57   87.
 853. http://scholar.google.com/scholar?hl=en&q=shen,+libin+and+aravind+joshi.+2008.++ltag+dependency+parsing+with+bidirectional+incremental+construction.+in+proceedings+of+the+2008+conference+on+empirical+methods+in+natural+language+processing,+pages+495   504,+honolulu.+hi.
 854. http://scholar.google.com/scholar?hl=en&q=shen,+libin,+giorgio+satta,+and+aravind+joshi.+2007.++guided+learning+for+bidirectional+sequence+classification.+in+proceedings+of+acl,+pages+760   767,+prague.
 855. http://scholar.google.com/scholar?hl=en&q=song,+linfeng,+yue+zhang,+kai+song,+and+qun+liu.+2014.++joint+morphological+generation+and+syntactic+linearization.+in+proceedings+of+the+twenty-eighth+aaai+conference,+quebec.
 856. http://scholar.google.com/scholar?hl=en&q=steedman,+mark.+2000.+the+syntactic+process.+the+mit+press,+cambridge,+ma.
 857. https://www.mitpressjournals.org/servlet/linkout?suffix=r45&dbid=16&doi=10.1162/coli_a_00229&key=10.3115/1596324.1596352
 858. http://scholar.google.com/scholar?hl=en&q=surdeanu,+mihai,+richard+johansson,+adam+meyers,+llu  s+m  rquez,+and+joakim+nivre.+2008.++the+conll-2008+shared+task+on+joint+parsing+of+syntactic+and+semantic+dependencies.+in+proceedings+of+the+twelfth+conference+on+computational+natural+language+learning,+pages+159   177,+manchester.
 859. http://scholar.google.com/scholar?hl=en&q=wan,+stephen,+mark+dras,+robert+dale,+and+c  cile+paris.+2009.++improving+grammaticality+in+statistical+sentence+generation:+introducing+a+dependency+spanning+tree+algorithm+with+an+argument+satisfaction+model.+in+proceedings+of+the+12th+conference+of+the+european+chapter+of+the+acl+(eacl+2009),+pages+852   860,+athens.
 860. http://scholar.google.com/scholar?hl=en&q=weir,+david.+1988.+characterizing+mildly+context-sensitive+grammar+formalisms.+ph.d.+thesis,+university+of+pennsylviania.
 861. http://scholar.google.com/scholar?hl=en&q=white,+michael.+2004.++reining+in+id35+chart+realization.+in+proceedings+of+iid86-04,+pages+182   191,+brockenhurst.
 862. https://www.mitpressjournals.org/servlet/linkout?suffix=r49&dbid=16&doi=10.1162/coli_a_00229&key=10.1007/s11168-006-9010-2
 863. http://scholar.google.com/scholar?hl=en&q=white,+michael.+2006.++efficient+realization+of+coordinate+structures+in+combinatory+categorial+grammar.+research+on+language+&+computation,+4(1):39   75.
 864. https://www.mitpressjournals.org/servlet/linkout?suffix=r50&dbid=16&doi=10.1162/coli_a_00229&key=10.3115/1699510.1699564
 865. http://scholar.google.com/scholar?hl=en&q=white,+michael+and+rajakrishnan+rajkumar.+2009.++id88+reranking+for+id35+realization.+in+proceedings+of+the+2009+conference+on+empirical+methods+in+natural+language+processing,+pages+410   419,+singapore.
 866. http://scholar.google.com/scholar?hl=en&q=wu,+dekai.+1997.++stochastic+inversion+transduction+grammars+and+bilingual+parsing+of+parallel+corpora.+computational+linguistics,+23(3):377   403.
 867. http://scholar.google.com/scholar?hl=en&q=xu,+peng,+ciprian+chelba,+and+frederick+jelinek.+2002.++a+study+on+richer+syntactic+dependencies+for+structured+language+modeling.+in+proceedings+of+the+40th+annual+meeting+of+the+association+for+computational+linguistics,+pages+191   198,+philadelphia,+pa.
 868. http://scholar.google.com/scholar?hl=en&q=zettlemoyer,+luke+s.+and+michael+collins.+2005.++learning+to+map+sentences+to+logical+form:+structured+classification+with+probabilistic+categorial+grammars.+in+proceedings+of+the+21st+conference+on+uncertainty+in+artificial+intelligence,+pages+658   666,+edinburgh.
 869. http://scholar.google.com/scholar?hl=en&q=zhang,+yue.+2013.++partial-tree+linearization:+generalized+word+ordering+for+text+synthesis.+in+proceedings+of+ijcai,+pages+2232   2238,+beijing.
 870. http://scholar.google.com/scholar?hl=en&q=zhang,+yue,+graeme+blackwood,+and+stephen+clark.+2012.++syntax-based+word+ordering+incorporating+a+large-scale+language+model.+in+proceedings+of+the+13th+conference+of+the+european+chapter+of+the+association+for+computational+linguistics,+pages+736   746,+avignon.
 871. http://scholar.google.com/scholar?hl=en&q=zhang,+yue+and+stephen+clark.+2008.++joint+word+segmentation+and+pos+tagging+using+a+single+id88.+in+proceedings+of+acl/hlt,+pages+888   896,+columbus,+oh.
 872. http://scholar.google.com/scholar?hl=en&q=zhang,+yue+and+stephen+clark.+2010.++a+fast+decoder+for+joint+word+segmentation+and+pos-tagging+using+a+single+discriminative+model.+in+proceedings+of+the+2010+conference+on+empirical+methods+in+natural+language+processing,+pages+843   852,+cambridge,+ma.
 873. https://www.mitpressjournals.org/doi/10.1162/coli_a_00037
 874. http://scholar.google.com/scholar?hl=en&q=zhang,+yue+and+stephen+clark.+2011.++syntactic+processing+using+the+generalized+id88+and+beam+search.+computational+linguistics,+37(1):105   151.
 875. http://scholar.google.com/scholar?hl=en&q=zhang,+yue+and+joakim+nivre.+2011.++transition-based+dependency+parsing+with+rich+non-local+features.+in+proceedings+of+the+49th+annual+meeting+of+the+association+for+computational+linguistics:+human+language+technologies,+pages+188   193,+portland,+or.
 876. https://www.mitpressjournals.org/cdn-cgi/l/email-protection#3b424e5e6441535a555c7b484e4f5f155e5f4e15485c
 877. https://www.mitpressjournals.org/cdn-cgi/l/email-protection#c3b0b7a6b3aba6adeda0afa2b1a883a0afeda0a2aeeda2a0edb6a8
 878. https://www.mitpressjournals.org/personalize/addfavoritepublication?doi=10.1162/coli_a_00229
 879. https://www.mitpressjournals.org/action/showcitformats?doi=10.1162/coli_a_00229
 880. https://www.mitpressjournals.org/action/showfeed?jc=coli&type=etoc&feed=rss
 881. https://www.mitpressjournals.org/action/showfeed?jc=coli&doi=10.1162/coli_a_00229&type=citrack&feed=rss
 882. https://www.mitpressjournals.org/journals/coli/sub
 883. https://giving.mit.edu/taxonomy/term/79#3920880
 884. https://www.mitpressjournals.org/
 885. http://www.mitpressjournals.org/action/showpublications
 886. http://mitpress.mit.edu/
 887. https://www.mitpressjournals.org/terms
 888. https://www.mitpressjournals.org/privacy
 889. https://www.mitpressjournals.org/contact_info
 890. https://www.facebook.com/mitpress
 891. http://www.twitter.com/mitpress
 892. https://plus.google.com/106848724929282487337?prsrc=3
 893. https://www.pinterest.com/mitpress/
 894. https://www.instagram.com/mitpress/
 895. https://www.youtube.com/channel/uceh0hmlpjgw2dn0ntmd0fcq
 896. https://www.atypon.com/
 897. https://www.crossref.org/
 898. https://www.projectcounter.org/
 899. https://www.mitpressjournals.org/help/main

   hidden links:
 901. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 902. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 903. https://www.mitpressjournals.org/action/oauth/linkedin?start=1&redirecturi=%2fdoi%2f10.1162%2fcoli_a_00229
 904. https://www.mitpressjournals.org/action/oauth/facebook?start=1&redirecturi=%2fdoi%2f10.1162%2fcoli_a_00229
 905. https://www.mitpressjournals.org/action/oauth/twitter?start=1&redirecturi=%2fdoi%2f10.1162%2fcoli_a_00229
 906. https://www.mitpressjournals.org/action/registration
 907. https://www.mitpressjournals.org/action/ssostart?redirecturi=%2fdoi%2f10.1162%2fcoli_a_00229
 908. https://www.mitpressjournals.org/doi/10.1162/coli_a_00229
 909. https://www.mitpressjournals.org/action/showlogin?uri=%2fdoi%2f10.1162%2fcoli_a_00229
 910. https://www.mitpressjournals.org/action/showcart?flowid=1
 911. javascript:popref('s4')
 912. javascript:popref('s2')
 913. javascript:popref('s3d')
 914. javascript:popref('s4')
 915. javascript:popref('s2')
 916. javascript:popref('s3d')
 917. javascript:popref('s4')
 918. https://www.mitpressjournals.org/action/addcitationalert?doi=10.1162/coli_a_00229&referrer=10.1162/coli_a_00229
