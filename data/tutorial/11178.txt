7
1
0
2

 
r
p
a
4
2

 

 
 
]
l
c
.
s
c
[
 
 

2
v
6
9
8
3
0

.

1
0
6
1
:
v
i
x
r
a

automatic description generation from images: a survey

automatic description generation from images: a survey

of models, datasets, and evaluation measures

ra   aella bernardi
university of trento, italy

ruket cakici
middle east technical university, turkey

desmond elliott
university of amsterdam, netherlands

aykut erdem
erkut erdem
nazli ikizler-cinbis
hacettepe university, turkey

frank keller
university of edinburgh, uk

adrian muscat
university of malta, malta

barbara plank
university of copenhagen, denmark

bernardi@disi.unitn.it

ruken@ceng.metu.edu.tr

d.elliott@uva.nl

aykut@cs.hacettepe.edu.tr

erkut@cs.hacettepe.edu.tr

nazli@cs.hacettepe.edu.tr

keller@inf.ed.ac.uk

adrian.muscat@um.edu.mt

bplank@cst.dk

abstract

automatic description generation from natural images is a challenging problem that
has recently received a large amount of interest from the id161 and natural lan-
guage processing communities. in this survey, we classify the existing approaches based
on how they conceptualize this problem, viz., models that cast description as either gen-
eration problem or as a retrieval problem over a visual or multimodal representational
space. we provide a detailed review of existing models, highlighting their advantages and
disadvantages. moreover, we give an overview of the benchmark image datasets and the
evaluation measures that have been developed to assess the quality of machine-generated
image descriptions. finally we extrapolate future directions in the area of automatic image
description generation.

1. introduction

over the past two decades, the    elds of natural language processing (nlp) and computer
vision (cv) have seen great advances in their respective goals of analyzing and generating
text, and of understanding images and videos. while both    elds share a similar set of meth-
ods rooted in arti   cial intelligence and machine learning, they have historically developed
separately, and their scienti   c communities have typically interacted very little.

recent years, however, have seen an upsurge of interest in problems that require a
combination of linguistic and visual information. a lot of everyday tasks are of this nature,
e.g., interpreting a photo in the context of a newspaper article, following instructions in
conjunction with a diagram or a map, understanding slides while listening to a lecture. in

1

bernardi et al.

addition to this, the web provides a vast amount of data that combines linguistic and visual
information: tagged photographs, illustrations in newspaper articles, videos with subtitles,
and multimodal feeds on social media. to tackle combined language and vision tasks and to
exploit the large amounts of multimodal data, the cv and nlp communities have moved
closer together, for example by organizing workshops on language and vision that have been
held regularly at both cv and nlp conferences over the past few years.

in this new language-vision community, automatic image description has emerged as a
key task. this task involves taking an image, analyzing its visual content, and generating
a textual description (typically a sentence) that verbalizes the most salient aspects of the
image. this is challenging from a cv point of view, as the description could in principle
talk about any visual aspect of the image: it can mention objects and their attributes, it
can talk about features of the scene (e.g., indoor/outdoor), or verbalize how the people
and objects in the scene interact. more challenging still, the description could even refer to
objects that are not depicted (e.g., it can talk about people waiting for a train, even when
the train is not visible because it has not arrived yet) and provide background knowledge
that cannot be derived directly from the image (e.g., the person depicted is the mona lisa).
in short, a good image description requires full image understanding, and therefore the
description task is an excellent test bed for id161 systems, one that is much more
comprehensive than standard cv evaluations that typically test, for instance, the accuracy
of object detectors or scene classi   ers over a limited set of classes.

image understanding is necessary, but not su   cient for producing a good description.
imagine we apply an array of state-of-the-art detectors to the image to localize objects
(e.g., felzenszwalb, girshick, mcallester, & ramanan, 2010; girshick, donahue, darrell,
& malik, 2014), determine attributes (e.g., lampert, nickisch, & harmeling, 2009; berg,
berg, & shih, 2010; parikh & grauman, 2011), compute scene properties (e.g., oliva &
torralba, 2001; lazebnik, schmid, & ponce, 2006), and recognize human-object interac-
tions (e.g., prest, schmid, & ferrari, 2012; yao & fei-fei, 2010). the result would be a
long, unstructured list of labels (detector outputs), which would be unusable as an image
description. a good image description, in contrast, has to be comprehensive but concise
(talk about all and only the important things in the image), and has to be formally correct,
i.e., consists of grammatically well-formed sentences.

from an nlp point of view, generating such a description is a natural language gener-
ation (id86) problem. the task of id86 is to turn a non-linguistic representation into
human-readable text. classically, the non-linguistic representation is a logical form, a
database query, or a set of numbers.
in image description, the input is an image rep-
resentation (e.g., the detector outputs listed in the previous paragraph), which the id86
model has to turn into sentences. generating text involves a series of steps, traditionally
referred to as the nlp pipeline (reiter & dale, 2006): we need to decide which aspects
of the input to talk about (content selection), then we need to organize the content (text
planning) and verbalize it (surface realization). surface realization in turn requires choos-
ing the right words (lexicalization), using pronouns if appropriate (referential expression
generation), and grouping related information together (aggregation).

in other words, automatic image description requires not only full image understanding,
but also sophisticated id86. this is what makes it such an interesting

2

automatic description generation from images: a survey

task that has been embraced by both the cv and the nlp communities.1 note that
the description task can become even more challenging when we take into account that
good descriptions are often user-speci   c. for instance, an art critic will require a di   erent
description than a librarian or a journalist, even for the same photograph. we will brie   y
touch upon this issue when we talk about the di   erence between descriptions and captions
in section 3 and discuss future directions in section 4.

given that automatic image description is such an interesting task, and it is driven by the
existence of mature cv and nlp methods and the availability of relevant datasets, a large
image description literature has appeared over the last    ve years. the aim of this survey
article is to give a comprehensive overview of this literature, covering models, datasets, and
id74.

we sort the existing literature into three categories based on the image description
models used. the    rst group of models follows the classical pipeline we outlined above: they
   rst detect or predict the image content in terms of objects, attributes, scene types, and
actions, based on a set of visual features. then, these models use this content information
to drive a id86 system that outputs an image description. we will
term these approaches direct generation models.

the second group of models cast the problem as a retrieval problem. that is, to create a
description for a novel image, these models search for images in a database that are similar to
the novel image. then they build a description for the novel image based on the descriptions
of the set of similar images that was retrieved. the novel image is described by simply
reusing the description of the most similar retrieved image (transfer), or by synthesizing a
novel description based on the description of a set of similar images. retrieval-based models
can be further subdivided based on what type of approach they use to represent images
and compute similarity. the    rst subgroup of models uses a visual space to retrieve images,
while the second subgroup uses a multimodal space that represents images and text jointly.
for an overview of the models that will be reviewed in this survey, and which category they
fall into, see table 1.

generating natural language descriptions from videos presents unique challenges over
and above image-based description, as it additionally requires analyzing the objects and
their attributes and actions in the temporal dimension. models that aim to solve descrip-
tion generation from videos have been proposed in the literature (e.g., khan, zhang, &
gotoh, 2011; guadarrama, krishnamoorthy, malkarnenkar, venugopalan, mooney, darrell,
& saenko, 2013; krishnamoorthy, malkarnenkar, mooney, saenko, & guadarrama, 2013;
rohrbach, qiu, titov, thater, pinkal, & schiele, 2013; thomason, venugopalan, guadar-
rama, saenko, & mooney, 2014; rohrbach, rohrback, tandon, & schiele, 2015; yao, torabi,
cho, ballas, pal, larochelle, & courville, 2015; zhu, kiros, zemel, salakhutdinov, urtasun,
torralba, & fidler, 2015). however, most existing work on description generation has used
static images, and this is what we will focus on in this survey.2

in this survey article, we    rst group automatic image description models into the three
categories outlined above and provide a comprehensive overview of the models in each

1. though some image description approaches circumvent the id86 aspect by transferring human-authored

descriptions, see sections 2.2 and 2.3.

2. an interesting intermediate approach involves the annotation of image streams with sequences of sen-

tences, see the work of park and kim (2015).

3

bernardi et al.

category in section 2. we then examine the available multimodal image datasets used for
training and testing description generation models in section 3. furthermore, we review
evaluation measures that have been used to gauge the quality of generated descriptions in
section 3. finally, in section 4, we discuss future research directions, including possible
new tasks related to image description, such as visual id53.

2. image description models

generating automatic descriptions from images requires an understanding of how humans
describe images. an image description can be analyzed in several di   erent dimensions (shat-
ford, 1986; jaimes & chang, 2000). we follow hodosh, young, and hockenmaier (2013)
and assume that the descriptions that are of interest for this survey article are the ones
that verbalize visual and conceptual information depicted in the image, i.e., descriptions
that refer to the depicted entities, their attributes and relations, and the actions they are
involved in. outside the scope of automatic image description are non-visual descriptions,
which give background information or refer to objects not depicted in the image (e.g., the
location at which the image was taken or who took the picture). also, not relevant for
standard approaches to image description are perceptual descriptions, which capture the
global low-level visual characteristics of images (e.g., the dominant color in the image or
the type of the media such as photograph, drawing, animation, etc.).

in the following subsections, we give a comprehensive overview of state-of-the-art ap-
proaches to description generation. table 1 o   ers a high-level summary of the    eld, using
the three categories of models outlined in the introduction: direct generation models, re-
trieval models from visual space, and retrieval model from multimodal space.

2.1 description as generation from visual input

the general approach of the studies in this group is to    rst predict the most likely meaning
of a given image by analyzing its visual content, and then generate a sentence re   ecting
this meaning. all models in this category achieve this using the following general pipeline
architecture:

1. id161 techniques are applied to classify the scene type, to detect the ob-
jects present in the image, to predict their attributes and the relationships that hold
between them, and to recognize the actions taking place.

2. this is followed by a generation phase that turns the detector outputs into words or
phrases. these are then combined to produce a natural language description of the
image, using techniques from id86 (e.g., templates, id165s,
grammar rules).

the approaches reviewed in this section perform an explicit mapping from images to
descriptions, which di   erentiates them from the studies described in section 2.2 and 2.3,
which incorporate implicit vision and language models. an illustration of a sample model is
shown in figure 1. an explicit pipeline architecture, while tailored to the problem at hand,
constrains the generated descriptions, as it relies on a prede   ned sets of semantic classes of
scenes, objects, attributes, and actions. moreover, such an architecture crucially assumes

4

automatic description generation from images: a survey

reference

generation

retrieval from

visual space multimodal space

x

x

x

x

x

x

farhadi et al. (2010)
kulkarni et al. (2011)
li et al. (2011)
ordonez et al. (2011)
yang et al. (2011)
gupta et al. (2012)
kuznetsova et al. (2012)
mitchell et al. (2012)
elliott and keller (2013)
hodosh et al. (2013)
gong et al. (2014)
karpathy et al. (2014)
kuznetsova et al. (2014)
mason and charniak (2014)
patterson et al. (2014)
socher et al. (2014)
verma and jawahar (2014)
yatskar et al. (2014)
chen and zitnick (2015)
donahue et al. (2015)
devlin et al. (2015)
elliott and de vries (2015)
fang et al. (2015)
jia et al. (2015)
x
karpathy and fei-fei (2015) x
kiros et al. (2015)
x
lebret et al. (2015)
lin et al. (2015)
mao et al. (2015a)
ortiz et al. (2015)
pinheiro et al. (2015)
ushiku et al. (2015)
vinyals et al. (2015)
xu et al. (2015)
yagcioglu et al. (2015)

x

x

x

x

x

x

x

x

x

x

x

x

x

x

x

x

x

x

x

x

x

x

x

x

x

x

x

x

x

x

x

x

x

x

x

table 1: an overview of existing approaches to automatic image description. we have
categorized the literature into approaches that directly generate a description of an image
(section 2.1), approaches that retrieve images via visual similarity and transfer their de-
scription to the new image (section 2.2), and approaches that frame the task as retrieving
descriptions and images from a multimodal space (section 2.3).

5

bernardi et al.

figure 1: the automatic image description generation system proposed by kulkarni et al.
(2011).

the accuracy of the detectors for each semantic class, an assumption that is not always met
in practice.

approaches to description generation di   er along two main dimensions: (a) which image
representations they derive descriptions from, and (b) how they address the sentence gen-
eration problem. in terms of the representations used, existing models have conceptualized
images in a number of di   erent ways, relying on spatial relationships (farhadi et al., 2010),
corpus-based relationships (yang et al., 2011), or spatial and visual attributes (kulkarni
et al., 2011). another group of papers utilizes an abstract image representation in the
form of meaning tuples which capture di   erent aspects of an image: the objects detected,
the attributes of those detections, the spatial relations between them, and the scene type
(farhadi et al., 2010; yang et al., 2011; kulkarni et al., 2011; li et al., 2011; mitchell et al.,
2012). more recently, yatskar et al. (2014) proposed to generate descriptions from densely-
labeled images, which incorporate object, attribute, action, and scene annotations. similar
in spirit is the work by fang et al. (2015), which does not rely on prior labeling of objects,
attributes, etc. rather, the authors train    word detectors    directly from images and their
associated descriptions using multi-instance learning (a weakly supervised approach for the
training of object detectors). the words returned by these detectors are then fed into a
language model for sentence generation, followed by a re-ranking step.

the    rst framework to explicitly represent how the structure of an image relates to
the structure of its description is the visual dependency representations (vdr) method
proposed by elliott and keller (2013). a vdr captures the spatial relations between the
objects in an image in the form of a dependency graph. this graph can then be related to the
syntactic dependency tree of the description of the image.3 while initial work using vdrs
has relied on a corpus of manually annotated vdrs for training, more recent approaches
induce vdrs automatically based on the output of an object detector (elliott & de vries,
2015) or the labels present in abstract scenes (ortiz et al., 2015).4 the idea of explicitly
representing image structure and using it for description generation has been picked up

3. vdrs have proven useful not only for description generation, but also for id162 (elliott,

lavrenko, & keller, 2014).

4. abstract scenes are schematic images, typically constructed using clip-art. they are employed to avoid
the need for an object detector, as the labels and positions of all objects are know. an example is zitnick
and parikh   s (2013) dataset, see section 3 for details.

6

automatic description generation from images: a survey

by lin et al. (2015), who parse images into scene graphs, which are similar to vdrs and
represent the relations between the objects in a scene. they then generate from scene
graphs using a semantic grammar.5

existing approaches also vary along the second dimension, viz., in how they approach
the sentence generation problem. at the one end of the scale, there are approaches that use
id165-based language models. examples include the works by kulkarni et al. (2011) and
li et al. (2011), which both generate descriptions using id165 language models trained on
a subset of wikipedia. these approaches    rst determine the attributes and relationships
between regions in an image as region   preposition   region triples. the id165 language
model is then used to compose an image description that is    uent, given the language model.
the approach of fang et al. (2015) is similar, but uses a maximum id178 language model
instead of an id165 model to generate descriptions. this gives the authors more    exibility
in handling the output of the word detectors that are at the core of their model.

recent image description work using recurrent neural networks (id56s) can also be
regarded as relying on id38. a classical id56 is a language model: it captures
the id203 of generating a given word in a string, given the words generated so far. in
an image description setup, the id56 is trained to generate the next word given not only
the string so far, but also a set of image features. in this setting, the id56 is therefore not
purely a language model (as in the case of an id165 model, for instance), but it is a hybrid
model that relies on a representation that incorporates both visual and linguistic features.
we will return to this in more detail in section 2.3.

a second set of approaches use sentence templates to generate descriptions. these
are (typically manually) pre-de   ned sentence frames in which open slots need to be    lled
with labels for objects, relations, or attributes. for instance, yang et al. (2011)    ll in
a sentence template by selecting the likely objects, verbs, prepositions, and scene types
based on a hidden markov model. verbs are generated by    nding the most likely pairing
of object labels in the gigaword external corpus. the generation model of elliott and
keller (2013) parses an image into a vdr, and then traverses the vdrs to    ll the slots
of sentence templates. this approach also performs a limited from of content selection by
learning associations between vdrs and syntactic dependency trees at training time; these
associations then allow to select the most appropriate verb for a description at test time.

other approaches have used more linguistically sophisticated approaches to generation.
mitchell et al. (2012) over-generate syntactically well-formed sentence fragments and then
recombine these using a tree-substitution grammar. a related approach has been pursued
by kuznetsova et al. (2014), where tree-fragments are learnt from a training set of existing
descriptions and then these fragments are combined at test time to form new descriptions.
another linguistically expressive model has recently been proposed by ortiz et al. (2015).
the authors model image description as machine translation over vdr   sentence pairs and
perform explicit content selection and surface realization using an integer linear program
over linguistic constraints.

the systems presented so far aimed at directly generating novel descriptions. however,
as argued by hodosh et al. (2013), framing image description as a natural language gener-
ation (id86) task makes it di   cult to objectively evaluate the quality of novel descriptions

5. note that graphs are also used for id162 by johnson, krishna, stark, li, shamma, bernstein,

and fei-fei (2015) and schuster, krishna, chang, fei-fei, and manning (2015).

7

bernardi et al.

figure 2: the description model based on retrieval from visual space proposed by ordonez
et al. (2011).

as it    introduces a number of linguistic di   culties that detract attention from the under-
lying image understanding problem    (hodosh et al., 2013). at the same time, evaluation
of generation systems is known to be di   cult (reiter & belz, 2009). hodosh et al. there-
fore propose an approach that makes it possible to evaluate the mapping between images
and sentences independently of the generation aspect. models that follow this approach
conceptualize image description as a retrieval problem: they associate an image with a
description by retrieving and ranking a set of similar images with candidate descriptions.
these candidate descriptions can then either be used directly (description transfer) or a
novel description can be synthesized from the candidates (description generation).

the retrieval of images and ranking of their descriptions can be carried out in two ways:
either from a visual space or from a multimodal space that combines textual and visual
information space. in the following subsections, we will survey work that follows these two
approaches.

2.2 description as a retrieval in visual space

the studies in this group pose the problem of automatically generating the description
of an image by retrieving images similar to the query image (i.e., the new image to be
described); this is illustrated in figure 2. in other words, these systems exploit similarity
in the visual space to transfer descriptions to the query images. compared to models that
generate descriptions directly (section 2.1), retrieval models typically require a large amount
of training data in order to provide relevant descriptions.

in terms of their algorithmic components, visual retrieval approaches typically follow a

pipeline of three main steps:

1. represent the given query image by speci   c visual features.

2. retrieve a candidate set of images from the training set based on a similarity measure

in the feature space used.

3. re-rank the descriptions of the candidate images by further making use of visual
and/or textual information contained in the retrieval set, or alternatively combine
fragments of the candidate descriptions according to certain rules or schemes.

one of the    rst model to follow this approach was the im2text model of ordonez et al.
(2011). gist (oliva & torralba, 2001) and tiny image (torralba, fergus, & freeman, 2008)

8

automatic description generation from images: a survey

descriptors are employed to represent the query image and to determine the visually similar
images in the    rst retrieval step. most of the retrieval-based models consider the result of
this step as a baseline. for the re-ranking step, a range of detectors (e.g., object, stu   ,
pedestrian, action detectors) and scene classi   ers speci   c to the entities mentioned in the
candidate descriptions are    rst applied to the images to better capture their visual content,
and the images are represented by means of these detector and classi   er responses. finally,
the re-ranking is carried out via a classi   er trained over these semantic features.

the model proposed by kuznetsova et al. (2012)    rst runs the detectors and the clas-
si   ers used in the re-ranking step of the im2text model on a query image to extract and
represent its semantic content. then, instead of performing a single retrieval by combining
the responses of these detectors and classi   ers as the im2text model does, it carries out a
separate id162 step for each visual entity present in the query image to collect re-
lated phrases from the retrieved descriptions. for instance, if a dog is detected in the given
image, then the retrieval process returns the phrases referring to visually similar dogs in the
training set. more speci   cally, this step is used to collect three di   erent kinds of phrases.
noun and verb phrases are extracted from descriptions in the training set based on the
visual similarity between object regions detected in the training images and in the query
image. similarly, prepositional phrases are collected for each stu    detection in the query
image by measuring the visual similarity between the detections in the query and training
images based on their appearance and geometric arrangements. prepositional phrases are
additionally collected for each scene context detection by measuring the global scene simi-
larity computed between the query and training images. finally, a description is generated
from these collected phrases for each detected object via integer id135 (ilp)
which considers factors such as word ordering, redundancy, etc.

the method of gupta et al. (2012) is another phrase-based approach. to retrieve
visually similar images, the authors employ simple rgb and hsv color histograms,
gabor and haar descriptors, gist and sift (lowe, 2004) descriptors as image fea-
tures. then, instead of using visual object detectors or scene classi   ers, they rely only
on the textual information in the descriptions of the visually similar images to extract
the visual content of the input image. speci   cally, the candidate descriptions are seg-
mented into phrases of a certain type such as (subject, verb), (subject, prep, object),
(verb, prep, object), (attribute, object), etc. those that best describe the input im-
age are determined according to a joint id203 model based on image similar-
ity and google search counts, and the image is represented by triplets of the form
{((attribute1, object1), verb), (verb, prep, (attribute2, object2)), (object1, prep, object2)}. in
the end, the description is generated using the three top-scoring triplets based on a    xed
template. to increase the quality of the descriptions, the authors also apply syntactic
aggregation and some subject and predicate grouping rules before the generation step.

patterson et al. (2014) were the    rst to present a large-scale scene attribute dataset
in the id161 community. the dataset includes 14,340 images from 707 scene
categories, which are annotated with certain attributes from a list of 102 discriminative
attributes related to materials, surface properties, lighting, a   ordances, and spatial layout.
this allows them to train attribute classi   ers from this dataset. in their paper, the authors
also demonstrate that the responses of these attribute classi   ers can be used as a global
image descriptor which captures the semantic content better than the standard global image

9

bernardi et al.

descriptors such as gist. as an application, they extended the baseline model of im2text
by replacing the global features with automatically extracted scene attributes, giving better
id162 and description results.

mason and charniak   s (2014) description generation approach di   ers from the models
discussed above in that it formulates description generation as an extractive summarization
problem, and it selects the output description by considering only the textual information
in the    nal re-ranking step. in particular, the authors represented images by using the scene
attributes descriptor of patterson et al. (2014). once the visually similar images are identi-
   ed from the training set, in the next step, the conditional probabilities of observing a word
in the description of the query image are estimated via non-parametric density estimation
using the descriptions of the retrieved images. the    nal output description is then deter-
mined by using two di   erent extractive summarization techniques, one depending on the
sumbasic model (nenkova & vanderwende, 2005) and the other based on kullback-leibler
divergence between the word distributions of the query and the candidate descriptions.

yagcioglu et al. (2015) proposed an average id183 approach which is based on
compositional distributed semantics. to represent images, they use features extracted from
the recently proposed visual geometry group convolutional neural network (vgg-id98;
chat   eld, simonyan, vedaldi, & zisserman, 2014). these features are the activations of
the last layer of a deep neural network trained on id163, which have been proven to
be e   ective in many id161 problems. then, the original query is expanded as
the average of the distributed representations of retrieved descriptions, weighted by their
similarity to the input image.

the approach of devlin et al. (2015) also utilizes id98 activations as the global image
descriptor and performs k-nearest neighbor retrieval to determine the images from the
training set that are visually similar to the query image.
it then selects a description
from the candidate descriptions associated with the retrieved images that best describes
the images that are similar to the query image, just like the approaches by mason and
charniak (2014) and yagcioglu et al. (2015). their approach di   ers in terms of how they
represent the similarity between description and how they select the best candidate over
the whole set. speci   cally, they propose to compute the description similarity based on
the id165 overlap f-score between the descriptions. they suggest to choose the output
description by    nding the description that corresponds to the description with the highest
mean id165 overlap with the other candidate descriptions (k-nearest neighbor centroid
description) estimated via an id165 similarity measure.

2.3 description as a retrieval in multimodal space

the third group of studies casts image description generation again as a retrieval problem,
but from a multimodal space (hodosh et al., 2013; socher et al., 2014; karpathy et al., 2014).
the intuition behind these models is illustrated in figure 3, and the overall approach can
be characterized as follows:

1. learn a common multimodal space for the visual and textual data using a training

set of image   description pairs.

10

automatic description generation from images: a survey

2. given a query, use the joint representation space to perform cross-modal (image   

sentence) retrieval.

3:
(2013);

image descriptions

figure
et al.
http://nlp.cs.illinois.edu/hockenmaiergroup/framingi image description/)

(2014); karpathy et al.

socher et al.

as proposed in hodosh
(2014).
(image source

retrieval

task

as

a

in contrast to the retrieval models that work on a visual space (section 2.2), where
unimodal id162 is followed by ranking of the retrieved descriptions, here image
and sentence features are projected into a common multimodal space. then, the multimodal
space is used to retrieve descriptions for a given image. the advantage of this approach is
that it allows bi-directional models, i.e., the common space can also be used for the other
direction, retrieving the most appropriate image for a query sentence.

in this section, we    rst discuss the seminal paper of hodosh et al. (2013) on description
retrieval, and then present more recent approaches that combine a retrieval approach with
some form of id86. hodosh et al. (2013) map both images and
sentences into a common space. the joint space can be used for both image search (   nd
the most plausible image given a sentence) and image annotation (   nd the sentence that
describes the image well), see figure 3. in an earlier study the authors proposed to learn
a common meaning space (farhadi et al., 2010) consisting of a triple representation of the
form hobject, action, scenei. the representation was thus limited to a set of pre-de   ned
discrete slot    llers, which was given as training information. instead, hodosh et al. (2013)
use kcca, a kernelized version of cca, canonical correlation analysis (hotelling, 1936),
to learn the joint space. cca takes a training dataset of image-sentence pairs, i.e., dtrain =
{hi, si}, thus input from two di   erent feature spaces, and    nds linear projections into a newly
induced common space. in kcca, id81s map the original items into higher-order
space in order to capture the patterns needed to associate image and text. kcca has been
shown previously to be successful in associating images (hardoon, szedmak, & shawe-
taylor, 2004) or image regions (socher & fei-fei, 2010) with individual words or set of
tags.

hodosh et al. (2013) compare their kcca approach to a nearest-neighbor (nn) baseline
that uses unimodal text and image spaces, without constructing a joint space. a drawback of
kcca is that it is only applicable to smaller datasets, as it requires the two kernel matrices
to be kept in memory during training. this becomes prohibitive for very large datasets.

11

bernardi et al.

some attempts have been made to circumvent the computational burden of kcca, e.g.,
by resorting to linear models (hodosh & hockenmaier, 2013). however, recent work on
description retrieval has instead utilized neural networks to construct a joint space for
image description generation.

socher et al. (2014) use neural networks for building sentence and image vector rep-
resentations that are then mapped into a common embedding space. a novelty of their
work is that they use compositional sentence vector representations. first, image and word
representations are learned in their single modalities, and    nally mapped into a common
multimodal space. in particular, they use a dt-id56 (dependency tree recursive neural
network) for composing language vectors to abstract over word order and syntactic di   er-
ence that are semantically irrelevant. this results in 50-dimensional id27s. for
the image space, the authors use a nine layer neural network trained on id163 data,
using unsupervised pre-training.
image embeddings are derived by taking the output of
the last layer (4,096 dimensions). the two spaces are then projected into a multi-modal
space through a max-margin objective function that intuitively trains pairs of correct im-
age and sentence vectors to have a high inner product. the authors show that their model
outperforms previously used kcca approaches such as hodosh and hockenmaier (2013).
karpathy et al. (2014) extend the previous multi-modal embeddings model. rather
than directly mapping entire images and sentences into a common embedding space, their
model embeds more    ne-grained units, i.e., fragments of images (objects) and sentences
(dependency tree fragments), into a common space. their    nal model integrates both global
(sentence and image-level) as well as    ner-grained information and outperforms previous
approaches, such as dt-id56 (socher et al., 2014). a similar approach is pursued by
pinheiro et al. (2015), who propose a bilinear phrase-based model that learns a mapping
between image representations and sentences. a constrained language model is then used
to generate from this representation. a conceptually related approach is pursued by ushiku
et al. (2015): the authors use a common subspace model which maps all feature vectors
associated with the same phrase into nearby regions of the space. for generation, a beam-
search based decoder or templates are used.

description generation systems are di   cult to evaluate, therefore the studies reviewed
above treat the problem as a retrieval and ranking task (hodosh et al., 2013; socher et al.,
2014). while such an approach has been valuable because it enables comparative evaluation,
retrieval and ranking is limited by the availability of existing datasets with descriptions. to
alleviate this problem, recent models have been developed that are extensions of multimodal
spaces; they are able to not only rank sentences, but can also generate them (chen & zitnick,
2015; donahue et al., 2015; karpathy & fei-fei, 2015; kiros et al., 2015; lebret et al., 2015;
mao et al., 2015a; vinyals et al., 2015; xu et al., 2015).

kiros et al. (2015) introduced a general encoder-decoder framework for image description
ranking and generation, illustrated in figure 4. intuitively the method works as follows.
the encoder    rst constructs a joint multimodal space. this space can be used to rank
images and descriptions. the second stage (decoder) then uses the shared multimodal
representation to generate novel descriptions. their model, directly inspired by recent
work in machine translation, encodes sentences using a long   short term memory (lstm)
recurrent neural network, and image features using a deep convolutional network (id98).
lstm is an extension of the recurrent neural network (id56) that incorporates built-

12

automatic description generation from images: a survey

figure 4: the encoder-decoder model proposed by kiros et al. (2015).

in memory to store information and exploit long range context.
in kiros et al.   s (2015)
encoder-decoder model, the vision space is projected into the embedding space of the lstm
hidden states; a pairwise ranking loss is minimized to learn the ranking of images and their
descriptions. the decoder, a neural-network-based language model, is able to generate novel
descriptions from this multimodal space.

work that has been carried out at the same time and is similar to the latter is described
in the paper by donahue et al. (2015). the authors propose a model that is also based
on the lstm neural architecture. however, rather than projecting the vision space into
the embedding space of the hidden states, the model takes a copy of the static image and
the previous word directly as input, that is then fed to a stack of four lstms. another
lstm-based model is proposed by jia et al. (2015), who added semantic image information
as additional input to the lstm. the model by kiros et al. (2015) outperforms the prior
dt-id56 model (socher et al., 2014);
in turn, donahue et al. (2015) report that they
outperform kiros et al. (2015) on the task of image description retrieval. subsequent work
includes the id56-based architectures by mao et al. (2015a) and vinyals et al. (2015),
who are very similar to the one proposed by kiros et al. (2015) and achieve comparable
results on standard datasets. mao, wei, yang, wang, huang, and yuille (2015b) propose an
interesting extension of mao et al.   s (2015a) model for the learning of novel visual concepts.
karpathy and fei-fei (2015) improve on previous models by proposing a deep visual-
semantic alignment model with a simpler architecture and objective function. their key
insight is to assume that parts of the sentence refer to particular but unknown regions in the
image. their model tries to infer the alignments between segments of sentences and regions
of images and is based on convolutional neural networks over image regions, bidirectional
id56 over sentences and a structured objective that aligns the two modalities. words
and image regions are mapped into a common multimodal embedding. the multimodal
recurrent neural network architecture uses the inferred alignments to learn and generate
novel descriptions. here, the image is used as condition for the    rst state in the recurrent
neural network, which then generates image descriptions.

another model that can generate novel sentences is proposed in chen and zitnick (2015).
in contrast to the previous work, their model dynamically builds a visual representation of
the scene as a description is being generated. that is, a word is read or generated and the

13

bernardi et al.

visual representation is updated to re   ect the new information. they accomplish this with
a simple id56. the model achieves comparable or better results than most prior studies,
except for the recently proposed deep visual-semantic alignment model (karpathy & fei-fei,
2015). the model of xu et al. (2015) is closely related in that it also uses an id56-based
architecture in which the visual representations are dynamically updated. xu et al.   s (2015)
model incorporates an attentional component, which gives it a way of determining which
regions in an image are salient, and it can focus its description on those regions. while
resulting in an improvement in description accuracy, it also makes it possible to analyze
model behavior by visualizing the regions that were attended to during each word that was
generated by the model.

the general id56-based ranking and generation approach is also followed by lebret
et al. (2015). here, the main innovation is on the linguistic side: they employ a bilinear
model to learn a common space of image features and syntactic phrases (noun phrases, verb
phrases, and prepositional phrases). a markov model is then utilized to generate sentences
from these phrase embedding. on the visual side, standard id98-based features are used.
this results in an elegant modeling framework, whose performance is broadly comparable
to the state of the art.

finally, two important directions that are less explored are: portability and weakly su-
pervised learning. verma and jawahar (2014) evaluate the portability of a bi-directional
model based on topic models, showing that performance signi   cantly degrades. they high-
light the importance of cross-dataset image description retrieval evaluation. another inter-
esting observation is that all of the above models require a training set of fully-annotated
image-sentence pairs. however, obtaining such data in large quantities is prohibitively ex-
pensive. gong et al. (2014) propose an approach based on weak supervision that transfers
knowledge from millions of weakly annotated images to improve the accuracy of description
retrieval.

2.4 comparison of existing approaches

the discussion in the previous subsections makes it clear that each approach to image
description has its particular strengths and weaknesses. for example, the methods that
cast the task as a generation problem (section 2.1) have an advantage over other types of
approaches in that they can produce novel sentences to describe a given image. however,
their success relies heavily on how accurately they estimate the visual content and how
well they are able to verbalize this content. in particular, they explicitly employ computer
vision techniques to predict the most likely meaning of a given image; these methods have
limited accuracy in practice, hence if they fail to identify the most important objects and
their attributes, then no valid description can be generated. another di   culty lies in the
   nal description generation step; sophisticated id86 is crucial to
guarantee    uency and grammatical correctness of the generated sentences. this can come
at the price of considerable algorithmic complexity.

in contrast, image description methods that cast the problem as a retrieval from a
visual space problem and transfer the retrieved descriptions to a novel image (section 2.2)
always produce grammatically correct descriptions. this is guaranteed by design, as these
systems fetch human-generated sentences from visually similar images. the main issue with

14

automatic description generation from images: a survey

this approach is that it requires large amounts of images with human-written descriptions.
that is, the accuracy (but not the grammaticality) of the descriptions reduces as the size
of the training set decreases. the training set also needs to be diverse (in addition to being
large), in order for visual retrieval-based approaches to produce image descriptions that are
adequate for novel test images (devlin et al., 2015). though this problem can be mitigated
by re-synthesizing a novel description from the retrieved ones (see section 2.2).

approaches that cast image description as a retrieval from a multimodal space problem
(section 2.3) also have the advantage of generating human-like descriptions as they are
able to retrieve the most appropriate ones from a pre-de   ned large pool of descriptions.
however, ranking these descriptions requires a cross-modal similarity metric that compares
images and sentences. such metrics are di   cult to de   ne, compared to the unimodal
image-to-image similarity metrics used by retrieval models that work on a visual space.
additionally, training a common space for images and sentences requires a large training
set of images annotated with human-generated descriptions. on the plus side, such a
multimodal embedding space can also be used for the reverse problem, i.e., for retrieving
the most appropriate image for a query sentence. this is something generation-based or
visual retrieval-based approaches are not capable of.

3. datasets and evaluation

there is a wide range of datasets for automatic image description research. the images in
these datasets are associated with textual descriptions and di   er from each other in certain
aspects such as in size, the format of the descriptions and in how the descriptions were col-
lected. here we review common approaches for collecting datasets, the datasets themselves,
and evaluation measures for comparing generated descriptions with ground-truth texts. the
datasets are summarized in table 2, and examples of images and descriptions are given in
figure 5. the readers can also refer to the dataset survey by ferraro, mostafazadeh, huang,
vanderwende, devlin, galley, and mitchell (2015) for an analysis similar to ours. it provides
a basic comparison of some of the existing language and vision datasets. it is not limited
to automatic image description, and it reports some simple statistics and quality metrics
such as perplexity, syntactic complexity, and abstract to concrete word ratios.

3.1 image-description datasets

the pascal1k sentence dataset (rashtchian et al., 2010) is a dataset which is commonly
used as a benchmark for evaluating the quality of description generation systems. this
medium-scale dataset, consists of 1,000 images that were selected from the pascal 2008
object recognition dataset (everingham, van gool, williams, winn, & zisserman, 2010)
and includes objects from di   erent visual classes, such as humans, animals, and vehicles.
each image is associated with    ve descriptions generated by humans on amazon mechanical
turk (amt) service.

the visual and linguistic treebank (vlt2k; elliott & keller, 2013) makes use of images
from the pascal 2010 action recognition dataset. it augments these images with three, two-
sentence descriptions per image. these descriptions were collected on amt with speci   c

7. kuznetsova et al. (2014) ran a human judgments study on 1,000 images from this dataset.

15

bernardi et al.

images

texts

judgments

objects

pascal1k (rashtchian et al., 2010)
vlt2k (elliott & keller, 2013)
flickr8k (hodosh & hockenmaier, 2013)
flickr30k (young et al., 2014)
abstract scenes (zitnick & parikh, 2013)
iapr-tc12 (grubinger et al., 2006)
ms coco (lin et al., 2014)

bbc news (feng & lapata, 2008)
sbu1m captions (ordonez et al., 2011)
d  ej`a-image captions (chen et al., 2015)

1,000
2,424
8,108
31,783
10,000
20,000
164,062

3,361

5
3
5
5
6
1   5
5

1
1

no

partial

yes
no
no
no
soon

no

possibly6

no

partial
partial

no
no

complete
segmented

partial

no
no
no

1,000,000
4,000,000 varies

table 2: image datasets for the automatic description generation models. we have split
the overview into image description datasets (top) and caption datasets (bottom)     see the
main text for an explanation of this distinction.

instructions to verbalize the main action depicted in the image and the actors involved (   rst
sentence), while also mentioning the most important background objects (second sentence).
for a subset of 341 images of the visual and linguistic treebank, object annotation is
available (in the form of polygons around all objects mentioned in the descriptions). for
this subset, manually created visual dependency representations (see section 2.1) are also
included (three vdrs per images, i.e., a total of 1023).

the flickr8k dataset (hodosh et al., 2013) and its extended version flickr30k
dataset (young et al., 2014) contain images from flickr, comprising approximately 8,000
and 30,000 images, respectively. the images in these two datasets were selected through
user queries for speci   c objects and actions. these datasets contain    ve descriptions per im-
age which were collected from amt workers using a strategy similar to that of the pascal1k
dataset.

the abstract scenes dataset (zitnick & parikh, 2013; zitnick, parikh, & vanderwende,
2013) consists of 10,000 clip-art images and their descriptions. the images were created
through amt, where workers were asked to place a    xed vocabulary of 80 clip-art objects
into a scene of their choosing. the descriptions were then sourced for these worker-created
scenes. the authors provided these descriptions in two di   erent forms. while the    rst
group contains a single sentence description for each image, the second group includes two
alternative descriptions per image. each of these two descriptions consist of three simple
sentences with each sentence describing a di   erent aspect of the scene. the main advantage
of this dataset is it a   ords the opportunity to explore image description generation without
the need for automatic object recognition, thus avoiding the associated noise. a more
recent version of this dataset has been created as a part of the visual question-answering
(vqa) dataset (antol, agrawal, lu, mitchell, batra, zitnick, & parikh, 2015). it contains
50,000 di   erent scene images with more realistic human models and with    ve single-sentence
descriptions.

16

automatic description generation from images: a survey

1. one jet lands at an airport while another takes o   

1. there are several people in chairs and a small child

next to it.

2. two airplanes parked in an airport.
3. two jets taxi past each other.
4. two parked jet airplanes facing opposite direc-

tions.

5. two passenger planes on a grassy plain

watching one of them play a trumpet

2. a man is playing a trumpet in front of a little boy.
3. people sitting on a sofa with a man playing an

instrument for entertainment.

(a) pascal1k7

(b) vlt2k8

1. a man is snowboarding over a structure on a snowy

1. a yellow building with white columns in the back-

hill.

ground

2. a snowboarder jumps through the air on a snowy

hill.

3. a snowboarder wearing green pants doing a trick

on a high bench

4. someone in yellow pants is on a ramp over the

snow.

5. the man is performing a trick on a snowboard high

in the air.

2. two palm trees in front of the house
3. cars are parking in front of the house
4. a woman and a child are walking over the square

(c) flickr8k9

(d) iapr-tc1210

1. a cat anxiously sits in the park and stares at an
unattended hot dog that someone has left on a
yellow bench

1. a blue smart car parked in a parking lot.
2. some vehicles on a very wet wide city street.
3. several cars and a motorcycle are on a snow cov-

ered street.

4. many vehicles drive down an icy street.
5. a small smart car driving in the city.

(e) abstract scenes11

(f) ms coco12

figure 5: example images and descriptions from the benchmark image datasets.

17

bernardi et al.

the iapr-tc12 dataset introduced by grubinger et al. (2006) is one of the earliest
multi-modal datasets and contains 20,000 images with descriptions. the images were orig-
inally retrieved via search engines such as google, bing and yahoo, and the descriptions
were produced in multiple languages (predominantly english and german). each image is
associated with one to    ve descriptions, where each description refers to a di   erent aspect
of the image, where applicable. the dataset also contains complete pixel-level segmentation
of the objects.

the ms coco dataset (lin et al., 2014) currently consists of 123,287 images with    ve
di   erent descriptions per image. images in this dataset are annotated for 80 object cat-
egories, which means that bounding boxes around all instances in one of these categories
are available for all images. the ms coco dataset has been widely used for image de-
scription, something that is facilitated by the standard evaluation server that has recently
become available13. extensions of ms coco are currently under development, including
the addition of questions and answers (antol et al., 2015).

one paper (lin et al., 2015) uses an the nyu dataset (silberman, kohli, hoiem, &
fergus, 2012), which contains 1,449 indoor scenes with 3d object segmentation. this
dataset has been augmented with    ve descriptions per image by lin et al.

3.2 image-caption datasets

image descriptions verbalize what can be seen in the image, i.e., they refer to the objects,
actions, and attributes depicted, mention the scene type, etc. captions, on the other hand,
are typically texts associated with images that verbalize information that cannot be seen
in the image. a caption provides personal, cultural, or historical context for the image
(panofsky, 1939). images shared through social networking or photo-sharing websites can
be accompanied by descriptions or captions, or a mixtures of both types of text. the images
in a newspaper or a museum will typically contain cultural or historical texts, i.e., captions
not descriptions.

the bbc news dataset (feng & lapata, 2008) was one of the earliest collections of
images and co-occurring texts. feng and lapata (2008) harvested 3,361 news articles from
the british broadcasting corporation news website, with the constraint that the article
includes an image and a caption.

the sbu1m captions dataset introduced by ordonez et al. (2011) di   ers from the
previous datasets in that it is a web-scale dataset containing approximately one million
captioned images. it is compiled from data available on flickr with user-provided image
descriptions. the images were downloaded and    ltered from flickr with the constraint that
an image contained at least one noun and one verb on prede   ned control lists. the resulting
dataset is provided as a csv    le of urls.

8. source http://nlp.cs.illinois.edu/hockenmaiergroup/pascal-sentences/index.html
9. source http://github.com/elliottd/vlt

10. source https://illinois.edu/fb/sec/1713398
11. source http://imageclef.org/photodata
12. source http://research.microsoft.com/en-us/um/people/larryz/clipart/semanticclassesrender

/classes_v1.html

13. source http://mscoco.org/explore
13. source http://mscoco.org/dataset/#captions-eval

18

automatic description generation from images: a survey

the d  ej`a-image captions dataset (chen et al., 2015) contains 4,000,000 images with
180,000 near-identical captions harvested from flickr. 760 million images were downloaded
from flickr during the calendar year 2013 using a set of 693 nouns as queries. the image
captions are normalized through lemmatization and stop word removal to create a corpus
of the near-identical texts. for instance, the sentences the bird    ies in blue sky and a bird
   ying into the blue sky were normalized to bird    y in blue sky (chen et al., 2015). image   
caption pairs are retained if the captions are repeated by more than one user in normalized
form.

3.3 collecting datasets

collecting new image   text datasets is typically performed through crowd-sourcing or har-
vesting data from the web. the images for these datasets have either been sourced from
an existing task in the id161 community     the pascal challenge (everingham
et al., 2010) was used to the pascal1k and vlt2k datasets     directly from flickr, in the
case of flickr8k/30k, ms coco, sbu1m captions, and d  ej`a-image captions datasets, or
crowdsourced, in the case of the abstract scenes dataset. the texts in image   description
datasets are usually crowd-sourced from amazon mechanical turk or crowd   ower; whereas
the texts in image   caption datasets have been harvested from photo-sharing sites, such as
flickr, or from news providers. captions are usually collected without    nancial incentive
because they are written by the people sharing their own images, or by journalists.

crowd-sourcing the descriptions of images involves de   ning a simple task that can be
performed by untrained workers. examples of the task guidelines used by hodosh et al.
(2013) and elliott and keller (2013) are given in figure 6. in both instances, care was taken
to clearly inform the potential workers about the expectations for the task. in particular,
explicit instructions were given on how the descriptions should be written, and examples of
good texts were provided. in addition, hodosh et al. provided more extensive examples to
explain what would constitute unsatisfactory texts. further options are available to control
the quality of the collected texts: a minimum performance rate for workers is a common
choice; and a pre-task selection quiz may be used to determine whether workers have a
su   cient grasp of the english language (hodosh et al., 2013).

the issue of remuneration for crowd-sourced workers is controversial, and higher pay-
ments do not always lead to better quality in a crowd-sourced environment (mason & watts,
2009). rashtchian et al. (2010) paid $0.01/description, elliott and keller (2013) paid $0.04
for an average of 67 seconds of work to produce a two-sentence description. to the best of
our knowledge, such information is not available for the other datasets.

3.4 evaluation measures

evaluating the output of a id86 (id86) system is a fundamentally
di   cult task (dale & white, 2007; reiter & belz, 2009). the most common way to assess
the quality of automatically generated texts is the subjective evaluation by human experts.
id86-produced text is typically judged in terms of grammar and content, indicating how
syntactically correct and how relevant the text is, respectively. fluency of the generated

15. source appendix of the work by hodosh et al. (2013)

19

bernardi et al.

(a) mechanical turk interface used to collect flickr8k dataset14.

(b) mechanical turk interface used to collect vlt2k dataset.

figure 6: examples of mechanical turk interfaces for collecting descriptions.

text is sometimes tested as well, especially when a surface realization technique is involved
during the generation process. automatically generated descriptions for images can be
evaluated using the same id86 techniques. typically, judges are provided with the image
as well as with the description during evaluation tasks. subjective human evaluations of
machine generated image descriptions are often performed on mechanical turk with the help

20

automatic description generation from images: a survey

of questions. so far, the following likert-scale questions have been used to test datasets
and user groups of various sizes.

    the description accurately describes the image (kulkarni et al., 2011; li et al., 2011;
mitchell et al., 2012; kuznetsova et al., 2012; elliott & keller, 2013; hodosh et al.,
2013).

    the description is grammatically correct (yang et al., 2011; mitchell et al., 2012;

kuznetsova et al., 2012; elliott & keller, 2013, inter alia).

    the description has no incorrect information (mitchell et al., 2012).

    the description is relevant for this image (li et al., 2011; yang et al., 2011).

    the description is creatively constructed (li et al., 2011).

    the description is human-like (mitchell et al., 2012).

another approach for evaluating descriptions is to use automatic measures, such as
id7 (papineni, roukos, ward, & zhu, 2002), id8 (lin & hovy, 2008), translation
error rate (feng & lapata, 2013), meteor (denkowski & lavie, 2014), or cider (vedan-
tam, lawrence zitnick, & parikh, 2015). these measures were originally developed to eval-
uate the output of machine translation engines or text summarization systems, with the
exception of cider, which was developed speci   cally for image description evaluation. all
these measures compute a score that indicates the similarity between the system output and
one or more human-written reference texts (e.g., ground truth translations or summaries).
this approach to evaluation has been subject to much discussion and critique (kulkarni
et al., 2011; hodosh et al., 2013; elliott & keller, 2014). kulkarni et al.
found weakly
negative or no correlation between human judgments and unigram id7 on the pascal 1k
dataset (pearson   s    = -0.17 and 0.05). hodosh et al. studied the cohen   s    correlation of
expert human judgments and binarized unigram id7 and unigram id8 of retrieved
descriptions on the flickr8k dataset. they found the best agreement between humans and
id7 (   = 0.72) or id8 (   = 0.54) when the system retrieved the sentences origi-
nally associated with the images. agreement dropped when only one reference sentence
was available, or when the reference sentences were disjoint from the proposal sentences.
they concluded that neither measure was appropriate for image description evaluation and
subsequently proposed image   sentence ranking experiments, discussed in more detail be-
low. elliott and keller analyzed the correlation between human judgments and automatic
evaluation measures for retrieved and system-generated image descriptions in the flickr8k
and vlt2k datasets. they showed that sentence-level unigram id7, which at that point
in time was the de facto standard measure for image description evaluation, is only weakly
correlated with human judgments. meteor (banerjee & lavie, 2005), a less frequently used
translation evaluation measure, exhibited the highest correlation with human judgments.
however, kuznetsova et al. (2014) found that unigram id7 was more strongly correlated
with human judgments than meteor for image id134.

the    rst large-scale image description evaluation took place during the ms coco
captions challenge 2015,15 featuring 15 teams with a dataset of 123,716 training images

15. source http://mscoco.org/dataset/cap2015

21

bernardi et al.

reference

approach

datasets

farhadi et al. (2010)
kulkarni et al. (2011)
li et al. (2011)
ordonez et al. (2011)
yang et al. (2011)

gupta et al. (2012)
kuznetsova et al. (2012)
mitchell et al. (2012)
elliott and keller (2013)
hodosh et al. (2013)

multretrieval pascal1k
generation
pascal1k
pascal1k
generation
sbu1m
visretrieval
iapr,
generation
coco
pascal1k, iapr
sbu1m
pascal1k
vlt2k

visretrieval
visretrieval
generation
generation
multretrieval pascal1k, flickr8k

flickr8k/30k,

multretrieval sbu1m, flickr30k
multretrieval flickr8k/30k, coco
generation

gong et al. (2014)
karpathy et al. (2014)
kuznetsova et al. (2014)
mason and charniak (2014) visretrieval
visretrieval
patterson et al. (2014)
socher et al. (2014)
multretrieval pascal1k
verma and jawahar (2014) multretrieval
yatskar et al. (2014)
chen and zitnick (2015)

sbu1m
sbu1m
sbu1m

generation
multretrieval flickr8k/30k, coco

donahue et al. (2015)

multretrieval flickr30k, coco

devlin et al. (2015)
elliott and de vries (2015) generation
fang et al. (2015)
generation

visretrieval

coco
vlt2k, pascal1k
coco

jia et al. (2015)
flickr8k/30k, coco
karpathy and fei-fei (2015) multretrieval flickr8k/30k, coco

generation

iapr, sbu1m, pascal1k id7, id8, p@k
own data

measures

id7
human, id7
human, id7

id7, id8, meteor,
cider, r@k
human, id7, id8
human, id7
human
human, id7
human, id7, id8,
mrank, r@k
r@k
id7, meteor, cider
human, id7, meteor
human, id7
id7
mrank, r@k

human, id7
id7, meteor, cider,
mrank, r@k
human, id7, mrank,
r@k
id7, meteor
id7, meteor
human, id7, id8,
meteor, cider
id7, meteor
id7, meteor, cider,
mrank, r@k
r@k
id7, r@k
id8

human, id7, meteor
id7
id7

kiros et al. (2015)
lebret et al. (2015)
lin et al. (2015)
mao et al. (2015a)
ortiz et al. (2015)
pinheiro et al. (2015)
ushiku et al. (2015)

multretrieval flickr8k/30k
multretrieval flickr30k, coco
generation
multretrieval
generation
multretrieval coco
generation

nyu
iapr, flickr30k, coco id7, mrank, r@k
abstract scenes

pascal1k, iapr, sbu1m,
coco

vinyals et al. (2015)

multretrieval pascal1k,

sbu1m,

xu et al. (2015)
yagcioglu et al. (2015)

multretrieval flickr8k/30k, coco
visretrieval
flickr8k/30k, coco

flickr8k/30k

id7, meteor, cider,
mrank, r@k
id7, meteor
human, id7, meteor,
cider

table 3: an overview of the approaches, datasets, and evaluation measures reviewed in this
survey and organised in chronological order.

22

automatic description generation from images: a survey

and 41,000 images in a withheld test dataset. the number of reference texts for each testing
image was either    ve or 40, based on the insight that some measures may bene   t from
larger reference sets (vedantam et al., 2015). when automatic evaluation measures were
used, some of the image description systems outperformed a human   human upper bound,16
whether    ve or 40 reference descriptions were provided. however, none of the systems
outperformed human   human evaluation when a judgment elicitation task was used. meteor
was found to be the most robust measure, with the systems beating the human text on one
and two submissions (depending on the number of references); the systems outperformed
humans seven or    ve times measured with cider; according to id8 and id7, the
system nearly always outperformed the humans, further con   rming the unsuitability of
these evaluation measures.

the models that approach the description generation problem from a cross-modal re-
trieval perspective (hodosh & hockenmaier, 2013; hodosh et al., 2013; socher et al., 2014;
gong et al., 2014; karpathy et al., 2014; verma & jawahar, 2014) are also able to use mea-
sures from information retrieval, such as median rank (mrank), precision at k (s@k), or
recall at k (r@k) to evaluate the descriptions they return, in addition to the text-similarity
measures reported above. this evaluation paradigm was    rst proposed by hodosh et al.,
who reported high correlation with human judgments for image   sentence based ranking
evaluations.

in table 3, we summarize all the image description approaches discussed in this survey,
and list the datasets and evaluation measures employed by each of these approaches. it
can be seen that more recent systems (starting in 2014) have converged on the use of
large description datasets (flickr8k/30k, ms coco) and employ evaluation measures that
perform well in terms of correlation with human judgments (meteor, cider). however, the
use of id7, despite its limitations, is still widespread; also the use of human evaluation
is by no means universal in the literature.

4. future directions

as this survey demonstrates, the cv and nlp communities have witnessed an upsurge in
interest in automatic image description systems. with the help of recent advances in deep
learning models for images and text, substantial improvements in the quality of automat-
ically generated descriptions has been registered. nevertheless, a series of challenges for
image description research remain. in the following, we discuss future directions that this
line of research is likely to bene   t from.

4.1 datasets

the earliest work on image description used relatively small datasets (farhadi et al., 2010;
kulkarni et al., 2011; elliott & keller, 2013). recently, the introduction of flickr30k,
ms coco and other large datasets has enabled the training of more complex models such
as neural networks. still, the area is likely to bene   t from larger and diversi   ed datasets
that share a common, uni   ed, comprehensive vocabulary. vinyals et al. (2015) argue that

16. calculated by collecting an additional human-written description, which was then compared to the

reference descriptions.

23

bernardi et al.

the collection process and the quality of the descriptions in the datasets a   ect performance
signi   cantly, and make id21 between datasets not as e   ective as expected.
they show that learning a model from ms coco and applying it to datasets collected in
di   erent settings such as sbu1m captions or pascal1k, leads to a degradation in id7
performance. this is surprising, since ms coco o   ers a much larger amount of training
data than pascal1k. as vinyals et al. put it, this is largely due to the di   erences in
vocabulary and in the quality of descriptions. most learning approaches are likely to su   er
from such situations. collecting larger and comprehensive datasets and developing more
generic approaches that are capable of generating naturalistic descriptions across domains
therefore is an open challenge.

while supervised algorithms are likely to take advantage of carefully collected large
datasets, lowering the amount of supervision in exchange of access to larger unsupervised
data is also an interesting avenue for future research. leveraging unsupervised data for
building richer representations and description models is another open research challenge
in this context.

4.2 measures

designing automatic measures that can mimic human judgments in evaluating the suitabil-
ity of image descriptions is perhaps the most urgent need in the area of image description
(elliott & keller, 2014). this need can be dramatically observed at the latest evaluation re-
sults of ms coco challenge. according to existing measures, including the latest cider
measure (vedantam et al., 2015), several automatic methods outperform the human up-
per bound (this upper bound indicates how similar human descriptions are to each other).
the counterintuitive nature of this result is con   rmed by the fact that when human judg-
ments are used for evaluation, the output of even the best system is judged as worse than
a human generated description for most of the time (fang et al., 2015). however, since
conducting human judgment experiments is costly, there is a major need for improved au-
tomatic measures that are more highly correlated with human judgments. figure 7 plots the
epanechnikov id203 density estimate (a non-parametric optimal estimator) for id7,
meteor, id8, and cider scores per subjective judgment in flickr8k dataset. the hu-
man judgments were obtained from human experts (hodosh et al., 2013). id7 is once
again con   rmed to be unable to su   ciently discriminate between the lowest three human
judgments, while meteor and cider show signs of moving towards a useful separation.

4.3 diversity and originality

current algorithms often rely on direct representations of the descriptions they see at train-
ing time, making the descriptions generated at test time very similar. this results in many
repetitions and limits the diversity of the generated descriptions, making it di   cult to reach
human levels of performance. this situation has been demonstrated by devlin et al. (2015),
who show that their best model is able to generate only 47.0% of unique descriptions. sys-
tems that generate diverse and original descriptions that do not just repeat what is already
seen, but also infer the underlying semantics therefore remain as an open challenge. chen
and zitnick (2015) and related approaches take a step towards addressing such limitations
by coupling description and visual representation generation.

24

automatic description generation from images: a survey

id7

meteor

id8

cider

figure 7: id203 density estimates of id7, meteor, id8, and cider scores
against human judgments in the flickr8k dataset. the y-axis shows the id203 density,
and the x-axis is the score computed by the measure.

25

bernardi et al.

jas and parikh (2015) introduces the notion of image speci   city, arguing that the do-
main of image descriptions is not uniform, certain images being more speci   c than others.
descriptions of non-speci   c images tend to vary a lot as people tend to describe a non-
speci   c scene from di   erent aspects. this notion and its e   ects to description systems and
measures should be investigated in further detail.

4.4 further tasks

another open challenge is visual question-answering (vqa). while natural
language
question-answering based on text has been a signi   cant goal of nlp research for a long
time (e.g., liang, jordan, & klein, 2012; fader, zettlemoyer, & etzioni, 2013; richard-
son, burges, & renshaw, 2013; fader, zettlemoyer, & etzioni, 2014), answering questions
about images is a task that has recently emerged. towards achieving this goal, malinowski
and fritz (2014a) propose a bayesian framework that connects natural language question-
answering with the visual information extracted from image parts. more recently, image
id53 methods based on neural networks have been developed (gao, mao,
zhou, huang, & yuille, 2015; ren, kiros, & zemel, 2015; malinowski, rohrbach, & fritz,
2015; ma, lu, & li, 2016). following this e   ort, several datasets on this task are being
released: daquar (malinowski & fritz, 2014a) was compiled from scene depth images
and mainly focuses on questions about the type, quantity and color of objects; coco-
qa (ren et al., 2015) was constructed by converting image descriptions to vqa format
over a subset of images from the ms coco dataset; the freestyle multilingual image ques-
tion answering (fm-iqa) dataset (gao et al., 2015), visual madlibs dataset (yu, park,
berg, & berg, 2015) and the vqa dataset (antol et al., 2015), were again built for images
from ms coco, but this time question-answer pairs are collected via human annotators
in a freestyle paradigm. research in this emerging    eld is likely to    ourish in the near fu-
ture. the ultimate goal of vqa is to build systems that can pass the (recently developed)
visual turing test by being able to answer arbitrary questions about images with the same
precision as a human observer (malinowski & fritz, 2014b; geman, geman, hallonquist, &
younes, 2015).

having multilingual repositories for image description is an interesting direction to
explore. currently, among the available benchmark datasets, only the iapr-tc12
dataset (grubinger et al., 2006) has multilingual descriptions (in english and german).
future work should investigate whether transferring multimodal features between monolin-
gual description models results in improved descriptions compared to monolingual baselines.
it would be interesting to study di   erent models and new tasks in a multilingual multimodal
setting using larger and more syntactically diverse multilingual description corpora.17

overall, image understanding is the ultimate goal of id161 and natural lan-
guage generation is one of the ultimate goals of nlp. image description is where these both
goals are interconnected and this topic is therefore likely to bene   t from individual advances
in each of these two    elds.

17. the multimodal translation shared task at the 2016 workshop on machine translation will use an
english and german translated version of the flickr30k corpora. see http://www.statmt.org/wmt16/
multimodal-task.html for more details.

26

automatic description generation from images: a survey

5. conclusions

in this survey, we discuss recent advances in automatic image description and closely related
problems. we review and analyze a large body of the existing work by highlighting common
characteristics and di   erences between existing research. in particular, we categorize the
related work into three groups: (i) direct description generation from images, (i) retrieval
of images from a visual space, and (iii) retrieval of images from multimodal (joint visual
and linguistic) space. in addition, we provided a brief review of the existing corpora and
automatic evaluation measures, and discussed some future directions for vision and language
research.

compared to traditional keyword-based image annotation (using object recognition,
attribute detection, scene labeling, etc.), automatic image description systems produce more
human-like explanations of visual content, providing a more complete picture of the scene.
advancements in this    eld could lead to more intelligent arti   cial vision systems, which
can make id136s about the scenes through the generated grounded image descriptions
and therefore interact with their environments in a more natural manner. they could also
have a direct impact on technological applications from which visually impaired people can
bene   t through more accessible interfaces.

despite the remarkable increase in the number of image description systems in recent
years, experimental results suggest that system performance still falls short of human per-
formance. a similar challenge lies in the automatic evaluation of systems using reference
descriptions. the measures and the tools currently in use are not su   ciently highly cor-
related with human judgments, indicating a need for measures that can deal with the
complexity of the image description problem adequately.

references

antol, s., agrawal, a., lu, j., mitchell, m., batra, d., zitnick, c. l., & parikh, d. (2015).

vqa: visual id53. in international conference on id161.

banerjee, s., & lavie, a. (2005). meteor: an automatic metric for mt evaluation with
improved correlation with human judgments. in annual meeting of the associa-
tion for computational linguistics workshop on intrinsic and extrinsic evaluation
measures for mt and/or summarization.

berg, t. l., berg, a. c., & shih, j. (2010). automatic attribute discovery and characteri-

zation from noisy web data. in european conference on id161.

chat   eld, k., simonyan, k., vedaldi, a., & zisserman, a. (2014). return of the devil in the
details: delving deep into convolutional nets. in british machine vision conference.

chen, j., kuznetsova, p., warren, d., & choi, y. (2015). d  ej`a image-captions: a corpus of
expressive descriptions in repetition. in north american chapter of the association
for computational linguistics.

chen, x., & zitnick, c. l. (2015). mind   s eye: a recurrent visual representation for image
id134. in ieee conference on id161 and pattern recognition.

dale, r., & white, m. e. (eds.). (2007). workshop on shared tasks and comparative

evaluation in id86: position papers.

27

bernardi et al.

denkowski, m., & lavie, a. (2014). meteor universal: language speci   c translation eval-
uation for any target language. in conference of the european chapter of the asso-
ciation for computational linguistics workshop on id151.

devlin, j., cheng, h., fang, h., gupta, s., deng, l., he, x., zweig, g., & mitchell, m.
(2015). language models for image captioning: the quirks and what works. in
annual meeting of the association for computational linguistics.

donahue, j., hendricks, l. a., guadarrama, s., rohrbach, m., venugopalan, s., saenko,
k., & darrell, t. (2015). long-term recurrent convolutional networks for visual recog-
nition and description. in ieee conference on id161 and pattern recog-
nition.

elliott, d., & de vries, a. p. (2015). describing images using inferred visual dependency
representations. in annual meeting of the association for computational linguistics.

elliott, d., & keller, f. (2013). image description using visual dependency representa-

tions. in conference on empirical methods in natural language processing.

elliott, d., & keller, f. (2014). comparing automatic evaluation measures for image

description. in annual meeting of the association for computational linguistics.

elliott, d., lavrenko, v., & keller, f. (2014). query-by-example id162 using
visual dependency representations. in international conference on computational
linguistics.

everingham, m., van gool, l., williams, c. k. i., winn, j., & zisserman, a. (2010). the
pascal visual object classes (voc) challenge. international journal of computer
vision, 88 (2), 303   338.

fader, a., zettlemoyer, l., & etzioni, o. (2013). paraphrase-driven learning for open ques-
tion answering. in annual meeting of the association for computational linguistics.

fader, a., zettlemoyer, l., & etzioni, o. (2014). open id53 over curated and
extracted knowledge bases. in acm sigkdd conference on knowledge discovery
and data mining.

fang, h., gupta, s., iandola, f., srivastava, r., deng, l., doll  ar, p., gao, j., he, x.,
mitchell, m., platt, j., zitnick, c. l., & zweig, g. (2015). from captions to visual
concepts and back. in ieee conference on id161 and pattern recognition.

farhadi, a., hejrati, m., sadeghi, m. a., young, p., rashtchian, c., hockenmaier, j., &
forsyth, d. (2010). every picture tells a story: generating sentences from images. in
european conference on id161.

felzenszwalb, p. f., girshick, r. b., mcallester, d., & ramanan, d. (2010). object detec-
tion with discriminatively trained part-based models. ieee transactions on pattern
analysis and machine intelligence, 32 (9), 1627   1645.

feng, y., & lapata, m. (2008). automatic image annotation using auxiliary text infor-

mation. in annual meeting of the association for computational linguistics.

feng, y., & lapata, m. (2013). automatic id134 for news images.
transactions on pattern analysis and machine intelligence, 35 (4), 797   812.

ieee

28

automatic description generation from images: a survey

ferraro, f., mostafazadeh, n., huang, t., vanderwende, l., devlin, j., galley, m., &
mitchell, m. (2015). a survey of current datasets for vision and language research. in
conference on empirical methods in natural language processing.

gao, h., mao, j., zhou, j., huang, z., & yuille, a. (2015). are you talking to a machine?
in international

dataset and methods for multilingual image id53.
conference on learning representations.

geman, d., geman, s., hallonquist, n., & younes, l. (2015). visual turing test for computer
vision systems. proceedings of the national academy of sciences, 112 (12), 3618   3623.

girshick, r., donahue, j., darrell, t., & malik, j. (2014). rich feature hierarchies for accu-
rate id164 and semantic segmentation. in ieee conference on computer
vision and pattern recognition.

gong, y., wang, l., hodosh, m., hockenmaier, j., & lazebnik, s. (2014). improving image-
sentence embeddings using large weakly annotated photo collections. in european
conference on id161.

grubinger, m., clough, p., muller, h., & deselaers, t. (2006). the iapr tc-12 benchmark:
a new evaluation resource for visual information systems. in international conference
on language resources and evaluation.

guadarrama, s., krishnamoorthy, n., malkarnenkar, g., venugopalan, s., mooney, r., dar-
rell, t., & saenko, k. (2013). youtube2text: recognizing and describing arbitrary
activities using semantic hierarchies and zero-shot recognition. in international con-
ference on id161.

gupta, a., verma, y., & jawahar, c. v. (2012). choosing linguistics over vision to describe

images. in aaai conference on arti   cial intelligence.

hardoon, d. r., szedmak, s., & shawe-taylor, j. (2004). canonical correlation analysis:
an overview with application to learning methods. neural computation, 16 (12),
2639   2664.

hodosh, m., & hockenmaier, j. (2013). sentence-based image description with scalable,
explicit models. in ieee conference on id161 and pattern recognition
workshops.

hodosh, m., young, p., & hockenmaier, j. (2013). framing image description as a rank-
ing task: data, models and id74. journal of arti   cial intelligence
research, 47, 853   899.

hotelling, h. (1936). relations between two sets of variates. biometrika, 0, 321   377.

jaimes, a., & chang, s.-f. (2000). a conceptual framework for indexing visual information

at multiple levels. in ist spie internet imaging.

jas, m., & parikh, d. (2015). image speci   city. in ieee conference on id161

and pattern recognition.

jia, x., gavves, e., fernando, b., & tuytelaars, t. (2015). guiding the long-short term
memory model for image id134. in international conference on com-
puter vision.

29

bernardi et al.

johnson, j., krishna, r., stark, m., li, l.-j., shamma, d. a., bernstein, m., & fei-fei, l.
(2015). id162 using scene graphs. in ieee conference on id161
and pattern recognition.

karpathy, a., & fei-fei, l. (2015). deep visual-semantic alignments for generating image

descriptions. in ieee conference on id161 and pattern recognition.

karpathy, a., joulin, a., & fei-fei, l. (2014). deep fragment embeddings for bidirectional

image sentence mapping. in advances in neural information processing systems.

khan, m. u. g., zhang, l., & gotoh, y. (2011). towards coherent natural language descrip-
tion of video streams. in international conference on id161 workshops.

kiros, r., salakhutdinov, r., & zemel, r. s. (2015). unifying visual-semantic embeddings
with multimodal neural language models. in advances in neural information pro-
cessing systems deep learning workshop.

krishnamoorthy, n., malkarnenkar, g., mooney, r., saenko, k., & guadarrama, s. (2013).
generating natural-language video descriptions using text-mined knowledge. in
annual conference of the north american chapter of the association for computa-
tional linguistics: human language technologies.

kulkarni, g., premraj, v., dhar, s., li, s., choi, y., berg, a. c., & berg, t. l. (2011). baby
talk: understanding and generating simple image descriptions. in ieee conference
on id161 and pattern recognition.

kuznetsova, p., ordonez, v., berg, a. c., berg, t. l., & choi, y. (2012). collective
generation of natural image descriptions. in annual meeting of the association for
computational linguistics.

kuznetsova, p., ordonezz, v., berg, t. l., & choi, y. (2014). treetalk: composition
and compression of trees for image descriptions. in conference on empirical methods
in natural language processing.

lampert, c. h., nickisch, h., & harmeling, s. (2009). learning to detect unseen object
classes by between-class attribute transfer. in ieee conference on id161
and pattern recognition.

lazebnik, s., schmid, c., & ponce, j. (2006). beyond bags of features: spatial pyramid
matching for recognizing natural scene categories. in ieee conference on computer
vision and pattern recognition.

lebret, r., pinheiro, p. o., & collobert, r. (2015). phrase-based image captioning.

in

international conference on machine learning.

li, s., kulkarni, g., berg, t. l., berg, a. c., & choi, y. (2011). composing simple image
descriptions using web-scale id165s. in the signll conference on computational
natural language learning.

liang, p., jordan, m. i., & klein, d. (2012). learning dependency-based compositional

semantics. computational linguistics, 39 (2), 389   446.

lin, c.-y., & hovy, e. (2008). automatic evaluation of summaries using id165 co-
occurrence statistics. in annual conference of the north american chapter of the
association for computational linguistics: human language technologies.

30

automatic description generation from images: a survey

lin, d., fidler, s., kong, c., & urtasun, r. (2015). generating multi-sentence natural

language descriptions of indoor scenes. in british machine vision conference.

lin, t.-y., maire, m., belongie, s., hays, j., perona, p., ramanan, d., doll  ar, p., & zitnick,
c. l. (2014). microsoft coco: common objects in context. in european conference
on id161.

lowe, d. (2004). distinctive image features from scale-invariant keypoints. international

journal of id161, 60 (4), 91   110.

ma, l., lu, z., & li, h. (2016). learning to answer questions from image using convolutional

neural network. in aaai conference on arti   cial intelligence.

malinowski, m., & fritz, m. (2014a). a multi-world approach to id53 about
real-world scenes based on uncertain input. in advances in neural information pro-
cessing systems.

malinowski, m., & fritz, m. (2014b). towards a visual turing challenge. in advances in

neural information processing systems workshop on learning semantics.

malinowski, m., rohrbach, m., & fritz, m. (2015). ask your neurons: a neural-based
approach to answering questions about images. in international conference on com-
puter vision.

mao, j., xu, w., yang, y., wang, j., & yuille, a. l. (2015a). deep captioning with multi-
modal recurrent neural networks (m-id56). in international conference on learning
representations.

mao, j., wei, x., yang, y., wang, j., huang, z., & yuille, a. l. (2015b). learning like
a child: fast novel visual concept learning from sentence descriptions of images. in
international conference on id161.

mason, r., & charniak, e. (2014). nonparametric method for data-driven image caption-

ing. in annual meeting of the association for computational linguistics.

mason, w. a., & watts, d. j. (2009). financial incentives and the    performance of crowds   .

in acm sigkdd workshop on human computation.

mitchell, m., han, x., dodge, j., mensch, a., goyal, a., berg, a. c., yamaguchi, k.,
berg, t. l., stratos, k., daume, iii, h., & iii (2012). midge: generating image
descriptions from id161 detections. in conference of the european chapter
of the association for computational linguistics.

nenkova, a., & vanderwende, l. (2005). the impact of frequency on summarization. tech.

rep., microsoft research.

oliva, a., & torralba, a. (2001). modeling the shape of the scene: a holistic representation

of the spatial envelope. international journal of id161, 42 (3), 145   175.

ordonez, v., kulkarni, g., & berg, t. l. (2011). im2text: describing images using 1 million

captioned photographs. in advances in neural information processing systems.

ortiz, l. m. g., wol   , c., & lapata, m. (2015). learning to interpret and describe
abstract scenes. in conference of the north american chapter of the association of
computational linguistics.

31

bernardi et al.

panofsky, e. (1939). studies in iconology. oxford university press.

papineni, k., roukos, s., ward, t., & zhu, w.-j. (2002). id7: a method for auto-
matic evaluation of machine translation. in annual meeting of the association for
computational linguistics.

parikh, d., & grauman, k. (2011). relative attributes. in international conference on

id161.

park, c., & kim, g. (2015). expressing an image stream with a sequence of natural

sentences. in advances in neural information processing systems.

patterson, g., xu, c., su, h., & hays, j. (2014). the sun attribute database: beyond cat-
egories for deeper scene understanding. international journal of id161,
108 (1-2), 59   81.

pinheiro, p., lebret, r., & collobert, r. (2015). simple image description generator via a
linear phrase-based model. in international conference on learning representations
workshop.

prest, a., schmid, c., & ferrari, v. (2012). weakly supervised learning of interactions
between humans and objects. ieee transactions on pattern analysis and machine
intelligence, 34 (3), 601   614.

rashtchian, c., young, p., hodosh, m., & hockenmaier, j. (2010). collecting image annota-
tions using amazon   s mechanical turk. in north american chapter of the association
for computational linguistics: human language technologies workshop on creating
speech and language data with amazon   s mechanical turk.

reiter, e., & belz, a. (2009). an investigation into the validity of some metrics for auto-
matically evaluating id86 systems. computational linguistics,
35 (4), 529   588.

reiter, e., & dale, r. (2006). building id86 systems. cambridge

university press.

ren, m., kiros, r., & zemel, r. (2015). image id53: a visual semantic em-
bedding model and a new dataset. in international conference on machine learningt
deep learning workshop.

richardson, m., burges, c. j., & renshaw, e. (2013). mctest: a challenge dataset for the
open-domain machine comprehension of text. in conference on empirical methods in
natural language processing.

rohrbach, a., rohrback, m., tandon, n., & schiele, b. (2015). a dataset for movie de-

scription. in international conference on id161.

rohrbach, m., qiu, w., titov, i., thater, s., pinkal, m., & schiele, b. (2013). translating
in international conference on

video content to natural language descriptions.
id161.

schuster, s., krishna, r., chang, a., fei-fei, l., & manning, c. d. (2015). generating
semantically precise scene graphs from textual descriptions for improved image re-
trieval. in conference on empirical methods in natural language processing vision
and language workshop.

32

automatic description generation from images: a survey

shatford, s. (1986). analyzing the subject of a picture: a theoretical approach. cataloging

& classi   cation quarterly, 6, 39   62.

silberman, n., kohli, p., hoiem, d., & fergus, r. (2012). indoor segmentation and support

id136 from rgbd images. in european conference on id161.

socher, r., & fei-fei, l. (2010). connecting modalities: semi-supervised segmentation
in ieee conference on

and annotation of im- ages using unaligned text corpora.
id161 and pattern recognition.

socher, r., karpathy, a., le, q. v., manning, c. d., & ng, a. (2014). grounded compo-
sitional semantics for finding and describing images with sentences. transactions
of the association for computational linguistics, 2, 207   218.

thomason, j., venugopalan, s., guadarrama, s., saenko, k., & mooney, r. (2014). inte-
grating language and vision to generate natural language descriptions of videos
in the wild. in international conference on computational linguistics.

torralba, a., fergus, r., & freeman, w. t. (2008). 80 million tiny images: a large data
set for nonparametric object and scene recognition. ieee transactions on pattern
analysis and machine intelligence, 30 (11), 1958   1970.

ushiku, y., yamaguchi, m., mukuta, y., & harada, t. (2015). common subspace for model
and similarity: phrase learning for id134 from images. in international
conference on id161.

vedantam, r., lawrence zitnick, c., & parikh, d. (2015). cider: consensus-based im-
in ieee conference on id161 and pattern

age description evaluation.
recognition.

verma, y., & jawahar, c. v. (2014). im2text and text2im: associating images and texts

for cross-modal retrieval. in british machine vision conference.

vinyals, o., toshev, a., bengio, s., & erhan, d. (2015). show and tell: a neural image
caption generator. in ieee conference on id161 and pattern recognition.

xu, k., ba, j., kiros, r., cho, k., courville, a., salakhutdinov, r., zemel, r., & bengio, y.
(2015). show, attend and tell: neural image id134 with visual attention.
in international conference on machine learning.

yagcioglu, s., erdem, e., erdem, a., & cakici, r. (2015). a distributed representation
based id183 approach for image captioning. in annual meeting of the
association for computational linguistics.

yang, y., teo, c. l., daume, iii, h., & aloimonos, y. (2011). corpus-guided sentence gen-
eration of natural images. in conference on empirical methods in natural language
processing.

yao, b., & fei-fei, l. (2010). grouplet: a structured image representation for recognizing
human and object interactions. in ieee conference on id161 and pattern
recognition.

yao, l., torabi, a., cho, k., ballas, n., pal, c., larochelle, h., & courville, a. (2015).
describing videos by exploiting temporal structure. in international conference on
id161.

33

bernardi et al.

yatskar, m., galley, m., vanderwende, l., & zettlemoyer, l. (2014). see no evil, say no
evil: description generation from densely labeled images. in joint conference on
lexical and computation semantics.

young, p., lai, a., hodosh, m., & hockenmaier, j. (2014). from image descriptions to visual
denotations: new similarity metrics for semantic id136 over event descriptions.
transactions of the association for computational linguistics, 2, 67   78.

yu, l., park, e., berg, a. c., & berg, t. l. (2015). visual madlibs: fill in the blank
description generation and id53. in international conference on com-
puter vision.

zhu, y., kiros, r., zemel, r., salakhutdinov, r., urtasun, r., torralba, a., & fidler, s.
(2015). aligning books and movies: towards story-like visual explanations by watching
movies and reading books. in international conference on id161.

zitnick, c. l., parikh, d., & vanderwende, l. (2013). learning the visual interpretation of

sentences. in international conference on id161.

zitnick, c. l., & parikh, d. (2013). bringing semantics into focus using visual abstraction.

in ieee conference on id161 and pattern recognition.

34

