learning semantic similarity for very short texts

cedric de boom, steven van canneyt, steven bohez, thomas demeester, bart dhoedt

ghent university     iminds

{cedric.deboom, steven.vancanneyt, steven.bohez, thomas.demeester, bart.dhoedt}@ugent.be

gaston crommenlaan 8-201, 9050 ghent, belgium

5
1
0
2

 
c
e
d
2

 

 
 
]

r

i
.
s
c
[
 
 

1
v
5
6
7
0
0

.

2
1
5
1
:
v
i
x
r
a

text

abstract   levering data on social media, such as twitter
and facebook, requires information retrieval algorithms to
become able to relate very short
fragments to each
other. traditional text similarity methods such as tf-idf cosine-
similarity, based on word overlap, mostly fail to produce
good results in this case, since word overlap is little or non-
existent. recently, distributed word representations, or word
embeddings, have been shown to successfully allow words
to match on the semantic level. in order to pair short text
fragments   as a concatenation of separate words   an adequate
distributed sentence representation is needed, in existing lit-
erature often obtained by naively combining the individual
word representations. we therefore investigated several text
representations as a combination of id27s in the
context of semantic pair matching. this paper investigates the
effectiveness of several such naive techniques, as well as tradi-
tional tf-idf similarity, for fragments of different lengths. our
main contribution is a    rst step towards a hybrid method that
combines the strength of dense distributed representations   
as opposed to sparse term matching   with the strength of
tf-idf based methods to automatically reduce the impact of
less informative terms. our new approach outperforms the
existing techniques in a toy experimental set-up, leading to the
conclusion that the combination of id27s and tf-idf
information might lead to a better model for semantic content
within very short text fragments.

i. introduction

on social media billions of small text messages are made
public every day: own research indicates that almost every
tweet is comprised of one up to approximately thirty words.
to tap into this stream of extremely short text fragments, we
need appropriate information retrieval algorithms. tf-idf is
an example of a traditional and very popular representation
to compare texts, such as news articles, with each other [1],
[2]. it relies on word overlap to    nd similarities, but in very
short texts, in which word overlap is rare, tf-idf often fails.
for this reason we need sentence representations that grasp
more than just word contents.

in 2013, mikolov et al. published three papers on the
topic of distributed id27s to catch semantic
similarities between words [3], [4], [5], which resulted
in google   s id97 software. since then scientists have
extensively used such embeddings to improve state-of-the-
art algorithms in natural language processing, such as part-
of-speech tagging [6], sentence completion [7], hashtag
prediction [8], etc. there is however a lack of research
and insight on how to effectively combine embeddings into

a single sentence representation that contains most of its
semantic information. many authors choose to average or
maximize across the embeddings in a text [8], [9], [10] or
combine them through a multi-layer id88 [6], [11], by
id91 [12], or by trimming the text to a    xed length
[11].

the paragraph vector algorithm by le and mikolov   
also termed paragraph2vec   is a powerful method to    nd
suitable vector representations for sentences, paragraphs and
documents of variable length [13]. the algorithm tries to
   nd embeddings for separate words and paragraphs at the
same time through a procedure similar to id97. the
collection of paragraphs, however, is known beforehand.
this implies that    nding a vector representation for a new
and probably unseen paragraph   the theoretical number of
different paragraphs is after all many times higher than
the number of different words   requires additional training.
paragraph2vec is therefore not a    t candidate to be used in,
e.g., a stream of messages as is the case with social media.
further research is thus needed to derive optimal sentence
representations based on id27s. by investigating
and comparing the performance of several word combination
approaches in a short-text matching task, we arrive at a
novel technique in which we aggregate both tf-idf and word
embedding signals. in this paper, we show how word em-
beddings can be combined into a new vector representation
for the entire considered fragment, in which the impact
of frequent words   i.e. with a low idf-component, and
therefore mostly non-informative   is reduced with respect to
more informative words. this leads to a signi   cant increase
in the effectiveness of detecting semantically similar short-
text fragments, compared to traditional
tf-idf techniques
or simple heuristic methods to combine id27s.
our approach is a    rst step towards a hybrid method that
unites id27 and tf-idf information of a short text
fragment into a distributed representation that catches most
of that fragment   s semantic information.

very recently, kusner et al. devised a simple method
to measure the similarity between documents based on the
minimal distance id27s have to travel from one
document to another [14]. in this, the authors only consider
non stop words, and evaluate their distance measure using
knn classi   cation. we, however, will learn vector repres-
entations for documents, and we will evaluate our technique

through a newly crafted dataset of related text fragments.
also recently, zheng and callan created a simple algorithm
based on id75 to    nd relevant terms in a query
[15]. the authors learned weights for each dimension in a
id27 using a supervised relevance signal. our
work is different in that we will learn weights for entire
word vectors instead of separate dimensions. for this we
will use tf-idf information, instead of only id27
features. furthermore, we will arrive at a globally applicable
weighting scheme instead of a query-dependent one.

in the next section we will discuss our experimental setup
and method of data collection, and explain how well a
number of traditional techniques perform on our dataset.
we will then use the gained insights to create more effect-
ive distributed representations by integration of the tf-idf
information.

ii. experimental set-up and analysis

to evaluate techniques that measure semantic similarity
between short text fragments, we need a reference set with
couples of fragments that are semantically related, and
couples which are not related. we denote the former as
a pair, and the latter as a non-pair. every couple consists
of two texts, which are built up as a sequence of words.
for an arbitrary couple we introduce the notation c, and the
two texts in c are denoted as the sequences (c1) and (c2).
element j of sequence (c1) is the vector of word j in the
j :
   rst text of c, denoted as w1
1, w1

(cid:0)c1(cid:1) (cid:44)(cid:0)w1

3, . . .(cid:1) .

2, w1

a vector representation for (c1), combining the word rep-
resentations contained in (c1), is written as o1, and as o2
for (c2) respectively.

in this paper we strive to give the initial

impetus to
learning sentence representations of very short text frag-
ments mainly found on social media, but for now we will
perform our experiments in a toy environment using english
wikipedia articles. these are of course very different textual
media   which has some disadvantages, as we will discuss
later   but wikipedia articles have the bene   t of being well-
structured, which allows us to extract related texts more
easily. in our experiments we use the wikipedia dump of
march 4th, 2015, after cleaning the articles by removing
markup and punctuation. we convert all texts to lowercase
and replace the numbers by a single character    0   . in our toy
setting we require that the texts of all couples are composed
of the same number of words, i.e. the length of the sequences
(c1) and (c2) is equal to nc. to extract a pair of texts,
each containing nc words, we take the    rst nc words of
a paragraph, skip the next two words, and then take the
following nc words. to extract a non-pair, we take nc words
out of two random paragraphs of different articles. this
approach is closely related to the one used by hu et al. to
extract pairs and non-pairs from the reuters corpus [7]. in

total we extract    ve million pairs and    ve million non-pairs,
for texts of ten, twenty, and thirty words long1.

to represent words as a vector, we train id27s
on the entire wikipedia corpus. we do this through google   s
id97 software, using skip-gram with negative sampling,
a context window of    ve words, and 400 dimensions. we
also calculate document frequencies for every word using
the same wikipedia corpus.

we regard two text fragments to be semantically similar if
their corresponding vector representations lie close to each
other according to some distance measure, and dissimilar if
the vectors lie farther apart. semantic similarity between text
fragments is therefore related to semantic similarity between
skip-gram id27s, in which the cosine distance
between related words is smaller compared to unrelated
words. this is also the reason why we do not use paraphrase
datasets, such as the microsoft research paraphrase corpus
or the semeval2015 twitter paraphrase dataset, to perform
our experiments. after all, the notion of semantic relatedness
in these datasets is often too narrow: if one sentence is about
star wars and another about anakin skywalker, they are
semantically related although they might not be paraphrases
of each other.

to verify whether our own dataset of 10 million wikipedia
couples is a valid candidate to perform similarity experi-
ments on, we will test different techniques that try to en-
hance the discriminative power between pairs and non-pairs
as much as possible   i.e. leading to small distances between
pairs and larger distances between non-pairs. we start with
techniques ranging from plain tf-idf to naive combinations
of id27s, after which we investigate elementary
mixtures of tf-idf and id27 signals.

for every couple c we create a tf-idf representation for
both (c1) and (c2), and calculate the cosine similarity
between (c1) and (c2). figure 1 shows a histogram plot of
the number of couples as a function of their cosine similarity,
for both pairs and non-pairs separately, and for texts of 20
words long. we see that there are many couples having a
very low cosine similarity, which is due to the very short
length of the text fragments. there are many more pairs
having a larger cosine similarity than there are non-pairs, but
non-related texts can also exhibit relatively large similarity
values, which is due to coincidental overlap of mostly non-
informative words.

as for id27s, we create two traditional sen-
tence representations as a baseline. in a    rst representation
we take the mean of all id27s in the text:

   (cid:96)     {1, 2} : o(cid:96) =

1
nc

w(cid:96)
j,

(1)

nc(cid:88)

j=1

represents the id97 id27
in which w(cid:96)
j
vector of the j   th word in text sequence (cid:96). in a second

1the dataset can be obtained upon request.

106

105

104

103

102

101

s
e
l
p
u
o
c

f
o

r
e
b
m
u
n

100

0.0

0.2

250000

200000

150000

100000

50000

s
e
l
p
u
o
c

f
o

r
e
b
m
u
n

0.8

1.0

0
0.0

0.2

0.4

0.6

cosine similarity

0.8

1.0

0.4

0.6

cosine similarity

figure 1. histogram plot of the number of couples as a function of their
cosine similarity using tf-idf, for both pairs (dark grey) and non-pairs (light
grey).

figure 2. histogram plot of the number of couples as a function of their
cosine similarity using the mean of the id27s, for both pairs
(dark grey) and non-pairs (light grey).

representation, we take for each dimension the maximum
across all embeddings:

   (cid:96)     {1, 2}, k     {1, . . . , 400} : o(cid:96)

k = max

w(cid:96)

j,k.

(2)

j

between two such representations we then calculate the
cosine similarity as before.

figure 2 shows a histogram plot for the mean of the
embeddings, and for texts of 20 words long. the graph
shows two curves with a stretched tail towards lower cosine
similarities. we see that the mode of the non-pairs curve
lies more to the left than the mode of the pairs curve, but
still close to each other. our hypothesis is that this is due to
overlap in non-informative but frequently occurring words,
such as articles and prepositions. such words, however,
contribute little to the semantic meaning of a text, and by
reducing the in   uence of such words, we want to accentuate
the true semantics of a text fragment. by reducing this coin-
cidental similarity, we intend to shift the non-pairs stronger
toward lower similarities than the pairs, hence increasing the
resolution between both.

since less informative terms are common to many sen-
tences, they mostly have a high document frequency as well.
we therefore implement the mean and max techniques again,
but this time we only use the top 30% of the words with
the highest idf component. in a    nal technique we use all
words, but we weigh each word vector with its idf value,
after which we take the mean.

as id27s contain both positive and negative
numbers, we also test the in   uence of the sign in these
embeddings. in a    rst experiment we take, instead of the
maximum, the minimum across all id27s in a
text. in a second experiment we test whether extremes, either
positive or negative, are important indicators for semantic
similarity. we therefore simply concatenate the maximum

vector and the minimum vector to form a new vector
representation.

to evaluate the power of the previously described tech-
niques to discriminate between pairs and non-pairs, we
calculate two performance metrics: optimal split error and
jensen-shannon (js) divergence. we obtain the former using
the optimal threshold for which the number of misclassi   ed
couples is minimal. the latter is a symmetric measure ex-
pressing the similarity between two id203 distributions,
based on the well-known   but asymmetric   kl divergence.
the lower the optimal split error or the higher the js
divergence, the better a technique can distinguish pairs from
non-pairs. table i shows the results, for texts of 10, 20 and
30 words.

as for the traditional techniques, the max approach works
best for 10 and 20 words, but as the number of words
increases to 30, tf-idf performs best, which is logical since
word overlap rises with a growing number of words, as can
be seen in figure 1 as well. the min approach performs
almost as well as the max approach. by concatenating the
minimum and maximum vectors, we see that split error
and js divergence are improved by a large margin. we
can thus conclude that the sign in id27s holds
complementary semantic information. by incorporating doc-
ument frequency information, we do better than all previous
techniques for texts of 10 words long. but for longer texts
the mean approach performs best, while the min and max
combinations achieve worse results than when using the
complete text. the best performing techniques are the idf-
weighed mean approach and the approach taking the mean
of 30% of the word vectors with the highest idf-components,
as they perform similarly across the different word lengths.
we also investigate the in   uence of the used distance
metric. we test cosine distance, euclidean distance, l3-
norm, l4-norm and bray-curtis distance, which are nor-

0.00.20.40.60.81.01001011021031041051060.00.20.40.60.81.0050000100000150000200000250000comparison of different word vector aggregation techniques with cosine similarity.

table i

10 words

js divergence

20 words

js divergence

30 words

js divergence

tf-idf
mean
max
min
min/max
mean, top 30% idf
max, top 30% idf
min/max, top 30% idf
mean, idf weighed

split error
36.15%
30.67%
28.27%
28.89%
26.89%
23.69%
26.56%
25.43%
23.35%

0.11946
0.16186
0.20831
0.19694
0.22946
0.30044
0.24028
0.25761
0.30400

split error
20.09%
21.05%
19.06%
19.54%
16.86%
15.86%
20.63%
19.13%
15.77%

0.35991
0.34109
0.40030
0.38696
0.45004
0.48260
0.36809
0.39775
0.48028

split error
12.55%
16.33%
15.18%
15.69%
12.76%
12.42%
16.66%
14.91%
12.43%

0.54468
0.45534
0.49882
0.48592
0.56253
0.56456
0.45522
0.49927
0.56028

comparison of different distance metrics for texts of 20

table ii

words long.

mean, cosine
mean, euclidean
mean, l3
mean, l4
mean, bray-curtis

split error
21.05%
19.55%
19.62%
19.77%
21.22%

js divergence

0.34109
0.37788
0.37511
0.37061
0.33775

malised between 0 and 1. we use texts of 20 words long
and the mean of the embeddings. table ii shows the results,
again expressed in terms of split error and js divergence.
euclidean distances perform best in our tests, so we continue
to use euclidean distances hereafter.

iii. learning semantic similarity

as became clear

from the data analysis, combining
knowledge from both tf-idf and id27s can be
bene   cial. using only the portion with the highest idf com-
ponent of all words clearly reduces split error and improves
js divergence. after all, low-idf words have no clear-cut
semantic meaning, and since these words are present in
many sentences, there is more coincidental overlap between
non-related sentences. removing these words   or lowering
their in   uence   from a text representation thus succeeds
in pulling apart the average similarity between pairs and
between non-pairs.

in this section we investigate how we can learn to op-
timally weigh words in a short text. this way we intend
to do better than just taking the top-idf words or weighing
these words with their idf component, in order to maximize
the average distance between pairs and non-pairs. as before,
we perform the experiments in a toy setting on couples of
short wikipedia texts, with as many pairs as non-pairs. we
divide the total dataset into a training set d of 1.5 million
couples, a test set t of 1.5 million couples and a validation
set v of 2.0 million couples. since we describe ongoing
research and present the    rst steps towards a    exible hybrid
technique, we only consider texts of 20 words long in this

nc(cid:88)

j=1

section, and varying the fragment length will be suggested
as future work.

we implement the following learning procedure. for every
couple c in the training set we sort
the words in both
texts (c1) and (c2) according to their document frequency   
i.e. the word with the lowest document frequency comes
   rst   arriving at (c1(cid:48)
). next we multiply the
id27 vector of each word w1(cid:48)
j with
an importance factor ij; these importance factors are global
weights that will be learned. finally, we take the mean of
these weighed embeddings to obtain a    xed-length vector
o1 for (c1) and o2 for (c2):

) and (c2(cid:48)

and w2(cid:48)

j

   (cid:96)     {1, 2} : o(cid:96) =

1
nc

ij    w(cid:96)(cid:48)
j .

(3)

we take the mean since it is the best performing technique
in the third part of table i. figure 3 illustrates the en-
tire procedure of calculating a vector representation for a
sentence using the importance factor method. we see that
   rst the words in the sentence are sorted according to their
idf-component; next, their 400-dimensional id27
vectors are multiplied by importance factors, and    nally the
mean is taken.

to learn the importance factors, we de   ne a id168
as a function of any couple c that minimizes the distance
between the vectors of a pair, and maximizes the distance
between the vectors of a non-pair:

f (c) (cid:44)

d(o1, o2)
   d(o1, o2)

if c is a pair
if c is a non-pair

(4)

with d(  ) a distance function of choice. we use a squared
euclidean distance as distance function:

d(o1, o2) =

j     o2

j )2.

(o1

(5)

j=1

we then optimize the following objective as a function of

nc(cid:88)

(cid:40)

i1

i9

      

  

1/9

figure 3.
of nine words long.

illustration of the importance factor approach for a toy sentence

1

j
i

e
d
u
t
i
n
g
a
m

0

1

6

16
importance factor index j

11

figure 4. plot of the importance factor magnitudes.

the importance factors:

j(i1, . . . , inc) =

(cid:88)

c   d

1
|d|

nc(cid:88)

j=1

f (c) +   

i2
j .

(6)

to minimize this objective function we use stochastic gradi-
ent descent with batches of 100 couples, a learning rate of
0.1, a momentum of 0.9, and a id173 constant    of
0.0015. we start the optimization with all importance factors
equal to 0.5. thanks to the large amount of couples in the
training set, we can stop the optimization after one epoch
of training. by then, the factors have settled to an optimum
and the procedure has seen all training couples exactly once,
thereby reducing chances of over   tting.

figure 4 shows a plot of the importance factors that were
learned through the earlier-described optimization proced-
ure. we clearly notice that the importance factors steadily
decrease in magnitude; words with a low document fre-
quency therefore weigh much more than words with a high
document frequency, which con   rms our hypothesis. the
factors at the end are very close to zero.

300000

250000

200000

150000

100000

50000

s
e
l
p
u
o
c

f
o

r
e
b
m
u
n

0
0.6

importance factors

mean

0.7

0.8

similarity

0.9

1.0

figure 5.
factor approach, for both pairs (dark grey) and non-pairs (light grey).

comparison between mean embeddings and the importance

to compare the performance of our importance factor
approach to the earlier-described combination techniques,
we calculate the optimal split point between pairs and non-
pairs on our validation set for each of the techniques. using
these optimal split points, we then calculate the    nal split
error rate on our test set. as a distance metric we use a
normalized euclidean distance, except for tf-idf for which
we used the standard cosine distance. table iii shows the
error rates for the different techniques we tested. we notice
that the importance factor technique outperforms the other
approaches by approx 2.0% in error rate. this is a signi   cant
decrease, as p < 0.001 in a two-tailed binomial test. we
compare our importance factor approach with the plain mean
technique on the complete dataset in figure 5, in which we
see that in our approach the variance of the curves is smaller
and that there is less overlap between the pairs and non-pairs.

iv. discussion and conclusion

we gained insight in the power of tf-idf and several word
embedding aggregation techniques to relate pairs of very
short texts to each other. we also learned how to optimally
combine knowledge from both tf-idf and id27s
to maximize the separation between pairs and non-pairs. the
best performing traditional technique is a concatenation of
maximum and minimum vectors, and for a large number of
words tf-idf produces comparable results. our importance
factor approach, however, signi   cantly outperforms all other
techniques by a large margin.

in this paper we have laid out the    rst steps towards a
   exible and hybrid technique to combine id27s
with tf-idf information. since this is still ongoing research,
a few remarks need, however, to be made. first, our ex-
perimental set-up is close to a toy setting. wikipedia is a
completely different textual medium than a social platform
such as twitter, in which the used language is full of slang,
hashtags and spelling errors. in future work we will therefore

the   quick   brown   fox   jumped   over   the   lazy   dogjumped   lazy   quick   dog   fox   brown   over   the   thesort400400051015200.00.20.40.60.81.00.60.70.80.91.0050000100000150000200000250000300000comparison of error rates on the test set, indicating a

significant reduction for the importance factor approach.

table iii

tf-idf
mean
max
min/max
mean, top 30% idf
max, top 30% idf
min/max, top 30% idf
mean, idf weighed
mean, importance factors

error rate
19.60%
19.43%
19.05%
16.78%
17.02%
18.05%
16.40%
24.00%
14.44%

adapt our novel technique to texts found in social media
posts.

secondly, our current approach is limited to texts of a
   xed length, while all other techniques in table iii do not
suffer from this restriction. this forms a strong constraint on
the applicability of the technique. however, current research
shows promise that the approach can be extended to texts of
arbitrary length as well, but this still needs to be investigated
further in future work.

in other future work we will discuss yet other combination
schemes of id27s and tf-idf, and experiment
with the number of dimensions in the embeddings. we will
also investigate and compare the performance of lsi, topic
models such as lda, and other state-of-the-art document
distance measures based on id27s.

v. acknowledgments

cedric de boom is funded by a ph.d. grant of ghent uni-
versity, special research fund (bof) and of the flanders
research foundation (fwo).
steven van canneyt and steven bohez are funded by a
ph.d. grant of the agency for innovation by science and
technology in flanders (iwt).

references

[1] c. manning, p. raghavan, and h. sch  utze, an introduction
to information retrieval. cambridge university press, apr.
2009.

[2] p. achananuparp, x. hu, and x. shen,    the evaluation
of sentence similarity measures,    in dawak 2008: inter-
national conference on data warehousing and knowledge
discovery, jul. 2008.

[3] t. mikolov, w.-t. yih, and g. zweig,    linguistic regularities
in continuous space word representations,    in proceedings
of naacl hlt, apr. 2013.

[4] t. mikolov, i. sutskever, k. chen, g. corrado, and j. dean,
   distributed representations of words and phrases and their
compositionality,    in nips 2013: advances in neural inform-
ation processing systems, oct. 2013.

[5] t. mikolov, k. chen, g. corrado, and j. dean,    ef   cient
estimation of word representations in vector space,    in
proceedings of workshop at iclr, jan. 2013.

[6] f. godin, b. vandersmissen, a. jalalvand, w. de neve, and
r. van de walle,    alleviating manual feature engineer-
ing for part-of-speech tagging of twitter microposts using
distributed word representations,    in workshop on modern
machine learning and natural language processing, nips
2014, oct. 2014.

[7] b. hu, z. lu, h. li, and q. chen,    convolutional neural
network architectures for matching natural language sen-
tences,    in nips 2014: advances in neural information
processing systems, 2014, pp. 2042   2050.

[8] j. weston, s. chopra, and k. adams,    #tagspace: semantic
embeddings from hashtags,    in proceedings of
the 2014
conference on empirical methods in natural language pro-
cessing (emnlp), 2014.

[9] c. n. dos santos and m. gatti,    deep convolutional neural
networks for id31 of short texts,    in coling
2014, the 25th international conference on computational
linguistics, dublin, jul. 2014, pp. 69   78.

[10] r. collobert, j. weston, l. bottou, m. karlen, k. kavukcuo-
glu, and p. kuksa,    natural language processing (almost)
from scratch,    the journal of machine learning research,
vol. 12, feb. 2011.

[11] l. kang, b. hu, x. wu, q. chen, and y. he,    a short
texts matching method using shallow features and deep
features,    in third ccf conference, nlpcc 2014, nov.
2014.

[12] x. zhang and y. lecun,    text understanding from scratch,   

arxiv.org, feb. 2015.

[13] q. v. le and t. mikolov,    distributed representations of

sentences and documents,    arxiv.org, may 2014.

[14] m. j. kusner, y. sun, n. i. kolkin, and k. q. weinberger,
   from id27s to document distances.    icml,
pp. 957   966, 2015.

[15] g. zheng and j. callan,    learning to reweight terms with
distributed representations,    in the 38th international acm
sigir conference. new york, new york, usa: acm press,
2015, pp. 575   584.

