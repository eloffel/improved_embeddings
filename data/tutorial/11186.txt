text matching as image recognition

liang pang   , yanyan lan   , jiafeng guo   , jun xu   , shengxian wan   , and xueqi cheng   

cas key laboratory of network data science and technology,

institute of computing technology, chinese academy of sciences, beijing 100190, china

   {pangliang,wanshengxian}@software.ict.ac.cn,    {lanyanyan,guojiafeng,junxu,cxq}@ict.ac.cn

6
1
0
2

 

b
e
f
0
2

 

 
 
]
l
c
.
s
c
[
 
 

1
v
9
5
3
6
0

.

2
0
6
1
:
v
i
x
r
a

abstract

matching two texts is a fundamental problem in many
natural language processing tasks. an effective way is
to extract meaningful matching patterns from words,
phrases, and sentences to produce the matching score.
inspired by the success of convolutional neural network
in image recognition, where neurons can capture many
complicated patterns based on the extracted elementary
visual patterns such as oriented edges and corners, we
propose to model text matching as the problem of image
recognition. firstly, a matching matrix whose entries
represent the similarities between words is constructed
and viewed as an image. then a convolutional neural
network is utilized to capture rich matching patterns in
a layer-by-layer way. we show that by resembling the
compositional hierarchies of patterns in image recogni-
tion, our model can successfully identify salient signals
such as id165 and n-term matchings. experimental re-
sults demonstrate its superiority against the baselines.

introduction

matching two texts is central to many natural language
applications, such as machine translation (brown et al.
1993), question and answering (xue, jeon, and croft 2008),
paraphrase identi   cation (socher et al. 2011) and docu-
ment retrieval (li and xu 2014). given two texts t1 =
(w1, w2, . . . , wm) and t2 = (v1, v2, . . . , vn), the degree of
matching is typically measured as a score produced by a
scoring function on the representation of each text:

match(t1, t2) = f(cid:0)  (t1),   (t2)(cid:1),

(1)

where wi and vj denotes the i-th and j-th word in t1 and
t2, respectively.    is a function to map each text to a vector,
and f is the scoring function for modeling the interactions
between them.

a successful matching algorithm needs to capture the rich
interaction structures in the matching process. taking the
task of paraphrase identi   cation for example, given the fol-
lowing two texts:
t1 : down the ages noodles and dumplings were famous

chinese food.

copyright c(cid:13) 2016, association for the advancement of arti   cial
intelligence (www.aaai.org). all rights reserved.

t2 : down the ages dumplings and noodles were popular in

china.

we can see that the interaction structures are of different
levels, from words, phrases to sentences. firstly, there are
many word level matching signals, including identical word
matching between    down    in t1 and    down    in t2, and sim-
ilar word matching between    famous    in t1 and    popular   
in t2. these signals compose phrase level matching sig-
nals, including id165 matching between    down the ages   
in t1 and    down the ages    in t2, unordered n-term match-
ing between    noodles and dumplings    in t1 and    dumplings
and noodles    in t2, and semantic n-term matching between
   were famous chinese food    in t1 and    were popular in
china    in t2. they further form sentence level matching sig-
nals, which are critical for determining the matching degree
of t1 and t2. how to automatically    nd and utilize these
hierarchical interaction patterns remains a challenging prob-
lem.

in image recognition, it has been widely observed that
the convolutional neural network (id98) (lecun et al. 1998;
simard, steinkraus, and platt 2003) can successfully abstract
visual patterns from raw pixels with layer-by-layer compo-
sition (girshick et al. 2014). inspired by this observation, we
propose to view text matching as image recognition and use
id98 to solve the above problem. speci   cally, we    rst con-
struct a word level similarity matrix, namely matching ma-
trix, to capture the basic word level matching signals. the
matching matrix can be viewed as: 1) a binary image if we
de   ne the similarity to be 0-1, indicating whether the two
corresponding words are identical; 2) a gray image if we de-
   ne the similarity to be real valued, which can be achieved
by calculating the cosine or inner product based on the word
embeddings. then we apply a convolutional neural network
on this matrix. meaningful matching patterns such as id165
and n-term can therefore be fully captured within this ar-
chitecture. we can see that our model takes text matching
as a multi-level abstraction of interaction patterns between
words, phrases and sentences, with a layer-by-layer archi-
tecture, so we name it matchpyramid.

the experiments on the task of paraphrase identi   cation
show that matchpyramid (with 0-1 matching matrix) out-
performs the baselines, by solely leveraging interactions be-
tween texts. while for other tasks such as paper citation
matching, where semantic is somehow important, match-

pyramid (with real-valued matching matrix) performs the
best by considering both interactions and semantic represen-
tations.

contributions of this paper include: 1) a novel view of
text matching as image recognition; 2) the proposal of a new
deep architecture based on the matching matrix, which can
capture the rich matching patterns at different levels, from
words, phrases, to the whole sentences; 3) experimental
analysis on different tasks to demonstrate the superior power
of the proposed architecture against competitor matching al-
gorithms.

motivation

it has been widely recognized that making a good match-
ing decision requires to take into account the rich interaction
structures in the text matching process, starting from the in-
teractions between words, to various matching patterns in
the phrases and the whole sentences. taking the aforemen-
tioned two sentences as an example, the interaction struc-
tures are of different levels, as illustrated in figure 1.

figure 1: an example of interaction structures in paraphrase
identi   cation.

word level matching signals refer to matchings be-
tween words in the two texts, including not only iden-
tical word matchings, such as    down   down   ,    the   the   ,
   ages   ages   ,    noodles   noodles   ,    and   and   ,   dumplings   
dumplings    and    were   were   , but also similar word match-
ings, such as    famous   popular    and    chinese   china   .

phrase level matching signals refer to matchings be-
tween phrases, including id165 and n-term. id165 match-
ing occurs with n exactly matched successive words, e.g.
   (down the ages)   (down the ages)   . while n-term matching
allows for order or semantic alternatives, e.g.    (noodles and
dumplings)   (dumplings and noodles)   , and    (were famous
chinese food)   (were popular in china)   .

sentence level matching signals refer to matchings be-
tween sentences, which are composed of multiple lower
level matching signals, e.g. the three successive phrase level
matchings mentioned above. when we consider matchings
between paragraphs that contain multiple sentences, the
whole paragraph will be viewed as a long sentence and the
same composition strategy would generate paragraph level
matching signals.

to sum up, the interaction structures are compositional
hierarchies, in which higher level signals are obtained by
composing lower level ones. this is similar to image recog-
nition. in an image, raw pixels provide basic units of the
image, and each patch may contain some elementary visual
features such as oriented edges and corners. local combina-
tions of edges form motifs, motifs assemble into parts, and
parts form objects. we give an example to show the relation-
ships between text matching and image recognition (jia et al.
2014), as illustrated in figure 2. in the area of image recog-
nition, id98 has been recognized as one the most successful

figure 3: an overview of matchpyramid on text matching.

way to capture different levels of patterns in image (zeiler
and fergus 2014). therefore, it inspires us to transform text
matching to image recognition and employ id98 to solve it.
however, the representations of text and image are so dif-
ferent that it remains a challenging problem to perform such
transformation.

matchpyramid

in this section we introduce a new deep architecture for
text matching, namely matchpyramid. the main idea comes
from modeling text matching as image recognition, by tak-
ing the matching matrix as an image, as illustrated in fig-
ure 3.
matching matrix: bridging the gap between text
matching and image recognition
as discussed before, one challenging problem by modeling
text matching as image recognition lies in the different rep-
resentations of text and image: the former are two 1d (one-
dimensional) word sequences while the latter is typically a
2d pixel grid. to address this issue, we represent the input
of text matching as a matching matrix m, with each ele-
ment mij standing for the basic interaction, i.e. similarity
between word wi and vj (see eq. 2). here for convenience,
wi and vj denotes the i-th and j-th word in two texts re-
spectively, and     stands for a general operator to obtain the
similarity.

(2)
in this way, we can view the matching matrix m as an im-
age, where each entry (i.e. the similarity between two words)
stands for the corresponding pixel value. we can adopt dif-
ferent kinds of     to model the interactions between two

mij = wi     vj.

figure 2: relationships between text matching and image recognition.

words, leading to different kinds of raw images. in this pa-
per, we give three examples as follows.

indicator function produces either 1 or 0 to indicate

whether two words are identical.

mij = i{wi=vj} =

if wi = vj
otherwise.

(3)

(cid:26) 1,

0,

one limitation of the indicator function is that it cannot
capture the semantic matching between similar words. to
tackle this problem, we de   ne     based on word embed-
dings, which will make the matrix more    exible to capture
semantic interactions. given the embedding of each word
(cid:126)  i =   (wi) and (cid:126)  j =   (vj), which can be obtained by
recent id97 (mikolov et al. 2013) technique, we in-
troduce the other two operators: cosine and dot product.

cosine views angles between word vectors as the similar-

ity, and it acts as a soft indicator function.

mij =

(cid:62) (cid:126)  j

(cid:126)  i

(cid:107) (cid:126)  i(cid:107)    (cid:107) (cid:126)  j(cid:107) ,

(4)

where (cid:107)    (cid:107) stands for the norm of a vector, and (cid:96)2 norm is
used in this paper.
dot product further considers the norm of word vectors,

as compared to cosine.

mij = (cid:126)  i

(cid:62) (cid:126)  j.

(5)

based on these three different operators, the matching ma-
trices of the given example are shown in fig 4. obviously
we can see that fig 4(a) corresponds to a binary image, and
fig 4(b) correspond to gray images.

hierarchical convolution: a way to capture rich
matching patterns
the body of matchpyramid is a typical convolutional neu-
ral network, which can extract different levels of matching
patterns. for the    rst layer of id98, the k-th kernel w(1,k)
scans over the whole matching matrix z(0) = m to generate
a feature map z(1,k):

(cid:19)

(cid:18)rk   1(cid:88)

rk   1(cid:88)

s=0

t=0

(a) indicator

(b) dot product

figure 4: three different matching matrices, where solid cir-
cles elements are all valued 0.

where rk denotes the size of the k-th kernel. in this paper
we use square kernel, and relu (dahl, sainath, and hinton
2013) is adopted as the active function   .

dynamic pooling strategy (socher et al. 2011) is then
used to deal with the text length variability. by applying dy-
namic pooling, we will get    xed-size feature maps:

z(2,k)
i,j = max
0   s<dk

max
0   t<d(cid:48)

k

z(1,k)
i  dk+s,j  d(cid:48)

k+t,

(7)

where dk and d(cid:48)
k denote the width and length of the corre-
sponding pooling kernel, which are determined by the text
lengths n and m, and output feature map size n(cid:48)    m(cid:48),
i.e. dk = (cid:100)n/n(cid:48)(cid:101), d(cid:48)
after the    rst convolution and dynamic pooling, we con-
tinue to obtain higher level features z(l), l     2 by further
convolution and max-pooling, with general formulations:

k = (cid:100)m/m(cid:48)(cid:101).

z(l+1,k(cid:48))

i,j

=  

w(l+1,k(cid:48))

s,t

  z(l,k)
i+s,j+t+b(l+1,k)

k=0

s=0
l = 2, 4, 6, . . . ,

t=0

z(l+1,k)
i,j

= max
0   s<dk

max
0   t<dk

z(l,k)
i  dk+s,j  dk+t,

l = 3, 5, 7, . . . ,

(8)

(9)

(cid:18)cl   1(cid:88)

rk   1(cid:88)

rk   1(cid:88)

(cid:19)

,

z(1,k)
i,j =   

w(1,k)

s,t

   z(0)

i+s,j+t + b(1,k)

,

(6)

where cl denote the number of feature maps in the l-th layer.

analysis of hierarchical convolution

figure 5: an illustration of hierarchical convolution.

similar to id98 in image recognition where it can make
abstractions based on extracted elementary visual patterns
such as oriented edges and corners, the hierarchical convo-
lution in matchpyramid can also capture important phrase
level interactions from word level matching and make fur-
ther compositions. we revisit our example, and show how it
works1, as illustrated in figure 5.

(1) with the given two kernels, we can see clearly that
the    rst convolutional layer can capture both id165 match-
ing signals    (down the ages)   (down the ages)    and n-term
matching signal    (noodles and dumplings)   (dumplings and
noodles)   . the extracted matching patterns are like edges in
image recognition (refer figure 2).

(2) the following convolutional layers make composi-
tions and form higher level of matching patterns. for exam-
ple, from the second layer, we can see that a more compli-
cated    t-type    pattern captured with the given 3d kernel. it
looks like some motif (parts) obtained in image recognition
(refer figure 2).

from the above analysis we can see that matchpyramid
can abstract complicated matching patterns, from phrase to
sentence level, by hierarchical convolution.

matching score and training
we use a mlp (multi-layer perception) to produce the    -
nal matching score. take binary classi   cation and two-layer
id88 for example, we will obtain a 2-dimensional
matching score vector:

(s0, s1)(cid:62) =w2  (cid:0)w1z + b1

(cid:1) + b2,

(10)

where s0 and s1 are the matching scores of the correspond-
ing class, z is the output of the hierarchical convolution, wi
is the weight of the i-th mlp layer and    denotes the activa-
tion function.

softmax function is utilized to output the id203 of
belonging to each class, and cross id178 is used as the
objective function for training. therefore the optimization

1here we take the matching matrix with indicator function as
example, similar observations can be obtained for other matching
matrix with cosine similarity and dot product.

becomes minimizing:

(cid:104)
loss =     n(cid:88)

y(i) log(p(i)

1 )+(1     y(i)) log(p(i)
0 )

(cid:105)

,

(11)

i=1
esk

es0 + es1

pk =

,

k = 0, 1,

where y(i) is the label of the i-th training instance. the
optimization is relatively straightforward with the standard
back-propagation (williams and hinton 1986). we apply
stochastic id119 method adagrad (duchi, hazan,
and singer 2011) for the optimization of models. it performs
better when we use the mini-batch strategy (32   50 in size),
which can be easily parallelized on single machine with
multi-cores. for id173, we    nd that some common
strategies like early stopping (giles 2001) and dropout (hin-
ton et al. 2012) are enough for our model.

experiments

in this section, we conduct experiments on two tasks, i.e.
paraphrase identi   cation and paper citation matching, to
demonstrate the superiority of matchpyramid against base-
lines.

competitor methods and experimental settings
allpositive: all of the test data are predicted as positive.
tf-idf: tf-idf (salton, fox, and wu 1983) is a widely
used method in id111. in this method, each text is
represented as a |v |-dimensional vector with each element
stands for the tf-idf score of the corresponding word in the
text, where |v | is the vocabulary size. in this paper, idf score
is calculated in the whole dataset. the    nal matching score
is produced by the inner product of the two vectors.

dssm/cdssm: since dssm (huang et al. 2013) and
cdssm (gao et al. 2014; shen et al. 2014) need large data
for training, we directly use the released models2 (trained on
large click-through dataset) on our test data.

2http://research.microsoft.com/en-us/downloads/731572aa-

98e4-4c50-b99d-ae3f0c9562b9/

arc-i/arc-ii: we implement arc-i and arc-ii (hu
et al. 2014) due to there is no publicly available codes, using
exactly the same setting as described in the original paper.

there are three versions of matchpyramid, depending on
different methods used for constructing the matching ma-
trices, denoted as mp-ind, mp-cos, and mp-dot, re-
spectively. all these models use two convolutional layers,
two max-pooling layers (one of which is a dynamic pooling
layer for variable length) and two full connection layers. the
number of feature maps is 8 and 16 for the    rst and second
convolutional layer, respectively. while the kernel size is set
to be 5    5 and 3    3, respectively. unlike arc-ii which
initiates with id97 trained on wikipedia, we initi-
ate the word vectors in mp-cos and mp-dot randomly
from a unit ball. thus our model do not require any external
sources.

experiment i: paraphrase identi   cation
paraphrase identi   cation aims to determine whether two
sentences have the same meaning, a problem considered as a
touchstone of natural language understanding. here we use
the benchmark msrp dataset (dolan and brockett 2005),
which contains 4076 instances for training and 1725 for test-
ing. the experimental results are listed in table 1. we can

table 1: results on msrp.

model
allpositive
tf-idf
dssm
cdssm
arc-i
arc-ii
mp-ind
mp-cos
mp-dot

acc.(%) f1(%)
79.87
77.62
80.96
80.42
80.27
80.91
82.66
82.45
83.01

66.50
70.31
70.09
69.80
69.60
69.90
75.77
75.13
75.94

see that traditional simple model such as tf-idf has already
achieved a high accuracy of about 70%, though it only uses
the unigram matching signals. our methods performs much
better than tf-idf, which indicates that the complicated
matching patterns captured by hierarchical convolution are
important to the text matching task. for the comparison with
recent deep models, we can see that dssm performs better
than the others (though the improvement is quite limited),
and our models (mp-ind, mp-cos and mp-dot) outper-
form all of them. though the best performance of our model
(75.94%/83.01%) is still slightly worse than urae (socher
et al. 2011) (76.8%/83.6%), urae relies heavily on pre-
training with an external large dataset annotated with parse
tree information. in the future work, we will study how to
utilize external data to further improve our models.

we also visualize what we have learned in matchpyra-
m , with expectation that we can gain some insights from

3here we only demonstrate the case of mp-dot due to space
limitation. similar results can be observed with mp-ind and mp-
cos.

the process. speci   cally, we take a pair of texts as an ex-
ample (selected from the msrp dataset), and illustrate the
feature maps and kernels in figure 6. we can see that id165
and n-term matching, which are emphasized in the blue and
yellow color in original texts, are represented as a diago-
nal sub-matrix emphasized with the blue and yellow rectan-
gles in the matching matrix, respectively. kernel 1 and ker-
nel 2 are the two kernels learned in the    rst convolutional
layer, which well captures the important id165 and n-term
matching signals respectively. we can see that these patterns
are quite similar to the edge extracted by id98 in image
recognition (see figure 2). we also give some more kernels
and show some patterns learned in the second convolutional
layer. we can see that the latter layer make compositions and
keep the useful matching signals until passing it to the mlp
classi   er. this explains clearly why our model works well:
matchpyramid captures useful matching patterns at differ-
ent levels, from words, phrase, to sentences, with a similar
process in image recognition.

experiment ii: paper citation matching
we evaluate the effectiveness of matchpyramid with another
text matching task called paper citation matching, based on
a large academic dataset4. basically, we are given a set of
papers along with their abstracts. a paper and its citations   
abstracts then becomes a pair of texts, and de   ned as a type
of matching. one representative example is given as follows:
t1 : this article describes pulsed thermal time of    ight ttof
   ow sensor system as two subsystems pulsed wire sys-
tem and heat    ow system the entire    ow sensor is re-
garded system theoretically as linear.

t2 : the authors report on novel linear time invariant lti
modeling of    ow sensor system based on thermal time
of    ight tof principle by using pulsed hot wire anemom-
etry thermal he at pulses.

we can see that the matching here should take both lexi-
cal and semantic information into consideration. the dataset
is collected from a commercial academic website. it con-
tains 838 908 instances (text pairs) in total, where there are
279 636 positive (matched) instances and 559 272 negative
(mismatch) instances. the negative instances are randomly
sampled papers which have no citation relations. we split the
whole dataset into three parts, 599 196 instances for training,
119 829 for validation and 119 883 for testing.

the results in table 2 show that tf-idf is also a strong
baseline on this dataset, which is even better than some deep
models such as dssm and cdssm. this may be caused by
the large difference between the testing data (paper citation
data) and training data (click-through data) used in dssm
and cdssm. arc-i and arc-ii gain a signi   cant improve-
ment over these models, which may bene   t much from the
large training data. as for our models, the best performance
is still achieved by mp-dot (88.73%/82.86%), which is
better than arc-ii (86.84%/79.57%). mp-cos also gains
a better result than arc-ii. the reason of the poor perfor-
mance of mp-ind on this task may lie in that the indicator

4we only use the    rst 32 words in the abstract.

figure 6: analysis of the feature maps and kernels in the matchpyramid model. the brighter the pixel is, the larger value it has.
better viewed in color.

table 2: results on the task of paper citation matching.

model
allpositive
tf-idf
dssm
cdssm
arc-i
arc-ii
mp-ind
mp-cos
mp-dot

acc.(%) f1(%)
50.00
70.21
29.88
19.97
76.79
79.57
44.71
79.70
82.86

33.33
82.63
71.97
69.84
84.51
86.48
73.76
86.65
88.73

function only captures the exact matching between words,
but omits the semantic similarity.

table 3: the norm of learned id27s on the task
of paper citation matching.
with
0.508
java
1.576

for
0.509
0.510
snakes musical
1.610
1.589

the
0.448
robotics
1.572

word
len
word
len

are
0.515
r   d
1.878

be

we further show the reason why mp-dot performs better
than mp-cos by analyzing the learned id27s.
speci   cally, we pick some words with large and small norm,
listed in table 3. we can see that most words with small
norm are indeed useless for matching, while most words
with large norm (such as robotics and java) are domain
terms which play an important role in paper citation match-
ing. by further considering the importance of words, mp-
dot can capture more semantic information than mp-cos
and thus achieve better performance.

related work

most previous work on text matching tries to    nd good rep-
resentations for a single text, and usually use a simple scor-
ing function to obtain the matching results. examples in-
clude partial least square (wu, li, and xu 2013), canoni-
cal correlation analysis (hardoon and shawe-taylor 2003)

and some deep models such as dssm (huang et al. 2013),
cdssm (gao et al. 2014; shen et al. 2014) and arc-i (hu
et al. 2014).

recently, a brand new approach focusing on modeling the
interaction between two sentences has been proposed and
gained much attention, examples include deepmatch (lu
and li 2013), urae (socher et al. 2011) and arc-ii (hu
et al. 2014). our model falls into this category, thus we give
some detailed discussions on the differences of our model
against these methods.

deepmatch uses topic model to construct the interac-
tions between two texts, and then make different levels of
abstractions by a hierarchical architecture based on the rela-
tionships between topics. compared with our matching ma-
trix de   ned at word level, deepmatch uses topic informa-
tion with more rough granularity. moreover, it relies largely
on the quality of learned topic model, and the hierarchies are
usually ambiguous since the relationships between topics are
not absolute. on the contrary, matchpyramid clearly models
the interactions at different levels, from words, phrases to
sentences.

urae constructs the interactions between two texts
based on the syntactic trees, thus it relies on a prede   ned
compact vectorial representation of text. speci   cally, urae
   rst learns the representation of each node on the tree by a
auto-encoder, then directly inserts different levels of inter-
action, such as word, prase and sentence, to a single matrix.
different from that, our matchpyramid is end-to-end, and
captures different levels of interactions in a hierarchical way.
arc-ii and arc-i are both proposed based on convolu-
tional sentence model did98 (kalchbrenner, grefenstette,
and blunsom 2014). different from arc-i which defers the
interaction of two texts to the end of the process, arc-
ii lets them meet early by directly interleaving them to a
single representation, and makes abstractions on this basis.
therefore, arc-ii is capturing sentence level interactions
directly. however, it is not clear what exactly the interac-
tions are, since they used a sum operation. our model is also
based on a convolutional neural network, but the idea is quite
different from that of arc-ii. it is clear that we start from
word level matching patterns, and compose to phrase and
sentence level matching pattern layer by layer.

conclusion

in this paper, we view text matching as image recognition,
and propose a new deep architecture, namely matchpyra-
mid. our model can automatically capture important match-
ing patterns such as unigram, id165 and n-term at different
levels. experimental results show that our model can out-
perform baselines, including some recently proposed deep
matching algorithms.

acknowledgments

this work was funded by 973 program of china under
grants no. 2014cb340401 and 2012cb316303, 863 pro-
gram of china under grants no. 2014aa015204, the na-
tional natural science foundation of china (nsfc) un-
der grants no. 61472401, 61433014, 61425016, 61425016,
and 61203298, key research program of the chinese
academy of sciences under grant no. kgzd-ew-t03-2,
and youth innovation promotion association cas under
grants no. 20144310.

references

[brown et al. 1993] brown, p. f.; pietra, v. j. d.; pietra, s.
a. d.; and mercer, r. l. 1993. the mathematics of statis-
tical machine translation: parameter estimation. computa-
tional linguistics 19(2):263   311.
[dahl, sainath, and hinton 2013] dahl, g. e.; sainath, t. n.;
and hinton, g. e. 2013. improving deep neural networks for
in acoustics,
lvcsr using recti   ed linear units and dropout.
speech and signal processing (icassp), 2013 ieee interna-
tional conference on, 8609   8613. ieee.
[dolan and brockett 2005] dolan, w. b., and brockett, c.
2005. automatically constructing a corpus of sentential para-
phrases. in proc. of iwp.
[duchi, hazan, and singer 2011] duchi, j.; hazan, e.; and
singer, y. 2011. adaptive subgradient methods for online
learning and stochastic optimization. the journal of machine
learning research 12:2121   2159.
[gao et al. 2014] gao, j.; pantel, p.; gamon, m.; he, x.;
deng, l.; and shen, y. 2014. modeling interestingness with
deep neural networks. in proceedings of the 2013 conference
on empirical methods in natural language processing.
[giles 2001] giles, r. c. s. l. l. 2001. over   tting in neu-
ral nets: id26, conjugate gradient, and early stop-
ping. in advances in neural information processing systems
13: proceedings of the 2000 conference, volume 13, 402.
mit press.
[girshick et al. 2014] girshick, r.; donahue, j.; darrell, t.;
and malik, j. 2014. rich feature hierarchies for accurate
id164 and semantic segmentation. in computer vi-
sion and pattern recognition (cvpr), 2014 ieee conference
on, 580   587. ieee.
[hardoon and shawe-taylor 2003] hardoon, d. r.,
and
shawe-taylor, j. 2003. kcca for different level precision
in proceedings of third
in content-based id162.
international workshop on content-based multimedia
indexing, irisa, rennes, france.

[hinton et al. 2012] hinton, g. e.; srivastava, n.; krizhevsky,
a.; sutskever, i.; and salakhutdinov, r. 2012. improving neu-
ral networks by preventing co-adaptation of feature detectors.
corr abs/1207.0580.
[hu et al. 2014] hu, b.; lu, z.; li, h.; and chen, q. 2014.
convolutional neural network architectures for matching nat-
ural language sentences. in advances in neural information
processing systems, 2042   2050.
[huang et al. 2013] huang, p.-s.; he, x.; gao, j.; deng, l.;
acero, a.; and heck, l. 2013. learning deep structured
semantic models for web search using clickthrough data.
in proceedings of the 22nd acm international conference
on conference on information and knowledge management,
2333   2338. acm.
[jia et al. 2014] jia, y.; shelhamer, e.; donahue, j.; karayev,
s.; long, j.; girshick, r.; guadarrama, s.; and darrell, t.
2014. caffe: convolutional architecture for fast feature em-
bedding. arxiv preprint arxiv:1408.5093.
[kalchbrenner, grefenstette, and blunsom 2014]
kalchbrenner, n.; grefenstette, e.; and blunsom, p. 2014. a
convolutional neural network for modelling sentences. corr
abs/1404.2188.
[lecun et al. 1998] lecun, y.; bottou, l.; bengio, y.; and
haffner, p. 1998. gradient-based learning applied to doc-
ument recognition. proceedings of the ieee 86(11):2278   
2324.
[li and xu 2014] li, h., and xu, j. 2014. semantic matching
in search. foundations and trends in information retrieval
7(5):343   469.
[lu and li 2013] lu, z., and li, h. 2013. a deep architecture
for matching short texts. in advances in neural information
processing systems, 1367   1375.
[mikolov et al. 2013] mikolov, t.; chen, k.; corrado, g.; and
dean, j. 2013. ef   cient estimation of word representations
in vector space. corr abs/1301.3781.
[salton, fox, and wu 1983] salton, g.; fox, e. a.; and wu,
h. 1983. extended boolean information retrieval. communi-
cations of the acm 26(11):1022   1036.
[shen et al. 2014] shen, y.; he, x.; gao, j.; deng, l.; and
mesnil, g. 2014. a latent semantic model with convolutional-
pooling structure for information retrieval. in proceedings of
the 23rd acm international conference on conference on in-
formation and knowledge management, 101   110. acm.
y.;
[simard, steinkraus, and platt 2003] simard,
steinkraus, d.; and platt, j. c.
best practices
2003.
for convolutional neural networks applied to visual document
in 2013 12th international conference on docu-
analysis.
ment analysis and recognition, volume 2, 958   958.
ieee
computer society.
[socher et al. 2011] socher, r.; huang, e. h.; pennin, j.;
manning, c. d.; and ng, a. y. 2011. dynamic pooling
and unfolding recursive autoencoders for paraphrase detec-
tion. in advances in neural information processing systems,
801   809.
[williams and hinton 1986] williams, d. r. g. h. r., and
hinton, g.
learning representations by back-
propagating errors. nature 323   533.

1986.

p.

[wu, li, and xu 2013] wu, w.; li, h.; and xu, j.
2013.
learning query and document similarities from click-through
in proceedings of the sixth
bipartite graph with metadata.
acm international conference on wsdm, 687   696. acm.
[xue, jeon, and croft 2008] xue, x.; jeon, j.; and croft,
2008. retrieval models for question and answer
w. b.
in proceedings of the 31st annual international
archives.
acm sigir conference on research and development in in-
formation retrieval, 475   482. acm.
[zeiler and fergus 2014] zeiler, m. d., and fergus, r. 2014.
visualizing and understanding convolutional networks.
in
id161   eccv 2014. springer. 818   833.

