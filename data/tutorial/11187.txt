empath: understanding topic signals in large-scale text

ethan fast, binbin chen, michael s. bernstein

{ethan.fast, msb}@cs.stanford.edu, bchen45@stanford.edu

stanford university

6
1
0
2

 

b
e
f
2
2

 

 
 
]
l
c
.
s
c
[
 
 

1
v
9
7
9
6
0

.

2
0
6
1
:
v
i
x
r
a

figure 1. empath analyzes text across 200 gold standard topics and emotions (e.g., childishness or violence), and can generate and validate new lexical
categories on demand from a user-generated set of seed terms. the empath web interface highlights category counts for the current document (right).

abstract
human language is colored by a broad range of topics, but
existing text analysis tools only focus on a small number of
them. we present empath, a tool that can generate and val-
idate new lexical categories on demand from a small set of
seed terms (like    bleed    and    punch    to generate the cate-
gory violence). empath draws connotations between words
and phrases by deep learning a neural embedding across more
than 1.8 billion words of modern    ction. given a small set of
seed words that characterize a category, empath uses its neu-
ral embedding to discover new related terms, then validates
the category with a crowd-powered    lter. empath also an-
alyzes text across 200 built-in, pre-validated categories we
have generated from common topics in our web dataset, like
neglect, government, and social media. we show that em-
path   s data-driven, human validated categories are highly cor-
related (r=0.906) with similar categories in liwc.

permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for pro   t or commercial advantage and that copies bear this notice and the full cita-
tion on the    rst page. copyrights for components of this work owned by others than
acm must be honored. abstracting with credit is permitted. to copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speci   c permission
and/or a fee. request permissions from permissions@acm.org. chi   16, may 07-12,
2016, san jose, ca, usa 2016 acm. isbn 978-1-4503-3362-7/16/05$15.00 doi:
http://dx.doi.org/10.1145/2858036.2858535

author keywords
social computing, computational social science,    ction

acm classi   cation keywords
h.5.2. information interfaces and presentation: group and
organization interfaces

introduction
language is rich in subtle signals. the previous sentence, for
example, conveys connotations of wealth (   rich   ), cleverness
(   subtle   ), communication (   language   ,    signals   ), and pos-
itive sentiment (   rich   ). a growing body of work in human-
computer interaction, computational social science and social
computing uses tools to identify these signals: for example,
detecting emotional contagion in status updates [19], linguis-
tic correlates of deception [31], or conversational signs of be-
trayal [30]. as we gain access to ever larger and more diverse
datasets, it becomes important to scale our ability to conduct
such analyses with breadth and accuracy.
high quality lexicons allow us to analyze language at scale
and across a broad range of signals. for example, researchers
often use liwc (linguistic inquiry and word count) to ana-
lyze social media posts, counting words in lexical categories
like sadness, health, and positive emotion [33]. liwc of-
fers many advantages: it is fast, easy to interpret, and exten-
sively validated. researchers can easily inspect and modify

figure 2. empath learns id27s from 1.8 billion words of    ction, makes a vector space from these embeddings that measures the similarity
between words, uses seed terms to de   ne and discover new words for each of its categories, and    nally    lters its categories using crowds.

the terms in its categories     word lists that, for example, re-
late    scream    and    war    to the emotion anger. but like other
popular lexicons, liwc is small: it has only 40 topical and
emotional categories, many of which contain fewer than 100
words. further, many potentially useful categories like vio-
lence or social media don   t exist in current lexicons, requir-
ing the ad hoc curation and validation of new gold standard
word lists. other categories may bene   t from updating with
modern terms like    paypal    for money or    sel   e    for leisure.
to address these problems, we present empath: a living
lexicon mined from modern text on the web. empath al-
lows researchers to generate and validate new lexical cate-
gories on demand, using a combination of deep learning and
id104. for example, using the seed terms    twitter   
and    facebook,    we can generate and validate a category for
social media. empath also analyzes text across 200 built-
in, pre-validated categories drawn from existing knowledge
bases and literature on human emotions, like neglect (deprive,
refusal), government (embassy, democrat), strength (tough,
forceful), and technology (ipad, android). empath combines
modern nlp techniques with the bene   ts of handmade lexi-
cons: its categories are transparent word lists, easily extended
and fast. and like liwc (but unlike other machine learning
models), empath   s contents are validated by humans.
while empath presents an approach that can be trained on any
text corpora, in this paper we use 1.8 billion words of modern
amateur    ction. why would    ction be the right tool to train an
externally-valid measure of topical and emotional categories?
general web text suffers from sparsity when learning cate-
gories focused on the human internal states (e.g., remorse)
or the physical world, for example connecting    circular    and
   boxy    to the topic shape and size [16, 10]. on the other
hand, amateur    ction tends to be explicit about both scene-
setting and emotion, with a higher density of adjective de-
scriptors (e.g.,    the broken vending machine perplexed her.   ).
fiction is    lled with emotion and description     it is what
gives novels their appeal.
to build empath, we extend a deep learning skip-gram net-
work to capture words in a neural embedding [23]. this em-
bedding learns associations between words and their context,
providing a model of connotation. we can then use similarity
comparisons in the resulting vector space to map a vocabu-
lary of 59,690 words onto empath   s 200 categories (and be-
yond, onto user-de   ned categories). for example, the word
   self-harming    shares high cosine similarity with the cate-
gories depressed and pain. finally, we demonstrate how we
can    lter these relationships through the crowd to ef   ciently
construct new, human validated dictionaries.

we show how the open-ended nature of empath   s model can
replicate and extend classic work in classifying deceptive lan-
guage [31], identifying the patterns of language in movie re-
views [32], and analyzing mood on twitter [13]. for example,
empath reveals that 13 emotional categories are elevated in
the language of liars, suggesting a novel result that liars tend
to use more evocative language. in the movie review dataset,
we    nd positive reviews are more strongly connected with
intellectual categories like philosophy, politics, and law.
our evaluation validates empath by comparing its analyses
against liwc, a lexicon of gold standard categories that have
been psychometrically validated. we    nd the correlation be-
tween empath and liwc across a mixed-corpus dataset is
high (r=0.906), and remains high even without the crowd    l-
ter (0.90), which suggests empath   s data-driven word counts
are very similar to those made by a heavily validated dic-
tionary. when we instead train empath on the 100 billion
word google news corpus, its unsupervised model shows less
agreement (0.84) with liwc. in sum, empath shares high
correlation with gold standard lexicons, yet it also offers anal-
yses over a broad and dynamic set of categories.
this paper   s contributions include:
    empath: a text analysis tool that allows users to construct
and validate new categories on demand using a few seed
terms. it also covers a broad, pre-validated set of 200 emo-
tional and topical categories.
    an approach to generating and validating word classi   ca-
tion dictionaries using a combination of deep learning and
microtask id104.
    results that suggest empath can generate categories ex-
tremely similar to categories that have been hand-tuned and
psychometrically validated by humans (average pearson
correlation of 0.906), even without a crowd    lter (0.90).

related work
empath inherits from a rich ecosystem of tools and applica-
tions for text analysis, and draws on the insights of prior work
in data mining and unsupervised id38.

extracting signal from text
text analysis via dictionary categories has a long history in
academic research. liwc, for example, is an extensively
validated dictionary that offers a total of 62 syntactic (e.g.,
present tense verbs, pronouns), topical (e.g., home, work,
family) and emotional (e.g., anger, sadness) categories [33].
the general inquirer (gi) is another human curated dictio-
nary that operates over a broader set of topics than liwc

figure 3. example counts over four example categories on sample tweets from our validation dataset. words in each category are discovered by
unsupervised id38, and then validated by crowds.

(e.g., power, weakness), but fewer emotions [40]. other tools
like emolex, anew, and sentiid138 are designed to an-
alyze larger sets of emotional categories [27, 3, 9]. while
empath   s analyses are similarly driven by dictionary-based
word counts, empath operates over a more extensive set of
categories, and can generate and validate new categories on
demand using unsupervised id38.
work in id31, in combination with deep learn-
ing, has developed powerful techniques to classify text across
positive and negative polarity [39], but has also bene   ted
from simpler, transparent models and rules [15]. empath
draws on the complementary strengths of these ideas, using
the power of unsupervised deep learning to create human-
interpretable feature sets for the analysis of text. one of em-
path   s goals is to embed modern nlp techniques in a way
that offers the transparency of dictionaries like liwc.

applications for text analysis
as social computing and computational social science re-
searchers have gained access to large textual datasets, they
have increasingly adopted analyses that cover a wide range of
textual signal. for example, researchers have investigated the
public   s response to major holidays and news events [2], how
conversational partners mirror each others [18], the topical
and emotional content of blogs [27, 16, 29], and whether one
person   s writing may in   uence her friends when she posts to
social media like facebook [19] or twitter [7]. each of these
analyses builds a model of the categories that represent their
constructs of interest, or uses a word-category dictionary such
as liwc. through empath, we aim to empower researchers
with the ability to generate and validate these categories.
other work in human-computer interaction has relied upon
text analysis tools to build new interactive systems. for ex-
ample, researchers have automatically generated audio tran-
sitions for interviews, cued by signals of mood in the tran-
scripts [34], dynamically generated soundtracks for novels
using an emotional lexicon [6], or mapped ambiguous nat-
ural language onto its visual meaning [12]. empath   s ability
to generate lexical categories on demand potentially enables
new interactive systems, cued on nuanced emotional signals
like jealousy, or diverse topics that    t the new domain.

data mining and modeling
a large body of prior work has investigated unsupervised
id38. for example, researchers have learned
sentiment models from the relationships between words [14],
classi   ed the polarity of reviews in an unsupervised fashion

[43], discovered patterns of narrative in text [4], and (more
recently) used neural networks to model word meanings in
a vector space [23, 24]. we borrow from the last of these
approaches in constructing of empath   s unsupervised model.
empath also takes inspiration from techniques for mining hu-
man patterns from data. augur likewise mines    ction, but it
does so to learn human activities for interactive systems [10].
augur   s evaluation indicated that with regard to low-level be-
haviors such as actions,    ction provides a surprisingly accu-
rate mirror of human behavior. empath contributes a different
perspective, that    ction can be an appropriate tool for learn-
ing a breadth of topical and emotional categories, to the ben-
e   t of social science. in other research communities, systems
have used unsupervised models to capture emergent practice
in open source code [11] or design [20]. in empath, we adapt
these techniques to mine natural language for its relation to
emotional and topical categories.
finally, empath also bene   ts from prior work in common-
sense id99. existing databases of lin-
guistic and commonsense knowledge provide networks of
facts that computers should know about the world [21, 25, 9].
we draw on some of this knowledge, like the conceptnet hi-
erarchy, when seeding empath   s categories. further, empath
itself captures a set of relations on the topical and emotional
connotations of words. some aspects of these connotations
may be mineable from social media, if they are of the sort
that people are likely to advertise on twitter [17]. we    nd
that    ction offers a richer source of affective signal.

empath applications
to motivate the opportunities that empath creates, we    rst
present three example analyses that illustrate its breadth and
   exibility. in general, empath allows researchers to perform
text analyses over a broader set of topical and emotional cate-
gories than existing tools, and also to create and validate new
categories on demand. following this section, we explain the
techniques behind empath   s model in more detail.

example 1: understanding deception in hotel reviews
what kinds of words accompany our lies? in our    rst exam-
ple, we use empath to analyze a dataset of deceptive hotel
reviews reported previously by ott el al. [31]. this dataset
contains 3200 truthful hotel reviews mined from tripadvi-
sor.com and deceptive reviews created by workers on ama-
zon mechanical turk, split among positive and negative rat-
ings. the original study found that liars tend to write more

figure 4. deceptive reviews convey stronger sentiment across both pos-
itively and negatively charged categories. in contrast, truthful reviews
show a tendency towards more mundane activities and physical objects.

figure 5. positive reviews are associated with the deeper organizing
principals of human life, like politics, philosophy or law. negative re-
views show metaphorical connections to animals, cleaning, and smells.

imaginatively, use less concrete language, and incorporate
less spatial information into their lies.

tional categories. truthful reviews, on the other hand, display
higher odds ratios for none of empath   s emotional categories.

   5).

exploring the deception dataset
we ran empath   s full set of categories over the truthful and
deceptive reviews, and produced aggregate statistics for each.
using normalized means of the category counts for each
group, we then computed odds ratios and p-values for the
categories most likely to appear in deceptive and truthful re-
views. all the results we report are signi   cant after a bonfer-
roni correction (   = 2.5e
our results provide new evidence in support of the ott et
al. study, suggesting that deceptive reviews convey stronger
sentiment across both positively and negatively charged cat-
egories, and tend towards exaggerated language (figure 4).
the liars more often use language that is tormented (2.5 odds)
or joyous (2.3 odds), for example    it was torture hearing the
sounds of the elevator which just would never stop    or    i got
a great deal and i am so happy that i stayed here.    the truth-
tellers more often discuss concrete ideas and phenomena like
the ocean (1.6 odds,), vehicles (1.7 odds) or noises (1.7 odds),
for example    it seemed like a nice enough place with rea-
sonably close beach access    or    they took forever to valet
our car.    we see a tendency towards more mundane activi-
ties among the truth-tellers through categories like eating (1.3
odds), cleaning (1.3 odds), or hygiene (1.2 odds).    i ran the
shower for ten minutes without ever receiving any hot water.   
for the liars interactions seem to be more evocative, involv-
ing death (1.6 odds) or partying (1.3 odds).    the party that
keeps you awake will not be your favorite band practicing for
their next concert.   
for exploratory research questions, empath provides a high-
level view over many potential categories, some of which a
researcher may not have thought to investigate. lying ho-
tel reviewers, for example, may not have realized they give
themselves away by    xating on smell (1.4 odds),    the room
was pungent with what smelled like human excrement   , or
their systematic overuse of emotional terms, producing sig-
ni   cantly higher odds ratios for 13 of empath   s 32 emo-

spatial language in lies
while the original study provided some evidence that liars
use less spatially descriptive language, it wasn   t able to test
the theory directly. using empath, we can generate a new
set of human validated terms that capture this idea, creating
a new spatial category. to do so, we tell empath to seed
the category with the terms    big   ,    small   , and    circular   .
empath then discovers a series of related terms and uses the
crowd to validate them, producing the cluster:

circular, small, big, large, huge, gigantic, tiny, rectangular,
rectangle, massive, giant, enormous, smallish, rounded, mid-
dle, oval, sized, size, miniature, circle, colossal, center, trian-
gular, shape, boxy, round, shaped, decorative, ...

when we then add the new spatial category to our analysis,
we    nd it favors truthful reviews by 1.2 odds (p < 0.001).
truth-tellers use more spatial language, for example,    the
room that we originally were in had a huge square cut out
of the wall that had exposed pipes, bricks, dirt and dust.    in
aggregate, liars are not as apt in these concrete details.

example 2: understanding language in movie reviews
what kinds of movies do reviewers enjoy? what words do
reviewers use to praise or pan them? in our second example,
we show how empath can help us discover trends in a dataset
of movie reviews collected by pang et al. [32]. this dataset
contains 2000 movie reviews, divided evenly across positive
and negative sentiment.

exploring the movie dataset
the movie review dataset reveals, unsurprisingly, a strong
correlation between negative reviews and negatively charged
categories (figure 5). for instance, uglyness is 1.4 times
more likely to appear in a negative review, swear words are
1.3 times more likely, and pain is 1.3 times more likely. for
example,    oh bacon glistens when he gets wet all right and

the original paper shows a low of negative sentiment in the
morning that rises over the rest of the day. we    nd a similar
relationship on our data with both empath and liwc: a low
in the morning (around 8am), peaking to a high around 11pm.
the signals reported by empath and liwc over each hour are
strongly correlated (r=0.90). using a 1-way anova to test
for changes in mean negative affect by hour, empath reports
a highly signi   cant difference (f(23, 591520) = 17.2, p <
0.001), as does liwc (f = 6.8, p < 0.001). for positive
sentiment, empath and liwc again replicate similarly with
strong correlation between tools (r=0.87). both tools once
more report highly signi   cant anovas by hour: empath f =
5.9, p < 0.001; liwc f = 7.3, p < 0.001.

empath
empath analyzes text across hundreds of topics and emotions.
like liwc and other dictionary-based tools, it counts cate-
gory terms in a text document. however, empath covers a
broader set of categories than other tools, and users can gen-
erate and validate new categories with a few seed words.

designing empath   s data-driven categories
when analyzing textual data, researchers collectively engage
with many possible linguistic categories. for example, so-
cial scientists study the networks of conversations that sur-
round depression on twitter [38], psychologists the role of
self-presentation in online dating communities [42], or digi-
tal humanists the role of femininity in greek literature [44].
empath aims to make possible all of these analyses (and
more) through its 200 human validated categories, which
cover topics like violence, depression, or femininity. where
do the names of these categories come from? we adopt a
data-driven approach using the conceptnet knowledge base
[21]. the dependency relationships in conceptnet provide a
hierarchy of information and facts that act as a source of cat-
egory names and seed words for empath (e.g., war is a form
of con   ict, running is a form of exercise). we prefer this ap-
proach to a purely manual one as it can potentially scale to
thousands of other new categories.
for example, when a researcher provides    shirt    and    hat    as
seed words, conceptnet tells us shirts and hats are articles
of clothing. so, empath can create and validate a clothing
category, using    shirt    and    hat    as seed words:

blazer, vest, sweater, sleeveless, blouse, plaid, tights, under-
shirt, wearing, jacket, buttondown, longsleeve, skirt, singlet,
buttonup, longsleeved, hoody, tanktop, leggings, ...

speci   cally, to generate empath   s category names and seed
terms, we selected 200 common dependency relationships in
conceptnet, conditioned on 10,000 common words in our
corpus. we then manually re   ned this list, eliminating re-
dundant or sparse categories. for some categories we added
additional seed terms to better represent the concept, resulting
in a    nal set of two to    ve seed terms for each category.
for emotional analyses, empath likewise draws upon the hi-
erarchy of emotions introduced by parrott [36], in which emo-
tions are de   ned by other emotions. for example, lust is
de   ned by    desire   ,    passion   , and    infatuation   , so these

figure 6. we use empath to replicate the work of golder and macy,
investigating how mood on twitter relates to time of day. the signals
reported by empath and liwc by hour are strongly correlated for pos-
itive (r=0.87) and negative (r=0.90) sentiment.

looks like a rather fatty side of cheap    ank steak   , or    any-
thing to avoid this painful movie.    similarly, positive re-
views are associated with more positively charged categories:
beauty is 1.8 times more likely to appear in a positive review,
joy is 1.5 times more likely, and pride is 1.4 times more likely.
for example,    a wonderfully expressive speaking voice full
of youthful vigor, and gorgeous singing voice,    or    it   s the
triumph of secrets & lies, then, that it goes beyond gestures
of sympathy for the common people.   
beyond these obviously polarized categories, we    nd inter-
esting trends in the topics associated with positive and neg-
ative reviews. positive reviews tend to co-occur with the
deeper organizing principals of human life, like politics (1.4
odds), philosophy (1.4 odds), and law (1.3 odds)     possi-
bly indicating the interests of    lm reviewers. for example,
   branagh has concentrated on serious issues: morality, phi-
losophy and human elements of the story   , or    for all of the
idealism, it makes me feel good.    alternatively, negative re-
views adopt what appear to be metaphorical connections to
animals (1.5 odds), cleaning (1.3 odds), smell (1.2 odds) and
alcohol (1.2 odds). for example,    no matter how shiny the
super   cial sheen is, this is still trash, and, like all garbage,
it stinks   , or    a punch-drunk mess of a movie   , or    no free
popcorn coupon can ever restore to us the time we   ve spent
or wash the awful images from our mind.   
the movie dataset also allows us to demonstrate convergent
validity between empath and gold standard tools like liwc.
for example, are empath   s categories as good as liwc   s for
classi   cation? using    ve shared emotional categories as fea-
tures in a id28 to predict positive and negative
movie reviews, we compare empath and liwc under a 10-
fold cross-validation t-test that exhibits low type ii error [8].
we    nd no signi   cant difference between tools (p = 0.43).

example 3: mood on twitter and time of day
in our    nal example, we use empath to investigate the rela-
tionship between mood on twitter and time of day, replicat-
ing the work of golder and macy [13]. while the corpus of
tweets analyzed by the original paper is not publicly avail-
able, we reproduce the paper   s    ndings on a smaller corpus
of 591,520 tweets from the pst time-zone, running liwc on
our data as an additional benchmark (figure 6).

social media
facebook
instagram
noti   cation
sel   e
account
timeline
follower
table 1. empath can analyze text across hundreds of data-driven categories. here we provide a sample of representative terms in 8 sample categories.

technology
ipad
internet
download
wireless
computer
email
virus

contempt
disdain
mockery
grudging
haughty
caustic
censure
sneer

war
attack
battle   eld
soldier
troop
army
enemy
civilian

pain
hurt
pounding
sobbing
gasp
torment
groan
stung

violence
hurt
break
bleed
broken
scar
hurting
injury

hipster
vintage
trendy
fashion
designer
artsy
1950s
edgy

fear
horror
paralyze
dread
scared
tremor
despair
panic

words become its seed terms in empath. similar relation-
ships hold for the other primary and secondary emotions in
parrott   s hierarchy, which we use to bootstrap empath   s base
set of 32 emotional categories.
while empath   s topical and emotional categories stem from
different sources of knowledge, we generate member terms
for both kinds of categories in the same way. given a set
of seed terms, empath learns from a large corpus of text to
predict and validate hundreds of similar categorical terms.

learning category terms from a text corpus
where do category terms come from? how do we connect a
term like    rampage    with the category violent? empath takes
advantage of recent advances in deep learning to discover cat-
egory terms in an unsupervised fashion (figure 3).
as we have discussed, each of empath   s categories is de   ned
by seed words (e.g., lust: desire, passion; clothing: shirt, hat;
social media: facebook, twitter). empath   s model uses these
seed words to generate a candidate set of member terms for
its categories, which we validate through paid id104.
empath generates these category terms by querying a vector
space model (vsm) trained by a neural network on a large
corpus of text. this vsm allows empath to examine the sim-
ilarity between words across many dimensions of meaning.
for example, given seed words like    facebook    and    twitter,   
empath    nds related terms like    pinterest    and    sel   e.   
while empath can be trained on any text corpus, for the anal-
yses in this paper we use a dataset of modern    ction from
wattpad,1 a community of amateur writers. this corpus con-
tains more than 1.8 billion words of    ction written by hun-
dreds of thousands of authors.

training a neural vector space model
to train empath   s model, we adapt the deep learning skip-
gram architecture introduced by mikolov et al. [23]. this is
an unsupervised learning task where the basic idea is to teach
a neural network to predict co-occurring words in a corpus.
for example, the network might learn that    death    predicts a
nearby occurrence of the word    carrion,    but not of    incest.   
after enough training, the network learns a deep representa-
tion of each word that is predictive of its context. we can
then borrow these representations, called neural embeddings,
to map words onto a vector space.
more formally, for word w and context c in a network with
negative sampling, a skip-gram network will learn weights
that maximize the dot product w   wc and minimize w   wn for
wc     c and wn sampled randomly from the vocabulary. the

context c of a word is determined by a sliding window over
the document, of a size typically in (0,7).
we train our skip-gram network on the    ction corpus from
wattpad, lemmatizing all words through a preprocessing step.
the network uses a hidden layer of 150 neurons (which de-
   nes the dimensionality of the embedding space), a sliding
window size of    ve, a minimum word count of thirty (i.e., a
word must occur at least thirty times to appear in the train-
ing set), negative sampling, and down-sampling of frequent
terms. we de   ne and ignore stopwords as words with log-
adjusted id203 greater than -8, according to the spacy
nlp toolkit.2 these techniques re   ect current best practices
in deep learning for language models [24].

building categories with a vector space
we use the neural embeddings created by our skip-gram net-
work to construct a vector space model (vsm). similar mod-
els trained on neural embeddings, such as id97, are well
known to enable powerful forms of analogous reasoning (e.g.,
the vector arithmetic for the terms    king - man + queen    pro-
duces a vector close to    woman   ) [22]. in our case, vsms
allow empath to discover member terms for categories.
vsms encode concepts as vectors, where each dimension of
the vector v     rn conveys a feature relevant to the con-
cept. for empath, each vector v is a word, and each of its
dimensions de   nes the weight of its connection to one of the
hidden layer neurons (the neural embeddings). the space is
m(n    h) where n is the size of our vocabulary (40,000), and
h the number of hidden nodes in the network (150).
empath   s vsm selects member terms for its categories (e.g.,
social media, violence, shame) by using cosine similarity     a
similarity measure over vector spaces     to    nd nearby terms
in the space. concretely, cosine similarity is a scaled version
of the dot product between two vectors, de   ned as:

cos(  ) = a    b / ||a||    ||b||

using an embedding function v that maps a word to the vector
space, we can    nd the eight terms nearest to v(depressed),
by comparing its cosine similarity with all other terms in the
space, and selecting the ones that are most similar:

sad (0.75), heartbroken (0.74), suicidal (0.73), stressed (0.72),
self-harming (0.71), mopey (0.70), sick (0.69), moody (0.68)

we can also search the vector spaces on multiple terms by
querying on the vector sum of those terms     a kind of rea-
soning by analogy. to generate the query vector for one of
empath   s categories, we add the vector corresponding to the

1http://wattpad.com

2http://spacy.io

name of that category (if it exists in the space), to all the vec-
tors that correspond with its seed terms:

query(c, s) = v(c) +

v(t)

(cid:88)

t   s

where v is the embedding function, c is the name a category,
and s is a set of seed words that belong to the category.
for example, to generate the terms for clothing:

query(clothing,{shirt, hat}) = v(clothing) + v(shirt) + v(hat)

from a small seed of words, empath can gather hundreds of
terms related to a given category, and then use these terms for
textual analysis.

re   ning categories with crowd validation
human-validated categories can ensure that accidental terms
do not slip into a lexicon. by    ltering empath   s categories
through the crowd, we offer the bene   ts of both modern nlp
and human validation:
increasing category precision, and
more carefully validating category contents.
to validate each of empath   s categories, we have created
a id104 pipeline on amazon mechanical turk.
speci   cally, we ask crowdworkers:

for each word, tell us how strongly it relates to the topic. for
example, given the topic war, the words    sword    and   tank   
would be strongly related, the word    government    would be
related, the word    tactics    would be weakly related, and the
words    house    and    desk    would be unrelated.

prior work has adopted a similar question and scale [27].
we divide the total number of words to be    ltered across
many separate tasks, where each task consists of twenty
words to be rated for a given category. for each of these
words, workers select a relationship on a four point scale: not
related, weakly related, related, and strongly related. we ask
three independent workers to complete each task at a cost of
$0.14 per task, resulting in an hourly wage in line with ethi-
cal guidelines for amt research [35]. prior work has shown
that three workers are enough for reliable results in labeling
tasks, given high quality contributors [37]. so, if we want to
   lter a category of 200 words, we would have 200/20 = 10
tasks, which must be completed by three workers, at a total
cost of 10     3     0.14 = $4.2 for this category.
we limit tasks to masters workers to ensure quality [26] and
we aggregate crowdworker feedback by majority vote. if two
of three workers believe a word is at least weakly related to
the category, then empath will keep the word, otherwise we
remove it from the category. we chose this lower threshold
for term inclusion as it showed the highest agreement with
liwc in our benchmarks. on thresholds above weakly re-
lated, turkers tended to discard many more terms, increasing
category precision at the cost of much lower recall and hurt-
ing overall performance.

contents and ef   ciency
prior work suggests that category construction is subjective,
often resulting in low agreement among humans [41, 27].

empath cat.
domestic work
dance
aggression
attractive
nervousness
furniture
heroic
exotic
meeting
fashion

words that passed    lter
chore, vacuum, scrubbing, laundry
ballet, rhythm, jukebox, dj, song
lethal, spite, betray, territorial
alluring, cute, swoon, dreamy, cute
uneasiness, paranoid, fear, worry
chair, mattress, desk, antique
underdog, gutsy, rescue, underdog
aquatic, tourist, colorful, seaside
of   ce, boardroom, presentation
stylist, shoe, tailor, salon, trendy

did not pass
   nd
buds
censure
de   antly
nostalgia
crate
spoof
rural
homework
yoga

table 2. crowd workers found 95% of the words generated by em-
path   s unsupervised model to be related to its categories. however, ma-
chine learning is not perfect, and some unrelated terms slipped through
(   did not pass    above), which the crowd then removed.

how well do human workers agree with empath? on av-
erage over 200 categories, workers rated 96% of the words
generated by empath as related to its categories, and agreed
among themselves (voting unanimously with unrelated or re-
lated scores) at a rate of 81%. we provide a sample of terms
accepted and rejected by the crowd in table 2.
an acceptance score of 96% allows us to ef   ciently col-
lect validated words for empath   s categories. prior work
in human-validated category construction has typically re-
lied upon less ef   cient approaches, for example using crowd
workers to annotate the 10,000 most common dictionary
words with scores over all categories in question [27]. em-
path   s unsupervised accuracy allows us to validate the same
size categories with far fewer id104 tasks. at scale,
the cost of id104 new lexicons is expensive. to val-
idate 200 categories naively over 5,000 words would cost
$21,000, assuming 14 cents a task. we have validated em-
path   s 200 categories (with a vocabulary of more than 10,000
words) at a total cost of $840.
our experience con   rms the    ndings of prior work that cat-
egory construction is somewhat subjective. not all of the
words rejected by majority vote are necessarily unrelated to a
category, and in fact 36% of them had a worker cast a minor-
ity vote for relevance.

empath api and web service
finally, to help researchers analyze text over new kinds of cat-
egories, we have released empath as a web service and open
source library. the web service3 allows users to analyze doc-
uments across empath   s built-in categories (figure 1), gener-
ate new unsupervised categories, and request new categories
be validated using our id104 pipeline. the open
source library4 is written in python and similarly returns doc-
ument counts across empath   s built-in validated categories.

evaluation
can    ction teach computers about the emotional and topical
connotations of words? here we evaluate empath   s crowd    l-
tered and unsupervised predictions against similar gold stan-
dard categories in liwc.

3http://empath.stanford.edu
4https://github.com/ejhfast/empath

comparing empath and liwc
the broad reach of our dataset allows empath to classify doc-
uments among a large number of categories. but how accu-
rate are these categorical associations? human inspection and
crowd    ltering of empath   s categories (table 2) provide some
evidence, but ideally we would like to answer this question in
a more quantitative way.
fortunately, liwc has been extensively validated by re-
searchers [33], so we can use it to benchmark empath   s pre-
dictions across the categories that they share in common. if
we can demonstrate that empath provides very similar results
across these categories, this would suggest that empath   s pre-
dictions are close to achieving gold standard accuracy.
here we compare the predictions of empath and liwc over
12 shared categories: sadness, anger, positive emotion, nega-
tive emotion, sexual, money, death, achievement, home, reli-
gion, work, and health.

method
to compare all tools, we created a mixed textual dataset
evenly divided among tweets [28], stackexchange opinions
[5], movie reviews [32], hotel reviews [31], and chapters sam-
pled from four classic novels on project gutenberg (david
copper   eld, moby dick, anna karenina, and the count of
monte cristo) [1]. this mixed corpus contains more than 2
million words in total across 4500 individual documents.
next we selected two parameters for empath: the minimum
cosine similarity for category inclusion and the seed words
for each category (we    xed the size of each category at a max-
imum of 200 words). to choose these parameters, we divided
our mixed text dataset into a training corpus of 900 docu-
ments and a test corpus of 3500 documents. we selected up to
   ve seed words that best approximated each liwc category,
and found that a minimum cosine similarity of 0.5 offered the
best performance. we created crowd    ltered versions of these
categories as described in the previous section.
we then ran all tools over the documents in the test corpus,
recorded their category word counts, then used these counts to
compute pearson correlations between all shared categories,
as well as aggregate overall correlations. pearson   s r mea-
sures the linear correlation between two variables, and returns
a value between (-1,1), where 1 is total positive correlation,
0 is no correlation, and 1 is total negative correlation. in our
experiment, these correlations speak to how well one tool ap-
proximates another.
to anchor this analysis, we collected benchmark pearson cor-
relations against liwc for gi and emolex (two existing hu-
man validated lexicons). we found a benchmark correlation
of 0.876 between gi and liwc over positive emotion, nega-
tive emotion, religion, work, and achievement, and a correla-
tion of 0.899 between emolex and liwc over positive emo-
tion, negative emotion, anger, and sadness. while emolex
and gi are commonly regarded as gold standards, they cor-
relate imperfectly with liwc. we take this as evidence that
gold standard lexicons can disagree: if empath approximates
their performance against liwc, it agrees with liwc as well
as other carefully-validated dictionaries agree with liwc.

liwc cat.
positive
negative
sadness
anger
achievement
religion
work
home
money
health
sex
death
average

empath
0.944
0.941
0.890
0.889
0.915
0.893
0.859
0.919
0.902
0.866
0.928
0.856
0.900

empath+crowd
0.950
0.936
0.907
0.894
0.903
0.908
0.820
0.941
0.878
0.898
0.935
0.901
0.906

emolex
0.955
0.954
0.852
0.837

gi
0.971
0.945

0.817
0.902
0.745

0.899

0.876

table 3. we compared the classi   cations of liwc, emolex and empath
across thirteen categories,    nding strong correlation between tools. the
   rst column represents comparisons between empath   s unsupervised
model against liwc, the second after crowd    ltering against liwc, the
third between emolex and liwc, and the fourth between the general
inquirer and liwc.

finally, to test the importance of choosing seed terms, we
re-ran our evaluation while permuting the seed words in em-
path   s categories. over one trial, we dropped one seed term
from each category. over another, we replaced one term from
each category with a similar alternative (e.g.,    church    to
   chapel   , or    kill    to    murder   ).

results
empath shares overall average pearson correlations of 0.90
(unsupervised) and 0.906 (crowd) with liwc (table 3). over
the emotional categories, empath and liwc agree at corre-
lations of 0.884 (unsupervised) and 0.90 (crowd), compar-
ing favorably with emolex   s correlation of 0.899. over gi   s
benchmark categories, empath reports 0.893 (unsupervised)
and 0.91 (crowd) correlations against liwc, stronger perfor-
mance than gi (0.876). on average, adding a crowd    lter
to empath improves its correlations with liwc by 0.006.
we plot empath   s best and worst category correlations with
liwc in figure 7. these scores indicate that empath and
liwc are strongly correlated     similar to the correlation be-
tween liwc and other published and validated tools.
in permuting empath   s seed terms, we found it retained high
unsupervised agreement with liwc (between 0.82 and 0.88).
the correlation between tools was most strongly affected
when we dropped seeds that added a unique meaning to a cat-
egory. for example, death is seeded with the words    bury   ,
   cof   n   ,    kill   , and    corpse.    when we removed    kill    from
the death   s seed list, empath lost the adversarial aspects of
death (embodied in words like    war   ,    execute   , or    mur-
der   ) and fell to 0.82 correlation with liwc for that cate-
gory. removing death   s other seed words did not have nearly
so strong an affect. on the other hand, replacing seeds with
alternative forms or synonyms (e.g.,    hate    to    hatred   , or
   kill    to    murder   ) usually had little impact on empath   s cor-
relations with liwc.

discussion
empath demonstrates an approach that crosses traditional text
analysis metaphors with advances in deep learning. here we
discuss our results and the limitations of our approach.

figure 7. empath categories strongly agreed with liwc, at an average pearson correlation of 0.90. here we plot empath   s best and worst correlations
with liwc. each dot in the plot corresponds to one document. empath   s counts are graphed on the x-axis, liwc   s on the y-axis.

the role of human validation
while adding a crowd    lter to empath improves its overall
correlations with liwc, the improvement is not statistically
signi   cant. even more surprisingly, the crowd does not al-
ways improve agreement at the level of individual categories.
for example, across the categories negative emotion, achieve-
ment, and work, the crowd    lter slightly decreases empath   s
agreement with liwc. when we inspected the output of the
crowd    ltering step to determine what had caused this effect,
we found in a small number of cases in which the crowd was
overzealous. for example, the word    semester    appears in
liwc   s work category, but the crowd removed it from em-
path. should    semester    be in a work category? this dis-
agreement highlights the inherent ambiguity of constructing
lexicons.
in our case, when the crowd    lters out a com-
mon word shared by liwc (like    semester   ), this causes
overall agreement across the corpus to decrease (through ad-
ditional false negatives), despite the appropriate removal of
many other less common words.
as we see in our results, this scenario does not happen of-
ten, and when it does happen the effect size is small. we
suggest that crowd validation offers the qualitative bene   t of
removing false positives from analyses, while on the whole
performing almost identically to (and usually slightly better
than) the un   ltered version of empath.

data-driven: who is actually driving?
empath, like any data-driven system, is ultimately at the
mercy of its data     garbage in, garbage out. while    ction al-
lows empath to learn an approximation of the gold-standard
categories that de   ne tools like liwc, its data-driven reason-
ing may succeed less well on corner cases of analysis and
connotation. just because    ctional characters often pull guns

out of gloveboxes, for example, does not mean the two should
be strongly connected in empath   s categories.
contrary to this critique, we have found that    ction is a use-
ful training dataset for empath given its abundance of con-
crete descriptors and emotional terms. when we replaced the
id27s learned by our model with alternative em-
beddings trained on google news [23], we found its average
unsupervised correlation with liwc decreased to 0.84. the
google news embeddings performed better after signi   cance
testing on only one category, death (0.91), and much worse
on several of the others, including religion (0.78) and work
(0.69). this may speak to the limited in   uence of    ction bias.
fiction may suffer from the overly fanciful plot events and
motifs that surround death (e.g. suffocation, torture), but it
captures more relevant words around most categories.

limitations
empath   s design decisions suggest a set of limitations, many
of which we hope to address in future work.
first, while empath reports high pearson correlations with
liwc   s categories, it is possible that other more qualitative
properties are important to lexical categories. two lexicons
can be statistically similar on the basis of word counts, and
yet one might be easier to interpret than the other, offer more
representative words, or present fewer false positives or neg-
atives. at a higher level, the number and kinds of categories
available in empath present a related concern. we created
these categories in a data-driven manner. do they offer the
right balance and breadth of topics? we have not evaluated
empath over these more qualitative aspects of usability.
second, we have not tested how well empath   s categories
generalize beyond the core set it shares with liwc. do these

new categories perform as well in practice? while empath   s
categories are all generated and validated in the same way,
we have seen though our evaluation that choice of seed words
can be important. what makes for a good set of seed terms?
and how do we best discover them? in future work, we hope
to investigate these questions more closely.
finally, while    ction provides a powerful model for gener-
ating lexical categories, we have also seen that, for certain
topics (e.g. death in google news), other corpora may have
even greater potential. could different datasets be targeted at
speci   c categories? mining an online fashion forum, for ex-
ample, might allow empath to learn a more comprehensive
sense of style, or hacker news might give it a more nuanced
view of technology and startups. we see potential for training
empath on other text beyond    ction.

statistical false positives
social science aims to avoid type i errors     false claims that
statistically appear to be true. because empath expands the
number of categories available for analysis, it is important to
consider the risk of a scientist analyzing so many categories
that one of them, through sheer randomness, appears to be
elevated in the text. in this paper, we used bonferroni correc-
tion to handle the issue, but there are more mature methods
available. for example, holm   s method and fdr are often
used in statistical genomics to test thousands of hypotheses.
in the case of regression analysis, it is likewise important not
to do so-called    garbage can regressions    that include every
possible predictor.
in this case, models that penalize com-
plexity (e.g., non-zero coef   cients) are most appropriate, for
example lasso or id28 with an l1 penalty.

conclusion
empath aims to combine modern nlp techniques with the
transparency of dictionaries like liwc. in doing so, it pro-
vides both broader and deeper forms of text analysis than
existing tools.
in breadth, empath offers hundreds of pre-
de   ned lenses through which researchers can analyze text. in
depth, its user-de   ned categories provide a    exible means by
which researchers can ask domain-speci   c questions. these
questions are ever changing, as is our use of language. em-
path is a living lexicon     able to keep up with each.

acknowledgments
special thanks to our reviewers and colleagues at stanford
for their helpful feedback. this work is supported by a nsf
graduate fellowship and a nih and stanford medical scien-
tist training grant.

references
1. project gutenberg. in https://www.gutenberg.org/.
2. johan bollen, alberto pepe, and huina mao. 2009.

modeling public mood and emotion: twitter sentiment
and socio-economic phenomena. in arxiv preprint
arxiv:0911.1583.

3. margaret m bradley and peter j lang. 1999. affective
norms for english words (anew): instruction manual
and affective ratings. in technical report c-1, the

center for research in psychophysiology, university of
florida.

4. nathanael chambers and dan jurafsky. unsupervised
learning of narrative schemas and their participants.
in proc. acl 2009.

5. cristian danescu-niculescu-mizil, moritz sudhof, dan

jurafsky, jure leskovec, and christopher potts. a
computational approach to politeness with application to
social factors. in proc of acl 2013.

6. hannah davis and saif m mohammad. 2014.

generating music from literature. in arxiv preprint
arxiv:1403.2124.

7. munmun de choudhury. you   re happy, i   m happy:
diffusion of mood expression on twitter. in proc. of
hci korea 2014.

8. thomas g dietterich. 1998. approximate statistical
tests for comparing supervised classi   cation learning
algorithms. in neural computation, vol. 10. mit press,
1895   1923.

9. andrea esuli and fabrizio sebastiani. sentiid138: a

publicly available lexical resource for opinion mining. in
proceedings of lrec 2006.

10. ethan fast, will mcgrath, pranav rajpurkar, and

michael bernstein. mining human behaviors from
fiction to power interactive systems. in proc. chi
2016.

11. ethan fast, daniel steffe, lucy wang, michael

bernstein, and joel brandt. emergent, crowd-scale
programming practice in the ide. in proc. chi 2014.
12. tong gao, mira dontcheva, eytan adar, zhicheng liu,
and karrie karahalios. datatone: managing ambiguity
in id139 for data visualization.
13. scott a. golder and michael w. macy. 2011. diurnal

and seasonal mood vary with work, sleep, and
daylength across diverse cultures. in science, vol.
333. 1878   1881.

14. vasileios hatzivassiloglou and kathleen r mckeown.
1997. predicting the semantic orientation of adjectives.
in proceedings of the 35th annual meeting of the
association for computational linguistics and eighth
conference of the european chapter of the association
for computational linguistics. association for
computational linguistics, 174   181.

15. c. hutto and eric gilbert. vader: a parsimonious
rule-based model for id31 of social
media text. in proc. aaai 2014.

16. sepandar d kamvar and jonathan harris. 2011. we feel
   ne and searching the emotional web. in proceedings of
the fourth acm international conference on web search
and data mining. acm, 117   126.

17. emre kiciman. 2015. towards learning a knowledge

base of actions from experiential microblogs. in aaai
spring symposium.

18. suin kim, jinyeong bak, and alice haeyun oh. 2012.
do you feel what i feel? social aspects of emotions
in twitter conversations.. in icwsm.

33. james w pennebaker, martha e francis, and roger j

booth. linguistic inquiry and word count: liwc 2001.
in mahway: lawrence erlbaum associates 71 2001.

19. adam d. i. kramer, jamie e. guillory, and jeffrey t.

hancock. 2014. experimental evidence of massive-scale
emotional contagion through social networks. in
proceedings of the national academy of sciences, vol.
111. 8788   8790.

20. ranjitha kumar, arvind satyanarayan, cesar torres,
maxine lim, salman ahmad, scott r klemmer, and
jerry o talton. webzeitgeist: design mining the web. in
proc. chi 2013.

21. h. liu and p. singh. conceptnet     a practical

commonsense reasoning tool-kit. in bt technology
journal 2004.

22. qun luo and weiran xu. learning word vectors

ef   ciently using shared representations and document
representations. in proc. aaai 2015.

23. tomas mikolov, ilya sutskever, kai chen, greg s.

corrado, and jeff dean. distributed representations of
words and phrases and their compositionality. in proc.
nips 2013.

24. tomas mikolov, wen tau yih, and geoffrey zweig.

linguistic regularities in continuous space word
representations. in proc. naacl-hlt 2013.

25. george a. miller. id138: a lexical database for

english. in in commun. acm 1995.

26. tanushree mitra, c.j. hutto, and eric gilbert.

comparing person- and process-centric strategies for
obtaining quality data on amazon mechanical turk. in
proc. chi    15.

27. saif m. mohammad and peter d. turney. 2013.

id104 a word-emotion association lexicon.
in computational intelligence, vol. 29. 436   465.

28. saif m mohammad, xiaodan zhu, svetlana

kiritchenko, and joel martin. 2014. sentiment, emotion,
purpose, and style in electoral tweets. in information
processing & management. elsevier.

29. alena neviarouskaya, helmut prendinger, and mitsuru

ishizuka. 2007. narrowing the social gap among
people involved in global dialog: automatic emotion
detection in blog posts.. in icwsm. citeseer.

30. vlad niculae, srijan kumar, jordan l. boyd-graber,

and cristian danescu-niculescu-mizil. 2015. linguistic
harbingers of betrayal: a case study on an online
strategy game. corr abs/1506.04744 (2015).

31. myle ott, yejin choi, claire cardie, and jeffrey t

hancock. finding deceptive opinion spam by any stretch
of the imagination. in proc. acl 2011.

32. bo pang, lillian lee, and shivakumar vaithyanathan.
thumbs up?: sentiment classi   cation using machine
learning techniques. proc. acl 2002.

34. steve rubin and maneesh agrawala. generating

emotionally relevant musical scores for audio stories. in
proc. uist 2014.

35. niloufar salehi, lilly c irani, and michael s bernstein.
we are dynamo: overcoming stalling and friction in
collective action for crowd workers. in proc. chi
2015.

36. phillip shaver, judith schwartz, donald kirson, and
cary o   connor. 1987. emotion knowledge: further
exploration of a prototype approach. in journal of
personality and social psychology, vol. 52. american
psychological association, 1061.

37. victor s sheng, foster provost, and panagiotis g

ipeirotis. get another label? improving data quality and
data mining using multiple, noisy labelers. in proc.
sigkdd 2008.

38. tamara a small. what the hashtag? a content analysis

of canadian politics on twitter. in information,
communication & society 2011.

39. richard socher, alex perelygin, jean y. wu, jason

chuang, christopher d. manning, andrew y. ng, and
christopher potts. proc. emnlp 2013. recursive deep
models for semantic compositionality over a
sentiment treebank.

40. philip j stone, dexter c dunphy, and marshall s smith.

1966. the general inquirer: a computer approach to
content analysis. mit press.

41. yla r tausczik and james w pennebaker. 2010. the

psychological meaning of words: liwc and
computerized text analysis methods. in journal of
language and social psychology, vol. 29. sage
publications, 24   54.

42. catalina l toma, jeffrey t hancock, and nicole b

ellison. 2008. separating fact from    ction: an
examination of deceptive self-presentation in online
dating pro   les. in personality and social psychology
bulletin, vol. 34. sage publications, 1023   1036.
43. peter d. turney. thumbs up or thumbs down?:

semantic orientation applied to unsupervised
classi   cation of reviews. in proc. acl 2002.

44. froma i zeitlin. 1996. playing the other: gender and

society in classical greek literature. university of
chicago press.

