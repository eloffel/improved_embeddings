alignment-based id152 for instruction following

jacob andreas and dan klein

computer science division

university of california, berkeley

{jda,klein}@cs.berkeley.edu

7
1
0
2

 
r
p
a
2
1

 

 
 
]
l
c
.
s
c
[
 
 

2
v
1
9
4
6
0

.

8
0
5
1
:
v
i
x
r
a

abstract

this paper describes an alignment-based
model for interpreting natural language in-
structions in context. we approach in-
struction following as a search over plans,
scoring sequences of actions conditioned
on structured observations of text and the
environment. by explicitly modeling both
the low-level compositional structure of
individual actions and the high-level struc-
ture of full plans, we are able to learn
both grounded representations of sentence
meaning and pragmatic constraints on in-
terpretation. to demonstrate the model   s
   exibility, we apply it to a diverse set
of benchmark tasks. on every task, we
outperform strong task-speci   c baselines,
and achieve several new state-of-the-art
results.

1

introduction

in instruction-following tasks, an agent executes
a sequence of actions in a real or simulated envi-
ronment, in response to a sequence of natural lan-
guage commands. examples include giving nav-
igational directions to robots and providing hints
to automated game-playing agents. plans speci-
   ed with natural language exhibit compositional-
ity both at the level of individual actions and at
the overall sequence level. this paper describes a
framework for learning to follow instructions by
leveraging structure at both levels.

our primary contribution is a new, alignment-
based approach to grounded compositional se-
mantics. building on related logical approaches
(reddy et al., 2014; pourdamghani et al., 2014),
we recast instruction following as a pair of nested,
structured alignment problems. given instructions
and a candidate plan, the model infers a sequence-
to-sequence alignment between sentences and

atomic actions. within each sentence   action pair,
the model infers a structure-to-structure alignment
between the syntax of the sentence and a graph-
based representation of the action.

at a high level, our agent is a block-structured,
graph-valued conditional random    eld, with align-
ment potentials to relate instructions to actions and
transition potentials to encode the environment
model (figure 3). explicitly modeling sequence-
to-sequence alignments between text and actions
allows    exible reasoning about action sequences,
enabling the agent to determine which actions are
speci   ed (perhaps redundantly) by text, and which
actions must be performed automatically (in or-
der to satisfy pragmatic constraints on interpreta-
tion). treating instruction following as a sequence
prediction problem, rather than a series of inde-
pendent decisions (branavan et al., 2009; artzi
and zettlemoyer, 2013), makes it possible to use
general-purpose planning machinery, greatly in-
creasing inferential power.

the fragment of semantics necessary to com-
plete most instruction-following tasks is essen-
tially predicate   argument structure, with limited
in   uence from quanti   cation and scoping. thus
the problem of sentence interpretation can reason-
ably be modeled as one of    nding an alignment be-
tween language and the environment it describes.
we allow this structure-to-structure alignment   
an    overlay    of language onto the world   to be
mediated by linguistic structure (in the form of
dependency parses) and structured perception (in
what we term grounding graphs). our model
thereby reasons directly about the relationship be-
tween language and observations of the environ-
ment, without the need for an intermediate logi-
cal representation of sentence meaning. this, in
turn, makes it possible to incorporate    exible fea-
ture representations that have been dif   cult to in-
tegrate with previous work in id29.

we apply our approach to three established

. . . right round the white water but
stay quite close    cause you don   t
otherwise you   re going to be in that
stone creek . . .

go down the yellow hall. turn left
at the intersection of the yellow and
the gray.

clear the right column. then the
other column. then the row.

(a) map reading

(b) maze navigation

(c) puzzle solving

figure 1: example tasks handled by our framework. the tasks feature noisy text, over- and under-speci   cation of plans, and
challenging search problems.

instruction-following benchmarks: the map read-
ing task of vogel and jurafsky (2010), the maze
navigation task of macmahon et al. (2006), and
the puzzle solving task of branavan et al. (2009).
an example from each is shown in figure 1.
these benchmarks exhibit a range of qualitative
properties   both in the length and complexity of
their plans, and in the quantity and quality of ac-
companying language. each task has been stud-
ied in isolation, but we are unaware of any pub-
lished approaches capable of robustly handling
all three. our general model outperforms strong,
task-speci   c baselines in each case, achieving
relative error reductions of 15   20% over sev-
eral state-of-the-art results. experiments demon-
strate the importance of our contributions in both
id152 and search over plans.
we have released all code for this project at
github.com/jacobandreas/instructions.

2 related work

existing work on instruction following can be
roughly divided into two families:
semantic
parsers and linear policy estimators.

into a formal

parsers parser-based

semantic
approaches
(chen and mooney, 2011; artzi and zettlemoyer,
2013; kim and mooney, 2013; tellex et al.,
2011) map from text
language
these take familiar
representing commands.
id170 models for id29
(zettlemoyer and collins, 2005; wong and
mooney, 2006), and train them with task-provided
supervision.
instead of attempting to match the
structure of a manually-annotated semantic parse,
semantic parsers for instruction following are

trained to maximize a reward signal provided by
black-box execution of the predicted command
in the environment.
(it is possible to think of
response-based learning for id53
(liang et al., 2013) as a special case.)

this approach uses a well-studied mechanism
for compositional interpretation of language, but is
subject to certain limitations. because the environ-
ment is manipulated only through black-box exe-
cution of the completed semantic parse, there is no
way to incorporate current or future environment
state into the scoring function. it is also in general
necessary to hand-engineer a task-speci   c formal
language for describing agent behavior. thus it is
extremely dif   cult to work with environments that
cannot be modeled with a    xed inventory of pred-
icates (e.g. those involving novel strings or arbi-
trary real quantities).

much of contemporary work in this family is
evaluated on the maze navigation task introduced
by macmahon et al. (2006). dukes (2013) also in-
troduced a    blocks world    task for situated parsing
of spatial robot commands.

linear policy estimators an alternative fam-
ily of approaches is based on learning a pol-
icy over primitive actions directly (branavan et
al., 2009; vogel and jurafsky, 2010).1 policy-
based approaches instantiate a markov decision
process representing the action domain, and ap-
ply standard supervised or reinforcement-learning
approaches to learn a function for greedily select-
ing among actions. in linear policy approximators,
natural language instructions are incorporated di-

1this is distinct from semantic parsers in which greedy
id136 happens to have an interpretation as a policy (vla-
chos and clark, 2014).

213213213rectly into state observations, and reading order
becomes part of the action selection process.

almost all existing policy-learning approaches
make use of an unstructured parameterization,
with a single (   at) feature vector representing all
text and observations. such approaches are thus
restricted to problems that are simple enough (and
have small enough action spaces) to be effectively
characterized in this fashion. while there is a great
deal of    exibility in the choice of feature func-
tion (which is free to inspect the current and fu-
ture state of the environment, the whole instruc-
tion sequence, etc.), standard linear policy estima-
tors have no way to model compositionality in lan-
guage or actions.

agents in this family have been evaluated on a
variety of tasks, including map reading (anderson
et al., 1991) and gameplay (branavan et al., 2009).

though both families address the same class
of instruction-following problems, they have been
applied to a totally disjoint set of tasks. it should
be emphasized that there is nothing inherent to
policy learning that prevents the use of composi-
tional structure, and nothing inherent to general
compositional models that prevents more compli-
cated dependence on environment state. indeed,
previous work (branavan et al., 2011; narasimhan
et al., 2015) uses aspects of both to solve a differ-
ent class of gameplay problems. in some sense,
our goal in this paper is simply to combine the
strengths of semantic parsers and linear policy es-
timators for fully general instruction following.
as we shall see, however, this requires changes
to many aspects of representation, learning and in-
ference.

3 representations
we wish to train a model capable of following
commands in a simulated environment. we do so
by presenting the model with a sequence of train-
ing pairs (x, y), where each x is a sequence of nat-
ural language instructions (x1, x2, . . . , xm), e.g.:

(go down the yellow hall., turn left.,

. . . )

and each y is a demonstrated action sequence
(y1, y2, . . . , yn), e.g.:

(rotate(90), move(2),

. . . )

given a start state, y can equivalently be char-
acterized by a sequence of (state, action, state)

figure 2: structure-to-structure alignment connecting a sin-
gle sentence (via its syntactic analysis) to the environment
state (via its grounding graph). the connecting alignments
take the place of a traditional semantic parse and allow    exi-
ble, feature-driven linking between lexical primitives and per-
ceptual factors.

triples resulting from execution of the environ-
ment model. an example instruction is shown in
figure 2a. an example action, situated in the en-
vironment where it occurs, is shown in figure 2e.
our model performs compositional interpreta-
tion of instructions by leveraging existing struc-
ture inherent in both text and actions. thus we
interpret xi and yj not as raw strings and primitive
actions, but rather as structured objects.
linguistic structure we assume access to a pre-
trained parser, and in particular that each of the
instructions xi is represented by a tree-structured
dependency parse. an example is shown in fig-
ure 2b.
action structure by analogy to the represen-
tation of instructions as parse trees, we assume
that each (state, action, state) triple (provided by
the environment model) can be characterized by a
grounding graph.2 the structure and content of
2we note that the instruction following model of tellex et
al. (2011) features a similarly named    generalized ground-

go down the yellow hallgodownhallthe(a)  text(b)  syntax(c)  alignment(d)  perception(e)  environment*    go    down    the   yellow hallyellowmove(2)this representation is task-speci   c. an example
grounding graph for the maze navigation task is
shown in figure 2d. the example contains a node
corresponding to the primitive action move(2)
(in the upper left), and several nodes correspond-
ing to locations in the environment that are visible
after the action is performed.

each node in the graph (and, though not de-
picted, each edge) is decorated with a list of fea-
tures. these features might be simple indica-
tors (e.g. whether the primitive action performed
was move or rotate), real values (the distance
traveled) or even string-valued (english-language
names of visible landmarks, if available in the
environment description). formally, a grounding
graph consists of a tuple (v, e,l, fv , fe), with

    v a set of vertices
    e     v    v a set of (directed) edges
    l a space of labels (numbers, strings, etc.)
    fv : v     2l a vertex feature function
    fe : e     2l an edge feature function
in this paper we have tried to remain agnostic
to details of graph construction. our goal with the
grounding graph framework is simply to accom-
modate a wider range of modeling decisions than
allowed by existing formalisms. graphs might
be constructed directly, given access to a struc-
tured virtual environment (as in all experiments
in this paper), or alternatively from outputs of a
perceptual system. for our experiments, we have
remained as close as possible to task representa-
tions described in the existing literature. details
for each task can be found in the accompanying
software package.

graph-based representations are extremely
common in formal semantics (jones et al., 2012;
reddy et al., 2014), and the version presented here
corresponds to a simple generalization of famil-
iar formal methods. indeed, if l is the set of all
atomic entities and relations, fv returns a unique
label for every v     v , and fe always returns
a vector with one active feature, we recover the
existentially-quanti   ed portion of    rst order logic
exactly, and in this form can implement large parts
of classical neo-davidsonian semantics (parsons,
1990) using grounding graphs.

ing graph    (g3) formalism. a g3 links the syntax of the in-
put command to the action ultimately executed, and is thus
more analogous to our structured alignment variable (fig-
ure 2c) than our perceptual representation.

crucially, with an appropriate choice of l this
formalism also makes it possible to go beyond set-
theoretic relations, and incorporate string-valued
features (like names of entities and landmarks) and
real-valued features (like colors and positions) as
well.

lexical semantics we must eventually combine
features provided by parse trees with features pro-
vided by the environment. examples here might
include simple conjunctions (word=yellow    
rgb=(0.5, 0.5, 0.0)) or more compli-
cated computations like id153 between
landmark names and lexical items. features of
the latter kind make it possible to behave correctly
in environments containing novel strings or other
features unseen during training.

this aspect of

the syntax   semantics inter-
face has been troublesome for some logic-based
approaches: while past work has used related
machinery for selecting lexicon entries (berant
and liang, 2014) or for rewriting logical forms
(kwiatkowski et al., 2013), the relationship be-
tween text and the environment has ultimately
been mediated by a discrete (and indeed    nite) in-
ventory of predicates. several recent papers have
investigated simple grounded models with real-
valued output spaces (andreas and klein, 2014;
mcmahan and stone, 2015), but we are unaware
of any fully compositional system in recent lit-
erature that can incorporate observations of these
kinds.
formally, we assume access to a joining feature
function    : (2l    2l)     (cid:82)d. as with grounding
graphs, our goal is to make the general framework
as    exible as possible, and for individual exper-
iments have chosen    to emulate modeling deci-
sions from previous work.

4 model

as noted in the introduction, we approach instruc-
tion following as a sequence prediction problem.
thus we must place a distribution over sequences
of actions conditioned on instructions. we decom-
pose the problem into two components, describing
interlocking models of    path structure    and    ac-
tion structure   . path structure captures how se-
quences of instructions give rise to sequences of
actions, while action structure captures the com-
positional relationship between individual utter-
ances and the actions they specify.

1 . . . n (recalling that n is the number of actions).
then we have3

p(y,a|x;   )     exp

  (n) +

n(cid:88)

j=1

  (yj)

(cid:27)

(cid:26)
n(cid:88)

m(cid:88)

+

1[aj = i]   (xi, yj)

(1)

figure 3: our model is a conditional random    eld that de-
scribes distributions over state-action sequences conditioned
on input text. each variable   s domain is a structured value.
sentences align to a subset of the state   action sequences,
with the rest of the states    lled in by pragmatic (planning)
implication. state-to-state structure represents planning con-
straints (environment model) while state-to-text structure rep-
resents compositional alignment. all potentials are log-linear
and feature-driven.

path structure: aligning utterances to actions
the high-level path structure in the model is de-
picted in figure 3. our goal here is to permit both
under- and over-speci   cation of plans, and to ex-
pose a planning framework which allows plans to
be computed with lookahead (i.e. non-greedily).

these goals are achieved by introducing a se-
quence of latent alignments between instructions
and actions. consider the multi-step example in
figure 1b. if the    rst instruction go down the yel-
low hall were interpreted immediately, we would
have a presupposition failure   the agent is facing
a wall, and cannot move forward at all. thus an
implicit rotate action, unspeci   ed by text, must
be performed before any explicit instructions can
be followed.

to model this, we take the id203 of a (text,
plan, alignment) triple to be log-proportional to
the sum of two quantities:

1. a path-only score   (n;   ) +(cid:80)

j   (yj;   )

2. a path-and-text score, itself the sum of all pair
scores   (xi, yj;   ) licensed by the alignment

(1) captures our desire for pragmatic constraints
on interpretation, and provides a means of encod-
ing the inherent plausibility of paths. we take
  (n;   ) and   (y;   ) to be linear functions of   .
(2) provides context-dependent interpretation of
text by means of the structured scoring function
  (x, y;   ), described in the next section.
formally, we associate with each instruction xi
a sequence-to-sequence alignment variable ai    

i=1

j=1

we additionally place a monotonicity constraint
on the alignment variables. this model is globally
normalized, and for a    xed alignment is equiva-
lent to a linear-chain crf. in this sense it is analo-
gous to ibm model i (brown et al., 1993), with the
structured potentials   (xi, yj) taking the place of
lexical translation probabilities. while alignment
models from machine translation have previously
been used to align words to fragments of semantic
parses (wong and mooney, 2006; pourdamghani
et al., 2014), we are unaware of such models be-
ing used to align entire instruction sequences to
demonstrations.

action structure: aligning words to percepts
intuitively, this scoring function   (x, y) should
capture how well a given utterance describes an
action. if neither the utterances nor the actions had
structure (i.e. both could be represented with sim-
ple bags of features), we would recover something
analogous to the conventional policy-learning ap-
proach. as structure is essential for some of our
tasks,   (x, y) must instead    ll the role of a seman-
tic parser in a conventional compositional model.
our choice of   (x, y) is driven by the following
fundamental assumptions: syntactic relations ap-
proximately represent semantic relations. syntac-
tic proximity implies relational proximity. in this
view, there is an additional hidden structure-to-
structure alignment between the grounding graph
and the parsed text describing it. 4 words line up
with nodes, and dependencies line up with rela-
tions. visualizations are shown in figure 2c and
the zoomed-in portion of figure 3.

as with the top-level alignment variables, this
approach can viewed as a simple relaxation of a
familiar model. id35-based parsers assume that
3here and in the remainder of this paper, we suppress the
dependence of the various potentials on    in the interest of
readability.

is formally possible to regard the sequence-to-
sequence and structure-to-structure alignments as a single
(structured) random variable. however, the two kinds of
alignments are treated differently for purposes of id136,
so it is useful to maintain a notational distinction.

4it

turn left.go down the yellow hall.turnlefttextalignmentsplanssyntactic type strictly determines semantic type,
and that each lexical item is associated with a
small set of functional forms. here we simply
allow all words to license all predicates, multi-
ple words to specify the same predicate, and some
edges to be skipped. we instead rely on a scoring
function to impose soft versions of the hard con-
straints typically provided by a grammar. related
models have previously been used for question an-
swering (reddy et al., 2014; pasupat and liang,
2015).

for the moment let us introduce variables b
to denote these structure-to-structure alignments.
(as will be seen in the following section, it is
straightforward to marginalize over all choices of
b. thus the structure-to-structure alignments are
never explicitly instantiated during id136, and
do not appear in the    nal form of   (x, y).) for
a    xed alignment, we de   ne   (x, y, b) according
to a recurrence relation. let xi be the ith word of
the sentence, and let yj be the jth node in the ac-
tion graph (under some topological ordering). let
c(i) and c(j) give the indices of the dependents of
xi and children of yj respectively. finally, let xik
and yjl denote the associated dependency type or
relation. de   ne a    descendant    function:

d(i, j) =(cid:8)(k, l) : k     c(i), l     c(j), (k, l)     b(cid:9)

(cid:26)
(cid:105)(cid:27)
(cid:104)
  (cid:62)  (cid:0)xik, yjl(cid:1)      (xk, yl, b)

  (cid:62)  (xi, yj)

then,

  (xi, yj, b) = exp

(cid:88)

+

(k,l)   d(x,y)

this is just an unnormalized synchronous deriva-
tion between x and y   at any aligned (node, word)
pair, the score for the entire derivation is the score
produced by combining that word and node, times
the scores at all the aligned descendants. observe
that as long as there are no cycles in the depen-
dency parse, it is perfectly acceptable for the rela-
tion graph to contain cycles and even self-loops   
the recurrence still bottoms out appropriately.

5 learning and id136
given a sequence of training pairs (x, y), we
wish to    nd a parameter setting that maximizes
p(y|x;   ).
if there were no latent alignments a
or b, this would simply involve minimization of
a convex objective. the presence of latent vari-
ables complicates things. ideally, we would like

algorithm 1 computing structure-to-structure
alignments

xi are words in reverse topological order
yj are grounding graph nodes (root last)
chart is an m    n array
for i = 1 to |x| do
for j = 1 to |y| do

score     exp(cid:8)  (cid:62)  (xi, yj)(cid:9)
s    (cid:80)

for (k, l)     d(i, j) do

exp(cid:8)  (cid:62)  (xik, yjl)(cid:9)

(cid:104)

l   c(j)

(cid:105)

   chart[k, l]

score     score    s
end for
chart[i, j]     score

end for

end for
return chart[n, m]

to sum over the latent variables, but that sum is in-
tractable. instead we make a series of variational
approximations:    rst we replace the sum with a
maximization, then perform iterated conditional
modes, alternating between maximization of the
id155 of a and   . we begin by
initializing    randomly.

as noted in the preceding section,

  (x, y) = (cid:80)

the vari-
able b does not appear in these equations. con-
ditioned on a, the sum over structure-to-structure
b   (x, y, b) can be performed ex-
actly using a simple dynamic program which runs
in time o(|x||y|) (assuming out-degree bounded
by a constant, and with |x| and |y| the number of
words and graph nodes respectively). this is al-
gorithm 1.

in our experiments,    is optimized using l-
bfgs (liu and nocedal, 1989). calculation of
the gradient with respect to    requires computa-
tion of a normalizing constant involving the sum
over p(x, y(cid:48), a) for all y(cid:48). while in principle the
normalizing constant can be computed using the
forward algorithm, in practice the state spaces un-
der consideration are so large that even this is in-
tractable. thus we make an additional approxima-
tion, constructing a set   y of alternative actions and
taking

exp(cid:8)  (yj )+(cid:80)m
  y      y exp(cid:8)  (  y)+(cid:80)m
(cid:80)

i=1 1[ai=j]  (xi,yi)(cid:9)
i=1 1[ai=j]  (xi,  y)(cid:9)

p(y, a|x)     n(cid:88)

j=1

  y is constructed by sampling alternative actions
from the environment model. meanwhile, maxi-
mization of a can be performed exactly using the
viterbi algorithm, without computation of normal-
izers.

id136 at test time involves a slightly differ-
ent pair of optimization problems. we again per-
form iterated conditional modes, here on the align-
ments a and the unknown output path y. max-
imization of a is accomplished with the viterbi
algorithm, exactly as before; maximization of y
also uses the viterbi algorithm, or a id125
when this is computationally infeasible. if bounds
on path length are known, it is straightforward to
adapt these dynamic programs to ef   ciently con-
sider paths of all lengths.

6 evaluation
as one of the main advantages of this approach
is its generality, we evaluate on several different
benchmark tasks for instruction following. these
exhibit great diversity in both environment struc-
ture and language use. we compare our full
system to recent state-of-the-art approaches to
each task.
in the introduction, we highlighted
two core aspects of our approach to semantics:
compositionality (by way of grounding graphs
and structure-to-structure alignments) and plan-
ning (by way of id136 with lookahead and
sequence-to-sequence alignments). to evaluate
these, we additionally present a pair of ablation ex-
periments: no grounding graphs (an agent with an
unstructured representation of environment state),
and no planning (a re   ex agent with no looka-
head).
map reading our    rst application is the map
navigation task established by vogel and jurafsky
(2010), based on data collected for a psychological
experiment by anderson et al. (1991) (figure 1a).
each training datum consists of a map with a des-
ignated starting position, and a collection of land-
marks, each labeled with a spatial coordinate and
a string name. names are not always unique, and
landmarks in the test set are never observed dur-
ing training. this map is accompanied by a set
of instructions specifying a path from the start-
ing position to some (unlabeled) destination point.
these instruction sets are informal and redundant,
involving as many as a hundred utterances. they
are transcribed from spoken text, so grammatical
errors, dis   uencies, etc. are common. this is a

vogel and jurafsky (2010)
andreas and klein (2014)
model [no planning]
model [no grounding graphs]
model [full]

p
0.46
0.43
0.44
0.52
0.51

r
0.51
0.51
0.46
0.52
0.60

f1
0.48
0.45
0.45
0.52
0.55

table 1: evaluation results for the map-reading task. p is pre-
cision, r is recall and f1 is f-measure. scores are calculated
with respect to transitions between landmarks appearing in
the reference path (for details see vogel and jurafsky (2010)).
we use the same train / test split. some variant of our model
achieves the best published results on all three metrics.

feature
word=top     side=north
word=top     side=south
word=top     side=east
dist=0
dist=1
dist=4

weight

1.31
0.61
   0.93
4.51
2.78
1.54

table 2: learned feature values. the model learns that the
word top often instructs the navigator to position itself above
a landmark, occasionally to position itself below a landmark,
but rarely to the side. the bottom portion of the table shows
learned text-independent constraints: given a choice, near
destinations are preferred to far ones (so shorter paths are pre-
ferred overall).

prime example of a domain that does not lend it-
self to logical representation   grammars may be
too rigid, and previously-unseen landmarks and
real-valued positions are handled more easily with
feature machinery than predicate logic.

the map task was previously studied by vo-
gel and jurafsky (2010), who implemented sarsa
with a simple set of features. by combining these
features with our alignment model and search pro-
cedure, we achieve state-of-the-art results on this
task by a substantial margin (table 1).

some learned feature values are shown in ta-
ble 2. the model correctly infers cardinal direc-
tions (the example shows the preferred side of a
destination landmark modi   ed by the word top).
like vogel et al., we see support for both allocen-
tric references (you are on top of the hill) and ego-
centric references (the hill is on top of you). we
can also see pragmatics at work: the model learns
useful text-independent constraints   in this case,
that near destinations should be preferred to far
ones.

maze navigation the next application we con-
sider is the maze navigation task of macmahon et
al. (2006) (figure 1b). here, a virtual agent is sit-

success (%)

match (%)

success (%)

kim and mooney (2012)
chen (2012)
model [no planning]
model [no grounding graphs]
model [full]
kim and mooney (2013) [reranked]
artzi et al. (2014) [semi-supervised]

57.2
57.3

58.9
51.7
59.6

62.8
65.3

table 3: evaluation results for the maze navigation task.
   success    shows the percentage of actions resulting in a cor-
rect position and orientation after observing a single instruc-
tion. we use the leave-one-map-out evaluation employed by
previous work.5 all systems are trained on full action se-
quences. our model outperforms several task-speci   c base-
lines, as well as a baseline with path structure but no action
structure.

uated in a maze (whose hallways are distinguished
with various wallpapers, carpets, and the presence
of a small set of standard objects), and again given
instructions for getting from one point to another.
this task has been the subject of focused attention
in id29 for several years, resulting in
a variety of sophisticated approaches.

despite super   cial similarity to the previous
navigation task, the language and plans required
for this task are quite different. the proportion of
instructions to actions is much higher (so redun-
dancy much lower), and the interpretation of lan-
guage is highly compositional.

as can be seen in table 3, we outperform a
number of systems purpose-built for this naviga-
tion task. we also outperform both variants of
our system, most conspicuously the variant with-
out grounding graphs. this highlights the impor-
tance of compositional structure. recent work by
kim and mooney (2013) and artzi et al. (2014)
has achieved better results; these systems make
use of techniques and resources (respectively, dis-
criminative reranking and a seed lexicon of hand-
annotated logical forms) that are largely orthogo-
nal to the ones used here, and might be applied to
improve our own results as well.

puzzle solving the last task we consider is the
crossblock task studied by branavan et al. (2009)
(figure 1c). here, again, natural language is used
to specify a sequence of actions, in this case the
solution to a simple game. the environment is
simple enough to be captured with a    at feature

5we speci   cally targeted the single-sentence version of
this evaluation, as an alternative full-sequence evaluation
does not align precisely with our data condition.

no text
branavan    09
model [no planning]
model [full]

54
63

64
70

78
   

66
86

table 4: results for the puzzle solving task.    match    shows
the percentage of predicted action sequences that exactly
match the annotation.    success    shows the percentage of
predicted action sequences that result in a winning game con-
   guration, regardless of the action sequence performed. fol-
lowing branavan et al. (2009), we average across    ve random
train / test folds. our model achieves state-of-the-art results
on this task.

representation, so there is no distinction between
the full model and the variant without grounding
graphs.

unlike the other tasks we consider, crossblock
is distinguished by a challenging associated search
problem. here it is nontrivial to    nd any sequence
that eliminates all the blocks (the goal of the puz-
zle). thus this example allows us measure the ef-
fectiveness of our search procedure.

results are shown in table 4. as can be seen,
our model achieves state-of-the-art performance
on this task when attempting to match the human-
speci   ed plan exactly. if we are purely concerned
with task completion (i.e. solving the puzzle, per-
haps not with the exact set of moves speci   ed
in the instructions) we can measure this directly.
here, too, we substantially outperform a no-text
baseline. thus it can be seen that text induces a
useful heuristic, allowing the model to solve a con-
siderable fraction of problem instances not solved
by na    ve id125.

the problem of inducing planning heuristics
from side information like text is an important
one in its own right, and future work might focus
speci   cally on coupling our system with a more
sophisticated planner. even at present, the re-
sults in this section demonstrate the importance of
lookahead and high-level reasoning in instruction
following.

7 conclusion

we have described a new alignment-based com-
positional model for following sequences of nat-
ural language instructions, and demonstrated the
effectiveness of this model on a variety of tasks. a
fully general solution to the problem of contextual
interpretation must address a wide range of well-
studied problems, but the work we have described

here provides modular interfaces for the study of
a number of fundamental linguistic issues from a
machine learning perspective. these include:

pragmatics how do we respond to presup-
position failures, and choose among possible
interpretations of an instruction disambiguated
only by context? the mechanism provided by
the sequence-prediction architecture we have de-
scribed provides a simple answer to this ques-
tion, and our experimental results demonstrate that
the learned pragmatics aid interpretation of in-
structions in a number of concrete ways: am-
biguous references are resolved by proximity in
the map reading task, missing steps are inferred
from an environment model in the maze naviga-
tion task, and vague hints are turned into real plans
by knowledge of the rules in crossblock. a more
comprehensive solution might explicitly describe
the process by which instruction-givers    own be-
liefs (expressed as distributions over sequences)
give rise to instructions.

id152 the graph alignment
model of semantics presented here is an expres-
sive and computationally ef   cient generalization
of classical logical techniques to accommodate en-
vironments like the map task, or those explored
in our previous work (andreas and klein, 2014).
more broadly, our model provides a compositional
approach to semantics that does not require an
explicit formal language for encoding sentence
meaning. future work might extend this approach
to tasks like id53, where logic-
based approaches have been successful.

our primary goal in this paper has been to ex-
plore methods for integrating compositional se-
mantics and the pragmatic context provided by se-
quential structures. while there is a great deal
of work left to do, we    nd it encouraging that
this general approach results in substantial gains
across multiple tasks and contexts.

acknowledgments

the authors would like to thank s.r.k. brana-
van for assistance with the crossblock evaluation.
the    rst author is supported by a national science
foundation graduate fellowship.

references
anne h. anderson, miles bader, ellen gurman bard,
elizabeth boyle, gwyneth doherty, simon garrod,
stephen isard, jacqueline kowtko, jan mcallister,
jim miller, et al. 1991. the hcrc map task corpus.
language and speech, 34(4):351   366.

jacob andreas and dan klein. 2014. grounding lan-
guage with points and paths in continuous spaces. in
proceedings of the conference on natural language
learning.

yoav artzi and luke zettlemoyer. 2013. weakly su-
pervised learning of semantic parsers for mapping
instructions to actions. transactions of the associa-
tion for computational linguistics, 1(1):49   62.

yoav artzi, dipanjan das, and slav petrov.

2014.
learning compact lexicons for id35 semantic pars-
in proceedings of the conference on empiri-
ing.
cal methods in natural language processing, pages
1273   1283, doha, qatar, october. association for
computational linguistics.

jonathan berant and percy liang. 2014. semantic
parsing via id141. in proceedings of the an-
nual meeting of the association for computational
linguistics, page 92.

s.r.k. branavan, harr chen, luke s. zettlemoyer, and
regina barzilay. 2009. id23 for
mapping instructions to actions. in proceedings of
the annual meeting of the association for compu-
tational linguistics, pages 82   90. association for
computational linguistics.

s.r.k. branavan, david silver, and regina barzilay.
2011. learning to win by reading manuals in a
monte-carlo framework. in proceedings of the hu-
man language technology conference of the asso-
ciation for computational linguistics, pages 268   
277.

peter brown, vincent della pietra, stephen della
pietra, and robert mercer. 1993. the mathemat-
ics of id151: parameter esti-
mation. computational linguistics, 19(2):263   311,
june.

david l. chen and raymond j. mooney. 2011. learn-
ing to interpret natural language navigation instruc-
tions from observations. in proceedings of the meet-
ing of the association for the advancement of arti   -
cial intelligence, volume 2, pages 1   2.

david l chen. 2012. fast online lexicon learning for
in proceedings of
grounded id146.
the annual meeting of the association for computa-
tional linguistics, pages 430   439.

kais dukes. 2013. semantic annotation of robotic spa-
tial commands. in language and technology con-
ference (ltc).

stefanie tellex, thomas kollar, steven dickerson,
matthew r. walter, ashis gopal banerjee, seth
teller, and nicholas roy. 2011. understanding nat-
ural language commands for robotic navigation and
mobile manipulation. in in proceedings of the na-
tional conference on arti   cial intelligence.

andreas vlachos and stephen clark. 2014. a new cor-
pus and imitation learning framework for context-
dependent id29. transactions of the as-
sociation for computational linguistics, 2:547   559.

adam vogel and dan jurafsky. 2010. learning to
in proceedings of
follow navigational directions.
the annual meeting of the association for compu-
tational linguistics, pages 806   814. association for
computational linguistics.

yuk wah wong and raymond mooney. 2006. learn-
ing for id29 with statistical machine
in proceedings of the human lan-
translation.
guage technology conference of the north ameri-
can chapter of the association for computational
linguistics, pages 439   446, new york, new york.

luke s. zettlemoyer and michael collins.

2005.
learning to map sentences to logical form: struc-
tured classi   cation with probabilistic categorial
grammars. in proceedings of the conference on un-
certainty in arti   cial intelligence, pages 658   666.

bevan

jones,

jacob andreas, daniel bauer,
karl moritz hermann, and kevin knight. 2012.
semantics-based machine translation with hyper-
in proceedings of
edge replacement grammars.
the international conference on computational
linguistics, pages 1359   1376.

joohyun kim and raymond j. mooney. 2012. un-
supervised pid18 induction for grounded language
learning with highly ambiguous supervision. in pro-
ceedings of the conference on empirical methods in
natural language processing, pages 433   444.

joohyun kim and raymond j. mooney. 2013. adapt-
ing discriminative reranking to grounded language
learning. in proceedings of the annual meeting of
the association for computational linguistics.

tom kwiatkowski, eunsol choi, yoav artzi, and luke
zettlemoyer. 2013. scaling semantic parsers with
on-the-   y ontology matching. in proceedings of the
conference on empirical methods in natural lan-
guage processing.

percy liang, michael i. jordan, and dan klein. 2013.
learning dependency-based compositional seman-
tics. computational linguistics, 39(2):389   446.

dong liu and jorge nocedal. 1989. on the limited
memory bfgs method for large scale optimization.
mathematical programming, 45(1-3):503   528.

matt macmahon, brian stankiewicz, and benjamin
kuipers. 2006. walk the talk: connecting language,
knowledge, and action in route instructions. pro-
ceedings of the meeting of the association for the
advancement of arti   cial intelligence, 2(6):4.

brian mcmahan and matthew stone.

2015. a
bayesian model of grounded color semantics.
transactions of the association for computational
linguistics, 3:103   115.

karthik narasimhan, tejas kulkarni, and regina
barzilay. 2015. language understanding for text-
based games using deep id23. in
proceedings of the conference on empirical meth-
ods in natural language processing.

terence parsons. 1990. events in the semantics of en-

glish. mit press.

panupong pasupat and percy liang. 2015. composi-
tional id29 on semi-structured tables. in
proceedings of the annual meeting of the associa-
tion for computational linguistics.

nima pourdamghani, yang gao, ulf hermjakob, and
kevin knight. 2014. aligning english strings with
id15 graphs. in proceed-
ings of the conference on empirical methods in nat-
ural language processing.

siva reddy, mirella lapata, and mark steedman.
2014.
large-scale id29 without
question-answer pairs. transactions of the associ-
ation for computational linguistics, 2:377   392.

