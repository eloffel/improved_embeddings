multi-task cross-lingual sequence tagging from scratch

zhilin yang ruslan salakhutdinov william cohen

school of computer science
carnegie mellon university

{zhiliny,rsalakhu,wcohen}@cs.cmu.edu

6
1
0
2

 

g
u
a
9

 

 
 
]
l
c
.
s
c
[
 
 

2
v
0
7
2
6
0

.

3
0
6
1
:
v
i
x
r
a

abstract

we present a deep hierarchical recur-
rent neural network for sequence tagging.
given a sequence of words, our model em-
ploys deep id149 on both
character and word levels to encode mor-
phology and context information, and ap-
plies a conditional random    eld layer to
predict the tags. our model is task inde-
pendent, language independent, and fea-
ture engineering free. we further ex-
tend our model to multi-task and cross-
lingual joint training by sharing the ar-
chitecture and parameters. our model
achieves state-of-the-art results in multiple
languages on several benchmark tasks in-
cluding id52, chunking, and ner.
we also demonstrate that multi-task and
cross-lingual joint training can improve
the performance in various cases.

introduction

1
sequence tagging is a fundamental problem in nat-
ural language processing which has many wide
applications, including part-of-speech (pos) tag-
ging, chunking, and id39
(ner). given a sequence of words, sequence tag-
ging aims to predict a linguistic tag for each word
such as the pos tag. recently progress has been
made on neural sequence-tagging models which
make only minimal assumptions about the lan-
guage, task, and feature set (collobert et al., 2011)
this paper explores an important potential
advantage of these task-independent,
language-
independent and feature-engineering free mod-
els:
their ability to be jointly trained on multi-
ple tasks. in particular, we explore two types of
joint training. in multi-task joint training, a model
is jointly trained to perform multiple sequence-

tagging tasks in the same language   e.g., pos
in cross-lingual
tagging and ner for english.
joint training, a model is trained to perform the
same task in multiple languages   e.g., ner in
english and spanish.

multi-task joint training can exploit the fact that
different sequence tagging tasks in one language
share language-speci   c regularities. for example,
models of english id52 and english ner
might bene   t from using similar underlying rep-
resentations for words, and in past work, certain
sequence-tagging tasks have bene   tted by lever-
aging the underlying similarity of related tasks
(ando and zhang, 2005). currently, however, the
best results on speci   c sequence-tagging tasks are
usually achieved by approaches that target only
one speci   c task, either id52 (s  gaard,
2011; toutanova et al., 2003), chunking (shen and
sarkar, 2005), or ner (luo et al., 2015; pas-
sos et al., 2014). such approaches employ sep-
arate model development for each individual task,
which makes joint training dif   cult. in other work,
some recent neural approaches have been pro-
posed to address multiple sequence tagging prob-
lems in a uni   ed framework (huang et al., 2015).
though gains have been shown using multi-task
joint training, the prior models that bene   t from
multi-task joint training did not achieve state-of-
the-art performance (collobert et al., 2011); thus
the question of whether joint training can improve
over strong baseline methods is still unresolved.

cross-lingual joint training typically uses word
alignments or parallel corpora to improve the per-
formance on different languages (kiros et al.,
2014; gouws et al., 2014). however, many suc-
cessful approaches in sequence tagging rely heav-
ily on feature engineering to handcraft language-
dependent features, such as character-level mor-
phological features and word-level id165 pat-
terns (huang et al., 2015; toutanova et al., 2003;

sun et al., 2008), making it dif   cult to share la-
tent representations between different languages.
some multilingual taggers that do not rely on fea-
ture engineering have also been presented (lam-
ple et al., 2016; dos santos et al., 2015), but while
these methods are language-independent, they do
not study the effect of cross-lingual joint training.
in this work, we focus on developing a general
model that can be applied in both multi-task and
cross-lingual settings by learning from scratch,
i.e., without feature engineering or pipelines.
given a sequence of words, our model employs
deep id149 on both character and
word levels, and applies a conditional random    eld
layer to make the id170. on the
character level, the id149 capture
the morphological information; on the word level,
the id149 learn id165 patterns and
word semantics.

our model can handle both multi-task and
cross-lingual joint training in a uni   ed manner
by simply sharing the network architecture and
model parameters between tasks and languages.
for multi-task joint training, we share both char-
acter and word level parameters between tasks
to learn language-speci   c regularities. for cross-
lingual joint training, we share the character-level
parameters to capture the morphological similarity
between languages without use of parallel corpora
or word alignments.

we evaluate our model on    ve datasets of differ-
ent tasks and languages, including id52,
chunking and ner in english; and ner in dutch
and spanish. we achieve state-of-the-art results
on several standard benchmarks: conll 2000
chunking (95.41%), conll 2002 dutch ner
(85.19%), conll 2003 spanish ner (85.77%),
and conll 2003 english ner (91.20%). we also
achieve very competitive results on penn tree-
bank id52 (97.55%, the second best re-
sult in the literature). finally, we conduct experi-
ments to systematically explore the effectiveness
of multi-task and cross-lingual joint training on
several tasks.

2 related work

ando and zhang (2005) proposed a multi-task
joint training framework that shares structural pa-
rameters among multiple tasks, and improved the
performance on various tasks including ner. col-
lobert et al.
(2011) presented a task indepen-

dent convolutional network and employed multi-
task joint training to improve the performance of
chunking. however, there is still a gap between
these multi-task approaches and the state-of-the-
art results on individual tasks. furthermore, it is
unclear whether these approaches can be effective
in a cross-lingual setting.

multilingual resources were extensively used
for cross-lingual sequence tagging through vari-
ous ways, such as cross-lingual feature extraction
(darwish, 2013), text categorization (virga and
khudanpur, 2003), and bayesian parallel data pre-
diction (snyder et al., 2008). parallel corpora and
word alignments are also used for training cross-
lingual distributed word representations (kiros et
al., 2014; gouws et al., 2014; zhou et al., 2015).
unlike these approaches, our method mainly fo-
cuses on using morphological similarity for cross-
lingual joint training.

several neural architectures based on recurrent
networks were proposed for sequence tagging.
huang et al. (2015) used word-level long short-
term memory (lstm) units based on handcrafted
features; dos santos et al. (2015) employed convo-
lutional layers on both character and word levels;
chiu and nichols (2015) applied convolutional
layers on the character level and lstm units on
the word level; gillick et al.
(2015) employed
a sequence-to-sequence lstm with a novel tag-
ging scheme. we show that our architecture gives
better performance experimentally than these ap-
proaches in section 5.

most similar to our work is the recent approach
independently developed by lample et al. (2016)
(published two weeks before our submission),
which employs lstm on both character and word
levels. however, there are several crucial differ-
ences. first, we study cross-lingual joint train-
ing and show improvement over their approach
in various cases. second, while they mainly fo-
cus on ner, we generalize our model to other se-
quence tagging tasks, and also demonstrate the ef-
fectiveness of multi-task joint training. there are
also differences in the technical aspect, such as
the cost-sensitive id168 and gated recurrent
units used in our work.

3 model

in this section, we present our model for sequence
tagging based on deep hierarchical gated recurrent
units and conditional random    elds. our recurrent

figure 1: the architecture of our hierarchical gru network with crf, when lc = lw = 1 (only one layer for word-level
and character-level grus respectively). we only display the character-level gru for the word mike and omit others.

networks are hierarchical since we have multiple
layers on both word and character levels in a hier-
archy.

3.1 gated recurrent unit
a gated recurrent unit (gru) network is a type
of recurrent neural networks    rst introduced for
machine translation (cho et al., 2014). a recur-
rent network can be represented as a sequence
of units, corresponding to the input sequence
(x1, x2,       , xt ), which can be either a word se-
quence in a sentence or a character sequence in a
word. the unit at position t takes xt and the pre-
vious hidden state ht   1 as input, and outputs the
current hidden state ht. the model parameters are
shared between different units in the sequence.

a gated recurrent unit at position t has two
gates, an update gate zt and a reset gate rt. more
speci   cally, each gated recurrent unit can be ex-
pressed as follows

rt =   (wrxxt + wrhht   1)
zt =   (wzxxt + wzhht   1)
  ht = tanh(whxxt + whh(rt (cid:12) ht   1))
ht = zt (cid:12) ht   1 + (1     zt) (cid:12)   ht,

where w    s are model parameters of each unit,   ht
is a candidate hidden state that is used to compute
ht,    is an element-wise sigmoid logistic function
de   ned as   (x) = 1/(1 + e   x), and (cid:12) denotes
element-wise multiplication of two vectors. intu-
itively, the update gate zt controls how much the
unit updates its hidden state, and the reset gate rt
determines how much information from the previ-
ous hidden state needs to be reset.

since a recurrent neural network only models
the information    ow in one direction, it is usually
helpful to use an additional recurrent network that
goes in the reverse direction. more speci   cally,
we use bidirectional id149, where
given a sequence of length t , we have one gru
going from 1 to t and the other from t to 1. let
      
      
h t and
h t denote the hidden states at position t
of the forward and backward grus respectively.
we concatenate the two hidden states to form the
   nal hidden state ht = [

      
h t].

      
h t,

we stack multiple recurrent layers together to
form a deep recurrent network (sutskever et al.,
2014). each layer learns a more effective repre-
sentation taking the hidden states of the previous
layer as input. let hl,t denote the hidden state at
position t in layer l. the forward gru at posi-
      
tion t in layer l computes
h l,t   1 and
hl   1,t as input, and the backward gru performs
similar operations but in a reverse direction.

      
h l,t using

3.2 hierarchical gru

our model employs a hierarchical gru that en-
codes both word-level and character-level sequen-
tial information.

the input of our model is a sequence of words
(x1, x2,       , xt ) of length t , where xt is a one-
of-k embedding of the t-th word. the word at
each position t also has a character-level repre-
sentation, denoted as a sequence of length st,
(ct,1, ct,2,       , ct,st) where ct,s is the one-of-k
embedding of the s-th character in the t-th word.

3.2.1 character-level gru
given a word, we    rst employ a deep bidirectional
gru to learn useful morphological representation
from the character sequence of the word. suppose
the character-level gru has lc layers, we then
      
obtain forward and backward hidden states
h lc,s
      
and
h lc,s at each position s in the character se-
quence. since recurrent networks usually tend to
memorize more short-term patterns, we concate-
nate the    rst hidden state of the backward gru
and the last hidden state of the forward gru to en-
code character-level morphology in both pre   xes
and suf   xes. we further concatenate the character-
level representation with the one-of-k word em-
bedding xt to form the    nal representation hw
t for
the t-th word. more speci   cally, we have

      
h lc,st,

      
h lc,1, xt],

hw

t = [

where hw
is a representation of the t-th word,
t
which encodes both character-level morphology
and word-level semantics, as shown in figure 1.
3.2.2 word-level gru
the character-level gru outputs a sequence of
2 ,       , hw
word representations hw = (hw
t ).
we employ a word-level deep bidirectional gru
with lw layers on top of these word representa-
tions. the word-level gru takes the sequence hw
as input, and computes a sequence of hidden states
h = (h1, h2,       , ht ).

1 , hw

different from the character-level gru, the
word-level gru aims to extract the context in-
formation in the word sequence, such as id165
patterns and neighbor word dependencies. such
information is usually encoded using handcrafted
features. however, as we show in our experimen-
tal results, the word-level gru can learn the rele-
vant information without being language-speci   c
or task-speci   c. the hidden states h output by the
word-level gru will be used as input features for
the next layers.

3.3 conditional random field
the goal of sequence tagging is to predict a se-
quence of tags y = (y1, y2,       , yt ). to model
the dependencies between tags in a sequence, we
apply a conditional random    eld (lafferty et al.,
2001) layer on top of the hidden states h output
by the word-level gru (huang et al., 2015). let
y(h) denote the space of tag sequences for h. the
conditional log id203 of a tag sequence y,

given the hidden state sequence h, can be written
as
log p(y|h) = f (h, y)     log

exp f (h, y(cid:48)),

(cid:88)

y(cid:48)   y(h)

(1)
where f is a function that assigns a score for each
pair of h and y.

to de   ne the function f (h, y), for each posi-
tion t, we multiply the hidden state hw
t with a pa-
rameter vector wyt that is indexed by the the tag
yt, to obtain the score for assigning yt at position t.
since we also need to consider the correlation be-
tween tags, we impose    rst order dependency by
adding a score ayt   1,yt at position t, where a is
a parameter matrix de   ning the similarity scores
between different tag pairs. formally, the function
f can be written as

t(cid:88)

t(cid:88)

f (h, y) =

wt

ythw

t +

ayt   1,yt,

t=1

t=1

where we set y0 to be a start token.

it is possible to directly maximize the condi-
tional log likelihood based on eq. (1). however,
this training objective is usually not optimal since
each possible y(cid:48) contributes equally to the objec-
tive function. therefore, we add a cost function
between y and y(cid:48) based on the max-margin princi-
ple that high-cost tags y(cid:48) should be penalized more
heavily (gimpel and smith, 2010). more speci   -
cally, the objective function to maximize for each
training instance y and h is written as
f (h, y)    log

exp(f (h, y(cid:48)) + cost(y, y(cid:48))).

(cid:88)

y(cid:48)   y(h)

(2)
in our work, the cost function is de   ned as
the tag-wise hamming loss between two tag se-
quences multiplied by a constant. the objective
function on the training set is the sum of eq. (2)
over all the training instances. the full architec-
ture of our model is illustrated in figure 1.

3.4 training
we employ mini-batch adagrad (duchi et al.,
2011) to train our neural network in an end-to-
end manner with id26. both the char-
acter embeddings and id27s are    ne-
tuned during training. we use dynamic program-
ming to compute the normalizer of the crf layer
in eq. (2). when making prediction, we again use
id145 in the crf layer to decode
the most probable tag sequence.

algorithm in the multi-task setting. suppose we
have d tasks, with the training instances of each
task being (x1, x2,       , xd). each task d has a
set of model parameters wd, which is divided into
two sets, task speci   c parameters and shared pa-
rameters, i.e.,

wd = wd,spec     wshared,

where shared parameters wshared are a set of pa-
rameters that are shared among the d tasks, while
task speci   c parameters wd,spec are the rest of the
parameters that are trained for each task d sepa-
rately.

during joint training, we are optimizing the av-
erage over all objective functions of d tasks. we
iterate over each task d, sample a batch of training
instances from xd, and perform a id119
step to update model parameters wd. similarly,
we can derive a cross-lingual joint training algo-
rithm by replacing d tasks with d languages.

the network architectures we employ for joint
training are illustrated in figure 2. for multi-task
joint training, we share all the parameters below
the crf layer including id27s to learn
language-speci   c regularities shared by the tasks.
for cross-lingual joint training, we share the pa-
rameters of the character-level gru to capture
the morphological similarity between languages.
note that since we do not consider using paral-
lel corpus in this work, we mainly focus on joint
training between languages with similar morphol-
ogy. we leave the study of cross-lingual joint
training by sharing word semantics based on par-
allel corpora to future work.

5 experiments
in this section, we use several benchmark datasets
for multiple tasks in multiple languages to evaluate
our model as well as the joint training algorithm.

5.1 datasets and settings
we use the following benchmark datasets in our
experiments: id32 (ptb) id52,
conll 2000 chunking, conll 2003 english
ner, conll 2002 dutch ner and conll 2002
spanish ner. the statistics of the datasets are de-
scribed in table 1.

we construct the id52 dataset with the
instructions described in toutanova et al. (2003).
note that as a standard practice, the pos tags are
extracted from the parsed trees.

(a) multi-task joint training

(b) cross-lingual joint training

figure 2: network architectures for multi-task and cross-
lingual joint training. red boxes indicate shared architecture
and parameters. blue boxes are task/language speci   c com-
ponents trained separately. eng, span, char, and emb refer
to english, spanish, character and embeddings.

4 multi-task and cross-lingual joint

training

in this section we study joint training of multiple
tasks and multiple languages. on one hand, differ-
ent sequence tagging tasks in the same language
share language-speci   c regularities. for exam-
ple, id52 and ner in english should learn
similar underlying representation since they are in
the same language. on the other hand, some lan-
guages share character-level morphologies, such
as english and spanish. therefore, it is desirable
to leverage multi-task and cross-lingual joint train-
ing to boost model performance.

since our model is generally applicable to dif-
ferent tasks in different languages, it can be nat-
urally extended to multi-task and cross-lingual
joint training. the basic idea is to share part of
the architecture and parameters between tasks and
languages, and to jointly train multiple objective
functions with respect to different tasks and lan-
guages.

we now discuss the details of our joint training

benchmark

task

language

# training tokens

# dev tokens

# test tokens

table 1: dataset statistics

ptb (2003)
conll 2000 chunking
conll 2003 ner
conll 2002 ner
conll 2002 ner

id52 english
english
english
dutch
spanish

912,344
211,727
204,567
202,931
207,484

131,768
-
51,578
37,761
51,645

129,654
47,377
46,666
68,994
52,098

table 2: comparison with state-of-the-art results on
conll 2003 english ner when trained with training set
only.     means using handcrafted features.     means being
task-speci   c.

model
chieu et al. (2002)      
florian et al. (2003)      
ando and zhang (2005)   
lin and wu (2009)      
collobert et al. (2011)
huang et al. (2015)   
ours

f1 (%)

88.31
88.76
89.31
90.90
89.59
90.10
90.94

for the task of conll 2003 english ner,
we follow previous works (collobert et al., 2011;
huang et al., 2015; chiu and nichols, 2015) to ap-
pend one-hot gazetteer features to the input of the
crf layer for fair comparison.1

we set the hidden state dimensions to be 300 for
the word-level gru. we set the number of gru
layers to lc = lw = 2 (two layers for the word-
level and character-level grus respectively). the
learning rate is    xed at 0.01. we use the develop-
ment set to tune the other hyperparameters of our
model. since the conll 2000 chunking dataset
does not have a development set, we hold out one
   fth of the training set for parameter tuning.

we truncate all words whose character sequence
length is longer than a threshold (17 for english,
35 for dutch, and 20 for spanish). we replace
all numeric characters with    0   . we also use the
bioes (begin, inside, outside, end, single) tag-
ging scheme (ratinov and roth, 2009).

5.2 pre-trained id27s
since the training corpus for a sequence tagging
task is relatively small, it is dif   cult to train ran-

table 3: comparison with state-of-the-art results on
conll 2003 english ner when trained with both training
and dev sets.     means using handcrafted features.     means
being task-speci   c.     means not using gazetteer lists.
f1 (%)

model
ratinov and roth (2009)      
passos et al. (2014)      
chiu and nichols (2015)
luo et al. (2015)      
lample et al. (2016)   
ours
ours     no gazetteer   
ours     no char gru
ours     no id27s

90.80
90.90
90.77
91.2
90.94
91.20
90.96
88.00
77.20

domly initialized id27s to accurately
capture the word semantics. therefore, we lever-
age id27s pre-trained on large-scale
corpora. all the pre-trained embeddings we use
are publicly available.

on the english datasets, following previous
works that are based on neural networks (col-
lobert et al., 2011; huang et al., 2015; chiu
and nichols, 2015), we use the 50-dimensional
senna embeddings2 trained on wikipedia. for
spanish and dutch, we use the 64-dimensional
polyglot embeddings3 (al-rfou et al., 2013),
which are trained on wikipedia articles of the cor-
responding languages. we use pre-trained word
embeddings as initialization, and    ne-tune the em-
beddings during training.

5.3 performance
in this section, we report the results of our model
on the benchmark datasets and compare to the
previously-reported state-of-the-art results.

1although gazetteers are arguably a type of feature engi-
neering, we note that unlike most feature engineering tech-
niques they are straightforward to include in a model. we use
only the gazetteer    le provided by the conll 2003 shared
task, and do not use gazetteers for any other tasks or lan-
guages described here.

2http://ronan.collobert.com/senna/
3https://sites.google.com/site/rmyeid/

projects/polyglot

4we note that this number is often mistakenly cited as
95.23, which is actually the score on base np chunking rather
than conll 2000.

table 4: comparison with state-of-the-art results on
conll 2002 dutch ner.     means using handcrafted fea-
tures.     means being task-speci   c.

model
carreras et al. (2002)      
nothman et al. (2013)      
gillick et al. (2015)
lample et al. (2016)
ours
ours + joint training
ours     no char gru
ours     no id27s

f1 (%)

77.05
78.6
82.84
81.74
85.00
85.19
77.76
67.36

table 5: comparison with state-of-the-art results on
conll 2002 spanish ner.     means using handcrafted fea-
tures.     means being task-speci   c.

model
carreras et al. (2002)      
dos santos et al. (2015)
gillick et al. (2015)
lample et al. (2016)
ours
ours + joint training
ours     no char gru
ours     no id27s

f1 (%)

81.39
82.21
82.95
85.75
84.69
85.77
83.03
73.34

for english ner,

there are two evaluation
methods used in the literature. some models are
trained with both the training and development set,
while others are trained with the training set only.
we report our results in both cases.
in the    rst
case, we tune the hyperparameters by training on
the training set and testing on the development set.
besides our standalone model, we experi-
mented with multi-task and cross-lingual
joint
training as well, using the architecture described in
section 4. for multi-task joint training, we jointly
train all tasks in english, including id52,
chunking and ner. for cross-lingual joint train-
ing, we jointly train ner in english, dutch and
spanish. we also remove the id27s
and the character-level gru respectively to ana-
lyze the contribution of different components.

the results are shown in tables 2, 3, 4, 5, 6
and 7. we achieve state-of-the-art results on en-
glish ner, dutch ner, spanish ner and en-
glish chunking. our model outperforms the best
previously-reported results on dutch ner and en-
glish chunking by 2.35 points and 0.95 points re-
spectively. we also achieve the second best re-

2-dimensional

figure 3:
id167 visualization of the
character-level gru output for country names in english and
spanish. black words are english and red ones are spanish.
note that all corresponding pairs are nearest neighbors in the
original embedding space.

table 6: comparison with state-of-the-art results on
conll 2000 english chunking.     means using handcrafted
features.     means being task-speci   c.

model
kudo and matsumoto (2001)      
shen and sarkar (2005)      
sun et al. (2008)      
collobert et al. (2011)
huang et al. (2015)   
ours
ours + joint training
ours     no char gru
ours     no id27s

f1 (%)

93.91
94.014
94.34
94.32
94.46
94.66
95.41
94.44
88.13

sult on english id52, which is 0.23 points
worse than the current state-of-the-art.

joint

training improves the performance on
spanish ner, dutch ner and english chunking
by 1.08 points, 0.19 points and 0.75 points respec-
tively, and has no signi   cant improvement on en-
glish id52 and english ner.

on id52, the best result is 97.78% re-
ported by ling et al. (2015). however, the em-
beddings they used are not publicly available. to
demonstrate the effectiveness of our model, we
slightly revise our model to reimplement their
model with the same parameter settings described
in their original paper. we use senna embed-
dings to initialize the reimplemented model for
fair comparison, and obtain an accuracy of 97.41%
that is 0.14 points worse than our result, which in-
dicates that our model is more effective and the
main difference lies in using different pre-trained
embeddings.

by comparing the results without the character-

table 7: comparison with state-of-the-art results on ptb
id52.     means using handcrafted features.     means
being task-speci   c.     indicates our reimplementation (using
senna embeddings).

model
toutanova et al. (2003)      
shen et al. (2007)      
s  gaard et al. (2011)      
collobert et al. (2011)
huang et al. (2015)   
ling et al. (2015)
ling et al. (2015) (senna)   
ours (senna)
ours     no char gru
ours     no id27s

accuracy (%)

97.24
97.33
97.50
97.29
97.55
97.78
97.41
97.55
96.69
95.43

level gru and without id27s, we can
observe that both components contribute to the
   nal results.
it is also clear that word embed-
dings have signi   cantly more contribution than
the character-level gru, which indicates that our
model largely depends on memorizing the word
semantics. character-level morphology, on the
other hand, has relatively smaller but still critical
contribution.

joint training

5.4
in this section, we analyze the effectiveness of
multi-task and cross-lingual joint training in more
detail. in order to explore possible gains in per-
formance of joint training for resource-poor lan-
guages or tasks, we consider joint training of vari-
ous task pairs and language pairs where different-
sized subsets of the actual labeled corpora are
made available. given a pair of tasks of languages,
we jointly train one task with full labels and the
other with partial labels. in particular, we intro-
duce a labeling rate r, and sample a fraction r
of the sentences in the training set, discarding the
rest. evaluation is based on the partially-labeled
task. the results are reported in table 8.

we observe that the performance of a speci   c
task with relatively lower labeling rates (0.1 and
0.3) can usually bene   t from other tasks with
full labels through multi-task or cross-lingual joint
training. the performance gain can be up to 1.99
points when the labeling rate of the target task
is 0.1. the improvement with 0.1 labeling rate is
on average 0.37 points larger than with 0.3 label-
ing rate, which indicates that the improvement of
joint training is more signi   cant when the target

table 8: multi-task and cross-lingual joint training. we
compare the results obtained by a standalone model and joint
training with another task or language. the number following
a task is the labeling rate (0.1 or 0.3). eng and ner both refer
to english ner, span means spanish. in the column titles,
task is the target task, j. task is the jointly-trained task with
full labels, sep. is the f1/accuracy of the target task trained
separately, joint is the f1/accuracy of the target task with
joint training, and delta is the improvement.

task

j. task sep.

joint delta

eng
span 0.1
eng
span 0.3
span
eng 0.1
span
eng 0.3
ner
pos 0.1
ner
pos 0.3
pos
ner 0.1
ner 0.3
pos
chunk 0.1 ner
chunk 0.3 ner

74.53
80.81
86.21
88.54
96.59
97.03
86.21
88.54
90.65
92.51

76.52 +1.99
80.20 +0.61
86.51 +0.30
88.79 +0.25
96.79 +0.20
97.14 +0.11
87.02 +0.81
89.16 +0.62
91.16 +0.51
92.87 +0.36

task has less labeled data.

we also use id167 (van der maaten and hin-
ton, 2008) to obtain a 2-dimensional visualization
of the character-level gru output for the coun-
try names in english and spanish, shown in fig-
ure 3. we can clearly see that our model cap-
tures the morphological similarity between two
languages through joint training, since all corre-
sponding pairs are nearest neighbors in the origi-
nal embedding space.

6 conclusion

we presented a new model for sequence tagging
based on id149 and conditional ran-
dom    elds. we explored multi-task and cross-
lingual joint training through sharing part of the
network architecture and model parameters. we
achieved state-of-the-art results on various tasks
including id52, chunking, and ner, in
multiple languages. we also demonstrated that
joint training can improve model performance in
various cases.

in this work, we mainly focus on leveraging
morphological similarities for cross-lingual joint
training. in the future, an important problem will
be joint training based on cross-lingual word se-
mantics with the help of parallel data. further-
more,
it will be interesting to apply our joint
training approach to low-resource tasks and lan-
guages.

acknowledgements

this work was funded by the nsf under grant iis-
1250956.

references
[al-rfou et al.2013] rami al-rfou, bryan perozzi, and
steven skiena. 2013. polyglot: distributed word
representations for multilingual nlp. in acl.

[ando and zhang2005] rie kubota ando and tong
zhang. 2005. a framework for learning predictive
structures from multiple tasks and unlabeled data.
jmlr, 6:1817   1853.

[carreras et al.2002] xavier carreras, lluis marquez,
and llu    s padr  o. 2002. named entity extraction us-
ing adaboost. in conll, pages 1   4.

[chieu and ng2002] hai leong chieu and hwee tou
ng. 2002. id39: a maximum
id178 approach using global information. in col-
ing, pages 1   7.

[chiu and nichols2015] jason pc chiu and eric
id39
arxiv preprint

lstm-id98s.

nichols.
2015.
with bidirectional
arxiv:1511.08308.

[cho et al.2014] kyunghyun

bart
van
merri  enboer, dzmitry bahdanau,
and yoshua
bengio. 2014. on the properties of neural machine
translation: encoder-decoder approaches. in acl.

cho,

[collobert et al.2011] ronan collobert, jason weston,
l  eon bottou, michael karlen, koray kavukcuoglu,
and pavel kuksa. 2011. natural language process-
ing (almost) from scratch. jmlr, 12:2493   2537.

[darwish2013] kareem darwish. 2013. named entity
recognition using cross-lingual resources: arabic as
an example. in acl, pages 1558   1567.

[dos santos et al.2015] c  cero

dos santos, victor
guimaraes, rj niter  oi, and rio de janeiro. 2015.
boosting id39 with neural
in proceedings of news
character embeddings.
2015 the fifth named entities workshop, page 25.

[duchi et al.2011] john duchi, elad hazan, and yoram
singer. 2011. adaptive subgradient methods for
online learning and stochastic optimization. jmlr,
12:2121   2159.

[gimpel and smith2010] kevin gimpel and noah a
smith. 2010. softmax-margin crfs: training log-
linear models with cost functions. in naacl, pages
733   736.

[gouws et al.2014] stephan gouws, yoshua bengio,
and greg corrado. 2014. bilbowa: fast bilingual
distributed representations without word alignments.
in icml.

[huang et al.2015] zhiheng huang, wei xu, and kai
yu. 2015. bidirectional lstm-crf models for se-
quence tagging. arxiv preprint arxiv:1508.01991.

[kiros et al.2014] ryan kiros, richard zemel, and
ruslan r salakhutdinov. 2014. a multiplicative
model for learning distributed text-based attribute
representations. in nips, pages 2348   2356.

[kudo and matsumoto2001] taku kudo and yuji mat-
sumoto. 2001. chunking with support vector ma-
chines. in naacl, pages 1   8.

[lafferty et al.2001] john lafferty, andrew mccallum,
and fernando cn pereira. 2001. conditional ran-
dom    elds: probabilistic models for segmenting and
labeling sequence data. in icml.

[lample et al.2016] guillaume lample, miguel balles-
teros, sandeep subramanian, kazuya kawakami,
neural architectures
and chris dyer.
arxiv preprint
for id39.
arxiv:1603.01360.

2016.

[lin and wu2009] dekang lin and xiaoyun wu. 2009.
in

phrase id91 for discriminative learning.
acl, pages 1030   1038.

[ling et al.2015] wang ling, tiago lu    s, lu    s marujo,
ram  on fernandez astudillo, silvio amir, chris
dyer, alan w black, and isabel trancoso. 2015.
finding function in form: compositional character
models for open vocabulary word representation. in
emnlp.

[luo et al.2015] gang luo, xiaojiang huang, chin-
yew lin, and zaiqing nie. 2015. joint named entity
recognition and disambiguation. in acl.

[nothman et al.2013] joel nothman, nicky ringland,
will radford, tara murphy, and james r cur-
ran.
2013. learning multilingual named entity
recognition from wikipedia. arti   cial intelligence,
194:151   175.

[florian et al.2003] radu florian, abe ittycheriah,
hongyan jing, and tong zhang. 2003. named en-
tity recognition through classi   er combination.
in
hlt-naacl, pages 168   171.

[passos et al.2014] alexandre passos, vineet kumar,
and andrew mccallum. 2014. lexicon infused
phrase embeddings for named entity resolution. in
acl.

[gillick et al.2015] dan gillick, cliff brunk, oriol
vinyals, and amarnag subramanya. 2015. mul-
arxiv
tilingual language processing from bytes.
preprint arxiv:1512.00103.

[ratinov and roth2009] lev ratinov and dan roth.
2009. design challenges and misconceptions in
in conll, pages 147   
id39.
155.

[shen and sarkar2005] hong shen and anoop sarkar.
2005. voting between multiple data representations
for text chunking. springer.

[shen et al.2007] libin shen, giorgio satta, and ar-
avind joshi. 2007. guided learning for bidirectional
sequence classi   cation. in acl, pages 760   767.

[snyder et al.2008] benjamin snyder, tahira naseem,
jacob eisenstein, and regina barzilay. 2008. un-
supervised multilingual learning for id52. in
emnlp, pages 1041   1050.

[s  gaard2011] anders s  gaard. 2011. semisupervised
condensed nearest neighbor for part-of-speech tag-
ging. in acl, pages 48   52.

[sun et al.2008] xu sun, louis-philippe morency,
daisuke okanohara, and jun   ichi tsujii.
2008.
modeling latent-dynamic in id66: a la-
tent conditional model with improved id136. in
coling, pages 841   848.

[sutskever et al.2014] ilya sutskever, oriol vinyals,
and quoc v le. 2014. sequence to sequence learn-
in nips, pages 3104   
ing with neural networks.
3112.

[toutanova et al.2003] kristina toutanova, dan klein,
christopher d manning, and yoram singer. 2003.
feature-rich part-of-speech tagging with a cyclic de-
pendency network. in naacl, pages 173   180.

[van der maaten and hinton2008] laurens van

der
maaten and geoffrey hinton. 2008. visualizing
data using id167. jmlr, 9(2579-2605):85.

[virga and khudanpur2003] paola virga and sanjeev
khudanpur. 2003. id68 of proper names
in proceed-
in cross-lingual information retrieval.
ings of the acl 2003 workshop on multilingual and
mixed-language id39-volume
15, pages 57   64.

[zhou et al.2015] huiwei zhou, long chen, fulin shi,
and degen huang. 2015. learning bilingual sen-
timent id27s for cross-language senti-
ment classi   cation. in acl.

