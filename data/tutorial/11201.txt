classifying syntactic regularities for hundreds of languages

reed coke

department of eecs
university of michigan

ann arbor

ben king

ubiquiti, inc.

benking@ubiquiti.com

dragomir radev

department of eecs
school of information
university of michigan

ann arbor

6
1
0
2

 
r
p
a
7
2

 

 
 
]
l
c
.
s
c
[
 
 

2
v
6
1
0
8
0

.

3
0
6
1
:
v
i
x
r
a

reedcoke@umich.edu

abstract

this paper presents a comparison of classi   -
cation methods for linguistic typology for the
purpose of expanding an extensive, but sparse
language resource:
the world atlas of lan-
guage structures (wals) (dryer and haspel-
math, 2013). we experimented with a vari-
ety of regression and nearest-neighbor meth-
ods for use in classi   cation over a set of 325
languages and six syntactic rules drawn from
wals. to classify each rule, we consider the
typological features of the other    ve rules; lin-
guistic features extracted from a word-aligned
bible in each language; and genealogical fea-
tures (genus and family) of each language.
in general, we    nd that propagating the ma-
jority label among all languages of the same
genus achieves the best accuracy in label pre-
diction. following this, a id28
model that combines typological and linguis-
tic features offers the next best performance.
interestingly, this model actually outperforms
the majority labels among all languages of the
same family.

introduction

1
linguistic typology is a sub   eld of linguistics con-
cerned with understanding the various patterns that
are present across the world   s languages and how
languages can be grouped via these patterns as well
as other historic and geographic factors. other
   elds of linguistics, such as historical linguistics and
the study of endangered languages, depend heavily
on knowledge drawn from typological comparisons.
linguistic typology is also useful in many areas of
natural language processing.

radev@umich.edu

wals is an expansive linguistic resource useful
in a variety of different nlp applications. wals
has data for over 2,500 languages regarding almost
200 rules compiled by a group of 55 authors span-
ning phonology, syntax, and lexicology. examples
of such rules and their possible values are given in
section 3.2. wals has been used as a lens through
which to view worldwide typological relations (lit-
tauer et al., 2012) as well as a tool with which to
discover which linguistic rules are rare across the
world   s languages (cysouw, 2011). unsurprisingly,
it has also served as a baseline for typological sim-
ilarity measurements (berzak et al., 2014). in this
work the authors also show that the accuracy of the
baseline increases with the number of wals rules
considered, but note the sparseness of these rules.
indeed, the sparseness of wals has been noted by
several researchers (georgi et al., 2010; cysouw
and comrie, 2008; teh et al., 2008). in fact, cur-
rently the average rule in wals has data for only
400 languages. this represents a matrix that is only
16% populated. to this end, we examine meth-
ods for automatically identifying rule values in the
wals database for the purpose of reducing sparse-
ness in wals. our methods use the wals database
as of march 2015 as well as an additional copy of
the bible in each target language downloaded from
bible.com or bible.is and word-aligned to an
english version.

despite its sparseness, wals is still an excel-
lent resource. consider a similar resource, syn-
tactic structures of the world   s languages (sswl)
(collins and kayne, 2011). sswl is considerably
less sparse, with 56% of all possible data for 112

rules and 251 languages. in reality, many of these
rules actually represent pieces of a single rule in
wals - sswl rule 3 is verb object while sswl
rule 4 is object verb. in wals, both of these rules
are possible values for rule 83a: order of object
and verb. thus, sswl lacks the breadth that makes
wals such an appealing resource. wals has an
order of magnitude more languages and many more
rules. in addition, wals has genealogical informa-
tion about each language that sswl lacks.

while many of the rules in wals, such as rule
83a: order of object and verb, may seem simple,
properties of linguistic universals can be exploited to
extrapolate further information. joseph greenberg
proposed a set of linguistic universals (greenberg,
1963) that refer to statistical tendencies across the
world   s languages. further universals have been de-
scribed since, particularly with respect to word or-
der (hawkins, 1983). while our system does not
directly make use of these universals, the core me-
chanic of classifying rules based on the labels of
other rules for the same language is central to our
experimentation. this is the grounding theory be-
hind the use of typological features.

applying effective, automated methods for ex-
panding this database, or similar databases, will not
only create a better resource, but will also lead to
improvements in multilingual nlp via better pars-
ing, a better understanding of linguistic typology,
and many other basic nlp tasks such as machine
translation (mikolov et al., 2013) and part-of-speech
tagging (das and petrov, 2011).
in addition, hav-
ing a more expansive resource will allow for nlp to
be leveraged in many under-resourced languages not
only in europe, but around the world.

the rest of this paper is structured as follows:
section 2 discusses other work in the    eld of nlp
that has made of use of features in wals; sec-
tion 3 describes in detail how we generated our fea-
ture vectors and present the classi   cation models we
compared; section 4 presents our results and pro-
vides an analysis; section 5 concludes our work; and
section 6 suggests future work.

2 related work

typological similarity has previously been shown
to correlate with genealogical similarity both in the

   elds of nlp (rama and kolachina, 2012) and his-
torical linguistics (dunn et al., 2005). due to this,
wals has been used to study linguistic typology
via computational methods. in order to determine
language similarity via genealogical relatedness, it
is important to know which rules are more telling of
historical relatedness. to do this, (wichmann and
kamholz, 2008) measured the variance of linguistic
rules within language families, at the genus level, to
determine their stability. rules that change less of-
ten within language families, then, are more indica-
tive of historical relationships when they are shared
between languages. to do this, the authors extracted
data for language families from wals and calcu-
lated the id203 of features being shared within
and across language families. feature variation was
shown to be signi   cantly different across genera.
this indicates that language families are more simi-
lar internally than across genera, which in turn sup-
ports the concept of using typological similarity to
predict language similarity.

these syntactic regularities are not only pre-
dictable within language genera, however. (daum  e
and campbell, 2007) used wals as a database to
discover implicative associations, similar to green-
berg   s universals. not only did they recover many
of greenberg   s universals, but they also uncovered a
host of other implicatures.

in similar work, (cysouw and comrie, 2008)
sought to determine how consistent certain linguis-
tic rules are in an effort to determine languages that
are    more central for the structure of human lan-
guage.    the authors cite    widely varying frequen-
cies of available data    as a dif   culty in their study,
but conclude that many word order features seem to
be central rules in language. among these are rules
83a, 85a, 86a, 88a, and 107a,    ve of the six rules
considered in this study.

apart from typology, the data from wals have
also been used in natural language processing to
advance several core nlp tasks.
(naseem et al.,
2012) used language similarity information to im-
prove multilingual parsing by de   ned a distance
metric over wals rules. the authors considered six
wals rules to create their similarity metric, three
of which (85a, 86a, and 88a), are also used in this
study.

(bender et al., 2013) attempted a similar task, us-

ing interlinear glossed text (igt) to predict typolog-
ical features such as major consituent order and case
system.
(lewis and xia, 2008) also predict typo-
logical features from igt by    rst learning a context
free grammar for a language and then examining its
structure. both studies use data from wals to eval-
uate their performance. we address a similar task,
but we utilize projected dependency parses from en-
glish text as a source of knowledge for each foreign
language, as many do not have extensive resources
of their own. this is similar to igt, but does not
contain morphological information to the same ex-
tent. this method of projecting dependencies has
been used to bootstrap linguistic resources in the
past (xia, 2007), (hwa et al., 2005).

3 experiments

we    rst created a corpus of bibles word-aligned to
an english bible. the 325 languages represented in
the corpus are very diverse, ranging from french,
german, and modern standard arabic to acholi,
basque, and tamil. note that translated bibles exist
for many other languages that were not included in
this study. in total, we use over 2 million aligned
sentences. the rules were selected based on the fre-
quency of their use in previous nlp research, a mea-
sure of usefulness. more information about them is
provided in section 3.2. for each rule we ran experi-
ments using a variety of classi   ers and feature vector
combinations. all results are given in section 4.

3.1 general textual feature extraction
for each rule described in section 3.2, we extract
english dependencies according to certain criteria
unique to that rule. we then project these de-
pendencies onto the foreign language biblical sen-
tences using word alignments determined by berke-
leyaligner (denero and klein, 2007; liang et al.,
2006). using these inferred dependencies, we cal-
culate the feature vectors for the rule in question by
the policy described in section 3.2.

to give a simple example of projecting word
alignments, consider the english sentence and its
ma   di translation in figure 1. is and resurrection are
the verb and object of the english sentence, while i
and onzika are the ma   di alignments. these align-
ments are shown using blue edges. as these words

figure 1: word alignments for an example sentence in
ma   di and english

are aligned, we assume they serve the same gram-
matical function. we can see that the english sen-
tence shows a verb-object ordering, while the ma   di
sentence demonstrates an object-verb ordering. this
would then add one to the count of object-verb or-
derings to the linguistic feature vector of ma   di for
rule 83a.

next, we give a detailed example of how the fea-
tures for wals rule 83a order of object and verb
can be drawn from projected dependency parses of
english text as in the german example in figures 2
and 3.

in this example, german would receive one count
for having the same verb-object ordering in any
phrase due to the (dothan, found) dependency and
one count for different ordering due to the (broth-
ers, after) dependency. this sentence is not a
question or a dependent clause, so the correspond-
ing same/different counts would not be changed for
those features. this process is then repeated for all
aligned sentences in all languages with a known la-
bel for the rule.

3.2 speci   c feature extraction

each rule is described in detail below, along with
de   nitions of its possible classes, rounded distribu-
tional information over the languages considered in
this paper, and a brief motivation for why we chose
to classify this rule. all parses referred to in this
section were obtained using a hash kernel parser
(bohnet, 2010) and represented in the conll de-
pendency parsing format. we restrict our system to
instances where the english sentence aligns to only
one foreign language sentence. the count of lan-
guages given for each rule is the size of the set of
languages whose label for this rule is given in wals
and for whom we have a bible.

root

conj

cc

adpobj

advmod

nsubj

adpmod

poss

adpmod

dobj

adpobj

so joseph went after his brothers and found them at dothan

figure 2: sample english dependency parse

root

conj

cc

adpmod

adpobj

advmod

nsubj

poss

adpmod

dobj

adpobj

da ging joseph seinen br  udern nach und fand sie zu dothan

figure 3: sample german dependency parse

3.2.1 rule 83a: order of object and verb

rule 83a de   nes the order in which the direct ob-
ject and verb of a sentence occur. its label is known
for 299 languages. the three possible classes are
object verb (the man saw the dog. 38%), verb ob-
ject (the man the dog saw. 58%), and no domi-
nant order (both valid in certain conditions. 4%).
besides being fundamental to any sort of parsing
or chunking task, many other typological rules have
been found to be statistically related to the order of
subject, object, and verb (greenberg, 1963). as it
has also been found that subject generally precedes
object (greenberg, 1963), knowing the order of the
object and verb allows not only accurate guesses
about the order of subject, object, and verb, but also
about many other correlated rules. to identify all
occurrences, we examine all dependencies with the
label dobj. the instance is kept as long as both of
the english and foreign object words align only to
each other and both of the english and foreign verb
words align only to each other. the resulting feature
vector has six columns: the count of all sentences in
which the verb-object ordering is identical between
english and the foreign language; the count of all
sentence in which the ordering is reversed; the count

of all questions in which the ordering is identical;
the count of all questions in which it is reversed; and
   nally the count of dependent clauses where the or-
der is identical and the count of dependent clauses
in which the order is reversed.

3.2.2 rule 85a: order of adposition and noun

phrase

rule 85a de   nes the order in which adpositions
occur relative to their governing noun phrase.
its
label is known for 256 languages. the    ve possi-
ble classes are postpositions (the dog went the park
to. 43%), prepositions (the dog went to the park.
49%), inpositions (the dog went the to park. 0%),
more than one type with none dominant (multiple
valid. 6%), and no adpositions (the dog went the
park. 1%). this rule is critical for having any sort
of successful semantic parse or narrative processing.
we examine all dependencies with the label adpmod
and have a parent word in the sentence. the instance
is kept as long as the english and foreign adpositions
align only to each other and the english and foreign
governing nouns align only to each other. the fea-
ture vector has a dimensionality of six and consists
of the same features as the feature vector for rule
83a, simply computed for the order of the adposi-

tion and noun phrase, rather than object and verb.

3.2.3 rule 86a: order of genitive and noun

rule 86a de   nes the order in which genitives ap-
pear with respect to their governing noun. 254 lan-
guages in our dataset have a known label for this
rule. the three possible classes are genitive-noun
(my dog 47%), noun-genitive (dog my 45%), and
both occur, none dominant (both valid 8%). this
rule is absolutely necessary for coreference reso-
lution. we examine all dependencies with the la-
bel poss, as long as the genitive and noun align
only to their respective foreign language counter-
part. again, we use a length six feature vector with
the same procedure over this set of dependencies.

3.2.4 rule 88a: order of demonstrative and

noun

rule 88a de   nes the order in which demonstra-
tives appear together with nouns. there are 171 lan-
guages in our set of bibles with a known label for
this rule. the six possible classes are demonstra-
tive noun (this dog is shaggy. 50%), noun demon-
strative (dog this is shaggy. 42%), pre   x on noun
(this-dog is shaggy. 0%), suf   x on noun (dog-this
is shaggy. 2%), demonstrative noun demonstrative
(this dog this is shaggy. 2%), and two or more of
the previous options, none dominant (multiple valid.
4%). this rule is also necessary for coreference res-
olution. we select all dependencies that are of type
det or pron, have a lexical value of this, that, these,
or those, and whose parent is a noun. we compute
the feature vector in the same fashion as the above
rules.

3.2.5 rule 92a: position of polar question

particles

rule 92a de   nes the position that a polar ques-
tion particle appears in a sentence. its rule is known
for 91 languages. polar question particles signal
grammatically that the sentence is a yes or no ques-
tion. the six possible classes are beginning of sen-
tence (23%), end of sentence (36%), second word
in sentence (3%), anywhere else (2%), either of
two positions (1%), no question particle (34%). in
languages with polar questions, this rule would al-
low for much higher quality id53 and
dialogue systems. our selection of this rule also

demonstrates that the phenomenon does not even
need to occur in the source language in order to be
approached. we select all dependencies that contain
questions. we then use the presence of an english
wh- word to sort these questions into polar and in-
formation questions. using these labels, we examine
the foreign language questions for the word that ap-
pears most often in polar questions but not informa-
tion questions using the ratio of relative frequencies.
we then count the number of times this word ap-
pears in each of the possible positions. the feature
vector has a length of four, and each entry corre-
sponds directly to a label class: we count how often
the inferred question word appears initially, second,
last, or elsewhere.

3.2.6 rule 107a: passive constructions

107a describes the presence or absence of passive
constructions. we have bibles for 93 of the wals
languages with a known label for this rule. the two
possible classes are passive construction (the dog
was seen by the butler. 53%) and no passive con-
struction (the butler saw the dog. 47%). this rule
was chosen not only because it would allow for bet-
ter grammar induction, a very relevant task for such
under-resourced languages, but also to demonstrate
that our system and approach can handle rules other
than simple word order rules. we select all depen-
dencies that are of type nsubjpass, signaling a pas-
sive subject, and nsubj, signaling an active subject.
we then determine the number of times that the or-
der of subject and verb differs between these two
sentence types in the foreign language. the feature
vector is built identically to all other rules except for
92a.

3.3 text classi   er training
in order to train the text classi   ers, we used word-
aligned biblical texts in english and each of the
languages considered along with the labels from
wals. for each rule discussed in section 3.2, we
select dependencies and create normalized feature
vectors according to the discussed method. this
forms the text feature set in section 4.

3.4 typological classi   er training
we also consider that some labels can be predicted
from the labels of other rules within a language. we

create feature vectors for each language using    ve of
the six rules discussed in section 3.2 in order to clas-
sify the sixth rule. this comprises the rules feature
set in section 4.

3.5 genealogical classi   er training
previous work has shown that propagating knowl-
edge from genealogically similar languages has
demonstrated markedly better results than propagat-
ing knowledge from a random language (naseem et
al., 2012). for our purposes we consider the genus
and family of each language as given by wals. to
this end, we consider simply propagating the major-
ity label from all languages of the same genus and,
separately, all languages of the same family. we also
create feature vectors such as those for the text and
rules features, as described above.

4 results and discussion

majority is simply the majority class as shown in
3.2. na    ve bayes is run using the default imple-
ment in weka (hall et al., 2009). for logistic regres-
sion, as we do not have a developmental data set, we
simply experiment with    ve common id173
parameters - 1.0, 0.5, 0.1, 0.01, and 10   8 (the de-
fault value in weka). as we are only evaluating over
languages that already have known labels in wals,
we consider the accuracy of each classi   er through
leave-one-out cross-validation.

4.1 simple linguistic features
it is clear from tables 1 - 6 that the combination of
textual and typological features works best. it can
also be seen that almost regardless of the regulariza-
tion parameter, id28 models this data
with higher accuracy than na    ve bayes. in the cases
that this is not true, rule 83a and rule 88a, it is the
linguistic features that perform best. this is also en-
couraging, as the main problem we are trying to ad-
dress is the sparseness of the wals data.

there are also two rules that perform quite poorly,
rule 92a and rule 107a, depicted in tables 5 and 6.
these rules are signi   cantly more subtle linguisti-
cally than the rest, which are relatively straightfor-
ward for our parser to detect accurately. the com-
plexity of the pipeline for rule 92a and the numer-
ous ways in which languages can mark passive con-

structions, regarding rule 107a, are simply dif   cult
to perceive.

we also    nd that id173 does make a dif-
ference for the various id28 models. in
some cases, such as table 5, the difference can be as
large as 10% accuracy. in others, such as table 3, the
difference is only 0.8%. in general, it appears that a
id173 parameter between 1 and 0.5 yields
the best, most consistent results for the combined
feature vector case. this is likely due to the nature
of the two feature vectors being combined, as one is
inherently discrete while the other is continuous, but
more experimentation is necessary to determine the
relationship more precisely.

4.2 genealogy-based classi   cation
the genealogical experiments led to rather different
results. we    rst tested a very simple approach, as-
signing the majority label for languages of the same
genus or same family. table 7 demonstrates these
results.

it is very clear that this majority voting scheme
works much better than the other approaches. this
is particularly interesting, given the argument that
typological features have been claimed to be an ef-
fective proxy for genealogical data (rama and ko-
lachina, 2012).

we also attempted to incorporate the genealogical
features as a third feature source, similar to our tex-
tual and typological features. combining genealogi-
cal features with the either text or rule features did
not outperform the systems already presented, nor
did the combination of all three for the na    ve bayes
or any id28 model. we also considered
that perhaps there is something intrinsic to majority
voting that is boosting the performance of this sim-
ple approach. to con   rm, we ran experiments on the
text and rule feature vectors using k-nearest neigh-
bor classi   cation. again, these experiments did not
rival the performance of our other classi   ers, regard-
less of the choice of k.

it has been estimated that the accuracy of the cur-
rent wals database is possibly close to 96%, based
on examination of the entries for latvian (cysouw,
2011). the results in table 7 are just shy of this
number for many of the rules. this means that for
many rules, the simple genealogical majority mea-
sure could be used as is to expand wals. obviously

features majority na    ve bayes

text
rules

text+rules

58.0%
58.0%
58.0%

92.0%
82.2%
90.6%

lr.1

lr.5

lr-8
lr1
92.3% 92.3% 92.3% 92.3% 92.0%
83.3% 83.3% 83.6% 83.6% 83.6%
90.6% 90.0% 88.9% 89.6% 89.0%

lr.01

table 1: accuracy for rule 83a: order of object and verb n=299

features majority na    ve bayes

text
rules

text+rules

49.6%
49.6%
49.6%

82.0%
82.0%
83.2%

lr.1

lr.5

lr.01

lr1
lr-8
83.2% 83.2% 84.4% 84.4% 84.0%
84.8% 84.8% 84.4% 84.0% 83.6%
86.7% 85.5% 84.8
83.2% 82.4%

table 2: accuracy for rule 85a: order of adposition and noun phrase n=256

features majority na    ve bayes

text
rules

text+rules

47.2%
47.2%
47.2%

66.9%
81.6%
79.1%

lr.1

lr.5

lr.01

lr1
lr-8
66.5% 66.9% 66.1% 66.1% 66.1%
81.3% 80.5% 80.5% 80.1% 79.8%
81.1% 81.5% 81.9% 81.5% 81.1%

table 3: accuracy for rule 86a: order of genitive and noun n=254

features majority na    ve bayes

text
rules

text+rules

50.2%
50.2%
50.2%

74.3%
60.8%
74.2%

lr.1

lr.5

lr.01

lr1
lr-8
83.6% 83.6% 84.2% 84.2% 84.2%
67.3% 66.7% 64.9% 64.3% 63.7%
81.3% 81.3% 80.7% 78.9% 76.0%

table 4: accuracy for rule 88a: order of demonstrative and noun n=171

features majority na    ve bayes

text
rules

text+rules

36.2%
36.2%
36.2%

26.4%
38.5%
33.0%

lr.1

lr.5

lr.01

lr1
lr-8
27.5% 27.5% 29.7% 28.6% 27.5%
37.4% 36.3% 40.7% 39.6% 38.5%
42.9% 35.2% 34.1% 33.0% 31.9%

table 5: accuracy for rule 92a: position of polar question particles n=91

features majority na    ve bayes

text
rules

text+rules

52.6%
52.6%
52.6%

50.5%
55.9%
57.0%

lr.1

lr.5

lr.01

lr1
lr-8
47.3% 48.4% 47.3% 45.2% 44.1%
55.9% 58.1% 58.1% 58.1% 55.9%
58.1% 59.1% 57.0
55.9% 54.8%

table 6: accuracy for rule 107a: passive constructions n=93

this system will not be as accurate as a trained lin-
guist, and we are not suggesting that if we expanded
wals this way it would be    t to replace the current
database, but rather that the system could be used
to create a more expansive database at the cost of
slightly lower accuracy.

5 conclusion

in this study, we examined the effectiveness of clas-
sifying a series of syntactic rules across a set of for-
eign languages. this is doable by considering the
values of other syntactic rules in the database. how-
ever, it is also worth considering actual linguistic
features for this linguistic task, as well as combin-

rule best other same genus same family
83a
85a
86a
88a
92a
107a

95.8%
94.3%
93.4%
88.0%
91.1%
95.9%

86.3%
84.0%
80.8%
81.3%
73.3%
81.3%

92.3%
86.7%
81.9%
84.2%
42.9%
59.1%

table 7: accuracy using genealogical neighbors

ing the two disparate sources. finally, we also con-
sidered genealogical data available from the wals
database.

we have shown that with proper id173,
we can achieve better classi   cation accuracy by aug-
menting the currently available wals data with lin-
guistic features. though in some cases the individ-
ual linguistic features actually outperform the com-
bined features, this is not generally the case and it
only applies when the linguistic features are per-
forming well on their own.
in addition, it is not
possible to say from our results which regulariza-
tion parameter is best. we do, however, demonstrate
that this combination of features can lead to greater
success than either feature independently. this re-
sult is important despite performing worse than the
genealogical approach as the goal of our study is
ultimately to    nd accurate methods to classify fea-
tures for languages that are not already contained in
wals as well as features that are missing for lan-
guages already present.

considering only the genus of the language, we
are within 2% of achieving human-like accuracy for
half of our six rules. this is encouraging and further
experimentation needs to be done over the entire set
of rules in wals. however, this approach is limited
as the fundamental problem of sparsity applies to ty-
pological data, which is also used to determine the
true testing labels in this case. for this reason, it is
also important that we have demonstrated the use-
fulness of purely linguistic features that are separate
from the wals database, as well as their compati-
bility with the typological values in wals.

6 future work

moving ahead, more complex wals rules could be
addressed with this method. while the rules we

consider are all useful in nlp research, there are
other rules that are almost as frequently used that
are more complex, such as those governing nega-
tion. in particular, modules concerning morphologi-
cal features that are not easily detectable using word
alignments are the next logical direction. it would
also be worth gathering word aligned bibles from
other well-supported source languages besides en-
glish. as was demonstrated by rule 92a: position
of polar question particle, the system can work even
when the feature does not appear in the source lan-
guage, but it is certainly more dif   cult to account
for. having several source languages would mini-
mize the number of rules for which this is the case.
in addition, experiments with combinations of ge-
nealogical, textual, and typological features should
be performed. however, rather than simply concate-
nating these features, the features themselves should
be combined into compound features, such as    ger-
manic and 83a-no dominant order   . we believe
these types of features could improve upon the per-
formance of the majority-vote features that are cur-
rently the best measure. the system might then
achieve even higher accuracy, possibly on par with
human performance for some rules, allowing the
wals database to be automatically expanded, per-
haps as a parallel resource to the current database.

in addition, having shown that this is a reasonably
accurate method for determining values of wals
rules, the method should be applied on as of yet un-
labeled languages in the wals database. we also
have nearly 400 such bibles in languages not con-
tained in wals, which have not been used in this
experiment. adding them to wals would expand
the number of languages in the database by more
than ten percent. while we would not be able to
apply the most accurate, genealogical method, we

could certainly make use of linguistic features from
the bibles.

it would also be possible to use semi-supervised
methods such as semi-supervised k-nearest neigh-
bors and graph id173 (zhu et al., 2003) as an
improvement to our classi   cation method. though
we did experiment with k-nearest neighbors in
this study, it is possible that a different approach
could still yield good results. the idea of nearest-
neighbors classi   cation stands to be very useful, as
shown by the majority label propagation. graph reg-
ularization has been shown to be effective in weakly-
supervised situations where there is sparse training
data (hassan et al., 2014). therefore, it would be a
good idea to apply it to the wals data. one could
build a similarity network of all the languages using
the known wals rules that they share in common
and regularize this graph in order to propagate la-
bels to some extent whenever possible. constraints
such as the genus-majority relationship discussed in
this paper as well as constraints borrowed from lin-
guistic universals, representing conditional depen-
dencies between rules, could also be used to help
regularize the entire network.

references
[bender et al.2013] emily m. bender, michael wayne
goodman, joshua crowgey, and fei xia. 2013. to-
wards creating precision grammars from interlinear
glossed text: inferring large-scale typological prop-
in proceedings of the 7th workshop on lan-
erties.
guage technology for cultural heritage, social sci-
ences, and humanities, pages 74   83, so   a, bulgaria,
august. association for computational linguistics.

[berzak et al.2014] yevgeni berzak, roi reichart, and
boris katz.
reconstructing native lan-
guage typology from foreign language usage. corr,
abs/1404.6312.

2014.

[bohnet2010] bernd bohnet. 2010. very high accuracy
and fast id33 is not a contradiction. in
proceedings of the 23rd international conference on
computational linguistics, pages 89   97. association
for computational linguistics.

[collins and kayne2011] chris collins

and richard
kayne.
2011. syntactic structures of the world   s
languages. http://sswl.railsplayground.
net.

[cysouw and comrie2008] michael cysouw and bernard
comrie. 2008. how varied typologically are the lan-

guages of africa? in r botha and c knight, editors,
the cradle of language, studies in the evolution of
language, pages 190   203. oxford university press,
oxford, uk.

[cysouw2011] michael cysouw. 2011. quantitative ex-
plorations of the world-wide distribution of rare char-
acteristics, or: the exceptionality of northwestern eu-
ropean languages. expecting the unexpected, pages
411   431.

[das and petrov2011] dipanjan das and slav petrov.
2011. unsupervised part-of-speech tagging with bilin-
in proceedings of the
gual graph-based projections.
49th annual meeting of the association for compu-
tational linguistics: human language technologies -
volume 1, hlt    11, pages 600   609, stroudsburg, pa,
usa. association for computational linguistics.

[daum  e and campbell2007] hal

iii daum  e and lyle
campbell. 2007. a bayesian model for discovering
typological implications. corr, abs/0907.0785.

[denero and klein2007] john denero and dan klein.
2007. tailoring word alignments to syntactic machine
in annual meeting-association
translation.
for computational linguistics, volume 45,
page 17. citeseer.

[dryer and haspelmath2013] matthew s. dryer and mar-
2013. wals online.
tin haspelmath, editors.
max planck institute for evolutionary anthropology,
leipzig.

[dunn et al.2005] michael dunn, angela terrill, ger
reesink, robert a foley, and stephen c levin-
structural phylogenetics and the re-
son.
science,
construction of ancient language history.
309(5743):2072   2075.

2005.

[georgi et al.2010] ryan georgi, fei xia, and william
lewis. 2010. comparing language similarity across
in pro-
genetic and typologically-based groupings.
ceedings of
the 23rd international conference on
computational linguistics, coling    10, pages 385   
393, stroudsburg, pa, usa. association for compu-
tational linguistics.

[greenberg1963] joseph h. greenberg. 1963. some uni-
versals of grammar with particular reference to the or-
der of meaningful elements. in joseph h. greenberg,
editor, universals of human language, pages 73   113.
mit press, cambridge, mass.

[hall et al.2009] mark hall, eibe frank, geoffrey
holmes, bernhard pfahringer, peter reutemann, and
ian h. witten. 2009. the weka data mining software:
sigkdd explor. newsl., 11(1):10   18,
an update.
november.

[hassan et al.2014] ahmed hassan, amjad abu-jbara,
wanchen lu, and dragomir r. radev. 2014. a ran-
dom walk based model for identifying semantic orien-
tation. computational linguistics, 40.

gaussian    elds and id94. in icml, vol-
ume 3, pages 912   919.

[hawkins1983] john a hawkins. 1983. word order uni-

versals: quantitative analyses of linguistic structure.

[hwa et al.2005] rebecca hwa, philip resnik, amy
weinberg, clara cabezas, and okan kolak. 2005.
id64 parsers via syntactic projection across
parallel texts. natural language engineering, 11:11   
311.

[lewis and xia2008] william d lewis and fei xia.
2008. automatically identifying computationally rele-
vant typological features. in ijcnlp, pages 685   690.
[liang et al.2006] percy liang, ben taskar, and dan
in pro-
klein.
ceedings of the main conference on human language
technology conference of the north american chap-
ter of the association of computational linguistics,
hlt-naacl    06, pages 104   111, stroudsburg, pa,
usa. association for computational linguistics.

2006. alignment by agreement.

[littauer et al.2012] richard littauer, rory turnbull, and
alexis palmer. 2012. visualising typological rela-
tionships: plotting wals with heat maps. in proceed-
ings of the eacl 2012 joint workshop of lingvis &
unclh, eacl 2012, pages 30   34, stroudsburg, pa,
usa. association for computational linguistics.

[mikolov et al.2013] tomas mikolov, quoc v. le, and
ilya sutskever.
exploiting similarities
among languages for machine translation. corr,
abs/1309.4168.

2013.

[naseem et al.2012] tahira naseem, regina barzilay, and
amir globerson. 2012. selective sharing for mul-
in proceedings of the
tilingual id33.
50th annual meeting of the association for compu-
tational linguistics: long papers - volume 1, acl
   12, pages 629   637, stroudsburg, pa, usa. associ-
ation for computational linguistics.

[rama and kolachina2012] taraka rama and prasanth
kolachina. 2012. how good are typological distances
for determining genealogical relationships among lan-
in proceedings of the 24th international
guages?
conference on computational linguistics.

[teh et al.2008] yee whye teh, hal daum iii, and daniel
roy. 2008. bayesian agglomerative id91 with
in in advances in neural information
coalescents.
processing systems.

[wichmann and kamholz2008] s  ren wichmann

and
a stability metric for
david kamholz.
stuf-language typology
typological
and universals sprachtypologie und universalien-
forschung, 61(3):251   262.

features.

2008.

[xia2007] fei xia. 2007. multilingual structural projec-
tion across interlinear text. in in proc. of the confer-
ence on human language technologies (hlt/naacl
2007, pages 452   459.

[zhu et al.2003] xiaojin zhu, zoubin ghahramani, john
lafferty, et al. 2003. semi-supervised learning using

