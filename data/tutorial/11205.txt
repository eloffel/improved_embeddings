552

san diego, california, june 12-17, 2016. c(cid:13)2016 association for computational linguistics

proceedings of naacl-hlt 2016, pages 552   557,

structuredpredictionwithoutputembeddingsforsemanticimageannotationariadnaquattoni1,arnauramisa2,pranavaswaroopmadhyastha3edgarsimo-serra4,francescmoreno-noguer21xeroxresearchcentereurope,ariadna.quattoni@xrce.xerox.com2institutderob`oticaiinform`aticaindustrial(csic-upc),{aramisa,fmoreno}@iri.upc.edu3talpresearchcenter,universitatpolit`ecnicadecatalunya,pranava@cs.upc.edu4wasedauniversity,esimo@aoni.waseda.jpabstractweaddressthetaskofannotatingimageswithsemantictuples.solvingthisproblemrequiresanalgorithmabletodealwithhundredsofclassesforeachargumentofthetuple.insuchcontexts,datasparsitybecomesakeychallenge.weproposehandlingthisspar-sitybyincorporatingfeaturerepresentationsofboththeinputs(images)andoutputs(ar-gumentclasses)intoafactorizedlog-linearmodel.1introductionmanyimportantproblemsinmachinelearningcanbeframedasstructuredpredictiontaskswherethegoalistolearnfunctionsthatmapinputstostruc-turedoutputssuchassequences,treesorgeneralgraphs.awiderangeofapplicationsinvolvelearn-ingoverlargestatespaces,e.g.,iftheoutputisalabeledgraph,eachnodeofthegraphmaytakeval-uesoverapotentiallylargesetoflabels.dataspar-sitythenbecomesachallenge,astherewillbemanyclasseswithveryfewtrainingexamples.withinthiscontext,weareinterestedinthetaskofpredictingsemantictuplesforimages.thatis,givenaninputimageweseektopredictwhataretheeventsoractions(referredhereaspredicates),whoandwhataretheparticipants(referredhereasactors)oftheactionsandwhereistheactiontakingplace(referredhereaslocatives).forexample,animagemightbeannotatedwiththesemantictuples:hrun,dog,parkiandhplay,dog,grassi.wecalleach   eldofatupleanargument.tohandlethedatasparsitychallengeimposedbythelargestatespace,wewillleverageanapproachthathasproventobeusefulinmulticlassandmul-tilabelpredictiontasks(westonetal.,2010;akataetal.,2013).theideaistorepresentavalueforanargumentausingafeaturevectorrepresentation     irn.wewillintegratethisargumentrepresen-tationintothestructuredpredictionmodel.insummary,ourmaincontributionistopro-poseanapproachthatincorporatesfeaturerepresen-tationsoftheoutputsintoastructuredpredictionmodel,andapplyittotheproblemofannotatingimageswithsemantictuples.wepresentanexperi-mentalstudyusingdifferentoutputfeaturerepresen-tationsandanalyzehowtheyaffectperformancefordifferentargumenttypes.2semantictupleimageannotationtask:wewilladdressthetaskofpredictingse-mantictuplesforimages.followingfarhadietal.(2010),wewillfocusonasimplesemanticrepre-sentationthatconsidersthreebasicarguments:pred-icate,actorsandlocatives.forexample,inthetuplehplay,dog,grassi,   play   isthepredicate,   dog   istheactorand   grass   isthelocative.giventhisrepresentation,wecanformallyde-   neourproblemasthatoflearningafunction  :x  p  a  l   irthatscoresthecompatibilitybetweenimagesandsemantictuples.herexisthespaceofimages;p,aandlaredis-cretesetsofpredicate,actorandlocativeargumentsrespectively,andhpaliisaspeci   ctupleinstance.theoveralllearningprocessisillustratedinfig.1.dataset:forourexperimentsweusedasubsetoftheflickr8kdataset,proposedinhodoshetal.(2013).thisdataset(subsetbinfig.1)consistsof8,000imagesfromflickrofpeopleandanimals553

training'image'x (from!q)'training'senten2al'descrip2ons'(from'q)'seman2c'tuple'extractor'(trained)using)l)))seman2c'tuples'image'features'a)brown)dog)is)running)in)a)grassy)plain.)a)brown)dog)runs)along)a)path)in)the)grass.)dog)running)in)   eld.))dog)running)in)narrow)dirt)path.)the)dog)is)running)through)the)uncut)grass.)<act=dog,)pre=run,)loc=plain>)<act=dog,)pre=run,)loc=grass>)<act=dog,)pre=run,)loc=   eld>)<act=dog,)pre=run,)loc=path>)<act=dog,)pre=run,)loc=grass>))<   a(x),   p(x),   l(x) >  convolu2onal'nn'(trained)using)u)))image'to'seman2c'tuple'predictor'u)(id163):)images)annotated)with)keywords)(cheap,)exploits)readily)available)dataset))q)(flickr8k):)images)annotated)with)descripjve)sentences)(relajvely)cheap,)exploits)readily)available)resources))l)(spmdataset):)images)annotated)with)descripjve)sentences)and)semanjc)tuples)(this)is)a)small)subset)of)q,)expensive)annotajon,)requires)experjse))training'data'embedded'crf''(implicitly)induces)embedding)of)image)features)and)arguments))figure1:overviewofourapproach.first,imagesx   aarerepresentedusingimagefeatures  s(x),andsemantictuplesareobtainedapplyingoursemantictupleextractor(learnedfromthesubsetc)totheircorrespondingcaptions.theresultingenlargedtrainingset,isusedtotrainourembeddedcrfmodelthatmapsimagestosemantictuples.(mostlydogs)performingsomeaction,with   vecrowd-sourceddescriptivecaptionsforeachone.we   rstmanuallyannotated1,544captions,cor-respondingto311images(approximatelyonethirdofthedevelopmentset(subsetcinfig.1),produc-ingmorethan2,000semantictuplesofpredicate,ac-torandlocative.fortheexperimentswepartitionedtheimagesandannotationsintotraining,validationandtestsetsof150,50and100imagesrespectively.dataaugmentation:toenlargethemanuallyan-notateddatasetwetrainedamodelabletopredictsemantictuplesfromcaptionsusingstandardshal-lowanddeeplinguisticfeatures(e.g.,postags,de-pendencyparsing,semanticrolelabeling).weex-tractthepredicatesbylookingatthewordstaggedasverbsbythepostagger.then,theextractionofargumentsforeachpredicateisresolvedasaclassi-   cationproblem.morespeci   cally,foreachdetectedpredicateinasentenceweregardeachnounasapositiveorneg-ativetrainingexampleofagivenrelationdepend-ingonwhetherthecandidatenounisorisnotanargumentofthepredicate.weusetheseexamplestotrainaid166classi   erthatpredictsifacandidatenounisanargumentofagivenpredicatebasedonseverallinguisticfeaturescomputedoverthesyntac-ticpathofthedependencytreethatconnectsthem.werunthelearnedtuplepredictormodelonallthecaptionsofthefickr8kdatasettoobtainalargerdatasetof8,000imagespairedwithsemantictuples.3bilinearmodelswithoutputfeaturesinthissectionweexplainhowweincorporateout-putfeaturerepresentationsintoafactorizedlinearmodel.forsimplicity,wewillconsiderfactorizedsequencemodelsoversequencesof   xedlength.however,itshouldnotbehardtoseethatalltheideaspresentedherecanbeeasilygeneralizedtootherstructuredpredictionsettings.lety=[y1...yt]beasetoflabelsands=[s1,...,st]bethesetofpossiblelabelvalues,whereyi   si.weareinterestedinlearningamodelthatcomputesp(y|x),i.e.,theconditionalprobabil-ityofasequenceygivensomeinputx.wewillconsiderfactorizedlog-linearmodelsthattaketheform:p(y|x)=exp  (x,y)pyexp  (x,y)(1)thescoringfunction  (x,y)ismodeledasasumofunaryandbinarybilinearpotentialsandisde   nedas:  (x,y)=txt=1v>ytwt  (x,t)+txt=1v>ytztvyt+1(2)wherevyt   irntisant   dimensionalfeaturerep-resentationoflabelargumentsyt   stand  (x,t)   irdtisadt   dimensionalfeaturerepresentationofthetthinputfactorofx.the   rstsetoftermsintheaboveequationareusuallyreferredasunarypotentialsandmeasurethecompatibilitybetweenasinglestateattandthefea-turerepresentationofinputfactort.thesecondsetoftermsarethebinarypotentialsandmeasurethecompatibilitybetweenpairsofstatesatadjacentfac-tors.thescoring  (x,y)functionisfullyparameter-izedbytheunaryparametermatriceswt   irnt  dtandthebinaryparametermatriceszt   irnt  nt.themainideaistode   neafeaturespacewheresemanticallysimilarlabelswillbeclose.likeinthe554

multilabelscenario(westonetal.,2010;akataetal.,2013),havingfullfeaturerepresentationsforar-gumentswillallowustoshareinformationacrossdifferentclassesandgeneralizebetter.withagoodoutputfeaturerepresentation,ourmodelshouldbeabletomakesensiblepredictionsaboutpairsofar-gumentsthatithasnotobservedattraining.thisiseasytosee:consideracasewerewehaveapairofargumentsrepresentedwithfeaturevectorsa1anda2andsupposethatwehavenotobservedthefactora1,a2inourtrainingdatabutwehaveobservedthefactorb1,b2.thenifa1iscloseinthefeaturespacetoargumentb1anda2isclosetob2ourmodelwillpredictthata1anda2arecompatible.thatisitwillassignid203tothefactora1,a2whichseemsanaturalgeneralizationfromtheobservedtrainingdata.nowweshowthattherankofwandzhaveuse-fulinterpretations.letw=u  vbethesingularvaluedecompositionofw.wecanthenwriteunarypotentials:v>yw  (x,t)as:v>yu  [v  (x,t)]thuswecanregardthebilinearformasafunctioncom-putingaweightedinnerproductoversomerealem-beddingv>yurepresentingstateyandsomerealem-bedding[v  (x,t)]representinginputfactort.therankofwgivesustheintrinsicdimensionalityoftheembedding.thusifwewanttoinducesharedlow-dimensionalembeddingsacrossdifferentstatesitseemsreasonabletoimposealowrankpenaltyonw.similarly,letz=u  vbenowthesingularvaluedecompositionofz.wecanwritethebinarypotentialsv>yzvy0as:v>yu  vvy0andthusthebi-narypotentialscomputeaweightedinnerproductbetweenarealembeddingofstateyandarealem-beddingofstatey0.asbefore,therankofzgivesustheintrinsicdimensionalityoftheembeddingand,toinducealowdimensionalembeddingforbinarypotentials,wewillimposealowrankpenaltyonz.afterhavingdescribedthetypeofscoringfunc-tionsweareinterestedin,wenowturnourat-tentiontothelearningproblem.thatis,givenatrainingsetd={hxyi}ofpairsofin-putsxandoutputsequencesyweneedtolearntheparameters{w}and{z}.forthispurposewewilldostandardmax-likelihoodestimationand   ndtheparametersthatminimizetheconditionalnegativelog-likelihoodofthedataind.thatis,wewill   ndthe{w}and{z}thatmini-mizethefollowinglossfunctionl(d,{w},{z}):   phxyi   dlogp(y|x;{w},{z})recallthatweareinterestedinlearninglow-rankunaryandbinarypotentials.toachievethiswetakeacommonap-proachwhichistouseasthenuclearnorm|w|   and|z|   asaconvexapproximationoftherankfunction,the   naloptimizationproblembecomes:min{w}l(d,{w})+xt  |wt|   +  |zt|   (3)wherel(d,{w})=pd   dloss(d,{w})isthenegativeloglikelihoodfunctionand  and  aretwoconstantsthatcontrolthetradeoffbetweenminimizingthelossandtheimplicitdimensionalityoftheembeddings.weuseasimpleoptimizationschemeknownasforwardbackwardsplitting,orfobos(duchiandsinger,2009).forourtaskwewillconsiderasimplefactor-izedscoringfunction:  (x,hpali)thathasonefactorassociatedwiththelocative-predicatepairandonefactorassociatedwiththepredicate-actorpair.sincethiscorrespondstoachainstructure,argmaxt   t  (x;hpali)canbeef   cientlycomputedusingviterbidecodingintimeo(n2),wheren=max(|p|,|a|,|l|).similarly,wecanalso   ndthetopkpredictionsino(kn2).thusforthisappli-cationthescoringfunctionofthebilinearcrfwilltaketheformof:  (x,hpali)=  loc(l)>wloc  loc(l)+  pre(p)>wpre  pre(p)+  act(a)>wact  act(a)+  loc(l)>wlocpre  pre(p)+  pre(p)>wpreact  act(a)(4)theunarypotentialsmeasurethecompatibilitybe-tweenanimageandasemanticargument,the   rstbinarypotentialmeasuresthecompatibilitybetweenalocativeandapredicate,andthesecondbinarypo-tentialmeasuresthecompatibilitybetweenapredi-cateandanactor.thescoringfunctionisfullypa-rameterizedbytheunaryparametermatriceswloc   irdl  nl,wpre   irdp  npandwa   irda  naandthebinaryparametermatriceswlocpre   irnl  npandwpreact   irnp  na.where,nl,npandnaarethedi-mensionalityofthelocatives,predicatesandactorsfeaturerepresentations,respectivelyanddl,dpand555

daarethedimensionalityoftheimagerepresenta-tions.noticethatifwelettheargumentrepresenta-tion  t(r)   ir|st|beanindicatorvectorforlabelargumentt,weobtaintheusualparametrizationofastandardfactorizedlinearmodel,whilehavingadensefeaturerepresentationsforargumentsinsteadofindicatorvectorswillallowustoshareinforma-tionacrossdifferentclasses.4representingsemanticargumentswewillconductexperimentswithtwodiffer-entfeaturerepresentations:1)fullyunsupervisedskip-grambasedcontinuouswordrepresentations(scwr)representation(mikolovetal.,2013)and2)afeaturerepresentationcomputedusingthehcaption,semantic-tuplesipairs,thatwecallse-manticequivalencerepresentation(ser).wedecidedtoexploitthedatasetofcaptionspairedwithsemantictuplestoinduceausefulfea-turerepresentationforarguments.theideaisquitesimple:wewishtoleveragethefactthatanypairofsemantictuplesassociatedwiththesameimagewillbelikelydescribingthesameevent.thus,theyareinessencedifferentwaysoflexicalizingthesameunderlyingconcept.let   slookatacon-creteexample.imaginethatwehaveanimagean-notatedwiththetuples:hplay,dog,wateriandhplay,dog,riveri.sincebothtuplesdescribethesameimage,itisquitelikelythatboth   river   and   water   refertothesamerealworldentity,i.e,   river   and   water   are   semanticallyequivalent   forthisimage.usingthisideawecanbuildarep-resentation  loc(i)   ir|l|wherethej-thdimen-sioncorrespondstothenumberoftimestheargu-mentjhasbeensemanticallyequivalenttoargu-menti.moreprecisely,wecomputetheprobabil-itythatargumentjcanbeexchangedwithargumentias:[i,j]sr(cid:80)j[i,j]srwhere[i,j]sristhenumberoftimesthatiandjhaveappearedasannotationsofthesameimageandwiththesameotherarguments.forex-ample,fortheactorarguments[i,j]srrepresentsthenumberoftimethatactoriandactorjhaveappearedwiththesamelocativeandpredicateasdescriptionsofthesameimage.5relatedworkinrecentyears,someworkshavetackledtheprob-lemofgeneratingrichtextualdescriptionsofim-ages.oneofthepioneersis(kulkarnietal.,2011),whereacrfmodelcombinestheoutputofseveralvisionsystemstoproduceinputforalanguagegen-erationmethod.infarhadietal.(2010),theauthors   ndthesimilaritybetweensentencesandimagesina   meaning   space,representedbysemantictupleswhichareverysimilartoourtriplets.otherworksfocusonasimpli   edproblem:rankingofhuman-generatedcaptionsforimages.hodoshetal.(2013)proposetousekernelcanonicalcorrelationanal-ysistoprojectimagesandtheircaptionsintoajointrepresentationspace,inwhichimagesandcaptionscanberelatedandrankedtoperformillustrationandannotationtasks.socheretal.(2014)alsoaddresstherankingofimagesgivenasentenceandvice-versausingacommonsubspacelearnedviarecur-siveneuralnetworks.otherrecentworksalsoex-ploitdeepnetworkstoaddresstheproblem(vinyalsetal.,2015;karpathyandfei-fei,2015).usingla-belembeddingscombinedwithbilinearformshasbeenpreviouslyproposedinthecontextofmulti-classandmultilabelimageclassi   cation(westonetal.,2010;akataetal.,2013).6experimentsforimagefeaturesweusethe4,096-dimensionalsecondtolastlayerofbvlcimplementationof   alexnet   id163model,aconvolutionalneu-ralnetwork(id98)asdescribedinjiaetal.(2014).totestourmethodweusedthe100testimagesthatwereannotatedwithground-truthsemantictuples.tomeasureperformancewe   rstpredictthetoptu-pleforeachimageandthenmeasureaccuracyforeachargumenttype(i.e.thenumberofcorrectpre-dictionsamongthetop1triplets).theid173parametersofeachmodelweresetusingthevalida-tionset.wecomparetheperformanceofthefol-lowingmodels:1)baselineseparatepredictors(s-pred):weconsiderabaselinemadeofindependentpredictorsforeachargumenttype.morespeci   callywetrainone-vs-allid166s(wealsotriedmulti-classid166sbuttheydidnotim-proveperformance)toindependentlypredictloca-tives,predicatesandactors.foreachargumenttypeandcandidatelabelwehaveascorecomputedbythecorrespondingid166.givenanimagewegener-atethetoptuplesthatmaximizethesumofscoresforeachargumenttype;2)baselinekcca:thismodelimplementsthekernelcanonicalcorrelationanal-556

<act=dog,pre=run,loc=beach>3<act=girl,pre=sit,loc=pool>3<act=dog,pre=run,loc=grass>3<act=man,pre=stand,loc=street>3<act=man,3pre=ride,3loc=street>3<act=boy,pre=play,loc=   eld>3<act=people,pre=sit,loc=camera>3<act=dog,pre=run,loc=water>3<act=boy,pre=sit,loc=pool>3<act=dog,3pre=run,3loc=water>3<act=dog,pre=stand,loc=   eld>3<act=dog,pre=perform,loc=air>3<act=woman,pre=sit,loc=pool>3<act=player,pre=hold,loc=football>3incorrect(loca+ve(&(ac+on(incorrect(actor(incorrect(actor(incorrect(actor(&(ac+on(incorrect(actor(&(loca+ve(training(sentences(a3guy3is3doing3a3skateboard3trick3in3front3of3a3crowd3a3man3is3skateboarding3in3front3of3a3group3of3people.3a3skateboarder3performs3a3trick3in3front3of3a3large3crowd3.3333a3skateboarder3leaping3from3a3pool3in3front3of3a3crowd.333skateboarder3does3tricks3in3front3of3crowd3while3photographer33watches33333333333<act=people,pre=perform,loc=air>3<act=people,pre=jump,loc=air>3<act=people,pre=wear,loc=air>3<act=people,pre=watch,loc=air>3<act=people,pre=perform,loc=pool>3<act=people,pre=sit,loc=air>3<act=people,pre=gather,loc=air>3figure2:samplesofpredictedtuples.top-left:examplesofvisuallycorrectpredictions.bottom:typicalerrorsononeorseveralarguments.top-right:sampleimageanditstoppredictedtuples.thetuplesinbluewerenotobservedneitherinthesp-datasetnorintheautomaticallyenlargeddataset.notethatallofthemaredescriptiveofwhatisoccurringinthescene.ysisapproachofhodoshetal.(2013).we   rstnotethatthisapproachisabletorankalistofcandidatecaptionsbutcannotdirectlygeneratetuples.togen-eratetuplesfortestimages,we   rst   ndthecaptioninthetrainingsetthathasthehighestrankingscoreforthatimageandthenextractthecorrespondingse-mantictuplesfromthatcaption;3)indicatorfea-tures(ind),thisisastandardfactorizedlog-linearmodelthatdoesnotuseanyfeaturerepresentationfortheoutputs;4)amodelthatusestheskip-gramcontinuouswordrepresentationofoutputs(scwr);5)amodelthatusesthatsemanticequivalencerep-resentationofoutputs(ser);6)acombinedmodelthatmakespredictionsusingthebestfeaturerepre-sentationforeachargumenttype(combo).s-predkccaindscwrsercomboloc1523322833pred1120243325act3025525150mean18.622.63637.33639.3table1:comparisonofoutputfeaturerepresentation.table1showstheresults.weobservethatourproposedmethodperformssigni   cantlybetterthanthebaselines.thesecondobservationisthatthebestperformingoutputfeaturerepresentationisdif-ferentfordifferentargumenttypes,forthelocativesthebestrepresentationisser,forthepredicatesisthescwrandfortheactorsusinganoutputfea-turerepresentationactuallyhurtsperformance.thebiggestimprovementwegetisonthepredicatear-guments,whereweimprovealmostby10%inaver-ageprecisionoverthebaselineusingtheskip-gramwordrepresentation.overall,themodelthatusesthebestrepresentationperformsbetterthantheindi-catorbaseline.regardingtherankoftheparametermatrices,weobservedthatthelearnedmodelscanworkwellevenifwedroptherankto10%ofitsmaximumrank.thisshowsthatthelearnedmodelsareef   -cientinthesensethattheycanworkwellwithlow-dimensionalprojectionsofthefeatures.7conclusioninthispaperwehavepresentedaframeworkforex-ploitinginputandoutputembeddingsinthecontextofstructuredprediction.wehaveappliedthisframe-worktotheproblemofpredictingcompositionalse-manticdescriptionsofimages.ourresultsshowtheadvantagesofusingoutputembeddingsandinduc-inglow-dimensionalembeddingsforhandlinglargestatespacesinstructuredpredictionproblems.theframeworkweproposeisgeneralenoughtoconsideradditionalsourcesofinformation.8acknowledgmentsthisworkwaspartlyfundedbythespanishminecoprojectrobinstructtin2014-58178-randbytheera-netchisteraprojectsvisenpcin-2013-047andi-dresspcin-2015-147.theauthorsaregratefultothenvidiadonationpro-gramforitssupportwithgpucards.557

references[akataetal.2013]zeynepakata,florentperronnin,zaidharchaoui,andcordeliaschmid.2013.label-embeddingforattribute-basedclassi   cation.inproc.ieeeconferenceoncomputervisionandpatternrecognition(cvpr).[duchiandsinger2009]johnduchiandyoramsinger.2009.ef   cientonlineandbatchlearningusingfor-wardbackwardsplitting.journalofmachinelearn-ingresearch(jmlr),10:2899   2934.[farhadietal.2010]alifarhadi,mohsenhejrati,mohammadaminsadeghi,peteryoung,cyrusrashtchian,juliahockenmaier,anddavidforsyth.2010.everypicturetellsastory:generatingsen-tencesfromimages.inproc.europeanconferenceoncomputervision(eccv).[hodoshetal.2013]micahhodosh,peteryoung,andju-liahockenmaier.2013.framingimagedescrip-tionasarankingtask:data,modelsandevaluationmetrics.journalofarti   cialintelligenceresearch(jair),47:853   899.[jiaetal.2014]yangqingjia,evanshelhamer,jeffdon-ahue,sergeykarayev,jonathanlong,rossgirshick,sergioguadarrama,andtrevordarrell.2014.caffe:convolutionalarchitectureforfastfeatureembedding.arxivpreprintarxiv:1408.5093.[karpathyandfei-fei2015]andrejkarpathyandlifei-fei.2015.deepvisual-semanticalignmentsforgen-eratingimagedescriptions.inproc.ieeeconferenceoncomputervisionandpatternrecognition(cvpr).[kulkarnietal.2011]girishkulkarni,visruthpremraj,sagnikdhar,simingli,yejinchoi,alexandercberg,andtamaralberg.2011.babytalk:un-derstandingandgeneratingimagedescriptions.inproc.ieeeconferenceoncomputervisionandpat-ternrecognition(cvpr).[mikolovetal.2013]tomasmikolov,kaichen,gregcorrado,andjeffreydean.2013.ef   cientestimationofwordrepresentationsinvectorspace.inproc.in-ternationalconferenceonlearningrepresentations(iclr).[socheretal.2014]richardsocher,andrejkarpathy,quocv.le,christopherd.manning,andandrewy.ng.2014.groundedcompositionalsemanticsfor   ndinganddescribingimageswithsentences.trans-actionsoftheassociationofcomputationallinguis-tics(tacl),2:207   218.[vinyalsetal.2015]oriolvinyals,alexandertoshev,samybengio,anddumitruerhan.2015.showandtell:aneuralimagecaptiongenerator.inproc.ieeeconferenceoncomputervisionandpatternrecogni-tion(cvpr).[westonetal.2010]jasonweston,samybengio,andnicolasusunier.2010.largescaleimageannota-tion:learningtorankwithjointword-imageembed-dings.inproc.europeanconferenceoncomputervision(eccv).