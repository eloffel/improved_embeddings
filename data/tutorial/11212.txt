927

san diego, california, june 12-17, 2016. c(cid:13)2016 association for computational linguistics

proceedings of naacl-hlt 2016, pages 927   936,

bayesiansuperviseddomainadaptationforshorttextsimilaritymdarafatsultan1,2jordanboyd-graber2tamarasumner1,21instituteofcognitivescience2departmentofcomputerscienceuniversityofcolorado,boulder,co{arafat.sultan,jordan.boyd.graber,sumner}@colorado.eduabstractidenti   cationofshorttextsimilarity(sts)isahigh-utilitynlptaskwithapplicationsinava-rietyofdomains.weexploreadaptationofstsalgorithmstodifferenttargetdomainsandap-plications.atwo-levelhierarchicalbayesianmodelisemployedfordomainadaptation(da)ofalinearstsmodeltotextfromdifferentsources(e.g.,news,tweets).thismodelisthenfurtherextendedformultitasklearning(mtl)ofthreerelatedtasks:sts,shortanswerscor-ing(sas)andanswersentenceranking(asr).inourexperiments,theadaptivemodeldemon-stratesbetteroverallcross-domainandcross-taskperformanceovertwonon-adaptivebase-lines.1shorttextsimilarity:theneedfordomainadaptationgiventwosnippetsoftext   neitherlongerthanafewsentences   shorttextsimilarity(sts)determineshowsemanticallyclosetheyare.stshasabroadrangeofapplications:questionanswering(yaoetal.,2013;severynandmoschitti,2015),textsumma-rization(dasguptaetal.,2013;wangetal.,2013),machinetranslationevaluation(chanandng,2008;liuetal.,2011),andgradingofstudentanswersinacademictests(mohleretal.,2011;ramachandranetal.,2015).stsistypicallyviewedasasupervisedmachinelearningproblem(b  aretal.,2012;lynumetal.,2014;h  anigetal.,2015).semevalcontests(agirreetal.,2012;agirreetal.,2015)havespurredrecentprogressinstsandhaveprovidedvaluabletrainingdataforthesesupervisedapproaches.however,simi-larityvariesacrossdomains,asdoestheunderlyingtext;e.g.,syntacticallywell-formedacademictextversusinformalenglishinforumqa.ourgoalistoeffectivelyusedomainadaptation(da)totransferinformationfromthesedisparatestsdomains.while   domain   cantakearangeofmean-ings,weconsideradaptationtodifferent(1)sourcesoftext(e.g.,newsheadlines,tweets),and(2)applica-tionsofsts(e.g.,qavs.answergrading).ourgoalistoimproveperformanceinanewdomainwithfewin-domainannotationsbyusingmanyout-of-domainones(section2).insection3,wedescribeourbayesianapproachthatpositsthatper-domainparametervectorsshareacommongaussianpriorthatrepresentstheglobalparametervector.importantly,thisideacanbeex-tendedwithlittleefforttoanesteddomainhierarchy(domainswithindomains),whichallowsustocreateasingle,uni   edstsmodelthatgeneralizesacrossdomainsaswellastasks,capturingthenuancesthatanstssystemmusthavefortaskssuchasshortan-swerscoringorquestionanswering.wecompareourdamethodsagainsttwobaselines:(1)adomain-agnosticmodelthatusesalltrainingdataanddoesnotdistinguishbetweenin-domainandout-of-domainexamples,and(2)amodelthatlearnsonlyfromin-domainexamples.section5showsthatacrosstendifferentstsdomains,theadaptivemodelconsistentlyoutperformsthe   rstbaselinewhileper-formingatleastaswellasthesecondacrosstrainingdatasetsofdifferentsizes.ourmultitaskmodelalsoyieldsbetteroverallresultsoverthesamebaselinesacrossthreerelatedtasks:(1)sts,(2)shortanswerscoring(sas),and(3)answersentenceranking(asr)forquestionanswering.928

2tasksanddatasetsshorttextsimilarity(sts)giventwoshorttexts,stsprovidesareal-valuedscorethatrepresentstheirdegreeofsemanticsimilarity.ourstsdatasetscomefromthesemeval2012   2015corpora,containingover14,000human-annotatedsentencepairs(viaamazonmechanicalturk)fromdomainslikenews,tweets,forumposts,andimagedescriptions.forourexperiments,weselecttendatasetsfromtendifferentdomains,containing6,450sentencepairs.1thisselectionisintendedtomaximize(a)thenumberofdomains,(b)domainuniqueness:ofthreedifferentnewsheadlinesdatasets,forexample,weselectthemostrecent(2015),discardingolderones(2013,2014),and(c)amountofper-domaindataavailable:weexcludethefnwn(2013)datasetwith189annotations,forexample,becauseitlimitsper-domaintrainingdatainourexperiments.sizesoftheselecteddatasetsrangefrom375to750pairs.averagecorrelation(pearson   sr)amongannotatorsrangesfrom58.6%to88.8%onindividualdatasets(above70%formost)(agirreetal.,2012;agirreetal.,2013;agirreetal.,2014;agirreetal.,2015).shortanswerscoring(sas)sascomesindif-ferentforms;weexploreaformwhereforashort-answerquestion,agoldanswerisprovided,andthegoalistogradestudentanswersbasedonhowsim-ilartheyaretothegoldanswer(ramachandranetal.,2015).weuseadatasetofundergraduatedatastructuresquestionsandstudentresponsesgradedbytwojudges(mohleretal.,2011).theseques-tionsarespreadacrosstendifferentassignmentsandtwoexaminations,eachonarelatedsetoftop-ics(e.g.,programmingbasics,sortingalgorithms).inter-annotatoragreementis58.6%(pearson   s  )and0.659(rmseona5-pointscale).wediscardas-signmentswithfewerthan200pairs,retaining1,182studentresponsestofortyquestionsspreadacross   veassignmentsandtests.2answersentenceranking(asr)givenafactoidquestionandasetofcandidateanswersentences,asrorderscandidatessothatsentencescontaining12012:msrpar-test;2013:smt;2014:deft-forum,onwn,tweet-news;2015:answers-forums,answers-students,belief,headlinesandimages.2assignments:#1,#2,and#3;exams:#11and#12.theanswerarerankedhigher.textsimilarityisthefoundationofmostpriorwork:acandidatesentence   srelevanceisbasedonitssimilaritywiththeques-tion(wangetal.,2007;yaoetal.,2013;severynandmoschitti,2015).forourasrexperiments,weusefactoidquestionsdevelopedbywangetal.(2007)fromtextretrievalconferences(trec)8   13.candidateqapairsofaquestionandacandidatewerelabeledwithwhetherthecandidateanswersthequestion.thequestionsareofdifferenttypes(e.g.,what,where);weretain2,247qapairsunderfourquestiontypes,eachwithatleast200answercandidatesinthecombineddevelopmentandtestsets.3eachquestiontyperepresentsauniquetopicaldomain   whoquestionsareaboutpersonsandhowmanyquestionsareaboutquantities.3bayesiandomainadaptationforstswe   rstdiscussourbaselinearmodelsforthethreetasks:bayesianl2-regularizedlinear(forstsandsas)andlogistic(forasr)regression.weextendthesemodelsfor(1)adaptationacrossdifferentshorttextsimilaritydomains,and(2)multitasklearningofshorttextsimilarity(sts),shortanswerscoring(sas),andanswersentenceranking(asr).3.1basemodelsinourbasemodels(figure1),thefeaturevectorfcombineswiththefeatureweightvectorw(includingabiastermw0)toformpredictions.eachparameterwi   whasitsownzero-meangaussianpriorwithitsstandarddeviation  widistributeduniformlyin[0,m  w],thecovariancematrix  wisdiagonal,andthezero-meanpriorl2regularizesthemodel.inthelinearmodel(figure1a),sistheoutput(similarityscoreforsts;answerscoreforsas)andisnormallydistributedaroundthedotproductwtf.themodelerror  shasauniformprioroverapre-speci   edrange[0,m  s].inthelogisticmodel(fig-ure1b)forasr,theid203pthatthecandidatesentenceanswersthequestion,is(1)thesigmoidofwtf,and(2)thebernoullipriorofa,whetherornotthecandidateanswersthequestion.thecommonvectorswandfinthesemodelsen-ablejointparameterlearningandconsequentlymul-titasklearning(section3.3).3what,when,whoandhowmany.929

        w    w  wws                    f        ~    0,                w~        ,        w  w=                    ww~        ,  ws~    w    f,        2(a)bayesianridgeregressionforstsandsas.        w    w    w~        ,        w  w=                    ww~        ,  wp=                            w    fa~                                        paf  ww(b)bayesianlogisticregressionforasr.figure1:basemodelsforsts,sasandasr.platesrepresentreplicationacrosssentencepairs.eachmodellearnsweightvectorw.forstsandsas,thereal-valuedoutputs(similarityorstudentscore)isnormallydistributedaroundtheweight-featuredotproductwtf.forasr,thesigmoidofthisdotprod-uctisthebernoullipriorforthebinaryoutputa,relevanceofthequestion   sanswercandidate.3.2adaptationtostsdomainsdomainadaptationforthelinearmodel(figure1a)learnsaseparateweightvectorwdforeachdomaind(i.e.,appliedtosimilaritycomputationsfortestpairsindomaind)alongsideacommon,globaldomain-agnosticweightvectorw   ,whichhasazero-meangaussianpriorandservesasthegaussianpriormeanforeachwd.figure2showsthemodel.bothw   andwdhavehyperpriorsidenticaltowinfigure1a.4eachwddependsnotjustonitsdomain-speci   cobservationsbutalsooninformationderivedfromtheglobal,sharedparameterw   .thebalancebetweencapturingin-domaininformationandinductivetrans-4resultsdonotimprovewithindividualdomain-speci   cinstancesof  sand  w,consistentwithfinkelandmanning(2009)fordependencyparsingandnamedentityrecognition.    w   ~        ,        w     w   =                    w   w   ~        ,  w           w     w           w    w    w~        ,        w  w  w=                    ww   w    w    ~    w   ,  w    w   fs                            ~    0,            s~    w        f,        2figure2:adaptationtodifferentstsdomains.theouterplaterepresentsreplicationacrossdomains.jointlearningofaglobalweightvectorw   alongwithindividualdomain-speci   cvectorswdenablesinductivetransferamongdomains.ferisregulatedby  w;largervarianceallowswdmorefreedomtore   ectthedomain.3.3multitasklearninganadvantageofhierarchicaldaisthatitextendseasilytoarbitrarilynesteddomains.ourmultitasklearningmodel(figure3)modelstopicaldomainsnestedwithinoneofthreerelatedtasks:sts,sas,andasr(section2).thismodeladdsaleveltothehierarchyofweightvectors:eachdomain-levelwdisnownormallydistributedaroundatask-levelweightvector(e.g.,wsts),whichinturnhasglobalgaussianmeanw   .5likethedamodel,allweightsinthesamelevelsharecommonvariancehyperparameterswhilethoseacrossdifferentlevelsareseparate.again,thishierarchicalstructure(1)jointlylearnsglobal,task-levelanddomain-levelfeatureweightsenablinginductivetransferamongtasksanddo-mainswhile(2)retainingthedistinctionbetweenin-domainandout-of-domainannotations.atask-speci   cmodel(figure1)thatonlylearnsfromin-domainannotationssupportsonly(2).ontheotherhand,anon-hierarchicaljointmodel(figure4)sup-portsonly(1):itlearnsasinglesharedwappliedtoanytestpairregardlessoftaskordomain.wecomparethesemodelsinsection5.5weusethesamevariableforthedomain-speci   cparameterwdacrosstaskstosimplifynotation.930

    asr    sas    sts        w(0)        w(1)        w(2)    w(    )~        ,        w(    ),    =0,1,2    w(    )=                    w    ,    =0,1,2    w(0)    w(1)    w(2)    w(0)    w(1)    w(2)                            ~    0,            w   ~        ,    w(0)wsts~    w   ,    w(1)wsas~    w   ,    w(1)wasr~    w   ,    w(1)w   wstswsaswasrw    fsfsw    ~    wsts,    w(2)s~    w        f,        2w    ~    wsas,    w(2)s~    w        f,        2w    ~    wasr,    w(2)    =                            w        f    ~                                    (    )w    w    fpafigure3:multitasklearning:sts,sasandasr.global(w   ),task-speci   c(wsts,wsas,wasr)anddomain-speci   c(wd)weightvectorsarejointlylearned,enablingtransferacrossdomainsandtasks.    asr    sts       sas        ~    0,            s~    w    f,        2        w    w  www~        ,  w    w~        ,        w  w=                    w                    fsfpap=                            w    fa~                                        figure4:anon-hierarchicaljointmodelforsts,sasandasr.acommonweightvectorwislearnedforalltasksanddomains.4featuresanyfeature-basedstsmodelcanserveasthebasemodelforahierarchicalbayesianadaptationframe-work.forourexperiments,weadoptthefeaturesetoftheridgeregressionmodelinsultanetal.(2015),thebest-performingsystematsemeval-2015(agirreetal.,2015).inputsentencess(1)=(w(1)1,...,w(1)n)ands(2)=(w(2)1,...,w(2)m)(whereeachwisatoken)producetwosimilarityfeatures.the   rstisthepro-portionofcontentwordsins(1)ands(2)(combined)thathaveasemanticallysimilarword   identi   edus-ingamonolingualwordaligner(sultanetal.,2014)   intheothersentence.theoverallsemanticsimi-larityofawordpair(w(1)i,w(2)j)   s(1)  s(2)isaweightedsumoflexicalandcontextualsimilari-ties:aparaphrasedatabase(ganitkevitchetal.,2013,ppdb)identi   eslexicallysimilarwords;contextualsimilarityistheaveragelexicalsimilarityin(1)de-pendenciesofw(1)iins(1)andw(2)jins(2),and(2)contentwordsin[-3,3]windowsaroundw(1)iins(1)andw(2)jins(2).lexicalsimilarityscoresofpairsinppdbaswellasweightsofwordandcon-textualsimilaritiesareoptimizedonanalignmentdataset(brockett,2007).toavoidpenalizinglonganswersnippets(thatstillhavethedesiredsemanticcontent)insasandasr,wordalignmentpropor-tionsoutsidethereference(gold)answer(sas)andthequestion(asr)areignored.thesecondfeaturecaptures   ner-grainedsim-ilaritiesbetweenrelatedwords(e.g.,cellandorganism).giventhe400-dimensionalembed-ding(baronietal.,2014)ofeachcontentword(lem-matized)inaninputsentence,wecomputeasentencevectorbyaddingitscontentlemmavectors.theco-931

taskcurrentsoaourmodelstspearson   sr=73.6%pearson   sr=73.7%saspearson   sr=51.8%pearson   sr=56.4%rmse=19.6%rmse=18.1%asrmap=74.6%map=76.0%mrr=80.8%mrr=82.8%table1:ourbaselinearmodelsbeatthestateoftheartinsts,sasandasr.sinesimilaritybetweenthes(1)ands(2)vectorsisthenusedasanstsfeature.baronietal.developthewordembeddingsusingid976fromacor-pusofabout2.8billiontokens,usingthecontinuousbag-of-words(cbow)modelproposedbymikolovetal.(2013).5experimentsforeachofthethreetasks,we   rstassesstheperfor-manceofourbasemodelto(1)verifyoursampling-basedbayesianimplementations,and(2)comparetothestateoftheart.wetraineachmodelwithametropolis-within-gibbssamplerwith50,000sam-plesusingpymc(patiletal.,2010;salvatieretal.,2015),discardingthe   rsthalfofthesamplesasburn-in.thevariancesm  wandm  sarebothsetto100.basemodelsareevaluatedontheentiretestsetforeachtask,andthesametrainingexamplesasinthestate-of-the-artsystemsareused.table1showstheresults.followingsemeval,wereportaweightedsumofcorrelations(pearson   sr)acrossalltestsetsforsts,wheretheweightofatestsetisproportionaltoitsnumberofpairs.ourmodelandsultanetal.(2015)arealmostidenticalonalltwentytestsetsfromse-meval2012   2015,supportingthecorrectnessofourbayesianimplementation.followingmohleretal.(2011),forsasweusermseandpearson   srwithgoldscoresoverallan-swers.thesemetricsarecomplementary:correlationisameasureofconsistencyacrossstudentswhileerrormeasuresdeviationfromindividualscores.ourmodelbeatsthestate-of-the-arttextmatchingmodelofmohleretal.(2011)onbothmetrics.76https://code.google.com/p/id97/7ramachandranetal.(2015)reportbetterresults;however,theyevaluateonamuchsmallerrandomsubsetofthetestdataandusein-domainannotationsformodeltraining.finally,forasr,weadopttwometricswidelyusedininformationretrieval:meanaveragepreci-sion(map)andmeanreciprocalrank(mrr).mapassessesthequalityoftherankingasawholewhereasmrrevaluatesonlythetop-rankedanswersentence.severynandmoschitti(2015)reportaconvolutionalneuralnetworkmodeloftextsimilaritywhichshowstopasrresultsonthewangetal.(2007)dataset.ourmodeloutperformsthismodelonbothmetrics.5.1adaptationtostsdomainsideally,ourdomainadaptation(da)shouldallowtheapplicationoflargeamountsofout-of-domaintrainingdataalongwithfewin-domainexamplestoimprovein-domainperformance.givendatafromndomains,twootheralternativesinsuchscenariosare:(1)totrainasingleglobalmodelusingallavail-abletrainingexamples,and(2)totrainnindividualmodels,oneforeachdomain,usingonlyin-domainexamples.wepresentresultsfromourdamodelandthesetwobaselinesonthetenstsdatasetsdis-cussedinsection2.we   xthetrainingsetsizeperdomainandspliteachdomainintotrainandtestfoldsrandomly.modelshaveaccesstotrainingdatafromalltendomains(thusninetimesmoreout-of-domainex-amplesthanin-domainones).eachmodel(global,individual,andadaptive)istrainedonrelevantanno-tationsandappliedtotestpairs,andpearson   srwithgoldscoresiscomputedforeachmodeloneachin-dividualtestset.sinceperformancecanvaryacrossdifferentsplits,weaverageover20splitsofthesametrain/testratioperdataset.finally,weevaluateeachmodelwithaweightedsumofaveragecorrelationsacrossalltestsets,wheretheweightofatestsetisproportionaltoitsnumberofpairs.figure5showshowmodelsadaptasthetrain-ingsetgrows.theglobalmodelclearlyfalterswithlargertrainingsetsincomparisontotheothertwomodels.ontheotherhand,thedomain-speci   cmodel(i.e.,thetenindividualmodels)performspoorlywhenin-domainannotationsarescarce.im-portantly,theadaptivemodelperformswellacrossdifferentamountsofavailabletrainingdata.togainadeeperunderstandingofmodelperfor-mance,weexamineresultsinindividualdomains.asingleperformancescoreiscomputedforeverymodel-domainpairbytakingthemodel   saverage932

global72.08  0.1472.21  0.2172.21  0.2872.27  0.3172.32  0.3572.39  0.5372.39  0.63individual71.18  0.8972.16  0.6272.21  0.5472.63  0.472.8  0.4172.98  0.5373.01  0.6adaptive72.14  0.1872.5  0.2572.43  0.3472.69  0.3572.86  0.3772.98  0.5573.03  0.6205075100150200300# of training pairs per dataset71.071.572.072.573.0pearson's r (%)globalindividualadaptivefigure5:resultsofadaptationtostsdomainsacrossdifferentamountsoftrainingdata.tableshowsmean  sdfrom20randomtrain/testsplits.whilethebaselinesfalteratextremes,theadaptivemodelshowsconsistentperformance.correlationinthatdomainoverallseventrainingsetsizesoffigure5.wethennormalizeeachscorebydividingbythebestscoreinthatdomain.eachcellintable2showsthisscoreforamodel-domainpair.forexample,row1showsthat   onaverage   theindividualmodelperformsthebest(henceacorrela-tionratioof1.0)onqaforumanswerpairswhiletheglobalmodelperformstheworst.whiletheadaptivemodelisnotthebestineverydomain,ithasthebestworst-caseperformanceacrossdomains.theglobalmodelsuffersindomainsthathaveuniqueparameterdistributions(e.g.,msrpar-test:aparaphrasedataset).theindividualmodelperformspoorlywithfewtrainingexamplesandindomainswithnoisyannotations(e.g.,smt:ama-chinetranslationevaluationdataset).theadaptivemodelismuchlessaffectedinsuchextremecases.thesummarystatistics(weightedbydatasetsize)con   rmthatitnotonlystaystheclosesttothebestmodelonaverage,butalsodeviatestheleastfromitsmeanperformancelevel.5.1.1qualitativeanalysiswefurtherexaminethemodelstounderstandwhytheadaptivemodelperformswellindifferentextremescenarios,i.e.,whenoneofthetwobaselinemodelsperformsworsethantheother.table3showsfea-tureweightslearnedbyeachmodelfromasplitwithdatasetglob.indiv.adapt.answers-forums(2015).98471.9999answers-students(2015).98501.9983belief(2015)1.9915.9970headlines(2015).9971.99981images(2015).9992.99861deft-forum(2014)1.9775.9943onwn(2014).9946.99901tweet-news(2014).9998.99501smt(2013)1.9483.9816msrpar-test(2012).96151.9923mean.9918.9911.9962sd.0122.0165.0059table2:correlationratiosofthethreemodelsvs.thebestmodelacrossstsdomains.bestscoresareboldfaced,worstscoresareunderlined.theadap-tivemodelhasthebest(1)overallscore,and(2)consistencyacrossdomains.datasetvar.glob.indiv.adapt.smtw1.577.214.195w2.406-.034.134r.4071.3866.4071msrpar-testw1.5771.0.797w2.406-.378.050r.6178.6542.6469answers-studentsw1.577.947.865w2.406.073.047r.7677.7865.7844table3:featureweightsandcorrelationsofdifferentmodelsinthreeextremescenarios.ineachcase,theadaptivemodellearnsrelativeweightsthataremoresimilartothoseinthebestbaselinemodel.seventy-   vetrainingpairsperdomainandhowwelleachmodeldoes.allthreedomainshaveverydifferentoutcomesforthebaselinemodels.weshowweightsforthealignment(w1)andembeddingfeatures(w2).ineachdomain,(1)therelativeweightslearnedbythetwobaselinemodelsareverydifferent,and(2)theadaptivemodellearnsrelativeweightsthatareclosertothoseofthebestmodel.insmt,forexample,thepredictorweightslearnedbytheadaptivemodelhavearatioverysimilartotheglobalmodel   sanddoesjustaswell.onanswers-students,however,itlearnsweightssimilartothoseofthein-domainmodel,againapproachingbestresultsforthedomain.933

now,thelaborofcleaningupatthekaraokeparlorisrealized.gold=.52   g=.1943   i=.2738   a=.2024uptillnowonthelocationthecleaningworkisalreadycompleted.thechelseadefendermarceldesaillyhasbeenthelatesttospeakout.gold=.45   g=.2513   i=.2222   a=.2245marceldesailly,thefrancecaptainandchelseadefender,believesthelatteristrue.table4:sentencepairsfromsmtandmsrpar-testwithgoldsimilarityscoresandmodelerrors(global,individualandadaptive).theadaptivemodelerrorisveryclosetothebestmodelerrorineachcase.table4showstheeffectofthisontwospeci   csentencepairsasexamples.the   rstpairisfromsmt;theadaptivemodelhasamuchlowererrorthantheindividualmodelonthispair,asitlearnsahigherrelativeweightfortheembeddingfeatureinthisdomain(table3)viainductivetransferfromout-of-domainannotations.thesecondpair,frommsrpar-test,showstheopposite:in-domainannota-tionshelptheadaptivemodel   xthefaultyoutputoftheglobalmodelbyupweightingthealignmentfeatureanddownweightingtheembeddingfeature.theadaptivemodelgainsfromthestrengthsofbothin-domain(higherrelevance)andout-of-domain(moretrainingdata)annotations,leadingtogoodre-sultseveninextremescenarios(e.g.,indomainswithuniqueparameterdistributionsornoisyannotations).5.2multitasklearningwenowanalyzeperformanceofourmultitasklearn-ing(mtl)modelineachofthethreetasks:sts,sasandasr.multitaskbaselinesresembleda   s:(1)aglobalmodeltrainedonallavailabletrainingdata(figure4),and(2)nineteentask-speci   cmodels,eachtrainedonanindividualdatasetfromoneofthethreetasks(figure1).thesmallestofthesedatasetshasonly204pairs(sasassignment#1);therefore,weusetrainingsetswithupto175pairsperdataset.becausethemtlmodelismorecomplex,weuseastrongerid173forthismodel(m  w=10)whilekeepingthenumberofmcmcsamplesun-changed.asinthedaexperiments,wecomputeaverageperformanceovertwentyrandomtrain/testsplitsforeachtrainingsetsize.figure6showsstsresultsforallmodelsacrossglobal71.79  0.3971.94  0.3472.05  0.3972.07  0.2972.11  0.3872.23  0.3172.05  0.41individual70.57  1.4572.06  0.5672.32  0.5572.67  0.4472.73  0.5172.9  0.3372.75  0.41adaptive71.99  0.4372.18  0.2772.55  0.3372.67  0.3572.75  0.4372.93  0.3472.8  0.37205075100125150175# of training pairs per dataset70.571.071.572.072.573.0pearson's r (%)globalindividualadaptivefigure6:multitasklearningforsts:mean  sdfromtwentyrandomtrain/testsplits.theadaptivemodelconsistentlyperformswellwhilethebaselineshavedifferentfailuremodes.differenttrainingsetsizes.likeda,theadaptivemodelconsistentlyperformswellwhiletheglobalandindividualmodelshavedifferentfailuremodes.however,theindividualmodeldoesbetterthaninda:itovertakestheglobalmodelwithfewertrainingexamplesandthedifferenceswiththeadaptivemodelaresmaller.thissuggeststhatinductivetransferandthereforeadaptationislesseffectiveforstsinthemtlsetupthaninda.laterinthissection,coarse-grainedasrannotations(binaryasopposedtoreal-valued)iid4lmayprovideanexplanationforthis.theperformancedropafter150trainingpairsisalikelyconsequenceoftherandomtrain/testselectionprocess.forsas,theadaptivemodelagainhasthebestoverallperformanceforbothcorrelationanderror(figure7).thecorrelationplotisqualitativelysimi-lartothestsplot,buttheglobalmodelhasamuchhigherrmseacrossalltrainingsetsizes,indicatingaparametershiftacrosstasks.importantly,theadap-tivemodelremainsunaffectedbythisshift.theasrresultsinfigure8showadifferentpat-tern.contrarytoallresultsthusfar,theglobalmodelperformsthebestinthistask.theindivid-ualmodelconsistentlyhaslowerscores,regardlessoftheamountoftrainingdata.importantly,theadap-tivemodelstaysclosetotheglobalmodelevenwithveryfewtrainingexamples.theasrdatasetsareheavilybiasedtowardsnegativeexamples;thus,we934

global58.49  1.1258.84  0.8858.81  1.1858.94  1.5858.59  2.3959.25  2.7960.14  2.77individual55.8  4.6560.15  1.8660.98  1.1561.38  2.061.45  2.2161.79  2.5263.02  2.51adaptive59.64  1.7460.97  1.5161.4  1.0761.59  1.8961.67  2.361.85  2.5263.16  2.49205075100125150175# of training pairs per dataset5658606264pearson's r (%)globalindividualadaptive(a)correlation.global29.01  0.9228.95  0.6629.01  0.7828.9  0.5228.9  0.6828.59  0.7228.06  0.8individual19.94  0.8819.03  0.4118.76  0.3318.81  0.4518.57  0.5218.65  0.5818.37  0.84adaptive19.22  0.3218.9  0.3618.68  0.318.77  0.4418.53  0.5318.64  0.5918.35  0.83205075100125150175# of training pairs per dataset18202224262830rmse (%)globalindividualadaptive(b)error.figure7:multitasklearningforsas:mean  sdfrom20randomtrain/testsplits.theadaptivemodelperformsthebest,andsuccessfullyhandlesdomainshiftevidentfromtheglobalmodelerror.global75.86  0.3976.16  0.876.32  0.9676.3  1.3175.95  1.2276.78  1.2476.41  1.31individual70.0  1.4574.53  1.375.15  1.2575.66  1.2775.13  1.1176.21  1.275.76  1.17adaptive75.39  1.1475.95  0.876.0  1.0776.04  1.2175.47  1.076.35  1.2676.21  1.23205075100125150175# of training pairs per dataset7071727374757677mean average precision (%)globalindividualadaptive(a)meanaverageprecision.global82.82  0.6382.95  0.9183.23  1.1582.78  1.5982.18  1.4383.1  1.382.27  1.48individual76.61  4.5681.23  1.6481.91  1.5782.03  1.4481.36  1.3782.34  1.2481.66  1.72adaptive82.31  1.3682.71  0.8682.72  1.2382.44  1.3981.66  1.2682.56  1.4282.07  1.67205075100125150175# of training pairs per dataset767778798081828384mean reciprocal rank (%)globalindividualadaptive(b)meanreciprocalrank.figure8:multitasklearningforasr:mean  sdfrom20randomtrain/testsplits.leastaffectedbycoarse-grainedin-domainannotations,theglobalmodelperformsthebest;theadaptivemodelstayscloseacrossalltrainingsetsizes.935

usestrati   edsamplingtoensureeachasrtrainingsethasbalancedexamples.areasonfortheglobalmodel   sstrengthatasrmaylieinthe   nergranularityofthereal-valuedstsandsasscorescomparedtobinaryasranno-tations.ifa   negranularityisindeeddesirableintrainingdata,asamodelthatignoresin-domainandout-of-domaindistinction,theglobalmodelwouldbeaffectedtheleastbycoarse-grainedasrannotations.totestthishypothesis,wetrainalinearmodelonallstsexamplesfromsemeval2012   2015andapplyittotheasrtestsetviaalogistictransformation.thismodelindeeddemonstratesbetterresults(map=.766,mrr=.839)thanourbasemodeltrainedonasranno-tations(table1).thisisanunusualscenariowherein-domaintrainingexamplesmatterlessthanout-of-domainones,hurtingdomain-speci   candadaptivemodels.goingbacktosts,this   ndingalsooffersanexpla-nationofwhyadaptationmighthavebeenlessusefulinmultitasklearningthanindomainadaptation,asonlytheformerhasasrannotations.6discussionandrelatedworkforavarietyofshorttextsimilaritytasks,domainadaptationimprovesaverageperformanceacrossdif-ferentdomains,tasks,andtrainingsetsizes.ouradaptivemodelisalsobyfartheleastaffectedbyad-versefactorssuchasnoisytrainingdataandscarcityorcoarsegranularityofin-domainexamples.thiscombinationofexcellentaverage-caseandveryreli-ableworst-caseperformancemakesitthemodelofchoicefornewstsdomainsandapplications.althoughstsisausefultaskwithsparsedata,fewdomainadaptationstudieshavebeenreported.amongthoseisthesupervisedmodelofheilmanandmadnani(2013a;2013b)basedonthemultilevelmodelofdaum  eiii(2007).gellaetal.(2013)reportusingatwo-levelstackedregressor,wherethesecondlevelcombinespredictionsfromnlevel1models,eachtrainedondatafromaseparatedomain.unsu-pervisedmodelsusetechniquessuchastaggingex-ampleswiththeirsourcedatasets(gellaetal.,2013;severynetal.,2013)andcomputingvocabularysim-ilaritybetweensourceandtargetdomains(aroraetal.,2015).tothebestofourknowledge,oursisthe   rstsystematicstudyofsuperviseddaandmtltechniquesforstswithdetailedcomparisonswithcomparablenon-adaptivebaselines.7conclusionsandfutureworkwepresenthierarchicalbayesianmodelsforsuper-viseddomainadaptationandmultitasklearningofshorttextsimilaritymodels.inourexperiments,thesemodelsshowimprovedoverallperformanceacrossdifferentdomainsandtasks.weintendtoex-ploreadaptationtootherstsapplicationsandwithadditionalstsfeatures(e.g.,wordandcharacterid165overlap)infuture.unsupervisedandsemi-superviseddomainadaptationtechniquesthatdonotassumetheavailabilityofin-domainannotationsorthatlearneffectivedomainssplits(huetal.,2014)provideanotheravenueforfutureresearch.acknowledgmentsthismaterialisbasedinpartuponworksup-portedbythensfundergrantsehr/0835393andehr/0835381.boyd-graberissupportedbynsfgrantsiis/1320538,iis/1409287,andncse/1422492.anyopinions,   ndings,conclusions,orrecommenda-tionsexpressedherearethoseoftheauthorsanddonotnecessarilyre   ecttheviewofthesponsor.referencesenekoagirre,danielcer,monadiab,andaitorgonzalez-agirre.2012.semeval-2012task6:apilotonsemantictextualsimilarity.insemeval.enekoagirre,danielcer,monadiab,aitorgonzalez-agirre,andweiweiguo.2013.*sem2013sharedtask:semantictextualsimilarity.in*sem.enekoagirre,carmenbanea,clairecardie,danielcer,monadiab,aitorgonzalez-agirre,weiweiguo,radamihalcea,germanrigau,andjanycewiebe.2014.semeval-2014task10:multilingualsemantictextualsimilarity.insemeval.enekoagirre,carmenbanea,clairecardie,danielcer,monadiab,aitorgonzalez-agirre,weiweiguo,i  nigolopez-gazpio,montsemaritxalar,radamihalcea,ger-manrigau,larraitzuria,andjanycewiebe.2015.semeval-2015task2:semantictextualsimilarity,en-glish,spanishandpilotoninterpretability.insemeval.piyusharora,chrishokamp,jenniferfoster,andgarethj.f.jones.2015.dcu:usingdistributionalsemanticsanddomainadaptationforthesemantictextualsimilar-itysemeval-2015task2.insemeval.936

danielb  ar,chrisbiemann,irynagurevych,andtorstenzesch.2012.ukp:computingsemantictextualsim-ilaritybycombiningmultiplecontentsimilaritymea-sures.insemeval.marcobaroni,georgianadinu,andgerm  ankruszewski.2014.don   tcountandpredict!asystematiccompari-sonofcontext-countingvs.context-predictingsemanticvectors.inproceedingsoftheassociationforcompu-tationallinguistics.chrisbrockett.2007.aligningtherte2006corpus.technicalreportmsr-tr-2007-77,microsoftre-search.yeesengchanandhweetoung.2008.maxsim:amaximumsimilaritymetricformachinetranslationevaluation.inproceedingsoftheassociationforcom-putationallinguistics.anirbandasgupta,ravikumar,andsujithravi.2013.summarizationthroughsubmodularityanddispersion.inproceedingsoftheassociationforcomputationallinguistics.haldaum  eiii.2007.frustratinglyeasydomainadapta-tion.inproceedingsoftheassociationforcomputa-tionallinguistics.jennyrosefinkelandchristopherd.manning.2009.hierarchicalbayesiandomainadaptation.inconfer-enceofthenorthamericanchapteroftheassociationforcomputationallinguistics,morristown,nj,usa.juriganitkevitch,benjaminvandurme,andchriscallison-burch.2013.ppdb:theparaphrasedatabase.inconferenceofthenorthamericanchap-teroftheassociationforcomputationallinguistics.spandanagella,baharsalehi,marcolui,karlgrieser,paulcook,andtimothybaldwin.2013.unimelbnlp-core:integratingpredictionsfrommultipledomainsandfeaturesetsforestimatingse-mantictextualsimilarity.in*sem.christianh  anig,robertremus,andxosedelapuente.2015.exbthemis:extensivefeatureextractionfromwordalignmentsforsemantictextualsimilarity.insemeval.michaelheilmanandnitinmadnani.2013a.ets:do-mainadaptationandstackingforshortanswerscoring.insemeval.michaelheilmanandnitinmadnani.2013b.henry-core:domainadaptationandstackingfortextsimi-larity.insemeval.yueninghu,kezhai,vladeidelman,andjordanboyd-graber.2014.polylingualtree-basedtopicmodelsfortranslationdomainadaptation.inassociationforcomputationallinguistics.changliu,danieldahlmeier,andhweetoung.2011.betterevaluationmetricsleadtobettermachinetransla-tion.inproceedingsofempiricalmethodsinnaturallanguageprocessing.andr  elynum,parthapakray,bj  orngamb  ack,andsergiojimenez.2014.ntnu:measuringsemanticsimilaritywithsublexicalfeaturerepresentationsandsoftcardi-nality.insemeval.tomasmikolov,kaichen,gregcorrado,andjeffreydean.2013.ef   cientestimationofwordrepresenta-tionsinvectorspace.inproceedingsoftheinterna-tionalconferenceonlearningrepresentationswork-shop.michaelmohler,razvanbunescu,andradamihalcea.2011.learningtogradeshortanswerquestionsusingsemanticsimilaritymeasuresanddependencygraphalignments.inproceedingsoftheassociationforcom-putationallinguistics.anandpatil,davidhuard,andchristopherj.fonnesbeck.2010.pymc:bayesianstochasticmodellinginpython.journalofstatisticalsoftware,35(4).lakshmiramachandran,jiancheng,andpeterfoltz.2015.identifyingpatternsforshortanswerscoringusinggraph-basedlexico-semantictextmatching.innaacl-bea.johnsalvatier,thomasv.wiecki,andchristopherfon-nesbeck.2015.probabilisticprogramminginpythonusingpymc.arxiv:1507.08050v1.aliakseiseverynandalessandromoschitti.2015.learn-ingtorankshorttextpairswithconvolutionaldeepneuralnetworks.inproceedingsoftheacmsigirconferenceonresearchanddevelopmentininforma-tionretrieval.aliakseiseveryn,massimonicosia,andalessandromos-chitti.2013.learningsemantictextualsimilaritywithstructuralrepresentations.inproceedingsoftheasso-ciationforcomputationallinguistics.mdarafatsultan,stevenbethard,andtamarasum-ner.2014.backtobasicsformonolingualalignment:exploitingwordsimilarityandcontextualevidence.tacl,2.mdarafatsultan,stevenbethard,andtamarasumner.2015.dls@cu:sentencesimilarityfromwordalign-mentandsemanticvectorcomposition.insemeval.mengqiuwang,noaha.smith,andterukomita-mura.2007.whatisthejeopardymodel?aquasi-synchronousgrammarforqa.inproceedingsofem-piricalmethodsinnaturallanguageprocessing.luwang,hemaraghavan,vittoriocastelli,raduflo-rian,andclairecardie.2013.asentencecompressionbasedframeworktoquery-focusedmulti-documentsummarization.inproceedingsoftheassociationforcomputationallinguistics.xuchenyao,benjaminvandurme,chriscallison-burch,andpeterclark.2013.answerextractionassequencetaggingwithtreeeditdistance.inconferenceofthenorthamericanchapteroftheassociationforcompu-tationallinguistics.