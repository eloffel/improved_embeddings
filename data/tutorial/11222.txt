6
1
0
2

 

y
a
m
7
1

 

 
 
]
l
c
.
s
c
[
 
 

4
v
7
2
1
6
0

.

3
0
6
1
:
v
i
x
r
a

sentence pair scoring: towards uni   ed framework

for text comprehension

petr baudi  s, jan pichl, tom  a  s vysko  cil and jan   sediv  y

fee ctu prague

department of cybernetics

technick  a 2, prague, czech republic

baudipet@fel.cvut.cz

abstract

we review the task of sentence pair scor-
ing, popular in the literature in various
forms     viewed as answer sentence se-
lection, semantic text scoring, next ut-
terance ranking, recognizing textual en-
tailment, id141 or e.g. a component
of memory networks.

we argue that all such tasks are similar
from the model perspective and propose
new baselines by comparing the perfor-
mance of common ir metrics and popu-
lar convolutional, recurrent and attention-
based neural models across many sen-
tence pair scoring tasks and datasets. we
discuss the problem of evaluating ran-
domized models, propose a statistically
grounded methodology, and attempt
to
improve comparisons by releasing new
datasets that are much harder than some
of the currently used well explored bench-
marks. we introduce a uni   ed open source
software framework with easily pluggable
models and tasks, which enables us to
experiment with multi-task reusability of
trained sentence models. we set a new
state-of-art in performance on the ubuntu
dialogue dataset.

1 introduction

an nlp machine learning task often involves clas-
sifying a sequence of tokens such as a sentence or
a document, i.e. approximating a function f1(s)    
[0, 1] (where f1 may determine a domain, senti-
ment, etc.). but there is a large class of prob-
lems that involve classifying a pair of sentences,
f2(s0, s1)     r (where s0, s1 are sequences of to-
kens, typically sentences).

typically, the function f2 represents some sort
of semantic similarity, that is whether (or how
much) the two sequences are semantically related.
this formulation allows f2 to be a measure for
tasks as different as topic relatedness, paraphras-
ing, degree of entailment, a pointwise ranking
task for answer-bearing sentences or next utter-
ance classi   cation.

in this work, we adopt the working assumption
that there exist certain universal f2 type measures
that may be successfuly applied to a wide variety
of semantic similarity tasks     in the case of neu-
ral network models trained to represent universal
semantic comprehension of sentences and adapted
to the given task by just    ne-tuning or adapting the
output neural layer (in terms of architecture or just
weights). our argument for preferring f2 to f1 in
this pursuit is the fact that the other sentence in
the pair is essentially a very complex label when
training the sequence model, which can therefore
discern semantically rich structures and dependen-
cies. determining and demonstrating such univer-
sal semantic comprehension models across mul-
tiple tasks remains a few steps ahead, since the
research landscape is fragmented in this regard.
model research is typically reported within the
context of just a single f2-type task, each dataset
requires sometimes substantial engineering work
before measurements are possible, and results are
reported in ways that make meaningful model
comparisons problematic.

our main aims are as follows.

(a) unify re-
search within a single framework that employs
task-independent models and task-speci   c adapta-
tion modules.
(b) improve the methodology of
model evaluation in terms of statistics, compar-
ing with strong non-neural ir baselines, and in-
troducing new datasets with better characteristics.
(c) demonstrate the feasibility of pursuing uni-
versal, task-independent f2 models, showing that

even simple neural models learn universal seman-
tic comprehension by employing cross-task trans-
fer learning.

the paper is structured as follows.

in sec. 2,
we outline possible speci   c f2 tasks and available
datasets; in sec. 3, we survey the popular non-
neural and neural baselines in the context of these
tasks;    nally, in sec. 4, we present model-task
evaluations within a uni   ed framework to estab-
lish the watermark for future research as well as
gain insight into the suitability of models across
a variety of tasks. in sec. 5, we demonstrate that
id21 across tasks is helpful to power-
fully seed models. we conclude with sec. 6, sum-
marizing our    ndings and outlining several future
research directions.

2 tasks and datasets

the tasks we are aware of that can be phrased
as f2-type problems are listed below.
in gen-
eral, we primarily focus on tasks that have rea-
sonably large and realistically complex datasets
freely available. on the contrary, we have explic-
itly avoided datasets that have licence restrictions
on availability or commercial usage.

2.1 answer sentence selection
given a factoid question and a set of candidate
answer-bearing sentences in encyclopedic style,
the    rst task is to rank higher sentences that are
more likely to contain the answer to the question.
as it is fundamentally an information retrival
task in nature, the model performance is com-
monly evaluated in terms of mean average preci-
sion (map) and mean reciprocial rank (mrr).
this task is popular in the nlp research com-
munity thanks to the dataset introduced in (wang
et al., 2007) (which we refer to as wang), with
six papers published between february 2015 and
2016 alone and neural models substantially im-
proving over classical approaches based primarily
on parse tree edits.1
it is possibly the main re-
search testbed for f2-style task models. this task
has also immediate applications e.g. in question
answering systems.

in the context of practical applications, the so-
far standard wang dataset has several downsides
we observed when tuning and evaluating our mod-
els, illustrated numerically in fig. 1     the set of

1http://aclweb.org/aclwiki/index.php?
title=question_answering_(state_of_the_
art)

candidate sentences is often very small and quite
uneven (which also makes rank-based measures
unstable) and the total number of individual sen-
tence pairs as well as questions is relatively small.
furthermore, the validation and test set are very
small, which makes for noisy performance mea-
surements; the splits also seem quite different in
the nature of questions since we see minimum
correlation between performance on the validation
and test sets, which calls the parameter tuning pro-
cedures and epoch selection for early stopping into
question. alternative datasets wikiqa (yang et
al., 2015) and insuranceqa (tan et al., 2015) were
proposed, but are encumbered by licence restric-
tions. furthermore, we speculate that they may
suffer from many of the problems above2 (even if
they are somewhat larger).

to alleviate the problems listed above, we are
introducing a new dataset yodaqa/large2470
based on an extension of the curatedv2 ques-
tion dataset (introduced in (baudi  s and   sediv  y,
2015), further denoisi   ed by mechanical turkers)
with candidate sentences as retrieved by the yo-
daqa id53 system (baudi  s, 2015)
from english wikipedia and labelled by matching
the gold standard answers in the passages.3

motivated by another problem related to the yo-
daqa system, we also introduce another dataset
wqmprop, where s0 are again question sentences,
but s1 are english labels of properties that make
a path within the freebase knowledge base that
connects an entity linked in the question to the cor-
rect answer. this task (property selection) can be
evaluated identically to the previous task, and so-
lutions often involving convolutional neural net-
works have been studied in the question answer-
ing literature (yih et al., 2015) (xu et al., 2016).
our sentences have been derived from the we-
bquestions dataset (berant et al., 2013) extended
with the moviese dataset questions (originally in-
troduced in (baudi  s and   sediv  y, 2015)); the prop-
erty paths are based on the freebase knowledge
graph dump, generated based on entity linking and
exploration procedure of yodaqa v1.5.4

2moreover, insuranceqa is effectively a classi   cation
task rather than a ranking task, which we do not    nd as ap-
pealing in the context of practical applications.

3note that the wang and yodaqa datasets however share
a common ancestry regarding the set of questions and there
may be some overlaps, even across train and test splits.
therefore, mixing training and evaluation on wang and yo-
daqa datasets within a single model instance is not advisable.

4https://github.com/brmson/

fig. 1 compares the critical characteristics of
the datasets.
furthermore, as apparent below,
the baseline performances on the newly proposed
datasets are much lower, which suggests that fu-
ture model improvements will be more apparent
in evaluation.

2.2 next utterance ranking

(lowe et al., 2015) proposed the new large-scale
real-world ubuntu dialogue dataset for an f2-style
task of ranking candidates for the next utterance in
a chat dialog, given the dialog context. the tech-
nical formulation of the task is the same as for an-
swer sentence selection, but semantically, choos-
ing the best followup has different concerns than
choosing an answer-bearing sentence. recall at
top-ranked 1, 2 or 5 utterances out of either 2 or 10
candidates is reported; we also propose reporting
the utterance mrr as a more aggregate measure.
the newly proposed ubuntu dialogue dataset is
based on irc chat logs of the ubuntu commu-
nity technical support channels and contains casu-
ally typed interactions regarding computer-related
problems.5 while the training set consists of in-
dividual labelled pairs, during evaluation 10 fol-
lowups to given message(s) are ranked. the se-
quences might be over 200 tokens long.

our primary motivation for using this dataset
is its size. the numerical characteristics of this
dataset are shown in table 1.6 we use the v2
version of the dataset.7 research published on
this dataset so far relies on simple neural models.
(lowe et al., 2015) (kadlec et al., 2015)

2.3 recognizing id123 and

semantic textual similarity

one of the classic tasks at the boundary of natural
language processing and arti   cial intelligence is
the id136 problem of recognizing textual en-
tailment (dagan et al., 2006)     given a pair of a
factual sentence and a hypothesis sentence, we are
to determine whether the hypothesis represents a
contradiction, entailment or is neutral (cannot be
proven or disproven).

dataset-factoid-webquestions branch movies

5in a manner, they resemble tweet data, but without the
length restriction and with heavily technical jargon, inter-
spersed command sequences etc.

6as in past papers, we use only the    rst 1m pairs (10%)

of the training set.

7https://github.com/rkadlec/
ubuntu-ranking-dataset-creator

we include two current popular machine learn-
ing datasets for this task. the stanford natural
language id136 snli dataset (bowman et al.,
2015) consists of 570k english sentence pairs with
the facts based on image captions, and 10k + 10k
of the pairs held out as validation and test sets. the
sick-2014 dataset (marelli et al., 2014) was in-
troduced as task 1 of the semeval 2014 confer-
ence and in contrast to snli, it is geared at speci   -
cally benchmarking semantic compositional meth-
ods, aiming to capture only similarities on purely
language and common knowledge level, without
relying on domain knowledge, and there are no
named entities or multi-word idioms; it consists of
4500 training pairs, 500 validation pairs and 4927
testing pairs.

for the sick-2014 dataset, we also report re-
sults on the semantic textual similarity. this task
originates in the sts track of the semeval con-
ferences (agirre et al., 2015) and involves scoring
pairs of sentences from 0 to 5 with the objective of
maximizing correlation (pearson   s r) with manu-
ally annotated gold standard.

3 models

is a universal

as our goal
text comprehen-
sion model, we focus on neural network models
architecture-wise. we assume that the sequence
is transformed using n-dimensional word embed-
dings on input, and employ models that produce
a pair of sentence embeddings e0, e1 from the
sequences of id27s e0, e1. unless
noted otherwise, a siamese architecture is used
that shares weights among both sentenes.

a scorer module that compares the e0, e1 sen-
tence embeddings to produce a scalar result is con-
nected to the model; for speci   c task-model con-
   gurations, we use either the dot-product module
1 (representing non-normalized vector an-
e0    e t
gle, as in e.g. (yu et al., 2014) or (weston et al.,
2014))8 or the mlp module that takes element-
wise product and sum of the embeddings and feeds
them to a two-layer id88 with hidden layer
of width 2n (as in e.g. (tai et al., 2015)).9 for the
sts task, we follow this by score regression using

8not normalizing the vectors acts as a id173 for
their size. in all our experiments, cosine distance fared much
worse.

9the motivation is to capture both angle and euclid dis-
tance in multiple weighed sums. past literature uses absolute
difference rather than sum, but both performed equally in our
experiments and we adopted sum for technical reasons.

dataset
wang

yodaqa/large2470

wqmprop

train pairs val. pairs test pairs val.-test r ev. #s0 ev. #s1 per s0
34.9   131%
159.2   100%
118.753   85%

44648
220846
407465

1149
55052
137235
195600

1518
120069
277509
189200

-0.078
0.348
0.836
0.884

178
1100
3430
38480

ubuntu dialogue v2

1m

10

figure 1: the val.-test column shows inter-trial pearson   s r of validation and test mrrs, averaged across the models
we benchmarked (see below). the s0 and s1 statistics are shown for the evaluation (ev.     validation and test) portion of
the datasets. the last column includes relative standard deviation of the number of candidate sentences per question, which
corresponds to the variation in the dif   culty of the ranking task (as well as variation in expected measure values for individual
questions).

class interpolation as in (tai et al., 2015).

when training for a ranking task (answer sen-
tence selection), we use the bipartite ranking ver-
sion of ranknet (burges et al., 2005) as the objec-
tive; when training for sts task, we use pearson   s
r formula as the objective; for binary classi   cation
tasks, we use the binary crossid178 objective.

3.1 baselines

in order to anchor the reported performance, we
report several basic methods. weighed word
overlaps metrics tf-idf and bm25 (robertson
et al., 1995) are inspired by ir research and pro-
vide strong baselines for many tasks. we treat s0
as the query and s1 as the document, counting the
number of common words and weighing them ap-
propriately. idf is determined on the training set.
the avg metric represents the baseline method
when using id27s that proved success-
ful e.g. in (yu et al., 2014) or (weston et al., 2014),
simply taking the mean vector of the word em-
bedding sequence and training an u weight matrix
n   2n that projects both embeddings to the same
vector space, ei = tanh(u      ei), where the mlp
scorer compares them. during training, p = 1/3
standard (elementwise) dropout is applied on the
input embeddings.

a simple extension of the above are the dan
deep averaging networks (iyyer et al., 2015),
which were shown to adequately replace much
more complex models in some tasks. two dense
id88 layers are stacked between the mean
and projection, relu is used instead of tanh as the
non-linearity, and word-level dropout is used in-
stead of elementwise dropout.

3.2 recurrent neural networks

id56 with memory units are popular models for
processing sentenes (tan et al., 2015) (lowe et
al., 2015) (bowman et al., 2015). we use a bidi-

rectional network with 2n gru memory units10
(cho et al., 2014) in each direction; the    nal unit
states are summed across the per-direction grus
to yield a 2n vector representation of the sen-
tence. like in the avg baseline, a projection matrix
is applied on this representation and    nal vectors
compared by an mlp scorer. we have found that
applying massive dropout p = 4/5 both on the in-
put and output of the network helps to avoid over-
   tting even early in the training.

3.3 convolutional neural networks
id98 with sentence-wide pooling layer are also
popular models for processing sentences (yu et al.,
2014) (tan et al., 2015) (severyn and moschitti,
2015) (he et al., 2015) (kadlec et al., 2015). we
apply a multi-channel convolution (kim, 2014)
with single-token channel of n convolutions and
2, 3, 4 and 5-token channels of n/2 convolu-
tions each, relu transfer function, max-pooling
over the whole sentence, and as above a projec-
tion to shared space and an mlp scorer. dropout
is not applied.

3.4 id56-id98 model
the id56-id98 model aims to combine both re-
current and convolutional networks by using the
memory unit states in each token as the new repre-
sentation of the token which is then fed to the con-
volutional network. inspired by (tan et al., 2015),
the aim of this model is to allow the id56 to model
long-term dependencies and model contextual rep-
resentations of words, while taking advantage of
the id98 and pooling operation for crisp selection
of the gist of the sentence. we use the same pa-
rameters as for the individual models, but with no
dropout and reducing the number of parameters by
using only n memory units per direction.

10while the lstm architecture is more popular, we have
found the gru results are equivalent while the number of
parameters is reduced.

3.5 attention-based models
the idea of id12 is to attend preferren-
tially to some parts of the sentence when building
its representation (hermann et al., 2015) (tan et
al., 2015) (dos santos et al., 2016) (rockt  aschel
et al., 2015). there are many ways to model at-
tention, we adopt the (tan et al., 2015) model
attn1511 as a conceptually simple and easy to im-
plement baseline.
it asymmetrically extends the
id56-id98 model by extra links from s0 id98
output to the post-recurrent representation of each
s1 token, determining an attention level for each
token by weighed sum of the token vector ele-
ments, focusing on the relevant s1 segment by
transforming the attention levels using softmax
and multiplying the token representations by the
attention levels before they are fed to the convolu-
tional network.

convolutional network weights are not shared
between the two sentences and the convolutional
network output is not projected before applying
the mlp scorer. the id98 used here is single-
channel with 2n convolution    lters 3 tokens wide.

4 model performance

4.1 dataset-sts framework
to easily implement models, dataset
loaders
and task adapters in a modular fashion so that
any model can be easily run on any f2-type
task, we have created a new software pack-
age dataset-sts that integrates a variety of
datasets, a python dataset adapter pysts and a
python library for easy construction of deep neu-
ral nlp models for semantic sentence pair scoring
kerasts that uses the keras machine learning li-
brary (chollet, 2015). the framework is available
for other researchers as open source on github.11

4.2 experimental setting
we use n = 300 dimensional glove embed-
dings matrix pretrained on wikipedia 2014 + gi-
gaword 5 (pennington et al., 2014) that we keep
adaptable during training; words in the training set
not included in the pretrained model are initial-
ized by random vectors uniformly sampled from
[   0.25, +0.25] to match the embedding standard
deviation.

word overlap is an important feature in many
f2-type tasks (yu et al., 2014) (severyn and mos-

11https://github.com/brmson/dataset-sts

chitti, 2015), especially when the sentences may
contain named entities, numeric or other data
for which no embedding is available. as a
workaround, ensemble of world overlap count and
neural model score is typically used to produce
the    nal score. in line with this idea, in the an-
swer sentence selection wang and large2470
datasets, we use the bm25 overlap baseline as
an additional input to the mlp scoring module,
and prune the scored samples to top 20 based on
bm25.12 furthermore, we extend the embedding
of each input token by several extra dimensions
carrying boolean    ags     bigram overlap, unigram
overlap (except stopwords and interpunction), and
whether the token starts with a capital letter or is a
number.

particular hyperparameters are tuned primar-
ily on the yodaqa/large2470 dataset unless
noted otherwise in the respective results table
caption. we apply 10   4 l2 id173 and
use adam optimization with standard parameters
(kingma and ba, 2014). in the answer selection
tasks, we train on 1/4 of the dataset in each epoch.
after training, we use the epoch with best vali-
dation performance; sadly, we typically observe
heavy over   tting as training progresses and rarely
use a model from later than a couple of epochs.

4.3 evaluation methodology
we report model performance averaged across 16
training runs (with different seeds). a consid-
eration we must emphasize is that randomness
plays a large role in neural models both in terms
of randomized weight initialization and stochas-
tic dropout. for example, the typical methodol-
ogy for reporting results on the wang dataset is
to evaluate and report a single test run after tun-
ing on the dev set,13 but wang test mrr has em-
pirical standard deviation of 0.025 across repeated
runs of our attn1511 model, which is more than
twice the gap between every two successive papers
pushing the state-of-art on this dataset! see the    -
marked sample in fig. 2 for a practical example
of this phenomenon. furthermore, on more com-
plex tasks (answer sentence selection in particu-
lar, see fig. 1) the validation set performance is not

12this reduces the number of (massively irrelevant) train-
ing samples, but we observed no adverse effects of that, while
it speeds up training greatly and models well a typical in-
formation retrieval scenario where fast pre-scoring of candi-
dates is essential.

13con   rmed by personal communication with paper au-

thors.

a great approximator for test set performance and a
strategy like picking the training run with best val-
idation performance would lead just to over   tting
on the validation set.

to allow comparison between models (and with
future models), we therefore report also 95% con-
   dence intervals for each model performance es-
timate, as determined from the empirical standard
deviation using student   s t-distribution.14

4.4 results

in fig. 2 to 4, we show the cross-task performance
of our models. we can observe an effect analo-
gous to what has been described in (kadlec et al.,
2015)     when the dataset is smaller, id98 models
are preferrable, while larger dataset allows id56
models to capture the text comprehension task bet-
ter. ir baselines provide strong competition and
   nding new ways to ensemble them with models
should prove bene   cial in the future.15 this is
especially apparent in the new answer sentence
selection datasets that have very large number of
sentence candidates per question. the attention
mechanism also has the highest impact in this kind
of information retrieval task.

on the ubuntu dialog dataset, even the sim-
ple lstm model in our proposed setting beats the
baseline performance reported by lowe, while our
id56-id98 model establishes the new state-of-art,
beating the three-hop memory network of (dodge
et al., 2015). it is not possible to statistically de-
termine the relation of our models to state-of-art
on the wang answer sentence selection dataset.
our models clearly yet lag behind the state-of-art
on the rte and sts tasks, where we did not care-
fully tune their parameters, but also did not em-
ploy data augmentation strategies like synonyme
substitution in (mueller and thyagarajan, 2016),
which might be necessary for good performance
on small datasets even when using transfer learn-
ing.

5 model reusability

to con   rm the hypothesis that our models learn a
generic task akin to some form of text comprehen-

14over larger number of samples, this estimate converges
to the normal distribution con   dence levels. note that the
con   dence interval determines the range of the true expected
evaluation, not evaluation of any measured sample.

15we have tried simple averaging of predictions (as per
(kadlec et al., 2015)), but the bene   t was small and incon-
sistent.

sion, we trained a model on the large ubuntu di-
alogue dataset (next utterance ranking task) and
transferred the weights and retrained the model in-
stance on other tasks. we used the id56 model for
the experiment in a con   guration with dot-product
scorer and smaller dimensionality (which works
much better on the ubuntu dataset). this con   g-
uration is shown in the respective result tables as
ubu. id56 and it consistently ranks as the best
or among the best classi   ers, dramatically outper-
foring the baseline id56 model.16

during our experiments, we have noticed that
it is important not to apply dropout during re-
training if it wasn   t applied during the source
model training, to balance the dataset labels, and
we used the rmsprop training procedure since
adam   s learning rate annealing schedule might
not be appropriate for weight re-training. we have
also tried freezing the weights of some layers, but
this never yielded a signi   cant improvement.

(bowman et al., 2015) have shown that such
a model transfer is bene   cial by reusing an rte
model trained on the snli dataset to the sick-
2014 dataset. we have tried the same, shown as
snli id56, and while we see an improvement
when reusing it on an rte task, on other tasks it is
the same or worse than the ubuntu dialogue based
transfer, possibly because the ubu. task sees more
versatile and less clean data.

6 conclusion

we have uni   ed a variety of tasks in a single sci-
enti   c framework of sentence pair scoring, and
demonstrated a platform for general modelling of
this problem and aggregate benchmarking of these
models across many datasets. promising initial
id21 results suggest that a quest for
generic neural model capable of task-independent
text comprehension is becoming a meaningful pur-
suit. the open source nature of our framework
and the implementation choice of a popular and
extensible deep learning library allows for high
reusability of our research and easy extensions
with further more advanced models.

based on our benchmarks, as a primary model
for applications on new f2-type tasks, we can rec-
ommend either the id56-id98 model or transfer
learning based on the ubu. id56 model.

16the id56 con   guration used for the transfer, when
trained only on the target task, is not shown in the tables but
has always been worse than the baseline id56 con   guration.

model

wang map wang mrr l2470 map l2470 mrr wqm map wqm mrr

0.728
0.753
0.578
0.630
0.649
  0.011

0.713
  0.003

0.709
  0.004

0.696
  0.006

0.717
  0.005

0.729

  0.006

0.732

  0.006

0.756
0.731

  0.007

0.832
0.851
0.709
0.765
0.743
  0.010

0.806
  0.005

0.787
  0.007

0.785
  0.007

0.793
  0.005

0.810
  0.009

0.817
  0.012

0.859
0.814
  0.008

(tan et al., 2015)

(dos santos et al., 2016)

tf-idf
bm25

id56 w/o bm25

avg

dan

id56

id98

id56-id98

attn1511

   attn1511

ubu. id56 w/o bm25

ubu. id56

snli id56

0.267
0.314
0.262
  0.003

0.278
  0.003

0.282
  0.004

0.277
  0.004

0.288
  0.003

0.288
  0.004

0.286
  0.003

0.359

  0.003

0.291
  0.002

0.264
  0.005

0.363
0.491
0.381
  0.008

0.481
  0.008

0.490
  0.010

0.487
  0.008

0.499
  0.007

0.503
  0.010

0.499
  0.009

0.539

  0.006

0.515
  0.004

0.460
  0.015

0.216

0.194

0.462
  0.013

0.457
  0.007

0.653
  0.068

0.664
  0.021

0.517
  0.052

0.701

  0.008

0.506
  0.015

0.503
  0.008

0.682
  0.065

0.694
  0.019

0.556
  0.048

0.729

  0.005

figure 2: model results on the answer sentence selection task, as measured on the wang, yodaqa/large2470 and
wqmprop datasets. wqmprop does not use the bm25 ensembling, and id98 is not siamese.
    demonstration of the problematic single-measurement result reporting in past literature     an outlier sample in our 16-trial
attn1511 benchmark that would score as a state of art; in total, three outliers in the trial (12.5%) scored better than (tan et al.,
2015).

6.1 future work

due to the very wide scope of the f2-problem
scope, we leave some popular tasks and datasets
as future work. a popular instance of sen-
tence pair scoring is the id53 task
of the memory networks (supported by the
babi dataset) (weston et al., 2015). a realis-
tic large question id141 dataset based on
the askubuntu stack over   ow forum had been
recently proposed (lei et al., 2015).17
in a
multi-lingual context, sentence-level mt quality
estimation is a meta-task with several available
datasets.18 while the tasks of semantic textual
similarity (supported by a dataset from the sts
track of the semeval conferences (agirre et al.,

17the task resembles id141, but is evaluated as an
information retrieval task much closer to answer sentence
selection.

18http://www.statmt.org/wmt15/

quality-estimation-task.html

2015)) and id141 (based on the microsoft
research paraphrase corpus (dolan and brockett,
2005) right now) are available within our frame-
work, we do not report the results here as the
models lag behind the state-of-art signi   cantly and
show little difference in results. advancing the
models to be competitive remains future work. a
generalization of our proposed architecture could
be applied to the hypothesis evidencing task of
binary classi   cation of a hypothesis sentence s0
based on a number of memory sentences s1, for
example within the mctext (richardson et al.,
2013) dataset. we also did not include several
major classes of models in our initial evaluation.
most notably, this includes serial id56s with at-
tention as used e.g. for the rte task (rockt  aschel
et al., 2015), and the skip-thoughts method of sen-
tence embedding. (kiros et al., 2015)

we believe that the ubuntu dialogue dataset
results demonstrate that the time is ripe to push

model

    tf-idf
    id56
    lstm

    memn2n 3-hop

avg

dan

id56

id98

id56-id98

attn1511

mrr

1-2 r@1

1-10 r@1

1-10 r@2

1-10 r@5

0.749
0.777
0.869

0.624

0.793

0.488
0.379
0.552
0.637
0.472

  0.002

  0.002

  0.002

0.578

0.792

  0.070

  0.035

0.781

0.907

  0.003

  0.002

0.718

0.863

  0.003

  0.002

0.788

0.911

  0.001

  0.001

0.772

0.903

  0.004

  0.002

0.493

  0.074

0.664

  0.004

0.587

  0.004

0.672

  0.002

0.653

  0.005

0.587
0.561
0.721

0.608

  0.002

0.615

  0.059

0.799

  0.004

0.721

  0.005

0.809

  0.002

0.788

  0.005

0.763
0.836
0.924

0.836

  0.003

0.830

  0.033

0.951

  0.001

0.907

  0.003

0.956

  0.001

0.945

  0.002

figure 3: model results on the ubuntu dialogue next utterance ranking task. models use slightly speci   c con   guration due
to much bigger dataset (in terms of both samples and sentence lengths)     only 160 tokens are considered per input, no dropout
is applied, id56 use n memory units, projection matrix is only n    n and the dot-product scorer is used for comparison. the
attn1511 model furthermore has only n/2 id56 memory units and n/2 id98    lters.
    exact models from (lowe et al., 2015) reran on the v2 version of the dataset (by personal communication with ryan lowe)
    note that the results in (lowe et al., 2015) and (kadlec et al., 2015) are on dataset v1 and not directly comparable.

model

(mueller and thyagarajan, 2016)

(lai and hockenmaier, 2014)
(bowman et al., 2015) lstm
(bowman et al., 2015) tran.

(cheng et al., 2016)

tf-idf
bm25

avg

dan

id56

id98

id56-id98

attn1511

ubu. id56

snli id56

sick-2014 sick-2014 sick-2014

sts r
test
0.882

0.479
0.474
0.621
  0.017

0.642
  0.016

0.664
  0.022

0.762
  0.006

0.790

  0.005

0.723
  0.009

0.799

  0.009

0.798

  0.007

3-rte
train

0.842
1.000
0.999

0.770
  0.020

0.715
  0.010

0.759
  0.016

0.927
  0.008

0.765
  0.084

0.858
  0.010

0.931
  0.017

0.927
  0.006

3-rte

test
0.842
0.845
0.713
0.808

0.652
  0.017

0.662
  0.003

0.732
  0.010

0.799
  0.004

0.709
  0.059

0.767
  0.004

0.813
  0.005

0.831

  0.002

snli
3-rte
train

snli
3-rte

test

0.848

0.776

0.921

0.890

0.735
  0.014

0.718
  0.009

0.784
  0.019

0.710
  0.008

0.708
  0.002

0.749
  0.010

0.811
  0.037

0.829
  0.014

0.753
  0.008

0.774

  0.004

figure 4: model results on the sts and rte tasks, reporting pearson   s r and 3-class accuracy, respectively. the snli tran.
baseline transfers snli-learned weights to the sick-2014 task.

the research models further towards the real-world
by allowing for wider sentence variability and less
explicit supervision. but in particular, we believe
that new models should be developed and tested
on tasks with long sentences and wide vocabulary.
in terms of models, recent work in many nlp
domains (dos santos et al., 2016) (cheng et al.,
2016) (kumar et al., 2015) clearly points towards
various forms of attention modelling to remove the
bottleneck of having to compress the full spectrum
of semantics into a single vector of    xed dimen-
sionality. in this paper, we have shown the bene   t
of training a model on a single dataset and then
applying it on another dataset. one open ques-
tion is whether we could jointly train a model on
multiple tasks simultaneously (even if they do not
share some output layers). another option would
be to include extra supervision similar to the token
overlap features that we already employ; for ex-
ample, in the new answer sentence selection task
datasets, we can explicitly mark the actual tokens
representing the answer.

acknowledgments
this work was    nancially supported by the grant agency
of the czech technical university in prague, grant no.
sgs16/ 084/ohk3/1t/13, and the augur project of the fore-
cast foundation. computational resources were provided by
the cesnet lm2015042 and the cerit scienti   c cloud
lm2015085, provided under the programme    projects of
large research, development, and innovations infrastruc-
tures.   

we   d like to thank tom  a  s tunys, rudolf kadlec, ryan
lowe, cicero nogueira dos santos and bowen zhou for help-
ful discussions and their insights, and silvestr stanko and ji  r    
n  advorn    k for their software contributions.

references
eneko agirre, carmen banea, claire cardie, daniel
cer, mona diab, aitor gonzalez-agirre, wei-
wei guof, inigo lopez-gazpio, montse maritxalar,
rada mihalcea, et al. 2015. semeval-2015 task 2:
semantic textual similarity, english, spanish and pi-
lot on interpretability.

petr baudi  s and jan   sediv  y. 2015. modeling of the
id53 task in the yodaqa system. in
experimental ir meets multilinguality, multimodal-
ity, and interaction, pages 222   228. springer.

petr baudi  s. 2015. yodaqa: a modular question an-
swering system pipeline. in poster 2015 - 19th
international student conference on electrical en-
gineering.

jonathan berant, andrew chou, roy frostig, and percy
liang. 2013. id29 on freebase from

question-answer pairs.
1544.

in emnlp, pages 1533   

samuel r. bowman, gabor angeli, christopher potts,
and christopher d. manning. 2015. a large an-
notated corpus for learning natural language infer-
ence.
in proceedings of the 2015 conference on
empirical methods in natural language processing
(emnlp). association for computational linguis-
tics.

chris burges, tal shaked, erin renshaw, ari lazier,
matt deeds, nicole hamilton, and greg hullender.
2005. learning to rank using id119. in
proceedings of the 22nd international conference on
machine learning, pages 89   96. acm.

jianpeng cheng, li dong, and mirella lapata. 2016.
long short-term memory-networks for machine
reading. corr, abs/1601.06733.

kyunghyun cho, bart van merrienboer, dzmitry bah-
danau, and yoshua bengio. 2014. on the properties
of id4: encoder-decoder ap-
proaches. corr, abs/1409.1259.

franc  ois chollet. 2015. keras. https://github.

com/fchollet/keras.

ido dagan, oren glickman, and bernardo magnini.
2006. the pascal recognising id123
challenge. in machine learning challenges. evalu-
ating predictive uncertainty, visual object classi   ca-
tion, and recognising tectual entailment, pages 177   
190. springer.

jesse dodge, andreea gane, xiang zhang, antoine
bordes, sumit chopra, alexander miller, arthur
szlam, and jason weston. 2015. evaluating prereq-
uisite qualities for learning end-to-end dialog sys-
tems. corr, abs/1511.06931.

william b dolan and chris brockett. 2005. automati-
cally constructing a corpus of sentential paraphrases.

c    cero nogueira dos santos, ming tan, bing xiang,
and bowen zhou. 2016. attentive pooling net-
works. corr, abs/1602.03609.

hua he, kevin gimpel, and jimmy lin. 2015. multi-
perspective sentence similarity modeling with con-
volutional neural networks.

karl moritz hermann, tomas kocisky, edward
grefenstette, lasse espeholt, will kay, mustafa su-
leyman, and phil blunsom. 2015. teaching ma-
chines to read and comprehend. in advances in neu-
ral information processing systems, pages 1684   
1692.

mohit iyyer, varun manjunatha, jordan boyd-graber,
and hal daum  e iii. 2015. deep unordered compo-
sition rivals syntactic methods for text classi   cation.

rudolf kadlec, martin schmid, and jan kleindienst.
2015. improved deep learning baselines for ubuntu
corpus dialogs. arxiv preprint arxiv:1510.03753.

aliaksei severyn and alessandro moschitti.

2015.
learning to rank short text pairs with convolutional
deep neural networks.
in proceedings of the 38th
international acm sigir conference on research
and development in information retrieval, pages
373   382. acm.

kai sheng tai, richard socher, and christopher d.
manning. 2015.
improved semantic representa-
tions from tree-structured long short-term memory
networks. corr, abs/1503.00075.

ming tan, bing xiang, and bowen zhou. 2015. lstm-
based deep learning models for non-factoid answer
selection. corr, abs/1511.04108.

mengqiu wang, noah a smith, and teruko mita-
mura. 2007. what is the jeopardy model? a quasi-
synchronous grammar for qa.
in emnlp-conll,
volume 7, pages 22   32.

jason weston, sumit chopra, and antoine bordes.

2014. memory networks. corr, abs/1410.3916.

jason weston, antoine bordes, sumit chopra, and
tomas mikolov. 2015. towards ai-complete ques-
tion answering: a set of prerequisite toy tasks.
corr, abs/1502.05698.

kun xu, yansong feng, siva reddy, songfang huang,
and dongyan zhao.
2016. enhancing freebase
id53 using textual evidence. corr,
abs/1603.00957.

yi yang, wen-tau yih, and christopher meek. 2015.
wikiqa: a challenge dataset for open-domain ques-
tion answering.

wen-tau yih, ming-wei chang, xiaodong he, and
jianfeng gao. 2015. id29 via staged
query graph generation: id53 with
knowledge base. july.

lei yu, karl moritz hermann, phil blunsom, and
stephen pulman. 2014. deep learning for answer
sentence selection. corr, abs/1412.1632.

yoon kim.

2014.

works for sentence classi   cation.
arxiv:1408.5882.

convolutional neural net-
arxiv preprint

diederik kingma and jimmy ba. 2014. adam: a
method for stochastic optimization. arxiv preprint
arxiv:1412.6980.

ryan kiros, yukun zhu, ruslan r salakhutdinov,
richard zemel, raquel urtasun, antonio torralba,
and sanja fidler.
skip-thought vectors.
in advances in neural information processing sys-
tems, pages 3276   3284.

2015.

ankit kumar, ozan irsoy, jonathan su, james brad-
bury, robert english, brian pierce, peter ondruska,
ishaan gulrajani, and richard socher. 2015. ask
me anything: dynamic memory networks for natu-
ral language processing. corr, abs/1506.07285.

alice lai and julia hockenmaier. 2014. illinois-lh: a
denotational and distributional approach to seman-
tics. proc. semeval.

tao lei, hrishikesh joshi, regina barzilay, tommi s.
jaakkola, kateryna tymoshenko, alessandro mos-
chitti, and llu    s m`arquez i villodre. 2015. de-
noising bodies to titles: retrieving similar ques-
tions with recurrent convolutional models. corr,
abs/1512.05726.

ryan lowe, nissan pow, iulian serban, and joelle
pineau. 2015. the ubuntu dialogue corpus: a large
dataset for research in unstructured multi-turn dia-
logue systems. corr, abs/1506.08909.

marco marelli, luisa bentivogli, marco baroni, raf-
faella bernardi, stefano menini, and roberto zam-
parelli. 2014. semeval-2014 task 1: evaluation of
compositional distributional semantic models on full
sentences through semantic relatedness and textual
entailment. semeval-2014.

jonas mueller and aditya thyagarajan. 2016. siamese
recurrent architectures for learning sentence similar-
ity. in thirtieth aaai conference on arti   cial intel-
ligence.

jeffrey pennington, richard socher, and christo-
pher d. manning. 2014. glove: global vectors for
word representation. in empirical methods in nat-
ural language processing (emnlp), pages 1532   
1543.

matthew richardson, christopher jc burges, and erin
renshaw. 2013. mctest: a challenge dataset for the
open-domain machine comprehension of text.

stephen e robertson, steve walker, susan jones, et al.

1995. okapi at trec-3.

tim rockt  aschel, edward grefenstette, karl moritz
hermann, tom  as kocisk  y, and phil blunsom. 2015.
reasoning about entailment with neural attention.
corr, abs/1509.06664.

