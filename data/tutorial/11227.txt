multi30k: multilingual english-german image descriptions

desmond elliott and stella frank and khalil sima   an

illc, university of amsterdam

{d.elliott, s.c.frank, k.simaan}@uva.nl

lucia specia

shef   eld university

l.specia@sheffield.ac.uk

6
1
0
2

 

y
a
m
2

 

 
 
]
l
c
.
s
c
[
 
 

1
v
9
5
4
0
0

.

5
0
6
1
:
v
i
x
r
a

abstract

we introduce the multi30k dataset
to
stimulate multilingual multimodal
re-
search. recent advances in image descrip-
tion have been demonstrated on english-
language datasets almost exclusively, but
image description should not be limited
to english.
this dataset extends the
flickr30k dataset with i) german trans-
lations created by professional translators
over a subset of the english descriptions,
and ii) descriptions crowdsourced inde-
pendently of the original english descrip-
tions. we outline how the data can be
used for multilingual
image description
and multimodal machine translation, but
we anticipate the data will be useful for a
broader range of tasks.

1 introduction
image description is one of
the core chal-
lenges at the intersection of natural language
processing (nlp) and id161 (cv)
(bernardi et al., 2016). this task has only re-
ceived attention in a monolingual english set-
ting, helped by the availability of english
datasets, e.g.
flickr8k (hodosh et al., 2013),
flickr30k (young et al., 2014), and ms coco
(chen et al., 2015). however, the possible appli-
cations of image description are useful for all lan-
guages, such as searching for images using natural
language, or providing alternative-description text
for visually impaired web users.

we introduce a large-scale dataset of images
paired with sentences in english and german as
an initial step towards studying the value and the
characteristics of multilingual-multimodal data.
multi30k is an extension of the flickr30k dataset
(young et al., 2014) with 31,014 german transla-

tions of english descriptions and 155,070 indepen-
dently collected german descriptions. the trans-
lations were collected from professionally con-
tracted translators, whereas the descriptions were
collected from untrained crowdworkers. the key
difference between these corpora is the relation-
ship between the sentences in different languages.
in the translated corpus, we know there is a strong
correspondence between the sentences in both lan-
guages. in the descriptions corpus, we only know
that the sentences, regardless of the language, are
supposed to describe the same image.

a dataset of images paired with sentences
in multiple languages broadens the scope of
multimodal nlp research. image description with
multilingual data can also be seen as machine
translation in a multimodal context. this opens
up new avenues for
researchers in machine
translation (koehn et al., 2003; chiang, 2005;
sutskever et al., 2014;
bahdanau et al., 2015,
inter-alia) to work with multilingual multimodal
data.
image   sentence ranking using monolin-
gual multimodal datasets (hodosh et al., 2013,
inter-alia) is also a natural task for multilingual
modelling.

of

by

the

only

existing

datasets

described

professionally

language:
english-german

images
paired with multilingual sentences were cre-
translating english
ated
iapr-tc12 with
into the target
20,000
images
(grubinger et al., 2006), and the pascal sentences
dataset of 1,000 japanese-english described
images (funaki and nakayama, 2015). multi30k
dataset is larger than both of these and contains
both independent and translated sentences. we
hope this dataset will be of broad interest across
nlp and cv research and anticipate that these
communities will put the data to use in a broader
range of tasks than we can foresee.

1. brick layers constructing a wall.

2. maurer bauen eine wand.

1. trendy girl talking on her cellphone
while gliding slowly down the street

2. ein schickes m  adchen spricht mit
dem handy w  ahrend sie langsam die
stra  e entlangschwebt.

1. the two men on the scaffolding are

helping to build a red brick wall.

2. zwei mauerer mauern ein haus

zusammen.

1. there is a young girl on her cell-

phone while skating.

2. eine frau im blauen shirt telefoniert

beim rollschuhfahren.

(a) translations

(b) independent descriptions

figure 1: multilingual examples in the multi30k dataset. the independent sentences are all accurate
descriptions of the image but do not contain the same details in both languages, such as shirt colour
or the scaffolding. in the second translation pair (bottom left) the translator has translated    glide    as
   schweben    (   to    oat   ) probably due to not seeing the image context (see section 2.1 for more details).

2 the multi30k dataset
the flickr30k dataset contains 31,014 im-
ages sourced from online photo-sharing websites
(young et al., 2014). each image is paired with
   ve english descriptions, which were collected
from amazon mechanical turk1. the dataset con-
tains 145,000 training, 5,070 development, and
5,000 test descriptions. the multi30k dataset ex-
tends the flickr30k dataset with translated and in-
dependent german sentences.
2.1 translations
the translations were collected from professional
english-german translators contracted via an es-
tablished language service in germany. fig-
ure 1 presents an example of the differences be-
tween the types of data. we collected one trans-
lated description per image, resulting in a total of
31,014 translations. to ensure an even distribu-
tion over description length, the english descrip-
tions were chosen based on their relative length,
with an equal number of longest, shortest, and me-
dian length source descriptions. we paid a total
of e23,000 to collect the data (e0.06 per word).
translators were shown an english language sen-
tences and asked to produce a correct and    uent
translation for it in german, without seeing the
image. we decided against showing the images
to translators to make this as close as possible to a
standard translation task, also making the data col-

1http://www.mturk.com

lected here distinct from the independent descrip-
tions collected as described in section 2.2.
2.2 independent descriptions
the descriptions were collected from crowdwork-
ers via the crowd   ower platform2. we col-
lected    ve descriptions per image in the flickr30k
dataset, resulting in a total of 155,070 sentences.
workers were presented with a translated ver-
sion of the data collection interface used by
(hodosh et al., 2013), as shown in figure 2. we
translated the interface to make the task as simi-
lar as possible to the id104 of the english
sentences. the instructions were translated by one
of the authors and checked by a native german
ph.d student.

185 crowdworkers took part in the task over a
period of 31 days. we split the task into 1,000
randomly selected images per day to control the
quality of the data and to prevent worker fatigue.
workers were required to have a german-language
skill certi   cation and be at least a crowd   ower
level 2 worker: they have participated in at least
10 different crowd   ower jobs, has passed at least
100 quality-control questions, and has an job ac-
ceptance rate of at least 85%.

the descriptions were collected in batches of
   ve images per job. each image was randomly
selected from the complete set of 1,000 images for
that day, and workers were limited to writing at

2http://www.crowdflower.com

figure 2: the german instructions shown to crowdworkers were translated from the original instructions.

most 250 descriptions per day. we paid workers
$0.05 per description3 and limited them submit-
ting faster than 90s per job to discourage poor/low-
quality work. (this works out at a rate of 40 jobs
per hour, i.e. 200 descriptions / hour.) we con   g-
ured crowd   ower to automatically ban users who
worked faster than this rate. thus the theoretical
maximum wage per hour was $10/hour. we paid a
total of $9,591.24 towards collecting the data and
paying the crowd   ower platform fees.

during the collection of the data, we assessed
the quality both by manually checking a subset of
the descriptions and also with automated checks.
we inspected the submissions of users who wrote
sentences with less than    ve words, and users with
high type to token ratios (to detect repetition). we
also used a character-level 6-gram lm to    ag de-
scriptions with high perplexity, which was very ef-
fective at catching nonsense sentences. in general
we did not have to ban or reject many users and
overall description quality was high.
2.3 translated vs. independent descriptions
we now analyse the differences between the trans-
lated and the description corpora. for this anal-
ysis, all sentences were stripped of punctuation
and truecased using the moses truecaser.pl
script trained over europarl v7 and news com-
mentary v11 english-german parallel corpora.

table 1 shows the differences between the cor-
pora. the german translations are longer than
the independent descriptions (11.1 vs. 9.6 words),
while the english descriptions selected for trans-

3this is the same rate as rashtchian et al. (2010) and

elliott and keller (2013) paid to collect english sentences.

lation are slightly shorter, on average, than the
flickr30k average (11.9 vs. 12.3). when we com-
pare the german translation dataset against an
equal number of sentences from the german de-
scriptions dataset, we    nd that the translations
also have more word types (19.3k vs. 17.6k), and
more singleton types occurring only once (11.3k
vs. 10.2k; in both datasets singletons comprise
58% of the vocabulary). the translations thus have
a wider vocabulary, despite being generated by a
smaller number of authors. the english datasets
(all descriptions vs. those selected for translation)
show a similar trend, indicating that these differ-
ences may be a result of the decision to select
equal numbers of short, medium, and long english
sentences for translation.
2.4 english vs. german
the english image descriptions are generally
longer than the german descriptions, both in terms
of number of words and characters. note that
the difference is much less smaller when measur-
ing characters: german uses 22% fewer words
but only 2.5% fewer characters. however, we
observe a different pattern in the translation cor-
pora: german uses 6.6% fewer words than en-
glish but 17.1% more characters. the vocabulary
of the german description and translation corpora
are more than twice as large as the english cor-
pora. additionally, the german corpora have two-
to-three times as many singletons. this is likely
due to richer morphological variation in german,
as well as word compounding.

tokens

types characters avg. length

singletons

translations (31,014 sentences)

english
german

357,172
333,833

11,420
19,397

1,472,251
1,774,234

descriptions (155,070 sentences)

english
german

1,841,159
1,434,998

22,815
46,138

7,611,033
7,418,572

11.9
11.1

12.3
9.6

5,073
11,285

9,230
26,510

table 1: corpus-level statistics about the translation and the description data.

inter-alia).

3 discussion
the multi30k dataset
is immediately suitable
for research on a wide range of tasks, including
but not limited to automatic image description,
image   sentence ranking, multimodal and multi-
lingual semantics, and machine translation.
3.1 multi30k for image description
deep neural networks for image description typi-
cally integrate visual features into a recurrent neu-
ral network language model (vinyals et al., 2015;
xu et al., 2015,
elliott et al. (2015)
demonstrated how to build multilingual image de-
scription models that learn and transfer features
between monolingual
image description mod-
els. they performed a series of experiments on
the iapr-tc12 dataset (grubinger et al., 2006) of
images aligned with german translations, show-
ing that both english and german image descrip-
tion could be improved by transferring features
from a multimodal neural language model trained
to generate descriptions in the other language. the
multi30k dataset will enable further research in
this direction, allowing researchers to work with
larger datasets with multiple references per image.
3.2 multi30k for machine translation
machine translation is typically performed using
only textual data, for example news data, the eu-
roparl corpora, or corpora harvested from the web
(commoncrawl, wikipedia, etc.). the multi30k
dataset makes it possible to further develop ma-
chine translation in a setting where multimodal
data, such as images or video, are observed along-
side text. the potential advantages of using multi-
modal information for machine translation include
the ability to better deal with ambiguous source
text and to avoid (untranslated) out-of-vocabulary
words in the target language (calixto et al., 2012).

hitschler and riezler (2016) have demonstrated
the potential of multimodal features in a target-
side translation reranking model. their approach
is initially trained over large text-only translation
copora and then    ne-tuned with a small amount
of in-domain data, such as our dataset. we expect
a variety of translation models can be adapted to
take advantage of multimodal data as features in
a log-linear model or as feature vectors in neural
machine translation models.
4 conclusions
we introduced multi30k: a large-scale multilin-
gual multimodal dataset for interdisciplinary ma-
chine learning research. our dataset is an exten-
sion of the popular flickr30k dataset with descrip-
tions and professional translations in german.

the descriptions were collected from a crowd-
sourcing platform, while the translations were col-
lected from professionally contracted translators.
these differences are deliberate and part of the
larger scope of studying multilingual multimodal
data in different contexts. the descriptions were
collected as similarly as possible to the original
flickr30k dataset by translating the instructions
used by young et al. (2014) into german. the
translations were collected without showing the
images to the translators to keep it as close to a
standard translation task as possible.

there are substantial differences between the
translated and the description datasets. the trans-
lations contain approximately the same number of
tokens and have sentences of approximately the
same length in both languages. these properties
make them suited to machine translations mod-
els. the description datasets are very different in
terms of average sentence lengths and the number
of word types per language. this is likely to cause
different engineering and scienti   c challenges be-

cause the descriptions are independently collected
corpora instead of a sentence-level aligned corpus.
in the future, we want to study multilingual
multimodality over a wider range of languages, for
example beyond indo-european families. we call
on the community to engage with us on creating
massively multilingual multimodal datasets.
acknowledgements
de and ks were supported by the nwo vici grant
nr. 277-89-002. sf was supported by european
union   s horizon 2020 research and innovation
programme under grant agreement nr. 645452. we
are grateful to philip schulz for checking the ger-
man translation of the worker instructions, and to
joachim daiber for providing the pre-trained true-
casing models.

references
[bahdanau et al.2015] dzmitry bahdanau, kyunghyun
cho, and yoshua bengio. 2015. neural machine
translation by jointly learning to align and translate.
in proceedings of iclr.

[bernardi et al.2016] raffaella bernardi, ruken cakici,
desmond elliott, aykut erdem, erkut erdem, na-
zli ikizler-cinbis, frank keller, adrian muscat, and
barbara plank. 2016. automatic description gen-
eration from images: a survey of models, datasets,
and evaluation measures. journal of arti   cial intel-
ligence research, 55:409   442.

[calixto et al.2012] iacer calixto, teo e. de campos,
and lucia specia. 2012. images as context in statis-
tical machine translation. in second annual meet-
ing of the epsrc network on vision & language,
shef   eld.

[chen et al.2015] xinlei chen, hao fang, tsung-yi
lin, ramakrishna vedantam, saurabh gupta, pi-
otr doll  ar, and c. lawrence zitnick. 2015. mi-
crosoft coco captions: data collection and eval-
uation server. corr, abs/1504.00325.

[chiang2005] david chiang.

2005. a hierarchical
phrase-based model for statistical machine transla-
tion.
in proceedings of the 43rd annual meeting
on association for computational linguistics, pages
263   270. association for computational linguis-
tics.

[elliott and keller2013] desmond elliott and frank
keller. 2013. image description using visual de-
pendency representations. in emnlp, pages 1292   
1302.

[elliott et al.2015] desmond elliott, stella frank, and
2015. multi-language image de-
corr,

eva hasler.
scription with neural sequence models.
abs/1510.04709.

[funaki and nakayama2015] ruka funaki and hideki
nakayama.
image-mediated learning for
zero-shot cross-lingual document retrieval. in em-
pirical methods in natural language processing
2015, pages 585   590.

2015.

[grubinger et al.2006] michael grubinger, paul d.
clough, henning muller, and thomas desealers.
2006. the iapr tc-12 benchmark: a new eval-
uation resource for visual information systems. in
lrec.

[hitschler and riezler2016] julian hitschler and stefan
riezler. 2016. multimodal pivots for image caption
translation. corr, abs/1601.03916.

[hodosh et al.2013] micah hodosh, peter young, and
julia hockenmaier. 2013. framing image descrip-
tion as a ranking task: data, models and evalua-
tion metrics. journal of arti   cial intelligence re-
search, 47:853   899.

[koehn et al.2003] philipp koehn, franz josef och,
and daniel marcu. 2003. statistical phrase-based
translation. in proceedings of the 2003 conference
of the north american chapter of the association
for computational linguistics on human language
technology - volume 1, naacl    03, pages 48   54,
stroudsburg, pa, usa. association for computa-
tional linguistics.

[rashtchian et al.2010] cyrus rashtchian, peter young,
micha hodosh, and julia hockenmaier. 2010. col-
lecting image annotations using amazon   s mechan-
ical turk.
in naaclhlt workshop on creating
speech and language data with amazon   s mechan-
ical turk.

[sutskever et al.2014] ilya sutskever, oriol vinyals,
and quoc v. v le. 2014. sequence to sequence
learning with neural networks.
in z. ghahramani,
m. welling, c. cortes, n.d. lawrence, and k.q.
weinberger, editors, advances in neural informa-
tion processing systems 27, pages 3104   3112. cur-
ran associates, inc.

[vinyals et al.2015] oriol vinyals, alexander toshev,
samy bengio, and dumitru erhan. 2015. show and
tell: a neural image caption generator. in the ieee
conference on id161 and pattern recog-
nition (cvpr), june.

[xu et al.2015] kelvin xu, jimmy ba, ryan kiros,
kyunghyun cho, aaron c. courville, ruslan
salakhutdinov, richard s. zemel, and yoshua ben-
gio.
2015. show, attend and tell: neural im-
age id134 with visual attention. corr,
abs/1502.03044.

[young et al.2014] peter young, alice lai, micha ho-
dosh, and julia hockenmaier. 2014. from image
descriptions to visual denotations: new similarity
metrics for semantic id136 over event descrip-
tions. transactions of the association for computa-
tional linguistics.

