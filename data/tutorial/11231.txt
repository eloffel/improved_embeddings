multi-field structural decomposition for id53

tomasz jurczyk

jinho d. choi

mathematics and computer science

mathematics and computer science

emory university

atlanta, ga 30322, usa

emory university

atlanta, ga 30322, usa

6
1
0
2

 
r
p
a
4

 

 
 
]
l
c
.
s
c
[
 
 

1
v
8
3
9
0
0

.

4
0
6
1
:
v
i
x
r
a

tomasz.jurczyk@emory.edu

jinho.choi@emory.edu

abstract

this paper presents a precursory yet novel
approach to the id53 task us-
ing structural decomposition. our system
   rst generates linguistic structures such as
syntactic and semantic trees from text, de-
composes them into multiple    elds, then in-
dexes the terms in each    eld. for each ques-
tion, it decomposes the question into multi-
ple    elds, measures the relevance score of
each    eld to the indexed ones, then ranks
all documents by their relevance scores and
weights associated with the    elds, where
the weights are learned through statistical
modeling. our    nal model gives an abso-
lute improvement of over 40% to the base-
line approach using simple search for de-
tecting documents containing answers.

1

introduction

towards machine reading, id53 has
recently gained lots of interest among researchers
from both natural language processing (moschitti
and quarteroni, 2011; yih et al., 2013; hixon et al.,
2015) and information retrieval (schiffman et al.,
2007; kolomiyets and moens, 2011). people from
these two research    elds, nlp and ir, have shown
tremendous progress on id53, yet
only few efforts have been made to adapt technolo-
gies from both sides. the nlp side often tackles
the task by analyzing linguistic aspects, whereas
the ir side tackles it by searching likely patterns.
while these two approaches perform well indi-
vidually, more sophisticated solutions are needed to
handle a wide range of questions. by considering
linguistic structures such as syntactic and seman-
tic trees, qa systems can infer deeper meaning of
the context and handle more complex questions.
however, extracting answers from these structures
through either graph matching or predicate logic is

not necessarily scalable when the size of the con-
text is large. on the other hand, searching patterns
is scalable for large data, especially when coupled
with indexing, although it does not always concern
with the actual meaning of the context.

we present a multi-   eld weighted indexing ap-
proach for id53 that combines good
aspects of both nlp and ir. we begin by describ-
ing how linguistic structures are decomposed into
multiple    elds (section 3.3), and explain how the
decomposed    elds are used to rank documents con-
taining answers through statistical learning (sec-
tions 3.4 and 3.5). we evaluate our approach to 8
types of questions; our    nal model shows signi   -
cant improvement over the baseline model using
simple search (section 4).

2 related work

shen and lapata (2007) assessed the contribution
of semantic roles to factoid id53
and showed promising results. pizzato and moll  a
(2008) proposed a question prediction language
model providing rich information and achieved im-
proved speed and accuracy. although related, our
work is distinguished from theirs because we con-
sider multiple    elds whereas the others consider
only one    eld representing semantic roles. ferrucci
et al. (2010) presented ibm watson taking a hybrid
approach between nlp and ir, and advanced the
id53 task to another level.

fader et al. (2013) proposed a paraphrase-driven
id88 learning approach using a seed lexicon.
our learning process is similar; however, it is dis-
tinguished in a way that we learn weights for indi-
vidual    elds instead of lexicons. yih et al. (2014)
introduced a id29 framework for open
domain id53, which used convolu-
tional neural networks for measuring similarities
between decomposed entities. weston et al. (2015)
presented the memory networks models designed
to memorize information about known objects and

figure 1: the overall framework of our id53 system.

actors. our work is related to the this work; how-
ever, memory networks are designed to store and
manipulate information about speci   c types of ob-
jects while our framework is generalizable to any
type of objects induced from the context.

3 approach

3.1 overall framework
figure 1 shows the overall framework. our system
is designed in a modular architectural way, so any
further extension of    elds can be easily integrated.
the system takes input documents, generates lin-
guistic structures using nlp tools, decomposes
them into multiple    elds, and indexes those    elds.
questions are processed in the same way. to an-
swer a question, the system queries the index for
each    eld extracted from the question and measures
the relevance score. all documents are ranked with
respect to the relevance scores and their weights
associated with the    elds, and the document with
the highest score is selected as the answer.

3.2 modules
our system consists of several modules closely con-
nected together providing a fully working solution
for the id53 selection task.

3.2.1 documents and questions
documents provide the context where the questions
   nd their answers from. each document can con-
tain one or more sentences, in which answers for
coming questions are annotated for training. doc-
uments may simply be wikipedia articles, news
articles,    ctional stories, etc. questions are treated
as regular documents containing only one sentence.

3.2.2 nlp tools
for the generation of syntactic and semantic struc-
tures, we used the part-of-speech tagger (choi and
palmer, 2012), the dependency parser (choi and
mccallum, 2013), the semantic role labeler (choi
and palmer, 2011), and the coreference resolution
tool in clearnlp1. ensuring good and robust accu-
racy for these nlp tools is important because all
the following modules depend on their output.

3.2.3 field extractor
the    eld extractor takes the linguistic structures
from the nlp tools and decomposes them into
multiple    elds (section 3.3). all    elds extracted
from the documents are passed to the index engine,
whereas    elds extracted from the questions are sent
directly to the answer ranker module.

index engine

3.2.4
the index engine is a search server that receives
a list of    elds decomposed by the    eld extractor,
indexes terms in the    elds, and responses to the
queries generated from questions with their rele-
vance scores. we used elastic search2, as it pro-
vides a distributed, multi-tenancy-capable search.

3.2.5 answer ranker
the answer ranker takes the decomposed    elds ex-
tracted from a question, converts them into queries,
and builds a matrix of documents with their rel-
evance scores across all    elds through the index
engine (section 3.4). it also uses different weights
for individual    elds trained by statistical modeling
(section 3.5).

1http://www.clearnlp.com
2https://www.elastic.co

contextfield extractornlp toolsdocumentsdiskindex enginequeryingqquestionsanswer rankerquestionsaanswersquestionscontextcontext3.3 structural decomposition

and individual weights for all    elds are summed,
and the document   d with the highest score f is
taken as the answer. note that in our dataset, each
document contains only one sentence so that re-
trieving a document is equivalent to retrieving a
sentence. the following equations describe how
the document   d is selected by measuring the overall
score f (q, dt) using the relevance scores r(qi, dt
i)
and the weights   i.

f (q, dt)

  d = arg max
dt   d
  i    r(qi, dt
i)

n(cid:88)
(cid:88)

i=1

f (q, dt) =

r(qi, dt

i) =

v   qi   dt

i

i(v)    idf i(v)    normt
tf t

i(v)

figure 2: the    ow of the sentence, julie is either
in the school or the cinema, through our system.

each sentence is represented by the index engine
as a document with multiple    elds grouped into
categories. figure 2 shows an example of how the
sentence is decomposed into multiple    elds con-
sisting of syntactic and semantic structures. due
to the extensible nature of our    eld extractor, ad-
ditional groups and    elds can be easily integrated.
currently, our system supports 24    elds grouped
into the following three categories:

    lexical    elds (e.g., word-forms, lemmas).
    syntactic    elds (e.g., dependency labels).
    semantic    elds (e.g., semantic roles, distances

between predicates).

3.4 answer ranking
when a question q is asked, it is decomposed into
the n-number of    elds. each    eld is transformed
into a query where certain words are replaced with
wildcards (e.g., {where a1, is pred, she a2}    
{* a1 is pred she a2}). then, the relevance score
r is measured between each    eld in the question
and the same    eld in each document dt     d by the
index engine.3 the product of the relevance scores

3we set the elastic search results limit to 20.

3.5 training weights for individual    elds
algorithm 1 shows how the weights for all    elds
are learned during training. we adapt the averaged
id88 algorithm, which has been widely used
for many nlp tasks. all the weights (cid:126)   are initial-
ized to 1. for each question q     q, it predicts the
document   d that most likely contains the answer. if
  d is incorrect, then it compares the relevance score
r between (q,   d) and (q, d) for each    eld, and up-
dates the weight accordingly, where d is the true
document from the oracle. this procedure is re-
peated multiple times through iterations. finally,
the algorithm returns the averaged weights, where
each dimension represents the weight for each    eld.

algorithm 1 averaged id88 training.

input: d: document set, q: question set.

output:

m: max-number of iterations,   : learning rate.
the averaged weight vector.

1:
2:
3:
4:
5:
6:
7:
8:
9:
10:

(cid:126)       1; (cid:126)  (cid:48)     0
for iter     [1, m ] do
foreach q     q do

  d = arg maxdt   d f (q, dt)
if   d (cid:54)= d then

foreach i     [1, n] do

# d is the oracle
# for each    eld

             sign[r(qi, di)     r(qi,   di)]
  i       i +   

(cid:126)  (cid:48)     (cid:126)  (cid:48) + (cid:126)  
m   |q|

return (cid:126)  (cid:48)   

1

all hyper-parameters were optimized on the devel-
opment sets and evaluated on the test sets. for
our experiments, we used the following hyper-
parameters: m = 40,    = 0.002.

julie is either in the school   or the cinemafield extractorindexed documents   {julie_a1_is, school_a2_is,   cinema_a2_is    }f3diskindex{july_nsubj, is_root, either_preconj,    }f2{julie, is, either, in,   the, school,    }f1{   }fnlexiconsyntaxsemanticsmore fields   type

1 (qa1)
2 (qa4)
3 (qa5)
4 (qa6)
5 (qa9)
6 (qa10)
7 (qa12)
8 (qa20)

avg.

   = 1

   = 1

   = 1

lexical

   is learned

   is learned

lexical + syntax

lexical + syntax + semantics
   is learned
map mrr map mrr map mrr map mrr map mrr map mrr
100.0
39.62
82.05
62.90
96.33
37.10
94.27
64.00
96.72
47.90
98.23
47.80
99.80
19.20
56.32
37.10
90.47
44.45

100.0
64.10
94.20
89.30
94.40
96.90
99.60
42.80
85.16

61.73
81.45
54.00
75.07
63.50
63.78
38.68
51.82
61.25

39.62
62.90
38.20
64.00
48.10
47.90
19.20
37.10
44.63

61.73
81.45
54.70
75.07
63.62
63.92
38.68
51.82
61.37

29.90
64.00
48.00
65.80
47.90
49.20
25.10
31.40
45.16

48.05
82.00
62.15
78.47
63.67
65.52
40.83
42.00
60.34

40.50
64.00
48.40
66.10
50.50
50.20
31.90
35.70
48.41

61.47
82.00
62.25
78.53
65.47
66.33
49.82
44.22
63.76

72.60
55.70
72.60
78.20
53.90
57.60
55.00
31.20
59.60

85.07
77.85
82.65
88.33
67.88
70.68
70.60
46.50
73.70

table 1: results from our question-answering system on 8 types of questions in the babi tasks.

4 experiments

4.1 data and id74
our approach is evaluated on a subset of the babi
tasks (weston et al., 2015). the original data con-
tains 20 tasks, where each task represents a dif-
ferent kind of id53 challenge. we
select 8 tasks, in which answer for a single question
is located within a single sentence. for consistency
and replicability, we follow the same training, de-
velopment, and evaluation set splits as provided,
where every set contains 1,000 questions.

for the id74, we use mean average
precision (map) and mean reciprocal rank (mrr)
of the top-3 predictions. the mean average preci-
sion is measured by counting the number of ques-
tions, for which sentences containing the answers
are correctly selected as the best predictions. the
reciprocal rank of a query response is the multi-
plicative inverse of the rank of the    rst correct an-
swer. mean reciprocal rank is the average of the
reciprocal ranks of all question queries.

4.2 evaluation
table 1 shows the results from our system on dif-
ferent types of questions. the map and mrr show
clear correlation with respect to the number of ac-
tive    elds. for the majority of tasks, using only the
lexical    elds does not perform well. the    ctional
stories included in this data often contain multiple
occurrences of the same lexicons, and the lexical
   elds alone are not able to select the correct answer.
signi   cantly lower accuracy for the last task is due
to a fact that besides an answer is located within
a single sentence, multiple passages for the single
question are required to correctly locate the sen-
tence with the answers. lexical    elds coupled with
only syntactic    elds do not perform much better. it
may be due to a fact that the syntactic    elds con-

taining ordinary dependency labels do not provide
suf   cient context-wise information so that they do
not generate enough features for statistical learn-
ing to capture speci   c characteristic of the context.
the signi   cant improvement, however, is reached
when the semantics    elds are added as they provide
deeper understanding of the context.

not that this data set has also been used for eval-
uating the memory networks approach to ques-
tion answering (weston et al., 2015). the authors
achieved high accuracy, reaching 100% in several
tasks; however, our work still    nds its own value
because our approach is completely data-driven
such that it can be easily adapted or extended to
other types of questions. as a matter of fact, we
are using the same system for all tasks with differ-
ent trained models, yet still able to achieve high
accuracy for most tasks we evaluate on.

5 conclusion

this paper presents a multi-   eld weighted indexing
approach for id53. our system de-
composes linguistic structures into multiple    elds,
indexes terms of individual    elds, and retrieves the
documents containing the answers with respect to
the relevance scores weighted differently. we ob-
serve signi   cant improvement as we add more se-
mantic    elds and apply averaged id88 learn-
ing to statistically designate weights for the    elds.
in the future, we plan to extend our work by in-
tegrating additional layers of    elds (e.g., freebase,
id138). furthermore, we plan to improve our
nlp tools to enable even deeper understanding of
the context for more complex id53.

references
jinho d. choi and andrew mccallum.

2013.
transition-based id33 with selec-

methods in natural language processing and com-
putational natural language learning, emnlp-
conll   07, pages 12   21.

jason weston, antoine bordes, sumit chopra, and
tomas mikolov. 2015. towards ai-complete ques-
tion answering: a set of prerequisite toy tasks.
arxiv:1502.05698.

wen-tau yih, ming-wei chang, christopher meek, and
andrzej pastusiak. 2013. id53 us-
in pro-
ing enhanced lexical semantic models.
ceedings of the 51st annual meeting of the associ-
ation for computational linguistics, acl   13, pages
1744   1753.

wen-tau yih, xiaodong he, and christopher meek.
2014. id29 for single-relation ques-
tion answering. in proceedings of the 52nd annual
meeting of the association for computational lin-
guistics, acl   14, pages 643   648.

tional branching. in proceedings of the 51st annual
meeting of the association for computational lin-
guistics (volume 1: long papers), acl   13, pages
1052   1062, so   a, bulgaria, august.

jinho d. choi and martha palmer. 2011. transition-
based id14 using predicate
in proceedings of acl
argument id91.
workshop on relational models of semantics,
relms   11, pages 37   45.

jinho d. choi and martha palmer. 2012. fast and ro-
bust part-of-speech tagging using dynamic model
selection. in proceedings of the 50th annual meet-
ing of the association for computational linguistics
(volume 2: short papers), acl   12, pages 363   367,
jeju island, korea, july.

anthony fader, luke zettlemoyer, and oren etzioni.
2013. paraphrase-driven learning for open ques-
tion answering. in proceedings of the 51st annual
meeting of the association for computational lin-
guistics, acl   13, pages 1608   1618.

david a. ferrucci, eric w. brown, jennifer chu-
carroll, james fan, david gondek, aditya kalyan-
pur, adam lally, j. william murdock, eric nyberg,
john m. prager, nico schlaefer, and christopher a.
welty. 2010. building watson: an overview of the
deepqa project. ai magazine, 31(3):59   79.

ben hixon, peter clark, and hannaneh hajishirzi.
2015.
learning id13s for ques-
tion answering through conversational dialog. in
proceedings of the 2015 conference of the north
american chapter of the association for computa-
tional linguistics: human language technologies,
naacl   15, pages 851   861.

oleksandr kolomiyets and marie-francine moens.
2011. a survey on id53 technology
from an information retrieval perspective. informa-
tion sciences, 181(24):5412   5434.

alessandro moschitti and silvia quarteroni.

2011.
linguistic kernels for answer re-ranking in ques-
information and process-
tion answering systems.
ing management, 47(6):825   842.

luiz augusto pizzato and diego moll  a. 2008.

in-
dexing on semantic roles for id53.
in proceedings of the 2nd workshop on information
retrieval for id53, ir4qa   08, pages
74   81.

barry schiffman, kathleen mckeown, ralph grish-
man, and james allan. 2007. id53
using integrated information retrieval and informa-
in the conference of the north
tion extraction.
american chapter of the association for computa-
tional linguistics, acl   07, pages 532   539.

dan shen and mirella lapata. 2007. using seman-
tic roles to improve id53. in pro-
ceedings of the 2007 joint conference on empirical

