5
1
0
2

 
r
p
a
0
1

 

 
 
]

g
l
.
s
c
[
 
 

4
v
8
2
0
7

.

2
1
4
1
:
v
i
x
r
a

publishedasaconferencepaperaticlr2015jointid56-basedgreedyparsingandwordcompositionjo  ellegrandidiapresearchinstitute,martigny,switzerland  ecolepolytechniquef  ed  eraledelausanne(epfl),lausanneswitzerlandjoel.legrand@idiap.chronancollobert   facebookairesearch,menlopark,ca,usaidiapresearchinstitute,martigny,switzerlandronan@collobert.comabstractthispaperintroducesagreedyparserbasedonneuralnetworks,whichleveragesanewcompositionalsub-treerepresentation.thegreedyparserandthecom-positionalprocedurearejointlytrained,andtightlydependsoneach-other.thecompositionprocedureoutputsavectorrepresentationwhichsummarizessyntac-tically(parsingtags)andsemantically(words)sub-trees.compositionandtag-gingisachievedovercontinuous(wordortag)representations,andrecurrentneu-ralnetworks.wereachf1performanceonparwithwell-knownexistingparsers,whilehavingtheadvantageofspeed,thankstothegreedynatureoftheparser.weprovideafullyfunctionalimplementationofthemethoddescribedinthispaper1.1introductioninnaturallanguageprocessing(nlp),theparsingtaskaimsatanalysingtheunderlyingsyntacticstructureofanaturallanguagesequenceofwords(asentence).theanalysisisexpressedasatreeofsyntacticrelationsbetweensub-constituentsofthesentence.inthelinguisticworld,chomsky(1956)   rstintroducedformallytheparsingtask,byde   ningthenaturallanguagesyntaxasasetofcontext-freegrammarrules(aparticulartypeofformalgrammar),combinedwithtransformationsrules.automatedsyntacticparsingbecamerapidlyakeytaskincomputationallinguistic.aparsetreenotonlycarriessyntaxinformation,butmightalsoembedsomesemanticinformation(inthesensethatitcandisambiguatedifferentinterpretationsofagivensentence).inthatrespect,parsingithasbeenwidelyusedasaninputfeatureforseveralothernlptaskssuchasmachinetransla-tion(zollmann&venugopal,2006),informationretrieval(alonsoetal.,2002),orsemanticrolelabeling(punyakanoketal.,2008).thispaperintroducesagreedyparserwhichleveragesanewcompositionapproachtokeepanhis-toryofwhathasbeenpredictedsofar.thecompositionperformsasyntacticandsemanticsummaryofthecontentsofasub-treeintheformofavectorrepresentation.thecompositionisperformedalongthetree:bottomtreenoderepresentationsareobtainedbycomposingcontinuouswordvectorrepresentations,andproducesvectorrepresentationswhichareinturncomposedtogetherinsubse-quentnodesofthetree.thecompositionoperationaswellastreenodetaggingandpredictionsareachievedwitharecurrentneuralnetwork(id56).boththecompositionandnodepredictionaretrainedjointly.section2presentsseveralrelatedapproaches.section3detailsourparsingarchitecture.anempir-icalevaluationofourmodelsaswellasourcompositionalvectorsisgiveninsection4.   allresearchwasconductedattheidiapresearchinstitute,beforeronancollobertjoinedfacebookairesearch.1theparsercanbedownloadedatjoel-legrand.fr/parser.1publishedasaconferencepaperaticlr20152relatedworkthe   rstattemptstoautomaticallyparsenaturallanguageweremainlyconductedusinggenerativemodels.awiderangeofparserwere,andstillare,basedonprobabilisticcontext-freegrammar(pid18s)(magerman,1995;collins,2003;charniak,2000).thesetypesofparsersmodelthesyn-tacticgrammarbycomputingstatisticsofsimplegrammarrules(overparsingtags)occurringinatrainingcorpus.however,manylanguageambiguitiescannotbecaughtwithsimpletag-basedpid18rules.akeyelementinthesuccessofpid18sistore   netheruleswithawordlexicon.thisisusuallyachievedbyattachingtopid18salexicalinformationcalledthehead-word.sev-eralhead-wordvariantsexist,buttheyallrelyonadeterministicprocedurewhichleveragescleverlinguisticknowledge.parsingid136ismostlyachievedusingsimplebottom-upchartparser(kasami,1965;earley,1970;kay,1986).thesemethodsfaceaclassicallearningdilemma:ononehandpid18ruleshavetobere   nedenoughtoavoidanyambiguitiesintheprediction.ontheotherhand,toomuchre   nementintheserulesimpliesloweroccurrencesinthetrainingset,andthusapossiblegeneralizationissue.pid18s-basedparsersarethusjudiciouslycomposedwithcarefullychosenpid18rulesandcleverid173tricks.2.1state-of-the-artdiscriminativeapproachesfromhenderson(2004);charniak&johnson(2005)outperformstan-dardpid18-basedgenerativeparsers,butonlybydiscriminativelyre-rankingthek-bestpredictedtreescomingoutofagenerativeparser.toourknowledge,thestateoftheartinsyntacticparsingisstillheldbymccloskyetal.(2006),wholeveragesdiscriminativere-ranking,aswellasself-trainingoverunlabeledcorpora:are-rankeristrainedoveragenerativemodelwhichisthenusedtolabeltheunlabeleddataset.theoriginalparseristhenre-trainedwiththisnew   labeled   corpus.petrov&klein(2007)introducedamethodtoautomaticalyre   nepid18rulesbyiterativelysplitingthem.thismethodleveragesanef   cientcoarse-to-   neproceduretospeedupthedecodingprocess.morerecently,finkeletal.(2008);petrov&klein(2008)proposedpid18-baseddiscriminativeparsersreachingtheperformanceoftheirgenerativecounterparts.conditionalrandomfields(crfs)areatthecoreofsuchapproaches.carrerasetal.(2008)currentlyholdsthestate-of-the-artamongthe(non-reranking)discriminativeparsers.theirparserleveragesaglobal-linearmodel(insteadofacrf)withpid18s,togetherwithvariousnewadvancedfeatures.z.etal.(2010)showedthatjointlyusingmultipleself-trainedgrammarscanachievehigheraccuracythananindividualgrammar.incontrasttotheseexistingapproaches,ourparserdoesnotrelyonpid18s,noronre   nedfeatureslikehead-words.taggingnodesisachievedinagreedymanner,usingonlyrawwordsandpart-of-speech(pos)asfeatures.treenodehistoryismaintainedasavectorrepresentationobtainedinarecurrentfashion,bycomposingpastnoderepresentationsandtagpredictions.2.2greedyparsingmanydiscriminativeparsersfollowsagreedystrategybecauseofthelack(ortheintractability)ofaglobaltreescoreforanentirederivationpathwhichwouldcombineindependentnodedecisions.adoptingagreedystrategythatmaximizelocalscoresforindividualdecisionsisthenasolutionworthinvestigating.oneofthe   rstsuccessfuldiscriminativeparsers(ratnaparkhi,1999)wasbasedonmaxentclassi   ers(trainedoveralargenumberofdifferentfeatures)andpoweredagreedyshift-reducestrategy.henderson(2003)introducedagenerativeleft-cornerparserwheretheid203ofaderivationgiventhederivationhistoricwasapproximatedusingasimplesynchronynetworks(snn),whichisaneuralnetworkspeci   callydesignedforprocessingstructures.turian&melamed(2006)laterproposedabottom-upgreedyalgorithmfollowingaleft-to-rightoraright-toleftstrategyandusingusingafeatureboostingapproach.inthisapproach,greedydecisionsregardingthetreeconstructionaremadeusingdecisiontreeclassi   ers.theirmodelwasneverthelesslimitedtoshortlengthsentences.zhuetal.(2013)proposedashift-reduceparserwhichachievesresultscomparabletotheirchart-basedcounterparts.thisisdonebyleveragingseveralunsuperviselytrainedfeatures(wordbrown2publishedasaconferencepaperaticlr2015id91,dependencyrelations,dependencylanguagemodel)combinedwithasmartbeamsearchstrategy.2.3parsingwithrecurrentneuralnetworksrecurrentneuralnetworks(id56s)wereseenveryearly(elman,1991)asawaytotackletheproblemofparsing,astheycannaturallyrecuralongtheparsetree.a   rstpracticalapplicationofid56onsyntacticparsingwereproposedbycostaetal.(2002).theirapproachwasbasedonaleft-to-rightincrementalparser,wherearecursiveneuralnetworkwasusedtore-rankpossiblephraseattachments.thegoaloftheircontributionwas,intheirownterms,theassessmentofamethodologyratherthanafullyfunctionalsystem.theydemonstratedthatid56swereabletocaptureenoughinformationtomakecorrectparsingdecisions.collobert(2011)proposedapurelydiscriminativeparserbasedonneuralnetworks.thismodelleveragedcontinuousvectorrepresentationsfromcollobert&weston(2008),andbuildsthefullparsingtreeinabottom-upmanner.todealwiththerecursivestructureinherenttosyntacticparsing,averysimplehistorywasgiventothenetworkasanewvectorfeature(correspondingtothenearesttagspanningthewordbeingtagged).socheretal.(2011)alsoleveragedcontinuousvectorsfromcollobert&weston(2008),combiningthemtobuildatreeinagreedymanner.however,thisworkdidnottacklethefullparsetreeproblem,butwasrestrictedtounlabeledbracketing.socheretal.(2013)introducedthecompositionalvectorgrammar(cvg)whichcombinespid18swithasyntacticallyuntiedrecursiveneuralnetwork(su-id56).compositionisperformedoverabinarytree,thenusedtoscorethek-besttreescomingoutofagenerativeparser.foragiven(parent)nodeofthetree,theauthorsapplyacompositionoperationoveritschildnodes,conditionedwiththeirsyntaxinformation.incontrast,wecomposephrases(notlimitedtotwowords).boththewordsandsyntaxinformationofthechildnodesarefedtoeachcompositionoperation,leadingtoavectorrepresentationofeachtreenodecarryingbothsomesemanticandsyntacticinformation.wealsodonotrelyonanygenerativeparserasourmodeljointlytrainsthetaskofnodeprediction,andthetaskofnodecomposition.legrand&collobert(2014)proposedagreedyid56-basedparser.theneuralnetworkwasrecur-rentonlyinthesenseitusedpreviouslypredictedtagstoproducenexttreenodetags.contrarytosocheretal.(2013),itdidnotinvolvecomposingsub-treerepresentations.instead,head-wordswereusedasakeyfeature.ourapproachsharessomesimilaritieswith(legrand&collobert,2014),asitisalsoagreedyparserbasedonid56s.however,insteadofrelyingonhead-words(whichcouldbeseenasasimplisticrepresentationsofsub-trees),weleveragecompositionalsub-treevectorrep-resentationstrainedjointlywiththeparser.thisapproachleadstomuchbetterparsingaccuracy,whilerelyingonlyonafewsimplefeatures(wordsandpostags).ourmodelhasalsotheabilityofproducingphraseembeddings,whichmayrepresentavaluablefeatureforothernlptasks.chen&manning(2014)proposedagreedytransition-baseddependencyparserbasedonneuralnetworks,fedwithdensewordandtagvectorrepresentations.incontrasttoourapproach,itdoesnotintegrateacompositionalprocedureoversentencesub-trees.thenetworkisonlyinvolvedinpredictingcorrecttransitionsateachstepoftheparsingprocess.3greedyid56parsingourparserisbasedonaneuralnetworktagger,andperformparsinginagreedyrecurrentway.ourapproachisabottom-upiterativeprocedure:thetreeisconstructedstartingfromtheterminalnodes(sentencewords),asshowninfigure1.ateachiteration,1.welookforallpossiblenewtreenodesmerginginputconstituents(i.e.,headsofthetreespredictedsofarorleaveswhichhavenotbeencomposedsofar).forthatpurpose,weapplyaneuralnetwork(seefigure3)slidingwindowtaggeroverinputconstituentsx1,...,xn.consideringanarbitraryrulea   xi,xi+1,...,xj3publishedasaconferencepaperaticlr2015iw:lookaroundandchooseyourownground.it:vbrpccvbprp$jjnn.o:os-prtoob-npi-npe-npoiw:lookr1andchooser2.it:vbprtccvbnp.o:b-vpe-vpob-vpe-vpoiw:r3andr4.it:vpccvp.o:b-vpi-vpe-vpoiw:r5.it:vp.o:b-se-sfigure1:greedyparsingalgorithm,onthesentence   lookaroundandchooseyourownground.   .iw,itandostandforinputwords(orcomposedwordrepre-sentationsri),inputsyntactictags(parsingorpart-of-speech)andoutputtags(pars-ing),respectively.seefigure2andsection3.2forthewordcompositionprocedure.thetreeproducedafter4greedyiterations(asshownhere)canbereconstructedasthefollowing:(s(vp(vp(vblook)(prt(rparound)))(ccand)(vp(vbchoose)(np(prp$your)(jjown)(nnground))))(..)).de   ninganewnodewithtaga,thetaggerwillproducepre   xedtagsb-a,i-a,...e-a,respectivelyforconstituentsxi,xi+1,...,xj,followingaclassicalbioespre   xingscheme2.2.asimpledynamicprogrammingisperformed,onlytoinsurethecoherenceofthetagprediction(e.g.,ab-acanbefollowedonlybyai-aorae-a).3.a(neuralnetwork)compositionmodulecomputesvectorrepresentationsofthenewnodes,accordingtotherepresentationsofthemergedconstituents,aswellasthetagpredictions(seefigure2).4.newpredictednodesbecomeinputconstituentsandwegobackto1(seefigure1).oursystemisrecurrentintwoways:newlypredictedparsingnodelabelsaswellasvectorrepresen-tationsobtainedbycomposingthesepredictednodes,areusedinthenextiterationofouralgorithm.wewilldetailourarchitectureinthefollowing.3.1wordembeddingsfollowingtheworkfromcollobert&weston(2008)onvariousnlptasks,ourparserreliesonrawwords.eachwordina   nitedictionaryw,isassignedacontinuousvectorrepresentation.theserepresentationsasallparametersofourarchitecturearetrainedbyback-propagation.moreformally,givenasentenceofnwords,w1,w2,...,wn,eachwordwn   wis   rstembeddedinad-dimensionalvectorspacebyapplyingalookup-tableoperation:ltw(wn)=wwn,wherethematrixw   rd  |w|representstheparameterstobetrainedinthislookuplayer.eachcolumnwn   rdcorrespondstothevectorembeddingofthenthwordinourdictionaryw.inthiswork,twokindoffeaturesareusedtofeedthenetworks:words(orwordcompositions)andpostagst(orparsingtagsp).asforwords,alookup-tableassociateseachtagtinthe   nitesetoftagt   pwithacontinuousvectorrepresentationofsizet.theoutputvectorsofthedifferentlookup-tablesaresimplyconcatenatedtoformtheinputofthenextlayer.2begin,intermediate,other,end,single.thisapproachisveryoftenusedinnlp,whenonewantstorewriteachunk(herenode)predictionproblemintoawordtaggingproblem.4publishedasaconferencepaperaticlr2015choosevbyourprp$ownjjgroundnnc3r2c2r4figure2:recurrentcompositionofthesub-tree(vp(vbchoose)(np(prp$your)(jjown)(nnground))).therepresentationr2is   rstcomputedusingthe3-inputsmodulec3withyour/prp$own/jjground/nnasinput.r4isob-tainedbyusingthe2-inputsmodulec2withchoose/vbr1/npasinputxi   2xi   1xixi+1xi+2concath(m1  .)m2  .s1s2s|  p|...figure3:aconstituentxiistaggedbycon-sideringa   xedsizecontextwindowofsizek(herek=5).theconcatenatedoutputofthecompositionalhistoryandconstituenttagsisfedasinputtothetagger.itoutputsascoreforeachbioes-pre   xedparsingtag.thetaggerisastandardtwo-layerneuralnet-work.tagsforthecurrentsequenceofcon-stituentsx1,...,xnisobtainedbysimplyslidingthisnetworkoverthesequence.usingcontinuouswordvectorsasinputallowsustotakeadvantageofunsuperviselypre-trainedwordembeddings.lotofworkonthisdomainhasbeendoneinrecentyear,includingcollobert&weston(2008),mikolovetal.(2013).inthispaper,wechosetousetherepresentationsfroid113bret&collobert(2014),obtainedbyasimplepcaonamatrixofwordco-occurrences.3.2word-tagcompositionateachstepoftheparsingprocedure,werepresentseachnodeofthetreeasavectorrepresentation,whichsummarizesboththesyntax(predictedposorparsingtags)andthesemantic(words)ofthesub-treecorrespondingtothegivennode.asshowninfigure2,thevectorrepresentationisobtainedbyasimplerecurrentprocedure,whichinvolvesseveralcomponents:   wordvectorrepresentationsfortheleaves(comingoutfromalookuptable)(dimensiond).   tag(posfortheleaves,predictedtagsotherwise)vectorrepresentations(alsocomingoutforanotherlookuptable,asexplainedinsection3.1)(dimensiont).   compositionalnetworksck().eachofthemcancompresstherepresentationofachunkofsizekintoad-dimensionalvector.compositionalnetworkstakeasinputboththemergednoderepresentationsandpredictedtagrep-resentations.thereisonedifferentnetworkckforeachpossiblenodewithanumberofkmergedconstituent.inpracticemosttreenodesdonotmergemorethanafewconstituents3.inourcase,denotingz   r(d+t)  ktheconcatenationofthemergedconstituentrepresentations(kvectorsoftagsandconstituentrepresentations),thecompositionalnetworkissimplyamatrix-vectoroperationfollowedbyanon-linearityck(z)=h(mkz),wheremk   rd  (k(d+t))isamatrixofparameterstobetrained,andh()isasimplenon-linearitysuchasapointwisehyperbolictangent.3taking1   k   5coversalready98.6%ofthenodesinthewallstreetjournaltrainingcorpus,and1   k   7covers99.8%.5publishedasaconferencepaperaticlr2015notethatnodeandwordrepresentationsareembeddedinthesamespace.thisway,thecom-positionalnetworksckcancompressindifferentlyinformationcomingfroid113avesorsub-trees.implementation-wise,onecanstorenewnoderepresentationsintothewordlookup-tableasthetreeiscreated,suchthatsubsequentcompositionortaggingoperationscanbeachievedinanef   cientmanner.3.3slidingwindowbioestaggerthetaggingmoduleofourarchitecture(seefigure3)isatwo-layerneuralnetworkwhichappliesaslidingwindowovertheinputconstituentrepresentations(ascomputedinsection3.2),aswellastheinputconstituenttagrepresentations.consideringninputconstituentsx1,...,xn,ifweassumetheirrespectiverepresentationshasbeenstoredsofarinlookuptables,thenthwindowisde   nedasun=[lt(xn   k   12),...,lt(xn),...,lt(xn+k   12)],wherekisthesizeofwindow.denoting  pthesetofbioes-pre   xedparsingtagsfromp,themoduleoutputsavectorofscoress(un)=[s1,...,s|  p|](wherestisthescoreofthebioes-pre   xedparsingtagt     pfortheconstituentxn).theconstituentwithindicesexceedingtheinputboundaries(n   (k   1)/2<1orn+(k   1)/2>n)aremappedtoaspecialpaddingvector(whichisalsolearned).asanyclassicaltwo-layerneuralnetwork,ourarchitectureperformsseveralmatrix-vectoroperationsonitsinputs,interleavedwithsomenon-lineartransferfunctionh(  ),s(un)=m2h(m1un),wherethematricesm1   rh  k|d|andm1   r|  p|  harethetrainedparametersofthenetwork,andh()isapointwisenon-linearfunctionsuchasthehyperbolictangent.thenumberofhiddenunitshisahyper-parametertobetuned.3.4coherentbioespredictionsthenextmoduleofourarchitectureaggregatesthebioes-pre   xedparsingtagsfromourtaggermoduleinacoherentmanner.itisimplementedasaviterbidecodingalgorithmoveraconstrainedgraphg,whichencodesallthepossiblevalidsequencesofbioes-pre   xedtagsoverconstituents:e.g.b-atagscanonlybefollowedbyi-aore-atags,foranyparsinglabela.eachnodeofthegraphisassignedascoreproducedbythepreviousneuralnetworkmodule(scoreforeachbioes-pre   xedtag,andforeachword).thescores([t]n1,[x]n1,  )forasequenceoftags[t]n1inthelatticegissimplyobtainedbysummingscoresalongthepath([x]n1beingtheinputsequenceofconstituentsand  alltheparametersofthemodel).wefollowedtheexactsameapproachasin(legrand&collobert,2014),exceptthattransitionscores(edgesonthegraph)wereallsettozero.indeed,weobservedinempiricalexperimentsthataddingtransitionsscoresdoesnotimprovef1-scoreperformance.thisdecodingisthuspresentonlytoinsurecoherenceinthepredictedsequenceoftags.3.5trainingprocedureboththecompositionnetworkandtaggingnetworksaretrainedbymaximizingalikelihoodoverthetrainingdatausingstochasticgradientascent.weperformedallpossibleiterations,overalltrainingsentences,ofthegreedyprocedurepresentedinfigure1constrainedwiththeprovidedlabeledparsetree.thisleadstoourtrainingsetofsequencesoftreenodes.whilethisprocedureissimilarto(legrand&collobert,2014),itisworthmentioningthatitimpliesthesystemisonlytrainedoncorrectsequencesoftreenodes.inthatrespect,itisnottrainedtorecoverfrompastmistakesitcouldhavemadeduringtherecurrentprocess.foreverytreenode,thesub-trees(structureandtags)werealsoextractedduringthisprocedure.trainingthesystemconsistsinrepeatingthefollowingsteps:   pickarandomsequenceofnodesextractedinthetrainingset,asdescribedabove.considertheassociatedsub-treesforeachnodewhichisnotaleaf.   performaforwardpassoftheword-tagcomposer(seesection3.2)alongthesesub-trees.6publishedasaconferencepaperaticlr2015   forallnodesinthesequence,performaforwardpassofthetaggeraccordingtoword(orsub-tree)representations,aswellasconstituenttags.   computealikelihoodoftherightsequenceofbios-pre   xedtags(seebelow),giventhescoresofthetagger.   backwardgradientthroughthetaggeruptotheword(orsub-tree)andtagrepresentations.   backwardgradientthroughtheword-tagcomposeruptothewordandtagrepresentation.   updateallmodelparameters(fromcompositionalnetworksci,taggernetwork,andlookuptables)witha   xedlearningrate.detailsaboutthetraininglikelihoodcanbefoundin(legrand&collobert,2014).thescores([t]n1,[x]n1,  )ofthetruesequenceofbios-pre   xedtags[t]n1,giventheinputnodesequence[x]n1canbeinterpretedasaconditionalid203byexponentiatingthisscore(thusmakingitpositive)andnormalizingitwithrespecttoallpossiblepathscores.thelog-id203ofase-quenceoftags[t]n1fortheinputsequenceofconstituents[x]n1isgivenby:logp([t]n1|[x]n1,  )=s([t]n1,[x]n1,  )(1)   log      x   [t0]n1exps([t0]n1,[x]n1,  )      .thesecondtermofthisequation(whichcorrespondtothenormalisationterm)canbecomputedinlineartimethankstoarecursionsimilartotheviterbialgorithm(seerabiner,1989).similartrainingprocedureshavebeenproposedinthepastforstructureddata(denker&burges,1995;bottouetal.,1997;laffertyetal.,2001).4experiments4.1corpusexperimentswereconductedusingthestandardenglishpenntreebankdataset(marcusetal.,1993).weadoptedtheclassicalsetup,withsections02-21fortrain,section22forvalidation,andsection23fortest.thevalidationcorpuswasusedtoselectourhyper-parametersandbestmodels.wepre-processedthedataonlywithasubsetofoperationswhichareachievedinstandardparsers:(1)functionallabels,traceswereremoved,(2)theprtlabelwasreplacedbyadvp(magerman,1995).(3)wetackledtheunarychainissue-non-terminalswithasinglenon-terminalchild-bymergingthenodestogetherandassigningastagtheconcatenationofthemergednodetags.thiswasdoneinordertoavoidloopingissuesintheparsingalgorithm(e.g.anodebeingrepetitivelytaggedwithtwodifferenttagsinouriterativeprocess)andensuretheconvergenceoftheparsingprocess.onlyconcatenatedlabelswhichoccurredatleast30times(correspondingtothelowestnumberofoccurrencesofthelesscommonoriginalparsingtag)werekept,leadingto11additionalparsingtags.addedtotheoriginal26parsingtags,thisresultedin161tagsproducedbyourparser.attesttime,theinverseoperationisperformed:concatenatedtagnodesaresimplyexpandedintotheiroriginalform.4.2detailedsetupoursystemsweretrainedusingastochasticgradientdescentovertheavailabletrainingdataun-tilconvergenceonthevalidationset.hyper-parameterswerechosenaccordingtothevalidation.lookup-tablesizesforthewordsandtags(part-of-speechandparsing)are200and20,respectively.thewindowsizeforthetaggerisk=7(3neighboursfromeachside).thesizeofthetagger   shiddenlayerish=500.weusedthewordembeddingsobtainedfroid113bret&collobert(2014)toinitializethewordlookup-table.theseembeddingswerethen   ne-tunedduringthetrainingpro-cess.we   xedthelearningrateto  =0.15duringthestochasticgradientprocedure.assuggestedinplaut&hinton(1987),thelearningratewasdividedbythesizeoftheinputvectorofeachlayer.thepart-of-speechtagswereobtainedusingthefreelyavailablesoftwaresenna4.4http://ml.nec-labs.com/senna7publishedasaconferencepaperaticlr2015051015859095100iterationsf1-scoretrain(standard)valid(standard)train(dropout)valid(dropout)figure4:trainandvalidationf1-score,ac-cordingtothenumberoftrainingiterations,withandwithoutthe   dropout   procedure.0102030405060708090100sentencelengthvalidationf1validationf1#sentences0200400600#sentencesfigure5:validationf1andnumberofsen-tences,accordingthethesentencelength.4.3wordembeddingdropoutid173wefoundthatoursystemwaseasilysubjecttoover   tting(trainingf1-scoreincreasingwhilethevalidationcurvewaseventuallydecreasingasshowninfigure4).asthecapacityofournetworkmainlyliesonthewordsandtagembeddings,weadoptedadropoutid173strategy(seehintonetal.,2012)forthelookuptables.thekeyideaofthedropoutid173istorandomlydropunits(alongwiththeirconnections)fromtheneuralnetworkduringtraining.thispreventsunitsfromco-adaptingtoomuch.inourcase,duringthetrainingphase,a   dropoutmask   isappliedtotheoutputofthelookup-tables:eachelementoftheoutputissetto0withaid2030.25.attesttime,nopatchisappliedbuttheoutputisre-weighted,scalingitby0.75.weobservedagoodimprovementinf1-scoreperformance,asshowninfigure4.4.4performancecomparisonf1performancescoresarereportedintable1.scoreswereobtainedusingtheevalbimplemen-tation5.wecomparedoursystemiscomparedwitharangeofdifferentstate-of-the-artparsers.inadditiontothethefourmaingenerativeparsers,wereportthescoresofwellknownre-rankingparsers(includingthestate-of-the-artfrommccloskyetal.(2006))aswellasfortwomajorpurelydiscriminativeparsers.detailederroranalysiscomparedagainstasubsetoftheseparsersisreportedintable2,usingthecodeprovidedbykummerfeldetal.(2012).performancewithrespecttosentencelengthisreportedinfigure5.weincludedavotingprocedureusingseveralmodelstrainedstartingfromdifferentrandominitial-izations.thevotingprocedureisachievedinthefollowingway:ateachiterationofthegreedyparsingprocedure,giventheinputsequenceofconstituents,(1)noderepresentationsarecomputedforeachmodelbycomposingthesub-treerepresentationscorrespondingtothegivenmodelandus-ingitsowncompositionalnetwork(2)eachmodelcomputestagscoresusingitsowntaggernetwork(3)tagscoresareaveraged(4)acoherentpathoftagispredictedusingtheviterbialgorithm.finally,wereportabriefquantitativeevaluationofourcompositionalrepresentationsintable3.randomphraseswerepickedinthewsjcorpus,andclosestneighbors(accordingtotheeuclideandistance)withotherphrasesofthecorpusarereported.5availableathttp://nlp.cs.nyu.edu/evalb8publishedasaconferencepaperaticlr2015table1:performancecomparisonofdifferentstate-of-the-artparsers,intermsofprecision(p),recall(r),andf1score,forsentencesofsize   40words,andonthefullwsjtestset.vxdenotesavotingprocedurewithxmodels.thereportedtime(inseconds)isthetimetoparsethefullwsjtestcorpus.<40fullmodel(r)(p)f1(r)(p)f1timemagerman(1995)84.684.984.8generativecollins(1999)88.588.788.688.188.388.21247charniak(2000)90.190.190.189.689.589.6petrov&klein(2007)90.790.590.690.298.990.1307generativehenderson(2004)89.890.490.1withcharniak&johnson(2005)92.091.1re-rankingmccloskyetal(2006)92.1socheretal(2013)91.190.4390carrerasetal.(2008)90.791.491.1discriminativelegrand&collobert(2014)(v10)90.090.190.189.689.789.6legrand&collobert(2014)+dropout(v10)90.690.190.490.289.789.9thiswork88.889.189.088.288.688.4thiswork+dropout89.790.39089.189.989.530thiswork+dropout(v4)90.590.890.790.190.490.3120table2:detailedparsercomparison.wereporttheaveragenumberofbracketerrorspersentencefordifferenterrorcategories.ppclausediffmodnp1-wordnpattachattachlabelattachattachco-ordspanunaryint.othermccloskyetal(2006)0.600.380.310.250.250.230.200.140.140.50socheretal(2013)0.790.430.290.270.310.320.310.220.190.41legrandetal(2014)0.740.450.270.250.340.380.240.220.200.57thiswork+dropout0.780.440.290.270.360.420.240.210.200.60thiswork+dropout(v4)0.710.430.250.240.350.380.230.210.190.565conclusioninthispaper,weintroducedanewparsingarchitecturewhichleveragesid56-basedcompositionalrepresentationofparsingsub-trees,bothencodingthesyntactic(tags)andsemantic(words)infor-mation.theparsingprocedureistightlyintegratedwiththecompositionoperation,andallowsustoreachperformanceofverywell-knownparserswhile(1)adoptingagreedyandfastprocedure(2)avoidstandardre   nedfeaturessuchasheadwords.acknowledgmentspartofthisworkwassupportedbyneclaboratoriesamerica.referencesalonso,m.a.,vilares,j.,anddarriba,v.m.ontheusefulnessofextractingsyntacticdependenciesfortextindexing.inarti   cialintelligenceandcognitivescience.2002.bottou,l.,lecun,y.,andbengio,y.globaltrainingofdocumentprocessingsystemsusinggraphtransformernetworks.inproc.ofcomputervisionandpatternrecognition,1997.carreras,x.,collins,m.,andkoo,t.tag,dynamicprogramming,andtheid88foref   -cient,feature-richparsing.inproceedingsofthetwelfthconferenceoncomputationalnaturallanguagelearning,2008.charniak,e.amaximum-id178-inspiredparser.inproceedingsofthe1stnorthamericanchap-teroftheassociationforcomputationallinguisticsconference,2000.charniak,e.andjohnson,m.coarse-to-   nen-bestparsingandmaxentdiscriminativereranking.inproceedingsofthe43rdannualmeetingonassociationforcomputationallinguistics,2005.9publishedasaconferencepaperaticlr2015table3:nearestneighbors(intermsofvectorrepresentationeuclideandistance)forseveralphrasesinthewsjcorpus.foreverynodeinthecorpus,thesub-treerepresentationswerecomputed.then,fortheselectedphrases,wecomputedalleuclideandistances.wereportbelowthe5topclosestotherphrasesinwsj.brendanbarba,chairmanofthemoonachie,n.j.,makerofplastic   lmproductsedmundedelman,chairmanofthelosangelescountyboardofsupervisorsestherdyson,editorofrelease0.0,anindustrynewsletterthatspotsnewdevelopmentsmichaelslater,editorofthemicroprocessorreport,anindustrynewsletterbrucemiller,presidentofartfundingcorp.,anartlenderjeffreynichols,presidentofapmscanada,torontopreciousmetalsadvisers,elililly&co.,indianapolis,johnkinnard&co.,minneapolis,procter&gambleco.,cincinnati,anbinvestmentmanagementco.,chicago,scimedlifesystemsinc.,minneapolis,rjrnabiscoinc.   sfrenchcrackersubsidiary,belin,mr.engelken   ssister,martha,whowasborntwodaysbeforethehomerun,thecompany   spresident,n.j.nicholas,whowilleventuallybeco-chiefexecutiveoftimewarneralongsidemr.ross,claudio   ssister,isabella,anovitiateinaconvent,herdaughter,elizabeth,anattorneywhoisvicechairman,hisbrother,parkhaji,whoseheadisswathedinagorgeouscrimsonturban,mrs.coleman   shusband,joseph,aphysician,chairmanandchiefexecutiveof   cerpresidentandchiefexecutiveof   cerpresidentandchiefoperatingof   cerchairmanandchiefexecutiveexecutivevicepresidentandchief   nancialof   cerexecutivevicepresidentandchiefoperatingof   cer10publishedasaconferencepaperaticlr2015chen,d.andmanning,c.d.afastandaccuratedependencyparserusingneuralnetworks.inproceedingsofemnlp,2014.chomsky,n.threemodelsforthedescriptionoflanguage.iretransactionsoninformationtheory,1956.collins,m.head-drivenstatisticalmodelsfornaturallanguageparsing.comput.linguist.,2003.collobert,r.deeplearningforef   cientdiscriminativeparsing.inaistats,2011.collobert,r.andweston,j.auni   edarchitecturefornaturallanguageprocessing:deepneuralnetworkswithmultitasklearning.ininternationalconferenceonmachinelearning,icml,2008.costa,f.,frasconi,p.,lombardo,v.,andsoda,g.towardsincrementalparsingofnaturallanguageusingrecursiveneuralnetworks,2002.denker,j.s.andburges,c.j.c.imagesegmentationandrecognition.ininthemathematicsofinduction,1995.earley,j.anef   cientcontext-freeparsingalgorithm.1970.elman,j.l.distributedrepresentations,simplerecurrentnetworks,andgrammaticalstructure.mach.learn.,1991.finkel,j.r.,kleeman,a.,andmanning,c.d.ef   cient,feature-based,conditionalrandom   eldparsing.ininproc.acl/hlt,2008.henderson,j.inducinghistoryrepresentationsforbroadcoveragestatisticalparsing.inproceedingsofthe2003conferenceofthenorthamericanchapteroftheassociationforcomputationallinguisticsonhumanlanguagetechnology-volume1,2003.henderson,j.discriminativetrainingofaneuralnetworkstatisticalparser.inproceedingsofthe42ndannualmeetingonassociationforcomputationallinguistics,2004.hinton,ge.,srivastava,n.,krizhevsky,a.,sutskever,i.,andsalakhutdinov,r.improvingneuralnetworksbypreventingco-adaptationoffeaturedetectors.corr,2012.kasami,t.anef   cientrecognitionandsyntaxanalysisalgorithmforcontext-freelanguages.tech-nicalreport,1965.kay,m.readingsinnaturallanguageprocessing.chapteralgorithmschemataanddatastructuresinsyntacticprocessing.1986.kummerfeld,j.k.,hall,d.,curran,j.r.,andklein,d.parsershowdownatthewallstreetcorral:anempiricalinvestigationoferrortypesinparseroutput.inproceedingsofthe2012jointconferenceonempiricalmethodsinnaturallanguageprocessingandcomputationalnaturallanguagelearning,2012.lafferty,j.,mccallum,a.,andpereira,f.conditionalrandom   elds:probabilisticmodelsforsegmentingandlabelingsequencedata.ineighteenthinternationalconferenceonmachinelearning,icml,2001.lebret,r.andcollobert,r.wordembeddingsthroughhellingerpca.inproceedingsofthe14thconferenceoftheeuropeanchapteroftheassociationforcomputationallinguistics,2014.legrand,j.andcollobert,r.recurrentgreedyparsingwithneuralnetworks.ineuropeanconfer-enceonmachinelearning,2014.magerman,d.m.statisticaldecision-treemodelsforparsing.ininproceedingsofthe33rdannualmeetingoftheassociationforcomputationallinguistics,1995.marcus,m.p.,santorini,b.,andmarcinkiewicz,m.a.buildingalargeannotatedcorpusofenglish:thepenntreebank.computationallinguistics,1993.11