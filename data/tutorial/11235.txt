learning executable semantic parsers for

natural language understanding

percy liang

computer science department

stanford university

stanford, ca

pliang@cs.stanford.edu

6
1
0
2

 
r
a

 

m
2
2

 
 
]
l
c
.
s
c
[
 
 

1
v
7
7
6
6
0

.

3
0
6
1
:
v
i
x
r
a

abstract
for building id53 systems and natural lan-
guage interfaces, id29 has emerged as an impor-
tant and powerful paradigm. semantic parsers map natural
language into logical forms, the classic representation for
many important linguistic phenomena. the modern twist
is that we are interested in learning semantic parsers from
data, which introduces a new layer of statistical and com-
putational issues. this article lays out the components of a
statistical semantic parser, highlighting the key challenges.
we will see that id29 is a rich fusion of the logi-
cal and the statistical world, and that this fusion will play an
integral role in the future of natural language understanding
systems.

categories and subject descriptors
i.2.7 [arti   cial intelligence]: natural language process-
ing   language parsing and understanding

introduction

1.
a long-standing goal of arti   cial intelligence (ai) is to build
systems capable of understanding natural language. to fo-
cus the notion of    understanding    a bit, let us say that the
system must produce an appropriate action upon receiving
an input utterance from a human. for example:

context:

knowledge of mathematics

utterance: what is the largest prime less than 10?

action:

7

context:

knowledge of geography

utterance: what is the tallest mountain in europe?

action: mt. elbrus

context:

user   s calendar

utterance: cancel all my meetings after 4pm tomorrow.

action:

(removes meetings from calendar)

we are interested in utterances such as the ones above,
which require deep understanding and reasoning. this arti-

cle focuses on id29, an area within the    eld of
natural language processing (nlp), which has been growing
over the last decade. semantic parsers map input utter-
ances into semantic representations called logical forms that
support this form of reasoning. for example, the    rst utter-
ance above would map onto the logical form max(primes    
(      , 10)). we can think of the logical form as a program
that is executed to yield the desired behavior (e.g., answer-
ing 7). the second utterance would map onto a database
query; the third, onto an invocation of a calendar api.

id29 is rooted in formal semantics, pioneered by
logician richard montague [25], who famously argued that
there is    no important theoretical di   erence between natural
languages and the arti   cial languages of logicians.    semantic
parsing, by residing in the practical realm, is more exposed
to the di   erences between natural language and logic, but
it inherits two general insights from formal semantics: the
   rst idea is model theory, which states that expressions (e.g.,
primes) are mere symbols which only obtain their meaning
or denotation (e.g., {2, 3, 5, . . .}) by executing the expression
with respect to a model, or in our terminology, a context.
this property allows us to factor out the understanding of
language (id29) from world knowledge (execu-
tion). indeed, one can understand the utterance    what is
the largest prime less than 10?     without actually computing
the answer. the second idea is compositionality, a principle
often attributed to gottlob frege, which states that the de-
notation of an expression is de   ned recursively in terms of
the denotations of its subexpressions. for example, primes
denotes the set of primes, (      , 10) denotes the set of num-
bers smaller than 10, and so primes     (      , 10) denotes the
intersection of those two sets. this compositionality is what
allows us to have a succinct characterization of meaning for
a combinatorial range of possible utterances.

early systems. logical forms have played a foundational
role in natural language understanding systems since their
genesis in the 1960s. early examples included lunar, a
natural language interface into a database about moon rocks
[34], and shrdlu, a system that could both answer ques-
tions and perform actions in a toy blocks world environment
[32]. for their time, these systems were signi   cant achieve-
ments. they were able to handle fairly complex linguistic
phenomena and integrate syntax, semantics, and reasoning
in an end-to-end application. for example, shrdlu was
able to process    find a block which is taller than the one you
are holding and put it into the box.    however, as the systems
were based on hand-crafted rules, it became increasingly dif-

   cult to generalize beyond the narrow domains and handle
the intricacies of general language.

rise of machine learning. in the early 1990s, in   uenced
by the successes of statistical techniques in the neighbor-
ing id103 community, the    eld of nlp under-
went a statistical revolution. machine learning o   ered a new
paradigm: collect examples of the desired input-output be-
havior and then    t a statistical model to these examples.
the simplicity of this paradigm coupled with the increase in
data and computation allowed machine learning to prevail.

what fell out of favor was not only rule-based methods,
but also the natural language understanding problems. in
the statistical nlp era, much of the community   s atten-
tion turned to tasks   documentation classi   cation, part-of-
speech tagging, and syntactic parsing   which fell short of
full end-to-end understanding. even id53 sys-
tems relied less on understanding and more on a shallower
analysis coupled with a large collection of unstructured text
documents [10], typi   ed by the trec competitions.

statistical id29. the spirit of deep under-
standing was kept alive by researchers in statistical semantic
parsing [36, 24, 33, 37, 19]. a variety of di   erent semantic
representations and learning algorithms were employed, but
all of these approaches relied on having a labeled dataset of
natural language utterances paired with annotated logical
forms, for example:

utterance: what is the largest prime less than 10?

logical form: max(primes     (      , 10))

weak supervision. over the last few years, two exciting
developments have really spurred interest in semantic pars-
ing. the    rst is reducing the amount of supervision from
annotated logical forms to answers [13, 21]:

utterance: what is the largest prime less than 10?

action:

7

this form of supervision is much easier to obtain via crowd-
sourcing. although the logical forms are not observed, they
are still modeled as latent variables, which must be inferred
from the answer. this results in a more di   cult learning
problem, but [21] showed that it is possible to solve it with-
out degrading accuracy.

scaling up. the second development is the scaling up of
semantic parsers to more complex domains. previous se-
mantic parsers had only been trained on limited domains
such as us geography, but the creation of broad-coverage
knowledge bases such as freebase [8] set the stage for a new
generation of semantic parsers for id53. ini-
tial systems required annotated logical forms [11], but soon,
systems became trainable from answers [4, 18, 5]. semantic
parsers have even been extended beyond    xed knowledge
bases to semi-structured tables [26]. with the ability to
learn semantic parsers from question-answer pairs, it is easy
to collect datasets via id104. as a result, semantic
parsing datasets have grown by an order of magnitude.

in addition, semantic parsers have been applied to a number
of applications outside id53: robot navigation

figure 1: a natural language understanding prob-
lem where the goal is to map an utterance x in a
context c to an action y.

[29, 2], identifying objects in a scene [23, 15], converting nat-
ural language to id157 [17], and many others.

outlook. today, the id29 community is a vi-
brant    eld, but it is still young and grappling with the com-
plexities of the natural language understanding problem. se-
mantic parsing poses three types of challenges:

    linguistic: how should we represent the semantics of
natural language and construct it compositionally from
the natural language?

    statistical: how can we learn semantic parsers from
weak supervision and generalize well to new examples?
    computational: how do we e   ciently search over the
combinatorially large space of possible logical forms?

the rest of the paper is structured as follows: we    rst
present a general framework for id29,
intro-
ducing the key components (section 2). the framework is
pleasantly modular, with di   erent choices of the components
corresponding to existing semantic parsers in the literature
(section 3). we describe the datasets (section 4) and then
conclude (section 5).

2. framework
natural language understanding problem. in this ar-
ticle, we focus on the following natural language understand-
ing problem: given an utterance x in a context c, output the
desired action y. figure 1 shows the setup for a question
answering application, in which case x is a question, c is a
knowledge base, and y is the answer. in a robotics applica-
tion, x is a command, c represents the robot   s environment,
and y is the desired sequence of actions to be carried by the
robot [29]. to build such a system, assume that we are given
a set of n examples {(xi, ci, yi)}n
i=1. we would like to use
these examples to train a model that can generalize to new
unseen utterances and contexts.

id29 components. this article focuses on
a statistical id29 approach to the above prob-
lem, where the key is to posit an intermediate logical form
z that connects x and y. speci   cally, z captures the seman-
tics of the utterance x, and it also executes to the action y
(in the context of c). in our running example, z would be
max(primes     (      , 10)). our id29 framework
consists of the following    ve components (see figure 2):

1. executor: computes the denotation (action) y =(cid:74)z(cid:75)c

given a logical form z and context c. this de   nes the
semantic representation (logical forms along with their
denotations).

x:whatisthelargestprimelessthan10?c:primes:{2,3,5,7,11,...}y:7what is the

largest

prime

(r1)

np[primes]

less than

10

?

(r2)

np[10]

(r3)

qp[(      , 10)]

(r4)

np[primes     (      , 10)]
np[max(primes     (      , 10))]

(r5)

figure 2:
id29 framework depicting
the executor, grammar, and model. the parser and
learner are algorithmic components that are respon-
sible for generating the logical form z and parame-
ters   , respectively.

2. grammar: a set of rules g that produces d(x, c), a

set of candidate derivations of logical forms.

3. model: speci   es a distribution p  (d | x, c) over deriva-

tions d parameterized by   .

4. parser: searches for high id203 derivations d un-

der the model p  .

root[max(primes     (      , 10))]

(r7)

(1)

for example, applying (r3) produces category qp and logi-
cal form (      , 10) over span [5 : 7] corresponding to    less than
10    . we stop when we produce the designated root cate-
gory over the entire utterance. note that we could have also
applied (r6) instead of (r5) to generate the (incorrect) log-
ical form min(primes     (      , 10)); let this derivation be d2.
we have d(x, c) = {d1, d2} here, but in general, there could
be exponentially many derivations, and multiple derivations
can generate the same logical form. in general, the grammar
might contain nonsense rules (r6) that do not re   ect ambi-
guity in language but are rather due to model uncertainty
prior to learning.

5. learner: estimates the parameters    (and possibly

rules in g) given training examples {(xi, ci, yi)}n

i=1.

we now instantiate each of these components for our running
example:    what is the largest prime less than 10?    

executor. let the semantic representation be the language
of mathematics, and the executor is the standard interpre-
tation, where the interpretations of predicates (e.g., primes)
are given by c. with c(primes) = {2, 3, 5, 7, 11, . . . ,}, the

denotation is(cid:74)primes     (      , 10)(cid:75)c = {2, 3, 5, 7}.

grammar. the grammar g connects utterances to possible
derivations of logical forms. formally, the grammar is a set
of rules of the form          .1 here is a simple grammar for
our running example:

    np[primes]
prime
(r1)
    np[10]
10
(r2)
    qp[(      , z)]
less than np[z]
(r3)
    np[z1     z2]
(r4) np[z1] qp[z2]
    np[max(z)]
largest np[z]
(r5)
    np[min(z)]
largest np[z]
(r6)
(r7) what is the np[z]?     root[z]

we start with the input utterance and repeatedly apply rules
in g. a rule           can be applied if some span of the
utterance matches   , in which case a derivation over the
same span with a new syntactic category and logical form
according to    is produced. here is one possible derivation
(call it d1) for our running example:

1 the standard way context-free grammar rules are written
is          . because our rules build logical forms, reversing
the arrow is more natural.

model. the model scores the set of candidate derivations
generated by the grammar. a common choice used by virtu-
ally all existing semantic parsers are id148 (gen-
eralizations of id28s).
in a log-linear model,
de   ne a feature vector   (x, c, d)     rf for each possible
derivation d. we can think of each feature as casting a
vote for various derivations d based on some coarse prop-
erty of the derivation. for example, de   ne f = 7 features,
each counting the number of times a given grammar rule
is invoked in d, so that   (x, c, d1) = [1, 1, 1, 1, 1, 0, 1] and
  (x, c, d2) = [1, 1, 1, 1, 0, 1, 1].
next, let        rf denote the parameter vector, which de-
   nes a weight for each feature representing how reliable
that feature is. their weighted combination score(x, c, d) =
  (x, c, d)       represents how good the derivation is. we can
exponentiate and normalize these scores to obtain a distri-
bution over derivations:

p  (d | x, c) =

exp(score(x, c, d))

d(cid:48)   d(x,c) exp(score(x, c, d(cid:48)))

.

(2)

if    = [0, 0, 0, 0, +1,   1, 0], then p   would assign id203
exp(1)+exp(   1)     0.88 to d1 and     0.12 to d2.

exp(1)

parser. given a trained model p  , the parser (approxi-
mately) computes the highest id203 derivation(s) for
an utterance x under p  . assume the utterance x is repre-
sented as a sequence of tokens (words). a standard approach
is to use a chart parser, which recursively builds derivations
for each span of the utterance. speci   cally, for each category
a and span [i : j] (where 0     i < j     length(x)), we loop
over the applicable rules in the grammar g and apply each
one to build new derivations of category a over [i : j]. for
binary rules   those of the form b c     a such as (r4), we
loop over split points k (where i < k     j), recursively com-
pute derivations b[z1] over [i : k] and c[z2] over [k : j], and

(cid:80)

xgrammarcd  modelzexecutorywhatisthelargestprimelessthan10?primes:{2,3,5,7,11,...}max(primes   (      ,10))7(utterance)(context)(parameters)(logicalform)(denotation)(derivations)combine them into a new derivation a[z] over [i : j], where z
is determined by the rule; for example, z = z1     z2 for (r4).
the    nal derivations for the utterance are collected in the
root category over span [0 : length(x)].

(cid:80)

the above procedure would generate all derivations, which
could be exponentially large. generally, we only wish to
compute compute the derivations with high id203 un-
der our model p  . if the features of p   were to decompose as
a sum over the rule applications in d   that is,   (x, c, d) =
(r,i,j)   d   rule(x, c, r, i, j), then we could use dynamic pro-
gramming: for each category a over [i : j], compute the high-
est id203 derivation. however, in executable semantic
parsing, feature decomposition isn   t su   cient, since during
learning, we also need to incorporate the constraint that the

logical form executes to the true denotation (i[(cid:74)d.z(cid:75)c = y]);

see (6) below.
to maintain exact computation in this
setting, the id145 state would need to in-
clude the entire logical form d.z, which is infeasible, since
there are exponentially many logical forms.
therefore,
id125 is generally employed, where we keep only the
k sub-derivations with the highest model score based on
only features of the sub-derivations. id125 is not
guaranteed to return the k highest scoring derivations, but
it is often an e   ective heuristic.

learner. while the parser turns parameters into deriva-
tions, the learner solves the inverse problem. the dominant
paradigm in machine learning is to set up an objective func-
tion and optimize it. a standard principle is to maximize
the likelihood of the training data {(xi, ci, yi)}n
i=1. an im-
portant point is that we don   t observe the correct derivation
for each example, but only the action yi, so we must consider

all derivations d whose logical form d.z satisfy(cid:74)d.z(cid:75)ci = yi.

this results in the log-likelihood of the observed action yi:

oi(  ) def= log

p  (d | xi, ci).

(3)

the    nal objective is then simply the sum across all n train-
ing examples:

o(  ) def=

oi(  ),

(4)

i=1

the simplest approach to maximize o(  ) is to use stochastic
id119 (sgd), an iterative algorithm that takes
multiple passes (e.g., say 5) over the training data and makes
the following update on example i:

          +      oi(  ),

(5)

(cid:88)

where    is a step size that governs how aggressively we want
to update parameters (e.g.,    = 0.1). in the case of log-linear
models, the gradient has a nice interpretable form:

   oi(  ) =

(q(d)     p  (d | xi, ci))  (xi, ci, d),

(6)

d   d(xi,ci)

where q(d)     p  (d | xi, ci)i[(cid:74)d.z(cid:75)ci = yi] is the model distri-

bution p   over derivations d, but restricted to ones consis-
tent with yi. the gradient pushes    to put more id203
mass on q and less on p  . for example, if p   assigns prob-
abilities [0.2, 0.4, 0.1, 0.3] to four derivations and the middle
two derivations are consistent, then q assigns probabilities
[0, 0.8, 0.2, 0].

d   d(xi,ci)

(cid:88)
(cid:74)d.z(cid:75)ci =yi
n(cid:88)

the objective function o(  ) is not concave, so sgd is at best
guaranteed to converge to a local optimum, not a global one.
another problem is that we cannot enumerate all derivations
d(xi, ci) generated by the grammar, so we approximate this
set with the result of id125, which yields k candidates
(typically k = 200); p   is normalized over this set. note
that this candidate set depends on the current parameters
  , resulting a heuristic approximation of the gradient    oi.

summary. we have covered the components of a semantic
parsing system. observe that the components are relatively
loosely coupled: the executor is concerned purely with what
we want to express independent of how it would be expressed
in natural language. the grammar describes how candidate
logical forms are constructed from the utterance but does
not provide algorithmic guidance nor specify a way to score
the candidates. the model focuses on a particular deriva-
tion and de   nes features that could be helpful for predicting
accurately. the parser and the learner provide algorithms
largely independent of semantic representations. this mod-
ularity allows us to improve each component in isolation.

3. refining the components
having toured the components of a id29 system,
we now return to each component and discuss the key design
decisions and possibilities for improvement.

3.1 executor
by describing an executor, we are really describing the lan-
guage of the logical form. a basic textbook representation
of language is    rst-order logic, which can be used to make
quanti   ed statements about relations between objects. for
example,    every prime greater than two is odd.    would be
expressed in    rst-order logic as    x.prime(x)     more(x, 2)    
odd(x). here, the context c is a model (in the model theory
sense), which maps predicates to sets of objects or object
pairs. the execution of the above logical form with respect
to the standard mathematical context would be true.
[7]
gives a detailed account on how    rst-order logic is used for
natural language semantics.

id85 is reasonably powerful, but it fails to cap-
ture some common phenomena in language. for example,
   how many primes are less than 10?     requires construct-
ing a set and manipulating it and thus goes beyond the
power of    rst-order logic. we can instead augment    rst-
order logic with constructs from id198. the log-
ical form corresponding to the above question would be
count(  x.prime(x)    less(x, 10)), where the    operator can
be thought of as constructing a set of all x that satisfy the

condition; in symbols, (cid:74)  x.f (x)(cid:75)c = {x : (cid:74)f (x)(cid:75)c = true}.

note that count is a higher-order functions that takes a
function as an argument.

another logical language, which can be viewed as syntactic
sugar for id198, is lambda dependency-based se-
mantics (dcs) [20]. in lambda dcs, the above logical form
would be count(prime (cid:117) (less.10)), where the constant 10
represent   x.(x = 10), the intersection operator z1 (cid:117) z2 rep-
resents   x.z1(x)    z2(x), and the join operator r.z represents
  x.   y.r(x, y)     z(y).

lambda dcs is    lifted    in the sense that operations combine
functions from objects to truth values (think sets) rather

than truth values. as a result, lambda dcs logical forms
partially eliminate the need for variables. noun phrases in
natural language (e.g.,    prime less than 10    ) also denote
sets. thus, lambda dcs arguably provides a transparent
interface with natural language.

from a linguistic point of view, the logical language seeks
primarily to model natural language phenomena. from an
application point of view, the logical language dictates what
actions we support. it is thus common to use application-
speci   c logical forms, for example, id157 [17].
note that the other components of the framework are largely
independent of the exact logical language used.

3.2 grammar
recall that the goal of the grammar in this article is just to
de   ne a set of candidate derivations for each utterance and
context. note that this is in contrast to a conventional no-
tion of a grammar in linguistics, where the goal is to precisely
characterize the set of valid sentences and interpretations.
this divergence is due to two factors: first, we will learn
a statistical model over the derivations generated by the
grammar anyway, so the grammar can be simple and coarse.
second, we might be interested in application-speci   c logi-
cal forms. in a    ight reservation domain, the logical form
we wish to extract from    i   m in boston and would like to go
to portland     is flight (cid:117) from.boston (cid:117) to.portland, which
is certainly not the full linguistic meaning of the utterance,
but su   ces for the task at hand. note that the connection
here between language and logic is less direct compared to
   prime less than 10         prime (cid:117) (less.10).

id35. one common approach to the grammar in seman-
tic parsing is id35 (id35) [28],
which had been developed extensively in linguistics before
it was    rst used for id29 [37]. id35 is typically
coupled with logical forms in id198. here is an
example of a id35 derivation:

prime

less than

10

n[  x.prime(x)]

(n\n)/np[  y.  f.  x.f (x)     less(x, y) np[10]

n\n[  f.  x.f (x)     less(x, 10)]

(>)

n[  x.prime(x)     less(x, 10)]

(<)

(7)
syntactic categories in id35 include nouns (n) denoting
sets and noun phrases (np) denoting objects. composite
categories ((n\n)/np) represent functions of multiple ar-
guments, but where the directionality of the slashes indicate
the location of the arguments. most rules in id35 are lexical
(e.g., [prime     n[  x.prime(x)]), which mention particular
words. the rest of the rules are glue; for example, we have a
forward application (>) and backward application (<) rule:

(>) a/b[f ] b[x]
(<) b[x]

    a[f (x)]
a\b[f ]     a[f (x)]

language [38], although these issues can also be handled by
having a more expansive lexicon [19].

crude rules. in id35, the lexical rules can become quite
complicated. an alternative approach that appears mostly
in work on lambda dcs is to adopt a much cruder grammar
whose rules correspond directly to the lambda dcs con-
structions, join and intersect. this results in the following
derivation:

less than

prime
10
n[prime] n|n[less] n[10]

(join)

n[less.10]
n[prime (cid:117) less.10]

(intersect)

(8)

(join)
(intersect) n[z1]

n|n[r] n[z]     n[r.z]

n[z2]     n[z1 (cid:117) z2]

the lack of constraints permits the derivation of other logi-
cal forms such as 10 (cid:117) less.prime, but these incorrect logi-
cal forms can be ruled out statistically via the model. the
principle is to keep the lexicon simple and lean more heavily
on features (whose weights are learned from data) to derive
drive the selection of logical forms.

floating rules. while this new lexicon is simpler, the bulk
of the complexity comes from having it in the    rst place.
where does the rules [prime     n|n : prime] and [less than
    n|n : less] come from? we can reduce the lexicon even
further by introducing    oating rules [        n|n : prime] and
[        n|n : less], which can be applied in the absence of
any lexical trigger. this way, the utterance    primes smaller
than 10     would also be able to generate prime (cid:117) less.10,
where the predicates prime and less are not anchored to
any speci   c words.

while this relaxation may seem linguistically blasphemous,
recall that the purpose of the grammar is to merely deliver
a set of logical forms, so    oating rules are quite sensible pro-
vided we can keep the set of logical forms under control. of
course, since the grammar is so    exible, even more nonsen-
sical logical forms are generated, so we must lean heavily
on the features to score the derivations properly. when the
logical forms are simple and we have strong type constraints,
this strategy can be quite e   ective [5, 30, 26].

the choice of grammar is arguably the most important com-
ponent of a semantic parser, as it most directly governs the
tradeo    between expressivity and statistical/computational
complexity. we have explored three grammars, from a tightly-
constrained id35 to a very laissez faire    oating grammar.
id35 is natural for capturing complex linguistic phenomena
in well-structured sentences, whereas for applications where
the utterances are noisy but the logical forms are simple,
more    exible grammars are appropriate.
in practice, one
would probably want to mix and match depending on the
domain.

it is common to use other combinators which both handle
more complex linguistic phenomena such as np coordina-
tion    integers that divide 2 or 3     as well as ungrammatical

3.3 model
at the core of a statistical model is a function score(x, c, d)
that judges how good a derivation d is with respect to the

utterance x and context c. in section 2, we described a sim-
ple log-linear model (2) in which score(x, c, d) =   (x, c, d)     ,
and with simple rule features. there are two ways to im-
prove the model: use more expressive features and and use
a non-linear scoring function.

this causes the parser to waste equal resources on non-
promising spans which are unlikely to participate in the    nal
derivation that is chosen. this motivates agenda-based pars-
ing, in which derivations that are deemed more promising
by the model are built    rst [6].

most existing semantic parsers stick with a linear model and
leverage more targeted features. one simple class of features
{  a,b} is equal to the number of times word a appears in the
utterance and predicate b occurs in the logical form; if the
predicate b is generated by a    oating rule, then a is allowed
to range over the entire utterance; otherwise, it must appear
in the span of b.

the above features are quite numerous, which provides    ex-
ibility but require a lot of data. note that logical predicates
(e.g., birthplace) and the natural language (e.g., born in)
often share a common vocabulary (english words). one can
leverage this structure by de   ning a few features based on
statistics from large corpora, external lexical resources, or
simply string overlap. for example,   match might be the
number of word-predicate pairs with that match almost ex-
actly. this way, the lexical mapping need not be learned
from scratch using only the training data [4, 18].

a second class of useful features are based on the denota-

tion(cid:74)d.z(cid:75)c of the predicted logical form. for example, in a

robot control setting, a feature would encode whether the
predicted actions are valid in the environment c (picking up
a cup requires the cup to be present). these features make
it amply clear that id29 is not simply a natu-
ral language problem but one that involves jointly reasoning
about the non-linguistic world. in addition, we typically in-
clude type constraints (features with weight       ) to prune
out ill-formed logical forms such as birthplace.4 that are
licensed the grammar.

linear basis functions: score(x, c, d) =(cid:80)m

one could also adopt a non-linear scoring function, which
does not require any domain knowledge about semantic pars-
ing. for example, one could use a simple one-layer neu-
ral network, which takes a weighted combination of m non-
i=1   i tanh(  (x, c, d)  
wi). the parameters of the model that we would learn are
then the m top-level weights {  i} and the mf bottom-
level weights {wij}; recall that f is the number of features
(  (x, c, d)     rf ). with more parameters, the model be-
comes more powerful, but also requires more data to learn.

3.4 parsing
most id29 algorithms are based on chart pars-
ing, where we recursively generate a set of candidate deriva-
tions for each syntactic category (e.g., np) and span (e.g.,
[3 : 5]). there are two disadvantages of chart parsing: first,
it does not easily support incremental contextual interpre-
tation: the features of a span [i : j] can only depend on
the sub-derivations in that span, not on the derivations con-
structed before i. this makes anaphora (resolution of    it   )
di   cult to model. a solution is to use id132
rather than chart parsing [40]. here, one parses the utter-
ance from left to right, and new sub-derivations can depend
arbitrarily on the sub-derivations constructed thus far.

a second problem is that the chart parsing generally builds
derivations in some    xed order   e.g., of increasing span size.

3.5 learner
in learning, we are given examples of utterance-context-
response triples (x, c, y). there are two aspects of learning:
inducing the grammar rules and estimating the model pa-
rameters. it is important to remember that practical seman-
tic parsers do not do everything from scratch, and often the
hard-coded grammar rules are as important as the training
examples. first, some lexical rules that map named entities
(e.g., [paris     parisfrance]), dates, and numbers are gen-
erally assumed to be given [37], though we need not assume
that these rules are perfect [21]. these rules are also often
represented implicitly [21, 4].

how the rest of the grammar is handled varies across ap-
proaches. in id35-style approach, inducing lexical rules is
an important part of learning. in [37], a procedure called
genlex is used to generate candidate lexical rules from a
utterance-logical form pair (x, z). a more generic induction
algorithm based on higher-order uni   cation does not require
any initial grammar [19]. [33] use machine translation ideas
to induce a synchronous grammar (which can also be used to
generate utterances from logical forms). however, all these
lexicon induction methods require annotated logical forms
z. in approaches that learn from denotations y [21, 4], an
initial crude grammar is used to generate candidate logical
forms, and rest of the work is done by the features.

as we discussed earlier, parameter estimation can be per-
formed by stochastic id119 on the log-likelihood;
similar objectives based on max-margin are also possible
[22]. it can be helpful to also add an l1 id173 term
  (cid:107)  (cid:107)1, which encourages feature weights to be zero, which
produces a more compact model that generalizes better [5].
in addition, one can use adagrad [14], which maintains a
separate step size for each feature. this can improve stabil-
ity and convergence.

4. datasets and results
in a strong sense, datasets are the main driver of progress
for statistical approaches. we will now survey some of the
existing datasets, describe their properties, and discuss the
state-of-the-art.

the geo880 dataset [36] drove nearly a decade of seman-
tic parsing research. this dataset consists of 880 questions
and database queries about us geography (e.g.,    what is
the highest point in the largest state?    ). the utterances are
compositional, but the language is clean and the domain is
limited. on this dataset, learning from logical forms [18]
and answers [21] both achieve around 90%

the atis-3 dataset [38] consists of 5418 utterances paired
with logical forms (e.g.,    show me information on ameri-
can airlines from fort worth texas to philadelphia   ). the
utterances contain more dis   uencies and    exible word order
compared with geo880, but they are logically simpler. as a
result, slot    lling methods have been a successful paradigm
in the spoken language understanding community for this

domain since the 1990s. the best reported result is based
on id29 and obtains 84.6%

the regexp824 dataset [17] consists of 824 natural language
and regular expression pairs (e.g.,    three letter word starting
with    x       ). the main challenge here is that there are many
logically equivalent id157, some aligning bet-
ter to the natural language than others. [17] uses semantic
uni   cation to test for logical form equivalence and obtains
65.6%

the free917 dataset [11] consists of 917 examples of question-
logical form pairs that can be answered via freebase, e.g.
   how many works did mozart dedicate to joseph haydn?    
the questions are logically less complex than those in the
id29 datasets above, but introduces the new
challenge of scaling up to many more predicates (but is in
practice manageable by assuming perfect named entity res-
olution and leveraging the strong type constraints in free-
base). the state-of-the-art accuracy is 68%

webquestions [4] is another dataset on freebase consist-
ing of 5810 question-answer pairs (no logical forms) such
as    what do australia call their money?     like free917, the
questions are not very compositional, but unlike free917,
they are real questions asked by people on the web inde-
pendent from freebase, so they are more realistic and more
varied. because the answers are required to come from a
single freebase page, a noticeable fraction of the answers
are imperfect. the current state-of-the-art is 52.5%

the goal of wikitablequestions [26] is to extend question
answering beyond freebase to html tables on wikipedia,
which are semi-structured. the dataset consists of 22033
question-table-answer triples (e.g.,    how many runners took
2 minutes at the most to run 1500 meters?    ), where each
question can be answered by aggregating information across
the table. at test time, we are given new tables, so methods
must learn how to generalize to new predicates. the result
on this new dataset is 37.1%

[30] proposed a new recipe for quickly using id104 to
generate new compositional id29 datasets con-
sisting of question-logical form pairs. using this recipe, they
created eight new datasets in small domains consisting of
12602 total question-answer pairs, and achieved an average
accuracy on across datasets of 58.8%

[12] introduced a dataset of 706 navigation instructions (e.g.,
   facing the lamp go until you reach a chair    ) in a simple
grid world. each instruction sequence contains multiple sen-
tences with various imperative and context-dependent con-
structions not found in previous datasets. [2] obtained 65.3%

5. discussion
we have presented a id29 framework for the
problem of natural language understanding. going forward,
the two big questions are (i) how to represent the semantics
of language and (ii) what supervision to use to learn the
semantics from.

alternative semantic representations. one of the main
di   culties with id29 is the divergence between
the structure of the natural language and the logical forms   

purely id152 will not work. this has led
to some e   orts to introduce an intermediate layer between
utterances and logical forms. one idea is to use general para-
phrasing models to map input utterances to the    canonical
utterances    of logical forms [5, 30]. this reduces semantic
parsing to a text-only problem for which there is much more
data and resources.

one could also use domain-general logical forms that capture
the basic predicate-argument structures of sentences [18].
id15 (amr) [3] is one popular
representation backed by an extension linguistic annotation
e   ort. multiple amr parsers have been developed, includ-
ing one based on id35 [1]. while these representations of-
fer broad coverage, solving downstream understanding tasks
still require additional work (e.g., inferring that    population
of x     is synonymous with    number of people living in x    ).
in contrast, executable id29 operates on the full
pipeline from utterance to task output, but compromises on
domain-generality. this is partially inevitable, as any un-
derstanding task requires some domain-speci   c knowledge
or grounding. designing the best general representation
that supports many downstream tasks remains an open chal-
lenge.

alternative supervision. much of the progress in seman-
tic parsing has been due to being able to learn from weaker
supervision. in the framework we presented, this supervi-
sion are the desired actions y (e.g., answers to questions).
one can use a large corpus of text to exploit even weaker
supervision [16, 27]. more generally, one can think about
language interpretation in a id23 setting
[9], where an agent who presented with an utterance in some
context performs some action, and receives a corresponding
reward signal. this framework highlights the importance of
context-dependence in language interpretation [39, 2].

due to their empirical success, there has been a recent surge
of interest in using recurrent neural networks and their ex-
tensions for solving nlp tasks such as machine translation
and id53 [35, 31]. these methods share the
same spirit of end-to-end utterance-to-behavior learning as
executable id29, but they do not explicitly sep-
arate parsing from execution. this makes them architec-
turally simpler than semantic parsers, but they are more
data hungry and it is unclear whether they can learn to
perform complex logical reasoning in a generalizable way.
nonetheless, it is quite likely that these methods will play
an important role in the story of language understanding.

outlook. this is an exciting time for id29
and natural language understanding. as natural language
interfaces (e.g., siri) become more ubiquitous, the demand
for deep understanding will continue to grow. at the same
time, there is a fresh wave of ambition pushing the limits of
what can be machine learnable. the con   uence of these two
factors will likely generate both end-user impact as well as
new insights into the nature of language and learning.

6. references
[1] y. artzi and k. l. l. zettlemoyer. broad-coverage id35

id29 with amr. in empirical methods in
natural language processing (emnlp), 2015.

[2] y. artzi and l. zettlemoyer. weakly supervised learning of

semantic parsers for mapping instructions to actions.

transactions of the association for computational
linguistics (tacl), 1:49   62, 2013.

[3] l. banarescu, c. b. s. cai, m. georgescu, k. gri   tt,

u. hermjakob, k. knight, p. koehn, m. palmer, and
n. schneider. id15 for
sembanking. in 7th linguistic annotation workshop and
interoperability with discourse, 2013.

[4] j. berant, a. chou, r. frostig, and p. liang. semantic

parsing on freebase from question-answer pairs. in
empirical methods in natural language processing
(emnlp), 2013.

[5] j. berant and p. liang. id29 via id141.
in association for computational linguistics (acl), 2014.
[6] j. berant and p. liang. imitation learning of agenda-based

semantic parsers. transactions of the association for
computational linguistics (tacl), 0, 2015.

[7] p. blackburn and j. bos. representation and id136 for

natural language: a first course in computational
semantics. csli publishers, 2005.

[8] k. bollacker, c. evans, p. paritosh, t. sturge, and
j. taylor. freebase: a collaboratively created graph
database for structuring human knowledge. in
international conference on management of data
(sigmod), pages 1247   1250, 2008.

[9] s. branavan, h. chen, l. s. zettlemoyer, and r. barzilay.

id23 for mapping instructions to actions.
in association for computational linguistics and
international joint conference on natural language
processing (acl-ijcnlp), pages 82   90, 2009.

[10] e. brill, s. dumais, and m. banko. an analysis of the

askmsr question-answering system. in association for
computational linguistics (acl), pages 257   264, 2002.

[11] q. cai and a. yates. large-scale id29 via

schema matching and lexicon extension. in association for
computational linguistics (acl), 2013.

[12] d. l. chen and r. j. mooney. learning to interpret natural

language navigation instructions from observations. in
association for the advancement of arti   cial intelligence
(aaai), pages 859   865, 2011.

[13] j. clarke, d. goldwasser, m. chang, and d. roth. driving

id29 from the world   s response. in
computational natural language learning (conll),
pages 18   27, 2010.

[14] j. duchi, e. hazan, and y. singer. adaptive subgradient

methods for online learning and stochastic optimization. in
conference on learning theory (colt), 2010.

[15] j. krishnamurthy and t. kollar. jointly learning to parse
and perceive: connecting natural language to the physical
world. transactions of the association for computational
linguistics (tacl), 1:193   206, 2013.

[16] j. krishnamurthy and t. mitchell. weakly supervised
training of semantic parsers. in empirical methods in
natural language processing and computational natural
language learning (emnlp/conll), pages 754   765,
2012.

[17] n. kushman and r. barzilay. using semantic uni   cation to

generate id157 from natural language. in
human language technology and north american
association for computational linguistics (hlt/naacl),
pages 826   836, 2013.

[18] t. kwiatkowski, e. choi, y. artzi, and l. zettlemoyer.

scaling semantic parsers with on-the-   y ontology matching.
in empirical methods in natural language processing
(emnlp), 2013.

[19] t. kwiatkowski, l. zettlemoyer, s. goldwater, and

m. steedman. inducing probabilistic id35 grammars from
logical form with higher-order uni   cation. in empirical
methods in natural language processing (emnlp), pages
1223   1233, 2010.

[20] p. liang. lambda dependency-based compositional

semantics. arxiv, 2013.

[21] p. liang, m. i. jordan, and d. klein. learning

dependency-based id152. in association

for computational linguistics (acl), pages 590   599, 2011.

[22] p. liang and c. potts. bringing machine learning and
id152 together. annual reviews of
linguistics, 1(1):355   376, 2015.

[23] c. matuszek, n. fitzgerald, l. zettlemoyer, l. bo, and

d. fox. a joint model of language and perception for
grounded attribute learning. in international conference
on machine learning (icml), pages 1671   1678, 2012.

[24] s. miller, d. stallard, r. bobrow, and r. schwartz. a fully

statistical approach to id139. in
association for computational linguistics (acl), pages
55   61, 1996.

[25] r. montague. the proper treatment of quanti   cation in

ordinary english. in approaches to natural language,
pages 221   242, 1973.

[26] p. pasupat and p. liang. compositional id29

on semi-structured tables. in association for
computational linguistics (acl), 2015.

[27] s. reddy, m. lapata, and m. steedman. large-scale

id29 without question-answer pairs.
transactions of the association for computational
linguistics (tacl), 2(10):377   392, 2014.

[28] m. steedman. the syntactic process. mit press, 2000.
[29] s. tellex, t. kollar, s. dickerson, m. r. walter, a. g.

banerjee, s. j. teller, and n. roy. understanding natural
language commands for robotic navigation and mobile
manipulation. in association for the advancement of
arti   cial intelligence (aaai), 2011.

[30] y. wang, j. berant, and p. liang. building a semantic

parser overnight. in association for computational
linguistics (acl), 2015.

[31] j. weston, s. chopra, and a. bordes. memory networks. in

international conference on learning representations
(iclr), 2015.

[32] t. winograd. understanding natural language. academic

press, 1972.

[33] y. w. wong and r. j. mooney. learning synchronous

grammars for id29 with id198. in
association for computational linguistics (acl), pages
960   967, 2007.

[34] w. a. woods, r. m. kaplan, and b. n. webber. the lunar
sciences natural language information system: final report.
technical report, bbn report 2378, bolt beranek and
newman inc., 1972.

[35] w. yih, m. chang, x. he, and j. gao. id29

via staged query graph generation: id53
with knowledge base. in association for computational
linguistics (acl), 2015.

[36] m. zelle and r. j. mooney. learning to parse database

queries using inductive logic programming. in association
for the advancement of arti   cial intelligence (aaai),
pages 1050   1055, 1996.

[37] l. s. zettlemoyer and m. collins. learning to map

sentences to logical form: structured classi   cation with
probabilistic categorial grammars. in uncertainty in
arti   cial intelligence (uai), pages 658   666, 2005.

[38] l. s. zettlemoyer and m. collins. online learning of

relaxed id35 grammars for parsing to logical form. in
empirical methods in natural language processing and
computational natural language learning
(emnlp/conll), pages 678   687, 2007.

[39] l. s. zettlemoyer and m. collins. learning

context-dependent mappings from sentences to logical form.
in association for computational linguistics and
international joint conference on natural language
processing (acl-ijcnlp), 2009.

[40] k. zhao and l. huang. type-driven incremental semantic

parsing with polymorphism. in north american
association for computational linguistics (naacl), 2015.

