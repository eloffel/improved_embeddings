conversational markers of constructive discussions

vlad niculae and cristian danescu-niculescu-mizil

{vlad|cristian}@cs.cornell.edu

cornell university

6
1
0
2

 
r
p
a
5
2

 

 
 
]
l
c
.
s
c
[
 
 

1
v
7
0
4
7
0

.

4
0
6
1
:
v
i
x
r
a

abstract

group discussions are essential for organiz-
ing every aspect of modern life, from faculty
meetings to senate debates, from grant review
panels to papal conclaves. while costly in
terms of time and organization effort, group
discussions are commonly seen as a way of
reaching better decisions compared to solu-
tions that do not require coordination between
the individuals (e.g. voting)   through discus-
sion, the sum becomes greater than the parts.
however, this assumption is not irrefutable:
anecdotal evidence of wasteful discussions
abounds, and in our own experiments we    nd
that over 30% of discussions are unproductive.
we propose a framework for analyzing con-
versational dynamics in order to determine
whether a given task-oriented discussion is
worth having or not. we exploit conversa-
tional patterns re   ecting the    ow of ideas and
the balance between the participants, as well
as their linguistic choices. we apply this
framework to conversations naturally occur-
ring in an online collaborative world explo-
ration game developed and deployed to sup-
port this research. using this setting, we show
that linguistic cues and conversational patterns
extracted from the    rst 20 seconds of a team
discussion are predictive of whether it will be
a wasteful or a productive one.

1

introduction

working in teams is a common strategy for decision
making and problem solving, as building on effec-
tive social interaction and on the abilities of each
member can enable a team to outperform lone in-
dividuals. evidence shows that teams often per-
form better than individuals (williams and stern-
berg, 1988) and even have high chances of reaching

correct answers when all team members were pre-
viously wrong (laughlin and adamopoulos, 1980).
furthermore, team performance is not a factor of in-
dividual intelligence, but of collective intelligence
(woolley et al., 2010), with interpersonal interac-
tions and emotional intelligence playing an impor-
tant role (jordan et al., 2002).

yet, as most people can attest from experience,
team interaction is not always smooth, and poor co-
ordination can lead to unproductive meetings and
wasted time. in fact, romano jr and nunamaker jr
(2001) report that one third of work-related meet-
ings in the u. s. are considered unproductive, while
a 2005 microsoft employee survey reports that 69%
of meetings are ineffective.1 as such, many grow
cynical of meetings.

computational methods with the ability to reli-
ably recognize unproductive discussions could have
an important impact on our society.
ideally, such
a system could provide actionable information as a
discussion progresses, indicating whether it is likely
to turn out to be productive, rather than a waste of
time. in this paper we focus on the conversational
aspects of productive interactions and take the fol-
lowing steps:

    introduce a constructiveness framework that al-
lows us to characterize teams where discussion
enables better performance than the individu-
als could reach, and, conversely, teams better
off not having discussed at all (section 3);

    create a setting that is conducive to decision-
making discussions, where all steps of the
process (e.g., individual answers, intermedi-
ate guesses) are observable to researchers: the
streetcrowd game (sections 4   5);

1money.id98.com/2005/03/16/technology/survey/

    develop a novel framework for conversational
analysis in small group discussions, studying
aspects such as the    ow of ideas, conversational
dynamics, and group balance (sections 6   7).

we reveal differences in the collective decision
process characteristic of productive and unproduc-
tive teams, and show that these differences are re-
   ected in their conversational patterns. for exam-
ple, the language used when new ideas are intro-
duced and adopted encodes important discrimina-
tive cues. measures of interactional balance and
language matching (niederhoffer and pennebaker,
2002; danescu-niculescu-mizil et al., 2011) also
prove to be informative, suggesting that more bal-
anced discussions are most productive. our results
underline the potential held by computational ap-
proaches to conversational dynamics. to encour-
age further work in this direction, we render our
dataset of task-oriented discussions and our feature-
extraction code publicly available.2

2 related work

existing computational work on task-oriented group
interaction is largely focused on how well the team
performs. coetzee et al. (2015) deployed and stud-
ied the impact of a chat-based team interaction plat-
form in massive open online courses,    nding that
teams reach more correct answers than individuals,
and that the experience is more enjoyable. one of-
ten studied experimental setting is the hcrc map
task corpus (anderson et al., 1991), consisting of
128 conversations between pairs of people, where a
designated one gives directions to the other. this
simpli   ed setting avoids issues like role establish-
ment and leadership. reitter and moore (2007)    nd
that successful dialogs are characterized by long-
term adaptation and alignment of linguistic struc-
tures at syntactic, lexical and character level. a
notable feature of this work is the success predic-
tion task attempted using only the    rst 5 minutes
of conversation. other attempts use authority level
features inspired from negotiation theory, experi-
mental meta-features, task-speci   c features (may-
   eld et al., 2011), and sociolinguistic spelling differ-
ences (may   eld et al., 2012). another research path

2https://vene.ro/constructive/

uses negotiation tasks from the inspire dataset (ker-
sten and zhang, 2003), a collection of 1525 online
bilateral negotiations where roles are    xed (buyer
and seller) and success is de   ned by the sale going
through. sokolova et al. (2008) use a bag-of-words
model and investigate the importance of temporal
aspects. sokolova and lapalme (2012) measure in-
formativeness, quanti   ed by lexical sets of degrees,
scalars and comparatives.

research on success in groups with more than two
members is less common. friedberg et al. (2012)
model the grades of 27 group assignments from a
class using measures of average entrainment,    nding
task-speci   c words to be a strong cue. jung (2011)
shows how the affective balance expressed in teams
correlates with performance on engineering tasks,
in 30 teams of up to 4 students. in a related study
the balance in the    rst 5 minutes of an interaction is
found predictive of performance (jung et al., 2012).
none of the research we are aware of controls for
initial skill or potential of the team members.

in management science, network analysis reveals
that certain subgraphs found in long-term, structured
teams indicate better performance, as rated by se-
nior managers (cummings and cross, 2003); con-
trolled experiments show that optimal structures de-
pend on the complexity of the task (guetzkow and
simon, 1955; bavelas, 1950). these studies, as well
as much of the research on effective team crowd-
sourcing (lasecki et al., 2012; wang et al., 2011,
inter alia), do not focus on linguistic and conversa-
tional factors.

3 constructive discussions

the    rst hurdle is to reliably quantify how produc-
tive group conversations are.
in problem-solving,
the ultimate goal is to    nd the correct answer, or,
failing that, to come as close to it as possible. to
quantify closeness to the correct answer, a score
is often used, such that better guesses get higher
scores; for example, school grades.

in contrast, our goal is to measure how produc-
tive a team   s interaction is. scores are measures of
correctness, so using them as a proxy for interaction
quality is not ideal: a team of straight a students can
manage to get an a on a project without exchang-
ing ideas, while a group of d students getting a b is

team member
that perform better than the best
(cbest > 0) and worse than the worst member
(cworst < 0), where:

cbest = score(t)     max
cworst = score(t)     min

i

score(gi)

score(gi).

i

one way to think of the extreme cases is to imag-
ine a team supervisor that collects the individual an-
swers and aggregates them, without any external in-
formation. an oracle supervisor can do no better
than choosing the best answer. the discussion and
interaction of teams where cbest > 0 leads to a better
answer than such an oracle could achieve. (one such
scenario is illustrated by the dashed light green cir-
cle in figure 1.) similarly, teams where cworst < 0
waste their time completely, as simply picking one
of their members    answers at random is guaranteed
to do better. (the dashed red circle in figure 1 illus-
trates this scenario.)

the most important aspect of the constructive-
ness framework, in contrast to traditional measures
of correctness or success, is that all constructiveness
measures are designed to control for initial perfor-
mance or potential of the team members, in order to
focus on the effect of the discussion.4

in settings of importance, the true answer is not
known a priori, and this constructiveness cannot
be calculated directly. we therefore seek out to
model constructiveness using observable conversa-
tional and linguistic correlates (sections 6   7). to
develop such a model, we design a large-scale exper-
imental setting where the true answer is available to
researchers, but unknown by the players (section 4).

4 experimental setting
4.1 streetcrowd
in order to study the constructiveness of task-
oriented group discussion, we need a setting that is
conducive to decision-making discussions, where all
steps of the process (individual answers, intermedi-
ate guesses, group discussions and decisions) are ob-
servable. furthermore, to study at scale, we need to
4due to its relative nature, constructiveness also accounts
for variation in task dif   culty in most scenarios. for example,
in terms of cworst, when a team cannot even match its worst per-
forming member, this is a sign of poor team interaction even if
the task is particularly challenging.

figure 1: intuitive sketch for constructiveness. the
solid green circle corresponds a team guess follow-
ing a constructive discussion (cavg > 0), the dashed
green circle corresponds to the scenario of a team
that outperforms its best member (cbest > 0), while
the dashed red circle corresponds to a team that un-
derperforms its worst member (cworst < 0).

more interesting. in the latter case, the team   s im-
proved performance is likely to come from a good
discussion and an ef   cient exchange of complemen-
tary ideas   making the sum greater than the parts.

to capture this intuition we say a team discussion
is constructive if it results in an improvement over
the potential of the individuals. we can then quan-
tify the degree of constructiveness cavg as the im-
provement of the team score t over the mean of the
initial scores gi of the n individuals in the team:

cavg = score(t)    

i=1 score(gi)

.

n

the higher cavg is, the more the team   s answer, af-
ter discussion, improves upon the individuals    aver-
age performance before discussion; zero construc-
tiveness (cavg = 0) means the team performed no
better than its members did before discussing, while
negative constructiveness (cavg < 0) corresponds
to non-constructive discussions.3 figure 1 sketches
the idea visually: the dark green circle corresponds
to the team   s score after a constructive discussion
(cavg > 0), being above the average individual score.
since individuals answers can sometimes vary
widely, we also consider the extreme cases of teams
3from an operational perspective, a team can choose, in-
stead of having a discussion, to aggregate individual guesses,
e.g., by majority voting or averaging. non-constructive dis-
cussions roughly correspond to cases where such an aggregate
guess would actually be better than what the team discussion
would accomplish.

(cid:80)n

g1g2g3g4true answerscore(g1)(best individual guess)avg[ score(gi) ](mean score ofindividual guesses)tttdiscussion startdiscussion endtimescorescore ofteam guesscavg(constructiveness)with these constraints

   nd a class of complex tasks with known solutions
that can be automatically generated, but that cannot
be easily solved by simply querying search engines.
in mind, we built
streetcrowd, an online multi-player world explo-
ration game.5 streetcrowd is played in teams of at
least two players and is built around a geographic
puzzle: determining your location based on    rst-
person images from the ground level.6 each location
generates a new puzzle.
solo phase. each player has 3 minutes to navi-
gate the surroundings, explore, and try to    nd clues.
this happens independently and without communi-
cating. at the end, the player is asked to make a
guess by placing a marker on the world   s map, and
is prompted for an explanation and for a con   dence
level. the answer is not yet revealed.
team phase. the team must then decide on a single,
common guess. to accomplish this, all teammates
are placed in a chatroom and are provided with a
map and a shared marker. any player can move the
marker at any point during the discussion. the game
ends when all players agree on the answer, or when
the time limit is reached. an example discussion is
given in figure 4.

guesses are scored according to their distance to
the true location using the spherical law of cosines:

score(guess, true) =    rd(guess, true)

where d is the arc distance on a sphere, and r de-
notes the radius of the earth, assumed spherical. the
score is given by the negative distance in kilometers,
such that higher means better. to motivate players
and emphasize collaboration, the main streetcrowd
page displays a leaderboard consisting of the best
team players.

the key aspects of the streetcrowd design are:
    the puzzles are complex and can be generated

automatically in large numbers;

    the true answers are known to researchers, but
hard to obtain without solving the puzzle, al-
lowing for objective evaluation of both individ-
ual and group performance;

5http://streetcrowd.us/start
(the experiment was approved by the irb).
6we embed google street view data.

    scoring is continuous rather than discrete, al-
lowing us to quantify degrees of improvement
and capture incremental effects;

    each teammate has a different solo phase ex-
perience and background knowledge, making it
possible for the group discussion to shed light
on new ideas;

    the puzzles are engaging and naturally con-
ducive to collaboration, avoiding the use of
monetary incentives that can bias behavior.

4.2 preprocessing
in the    rst 8 months, over 1400 distinct players par-
ticipated in over 2800 streetcrowd games. we tok-
enize and part-of-speech tag the conversations.7 be-
fore analysis, due to the public nature of the game,
we perform several    ltering and quality check steps.
discarding trivial games. we remove all games
that the developers took part in. we    lter games
where the team fails to provide a guess, where fewer
than two team members engage in the team chat, and
puzzles with insuf   cient samples.
preventing and detecting cheating.
the
streetcrowd tutorial asks players to avoid us-
ing external resources to look up clues and get an
unfair advantage. to prevent cheating, we detect
and block chat messages that link to websites, and
we employ cookies and user accounts to prevent
people from playing the same puzzle multiple
times. to identify games that slip through this net,
we    ag cases where the team, or any individual
player, guesses within 10 km of the correct answer,
and leaves the window while playing. we further
remove a small set of games where the players
confess to cheating in the chat.

after    ltering, our dataset consists of 1450 games
on 70 different puzzles, with an average of 3.9
games per unique player, and 12.1 messages and
64.5 words in an average conversation.

5 constructiveness in streetcrowd

we    nd that, indeed, most of the games are con-
structive. there are, however, 32% non-constructive

7we use the tweetnlp toolkit (owoputi et al., 2013) with a
tag set developed for twitter data. manual examination reveals
this approach to be well suited for online chat data.

figure 2: distribution of team constructiveness.

games (cavg < 0); this re   ects very closely the sur-
vey by romano jr and nunamaker jr (2001).
in-
terestingly, in 36% of games, the team arrives at
a better answer than any of the individual guesses
(cbest > 0). the    ip side is also remarkably com-
mon, with 17% of teams performing even worse than
the worst individual (cworst < 0). the distribution of
constructiveness is shown in figure 2: the fat tails
indicate that cases of large improvements and large
deterioration are not uncommon.

collective decision process. due to the full instru-
mentation of the game interface, we can investigate
how constructiveness emerges out of the team   s in-
teraction. the team   s intermediate guesses during
discussion con   rm that a meaningful process leads
to the    nal team decision: guesses get closer and
closer to the    nal submitted guess (figure 3a); in
other words, the team converges to their    nal guess.

notably, when considering how correct the in-
termediate guesses are, we notice an important
difference between the way constructive and non-
constructive teams converge to their    nal guess (fig-
ure 3b). during their collaborative decision pro-
cess, constructive teams make guesses that get closer
and closer to the correct answer; in contrast, non-
constructive teams make guesses that take them far-
ther from the correct answer. this observation has
two important consequences. first, it shows that
the two types of teams behave differently through-
out, suggesting we could potentially detect non-
constructive discussions early on, using interaction
patterns. second, it emphasizes the potential practi-
cal value of such a task: stopping a non-constructive
team early could lead to a better answer than if they
would carry on.

(a) distance between the last three intermediate

guesses and the    nal guess.

(b) score of the    rst three intermediate guesses; the
mean score of the initial individual guesses and the
score of the    nal team guess are shown for reference.

figure 3: intermediate guesses offer a glimpse at the
decision process: (a) guesses converge rather than
zig-zag (b) in constructive games, guesses get in-
crementally better than the mean individual score,
while in non-constructive games, they get worse.
(games with     3 intermediate guesses.)

6 conversation analysis

the process of team convergence revealed in the pre-
vious section suggests a relation between the inter-
action leading to the    nal group decision and the rel-
ative quality of the outcome. in this section, we de-
velop a conversation analysis framework aimed at
characterizing this relation. this framework relies
on conversational patterns and linguistic features,
while steering away from lexicalized cues that might
not generalize well beyond our experimental setting.
to enable reproducibility, we make available the fea-
ture extraction code and the hand-crafted resources
on which it relies.8

8https://vene.ro/constructive/

15000100005000050001000015000constructiveness (improvement in km)050100150200250300350400450number of teams(++) cbest>0(+)   cavg>0(   )  cworst0( )    cavg0 3 2 1finalintermediate guess0.00.51.01.52.02.5distance  (x1000 km)indiv.123finalintermediate guess 10 8 6 4 20score (x1000 km)constructivenot constr.sation structure use unsupervised probabilistic mod-
els (ritter et al., 2010; elsner and charniak, 2010).
since streetcrowd conversations are short and fo-
cused, the adoption    lter is suf   cient to accurately
capture what ideas are being discussed; a manual
examination of the ideas reveals almost exclusively
place names and words such as    ag, sign, road   
highly relevant clues in the context of streetcrowd.
in figure 4, three ideas are adopted: china, build-
ings and shanghai. the only idea adopted by all
players is buildings, a good signal that this was the
most important clue. a notable limitation is that this
approach cannot capture the connections between
shanghai and china, or buildings and apartments.
further work is needed to robustly capture such vari-
ations in idea    ow, as they could reveal trajectories
(discussion getting more speci   c or more vague) or
lexical choice disagreement.

balance in idea contributions between the team
members is a good indicator of productive discus-
sions.
in particular, in the best teams (the ones
that outperform the best player, i.e., cbest > 0) the
most idea-proli   c player introduces fewer ideas, on
average, than in the rest of the games (figure 5a,
p = 0.01).9 in figure 4, e is the most proli   c player
and only introduces two ideas. to further capture the
balance in contribution between the team members,
we use the id178 of the number ideas introduced
by each player. we also count the number of ideas
adopted unanimously as an indicator of convergence
in the conversation.

in terms of the overall number of ideas dis-
cussed, both the best teams (the ones that outper-
form the best player) and the worst teams (the ones
that perform worse than the worst player) discuss
fewer ideas than the rest (figure 5b, p = 0.006).
indeed, an ideal interaction would avoid distract-
ing ideas, but in teams with communication break-
downs, members might fail to adequately discuss the
ideas that led them to their individual guesses.

the language used to introduce new ideas can
indicate con   dence or hesitation;
in figure 4, a
hedge (would) is used when introducing the build-
ings cue. we    nd that, in teams that outperform
the best player, ideas are less likely to be accom-

9all p-values reported reported in this section are based on

one-sided mann-whitney rank statistical signi   cance tests.

figure 4: example (constructive) conversation and
the corresponding    ow of ideas.
idea mentions
are in bold, and relevant word classess are un-
derlined. arrow colors map to introducer   adopter
pairs, matching the edges in the top-right graph.

idea    ow

6.1
task-oriented discussions are the the primary way of
exchanging ideas and opinions between the group
members; some are quickly discarded while others
prove useful to the    nal guess. the arrows in fig-
ure 4 show how ideas are introduced and discussed
in that example conversation. we attempt to cap-
ture the shape in which the ideas    ow in the discus-
sion. in particular, we are interested in how many
ideas are discussed, how widely they are adopted,
who tends to introduce them, and how.

we consider as candidate ideas all nouns, proper
nouns, adjectives and verbs that are not stopwords.
as soon as a candidate idea introduced by a player is
adopted by another, we count it. henceforth, we   ll
refer to such adopted ideas simply as ideas. in gen-
eral chat domains, state-of-the-art models of conver-

j:  heye:  heyl:  based on the buildings i would say chinae:  what do you guys think?      yeah, samej:  i feel like it is somewhere in south east asiae:  in shanghai the buildings all look like that      but shanghai is too densely urban      this is definitely somewhere on the outskirts of the cityl:  yeape:  any other ideas?j:  is there a place more rural with that kind of buildings?e:  haha china is huge      i couldn't guessj:  for sure it is in asia, but i don't know more...l:  let s pick in the subburbs of shanghaie:  yeahj:  alright, let's go with this.j:  tropical setting + some asia      writing signse:  lots of similar looking apartments -      like outskirts of urban chinal:  (none given)reasonschatidea  flowjel121(a) ideas by most
proli   c player

(b) ideas adopted

(c) hedged idea
introductions

(d) hedged idea

adoptions

(e) time between
turns (seconds)

(g) message id178

(f) guessing id178

(j) ratio of words
related to geography
figure 5: averages for some of the predictive features, based on idea    ow (a-d), balance (e-i), and lexicons
(j). error bars denote standard errors. legend: (++): teams that do better than their best member (n = 525),
(+): constructive (n = 986), (-): non-constructive (n = 464), (- -): worse than worst member (n = 248).

(i) overall pos
bigram matching

(h) max-pair content

word matching

panied by hedge words when introduced (figure 5c,
p < 10   4), showing less hesitation. furthermore,
the level of con   dence used when players adopt oth-
ers    ideas is also informative (figure 5d).
inter-
estingly, overall occurrences of certainty and hedge
words (detailed in section 6.3) are not predictive,
suggesting that ideas are good selectors for impor-
tant discussion segments.

interaction dynamics

6.2
balance. interpersonal balance has been shown to
be predictive of team performance (jung, 2011; jung
et al., 2012) and, similarly, forms of linguistic bal-
ance have been shown to characterize stable rela-
tionships (niculae et al., 2015). here we focus on
balance in contributions to the discussion and the de-
cision process. in search of measures applicable to
teams of arbitrary sizes, we use binary indicators of
whether all players participate in the discussion and
in moving the marker, as well as whether at least
two players move the marker. to measure team bal-
ance with respect to continuous user-level features,
we use the id178 of these features:

balance(s) =    (cid:88)

  s log|s|   s,

  s   s

where, for a given feature, s is the set of its val-
ues for each user, normalized to sum to 1. for in-
stance, the chat message id178 is 1 if everybody
chats equally, and decreases toward 0 as one or more

players dominate. we use the id178 of the num-
ber of messages, words per message, and number of
intermediate guesses. in teams that outperform the
best player, users take turns controlling the marker
more uniformly (figure 5f, p = 0.006), adding fur-
ther evidence that well-balanced teams perform best.
language matching. we investigate matching at
stopword, content word, and pos tag bigram level:
the stopword matching at a turn is given by the num-
ber of stopwords from the earlier message repeated
in the reply, divided by the total number of distinct
stopwords to choose from; similarly for the rest. we
micro-average over the conversation:

(cid:80)
(cid:80)
(msg,reply)   turns |msg     reply|
(msg,reply)   turns |msg|

match =

.

we also micro-average at the player-pair level, and
use the maximum pair value as a feature. this gives
an indication of how cohesive the closest pair is,
which can be a sign of the level of power imbal-
ance between the two (danescu-niculescu-mizil et
al., 2012). figure 5h shows that in teams that out-
perform the best individual the most cohesive pair
matches fewer content words (p = 0.023). overall
matching is also signi   cant, notably in terms of part-
of-speech bigrams; in teams that outperform the best
individual there is less overall matching (figure 5i,
p = 0.007). these results suggest that in construc-
tive teams the relationships between the members
are less subordinate.

+++    0.81.01.21.4+++    1.21.41.61.82.0+++    0.20.40.60.81.0+++    0.40.60.81.01.2+++    222426+++    0.250.300.350.40+++    0.910.920.930.94+++    0.150.200.25+++    0.81.01.21.41.6+++    0.060.070.08agreement and con   dence. we capture the amount
of agreement and disagreement using high-precision
keywords and    lters validated on a subset of the
data. (for instance, the word sure marks agreement
if found at the beginning of a message, but not other-
wise.) in figure 4, agreement signals are underlined
with purple; the team exhibits no disagreement.

the relative position of successive guesses made
can also indicate whether the team is re   ning a guess
or contradicting each other. we measure the median
distance between intermediate guesses, as well as
between guesses made by different players; in con-
structive teams, the jumps between different player
guesses are smaller (p < 10   16).

before the discussion starts, players are asked
to self-evaluate their con   dence in their individual
guesses. constructive teams have more con   dent
members on average (p < 10   5).
6.3 other linguistic features
length and variation. we measure the average
number of words per message, the total number of
words used to express the solo phase reasons, and
the overall type/token ratio of the conversation. we
also measure responsiveness in terms of the mean
time between turns and the total number of turns.
psycholinguistic lexicons. we use hand-crafted
lexicons inspired from liwc (tausczik and pen-
nebaker, 2010) to capture certainty and pronoun
use. for example, the conversation in figure 4 has
two con   dent phrases, underlined in red. we also
use a custom hedging lexicon adapted from hyland
(2005) for conversational data; hedging words are
underlined in blue in figure 4. to estimate how
grounded the conversation is, we measure the av-
erage concreteness of all content nouns, adjectives,
adverbs and verbs, using scalar word and bigram rat-
ings from brysbaert et al. (2014).10 concreteness
re   ects the degree to which a word denotes some-
thing perceptible, as opposed to ideas and concepts.
words like soil and coconut are highly concrete,
while words like trust have low concreteness.
game-speci   c words. we put together a lexicon of
geography terms and place names, to capture task-

10we scale the ratings to lie in [0, 1]. we extrapolate to out-
of-vocabulary words by regressing on dependency-based word
embeddings (levy and goldberg, 2014); this approach is highly
accurate (median absolute error of about 0.1).

first 20s

(+)
.52
.52
.56   
.59(cid:63)
.60(cid:63)

full conversation
(- -)
(++)
.55
.51
.54
.50
.55   
.53
.55   
.55
.56(cid:63)
.56

(- -)
features
.54
baseline
.50
linguistic
.56
interaction
.53
pos
.57   
all
table 1: cross-validation auc scores. signi   -
cantly better than chance scores after 5000 permu-
tations denoted with (cid:63) (p < 0.05) and     (p < 0.1).

(++)
.52
.50
.55   
.54
.56(cid:63)

(+)
.50
.51
.57(cid:63)
.54
.57(cid:63)

speci   c discussion. we use a small set of words
speci   c to the streetcrowd interface, such as map,
marker, and game, to capture phatic communica-
tion. figure 5j shows that constructive teams tend
to use more geography terms (p = 0.008), possi-
bly because of more on-topic discussion and a more
focused vocabulary.
part-of-speech patterns. we use id165s of coarse
part-of-speech tags as a general way of capturing
common syntactic patterns.

7 predicting constructiveness
7.1 experimental setup
so far we have characterized the relation between a
team   s interaction patterns and its level of produc-
tivity. this opens the door towards recognizing con-
structive and non-constructive interactions in real-
istic settings where the true answer is not known.
ideally, such an automatic system could prompt un-
productive teams to reconsider their approach, or
to aggregate their individual answers instead. with
early detection, non-constructive discussions could
be stopped or steered on the right track. in order to
assess the feasibility of such a challenging task and
to compare the predictive power of our features, we
consider three classi   cation objectives:
(++): team outperforms its best member (cbest > 0)?
(+): team is constructive (cavg > 0)?
(- -): team underperforms its worst member (cworst < 0)?
to investigate early detection, we evaluate the clas-
si   cation performance when using data from only
the    rst 20 seconds of the team   s interaction.11

11measured from the    rst chat message or guess. for this
evaluation, we remove teams where the    rst 20 seconds contain
over 75% of the interaction, to avoid distorting the results with

since all three objectives are imbalanced (fig-
ure 2), we use the area under the roc curve (auc)
as the performance metric, and we use logistic
regression models. we perform 20 iterations of
puzzle-aware shuf   ed train-validation splitting, fol-
lowed by 5000 iterations on the best models, to esti-
mate variance. this ensures that the models don   t
learn to over   t puzzle-speci   c signals. the com-
bined model uses weighted model averaging. we
use grid search for id173 parameters, feature
extraction parameters, and combination weights.

7.2 discussion of the results (table 1)
we compare to a baseline consisting of the team
size, average number of messages per player, and
conversation duration. for comparison, a bag-of-
words classi   er does no better than chance and is
on par with the baseline. we refer to idea    ow and
interaction dynamics features (section 6.2) as inter-
action, and to linguistic and lexical features (sec-
tion 6.3) as linguistic. the combination model in-
cluding baseline, interaction, linguistic and part-of-
speech id165 features, is consistently the best and
signi   cantly outperforms random guessing (auc
.50) in nearly all settings. while overall scores
are modest, the results con   rm that our conversa-
tional analysis framework has predictive power, and
that the high-stakes task of early prediction is fea-
sible. the language used when introducing and
adopting ideas, together with balance and language
matching features, are selected in nearly all settings.
the least represented class (- -) has the highest vari-
ance in prediction, suggesting that more data collec-
tion is needed to successfully capture extreme cases.
useful pos patterns capture the amount of proper
nouns and their contexts: proper nouns at the end of
messages are indicative of constructiveness, while
proper nouns followed by verbs are a negative fea-
ture. (the constructive discussion shown in figure 4
has most proper nouns at the end of messages.)

a manual error analysis of the false positives and
false negatives where our best model is most con-
   dent points to games with very short conversations
and spelling mistakes, con   rming that the noisy data
problem causes learning and modeling dif   culties.

teams who make their decision early, but take longer to submit.
the 20 second threshold was chosen as a trade-off in terms of
how much interaction it covers in the games.

8 conclusions and future work

we developed a framework based on conversational
dynamics in order to distinguish between produc-
tive and unproductive task-oriented discussions. by
applying it to an online collaborative game we de-
signed for this study, we reveal new interactions with
conversational patterns. constructive teams are gen-
erally well-balanced on multiple aspects, with team-
members participating equally in proposing ideas
and making guesses and showing little asymmetry in
language matching. also, the    ow of ideas between
teammates marks predictive linguistic cues, with the
most constructive teams using fewer hedges when
introducing and adopting ideas.

we show that such cues have predictive power
even when extracted from the    rst 20 seconds of
the conversations.
in future work, improved clas-
si   ers could lead to a system that can intervene in
non-constructive discussions early on, steering them
on track and preventing wasted time.

further improving classi   cation performance on
such a dif   cult task will hinge on better conversation
processing tools, adequate for the domain and robust
to the informal language style. in particular, we plan
to develop and evaluate models for idea    ow and
(dis)agreement, using more advanced features (e.g.,
from dependency relations and id13s).
the streetcrowd game is continuously accumu-
lating more data, enabling further development on
conversation analysis. our full control over the
game permits manipulation and intervention exper-
iments that can further advance research on team-
work.
in future work, we envision applying our
framework to settings where teamwork takes place
online, such as open-source software development,
wikipedia editing, or massive open online courses.
acknowledgements we are particularly grateful to
bob west for the poolside chat that inspired the de-
sign of the game, to daniel garay, jinjing liang
and neil parker for participating in its development,
and to the numerous passionate players. we are
also grateful to natalya bazarova, david byrne,
mala gaonkar, lillian lee, sendhil mullainathan,
andreas veit, connie yuan, justine zhang and the
anonymous reviewers for their insightful sugges-
tions. this work was supported in part by a google
faculty research award.

references
anne h anderson, miles bader, ellen gurman bard,
elizabeth boyle, gwyneth doherty, simon garrod,
stephen isard, jacqueline kowtko, jan mcallister, and
jim miller. 1991. the hcrc map task corpus. lan-
guage and speech, 34(4):351   366.

alex bavelas. 1950. communication patterns in task-
oriented groups. journal of the acoustical society of
america.

marc brysbaert, amy beth warriner, and victor kuper-
man. 2014. concreteness ratings for 40 thousand
generally known english word lemmas. behavior re-
search methods, 46(3):904   911.

derrick coetzee, seongtaek lim, armando fox, bjorn
hartmann, and marti a hearst. 2015. structuring
interactions for large-scale synchronous peer learning.
in proceedings of cscw.

jonathon n cummings and rob cross. 2003. structural
properties of work groups and their consequences for
performance. social networks, 25(3):197   210.

cristian danescu-niculescu-mizil, michael gamon, and
susan dumais. 2011. mark my words! linguistic
style accommodation in social media. in proceedings
of www.

cristian danescu-niculescu-mizil, lillian lee, bo pang,
and jon kleinberg. 2012. echoes of power: language
effects and power differences in social interaction. in
proceedings of www.

micha elsner and eugene charniak. 2010. disentan-
gling chat. computational linguistics, 36(3):389   
409.

heather friedberg, diane litman, and susannah bf
paletz. 2012. lexical entrainment and success in stu-
dent engineering groups. in proceedings of the spoken
language technology workshop.

harold guetzkow and herbert a simon. 1955. the im-
pact of certain communication nets upon organization
and performance in task-oriented groups. manage-
ment science, 1(3-4):233   250.

ken hyland. 2005. metadiscourse: exploring interac-

tion in writing. continuum.

peter j jordan, neal m ashkanasy, charmine ej h  artel,
and gregory s hooper. 2002. workgroup emotional
intelligence: scale development and relationship to
team process effectiveness and goal focus. human re-
source management review, 12(2):195   214.

malte f jung, jan chong, and larry leifer. 2012. group
hedonic balance and pair programming performance:
affective interaction dynamics as indicators of perfor-
mance. in proceedings of sigchi.

malte f jung. 2011. engineering team performance and
emotion: affective interaction dynamics as indicators

of design team performance. ph.d. thesis, stanford
university.

gregory e kersten and grant zhang. 2003. mining in-
spire data for the determinants of successful internet
negotiations. central european journal of operations
research, 11(3).

walter s lasecki, christopher d miller, adam sadilek,
andrew abumoussa, donato borrello, raja kushal-
nagar, and jeffrey p bigham. 2012. real-time cap-
in proceedings of
tioning by groups of non-experts.
uist.

patrick r laughlin and john adamopoulos. 1980. so-
cial combination processes and individual learning
for six-person cooperative groups on an intellective
task. journal of personality and social psychology,
38(6):941.

omer levy and yoav goldberg. 2014. dependency-

based id27s. in proceedings of acl.

elijah may   eld, michael garbus, david adamson, and
carolyn penstein ros  e. 2011. data-driven interac-
tion patterns: authority and information sharing in di-
in proceedings of aaai fall symposium on
alogue.
building common ground with intelligent agents.

elijah may   eld, david adamson, alexander rudnicky,
and carolyn penstein ros  e. 2012. computational rep-
resentation of discourse practices across populations in
task-based dialogue. in proceedings of icic.

vlad niculae, srijan kumar, jordan boyd-graber, and
cristian danescu-niculescu-mizil. 2015. linguistic
harbingers of betrayal: a case study on an online strat-
egy game. in proceedings of acl.

kate g niederhoffer and james w pennebaker. 2002.
linguistic style matching in social interaction. jour-
nal of language and social psychology, 21(4):337   
360.

olutobi owoputi, brendan o   connor, chris dyer, kevin
gimpel, nathan schneider, and noah a smith. 2013.
improved part-of-speech tagging for online conver-
in proceedings of
sational text with word clusters.
naacl.

david reitter and johanna d moore. 2007. predicting

success in dialogue. in acl, volume 45, page 808.

alan ritter, colin cherry, bill dolan, et al. 2010. un-
supervised modeling of twitter conversations. in pro-
ceedings of naacl.

nicholas c romano jr and jay f nunamaker jr. 2001.
meeting analysis: findings from research and prac-
tice. in proceedings of hicss.

marina sokolova and guy lapalme. 2012. how much
do we say? using informativeness of negotiation text
records for early prediction of negotiation outcomes.
group decision and negotiation, 21(3):363   379.

marina sokolova, vivi nastase, and stan szpakowicz.
2008. the telling tail: signals of success in electronic
negotiation texts.

yla r tausczik and james w pennebaker. 2010. the
psychological meaning of words: liwc and comput-
erized text analysis methods. journal of language and
social psychology, 29(1):24.

hao-chuan wang, susan r fussell, and dan cosley.
from diversity to creativity: stimulating
2011.
group brainstorming with cultural differences and
conversationally-retrieved pictures. in proceedings of
cscw.

wendy m williams and robert j sternberg. 1988. group
intelligence: why some groups are better than others.
intelligence, 12(4):351   377.

anita williams woolley, christopher f chabris, alex
pentland, nada hashmi, and thomas w malone.
2010.
evidence for a collective intelligence fac-
tor in the performance of human groups. science,
330(6004):686   688.

