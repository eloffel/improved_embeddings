6
1
0
2

 

y
a
m
3

 

 
 
]

g
l
.
s
c
[
 
 

5
v
6
0
6
6
0

.

1
1
5
1
:
v
i
x
r
a

published as a conference paper at iclr 2016

data representation and compression using
linear-programming approximations

hristo s. paskov
computer science department
stanford university
hpaskov@stanford.edu

john c. mitchell
computer science department
stanford university
jcm@stanford.edu

trevor j. hastie
statistics department
stanford university
hastie@stanford.edu

abstract

we propose    dracula   , a new framework for unsupervised feature selection from
sequential data such as text. dracula learns a dictionary of id165s that ef   ciently
compresses a given corpus and recursively compresses its own dictionary; in ef-
fect, dracula is a    deep    extension of compressive id171. it requires
solving a binary linear program that may be relaxed to a linear program. both
problems exhibit considerable structure, their solution paths are well behaved, and
we identify parameters which control the depth and diversity of the dictionary. we
also discuss how to derive features from the compressed documents and show that
while certain unregularized linear models are invariant to the structure of the com-
pressed dictionary, this structure may be used to regularize learning. experiments
are presented that demonstrate the ef   cacy of dracula   s features.

1

introduction

at the core of any successful machine learning problem is a good feature representation that high-
lights salient properties in the data and acts as an effective interface to the statistical model used
for id136. this paper focuses on using classical ideas from compression to derive useful fea-
ture representations for sequential data such as text. the basic tenets of compression abound in
machine learning: the minimum description length principle can be used to justify id173 as
well as various model selection criteria (gabrilovich & markovitch (2004)), while for unsupervised
problems deep autoencoders (salakhutdinov (2009)) and the classical id116 algorithm both seek
a parsimonious description of data. meanwhile, off-the-shelf compressors, such as lz-77 (ziv &
lempel (1977)), have been successfully applied to natural language problems as kernels that com-
pute pairwise document similarities (bratko et al. (2006)).
we propose a new framework, dracula, so called because it simultaneously    nds a useful data
representation and compression using linear-programming approximations of the criterion that mo-
tivates dictionary-based compressors like lz-77 (ziv & lempel (1977)). dracula    nds an explicit
feature representation for the documents in a corpus by learning a dictionary of id165s that is used
to losslessly compress the corpus. it then recursively compresses the dictionary. this recursion
makes dracula a deep extension of compressive id171 (cfl) (paskov et al. (2013)) that
can    nd exponentially smaller representations and promotes similar id165s to enter the dictionary.
as noted in (paskov et al. (2013)), feature representations derived from off-the-shelf compressors
are inferior because the algorithms used are sensitive to document order; both dracula and cfl are
invariant to document order.
our framework is expressed as a binary linear program (blp) that can viewed as a linear program
(lp) over a suf   ciently constrained polyhedron or relaxed to an lp by relaxing the integrality con-
straints. this is a notable departure from traditional deep learners (salakhutdinov (2009); socher &
manning (2013); lecun et al. (2006)), which are formulated as non-convex, non-linear optimiza-
tion problems. this structure makes it possible to analyze dracula in view of well known techniques
from convex analysis (e.g. the kkt conditions), polyhedral combinatorics, and id207. for
example, we show that dracula is easily parameterized to control the depth and diversity of its
dictionary and that its solutions are well behaved as its parameters vary.

1

published as a conference paper at iclr 2016

this paper introduces dracula in section 2 and discusses some of its problem structure and computa-
tional properties, including its np-completeness. section 3 uses dracula   s polyhedral interpretation
to explore the compressed representations it    nds as its storage cost model varies. it also discusses
how to extract features directly from a compression and how to integrate dictionary structure into
the features. finally, section 4 provides empirical evidence that deep compression    nds hierarchical
structure in data that is useful for learning and compression, and section 5 concludes.

2 dracula

this section introduces dracula by showing how to extend cfl to a deep architecture that com-
presses its own dictionary elements. we also show how to interpret any dracula solution as a di-
rected acyclic graph (dag) that makes precise the notion of depth and provides useful statistical
insights. finally, we prove that dracula is np-complete and discuss linear relaxation schemes.
notation throughout this paper    is a    xed    nite alphabet and c = {d1, . . . , dn} is a    xed
document corpus with each dk     c a string dk = ck
i       . an id165
is any substring of some dk and s is the set of all id165s in the document corpus, including
the original documents. for any s     s a pointer p is a triple p = (s, l     {1, . . . ,|s|}, z     s)
indicating that z = sl . . . sl+|z|   1. we say that p uses z at location l in s. let p be the set of
all valid pointers and for any p     p we use p (s) = {p     p|p = (s, l, z)} to select pointers
whose    rst element is s, e.g. p =    s   sp(s). moreover, p uses z     s if there is some p     p
using z, and p reconstructs s     s if every location in s is covered by at least one pointer, i.e.
   (s,l,v)   p (s){l, . . . , l + |v|     1} = {1, . . . ,|s|}. conceptually, s is recovered from p by iterating
through the (s, l, v)     p and    pasting    a copy of v into location l of a blank string. it will be helpful
to de   ne pc =    s   cp(s) to be the set of pointers that can only be used to reconstruct the corpus.

j of characters ck

1 . . . ck

2.1 cfl
cfl represents document corpus c by storing a dictionary s     s, a set of id165s, along with a
pointer set p     pc that only uses dictionary id165s and losslessly reconstructs each of the docu-
ments in c. importantly, cfl stores the dictionary directly in plaintext. the overall representation
is chosen to minimize its total storage cost for a given storage cost model that speci   es ds, the cost
of including id165 s     s in the dictionary, as well as cp, the cost of including pointer p     pc in
the pointer set. selecting an optimal cfl representation may thus be expressed as

subject to p reconstructs dk    dk     c; p only uses s     s.

ds

(1)

(cid:88)

p   p

(cid:88)

s   s

minimize
s   s,p   pc

cp +

this optimization problem naturally decomposes into subproblems by observing that when the dic-
tionary is    xed, selecting the optimal pointer set decouples into |c| separate problems of optimally
reconstructing each corpus document. we thus de   ne the reconstruction module for document
dk     c, which takes as input a dictionary s and outputs the minimum cost of reconstructing dk
with pointers that only use strings in s. note that speci   c pointers and dictionary strings can be
disallowed by setting their respective costs to    . for example setting ds =     for all s     s longer
than a certain length limits the size of dictionary id165s. of course, in practice, any variables with
in   nite costs are simply disregarded.
the reconstruction module can be expressed as a blp by associating with every pointer p     p(dk)
a binary indicator variable wp     {0, 1} whereby wp = 1 indicates that p is included in the optimal
pointer set for dk. we similarly use binary variables ts     {0, 1} to indicate that s     s is included
in the dictionary. since there is a one-to-one correspondence between pointer sets (dictionaries)
and w     {0, 1}|p(dk)| (t     {0, 1}|s|), the vector storing the wp (ts), we will directly refer to
these vectors as pointer sets (dictionaries). lossless reconstruction is encoded by the constraint
x dk w     1 where x dk     {0, 1}|dk|  |p(dk)| is a binary matrix indicating the indices of dk that
each pointer can reconstruct. in particular, for every p = (dk, l, z)     p(dk), column x dk
is
all zeros except for a contiguous sequence of 1   s in indices l, . . . , l + |z|     1. control of which
pointers may be used (based on the dictionary) is achieved by the constraint w     v dk t where
v dk     {0, 1}|p(dk)|  |s| contains a row for every pointer indicating the string it uses. in particular,

p

2

published as a conference paper at iclr 2016

for every p = (dk, l, z), v dk
may now be expressed as

rdk (t; c) = minimize

w   {0,1}|p(dk )|

p,z = 1 is the only non-zero entry in the row pertaining to p. the blp

wpcp

subject to x dk w     1; w     v dk t.

(2)

the optimization problem corresponding to an optimal cfl representation may now be written as a
blp by sharing the dictionary variable t among the reconstruction modules for all documents in c:

minimize
t   {0,1}|s|

rdk (t, c) +

tsds

(3)

(cid:88)

s   s

p   p(dk)

(cid:88)

(cid:88)

dk   c

2.2 adding depth with dracula

the simplicity of cfl   s dictionary storage scheme is a fundamental shortcoming that is demon-
strated by the string aa . . . a consisting of the character a replicated 22n times. let the cost of using
any pointer be cp = 1 and the cost of storing any dictionary id165 be its length, i.e. ds = |s|. the
best cfl can do is to store a single dictionary element of length 2n and repeat it 2n times, incurring
a total storage cost of 2n+1. in contrast, a    deep    compression scheme that recursively compresses
its own dictionary by allowing dictionary strings to be represented using pointers attains exponential
space savings relative to cfl. in particular, the deep scheme constructs dictionary strings of length
2, 4, . . . , 22n   1 recursively and incurs a total storage cost of 4n 1.
dracula extends cfl precisely in this hierarchical manner by allowing dictionary strings to be ex-
pressed as a combination of characters and pointers from shorter dictionary strings. cfl thus cor-
responds to a shallow special case of dracula which only uses characters to reconstruct dictionary
id165s. this depth allows dracula to leverage similarities among the dictionary strings to obtain
further compression of the data. it also establishes a hierarchy among dictionary strings that allows
us to interpret dracula   s representations as a directed acyclic graph (dag) that makes precise the
notion of representation depth.
formally, a dracula compression (compression for brevity) of corpus c is a triple d = (s     s, p    
pc,   p     p) consisting of dictionary, a pointer set p that reconstructs the documents in c, and a
pointer set   p that reconstructs every dictionary string in s. as with cfl, any pointers in p may
only use strings in s. however, a pointer p       p reconstructing a dictionary string s     s is valid if it
uses a unigram (irrespective of whether the unigram is in s) or a proper substring of s that is in s.
this is necessary because unigrams take on the special role of characters for dictionary strings. they
are the atomic units of any dictionary, so the character set    is assumed to be globally known for
dictionary reconstruction. in contrast, document pointers are not allowed to use characters and may
only use a unigram if it is present in s; this ensures that all strings used to reconstruct the corpus are
included in the dictionary for use as features.
finding an optimal dracula representation may also be expressed as a blp through simple mod-
i   cations of cfl   s objective function. in essence, the potential dictionary strings in s are treated
like documents that only need to be reconstructed if they are used by some pointer. we extend the
storage cost model to specify costs cp for all pointers p     pc used for document reconstruction
as well as costs   cp for all pointers p     p used for dictionary reconstruction. in keeping with the
aformentioned restrictions we assume that   cp =     if p = (s, 1, s) illegally tries to use s to recon-
struct s and s is not a unigram. the dictionary cost ds is now interpreted as the    overhead    cost of
including s     s in the dictionary without regard to how it is reconstructed; cfl uses the ds to also
encode the cost of storing s in plaintext (e.g. reconstructing it only with characters). finally, we
introduce dictionary reconstruction modules as analogs to the (document) reconstruction modules
for dictionary strings: the reconstruction module for s     s takes as input a dictionary and outputs
the cheapest valid reconstruction of s if s needs to be reconstructed. this can be written as the blp

  rs(t;   c) = minimize
w   {0,1}|p(s)|

wp  cp

subject to x sw     ts1; w       v st.

(4)

(cid:88)

p   p(s)

1note that the recursive model is allowed to use pointers in the dictionary and therefore selects from a larger
pointer set than cfl. care must be taken to ensure that the comparison is fair since the    size    of a compression
is determing by the storage cost model and we could    cheat    by setting all dictionary pointer costs to 0. setting
all pointer costs to 1 ensures fairness.

3

published as a conference paper at iclr 2016

here x s is analogously de   ned as in equation (4) and   v s is analogous to v s in equation (4) except
that it does not contain any rows for unigram pointers. with this setup in mind, the optimization
problem corresponding to an optimal dracula representation may be written as the blp

(cid:88)

dk   c

(cid:104)

(cid:88)

s   s

(cid:105)

minimize
t   {0,1}|s|

rdk (t, c) +

tsds +   rs(t;   c)

(5)

finally, any compression can be interpreted graphically as, and is equivalent to, a dag whose
vertices correspond to members of   , s, or c and whose labeled edge set is determined by the
pointers: for every (s, l, z)     p or   p there is a directed edge from z to s with label l. note that
d de   nes a multi-graph since there may be multiple edges between nodes. figure 1 shows the
graph corresponding to a simple compression. as this graph encodes all of the information stored
by d, and vice versa, we will at times treat d directly as a graph. since d has no cycles, we can
organize its vertices into layers akin to those formed by deep neural networks and with connections
determined by the pointer set: layer 0 consists only of characters (i.e.
there is a node for every
character in   ), layer 1 consists of all dictionary id165s constructed solely from characters, higher
levels pertain to longer dictionary id165s, and the highest level consists of the document corpus c.
while there are multiple ways to organize the intermediate layers, a simple strati   cation is obtained
by placing s     s into layer i only if   p (s) uses a string in layer i   1 and no strings in layers i+1, . . . .
we note that our architecture differs from most conventional deep learning architectures which tend
to focus on pairwise layer connections     we allow arbitrary connections to higher layers.

figure 1: compression of    aabaabaax    using a 3-layered dictionary. layer 0 consists of characters;
layers 1 and 2 are dictionary id165s. there are three kinds of pointers: character to dictionary
id165 (dashed blue lines), dictionary id165 to (longer) dictionary id165 (solid blue line), and
dictionary id165 to document (double red lines).

2.3 computational hardness and relaxation

the document and dictionary reconstruction modules rdk /   rs are the basic building blocks of
dracula; when dictionary t is    xed, solving equation (5) is tantamount to solving the reconstruction
modules separately. the discussion in the appendix section a.1 shows that for a    xed binary t,
solving rdk or   rs is easy because of the structure of the constraint matrices x dk /x s. in fact,
this problem is equivalent to a min-cost    ow problem. similarly, if the pointer sets are known
for each document or dictionary string then it is easy to    nd the corresponding dictionary t by
checking which strings are used (in linear time relative to the number of pointers). one would hope
that the easiness of dracula   s subproblems leads to an easy overall learning problem. however,
learning the dictionary and pointer sets simultaneously makes this problem hard: dracula is np-
complete. in particular, it requires solving a binary lp (which are np-complete in general) and it
generalizes cfl which is itself np-complete (paskov et al. (2014)) (see section 3.1.1 for how to
restrict representations to be shallow).
we thus turn to solving dracula approximately via its lp relaxation. this is obtained by replacing
all binary constraints in equations (2),(4),(5) with interval constraints [0, 1]. we let qc denote this
lp   s constraint polyhedron and note that it is a subset of the unit hypercube. importantly, we may
also interpret the original problem in equation (5) as an lp over a polyhedron q whose vertices
are always binary and hence always has binary basic solutions. here q2 is the convex hull of all
(binary) dracula solutions and q     qc; all valid dracula solutions may be obtained from the linear
relaxation. in fact, the chv  atal-gomory theorem (schrijver (2003)) shows that we may    prune   

2note that unlike qc, this polyhedron is likely to be dif   cult to describe succinctly unless p = n p .

4

published as a conference paper at iclr 2016

qc into q by adding additional constraints. we describe additional constraints in the appendix
section a.1.1 that leverage insights from suf   x trees to prune qc into a tighter approximation q(cid:48)
c    
qc of q. remarkably, when applied to natural language data, these constraints allowed gurobi
(gurobi optimization (2015)) to quickly    nd optimal binary solutions. while we did not use these
binary solutions in our learning experiments, they warrant further investigation.
as the pointer and dictionary costs vary, the resulting problems will vary in dif   culty as measured
by the gap between the objectives of the lp and binary solutions. when the costs force either t or the
wdk /ws to be binary, our earlier reasoning shows that the entire solution will lie on a binary vertex
of qc that is necessarily optimal for the corresponding blp and the gap will be 0. this reasoning
also shows how to round any continuous solution into a binary one by leveraging the easiness of the
individual subproblems. first set all non-zero entries in t to 1, then reconstruct the documents and
dictionary using this dictionary to yield binary pointers, and    nally    nd the minimum cost dictionary
based on which strings are used in the pointers.

3 learning with compressed features

this section explores the feature representations and compressions that can be obtained from drac-
ula. central to our discussion is the observation of section 2.3 that all compressions obtained from
dracula are the vertices of a polyhedron. each of these vertices can be obtained as the optimal com-
pression for an appropriate storage cost model3, so we take a dual perspective in which we vary the
storage costs to characterize which vertices exist and how they relate to one another. the    rst part of
this section shows how to    walk    around the surface of dracula   s polyhedron and it highlights some
   landmark    compressions that are encountered, including ones that lead to classical bag-of-id165s
features. our discussion applies to both, the binary and relaxed, versions of dracula since the former
can viewed as an lp over a polyhedron q with only binary vertices. the second part of this section
shows how to incorporate dictionary structure into features via a dictionary diffusion process.
we derive features from a compression in a bag-of-id165s (bon) manner by counting the number
of pointers that use each dictionary string or character. it will be useful to explicitly distinguish
between strings and characters when computing our representations and we will use squares brackets
to denote the character inside a unigram, e.g. [c] . recall that given a compression d = (s, p,   p ), a
unigram pointer in p (used to reconstruct a document) is interpreted as a string whereas a unigram
pointer in   p is interpreted as a character. we refer to any z     s        as a feature and associate
with every document dk     c or dictionary string s     s a bon feature vector xdk , xs     z|s|+|  |
,
respectively. entry xdk
z =
|{p     p (s)| p = (dk, l, z)}|, and will necessarily have xdk
z = 0 for all z       . dictionary strings
are treated analogously with the caveat that if p = (s, l, z)       p uses a unigram, p counts towards
the character entry xdk

counts the number of pointers that use z to reconstruct dk, i.e. xdk

+

z

[z] , not xdk
z .

3.1 dracula   s solution path

exploring dracula   s compressions is tantamount to varying the dictionary and pointer costs supplied
to dracula. when these costs can be expressed as continuous functions of a parameter        [0, 1],
i.e.    s     s, p     pc,   p     p the cost functions ds(  ), cp(  ),   c   p(  ) are continuous, the optimal
solution sets vary in a predictable manner around the surface of dracula   s constraint polyhedron q
or the polyhedron of its relaxation qc. we use f (q) to denote the set of faces of polyhedron q
(including q), and take the dimension of a face to be the dimension of its af   ne hull. we prove the
following theorem in the appendix section a.3:
theorem 1. let q     rd be a bounded polyhedron with nonempty interior and b : [0, 1]     rd a
continuous function. then for some n     z+   {   } there exists a countable partition    = {  i}n
i=0 of
[0, 1] with corresponding faces fi     f (q) satisfying fi (cid:54)= fi+1 and fi   fi+1 (cid:54)=    . for all          i,
the solution set of the lp constrained by q and using cost vector b(  ) is fi = arg minx   q xt b(  ).
moreover, fi never has the same dimension as fi+1 and the boundary between   i,   i+1 is )[ iff
dim fi < dim fi+1 and ]( otherwise.

3the storage costs pertaining to each vertex form a polyhedral cone, see (ziegler (1995)) for details.

5

published as a conference paper at iclr 2016

(a)

(b)

(c)

figure 2: part (a) shows a nonlinear projection of a subset of dracula   s constraint polyhedron q in
which every vertex corresponds to a distinct compression of    xaxabxabxacxac   . part (b) is the pro-
jection   s polar; its faces delineate the (linear) costs for which each vertex in (a) is optimal. the red/
purple/ blue line in (b) demonstrates a continuous family of costs. all red (blue) costs are uniquely
minimized by the vertex in (a) highlighted in red (blue), respectively; (c) shows the corresponding
compressions. purple costs lie on the edge between the faces containing the red and blue lines and
are minimized by any convex combination of the vertices highlighted in (a).

this theorem generalizes the notion of a continuous solution path typically seen in the context of
id173 (e.g. the lasso) to the lp setting where unique solutions are piecewise constant and
transitions occur by going through values of    for which the solution set is not unique. for instance,
suppose that vertex v0 is uniquely optimal for some   0     [0, 1), another vertex v1 is uniquely
optimal for a   0 <   1     1, and no other vertices are optimal in (  0,   1). then theorem 1 shows
that v0 and v1 must be connected by a face (typically an edge) and there must be some        (  0,   1)
for which this face is optimal. as such, varying dracula   s cost function continuously ensures that
the solution set for the binary or relaxed problem will not suddenly    jump    from one vertex to the
next; it must go through an intermediary connecting face. this behavior is depicted in figure 2 on a
nonlinear projection of dracula   s constraint polyhedron for the string    xaxabxabxacxac   .
it is worthwhile to note that determining the exact value of    for which the face connecting v0 and
v1 is optimal is unrealistic in practice, so transitions may appear abrupt. while it is possible to
smooth this behavior by adding a strongly convex term to the objective (e.g. an l2 penalty), the
important insight provided by this theorem is that the trajectory of the solution path depends entirely
on the combinatorial structure of q or qc. this structure is characterized by the face lattice4 of
the polyhedron and it shows which vertices are connected via edges, 2-faces, . . . , facets. it limits,
for example, the set of vertices reachable from v0 when the costs vary continuously and ensure that
transitions take place only along edges 5. this predictable behavior is desirable when    ne tuning the
compression for a learning task, akin to how one might tune the id173 parameter of a lasso,
and it is not possible to show in general for non-convex functions.

4we leave it as an open problem to analytically characterize dracula   s face lattice.
5restricting transitions only to edges is possible with id203 1 by adding a small amount of gaussian

noise to c.

6

published as a conference paper at iclr 2016

we now provide a simple linear cost scheme that has globally predictable effects on the dictionary.
for all s     s, p     pc,   p     p we set ds =   , cp = 1,   c   p =      if   p uses as unigram (i.e.
is a
character), and   c   p =    otherwise. we constrain   ,        0 and        [0, 1]. in words, all document
pointer costs are 1, all dictionary costs   , and dictionary pointer costs are    if they use a string and
     if they use a character. the effects these parameters have on the compression may be understood
by varying a single parameter and holding all others constant:
varying    controls the minimum frequency with which s     s must be used before it enters the
dictionary; if few pointers use s it is cheaper to construct s    in place    using shorter id165s. long
id165s appear less frequently so        biases the dictionary towards shorter id165s.
varying    has a similar effect to    in that it becomes more expensive to construct s as    increases,
so the overall cost of dictionary membership increases. the effect is more nuanced, however, since
the manner in which s is constructed also matters; s is more likely to enter the dictionary if it shares
long substrings with existing dictionary strings. this suggests a kind of grouping effect whereby
groups of strings that share many substrings are likely to enter together.
varying    controls the dracula   s propensity to use characters in place of pointers in the dictionary
and thereby directly modulates dictionary depth. when    < 1
k for k = 2, 3, . . . , all dictionary
id165s of length at most k are constructed entirely from characters.

3.1.1 landmarks on dracula   s polyhedron

while dracula   s representations are typically deep and space saving, it is important to note that valid
dracula solutions include all of cfl   s solutions as well as a set of fully redundant representations
that use as many pointers as possible. the bon features computed from these    space maximizing   
compressions yield the traditional bon features containing all id165s up to a maximum length k.
a cost scheme that includes all pointers using all id165s up to length k is obtained by setting all
costs to be negative, except for ts =     for all s     s where |s| > k (to disallow these strings).
the optimal compression then includes all pointers with negative cost and each document position is
reconstructed k times. moreover, it is possible to restrict representations to be valid cfl solutions
by disallowing all non-unigram pointers for dictionary reconstruction, i.e. by setting   cp =     if p is
not a single character string.

3.2 dictionary diffusion

we now discuss how to incorporate dictionary information from a compression d = (s, p,   p ) into
the bon features for each corpus document. it will be convenient to store the bon feature vectors
xdk for each document as rows in a feature matrix x     z|c|  (|s|+|  |) and the bon feature vectors
xs for each dictionary string as rows in a feature matrix g     z(|s|+|  |)  (|s|+|  |). we also include
rows of all 0   s for every character in    to make g a square matrix for mathematical convenience.
graphically, this procedure transforms d into a simpler dag, dr, by collapsing all multi-edges
into single edges and labeling the resulting edges with an appropriate xs
z. for any two features s, z,
we say that s is higher (lower) order than z if it is a successor (predecessor) of z in d.
once our feature extraction process throws away positional information in the pointers higher order
features capture more information than their lower order constituents since the presence of an s     s
formed by concatenating features z1 . . . zm indicates the order in which the zi appear and not just
that they appear. conversely, since each zi appears in the same locations as s (and typically many
others), we can obtain better estimates for coef   cients associated with zi than for the coef   cient of
s. if the learning problem does not require the information speci   ed by s we pay an unnecessary
cost in variance by using this feature over the more frequent zi.
in view of this reasoning, feature matrix x captures the highest order information about the docu-
ments but overlooks the features    lower order id165s (that are indirectly used to reconstruct docu-
ments). this latter information is provided by the dictionary   s structure in g and can be incorporated
by a graph diffusion process that propagates the counts of s in each document to its constituent zi,
which propagate these counts to the lower order features used to construct them, and so on. this
process stops once we reach the characters comprising s since they are atomic. we can express this
s xs spreads xdk
s
zi, the number of times each zi

information    ow in terms of g by noting that the product gt xdk =(cid:80)

to each of the zi used to reconstruct s by multiplying xdk

s with xs

s   s      xdk

7

published as a conference paper at iclr 2016

feature matrix   x = xh where h = i +(cid:80)   

units of    ow to each parent zi, and
is directly used in s. graphically, node s in dr sends xdk
zi, the strength of the edge connecting zi to s. performing
this    ow is modulated in proportion to xs
zi to the features
this procedure a second time, i.e. multiplying gt (gt xdk ), further spreads xdk
used to reconstruct zi, modulated in proportion to their usage. iterating this procedure de   nes a new
n=1 gn spreads the top level xdk to the entire graph6.
we can interpret the effect of the dictionary diffusion process in view of two equivalent regularized
learning problems that learn coef   cients   ,        r|s     | for every feature in s        by solving

s xs

s

minimize
     r|s     |
    minimize
     r|s     |

l(   x  ) +   r(  )
l(x  ) +   r ((i     g)  ) .

(6)

we assume that l is a convex loss (that may implicitly encode any labels), r is a convex regular-
ization penalty that attains its minimum at    = 0, and that a minimizer       exists. note that adding
an unpenalized offset does not affect our analysis. the two problems are equivalent because h is
de   ned in terms of a convergent neumann series and, in particular, h = (i     g)   1 is invertible.
we may switch from one problem to the other by setting    = h   1   or    = h  .
when    = 0 the two problems reduce to estimating   /   for unregularized models that only differ
in the features they use,   x or x respectively. the equivalence of the problems shows, however,
that using   x in place of x has no effect on the models as their predictions are always the same.
indeed, if       is optimal for the    rst problem then       = h      is optimal for the second and for
any z     r|s     |, the predictions zt       = (zt h)      are the same. unregularized linear models    
including generalized linear models     are therefore invariant to the dictionary reconstruction scheme
and only depend on the document feature counts xdk, i.e. how documents are reconstructed.
when    > 0, using   x in place of x results in a kind of graph laplacian regularizer that encourages
  s to be close to   t xs. one interpretation of this is effect is that   s acts a    label    for s: we use
its feature representation to make a prediction for what   s should be and penalize the model for
any deviations. a complementary line of reasoning uses the collapsed dag dr to show that (6)
favors lower order features. associated with every node s     s        is a    ow   s and node z sends
  z units of    ow to each of its children s. this    ow is attenuated (or ampli   ed) by xs
z, the strength
of the edge connecting z to s. in turn, s adds its incoming    ows and sends out   s units of    ow
to its children; each document   s prediction is given by the sum of its incoming    ows. here r acts
a kind of       ow conservation    penalty that penalizes nodes for sending out a different amount of
   ow than they receive and the lowest order nodes (characters) are penalized for any    ow. from this
viewpoint it follows that the model prefers to disrupt the    ow conservation of lower order nodes
whenever they suf   ciently decrease the loss since they in   uence the largest number documents.
higher order nodes in   uence fewer documents than their lower order constituents and act as high
frequency components.

4 experiments

this section presents experiments comparing traditional bon features with features derived from
dracula and cfl. our primary goal is investigate whether deep compression can provide better fea-
tures for learning than shallow compression or the traditional    fully redundant    bon representation
(using all id165s up to a maximum length). since any of these representations can be obtained
from dracula using an appropriate cost scheme, positive evidence for the deep compression implies
dracula is uncovering hierarchical structure which is simultaneously useful for compression and
learning. we also provide a measure of compressed size that counts the number of pointers used by
each representation, i.e. the result of evaluating each compression with a    common sense    space ob-
jective where all costs are 1. we use top to indicate bon features counting only document pointers
(x in previous section), flat for dictionary diffusion features (i.e.   x), cfl for bon features from
cfl, and all for traditional bon features using all id165s considered by dracula.

6this sum converges because g corresponds to a    nite dag so it can be permuted to a strictly lower

triangular matrix so that

n      gn = 0. see appendix section a.2 for weighted variations.
lim

8

published as a conference paper at iclr 2016

figure 3: proteins represented using the 4th and 5th singular vectors of top features from dracula.

table 1: bacteria identi   cation accuracy using protein data

svd rank

5

10

15

20

all
cfl
top

59.5
89.7
87.5

77.7
85.0
91.2

83.3
76.9
89.0

77.6
74.5
83.3

all

81.1
74.0
84.3

# pointers
4.54  105
2.69  104
1.76  104

we used gurobi (gurobi optimization (2015)) to solve the re   ned lp relaxation of dracula for all of
our experiments. while gurobi can solve impressively large lp   s, encoding dracula for a general-
purpose solver is inef   cient and limited the scale of our experiments. dedicated algorithms that
utilize problem structure, such as the network    ow interpretation of the reconstruction modules, are
the subject of a follow-up paper and will allow dracula to scale to large-scale datasets. we limited
our parameter tuning to the dictionary pointer cost    (discussed in the solution path section) as this
had the largest effect on performance. experiments were performed with    = 0,    = 1, a maximum
id165 length, and only on id165s that appear at least twice in each corpus.

protein data we ran dracula using 7-grams and    = 1 on 131 protein sequences that are labeled
with the kingdom and phylum of their organism of origin (pro). bacterial proteins (73) dominate this
dataset, 68 of which evenly come from actinobacteria (a) and fermicutes (f). the    rst 5 singular
values (sv   s) of the top features show a clear separation from the remaining sv   s and figure 3
plots the proteins when represented by their 4th and 5th principle components. they are labeled by
kingdom and, in more interesting cases, by phylum. note the clear separation of the kingdoms, the
two main bacterial phyla, and the cluster of plants separated from the other eukaryotes. table 1
shows the average accuracy of two binary classi   cation tasks in which bacteria are positive and we
hold out either phylum a or f, along with other randomly sampled phyla for negative cases, as a
testing set. we compare all features to top features from dracula and cfl using an (cid:96)2-regularized
id166 with c = 1. since there are many more features than training examples we plot the effect
of using the top k principle components of each feature matrix. flat features did not help and
performance strictly decreased if we limited the id165 length for all features, indicating that long
id165s contain essential information. both compression criteria perform well, but using a deep
dictionary seems to help as dracula   s pro   le is more stable than cfl   s.

stylometry we extracted 100 sentences from each of the training and testing splits of the reuters
dataset (liu) for 10 authors, i.e. 2, 000 total sentences, and replaced their words with part-of-speech
tags. the goal of this task is to predict the author of a given set of writing samples (that all come
from the same author). we make predictions by representing each author by the centroid of her
100 training sentences, averaging together the unknown writing samples, and reporting the nearest
author centroid to the sample centroid. we ran dracula on this representation with 10-grams and
normalized centroids by their (cid:96)1 norm and features by their standard deviation. table 2 compares
the performance of all features to top features derived from various      s for various testing sentence
sample sizes. we report the average of 1, 000 trials, where each trial tested every author once and

9

published as a conference paper at iclr 2016

table 2: author identi   cation accuracy

# samples

5

10

25

50

75

all
cfl    = 20
top    = 1
top    = 10
top    = 20

36.0
39.6
35.1
39.6
37.7

47.9
50.5
46.2
51.0
49.4

67.9
73.8
68.6
75.0
73.8

80.6
87.5
85.3
88.9
91.5

86.4
91.4
93.7
93.7
97.8

# pointers
5.01  105
3.33  104
2.39  104
3.00  104
3.32  104

table 3: sentiment classi   cation accuracy

  : mnl # pointers top

flat

id165 len. nb all

id166 (cid:96)1 all

id166 (cid:96)2 all

0.25
0.5
1
2
5

4.02
3.78
3.19
2.51
1.96

1.79  105
1.75  105
1.71  105
1.71  105
1.86  105

73.9
75.1
76.6
78.0
78.0

78.2
78.8
78.2
78.1
78.0

5
4
3
2
1

77.9
77.9
78.4
78.8
78.0

76.6
76.8
77.0
77.2
76.3

76.9
77.0
77.2
77.5
76.5

randomly selected a set of sample sentences from the testing split sentences. as in the protein data,
neither flat nor shorter id165 features helped, indicating that higher order features contain vital
information. cfl with    = 20 strictly dominated every other cfl representation and is the only
one included for brevity. dracula with    = 10 or    = 20 shows a clear separation from the other
schemes, indicating that the deep compression    nds useful structure.

sentiment prediction we use a dataset of 10, 662 movie review sentences (pang & lee (2005))
labeled as having positive or negative sentiment. bigrams achieve state-of-the-art accuracy on this
dataset and unigrams perform nearly as well (wang & manning (2012)), so enough information is
stored in low order id165s that the variance from longer id165s hurts prediction. we ran dracula
using 5-grams to highlight the utility of flat features, which focus the classi   er onto lower order
features. following (wang & manning (2012)), table 3 compares the 10-fold cv accuracy of a
multinomial na    ve-bayes (nb) classi   er using top or flat features with one using all id165s up
to a maximum length. the dictionary diffusion process successfully highlights relevant low or-
der features and allows the flat representation to be competitive with bigrams (the expected best
performer). the table also plots the mean id165 length (mnl) used by document pointers as a
function of   . the mnl decreases as    increases and this eventually pushes the top features to
behave like a mix of bigrams and unigrams. finally, we also show the performance of (cid:96)2 or (cid:96)1-
regularized support vector machines for which we tuned the id173 parameter to minimize
cv error (to avoid issues with parameter tuning). it is known that nb performs surprisingly well
relative to id166s on a variety of sentiment prediction tasks, so the dropoff in performance is ex-
pected. both id166s achieve their best accuracy with bigrams; the regularizers are unable to fully
remove the spurious features introduced by using overly long id165s. in contrast, flat achieves
its best performance with larger mnls which suggests that dracula performs a different kind of
feature selection than is possible with direct (cid:96)1/(cid:96)2 id173. moreover, tuning    combines fea-
ture selection with nb or any kind of classi   er, irrespective of whether it natively performs feature
selection.

5 conclusion

we have introduced a novel dictionary-based compression framework for feature selection from
sequential data such as text. dracula extends cfl, which    nds a shallow dictionary of id165s with
which to compress a document corpus, by applying the compression recursively to the dictionary. it
thereby learns a deep representation of the dictionary id165s and document corpus. experiments

10

published as a conference paper at iclr 2016

with biological, stylometric, and natural language data con   rm the usefulness of features derived
from dracula, suggesting that deep compression uncovers relevant structure in the data.
a variety of extensions are possible, the most immediate of which is the design of an algorithm that
takes advantage of the problem structure in dracula. we have identi   ed the basic subproblems com-
prising dracula, as well as essential structure in these subproblems, that can be leveraged to scale the
compression to large datasets. ultimately, we hope to use dracula to explore large and fundamental
datasets, such as the human genome, and to investigate the kinds of structures it uncovers.

acknowledgements
dedicated to ivan i kalinka hand(cid:25)ievi (ivan and kalinka handjievi). funding provided by
the air force of   ce of scienti   c research and the national science foundation.

references
protein classi   cation benchmark collection. http://hydra.icgeb.trieste.it/benchmark/index.php?page=00.

bertsimas, dimitris and weismantel, robert. optimization over integers. athena scienti   c, 2005.

isbn 978-0-97591-462-5.

bratko, andrej, filipi  c, bogdan, cormack, gordon v., lynam, thomas r., and zupan, bla  z. spam

   ltering using statistical data compression models. jmlr, 7:2673   2698, 2006.

gabrilovich, evgeniy and markovitch, shaul. text categorization with many redundant features:

using aggressive feature selection to make id166s competitive with c4.5. in icml, 2004.

gurobi optimization, inc. gurobi optimizer reference manual, 2015. url http://www.

gurobi.com.

gus   eld, dan. algorithms on strings, trees, and sequences - computer science and computational

biology. cambridge university press, 1997. isbn 0-521-58519-8.

lecun, yann, chopra, sumit, hadsell, raia, ranzato, marc   aurelio, and huang, fu-jie. a tutorial
on energy-based learning. in bakir, g., hofman, t., sch  olkopf, b., smola, a., and taskar, b.
(eds.), predicting structured data. mit press, 2006.

liu, zhi. reuter 50 50 data set.

lu, shu and robinson, stephen m. normal fans of polyhedral convex sets. set-valued analysis, 16

(2-3):281   305, 2008.

pang, bo and lee, lillian. seeing stars: exploiting class relationships for sentiment categorization
in proceedings of the 43rd annual meeting on association for

with respect to rating scales.
computational linguistics, pp. 115   124. association for computational linguistics, 2005.

paskov, hristo, west, robert, mitchell, john, and hastie, trevor. compressive id171. in

nips, 2013.

paskov, hristo, mitchell, john, and hastie, trevor. an ef   cient algorithm for large scale compressive

id171. in aistats, 2014.

salakhutdinov, ruslan. learning deep generative models, 2009.

schrijver, a. combinatorial optimization - polyhedra and ef   ciency. springer, 2003.

socher, richard and manning, christopher d. deep learning for nlp (without magic). in confer-
ence of the north american chapter of the association of computational linguistics, proceed-
ings, pp. 1   3, 2013.

wang, sida and manning, christopher d. baselines and bigrams: simple, good sentiment and topic
classi   cation. in proceedings of the 50th annual meeting of the association for computational
linguistics: short papers-volume 2, pp. 90   94. association for computational linguistics, 2012.

11

published as a conference paper at iclr 2016

ziegler, gunter m. lectures on polytopes. graduate texts in mathematics. springer-verlag, 1995.

ziv, jacob and lempel, abraham. a universal algorithm for sequential data compression. tit, 23

(3):337   343, 1977.

a appendix

a.1 reconstruction modules

the reconstruction modules rdk /   rs are the basic building blocks of dracula; when t is    xed
solving (5) is tantamount to solving the reconstruction modules separately. these simple blps
have a number of properties that result in computational savings because of the structure of the
constraint matrix x dk /x s. in order to simplify notation we de   ne

ts(t, v; b, v ) = minimize
w   {0,1}|p(s)|

wpbp

subject to x sw     v1; w     v t.

(7)

(cid:88)

p   p(s)

using tdk (t, 1; c, v dk ) = rdk (t; c) and ts(t, ts;   c,   v s) =   rs(t;   c) results in the document or
dictionary reconstruction modules. now note that every column in x s is all zeros except for a con-
tiguous sequence of ones so that x s is an interval matrix and therefore totally unimodular (tum).
de   ne t c

s to be the lp relaxation of ts obtained by replacing the integrality constraints:
subject to x sw     v1; w     v t.

(cid:88)

wpbp

s (t, v; b, v ) = minimize
t c
w   [0,1]|p(s)|

(8)

p   p(s)

aside from x, the remaining constraints on w are bound constraints. it follows from (bertsimas &
weismantel (2005)) that t c
proposition 1. if the arguments t, v are integral, then all basic solutions of t c

s is an lp over a integral polyhedron so we may conclude that

s (t, v; b, v ) are binary.

indeed, a simple dynamic program discussed in (paskov et al. (2013)) solves ts ef   ciently.
our second property reformulates t c
s by transforming the constraint matrix x s into a simpler form.
the resulting matrix has at most 2 non-zero entries per column instead of up to |s| non-zero entries
per column in x s. this form is more ef   cient to work with when solving the lp and it shows
s is equivalent to a min-cost    ow problem over an appropriately de   ned graph. de   ne q    
that t c
{0,  1}|s|  |s| be the full rank lower triangular matrix with entries qs
(i+1)i = 1 and 0
elsewhere (and qs|s||s| = 1). the interval structure of x s implies that column i of z s = qsx s is
all zeros except for z s
ij = 1 and k > j is the    rst
ik = 0 after the sequences of ones (if such a k exists). by introducing non-negative
row in which x s
slack variables for the x sw     v1 constraint, i.e. writing x sw = v1 +   , and noting that qs1 = e1,
where e1 is all zeros except for a 1 as its    rst entry, we arrive at:

ik = 1 where j is the    rst row in which x s

ii =    qs

ij =    z s

s (t, v; b, v ) = minimize
t c

w,  

wpbp

p   p(s)

subject to z sw     qs   = ve1,
0     w     v t, 0       .

(9)

the matrix    = [z s|     qs] has special structure since every column has at most one 1 and at most
one    1. this allows us to interpret    as the incidence matrix of a directed graph if we add source
and sink nodes with which to    ll all columns out so that they have exactly one 1 and one    1. t c
may then be interpreted as a min-cost    ow problem.

s

a.1.1 polyhedral refinement

we now show how to tighten dracula   s lp relaxation by adding additional constraints to qc to
shrink it closer to q. if every time we see a string s it is followed by the character    (in a given
corpus), the strings s and s   belong to the same equivalence class; the presence of s   conveys the
same information as the presence of s. importantly, the theory of suf   x trees shows that all substrings

12

(cid:88)

published as a conference paper at iclr 2016

(cid:80)
s      ts     1 to the lp relaxation to tighten qc will not remove any binary solutions.

of a document corpus can be grouped into at most 2n   1 equivalence classes (gus   eld (1997)) where
n is the word count of the corpus. we always take equivalence classes to be inclusion-wise maximal
sets and say that equivalence class        s appears at a location if any (i.e. all) of its members appear
at that location. we prove the following theorem below. this theorem veri   es common sense and
implies that, when the pointer costs do not favor any particular string in   , adding the constraint
theorem 2. let     denote the set of all equivalence classes in corpus c and suppose that all costs
are non-negative and              ,   z     s,   s, x       , the dictionary costs ds = dx are equal, the pointer
costs cz
q ) whenever
pointers p = q = (l, h) refer to the same location and use the same string (or character) h. then
there is an optimal compression d = (s, p   p ) in which s contains at most one member of   .

q) are equal when p = (l, s) and q = (l, x), and cs

p =   cx

p = cx

p = cz

p =   cz

q (   cs

q (  cz

q

p = cdk

proof. suppose that the conditions for theorem 1 hold, let    be an equivalence class, let d =
(s, p,   p ) be an optimal compression, and suppose for the sake of contradiction that s1, s2       
are both included in the optimal dictionary. without loss of generality we assume that |s1| < |s2|.
consider    rst document pointer p which uses s1 for document dk. by assumption there is another
so we are indifferent in our choice.
pointer q which uses s2 in the same location and cdk
we thereby may replace all document pointers that use s1 with equivalent ones that use s2 without
changing the objective value.
consider next the usage of s1 to construct higher order dictionary elements. we must be careful
here since if some dictionary element s3 is in the optimal dictionary s and can be expressed as
s3 = zs1 for some string z then we may not use s2 in place of s1 since it would lead to a different
dictionary string. the key step here is to realize that s3 must belong to the same equivalence class as
string zs2 and we can use zs2 in place of s3 in all documents. if s3 is itself used to construct higher
order dictionary elements, we can apply the same argument for s2 to zs2 in an inductive manner.
eventually, since our text is    nite, we will reach the highest order strings in the dictionary, none of
whose equivalence class peers construct any other dictionary id165s. our earlier argument shows
that we can simply take the longest of the highest order id165s that belong to the same equivalence
class. going back to s3, we note that our assumptions imply that the cost of constructing zs2 is
identical to the cost of constructing s3 so we may safely replace s3 with zs2. the only remaining
place where s1 may be used now is to construct s2. however, our assumptions imply that the cost of
constructing s1    in place    when constructing s2 is the same. by eliminating s1 we therefore never
can do worse, and we may strictly improve the objective if ts1 > 0 or s1 is used to construct s2 and
its pointer cost is non-zero. qed.

a.2 weighted diffusion
when g is generated from the relaxation of dracula and t     (0, 1]|s| are the dictionary coef   cients,
any s     s with ts < 1 will have gsz     ts   z     s. in order to prevent overly attenuating the
diffusion we may wish to normalize row s in g by t   1
for consistency. we note that a variety of
other weightings are also possible to different effects. for example, weighting g by a scalar        0
attenuates or enhances the entire diffusion process and mitigates or enhances the effect of features
the farther away they are from directly constructing any feature directly used in the documents.

s

a.3 proof of path theorem

theorem of id135 states that for any c     rd, s(c, q)    
the fundamental
arg minx   q xt c(  )     f (q) since q has non-empty interior and is therefore non-empty. we will
use a construction known as the normal fan of q, denoted by n (q), that partitions rd into a    nite
set of polyhedral cones pertaining to (linear) objectives for which each face in f (q) is the solution
set. we begin with some helpful de   nitions.

a partition p     2x of a set x is any collection of sets satisfying(cid:83)

p   p p = x and    p, q     p p (cid:54)= q
implies p   q =    . the relative interior of a convex set x     rd, denoted by relint x, is the interior
of x with respect to its af   ne hull. formally, relint x = {x     x |       > 0, b(x,   )     affx     x}.
the following de   nition is taken from (lu & robinson (2008)): a fan is a    nite set of nonempty
polyhedral convex cones in rd, n = {n1, n2, . . . , nm}, satisfying:

13

published as a conference paper at iclr 2016

1. any nonempty face of any cone in n is also in n ,
2. any nonempty intersection of any two cones in n is a face of both cones.

this de   nition leads to the following lemma, which is adapted from (lu & robinson (2008)):

lemma 1. let n be a fan in rd and s =(cid:83)

n   n n the union of its cones.

1. if two cones n1, n2     n satisfy (relintn1)     n2 (cid:54)=     then n1     n2,

2. the relative interiors of the cones in n partition s, i.e.(cid:83)

n   n relintn = s.

lemma 1 is subtle but important as it contains a key geometric insight that allow us to prove our
theorem. next, let q     rd be a bounded polyhedron with vertex set v and nonempty interior,
i.e. whose af   ne hull is d-dimensional. for any face f     f (q) de   ne v (f ) = f     v to

be the vertices of f and nf = (cid:8)y     rd |    x     f,   z     q, yt x     yt z(cid:9) to be the normal cone
(cid:8)y     rd |    x     v (f ),   z     v, yt x     yt z(cid:9). the normal fan for q, n (q) = {nf}f   f (q), is

to f . that nf is a (pointed) polyhedral cone follows from noting that it can be equivalently
expressed as a    nite collection of linear constraints involving the vertices of f and q: nf =

de   ned to be the set of all normal cones for faces of q. noting that q is bounded and therefore has
a recession cone of {0}, the following lemma is implied by proposition 1 and corollary 1 of (lu &
robinson (2008)):
lemma 2. let n (q) be the normal fan of a bounded polyhedron q with non-empty interior in r.
then

1. n (q) is a fan,
2. for any nonempty faces f1, f2     f (q), f1     f2 iff nf1     nf2,

3. (cid:83)
4. every nonempty face f     f (q) satis   es relintnf =(cid:8)y     rd | f = s(y, q)(cid:9).

f   f (q) relintnf = rd,

we will also makes use of the following two results. the    rst is implied by theorem 2.7, corollary
2.14, and problem 7.1 in ziegler (1995):
lemma 3. let q     rd be a bounded polyhedron with nonempty interior, f     f (q), and nf the
normal cone to f . then dim f + dim nf = d.
the second lemma states a kind of neighborliness for the cones in n (q):
lemma 4. let q     rd be a bounded polyhedron with nonempty interior. for any n     n (q)
and x     relint n there exists a    > 0 such that for any y     b(x,   ) there is a n(cid:48)     n (q) with
y     relint n(cid:48) and n     n(cid:48).

proof. let n     n (q) and x     n be given. we say that n(cid:48)     n (q) occurs within    (for    > 0) if
there is some y     b(x,   ) with y     relint n(cid:48). now suppose that there is an n(cid:48)     n (q) that occurs
within    for all    > 0. since n(cid:48) is a closed convex cone it must be that x     n(cid:48) so we may conclude
from lemma 1 that n     n(cid:48). next, let m be the set of cones in n (q) which do not contain n and
suppose that for all    > 0 there is some n(cid:48)     m that occurs within   . since |m| is    nite, this is
only possible if there is a cone n(cid:48)     m that occurs within    for all    > 0. however, this leads to a
contradiction since n(cid:48) must contain n so the lemma follows.

we are now ready to prove our main theorem which is restated below with s(c, q) =
arg minx   q xt c(  ) for simplicity.
theorem 3. let q     rd be a bounded polyhedron with nonempty interior and c : [0, 1]     rd a
continuous function. then for some n     z+     {   } there exists a countable partition    = {  i}n
of [0, 1] with corresponding faces fi     f (q) satisfying fi
(cid:54)= fi+1 and fi     fi+1 (cid:54)=     and
fi = s(c(  ), q)            i. moreover, fi never has the same dimension as fi+1 and the boundary
between   i,   i+1 is )[ iff dim fi < dim fi+1 and ]( otherwise.

i=0

14

published as a conference paper at iclr 2016

i

(cid:9)nk

i     f (q) satisfying f k

i = s(c(  ), q)            k
i .

(cid:8)x     [0, 1] | dim nf (x)     k(cid:9) to be the set of all arguments to c whose normal cone is at least k-
sis that for some nk     z+     {   } there exists a countable partition   k = (cid:8)  k

proof. for ease of notation let f (x) = s(c(x), q) and for k = 0, . . . , d de   ne   k =
dimensional. moreover, for any x     [0, 1] de   ne   (x) = {y     [0, x] |    z     [y, x], f (x) = f (z)}    
{y     [x, 1] |    z     [x, y], f (x) = f (z)} to be the largest contiguous set containing x over which f
remains constant and let m(x) = inf   (x) and m (x) = sup   (x) be its in   nimum and supremem,
respectively. the proof follows by induction on k = d, d     1, . . . , 0 with the inductive hypothe-
i=0 of   k with

there are two cases to consider.
thus,

corresponding faces f k
base case (k = d): let x       d so that   (x)       d. since nf (x) is d-dimensional, int nf (x) =
relint nf (x) so continuity of c implies that   (x) is a (non-empty) open interval with m(x) < m (x).
it follows that   k = {  (x) | x       d} de   nes a partition of   d into a set of open intervals. each in-
terval contains (an in   nite number) of rational numbers, and we see that   k is countable by assigning
to each interval a rational number that it contains.
inductive step: let x       k\  k+1.
if m(x) <
m (x) then (m(x), m (x))       (x) contains a rational number.
o =
{  (x) | x       k\  k+1, m(x) < m (x)} is countable. otherwise, if m(x) = x = m (x) then
by lemma 4 there is a    > 0 such that if y     b(x,   ) then nf (x)     ns(y,q). conti-
nuity of c implies that there is a    > 0 for which c((x       , x +   ))     b(x,   ) and hence
(x       , x +   )\{x}       k+1. assigning to x any rational number in (x       , x +   ) and letting
c = {  (x) | x       k\  k+1, m(x) = m (x)}, we may appeal to the inductive hypothesis to con-
  k
c       k+1 is a    nite union of countable sets and
clude that   k
therefore countable.
since   0 = [0, 1] we have shown that    =   0 is a countable partition of [0, 1] into intervals over
which f is constant. now consider two consecutive intervals   i,   i+1        and let m be the supre-
mum of   i. if m /      i then since cone nfi is closed, c(m )     nfi. since c(m )     relint nfi+1 by
assumption, it follows that nfi+1 is a proper subset of nfi and hence that fi is a proper subset of
fi+1. otherwise, if m       i then the continuity of c and lemma 4 imply that nfi is a proper subset
of nfi+1 so fi+1 is a proper subset of fi. in either case fi     fi+1 (cid:54)=     and lemma 3 implies the
dimensionality result of our theorem.

c is countable. finally,   k =   k

the set   k

o       k

15

