what a nerd! beating students and vector cosine 

in the esl and toefl datasets 

 

enrico santus*, tin-shing chiu*, qin lu*, alessandro lenci  , chu-ren huang* 

 

esantus@gmail.com, cstschiu@comp.polyu.edu.hk, {qin.lu, churen.huang}@polyu.edu.hk 

* the hong kong polytechnic university, hong kong 

   university of pisa, italy 
alessandro.lenci@unipi.it 

abstract 

in  this  paper,  we  claim  that  vector  cosine       which  is  generally  considered  one  of  the  most  efficient  unsupervised  measures  for 
identifying word similarity in vector space models     can be outperformed by a completely unsupervised measure that evaluates the 
extent of the intersection among the most associated contexts of two target words, weighting such intersection according to the rank of 
the shared contexts in the dependency ranked lists. this claim comes from the hypothesis that similar words do not simply occur in 
similar  contexts,  but  they  share  a  larger  portion  of  their  most  relevant  contexts  compared  to  other  related  words.  to  prove  it,  we 
describe and evaluate apsyn, a variant of average precision that     independently of the adopted parameters     outperforms the vector 
cosine and the co-occurrence on the esl and toefl test sets. in the best setting, apsyn reaches 0.73 accuracy on the esl dataset and 
0.70  accuracy  in  the  toefl  dataset,  beating  therefore  the  non-english  us  college  applicants  (whose  average,  as  reported  in  the 
literature, is 64.50%) and several state-of-the-art approaches. 
 
 
keywords: vector space models, vsms, distributional semantic models, dsms, semantic relations, words similarity 

 

1.  introduction 

word  similarity  detection  plays  an  important  role  in 
natural language processing (nlp), as it is the backbone 
of  several  applications,  such  as  id141,  query 
expansion,  word  sense  disambiguation,  automatic 
thesauri creation, and so on (terra and clarke, 2003). 
several approaches have been proposed to measure word 
similarity  (terra  and  clarke,  2003;  jarmasz  and 
szpakowicz,  2003;  mikolov  et  al.,  2013;  levy  et  al., 
2015;  santus  et  al.  2016a).  some  of  them  rely  on 
knowledge  resources  (such  as  lexicons  or  semantic 
networks), while others are corpus-based. 
the latter approaches generally exploit the distributional 
hypothesis,  according  to  which  words  that  occur  in 
similar  contexts  also  have  similar  meanings  (harris, 
1954). although  these approaches extract statistics from 
large corpora, they vary in the way they define what has to 
be  considered  context  (i.e.  lexical  context,  syntactic 
context, documents, etc.), how the association with such 
context  is  measured  (e.g.  frequency  of  co-occurrence, 
association measures like pointwise mutual information, 
etc.), and how the association with the contexts is used to 
identify  the  similarity  (terra  and  clarke,  2003;  hearst, 
1992; santus et al., 2014a; santus et al., 2014b; santus et 
al., 2016a). 
a common way to represent word meaning in nlp is by 
using vectors to encode the association between the target 
words  and  their  contexts.  the  resulting  vector  space  is 
generally referred as vector space model (vsm) or, more 
specifically, as distributional semantic model (dsm). in 
such  vector  space,  word  similarity  can  be  calculated  by 
using  the  vector  cosine,  which  measures  the  angle 

that  are 

implicitly 

learned  by 

between  the  vectors  (turney  and  pantel,  2010).  other 
measures       such  as  manhattan  distance,  dice   s 
coefficient,  euclidean  distance,  jaccard  similarity  and 
matching  coefficient       can  be  used  to  calculate  the 
distance between the vectors (gomaa and fahmy, 2013), 
but  the  vector  cosine  is  generally  considered  to  be  the 
optimal choice (bullinaria and levy 2007). 
another common way to represent word meaning is using 
word  embeddings,  which  are  vector-space  word 
representations 
the 
input-layer  weights  of  neural  networks.  these  models 
have  shown  a  strong  ability  to  capture  synonymy  and 
analogies (such as in the famous    king - man + woman = 
queen    example, where mikolov et al. (2013) subtract the 
vector of    man    from the one of    king   , and then add the 
vector of    woman   , obtaining a very similar vector to the 
one  of     queen   ),  even  though  levy  et  al.  (2015)  have 
claimed  that  traditional  count-based  dsms  can  achieve 
the  same  results  if  their  hyperparameters  are  properly 
optimized. 
a well-known problem with the distributional approaches 
is that they rely on a very loose definition of similarity. in 
fact,  vectors  have  as  nearest  neighbours  not  only 
synonyms, but also hypernyms, co-hyponyms, antonyms, 
as well as a wide range of other semantically related items 
(santus et al., 2015). 
for  this  reason,  several  datasets  have  been  proposed  by 
the  nlp  community  to  test  distributional  similarity 
measures.  among  the  most  common  ones,  there  are  the 
english  as  a  second  language 1 dataset  (esl;  turney, 

                                                           
1
state-of-the-art  on 

 for 

the 

the  esl, 

see: 

the 

2001)  and  the  test  of  english  as  foreign  language 2 
(toefl;  landauer  and  dumais,  1997).  the  former 
consists of 50 multiple-choice synonym questions, with 4 
choices  each,  while 
latter  consists  of  80 
multiple-choice synonym questions, with 4 choices each. 
in  this  paper,  we  describe  and  evaluate  apsyn,  a 
completely  unsupervised  measure  that  calculates  the 
extent  of  the  intersection  among  the  n  most  related 
contexts of two target words, weighting such intersection 
according to the rank of the  shared  contexts in a  mutual 
dependency ranked list. 
in our experiments, apsyn outperforms the vector cosine 
and the co-occurrence frequency, reaching 0.73 accuracy 
on  the  esl  dataset  and  0.70  accuracy  in  the  toefl 
dataset,  beating  therefore  the  non-english  us  college 
applicants (whose average, as reported in the literature, is 
64.50%) and several state-of-the-art approaches. 
 

2.  background 

based 

generally 

word similarity measures play a fundamental role in tasks 
such  as  information  retrieval  (ir),  text  classification 
(tc),  text  summarization  (ts),  question  answering 
(qa),  sentiment  analysis  (sa),  and  so  on  (terra  and 
clarke,  2003;  tungthamthiti  et  al.,  2015).  they  can  be 
either  knowledge-based  or  corpus-based  (gomaa  and 
fahmy,  2013).  the  former  rely  on  lexicons  or  semantic 
networks, such as id138 (fellbaum, 1998), measuring 
the distance between the nodes in the network. the latter, 
instead, compute the similarity between words relying on 
statistical  information  about  their  distributions  in  large 
corpora (church and hanks, 1990). 
knowledge 
exploit 
approaches 
hand-crafted resources. while being hand-crafted ensures 
high  quality,  it  also  entails  arbitrariness  and  high 
development  and  update  costs.  this  is  the  main  reason 
why these resources are known for their limited coverage 
(santus et al., 2015b). such limitation has often prompted 
researchers to pursue hybrid approaches (turney, 2001). 
a  key  assumption  of  corpus-based  approaches  is  that 
similarity between words can be measured by looking at 
words  co-occurrences.  in  particular,  following 
the 
distributional  hypothesis  (harris,  1954;  firth,  1957), 
these  methods  assume  that  words  occurring  in  similar 
contexts  are  also  similar.  these  methods  mainly  vary 
according  to  two  dimensions:  i)  how  they  define  the 
contexts  (e.g.  document,  paragraph,  sentence,  fixed-size 
window,  etc.);  and  ii)  how  they  measure  whether  the 
targets occur in similar contexts (e.g. weighted occurrence 
frequency, extent of the intersection, etc.). these models 
are generally referred as traditional count-based dsms. 
a  well-known  traditional  count-based  dsm  applied  to 
synonymy identification is the  latent semantic analysis 

                                                                                               
http://aclweb.org/aclwiki/index.php?title=toefl_synon
ym_questions_(state_of_the_art) 
2  for 
the  state-of-the-art  on 
http://aclweb.org/aclwiki/index.php?title=esl_synonym
_questions_(state_of_the_art) 

the  toefl,  see: 

(lsa;  landauer  and  dumais,  1997).  this  system  was 
tested on the 80 multiple-choice synonym questions of the 
toefl, achieving an accuracy of 64.38%, which is very 
close to the reported average of non-english us college 
applicant (i.e. 64.50%). 
another  interesting  way  to  learn  words  statistics  is 
generally referred as id27s and is discussed in 
mikolov  et  al.  (2013).  the  authors  report  that  when  a 
neural  network  language  model  is  trained,  it  is  not  only 
the  model  that  is  obtained,  but  also  distributed  words 
representations,  which  can  be  eventually  used  for  other 
goals,  such  as  in  collobert  and  weston  (2008).  in  their 
paper,  mikolov  et  al.  (2013)  show  that  these  words 
representations  capture  both  syntactic  and  semantic 
regularities,  performing  particularly  well 
in  word 
similarity identification and analogies. 
while  such  models  have  obtained  an  enthusiastic 
reception, with a consequent boost of papers using word 
embeddings,  levy  et  al.  (2015)  have  demonstrated  that 
similar  results  can  be  also  obtained  with  optimized 
traditional count-based dsms. 
 

2.1  distance measures 
independently  from  the  approach  that  is  used  to  learn 
words statistics, corpus-based approaches represent word 
meanings  as  vectors  in  vector  spaces,  generally  called 
semantic  spaces.  in  such  semantic  spaces,  words 
similarity  can  be  measured  as  the  proximity  between 
vectors.  several  measures  have  been  adopted  to  this 
scope. in the following rows, we briefly describe some of 
them,  while  defining  the  vector  cosine,  which  is 
generally considered the most efficient one. 
manhattan distance (l1) can be defined as the sum of the 
differences of the dimensions. euclidean distance (l2) is 
the  square  root of  the  sum  of  the  squared  differences  of 
the  dimensions.  dice   s  coefficient  is  instead  twice  the 
number  of  common  dimensions,  divided  by  the  total 
number  of  dimensions  in  the  two  vectors.  jaccard 
similarity is defined as the number of shared dimensions 
divided by the number of unique dimensions in both the 
vectors.  matching  coefficient 
the  number  of 
dimensions different from zero in both the vectors. vector 
cosine  looks  instead  at  the  normalized  correlation 
between the dimensions of two words,     1 and     2, and is 
described by the following equation: 
 

is 

cos(    1,     2) =  

        1       

    
    =1

    2    

      (    1    )2          (    2    )2

 

 
where              is the i-th dimension in the vector x. 
this measure has been extensively used to identify word 
similarity  in  vector  spaces  becoming  a  sort  of  de  facto 
standard  in  distributional  semantics  (landauer  and 
dumais, 1997; jarmasz and szpakowicz, 2003; mikolov 
et al., 2013; levy et al., 2015). 
 
 
 

2.2  state-of-the-art in the esl and toefl 
after  its  first  use  in  landauer  and  dumais  (1997),  the 
toefl  dataset  became  one  of  the  most  common 
benchmarks for vector space models testing: karlgren and 
sahlgren (2001), pado and lapata (2007), turney (2001), 
turney  (2008),  terra  and  clarke  (2003),  bullinaria  and 
levy  (2007),  matveeva  et  al.  (2005),  dob    and  csirik 
(2013) and rapp (2003). bullinaria and levy (2012) even  
achieved  100%  accuracy  on  this  dataset.  in  their  paper, 
the  authors  extensively  analyze  numerous  parameters, 
including  the  influence  of  corpus  size,  window  size, 
stop-lists, id30 and singular values decomposition 
(svd), until they find a perfectly optimized model. after 
achieving  perfect  precision  on  the  toefl,  the  authors 
acknowledge  that  while  these  results  are  impressive  for 
the  benchmark,  they  can  hardly  be  generalized  to  new 
tasks. 
few  years  after  the  introduction  of  toefl  as  a 
benchmark,  turney  (2001)  proposed  the  esl.  these  50 
multiple-choice  synonym  questions  are  provided  in  a 
sentence context, to facilitate sense disambiguation. esl 
has  soon  become  a  very  popular  benchmark  on  which 
several  models  have  been  evaluated.  the  best  reported 
corpus-based approaches in this benchmark were those of 
turney (2001), terra and clarke (2003) and jarmasz and 
szpakowicz (2003). the latter achieving the best result of 
82% accuracy. 
 

3.  method 

1954) 

(harris, 

given a traditional count-based dsm, where every word 
is  represented  as  a  vector  of  weighted  associations 
between  such  word  and  contexts,  we  can  re-think  the 
distributional  hypothesis 
by 
hypothesizing that similar words not only occur in similar 
contexts,  but       more  specifically       they  share  a  larger 
number  of  their  most  associated  contexts,  compared  to 
less similar ones. 
a way to test this hypothesis is by: i) measuring the extent 
of the intersection among the n most related contexts of 
two  target  words,  and  ii)  weighting  such  intersection 
according  to  the  rank  of  the  shared  contexts  in  the 
dependency  ranked  lists.  this  can  be  done  in  several 
steps.  first  of  all,  for  every  target  word  we  rank  the 
contexts  according  to  the  positive  pointwise  mutual 
information (ppmi; levy et al., 2015), which is defined 
as follows: 
 

            (    ,     ) = log (

) =   log (

    (    ,     )

    (    )        (    )

|    ,     |        
|    |    |    |

) 

 

 

                (    ,     ) = max (            (    ,     ), 0) 

were      is  the  target  word,      is  the  given  context,     (    ,     ) 
is the id203 of co-occurrence and      is the collection 
of observed word-context pairs. 
once the contexts are ranked according to their ppmi, for 
every  target  word  we  pick  the  top  n  contexts  and  we 

intersect them. at this point, for each shared context, we 
add one divided by the average rank of the shared context 
in  the  ppmi-ranked  contexts  lists.  we  formalize  this  as 
the apsyn similarity measure: 
 

                    (    1,     2) =      

           (    1)       (    2)

1

 
(                1(    ) +                 2(    ))/2 

 
for  every  feature      included  in  the  intersection  between 
the top n features of     1,     (    1), and     2,     (    2), apsyn will 
add 1 divided by the average rank of the feature, among 
the  top  ppmi  ranked  features  of     1,                 1(    1),  and     2, 
                2(    2). 
the  choice  of  the  weighting  function  is  a  parameter  of 
apsyn. in previous experiments, published in santus et al. 
(2016a), we used local mutual information (lmi; evert, 
2005)  to  rank  the  contexts,  instead  of  using  ppmi. 
however, the lmi-ranked apsyn obtained worse results 
than those reported in the current paper. such results were 
nonetheless still outperforming the vector cosine and the 
co-occurrence frequency. in section 9, we will comment 
on the differences. 
 

4.  evaluation 

in  the  following  paragraphs  we  describe  our  dsms,  the 
test sets and the task. 
 

4.1  distributional semantic model 
we  use  several  window-based  dsms,  recording  word 
co-occurrences within the k nearest content words to the 
left  and  right  of  each  target,  where  k  has  the  following 
values: 2, 3, 5 and 10. co-occurrences are extracted from 
a  combination  of  ukwac  and  wackypedia  corpora 
(around  2.7  billion  words)  for  content  words       namely 
adjectives, nouns and verbs     occurring over 1,000 times, 
and  are  weighted  with  ppmi.  the  model  consists  of 
28,870  word  vectors,  each  of  which  with  28.870 
dimensions. 
 

4.2  test sets 
in  order  to  evaluate  the  proposed  measure,  we  use  both 
the  esl  (turney,  2001)  and  toefl  (landauer  and 
dumais,  1997)  datasets.  the  former  consists  of  50 
questions,  while  the  latter  of  80  questions.  the  esl 
sentences were not used in our experiments. an example 
of esl question is the following: 
 

    

   an underground [passage] connected the house 
to the garage.    

a.  hallway 
b.  ticket 
c.  entrance 
d.  room 

 
for  both  datasets,  we  have  turned  each  question  in  four 
pairs,  each  of  which  containing  the  problem  word  and 

over 

frequency 

apossible  answer.  unfortunately,  we  do  not  have  a  full 
coverage of the datasets, because our model was built for 
content  words  with 
1000, 
parts-of-speech-tagged  either  as  adjectives,  nouns  or 
verbs.  in  the  esl  test  set,  4  out  of  50  questions  were 
excluded because the correct answers were not present in 
the dsm. in the toefl test set, 20 out of 80 questions 
were  excluded  for  the  same  reason.  few  questions, 
moreover, have one missing choice. in order to keep them 
for the evaluation, in case of correct answer, the score is 
increased  of  0.25     |                                                 | (where,  1  is  added 
only if all four choices are in the dsm). 
 

4.3  task 
we have assigned apsyn scores to all the pairs, and then     
for  every  problem  word       we  have  sorted  the  possible 
choices  in  a  decreasing  order.  we  considered  positive 
every  problem  word  having  the  correct  answer  on  top, 
negative all the others. 
 

5.  results 

in  table  1,  we  report  the  results  of  apsyn  and  the 
baselines in the esl test. 
 

apsyn  win 2 
0.73 
n=100 
0.71 
n=200 
0.71 
n=300 
n=400 
0.71 
0.71 
n=500 
0.71 
n=600 
0.71 
n=700 
0.71 
n=800 
n=900 
0.69 
0.67 
n=1000 

cosine 
co-occ 

0.46 
0.43 

win 3 
0.69 
0.69 
0.69 
0.69 
0.69 
0.69 
0.69 
0.69 
0.65 
0.62 
baselines 
0.46 
0.41 

win 5 
0.67 
0.67 
0.69 
0.67 
0.67 
0.65 
0.65 
0.67 
0.67 
0.65 

0.48 
0.39 

win 10 

0.62 
0.62 
0.62 
0.62 
0.62 
0.62 
0.67 
0.67 
0.65 
0.60 

0.48 
0.35 

table 1. accuracy in the esl test set for apsyn 
(100<n<1000), vector cosine and co-occurrence, 

in window 2, 3, 5 and 10 dsms. 

 
as it can be seen from table 1 and from figure 1, apsyn 
always outperforms the baselines. 
the  window  size  and  n  have  a  certain  impact  on  the 
performance  of  apsyn.  the  former  parameter  has  an 
impact  also  on  the  baselines  (vector  cosine  seems  to 
perform  slightly  better  for  larger  windows,  while  the 
co-occurrence  frequency  seems  to  prefer  smaller  ones). 
our  measure,  in  particular,  seems  to  perform  better  on 
smaller  windows  and  for  n  close  to  100,  while  its 
performance slightly drops for n close to 1000. 
a possible reason for such drop may be that if too many 
contexts  are  considered,  some  rumor  is  added.  this 
happens because with larger values of n, apsyn is forced 
to consider less important contexts of the targets. 
in  table  2,  we  report  the  scores  for  apsyn  and  the 
baselines in the evaluation on the toefl test set (see also 

figure  2).  apsyn  outperforms  the  baselines,  especially 
when the window size and n are small. interestingly, the 
vector  cosine  prefers  a  smaller  window  in  this  dataset 
(this is actually what we would have expected also for the 
esl,  as  smaller  windows  are  known  to  better  capture 
paradigmatic similarity). 
 

figure 1. accuracy in the esl test set for apsyn 
(100<n<1000), vector cosine and co-occurrence, 

in window 2, 3, 5 and 10 dsms. 

 
 

 
 

apsyn  win 2  win 3  win 5  win 10 
n=100 
n=200 
n=300 
n=400 
n=500 
n=600 
n=700 
n=800 
n=900 
n=1000 

0.68 
0.68 
0.70 
0.70 
0.70 
0.68 
0.68 
0.67 
0.67 
0.65 

0.59 
0.61 
0.61 
0.61 
0.61 
0.61 
0.62 
0.62 
0.62 
0.59 

0.67 
0.65 
0.67 
0.67 
0.67 
0.67 
0.65 
0.67 
0.65 
0.67 

0.62 
0.62 
0.62 
0.62 
0.62 
0.62 
0.62 
0.62 
0.66 
0.68 

baselines 

cosine 
co-occ 

0.58 
0.45 

0.53 
0.41 

0.50 
0.44 

0.46 
0.45 

table 2. accuracy in the toefl test set for apsyn 
(100<n<1000), vector cosine and co-occurrence, 

in window 2, 3, 5 and 10 dsms. 

figure 2. results in the toefl test set 

for apsyn (with n ranging from 100 to 1000, 
with increments of 100), vector cosine and 

co-occurrence, in different dsms. 

 
while considering the results, it must be kept in mind that 
those that may look as high variances, could be actually 

be very small ones, given the small  size of the test  sets. 
guessing one more question, for example, would have a 
large impact on the accuracy. 
 

answers.  namely  the  real  synonym  is  typically  ranked 
second  rather  than  first,  and  the  error  is  mostly  due  to 
sense ambiguity. 
 

6.  error analysis 

in  this  section  we  briefly  analyse  the  best  and  worst 
performance of apsyn in the esl dataset. 
from  the  previous  section,  we  have  seen  that  apsyn 
performs best with a window 2 dsm and n=100. in table 
3, we report the non-identified synonyms (with their parts 
of  speech:  n=noun,  v=verb  and  j=adjective),  with  their 
actual rank in the question pairs. as it can be seen, most of 
the non-identified synonyms are ranked second according 
to apsyn, and they are generally placed after a word that     
at least in certain contexts     has a very similar meaning to 
limb-trunk, 
the  problem  word 
passage-entrance,  refer-call,  scrape-slice,  steep-rugged, 
twist-curl).  unfortunately,  we  have  not  used 
the 
contextual sentences to disambiguate the problem words. 
 

(e.g.  grind-slice, 

problem 

word 
grind-v 

harvest-n 

limb-n 

1st 

slice-v 
stem-n 
trunk-n 

passage-n 

entrance-n 

syrup-n 
call-v 
slice-v 
rugged-j 

paste-n 
refer-v 
scrape-v 
steep-j 

substance-

n 

2nd 

rub-v 

intake-n 
branch-n 
hallway-n 
dough-n 
explain-v 
grate-v 
bare-j 

3rd 

4th 

hit-v 
tap-v 
split-n 
lump-n 
bark-n 
twig-n 
ticket-n 
room-n 
block-n 
jelly-n 
direct-v 
carry-v 
chop-v  mince-v 
sheer-j 
posture-

--- 

score-n 

n 

level-n 

thing-n 

twist-v 

curl-v 

clip-v 

fasten-v 

intertwi

ne-v 

yield-v 

scorn-v 

challenge-

submit-

boast-v 

v 

v 

table 3. non-identified esl synonyms for apsyn 

(window 2 and n=100), with their actual rank. 

the correct synonym is reported in bold. 

 
in table 4, we report the non-identified synonyms in the 
worst setting of apsyn, namely window 10 and n=1000. 
as it can be seen, most of those questions for which the 
synonym was not identified in the best setting (reported in 
italics in table 4) are kept as errors also in the worst one. 
it is also interesting to notice that not only the number of 
errors  increased,  but  also  that  the  positions  of  the  true 
synonyms in the error are lower than in the best setting. 
even  for  questions  that  were  already  wrong  in  the  best 
setting, the correct synonym was further penalized, losing 
a position (e.g. refer-direct and yield-submit). finally, it is 
evident  in  table  4  that  several  new  non-identified 
synonyms are introduced, affecting negatively the overall 
performance. 
to summarize, window 2 and n close to 100 are certainly 
the  best  parameters.  not  only  because  they  improve  the 
overall  accuracy,  but  also  because,  when  making  a 
mistake, they have a closer approximation to the correct 

problem 

word 

applause-

n 

1st 

2nd 

3rd 

4th 

shame-n 

approval-n 

fear-n 

friend-n 

approve-v 

anger-v 

boast-v 

scorn-v 

grind-v 
harvest-n 
hinder-v 
limb-n 
lump-n 

slice-v 
stem-n 
yield-v 
trunk-n 
limb-n 

rub-v 

intake-n 
assist-v 
branch-n 
stem-n 

mass-n 

element-n 

service-n 

passage-n 

entrance-n 

paste-n 

syrup-n 

hallway-n 
dough-n 

refer-v 

carry-v 

call-v 

scorn-v 
scrape-v 
steep-j 

substance-

n 

tap-v 

avoid-v 
chop-v 
rugged-j 

enjoy-v 
grate-v 
bare-j 

level-n 

thing-n 

knock-v 

drain-v 

tap-v 
split-n 
relieve-v 
bark-n 
trunk-n 
worship

-n 

room-n 
jelly-n 
explain-

v 

plan-v 
slice-v 
sheer-j 
posture-

n 

rap-v 

twist-v 

clip-v 

fasten-v 

curl-v 

verse-n 

branch-n 

twig-n 

weed-n 

yield-v 

challenge-

v 

scorn-v 

boast-v 

support

-v 

hit-v 

lump-n 
block-v 
twig-n 
chunk-n 

lump-n 

ticket-n 
block-n 

direct-v 

refuse-v 
mince-v 

--- 

score-n 

boil-v 
intertwi

ne-v 

section-

n 

submit-

v 

table 4. non-identified esl synonyms for apsyn 

(window 10 and n=1000), with their actual rank. the 
correct synonym is reported in bold. errors that were 

present also in the best model are reported in italics, while 

errors that were present in the best, but absent in the 

worst, are reported with strikethrough.  

 
the  worst  performance  of  apsyn  is  obtained  for  large 
window  and  large  n  (window  10  and  n=1000).  in  the 
error  analysis,  we  have  seen  that  the  worst  model  has 
generally bigger difficulty in classifying the synonym as 
somehow  similar  to  the  question  word,  often  ranking  it 
fourth. 
 

7.  comparison with lmi-based apsyn 

apsyn was introduced in santus et al. (2016a) and tested 
on the esl. in this paper,  the top features were selected 
after ranking them by lmi instead of ppmi. lmi is less 
biased  towards  infrequent  events  than  ppmi  and  it  is 
defined as follows: 
 

            (    ,     ) =     (    ,     )                    (    ,     ) 

 
where      is the target word,      is the given context,     (    ,     ) 
is the id203 of co-occurrence, as shown in the ppmi 
formula, above. 

as mentioned above, the performance of the lmi-based 
apsyn on a window 5 dsm was worse than what reported 
with  ppmi.  however,  its  58.33%  accuracy  was  much 
above  the  vector  cosine,  which  was  instead  blocked  at 
49.44%. 
in  table 5  we  show all the scores, recalculated  with the 
lmi-based apsyn. note that the recall in the models used 
for  this  paper  is  slightly  higher,  reaching  46  questions 
rather than 45, so the scores can be slightly different. 
despite results are worse than those obtained with ppmi, 
they are however relatively stable with reference to n, and 
almost always above the baseline. only with window 10 
and  n  close  to  100  the  performance  is  the  equal  to  the 
vector cosine. 
 
 

apsyn  win 2 
n=100 
0.61 
0.65 
n=200 
0.68 
n=300 
0.66 
n=400 
n=500 
0.66 
0.66 
n=600 
0.66 
n=700 
0.66 
n=800 
0.65 
n=900 
n=1000 
0.64 

cosine 
co-occ 

0.46 
0.43 

win 3 
0.59 
0.61 
0.61 
0.64 
0.59 
0.59 
0.59 
0.57 
0.57 
0.57 
baselines 
0.46 
0.41 

win 5 
0.573 
0.48 
0.57 
0.50 
0.53 
0.53 
0.53 
0.53 
0.53 
0.534 

0.48 
0.39 

win 10 

0.48 
0.48 
0.48 
0.52 
0.50 
0.50 
0.52 
0.52 
0.50 
0.50 

0.48 
0.35 

table 5. accuracy in the esl test set for apsyn 
(100<n<1000), vector cosine and co-occurrence, 

in window 2, 3, 5 and 10 dsms. 

 

8.  conclusions 

in  this  paper,  we  have  described  apsyn,  a  completely 
unsupervised  measure  based  on  the  evaluation  of  the 
extent and the relevance of the intersection among the top 
ranked distributional features of target words. apsyn was 
tested on the esl and toefl questions, outperforming 
the  vector  cosine  and  the  co-occurrence,  plus  several 
lexicon-based  and  hybrid  models.  in  particular,  our 
results  are  above  those  reported  in  the  literature  for 
non-english  us  college  applicants  on  the  toefl  test 
(64.50%). 
our experiments show that the intersection among the n 
most  related  contexts  of  the  target  words  is  in  fact  a 
reliable  index  of  similarity.  in  our  evaluations  we  have 
also  mentioned  the  role  of  both  the  window  size  and  n. 
apsyn  performs  better  on  smaller  windows  and  with  n 
                                                           
3  not  differently  from  santus  et  al.  (2016a),  the 
lmi-based apsyn guessed 26.25 questions (24 full and 3 
partial),  but  being  the  recall  higher  in  the  current  dsm, 
this number has been divided by 46. 
4  not  differently  from  santus  et  al.  (2016a),  the 
lmi-based apsyn guessed 24.25 questions (22 full and 3 
partial),  but  being  the  recall  higher  in  the  current  dsm, 
this number has been divided by 46. 

close to 100. in fact, the larger the amount of considered 
contexts,  the  lower  the  ability  of  identifying  similarity 
(exceptions to this consideration are minimal and can be 
appreciated  only  because  of  the  limited  size  of  the  test 
sets). this also confirms our hypothesis that similar words 
share  a  significantly  larger  number  of  top  mutually 
dependent  contexts,  but  such  intersection  becomes  less 
significant when not only the top contexts are considered, 
as rumor is introduced. given that, it is important to notice 
that  apsyn  performance  is  quite  stable,  in  respect  to  n 
variances. 
apsyn  has  been  recently  used  as  one  of  the  thirteen 
features  of  root13,  a  random-forest  based  supervised 
system for the identification of hypernyms, co-hyponyms 
and unrelated words. in a 10-fold evaluation on 9600 pairs 
extracted  from  evaluation  (santus  et  al.,  2015a), 
root13 achieved 88.3% accuracy when the three classes 
were  present,  93.4% 
for  hypernyms-co-hyponyms 
discrimination, 
hypernyms-random 
discrimination,  97.3%  for  co-hyponyms-random  (santus 
et al., 2016b). 
possible improvements to the  measure include changing 
the  numerator  to  a  more  significant  value,  rather  than 
simply  using  the  constant  1.  moreover,  it  would  be 
important to test the measure on optimized dsms, where 
more  parameters  are 
investigated  (e.g.  id30, 
dependency,  svd,  etc.).  moreover,  since  esl  and 
toefl are small test sets, apsyn performance should be 
further  explored  on 
the 
lenci/benotto  (benotto,  2015),  siid113x-999  (hill  et  al., 
2014) and evaluation (santus et al., 2015a). 
 

larger  datasets,  such  as 

92.3% 

for 

9.  acknowledgements 

this work is partially supported by hk phd fellowship 
scheme under pf12-13656 
 

10.  main references 

baroni,  m.  and  lenci,  a.  (2011).  how  we  blessed 
distributional  semantic  evaluation.  proceedings  of  the 
emnlp  2011  geometrical  models 
for  natural 
language  semantics 
(gems  2011)  workshop. 
edinburg, uk. 1-10.  

benotto,  giulia.  (2015).  distributional  models 

for 
semantic  relations:  a  sudy  on  hyponymy  and 
antonymy. phd thesis, university of pisa. 

bullinaria,  j.a.,  and  levy,  j.p.  (2007).  extracting 
semantic  representations  from  word  co-occurrence 
statistics:  a  computational  study.  behavior  research 
methods, 39(3), 510-526. 

bullinaria,  j.a.,  and  levy,  j.p.  (2012).  extracting 
semantic  representations  from  word  co-occurrence 
statistics:  stop-lists,  id30,  and  svd.  behavior 
research methods, 44(3):890-907. 

church,  k.  ward,  and  patrick  hanks.  (1990).  word 
association 
and 
id69.  in  journal  computational  linguistics, 
vol. 16 (1). 

norms,  mutual 

information, 

collobert,  robert,  and  jason  weston.  (2008).  a  unified 
architecture  for  natural  language  processing:  deep 
neural  networks  with  multitask  learning. 
in 
international  conference  on  machine  learning, 
icml. 

dob  ,  a.,  and  csirik,  j.  (2013).     computing  semantic 
similarity  using  large  static  corpora   .  in:  van  emde 
boas,  p.  et  al.  (eds.)  sofsem  2013:  theory  and 
practice  of  computer  science.  lncs,  vol.  7741. 
springer-verlag, berlin heidelberg, pp. 491-502 

evert,  s.  (2005).  the  statistics  of  word  cooccurrences: 
word pairs and collocations. dissertation, university 
of stuttgart. 

fellbaum  c.  (1998).  id138:  an  electronic  lexical 

database. cambridge, ma: mit press. 

firth,  j.  r.  (1957).  papers  in  linguistics  1934   1951. 

london: oxford university press. 

gomaa, wael h. and aly a. fahmy. (2013). a survey of 
text similarity approaches. in international journal of 
computer applications, vol. 68 (13). 

harris, z. (1954). distributional structure. word, vol. 10 

(23). 146-162. 

hearst,  m.  a.  (1992).  automatic  acquisition  of 
hyponyms  from  large  text  corpora.  proceedings  of 
the  14th  international  conference  on  computational 
linguistics. nantes, france. 539-545. 

hill,  felix,  roi  reichart  and  anna  korhonen.  (2014). 
siid113x-999:  evaluating  semantic  models  with 
(genuine)  similarity  estimation.  in computational 
linguistics. 

jarmasz,  m.  and  szpakowicz,  s.  (2003).  roget   s 
thesaurus  and  semantic  similarity,  proceedings  of 
ranlp 2003, borovets, bulgaria. 212-219. 

landauer,  t.  k.  and  dumais,  s.t.  (1997).  a  solution  to 
plato's problem: the latent semantic analysis theory of 
the  acquisition, 
induction,  and  representation  of 
knowledge. psychological review, 104(2):211-240. 

levy,  o.,  goldberg,  y.  and  dagan  i.  (2015).  improving 
distributional  similarity  with  lessons  learned  from 
id27s. tacl 2015. 

matveeva,  i.,  levow,  g.,  farahat,  a.,  and  royer,  c. 
(2005).  generalized  latent  semantic  analysis  for  term 
representation.  proceedings  of 
international 
conference on recent advances in natural language 
processing (ranlp-05), borovets, bulgaria. 

the 

mikolov,  t.,  yih,  w.  and  zweig  g.  (2013).  linguistic 
space  word 
of  hlt-naacl, 

regularities 
representations.  proceedings 
746-751. 

continuous 

in 

karlgren,  j.,  and  sahlgren,  m.  (2001).     from  words  to 
understanding   . in uesaka, y., kanerva, p., & asoh, h. 
(eds.),  foundations  of  real-world 
intelligence, 
stanford: csli publications, pp. 294   308. 

pado,  s.,  and  lapata,  m.  (2007).  dependency-based 
construction of semantic space models. computational 
linguistics, vol. 33 (2), pp. 161-199. 

rapp,  r.  (2003).  word  sense  discovery  based  on  sense 
descriptor  dissimilarity.  proceedings  of  the  ninth 
machine translation summit, pp. 315-322. 

santus,  e.,  chiu,  t.-s.,  lu,  q.,  lenci,  a.,  and  huang, 
c.-r. 
(2016a). unsupervised  measure  of  word 
similarity:  how  to  outperform  co-occurrence  and 
vector cosine in vsms. in proceedings of aaai 2016, 
phoenix, arizona (usa) 

santus,  e.,  chiu,  t.-s.,  lu,  q.,  lenci,  a.,  and  huang, 
c.-r. 
(2016b). root  13:  spotting  hypernyms, 
co-hyponyms and randoms. in proceedings of aaai 
2016, phoenix, arizona (usa) 

santus,  e.,  yung,  f.,  lenci,  a.  and  huang  c-r.  (2015). 
evalution  1.0:  an  evolving  semantic  dataset  for 
training  and  evaluation  of  distributional  semantic 
models.  proceedings  of  the  4th  workshop  on  linked 
data in linguistics, acl-ijcnlp 2015, 64 

santus,  e.,  lenci,  a.,  lu,  q.,  and  huang  c-r. 
(2015). when  similarity  becomes  opposition: 
synonyms 
in 
dsms. italian  journal  on  computational  linguistics, 
aaccademia university press 

and  antonyms  discrimination 

santus,  e.,  lenci,  a.,  lu,  q.  and  schulte  im  walde,  s. 
(2014a).  chasing  hypernyms  in  vector  spaces  with 
id178.  proceedings  of  eacl  2014,  2:38   42, 
gothenburg, sweden. 

santus,  e.,  lu,  q.  lenci,  a.  and  huang,  c-r.  (2014b). 
taking  antonymy  mask 
in  vector 
space.   proceedings  of  paclic  2014,  phuket, 
thailand. 

off 

terra, e. and clarke, c.l.a. (2003). frequency estimates 
for statistical word similarity measures. proceedings of 
hlt/naacl 2003. 244   251. 

tungthamthiti,  piyoros,  enrico  santus,  hongzhi  xu, 
chu-ren  huang  and  kiyoaki  shirai.  2015.  sentiment 
analyzer  with  rich  features  for  ironic  and  sarcastic 
tweets. in proceedings of paclic 2015, 178-187. 

turney,  p.d.  and  pantel,  p.  (2010).  from  frequency  to 
meaning: vector space models of semantics. journal 
of articial intelligence research, vol. 37. 141-188.  

turney,  p.d.  (2008).  a  uniform  approach  to  analogies, 
synonyms, antonyms, and associations. proceedings of 
the  22nd  international  conference  on  computational 
linguistics  (coling  2008),  manchester,  uk,  pp. 
905-912. 

turney,  p.d.  (2001).  mining  the  web  for  synonyms: 
pmi-ir  versus  lsa  on  toefl.  proceedings  of 
ecml-2001, freiburg, germany. 491-502. 

 

language resource references 

esl.  in:  turney,  p.d.  (2001).  mining  the  web  for 
synonyms:  pmi-ir  versus  lsa  on  toefl. 
proceedings  of  ecml-2001,  freiburg,  germany. 
491-502. 

evalution.  in:  santus,  e.,  yung,  f.,  lenci,  a.  and 
huang  c-r.  (2015).  evalution  1.0:  an  evolving 
semantic  dataset  for  training  and  evaluation  of 
distributional semantic models. proceedings of the 4th 
workshop on linked data in linguistics, acl-ijcnlp 
2015, 64. 

lenci/benotto. 

in: 

benotto,  giulia. 

(2015). 

distributional models for semantic relations: a sudy 
on  hyponymy  and  antonymy.  phd  thesis,  university 
of pisa. 

siid113x-999.  in:  hill,  felix,  roi  reichart  and  anna 
korhonen.  (2014).  siid113x-999:  evaluating  semantic 
(genuine)  similarity  estimation. 
models  with 

in computational linguistics. 

toefl. in: landauer, t. k. and dumais, s.t. (1997). a 
solution  to  plato's  problem:  the  latent  semantic 
analysis  theory  of  the  acquisition,  induction,  and 
representation  of  knowledge.  psychological  review, 
104(2):211-240.

 

