vectorweavers at semeval-2016 task 10: from incremental meaning to

semantic unit

(phrase by phrase)

6
1
0
2

 
r
p
a
7
1

 

 
 
]
l
c
.
s
c
[
 
 

1
v
3
7
8
4
0

.

4
0
6
1
:
v
i
x
r
a

andreas scherbakov

ekaterina vylomova

fei liu

timothy baldwin

andreas@softwareengineer.pro, evylomova@gmail.com,

fliu3@student.unimelb.edu.au, tb@ldwin.net

the university of melbourne

vic 3010, australia

abstract

this paper describes an experimental ap-
proach to detection of minimal semantic
units and their meaning (dimsum), ex-
plored within the framework of semeval   16
task 10. the approach is primarily based on a
combination of id27s and parser-
based features, and employs unidirectional in-
cremental computation of compositional em-
beddings for multiword expressions.

1 motivation
this paper proposes an approach to the segmenta-
tion of pos-tagged english sentences into minimal
semantic units along with labelling of units with
semantic classes (   supersenses   ). supersenses are
   lightweight semantic annotation[s] of text originat-
ing in id138    (schneider et al., 2012). here, we
investigate two major ideas, as follows.

first, inspired by salehi et al. (2015) we hypoth-
esise that id27s (wembs), e.g. from
id97 (mikolov et al., 2013a), could be an ex-
tremely valuable resource of knowledge for guess-
ing the sense of the word. wembs have been shown
to represent distinguishable sense components learnt
from large training corpora. many papers have de-
scribed experiments with word meaning extraction
from id27s, and demonstrated that it   s
possible to detect semantic relations between words
based on them. additionally, they may be used to
simulate various types of relations between words
with simple linear operations over word vectors,
and for capturing morphological properties of words
(mikolov et al., 2013b; vylomova et al., 2015).

however, more generally, they lack any coherent
sense-to-value correspondence. according to this,
it   s natural to attempt to use wembs as a source of
word meaning information in predicting the senses
of semantic units, as part of the larger task of si-
multaneously detecting minimal semantic units and
assigning them meaning.
it should be noted that
the general term    multiword expression    (mwe)
embraces expressions of different types (baldwin
and kim, 2010). some of them are meaning-based
and the meaning may be preserved under substitu-
tion with synonyms, e.g.; other kinds, on the other
hand, are semantically idiomatic word combina-
tions, where we would expect wembs to have less
utility. this    rst approach is thus an examination
of the use of wembs in analysing semantic units of
mixed semantic compositionality.

our second hypothesis is that parser-based fea-
tures will positively impact on the identi   cation of
mwes. our preliminary explorations showed that
the syntactic structure of a text is closely related
to the id203 of an mwe occurring. for in-
stance, in almost all cases an mwe is fully sub-
sumed within a single clause; in the dimsum train-
ing set, e.g., there is just one sentence (out of 4800)
where this condition is violated. additionally, all of
the components of an mwe tend to be directly con-
nected within a dependency graph. as we observe
strong correlation between the distance between two
words in a parse tree and their likelihood of form-
ing an mwe, we decided to employ parse trees as
a source of features. it may be noted that we ini-
tially attempted to create a system where an mwe
is treated as a special kind of clause: we modi   ed

the syntactic tree using knowledge of mwe bound-
aries in the training corpus and then trained a spe-
cial version of the parser aware of such modi   ed
trees. that special parser version was used to di-
rectly produce mwe-labeled clauses over an arbi-
trary text. although such a direct approach didn   t
yield good results, we believe that directly incorpo-
rating mwe identi   cation as part of the parsing pro-
cess is a promising and fruitful direction for future
work.

in our submission, we intentionally avoided the
use of any pre-existing lexical resources, including
multiword lexicons, to better focus our attention on
wembs.

our source code is available at:

https://github.com/andreas-
softwareengineer-pro/dimsum-
semeval2016.

2 overview

an overall architecture of our system is shown in
figure 1. the neural network consists of three
blocks: (1) an incremental vector (recurrency) cal-
culator for a candidate mwe; (2) a two-layer per-
ceptron for sense classi   cation; and (3) a (some-
what wider) two-layer id88 for mwe classi-
   cation. an incremental vector produced by the    rst
block is used as a source of feature vectors by each
of the latter blocks.
the incremental vector calculator produces a vec-
tor vn     irdv for a n-word semantic unit expres-
sion:

                  

vn = tanh

wv(pn)    wordvec(wn)+
wh(pn)    hash(wn)+
wf    wordf eat(wn)+
wc(pn)   

(cid:26) vn   1, n > 0

seed, n = 0

(cid:27)

                  

where wv     irdv  m , wh     irdh  m , wf    
irdf  m and wc     irdc  m are parameter matri-
ces, dv, dh, df and dc are their respective feature
vector sizes, m is the incremental vector size, wn
and pn are the nth word and its part of speech, re-
spectively, wordvec is a id27 lookup,
hash is a hash function over characters of the word
(used as a means of generating embeddings for un-
known words and preserve the ability to distinguish

concrete words), and wordf eat produces a vector of
various word-wise features (see table 2), in the spirit
of approaches like drahom    ra johanka spoustov  a et
al. (2009). when calculating v1 (for a single word),
an initial seed vector of parameters learnt through
id26 is used. the motivation behind us-
ing the n = 1 stage for every mwe (rather than
simply starting with n = 2 and two word vectors)
is based on an intention to avoid switching between
id27s (that play the role of features) and
internally calculated vi that we expect to better cap-
ture the structure of the mwe based on the wembs
(including following their dimension counts). also,
this technique makes the vector composition learn-
ing cycle more frequent w.r.t. the amount of train-
ing data, improving the learning rate and the    nal
penalty. this evaluation schema is inspired by the
work of socher et al. (2011; 2010; 2014). it   s actu-
ally a id56 (id56) but, in con-
trast to previously used techniques, the recursion is
based on candidate mwes rather than the whole
content of the sentence.

the incremental vector is used as an input to two
two-layer id88s: one for mwe classi   cation
and one for one-hot sense vector learning. back
propagation processing of these two also drives the
training of the incremental vector calculator, as de-
scribed above.

we calculate distance-based features based on po-
sitions of two adjacent words in an mwe. a posi-
tion here may mean a position in a sentence, in a
parse tree, or, say, inside or outside a quoted phrase.
the distance feature vector is supplied as input to
the mwe classi   er. as an alternative, it may be
supplied to the incremental vector calculator input
(shown by the dotted line at figure 1, although no
signi   cant difference in performance was observed
when we did this).

3 learning

as brie   y mentioned above, our procedure consid-
ers n-word expressions incrementally, starting with
single words.

3.1 mwe boundary
the system learns a single scalar output value of +1
for every extension from (n    1)-word pre   x to a n-

figure 1: an outline of the system architecture

word mwe (either complete or incomplete), where
n     2.
in such a way, every n-word expression
   nally yields n     1 incremental positive samples.

also, for every word, regardless of whether it is
a standalone word or a member of mwe, it learns
a    1 value (i.e. a negative sample) for its (unob-
served) extension to an mwe with a random word.
such a random word is chosen of the l words
ahead in the same sentence, not involved in the same
mwe with the current word.1

during the learning process (and, correspond-

1we limit l to 9 words, and also limit the id203 of ev-
ery word selection to < 0.25 (by randomly down-weighting the
sample processing for l < 4) in order to prevent possible bias
to the distribution of inter-word offsets occurring while process-
ing the last few words of a sentence. we solely pick the same
sentence words as negative samples in order to make learning
more focused on prediction environment patterns rather than
noise, as we use a small training corpus, and at the same time
to keep an approximate balance in count between negative and
positive samples.

ingly, during the prediction), recursive computations
of the incremental vector are solely fed by members
of the same mwe, and are restarted for each seman-
tic unit.

3.2 senses
we learn a sense once per complete n-word expres-
sion, including n = 1. we have a distinct output
per sense, including    unknown    (42 in total in the
dimsum data set). we factorise sense classi   ca-
tion error by the number of senses in order to bal-
ance the id26 of two id88s to the
incremental vector calculator.

4 prediction

we use a greedy procedure for mwe prediction in
a text. an outer loop iterates through all words of a
sentence.
it selects each word (not yet consumed
by some previously predicted mwe) as the head

weighted     hash sum word selector distance feature composer parser output mwe parent selector word feature composer weighted                   maxarg one hot tanh     word 2 vec                    2lp is mwe supersense switch head word next word selected by pos seed word vec  parameter word  vec incremental  n-word vector sentence weighted     word of a possible new mwe and restarts the in-
cremental vector computation. an inner loop iter-
ates through (up to l) remaining words in the sen-
tence following the current head word. each time,
a id203 for such a second word to be a con-
tinuation of the mwe is evaluated. once a new
mwe or an mwe extension is predicted, it con-
sumes the right hand word and designates it to be
the next word of the mwe; the incremental vector
is updated respectively with the new mwe member.
such a procedure is able to generate a deep stack of
nested mwes with gaps, but we restrict the depth to
be compliant to the dimsum data format.

5 features
5.1 id27s
we utilised a publicly available pre-trained cbow
id97 model, with 300-dimensional word vec-
tors based on the google news corpus. out-of-
vocabulary words are represented with zero-   lled
vectors.

we use the following search strategy when look-
ing up a word in the id97 dictionary (until
the    rst successful lookup returns the    nal embed-
ding):2.

1. strip off leading    #    and    @   
2. if the word is a number then replace by    num   
3. lowercase the word
4. (optionally) lemmatize
5. remove all non-alpha characters
6. return the id27 associated with the

word or if no match, return a zero vector

5.2 word hash sum
we produce a 64-dimensional +1/   1 hash sum vec-
tor for words where the embedding vector is un-
known; a zero vector was supplied for words (some-
how) found in the google news wemb database.
2we tried using light id147 to reduce the rate of
unknown words (a simple substitution of one character and then
selecting the most probable word according to its frequency
ranking). however it didn   t seem to be effective, as the num-
ber of mispredicted words was greater then the number of cor-
rectly predicted ones, in particular due to the absence of a great
number of frequent words (   stop-words   ) in the google news
vectors db. thus, a more sophisticated system would be needed
if one wants to correct typos.

pardist

feature
gap

range description
8 ,n    
n
z   0
{2; 0;
    3
2}

gap between word posi-
tions (divided by 8)
hierarchical distance be-
tween words in a parse
tree; three gradations for
distances of 1,2,    3
parparent {   1, 1} two words are head word
and child word of
the
same clause, according to
the parser output
interqt {   1, 1} double-quotes anywhere

between given words?

table 1: distance features

only alphabetic characters were counted in the hash
sum.

5.3 word distance features
table 1 displays inter-word distance-based features.
we used a parser prediction    le created for the
source text in order to evaluate a hierarchical dis-
tance between two candidate words. the hierarchi-
cal distance here means the maximum of two counts
of edges that connect two given words to the nearest
clause they share. for instance, two sibling nodes
have the hierarchical distance value of one. the
same is also true of a phrase head word and its
immediate dependent word (that case is indicated
by a distinct feature). we employed turboparser
v.2.0.0 (martins and almeida, 2014) and trained it
on a id32 data collection (some conver-
sion was needed to match with the part-of-speech
tagset used in the dimsum task).

5.4 heuristic word features
list of miscellaneous word features
the full
(concerning capitalization, punctuation characters,
lookup success etc.) is presented in table 2.

6 results
the system con   guration which was used as our of-
   cial run for the semeval 2016 task was parameter-
ized as follows: wide layer 1 in mwe prediction
id88 = 1024 nodes; hash sum size = 16bit,
calculated both for known and unknown word, us-
ing all characters in a word (alpha + non-alpha); a

feature
capfirst {   1, 1} word starts with a capital

description

value
range

letter

capnorm

[0, 1] word starts with a capital
letter, a value normalized
by dividing by the sen-
tence word count
ratio of uppercase letters
capratio [0, 1]
in the word
hasnalpha {   1, 1} word has non-alpha char-
{   1, 1} word has any digit inside
hasnum
hasprime {   1, 1} word has an apostrophe
{   1, 1} word starts with    @   
isat
{   1, 1} word starts with    #   
ishash
{   1, 1} word is num (integer,
isnum
   oat or    num    keyword)
{   1, 1} word contains punctua-
tion character(s) (one or
more of    !?.,;:[]()/    )

ispunct

acters

isunk

{   1, 1} id97

vocabulary word,
cluding stop words

out-of-
in-

in

found

{   1, 1} word is    url   

isurl
logrange [0, 12] smoothed logarithm of
frequency
the word   s
range
the
dictionary,
id97
log(0.1    range + 1); the
more frequent the word,
the lower the value
{   1, 1} a double-quote is located
at the previous word posi-
tion
quotpost {   1, 1} a double-quote is found
at the next word position

quotpre

table 2: word features

mean vector of id27s over the sentence
was included as extra feature; distance features are
supplied to the composed vector evaluator (as shown
with the dotted line in figure 1); a con   dence level
of    0.15 was applied to the mwe id88 when
predicting a two-word mwe pre   x (but not when
expanding it to the third and next words, encourag-
ing an mwe to start). the whole dimsum training

type

prec

recall

f1

mwes
supersenses
combined

mwe
supersenses
combined
macro

experiments
0.4697
0.5320
0.5194
of   cial
0.6122
0.5009
0.5114

0.4655
0.5138
0.5046

0.2807
0.5326
0.4846

46.76%
52.27%
51.19%

38.49%
51.62%
49.77%
49.94%

table 3: results

set was used to train the system.

table 3 displays the results, measured with the

dimsum evaluation script.

table 4 represents a brief ablation study with fea-
ture vector disabled. id27s seem to be
the critical source of supersense information, and
they are also one of the important contributors to
mwe recall. the incremental n-word vector is crit-
ical for mwe precision, and distance features are
of great importance both for precision and recall in
mwe detection, but especially for the recall. word
hash and, surprisingly, heuristic word features are of
much less signi   cance than other feature vectors.

7 findings & conclusions

the system captures supersenses rather well (when
we consider the large number of senses, small size
of the training data, and ambiguity in the sense
assignments). however, it frequently admits harsh
mispredictions that are probably caused by the
lack of global sense coherence in wembs (qu et
al., 2015). also, it suffers from the lack of (other
than mwe-related) context information in cases
of ambiguity.
improving context awareness may
be the most obvious next step. unsurprisingly, the
most frequent supersenses have the best recall, up
to 80% for v.stative. the mean recall value for
senses is around 50   60%, and there are senses (like
n.weather) that are never correctly predicted.
exceptions are n.attribute, n.location, and
v.change, where recall
is below 30% despite
them being reasonably frequent.
some of the
most frequent mispredictions are the following:
n.person     n.group; n.communication    

ablated
features
   recurrency
   heuristic
   distance
   word hash
   id97

f1

supersenses

prec recall

mwes
prec recall
0.2125 0.4960 29.75% 0.4638 0.3581 40.41% 0.3591 0.3843 37.13%
0.4230 0.5821 48.99% 0.5033 0.4797 49.12% 0.4822 0.4991 49.05%
0.3246 0.2771 29.90% 0.4494 0.4575 45.34% 0.4281 0.4232 42.56%
0.3640 0.6350 46.27% 0.5133 0.4811 49.67% 0.4674 0.5104 48.80%
0.3204 0.5193 39.63% 0.1898 0.1766 18.29% 0.2293 0.2418 23.54%

prec recall

combined

f1

f1

table 4: feature ablation results

n.artefact;
n.act     n.event;
v.cognition.

n.attribut     n.cognition;
and v.emotion    

mwe identi   cation looks generally reasonable
for all principal types of mwes (even, surprisingly,
ones of an idiomatic nature), but the overall accuracy
is low.

hysteresis bias pattern. the mwe classi   er
tends to be biased toward not joining two words into
a mwe3; at the same time, once an mwe is pre-
dicted, it tends to be extended excessively to include
a third and subsequent words, often selecting a non-
relevant word with some gap.4 this probably means
that the proposed sampling schema is not balanced
enough to work for small amounts of training data.
lookahead. the method obviously lacks the
ability to model context when predicting mwes.
in cases of expressions like john , mary & com-
pany, for example, our system will predict john
mary company with all the punctuation missing.
some bidirectional, attentional (bahdanau et al.,
2014; cohn et al., 2016) or lookahead-based ap-
proach is needed, as we may not have a reasonable
isolated rule for whether john should be joined to
the comma. a similar situation may also occur in
punctuation-less contexts.

parsing. the use of parsing results in markedly
better precision and recall. taking into account
the strong correlation between parser-based features
and mwe boundaries, further investigation of the
parser-based approach is warranted.

3an example from the dimsum test data: english speaking
advisor produces an mwe predicted to be english advisor (not
including speaking

4as a very rough compensation of such an effect, one may
use two con   dence levels, one (negative) at n = 2, another
(positive) at n     3.

multiword-to-sense collision. a shared n-
word incremental vector computation both for
mwe and sense learning imposes a collision.
it
may be observed that for better mwe training, the
distance features should be supplied to the computa-
tion input (as shown with the dotted line in figure 1),
but this will decreases the sense prediction score.

needs a deeper network. the training dynam-
ics observed in the experiments show that the neural
networks (especially the one used for mwe predic-
tion) need to be deeper (use more than two layers, as
suggested in sutskever et al. (2014)).

references
[bahdanau et al.2014] dzmitry bahdanau, kyunghyun
2014. neural machine
cho, and yoshua bengio.
translation by jointly learning to align and translate.
arxiv preprint arxiv:1409.0473.

[baldwin and kim2010] timothy baldwin and su nam
in nitin in-
kim. 2010. multiword expressions.
durkhya and fred j. damerau, editors, handbook of
natural language processing. crc press, boca ra-
ton, usa, 2nd edition.

[cohn et al.2016] trevor cohn, cong duy vu hoang,
ekaterina vymolova, kaisheng yao, chris dyer, and
gholamreza haffari. 2016.
incorporating structural
alignment biases into an attentional neural translation
model. arxiv preprint arxiv:1601.01085.

[drahom    ra johanka spoustov  a et al.2009] jan

dra-
hom    ra johanka spoustov  a, haji  c, jan raab, miroslav
spousta, et al. 2009. semi-supervised training for
in proceedings
the averaged id88 pos tagger.
of the 12th conference of the european chapter of
the association for computational linguistics, pages
763   771. association for computational linguistics.
[martins and almeida2014] andr  e ft martins and mari-
ana sc almeida. 2014. priberam: a turbo semantic
parser with second order features. in proceedings of
the 8th international workshop on semantic evalua-
tion (semeval 2014), pages 471   476.

evaluating the utility of vector differences for lexical
relation learning. corr, abs/1509.01692.

[mikolov et al.2013a] tomas mikolov,

ilya sutskever,
kai chen, greg s corrado, and jeff dean. 2013a.
distributed representations of words and phrases and
in advances in neural infor-
their compositionality.
mation processing systems, pages 3111   3119.

[mikolov et al.2013b] tomas mikolov, wen-tau yih, and
geoffrey zweig. 2013b. linguistic regularities in
in proceed-
continuous space word representations.
ings of the 2013 conference of the north american
chapter of the association for computational linguis-
tics: human language technologies (naacl hlt
2013), pages 746   751, atlanta, usa.

[qu et al.2015] lizhen qu, gabriela ferraro, liyuan
zhou, weiwei hou, nathan schneider, and timothy
baldwin. 2015. big data small data, in domain out-of
domain, known word unknown word: the impact of
word representation on sequence labelling tasks. arxiv
preprint arxiv:1504.05319.

[salehi et al.2015] bahar salehi, paul cook, and timo-
thy baldwin. 2015. a id27 approach to
predicting the compositionality of multiword expres-
sions. in proceedings of the 2015 conference of the
north american chapter of the association for com-
putational linguistics: human language technolo-
gies, pages 977   983.

[schneider et al.2012] nathan schneider, behrang mohit,
kemal o   azer, and noah a smith. 2012. coarse lex-
ical semantic annotation with supersenses: an arabic
case study. in proceedings of the 50th annual meet-
ing of the association for computational linguistics:
short papers-volume 2, pages 253   258. association
for computational linguistics.

[socher et al.2010] richard socher, christopher d man-
ning, and andrew y ng. 2010. learning continu-
ous phrase representations and syntactic parsing with
in proceedings of the
id56s.
nips-2010 deep learning and unsupervised feature
learning workshop, pages 1   9.

[socher et al.2011] richard socher, cliff c lin, chris
manning, and andrew y ng. 2011. parsing natural
scenes and natural language with recursive neural net-
works. in proceedings of the 28th international con-
ference on machine learning (icml-11), pages 129   
136.

[socher2014] richard socher. 2014. recursive deep
learning for natural language processing and com-
puter vision. ph.d. thesis, citeseer.

[sutskever et al.2014] ilya sutskever, oriol vinyals, and
quoc v le. 2014. sequence to sequence learning with
in advances in neural information
neural networks.
processing systems, pages 3104   3112.

[vylomova et al.2015] ekaterina vylomova,

laura
rimell, trevor cohn, and timothy baldwin. 2015.
take and took, gaggle and goose, book and read:

