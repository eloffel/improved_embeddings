improving hypernymy detection

with an integrated path-based and distributional method

vered shwartz yoav goldberg

ido dagan

computer science department

bar-ilan university
ramat-gan, israel

6
1
0
2

 

n
u
j
 

7

 
 
]
l
c
.
s
c
[
 
 

3
v
6
7
0
6
0

.

3
0
6
1
:
v
i
x
r
a

vered1986@gmail.com yoav.goldberg@gmail.com dagan@cs.biu.ac.il

abstract

detecting hypernymy relations is a key
task in nlp, which is addressed in
the literature using two complemen-
tary approaches. distributional methods,
whose supervised variants are the cur-
rent best performers, and path-based meth-
ods, which received less research atten-
tion. we suggest an improved path-based
algorithm, in which the dependency paths
are encoded using a recurrent neural net-
work,
that achieves results comparable
to distributional methods. we then ex-
tend the approach to integrate both path-
based and distributional signals, signi   -
cantly improving upon the state-of-the-art
on this task.

introduction

1
hypernymy is an important lexical-semantic rela-
tion for nlp tasks. for instance, knowing that
tom cruise is an actor can help a question an-
swering system answer the question    which ac-
tors are involved in scientology?   . while seman-
tic taxonomies, like id138 (fellbaum, 1998),
de   ne hypernymy relations between word types,
they are limited in scope and domain. therefore,
automated methods have been developed to deter-
mine, for a given term-pair (x, y), whether y is an
hypernym of x, based on their occurrences in a
large corpus.

for a couple of decades, this task has been ad-
dressed by two types of approaches: distributional,
and path-based. in distributional methods, the de-
cision whether y is a hypernym of x is based on
the distributional representations of these terms.
lately, with the popularity of id27s
(mikolov et al., 2013), most focus has shifted to-
wards supervised distributional methods, in which

each (x, y) term-pair is represented using some
combination of the terms    embedding vectors.

in contrast to distributional methods, in which
the decision is based on the separate contexts of
x and y, path-based methods base the decision on
the lexico-syntactic paths connecting the joint oc-
currences of x and y in a corpus. hearst (1992)
identi   ed a small set of frequent paths that indicate
hypernymy, e.g. y such as x. snow et al. (2004)
represented each (x, y) term-pair as the multiset of
dependency paths connecting their co-occurrences
in a corpus, and trained a classi   er to predict hy-
pernymy, based on these features.

using individual paths as features results in a
huge, sparse feature space. while some paths
are rare, they often consist of certain unimportant
components. for instance,    spelt is a species of
wheat    and    fantasy is a genre of    ction    yield
two different paths: x be species of y and x be
genre of y, while both indicating that x is-a y. a
possible solution is to generalize paths by replac-
ing words along the path with their part-of-speech
tags or with wild cards, as done in the patty sys-
tem (nakashole et al., 2012).

overall, the state-of-the-art path-based methods
perform worse than the distributional ones. this
stems from a major limitation of path-based meth-
ods:
they require that the terms of the pair oc-
cur together in the corpus, limiting the recall of
these methods. while distributional methods have
no such requirement, they are usually less precise
in detecting a speci   c semantic relation like hy-
pernymy, and perform best on detecting broad se-
mantic similarity between terms. though these
approaches seem complementary, there has been
rather little work on integrating them (mirkin et
al., 2006; kaji and kitsuregawa, 2008).

in this paper, we present hypenet, an inte-
grated path-based and distributional method for
hypernymy detection. inspired by recent progress

in relation classi   cation, we use a long short-
term memory (lstm) network (hochreiter and
schmidhuber, 1997) to encode dependency paths.
in order to create enough training data for our net-
work, we followed previous methodology of con-
structing a dataset based on knowledge resources.
we    rst show that our path-based approach, on
its own, substantially improves performance over
prior path-based methods, yielding performance
comparable to state-of-the-art distributional meth-
ods. our analysis suggests that the neural path rep-
resentation enables better generalizations. while
coarse-grained generalizations, such as replacing a
word by its pos tag, capture mostly syntactic sim-
ilarities between paths, hypenet captures also
semantic similarities.

we then show that we can easily integrate dis-
tributional signals in the network. the integration
results con   rm that the distributional and path-
based signals indeed provide complementary in-
formation, with the combined model yielding an
improvement of up to 14 f1 points over each indi-
vidual model.1

2 background

we introduce the two main approaches for hyper-
nymy detection: distributional (section 2.1), and
path-based (section 2.2). we then discuss the re-
cent use of recurrent neural networks in the related
task of relation classi   cation (section 2.3).

2.1 distributional methods
hypernymy detection is commonly addressed us-
ing distributional methods. in these methods, the
decision whether y is a hypernym of x is based on
the distributional representations of the two terms,
i.e., the contexts with which each term occurs sep-
arately in the corpus.

earlier methods developed unsupervised mea-
sures for hypernymy, starting with symmetric sim-
ilarity measures (lin, 1998), and followed by di-
rectional measures based on the distributional in-
clusion hypothesis (weeds and weir, 2003; kotler-
man et al., 2010). this hypothesis states that the
contexts of a hyponym are expected to be largely
included in those of its hypernym. more recent
work (santus et al., 2014; rimell, 2014) introduce
new measures, based on the assumption that the

1our code and data are available in:

https://github.com/vered1986/hypenet

most typical linguistic contexts of a hypernym are
less informative than those of its hyponyms.

more recently, the focus of the distributional ap-
proach shifted to supervised methods.
in these
methods, the (x, y) term-pair is represented by a
feature vector, and a classi   er is trained on these
vectors to predict hypernymy. several methods
are used to represent term-pairs as a combination
of each term   s embeddings vector: concatenation
(cid:126)x   (cid:126)y (baroni et al., 2012), difference (cid:126)y   (cid:126)x (roller
et al., 2014; weeds et al., 2014), and dot-product
(cid:126)x    (cid:126)y. using neural id27s (mikolov et
al., 2013; pennington et al., 2014), these methods
are easy to apply, and show good results (baroni
et al., 2012; roller et al., 2014).

2.2 path-based methods
a different approach to detecting hypernymy be-
tween a pair of terms (x, y) considers the lexico-
syntactic paths that connect the joint occurrences
of x and y in a large corpus. automatic acquisi-
tion of hypernyms from free text, based on such
paths, was    rst proposed by hearst (1992), who
identi   ed a small set of lexico-syntactic paths that
indicate hypernymy relations (e.g. y such as x, x
and other y).

in a later work, snow et al. (2004) learned to de-
tect hypernymy. rather than searching for speci   c
paths that indicate hypernymy, they represent each
(x, y) term-pair as the multiset of all dependency
paths that connect x and y in the corpus, and train
a id28 classi   er to predict whether y
is a hypernym of x, based on these paths.

paths that indicate hypernymy are those that
were assigned high weights by the classi   er. the
paths identi   ed by this method were shown to
subsume those found by hearst (1992), yield-
ing improved performance. variations of snow
et al.   s (2004) method were later used in tasks
such as taxonomy construction (snow et al., 2006;
kozareva and hovy, 2010; carlson et al., 2010;
riedel et al., 2013), analogy identi   cation (tur-
ney, 2006), and de   nition extraction (borg et al.,
2009; navigli and velardi, 2010).

a major

limitation in relying on lexico-
syntactic paths is the sparsity of the feature space.
since similar paths may somewhat vary at the lex-
ical level, generalizing such variations into more
abstract paths can increase recall. the patty al-
gorithm (nakashole et al., 2012) applied such gen-
eralizations for the purpose of acquiring a taxon-

nsubj

parrot
noun

is
verb

det

attr

a
det

bird
noun

figure 1: an example dependency tree of
the sen-
tence    parrot is a bird   , with x=parrot and y=bird, repre-
sented in our notation as x/noun/nsubj/< be/verb/root/-
y/noun/attr/>.

omy of term relations from free text. for each
path, they added generalized versions in which a
subset of words along the path were replaced by
either their pos tags, their ontological types or
wild-cards. this generalization increased recall
while maintaining the same level of precision.

2.3 id56s for relation classi   cation
relation classi   cation is a related task whose goal
is to classify the relation that is expressed between
two target terms in a given sentence to one of pre-
de   ned relation classes. to illustrate, consider
the following sentence, from the semeval-2010
relation classi   cation task dataset (hendrickx et
al., 2009):    the [apples]e1 are in the [basket]e2   .
here, the relation expressed between the target en-
tities is content     container(e1, e2).

the shortest dependency paths between the tar-
get entities were shown to be informative for this
task (fundel et al., 2007). recently, deep learning
techniques showed good performance in capturing
the indicative information in such paths.

in particular, several papers show improved per-
formance using recurrent neural networks (id56)
that process a dependency path edge-by-edge. xu
et al. (2015; 2016) apply a separate long short-
term memory (lstm) network to each sequence
of words, pos tags, dependency labels and word-
net hypernyms along the path. a max-pooling
layer on the lstm outputs is used as the in-
put of a network that predicts the classi   cation.
other papers suggest incorporating additional net-
work architectures to further improve performance
(nguyen and grishman, 2015; liu et al., 2015).

while relation classi   cation and hypernymy de-
tection are both concerned with identifying se-
mantic relations that hold for pairs of terms, they
differ in a major respect. in relation classi   cation
the relation should be expressed in the given text,
while in hypernymy detection, the goal is to rec-
ognize a generic lexical-semantic relation between
terms that holds in many contexts. accordingly,
in relation classi   cation a term-pair is represented

by a single dependency path, while in hypernymy
detection it is represented by the multiset of all de-
pendency paths in which they co-occur in the cor-
pus.

3 lstm-based hypernymy detection
we present hypenet, an lstm-based method
for hypernymy detection. we    rst focus on im-
proving path representation (section 3.1), and then
integrate distributional signals into our network,
resulting in a combined method (section 3.2).

3.1 path-based network
similarly to prior work, we represent each depen-
dency path as a sequence of edges that leads from
x to y in the dependency tree.2 each edge contains
the lemma and part-of-speech tag of the source
node, the dependency label, and the edge direction
between two subsequent nodes. we denote each
edge as lemma/p os/dep/dir. see    gure 1 for
an illustration.

rather than treating an entire dependency path
as a single feature, we encode the sequence of
edges using a long short-term memory (lstm)
network. the vectors obtained for the different
paths of a given (x, y) pair are pooled, and the re-
sulting vector is used for classi   cation. figure 2
depicts the overall network structure, which is de-
scribed below.
edge representation we represent each edge
by the concatenation of its components    vectors:

(cid:126)ve = [(cid:126)vl, (cid:126)vpos, (cid:126)vdep, (cid:126)vdir]

where (cid:126)vl, (cid:126)vpos, (cid:126)vdep, (cid:126)vdir represent the embedding
vectors of the lemma, part-of-speech, dependency
label and dependency direction (along the path
from x to y), respectively.
path representation for a path p composed of
edges e1, ..., ek, the edge vectors (cid:126)ve1, ..., (cid:126)vek are
fed in order to an lstm encoder, resulting in
a vector (cid:126)op representing the entire path p. the
lstm architecture is effective at capturing tem-
poral patterns in sequences. we expect the train-
ing procedure to drive the lstm encoder to focus
on parts of the path that are more informative for
the classi   cation task while ignoring others.

2like snow et al. (2004), we added for each path, addi-
tional paths containing single daughters of x or y not already
contained in the path, referred by snow et al. (2004) as    satel-
lite edges   . this enables including paths like such y as x, in
which the word    such    is not in the path between x and y.

embeddings:

lemma
pos
dependency label
direction

x/noun/nsubj/> be/verb/root/-

y/noun/attr/<

x/noun/dobj/> define/verb/root/-

as/adp/prep/<

y/noun/pobj/<

(x, y)

classi   cation

(softmax)

(cid:126)op

average
pooling

(cid:126)vxy

(cid:126)vwx

(cid:126)vwy

path lstm

term-pair classi   er

figure 2: an illustration of term-pair classi   cation. each term-pair is represented by several paths. each path is a sequence of
edges, and each edge consists of four components: lemma, pos, dependency label and dependency direction. each edge vector
is fed in sequence into the lstm, resulting in a path embedding vector (cid:126)op. the averaged path vector becomes the term-pair   s
feature vector, used for classi   cation. the dashed (cid:126)vwx , (cid:126)vwy vectors refer to the integrated network described in section 3.2.

term-pair classi   cation each (x, y) term-pair
is represented by the multiset of lexico-syntactic
paths that connected x and y in the corpus, de-
noted as paths(x, y), while the supervision is
given for the term pairs. we represent each (x, y)
term-pair as the weighted-average of its path vec-
tors, by applying average pooling on its path vec-
tors, as follows:

(cid:80)
(cid:80)
p   paths(x,y) fp,(x,y)   (cid:126)op
p   paths(x,y) fp,(x,y)

(1)

(cid:126)vxy = (cid:126)vpaths(x,y) =

where fp,(x,y) is the frequency of p in paths(x, y).
we then feed this path vector to a single-layer net-
work that performs binary classi   cation to decide
whether y is a hypernym of x.

c = sof tmax(w    (cid:126)vxy)

(2)

c is a 2-dimensional vector whose components
sum to 1, and we classify a pair as positive if
c[1] > 0.5.
implementation details to train the network,
we used pyid98.3 we minimize the cross en-
tropy loss using gradient-based optimization, with
mini-batches of size 10 and the adam update rule
(kingma and ba, 2014). id173 is applied
by a dropout on each of the components    embed-
dings. we tuned the hyper-parameters (learning
rate and dropout rate) on the validation set (see the
appendix for the hyper-parameters values).

we initialized the lemma embeddings with the
pre-trained glove id27s (pennington
et al., 2014), trained on wikipedia. we tried both

3https://github.com/clab/id98

the 50-dimensional and 100-dimensional embed-
ding vectors and selected the ones that yield bet-
ter performance on the validation set.4 the other
embeddings, as well as out-of-vocabulary lemmas,
are initialized randomly. we update all embedding
vectors during training.

integrated network

3.2
the network presented in section 3.1 classi   es
each (x, y) term-pair based on the paths that con-
nect x and y in the corpus. our goal was to im-
prove upon previous path-based methods for hy-
pernymy detection, and we show in section 6
that our network indeed outperforms them. yet,
as path-based and distributional methods are con-
sidered complementary, we present a simple way
to integrate distributional features in the network,
yielding improved performance.

information on each term.

we extended the network to take into account
distributional
in-
spired by the supervised distributional concatena-
tion method (baroni et al., 2012), we simply con-
catenate x and y id27s to the (x, y)
feature vector, rede   ning (cid:126)vxy:

(cid:126)vxy = [ (cid:126)vwx, (cid:126)vpaths(x,y), (cid:126)vwy ]

(3)

where (cid:126)vwx and (cid:126)vwy are x and y   s word embed-
dings, respectively, and (cid:126)vpaths(x,y) is the averaged
path vector de   ned in equation 1. this way, each
(x, y) pair is represented using both the distribu-
tional features of x and y, and their path-based
features.

4higher-dimensional embeddings seem not to improve

performance, while hurting the training runtime.

resource
id138
dbpedia
wikidata

yago

instance hypernym, hypernym

relations

type

subclass of, instance of

subclass of

table 1: hypernymy relations in each resource.

4 dataset
4.1 creating instances
neural networks typically require a large amount
of training data, whereas the existing hypernymy
datasets, like bless (baroni and lenci, 2011),
are relatively small. therefore, we followed the
common methodology of creating a dataset us-
ing distant supervision from knowledge resources
(snow et al., 2004; riedel et al., 2013). fol-
lowing snow et al. (2004), who constructed their
dataset based on id138 hypernymy, and aiming
to create a larger dataset, we extract hypernymy
relations from several resources: id138 (fell-
baum, 1998), dbpedia (auer et al., 2007), wiki-
data (vrande  ci  c, 2012) and yago (suchanek et al.,
2007).

all instances in our dataset, both positive and
negative, are pairs of terms that are directly re-
lated in at least one of the resources. these re-
sources contain thousands of relations, some of
which indicate hypernymy with varying degrees of
certainty. to avoid including questionable relation
types, we consider as denoting positive examples
only indisputable hypernymy relations (table 1),
which we manually selected from the set of hyper-
nymy indicating relations in shwartz et al. (2015).
term-pairs related by other relations (including
hyponymy), are considered as negative instances.
using related rather than random term-pairs as
negative instances tests our method   s ability to dis-
tinguish between hypernymy and other kinds of
semantic relatedness. we maintain a ratio of 1:4
positive to negative pairs in the dataset.

like snow et al. (2004), we include only term-
pairs that have joint occurrences in the corpus, re-
quiring at least two different dependency paths for
each pair.

4.2 random and lexical dataset splits
as our primary dataset, we perform standard ran-
dom splitting, with 70% train, 25% test and 5%
validation sets.

as pointed out by levy et al. (2015), super-
vised distributional lexical id136 methods tend
to perform    lexical memorization   , i.e., instead of

random split
lexical split

train
49,475
20,335

test
17,670
6,610

validation

3,534
1,350

all

70,679
28,295

table 2: the number of instances in each dataset.

learning a relation between the two terms, they
mostly learn an independent property of a single
term in the pair: whether it is a    prototypical hy-
pernym    or not. for instance, if the training set
contains term-pairs such as (dog, animal), (cat,
animal), and (cow, animal), all annotated as posi-
tive examples, the algorithm may learn that animal
is a prototypical hypernym, classifying any new (x,
animal) pair as positive, regardless of the relation
between x and animal. levy et al. (2015) sug-
gested to split the train and test sets such that each
will contain a distinct vocabulary (   lexical split   ),
in order to prevent the model from over   tting by
lexical memorization.

to investigate such behaviors, we present re-
sults also for a lexical split of our dataset. in this
case, we split the train, test and validation sets
such that each contains a distinct vocabulary. we
note that this differs from levy et al. (2015), who
split only the train and the test sets, and dedicated a
subset of the train for validation. we chose to devi-
ate from levy et al. (2015) because we noticed that
when the validation set contains terms from the
train set, the model is rewarded for lexical mem-
orization when tuning the hyper-parameters, con-
sequently yielding suboptimal performance on the
lexically-distinct test set. when each set has a dis-
tinct vocabulary, the hyper-parameters are tuned
to avoid lexical memorization and are likely to
perform better on the test set. we tried to keep
roughly the same 70/25/5 ratio in our lexical split.5
the sizes of the two datasets are shown in table 2.
indeed, training a model on a lexically split
dataset may result in a more general model, that
can better handle pairs consisting of two unseen
terms during id136. however, we argue that
in the common applied scenario, the id136 in-
volves an unseen pair (x, y), in which x and/or
y have already been observed separately. models
trained on a random split may introduce the model
with a term   s    prior id203    of being a hyper-
nym or a hyponym, and this information can be
exploited bene   cially at id136 time.

5the lexical split discards many pairs consisting of cross-

set terms.

x/noun/dobj/> establish/verb/root/- as/adp/prep/< y/noun/pobj/<

x/noun/dobj/> verb as/adp/prep/< y/noun/pobj/<

x/noun/dobj/> * as/adp/prep/< y/noun/pobj/<

x/noun/dobj/> establish/verb/root/- adp y/noun/pobj/<

path

table 3: example generalizations of x was established as y.

5 baselines

we compare hypenet with several state-of-the-
art methods for hypernymy detection, as described
in section 2: path-based methods (section 5.1),
and distributional methods (section 5.2). due to
different works using different datasets and cor-
pora, we replicated the baselines rather than com-
paring to the reported results.

we use the wikipedia dump from may 2015 as
the underlying corpus of all the methods, and parse
it using spacy.6 we perform model selection on
the validation set to tune the hyper-parameters of
each method.7 the best hyper-parameters are re-
ported in the appendix.

5.1 path-based methods
snow we follow the original paper, and extract
all shortest paths of four edges or less between
terms in a dependency tree. like snow et al.
(2004), we add paths with    satellite edges   , i.e.,
single words not already contained in the depen-
dency path, which are connected to either x or y,
allowing paths like such y as x. the number of
distinct paths was 324,578. we apply   2 feature
selection to keep only the 100,000 most informa-
tive paths and train a id28 classi   er.

generalization we also compare our method
to a baseline that uses generalized dependency
paths. following patty   s approach to general-
izing paths (nakashole et al., 2012), we replace
edges with their part-of-speech tags as well as with
wild cards. we generate the powerset of all possi-
ble generalizations, including the original paths.
see table 3 for examples. the number of features
after generalization went up to 2,093,220. simi-
larly to the    rst baseline, we apply feature selec-
tion, this time keeping the 1,000,000 most infor-
mative paths, and train a id28 classi-
   er over the generalized paths.8

6https://spacy.io/
7we applied grid search for a range of values, and picked
the ones that yield the highest f1 score on the validation set.
8we also tried keeping the 100,000 most informative

paths, but the performance was worse.

5.2 distributional methods
unsupervised slqs (santus et al., 2014) is
an id178-based measure for hypernymy detec-
tion, reported to outperform previous state-of-
the-art unsupervised methods (weeds and weir,
2003; kotlerman et al., 2010). the original paper
was evaluated on the bless dataset (baroni and
lenci, 2011), which consists of mostly frequent
words. applying the vanilla settings of slqs on
our dataset, that contains also rare terms, resulted
in low performance. therefore, we received as-
sistance from enrico santus, who kindly provided
the results of slqs on our dataset after tuning the
system as follows.

the validation set was used to tune the thresh-
old for classifying a pair as positive, as well as the
maximum number of each term   s most associated
contexts (n). in contrast to the original paper, in
which the number of each term   s contexts is    xed
to n, in this adaptation it was set to the minimum
between the number of contexts with lmi score
above zero and n. in addition, the slqs scores
were not multiplied by the cosine similarity scores
between terms, and terms were lemmatized prior
to computing the slqs scores, signi   cantly im-
proving recall.

as our results suggest, while this method is
state-of-the-art for unsupervised hypernymy de-
tection,
it is basically designed for classifying
speci   city level of related terms, rather than hy-
pernymy in particular.

supervised to represent term-pairs with distri-
butional features, we tried several state-of-the-art
methods: concatenation (cid:126)x   (cid:126)y (baroni et al., 2012),
difference (cid:126)y     (cid:126)x (roller et al., 2014; weeds et al.,
2014), and dot-product (cid:126)x    (cid:126)y. we downloaded sev-
eral pre-trained embeddings (mikolov et al., 2013;
pennington et al., 2014) of different sizes, and
trained a number of classi   ers: id28,
id166, and id166 with rbf kernel, which was re-
ported by levy et al. (2015) to perform best in this
setting. we perform model selection on the val-
idation set to select the best vectors, method and
id173 factor (see the appendix).

method

snow
snow + gen
hypenet path-based
slqs (santus et al., 2014)
best supervised (concatenation)
hypenet integrated

path-based

distributional

combined

0.843
0.852
0.811
0.491
0.901
0.913

random split

precision

recall
0.452
0.561
0.716
0.737
0.637
0.890

precision

lexical split
recall
0.438
0.530
0.632
0.610
0.551
0.617

0.760
0.759
0.691
0.375
0.754
0.809

f1
0.556
0.624
0.660
0.464
0.637
0.700

f1
0.589
0.676
0.761
0.589
0.746
0.901

table 4: performance scores of our method compared to the path-based baselines and the state-of-the-art distributional methods
for hypernymy detection, on both variations of the dataset     with lexical and random split to train / test / validation.

6 results

table 4 displays performance scores of hypenet
and the baselines. hypenet path-based is our
path-based recurrent neural network model (sec-
tion 3.1) and hypenet integrated is our combined
method (section 3.2). comparing the path-based
methods shows that generalizing paths improves
recall while maintaining similar levels of preci-
sion, reassessing the behavior found in nakas-
hole et al. (2012). hypenet path-based outper-
forms both path-based baselines by a signi   cant
improvement in recall and with slightly lower pre-
cision. the recall boost is due to better path gen-
eralization, as demonstrated in section 7.1.

regarding distributional methods, the unsuper-
vised slqs baseline performed slightly worse on
our dataset. the low precision stems from its
inability to distinguish between hypernyms and
meronyms, which are common in our dataset,
causing many false positive pairs such as (zabrze,
poland) and (kibbutz, israel). we sampled 50
false positive pairs of each dataset split, and found
that 38% of the false positive pairs in the random
split and 48% of those in the lexical split were
holonym-meronym pairs.

in accordance with previously reported results,
the supervised embedding-based method is the
best performing baseline on our dataset as well.
hypenet path-based performs slightly better,
achieving state-of-the-art results. adding distri-
butional features to our method shows that these
two approaches are indeed complementary. on
both dataset splits, the performance differences
between hypenet integrated and hypenet path-
based, as well as the supervised distributional
method, are substantial, and statistically signi   -
cant with p-value of 1% (paired t-test).

we also reassess that indeed supervised distri-
butional methods perform worse on a lexical split
(levy et al., 2015). we further observe a similar
reduction when using hypenet, which is not a re-
sult of lexical memorization, but rather stems from

over-generalization (section 7.1).

7 analysis
7.1 qualitative analysis of learned paths
we analyze hypenet   s ability to generalize over
path structures, by comparing prominent indica-
tive paths which were learned by each of the path-
based methods. we do so by    nding high-scoring
paths that contributed to the classi   cation of true-
positive pairs in the dataset.
in the path-based
baselines, these are the highest-weighted features
as learned by the id28 classi   er. in
the lstm-based method, it is less straightforward
to identify the most indicative paths. we assess the
contribution of a certain path p to classi   cation by
regarding it as the only path that appeared for the
term-pair, and compute its true label score from
the class distribution: sof tmax(w    (cid:126)vxy)[1], set-
ting (cid:126)vxy = [(cid:126)0, (cid:126)op,(cid:126)0].

a notable pattern is that snow   s method learns
speci   c paths, like x is y from (e.g. megadeth
is an american thrash metal band from los an-
geles). while snow   s method can only rely on
verbatim paths, limiting its recall, the generalized
version of snow often makes coarse generaliza-
tions, such as x verb y from. clearly, such a
path is too general, and almost any verb assigned
to it results in a non-indicative path (e.g. x take
y from). efforts by the learning method to avoid
such generalization, again, lower the recall. hy-
penet provides a better midpoint, making    ne-
grained generalizations by learning additional se-
mantically similar paths such as x become y from
and x remain y from. see table 5 for additional
example paths which illustrate these behaviors.

we also noticed that while on the random split
our model learns a range of speci   c paths such as
x is y published (learned for e.g. y=magazine)
and x is y produced (y=   lm), in the lexical split
it only learns the general x is y path for these re-
lations. we note that x is y is a rather    noisy   
path, which may occur in ad-hoc contexts with-

method

snow

snow +

gen

hypenet
integrated

path

x/noun/nsubj/> be/verb/root/- y/noun/attr/<

direct/verb/acl/>

x/noun/nsubj/> be/verb/root/- y/noun/attr/<

publish/verb/acl/>

x/noun/compound/> noun    be/verb/root/-

y/noun/attr/< base/verb/acl/>

x/noun/compound/> noun y/noun/compound/<

x/noun/nsubj/> be/verb/root/- y/noun/attr/<

(release|direct|produce|write)/verb/acl/>

x/noun/compound/>

example text

eyeball is a 1975 italian-spanish
   lm directed by umberto lenzi
allure is a u.s. women   s beauty

magazine published monthly

calico light weapons inc. (clws) is an
american privately held manufacturing
company based in cornelius, oregon

weston town council

blinky is a 1923 american comedy
   lm directed by edward sedgwick

(association|co.|company|corporation|foundation

|group|inc.|international|limited|ltd.)/noun/nsubj/>

be/verb/root/- y/noun/attr/<

((create|found|headquarter|own|specialize)/verb/acl/>)?

retalix ltd. is a software company

table 5: examples of indicative paths learned by each method, with corresponding true positive term-pairs from the random
split test set. hypernyms are marked red and hyponyms are marked blue.

relation
synonymy
hyponymy

holonymy / meronymy
hypernymy-like relations

other relations

%

21.37%
29.45%
9.36%
21.03%
18.77%

table 6: distribution of relations holding between each pair
of terms in the resources among false positive pairs.

out indicating generic hypernymy relations (e.g.
chocolate is a big problem in the context of chil-
dren   s health). while such a model may identify
hypernymy relations between unseen terms, based
on general paths, it is prone to over-generalization,
hurting its performance, as seen in table 4. as
discussed in    4.2, we suspect that this scenario, in
which both terms are unseen, is usually not com-
mon enough to justify this limiting training setup.

7.2 error analysis
false positives we categorized the false positive
pairs on the random split according to the rela-
tion holding between each pair of terms in the re-
sources used to construct the dataset. we grouped
several semantic relations from different resources
to broad categories, e.g. synonym includes also
alias and wikipedia redirection. table 6 displays
the distribution of semantic relations among false
positive pairs.

more than 20% of the errors stem from confus-
ing synonymy with hypernymy, which are known
to be dif   cult to distinguish.

an additional 30% of the term-pairs are re-
versed hypernym-hyponym pairs (y is a hyponym
of x). examining a sample of these pairs suggests
that they are usually near-synonyms, i.e., it is not
that clear whether one term is truely more general
than the other or not. for instance,    ction is an-
notated in id138 as a hypernym of story, while

error type
low statistics
infrequent term

rare hyponym sense

annotation error

%
80%
36%
16%
8%

1
2
3
4

table 7: (overlapping) categories of false negative pairs:
(1) x and y co-occurred less than 25 times (average co-
occurrences for true positive pairs is 99.7). (2) either x or
y is infrequent. (3) the hypernymy relation holds for a rare
sense of x. (4) (x, y) was incorrectly annotated as positive.

our method classi   ed    ction as its hyponym.

a possible future research direction might be
to quite simply extend our network to classify
term-pairs simultaneously to multiple semantic re-
lations, as in pavlick et al. (2015). such a multi-
class model can hopefully better distinguish be-
tween these similar semantic relations.

another notable category is hypernymy-like re-
lations: these are other relations in the resources
that could also be considered as hypernymy, but
were annotated as negative due to our restrictive
selection of only indisputable hypernymy relations
from the resources (see section 4.1). these in-
clude instances like (goethe, occupation, novelist)
and (homo, subdivisionranks, species).

lastly, other errors made by the model often
correspond to term-pairs that co-occur very few
times in the corpus, e.g. xebec, a studio produc-
ing anime, was falsely classi   ed as a hyponym of
anime.

false negatives we sampled 50 term-pairs that
were falsely annotated as negative, and analyzed
the major (overlapping) types of errors (table 7).
most of these pairs had only few co-occurrences
in the corpus. this is often either due to infre-
quent terms (e.g. cbc.ca), or a rare sense of x in
which the hypernymy relation holds (e.g. (night,

play) holding for    night   , a dramatic sketch by
harold pinter). such a term-pair may have too
few hypernymy-indicating paths, leading to clas-
sifying it as negative.

8 conclusion
we presented hypenet: a neural-networks-based
method for hypernymy detection. first, we fo-
cused on improving path representation using
lstm, resulting in a path-based model that per-
forms signi   cantly better than prior path-based
methods, and matches the performance of the pre-
viously superior distributional methods. in partic-
ular, we demonstrated that the increase in recall is
a result of generalizing semantically-similar paths,
in contrast to prior methods, which either make no
generalizations or over-generalize paths.

we then extended our network by integrating
distributional signals, yielding an improvement of
additional 14 f1 points, and demonstrating that the
path-based and the distributional approaches are
indeed complementary.

finally, our architecture seems straightfor-
wardly applicable for multi-class classi   cation,
which, in future work, could be used to classify
term-pairs to multiple semantic relations.

acknowledgments
we would like to thank omer levy for his in-
volvement and assistance in the early stage of this
project and enrico santus for helping us by com-
puting the results of slqs (santus et al., 2014) on
our dataset.

this work was partially supported by an in-
tel icri-ci grant, the israel science foundation
grant 880/12, and the german research founda-
tion through the german-israeli project coopera-
tion (dip, grant da 1600/1-1).

references
[auer et al.2007] s  oren auer, christian bizer, georgi
kobilarov, jens lehmann, richard cyganiak, and
zachary ives. 2007. dbpedia: a nucleus for a web
of open data. springer.

[baroni and lenci2011] marco baroni and alessandro
lenci. 2011. how we blessed distributional seman-
in proceedings of the gems 2011
tic evaluation.
workshop on geometrical models of natural lan-
guage semantics, pages 1   10.

[baroni et al.2012] marco baroni, raffaella bernardi,
ngoc-quynh do, and chung-chieh shan. 2012. en-

tailment above the word level in distributional se-
mantics. in eacl, pages 23   32.

[borg et al.2009] claudia borg, mike rosner, and gor-
don pace. 2009. evolutionary algorithms for de   ni-
tion extraction. in proceedings of the 1st workshop
on de   nition extraction, pages 26   32.

[carlson et al.2010] andrew carlson, justin betteridge,
bryan kisiel, burr settles, estevam r hruschka jr,
and tom m mitchell. 2010. toward an architecture
for never-ending language learning. in aaai.

[fellbaum1998] christiane fellbaum. 1998. id138.

wiley online library.

[fundel et al.2007] katrin fundel, robert k  uffner, and
ralf zimmer. 2007. relexid36 using
dependency parse trees. bioinformatics, pages 365   
371.

[hearst1992] marti a hearst. 1992. automatic acqui-
sition of hyponyms from large text corpora. in acl,
pages 539   545.

[hendrickx et al.2009] iris hendrickx, su nam kim,
zornitsa kozareva, preslav nakov, diarmuid
  o s  eaghdha, sebastian pad  o, marco pennacchiotti,
lorenza romano, and stan szpakowicz.
2009.
semeval-2010 task 8: multi-way classi   cation of se-
mantic relations between pairs of nominals. in se-
meval, pages 94   99.

[hochreiter and schmidhuber1997] sepp hochreiter
and j  urgen schmidhuber. 1997. long short-term
memory. neural computation, pages 1735   1780.

[kaji and kitsuregawa2008] nobuhiro kaji and masaru
kitsuregawa. 2008. using hidden markov random
   elds to combine distributional and pattern-based
word id91. in coling, pages 401   408.

[kingma and ba2014] diederik kingma and jimmy
ba. 2014. adam: a method for stochastic opti-
mization. arxiv preprint arxiv:1412.6980.

[kotlerman et al.2010] lili kotlerman, ido dagan, idan
szpektor, and maayan zhitomirsky-geffet. 2010.
directional distributional similarity for lexical infer-
ence. nle, pages 359   389.

[kozareva and hovy2010] zornitsa kozareva and ed-
uard hovy. 2010. a semi-supervised method to
learn and construct taxonomies using the web.
in
emnlp, pages 1110   1118.

[levy et al.2015] omer levy, steffen remus, chris
biemann, and ido dagan. 2015. do supervised dis-
tributional methods really learn lexical id136 re-
lations. naacl.

[lin1998] dekang lin.

1998.

theoretic de   nition of similarity.
296   304.

an information-
in icml, pages

[liu et al.2015] yang liu, furu wei, sujian li, heng
ji, ming zhou, and houfeng wang.
2015. a
dependency-based neural network for relation clas-
si   cation. arxiv preprint arxiv:1507.04646.

[mikolov et al.2013] tomas mikolov, ilya sutskever,
kai chen, gregory s corrado, and jeffrey dean.
2013. distributed representations of words and
phrases and their compositionality. in nips, pages
3111   3119.

[mirkin et al.2006] shachar mirkin, ido dagan, and
maayan geffet. 2006. integrating pattern-based and
distributional similarity methods for lexical entail-
ment acquisition. in coling and acl, pages 579   
586.

[nakashole et al.2012] ndapandula nakashole, ger-
hard weikum, and fabian suchanek. 2012. patty: a
taxonomy of relational patterns with semantic types.
in emnlp and conll, pages 1135   1145.

[navigli and velardi2010] roberto navigli and paola
velardi.
2010. learning word-class lattices for
de   nition and hypernym extraction. in acl, pages
1318   1327.

[nguyen and grishman2015] thien huu nguyen and
ralph grishman. 2015. combining neural networks
and id148 to improve id36.
arxiv preprint arxiv:1511.05926.

[pavlick et al.2015] ellie pavlick, johan bos, malvina
nissim, charley beller, benjamin van durme, and
chris callison-burch. 2015. adding semantics to
data-driven id141. in acl.

[pennington et al.2014] jeffrey pennington, richard
socher, and christopher d. manning. 2014. glove:
global vectors for word representation. in emnlp,
pages 1532   1543.

[riedel et al.2013] sebastian riedel, limin yao, an-
drew mccallum, and benjamin m marlin. 2013.
id36 with id105 and
universal schemas. in naacl.

[rimell2014] laura rimell. 2014. distributional lexi-
cal entailment by topic coherence. in eacl, pages
511   519.

[roller et al.2014] stephen roller, katrin erk, and
inclusive yet selective:
in

gemma boleda.
supervised distributional hypernymy detection.
coling, pages 1025   1036.

2014.

[santus et al.2014] enrico santus, alessandro lenci,
qin lu, and sabine schulte im walde. 2014. chas-
ing hypernyms in vector spaces with id178.
in
eacl, pages 38   42.

[shwartz et al.2015] vered shwartz, omer levy, ido
dagan, and jacob goldberger. 2015. learning to
exploit structured resources for lexical id136. in
conll, page 175.

[snow et al.2004] rion snow, daniel jurafsky, and an-
drew y ng. 2004. learning syntactic patterns for
automatic hypernym discovery. in nips.

[snow et al.2006] rion snow, daniel jurafsky, and an-
drew y ng. 2006. semantic taxonomy induction
in acl, pages 801   
from heterogenous evidence.
808.

[suchanek et al.2007] fabian m suchanek, gjergji
kasneci, and gerhard weikum. 2007. yago: a core
of semantic knowledge. in www, pages 697   706.

[turney2006] peter d turney. 2006. similarity of se-

mantic relations. cl, pages 379   416.

[vrande  ci  c2012] denny vrande  ci  c. 2012. wikidata: a
in

new platform for collaborative data collection.
www, pages 1063   1064.

[weeds and weir2003] julie weeds and david weir.
2003. a general framework for distributional sim-
ilarity. in emlp, pages 81   88.

[weeds et al.2014] julie weeds, daoud clarke, jeremy
ref   n, david weir, and bill keller. 2014. learn-
ing to distinguish hypernyms and co-hyponyms. in
coling, pages 2249   2259.

[xu et al.2015] yan xu, lili mou, ge li, yunchuan
chen, hao peng, and zhi jin. 2015. classifying re-
lations via long short term memory networks along
shortest dependency paths. in emnlp.

[xu et al.2016] yan xu, ran jia, lili mou, ge li,
yunchuan chen, yangyang lu, and zhi jin. 2016.
improved relation classi   cation by deep recurrent
arxiv
neural networks with data augmentation.
preprint arxiv:1601.03651.

appendix a best hyper-parameters
table 8 displays the chosen hyper-parameters of
each method, yielding the highest f1 score on the
validation set.

t
i
l

p
s

m
o
d
n
a
r

t
i
l

p
s

l
a
c
i
x
e
l

method
snow

snow + gen

lstm

slqs
best

supervised

lstm-
integrated

snow

snow + gen

lstm

slqs
best

supervised

lstm-
integrated

values

id173: l2
id173: l1

embeddings: glove-100-wiki

learning rate:    = 0.001

dropout: d = 0.5

n=100, threshold = 0.000464

method: concatenation, classi   er: id166

embeddings: glove-300-wiki
embeddings: glove-50-wiki
learning rate:    = 0.001
word dropout: d = 0.3

id173: l2
id173: l2

embeddings: glove-50-wiki
learning rate:    = 0.001

dropout: d = 0.5

n=100, threshold = 0.007629

method: concatenation, classi   er: id166

embeddings: glove-100-wikipedia

embeddings: glove-50-wiki
learning rate:    = 0.001
word dropout: d = 0.3

table 8: the best hyper-parameters in every model.

