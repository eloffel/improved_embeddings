semantic regularities in id194s

fei sun, jiafeng guo, yanyan lan, jun xu, and xueqi cheng

cas key lab of network data science and technology

institute of computing technology
chinese academy of sciences, china

ofey.sunfei@gmail.com

{guojiafeng, lanyanyan, junxu, cxq}@ict.ac.cn

6
1
0
2

 
r
a

 

m
4
2

 
 
]
l
c
.
s
c
[
 
 

1
v
3
0
6
7
0

.

3
0
6
1
:
v
i
x
r
a

abstract

recent work exhibited that distributed
word representations are good at capturing
linguistic regularities in language. this
allows vector-oriented reasoning based
on simple id202 between words.
since many different methods have been
proposed for learning document represen-
tations, it is natural to ask whether there is
also linear structure in these learned rep-
resentations to allow similar reasoning at
document level. to answer this question,
we design a new document analogy task
for testing the semantic regularities in doc-
ument representations, and conduct empir-
ical evaluations over several state-of-the-
art id194 models. the
results reveal that neural embedding based
id194s work better on
this analogy task than conventional meth-
ods, and we provide some preliminary ex-
planations over these observations.

1 introduction

recently, mikolov et al. (2013c)
discovered
that word representations learned by a recur-
sive neural net (id56) as well as by related
id148
(mikolov et al., 2013b) can
capture the linguistic regularities in language,
which allows easy solutions to analogy ques-
tions of the form    beijing:china as paris:
   
using simple id202. with this word
analogy task, a    urry of subsequent work ex-
hibited that similar
linear structure can also
be revealed from representations learned from
other methods
(mnih and kavukcuoglu, 2013;
pennington et al., 2014;
levy and goldberg, 2014b).

besides word representation, document rep-
resentation is also a fundamental and critical

problem in natural language processing. over the
past decades, various methods have been proposed
to represent the document as a vector, including
bag of words (bow) (harris, 1954), latent se-
mantic indexing (lsi) (deerwester et al., 1990),
non-negative
factorization
(nmf)
latent
and
dirichlet allocation (lda)
(blei et al., 2003).
recently, there is a rising enthusiasm for applying
the neural embedding methods to represent-
ing
(srivastava et al., 2013;
documents
le and mikolov, 2014).

(lee and seung, 1999))

matrix

the

it is, therefore, natural to ask whether there
is also linear structure in these learned docu-
ment representations to allow similar reasoning
at document level. for example, given three ar-
ticles talking about naive bayes, logistic regres-
sion, and hidden markov model, is it possible to
   nd the article about conditional random    elds
as the solution to the document analogy ques-
tion    naive bayes:
id28 as hid-
    (i.e., document pairs
den markov model:
explaining generative-discriminative model rela-
tions)? obviously, such reasoning is much more
complex in semantics and cannot be achieved by
simple retrieval or classi   cation based on lexi-
cal information. representation with such lin-
ear structure would be useful for many seman-
tic processing applications, e.g., it may help con-
troversial search (dori-hacohen et al., 2015) by
discovering document pairs talking about oppo-
site facts on controversial topics with some seed
pairs, or help non-local corpus navigation and
paper recommendation together with word vec-
tors (dai et al., 2014).

for this purpose, we introduce a new docu-
ment analogy task for evaluating the semantic reg-
ularities in id194s. since it
is non-trivial to directly label the analogy ques-
tions over documents, we leverage the existing
word/phrase semantic analogy test set and map

the words/phrases in these questions to wikipedia
articles through title matching.
in this way, we
obtain a large labeled analogy test set over docu-
ments. the task is then to test whether different
id194s over the wikipedia arti-
cles can    nd the right answers to these semantic
analogy questions.

based on this test set, we evaluate several
existing state-of-the-art id194s
and show that neural embedding based models
can achieve better performance than conventional
models. the major contributions of this paper in-
clude: 1) the introduction of a new document anal-
ogy task with benchmark dataset for evaluating
id194s; 2) empirical compari-
son among state-of-the-art models and preliminary
explanations over the results.

2 measuring semantic regularities

2.1 a document analogy test set
we propose to create a document analogy test set
so that we can quantitatively evaluate how well
different id194s capture seman-
tic regularities. following the idea of word anal-
ogy task, we try to build a test set of analogy
   ,
questions of the form    a is to b as c is to
where a, b, c are the identities of the documents.
however, it is not trivial to directly label the re-
lations between two arbitrary documents due to
the diversity in topics. fortunately, we found that
each wikipedia page is a concise document de-
scribing one speci   c concept, and thus the rela-
tions between the documents can be explained by
their corresponding concepts. therefore, we can
convert the task of labeling between documents
into that between concepts (which are of words or
phrases), where we already have a large labeled
data set from mikolov et al. (2013b).

based on the idea above, we build a docu-
ment analogy test set using wikipedia and existing
word and phrase analogy test set. speci   cally, we
adopt the publicly available april 2010 dump of
wikipedia1 (shaoul and westbury, 2010), which
has been widely used in (huang et al., 2012;
luong et al., 2013; neelakantan et al., 2014). the
corpus contains 3,035,070 articles and about 1
billion tokens. we then collect all the existing
word and phrase analogy test sets and match the
words/phrases in questions to wikipedia page ti-
tles. note here we do not take syntactic anal-

ogy questions of words into consideration because
the relations between documents are usually se-
mantic. by resolving the ambiguity in match-
ing, we    nally obtain 6112 analogy questions over
wikipedia documents. table 1 shows the details
of the test set.

2.2 analogy reasoning

in this work, we adopt the same vector offset
method (mikolov et al., 2013c) for analogy rea-
soning. to answer the questions like    a is to b as c
   , we try to    nd a document with vector ~x,
is to
which is the closest to ~b     ~a + ~c according to the
cosine similarity:

arg max

(~b + ~c     ~a)    ~x

x   d,x6=a
x6=b, x6=c

(1)

where ~b, ~c, ~a and ~x are the normalized document
vectors. the question is judged as correctly an-
swered only if x is exactly the answer document
in the evaluation set. the evaluation metric for
this task is the percentage of questions answered
correctly.

3 models

in this section, we brie   y summarize the models
used in this paper. before that, we    rst list the
notations.

over

the word

let d={d1, . . . , dn } denote a corpus of
vocabulary
n documents
let x     rn   |v | be a
v ={w1, . . . , w|v |}.
document-word matrix, where entry xij
in x
denotes the weight of the j-th word wj in the i-th
document dj.

bag of words (bow) model treats a document
as a bag (multiset) of its words.
it represents a
document di as a vector ~xi = (xi1,          , xi|v |),
where xij denotes the weight of the j-th word wj
in the i-th document dj. the most popular weight-
ing scheme for xij is tf-idf (jones, 1972). how-
ever, the bow model suffers from the sparsity
and curse of dimensionality due to treating indi-
vidual word as distinct feature.

id105 methods attempt

to
tackle the limitation of bow model through learn-
ing a low-dimensional vector for document by fac-
torizing the document-word matrix x.

deerwester et al. (1990) applied truncated sin-
gular value decomposition (svd) to document-
word matrix, namely id45

1http://nlp.stanford.edu/data/westburylab.wikicorp.201004.txt.bz2

count example

table 1: details of the test set, where the words/phases in example refers to wikipedia articles.
relation
capital-common-countries
capital-world
currency
city-in-state
family
newspapers
ice hockey
basketball
airlines
people-companies

beijing: china     paris: france
bangkok: thailand     cario: egypt
europe: euro     india: rupee
houston: texas     miami:    orida
boy: girl     man:woman
chicago: chicago tribune     houston: houston chronicle
boston: boston bruins     los angeles: los angeles kings
chicago: chicago bulls     dallas: dallas mavericks
canada: air canada     italy: alitalia
bill gates: microsoft     larry page: google

506
3991
88
277
56
20
462
306
306
100

(lsi). lsi approximates x by setting all but the
largest k singular values in    to 0 (  k), as

x     d  kwt

hence one might think of the rows of d  k as rep-
resentations for documents in the latent space.

an alternative way is factorizing x into two

non-negative matrices (lee and seung, 1999),

bag of id27s (bowe) model
tries to represent the document as a linear com-
bination of word vectors, where the word vectors
w can be obtained by tools like id97 or
glove. the low-dimensional representations of
documents in bowe can be written as

d = xw

x     dwt

where x denotes the bow representation of doc-
uments.

where the rows of d can be seen as the represen-
tations of documents.

unlike lsi which may have negative entries,
nmf has better interpretability with the non-
negative constraint.

topic models are also very popular in docu-
ment representation    elds because of their good
interpretability, generalization ability and extensi-
bility. the most representative work is the latent
dirichlet allocation (lda) model introduced by
blei et al. (2003). it represents the documents as
distributions over latent topics, where each topic
is characterized by a distribution over words.

neural embedding models have attracted
representations due
language

much attention in text
to its breakthrough in statistical
model (bengio et al., 2003).

the paragraph vector models are    rst intro-
duced in (le and mikolov, 2014) for document
representation. the distributed memory model
of paragraph vectors (pv-dm) captures the repre-
sentation of a document via inserting a document
vector to the continuous bag-of-words (cbow)
model (mikolov et al., 2013a). a simpler model
can be obtained by replacing the input word vector
with document vector in skip gram (sg) model,
which is called    distributed bag of words version
of paragraph vector    (pv-dbow).

4 experiments

in this section, we    rst describe our experimental
settings including the corpus, hyper-parameter se-
lections, and speci   cations for different document
representation methods. then we compare these
methods on document analogy task and discuss the
results.

4.1 corpus and preprocessing
the corpus used to learn id194s
in this experiment is the same wikipedia april
2010 dump as described in section 2.1.
in pre-
processing, we lowercase the corpus, remove pure
digit words, non-english characters and the words
occur less than 20 times.

4.2 experimental settings
the baseline methods used in this paper including
bow with tf-idf weight, lsi, nmf, lda, pv-
dm, pv-dbow, and bowe. for bow, lsi and
lda, we use the popular python topic model li-
brary gensim2. for nmf, we choose the python
machine learning library scikit learn3. we
implement pv-dm and pv-dbow models in c++
due to le and mikolov (2014) have not released

2http://radimrehurek.com/gensim
3http://scikit-learn.org

table 2: results on the document analogy task under dimension 100. bold scores are the best.
lda pv-dm pv-dbow bowe
relation
83.0
23.72
capital-common-countries
67.53
capital-world
9.15
14.77
0.0
currency
51.26
4.33
city-in-state
family
14.29
19.64
10.0
newspapers
40.0
33.33
0.0
ice hockey
38.56
basketball
0.0
42.48
2.61
airlines
0.0
people-companies
2.0
60.42
total
8.43

bow lsi nmf
9.29
5.06
0.0
6.50
1.79
10.0
2.16
1.63
2.94
0.0
4.81

23.12
9.97
0.0
7.94
5.36
25.0
3.68
4.25
9.15
1.0
9.88

0.0
0.8
0.0
0.0
19.64
5.0
0.0
0.0
11.76
2.0
1.34

60.87
43.62
4.55
33.57
21.43
5.0
12.12
10.13
12.42
6.0
37.47

54.15
42.65
3.41
34.30
21.43
50.0
20.13
14.71
20.26
12.0
37.76

source codes of pv models. for word embed-
dings in bowe, we use cbow in the id97
tool4. the negative sampling method is adopted
to take the place of the hierarchal softmax since
we found the former always achieves better perfor-
mance. the learning rate is linearly decayed to 0
as described in (mikolov et al., 2013a), where the
initial learning rate of pv-dm and cbow model
is 0.05, and pv-dbow is 0.025. we set context
window size as 10 and use 10 negative samples.

4.3 results
in table 2, we compare the results of 100-
dimensional document vectors from all the meth-
ods on different subtasks of document analogy. as
we can see, among all the methods, bow is al-
most the worst. this demonstrates the weakness
of simple vector space model on capturing seman-
tic regularities.

neural embedding models such as pv-dm
and pv-dbow perform much better than con-
ventional
latent models such as lsi, nmf,
and lda. this is quite amazing since pv
models can also be viewed as implicit matrix
factorization according to the explanations on
id97 (levy and goldberg, 2014a). a ma-
jor difference is that conventional latent models
usually work on matrix with each entry standing
for the frequency or tf-idf of a word in a docu-
ment, while pv models factorize a shifted point-
wise mutual information (shifted-pmi) matrix. as
discussed in (arora et al., 2015), pmi is a key fac-
tor why id97 can work well for word anal-

4https://code.google.com/p/id97/

ogy task. we guess this might also be a major
factor to explain the gap between pv models and
other latent models. therefore, we conducted fur-
ther experiments on lsi. as a result, we    nd that
the total accuracy of lsi can achieve 15.53% with
pmi matrix (about 57% performance gain over
lsi with tf-idf matrix). the results indicate that
pmi plays an important role in revealing linear
structure in id194s.

a surprising result is that the simple bowe
than any
model performs signi   cantly better
the subtasks (p-
other methods on almost all
value < 0.01).
there might be two possi-
ble reasons for the result. firstly, when learn-
ing word vectors alone with id97, one
can achieve very high scores on word analogy
tasks (mikolov et al., 2013c). bowe thus bene-
   ts from the strong linear structures in word vec-
tors by directly using word vectors as the repre-
sentation of a document. secondly, the calcula-
tion of euclidean distance5 between documents
under bowe is equivalent to using a relaxed word
mover   s distance (kusner et al., 2015), which has
been shown strong performance in measuring doc-
ument distance.

we also conduct the experiments on different
dimensions as shown in table 3. similar trending
can be found as that in table 2.

5 conclusion

in this paper, we introduce a new document anal-
ogy task for quantitatively evaluating how well

5note that dot product between normalized vectors in

analogy reasoning is equivalent to euclidean distance.

table 3: results on the document analogy task un-
der different dimensions.

model

50
1.34
bow
4.19
lsi
1.59
nmf
3.52
lda
pv-dm
25.62
pv-dbow 25.33
42.05
bowe

dimension
100
150
1.34
1.34
17.0
9.88
8.75
4.81
8.43
10.39
37.71
37.47
40.61
37.76
60.42
66.74

200
1.34
21.81
11.85
10.45
36.03
39.09
69.49

different id194s capture seman-
tic regularities. based on the introduced bench-
mark dataset, we conduct empirical comparisons
among several state-of-the-art document represen-
tation methods. the results reveal that neural
embedding based id194s work
better on this analogy task. we provide some pre-
liminary explanations on these observations, leav-
ing the inherent differences of these models to be
further investigated in the future. with this bench-
mark dataset, it would also be easier for us to de-
velop new id194 models and to
compare with existing methods.

references

[arora et al.2015] sanjeev arora, yuanzhi li, yingyu
liang, tengyu ma, and andrej risteski. 2015. ran-
dom walks on context spaces: towards an explana-
tion of the mysteries of semantic id27s.
corr, abs/1502.03520.

[bengio et al.2003] yoshua bengio, r  ejean ducharme,
pascal vincent, and christian janvin. 2003. a neu-
ral probabilistic language model. j. mach. learn.
res., 3:1137   1155, march.

[blei et al.2003] david m. blei, andrew y. ng, and
michael i. jordan. 2003. id44.
j. mach. learn. res., 3:993   1022, march.

[dai et al.2014] andrew m dai, christopher olah,
quoc v le, and greg s corrado.
2014. doc-
ument embedding with paragraph vectors. nips
deep learning workshop.

[deerwester et al.1990] scott deerwester, susan t. du-
mais, george w. furnas, thomas k. landauer, and
richard harshman. 1990.
indexing by latent se-
mantic analysis. journal of the american society
for information science, 41(6):391   407.

[dori-hacohen et al.2015] shiri dori-hacohen, elad
yom-tov, and james allan. 2015. navigating con-
troversy as a complex search task. in proceedings
of the first international workshop on supporting
complex search tasks co-located with the 37th eu-
ropean conference on information retrieval(ecir
2015). elsevier, march.

[harris1954] zellig harris. 1954. distributional struc-

ture. word, 10(23):146   162.

[huang et al.2012] eric h. huang, richard socher,
christopher d. manning, and andrew y. ng. 2012.
improving word representations via global context
and multiple word prototypes. in proceedings of the
50th annual meeting of the association for compu-
tational linguistics: long papers - volume 1, acl
   12, pages 873   882, stroudsburg, pa, usa. associ-
ation for computational linguistics.

[jones1972] karen sprck jones. 1972. a statistical in-
terpretation of term speci   city and its application in
retrieval. journal of documentation, 28(1):11   21.

[kusner et al.2015] matt j. kusner, yu sun, nicholas i.
kolkin, and kilian q. weinberger. 2015. from
id27s to document distances.
in pro-
ceedings of the 32th international conference on
machine learning (icml-15), icml    15. acm,
new york, ny, usa, july.

[le and mikolov2014] quoc le and tomas mikolov.
2014. distributed representations of sentences and
documents. in tony jebara and eric p. xing, editors,
proceedings of the 31st international conference
on machine learning (icml-14), pages 1188   1196.
jmlr workshop and conference proceedings.

[lee and seung1999] daniel d. lee and h. sebas-
tian seung.
learning the parts of ob-
jects by non-negative id105. nature,
401(6755):788   791, october.

1999.

[levy and goldberg2014a] omer levy and yoav gold-
berg. 2014a. neural id27 as implicit
id105. in advances in neural infor-
mation processing systems 27, pages 2177   2185.
curran associates, inc., montreal, quebec, canada.

[levy and goldberg2014b] omer levy and yoav gold-
berg, 2014b. proceedings of the eighteenth con-
ference on computational natural language learn-
ing, chapter linguistic regularities in sparse and
explicit word representations, pages 171   180. as-
sociation for computational linguistics.

[luong et al.2013] minh-thang

luong,

richard
socher, and christopher d. manning.
2013.
better word representations with recursive neural
networks for morphology.
in proceedings of the
seventeenth conference on computational natural
language learning, pages 104   113. association
for computational linguistics.

[mikolov et al.2013a] tomas mikolov, kai chen, greg
corrado, and jeffrey dean. 2013a. ef   cient esti-
mation of word representations in vector space. in
proceedings of workshop of iclr.

[mikolov et al.2013b] tomas mikolov, ilya sutskever,
kai chen, greg s corrado, and jeff dean. 2013b.
distributed representations of words and phrases
and their compositionality. in c.j.c. burges, l. bot-
tou, m. welling, z. ghahramani, and k.q. wein-
berger, editors, advances in neural information
processing systems 26, pages 3111   3119. curran
associates, inc.

[mikolov et al.2013c] tomas mikolov, wen tau yih,
and geoffrey zweig. 2013c. linguistic regulari-
ties in continuous space word representations.
in
proceedings of the 2013 conference of the north
american chapter of the association for computa-
tional linguistics: human language technologies
(naacl-hlt-2013). association for computational
linguistics, may.

[mnih and kavukcuoglu2013] andriy mnih and koray
kavukcuoglu. 2013. learning id27s ef-
   ciently with noise-contrastive estimation. in c.j.c.
burges, l. bottou, m. welling, z. ghahramani, and
k.q. weinberger, editors, advances in neural in-
formation processing systems 26, pages 2265   2273.
curran associates, inc.

[neelakantan et al.2014] arvind neelakantan, jeevan
shankar, alexandre passos, and andrew mccal-
lum.
2014. ef   cient non-parametric estimation
of multiple embeddings per word in vector space.
in proceedings of the 2014 conference on em-
pirical methods in natural language processing
(emnlp), pages 1059   1069, doha, qatar, october.
association for computational linguistics.

[pennington et al.2014] jeffrey pennington, richard
socher, and christopher d. manning. 2014. glove:
global vectors for word representation. in proceed-
ings of the 2014 conference on empirical methods
in natural language processing, emnlp 2014, oc-
tober 25-29, 2014, doha, qatar, a meeting of sig-
dat, a special interest group of the acl, pages
1532   1543.

[shaoul and westbury2010] cyrus shaoul and chris
westbury. 2010. the westbury lab wikipedia cor-
pus. edmonton, ab: university of alberta.

[srivastava et al.2013] nitish

srivastava,

ruslan
salakhutdinov, and geoffrey e. hinton.
2013.
modeling documents with deep boltzmann ma-
chines.
the twenty-ninth
conference on uncertainty in arti   cial intelligence,
pages 616   625, seattle, usa, august.

in proceedings of

