5
1
0
2

 

y
a
m
1

 

 
 
]
l
c
.
s
c
[
 
 

4
v
3
2
6
6

.

2
1
4
1
:
v
i
x
r
a

published as a conference paper at iclr 2015

word representations via
gaussian embedding

luke vilnis, andrew mccallum
school of computer science
university of massachusetts amherst
amherst, ma 01003
luke@cs.umass.edu, mccallum@cs.umass.edu

abstract

current work in lexical distributed representations maps each word to a point
vector in low-dimensional space. mapping instead to a density provides many
interesting advantages, including better capturing uncertainty about a representa-
tion and its relationships, expressing asymmetries more naturally than dot product
or cosine similarity, and enabling more expressive parameterization of decision
boundaries. this paper advocates for density-based distributed embeddings and
presents a method for learning representations in the space of gaussian distribu-
tions. we compare performance on various id27 benchmarks, inves-
tigate the ability of these embeddings to model entailment and other asymmetric
relationships, and explore novel properties of the representation.

1

introduction

in recent years there has been a surge of interest in learning compact distributed representations or
embeddings for many machine learning tasks, including collaborative    ltering (koren et al., 2009),
id162 (weston et al., 2011), id36 (riedel et al., 2013), word semantics and
id38 (bengio et al., 2006; mnih & hinton, 2008; mikolov et al., 2013), and many
others. in these approaches input objects (such as images, relations or words) are mapped to dense
vectors having lower-dimensionality than the cardinality of the inputs, with the goal that the ge-
ometry of his low-dimensional latent embedded space be smooth with respect to some measure of
similarity in the target domain. that is, objects associated with similar targets should be mapped to
nearby points in the embedded space.
while this approach has proven powerful, representing an object as a single point in space carries
some important limitations. an embedded vector representing a point estimate does not naturally
express uncertainty about the target concepts with which the input may be associated. point vec-
tors are typically compared by dot products, cosine-distance or euclean distance, none of which
provide for asymmetric comparisons between objects (as is necessary to represent inclusion or en-
tailment). relationships between points are normally measured by distances required to obey the
triangle inequality.
this paper advocates moving beyond vector point representations to potential functions (aizerman
et al., 1964), or continuous densities in latent space. in particular we explore gaussian function
embeddings (currently with diagonal covariance), in which both means and variances are learned
from data. gaussians innately represent uncertainty, and provide a distance function per object. kl-
divergence between gaussian distributions is straightforward to calculate, naturally asymmetric, and
has a geometric interpretation as an inclusion between families of ellipses.
there is a long line of previous work in mapping data cases to id203 distributions, perhaps
the most famous being radial basis functions (rbfs), used both in the kernel and neural network
literature. we draw inspiration from this work to propose novel id27 algorithms that
embed words directly as gaussian distributional potential functions in an in   nite dimensional func-
tion space. this allows us to map word types not only to vectors but to soft regions in space,
modeling uncertainty, inclusion, and entailment, as well as providing a rich geometry of the latent
space.

1

published as a conference paper at iclr 2015

figure 1: learned diagonal vari-
ances, as used in evaluation (section
6), for each word, with the    rst let-
ter of each word indicating the po-
sition of its mean. we project onto
generalized eigenvectors between the
mixture means and variance of query
word bach. nearby words to bach
are other composers e.g. mozart,
which lead to similar pictures.

after discussing related work and presenting our algorithms below we explore properties of our al-
gorithms with multiple qualitative and quantitative evaluation on several real and synthetic datasets.
we show that concept containment and speci   city matches common intuition on examples concern-
ing people, genres, foods, and others. we compare our embeddings to skip-gram on seven standard
word similarity tasks, and evaluate the ability of our method to learn unsupervised lexical entail-
ment. we also demonstrate that our training method also supports new styles of supervised training
that explicitly incorporate asymmetry into the objective.

2 related work

this paper builds on a long line of work on both distributed and distributional semantic word vec-
tors, including id65, neural language models, count-based language models, and,
more broadly, the    eld of representation learning.
related work in probabilistic id105 (mnih & salakhutdinov, 2007) embeds rows and
columns as gaussians, and some forms of this do provide each row and column with its own vari-
ance (salakhutdinov & mnih, 2008). given the parallels between embedding models and matrix
factorization (deerwester et al., 1990; riedel et al., 2013; levy & goldberg, 2014), this is relevant
to our approach. however, these bayesian methods apply bayes    rule to observed data to infer the
latent distributions, whereas our model works directly in the space of id203 distributions and
discriminatively trains them. this allows us to go beyond the bayesian approach and use arbitrary
(and even asymmetric) training criteria, and is more similar to methods that learn kernels (lanckriet
et al., 2004) or function-valued neural networks such as mixture density networks (bishop, 1994).
other work in multiplicative tensor factorization for id27s (kiros et al., 2014) and met-
ric learning (xing et al., 2002) learns some combinations of representations, clusters, and a distance
metric jointly; however, it does not effectively learn a distance function per item. fitting gaussian
mixture models on embeddings has been done in order to apply fisher kernels to entire documents
(clinchant & perronnin, 2013b;a). preliminary concurrent work from kiyoshiyo et al. (2014) de-
scribes a signi   cantly different model similar to bayesian id105, using a probabilistic
gaussian graphical model to de   ne a distribution over pairs of words, and they lack quantitative
experiments or evaluation.
in linguistic semantics, work on the distributional inclusion hypothesis (geffet & dagan, 2005), uses
traditional count-based vectors to de   ne regions in vector space (erk, 2009) such that subordinate
concepts are included in these regions. in fact, one strength of our proposed work is that we extend
these intuitively appealing ideas (as well as the ability to use a variety of asymmetric distances
between vectors) to the dense, low-dimensional distributed vectors that are now gaining popularity.

3 background
our goal is to map every word type w in some dictionary d and context word type c in a dictionary c
to a gaussian distribution over a latent embedding space, such that linguistic properties of the words

2

published as a conference paper at iclr 2015

are captured by properties of and relationships between the distributions. for precision, we call an
element of the dictionary a word type, and a particular observed token in some context a word token.
this is analogous to the class vs. instance distinction in object-oriented programming.
in unsupervised learning of word vectors, we observe a sequence of word tokens {t(w)i} for each
type w, and their contexts (sets of nearby word tokens), {c(w)i}. the goal is to map each word type
w and context word type c to a vector, such that types that appear in similar contexts have similar
vectors. when it is unambiguous, we also use the variables w and c to denote the vectors associated
to that given word type or context word type.
an energy function (lecun et al., 2006) is a function e  (x, y) that scores pairs of inputs x and
outputs y, parametrized by   . the goal of energy-based learning is to train the parameters of the
energy function to score observed positive input-output pairs higher (or lower, depending on sign
conventions) than negative pairs. this is accomplished by means of a id168 l which de   nes
which pairs are positive and negative according to some supervision, and provides gradients on the
parameters given the predictions of the energy function.
in prediction-based (energy-based) id27 models, the parameters    correspond to our
learned word representations, and the x and y input-output pairs correspond to word tokens and their
contexts. these contexts can be either positive (observed) or negative (often randomly sampled). in
the id97 skip-gram (mikolov et al., 2013) id27 model, the energy function takes
the form of a dot product between the vectors of an observed word and an observed context w(cid:62)c. the
id168 is a binary id28 classi   er that treats the score of a word and its observed
context as the score of a positive example, and the score of a word and a randomly sampled context
as the score of a negative example.
backpropagating (rumelhart et al., 1986) this loss to the word vectors trains them to be predictive of
their contexts, achieving the desired effect (words in similar contexts have similar vectors). in recent
work, id97 has been shown to be equivalent to factoring certain types of weighted pointwise
mutual information matrices (levy & goldberg, 2014).
in our work, we use a slightly different id168 than skip-gram id97 embeddings. our
energy functions take on a more limited range of values than do vector dot products, and their
dynamic ranges depend in complex ways on the parameters. therefore, we had dif   culty using the
id97 loss that treats scores of positive and negative pairs as positive and negative examples to a
binary classi   er, since this relies on the ability to push up on the energy surface in an absolute, rather
than relative, manner. to avoid the problem of absolute energies, we train with a ranking-based loss.
we chose a max-margin ranking objective, similar to that used in rank-id166 (joachims, 2002) or
wsabie (weston et al., 2011), which pushes scores of positive pairs above negatives by a margin:

lm(w, cp, cn) = max(0, m     e(w, cp) + e(w, cn))

in this terminology, the contribution of our work is a pair of energy functions for training gaussian
distributions to represent word types.

4 warmup: empirical covariances

given a pre-trained set of id27s trained from contexts, there is a simple way to construct
variances using the empirical variance of a word type   s set of context vectors.
for a word w with n word vector sets {c(w)i} representing the words found in its contexts, and
window size w , the empirical variance is

n(cid:88)

w(cid:88)

i

j

  w =

1

n w

(c(w)ij     w)(c(w)ij     w)(cid:62)

this is an estimator for the covariance of a distribution assuming that the mean is    xed at w. in
practice, it is also necessary to add a small ridge term    > 0 to the diagonal of the matrix to
regularize and avoid numerical problems when inverting.
however, in section 6.2 we note that the distributions learned by this empirical estimator do not
possess properties that we would want from gaussian distributional embeddings, such as unsuper-
vised entailment represented as inclusion between ellipsoids. by discriminatively embedding our

3

published as a conference paper at iclr 2015

predictive vectors in the space of gaussian distributions, we can improve this performance. our
models can learn certain forms of entailment during unsupervised training, as discussed in section
6.2 and exempli   ed in figure 1.

5 energy-based learning of gaussians

as discussed in section 3, our architecture learns gaussian distributional embeddings to predict
words in context given the current word, and ranks these over negatively sampled words. we present
two energy functions to train these embeddings.

5.1 symmetric similarity: expected likelihood or id203 product

kernel

while the dot product between two means of independent gaussians is a perfectly valid measure
of similarity (it is the expected dot product), it does not incorporate the covariances and would not
enable us to gain any bene   t from our probabilistic model.
the most logical next choice for a symmetric similarity function would be to take the inner product
between the distributions themselves. recall that for two (well-behaved) functions f, g     rn     r,
a standard choice of inner product is

(cid:90)

i.e. the continuous version of(cid:80)

f (x)g(x)dx

x   rn

i figi = (cid:104)f, g(cid:105) for discrete vectors f and g.

(cid:90)

this idea seems very natural, and indeed has appeared before     the idea of mapping data cases w into
id203 distributions (often over their contexts), and comparing them via integrals has a history
under the name of the expected likelihood or id203 product kernel (jebara et al., 2004).
for gaussians, the inner product is de   ned as

e(pi, pj) =

x   rn

n (x;   i,   i)n (x;   j,   j)dx = n (0;   i       j,   i +   j)

the proof of this identity follows from simple calculus. this is a consequence of the broader fact
that the gaussian is a stable distribution, i.e. the convolution of two gaussian random variables is
another gaussian.
since we aim to discriminatively train the weights of the energy function, and it is always positive,
we work not with this quantity directly, but with its logarithm. this has two motivations:    rstly, we
plan to use ranking loss, and ratios of densities and likelihoods are much more commonly worked
with than differences     differences in probabilities are less interpretable than an odds ratio. secondly,
it is easier numerically, as otherwise the quantities can get exponentially small and harder to deal
with.
the logarithm of the energy (in d dimensions) is
log n (0;   i     j,   i+  j) =     1
2

log(2  ).
   a log det a = a   1, and the gradient
recalling that the gradient of the log determinant is
   a x(cid:62)a   1y =    a   (cid:62)xy(cid:62)a   (cid:62) (petersen, 2006) we can take the gradient of this energy function
   
with respect to the means    and covariances   :

(  i     j)(cid:62)(  i+  j)   1(  i     j)    d
2

log det(  i+  j)    1
2

   

    log e(pi, pj)

     i

=         log e(pi, pj)

     j

=       ij

    log e(pi, pj)

     i

    log e(pi, pj)

=
where    ij = (  i +   j)   1(  i       j)

     j

=

1
2

(   ij   (cid:62)

ij     (  i +   j)   1)

for diagonal and spherical covariances, these matrix inverses are trivial to compute, and even in the
full-matrix case can be solved very ef   ciently for the small dimensionality common in embedding

4

published as a conference paper at iclr 2015

models. if the matrices have a low-rank plus diagonal structure, they can be computed and stored
even more ef   ciently using the matrix inversion lemma.
this log-energy has an intuitive geometric interpretation as a similarity measure. gaussians are
measured as close to one another based on the distance between their means, as measured through
the mahalanobis distance de   ned by their joint inverse covariance. recalling that log det a + const.
is equivalent to the log-volume of the ellipse spanned by the principle components of a, we can
interpret this other term of the energy as a regularizer that prevents us from decreasing the distance
by only increasing joint variance. this combination pushes the means together while encouraging
them to have more concentrated, sharply peaked distributions in order to have high energy.

5.2 asymmetric similarity: kl divergence

training vectors through kl-divergence to encode their context distributions, or even to incorporate
more explicit directional supervision re: entailment from a knowledge base or id138, is also
a sensible objective choice. we optimize the following energy function (which has a similarly
tractable closed form solution for gaussians):

   e(pi, pj) = dkl(nj||ni) =

x   rn

n (x;   i,   i) log

n (x;   j,   j)
n (x;   i,   i)
(  i       j)     d     log

dx

det(  j)
det(  i)

)

=

1
2

(tr(     1

i   j) + (  i       j)(cid:62)     1

i

(cid:90)

note the leading negative sign (we de   ne the negative energy), since kl is a distance function and
not a similarity. kl divergence is a natural energy function for representing entailment between
concepts     a low kl divergence from x to y indicates that we can encode y easily as x, implying
that y entails x. this can be more intuitively visualized and interpreted as a soft form of inclusion
between the level sets of ellipsoids generated by the two gaussians     if there is a relatively high
expected log-likelihood ratio (negative kl), then most of the mass of y lies inside x.
just as in the previous case, we can compute the gradients for this energy function in closed form:

   e(pi, pj)

     i

   e(pi, pj)

     i

   e(pi, pj)

     j

(     1

=

i +    (cid:48)

ij   

ij          1
(cid:48)(cid:62)

i

)

=        e(pi, pj)

=       (cid:48)

ij

     j
i   j     1
j          1
)
ij =      1

i

i

(     1
=
where    (cid:48)

1
2
1
2

(  i       j)

using the fact that    
2006).

   a tr(x(cid:62)a   1y ) =    (a   1y x(cid:62)a   1)(cid:62) and    

   a tr(xa) = x(cid:62) (petersen,

5.3 uncertainty of inner products

another bene   t of embedding objects as id203 distributions is that we can look at the distribu-
tion of dot products between vectors drawn from two gaussian representations. this distribution is
not itself a one-dimensional gaussian, but it has a    nite mean and variance with a simple structure
in the case where the two gaussians are assumed independent (brown & rutemiller, 1977). for the
distribution p (z = x(cid:62)y), we have

y   y  y + tr(  x  y)

this means we can    nd e.g. a lower or upper bound on the dot products of random samples from
these distributions, that should hold some given percent of the time. parametrizing this energy by
some number of standard deviations c, we can also get a range for the dot product as:

x   y    c
  (cid:62)

  (cid:62)
x   x  x +   (cid:62)

y   y  y + tr(  x  y)

  z =   (cid:62)
  z =   (cid:62)

x   y
x   x  x +   (cid:62)
(cid:113)

5

published as a conference paper at iclr 2015

we can choose c in a principled using an (incorrect) gaussian approximation, or more general con-
centration bounds such as chebyshev   s inequality.

5.4 learning

to learn our model, we need to pick an energy function (el or kl), a id168 (max-margin),
and a set of positive and negative training pairs. as the landscape is highly nonconvex, it is also
helpful to add some id173.
we regularize the means and covariances differently, since they are different types of geometric
objects. the means should not be allowed to grow too large, so we can add a simple hard constraint
to the (cid:96)2 norm:

(cid:107)  i(cid:107)2     c,    i

however, the covariance matrices need to be kept positive de   nite as well as reasonably sized. this
is achieved by adding a hard constraint that the eigenvalues   i lie within the hypercube [m, m ]d for
constants m and m.

mi       i     m i,    i

for diagonal covariances, this simply involves either applying the min or max function to each
element of the diagonal to keep it within the hypercube,   ii     max(m, min(m,   ii)).
controlling the bottom eigenvalues of the covariance is especially important when training with
expected likelihood, since the energy function includes a log det term that can give very high scores
to small covariances, dominating the rest of the energy.
we optimize the parameters using adagrad (duchi et al., 2011) and stochastic gradients in small
minibatches containing 20 sentences worth of tokens and contexts.

6 evaluation

we evaluate the representation learning algorithms on several qualitative and quantitative tasks,
including modeling asymmetric and linguistic relationships, uncertainty, and word similarity. all
gaussian experiments are conducted with 50-dimensional vectors, with diagonal variances except
where noted otherwise. unsupervised embeddings are learned on the concatenated ukwac and
wackypedia corpora (baroni et al., 2009), consisting of about 3 billion tokens. this matches the
experimental setup used by baroni et al. (2012), aside from leaving out the small british national
corpus, which is not publicly available and contains only 100 million tokens. all word types that
appear less than 100 times in the training set are dropped, leaving a vocabulary of approximately
280 thousand word types.
when training id97 skip-gram embeddings for baselines, we follow the above training setup
(50 dimensional embeddings), using our own implementation of id97 to change as little as
possible between the two models, only the id168. we train both models with one pass
over the data, using separate embeddings for the input and output contexts, 1 negative sample per
positive example, and the same subsampling procedure as in the id97 paper (mikolov et al.,
2013). the only other difference between the two training regimes is that we use a smaller (cid:96)2
id173 constraint when using the id97 id168, which improves performance vs.
the diagonal gaussian model which does better with    spikier    mean embeddings with larger norms
(see the comment in section 6.4). the original id97 implementation uses no (cid:96)2 constraint, but
we saw better performance when including it in our training setup.

6.1 specificity and uncertainty of embeddings

in figure 2, we examine some of the 100 nearest neighbors of several query words as we sort from
largest to smallest variance, as measured by determinant of the covariance matrix, using diagonal
gaussian embeddings. note that more speci   c words, such as joviality and electroclash have smaller
variance, while polysemous words or those denoting broader concepts have larger variances, such as
mix, mind, and graph. this is not merely an artifact of higher frequency words getting more variance
    when sorting by those words whose rank by frequency and rank by variance are most dissimilar,
we see that genres with names like chillout, avant, and shoegaze overindex their variance compared

6

published as a conference paper at iclr 2015

query word nearby words, descending variance
rock

food
feeling

algebra

mix sound blue folk jazz rap avant hardcore chillout shoegaze powerpop
electroclash
drink meal meat diet spice juice bacon soya gluten stevia
sense mind mood perception compassion sadness coldness sincerity
perplexity dif   dence joviality
theory graph equivalence    nite predicate congruence topology
quaternion symplectic homomorphism

figure 2: elements of the top 100 nearest neighbor sets for chosen query words, sorted by descend-
ing variance (as measured by determinant of covariance matrix). note that less speci   c and more
ambiguous words have greater variance.

test

model
baroni et al. (2012) e
e
empirical (d)
e
empirical (d)
empirical (s)
e
e
empirical (s)
e
learned (d)
e
learned (d)
e
learned (s)
learned (s)
e

similarity best f1 ap
balapinc
kl
cos
kl
cos
kl
cos
kl
cos

75.1
70.05
76.24
71.18
76.24
79.01
76.99
79.34
77.36

   
.68
.71
.69
.71
.80
.73
.78
.73

figure 3: entailment: we compare empirical and learned variances, both diagonal (d) and spherical
(s). e is the dataset of baroni et al. (2012). measures of similarity are symmetric (cosine between
means) and asymmetric (kl) divergence for gaussians. balapinc is an asymmetric similarity mea-
sure speci   c to sparse, distributional count-based representations.

to how frequent they are, since they appear in different contexts. similarly, common emotion words
like sadness and sincerity have less variance than their frequency would predict, since they have
fairly    xed meanings. another emotion word, coldness, is an uncommon word with a large variance
due to its polysemy.

6.2 entailment

as can be seen qualitatively in figure 1, our embeddings can learn some forms of unsupervised
entailment directly from the source data. we evaluate quantitatively on the entailment dataset of
baroni et al. (2012). our setup is essentially the same as theirs but uses slightly less data, as men-
tioned in the beginning of this section. we evaluate with average precision and best f1 score. we
include the best f1 score (by picking the optimal threshold at test) because this is used by baroni
et al. (2012), but we believe ap is better to demonstrate the correlation of various asymmetric and
symmetric measures with the entailment data.
in figure 3, we compare variances learned jointly during embedding training by using the expected
likelihood objective, with empirical variances gathered from contexts on pre-trained id97-style
embeddings. we compare both diagonal (d) and spherical (s) variances, using both cosine similarity
between means, and kl divergence. baseline asymmetric measurements, such as the difference
between the sizes of the two embeddings, did worse than the cosine. we see that kl divergence
between the entailed and entailing word does not give good performance for the empirical variances,
but beats the count-based balapinc measure when used with learned variances.
for the baseline empirical model to achieve reasonable performance when using kl divergence,
we regularized the covariance matrices, as the unregularized matrices had very small entries. we
regularized the empirical covariance by adding a small ridge    to the diagonal, which was tuned to
maximize performance, to give the largest possible advantage to the baseline model. interestingly,
the empirical variances do worse with kl than the symmetric cosine similarity when predicting en-
tailment. this appears to be because the empirically learned variances are so small that the choice is

7

published as a conference paper at iclr 2015

figure 4: synthetic experiments on embedding two simple hierarchies in two dimensions directly
using kl divergence. the embedding model captures all of the hierarchical relationships present in
the tree. sibling leaves are pushed into overlapping areas by the objective function.

between either leaving them small, making it very dif   cult to have one gaussian located    inside    an-
other gaussian, or regularizing so much that their discriminative power is washed out. additionally,
when examining the empirical variances, we noted that common words like    such,    which receive
very large variances in our learned model, have much smaller empirical variances relative to rarer
words. a possible explanation is that the contrastive objective forces variances of commonly sam-
pled words to spread out to avoid loss, while the empirical variance sees only    positive examples   
and has no penalty for being close to many contexts at once.
while these results indicate that we can do as well or better at unsupervised entailment than previ-
ous distributional semantic measures, we would like to move beyond purely unsupervised learning.
although certain forms of entailment can be learned in an unsupervised manner from distributional
data, many entailing relationships are not present in the training text in the form of lexical substitu-
tions that re   ect the is-a relationship. for example, one might see phrases such as    look at that bird,   
   look at that eagle,       look at that dog,    but rarely    look at that mammal.    one appealing aspect of
our models versus count-based ones is that they can be directly discriminatively trained to embed
hierarchies.

6.3 directly learning asymmetric relationships

in figure 4, we see the results of directly embedding simple tree hierarchies as gaussians. we
embed nodes as gaussians with diagonal variances in two-dimensional space using id119
on the kl divergence between parents and children. we create a gaussian for each node in the
tree, and randomly initialize means. negative contexts come from randomly sampled nodes that
are neither ancestors nor descendents, while positive contexts come from ancestors or descendents
using the appropriate directional kl divergence. unlike our experiments with symmetric energy, we
must use the same set of embeddings for nodes and contexts, or else the objective function will push
the variances to be unboundedly large. our training process captures the hierarchical relationships,
although leaf-level siblings are not differentiated from each other by this objective function. this
is because out of all the negative examples that a leaf node can receive, only one will push it away
from its sibling node.

6.4 word similarity benchmarks

we evaluate the embeddings on seven different standard word similarity benchmarks (rubenstein
& goodenough, 1965; szumlanski et al., 2013; hill et al., 2014; miller & charles, 1991; bruni
et al., 2014; yang & powers, 2006; finkelstein et al., 2001). a comparison to all of the state of
the art word-embedding numbers for different dimensionalities as in (baroni et al., 2014) is out of
the scope of this evaluation. however, we note that the overall performance of our 50-dimensional
embeddings matches or beats reported numbers on these datasets for the 80-dimensional skip-gram
vectors at wordvectors.org (faruqui & dyer, 2014), as well as our own skip-gram implementation.
note that the numbers are not directly comparable since we use a much older version of wikipedia
(circa 2009) in our wackypedia dataset, but this should not give us an edge.

8

published as a conference paper at iclr 2015

sg (50d)
dataset
29.39
siid113x
59.89
wordsim
wordsim-s
69.86
wordsim-r 53.03
70.27
men
63.96
mc
70.01
rg
yp
39.34
49.14
rel-122

sg (100d) lg/50/m/s lg/50/d/s lg/50/m/d lg/50/d/d
31.13
59.33
70.19
54.64
70.70
66.76
69.38
35.76
51.26

31.25
62.12
74.64
54.44
71.30
67.01
70.41
36.05
52.28

32.23
65.49
76.15
58.96
71.31
70.41
71.00
41.50
53.74

29.84
62.03
73.92
54.37
69.65
69.17
74.76
42.55
51.09

30.50
61.00
72.79
53.36
70.18
68.50
77.00
39.30
53.54

figure 5: similarity: we evaluate our learned gaussian embeddings (lg) with spherical (s) and
diagonal (d) variances, on several word similarity benchmarks, compared against standard skip-
gram (sg) embeddings on the trained on the same dataset. we evaluate gaussian embeddings with
both cosine between means (m), and cosine between the distributions themselves (d) as de   ned by
the expected likelihood inner product.

while it is good to sanity-check that our embedding algorithms can achieve standard measures of
distributional quality, these experiments also let us compare the different types of variances (spher-
ical and diagonal). we also compare against skip-gram embeddings with 100 latent dimensions,
since our diagonal variances have 50 extra parameters.
we see that the embeddings with spherical covariances have an overall slight edge over the embed-
dings with diagonal covariances in this case, in a reversal from the entailment experiments. this
could be due to the diagonal variance matrices making the embeddings more axis-aligned, making
it harder to learn all the similarities and reducing model capacity. to test this theory, we plotted
the absolute values of components of spherical and diagonal variance mean vectors on a q-q plot
and noted a signi   cant off-diagonal shift, indicating that diagonal variance embedding mean vectors
have    spikier    distributions of components, indicating more axis-alignment.
we also see that the distributions with diagonal variances bene   t more from including the variance
in the comparison (d) than the spherical variances. generally, the data sets in which the cosine
between distributions (d) outperforms cosine between means (m) are similar for both spherical and
diagonal covariances. using the cosine between distributions never helped when using empirical
variances, so we do not include those numbers.

7 conclusion and future work

in this work we introduced a method to embed word types into the space of gaussian distribu-
tions, and learn the embeddings directly in that space. this allows us to represent words not as
low-dimensional vectors, but as densities over a latent space, directly representing notions of uncer-
tainty and enabling a richer geometry in the embedded space. we demonstrated the effectiveness of
these embeddings on a linguistic task requiring asymmetric comparisons, as well as standard word
similarity benchmarks, learning of synthetic hierarchies, and several qualitative examinations.
in future work, we hope to move beyond spherical or diagonal covariances and into combinations
of low rank and diagonal matrices. ef   cient updates and scalable learning is still possible due to
the sherman-woodbury-morrison formula. additionally, going beyond diagonal covariances will
enable us to keep our semantics from being axis-aligned, which will increase model capacity and
expressivity. we also hope to move past stochastic id119 and warm starting and be able to
learn the gaussian representations robustly in one pass from scratch by using e.g. proximal or block
coordinate descent methods. improved optimization strategies will also be helpful on the highly
nonconvex problem of training supervised hierarchies with kl divergence.
representing words and concepts as different types of distributions (including other elliptic distri-
butions such as the student   s t) is an exciting direction     gaussians concentrate their density on
a thin spherical ellipsoidal shell, which can lead to counterintuitive behavior in high dimensions.
multimodal distributions represent another clear avenue for future work. combining ideas from

9

published as a conference paper at iclr 2015

kernel methods and manifold learning with deep learning and linguistic representation learning is
an exciting frontier.
in other domains, we want to extend the use of potential function representations to other tasks
requiring embeddings, such as relational learning with the universal schema (riedel et al., 2013).
we hope to leverage the asymmetric measures, probabilistic interpretation, and    exible training
criteria of our model to tackle tasks involving similarity-in-context, comparison of sentences and
paragraphs, and more general common sense reasoning.

8 acknowledgements

this work was supported in part by the center for intelligent information retrieval, in part by iarpa
via doi/nbc contract #d11pc20152, and in part by nsf grant #cns-0958392 the u.s. govern-
ment is authorized to reproduce and distribute reprint for governmental purposes notwithstanding
any copyright annotation thereon. any opinions,    ndings and conclusions or recommendations ex-
pressed in this material are those of the authors and do not necessarily re   ect those of the sponsor.

references
aizerman, m. a., braverman, e. a., and rozonoer, l. theoretical foundations of the potential
function method in pattern recognition learning. in automation and remote control,, number 25
in automation and remote control,, pp. 821   837, 1964.

baroni, marco, bernardini, silvia, ferraresi, adriano, and zanchetta, eros. the wacky wide web:
a collection of very large linguistically processed web-crawled corpora. language resources and
evaluation, 43(3):209   226, 2009.

baroni, marco, bernardi, raffaella, do, ngoc-quynh, and shan, chung-chieh. entailment above
the word level in id65. in proceedings of the 13th conference of the european
chapter of the association for computational linguistics, eacl    12, pp. 23   32, stroudsburg,
pa, usa, 2012. association for computational linguistics.

baroni, marco, dinu, georgiana, and kruszewski, germ  an. don   t count, predict! a systematic
comparison of context-counting vs. context-predicting semantic vectors. in proceedings of the
52nd annual meeting of the association for computational linguistics (volume 1: long papers),
pp. 238   247. association for computational linguistics, 2014.

bengio, yoshua, schwenk, holger, sen  ecal, jean-s  ebastien, morin, fr  ederic, and gauvain, jean-
luc. neural probabilistic language models. in innovations in machine learning, pp. 137   186.
springer, 2006.

bishop, christopher m. mixture density networks, 1994.

brown, gerald g and rutemiller, herbert c. means and variances of stochastic vector products with

applications to random linear models. management science, 24(2):210   216, 1977.

bruni, elia, tran, nam-khanh, and baroni, marco. multimodal id65. jair,

2014.

clinchant, st  ephane and perronnin, florent. textual similarity with a bag-of-embedded-words
model. in proceedings of the 2013 conference on the theory of information retrieval, ictir
   13, pp. 25:117   25:120, new york, ny, usa, 2013a. acm.

clinchant, st  ephane and perronnin, florent. aggregating continuous id27s for infor-

mation retrieval. acl 2013, pp. 100, 2013b.

deerwester, s., dumais, s.t., furnas, g.w., landauer, t.k., and harshman, r.a. indexing by latent
semantic analysis. journal of the american society for information science 41, pp. 391   407,
1990.

duchi, john, hazan, elad, and singer, yoram. adaptive subgradient methods for online learning

and stochastic optimization. the journal of machine learning research, 12:2121   2159, 2011.

10

published as a conference paper at iclr 2015

erk, katrin. representing words as regions in vector space. in proceedings of the thirteenth confer-
ence on computational natural language learning, pp. 57   65. association for computational
linguistics, 2009.

faruqui, manaal and dyer, chris. community evaluation and exchange of word vectors at word-
vectors.org. in proceedings of the 52nd annual meeting of the association for computational
linguistics: system demonstrations, baltimore, usa, june 2014. association for computational
linguistics.

finkelstein, lev, gabrilovich, evgeniy, matias, yossi, rivlin, ehud, solan, zach, wolfman, gadi,
and ruppin, eytan. placing search in context: the concept revisited. in proceedings of the 10th
international conference on world wide web, pp. 406   414. acm, 2001.

geffet, maayan and dagan, ido. the distributional inclusion hypotheses and lexical entailment.
in proceedings of the 43rd annual meeting on association for computational linguistics, pp.
107   114. association for computational linguistics, 2005.

hill, felix, reichart, roi, and korhonen, anna. siid113x-999: evaluating semantic models with

(genuine) similarity estimation. arxiv preprint arxiv:1408.3456, 2014.

jebara, tony, kondor, risi, and howard, andrew. id203 product kernels. the journal of

machine learning research, 5:819   844, 2004.

joachims, thorsten. optimizing search engines using clickthrough data.

in proceedings of the
eighth acm sigkdd international conference on knowledge discovery and data mining, pp.
133   142. acm, 2002.

kiros, ryan, zemel, richard, and salakhutdinov, ruslan r. a multiplicative model for learning

distributed text-based attribute representations. in nips, 2014.

kiyoshiyo, shimaoka, masayasu, muraoka, futo, yamamoto, watanabe, yotaro, okazaki, naoaki,
and inui, kentaro. distribution representation of the meaning of words and phrases by a gaussian
distribution. in language processing society 20th annual conference (in japanese), 2014.

koren, yehuda, bell, robert, and volinsky, chris. id105 techniques for recommender

systems. computer, 42(8):30   37, august 2009. issn 0018-9162.

lanckriet, gert rg, cristianini, nello, bartlett, peter, ghaoui, laurent el, and jordan, michael i.
learning the kernel matrix with semide   nite programming. the journal of machine learning
research, 5:27   72, 2004.

lecun, yann, chopra, sumit, and hadsell, raia. a tutorial on energy-based learning. 2006.

levy, omer and goldberg, yoav. neural id27 as implicit id105.

advances in neural information processing systems, pp. 2177   2185, 2014.

in

mikolov, tomas, sutskever, ilya, chen, kai, corrado, greg s, and dean, jeff. distributed repre-
sentations of words and phrases and their compositionality. in advances in neural information
processing systems, pp. 3111   3119, 2013.

miller, george a and charles, walter g. contextual correlates of semantic similarity. language

and cognitive processes, 6(1):1   28, 1991.

mnih, andriy and hinton, geoffrey e. a scalable hierarchical distributed language model.

advances in neural information processing systems, pp. 1081   1088, 2008.

in

mnih, andriy and salakhutdinov, ruslan. probabilistic id105. in advances in neural

information processing systems, pp. 1257   1264, 2007.

petersen, kaare brandt. the matrix cookbook. 2006.

riedel, sebastian, yao, limin, mccallum, andrew, and marlin, benjamin m. id36

with id105 and universal schemas. 2013.

11

published as a conference paper at iclr 2015

rubenstein, herbert and goodenough, john b. contextual correlates of synonymy. commun. acm,

8(10):627   633, october 1965. issn 0001-0782.

rumelhart, d.e., hintont, g.e., and williams, r.j. learning representations by back-propagating

errors. nature, 323(6088):533   536, 1986.

salakhutdinov, ruslan and mnih, andriy. bayesian probabilistic id105 using markov
chain monte carlo. in proceedings of the 25th international conference on machine learning, pp.
880   887. acm, 2008.

szumlanski, sean r, gomez, fernando, and sims, valerie k. a new set of norms for semantic

relatedness measures. in acl, 2013.

weston, jason, bengio, samy, and usunier, nicolas. wsabie: scaling up to large vocabulary image

annotation. in ijcai, volume 11, pp. 2764   2770, 2011.

xing, eric p, jordan, michael i, russell, stuart, and ng, andrew y. distance metric learning with
in advances in neural information processing

application to id91 with side-information.
systems, pp. 505   512, 2002.

yang, dongqiang and powers, david m. w. verb similarity on the taxonomy of id138. in in the

3rd international id138 conference (gwc-06), jeju island, korea, 2006.

12

