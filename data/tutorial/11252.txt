comparing fifty natural languages and twelve genetic languages using
id27 language divergence (weld) as a quantitative measure

of language distance

ehsaneddin asgari and mohammad r.k. mofrad

departments of bioengineering
university of california, berkeley

berkeley, ca 94720, usa

6
1
0
2

 
r
p
a
8
2

 

 
 
]
l
c
.
s
c
[
 
 

1
v
1
6
5
8
0

.

4
0
6
1
:
v
i
x
r
a

asgari@ischool.berkeley.edu, mofrad@berkeley.edu

abstract

we introduce a new measure of distance be-
tween languages based on id27,
called id27 language divergence
(weld). weld is de   ned as divergence be-
tween uni   ed similarity distribution of words
between languages. using such a measure,
we perform language comparison for    fty nat-
ural languages and twelve genetic languages.
our natural language dataset is a collection of
sentence-aligned parallel corpora from bible
translations for    fty languages spanning a va-
riety of language families. although we use
parallel corpora, which guarantees having the
same content in all languages, interestingly in
many cases languages within the same fam-
ily cluster together.
in addition to natural
languages, we perform language comparison
for the coding regions in the genomes of 12
different organisms (4 plants, 6 animals, and
two human subjects). our result con   rms
a signi   cant high-level difference in the ge-
netic language model of humans/animals ver-
sus plants. the proposed method is a step to-
ward de   ning a quantitative measure of simi-
larity between languages, with applications in
languages classi   cation, genre identi   cation,
dialect identi   cation, and evaluation of trans-
lations.

introduction

1
classi   cation of language varieties is one of the
prominent problems in linguistics (smith, 2016).
the term language variety can refer to different
styles, dialects, or even a distinct language (mar-
jorie and rees-miller, 2001).
it has been a long-
standing argument that strictly quantitative methods

can be applied to determine the degree of similar-
ity or dissimilarity between languages (kroeber and
chr  etien, 1937; sankaran et al., 1950; kr  amsk`y,
1959; mcmahon and mcmahon, 2003). the meth-
ods proposed in the 1990   s and early 2000    mostly
relied on utilization of intensive linguistic resources.
for instance, similarity between two languages was
de   ned based on the number of common cognates
or phonological patterns according to a manually
extracted list (kroeber and chr  etien, 1937; mcma-
hon and mcmahon, 2003). such an approach, of
course, is not easily extensible to problems involv-
ing new languages. recently, statistical methods
have been proposed to automatically detect cognates
(berg-kirkpatrick and klein, 2010; hall and klein,
2010; bouchard-c  ot  e et al., 2013; ciobanu and
dinu, 2014) and subsequently compare languages
based on the number of common cognates (ciobanu
and dinu, 2014).

in this paper our aim is to de   ne a quantitative
measure of distance between languages. such a met-
ric should reasonably take both syntactic and seman-
tic variability of languages into account. a measure
of distance between languages can have various ap-
plications including quantitative genetic/typological
language classi   cation, styles and genres identi   ca-
tion, and translation evaluation. in addition, compar-
ing the biological languages generating the genome
in different organisms can potentially shed light on
important biological facts.
1.1 problem de   nition
our goal is to be able to provide a quantitative es-
timate of distance for any two given languages. in
our framework, we de   ne a language as a weighted

graph    l(v, e), where v is a set of vertices (words),
and e : (v    v )     (cid:60) is a weight function map-
ping a pair of words to their similarity value. then
our goal of approximating the distance between the
two languages l and l(cid:48) can be transferred to the ap-
proximation of the distance between    l(v, e) and
   l(cid:48)(v (cid:48), e(cid:48)).
in order to approach such a problem
   rstly we need to address the following questions:
    what is a proper weight function e estimating a
similarity measure between words wi, wj     v
in a language l?
    how can we relate words in v to words in v (cid:48)?
    and    nally, how can we measure a distance
between languages    l and    l(cid:48), which means
d(   l,    l(cid:48))?

in the following section we explain how re-
searchers have addressed the above mentioned ques-
tions until now.
1.1.1 word similarity within a language

the main aim of word similarity methods is to
measure how similar pairs of words are to each-
other, semantically and syntactically (han et al.,
2013). such a problem has a wide range of appli-
cations in information retrieval, automatic speech
recognition, id51, and ma-
chine translation (collobert and weston, 2008; glo-
rot et al., 2011; mikolov et al., 2013c; turney et al.,
2010; resnik, 1999; schwenk, 2007).

various methods have been proposed to measure
word similarity, including thesaurus and taxonomy-
based approaches, data-driven methods, and hybrid
techniques (miller, 1995; mohammad and hirst,
2006; mikolov et al., 2013a; han et al., 2013).
taxonomy-based methods are not easily extensible
as they usually require extensive human interven-
tion for creation and maintenance (han et al., 2013).
one of the main advantages of data-driven methods
is that they can be employed even for domains with
shortage of manually annotated data.

almost all of the data-driven methods such as ma-
trix factorization (xu et al., 2003), word embed-
ding (mikolov et al., 2013a), topic models (blei,
2012), and mutual information (han et al., 2013)
are based on co-occurrences of words within de-
   ned units of text data. each method has its own
convention for unit of text, which can be a sen-
tence, paragraph or a sliding window around a word.

using distributed representations have been one of
the most successful approaches for computing word
similarity in natural language processing (collobert
et al., 2011). the main idea in distributed represen-
tation is characterizing words by the company they
keep (hinton, 1984; firth, 1975; collobert et al.,
2011).

recently,

continuous vector

representations
known as word vectors have become popular in
language processing (nlp) as an ef   -
natural
cient approach to represent
semantic/syntactic
units (mikolov et al., 2013a; collobert et al., 2011).
word vectors are trained in the course of training a
language model neural network from large amounts
of textual data (words and their contexts) (mikolov
et al., 2013a). more precisely, word representa-
tions are the outputs of the last hidden layer in
a trained neural network for id38.
thus, word vectors are supposed to encode the
most relevant features to id38 by
observing various samples.
in this representation
similar words have closer vectors, where similarity
is de   ned in terms of both syntax and semantics.
by training word vectors over large corpora of
natural languages,
interesting patterns have been
observed. words with similar vector representations
display multiple types of similarity. for instance,
                  
            
king                 
w oman is the closest vector to
               
m an +
queen (an instance of semantic
that of the word
            
quick                      
regularities) and
slowly
(an instance of syntactic regularities). a recent
work has proposed the use of word vectors to detect
linguistic changes within the same language over
time (kulkarni et al., 2015). the fact that various
degrees of similarity were captured by such a
representation convinced us to use it as a notion of
proximity for words.
1.1.2 word alignment

quickly              

slow                   

as we discussed in section 1.1, in order to com-
pare graphs    l and    (cid:48)
l, we need to have a uni-
   ed de   nition of words (vertices). thus, we need to
   nd a mapping function from the words in v to the
words in v (cid:48). obviously when two languages have
the same vocabulary set this step can be skipped,
which is the case when we perform within-language
genres analysis or linguistic drifts study (stamatatos
et al., 2000; kulkarni et al., 2015), or even when

we compare biological languages (dna or protein
languages) for different species (asgari and mofrad,
2015). however, when our goal is to compare dis-
tributional similarity of words for two different lan-
guages, such as french and german, we need to    nd
a mapping from words in french to german words.
finding a word mapping function between two
languages can be achieved using a dictionary or
using statistical word alignment
in parallel cor-
pora (och and ney, 2003; lardilleux and lep-
age, 2009).
statistical word alignment is a vi-
tal component in any statistical machine transla-
tion pipeline (fraser and marcu, 2007). various
methods/tools has been proposed for word align-
ment, such as giza++ (och, 2003) and any-
malign (lardilleux and lepage, 2009), which are
able to extract high quality word alignments from
sentence-aligned multilingual parallel corpora.

one of the data resources we use in this project
is a large collection of sentence-aligned parallel cor-
pora we extract from bible translations in    fty lan-
guages. thus, in order to    nd a word mapping func-
tion among all these languages we used statistical
word alignment techniques and in particular any-
malign (lardilleux and lepage, 2009), which can
process any number of languages at once.

1.1.3 network analysis of languages

the rather intuitive approach of treating lan-
guages as networks of words has been proposed
and explored in the last decade by a number of re-
searchers (i cancho and sol  e, 2001; liu and cong,
2013; cong and liu, 2014; gao et al., 2014).
in
like many other
these works, human languages,
aspects of human behavior, are modeled as com-
plex networks (costa et al., 2011), where the nodes
are essentially the words of the language and the
weights on the edges are calculated based on the
co-occurrences of the words (liu and cong, 2013;
i cancho and sol  e, 2001; gao et al., 2014). clus-
tering of 14 languages based on various parameters
of a complex network such as average degree, aver-
age path length, id91 coef   cient, network cen-
tralization, diameter, and network heterogeneity has
been done by (liu and cong, 2013). a similar ap-
proach is suggested by (gao et al., 2014) for anal-
ysis of the complexity of six languages. although,
all of the above mentioned methods have presented

promising results about similarity and regularity of
languages, to our understanding they need the fol-
lowing improvements:

measure of word similarity: considering co-
occurrences as a measure of similarity between
nodes, which is the basis of the above mentioned
complex network methods, is a naive estimate of
similarity, (liu and cong, 2013; i cancho and sol  e,
2001; gao et al., 2014). the most trivial cases are
synonyms, which we expect to be marked as the
most similar words to each other. however, since
they can only be used interchangeably with each
other in the same sentences, their co-occurrences
rate is very low. thus, raw co-occurrence is not nec-
essarily a good indicator of similarity.

independent vs. joint analysis: previous meth-
ods have compared the parameters of language
graphs independently, except for some relatively
small networks of words for illustration (liu and
cong, 2013; i cancho and sol  e, 2001; gao et al.,
2014). however, two languages may have similar
settings of the edges but for completely different
concepts. thus, a systematic way for joint compari-
son of these networks is essential.

language collection: the previous analysis was
performed on a relatively small number of lan-
guages. for instance in (liu and cong, 2013), four-
teen languages were studied where twelve of them
were from the slavic family of languages, and (gao
et al., 2014) studied six languages. clearly, study-
ing more languages from a broader set of language
families would be more indicative.
1.2 our contributions
in this paper, we suggest a heuristic method toward a
quantitative measure of distance between languages.
we propose divergence between uni   ed similarity
distribution of words as a quantitative measure of
distance between languages.

measure of word similarity: we use cosine
similarity between word vectors as the metric of
word similarities, which has been shown to take
into account both syntactic and semantic similari-
ties (mikolov et al., 2013a). thus, in the weighted
language graph    l(v, e), the weight function e :
(v   v )     (cid:60) is de   ned by word-vector cosine simi-
larities between pairs of words. although word vec-
tors are calculated based on co-occurrences of words

within sliding windows, they are capable of attribut-
ing a reasonable degree of similarity to close words
that do not co-occur.

joint analysis of language graphs: by having
word vector proximity as a measure of word similar-
ity, we can represent each language as a joint sim-
ilarity distribution of its words. unlike the meth-
ods mentioned in section 1.1.3 which focused on
network properties and did not consider a mapping
function between nodes across various languages,
we propose performing node alignment between
different languages (lardilleux and lepage, 2009).
consequently, calculation of jensen-shannon diver-
gence between uni   ed similarity distributions of the
languages can provide us with a measure of distance
between languages.

language collection: in this study we perform
language comparison for    fty natural languages and
twelve genetic language.

natural languages: we extracted a collection of
sentence-aligned parallel corpora from bible trans-
lations for    fty languages spanning a variety of lan-
guage families including indo-european (germanic,
italic, slavic, indo-iranian), austronesian, sino-
tibetan, altaic, uralic, afro-asiatic, etc. this set
of languages is relatively large and diverse in com-
parison with the corpora that have been used in pre-
vious studies (liu and cong, 2013; gao et al., 2014).
we calculated the jensen-shannon divergence be-
tween joint similarity distributions for    fty language
graphs consisting of 4,097 sets of aligned words in
all these    fty languages. using the mentioned diver-
gence we performed cluster analysis of languages.
interestingly in many cases languages within the
same family clustered together.
in some cases, a
lower degree of divergence from the source language
despite belonging to different language families was
indicative of a consistent translation.

genetic languages: nature uses certain lan-
guages to generate biological sequences such as
dna, rna, and proteins. biological organisms
use sophisticated languages to convey information
within and between cells, much like humans adopt
languages to communicate (yandell and majoros,
2002; searls, 2002).
inspired by this conceptual
analogy, we use our languages comparison method
for comparison of genetic languages in different
organisms. genome refers to a sequence of nu-

cleotides containing our genetic information. some
parts of our genome are coded in a way that can
be translated to proteins (exonic regions), while
some regions cannot be translated into proteins (in-
trons) (saxonov et al., 2000). in this study, we per-
form language comparison of coding regions in 12
different species (4 plants, 6 animals, and two hu-
man subjects). our language comparison method is
able to assign a reasonable relative distance between
species.
2 methods
as we discussed in 1.1, we transfer the problem of
   nding a measure of distance between languages l
and l(cid:48) to    nding the distance between their language
graphs    l(v, e) and    l(cid:48)(v (cid:48), e(cid:48)).
id27: we de   ne the edge weight
function e : (v   v )     (cid:60) to be the cosine similarity
between word vectors.

alignment: when two languages have different
words, in order to    nd a mapping between the words
in v and v (cid:48) we can perform statistical word align-
ment on parallel corpora.

divergence calculation: calculating jensen-
shannon divergence between joint similarity distri-
butions of the languages can provide us with a notion
of distance between languages.

our language comparison method has three com-
ponents. firstly, we need to learn word vectors from
large amounts of data in an unsupervised manner
for both of the languages we are going to compare.
secondly, we need to    nd a mapping function for
the words and    nally we need to calculate the diver-
gence between languages. in the following section
we explain each step aligned with the experiment we
perform on both natural languages and genetic lan-
guages.
2.1 learning id27
id27 can be trained in various frame-
works (e.g. non-negative id105 and
neural network methods (mikolov et al., 2013c;
levy and goldberg, 2014)). neural network word
embedding trained in the course of language mod-
eling is shown to capture interesting syntactic and
semantic regularities in the data (mikolov et al.,
2013c; mikolov et al., 2013a). such word embed-
ding known as word vectors need to be trained from
a large number of training examples, which are ba-

sically words and their corresponding contexts. in
this project, in particular we use an implementa-
tion of the skip-gram neural network (mikolov et al.,
2013b).

in training word vector representations, the skip-
gram neural network attempts to maximize the av-
erage id203 of contexts for given words in the
training data:

n(cid:88)

i=1

(cid:88)
(cid:80)w

   c   j   c,j(cid:54)=0
exp (v(cid:48)t
k=1 exp (v(cid:48)t

wi+j vwi)

vwi)

wk

argmax

v,v(cid:48)

1
n

p(wi+j|wi) =

log p(wi+j|wi)

(1)

,

where n is the length of the training, 2c is the
window size we consider as the context, wi is the
center of the window, w is the number of words in
the dictionary and vw and v(cid:48)
w are the n-dimensional
word representation and context representation of
word w, respectively. at the end of the training the
average of vw and v(cid:48)
w will be considered as the word
vector for w. the id203 p(wi+j|wi) is de   ned
using a softmax function.
in the implementation
we use (id97) (mikolov et al., 2013b) nega-
tive sampling has been utilized, which is considered
as the state-of-the-art for training word vector repre-
sentation.
2.1.1 natural languages data

for the purpose of language classi   cation we
need parallel corpora that are translated into a
large number of languages, so that we can    nd
the alignments using statistical methods. recently,
a massive parallel corpus based on 100 transla-
tions of the bible has been created in xml for-
mat (christodouloupoulos and steedman, 2015),
which we choose as the database for this project.
in order to make sure that we have a large enough
corpus for learning word vectors, we pick the lan-
guages for which translations of both the old tes-
tament and the new testament are available. from
among those languages we pick the ones contain-
ing all the verses in the hebrew version (which is
the source language for most of the data) and    -
nally we end up with almost 50 languages, con-
taining 24,785 aligned verses. for thai, japanese,
and chinese we use the tokenized versions in

the database (christodouloupoulos and steedman,
2015).
in addition, before feeding the skip-gram
neural network we remove all punctuation.

in our experiment, we use the id97 imple-
mentation of skip-gram (mikolov et al., 2013b). we
set the dimension of word vectors d to 100, and the
window size c to 10 and we sub-sample the frequent
words by the ratio 1
103 .

2.1.2 genetic languages data

in order to compare the various genetic languages
we use the intronexon database that contains coding
and non-coding regions of genomes for a number
of organisms (shepelev and fedorov, 2006). from
this database we extract a data-set of coding regions
(cr) from 12 organisms consisting of 4 plants (ara-
bidopsis, populus, moss, and rice), 6 animals (sea-
urchin, chicken, cow, dog, mouse, and rat), and two
human subjects. the number of coding regions we
have in the training data for each organism is sum-
marized in table 1. the next step is splitting each
sequence to a number of words. since the genome
is composed of the four dna nucleotides a,t,g and
c, if we split the sequences in the character level
the language network would be very small. we thus
split each sequence into id165s (n = 3, 4, 5, 6),
which is a common range of id165s in bioinfor-
matics(ganapathiraju et al., 2002; mantegna et al.,
1995). as suggested by(asgari and mofrad, 2015)
we split the sequence into non-overlapping id165s,
but we consider all possible ways of splitting for
each sequence.

organisms
arabidopsis
populus
moss
rice
sea-urchin
chicken
cow
dog
mouse
rat
human 1
human 2

# of cr # of 3-grams
179824
131844
167999
129726
143457
187761
196466
381147
215274
190989
319391
303872

42,618,288
28,478,304
38,471,771
34,507,116
27,974,115
34,735,785
43,222,520
70,512,195
34,874,388
41,635,602
86,874,352
77,791,232

table 1: the genome data-set for learning word vectors in
different organisms. the number of coding regions and the total
occurrences of 3-grams are presented. clearly, the total number
of all id165s (n=3,4,5,6) is almost the same.

we train the word vectors for each setting of n-
grams and organisms separately, again using skip-

gram neural network implementation (mikolov et
al., 2013b). we set the dimension of word vectors d
to 100, and window size of c to 40. in addition, we
sub-sample the frequent words by the ratio 10   3.
2.2 word alignment
the next step is to    nd a mapping between the nodes
in    l(v, e) and    l(cid:48)(v (cid:48), e(cid:48)). obviously in case of
quantitative comparison of styles within the same
language we do not need to    nd an alignment be-
tween the nodes in v and v (cid:48). however, when we are
comparing two distinct languages we need to    nd a
mapping from the words in language l to the words
in language l(cid:48).
2.2.1 word alignment for natural languages

as we mentioned in section 2.1.1, our parallel
corpora contain texts in    fty languages from a va-
riety of language families. we decided to use statis-
tical word alignments because we already have par-
allel corpora for these languages and therefore per-
forming statistical alignment is straightforward. in
addition, using statistical alignment we hope to see
evidences of consistent/inconsistent translations.

an

we

use

implementation

of anyma-
lign (lardilleux and lepage, 2009), which is
designed to extract high quality word alignments
from sentence-aligned multilingual parallel corpora.
although anymalign is capable of performing
alignments in several languages at the same time,
our empirical observation was that performing
alignments for all
languages against a single
language and then    nding the global alignment
through that alignment
is faster and results in
better alignments. we thus align all translations
with the hebrew version. to ensure the quality
of alignments we apply a high threshold on the
score of alignments. in a    nal step, we combine the
results and end up with a set of 4,097 multilingual
alignments. hence we have a mapping from any
of the 4,097 words in one language to one in any
other given language, where the hebrew words are
unique, but not necessarily the others.
2.2.2 genetic languages alignment

in genetic language comparison, since the n-
grams are generated from the same nucleotides
(a,t,c,g), no alignment is needed and v would be
the same as v (cid:48).

2.3 calculation of language divergence
in section 2.1 we explained how to make language
graphs    l(v, e) and    l(cid:48)(v (cid:48), e(cid:48)). then in sec-
tion 2.2 we proposed a statistical alignment method
to    nd the mapping function between the nodes in
v and v (cid:48). having achieved the mapping between
the words in v and the words in v (cid:48), the next step is
comparison of e and e(cid:48).

in comparing language graphs what is more cru-
cial is the relative similarities of words. intuitively
we know that the relative similarities of words vary
in different languages due to syntactic and seman-
tic differences. hence, we decided to use the di-
vergence between relative similarities of words as a
heuristic measure of the distance between two lan-
guages. to do so,    rstly we normalize the rela-
tive word vector similarities within each language.
then, knowing the mapping between words in v
and v (cid:48) we unify the coordinates of the normalized
similarity distributions. finally, we calculate the
jensen-shannon divergence between the normalized
and uni   ed similarity distributions of two languages:

dl,l(cid:48) = jsd(  e,   e(cid:48)),

where   e and   e(cid:48) are normalized and uni   ed simi-
larity distributions of word pairs in    l(v, e) and
   l(cid:48)(v (cid:48), e(cid:48)) respectively.
2.3.1 natural languages graphs

for the purpose of language classi   cation we
need to    nd pairwise distances between all of the
   fty languages we have in our corpora. using the
mapping function obtained from statistical align-
ments of bible translations, we produce the nor-
malized and uni   ed similarity distributions of word
pairs   e(k) for language l(k). therefore to compute
the quantitative distance between two languages l(i)
and l(j) we calculate dli,lj = jsd(   e(i),   e(j)).

consequently, we calculate a quantitative distance
between each pair of languages. in a    nal step, for
visualization purposes, we perform unweighted pair
group method with arithmetic mean (upgma) hi-
erarchical id91 on the pairwise distance matrix
of languages (johnson, 1967).

2.3.2 genetic languages graphs

the same approach as carried out for natural lan-
guages is applied to genetic languages corpora. pair-

wise distances of genetic languages were calculated
using jensen-shannon divergence between normal-
ized and uni   ed similarity distributions of word
pairs for each pair of languages.

we calculate the pairwise distance matrix of lan-
guages for each id165 separately to verify which
length of dna segment is more discriminative be-
tween different species.
3 results
3.1 classi   cation of natural languages
the result of the upgma hierarchical id91
of languages is shown in figure 1. as shown in
this    gure, many languages are clustered together
according to their family and sub-family. many
indo-european languages (shown in green) and aus-
tronesian languages (shown in pink) are within a
close proximity. even the proximity between lan-
guages within a sub-family are preserved with our
measure of language distance. for instance, roma-
nian, spanish, french, italian, and portuguese, all
of which belong to the italic sub-family of indo-
european languages, are in the same cluster. simi-
larly, the austronesian langauges cebuano, tagalog,
and maori as well as malagasy and indonesian are
grouped together.

although the id91 based on word em-
bedding language divergence matches the ge-
netic/typological classi   cation of languages in many
cases, for some pairs of languages their distance in
the id91 does not make any genetic or topo-
logical sense. for instance, we expected arabic
and somali as afro-asiatic languages to be within
a close proximity with hebrew. however, he-
brew is matched with norwegian, a germanic indo-
european language. after further investigations and
comparing word neighbors for several cases in these
languages,
it turns out that the norwegian bible
translation highly matches hebrew because of be-
ing a consistent and high-quality translation. in this
translation, synonym were not used interchangeably
and language usage stays more faithful to the struc-
ture of the hebrew text.
3.1.1 divergence between genetic languages

the pairwise distance matrix of the twelve ge-
netic languages for id165s (n = 3, 4, 5, 6) is shown
in figure 2. our results con   rm that evolutionar-
ily closer species have a reasonably higher level of

proximity in their language models. we can ob-
serve in figure 2, that as we increase the number
of id165s the distinction between animal/human
genome and plant genome increases.
4 conclusion
in this paper, we proposed id27 lan-
guage divergence (weld) as a new heuristic mea-
sure of distance between languages. consequently
we performed language comparison for    fty natural
languages and twelve genetic languages. our nat-
ural language dataset was a collection of sentence-
aligned parallel corpora from bible translations for
   fty languages spanning a variety of language fami-
lies. we calculated our id27 language
divergence for 4,097 sets of aligned words in all
these    fty languages. using the mentioned diver-
gence we performed cluster analysis of languages.

the corpora for all of the languages but one con-
sisted of translated text instead of original text in
those languages. this means many of the poten-
tial relations between words such as collocations
and culturally in   uenced semantic connotations did
not have the full chance to contribute to the mea-
sured language distances. this can potentially make
it harder for the algorithm to detect related lan-
guages. in spite of this, however in many cases lan-
guages within the same family/sub-family clustered
together.
in some cases, a lower degree of diver-
gence from the source language despite belonging to
different language families was indicative of a con-
sistent translation. this suggests that this method
can be a step toward de   ning a quantitative measure
of similarity between languages, with applications
in languages classi   cation, genres identi   cation, di-
alect identi   cation, and evaluation of translations.

in addition to the natural language data-set, we
performed language comparison of id165s in cod-
ing regions of the genome in 12 different species (4
plants, 6 animals, and two human subjects). our
language comparison method con   rmed that evolu-
tionarily closer species are closer in terms of genetic
language models. interestingly, as we increase the
number of id165s the distinction between genetic
language in animals/human versus plants increases.
this can be regarded as indicative of a high-level di-
versity between the genetic languages in plants ver-
sus animals.

figure 1: hierarchical id91 of    fty natural languages according to divergence of joint distance distribution of 4097 aligned
words in bible parallel corpora. subsequently we use colors to show the ground-truth about family of languages. for indo-european
languages we use different symbols to distinguish various sub-families of indo-european languages. we observe that the obtained
id91 reasonably discriminates between various families and subfamilies.

figure 2: visualization of id27 language divergence in twelve different genomes belonging to 12 organisms for
various id165 segments. our results indicate that evolutionarily closer species have higher proximity in the syntax and semantics
of their genomes.

indo-european germanicindo-european italicindo-european slavicindo-european indo-iranianindo-europeanaustronesiansino-tibetanaltaicuralicafro-asiaticothersothersacknowledgments

fruitful discussions with david bamman, meshkat
ahmadi, and mohsen mahdavi are gratefully ac-
knowledged.

references
[asgari and mofrad2015] ehsaneddin asgari and mo-
hammad rk mofrad. 2015. continuous distributed
representation of biological sequences for deep pro-
teomics and genomics. plos one, 10(11):e0141287.

[berg-kirkpatrick and klein2010] taylor
2010.

berg-
phylogenetic
kirkpatrick and dan klein.
the 48th
grammar induction.
annual meeting of
the association for computa-
tional linguistics, pages 1288   1297. association for
computational linguistics.

in proceedings of

[blei2012] david m blei. 2012. probabilistic topic mod-

els. communications of the acm, 55(4):77   84.

[bouchard-c  ot  e et al.2013] alexandre bouchard-c  ot  e,
david hall, thomas l grif   ths, and dan klein.
2013.
lan-
guages using probabilistic models of sound change.
proceedings of the national academy of sciences,
110(11):4224   4229.

automated reconstruction of ancient

[christodouloupoulos and steedman2015] christos

christodouloupoulos and mark steedman. 2015. a
massively parallel corpus: the bible in 100 languages.
language resources and evaluation, 49(2):375   395.

[ciobanu and dinu2014] alina maria ciobanu

and
liviu p. dinu. 2014. an etymological approach to
cross-language orthographic similarity. application on
romanian. in proceedings of the 2014 conference on
empirical methods in natural language processing
(emnlp), pages 1047   1058, doha, qatar, october.
association for computational linguistics.

[collobert and weston2008] ronan collobert and jason
weston. 2008. a uni   ed architecture for natural lan-
guage processing: deep neural networks with mul-
in proceedings of the 25th interna-
titask learning.
tional conference on machine learning, pages 160   
167. acm.

[collobert et al.2011] ronan collobert, jason weston,
l  eon bottou, michael karlen, koray kavukcuoglu,
and pavel kuksa. 2011. natural language process-
ing (almost) from scratch. the journal of machine
learning research, 12:2493   2537.

[cong and liu2014] jin cong and haitao liu. 2014. ap-
proaching human language with complex networks.
physics of life reviews, 11(4):598   618.

[costa et al.2011] luciano da fontoura costa, osvaldo n
oliveira jr, gonzalo travieso, francisco aparecido

rodrigues, paulino ribeiro villas boas, lucas an-
tiqueira, matheus palhares viana, and luis enrique
correa rocha. 2011. analyzing and modeling real-
world phenomena with complex networks: a survey of
applications. advances in physics, 60(3):329   412.

[firth1975] john rupert firth. 1975. modes of meaning.

college division of bobbs-merrill company.

[fraser and marcu2007] alexander fraser and daniel
marcu. 2007. measuring word alignment quality for
id151. computational lin-
guistics, 33(3):293   303.

[ganapathiraju et al.2002] madhavi ganapathiraju, deb-
orah weisser, roni rosenfeld, jaime carbonell, raj
reddy, and judith klein-seetharaman. 2002. com-
parative id165 analysis of whole-genome protein
in proceedings of the second interna-
sequences.
tional conference on human language technology
research, pages 76   81. morgan kaufmann publishers
inc.

[gao et al.2014] yuyang gao, wei liang, yuming shi,
comparison of di-
and qiuling huang.
rected and weighted co-occurrence networks of six
languages. physica a: statistical mechanics and its
applications, 393:579   589.

2014.

[glorot et al.2011] xavier glorot, antoine bordes, and
yoshua bengio. 2011. id20 for large-
scale sentiment classi   cation: a deep learning ap-
proach. in proceedings of the 28th international con-
ference on machine learning (icml-11), pages 513   
520.

[hall and klein2010] david hall and dan klein. 2010.
in pro-
finding cognate groups using phylogenies.
ceedings of the 48th annual meeting of the associa-
tion for computational linguistics, pages 1030   1039.
association for computational linguistics.

[han et al.2013] lushan han, tim finin, paul mcnamee,
akanksha joshi, and yelena yesha. 2013. improving
word similarity by augmenting pmi with estimates of
word polysemy. knowledge and data engineering,
ieee transactions on, 25(6):1307   1322.

[hinton1984] geoffrey e hinton.

1984. distributed
computer science department,

representations.
carnegie mellon university.

[i cancho and sol  e2001] ramon ferrer

i cancho and
richard v sol  e. 2001. the small world of human
language. proceedings of the royal society of london
b: biological sciences, 268(1482):2261   2265.

[johnson1967] stephen c johnson. 1967. hierarchical

id91 schemes. psychometrika, 32(3):241   254.

[kr  amsk`y1959] ji  ri kr  amsk`y. 1959. a quantitative ty-
pology of languages. language and speech, 2(2):72   
85.

[kroeber and chr  etien1937] alfred l kroeber

c douglas chr  etien.
   cation of indo-european languages.
13(2):83   103.

and
1937. quantitative classi-
language,

[kulkarni et al.2015] vivek kulkarni, rami al-rfou,
bryan perozzi, and steven skiena. 2015. statistically
signi   cant detection of linguistic change. in proceed-
ings of the 24th international conference on world
wide web, pages 625   635. international world wide
web conferences steering committee.

[lardilleux and lepage2009] adrien lardilleux and yves
lepage. 2009. sampling-based multilingual align-
ment. in recent advances in natural language pro-
cessing, pages 214   218.

[levy and goldberg2014] omer levy and yoav gold-
berg. 2014. neural id27 as implicit ma-
trix factorization. in advances in neural information
processing systems, pages 2177   2185.

[liu and cong2013] haitao liu and jin cong.

2013.
language id91 with word co-occurrence net-
works based on parallel texts. chinese science bul-
letin, 58(10):1139   1144.

[mantegna et al.1995] rn mantegna,

sv buldyrev,
al goldberger, s havlin, c-k peng, m simons, and
he stanley. 1995. systematic analysis of coding and
noncoding dna sequences using methods of statistical
linguistics. physical review e, 52(3):2939.

[marjorie and rees-miller2001] m marjorie and janie
rees-miller. 2001. language in social contexts. con-
temporary linguistics, pages 537   590.

[mcmahon and mcmahon2003] april mcmahon

and
robert mcmahon. 2003. finding families: quantita-
tive methods in language classi   cation. transactions
of the philological society, 101(1):7   55.

[mikolov et al.2013a] tomas mikolov, kai chen, greg
corrado, and jeffrey dean. 2013a. ef   cient estima-
tion of word representations in vector space. arxiv
preprint arxiv:1301.3781.

[mikolov et al.2013b] tomas mikolov,

ilya sutskever,
kai chen, greg s corrado, and jeff dean. 2013b.
distributed representations of words and phrases and
in advances in neural infor-
their compositionality.
mation processing systems, pages 3111   3119.

[mikolov et al.2013c] tomas mikolov, wen-tau yih, and
geoffrey zweig. 2013c. linguistic regularities in con-
tinuous space word representations. in hlt-naacl,
pages 746   751.

[miller1995] george a miller. 1995. id138: a lexical
database for english. communications of the acm,
38(11):39   41.

[mohammad and hirst2006] saif mohammad

graeme hirst.
concept-distance: a task-oriented evaluation.

and
2006. distributional measures of
in

proceedings of the 2006 conference on empirical
methods in natural language processing, pages
35   43. association for computational linguistics.

[och and ney2003] franz josef och and hermann ney.
2003. a systematic comparison of various statis-
tical alignment models. computational linguistics,
29(1):19   51.

[och2003] fj och. 2003. giza++ software.
[resnik1999] philip resnik. 1999. semantic similarity
in a taxonomy: an information-based measure and its
application to problems of ambiguity in natural lan-
guage. j. artif. intell. res.(jair), 11:95   130.

[sankaran et al.1950] cr sankaran, ad taskar, and
pc ganeshsundaram. 1950. quantitative classi   ca-
tion of languages. bulletin of the deccan college re-
search institute, pages 85   111.

[saxonov et al.2000] serge saxonov,

iraj daizadeh,
alexei fedorov, and walter gilbert.
2000. eid:
the exon   intron databasean exhaustive database of
protein-coding intron-containing genes. nucleic acids
research, 28(1):185   190.

[schwenk2007] holger schwenk.

2007. continuous
space language models. computer speech & lan-
guage, 21(3):492   518.

[searls2002] david b searls. 2002. the language of

genes. nature, 420(6912):211   217.

[shepelev and fedorov2006] valery shepelev and alexei
fedorov. 2006. advances in the exon   intron database
(eid). brie   ngs in bioinformatics, 7(2):178   185.

[smith2016] andrew dm smith. 2016. dynamic models

of language evolution: the linguistic perspective.

[stamatatos et al.2000] efstathios

stamatatos, nikos
fakotakis, and george kokkinakis.
text
genre detection using common word frequencies. in
proceedings of the 18th conference on computational
linguistics-volume 2, pages 808   814. association for
computational linguistics.

2000.

[turney et al.2010] peter d turney, patrick pantel, et al.
2010. from frequency to meaning: vector space mod-
els of semantics. journal of arti   cial intelligence re-
search, 37(1):141   188.

[xu et al.2003] wei xu, xin liu, and yihong gong.
2003. document id91 based on non-negative
in proceedings of the 26th an-
id105.
nual
international acm sigir conference on re-
search and development in informaion retrieval, pages
267   273. acm.

[yandell and majoros2002] mark

and
william h majoros.
2002. genomics and natu-
ral language processing. nature reviews genetics,
3(8):601   610.

yandell

d

