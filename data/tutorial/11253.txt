from the proceedings of eacl 2017 (valencia, spain). this version includes slightly more information

than the published version (january, 2017).

an incremental parser for id15

marco damonte

shay b. cohen

giorgio satta

school of informatics

school of informatics

dept. of information engineering

university of edinburgh
m.damonte@sms.ed.ac.uk

university of edinburgh

scohen@inf.ed.ac.uk

university of padua
satta@dei.unipd.it

7
1
0
2

 
r
p
a
0
1

 

 
 
]
l
c
.
s
c
[
 
 

5
v
1
1
1
6
0

.

8
0
6
1
:
v
i
x
r
a

abstract

id15 (amr)
is a semantic representation for natural
language that embeds annotations related
to traditional tasks such as named entity
recognition, id14, word
sense disambiguation and co-reference
resolution. we describe a transition-based
parser for amr that parses sentences left-
to-right, in linear time. we further pro-
pose a test-suite that assesses speci   c sub-
tasks that are helpful in comparing amr
parsers, and show that our parser is com-
petitive with the state of the art on the
ldc2015e86 dataset and that it outper-
forms state-of-the-art parsers for recover-
ing named entities and handling polarity.

1 introduction

id29 aims to solve the problem
of canonicalizing language and representing its
meaning: given an input sentence, it aims to ex-
tract a semantic representation of that sentence.
id15 (banarescu et al.,
2013), or amr for short, allows us to do that
with the inclusion of most of the shallow-semantic
natural language processing (nlp) tasks that are
usually addressed separately, such as named en-
tity recognition, id14 and co-
reference resolution. amr is partially motivated
by the need to provide the nlp community with
a single dataset that includes basic disambiguation
information, instead of having to rely on differ-
ent datasets for each disambiguation problem. the
annotation process is straightforward, enabling the
development of large datasets.

several parsers for amr have been recently de-
veloped (flanigan et al., 2014; wang et al., 2015a;
peng et al., 2015; pust et al., 2015; goodman
et al., 2016; rao et al., 2015; vanderwende et
al., 2015; artzi et al., 2015; barzdins and gosko,
2016; zhou et al., 2016). this line of research
is new and current results suggest a large room
for improvement. greedy transition-based meth-
ods (nivre, 2008) are one of the most popular
choices for id33, because of their
good balance between ef   ciency and accuracy.
these methods seem promising also for amr,
due to the similarity between dependency trees
and amr structures, i.e., both representations use
graphs with nodes that have lexical content and
edges that represent linguistic relations.

a transition system is an abstract machine char-
acterized by a set of con   gurations and transitions
between them. the basic components of a con-
   guration are a stack of partially processed words
and a buffer of unseen input words. starting from
an initial con   guration, the system applies tran-
sitions until a terminal con   guration is reached.
the sentence is scanned left to right, with linear
time complexity for id33. this is
made possible by the use of a greedy classi   er that
chooses the transition to be applied at each step.

in this paper we introduce a parser for amr that
is inspired by the arceager dependency tran-
sition system of nivre (2004). the main differ-
ence between our system and arceager is that
we need to account for the mapping from word
tokens to amr nodes, non-projectivity of amr
structures and reentrant nodes (multiple incom-
ing edges). our amr parser brings closer depen-
dency parsing and amr parsing by showing that
id33 algorithms, with some mod-
i   cations, can be used for amr. key properties

such as working left-to-right, incrementality1 and
linear complexity further strengthen its relevance.
the amr parser of wang et al. (2015a), called
camr, also de   nes a transition system. it differs
from ours because we process the sentence left-to-
right while they    rst acquire the entire dependency
tree and then process it bottom-up. more recently
zhou et al. (2016) presented a non-greedy tran-
sition system for amr parsing, based on arc-
standard (nivre, 2004). our transition sys-
tem is also related to an adaptation of arceager
for directed acyclic graphs (dags), introduced by
sagae and tsujii (2008). this is also the basis
for ribeyre et al. (2015), a transition system used
to parse dependency graphs. similarly, du et al.
(2014) also address dependency graph parsing by
means of transition systems. analogously to de-
pendency trees, dependency graphs have the prop-
erty that their nodes consist of the word tokens,
which is not true for amr. as such, these transi-
tion systems are more closely related to traditional
transition systems for id33.

our contributions in this paper are as follows:

    in   3 we develop a left-to-right, linear-time
transition system for amr parsing, inspired
by the arceager transition system for de-
pendency tree parsing;

    in   5 we claim that the smatch score (cai
and knight, 2013) is not suf   cient to evalu-
ate amr parsers and propose a set of metrics
to alleviate this problem and better compare
alternative parsers;

    in   6 we show that our algorithm is compet-
itive with publicly available state-of-the-art
parsers on several metrics.

2 background and notation

amr structures amrs are rooted and directed
graphs with node and edge labels. an annotation
example for the sentence i beg you to excuse me is
shown in figure 1, with the amr graph reported
in figure 2.

concepts are represented as labeled nodes in
the graph and can be either english words (e.g. i
and you) or propbank framesets (e.g. beg-01 and
excuse-01). each node in the graph is assigned to

1strictly speaking,

transition-based parsing cannot
achieve full incrementality, which requires to have a single
connected component at all times (nivre, 2004).

( b / beg-01

i

/

:arg0 ( i
:arg1 ( y / you)
:arg2 ( e / excuse-01

:arg0 y
:arg1 i ) )

figure 1: annotation for the sentence    i beg you to
excuse me.    variables are in boldface and concepts
and edge labels are in italics.

:top

beg-01

:arg0

i

:arg1

:arg2

you

:arg0

excuse-01

:arg1

figure 2: amr graph representation for figure 1.

a variable in the amr annotation so that a variable
re-used in the annotation corresponds to reentran-
cies (multiple incoming edges) in the graph. rela-
tions are represented as labeled and directed edges
in the graph.

notation for most sentences in our dataset, the
amr graph is a directed acyclic graph (dag),
with a few speci   c cases where cycles are permit-
ted. these cases are rare, and for the purpose of
this paper, we consider amr as dags.

we denote by [n] the set {1, . . . , n}. we de   ne
an amr structure as a tuple (g, x,   ), where x =
x1          xn is a sentence, with each xi, i     [n], a
word token, and g is a directed graph g = (v, e)
with v and e the set of nodes and edges, respec-
tively.2 we assume g comes along with a node
labeling function and an edge labeling function.
finally,    : v     [n] is a total alignment function
that maps every node of the graph to an index i
for the sentence x, with the meaning that node v
represents (part of) the concept expressed by the
word x  (v).3

2we collapse all multi-word named entities in a single to-
ken (e.g., united kingdom becomes united kingdom) both
in training and parsing.

3

   is a function because we do not consider co-references,
which would otherwise cause a node to map to multiple in-
dices. this is in line with current work on amr parsing.

we note that the function    is not invertible,
since it is neither injective nor surjective. for each
i     [n], we let

     1(i) = {v | v     v,   (v) = i}

be the pre-image of i under    (this set can be
empty for some i), which means that we map a to-
ken in the sentence to a set of nodes in the amr.
in this way we can align each index i for x to the
induced subgraph of g. more formally, we de   ne

         (i) = (     1(i), e     (     1(i)         1(i))),

(1)

with the node and edge labeling functions of          (i)
inherited from g. hence,          (i) returns the amr
subgraph aligned with a particular token in the
sentence.

2.1 transition-based amr parsing

similarly to id33, amr parsing is
partially based on the identi   cation of predicate-
argument structures. much of the dependency
parsing literature focuses on transition-based de-
pendency parsing   an approach to parsing that
scans the sentence from left to right in linear time
and updates an intermediate structure that eventu-
ally ends up being a dependency tree.

the two most common transition systems for
greedy id33 are arcstandard
and arceager. with arcstandard, a stack
is maintained along with a buffer on which the
left-to-right scan is performed. at each step, the
parser chooses to scan a word in the buffer and
shift it onto the stack, or else to create an arc
between the two top-most elements in the stack
and pop the dependent. arcstandard parses
a sentence in a pure bottom-up, left-to-right fash-
ion (similarly to shift-reduce context-free gram-
mar parsers), and must delay the construction of
right arcs until all the dependent node has been
completed. this imposes strong limitations on
the degree of incrementality of the parser. the
arceager system was designed to improve on
arcstandard by mixing bottom up and top-
down strategies. more precisely, in the arcea-
ger parser left arcs are constructed bottom-up and
right arcs are constructed top-down, so that right
dependents can be attached to their heads even if
some of their own dependents are not identi   ed
yet. in this way arcs are constructed as soon as the
head and the dependent are available in the stack.

   

i

beg

you

excuse

figure 3: amr   s edges for the sentence    i beg
you to excuse me.    mapped back to the sentence,
according to the alignment.     is a special token
representing the root.

because of the similarity of amr structures
to dependency structures, transition systems are
also helpful for amr parsing. starting from the
arceager system, we develop here a novel tran-
sition system, called amreager that parses sen-
tences into amr structures. there are three key
differences between amrs and dependency trees
that require further adjustments for dependency
parsers to be used with amrs.

non-projectivity a key difference between en-
glish dependency trees and amr structures is pro-
jectivity. dependency trees in english are usu-
ally projective, roughly meaning that there are no
crossing arcs if the edges are drawn in the semi-
plane above the words. while this restriction is
empirically motivated in syntactic theories for en-
glish, it is no longer motivated for amr struc-
tures.

the notion of projectivity can be generalized to
amr graphs as follows. the intuition is that we
can use the alignment    to map amr edges back
to the sentence x, and test whether there exist pairs
of crossing edges. figure 3 shows this mapping
for the amr of figure 2, where the edge connect-
ing excuse to i crosses another edge. more for-
mally, consider an amr edge e = (u,    , v). let
  (u) = i and   (v) = j, so that u is aligned with
xi and v is aligned with xj . the spanning set for
e, written s(e), is the set of all nodes w such that
  (w) = k and i < k < j if i < j or j < k < i
if j < i. we say that e is projective if, for every
node w     s(e), all of its parent and child nodes
are in s(e)     {u, v}; otherwise, we say that e is
non-projective. an amr is projective if all of its
edges are projective, and is non-projective other-
wise. this corresponds to the intuitive de   nition
of projectivity for dags introduced in sagae and
tsujii (2008) and is closely related to the de   ni-
tion of non-crossing graphs of kuhlmann and jon-
sson (2015).

non-projective edges
non-projective amrs
reentrant edges
amrs with at least one reentrancy

8%
35%
7%
51%

de   ned in equation (1). to obtain alignments be-
tween the tokens in the sentence and the nodes in
the amr graph of our training data, we run the
jamr aligner.7

table 1: statistics for non-projectivity and reen-
trancies in 200 amr manually aligned with the
associated sentences.5

table 1 demonstrates that a relatively small per-
centage of all amr edges are non-projective. yet,
35% of the sentences contain at least one non-
projective edge.

reentrancy amrs are graphs rather than trees
because they can have nodes with multiple par-
ents, called reentrant nodes, as in the node you for
the amr of figure 2. there are two phenomena
that cause reentrancies in amr: control, where a
reentrant edge appears between siblings of a con-
trol verb, and co-reference, where multiple men-
tions correspond to the same concept.6

in contrast, dependency trees do not have nodes
with multiple parents. therefore, when creating
a new arc, transition systems for dependency pars-
ing check that the dependent does not already have
a head node, preventing the node from having ad-
ditional parents. to handle reentrancy, which is
not uncommon in amr structures as shown in ta-
ble 1, we drop this constraint.

alignment another main difference with de-
pendency parsing is that
in amr there is no
straightforward mapping between a word in the
sentence and a node in the graph: words may gen-
erate no nodes, one node or multiple nodes.
in
addition, the labels at the nodes are often not eas-
ily determined by the word in the sentence. for
instance expectation translates to expect-01 and
teacher translates to the two nodes teach-01 and
person, connected through an :arg0 edge, ex-
pressing that a teacher is a person who teaches. a
mechanism of concept identi   cation is therefore
required to map each token xi to a subgraph with
the correct labels at its nodes and edges: if    is the
gold alignment, this should be the subgraph          (i)

5https://github.com/jflanigan/jamr/

blob/master/docs/hand_alignments.md

6a valid criticism of amr is that these two reentrancies
are of a completely different type, and should not be col-
lapsed together. co-reference is a discourse feature, work-
ing by extra-semantic mechanisms and able to cross sentence
boundaries, which are not crossed in amr annotation.

3 transition system for amr parsing

a stack    =   n|          |  1|  0 is a list of nodes
of the partially constructed amr graph, with the
top element   0 at the right. we use the sym-
bol    |    as the concatenation operator. a buffer
   =   0|  1|          |  n is a list of indices from x, with
the    rst element   0 at the left, representing the
word tokens from the input still to be processed.
a con   guration of our parser is a triple (  ,   , a),
where a is the set of amr edges that have been
constructed up to this point.

in order to introduce the transition actions of
our parser we need some additional notation. we
use a function a that maps indices from x to amr
graph fragments. for each i     [n], a(i) is a graph
ga = (va, ea), with single root root(ga), repre-
senting the semantic contribution of word xi to the
amr for x. as already mentioned, ga can have
a single node representing the concept associated
with xi, or it can have several nodes in case xi de-
notes a complex concept, or it can be empty.

the transition shift is used to decide if and
what to push on the stack after consuming a to-
ken from the buffer.
intuitively, the graph frag-
ment a(  0) obtained from the token   0, if not
empty, is    merged    with the graph we have con-
structed so far. we then push onto the stack the
node root(a(  0)) for further processing. larc(   )
creates an edge with label     between the top-most
node and the second top-most node in the stack,
and pops the latter. rarc(   ) is the symmetric op-
eration, but does not pop any node from the stack.
finally, reduce pops the top-most node from
the stack, and it also recovers reentrant edges be-
tween its sibling nodes, capturing for instance sev-
eral control verb patterns. to accomplish this, re-
duce decides whether to create an additional edge
between the node being removed and the previ-
ously created sibling in the partial graph. this way
of handling control verbs is similar to the reen-
trance transition of wang et al. (2015a).

the choice of popping the dependent in the
larc transition is inspired by arceager, where
left-arcs are constructed bottom-up to increase
the incrementality of the transition system (nivre,

7https://github.com/jflanigan/jamr

2004). this affects our ability to recover some
reentrant edges: consider a node u with two par-
ents v and v   , where the arc v     u is a left-arc and
v        u is any arc. if the    rst arc to be processed is
v     u, we use larc that pops u, hence making it
impossible to create the second arc v        u. nev-
ertheless, we discovered that this approach works
better than a completely unrestricted allowance of
reentrancy. the reason is that if we do not remove
dependents at all when    rst attached to a node, the
stack becomes larger, and nodes which should be
connected end up being distant from each other,
and as such, are never connected.

the initial con   guration of the system has a    
node (representing the root) in the stack and the
entire sentence in the buffer. the terminal con-
   guration consists of an empty buffer and a stack
with only the     node. the transitions required to
parse the sentence the boy and the girl are shown
in table 2, where the    rst line shows the initial
con   guration and the last line shows the terminal
con   guration.

similarly to the transitions of the arceager,
the above transitions construct edges as soon as the
head and the dependent are available in the stack,
with the aim of maximizing the parser incremen-
tality. we now show that our greedy transition-
based amr parser is linear-time in n, the length of
the input sentence x. we    rst claim that the output
graph has size o(n). each token in x is mapped to
a constant number of nodes in the graph by shift.
thus the number of nodes is o(n). furthermore,
each node can have at most three parent nodes,
created by transitions rarc, larc and reduce,
respectively. thus the number of edges is also
o(n). it is possible to bound the maximum num-
ber of transitions required to parse x: the number
of shift is bounded by n, and the number of re-
duce, larc and rarc is bounded by the size of
the graph, which is o(n). since each transition
can be carried out in constant time, we conclude
that our parser runs in linear time.

4 training the system

several components have to be learned: (1) a tran-
sition classi   er that predicts the next transition
given the current con   guration, (2) a binary clas-
si   er that decides whether or not to create a reen-
trancy after a reduce, (3) a concept identi   cation
step for each shift to compute a(  0), and 3) an-
other classi   er to label edges after each larc or

rarc.

4.1 oracle

training our
system from data requires an
oracle   an algorithm that given a gold-standard
amr graph and a sentence returns transition se-
quences that maximize the overlap between the
gold-standard graph and the graph dictated by the
sequence of transitions.

we adopt a shortest stack, static oracle similar
to chen and manning (2014).
informally, static
means that if the actual con   guration of the parser
has no mistakes, the oracle provides a transition
that does not introduce any mistake. shortest stack
means that the oracle prefers transitions where the
number of items in the stack is minimized. given
the current con   guration (  ,   , a) and the gold-
standard graph g = (vg, ag), the oracle is de   ned
as follows, where we test the conditions in the
given order and apply the action associated with
the    rst match:

1. if       [(  0,    ,   1)     ag] then larc(   );

2. if       [(  1,    ,   0)     ag] then rarc(   );

3. if      i,    [(  0,    ,   i)     ag     (  i,    ,   0)     ag]

then reduce;

4. shift otherwise.

the oracle    rst checks whether some gold-
standard edge can be constructed from the two el-
ements at the top of the stack (conditions 1 and 2).
if larc or rarc are not possible, the oracle checks
whether all possible edges in the gold graph in-
volving   0 have already been processed, in which
case it chooses reduce (conditions 3). to this
end, it suf   ces to check the buffer, since larc and
rarc have already been excluded and elements in
the stack deeper than position two can no longer
be accessed by the parser. if reduce is not possi-
ble, shift is chosen.

besides deciding on the next transition, the ora-
cle also needs the alignments, which we generate
with jamr, in order to know how to map the next
token in the sentence to its amr subgraph          (i)
de   ned in (1).

4.2 transition classi   er

like all other transition systems of this kind, our
transition system has a    controller    that predicts a
transition given the current con   guration (among

action
-
shift

shift

shift

larc

rarc

shift

shift

stack
[   ]
[   ]
[   , boy]
[   , boy, and ]
[   , and ]
[   , and ]
[   , and ]
[   , and, girl ]
[   , and, girl ]

rarc
reduce [   , and ]
reduce [   ]

buffer
[the,boy,and,the,girl]
[boy,and,the,girl]
[and,the,girl]
[the,girl]
[the,girl]
[the,girl]
[girl]
[]
[]
[]
[]

edges
{}
{}
{}
{}
{hand,:op1,boyi} = a1
a1     {h   ,:top,andi} = a2
a2
a2
a2     {hand,:op2,girli} = a3
a3
a3

table 2: parsing steps for the sentence    the boy and the girl.   

shift, larc, rarc and reduce). the examples
from which we learn this controller are based on
features extracted from the oracle transition se-
quences, where the oracle is applied on the train-
ing data.

as a classi   er, we use a feed-forward neural
network with two hidden layers of 200 tanh units
and learning rate set to 0.1, with linear decay-
ing. the input to the network consists of the
concatenation of embeddings for words, pos tags
and stanford parser dependencies, one-hot vec-
tors for named entities and additional sparse fea-
tures, extracted from the current con   guration of
the transition system; this is reported in more de-
tails in table 3. the embeddings for words and
pos tags were pre-trained on a large unanno-
tated corpus consisting of the    rst 1 billion char-
acters from wikipedia.8 for lexical information,
we also extract the leftmost (in the order of the
aligned words) child (c), leftmost parent (p) and
leftmost grandchild (cc). leftmost and rightmost
items are common features for transition-based
parsers (zhang and nivre, 2011; chen and man-
ning, 2014) but we found only leftmost to be
helpful in our case. all pos tags, dependencies
and named entities are generated using stanford
corenlp (manning et al., 2014). the accuracy of
this classi   er on the development set is 84%.

similarly, we train a binary classi   er for decid-
ing whether or not to create a reentrant edge after
a reduce: in this case we use word and pos em-
beddings for the two nodes being connected and
their parent as well as dependency label embed-
dings for the arcs between them.

8http://mattmahoney.net/dc/enwik9.zip

4.3 concept identi   cation

this routine is called every time the transition
classi   er decides to do a shift; it is denoted by a(  )
in   3. this component could be learned in a super-
vised manner, but we were not able to improve on
a simple heuristic, which works as follows: during
training, for each shift decided by the oracle, we
store the pair (  0,          (i)) in a phrase-table. dur-
ing parsing, the most frequent graph h for the
given token is then chosen. in other words, a(i)
approximates          (i) by means of the graph most
frequently seen among all occurrences of token xi
in the training set.

an obvious problem with the phrase-table ap-
proach is that it does not generalize to unseen
words. in addition, our heuristic relies on the fact
that the mappings observed in the data are correct,
which is not the case when the jamr-generated
alignments contain a mistake. in order to alleviate
this problem we observe that there are classes of
words such as named entities and numeric quan-
tities that can be disambiguated in a deterministic
manner. we therefore implement a set of    hooks   
that are triggered by the named entity tag of the
next token in the sentence. these hooks override
the normal shift mechanism and apply a    xed rule
instead. for instance, when we see the token new
york (the two tokens are collapsed in a single one
at preprocessing) we generate the subgraph of fig-
ure 4 and push its root onto the stack. similar sub-
graphs are generated for all states, cities, countries
and people. we also use hooks for ordinal num-
bers, percentages, money and dates.

depth
children
parents
lexical

pos
entities
dependency

d(  0), d(  1)
#c(  0), #c(  1)
#p(  0), #p(  1)
w(  0), w(  1), w(  0), w(  1),
w(p(  0)), w(c(  0)), w(cc(  0)),
w(p(  1)), w(c(  1)), w(cc(  1))
s(  0), s(  1), s(  0), s(  1)
e(  0), e(  1), e(  0), e(  1)
   (  0,   1),    (  1,   0),
   i     {0, 1}:    (  i,   0),    (  0,   i)
   i     {1, 2, 3}:    (  0,   i),    (  i,   0)
   i     {1, 2, 3}:    (  0,   i),    (  i,   0)

table 3: features used in transition classi   er. the
function d maps a stack element to the depth of
the associated graph fragment. the functions #c
and #p count the number of children and par-
ents, respectively, of a stack element. the function
w maps a stack/buffer element to the word em-
bedding for the associated word in the sentence.
the function p gives the leftmost (according to the
alignment) parent of a stack element, the function
c the leftmost child and the function cc the leftmost
grandchild. the function s maps a stack/buffer el-
ement to the part-of-speech embedding for the as-
sociated word. the function e maps a stack/buffer
element to its entity. finally, the function     maps
a pair of symbols to the dependency label embed-
ding, according to the edge (or lack of) in the de-
pendency tree for the two words these symbols are
mapped to.

4.4 edge labeling

edge labeling determines the labels for the edges
being created. every time the transition classi   er
decides to take an larc or rarc operation, the
edge labeler needs to decide on a label for it. there
are more than 100 possible labels such as :arg0,
:arg0-of, :arg1, :location, :time and :polarity.
we use a feed-forward neural network similar to
the one we trained for the transition classier, with
features shown in table 4. the accuracy of this
classi   er on the development set is 77%.

labeling rules sometimes the label predicted
by the neural network is not a label that satis   es
the requirements of amr. for instance, the la-
bel :top can only be applied when the node from
which the edge starts is the special     node. in or-
der to avoid generating such erroneous labels, we
use a set of rules, shown in table 5. these rules
determine which labels are allowed for the newly

:top

country

:name

name

:wiki

new york

:op1

new

:op2

york

figure 4: subgraph for    new york.   

name
depth
children
parents
lexical

pos
entities
dependency

feature template
d(  0), d(  1)
#c(  0), #c(  1)
#p(  0), #p(  1)
w(  0), w(  1),
w(p(  0)), w(c(  0)), w(cc(  0)),
w(p(  1)), w(c(  1)), w(cc(  1))
s(  0), s(  1)
e(  0), e(  1)
   (  0,   0),    (  0,   0)

table 4: features used in edge labeling. see ta-
ble 3 for a legend of symbols.

created edge so that we only consider those during
prediction. also arg roles cannot always be ap-
plied: each propbank frame allows a limited num-
ber of arguments. for example, while add-01 and
add-02 allow for :arg1 and :arg2 (and their in-
verse :arg1-of and :arg2-of ), add-03 and add-
04 only allow :arg2 (and :arg2-of ).

5 fine-grained evaluation

until now, amr parsers were evaluated using the
smatch score.9 given the candidate graphs and
the gold graphs in the form of amr annotations,
smatch    rst tries to    nd the best alignments be-
tween the variable names for each pair of graphs
and it then computes precision, recall and f1 of the
concepts and relations. we note that the smatch
score has two    aws: (1) while amr parsing in-
volves a large number of subtasks, the smatch
score consists of a single number that does not as-
sess the quality of each subtasks separately; (2) the
smatch score weighs different types of errors in

9since smatch is an approximate randomized algorithm,
decimal points in the results vary between different runs and
are not reported. this approach was also taken by wang et al.
(2015b) and others.

start
   

end

label
:top
:polarity
:mode

:value
:day
:month
:year
:decade
:century
:weekday

:quarter
:season

ex.
yes
yes
yes

no
no
no
no
no
no
yes

no
yes

d-ent
d-ent
d-ent
d-ent
d-ent
d-ent

d-ent
d-ent

:timezone yes

d-ent

-
inter.|
expr.|imp.
   \w+    |[0-9]+
[1|2|         |31]
[1|2|         |12]+
[0-9]+
[0-9]+
[0-9]+
[monday|         |
sunday]
[1|2|3|4]+
[winter|fall|
spring|summer]+
[a   z]3

table 5: labeling rules: for each edge label, we
provide id157 that must hold on the
labels at the start node (start) and the end node
(end) of the edge. ex. indicates when the rule is
exclusive, d-ent is the amr concept date-entity,
inter. is the amr constant interrogative, expr. is
the amr constant expressive, imp.
is the amr
constant imperative.

a way which is not necessarily useful for solving
a speci   c nlp problem. for example, for a spe-
ci   c problem concept detection might be deemed
more important than edge detection, or guessing
the wrong sense for a concept might be consid-
ered less severe than guessing the wrong verb al-
together.

consider the two parses for the sentence silvio
berlusconi gave lucio stanca his current role of
modernizing italy   s bureaucracy in figure 5. at
the top, we show the output of a parser (parse 1)
that is not able to deal with named entities. at
the bottom, we show the output of a parser (parse
2) which, except for :name, :op and :wiki, always
uses the edge label :arg0. the smatch scores for
the two parses are 56 and 78 respectively. both
parses make obvious mistakes but the three named
entity errors in parse 1 are considered more impor-
tant than the six wrong labels in parse 2. however,
without further analysis, it is not advisable to con-
clude that parse 2 is better than parse 1. in order
to better understand the limitations of the differ-
ent parsers,    nd their strengths and gain insight in
which downstream tasks they may be helpful, we
compute a set of metrics on the test set.

unlabeled is the smatch score computed on
the predicted graphs after removing all edge la-
bels. in this way, we only assess the node labels

( g / give-01

:arg0 ( p3 / silvio :mod ( n4 / berlusconi ) )
:arg1 ( r / role

:time ( c2 / current )
:mod ( m / modernize-01

:arg0 p4
:arg1 ( b / bureaucracy :part-of

( c3 /

italy ) ) )

:poss p4 )

:arg2 ( p4 / person lucio :mod stanca ) )

( g / give-01

:arg0 ( p3 / person :wiki     s i l v i o b e r l u s c o n i    

:name ( n4 / name :op1     s i l v i o     :op2     b e r l u s c o n i     ) )

:arg0 ( r / role

:arg0 ( c2 / current )
:arg0 ( m / modernize-01

:arg0 p4
:arg0 ( b / bureaucracy

:arg0 ( c3 / country :wiki     i t a l y    

:name ( n6 / name :op1     i t a l y     ) ) ) )

:arg0 p4 )

:arg0 ( p4 / person :wiki    

:name ( n5 / name :op1     l u c i o     :op2     s t a n c a     ) ) )

figure 5: two parses for the sentence    silvio
berlusconi gave lucio stanca his current role of
modernizing italy   s bureaucracy.   

and the graph topology, which may be enough to
bene   t several nlp tasks because it identi   es ba-
sic predicate-argument structure. for instance, we
may be interested in knowing whether two events
or entities are related to each other, while not being
concerned with the precise type of relation holding
between them.

no wsd gives a score that does not take into
account id51 errors. by ig-
noring the sense speci   ed by the propbank frame
used (e.g., duck-01 vs duck-02) we have a score
that does not take into account this additional com-
plexity in the parsing procedure. to compute this
score, we simply strip off the suf   xes from all
propbank frames and calculate the smatch score.
following sawai et al. (2015), we also evalu-
ate the parsers using the smatch score on noun
phrases only (np-only), by extracting from the
amr dataset all noun phrases that do not include
further nps.

as we previously discussed, reentrancy is a very
important characteristic of amr graphs and it is
not trivial to handle. we therefore implement a
test for it (reentrancy), where we compute the
smatch score only on reentrant edges.

concept identi   cation is another critical com-
ponent of the parsing process and we therefore
compute the f-score on the list of predicted con-
cepts (concepts) too. identifying the correct con-
cepts is fundamental:
if a concept is not identi-
   ed, it will not be possible to retrieve any edge
involving that concept, with likely signi   cant con-
sequences on accuracy. this metric is therefore

metric
smatch
unlabeled
no wsd
np-only
reentrancy
concepts
named ent.
wiki   cation
negations
srl

first parse second parse

56
65
56
39
69
56
0
0
0
69

78
100
78
86
46
100
100
100

0
54

metric
smatch
unlabeled
no wsd
np-only
reentrancy
concepts
named ent.
wiki   cation
negations
srl

j   14 c   15
58
61
58
47
38
79
75
0
16
55

63
69
64
54
41
80
75
0
18
60

j   16 ours
67
69
68
58
42
83
79
75
45
60

64
69
65
55
41
83
83
64
48
56

table 6: evaluation of the two parses in figure 5
with the proposed evaluation suite.

table 7: results on test split of ldc2015e86 for
jamr, camr and our amreager. j stands for
jamr and c for camr (followed by the year of
publication). best systems are in bold.

quite important to score highly on.

6 experiments

similarly to our score for concepts, we fur-
ther compute an f-score on the named entities
(named ent.) and wiki roles for named entities
(wiki   cation) that consider edges labeled with
:name and :wiki respectively. these two metrics
are strictly related to the concept score. how-
ever, since id39 is the fo-
cus of dedicated research, we believe it is impor-
tant to de   ne a metric that speci   cally assesses
this problem. negation detection is another task
which has received some attention. an f-score
for this (negations) is also de   ned, where we    nd
all negated concepts by looking for the :polarity
role. the reason we can compute a simple f-score
instead of using smatch for these metrics is that
there are no variable names involved.

finally we compute the smatch score on :arg
edges only, in order to have a score for semantic
role labeling (srl), which is another extremely
important subtask of amr, as it is based on the
identi   cation of predicate-argument structures.

using this evaluation suite we can evaluate
amrs on a wide range of metrics that can help us
   nd strengths and weakness of each parser, hence
speeding up the research in this area. table 6 re-
ports the scores for the two parses of figure 5,
where we see that parse 1 gets a high score for
id14 while parse 2 is optimal
for id39. moreover, we can
make additional observations such as that parse 2
is optimal with respect to unlabeled score and that
parse 1 recovers more reentrancies.

we compare our parser10 against two available
parsers: jamr (flanigan et al., 2014) and camr
(wang et al., 2015b; wang et al., 2015a), using the
ldc2015e86 dataset for evaluation. both parsers
are available online11 and were recently updated
for semeval-2016 task 8 (flanigan et al., 2016;
wang et al., 2016). however, camr   s semeval
system, which reports a smatch score of 67, is
not publicly available. camr has a quadratic
worst-case complexity (although linear in prac-
tice).
in jamr, the concept identi   cation step
is quadratic and the relation identi   cation step is
o(|v |2 log |v |), with |v | being the set of nodes in
the amr graph.

table 7 shows the results obtained by the
parsers on all metrics previously introduced. on
smatch, our system does not give state-of-the-art
results. however, we do obtain the best results
for unlabeled and concept and outperform the
other parses for named ent. and negations. our
score of reentrancy is also close the best scor-
ing system, which is particularly relevant given
the importance of reentrancies in amr. the use
of the reduce transition, which targets reentran-
cies caused by control verbs, is critical in order to
achieve this result.

the relatively high results we obtain for the un-

10our parser is available at https://github.com/
mdtux89/amr-eager, the evaluation suite at https:
//github.com/mdtux89/amr-evaluation
and a demo at http://cohort.inf.ed.ac.uk/
amreager.html

11jamr:

https://github.com/jflanigan/
jamr, camr: https://github.com/c-amr/camr.

labeled case suggests that our parser has dif   culty
in labeling the arcs. our score for concept identi-
   cation, which is on par with the best result from
the other parsers, demonstrates that there is a rel-
atively low level of token ambiguity. state-of-the-
art results for this problem can be obtained by
choosing the most frequent subgraph for a given
token based on a phrase-table constructed from
jamr alignments on the training data. the scores
for named entities and wiki   cation are heavily de-
pendent on the hooks mentioned in   4.3, which
in turn relies on the named entity recognizer to
make the correct predictions. in order to alleviate
the problem of wrong automatic alignments with
respect to polarity and better detect negation, we
performed a post-processing step on the aligner
output where we align the amr constant - (mi-
nus) with words bearing negative polarity such as
not, illegitimate and asymmetry.

our experiments demonstrate that there is no
parser for amr yet that conclusively does better
than all other parsers on all metrics. advantages
of our parser are the worst-case linear complexity
and the fact that is possible to perform incremen-
tal amr parsing, which is both helpful for real-
time applications and to investigate how meaning
of english sentences can be built incrementally
left-to-right.

7 related work

the    rst data-driven amr parser is due to flani-
gan et al. (2014). the problem is addressed in
two separate stages: concept identi   cation and re-
lation identi   cation. they use a sequence label-
ing algorithm to identify concepts and frame the
relation prediction task as a constrained combina-
torial optimization problem. werling et al. (2015)
notice that the dif   cult bit is the concept identi-
   cation and propose a better way to handle that
task: an action classi   er to generate concepts by
applying predetermined actions. other propos-
als involve a synchronous hyperedge replacement
grammar solution (peng et al., 2015), a syntax-
based machine translation approach (pust et al.,
2015) where a grammar of string-to-tree rules is
created after reducing amr graphs to trees by re-
moving all reentrancies, a id35 system that    rst
parses sentences into lambda-calculus representa-
tions (artzi et al., 2015). a systematic transla-
tion from amr to    rst order logic formulas, with
a special treatment for quanti   cation, reentrancy

and negation, is discussed in bos (2016). in van-
derwende et al. (2015), a pre-existing logical form
parser is used and the output is then converted into
amr graphs. yet another solution is proposed by
rao et al. (2015) who discuss a parser that uses
searn (daum  e iii et al., 2009), a    learning to
search    algorithm.

transition-based algorithms for amr parsing
are compelling because traditional graph-based
techniques are computationally expensive. wang
et al. (2015b) and wang et al. (2015a) propose a
framework that parses a sentence into its amr
structure through a two-stage process: a depen-
dency tree is generated from the input sentence
through a transition-based parser and then an-
other transition-based parser is used to generate
the amr. the main bene   t of this approach is that
the dependency parser can be trained on a training
set much larger than the training set for the tree-
to-graph algorithm. others further built on this
parser: goodman et al. (2016) use imitation learn-
ing to alleviate the probem of error propagation
in the greedy parser, while barzdins and gosko
(2016) create a wrapper around it to    x frequent
mistakes and investigate ensembles with a char-
acter level neural parser. more recently zhou et
al. (2016) presented a non-greedy transition sys-
tem for amr parsing, based on arcstandard
(nivre, 2004).

amr parsing as a whole is a complex task be-
cause it involves many subtasks including named
entity recognition, co-reference resolution and se-
mantic role labeling. sawai et al. (2015) do not
attempt at parsing amr graphs for entire sen-
tences but they instead handle simple noun phrases
(nps). they extract nps from the amr dataset
only when they do not include further nps, do not
include pronouns nor named entities. due to these
restrictions, the amrs are mostly trees and eas-
ier to handle than the original amr graphs. they
approach this task using a transition based system
inspired by arcstandard.

amr is not the only way to represent meaning
in natural language sentences. alternative seman-
tic representations have been developed and stud-
ied, such as boxer (bos et al., 2004), id35 (steed-
man, 1996; steedman, 2000) and ucca (abend
and rappoport, 2013).

8 conclusion

we presented a transition system that builds amr
graphs in linear time by processing the sentences
left-to-right, trained with feed-forward neural net-
works. the parser demonstrates that it is possi-
ble to perform amr parsing using techniques in-
spired by id33.

we also noted that it is less informative to eval-
uate the entire parsing process with smatch than
to use a collection of metrics aimed at evaluat-
ing the various subproblems in the parsing pro-
cess. we further showed that our left-to-right tran-
sition system is competitive with publicly avail-
able state-of-the-art parsers. although we do not
outperform the best baseline in terms of smatch
score, we show on par or better results for sev-
eral of the metrics proposed. we hope that moving
away from a single-metric evaluation will further
speed up progress in amr parsing.

acknowledgments

the authors would like to thank the three anony-
mous reviewers and sameer bansal, jeff flanigan,
sorcha gilroy, adam lopez, nikos papasaran-
topoulos, nathan schneider, mark steedman, sam
thomson, clara vania and chuan wang for their
help and comments. this research was supported
by a grant from bloomberg and by the h2020
project summa, under grant agreement 688139.

references

omri abend and ari rappoport. 2013. universal con-
ceptual cognitive annotation (ucca). in proceed-
ings of acl.

yoav artzi, kenton lee, and luke zettlemoyer. 2015.
broad-coverage id35 id29 with amr.
proceedings of emnlp.

laura banarescu, claire bonial, shu cai, madalina
georgescu, kira grif   tt, ulf hermjakob, kevin
knight, philipp koehn, martha palmer, and nathan
schneider. 2013. id15
for sembanking. proceedings of linguistic annota-
tion workshop.

guntis barzdins and didzis gosko. 2016. riga at
semeval-2016 task 8: impact of smatch extensions
and character-level neural translation on amr pars-
ing accuracy. arxiv preprint arxiv:1604.01278.

johan bos, stephen clark, mark steedman, james r
curran, and julia hockenmaier.
2004. wide-
coverage semantic representations from a id35 parser.

in proceedings of coling. association for compu-
tational linguistics.

johan bos. 2016. expressive power of abstract mean-
ing representations. computational linguistics, 42.

shu cai and kevin knight. 2013. smatch: an evalua-
tion metric for semantic feature structures. proceed-
ings of acl.

danqi chen and christopher d manning. 2014. a fast
and accurate dependency parser using neural net-
works. in proceesings of emnlp.

hal daum  e iii, john langford, and daniel marcu.
2009. search-based id170. machine
learning, 75(3):297   325.

yantao du, fan zhang, weiwei sun, and xiaojun wan.
2014. peking: pro   ling syntactic tree parsing tech-
niques for semantic graph parsing. in proceedings
of the 8th international workshop on semantic eval-
uation (semeval-2014), pages 459   464.

jeffrey flanigan, sam thomson, jaime g carbonell,
chris dyer, and noah a smith. 2014. a discrim-
inative graph-based parser for the abstract meaning
representation. proceedings of acl.

jeffrey flanigan, chris dyer, noah a smith, and jaime
carbonell. 2016. cmu at semeval-2016 task 8:
graph-based amr parsing with in   nite ramp loss.
proceedings of semeval, pages 1202   1206.

james goodman, andreas vlachos, and jason narad-
owsky. 2016. noise reduction and targeted explo-
ration in imitation learning for abstract meaning rep-
resentation parsing. proceedings of acl.

marco kuhlmann and peter jonsson.

2015. pars-
ing to noncrossing dependency graphs. transac-
tions of the association for computational linguis-
tics, pages 559   570.

christopher d. manning, mihai surdeanu, john bauer,
jenny finkel, steven j. bethard, and david mc-
closky. 2014. the stanford corenlp natural lan-
guage processing toolkit.
in acl system demon-
strations.

joakim nivre. 2004. incrementality in deterministic
id33. proceedings of the workshop
on incremental parsing: bringing engineering and
cognition together. acl.

joakim nivre. 2008. algorithms for deterministic in-
cremental id33. computational lin-
guistics, volume 34, number 4, december 2008.

xiaochang peng, linfeng song, and daniel gildea.
2015. a synchronous hyperedge replacement gram-
mar based approach for amr parsing. proceedings
of conll.

michael pust, ulf hermjakob, kevin knight, daniel
marcu, and jonathan may. 2015. using syntax-
based machine translation to parse english into
id15.
arxiv preprint
arxiv:1504.06665.

sudh rao, yogarshi vyas, hal daume iii, and philip
resnik. 2015. parser for abstract meaning represen-
tation using learning to search. arxiv:1510.07586.

corentin ribeyre,   eric villemonte de la clergerie, and
djam  e seddah. 2015. because syntax does matter:
improving predicate-argument structures parsing us-
ing syntactic features.
in conference of the north
american chapter of the association for computa-
tional linguistics: human language technologies.

kenji sagae and jun   ichi tsujii. 2008. shift-reduce
dependency dag parsing. proceedings of col-
ing.

yuichiro sawai, hiroyuki shindo, and yuji matsumoto.
2015. semantic structure analysis of noun phrases
using id15. proceedings
of acl.

mark steedman. 1996. surface structure and inter-

pretation. the mit press.

mark steedman. 2000. the syntactic process. the

mit press.

lucy vanderwende, arul menezes, and chris quirk.
2015. an amr parser for english, french, german,
spanish and japanese and a new amr-annotated
corpus. proceedings of naacl-hlt.

chuan wang, nianwen xue, and sameer pradhan.
2015a. boosting transition-based amr parsing
with re   ned actions and auxiliary analyzers. pro-
ceedings of acl.

chuan wang, nianwen xue, and sameer pradhan.
2015b. a transition-based algorithm for amr pars-
ing. proceedings of naacl.

chuan wang, sameer pradhan, nianwen xue, xiao-
man pan, and heng ji. 2016. camr at semeval-
2016 task 8: an extended transition-based amr
parser. proceedings of semeval.

keenon werling, gabor angeli, and christopher man-
ning. 2015. robust subgraph generation improves
id15 parsing.
arxiv
preprint arxiv:1506.03139.

yue zhang and joakim nivre.

2011. transition-
based id33 with rich non-local fea-
tures. proceedings of acl.

junsheng zhou, feiyu xu, hans uszkoreit, weiguang
qu, ran li, and yanhui gu. 2016. amr parsing
with an incremental joint model. in proceedings of
the 2016 conference on empirical methods in nat-
ural language processing.

