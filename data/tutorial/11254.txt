towards end-to-end id23 of

dialogue agents for information access

bhuwan dhingra(cid:63) lihong li    xiujun li   

jianfeng gao   

yun-nung chen    faisal ahmed    li deng   
(cid:63)carnegie mellon university, pittsburgh, pa, usa

   microsoft research, redmond, wa, usa
   national taiwan university, taipei, taiwan

(cid:63)bdhingra@andrew.cmu.edu    {lihongli,xiul,jfgao}@microsoft.com    y.v.chen@ieee.org

7
1
0
2

 
r
p
a
0
2

 

 
 
]
l
c
.
s
c
[
 
 

3
v
7
7
7
0
0

.

9
0
6
1
:
v
i
x
r
a

abstract

this paper proposes kb-infobot1     a
multi-turn dialogue agent which helps
users search knowledge bases (kbs)
without composing complicated queries.
such goal-oriented dialogue agents typ-
ically need to interact with an external
database to access real-world knowledge.
previous systems achieved this by issuing
a symbolic query to the kb to retrieve en-
tries based on their attributes. however,
such symbolic operations break the differ-
entiability of the system and prevent end-
to-end training of neural dialogue agents.
in this paper, we address this limitation
by replacing symbolic queries with an in-
duced    soft    posterior distribution over the
kb that indicates which entities the user is
interested in. integrating the soft retrieval
process with a reinforcement learner leads
to higher task success rate and reward in
both simulations and against real users.
we also present a fully neural end-to-end
agent, trained entirely from user feedback,
and discuss its application towards person-
alized dialogue agents.

introduction

1
the design of intelligent assistants which interact
with users in natural language ranks high on the
agenda of current nlp research. with an increas-
ing focus on the use of statistical and machine
learning based approaches (young et al., 2013),
the last few years have seen some truly remark-
able conversational agents appear on the market
(e.g. apple siri, microsoft cortana, google allo).
these agents can perform simple tasks, answer

1the source code is available at: https://github.

com/miulab/kb-infobot

factual questions, and sometimes also aiid113ssly
chit-chat with the user, but they still lag far be-
hind a human assistant in terms of both the va-
riety and complexity of tasks they can perform.
in particular, they lack the ability to learn from
interactions with a user in order to improve and
adapt with time. recently, reinforcement learn-
ing (rl) has been explored to leverage user inter-
actions to adapt various dialogue agents designed,
respectively, for task completion (ga  si  c et al.,
2013), information access (wen et al., 2016b), and
chitchat (li et al., 2016a).

we focus on kb-infobots, a particular type of
dialogue agent that helps users navigate a knowl-
edge base (kb) in search of an entity, as illus-
trated by the example in figure 1. such agents
must necessarily query databases in order to re-
trieve the requested information. this is usually
done by performing id29 on the input
to construct a symbolic query representing the be-
liefs of the agent about the user goal, such as wen
et al. (2016b), williams and zweig (2016), and li
et al. (2017)   s work. we call such an operation
a hard-kb lookup. while natural, this approach
has two drawbacks: (1) the retrieved results do not
carry any information about uncertainty in seman-
tic parsing, and (2) the retrieval operation is non
differentiable, and hence the parser and dialog pol-
icy are trained separately. this makes online end-
to-end learning from user feedback dif   cult once
the system is deployed.

in this work, we propose a probabilistic frame-
work for computing the posterior distribution of
the user target over a knowledge base, which we
term a soft-kb lookup. this distribution is con-
structed from the agent   s belief about the attributes
of the entity being searched for. the dialogue pol-
icy network, which decides the next system action,
receives as input this full distribution instead of a
handful of retrieved results. we show in our ex-

2 related work

our work is motivated by the neural genqa (yin
et al., 2016a) and neural enquirer (yin et al.,
2016b) models for querying kbs via natural lan-
guage in a fully    neuralized    way. however, the
key difference is that these systems assume that
users can compose a complicated, compositional
natural language query that can uniquely identify
the element/answer in the kb. the research task
is to parse the query, i.e., turning the natural lan-
guage query into a sequence of sql-like opera-
tions.
instead we focus on how to query a kb
interactively without composing such complicated
queries in the    rst place. our work is motivated
by the observations that (1) users are more used to
issuing simple queries of length less than 5 words
(spink et al., 2001); (2) in many cases, it is unrea-
sonable to assume that users can construct com-
positional queries without prior knowledge of the
structure of the kb to be queried.

also related is the growing body of literature
focused on building end-to-end dialogue systems,
which combine feature extraction and policy opti-
mization using deep neural networks. wen et al.
(2016b) introduced a modular neural dialogue
agent, which uses a hard-kb lookup, thus break-
ing the differentiability of the whole system. as a
result, training of various components of the di-
alogue system is performed separately. the in-
tent network and belief trackers are trained using
supervised labels speci   cally collected for them;
while the policy network and generation network
are trained separately on the system utterances.
we retain modularity of the network by keeping
the belief trackers separate, but replace the hard
lookup with a differentiable one.

dialogue agents can also interface with the
database by augmenting their output action space
with prede   ned api calls (williams and zweig,
2016; zhao and eskenazi, 2016; bordes and we-
ston, 2016; li et al., 2017). the api calls modify
a query hypothesis maintained outside the end-to-
end system which is used to retrieve results from
this kb. this framework does not deal with uncer-
tainty in language understanding since the query
hypothesis can only hold one slot-value at a time.
our approach, on the other hand, directly models
the uncertainty to construct the posterior over the
kb.

wu et al. (2015) presented an id178 mini-
mization dialogue management strategy for in-

figure 1: an interaction between a user looking
for a movie and the kb-infobot. an entity-centric
knowledge base is shown above the kb-infobot
(missing values denoted by x).

periments that this framework allows the agent to
achieve a higher task success rate in fewer dia-
logue turns. further, the retrieval process is dif-
ferentiable, allowing us to construct an end-to-end
trainable kb-infobot, all of whose components
are updated online using rl.

reinforcement learners typically require an en-
vironment to interact with, and hence static dia-
logue corpora cannot be used for their training.
running experiments on human subjects, on the
other hand, is unfortunately too expensive. a
common workaround in the dialogue community
(young et al., 2013; schatzmann et al., 2007b;
schef   er and young, 2002) is to instead use user
simulators which mimic the behavior of real users
in a consistent manner. for training kb-infobot,
we adapt the publicly available2 simulator de-
scribed in li et al. (2016b).

evaluation of dialogue agents has been the sub-
ject of much research (walker et al., 1997; m  oller
et al., 2006). while the metrics for evaluating an
infobot are relatively clear     the agent should re-
turn the correct entity in a minimum number of
turns     the environment for testing it not so much.
unlike previous kb-based qa systems, our focus
is on multi-turn interactions, and as such there are
no publicly available benchmarks for this prob-
lem. we evaluate several versions of kb-infobot
with the simulator and on real users, and show
that the proposed soft-kb lookup helps the re-
inforcement learner discover better dialogue poli-
cies. initial experiments on the end-to-end agent
also demonstrate its strong learning capability.

2https://github.com/miulab/tc-bot

movie=?actor=bill murrayrelease year=1993find me the bill murray   smovie.i think it came out in 1993.when was it released?groundhog day is a bill murray movie which came out in 1993. kb-infobotuserentity-centric knowledge basemovieactorrelease yeargroundhog daybill murray1993australianicole kidmanxmad max: fury roadx2015fobots. the agent always asks for the value of
the slot with maximum id178 over the remain-
ing entries in the database, which is optimal in
the absence of language understanding errors, and
serves as a baseline against our approach. rein-
forcement learning id63s (rl-
ntm) (zaremba and sutskever, 2015) also allow
neural controllers to interact with discrete external
interfaces. the interface considered in that work
is a one-dimensional memory tape, while in our
work it is an entity-centric kb.

3 probabilistic kb lookup

this section describes a probabilistic framework
for querying a kb given the agent   s beliefs over
the    elds in the kb.

3.1 entity-centric knowledge base (ec-kb)
a knowledge base consists of triples of the form
(h, r, t), which denotes that relation r holds be-
tween the head h and tail t. we assume that
the kb-infobot has access to a domain-speci   c
entity-centric knowledge base (ec-kb) (zwickl-
bauer et al., 2013) where all head entities are of
a particular type (such as movies or persons), and
the relations correspond to attributes of these head
entities. such a kb can be converted to a table
format whose rows correspond to the unique head
entities, columns correspond to the unique relation
types (slots henceforth), and some entries may be
missing. an example is shown in figure 1.

3.2 notations and assumptions
let t denote the kb table described above and
ti,j denote the jth slot-value of the ith entity.
1     i     n and 1     j     m. we let v j denote the
vocabulary of each slot, i.e. the set of all distinct
values in the j-th column. we denote missing val-
ues from the table with a special token and write
ti,j =   . mj = {i : ti,j =   } denotes the set
of entities for which the value of slot j is missing.
note that the user may still know the actual value
of ti,j, and we assume this lies in v j. we do not
deal with new entities or relations at test time.
we assume a uniform prior g     u[{1, ...n}]
over the rows in the table t , and let binary ran-
dom variables   j     {0, 1} indicate whether the
user knows the value of slot j or not. the agent
maintains m multinomial distributions pt
j(v) for
v     v j denoting the id203 at turn t that the
user constraint for slot j is v, given their utterances

1 till that turn. the agent also maintains m bi-
u t
j = pr(  j = 1) which denote the prob-
nomials qt
ability that the user knows the value of slot j.

we assume that column values are indepen-
dently distributed to each other. this is a strong
assumption but it allows us to model the user goal
for each slot independently, as opposed to model-
ing the user goal over kb entities directly. typi-
cally maxj |v j| < n and hence this assumption
reduces the number of parameters in the model.

3.3 soft-kb lookup
let ptt (i) = pr(g = i|u t
1) be the posterior prob-
ability that the user is interested in row i of the
table, given the utterances up to turn t. we as-
sume all probabilities are conditioned on user in-
1 and drop it from the notation below. from
puts u t
our assumption of independence of slot values,
j=1 pr(gj = i), where
pr(gj = i) denotes the posterior id203 of
user goal for slot j pointing to ti,j. marginalizing
this over   j gives:

we can write ptt (i)     (cid:81)m

pr(gj = i) =

(1)

1(cid:88)
pr(gj = i,   j =   )
j pr(gj = i|  j = 1)+
(1     qt

  =0

= qt

j) pr(gj = i|  j = 0).
for   j = 0, the user does not know the value of
the slot, and from the prior:
pr(gj = i|  j = 0) =
1
n

1     i     n (2)
for   j = 1, the user knows the value of slot j, but
this may be missing from t , and we again have
two cases:
pr(gj = i|  j = 1) =

(cid:40) 1

(3)

,

(cid:0)1     |mj|

i     mj
i (cid:54)    mj

(cid:1),

n ,
pt
j (v)
nj (v)

n

here, nj(v) is the count of value v in slot j. de-
tailed derivation for (3) is provided in appendix a.
combining (1), (2), and (3) gives us the procedure
for computing the posterior over kb entities.
4 towards an end-to-end-kb-infobot
we claim that the soft-kb lookup method has two
bene   ts over the hard-kb method     (1) it helps
the agent discover better dialogue policies by pro-
viding it more information from the language un-
derstanding unit, (2) it allows end-to-end training
of both dialogue policy and language understand-
ing in an online setting. in this section we describe
several agents to test these claims.

note the set of tokens in a string x3, then for each
slot in 1     j     m and each value v     v j, we
compute its matching score as follows:
|{w     ut}     {w     v}|

st
j[v] =

|{w     v}|

(4)

figure 2: high-level overview of the end-to-end
kb-infobot. components with trainable parame-
ters are highlighted in gray.

4.1 overview

figure 2 shows an overview of the components of
the kb-infobot. at each turn, the agent receives a
natural language utterance ut as input, and selects
an action at as output. the action space, denoted
by a, consists of m + 1 actions     request(slot=i)
for 1     i     m will ask the user for the value of
slot i, and inform(i) will inform the user with an
ordered list of results i from the kb. the dialogue
ends once the agent chooses inform.

we adopt a modular approach, typical to goal-
oriented dialogue systems (wen et al., 2016b),
consisting of: a belief tracker module for iden-
tifying user intents, extracting associated slots,
and tracking the dialogue state (yao et al., 2014;
hakkani-t  ur et al., 2016; chen et al., 2016b; hen-
derson et al., 2014; henderson, 2015); an inter-
face with the database to query for relevant results
(soft-kb lookup); a summary module to summa-
rize the state into a vector; a dialogue policy which
selects the next system action based on current
state (young et al., 2013). we assume the agent
only responds with dialogue acts. a template-
based natural language generator (id86) can
be easily constructed for converting dialogue acts
into natural language.

4.2 belief trackers

the infobot consists of m belief trackers, one for
each slot, which get the user input xt and produce
j, which we shall collectively
two outputs, pt
j and qt
call the belief state: pt
j is a multinomial distribu-
tion over the slot values v, and qt
j is a scalar prob-
ability of the user knowing the value of slot j. we
describe two versions of the belief tracker.

hand-crafted tracker: we    rst identify men-
tions of slot-names (such as    actor   ) or slot-values
(such as    bill murray   ) from the user input ut, us-
ing token-level keyword search. let {w     x} de-

a similar score bt
j is computed for the slot-names.
a one-hot vector reqt     {0, 1}m denotes the pre-
viously requested slot from the agent, if any. qt
j is
j[v] = 0    v     v j, i.e.
set to 0 if reqt[j] is 1 but st
the agent requested for a slot but did not receive a
valid value in return, else it is set to 1.

starting from an prior distribution p0
the counts of the values in the kb), pt
dated as:

j (based on
j[v] is up-

j + 1(reqt[j] = 1)(cid:1)

(5)

j[v] + bt

[v] + c(cid:0)st

j[v]     pt   1
pt

j

here c is a tuning parameter, and the normaliza-
tion is given by setting the sum over v to 1.
neural belief tracker: for the neural tracker
the user input ut is converted to a vector repre-
sentation xt, using a bag of id165s (with n = 2)
representation. each element of xt is an integer
indicating the count of a particular id165 in ut.
we let v n denote the number of unique id165s,
hence xt     nv n
0 .

recurrent neural networks have been used for
belief tracking (henderson et al., 2014; wen et al.,
2016b) since the output distribution at turn t de-
pends on all user inputs till that turn. we use a
gated recurrent unit (gru) (cho et al., 2014) for
j = 0 com-
each tracker, which, starting from h0
j = gru(x1, . . . , xt) (see appendix b for
putes ht
j     rd can be interpreted as a summary
details). ht
of what the user has said about slot j till turn t.
the belief states are computed from this vector as
follows:

j = softmax(w p
j + bp
pt
j ht
j )
qt
j =   (w   
j ht
j + b  
j )
j     rv j , w   
j     rv j  d, bp

(6)
(7)
j     rd and

here w p
j     r, are trainable parameters.
b  
4.3 soft-kb lookup + summary
this module uses the soft-kb lookup described
in section 3.3 to compute the posterior ptt     rn
over the ec-kb from the belief states (pt
j, qt
j).
3we use the nltk tokenizer available at http://www.

nltk.org/api/nltk.tokenize.html

belief trackerspolicy networkbeliefs summarysoft-kb lookupkb-infobotuseruser utterancesystem action(cid:88)

i:ti,j =  

j(v)     (cid:88)

wt

i:ti,j =v

1, qt

1, pt

m , qt

2, ..., qt

size(cid:80)

collectively, outputs of the belief trackers and the
soft-kb lookup can be viewed as the current dia-
logue state internal to the kb-infobot. let st =
m , ptt ] be the vector of
2, ..., pt
[pt
j v j + m + n denoting this state. it is pos-
sible for the agent to directly use this state vector
to select its next action at. however, the large size
of the state vector would lead to a large number of
parameters in the policy network. to improve ef   -
ciency we extract summary statistics from the be-
lief states, similar to (williams and young, 2005).
each slot is summarized into an id178 statistic
j computed from elements of

over a distribution wt
the kb posterior ptt as follows:

ptt (i) + p0

j (v)

ptt (i) .

(8)
here, p0
j is a prior distribution over the values of
slot j, estimated using counts of each value in the
kb. the id203 mass of v in this distribu-
tion is the agent   s con   dence that the user goal has
value v in slot j. this two terms in (8) correspond
to rows in kb which have value v, and rows whose
value is unknown (weighted by the prior probabil-
ity that an unknown might be v). then the sum-
mary statistic for slot j is the id178 h(wt
j). the
kb posterior ptt is also summarized into an en-
tropy statistic h(ptt ).

the scalar probabilities qt

j are passed as is to
the dialogue policy, and the    nal summary vector
is   st = [h(  pt
m , h(ptt )].
1, ..., qt
note that this vector has size 2m + 1.

1), ..., h(  pt

m ), qt

4.4 dialogue policy
the dialogue policy   s job is to select the next ac-
tion based on the current summary state   st and the
dialogue history. we present a hand-crafted base-
line and a neural policy network.
hand-crafted policy: the rule based policy is
adapted from (wu et al., 2015).
it asks for the
slot   j = arg min h(  pt
j) with the minimum en-
tropy, except if     (i) the kb posterior id178
j ),
h(ptt ) <   r, (ii) h(  pt
j) < min(  t ,   h(  p0
(iii) slot j has already been requested q times.   r,
  t ,   , q are tuned to maximize reward against the
simulator.
neural policy network: for the neural ap-
proach, similar to (williams and zweig, 2016;
zhao and eskenazi, 2016), we use an id56 to al-
low the network to maintain an internal state of

dialogue history. speci   cally, we use a gru unit
followed by a fully-connected layer and softmax
nonlinearity to model the policy    over actions in
a (w        r|a|  d, b       r|a|):

   = gru(  s1, ...,   st)
ht
   = softmax(w   ht

   + b  ) .

(9)
(10)

during training, the agent samples its actions
from the policy to encourage exploration. if this
action is inform(), it must also provide an ordered
set of entities indexed by i = (i1, i2, . . . , ir) in
the kb to the user. this is done by sampling r
items from the kb-posterior ptt . this mimics a
search engine type setting, where r may be the
number of results on the    rst page.

5 training

(cid:105)

(cid:104)(cid:80)h

parameters of the neural components (denoted by
  ) are trained using the reinforce algorithm
(williams, 1992). we assume that the learner has
access to a reward signal rt throughout the course
of the dialogue, details of which are in the next
section. we can write the expected discounted
return of the agent under policy    as j(  ) =
(   is the discounting factor). we
e  
also use a baseline reward signal b, which is the
average of all rewards in a batch, to reduce the
variance in the updates (greensmith et al., 2004).
when only training the dialogue policy    using
this signal, updates are given by (details in ap-
pendix c):

t=0   trt

(cid:104) h(cid:88)

h(cid:88)

(cid:105)

,

t=0

k=0

      log     (ak)

     j(  ) = e  

  t(rt   b)
(11)
for end-to-end training we need to update both
the dialogue policy and the belief trackers using
the reinforcement signal, and we can view the re-
trieval as another policy      (see appendix c). the
updates are given by:
     j(  ) =ea     ,i     

(cid:104)(cid:0)      log     (i)+
      log     (ah)(cid:1) h(cid:88)

  k(rk     b)

h(cid:88)

(cid:105)

,

h=0

k=0

(12)

in the case of end-to-end learning, we found that
for a moderately sized kb, the agent almost al-
ways fails if starting from random initialization.

in this case, credit assignment is dif   cult for the
agent, since it does not know whether the failure
is due to an incorrect sequence of actions or in-
correct set of results from the kb. hence, at the
beginning of training we have an imitation learn-
ing (il) phase where the belief trackers and pol-
icy network are trained to mimic the hand-crafted
agents. assume that   pt
j are the belief states
from a rule-based agent, and   at its action at turn t.
then the id168 for imitation learning is:

j and   qt

j||pt

j(  ))+h(  qt

j, qt

j(  ))   log     (  at)(cid:3)

l(  ) = e(cid:2)d(  pt

d(p||q) and h(p, q) denote the kl divergence
and cross-id178 between p and q respectively.

the expectations are estimated using a mini-
batch of dialogues of size b. for rl we use
rmsprop (hinton et al., 2012) and for il we use
vanilla sgd updates to train the parameters   .

6 experiments and results
previous work in kb-based qa has focused on
single-turn interactions and is not directly compa-
rable to the present study. instead we compare dif-
ferent versions of the kb-infobot described above
to test our claims.

6.1 kb-infobot versions
we have described two belief trackers     (a) hand-
crafted and (b) neural, and two dialogue policies
    (c) hand-crafted and (d) neural.

rule agents use the hand-crafted belief track-
ers and hand-crafted policy (a+c). rl agents use
the hand-crafted belief trackers and the neural pol-
icy (a+d). we compare three variants of both sets
of agents, which differ only in the inputs to the
dialogue policy. the no-kb version only takes
id178 h(  pt
j) of each of the slot distributions.
the hard-kb version performs a hard-kb lookup
and selects the next action based on the id178 of
the slots over retrieved results. this is the same
approach as in wen et al. (2016b), except that
we take id178 instead of summing probabilities.
the soft-kb version takes summary statistics of
the slots and kb posterior described in section 4.
at the end of the dialogue, all versions inform the
user with the top results from the kb posterior ptt ,
hence the difference only lies in the policy for ac-
tion selection. lastly, the e2e agent uses the neu-
ral belief tracker and the neural policy (b+d), with
a soft-kb lookup. for the rl agents, we also ap-
j and a one-hot encoding of the previous
pend   qt

n m maxj |v j|
kb-split
small
277
medium 428
857
large
x-large
3523

17
68
101
251

6
6
6
6

|mj|
20%
20%
20%
20%

table 1: movies-kb statistics for four splits. re-
fer to section 3.2 for description of columns.

agent action to the policy network input. hyperpa-
rameter details for the agents are provided in ap-
pendix d.

6.2 user simulator
training reinforcement learners is challenging be-
cause they need an environment to operate in. in
the dialogue community it is common to use sim-
ulated users for this purpose (schatzmann et al.,
2007a,b; cuay  ahuitl et al., 2005; asri et al., 2016).
in this work we adapt the publicly-available user
simulator presented in li et al. (2016b) to fol-
low a simple agenda while interacting with the
kb-infobot, as well as produce natural language
utterances . details about the simulator are in-
cluded in appendix e. during training, the sim-
ulated user also provides a reward signal at the
end of each dialogue. the dialogue is a success
if the user target is in top r = 5 results re-
turned by the agent; and the reward is computed
as max(0, 2(1     (r     1)/r)), where r is the ac-
tual rank of the target. for a failed dialogue the
agent receives a reward of    1, and at each turn it
receives a reward of    0.1 to encourage short ses-
sions4. the maximum length of a dialogue is 10
turns beyond which it is deemed a failure.

6.3 movies-kb
we use a movie-centric kb constructed using the
imdbpy5 package. we constructed four differ-
ent splits of the dataset, with increasing number of
entities, whose statistics are given in table 1. the
original kb was modi   ed to reduce the number
of actors and directors in order to make the task
more challenging6. we randomly remove 20% of
the values from the agent   s copy of the kb to sim-
ulate a scenario where the kb may be incomplete.
the user, however, may still know these values.

4a turn consists of one user action and one agent action.
5http://imdbpy.sourceforge.net/
6we restricted the vocabulary to the    rst few unique val-
ues of these slots and replaced all other values with a random
value from this set.

agent

small kb

r

medium kb

r

large kb

r

x-large kb

r

soft kb

rule
no kb
rl
hard kb rule
rl
rule
rl
e2e

.43  .02
.50  .02
.42  .02
.53  .02
.51  .02
.62  .02
.50  .02
1.37
table 2: performance comparison. average (  std error) for 5000 runs after choosing the best model
during training. t: average number of turns. s: success rate. r: average reward.

.26  .02
.24  .02
.25  .02
.35  .02
.32  .02
.43  .02
.48  .02
1.64

.74  .02
.87  .02
.75  .02
.86  .02
.83  .02
.98  .02
1.10  .02

.82  .02
.94  .02
.78  .02
.98  .02
.93  .02
1.05  .02
1.10  .02

max

1.78

1.73

t
4.84
3.64
4.84
2.88
4.51
3.65
3.98
3.97

t
5.05
3.32
3.66
3.07
3.94
3.37
3.27
2.96

t
4.93
3.71
4.27
3.53
3.74
3.79
3.51
3.26

t
5.04
2.65
5.04
3.36
2.12
2.93
3.13
3.44

s
.64
.56
.64
.62
.57
.63
.66
1.0

s
.66
.64
.65
.62
.66
.68
.65
1.0

s
.78
.79
.75
.79
.78
.83
.83
1.0

s
.77
.76
.73
.75
.76
.80
.83
1.0

6.4 simulated user evaluation
we compare each of the discussed versions along
three metrics: the average rewards obtained (r),
success rate (s) (where success is de   ned as pro-
viding the user target among top r results), and
the average number of turns per dialogue (t). for
the rl and e2e agents, during training we    x the
model every 100 updates and run 2000 simulations
with greedy action selection to evaluate its perfor-
mance. then after training we select the model
with the highest average reward and run a further
5000 simulations and report the performance in
table 2. for reference we also show the perfor-
mance of an agent which receives perfect informa-
tion about the user target without any errors, and
selects actions based on the id178 of the slots
(max). this can be considered as an upper bound
on the performance of any agent (wu et al., 2015).
in each case the soft-kb versions achieve the
highest average reward, which is the metric all
agents optimize. in general, the trade-off between
minimizing average turns and maximizing success
rate can be controlled by changing the reward sig-
nal. note that, except the e2e version, all versions
share the same belief trackers, but by re-asking
values of some slots they can have different pos-
teriors ptt to inform the results. this shows that
having full information about the current state of
beliefs over the kb helps the soft-kb agent dis-
cover better policies. further, reinforcement learn-
ing helps discover better policies than the hand-
crafted rule-based agents, and we see a higher re-
ward for rl agents compared to rule ones. this is
due to the noisy natural language inputs; with per-
fect information the rule-based strategy is optimal.
interestingly, the rl-hard agent has the minimum
number of turns in 2 out of the 4 settings, at the
cost of a lower success rate and average reward.
this agent does not receive any information about
the uncertainty in id29, and it tends to

figure 3: performance of kb-infobot versions
when tested against real users. left: success rate,
with the number of test dialogues indicated on
each bar, and the p-values from a two-sided per-
mutation test. right: distribution of the number
of turns in each dialogue (differences in mean are
signi   cant with p < 0.01).

inform as soon as the number of retrieved results
becomes small, even if they are incorrect.

among the soft-kb agents, we see that
e2e>rl>rule, except for the x-large kb. for
e2e, the action space grows exponentially with
the size of the kb, and hence credit assignment
gets more dif   cult. future work should focus on
improving the e2e agent in this setting. the dif-
   culty of a kb-split depends on number of enti-
ties it has, as well as the number of unique values
for each slot (more unique values make the prob-
lem easier). hence we see that both the    small   
and    x-large    settings lead to lower reward for
the agents, since maxj |v j|

is small for them.

n

6.5 human evaluation
we further evaluate the kb-infobot versions
trained using the simulator against real subjects,
recruited from the author   s af   liations.
in each
session, in a typed interaction, the subject was    rst
presented with a target movie from the    medium   
kb-split along with a subset of its associated slot-

rl hardrule softrl softe2e soft0.40.50.60.70.80.91.0success ratep=0.01nsp=0.03109105121103rl hardrule softrl softe2e soft12345678910# turnsfigure 4: sample dialogues between users and the kb-infobot (rl-soft version). each turn begins
with a user utterance followed by the agent response. rank denotes the rank of the target movie in the
kb-posterior after each turn.

values from the kb. to simulate the scenario
where end-users may not know slot values cor-
rectly, the subjects in our evaluation were pre-
sented multiple values for the slots from which
they could choose any one while interacting with
the agent. subjects were asked to initiate the con-
versation by specifying some of these values, and
respond to the agent   s subsequent requests, all in
natural language. we test rl-hard and the three
soft-kb agents in this study, and in each session
one of the agents was picked at random for test-
ing. in total, we collected 433 dialogues, around
20 per subject. figure 3 shows a comparison of
these agents in terms of success rate and number of
turns, and figure 4 shows some sample dialogues
from the user interactions with rl-soft.

in comparing hard-kb versus soft-kb lookup
methods we see that both rule-soft and rl-soft
agents achieve a higher success rate than rl-hard,
while e2e-soft does comparably. they do so in an
increased number of average turns, but achieve a
higher average reward as well. between rl-soft
and rule-soft agents, the success rate is similar,
however the rl agent achieves that rate in a lower
number of turns on average. rl-soft achieves a
success rate of 74% on the human evaluation and
80% against the simulated user, indicating mini-
mal over   tting. however, all agents take a higher
number of turns against real users as compared to
the simulator, due to the noisier inputs.

the e2e gets the highest success rate against
the simulator, however, when tested against real
users it performs poorly with a lower success
rate and a higher number of turns. since it has
more trainable components, this agent is also most
prone to over   tting. in particular, the vocabulary
of the simulator it is trained against is quite lim-
ited (v n = 3078), and hence when real users

figure 5: average rewards against simulator as
temperature of softmax in id86 output
is in-
creased. higher temperature leads to more noise
in output. average over 5000 simulations after se-
lecting the best model during training.

provided inputs outside this vocabulary, it per-
formed poorly.
in the future we plan to    x this
issue by employing a better architecture for the
language understanding and belief tracker com-
ponents hakkani-t  ur et al. (2016); liu and lane
(2016); chen et al. (2016b,a), as well as by pre-
training on separate data.

while its generalization performance is poor,
the e2e system also exhibits the strongest learn-
ing capability. in figure 5, we compare how dif-
ferent agents perform against the simulator as the
temperature of the output softmax in its id86 is in-
creased. a higher temperature means a more uni-
form output distribution, which leads to generic
simulator responses irrelevant to the agent ques-
tions. this is a simple way of introducing noise
in the utterances. the performance of all agents
drops as the temperature is increased, but less
so for the e2e agent, which can adapt its belief
tracker to the inputs it receives. such adaptation

turndialoguerankdialoguerankdialoguerank1can i get a movie directed by maiellaro75find a movie directed by hemecker7peter greene acted in a family comedy - what was it?35request actorrequest actorrequest actor2neal2i dont know7peter28request mpaa_ratingrequest mpaa_ratingrequest mpaa_rating3not sure about that2i dont know7i don't know that28request critic_ratingrequest critic_ratingrequest critic_rating4i don't remember27.613the critics rated it as 6.53request genrerequest critic_ratinginform5i think it's a crime movie17.923informrequest critic_rating67.741inform1.01.52.0id86 temperature0.20.40.60.81.01.2average rewardrl hardrl softend2endis key to the personalization of dialogue agents,
which motivates us to introduce the e2e agent.
7 conclusions and discussion
this work is aimed at facilitating the move to-
wards end-to-end trainable dialogue agents for in-
formation access. we propose a differentiable
probabilistic framework for querying a database
given the agent   s beliefs over its    elds (or slots).
we show that such a framework allows the down-
stream reinforcement learner to discover better di-
alogue policies by providing it more information.
we also present an e2e agent for the task, which
demonstrates a strong learning capacity in simula-
tions but suffers from over   tting when tested on
real users. given these results, we propose the
following deployment strategy that allows a dia-
logue system to be tailored to speci   c users via
learning from agent-user interactions. the system
could start off with an rl-soft agent (which gives
good performance out-of-the-box). as the user in-
teracts with this agent, the collected data can be
used to train the e2e agent, which has a strong
learning capability. gradually, as more experience
is collected, the system can switch from rl-soft
to the personalized e2e agent. effective imple-
mentation of this, however, requires the e2e agent
to learn quickly and this is the research direction
we plan to focus on in the future.
acknowledgements
we would like to thank dilek hakkani-t  ur and re-
viewers for their insightful comments on the pa-
per. we would also like to acknowledge the vol-
unteers from carnegie mellon university and mi-
crosoft research for helping us with the human
evaluation. yun-nung chen is supported by the
ministry of science and technology of taiwan un-
der the contract number 105-2218-e-002-033, in-
stitute for information industry, and mediatek.

references
layla el asri, jing he, and kaheer suleman. 2016.
a sequence-to-sequence model for user simula-
arxiv preprint
tion in spoken dialogue systems.
arxiv:1607.00070 .

antoine bordes and jason weston. 2016.

learn-
ing end-to-end goal-oriented dialog. arxiv preprint
arxiv:1605.07683 .

yun-nung chen, dilek hakanni-t  ur, gokhan tur, asli
celikyilmaz, jianfeng guo, and li deng. 2016a.

syntax or semantics? knowledge-guided joint se-
mantic frame parsing.

yun-nung chen, dilek hakkani-t  ur, gokhan tur,
jianfeng gao, and li deng. 2016b. end-to-end
memory networks with knowledge carryover for
multi-turn spoken language understanding. in pro-
ceedings of the 17th annual meeting of the interna-
tional speech communication association.

kyunghyun cho, bart van merri  enboer, caglar gul-
cehre, dzmitry bahdanau, fethi bougares, holger
schwenk, and yoshua bengio. 2014.
learning
phrase representations using id56 encoder-decoder
for id151. emnlp .

heriberto cuay  ahuitl, steve renals, oliver lemon, and
hiroshi shimodaira. 2005. human-computer dia-
logue simulation using id48.
in
automatic id103 and understanding,
2005 ieee workshop on. ieee, pages 290   295.

m ga  si  c, catherine breslin, matthew henderson,
dongho kim, martin szummer, blaise thomson,
pirros tsiakoulis, and steve young. 2013. on-
line policy optimisation of bayesian spoken dialogue
systems via human interaction. in 2013 ieee inter-
national conference on acoustics, speech and sig-
nal processing. ieee, pages 8367   8371.

peter w glynn. 1990. likelihood ratio gradient esti-
mation for stochastic systems. communications of
the acm 33(10):75   84.

evan greensmith, peter l bartlett, and jonathan bax-
ter. 2004. variance reduction techniques for gradi-
ent estimates in id23. journal of
machine learning research 5(nov):1471   1530.

dilek hakkani-t  ur, gokhan tur, asli celikyilmaz,
yun-nung chen, jianfeng gao, li deng, and ye-
yi wang. 2016. multi-domain joint semantic frame
in pro-
parsing using bi-directional id56-lstm.
ceedings of the 17th annual meeting of the interna-
tional speech communication association.

matthew henderson. 2015. machine learning for dia-
log state tracking: a review. machine learning in
spoken language processing workshop .

matthew henderson, blaise thomson, and steve
young. 2014. word-based dialog state tracking with
in proceedings of the
recurrent neural networks.
15th annual meeting of the special interest group
on discourse and dialogue (sigdial). pages 292   
299.

geoffrey hinton, n srivastava, and kevin swersky.
2012. lecture 6a overview of mini   batch gradi-
ent descent. coursera lecture slides https://class.
coursera. org/neuralnets-2012-001/lecture,[online .

jiwei li, will monroe, alan ritter, michel galley,
jianfeng gao, and dan jurafsky. 2016a. deep
reinforcement
learning for dialogue generation.
emnlp .

xiujun li, zachary c lipton, bhuwan dhingra, lihong
li, jianfeng gao, and yun-nung chen. 2016b. a
user simulator for task-completion dialogues. arxiv
preprint arxiv:1612.05688 .

xuijun li, yun-nung chen, lihong li, and jianfeng
gao. 2017. end-to-end task-completion neural dia-
logue systems. arxiv preprint arxiv:1703.01008 .

bing liu and ian lane. 2016. attention-based recur-
rent neural network models for joint intent detection
and slot    lling. interspeech 2016 pages 685   689.

sebastian m  oller, roman englert, klaus-peter engel-
brecht, verena vanessa hafner, anthony jameson,
antti oulasvirta, alexander raake, and norbert re-
ithinger. 2006. memo: towards automatic usability
evaluation of spoken dialogue services by user error
simulations. in interspeech.

jost schatzmann, blaise thomson, karl weilhammer,
hui ye, and steve young. 2007a. agenda-based
user simulation for id64 a pomdp dialogue
in human language technologies 2007:
system.
the conference of the north american chapter
of the association for computational linguistics;
companion volume, short papers. association for
computational linguistics, pages 149   152.

jost schatzmann, blaise thomson, and steve young.
2007b. statistical user simulation with a hidden
agenda. proc sigdial, antwerp 273282(9).

konrad schef   er and steve young. 2002. automatic
learning of dialogue strategy using dialogue simu-
in proceedings
lation and id23.
of the second international conference on human
language technology research. morgan kaufmann
publishers inc., pages 12   19.

amanda spink, dietmar wolfram, major bj jansen,
and tefko saracevic. 2001. searching the web: the
public and their queries. journal of the association
for information science and technology 52(3):226   
234.

marilyn a walker, diane j litman, candace a kamm,
and alicia abella. 1997. paradise: a framework for
evaluating spoken dialogue agents. in proceedings
of the eighth conference on european chapter of the
association for computational linguistics. associa-
tion for computational linguistics, pages 271   280.

tsung-hsien wen, milica ga  si  c, nikola mrk  si  c,
lina m. rojas-barahona, pei-hao su, stefan ultes,
david vandyke, and steve young. 2016a. condi-
tional generation and snapshot learning in neural di-
alogue systems. emnlp .

tsung-hsien wen, milica ga  si  c, nikola mrk  si  c,
lina m. rojas-barahona, pei-hao su, stefan ultes,
david vandyke, and steve young. 2016b.
a
network-based end-to-end trainable task-oriented di-
alogue system. arxiv preprint arxiv:1604.04562 .

tsung-hsien wen, milica ga  si  c, nikola mrk  si  c, pei-
hao su, david vandyke, and steve young. 2015.
semantically conditioned lstm-based natural lan-
guage generation for spoken dialogue systems.
emnlp .

jason d williams and steve young. 2005. scaling
up pomdps for dialog management: the    sum-
in ieee workshop on
mary pomdp    method.
automatic id103 and understanding,
2005.. ieee, pages 177   182.

jason d williams and geoffrey zweig. 2016. end-
to-end lstm-based dialog control optimized with su-
pervised and id23. arxiv preprint
arxiv:1606.01269 .

ronald j williams. 1992. simple statistical gradient-
following algorithms for connectionist reinforce-
ment learning. machine learning 8(3-4):229   256.

ji wu, miao li, and chin-hui lee. 2015. a proba-
bilistic framework for representing id71
and id178-based dialog management through dy-
namic stochastic state evolution. ieee/acm trans-
actions on audio, speech, and language processing
23(11):2026   2035.

kaisheng yao, baolin peng, yu zhang, dong yu, ge-
offrey zweig, and yangyang shi. 2014. spoken lan-
guage understanding using long short-term memory
in spoken language technology
neural networks.
workshop (slt), 2014 ieee. ieee, pages 189   194.

jun yin, xin jiang, zhengdong lu, lifeng shang,
hang li, and xiaoming li. 2016a. neural gener-
ative id53. international joint con-
ference on arti   cial intelligence .

pengcheng yin, zhengdong lu, hang li, and ben kao.
2016b. neural enquirer: learning to query tables.
international joint conference on arti   cial intelli-
gence .

steve young, milica ga  si  c, blaise thomson, and ja-
son d williams. 2013. pomdp-based statistical
spoken id71: a review. proceedings of
the ieee 101(5):1160   1179.

wojciech zaremba and ilya sutskever. 2015. rein-
forcement learning id63s-revised.
arxiv preprint arxiv:1505.00521 .

tiancheng zhao and maxine eskenazi. 2016. to-
wards end-to-end learning for dialog state tracking
and management using deep id23.
arxiv preprint arxiv:1606.02560 .

stefan zwicklbauer, christin seifert, and michael
granitzer. 2013. do we need entity-centric knowl-
in pro-
edge bases for entity disambiguation?
ceedings of the 13th international conference on
knowledge management and knowledge technolo-
gies. acm, page 4.

pr(gj = i)
= pr(gj (cid:54)    mj) pr(gj = i|gj (cid:54)    mj)

(cid:18)
1     |mj|

(cid:19)

n

=

   pt

j(v)
nj(v)

,

(14)

j(  ) = e

a posterior derivation

here, we present a derivation for equation 3, i.e.,
the posterior over the kb slot when the user knows
the value of that slot. for brevity, we drop   j = 0
from the condition in all probabilities below. for
the case when i     mj, we can write:

pr(gj = i)
= pr(gj     mj) pr(gj = i|gj     mj)

|mj|
n

=

1
|mj| =

1
n

,

(13)

where we assume all missing values to be equally
likely, and estimate the prior id203 of the
goal being missing from the count of missing val-
ues in that slot. for the case when i = v (cid:54)    mj:

where the second term comes from taking the
id203 mass associated with v in the belief
tracker and dividing it equally among all rows with
value v.

we can also verify that the above distribution is

valid: i.e., it sums to 1:

(cid:88)

(cid:88)
(cid:88)

i   mj

i

=

=

=

i   mj
|mj|
n
|mj|
n
|mj|
n
= 1 .

=

=

pr(gj = i)

pr(gj = i) +

pr(gj = i)

n

(cid:88)
i(cid:54)   mj
1     |mj|
(cid:19) (cid:88)
(cid:19)(cid:88)
(cid:19)

i(cid:54)   mj

i   v j
   1

(cid:18)

+

(cid:88)
(cid:18)
i(cid:54)   mj
1     |mj|
(cid:18)
1     |mj|
(cid:18)
1     |mj|

n

n

n

1
n

+

+

+

(cid:19) pt

j(v)
#jv

pt
j(v)
#jv

#jv

pt
j(v)
#jv

b id149

a gated recurrent unit (gru) (cho et al., 2014)
is a recurrent neural network which operates on an
input sequence x1, . . . , xt. starting from an initial

state h0 (usually set to 0 it iteratively computes the
   nal output ht as follows:

rt =   (wrxt + urht   1 + br)
zt =   (wzxt + uzht   1 + bz)
  ht = tanh(whxt + uh(rt (cid:12) ht   1) + bh)
ht = (1     zt) (cid:12) ht   1 + zt (cid:12)   ht .

(15)
here    denotes the sigmoid nonlinearity, and (cid:12) an
element-wise product.

c reinforce updates

we assume that the learner has access to a reward
signal rt throughout the course of the dialogue, de-
tails of which are in the next section. we can write
the expected discounted return of the agent under
policy    as follows:

(cid:35)

(cid:34) h(cid:88)

t=0

  trt

(16)

here, the expectation is over all possible trajecto-
ries    of the dialogue,    denotes the trainable pa-
rameters of the learner, h is the maximum length
of an episode, and    is the discounting factor. we
can use the likelihood ratio trick (glynn, 1990) to
write the gradient of the objective as follows:

     j(  ) = e

      log p  (   )

  trt

,

(17)

(cid:35)

h(cid:88)

t=0

where p  (   ) is the id203 of observing a par-
ticular trajectory under the current policy. with a
markovian assumption, we can write

p  (   ) = p(s0)

p(sk+1|sk, ak)    (ak|sk),

(18)
where    denotes dependence on the neural net-
work parameters. from 17,18 we obtain

     j(  ) = ea     

      log     (ak)

  trt

,

(19)
if we need to train both the policy network and
the belief trackers using the reinforcement signal,
we can view the kb posterior ptt as another pol-
icy. during training then, to encourage explo-
ration, when the agent selects the inform action we

(cid:105)

h(cid:88)

t=0

(cid:34)

h(cid:89)

k=0

(cid:104) h(cid:88)

k=0

we convert dialogue acts from the user into nat-
ural language utterances using a separately trained
natural language generator (id86). the id86 is
trained in a sequence-to-sequence fashion, us-
ing conversations between humans collected by
crowd-sourcing.
it takes the dialogue actions
(das) as input, and generates template-like sen-
tences with slot placeholders via an lstm de-
coder. then, a post-processing scan is performed
to replace the slot placeholders with their actual
values, which is similar to the decoder module in
(wen et al., 2015, 2016a). in the lstm decoder,
we apply id125, which iteratively consid-
ers the top k best sentences up to time step t when
generating the token of the time step t + 1. for the
sake of the trade-off between the speed and perfor-
mance, we use the beam size of 3 in the following
experiments.

there are several sources of error in user utter-
ances. any value provided by the user may be cor-
rupted by noise, or substituted completely with an
incorrect value of the same type (e.g.,    bill mur-
ray    might become just    bill    or    tom cruise   ).
the id86 described above is inherently stochas-
tic, and may sometimes generate utterances irrel-
evant to the agent request. by increasing the tem-
perature of the output softmax in the id86 we can
increase the noise in user utterances.

sample r results from the following distribution to
return to the user:

  (i) = ptt (i1)    ptt (i2)
1     ptt (i1)

          .

(20)

this formulation also leads to a modi   ed ver-
sion of the episodic reinforce update rule
(williams, 1992). speci   cally, eq. 18 now be-
comes,

(cid:34)

h(cid:89)

p  (   ) =

p(s0)

p(sk+1|sk, ak)    (ak|sk)

    (i),

(cid:35)

k=0

(21)
notice the last term      above which is the poste-
rior of a set of results from the kb. from 17,21 we
obtain
     j(  ) =ea     ,i     

(cid:104)(cid:0)      log     (i)+
      log     (ah)(cid:1) h(cid:88)

(22)

,

h(cid:88)

  krk

(cid:105)

h=0

k=0

d hyperparameters
we use gru hidden state size of d = 50 for the
rl agents and d = 100 for the e2e, a learning
rate of 0.05 for the imitation learning phase and
0.005 for the id23 phase, and
minibatch size 128. for the rule agents, hyperpa-
rameters were tuned to maximize the average re-
ward of each agent in simulations. for the e2e
agent, imitation learning was performed for 500
updates, after which the agent switched to rein-
forcement learning. the input vocabulary is con-
structed from the id86 vocabulary and bigrams in
the kb, and its size is 3078.

e user simulator
at the beginning of each dialogue, the simulated
user randomly samples a target entity from the ec-
kb and a random combination of informable slots
for which it knows the value of the target. the re-
maining slot-values are unknown to the user. the
user initiates the dialogue by providing a subset
of its informable slots to the agent and requesting
for an entity which matches them. in subsequent
turns, if the agent requests for the value of a slot,
the user complies by providing it or informs the
agent that it does not know that value. if the agent
informs results from the kb, the simulator checks
whether the target is among them and provides the
reward.

