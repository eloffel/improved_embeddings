6
1
0
2

 

g
u
a
1
1

 

 
 
]
l
c
.
s
c
[
 
 

1
v
8
4
4
3
0

.

8
0
6
1
:
v
i
x
r
a

sex, drugs, and violence

stefania raimondo

frank rudzicz

department of computer science

toronto rehabilitation institute-uhn, and

university of toronto

department of computer science

toronto, ca

sraimond@cs.toronto.edu

university of toronto

toronto, ca

frank@cs.toronto.edu

abstract

automatically detecting inappropriate content
can be a di   cult nlp task, requiring under-
standing context and innuendo, not just iden-
tifying speci   c keywords. due to the large
quantity of online user-generated content, au-
tomatic detection is becoming increasingly nec-
essary. we take a largely unsupervised ap-
proach using a large corpus of narratives from
a community-based self-publishing website and
a small segment of crowd-sourced annotations.
we explore topic modelling using latent dirich-
let allocation (and a variation), and use these
to regress appropriateness ratings, e   ectively
automating rating for suitability. the results
suggest that certain topics inferred may be use-
ful in detecting latent inappropriateness     yield-
ing recall up to 96% and low regression errors.

1 introduction

the internet has precipitated an explosion of shared
user-generated content, with unrestrained access to
edit boxes from barely vetted online community
members. while this freedom is a boon to many web-
sites, it can lead to abuse if content is inappropriate
to the context or viewership. unfortunately, massive
volume makes manual moderation impractical.

if inappropriate content is to be    agged or cen-
sored automatically, identifying keywords may be in-
su   cient. neither innuendo, ever changing slang,
nor misspellings can be detected by    nite lists of
words, which themselves may vary in o   ensiveness
depending on context. a further complication is
the paucity of annotated inappropriate data. in this
work, we present a large corpus of narrative content
published in the online community wattpad, aug-
mented by crowd-sourced annotations. while those
annotations are relatively sparse, they provide an op-

portunity to explore this domain using both unsuper-
vised and partially-supervised methods.

this paper provides a preliminary exploration of
these data using id44 (lda)
and an extension, partially-labeled dirichlet alloca-
tion (plda), in order to take advantage of the newly
acquired labels. speci   cally, the texts are classi-
   ed according to their degree of inappropriateness
in three separate categories: sex, violence, and sub-
stance abuse. our goals are to determine a) whether
lda can be adapted for inappropriateness detection
on narrative data, and b) whether plda can e   ec-
tively make use of sparse annotations in this domain.

2 background and related work

little work has been done on automatically detecting
inappropriate or o   ensive content. thus, major so-
cial media websites such as youtube and facebook
primarily rely on community moderation, in which
users       ag    inappropriate content which is subse-
quently dealt with by the organization.this approach
does not guarantee the timely removal of content.

some existing work aimed to detect short o   ensive
or abusive messages called       ames   . this included
the seminal work by spertus (1997), who used a 47-
element vector of hand-tuned syntactic and semantic
features in a decision tree that achieved     65%    ame-
detection and 98% clean-text detection accuracy.
razavi et al. (2010) used an adaptive three-tiered
classi   cation scheme using a bag-of-words and a dic-
tionary of abusive/insulting words, phrases and ex-
pressions. mahmud et al. (2008) also used keywords
and shallow syntactic rules. while    ames are a sub-
set of inappropriate content, these methods are un-
able to detect mature content, which is more related
to subject matter and not restricted to short mes-
sages. chen et al. (2012) and xu and zhu (2010)
also use hand-engineered syntactic features and key-
words to detect a limited category of o   ensive

passages, e.g., by de   ning o   ensive text as con-
taining lexical pejoratives, profanities, or obsceni-
ties. using message-level lexicosyntactic features
extracted from relatively short youtube comments,
chen et al. (2012) obtained at most 98.2% precision
and 94.3% recall, beating n -gram models, although
this depended on predetermined measures of word-
level    o   ensiveness    and no latent semantics capable
of capturing innuendo.

while research in this area has been minimal, it
falls under the larger umbrella of higher-level unsu-
pervised textual analysis of sentiment, opinion, and
subjectivity (liu, 2010; razavi et al., 2010). un-
supervised methods often rely on word contexts
and include methods of determining word embed-
dings and on topic modelling (blei et al., 2003).
xiang et al. (2012) used a combination of topic mod-
elling and keyword features as input in a few super-
vised o   ensiveness classi   ers (e.g., svn, random for-
est). they    bootstrapped    a set of labelled tweets
from known    o   ensive users    and seed words and
found a 5.4% increase in f1 over the baseline when
the number of topics was increased to 50, albeit us-
ing only short tweets, and focusing on    bad words   
rather than inappropriate content.

3 data

narrative data were shared by wattpad1, which is a
popular site for social storytelling. these data con-
sist of excerpts from short stories written by mem-
bers of the community, partitioned into a basic
subset from wattpad   s main website (2, 129, 156 ex-
cerpts, avg. 931 words/excerpt), and an afterdark
subset2 intended for ages 17+, and more mature au-
diences (47, 407 excerpts, avg. 1597 words/excerpt).
for the purposes of this initial analysis, approxi-
mately 21,000 excerpts are randomly selected from
the former and 10,400 from the latter. these two
subsets are then each split equally into training and
test sets.

additionally, over 1000 excerpts were each scored
by three annotators for mature content using the
crowdflower crowd-sourcing service3. annotators
were asked to identify speci   c segments within texts
which contained    inappropriate    or    mature    con-
tent, along each of three dimensions (sex, substance
abuse, and violence) according to the following 4-
point scale:

1. none: no inappropriate content.

1https://www.wattpad.com/
2https://www.wattpad.com/afterdark/
3https://www.crowd   ower.com/

2. pg: mildly suggestive content or language. not

suitable for children or a professional setting.

3. mature: content appropriate only for a mature

audience, as in an r-rated movie.

4. adult/xxx:

segment

contains

explicit,

graphic, or disturbing content.

annotated excerpts were similarly split into train-
ing and test sets, with 1/3 of the data compris-
ing the latter. the distribution of ratings in the
training and test sets is provided in table 1. ad-
ditionally, 289 texts in training and 127 in testing
were deemed to be completely appropriate for all
audiences. fleiss    inter-annotator agreement statis-
tic gives    = 0.13, p < 0.01 on sexual content,
   = 0.01, p = 0.33 on substance abuse, and    =
0.17, p < 0.01 on violence     i.e. there was only acci-
dental agreement on the rating of drugs. pearson   s
correlation, which is more sensitive to ordinal values
than fleiss, generally agrees, with pairwise annota-
tor agreement    > 0.20 for sex and violence, but
   < 0.12 for substance abuse.

table 1: rating frequencies in annotated data

sex

drugs

violence

rating train test train test train test

2
3
4

163
81
12

80
81
9

56
31
3

43
19
8

203
70
14

131
57
15

4 methods

id44 (lda) is a generative
probabilistic model which represents each docu-
ment as a    nite mixture over a set of topics
(blei et al., 2003). the only observed variables in
our model are the words, wij , which can be inferred
using id150 and expectation propagation
(blei et al., 2003), associated with inferred weight-
ings on topics. the number of topics and smoothing
hyper-parameters are empirically selected.

partially-labeled dirichlet allocation (plda) in-
corporates per-document labels into the topic model
of lda (ramage et al., 2011). plda assumes that
each label is associated with some number of topics
and that each topic is associated with a single la-
bel. thus, plda is appropriate for applications in
which labels are comprehensive and indicate distinct
content. like lda, it associates individual words in
a document with topics. while plda uses labeled
data, topics are still attributed using the same unsu-
pervised approach. it also di   ers from lda in that
each topic is considered to be an amalgamation of
sub-topics.

5 experiments

first, non-english texts are removed and text is
tokenized using the id32 3 tokenizer4.
punctuation is ignored along with 587 english stop
words (e.g., pronouns, conjunctions). proper names
can appear in many of the learned topics, since
they are important    subject    words. we also dis-
card documents with fewer than 20 words, and con-
sider only tokens which appear in d documents,
where d is selected for each experiment empirically.
lda and plda were performed using stanford   s
topic-modelling toolbox5. this implementation does
not optimize the term and topic smoothing hyper-
parameters, which were both set to 0.01 as suggested
by the authors of the toolbox.

5.1 lda on the unannotated corpus

in this experiment, lda is applied to the training
sets of basic and afterdark in order to learn a
representative set of combined topics. the inten-
tion is to determine whether there is a distinction
in topics associated with the two categories, and if
this distinction is also present in the    test    sets. a
reasonable way of selecting the number of topics is
by using corpus perplexity (blei et al., 2003). per-
plexity is expected to decrease as the number of top-
ics increases. as the model over   ts to the training
data, the perplexity of the test data is expected to
either decrease signi   cantly more slowly or begin to
increase; from 1, this occurs around     50 topics, so
we set n appropriately. furthermore, the minimum
number of documents in which a word may occur is
empirically set to 15 in order to reduce proper names
from dominating topics in the models with fewer top-
ics.

some of the top words associated with each topic
are provided in the supplemental material along with
inferred labels. a few observations can be made by
   slicing    the output according to the 4-point scale
(with the afterdark texts as a    fth point) and con-
sidering the per-document topic distribution in each
slice. sixteen topics di   er in occurrence by more
than 50% between the topics in the afterdark and
basic slices, with six being more prevalent in after-
dark including body/sexual (e.g., hips, hard), inter-
action (physical and otherwise, e.g., hands, feel), and
   crushing    (e.g., moment, stared).

5.2 plda on the annotated corpus

relied on wattpad   s
the previous
keyword-based rating system, which may not cap-

experiment

4http://nlp.stanford.edu/software/tokenizer.shtml
5http://nlp.stanford.edu/software/tmt/tmt-0.4/

y
t
i
x
e
p
r
e
p

l

2850

2800

2750

2700

2650

2600

2550

2500

2450

2400

2350

0

training
validation

10

20

30

40

50

60

70

80

# of topics

figure 1: perplexity of lda models on wattpad
training and validation data.

ture latent obscenity.
in this section, we pro-
vide crowd-sourced annotations to plda for each
text, consisting of the four labels of degree (see
section 3) provided by the annotators. following
miura et al. (2013), the    appropriate    label is added
to the set for each unlabeled text since annotators did
not label them otherwise. this provides texts with
explicit    appropriate    labels, similar to using a single
latent topic, which can appear in all documents.

here, we explore the space for the three hyper-
parameters using grid-search, speci   cally the mini-
mum number of documents per allowable token (min-
doc)     {1, 5, 10, 15}, the number of background top-
ics (n-bg)     {0, 1, 5, 10}, and the number of topics
per rating category and level (n-label)     {1, 5, 10}.

table 2 shows the precision, recall, speci   city, and
f1 score, averaged over level, for each category, us-
ing a threshold of 5% (there was no signi   cant e   ect
of moving this threshold from 0.1% to 10%). re-
call is generally in-line with previous work, albeit on
a more challenging task of topic modelling, rather
than keyword spotting. across all parameters, pre-
cision is <52%, as labels for inappropriateness tend
to be over-applied. this is especially true for the
   drugs    category, for which there are the fewest exam-
ples. although not shown, the distribution of error
rates over the individual rating levels indicates that
errors arise predominantly from level 4 (i.e., highly
inappropriate). since this analysis treats the labels
as nominal, interchangeable classes rather than lying
on a continuum, we also perform a regression analy-
sis in the next section.

5.3 regression on ratings

an eventual aim of this work is to automatically pro-
vide meaningful ratings for each of sexual, violent,
and drug-abuse content, to be used in practice. for
this experiment, we trained multilinear regressors for
each plda model, for each of sex, drugs, and vio-
lence separately using support vector machines with
stochastic conjugate id119 table 3 shows
the absolute distance between inferred and average
human ratings, for each category. the weighted av-
erage multiplies the contribution of each rating er-
ror by the inverse of that rating   s prevalence in the
data; e.g.,    appropriate    texts were weighted lower,
since there were more of them in the data. the ta-
ble also shows a    total    row which selects the best
plda model by averaging the errors for each rating
and the average error across rating levels (i.e., each
column in the table) for the sex, drugs, and violence
regressors trained on that plda model. the classi-
   cation rates are clearly lowest for the extremes of
rating levels 1 and 4. overall, the regression gives ab-
solute errors < 1 for each of the three categories of
content; while promising, future work with more an-
notated data and non-linear models is recommended.

sex
viol.
drugs
total

ntopics

9
20
19
19

1
1.74
1.55
1.42
1.5

2
0.69
0.53
0.44
0.48

3
0.34
0.47
0.54
0.55

4 w.avg.
0.77
0.73
0.83
0.77

1.19
0.65
0.91
1.34

table 3: lowest absolute error between inferred and
average human ratings, for each category, including
associated ntopics.

6 discussion and conclusion

these pilot studies in topic modelling and regression
demonstrate promise in the automatic detection of in-
appropriateness beyond mere keyword spotting. us-
ing unsupervised lda, as in section 5.1, appears to
distinguish subject matter that could easily be re-
lated to inappropriate content, as indicated by the
identi   cation of latent topics. the degree to which a
topic-based model identi   es inappropriate passages
missed by simplistic keyword spotting is the subject
of future work. as is typical, topics are open to qual-
itative interpretation, and additional manual valida-
tion is required. to a large extent, this work is in-
comparable to previous work since it is the    rst in
this space to capture latent semantics.

plda obviates the need for some manual con   r-
mation, and is largely successful, as it allows for la-
bel information to be directly incorporated into the

model, which guides topic selection. from the clas-
si   cation results of section 5.2, it appears as though
models bene   t from the ability of plda to associate
multiple topics with a single label. these models ac-
curately identify inappropriate content in test data
for both sex and violence, with some over-estimation.
this may partially be due variance in crowd-sourced
annotations, as indicated by fleiss    statistics below
0.2 in all cases (section 3).

further exploration in topic modelling of inappro-
priate content should aim to reduce false positive
rates. the plda model may also be incorporated
into a larger model that incorporates uncertainty into
the provided labels. expanding the set of annotated
data may also yield further improvements, and meth-
ods of id64, potentially starting with the
lda results, may be useful in overcoming the lack
of such data. regardless, the lda-approach pro-
vides a reasonable baseline for further explorations
and model development for the automatic detection
of inappropriate latent topics in narrative content.

references

[blei et al.2003] david m blei, andrew y ng, and
michael i jordan. 2003. id44.
the journal of machine learning research, 3:993   1022.
[chen et al.2012] ying chen, yilu zhou, sencun zhu, and
heng xu. 2012. detecting o   ensive language in so-
cial media to protect adolescent online safety. in pri-
vacy, security, risk and trust (passat), 2012 inter-
national conference on and 2012 international con-
fernece on social computing (socialcom), pages 71   
80. ieee.

[liu2010] bing liu. 2010. id31 and subjec-
tivity. handbook of natural language processing, 2:627   
666.

[mahmud et al.2008] altaf mahmud, kazi zubair ahmed,
and mumit khan. 2008. detecting    ames and insults
in text. in proceedings of the sixth international con-
ference on natural language processing.

[miura et al.2013] yasuhide miura, keigo hattori, and
tomoko ohkuma. 2013. id96 with sen-
timent clues and relaxed labeling schema. in pro-
ceedings of the 3rd workshop on id31
where ai meets psychology (saaip 2013).

[ramage et al.2011] daniel ramage, christopher d. man-
ning, and susan dumais. 2011. partially labeled
topic models for interpretable id111. in pro-
ceedings of the 17th acm sigkdd international con-
ference on knowledge discovery and data mining,
kdd    11, pages 457   465, new york, ny, usa. acm.
[razavi et al.2010] amir h razavi, diana inkpen, sasha
uritsky, and stan matwin. 2010. o   ensive language
detection using multi-level classi   cation. in advances
in arti   cial intelligence, pages 16   27. springer.

[spertus1997] ellen spertus. 1997. smokey: automatic
recognition of hostile messages.
in proceedings of
the fourteenth national conference on arti   cial in-
telligence and ninth conference on innovative ap-
plications of arti   cial intelligence, aaai   97/iaai   97,
pages 1058   1065, providence, rhode island. aaai
press.

[xiang et al.2012] guang xiang, bin fan, ling wang, ja-
son hong, and carolyn rose. 2012. detecting o   en-
sive tweets via topical feature discovery over a large
scale twitter corpus. in proceedings of the 21st acm
international conference on information and knowl-
edge management, pages 1980   1984. acm.

[xu and zhu2010] zhi xu and sencun zhu. 2010. filter-
ing o   ensive language in online communities using
grammatical relations. proceedings of collaboration,
electronic messaging, anti-abuse and spam confer-
ence 2010.

table 2: plda model classi   cation results for aggregated rating levels, threshold=5%

bg-topics
n-topics

precision
recall/sensitivity
speci   city
f1

precision
recall/sensitivity
speci   city
f1

precision
recall/sensitivity
speci   city
f1

0
1

0.36
0.75
0.41
0.48

0.5
0.79
0.31
0.61

0.2
0.68
0.35
0.31

5

0.33
0.89
0.19
0.48

0.5
0.94
0.18
0.65

0.21
0.57
0.51
0.31

10

0.32
0.96
0.1
0.48

0.48
0.92
0.13
0.63

0.19
0.46
0.55
0.27

1
1

0.34
0.71
0.4
0.46

0.52
0.77
0.37
0.62

0.21
0.68
0.38
0.32

5

0.32
0.88
0.18
0.47

0.49
0.93
0.15
0.64

0.22
0.6
0.5
0.32

10

0.33
0.91
0.16
0.48

0.48
0.92
0.13
0.63

0.2
0.43
0.6
0.27

5
1

0.31
0.5
0.51
0.38

0.5
0.6
0.49
0.55

0.18
0.52
0.45
0.27

5

0.32
0.8
0.23
0.43

0.48
0.86
0.18
0.61

0.2
0.48
0.55
0.28

10

0.32
0.9
0.15
0.47

0.48
0.91
0.16
0.63

0.22
0.44
0.62
0.29

10
1

0.35
0.56
0.53
0.43

0.49
0.56
0.5
0.52

0.18
0.43
0.55
0.25

5

10

0.33
0.81
0.28
0.47

0.48
0.82
0.25
0.61

0.21
0.49
0.56
0.29

0.33
0.88
0.22
0.49

0.49
0.9
0.2
0.63

0.23
0.43
0.67
0.3

sex

violence

drugs

