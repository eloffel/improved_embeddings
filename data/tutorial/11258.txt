squad: 100,000+ questions for machine comprehension of text

pranav rajpurkar and jian zhang and konstantin lopyrev and percy liang

{pranavsr,zjian,klopyrev,pliang}@cs.stanford.edu

computer science department

stanford university

6
1
0
2

 
t
c
o
1
1

 

 
 
]
l
c
.
s
c
[
 
 

3
v
0
5
2
5
0

.

6
0
6
1
:
v
i
x
r
a

abstract

we present the stanford question answer-
ing dataset (squad), a new reading compre-
hension dataset consisting of 100,000+ ques-
tions posed by crowdworkers on a set of
wikipedia articles, where the answer to each
question is a segment of text from the cor-
responding reading passage. we analyze the
dataset
to understand the types of reason-
ing required to answer the questions, lean-
ing heavily on dependency and constituency
trees. we build a strong id28
model, which achieves an f1 score of 51.0%,
a signi   cant improvement over a simple base-
line (20%). however, human performance
(86.8%) is much higher, indicating that the
dataset presents a good challenge problem for
future research. the dataset is freely available
at https://stanford-qa.com.

introduction

1
reading comprehension (rc), or the ability to read
text and then answer questions about it, is a chal-
lenging task for machines, requiring both under-
standing of natural language and knowledge about
the world. consider the question    what causes pre-
cipitation to fall?    posed on the passage in figure 1.
in order to answer the question, one might    rst lo-
cate the relevant part of the passage    precipitation ...
falls under gravity   , then reason that    under    refers
to a cause (not location), and thus determine the cor-
rect answer:    gravity   .

how can we get a machine to make progress
on the challenging task of reading comprehension?
historically,
large, realistic datasets have played

in meteorology, precipitation is any product
of the condensation of atmospheric water vapor
that falls under gravity. the main forms of pre-
cipitation include drizzle, rain, sleet, snow, grau-
pel and hail... precipitation forms as smaller
droplets coalesce via collision with other rain
drops or ice crystals within a cloud. short, in-
tense periods of rain in scattered locations are
called    showers   .

what causes precipitation to fall?
gravity

what is another main form of precipitation be-
sides drizzle, rain, snow, sleet and hail?
graupel

where do water droplets collide with ice crystals
to form precipitation?
within a cloud

figure 1: question-answer pairs for a sample passage in the
squad dataset. each of the answers is a segment of text from
the passage.

a critical role for driving    elds forward   famous
examples include id163 for object recognition
(deng et al., 2009) and the id32 for
syntactic parsing (marcus et al., 1993). existing
datasets for rc have one of two shortcomings: (i)
those that are high in quality (richardson et al.,
2013; berant et al., 2014) are too small for training
modern data-intensive models, while (ii) those that
are large (hermann et al., 2015; hill et al., 2015) are
semi-synthetic and do not share the same character-
istics as explicit reading comprehension questions.
to address the need for a large and high-quality
reading comprehension dataset, we present the stan-

ford id53 dataset v1.0 (squad),
freely available at https://stanford-qa.com, con-
sisting of questions posed by crowdworkers on a
set of wikipedia articles, where the answer to ev-
ery question is a segment of text, or span, from the
corresponding reading passage. squad contains
107,785 question-answer pairs on 536 articles, and
is almost two orders of magnitude larger than previ-
ous manually labeled rc datasets such as mctest
(richardson et al., 2013).

in contrast to prior datasets, squad does not
provide a list of answer choices for each question.
rather, systems must select the answer from all pos-
sible spans in the passage, thus needing to cope with
a fairly large number of candidates. while ques-
tions with span-based answers are more constrained
than the more interpretative questions found in more
advanced standardized tests, we still    nd a rich di-
versity of questions and answer types in squad.
we develop automatic techniques based on distances
in dependency trees to quantify this diversity and
stratify the questions by dif   culty. the span con-
straint also comes with the important bene   t that
span-based answers are easier to evaluate than free-
form answers.

to assess the dif   culty of squad, we imple-
mented a id28 model with a range of
features. we    nd that lexicalized and dependency
tree path features are important to the performance
of the model. we also    nd that the model perfor-
mance worsens with increasing complexity of (i) an-
swer types and (ii) syntactic divergence between the
question and the sentence containing the answer; in-
terestingly, there is no such degradation for humans.
our best model achieves an f1 score of 51.0%,1
which is much better than the sliding window base-
line (20%). over the last four months (since june
2016), we have witnessed signi   cant improvements
from more sophisticated neural network-based mod-
els. for example, wang and jiang (2016) obtained
70.3% f1 on squad v1.1 (results on v1.0 are sim-
ilar). these results are still well behind human
performance, which is 86.8% f1 based on inter-
annotator agreement. this suggests that there is
plenty of room for advancement in modeling and
learning on the squad dataset.

1all experimental results in this paper are on squad v1.0.

dataset

squad

mctest
(richardson et al., 2013)
algebra
(kushman et al., 2014)
science
(clark and etzioni, 2016)

wikiqa
(yang et al., 2015)
trec-qa
(voorhees and tice, 2000)

formulation

question
source
crowdsourced rc,

size

100k

2640

514

spans

in passage
rc, multiple
choice
computation

855

reasoning,
multiple
choice
ir, sentence
selection
ir, free form 1479

3047

crowdsourced

standardized
tests
standardized
tests

query logs

query logs +
human editor
summary
cloze
cloze

+

1.4m

rc,    ll
in
single entity
rc,    ll
in
single word

id98/daily
mail
(hermann et al., 2015)
cbt
(hill et al., 2015)
table 1: a survey of several reading comprehension and ques-
tion answering datasets. squad is much larger than all datasets
except the semi-synthetic cloze-style datasets, and it is similar
to trec-qa in the open-endedness of the answers.

688k

2 existing datasets

we begin with a survey of existing reading com-
prehension and id53 (qa) datasets,
highlighting a variety of task formulation and cre-
ation strategies (see table 1 for an overview).

reading comprehension. a data-driven approach
to reading comprehension goes back to hirschman
et al. (1999), who curated a dataset of 600 real 3rd   
6th grade reading comprehension questions. their
pattern matching baseline was subsequently im-
proved by a rule-based system (riloff and thelen,
2000) and a id28 model (ng et al.,
2000). more recently, richardson et al. (2013) cu-
rated mctest, which contains 660 stories created
by crowdworkers, with 4 questions per story and
4 answer choices per question. because many of
the questions require commonsense reasoning and
reasoning across multiple sentences, the dataset re-
mains quite challenging, though there has been no-
ticeable progress (narasimhan and barzilay, 2015;
sachan et al., 2015; wang et al., 2015). both curated
datasets, although real and dif   cult, are too small to
support very expressive statistical models.

some datasets focus on deeper reasoning abili-
ties. algebra word problems require understanding
a story well enough to turn it into a system of equa-

tions, which can be easily solved to produce the an-
swer (kushman et al., 2014; hosseini et al., 2014).
babi (weston et al., 2015), a fully synthetic rc
dataset, is strati   ed by different types of reasoning
required to solve each task. clark and etzioni (2016)
describe the task of solving 4th grade science exams,
and stress the need to reason with world knowledge.
open-domain id53. the goal of
open-domain qa is to answer a question from a
large collection of documents. the annual eval-
uations at the text retreival conference (trec)
(voorhees and tice, 2000) led to many advances
in open-domain qa, many of which were used in
ibm watson for jeopardy! (ferrucci et al., 2013).
recently, yang et al. (2015) created the wikiqa
dataset, which, like squad, use wikipedia pas-
sages as a source of answers, but their task is sen-
tence selection, while ours requires selecting a spe-
ci   c span in the sentence.

selecting the span of text that answers a question
is similar to answer extraction, the    nal step in the
open-domain qa pipeline, methods for which in-
clude id64 surface patterns (ravichandran
and hovy, 2002), using dependency trees (shen and
klakow, 2006), and using a factor graph over mul-
tiple sentences (sun et al., 2013). one key differ-
ence between our rc setting and answer extraction
is that answer extraction typically exploits the fact
that the answer occurs in multiple documents (brill
et al., 2002), which is more lenient than in our set-
ting, where a system only has access to a single read-
ing passage.
cloze datasets. recently, researchers have con-
structed cloze datasets, in which the goal is to pre-
dict the missing word (often a named entity) in a
passage. since these datasets can be automatically
generated from naturally occurring data, they can be
extremely large. the children   s book test (cbt)
(hill et al., 2015), for example, involves predicting
a blanked-out word of a sentence given the 20 previ-
ous sentences. hermann et al. (2015) constructed a
corpus of cloze style questions by blanking out enti-
ties in abstractive summaries of id98 / daily news
articles; the goal is to    ll in the entity based on the
original article. while the size of this dataset is im-
pressive, chen et al. (2016) showed that the dataset
requires less reasoning than previously thought, and

figure 2: the crowd-facing web interface used to collect the
dataset encourages crowdworkers to use their own words while
asking questions.

concluded that performance is almost saturated.

one difference between squad questions and
cloze-style queries is that answers to cloze queries
are single words or entities, while answers in
squad often include non-entities and can be much
longer phrases. another difference is that squad
focuses on questions whose answers are entailed
by the passage, whereas the answers to cloze-style
queries are merely suggested by the passage.

3 dataset collection

we collect our dataset in three stages: curating
passages, id104 question-answers on those
passages, and obtaining additional answers.

passage curation. to retrieve high-quality arti-
cles, we used project nayuki   s wikipedia   s internal
id95s to obtain the top 10000 articles of en-
glish wikipedia, from which we sampled 536 arti-
cles uniformly at random. from each of these ar-
ticles, we extracted individual paragraphs, stripping
away images,    gures, tables, and discarding para-
graphs shorter than 500 characters. the result was
23,215 paragraphs for the 536 articles covering a
wide range of topics, from musical celebrities to ab-
stract concepts. we partitioned the articles randomly
into a training set (80%), a development set (10%),

and a test set (10%).

question-answer collection. next, we employed
crowdworkers to create questions. we used the
daemo platform (gaikwad et al., 2015), with ama-
zon mechanical turk as its backend. crowdworkers
were required to have a 97% hit acceptance rate, a
minimum of 1000 hits, and be located in the united
states or canada. workers were asked to spend 4
minutes on every paragraph, and paid $9 per hour for
the number of hours required to complete the article.
the task was reviewed favorably by crowdworkers,
receiving positive comments on turkopticon.

on each paragraph, crowdworkers were tasked
with asking and answering up to 5 questions on the
content of that paragraph. the questions had to be
entered in a text    eld, and the answers had to be
highlighted in the paragraph. to guide the work-
ers, tasks contained a sample paragraph, and exam-
ples of good and bad questions and answers on that
paragraph along with the reasons they were cate-
gorized as such. additionally, crowdworkers were
encouraged to ask questions in their own words,
without copying word phrases from the paragraph.
on the interface, this was reinforced by a reminder
prompt at the beginning of every paragraph, and by
disabling copy-paste functionality on the paragraph
text.

additional answers collection. to get an indica-
tion of human performance on squad and to make
our evaluation more robust, we obtained at least 2
additional answers for each question in the develop-
ment and test sets. in the secondary answer gener-
ation task, each crowdworker was shown only the
questions along with the paragraphs of an article,
and asked to select the shortest span in the para-
graph that answered the question. if a question was
not answerable by a span in the paragraph, workers
were asked to submit the question without marking
an answer. workers were recommended a speed of 5
questions for 2 minutes, and paid at the same rate of
$9 per hour for the number of hours required for the
entire article. over the development and test sets,
2.6% of questions were marked unanswerable by at
least one of the additional crowdworkers.

answer type

percentage

example

date
other numeric
person
location
other entity
common noun phrase
adjective phrase
verb phrase
clause
other

8.9% 19 october 1512
10.9% 12
12.9% thomas coke
4.4% germany
15.3% abc sports
31.8% property damage
3.9% second-largest
5.5% returned to earth
3.7% to avoid trivialization
2.7% quietly

table 2: we automatically partition our answers into the fol-
lowing categories. our dataset consists of large number of an-
swers beyond proper noun entities.

4 dataset analysis

to understand the properties of squad, we analyze
the questions and answers in the development set.
speci   cally, we explore the (i) diversity of answer
types, (ii) the dif   culty of questions in terms of type
of reasoning required to answer them, and (iii) the
degree of syntactic divergence between the question
and answer sentences.

diversity in answers. we automatically catego-
rize the answers as follows: we    rst separate
the numerical and non-numerical answers. the
non-numerical answers are categorized using con-
stituency parses and pos tags generated by stan-
ford corenlp. the proper noun phrases are further
split into person, location and other entities using
ner tags. in table 2, we can see dates and other
numbers make up 19.8% of the data; 32.6% of the
answers are proper nouns of three different types;
31.8% are common noun phrases answers; and the
remaining 15.8% are made up of adjective phrases,
verb phrases, clauses and other types.

reasoning required to answer questions. to get
a better understanding of the reasoning required to
answer the questions, we sampled 4 questions from
each of the 48 articles in the development set, and
then manually labeled the examples with the cate-
gories shown in table 3. the results show that
all examples have some sort of lexical or syntactic
divergence between the question and the answer in
the passage. note that some examples fall into more
than one category.

reasoning

description

lexical variation
(synonymy)

lexical variation
(world knowledge)

major
correspondences between
the question and the answer sen-
tence are synonyms.

major
correspondences between
the question and the answer sen-
tence require world knowledge to
resolve.

syntactic variation after the question is paraphrased
into declarative form,
its syntac-
tic dependency structure does not
match that of the answer sentence
even after local modi   cations.

multiple sentence
reasoning

there is anaphora, or higher-level
fusion of multiple sentences is re-
quired.

ambiguous

we don   t agree with the crowd-
workers    answer, or the question
does not have a unique answer.

example
q: what is the rankine cycle sometimes called?
sentence: the rankine cycle is sometimes re-
ferred to as a practical carnot cycle.
q: which governing bodies have veto power?
sen.: the european parliament and the council of
the european union have powers of amendment
and veto during the legislative process.
q: what shakespeare scholar is currently on the
faculty?
sen.: current faculty include the anthropol-
ogist marshall sahlins,
..., shakespeare scholar
david bevington.
q: what collection does the v&a theatre & per-
formance galleries hold?
sen.: the v&a theatre & performance gal-
leries opened in march 2009.
they
hold the uk   s biggest national collection of
material about live performance.

...

q: what is the main goal of criminal punishment?
sen.: achieving crime control via incapacitation
and deterrence is a major goal of criminal punish-
ment.

percentage

33.3%

9.1%

64.1%

13.6%

6.1%

table 3: we manually labeled 192 examples into one or more of the above categories. words relevant to the corresponding
reasoning type are bolded, and the crowdsourced answer is underlined.

q: what department store is thought to be the    rst in the world?
s: bainbridge   s is often cited as the world   s    rst department store.

path:

   rst

   rst

edit cost:

xcomp               thought
   delete
   substitute
amod            store nmod             cited

nsubjpass

                   store

nsubjpass

                  bainbridge   s

det         what
   insert

1

+2

+1=4

figure 3: an example walking through the computation of the
syntactic divergence between the question q and answer sen-
tence s.

strati   cation by syntactic divergence. we also
develop an automatic method to quantify the syntac-
tic divergence between a question and the sentence
containing the answer. this provides another way to
measure the dif   culty of a question and to stratify
the dataset, which we return to in section 6.3.

we illustrate how we measure the divergence with
the example in figure 3. we    rst detect anchors
(word-lemma pairs common to both the question
and answer sentences); in the example, the anchor
is       rst   . the two unlexicalized paths, one from

the anchor       rst    in the question to the wh-word
   what   , and the other from the anchor in the answer
sentence and to the answer span    bainbridge   s   , are
then extracted from the dependency parse trees. we
measure the id153 between these two paths,
which we de   ne as the minimum number of dele-
tions or insertions to transform one path into the
other. the syntactic divergence is then de   ned as
the minimum id153 over all possible anchors.
the histogram in figure 4a shows that there is a
wide range of syntactic divergence in our dataset.
we also show a concrete example where the edit dis-
tance is 0 and another where it is 6. note that our
syntactic divergence ignores lexical variation. also,
small divergence does not mean that a question is
easy since there could be other candidates with sim-
ilarly small divergence.

5 methods

we developed a id28 model and com-
pare its accuracy with that of three baseline methods.

q: who went to wittenberg to hear luther speak?
s: students thronged to wittenberg to hear luther
speak.
path:
wittenberg
wittenberg

nmod            
went
nmod             thronged

nsubj             who
nsubj             students

(a) histogram of syntactic divergence.

(b) an example of a question-answer pair with id153 0 be-
tween the dependency paths (note that lexical variation is ignored
in the computation of id153).

q: what impact did the high school education movement have on the presence of skilled workers?
s: during the mass high school education movement from 1910     1940 , there was an increase in skilled workers.
path:
school
school

det      
what
nsubj             increase
(c) an example of a question-answer pair with id153 6.

compound                   movement
compound                   movement

dobj          impact
acl      
was

nsubj             have
nmod             1910

figure 4: we use the id153 between the unlexicalized dependency paths in the question and the sentence containing the
answer to measure syntactic divergence.

candidate answer generation. for all four meth-
ods, rather than considering all o(l2) spans as can-
didate answers, where l is the number of words
in the sentence, we only use spans which are con-
stituents in the constituency parse generated by
stanford corenlp. ignoring punctuation and arti-
cles, we    nd that 77.3% of the correct answers in the
development set are constituents. this places an ef-
fective ceiling on the accuracy of our methods. dur-
ing training, when the correct answer of an example
is not a constituent, we use the shortest constituent
containing the correct answer as the target.

5.1 sliding window baseline
for each candidate answer, we compute the uni-
gram/bigram overlap between the sentence contain-
ing it (excluding the candidate itself) and the ques-
tion. we keep all the candidates that have the max-
imal overlap. among these, we select the best
one using the sliding-window approach proposed
in richardson et al. (2013).

in addition to the basic sliding window ap-
proach, we also implemented the distance-based ex-
tension (richardson et al., 2013). whereas richard-
son et al. (2013) used the entire passage as the con-
text of an answer, we used only the sentence con-
taining the candidate answer for ef   ciency.

5.2 id28

in our id28 model, we extract several
types of features for each candidate answer. we
discretize each continuous feature into 10 equally-
sized buckets, building a total of 180 million fea-
tures, most of which are lexicalized features or de-
pendency tree path features. the descriptions and
examples of the features are summarized in table 4.
the matching word and bigram frequencies as
well as the root match features help the model pick
the correct sentences. length features bias the
model towards picking common lengths and posi-
tions for answer spans, while span word frequencies
bias the model against uninformative words. con-
stituent label and span pos tag features guide the
model towards the correct answer types.
in addi-
tion to these basic features, we resolve lexical vari-
ation using lexicalized features, and syntactic varia-
tion using dependency tree path features.

the multiclass log-likelihood loss is optimized
using adagrad with an initial learning rate of 0.1.
each update is performed on the batch of all ques-
tions in a paragraph for ef   ciency, since they share
the same candidates. l2 id173 is used, with
a coef   cient of 0.1 divided by the number of batches.
the model is trained with three passes over the train-

012345678syntactic divergence0.05.010.015.020.025.030.0percentagefeature groups

description

matching word
frequencies

matching bigram
frequencies
root match

lengths

span word
frequencies
constituent label

span pos tags

lexicalized

dependency tree
paths

sum of the tf-idf of the words that occur in both the question and the
sentence containing the candidate answer. separate features are used
for the words to the left, to the right, inside the span, and in the whole
sentence.
same as above, but using bigrams. we use the generalization of the
tf-idf described in shirakawa et al. (2015).
whether the dependency parse tree roots of the question and sentence
match, whether the sentence contains the root of the dependency parse
tree of the question, and whether the question contains the root of the
dependency parse tree of the sentence.
number of words to the left, to the right, inside the span, and in the
whole sentence.
sum of the tf-idf of the words in the span, regardless of whether they
appear in the question.
constituency parse tree label of the span, optionally combined with the
wh-word in the question.
sequence of the part-of-speech tags in the span, optionally combined
with the wh-word in the question.
lemmas of question words combined with the lemmas of words within
distance 2 to the span in the sentence based on the dependency parse
trees. separately, question word lemmas combined with answer word
lemmas.
for each word that occurs in both the question and sentence, the path
in the dependency parse tree from that word in the sentence to the span,
optionally combined with the path from the wh-word to the word in the
question. pos tags are included in the paths.

examples
span: [0     sum < 0.01]
left: [7.9     sum < 10.7]

span: [0     sum < 2.4]
left: [0     sum < 2.7]
root match = false

span: [1 <= num < 2]
left: [15     num < 19]
span: [5.2     sum < 6.9]

span: np
span: np, wh-word:    what   
span: [nn]
span: [nn], wh-word:    what   
q:    cause   , s:    under    case         
q:    fall   , a:    gravity   

vbz nmod             nn
what nsubj          vbz advcl         

+ vbz nmod            nn

table 4: features used in the id28 model with examples for the question    what causes precipitation to fall?   , sentence
   in meteorology, precipitation is any product of the condensation of atmospheric water vapor that falls under gravity.    and answer
   gravity   . q denotes question, a denotes candidate answer, and s denotes sentence containing the candidate answer.

ing data.

6 experiments

6.1 model evaluation
we use two different metrics to evaluate model accu-
racy. both metrics ignore punctuations and articles
(a, an, the).

exact match. this metric measures the percent-
age of predictions that match any one of the ground
truth answers exactly.

(macro-averaged) f1 score. this metric mea-
sures the average overlap between the prediction and
ground truth answer. we treat the prediction and
ground truth as bags of tokens, and compute their
f1. we take the maximum f1 over all of the ground
truth answers for a given question, and then average
over all of the questions.

6.2 human performance
we assess human performance on squad   s devel-
opment and test sets. recall that each of the ques-
tions in these sets has at least three answers. to eval-
uate human performance, we treat the second an-
swer to each question as the human prediction, and
keep the other answers as ground truth answers. the
resulting human performance score on the test set is
77.0% for the exact match metric, and 86.8% for f1.
mismatch occurs mostly due to inclusion/exclusion
of non-essential phrases (e.g., monsoon trough ver-
sus movement of the monsoon trough) rather than
fundamental disagreements about the answer.

6.3 model performance
table 5 shows the performance of our models along-
side human performance on the v1.0 of development
and test sets. the id28 model signi   -
cantly outperforms the baselines, but underperforms

exact match

f1

dev

test

dev

test

id28 human
dev f1

dev f1

1.3%

1.1%
4.3%
random guess
sliding window
13.2% 12.5% 20.2% 19.7%
sliding win. + dist. 13.3% 13.0% 20.2% 20.0%
id28 40.0% 40.4% 51.0% 51.0%
human
80.3% 77.0% 90.5% 86.8%

4.1%

table 5: performance of various methods and humans. logis-
tic regression outperforms the baselines, while there is still a
signi   cant gap between humans.

f1

train

dev

91.7% 51.0%
id28
33.9% 35.8%
    lex.,     dep. paths
53.5% 45.4%
    lexicalized
91.4% 46.4%
    dep. paths
91.7% 48.1%
    match. word freq.
    span pos tags
91.7% 49.7%
    match. bigram freq. 91.7% 50.3%
    constituent label
91.7% 50.4%
91.8% 50.5%
    lengths
91.7% 50.5%
    span word freq.
    root match
91.7% 50.6%

table 6: performance with feature ablations. we    nd that lexi-
calized and dependency tree path features are most important.

humans. we note that the model is able to select
the sentence containing the answer correctly with
79.3% accuracy; hence, the bulk of the dif   culty lies
in    nding the exact span within the sentence.
feature ablations.
in order to understand the fea-
tures that are responsible for the performance of the
id28 model, we perform a feature ab-
lation where we remove one group of features from
our model at a time. the results, shown in table 6,
indicate that lexicalized and dependency tree path
features are most important. comparing our analy-
sis to the one in chen et al. (2016), we note that the
dependency tree path features play a much bigger
role in our dataset. additionally, we note that with
lexicalized features, the model signi   cantly over   ts
the training set; however, we found that increasing
l2 id173 hurts performance on the develop-
ment set.
performance strati   ed by answer type. to gain
more insight into the performance of our logistic re-
gression model, we report its performance across

date
other numeric
person
location
other entity
common noun phrase
adjective phrase
verb phrase
clause
other

72.1%
62.5%
56.2%
55.4%
52.2%
46.5%
37.9%
31.2%
34.3%
34.8%

93.9%
92.9%
95.4%
94.1%
92.6%
88.3%
86.8%
82.4%
84.5%
86.1%

table 7: performance strati   ed by answer types. logistic re-
gression performs better on certain types of answers, namely
numbers and entities. on the other hand, human performance is
more uniform.

figure 5: performance strati   ed by syntactic divergence of
questions and sentences. the performance of logistic regres-
sion degrades with increasing divergence. in contrast, human
performance is stable across the full range of divergence.

the answer types explored in table 2. the re-
sults (shown in table 7) show that the model per-
forms best on dates and other numbers, categories
for which there are usually only a few plausible can-
didates, and most answers are single tokens. the
model is challenged more on other named entities
(i.e., location, person and other entities) because
there are many more plausible candidates. how-
ever, named entities are still relatively easy to iden-
tify by their pos tag features. the model performs
worst on other answer types, which together form
47.6% of the dataset. humans have exceptional per-
formance on dates, numbers and all named entities.
their performance on other answer types degrades
only slightly.

012345678syntactic divergence2030405060708090100preformance (%)id28 dev f1human dev f1performance strati   ed by syntactic divergence.
as discussed in section 4, another challenging as-
pect of the dataset is the syntactic divergence be-
tween the question and answer sentence. figure 5
shows that the more divergence there is, the lower
the performance of the id28 model.
interestingly, humans do not seem to be sensitive
to syntactic divergence, suggesting that deep under-
standing is not distracted by super   cial differences.
measuring the degree of degradation could therefore
be useful in determining the extent to which a model
is generalizing in the right way.

7 conclusion

towards the end goal of natural language under-
standing, we introduce the stanford question an-
swering dataset, a large reading comprehension
dataset on wikipedia articles with crowdsourced
question-answer pairs. squad features a diverse
range of question and answer types. the perfor-
mance of our id28 model, with 51.0%
f1, against the human f1 of 86.8% suggests ample
opportunity for improvement. we have made our
dataset freely available to encourage exploration of
more expressive models. since the release of our
dataset, we have already seen considerable interest
in building models on this dataset, and the gap be-
tween our id28 model and human per-
formance has more than halved (wang and jiang,
2016). we expect that the remaining gap will be
harder to close, but that such efforts will result in
signi   cant advances in reading comprehension.

reproducibility

all code, data, and experiments for this paper are
available on the codalab platform:
https://worksheets.codalab.org/worksheets/
0xd53d03a48ef64b329c16b9baf0f99b0c/ .

acknowledgments

we would like to thank durim morina and professor
michael bernstein for their help in id104
the collection of our dataset, both in terms of fund-
ing and technical support of the daemo platform.

references
j. berant, v. srikumar, p. chen, a. v. linden, b. harding,
b. huang, p. clark, and c. d. manning. 2014. mod-
eling biological processes for reading comprehension.
in empirical methods in natural language process-
ing (emnlp).

e. brill, s. dumais, and m. banko. 2002. an analysis of
the askmsr question-answering system. in associa-
tion for computational linguistics (acl), pages 257   
264.

d. chen, j. bolton, and c. d. manning.

2016. a
thorough examination of the id98 / daily mail read-
ing comprehension task. in association for computa-
tional linguistics (acl).

p. clark and o. etzioni. 2016. my computer is an honor
student but how intelligent is it? standardized tests as
a measure of ai. ai magazine, 37(1):5   12.

j. deng, w. dong, r. socher, l. li, k. li, and l. fei-
fei. 2009. id163: a large-scale hierarchical im-
age database. in id161 and pattern recog-
nition (cvpr), pages 248   255.

d. ferrucci, e. brown, j. chu-carroll, j. fan, d. gondek,
a. a. kalyanpur, a. lally, j. w. murdock, e. nyberg,
j. prager, n. schlaefer, and c. welty. 2013. build-
ing watson: an overview of the deepqa project. ai
magazine, 31(3):59   79.

s. n. gaikwad, d. morina, r. nistala, m. agarwal,
a. cossette, r. bhanu, s. savage, v. narwal, k. raj-
pal, j. regino, et al. 2015. daemo: a self-governed
in proceedings of the
id104 marketplace.
28th annual acm symposium on user interface soft-
ware & technology, pages 101   102.

k. m. hermann, t. ko  cisk  y, e. grefenstette, l. espeholt,
w. kay, m. suleyman, and p. blunsom. 2015. teach-
ing machines to read and comprehend. in advances in
neural information processing systems (nips).

f. hill, a. bordes, s. chopra, and j. weston. 2015.
the goldilocks principle: reading children   s books
with explicit memory representations. in international
conference on learning representations (iclr).

l. hirschman, m. light, e. breck, and j. d. burger.
1999. deep read: a reading comprehension system.
in association for computational linguistics (acl),
pages 325   332.

m. j. hosseini, h. hajishirzi, o. etzioni, and n. kush-
man. 2014. learning to solve arithmetic word prob-
in empirical meth-
lems with verb categorization.
ods in natural language processing (emnlp), pages
523   533.

n. kushman, y. artzi, l. zettlemoyer, and r. barzilay.
2014. learning to automatically solve algebra word
problems. in association for computational linguis-
tics (acl).

j. weston, a. bordes, s. chopra, and t. mikolov. 2015.
towards ai-complete id53: a set of
prerequisite toy tasks. arxiv.

y. yang, w. yih, and c. meek. 2015. wikiqa: a chal-
lenge dataset for open-domain id53. in
empirical methods in natural language processing
(emnlp), pages 2013   2018.

m. p. marcus, m. a. marcinkiewicz, and b. santorini.
1993.
building a large annotated corpus of en-
glish: the id32. computational linguistics,
19:313   330.

k. narasimhan and r. barzilay. 2015. machine compre-
in association for

hension with discourse relations.
computational linguistics (acl).

h. t. ng, l. h. teo, and j. l. p. kwan. 2000. a machine
learning approach to answering questions for reading
comprehension tests. in joint sigdat conference on
empirical methods in natural language processing and
very large corpora - volume 13, pages 124   132.

d. ravichandran and e. hovy. 2002. learning surface
text patterns for a id53 system. in as-
sociation for computational linguistics (acl), pages
41   47.

m. richardson, c. j. burges, and e. renshaw. 2013.
mctest: a challenge dataset for the open-domain ma-
chine comprehension of text. in empirical methods in
natural language processing (emnlp), pages 193   
203.

e. riloff and m. thelen. 2000. a rule-based question
answering system for reading comprehension tests. in
anlp/naacl workshop on reading comprehension
tests as evaluation for computer-based language un-
derstanding sytems - volume 6, pages 13   19.

m. sachan, a. dubey, e. p. xing, and m. richardson.
2015. learning answer-entailing structures for ma-
in association for computa-
chine comprehension.
tional linguistics (acl).

d. shen and d. klakow. 2006. exploring correlation of
dependency relation paths for answer extraction. in in-
ternational conference on computational linguistics
and association for computational linguistics (col-
ing/acl), pages 889   896.

m. shirakawa, t. hara, and s. nishio. 2015. id165 idf:
a global term weighting scheme based on information
distance. in world wide web (www), pages 960   970.
h. sun, n. duan, y. duan, and m. zhou. 2013. answer
extraction from passage graph for id53.
in international joint conference on arti   cial intelli-
gence (ijcai).

e. m. voorhees and d. m. tice. 2000. building a ques-
tion answering test collection. in acm special interest
group on information retreival (sigir), pages 200   
207.

shuohang wang and jing jiang. 2016. machine compre-
hension using match-lstm and answer pointer. corr,
abs/1608.07905.

h. wang, m. bansal, k. gimpel, and d. mcallester.
2015. machine comprehension with syntax, frames,
and semantics. in association for computational lin-
guistics (acl).

