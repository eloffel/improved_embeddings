6
1
0
2

 

n
u
j
 

8

 
 
]
l
c
.
s
c
[
 
 

2
v
8
7
2
4
0

.

5
0
6
1
:
v
i
x
r
a

universal dependencies for learner english

yevgeni berzak

csail mit

berzak@mit.edu

jessica kenney

eecs & linguistics mit

jessk@mit.edu

carolyn spadine
linguistics mit
cspadine@mit.edu

jing xian wang

eecs mit

jxwang@mit.edu

lucia lam
meche mit
lucci@mit.edu

keiko sophie mori

linguistics mit
ksmori@mit.edu

sebastian garza
linguistics mit
sjgarza@mit.edu

boris katz
csail mit
boris@mit.edu

abstract

we introduce the treebank of learner en-
glish (tle), the    rst publicly available
syntactic treebank for english as a sec-
ond language (esl). the tle provides
manually annotated pos tags and univer-
sal dependency (ud) trees for 5,124 sen-
tences from the cambridge first certi   -
cate in english (fce) corpus. the ud
annotations are tied to a pre-existing er-
ror annotation of the fce, whereby full
syntactic analyses are provided for both
the original and error corrected versions of
each sentence. further on, we delineate
esl annotation guidelines that allow for
consistent syntactic treatment of ungram-
matical english. finally, we benchmark
id52 and id33 per-
formance on the tle dataset and measure
the effect of grammatical errors on parsing
accuracy. we envision the treebank to sup-
port a wide range of linguistic and compu-
tational research on second language ac-
quisition as well as automatic processing
of ungrammatical language1.

1 introduction

the majority of the english text available world-
wide is generated by non-native speakers (crys-
tal, 2003). such texts introduce a variety of chal-
lenges, most notably grammatical errors, and are
of paramount importance for the scienti   c study
of id146 as well as for nlp. de-
spite the ubiquity of non-native english, there is

1the treebank is available at universaldependencies.org.
the annotation manual used in this project and a graphical
query engine are available at esltreebank.org.

currently no publicly available syntactic treebank
for english as a second language (esl).

to address this shortcoming, we present the
treebank of learner english (tle), a    rst of
its kind resource for non-native english, contain-
ing 5,124 sentences manually annotated with pos
tags and dependency trees. the tle sentences are
drawn from the fce dataset (yannakoudakis et al.,
2011), and authored by english learners from 10
different native language backgrounds. the tree-
bank uses the universal dependencies (ud) for-
malism (de marneffe et al., 2014; nivre et al.,
2016), which provides a uni   ed annotation frame-
work across different languages and is geared to-
wards multilingual nlp (mcdonald et al., 2013).
this characteristic allows our treebank to sup-
port computational analysis of esl using not only
english based but also multilingual approaches
which seek to relate esl phenomena to native lan-
guage syntax.

while the annotation inventory and guidelines
are de   ned by the english ud formalism, we
build on previous work in learner language anal-
ysis (d  az-negrillo et al., 2010; dickinson and
ragheb, 2013) to formulate an additional set of
annotation conventions aiming at a uniform treat-
ment of ungrammatical learner language. our
annotation scheme uses a two-layer analysis,
whereby a distinct syntactic annotation is pro-
vided for the original and the corrected version
of each sentence. this approach is enabled by a
pre-existing error annotation of the fce (nicholls,
2003) which is used to generate an error corrected
variant of the dataset. our inter-annotator agree-
ment results provide evidence for the ability of the
annotation scheme to support consistent annota-
tion of ungrammatical structures.

finally, a corpus that is annotated with both
grammatical errors and syntactic dependencies
paves the way for empirical investigation of the
relation between grammaticality and syntax. un-
derstanding this relation is vital for improving tag-
ging and parsing performance on learner language
(geertzen et al., 2013), syntax based grammati-
cal error correction (tetreault et al., 2010; ng et
al., 2014), and many other fundamental challenges
in nlp. in this work, we take the    rst step in
this direction by benchmarking tagging and pars-
ing accuracy on our dataset under different train-
ing regimes, and obtaining several estimates for
the impact of grammatical errors on these tasks.

to summarize, this paper presents three contri-
butions. first, we introduce the    rst large scale
syntactic treebank for esl, manually annotated
with pos tags and universal dependencies. sec-
ond, we describe a linguistically motivated anno-
tation scheme for ungrammatical learner english
and provide empirical support for its consistency
via inter-annotator agreement analysis. third, we
benchmark a state of the art parser on our dataset
and estimate the in   uence of grammatical errors
on the accuracy of automatic id52 and de-
pendency parsing.

the remainder of this paper is structured as fol-
lows. we start by presenting an overview of the
treebank in section 2.
in sections 3 and 4 we
provide background information on the annota-
tion project, and review the main annotation stages
leading to the current form of the dataset. the esl
annotation guidelines are summarized in section 5.
inter-annotator agreement analysis is presented in
section 6, followed by parsing experiments in sec-
tion 7. finally, we review related work in section
8 and present the conclusion in section 9.

2 treebank overview

the tle currently contains 5,124 sentences
(97,681 tokens) with pos tag and dependency an-
notations in the english universal dependencies
(ud) formalism (de marneffe et al., 2014; nivre
et al., 2016). the sentences were obtained from
the fce corpus (yannakoudakis et al., 2011), a
collection of upper intermediate english learner
essays, containing error annotations with 75 error
categories (nicholls, 2003). sentence level seg-
mentation was performed using an adaptation of
the nltk sentence tokenizer2. under-segmented

sentences were split further manually. word level
id121 was generated using the stanford
ptb word tokenizer3.

the treebank represents learners with 10 dif-
ferent native language backgrounds: chinese,
french, german, italian, japanese, korean, por-
tuguese, spanish, russian and turkish. for every
native language, we randomly sampled 500 au-
tomatically segmented sentences, under the con-
straint that selected sentences have to contain at
least one grammatical error that is not punctuation
or spelling.

the tle annotations are provided in two ver-
sions. the    rst version is the original sentence au-
thored by the learner, containing grammatical er-
rors. the second, corrected sentence version, is a
grammatical variant of the original sentence, gen-
erated by correcting all the grammatical errors in
the sentence according to the manual error anno-
tation provided in the fce dataset. the resulting
corrected sentences constitute a parallel corpus of
standard english. table 1 presents basic statistics
of both versions of the annotated sentences.

sentences
tokens
sentence length
errors per sentence
authors
native languages

original
5,124
97,681
19.06 (std 9.47)
2.67 (std 1.9)

corrected
5,124
98,976
19.32 (std 9.59)
-
924
10

table 1: statistics of the tle. standard deviations
are denoted in parenthesis.

to avoid potential annotation biases, the anno-
tations of the treebank were created manually from
scratch, without utilizing any automatic annota-
tion tools. to further assure annotation quality,
each annotated sentence was reviewed by two ad-
ditional annotators. to the best of our knowledge,
tle is the    rst large scale english treebank con-
structed in a completely manual fashion.

3 annotator training

the treebank was annotated by six students,    ve
undergraduates and one graduate. among the un-
dergraduates, three are linguistics majors and two
are engineering majors with a linguistic minor.
the graduate student is a linguist specializing in
syntax. an additional graduate student in nlp
participated in the    nal debugging of the dataset.

2http://www.nltk.org/api/nltk.tokenize.html

3http://nlp.stanford.edu/software/tokenizer.shtml

prior to annotating the treebank sentences, the
annotators were trained for about 8 weeks. dur-
ing the training, the annotators attended tutorials
on dependency grammars, and learned the english
ud guidelines4, the id32 pos guide-
lines (santorini, 1990), the grammatical error an-
notation scheme of the fce (nicholls, 2003), as
well as the esl guidelines described in section 5
and in the annotation manual.

furthermore, the annotators completed six an-
notation exercises, in which they were required to
annotate pos tags and dependencies for practice
sentences from scratch. the exercises were done
individually, and were followed by group meet-
ings in which annotation disagreements were dis-
cussed and resolved. each of the    rst three exer-
cises consisted of 20 sentences from the ud gold
standard for english, the english web treebank
(ewt) (silveira et al., 2014). the remaining three
exercises contained 20-30 esl sentences from the
fce. many of the esl guidelines were introduced
or re   ned based on the disagreements in the esl
practice exercises and the subsequent group dis-
cussions. several additional guidelines were in-
troduced in the course of the annotation process.

during the training period, the annotators also
learned to use a search tool that enables formulat-
ing queries over word and pos tag sequences as
id157 and obtaining their annotation
statistics in the ewt. after experimenting with
both textual and graphical interfaces for perform-
ing the annotations, we converged on a simple text
based format described in section 4.1, where the
annotations were    lled in using a spreadsheet or
a text editor, and tested with a script for detect-
ing annotation typos. the annotators continued to
meet and discuss annotation issues on a weekly
basis throughout the entire duration of the project.

4 annotation procedure

the formation of the treebank was carried out in
four steps: annotation, review, disagreement reso-
lution and targeted debugging.

4.1 annotation
in the    rst stage, the annotators were given sen-
tences for annotation from scratch. we use a
conll based textual template in which each word
is annotated in a separate line. each line contains
6 columns, the    rst of which has the word index

4http://universaldependencies.org/#en

(ind) and the second the word itself (word).
the remaining four columns had to be    lled in
with a universal pos tag (upos), a penn tree-
bank pos tag (pos), a head word index (hind)
and a dependency relation (rel) according to ver-
sion 1 of the english ud guidelines.

the annotation section of the sentence is pre-
ceded by a metadata header. the    rst    eld in this
header, denoted with sent, contains the fce er-
ror coded version of the sentence. the annotators
were instructed to verify the error annotation, and
add new error annotations if needed. corrections
to the sentence segmentation are speci   ed in the
segment    eld5. further down, the    eld typo
is designated for literal annotation of spelling er-
rors and ill formed words that happen to form valid
words (see section 5.2).

the example below presents a pre-annotated

original sentence given to an annotator.

#sent=that time i had to sleep in <ns type=
"md"><c>a</c></ns> tent.
#segment=
#typo=

upos

pos

hind

rel

#ind
1
2
3
4
5
6
7
8
9

word
that
time
i
had
to
sleep
in
tent
.

upon completion of the original sentence, the
annotators proceeded to annotate the corrected
sentence version. to reduce annotation time, an-
notators used a script that copies over annotations
from the original sentence and updates head in-
dices of tokens that appear in both sentence ver-
sions. head indices and relation labels were    lled
in only if the head word of the token appeared in
both the original and corrected sentence versions.
tokens with automatically    lled annotations in-
cluded an additional # sign in a seventh column
of each word   s annotation. the # signs had to
be removed, and the corresponding annotations ei-
ther approved or changed as appropriate. tokens
that did not appear in the original sentence version
were annotated from scratch.

5the released version of the treebank splits the sentences
according to the markings in the segment    eld when those
apply both to the original and corrected versions of the sen-
tence. resulting segments without grammatical errors in the
original version are currently discarded.

4.2 review

5 annotation scheme for esl

all annotated sentences were randomly assigned
to a second annotator (henceforth reviewer), in a
double blind manner. the reviewer   s task was to
mark all the annotations that they would have an-
notated differently. to assist the review process,
we compiled a list of common annotation errors,
available in the released annotation manual.

the annotations were reviewed using an active
editing scheme in which an explicit action was re-
quired for all the existing annotations. the scheme
was introduced to prevent reviewers from over-
looking annotation issues due to passive approval.
speci   cally, an additional # sign was added at the
seventh column of each token   s annotation. the
reviewer then had to either    sign off    on the exist-
ing annotation by erasing the # sign, or provide an
alternative annotation following the # sign.

4.3 disagreement resolution

in the    nal stage of the annotation process all
annotator-reviewer disagreements were resolved
by a third annotator (henceforth judge), whose
main task was to decide in favor of the annotator
or the reviewer. similarly to the review process,
the judging task was carried out in a double blind
manner. judges were allowed to resolve annotator-
reviewer disagreements with a third alternative, as
well as introduce new corrections for annotation
issues overlooked by the reviewers.

another task performed by the judges was to
mark acceptable alternative annotations for am-
biguous structures determined through review dis-
agreements or otherwise present in the sentence.
these annotations were speci   ed in an additional
metadata    eld called ambiguity. the ambigu-
ity markings are provided along with the resolved
version of the annotations.

4.4 final debugging

after applying the resolutions produced by the
judges, we queried the corpus with debugging
tests for speci   c linguistics constructions. this
additional testing phase further reduced the num-
ber of annotation errors and inconsistencies in the
treebank. including the training period, the tree-
bank creation lasted over a year, with an aggregate
of more than 2,000 annotation hours.

our annotations use the existing inventory of en-
glish ud pos tags and dependency relations, and
follow the standard ud annotation guidelines for
english. however,
these guidelines were for-
mulated with grammatical usage of english in
mind and do not cover non canonical syntactic
structures arising due to grammatical errors6. to
encourage consistent and linguistically motivated
annotation of such structures, we formulated a
complementary set of esl annotation guidelines.
our esl annotation guidelines follow the gen-
eral principle of literal reading, which emphasizes
syntactic analysis according to the observed lan-
guage usage. this strategy continues a line of
work in sla which advocates for centering analy-
sis of learner language around morpho-syntactic
surface evidence (ragheb and dickinson, 2012;
dickinson and ragheb, 2013). similarly to our
framework, which includes a parallel annotation
of corrected sentences, such strategies are often
presented in the context of multi-layer annota-
tion schemes that also account for error corrected
sentence forms (hirschmann et al., 2007; d  az-
negrillo et al., 2010; rosen et al., 2014).

deploying a strategy of literal annotation within
ud, a formalism which enforces cross-linguistic
consistency of annotations, will enable meaning-
ful comparisons between non-canonical structures
in english and canonical structures in the author   s
native language. as a result, a key novel character-
istic of our treebank is its ability to support cross-
lingual studies of learner language.

5.1 literal annotation

with respect to id52, literal annotation im-
plies adhering as much as possible to the observed
morphological forms of the words. syntactically,
argument structure is annotated according to the
usage of the word rather than its typical distribu-
tion in the relevant context. the following list of
conventions de   nes the notion of literal reading
for some of the common non canonical structures
associated with grammatical errors.

argument structure
extraneous prepositions we annotate all nominal
dependents introduced by extraneous prepositions

6the english ud guidelines do address several issues en-
countered in informal genres, such as the relation    goeswith   ,
which is used for fragmented words resulting from typos.

as nominal modi   ers. in the following sentence,
   him    is marked as a nominal modi   er (nmod) in-
stead of an indirect object (iobj) of    give   .

#sent=...i had to give <ns type="ut"><i>to</i> </ns>
him water...
...
21
22
23
24
25
26
27
...

nsubj
parataxis
mark
xcomp
case
nmod
dobj

i
had
to
give
to
him
water

pron
verb
part
verb
adp
pron
noun

prp
vbd
to
vb
in
prp
nn

22
5
24
22
26
24
24

omitted prepositions we treat nominal depen-
dents of a predicate that are lacking a preposition
as arguments rather than nominal modi   ers. in the
example below,    money    is marked as a direct ob-
ject (dobj) instead of a nominal modi   er (nmod)
of    ask   . as    you    functions in this context as a
second argument of    ask   , it is annotated as an in-
direct object (iobj) instead of a direct object (dobj).

#sent=...i have to ask you <ns type="mt">
<c>for</c></ns> the money <ns type= "rt">
<i>of</i><c>for</c></ns> the tickets back.
...
12
13
14
15
16
17
18
19
20
21
22
23

i
have
to
ask
you
the
money
of
the
tickets
back
.

pron
verb
part
verb
pron
det
noun
adp
det
noun
adv
punct

prp
vbp
to
vb
prp
dt
nn
in
dt
nns
rb
.

13
2
15
13
15
18
15
21
21
18
15
2

nsubj
conj
mark
xcomp
iobj
det
dobj
case
det
nmod
advmod
punct

tense
cases of erroneous tense usage are annotated ac-
cording to the morphological tense of the verb.
for example, below we annotate    shopping   
with present participle vbg, while the correction
   shop    is annotated in the corrected version of the
sentence as vbp.

#sent=...when you <ns type="tv"><i>shopping</i>
<c>shop</c></ns>...
...
4
5
6
...

when
you
shopping

advmod
nsubj
advcl

adv
pron
verb

wrb
prp
vbg

6
6
12

word formation
erroneous word formations that are contextually
plausible and can be assigned with a ptb tag
are annotated literally. in the following example,
   stuffs    is handled as a plural count noun.

#sent=...into fashionable <ns type="cn">
<i>stuffs</i><c>stuff</c></ns>...
...
7
8
9
...

into
adp
fashionable adj
stuffs

in
jj
nns

noun

9
9
2

case
amod
ccomp

similarly, in the example below we annotate

   necessaryiest    as a superlative.

#sent=the necessaryiest things...
dt
1
jjs
2
3
nns
...

the
necessaryiest
things

det
adj
noun

3
3
0

det
amod
root

5.2 exceptions to literal annotation
although our general annotation strategy for esl
follows literal sentence readings, several types of
word formation errors make such readings unin-
formative or impossible, essentially forcing cer-
tain words to be annotated using some degree of
interpretation (ros  en and de smedt, 2010). we
hence annotate the following cases in the original
sentence according to an interpretation of an in-
tended word meaning, obtained from the fce er-
ror correction.

spelling
spelling errors are annotated according to the cor-
rectly spelled version of the word. to support error
analysis of automatic annotation tools, misspelled
words that happen to form valid words are anno-
tated in the metadata    eld typo for pos tags
with respect to the most common usage of the
misspelled word form. in the example below, the
typo    eld contains the typical pos annotation of
   where   , which is clearly unintended in the con-
text of the sentence.

#sent=...we <ns type="sx"><i>where</i>
<c>were</c></ns> invited to visit...
#typo=5 adv wrb
...
4
5
6
7
8
...

we
where
invited
to
visit

pron
aux
verb
part
verb

prp
vbd
vbn
to
vb

6
6
0
8
6

nsubjpass
auxpass
root
mark
xcomp

word formation
erroneous word formations that cannot be as-
signed with an existing ptb tag are annotated with
respect to the correct word form.

#sent=i am <ns type="iv"><i>writting</i>
<c>writing</c></ns>...
pron
1
aux
2
3
verb
...

i
am
writting

prp
vbp
vbg

3
3
0

nsubj
aux
root

in particular, ill formed adjectives that have a
plural suf   x receive a standard adjectival pos tag.
when applicable, such cases also receive an addi-
tional marking for unnecessary agreement in the
error annotation using the attribute    ua   .

#sent=...<ns type="ij" ua=true>
<i>interestings</i><c>interesting</c></ns> things...

...
6
7
...

interestings adj
things

noun

jj
nns

7
3 dobj

amod

wrong word formations that result in a valid,
but contextually implausible word form are also
annotated according to the word correction.
in
the example below, the nominal form    sale    is
likely to be an unintended result of an ill formed
verb. similarly to spelling errors that result in
valid words, we mark the typical literal pos an-
notation in the typo metadata    eld.

#sent=...they do not <ns type="dv"><i>sale</i>
<c>sell</c></ns> them...
#typo=15 noun nn

...
12
13
14
15
16
...

they
do
not
sale
them

pron
aux
part
verb
pron

prp
vbp
rb
vb
prp

15
15
15
0
15

nsubj
aux
neg
root
dobj

taken together, our esl conventions cover
many of the annotation challenges related to gram-
matical errors present in the tle. in addition to
the presented overview, the complete manual of
esl guidelines used by the annotators is pub-
licly available. the manual contains further details
on our annotation scheme, additional annotation
guidelines and a list of common annotation errors.
we plan to extend and re   ne these guidelines in
future releases of the treebank.

6 editing agreement

we utilize our two step review process to estimate
agreement rates between annotators7. we measure
agreement as the fraction of annotation tokens ap-
proved by the editor. table 2 presents the agree-
ment between annotators and reviewers, as well as
the agreement between reviewers and the judges.
agreement measurements are provided for both
the original the corrected versions of the dataset.

overall, the results indicate a high agreement
rate in the two editing tasks. importantly, the gap
between the agreement on the original and cor-
rected sentences is small. note that this result is
obtained despite the introduction of several esl
annotation guidelines in the course of the annota-
tion process, which inevitably increased the num-
ber of edits related to grammatical errors. we in-
terpret this outcome as evidence for the effective-
ness of the esl annotation scheme in supporting
consistent annotations of learner language.

7all experimental results on agreement and parsing ex-

clude punctuation tokens.

annotator-reviewer upos pos
original
98.35
corrected
98.61
reviewer-judge
original
corrected

99.68
99.77

98.83
99.02

99.72
99.80

hind rel
97.74
96.98
97.20
97.97

99.37
99.45

99.15
99.28

table 2: inter-annotator agreement on the entire
tle corpus. agreement is measured as the frac-
tion of tokens that remain unchanged after an edit-
ing round. the four evaluation columns corre-
spond to universal pos tags, ptb pos tags, un-
labeled attachment, and dependency labels. co-
hen   s kappa scores (cohen, 1960) for pos tags
and dependency labels in all evaluation conditions
are above 0.96.

7 parsing experiments

the tle enables studying parsing for learner lan-
guage and exploring relationships between gram-
matical errors and parsing performance. here, we
present parsing benchmarks on our dataset, and
provide several estimates for the extent to which
grammatical errors degrade the quality of auto-
matic id52 and id33.

our    rst experiment measures tagging and pars-
ing accuracy on the tle and approximates the
global impact of grammatical errors on automatic
annotation via performance comparison between
the original and error corrected sentence versions.
in this, and subsequent experiments, we utilize
version 2.2 of the turbo tagger and turbo parser
(martins et al., 2013), state of the art tools for sta-
tistical id52 and id33.

table 3 presents tagging and parsing results on
a test set of 500 tle sentences (9,591 original to-
kens, 9,700 corrected tokens). results are pro-
vided for three different training regimes. the
   rst regime uses the training portion of version 1.3
of the ewt, the ud english treebank, contain-
ing 12,543 sentences (204,586 tokens). the sec-
ond training mode uses 4,124 training sentences
(78,541 original tokens, 79,581 corrected tokens)
from the tle corpus. in the third setup we com-
bine these two training corpora. the remaining
500 tle sentences (9,549 original tokens, 9,695
corrected tokens) are allocated to a development
set, not used in this experiment. parsing of the test
sentences was performed on predicted pos tags.
the ewt training regime, which uses out of do-
main texts written in standard english, provides
the lowest performance on all the evaluation met-

test set train set
tleorig ewt
tlecorr ewt
tleorig tleorig
tlecorr tlecorr
tleorig ewt+tleorig
tlecorr ewt+tlecorr

upos pos uas
86.51
91.87
92.9
88.37
87.71
95.88
89.69
96.92
90.3
93.33
94.27
92.15

94.28
95.17
94.94
95.17
95.77
96.48

la
88.07
89.74
89.26
90.92
91.09
92.54

las
81.44
83.8
83.4
85.64
86.27
88.3

table 3: tagging and parsing results on a test set of
500 sentences from the tle corpus. ewt is the
english ud treebank. tleorig are original sen-
tences from the tle. tlecorr are the correspond-
ing error corrected sentences.

rics. an additional factor which negatively af-
fects performance in this regime are systematic
differences in the ewt annotation of possessive
pronouns, expletives and names compared to the
ud guidelines, which are utilized in the tle. in
particular, the ewt annotates possessive pronoun
upos as pron rather than det, which leads the
upos results in this setup to be lower than the
ptb pos results. improved results are obtained
using the tle training data, which, despite its
smaller size, is closer in genre and syntactic char-
acteristics to the tle test set. the strongest ptb
id52 and parsing results are obtained by
combining the ewt with the tle training data,
yielding 95.77 pos accuracy and a uas of 90.3
on the original version of the tle test set.

the dual annotation of sentences in their orig-
inal and error corrected forms enables estimating
the impact of grammatical errors on tagging and
parsing by examining the performance gaps be-
tween the two sentence versions. averaged across
the three training conditions, the id52 ac-
curacy on the original sentences is lower than the
accuracy on the sentence corrections by 1.0 upos
and 0.61 pos. parsing performance degrades by
1.9 uas, 1.59 la and 2.21 las.

to further elucidate the in   uence of grammati-
cal errors on parsing quality, table 4 compares per-
formance on tokens in the original sentences ap-
pearing inside grammatical error tags to those ap-
pearing outside such tags. although grammatical
errors may lead to tagging and parsing errors with
respect to any element in the sentence, we expect
erroneous tokens to be more challenging to ana-
lyze compared to grammatical tokens.

this comparison indeed reveals a substantial
difference between the two types of tokens, with
an average gap of 5.0 upos, 6.65 pos, 4.67 uas,
6.56 la and 7.39 las. note that differently from

train set

tokens
ungrammatical ewt
grammatical
ewt
ungrammatical tleorig
grammatical
tleorig
ungrammatical ewt+tleorig
grammatical
ewt+tleorig

upos pos uas
82.66
87.97
92.62
87.26
83.81
90.76
88.46
96.86
86.32
89.76
94.02
91.07

88.61
95.37
88.68
96.14
90.97
96.7

la
82.66
89.11
83.31
90.41
85.96
92.08

las
74.93
82.7
77.22
84.59
80.37
87.41

table 4: tagging and parsing results on the origi-
nal version of the tle test set for tokens marked
with grammatical errors (ungrammatical) and to-
kens not marked for errors (grammatical).

the global measurements in the    rst experiment,
this analysis, which focuses on the local impact
of remove/replace errors, suggests a stronger ef-
fect of grammatical errors on the dependency la-
bels than on the dependency structure.

finally, we measure tagging and parsing perfor-
mance relative to the fraction of sentence tokens
marked with grammatical errors. similarly to the
previous experiment, this analysis focuses on re-
move/replace rather than insert errors.

e
r
o
c
s
 
e
c
n
e
t
n
e
s
 
r
e
p
 
n
a
e
m

100

98

96

94

92

90

88

86

84

82

80

78

76

pos original
pos corrected
uas original
uas corrected
las original
las corrected
35-40
0-5
(362)
(175)
% of original sentence tokens marked as grammatical errors

10-15
(1050)

5-10
(1033)

15-20
(955)

25-30
(372)

30-35
(214)

20-25
(613)

figure 1: mean per sentence pos accuracy, uas
and las of the turbo tagger and turbo parser, as
a function of the percentage of original sentence
tokens marked with grammatical errors. the tag-
ger and the parser are trained on the ewt cor-
pus, and tested on all 5,124 sentences of the tle.
points connected by continuous lines denote per-
formance on the original tle sentences. points
connected by dashed lines denote performance on
the corresponding error corrected sentences. the
number of sentences whose errors fall within each
percentage range appears in parenthesis.

figure 1 presents the average sentential perfor-
mance as a function of the percentage of tokens
in the original sentence marked with grammati-

cal errors. in this experiment, we train the parser
on the ewt training set and test on the entire
tle corpus. performance curves are presented
for pos, uas and las on the original and error
corrected versions of the annotations. we observe
that while the performance on the corrected sen-
tences is close to constant, original sentence per-
formance is decreasing as the percentage of the er-
roneous tokens in the sentence grows.

overall, our results suggest a negative, albeit
limited effect of grammatical errors on parsing.
this outcome contrasts a study by geertzen et al.
(2013) which reported a larger performance gap of
7.6 uas and 8.8 las between sentences with and
without grammatical errors. we believe that our
analysis provides a more accurate estimate of this
impact, as it controls for both sentence content and
sentence length. the latter factor is crucial, since
it correlates positively with the number of gram-
matical errors in the sentence, and negatively with
parsing accuracy.

8 related work

previous studies on learner language proposed
several annotation schemes for both pos tags and
syntax (hirschmann et al., 2007; d  az-negrillo et
al., 2010; dickinson and ragheb, 2013; rosen et
al., 2014). the unifying theme in these proposals
is a multi-layered analysis aiming to decouple the
observed language usage from conventional struc-
tures in the foreign language.

in the context of esl, d  az et al. (2010) pro-
pose three parallel pos tag annotations for the
lexical, morphological and distributional forms of
each word.
in our work, we adopt the distinc-
tion between morphological word forms, which
roughly correspond to our literal word readings,
and distributional forms as the error corrected
words. however, we account for morphological
forms only when these constitute valid existing
ptb pos tags and are contextually plausible. fur-
thermore, while the internal structure of invalid
word forms is an interesting object of investiga-
tion, we believe that it is more suitable for anno-
tation as word features rather than pos tags. our
treebank supports the addition of such features to
the existing annotations.

the work of ragheb and dickinson (2009;
2012; 2013) proposes esl annotation guidelines
for pos tags and syntactic dependencies based on
the childes annotation framework. this ap-

proach, called    morphosyntactic dependencies    is
related to our annotation scheme in its focus on
surface structures. differently from this proposal,
our annotations are grounded in a parallel anno-
tation of grammatical errors and include an ad-
ditional layer of analysis for the corrected forms.
moreover, we refrain from introducing new syn-
tactic categories and dependency relations speci   c
to esl, thereby supporting computational treat-
ment of esl using existing resources for standard
english. at the same time, we utilize a multilin-
gual formalism which, in conjunction with our lit-
eral annotation strategy, facilitates linking the an-
notations to native language syntax.

while the above mentioned studies focus on an-
notation guidelines, attention has also been drawn
to the topic of parsing in the learner language do-
main. however, due to the shortage of syntactic
resources for esl, much of the work in this area
resorted to using surrogates for learner data. for
example, in foster (2007) and foster et al. (2008)
parsing experiments are carried out on synthetic
learner-like data, that was created by automatic in-
sertion of grammatical errors to well formed en-
glish text. in cahill et al. (2014) a treebank of sec-
ondary level native students texts was used to ap-
proximate learner text in order to evaluate a parser
that utilizes unlabeled learner data.

syntactic annotations for esl were previously
developed by nagata et al. (2011), who annotate
an english learner corpus with pos tags and shal-
low syntactic parses. our work departs from shal-
low syntax to full syntactic analysis, and provides
annotations on a signi   cantly larger scale. fur-
thermore, differently from this annotation effort,
our treebank covers a wide range of learner na-
tive languages. an additional syntactic dataset for
esl, currently not available publicly, are 1,000
sentences from the efcamdat dataset (geertzen
et al., 2013), annotated with stanford dependen-
cies (de marneffe and manning, 2008). this
dataset was used to measure the impact of gram-
matical errors on parsing by comparing perfor-
mance on sentences with grammatical errors to er-
ror free sentences. the tle enables a more direct
way of estimating the magnitude of this perfor-
mance gap by comparing performance on the same
sentences in their original and error corrected ver-
sions. our comparison suggests that the effect of
grammatical errors on parsing is smaller that the
one reported in this study.

9 conclusion

the    rst

we present
large scale treebank of
learner language, manually annotated and double-
reviewed for pos tags and universal dependen-
cies. the annotation is accompanied by a linguis-
tically motivated framework for handling syntactic
structures associated with grammatical errors. fi-
nally, we benchmark automatic tagging and pars-
ing on our corpus, and measure the effect of gram-
matical errors on tagging and parsing quality. the
treebank will support empirical study of learner
syntax in nlp, corpus linguistics and second lan-
guage acquisition.

10 acknowledgements

we thank anna korhonen for helpful discussions
and insightful comments on this paper. we also
thank dora alexopoulou, andrei barbu, markus
dickinson, sue felshin, jeroen geertzen, yan
huang, detmar meurers, sampo pyysalo, roi re-
ichart and the anonymous reviewers for valuable
feedback on this work. this material is based
upon work supported by the center for brains,
minds, and machines (cbmm), funded by nsf
stc award ccf-1231216.

references
aoife cahill, binod gyawali, and james v bruno.
2014. self-training for parsing learner text.
in
proceedings of the first joint workshop on statisti-
cal parsing of morphologically rich languages and
syntactic analysis of non-canonical languages,
pages 66   73.

jacob cohen. 1960. a coef   cient of agreement for
nominal scales. educational and psychological
measurement, 20(1):37.

david crystal. 2003. english as a global language.

ernst klett sprachen.

marie-catherine de marneffe and christopher d man-
ning. 2008. stanford typed dependencies manual.
technical report, technical report, stanford univer-
sity.

marie-catherine de marneffe, timothy dozat, na-
talia silveira, katri haverinen, filip ginter, joakim
nivre, and christopher d manning. 2014. univer-
sal stanford dependencies: a cross-linguistic typol-
ogy. in proceedings of lrec, pages 4585   4592.

markus dickinson and marwa ragheb. 2009. depen-
dency annotation for learner corpora.
in proceed-
ings of the eighth workshop on treebanks and lin-
guistic theories (tlt-8), pages 59   70.

markus dickinson and marwa ragheb. 2013. annota-
tion for learner english guidelines, v. 0.1. technical
report, indiana university, bloomington, in, june.
june 9, 2013.

jennifer foster, joachim wagner, and josef van gen-
abith. 2008. adapting a wsj-trained parser to gram-
matically noisy text.
in proceedings of the 46th
annual meeting of the association for computa-
tional linguistics on human language technolo-
gies: short papers, pages 221   224. association for
computational linguistics.

jennifer foster. 2007. treebanks gone bad. interna-
tional journal of document analysis and recogni-
tion (ijdar), 10(3-4):129   145.

jeroen geertzen, theodora alexopoulou, and anna
korhonen. 2013. automatic linguistic annotation
of large scale l2 databases: the ef-cambridge open
language database (efcamdat). in proceedings of the
31st second language research forum. somerville,
ma: cascadilla proceedings project.

hagen hirschmann, seanna doolittle, and anke
syntactic annotation of non-

l  udeling.
canonical linguistic structures.

2007.

andr  e ft martins, miguel almeida, and noah a
smith. 2013. turning on the turbo: fast third-order
non-projective turbo parsers.
in acl (2), pages
617   622. citeseer.

ryan t mcdonald, joakim nivre, yvonne quirmbach-
brundage, yoav goldberg, dipanjan das, kuzman
ganchev, keith b hall, slav petrov, hao zhang,
oscar t  ackstr  om, et al. 2013. universal depen-
dency annotation for multilingual parsing. in acl
(2), pages 92   97. citeseer.

ryo nagata, edward whittaker, and vera shein-
2011. creating a manually error-tagged
man.
and shallow-parsed learner corpus.
in proceed-
ings of the 49th annual meeting of the association
for computational linguistics: human language
technologies-volume 1, pages 1210   1219. associ-
ation for computational linguistics.

hwee tou ng, siew mei wu, ted briscoe, christian
hadiwinoto, raymond hendy susanto, and christo-
pher bryant. 2014. the conll-2014 shared task
on grammatical error correction. in conll shared
task, pages 1   14.

ana d  az-negrillo, detmar meurers, salvador valera,
and holger wunsch. 2010. towards interlanguage
pos annotation for effective learner corpora in sla
and    t. language forum, 36(1   2):139   154.

diane nicholls. 2003. the cambridge learner corpus:
error coding and analysis for id69 and elt.
in proceedings of the corpus linguistics 2003 con-
ference, pages 572   581.

joakim nivre, marie-catherine de marneffe, filip gin-
ter, yoav goldberg, jan haji  c, christopher man-
ning, ryan mcdonald, slav petrov, sampo pyysalo,
natalia silveira, et al. 2016. universal dependen-
cies v1: a multilingual treebank collection. in pro-
ceedings of the 10th international conference on
language resources and evaluation (lrec 2016).

marwa ragheb and markus dickinson. 2012. de   n-
ing syntax for learner language annotation. in col-
ing (posters), pages 965   974.

victoria ros  en and koenraad de smedt. 2010. syn-
tactic annotation of learner corpora. systematisk,
variert, men ikke tilfeldig, pages 120   132.

alexandr rosen, jirka hana, barbora   stindlov  a, and
anna feldman. 2014. evaluating and automating
the annotation of a learner corpus. language re-
sources and evaluation, 48(1):65   92.

beatrice santorini.

part-of-speech tagging
guidelines for the id32 project (3rd revi-
sion). technical reports (cis).

1990.

natalia silveira, timothy dozat, marie-catherine
de marneffe, samuel r bowman, miriam connor,
john bauer, and christopher d manning. 2014. a
gold standard dependency corpus for english.
in
proceedings of the ninth international conference
on language resources and evaluation (lrec-
2014).

joel tetreault, jennifer foster, and martin chodorow.
2010. using parse features for preposition selection
and error detection. in proceedings of the acl 2010
conference short papers, pages 353   358. associa-
tion for computational linguistics.

helen yannakoudakis, ted briscoe, and ben medlock.
2011. a new dataset and method for automatically
grading esol texts. in acl, pages 180   189.

