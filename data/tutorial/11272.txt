wikireading: a novel large-scale language understanding task over

wikipedia

daniel hewlett, alexandre lacoste, llion jones, illia polosukhin,
andrew fandrianto, jay han, matthew kelcey and david berthelot

{dhewlett,allac,llion,ipolosukhin,fto,hanjay,matkelcey,dberth}@google.com

google research

7
1
0
2

 
r
a

 

m
5
1

 
 
]
l
c
.
s
c
[
 
 

2
v
2
4
5
3
0

.

8
0
6
1
:
v
i
x
r
a

abstract

we present wikireading, a large-scale
natural language understanding task and
publicly-available dataset with 18 million
instances. the task is to predict textual
values from the structured knowledge base
wikidata by reading the text of the cor-
responding wikipedia articles. the task
contains a rich variety of challenging clas-
si   cation and extraction sub-tasks, mak-
ing it well-suited for end-to-end models
such as deep neural networks (dnns).
we compare various state-of-the-art dnn-
based architectures for document classi   -
cation, information extraction, and ques-
tion answering. we    nd that models sup-
porting a rich answer space, such as word
or character sequences, perform best. our
best-performing model, a word-level se-
quence to sequence model with a mecha-
nism to copy out-of-vocabulary words, ob-
tains an accuracy of 71.8%.

introduction

1
a growing amount of research in natural language
understanding (nlu) explores end-to-end deep
neural network (dnn) architectures for tasks such
as text classi   cation (zhang et al., 2015), rela-
tion extraction (nguyen and grishman, 2015), and
id53 (weston et al., 2015). these
models offer the potential to remove the interme-
diate steps traditionally involved in processing nat-
ural language data by operating on increasingly
raw forms of text input, even unprocessed char-
acter or byte sequences. furthermore, while these
tasks are often studied in isolation, dnns have the
potential to combine multiple forms of reasoning
within a single model.

supervised training of dnns often requires a

large amount of high-quality training data. to this
end, we introduce a novel prediction task and ac-
companying large-scale dataset with a range of
sub-tasks combining text classi   cation and infor-
mation extraction. the dataset is made publicly-
available at http://goo.gl/wikireading.
the task, which we call wikireading, is to pre-
dict textual values from the open knowledge base
wikidata (vrande  ci  c and kr  otzsch, 2014) given
text from the corresponding articles on wikipedia
(ayers et al., 2008). example instances are shown
in table 1, illustrating the variety of subject mat-
ter and sub-tasks. the dataset contains 18.87m in-
stances across 867 sub-tasks, split roughly evenly
between classi   cation and extraction (see section
2 for more details).

in addition to its diversity, the wikireading
dataset is also at least an order of magnitude larger
than related nlu datasets. many natural lan-
guage datasets for id53 (qa), such
as wikiqa (yang et al., 2015), have only thou-
sands of examples and are thus too small for train-
ing end-to-end models. hermann et al. (2015)
proposed a task similar to qa, predicting entities
in news summaries from the text of the original
news articles, and generated a news dataset with
1m instances. the babi dataset (weston et al.,
2015) requires multiple forms of reasoning, but is
composed of synthetically generated documents.
wikiqa and news only involve pointing to lo-
cations within the document, and text classi   ca-
tion datasets often have small numbers of output
classes. in contrast, wikireading has a rich out-
put space of millions of answers, making it a chal-
lenging benchmark for state-of-the-art dnn archi-
tectures for qa or text classi   cation.

we implemented a large suite of recent models,
and for the    rst time evaluate them on common
grounds, placing the complexity of the task in con-
text and illustrating the tradeoffs inherent in each

document

categorization

towers

are
folkart
twin skyscrapers in the
bayrakli district of the
izmir.
turkish city of
reaching a
structural
height of 200 m (656 ft)
above ground level, they
are the tallest . . .

is a
angeles blancos
mexican telenovela pro-
duced by carlos so-
tomayor for televisa in
1990.
jacqueline an-
dere, rogelio guerra
and alfonso iturralde
star as the main . . .

canada is a country
in the northern part of
north america.
its ten
provinces and three ter-
ritories extend from the
atlantic to the paci   c
and northward into the
arctic ocean, . . .

extraction

property

country

answer

turkey

language

of

original
work
spanish

located next to body of
water
atlantic ocean, arctic
ocean, paci   c ocean

an
breaking bad is
american crime drama
television series created
and produced by vince
gilligan.
the show
originally aired on the
amc network for    ve
seasons, from january
20, 2008, to . . .
start time

20 january 2008

table 1: examples instances from wikireading. the task is to predict the answer given the document and property. answer
tokens that can be extracted are shown in bold, the remaining instances require classi   cation or another form of id136.

approach. the highest score of 71.8% is achieved
by a sequence to sequence model (kalchbrenner
and blunsom, 2013; cho et al., 2014) operating on
word-level input and output sequences, with spe-
cial handing for out-of-vocabulary words.

2 wikireading

we now provide background information relating
to wikidata, followed by a detailed description of
the wikireading prediction task and dataset.

2.1 wikidata
wikidata is a free collaborative knowledge
base containing information about approximately
16m items (vrande  ci  c and kr  otzsch, 2014).
knowledge related to each item is expressed
in a set of statements, each consisting of a
(property, value)
for example,
the item paris might have associated state-
ments asserting (instance of, city) or
(country, france). wikidata contains over
80m such statements across over 800 properties.
items may be linked to articles on wikipedia.

tuple.

2.2 dataset
we constructed the wikireading dataset from
wikidata and wikipedia as follows: we consoli-
dated all wikidata statements with the same item
and property into a single (item, property,
answer) triple, where answer is a set of val-
ues. replacing each item with the text of
the linked wikipedia article (discarding unlinked
items) yields a dataset of 18.58m (document,
property, answer) instances. importantly,
all elements in each instance are human-readable
strings, making the task entirely textual. the
only modi   cation we made to these strings was to

convert timestamps into a human-readable format
(e.g.,    4 july 1776   ).

the wikireading task, then, is to predict
the answer string for each tuple given the doc-
ument and property strings. this setup can be
seen as similar to information extraction, or ques-
tion answering where the property acts as a    ques-
tion   . we assigned each instance randomly to ei-
ther training (around 16.03m instances), valida-
tion (1.89m), and test (0.95m) sets following a
85/10/5 distribution.

2.3 documents
the dataset contains 4.7m unique wikipedia ar-
ticles, meaning that roughly 80% of the english-
language wikipedia is represented. multiple in-
stances can share the same document, with a mean
of 5.31 instances per article (median: 4, max:
879). the most common categories of docu-
ments are human, taxon, film, album, and
human settlement, making up 48.8% of the
documents and 9.1% of the instances. the mean
and median document lengths are 489.2 and 203
words.

2.4 properties
the dataset contains 867 unique properties,
though the distribution of properties across in-
stances is highly skewed: the top 20 proper-
ties cover 75% of the dataset, with 99% cov-
erage achieved after 180 properties. we divide
the properties broadly into two groups: categor-
ical properties, such as instance of, gender
and country, require selecting between a rel-
atively small number of possible answers, while
relational properties, such as date of birth,
parent, and capital,
typically require ex-

property
instance of
sex or gender
country
date of birth
given name
occupation
country of citizenship
located in . . . entity
place of birth
date of death

frequency
3,245,856
1,143,126
986,587
953,481
932,147
869,333
819,301
582,110
467,066
442,514

id178
0.429
0.186
0.518
0.916
0.755
0.588
0.508
0.800
0.795
0.922

table 2: training set frequency and scaled answer id178
for the 10 most frequent properties.

tracting rare or totally unique answers from the
document.

to quantify this difference, we compute the en-
tropy of the answer distribution a for each prop-
erty p, scaled to the [0, 1] range by dividing by the
id178 of a uniform distribution with the same
number of values, i.e.,   h(p) = h(ap)/ log |ap|.
properties that represent essentially one-to-one
mappings score near 1.0, while a property with
just a single answer would score 0.0. table 2 lists
id178 values for a subset of properties, showing
that the dataset contains a spectrum of sub-tasks.
we label properties with an id178 less than 0.7
as categorical, and those with a higher id178 as
relational. categorical properties cover 56.7% of
the instances in the dataset, with the remaining
43.3% being relational.

2.5 answers
the distribution of properties described above has
implications for the answer distribution. there are
a relatively small number of very high frequency
   head    answers, mostly for categorical properties,
and a vast number of very low frequency    tail    an-
swers, such as names and dates. at the extremes,
the most frequent answer human accounts for al-
most 7% of the dataset, while 54.7% of the an-
swers in the dataset are unique. there are some
special categories of answers which are systemati-
cally related, in particular dates, which comprise
8.9% of the dataset (with 7.2% being unique).
this distribution means that methods focused on
either head or tail answers can each perform mod-
erately well, but only a method that handles both
types of answers can achieve maximum perfor-
mance. another consequence of the long tail of
answers is that many (30.0%) of the answers in the
test set never appear in the training set, meaning
they must be read out of the document. an answer
is present verbatim in the document for 45.6% of

the instances.

3 methods
recently, neural network architectures for nlu
have been shown to meet or exceed the perfor-
mance of traditional methods (zhang et al., 2015;
dai and le, 2015). the move to deep neural
networks also allows for new ways of combin-
ing the property and document, inspired by recent
research in the    eld of id53 (with
the property serving as a question).
in sequen-
tial models such as recurrent neural networks
(id56s), the question could be prepended to the
document, allowing the model to    read    the doc-
ument differently for each question (hermann et
al., 2015). alternatively, the question could be
used to compute a form of attention (bahdanau et
al., 2014) over the document, to effectively focus
the model on the most predictive words or phrases
(sukhbaatar et al., 2015; hermann et al., 2015).
as this is currently an ongoing    eld of research,
we implemented a range of recent models and for
the    rst time compare them on common grounds.
we now describe these methods, grouping them
into broad categories by general approach and not-
ing necessary modi   cations. later, we introduce
some novel variations of these models.

3.1 answer classi   cation
perhaps the most straightforward approach to
wikireading is to consider it as a special case
of document classi   cation. to    t wikiread-
ing into this framework, we consider each pos-
sible answer as a class label, and incorporate fea-
tures based on the property so that the model can
make different predictions for the same document.
while the number of potential answers is too large
to be practical (and unbounded in principle), a sub-
stantial portion of the dataset can be covered by a
model with a tractable number of answers.
3.1.1 baseline
the most common approach to document classi-
   cation is to    t a linear model (e.g., logistic re-
gression) over bag of words (bow) features. to
serve as a baseline for our task, the linear model
needs to make different predictions for the same
wikipedia article depending on the property. we
enable this behavior by computing two nw ele-
ment bow vectors, one each for the document
and property, and concatenating them into a sin-
gle 2nw feature vector.

3.1.2 neural network methods
all of the methods described in this section en-
code the property and document into a joint rep-
resentation y     rdout, which serves as input for
a    nal softmax layer computing a id203 dis-
tribution over the top nans answers. namely, for
each answer i     {1, . . . , nans}, we have:
j=1 ey(cid:62)aj ,

p (i|x) = ey(cid:62)ai/(cid:80)nans

(1)
where ai     rdout corresponds to a learned vec-
tor associated with answer i. thus, these models
differ primarily in how they combine the property
and document to produce the joint representation.
for existing models from the literature, we provide
a brief description and note any important differ-
ences in our implementation, but refer the reader
to the original papers for further details.

except for character-level models, documents
and properties are tokenized into words. the nw
most frequent words are mapped to a vector in
rdin using a learned embedding matrix1. other
words are all mapped to a special out of vocabu-
lary (oov) token, which also has a learned em-
bedding. din and dout are hyperparameters for
these models.

averaged embeddings (bow): this is the neu-
ral network version of the baseline method de-
scribed in section 3.1.1. embeddings for words
in the document and property are separately aver-
aged. the concatenation of the resulting vectors
forms the joint representation of size 2din.
paragraph vector: we explore a variant of the
previous model where the document is encoded as
a paragraph vector (le and mikolov, 2014). we
apply the pv-dbow variant that learns an embed-
ding for a document by optimizing the prediction
of its constituent words. these unsupervised doc-
ument embeddings are treated as a    xed input to
the supervised classi   er, with no    ne-tuning.

lstm reader: this model is a simpli   ed ver-
sion of the deep lstm reader proposed by her-
mann et al. (2015).
in this model, an lstm
(hochreiter and schmidhuber, 1997) reads the
property and document sequences word-by-word
and the    nal state is used as the joint representa-
tion. this is the simplest model that respects the

1limited

experimentation with

from
publicly-available id97 embeddings (mikolov et al.,
2013) yielded no improvement in performance.

initialization

order of the words in the document. in our imple-
mentation we use a single layer instead of two and
a larger hidden size. more details on the architec-
ture can be found in section 4.1 and in table 4.

attentive reader: this model, also presented
in hermann et al. (2015), uses an attention mech-
anism to better focus on the relevant part of the
document for a given property. speci   cally, at-
tentive reader    rst generates a representation u of
the property using the    nal state of an lstm while
a second lstm is used to read the document and
generate a representation zt for each word. then,
conditioned on the property encoding u, a normal-
ized attention is computed over the document to
produce a weighted average of the word represen-
tations zt, which is then used to generate the joint
representation y. more precisely:

mt = tanh(w1 concat(zt, u))
  t = exp (v

(cid:124)

r =(cid:80)

  t(cid:80)

mt)
zt

       

t

y = tanh(w2 concat(r, u)),

where w1, w2, and v are learned parameters.

memory network: our implementation closely
follows the end-to-end memory network pro-
posed in sukhbaatar et al. (2015). this model
maps a property p and a list of sentences
x1, . . . , xn to a joint representation y by attend-
ing over sentences in the document as follows:
the input encoder i converts a sequence of words
xi = (xi1, . . . , xili) into a vector using an embed-
ding matrix (equation 2), where li is the length of
sentence i.2 the property is encoded with the em-
bedding matrix u (eqn. 3). each sentence is en-
coded into two vectors, a memory vector (eqn. 4)
and an output vector (eqn. 5), with embedding ma-
trices m and c, respectively. the property encod-
ing is used to compute a normalized attention vec-
tor over the memories (eqn. 6).3 the joint repre-
sentation is the sum of the output vectors weighted

2our    nal results use the position encoding method pro-
posed by sukhbaatar et al. (2015), which incorporates posi-
tional information in addition to id27s.

3instead of the linearization method of sukhbaatar et al.
(2015), we applied an id178 regularizer for the softmax at-
tention as described in kurach et al. (2015).

swers, the word sequences in the document and
answer must fully match4. instances where no an-
swer appears in the document are discarded for
training. the cost function is the average cross-
id178 for the outputs across the sequence. when
performing id136 on the test set, sequences of
consecutive locations scoring above a threshold
are chunked together as a single answer, and the
top-scoring answer is recorded for submission.5

3.3 sequence to sequence
recently,
sequence to sequence learning (or
id195) has shown promise for natural language
tasks, especially machine translation (cho et al.,
2014). these models combine two id56s: an en-
coder, which transforms the input sequence into a
vector representation, and a decoder, which con-
verts the encoder vector into a sequence of output
tokens, one token at a time. this makes them ca-
pable, in principle, of approximating any function
mapping sequential inputs to sequential outputs.
importantly, they are the    rst model we consider
that can perform any combination of answer clas-
si   cation and extraction.

3.3.1 basic id195
this model resembles lstm reader augmented
with a second id56 to decode the answer as a se-
quence of words. the embedding matrix is shared
across the two id56s but their state to state tran-
sition matrices are different (figure 1b). this
method extends the set of possible answers to any
sequence of words from the document vocabulary.

3.3.2 placeholder id195
while basic id195 already expands the expres-
siveness of lstm reader, it still has a limited
vocabulary and thus is unable to generate some
answers. as mentioned in section 3.2, id56 la-
beler can extract any sequence of words present in
the document, even if some are oov. we extend
the basic id195 model to handle oov words by
adding placeholders to our vocabulary, increasing
the vocabulary size from nw to nw + ndoc. then,
when an oov word occurs in the document, it
is replaced at random (without replacement). by
one of these placeholders. we also replace the
corresponding oov words in the target output se-

4dates were matched semantically to increase recall.
5we chose an arbitrary threshold of 0.5 for chunking. the
score of each chunk is obtained from the harmonic mean of
the predicted probabilities of its elements.

figure 1: illustration of id56 models. blocks with same
color share parameters. red words are out of vocabulary and
all share a common embedding.

by this attention (eqn. 7).

i(xi, w ) =(cid:80)

j w xij

u = i(p, u )

mi = i(xi, m )
ci = i(xi, c)
pi = softmax(q

y = u +(cid:80)

i pici

(cid:124)

mi)

(2)
(3)
(4)
(5)
(6)
(7)

(e.g.,

3.2 answer extraction
relational properties involve mappings between
entities
arbitrary
date of birth,
mother,
and author) and thus are less
amenable to document classi   cation. for these,
approaches
from information extraction (es-
pecially id36) are much more
appropriate.
in general, these methods seek to
identify a word or phrase in the text that stands
in a particular relation to a (possibly implicit)
subject. section 5 contains a discussion of prior
work applying nlp techniques involving entity
recognition and syntactic parsing to this problem.
id56s provide a natural    t for extraction, as
they can predict a value at every position in a
sequence, conditioned on the entire previous se-
quence. the most straightforward application to
wikireading is to predict the id203 that a
word at a given location is part of an answer. we
test this approach using an id56 that operates on
the sequence of words. at each time step, we use a
sigmoid activation for estimating whether the cur-
rent word is part of the answer or not. we refer
to this model as the id56 labeler and present it
graphically in figure 1a.

for training, we label all locations where any
answer appears in the document with a 1, and
other positions with a 0 (similar to distant super-
vision (mintz et al., 2009)). for multi-word an-

ada,daughteroflordbyronparent<sep>00001100lordbyron<go><end>byronlord(a) id56 labeler:(b) basic id195:(c) id195 with placeholders:ada,daughteroflordbyronparent<sep>lordph_7<go><end>ph_7lordph_3,daughteroflordph_7parent<sep>3.3.4 character id195 with pretraining
unfortunately, at the character level the length
of all sequences (documents, properties, and an-
swers) is greatly increased. this adds more se-
quential steps to the id56, requiring gradients to
propagate further, and increasing the chance of an
error during decoding. to address this issue in a
classi   cation context, dai and le (2015) showed
that initializing an lstm classi   er with weights
from a language model (lm) improved its accu-
racy. inspired by this result, we apply this prin-
ciple to the character id195 model with a two-
phase training process: in the    rst phase, we train
a character-level lm on the input character se-
quences from the wikireading training set (no
new data is introduced). in the second phase, the
weights from this lm are used to initialize the
   rst layer of the encoder and the decoder (purple
and green blocks in figure 2). after initialization,
training proceeds as in the basic character id195
model.

4 experiments

we evaluated all methods from section 3 on the
full test set with a single scoring framework. an
answer is correct when there is an exact string
match between the predicted answer and the gold
answer. however, as describe in section 2.2, some
answers are composed from a set of values (e.g.
third example in table 1). to handle this, we de-
   ne the mean f1 score as follows: for each in-
stance, we compute the f1-score (harmonic mean
of precision and recall) as a measure of the degree
of overlap between the predicted answer set and
the gold set for a given instance. the resulting per-
instance f1 scores are then averaged to produce a
single dataset-level score. this allows a method
to obtain partial credit for an instance when it an-
swers with at least one value from the golden set.
in this paper, we only consider methods for an-
swering with a single value, and most answers in
the dataset are also composed of a single value, so
this mean f1 metric is closely related to accuracy.
more precisely, a method using a single value as
answer is bounded by a mean f1 of 0.963.

4.1 training details
we implemented all models in a single frame-
work based on tensorflow (abadi et al., 2015)
with shared pre-processing and comparable hyper-
parameters whenever possible. all documents are

figure 2: character id195 model. blocks with the same
color share parameters. the same example as in figure 1 is
fed character by character.

quence by the same placeholder,6 as shown in fig-
ure 1c. luong et al. (2015) developed a similar
procedure for dealing with rare words in machine
translation, copying their locations into the output
sequence for further processing.

this makes the input and output sequences a
mixture of known words and placeholders, and al-
lows the model to produce any answer the id56
labeler can produce, in addition to the ones that
the basic id195 model could already produce.
this approach is comparable to entity anonymiza-
tion used in hermann et al. (2015), which replaces
named entities with random ids, but simpler be-
cause we use word-level placeholders without en-
tity recognition.

3.3.3 basic character id195
another way of handling rare words is to process
the input and output text as sequences of charac-
ters or bytes. id56s have shown some promise
working with character-level
including
state-of-the-art performance on a wikipedia text
classi   cation benchmark (dai and le, 2015). a
model that outputs answers character by character
can in principle generate any of the answers in the
test set, a major advantage for wikireading.

input,

this model, shown in figure 2, operates only on
sequences of mixed-case characters. the property
encoder id56 transforms the property, as a charac-
ter sequence, into a    xed-length vector. this prop-
erty encoding becomes the initial hidden state for
the second layer of a two-layer document encoder
id56, which reads the document, again, charac-
ter by character. finally, the answer decoder id56
uses the    nal state of the previous id56 to decode
the character sequence for the answer.

6the same oov word may occur several times in the
document. our simpli   ed approach will attribute a different
placeholder for each of these and will use the    rst occurrence
for the target answer.

lo<go>rada, .olpar         dau entn<end>    method
answer classi   er
sparse bow baseline
averaged embeddings
paragraph vector
lstm reader
attentive reader
memory network
answer extraction
id56 labeler
sequence to sequence
basic id195
placeholder id195
character id195
character id195 (lm)

mean f1

bound categorical relational

date

params

0.438
0.583
0.552
0.680
0.693
0.612

0.357

0.708
0.718
0.677
0.699

0.831

0.471

0.925
0.948
0.963
0.963

0.725
0.849
0.787
0.880
0.886
0.861

0.240

0.844
0.835
0.841
0.851

0.063
0.234
0.227
0.421
0.441
0.288

0.536

0.530
0.565
0.462
0.501

0.004
0.080
0.033
0.311
0.337
0.055

0.626

0.738
0.730
0.731
0.733

500.5m
120m
30m
45m
56m
90.1m

41m

32m
32m
4.1m
4.1m

table 3: results for all methods described in section 3 on the test set. f1 is the mean f1 score described in 4. bound is the
upper bound on mean f1 imposed by constraints in the method (see text for details). the remaining columns provide score
breakdowns by property type and the number of model parameters.

truncated to the    rst 300 words except for charac-
ter id195, which uses 400 characters. the em-
bedding matrix used to encode words in the doc-
ument uses din = 300 dimensions for the nw =
100, 000 most frequent words. similarly, answer
classi   cation over the nans = 50, 000 most fre-
quent answers is performed using an answer rep-
resentation of size dout = 300.7 the    rst 10
words of the properties are embedded using the
document embedding matrix. following cho et
al. (2014), id56s in id195 models use a gru
cell with a hidden state size of 1024. more details
on parameters are reported in table 4.

method

sparse
bow
baseline
paragraph
vector
character
id195
all others

emb.
dims

doc.
length

property
length

n/a

n/a

30

300

300
words

n/a
400
chars
300
words

10 words

10 words

20 chars

10 words

doc.
vocab.
size
50k
words

n/a
76
chars
100k
words

table 4: structural model parameters. note that the para-
graph vector method uses the output from a separate, unsu-
pervised model as a document encoding, which is not counted
in these parameters.

optimization was performed with the adam
stochastic optimizer8 (kingma and adam, 2015)
over mini-batches of 128 samples. gradient clip-
ping 9 (graves, 2013) is used to prevent instability
in training id56s. we performed a search over
7for models like averaged embedding and paragraph
vector, the concatenation imposes a greater dout.
8using   1 = 0.9,   2 = 0.999 and   = 10   8.
9when the norm of gradient g exceeds a threshold c, it is

50 randomly-sampled hyperparameter con   gura-
tions for the learning rate and gradient clip thresh-
old, selecting the one with the highest mean f1
on the validation set. learning rate and clipping
threshold are sampled uniformly, on a logarithmic
scale, over the range [10   5, 10   2] and [10   3, 101]
respectively.

4.2 results and discussion
results for all models on the held-out set of test in-
stances are presented in table 3. in addition to the
overall mean f1 scores, the model families differ
signi   cantly in mean f1 upper bound, and their
relative performance on the relational and categor-
ical properties de   ned in section 2.4. we also re-
port scores for properties containing dates, a sub-
set of relational properties, as a separate column
since they have a distinct format and organization.
for examples of model performance on individual
properties, see table 5.

as expected, all classi   er models perform well
for categorical properties, with more sophisticated
classi   ers generally outperforming simpler ones.
the difference in precision reading ability be-
tween models that use broad document statistics,
like averaged embeddings and paragraph vectors,
and the id56-based classi   ers is revealed in the
scores for relational and especially date proper-
ties. as shown in table 5, this difference is mag-
ni   ed in situations that are more dif   cult for a
classi   er, such as relational properties or proper-
ties with fewer training examples, where attentive
reader outperforms averaged embeddings by a
wide margin. this model family also has a high
scaled down i.e. g     g    min

(cid:16)

(cid:17)

.

1, c||g||

property

test
in-
stances

54624
34250
15494
2356

191157
66875
8117
890

categorical properties
instance of
sex or gen-
der
genre
instrument
relational properties
given
name
located in
parent
taxon
author
date properties
date
birth
date
death
publication
date
date
of   cial
opening

56221

26213

7680

269

of

of

of

averaged
embed-
dings

attentive
reader

memory
network

mean f1
basic
id195

placeholder
id195

character
id195

character
id195
(lm)

0.8545
0.9917
0.5320
0.7621

0.4973
0.4140
0.1990
0.0309

0.0626

0.0417

0.3909

0.8978
0.9966
0.6225
0.8415

0.8486
0.6195
0.3467
0.2088

0.3677

0.2949

0.5549

0.8720
0.9936
0.5625
0.7886

0.7206
0.4832
0.2077
0.1050

0.0016

0.0506

0.4851

0.8877
0.9968
0.5511
0.8377

0.8669
0.5484
0.2044
0.6094

0.8306

0.7974

0.5988

0.8775
0.9952
0.5260
0.8172

0.8868
0.6978
0.7997
0.6572

0.8259

0.7874

0.5902

0.8548
0.9943
0.5096
0.7529

0.8606
0.5496
0.4979
0.1403

0.8294

0.7897

0.5903

0.8659
0.9941
0.5283
0.7832

0.8729
0.6365
0.5748
0.3748

0.8303

0.7924

0.5943

0.1510

0.3047

0.1725

0.3333

0.3012

0.1457

0.1635

table 5: property-level mean f1 scores on the test set for selected methods and properties. for each property type, the two
most frequent properties are shown followed by two less frequent properties to illustrate long-tail behavior.

id56 labeler, shows a complementary set of
strengths, performing better on relational proper-
ties than categorical ones. while the mean f1 up-
per bound for this model is just 0.434 because it
can only produce answers that are present verba-
tim in the document text, it manages to achieve
most of this potential. the improvement on date
properties over the classi   er models demonstrates
its ability to identify answers that are typically
present in the document. we suspect that answer
extraction may be simpler than answer classi   ca-
tion because the model can learn robust patterns
that indicate a location without needing to learn
about each answer, as the classi   er models must.
the sequence to sequence models show a
greater degree of balance between relational and
categorical properties, reaching performance con-
sistent with classi   ers on the categorical questions
and with id56 labeler on relational questions.
placeholder id195 can in principle produce any
answer that id56 labeler can, and the perfor-
mance on relational properties is indeed similar.
as shown in table 5, placeholder id195 per-
forms especially well for properties where the an-
swer typically contains rare words such as the
name of a place or person. when the set of
possible answer tokens is more constrained, such

figure 3: per-answer mean f1 scores for attentive reader
(moving average of 1000), illustrating the decline in predic-
tion quality as the number of training examples per answer
decreases.

upper bound, as perfect classi   cation across the
50, 000 most frequent answers would yield a mean
f1 of 0.831. however, none of them approaches
this limit. part of the reason is that their accuracy
for a given answer decreases quickly as the fre-
quency of the answer in the training set decreases,
as illustrated in figure 3. as these models have
to learn a separate weight vector for each answer
as part of the softmax layer (see section 3.1), this
may suggest that they fail to generalize across an-
swers effectively and thus require signi   cant num-
ber of training examples per answer.

the only answer extraction model evaluated,

as in categorical or date properties,
the basic
id195 often performs slightly better. character
id195 has the highest upper bound, limited to
0.963 only because it cannot produce an answer
set with multiple elements. lm pretraining con-
sistently improves the performance of the charac-
ter id195 model, especially for relational prop-
erties as shown in table 5. the performance of
the character id195, especially with lm pre-
training, is a surprising result: it performs com-
parably to the word-level id195 models even
though it must copy long character strings when
doing extraction and has access to a smaller por-
tion of the document. we found the character
based models to be particularly sensitive to hyper-
parameters. however, using a pretrained language
model reduced this issue and signi   cantly accel-
erated training while improving the    nal score.
we believe that further research on pretraining for
character based models could improve this result.

5 related work

the goal of automatically extracting structured in-
formation from unstructured wikipedia text was
   rst advanced by wu and weld (2007). as wiki-
data did not exist at that time, the authors re-
lied on the structured infoboxes included in some
wikipedia articles for a relational representation
of wikipedia content. wikidata is a cleaner data
source, as the infobox data contains many slight
variations in schema related to page formatting.
partially to get around this issue, the authors re-
strict their prediction model kylin to 4 speci   c in-
fobox classes, and only common attributes within
each class.

a substantial body of work in relation extrac-
tion (re) follows the distant supervision paradigm
(craven and kumlien, 1999), where sentences
containing both arguments of a knowledge base
(kb) triple are assumed to express the triple   s re-
lation. broadly, these models use these distant la-
bels to identify syntactic features relating the sub-
ject and object entities in text that are indicative of
the relation. mintz et al. (2009) apply distant su-
pervision to extracting freebase triples (bollacker
et al., 2008) from wikipedia text, analogous to
the relational part of wikireading. extensions
to distant supervision include explicitly modelling
whether the relation is actually expressed in the
sentence (riedel et al., 2010), and jointly reason-
ing over larger sets of sentences and relations (sur-

deanu et al., 2012). recently, rockt  aschel et al.
(2015) developed methods for reducing the num-
ber of distant supervision examples required by
sharing information between relations.
6 conclusion
we have demonstrated the complexity of the
wikireading task and its suitability as a bench-
mark to guide future development of dnn models
for natural language understanding. after compar-
ing a diverse array of models spanning classi   ca-
tion and extraction, we conclude that end-to-end
sequence to sequence models are the most promis-
ing. these models simultaneously learned to clas-
sify documents and copy arbitrary strings from
them.
in light of this    nding, we suggest some
focus areas for future research.

our character-level model improved substan-
tially after language model pretraining, suggest-
ing that further training optimizations may yield
continued gains. document length poses a prob-
lem for id56-based models, which might be ad-
dressed with convolutional neural networks that
are easier to parallelize. finally, we note that these
models are not intrinsically limited to english, as
they rely on little or no pre-processing with tradi-
tional nlp systems. this means that they should
generalize effectively to other languages, which
could be demonstrated by a multilingual version
of wikireading.
acknowledgments
we thank jonathan berant for many helpful com-
ments on early drafts of the paper, and cather-
ine finegan-dollak for an early implementation of
id56 labeler.

references
[abadi et al.2015] mart  n abadi, ashish agarwal, paul
barham, eugene brevdo, zhifeng chen, craig
citro, greg s corrado, andy davis, jeffrey dean,
matthieu devin, et al. 2015. tensor   ow: large-
scale machine learning on heterogeneous systems.
software available from tensor   ow. org.

[ayers et al.2008] phoebe ayers, charles matthews,
and ben yates. 2008. how wikipedia works: and
how you can be a part of it. no starch press.

[bahdanau et al.2014] dzmitry bahdanau, kyunghyun
cho, and yoshua bengio. 2014. neural machine
translation by jointly learning to align and translate.
international conference on learning representa-
tions (iclr).

[bollacker et al.2008] kurt bollacker, colin evans,
praveen paritosh, tim sturge, and jamie taylor.
2008. freebase: a collaboratively created graph
database for structuring human knowledge. in pro-
ceedings of the 2008 acm sigmod international
conference on management of data, sigmod    08,
pages 1247   1250, new york, ny, usa. acm.

[cho et al.2014] kyunghyun

cho,

bart
van merri  enboer, c   alar g  ulc  ehre, dzmitry
bahdanau, fethi bougares, holger schwenk,
learning phrase
and yoshua bengio.
representations using id56 encoder   decoder
for
in proceedings
id151.
of the 2014 conference on empirical methods in
natural language processing (emnlp), pages
1724   1734, doha, qatar, october. association for
computational linguistics.

2014.

[craven and kumlien1999] mark craven and johan
kumlien. 1999. constructing biological knowledge
bases by extracting information from text sources.
in proceedings of the seventh international confer-
ence on intelligent systems for molecular biology,
pages 77   86. aaai press.

[dai and le2015] andrew m dai and quoc v le.
in ad-
2015. semi-supervised sequence learning.
vances in neural information processing systems,
pages 3061   3069.

[graves2013] alex graves.

2013. generating se-
quences with recurrent neural networks. corr,
abs/1308.0850.

[hermann et al.2015] karl moritz hermann, tomas
kocisky, edward grefenstette, lasse espeholt, will
kay, mustafa suleyman, and phil blunsom. 2015.
teaching machines to read and comprehend. in ad-
vances in neural information processing systems,
pages 1684   1692.

[hochreiter and schmidhuber1997] sepp hochreiter
and j  urgen schmidhuber. 1997. long short-term
memory. neural computation, 9(8):1735   1780.

[kalchbrenner and blunsom2013] nal kalchbrenner
and phil blunsom. 2013. recurrent convolutional
neural networks for discourse compositionality.
in proceedings of
the cvsc workshop, so   a,
bulgaria. association of computational linguistics.

[kingma and adam2015] diederik p kingma

and
jimmy ba adam. 2015. a method for stochastic
in international conference on
optimization.
learning representation.

[kurach et al.2015] karol kurach, marcin andrychow-
2015. neural random-
in international conference on

icz, and ilya sutskever.
access machines.
learning representations (iclr).

[le and mikolov2014] quoc v. le and tomas mikolov.
2014. distributed representations of sentences and
in proceedings of the 31st interna-
documents.
tional conference on machine learning, pp. , 2014,
pages 1188?   1196.

[luong et al.2015] thang luong, ilya sutskever, quoc
le, oriol vinyals, and wojciech zaremba. 2015.
addressing the rare word problem in neural ma-
chine translation. in proceedings of the 53rd annual
meeting of the association for computational lin-
guistics and the 7th international joint conference
on natural language processing (volume 1: long
papers), pages 11   19, beijing, china, july. associ-
ation for computational linguistics.

[mikolov et al.2013] tomas mikolov, ilya sutskever,
kai chen, greg s corrado, and jeff dean. 2013.
distributed representations of words and phrases
in advances in neural
and their compositionality.
information processing systems, pages 3111   3119.

[mintz et al.2009] mike mintz, steven bills, rion
snow, and dan jurafsky. 2009. distant supervi-
sion for id36 without labeled data. in
proceedings of the joint conference of the 47th an-
nual meeting of the acl and the 4th international
joint conference on natural language processing
of the afnlp: volume 2 - volume 2, acl    09, pages
1003   1011, stroudsburg, pa, usa. association for
computational linguistics.

[nguyen and grishman2015] thien huu nguyen and
ralph grishman. 2015. id36: per-
spective from convolutional neural networks.
in
proceedings of naacl-hlt, pages 39   48.

[riedel et al.2010] sebastian riedel, limin yao, and
andrew mccallum. 2010. modeling relations and
in machine
their mentions without labeled text.
learning and knowledge discovery in databases,
pages 148   163. springer.

[rockt  aschel et al.2015] tim rockt  aschel,
2015.

sameer
singh, and sebastian riedel.
injecting
logical background knowledge into embeddings
in annual conference of
for id36.
the north american chapter of the association for
computational linguistics (naacl).

[sukhbaatar et al.2015] sainbayar sukhbaatar,

jason
weston, rob fergus, et al. 2015. end-to-end mem-
in advances in neural information
ory networks.
processing systems, pages 2431   2439.

[surdeanu et al.2012] mihai surdeanu, julie tibshirani,
ramesh nallapati, and christopher d manning.
2012. multi-instance multi-label learning for rela-
in proceedings of the 2012 joint
tion extraction.
conference on empirical methods in natural lan-
guage processing and computational natural lan-
guage learning, pages 455   465. association for
computational linguistics.

[vrande  ci  c and kr  otzsch2014] denny vrande  ci  c and
markus kr  otzsch. 2014. wikidata: a free collab-
orative knowledgebase. commun. acm, 57:78   85.

[weston et al.2015] jason weston, antoine bordes,
sumit chopra, and tomas mikolov. 2015. towards
ai-complete id53: a set of prerequi-
site toy tasks. may.

[wu and weld2007] fei wu and daniel s weld. 2007.
autonomously semantifying wikipedia. in proceed-
ings of the sixteenth acm conference on conference
on information and knowledge management, pages
41   50. acm.

[yang et al.2015] yi yang, wen-tau yih, and christo-
pher meek. 2015. wikiqa: a challenge dataset for
open-domain id53. in proceedings of
the 2015 conference on empirical methods in nat-
ural language processing, pages 2013   2018.

[zhang et al.2015] xiang zhang, junbo zhao, and yann
lecun. 2015. character-level convolutional net-
works for text classi   cation. in advances in neural
information processing systems, pages 649   657.

