twitter as a lifeline: human-annotated twitter corpora for nlp of

crisis-related messages

muhammad imran1, prasenjit mitra1, carlos castillo2

1qatar computing research institute (hbku), doha, qatar

2eurecat, barcelona, spain

mimran@qf.org.qa, pmitra@qf.org.qa, chato@acm.org

6
1
0
2

 

y
a
m
1
3

 

 
 
]
l
c
.
s
c
[
 
 

2
v
4
9
8
5
0

.

5
0
6
1
:
v
i
x
r
a

abstract

microblogging platforms such as twitter provide active communication channels during mass convergence and emergency events such
as earthquakes, typhoons. during the sudden onset of a crisis situation, affected people post useful information on twitter that can be
used for situational awareness and other humanitarian disaster response efforts, if processed timely and effectively. processing social
media information pose multiple challenges such as parsing noisy, brief and informal messages, learning information categories from
the incoming stream of messages and classifying them into different classes among others. one of the basic necessities of many of these
tasks is the availability of data, in particular human-annotated data. in this paper, we present human-annotated twitter corpora collected
during 19 different crises that took place between 2013 and 2015. to demonstrate the utility of the annotations, we train machine
learning classi   ers. moreover, we publish    rst largest id97 id27s trained on 52 million crisis-related tweets. to deal
with tweets language issues, we present human-annotated normalized lexical resources for different lexical variations.

keywords: natural language processing, twitter, disaster response, supervised classi   cation

1.

introduction

twitter has been extensively used as an active communi-
cation channel, especially during mass convergence events
such as natural disasters like earthquakes,    oods,
ty-
phoons (imran et al., 2015; hughes and palen, 2009). dur-
ing the onset of a crisis, a variety of information is posted
in real-time by affected people; by people who are in need
of help (e.g., food, shelter, medical assistance, etc.) or by
people who are willing to donate or offer volunteering ser-
vices. moreover, humanitarian and formal crisis response
organizations such as government agencies, public health
care ngos, and military are tasked with responsibilities to
save lives, reach people who need help, etc. (vieweg et al.,
2014). situation-sensitive requirements arise during such
events and formal disaster response agencies look for ac-
tionable and tactical information in real-time to effectively
estimate early damage assessment, and to launch relief ef-
forts accordingly.
recent studies have shown the importance of social me-
dia messages to enhance situational awareness and also in-
dicate that these messages contain signi   cant actionable
and tactical information (cameron et al., 2012; imran et
al., 2013; purohit et al., 2013). many natural-language-
processing (nlp) techniques such as automatic summa-
rization, information classi   cation, named-entity recogni-
tion, information extraction can be used to process such
social media messages (bontcheva et al., 2013; imran et
al., 2015). however, many social media messages are
very brief, informal, and often contain slangs, typograpi-
cal errors, abbreviations, and incorrect grammar (han et al.,
2013). these issues degrade the performance of many nlp
techniques when used down the processing pipeline (ritter
et al., 2010; foster et al., 2011).
we present twitter corpora consisting of more than 52
million crisis-related messages collected during 19 differ-
ent crises. we provide human annotations (volunteers and

crowd-sourced workers) of two types. first, the tweets are
annotated with a set of categories such as displaced peo-
ple,    nancial needs, infrastructure, etc. these annotation
schemes were built using input taken from formal crisis re-
sponse agencies such as united nations of   ce for the coor-
dination of humanitarian affairs (un ocha). second, the
tweets are annotated to identify out-of-vocabulary(oov)
terms, such as slangs, places names, abbreviations, mis-
spellings, etc. and their corrections and normalized forms.
this dataset can form the basis for research in text classi   -
cation for short messages and for research on normalizing
informal language.
creating large corpora for training supervised machine-
learning models is hard because it requires time and money
that may not be available. however, since our dataset was
used for disaster relief efforts, volunteers were willing to
annotate it; this work can now be leveraged to improve
text classi   cation and language processing tasks. our work
provides annotations for around 50,000 thousand messages,
which is a signi   cant corpus, that will enable research into
applied machine learning and consequently bene   t the dis-
aster relief (and other) research communities. our dataset
has been collected from various countries and during vari-
ous times of the year. this diversity would make it an inter-
esting dataset that if used would be a foil to solutions that
only work for speci   c language    dialects   , e.g., american
english and would fail or suffer from degradation of quality
if applied to variations, such as indian english. our work
shows that when a dataset is used for a real application, we
could obtain larger number of annotations than otherwise.
these can then be used to improve text processing as a by-
product.
the annotated data is also used to train machine-learning
classi   ers.
in this case, we use three well-known learn-
ing algorithms: naive bayes, id79, and support
vector machines (id166). we remark that these classi   ers

are useful for formal crisis response organizations as well
as for the research community to build more effective com-
putational methods (pak and paroubek, 2010; imran et al.,
2015) on top. we also train id97 id27s
from all 52 million messages and make them available to
research community.
1.1. contributions
the contributions of this paper are as follows:

1. we present human-annotated crisis-related messages

collected during 19 different crises

2. we use human-annotations to built machine-learning
classi   ers in a multiclass classi   cation setting to clas-
sify messages that are useful for humanitarian efforts

3. we provide    rst largest id97 id27s

trained using 52 million crisis-related messages

4. we use the collected data to identify oov (out-of-
vocabulary) words and provide human-annotated nor-
malized lexical resources for different lexical varia-
tions

1.2. paper organization
the rest of the paper is organized as follows.
in the
next section, we describe datasets details and annotation
schemes. section 3 describes supervised classi   cation task
and id97 id27s. section 4 provides details
of text id172 and we present related work in section
5. we conclude the paper in section 6.

2. crises corpora collection and

annotation

2.1. data collection
we collected crisis-related messages from twitter posted
during 19 different crises that took place from 2013 to
2015. table 1 shows the list of crisis events along with their
names, crisis type (e.g. earthquake,    ood), countries where
they took place, and the number of tweets each crisis con-
tains. we collected these messages using our aidr (arti-
   cial intelligence for disaster response) platform (imran
et al., 2014). aidr is an open source platform to collect
and classify twitter messages during the onset of a human-
itarian crisis. aidr has been used by un ocha during
many major disasters such as nepal earthquake, typhoon
hagupit.
aidr provides different convenient ways to collect mes-
sages from twitter using the twitter   s streaming api. one
can use different data collection strategies. for exam-
ple, collecting tweets that contain some keywords and are
speci   cally from a particular geographical area/region/city
(e.g. new york). the detailed data collection strategies
used to collect the datasets shown in table 1 are included
in each dataset folder.
2.2. data annotation
messages posted on social media vary greatly in terms of
information they contain. for example, users post mes-
sages of personal nature, messages useful for situational

awareness (e.g. infrastructure damage, causalities, individ-
ual needs), or not related to the crisis at all. depending
on their information needs, different humanitarian organi-
zations use different annotation schemes to categories these
messages. in this work, we use a subset of the annotations
used by the united nations of   ce for the coordination of
humanitarian affairs (un ocha). the 9 category types
(including two catch-all classes:    other useful informa-
tion    and    irrelevant   ) used by the un ocha are shown
in the below-presented annotation scheme. for most of the
datasets we have performed annotations by employing vol-
unteers and paid workers.
to perform volunteered-based annotations, messages were
collected from twitter in real-time and passed through a de-
duplication process. only unique messages were consid-
ered for human-annotation. we use stand-by-task-force
(sbtf)1 volunteers to annotate messages using our mi-
cromappers platform.2 the real-time annotation process
helps train machine learning classi   ers rapidly, which are
then used to classify new incoming messages. this process
helps address time-critical information needs requirement
of many humanitarian organizations.
after the    rst round of annotations, we found that some cat-
egories are small in terms of number of labels thus showing
high class-imbalance. a dataset is said to be imbalanced if
at least one of the classes has signi   cantly fewer annotated
instances than the others. the class imbalance problem has
been known to hinder the learning performance of classi   -
cation algorithms. in this case, we performed another round
of annotations for datasets that have high class imbalance
using the paid id104 platform crowdflower.3
in both annotation processes, an annotation task consists
of a tweet and the list of categories listed below. a paid
worker or volunteer reads the message and selects one of
the categories most suitable for the message. messages that
do not belong to any category but contain some important
information are categorized as    other useful information   .
a task is    nalized (i.e. a category is assigned) when three
different volunteers/paid workers agree on a category.
according to the twitter   s data distribution policy, we are
not allowed to publish actual contents of more than 50k
tweets. for this reason, we publish all annotated tweets,
which are less than 50k, along with tweet-ids of all the
unannotated messages at http://crisisnlp.qcri.
org/. we also provide a tweets retrieval tool implemented
in java, which can be used to get full tweets content from
twitter.
in below we show the annotation scheme used for crisis
events caused by natural disasters. for other events, details
regarding their annotations are available with the published
data.
annotation scheme: categorizing messages by informa-
tion types

    injured or dead people: reports of casualties and/or

injured people due to the crisis

1http://blog.standbytaskforce.com/
2http://micromappers.org/
3http://crowdflower.com/

table 1: crises datasets details including crisis type, name, year, language of messages, country, # of tweets.

crisis type
earthquake
earthquake
earthquake
earthquake
earthquake
typhoon
typhoon
typhoon
volcano
landslide
landslide
landslide
floods
floods
war & con   ict
war & con   ict
biological
infectious disease
airline accident

crisis name
nepal earthquake
terremoto chile
chile earthquake
california earthquake
pakistan earthquake
cyclone pam
typhoon hagupit
hurricane odile
iceland volcano
landslides worldwide
landslides worldwide
landslides worldwide
pakistan floods
india floods
palestine con   ict
peshawar attack pakistan
middle east respiratory syndrome worldwide
worldwide
ebola virus outbreak
malaysia airlines    ight mh370
malaysia

country
nepal
chile
chile
usa
pakistan
vanuatu
phillippines
mexico
iceland
worldwide
worldwide
worldwide
pakistan
india
palestine
pakistan

language
english
spanish
english
english
english
english
english
english
english
english
french
spanish
english
english
english
english
english
english
english

# of tweets
4,223,937
842,209
368,630
254,525
156,905
490,402
625,976
62,058
83,470
382,626
17,329
75,244
1,236,610
5,259,681
27,770,276
1,135,655
215,370
5,107,139
4,507,157

start-date
2015-04-25
2014-04-02
2014-04-02
2014-08-24
2013-09-25
2015-03-11
2014-12-03
2014-09-15
2014-08-25
2014-03-12
2015-03-12
2015-03-12
2014-09-07
2014-08-10
2014-07-12
2014-12-16
2014-04-27
2014-08-02
2014-03-11

end-date
2015-05-19
2014-04-10
2014-04-17
2014-08-30
2013-10-10
2015-03-29
2014-12-16
2014-09-28
2014-09-01
2015-05-28
2015-06-23
2015-06-23
2014-09-22
2014-09-03
2014-10-02
2014-12-28
2014-07-14
2014-10-27
2014-07-12

    missing, trapped, or found people: reports and/or

questions about missing or found people

    displaced people and evacuations: people who have
relocated due to the crisis, even for a short time (in-
cludes evacuations)

    infrastructure and utilities damage: reports of dam-
aged buildings, roads, bridges, or utilities/services in-
terrupted or restored

    donation needs or offers or volunteering services: re-
ports of urgent needs or donations of shelter and/or
supplies such as food, water, clothing, money, medi-
cal supplies or blood; and volunteering services

    caution and advice: reports of warnings issued or

lifted, guidance and tips

    sympathy and emotional support: prayers, thoughts,

and emotional support

    other useful information: other useful information

that helps understand the situation

    not related or irrelevant: unrelated to the situation or

irrelevant

3. classi   cation of messages

to make sense of huge amounts of twitter messages posted
during crises, we consider a basic operation, that is, the
automatic categorization of messages into the categories
of interest. this is a multiclass categorization problem in
which instances are categorized into one of several classes.
speci   cally, we aim at learning a predictor h : x     y,
where x is the set of messages and y is a    nite set of cat-
egories. for this purpose, we use three well-known learn-
ing algorithms i.e. naive bayes (nb), support vector ma-
chines (id166), and id79 (rf).

3.1. preprocessing and feature extraction
prior to learning a classi   er, we perform the following
preprocessing steps. first, stop-words, urls, and user-
mentions are removed from the twitter messages. we per-
form id30 using the lovins stemmer. we use uni-
grams and bi-grams as our features. previous studies found
these two features outperform when used for similar clas-
si   cation tasks (imran et al., 2013). finally, we used the
information gain, a well-know feature selection method to
select top 1k features. the labeled data we used in this task
was annotated by the paid workers.
3.2. evaluation and results
we trained all three different kinds of classi   ers using the
preprocessed data. for the evaluation of the trained mod-
els, we used 10-folds cross-validation technique. table 2
shows the results of the classi   cation task in terms of area
under roc curve4 for all classes of the 8 different disaster
datasets. we also show the proportion of each class in each
dataset.
given the complexity of the task i.e. multiclass classi   -
cation of short messages, we can see that all three classi-
   ers have pretty decedent results. in this case, a random
classi   er represents an auc = 0.50 and higher values are
preferable. other than the    missing trapped or found peo-
ple    class, which is the smallest class in term of proportion
across all the datasets, results for most of the other classes
are at the acceptable level (i.e.     0.80).
3.3. crisis id27s
many applications of machine learning and computational
linguistics rely on semantic representations and relation-
ships between words of a text document. many different
types of methods have been proposed that use continuous
representations of words such as latent semantic analy-
sis (lsa) and id44 (lda). how-

4https://en.wikipedia.org/wiki/receiver_

operating_characteristic

table 2: classi   cation results in terms of area under roc curve for selected datasets across all classes using support
vector machines (id166), naive bayes (nb), and id79 (rf).

datasets

classi   er

2014 chile
earthquake

2015 nepal
earthquake

2013 pakistan
earthquake

2015 cyclone
pam

2014 typhoon
hagupit

2014 india
   oods

2014 pakistan
   oods

2014 california
earthquake

size(%)
id166
nb
rf
size(%)
id166
nb
rf
size(%)
id166
nb
rf
size(%)
id166
nb
rf
size(%)
id166
nb
rf
size(%)
id166
nb
rf
size(%)
id166
nb
rf
size(%)
id166
nb
rf

caution
and
advice

15%
0.87
0.86
0.83
2.10%
0.47
0.68
0.56
6.30%
0.77
0.82
0.68
7%
0.76
0.79
0.68
20%
0.74
0.75
0.71
3.60%
0.82
0.89
0.83
3.90%
0.71
0.83
0.72
6.30%
0.84
0.88
0.81

displaced
people and
evacuations
2.80%
0.89
0.93
0.86
3.10%
0.80
0.82
0.73
0.82%
0.80
0.87
0.70
3.10%
0.80
0.82
0.80
6.60%
0.95
0.96
0.97
1.40%
0.80
0.92
0.79
6.20%
0.84
0.80
0.80
0.48%
0.54
0.57
0.49

donation
needs or
offers

infrastructure
and utilities
damage

injured or
dead people

missing trapped
or found people

sympathy
emotional
support

other useful
information

not related
or irrelevant

0.76%
0.57
0.78
0.67
28%
0.89
0.91
0.89
15%
0.92
0.94
0.92
17%
0.92
0.92
0.90
5.50%
0.88
0.89
0.84
2.60%
0.92
0.93
0.86
25%
0.82
0.85
0.87
4.30%
0.93
0.94
0.87

1.70%
0.90
0.88
0.74
4.50%
0.85
0.90
0.74
2%
0.76
0.91
0.77
11%
0.85
0.86
0.80
5.10%
0.76
0.82
0.73
4.30%
0.92
0.90
0.87
5.40%
0.77
0.79
0.78
18%
0.88
0.86
0.89

5.60%
0.97
0.97
0.96
11%
0.95
0.95
0.94
17%
0.95
0.93
0.95
7.20%
0.95
0.97
0.95
3%
0.94
0.96
0.94
47%
0.97
0.93
0.97
13%
0.94
0.94
0.95
10%
0.97
0.97
0.98

0.54%
0.23
0.64
0.46
5.80%
0.86
0.89
0.87
0.49%
0.63
0.74
0.69
1.30%
0.39
0.56
0.47
0.58%
0.44
0.57
0.58
0.87%
0.66
0.79
0.66
6.40%
0.85
0.85
0.84
0.51%
0.62
0.79
0.57

25%
0.93
0.93
0.94
17%
0.88
0.91
0.89
5.60%
0.82
0.83
0.78
5%
0.66
0.79
0.71
13%
0.92
0.92
0.91
1.30%
0.63
0.83
0.65
6%
0.88
0.89
0.86
4.10%
0.84
0.90
0.88

30%
0.86
0.87
0.86
22%
0.76
0.79
0.76
35%
0.84
0.84
0.88
25%
0.77
0.80
0.79
33%
0.74
0.78
0.75
14%
0.87
0.89
0.91
32%
0.74
0.77
0.79
47%
0.77
0.78
0.81

19%
0.93
0.95
0.92
6.50%
0.75
0.84
0.75
18%
0.84
0.84
0.83
24%
0.90
0.94
0.92
13%
0.81
0.81
0.80
25%
0.97
0.98
0.96
2.30%
0.47
0.65
0.59
9.40%
0.72
0.77
0.77

ever, recently models based on distributional representa-
tions of words become more famous.
in this work, we
train id27s (i.e. distributed word representa-
tions) using the 52 million twitter messages in our datasets
and make it available to research community. to the best
of our knowledge this is the    rst largest id27s
that are trained on crisis-related tweets.
we use id97, a very popular software to train word
embedding (mikolov et al., 2013). as preprocessing, we
replaced urls, digits, and usernames with    xed constants
and removed special characters. finally, the word em-
beddings are generated using continuous bag of words
(cbow) architecture with negative sampling along with
300 word representation dimensionality.

4. twitter text id172
4.1. language issues in twitter messages
the quality   in terms of readability, grammar, sentence
structure etc.   of twitter messages vary signi   cantly. typ-
ically, twitter messages are brief, informal, noisy, unstruc-
tured, and often contain misspellings and grammatical mis-
takes. moreover, due to twitter   s 140 character limit re-
striction, twitter users intentionally shorten words by us-
ing abbreviations, acronyms, slangs, and sometimes words
without spaces. the accuracy of natural language process-
ing techniques would improve if we can identify the infor-
mal nature of the language in tweets and normalize oov
terms (han et al., 2013). we divide these lexical variations
into the following    ve categories:

1. typos/misspellings:

e.g.

earthquak (earthquake),

missin (missing), ovrcme (overcome)

2. single-word abbreviation/slangs: e.g. pls (please),

srsly (seriously), govt (government), msg (message)

3. multi-word abbreviation/slangs: e.g.
opinion), im (i am), brb (be right back)

imo (in my

4. id102 substitutions: e.g. 2morrow (tomorrow),

4ever (forever), 4g8 (forget), w8 (wait)

5. words without spaces: e.g. prayfornepal (pray for
nepal), wehelp (we help), weneedshelter (we need
shelter)

identi   cation of candidate oov words

4.2.
to identify candidate oov words that require normaliza-
tion, we    rst build initial vocabularies consisting of lexi-
cal variations mentioned in the previous section. we use a
dictionary available on the web to normalize abbreviations,
chat shortcuts, and slang.5 we also use the scowl (spell
checker oriented word lists) aspell english dictionary 6
that consists of 349,554 english words. the scowl dic-
tionary is suitable for english spell checkers for most of en-
glish dialects. although, the scowl dictionary contains
places names (e.g. names of countries and famous cities),
after testing it on nepal earthquake data, we found that its
coverage is not complete and a large number of cities/towns
of nepal are missing. to overcome this issue, we use the

5http://www.innocentenglish.com/news/

texting-abbreviations-collection-texting-
slang.html

6http://wordlist.aspell.net/

figure 1: id104 task for twitter out-of-vocabulary words id172

maxmind 7 world cities database that consists of 3,173,959
cities.
using the above resources, we try to    nd oov words in
the dataset. however, we observed that a large number of
oovs consist of misspelled words for which a correct form
can be obtained using one edit-distance change (i.e. by per-
forming one insertion, deletion, or substitution operation).
for this purpose, we train a language model using lists of
most frequent words from wiktionary,8 the british national
corpus,9 and words in our scowl dictionary. for a given

7https://www.maxmind.com/en/free-world-

cities-database

8http://en.wiktionary.org/wiki/wiktionary
9http://www.kilgarriff.co.uk/bnc-readme.

html

misspelled word w, we aim to    nd a correction c out of all
possible corrections where the id203 of c given w is
maximum, i.e., argmaxcp (c|w) by id47 this
is equivalent to:
argmaxcp (c|w) = argmaxcp (w|c)p (c)/p (w)
or it can be written as:
argmaxcp (c|w) = argmaxcp (w|c)p (c)
where p (c) is the id203 that c is the correct word and
p (w|c) is the id203 that the author typed w when c
was intended. we then restrict the language model to pre-
dict corrections within one edit-distance range and from
those choose the one with highest id203. misspellings
for which more than one change is required, we consider
them as oovs to be corrected by human workers.

4.3. id172 of oov words
to normalize the identi   ed oov words, we used the
crowdflower id104 platform. a id104
task in this case consists of a twitter message that con-
tains one or more oov words and a set of instructions
shown in figure 1. the workers were asked to read the
instructions and examples carefully before providing an an-
swer. a worker reads the given message and provides a cor-
rect oov tag (i.e. slang/abbreviation/acronym, a location
name, an organization name, a misspelled word, or a person
name). if an oov is a misspelled word, the worker also
provides its corrected form. we provide all the resources
and the results of crowdsoucing to research community.

5. related work

the use of microblogging platforms such as twitter during
the sudden onset of a crisis situation has been increased
in the last few years. thousands of crisis-related mes-
sages that are posted online contain important informa-
tion that can also be useful to humanitarian organizations
for disaster response efforts, if processed timely and effec-
tively (hughes and palen, 2009; imran et al., 2015).
many different types of processing techniques ranging from
machine learning to natural language processing to com-
putational linguistics have been developed (corvey et al.,
2010) for different purposes (imran et al., 2016). despite
there exists some resources e.g. (temnikova et al., 2015;
olteanu et al., 2015), however, due to the scarcity of rel-
evant data, in particular human-annotated data, crisis in-
formatics researchers still cannot fully utilize the capabili-
ties of different computational methods. to overcome these
issues, we present to research community a corpora con-
sisting of labeled and unlabeled crisis-related twitter mes-
sages. moreover, we also provide normalized lexical re-
sources useful for linguistic analysis of twitter messages.

6. conclusions

we present twitter corpora consisting of over 52 mil-
lion crisis-related tweets collected during 19 crisis events.
we provide two sets of annotations related to topic-
categorization of the tweets and tagging out-of-vocabulary
words and their id172s. we build machine-learning
classi   ers to empirically validate the effectiveness of the
annotated datasets. we also provide id97 word em-
beddings trained on 52 million messages. we believe that
these resources and the tools built using them will help
improve automatic natural language processing of crisis-
related messages and eventually be useful for humanitarian
organizations.

7. references

bontcheva, k., derczynski, l., funk, a., greenwood,
m. a., maynard, d., and aswani, n. (2013). twitie:
an open-source information extraction pipeline for mi-
croblog text. in ranlp, pages 83   90.

cameron, m. a., power, r., robinson, b., and yin, j.
(2012). emergency situation awareness from twitter for
in proc. of the 21st international
crisis management.
conference companion on world wide web, pages 695   
698.

corvey, w. j., vieweg, s., rood, t., and palmer, m. (2010).
twitter in mass emergency: what nlp techniques can con-
tribute. in proc. of the naacl hlt 2010 workshop on
computational linguistics in a world of social media,
pages 23   24.

foster, j., c   etinoglu,   o., wagner, j., le roux, j., hogan,
s., nivre, j., hogan, d., and van genabith, j. (2011).
# hardtoparse: id52 and parsing the twitterverse.
in aaai 2011 workshop on analyzing microtext, pages
20   25.

han, b., cook, p., and baldwin, t. (2013). lexical nor-
malization for social media text. acm transactions on
intelligent systems and technology (tist), 4(1):5.

hughes, a. l. and palen, l. (2009). twitter adoption and
use in mass convergence and emergency events. interna-
tional journal of emergency management, 6(3-4):248   
260.

imran, m., elbassuoni, s. m., castillo, c., diaz, f., and
meier, p. (2013). extracting information nuggets from
disaster-related messages in social media. proc. of is-
cram, baden-baden, germany.

imran, m., castillo, c., lucas, j., meier, p., and vieweg,
s. (2014). aidr: arti   cial intelligence for disaster re-
sponse. in proc. the 23rd international conference on
world wide web companion, pages 159   162.

imran, m., castillo, c., diaz, f., and vieweg, s. (2015).
processing social media messages in mass emergency:
a survey. acm computing surveys (csur), 47(4):67.

imran, m., meier, p., castillo, c., lesa, a., and garcia her-
ranz, m. (2016). enabling digital health by automatic
classi   cation of short messages. in proceedings of the
6th international conference on digital health confer-
ence, pages 61   65. acm.

mikolov, t., chen, k., corrado, g., and dean, j. (2013).
ef   cient estimation of word representations in vector
space. arxiv preprint arxiv:1301.3781.

olteanu, a., vieweg, s., and castillo, c. (2015). what
to expect when the unexpected happens: social media
communications across crises. in proc. of the 18th acm
conference on computer supported cooperative work
& social computing, pages 994   1009. acm.

pak, a. and paroubek, p. (2010). twitter as a corpus for
id31 and opinion mining. in lrec, vol-
ume 10, pages 1320   1326.

purohit, h., castillo, c., diaz, f., sheth, a., and meier,
p. (2013). emergency-relief coordination on social me-
dia: automatically matching resource requests and of-
fers. first monday, 19(1).

ritter, a., cherry, c., and dolan, b. (2010). unsupervised

modeling of twitter conversations. in proc of naacl.

temnikova,

i., castillo, c., and vieweg, s.

(2015).
in information systems for crisis re-

emterms 1.0.
sponse and management, iscram.

vieweg, s., castillo, c., and imran, m. (2014). integrating
social media communications into the rapid assessment
of sudden onset disasters. in social informatics, pages
444   461. springer.

