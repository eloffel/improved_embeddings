4
1
0
2

 
c
e
d
4
2

 

 
 
]

g
l
.
s
c
[
 
 

1
v
4
8
5
7

.

2
1
4
1
:
v
i
x
r
a

di   erential privacy and machine learning:

a survey and review

zhanglong ji, zachary c. lipton, charles elkan

december 25, 2014

abstract

the objective of machine learning is to extract useful information from
data, while privacy is preserved by concealing information. thus it seems
hard to reconcile these competing interests. however, they frequently
must be balanced when mining sensitive data. for example, medical re-
search represents an important application where it is necessary both to
extract useful information and protect patient privacy. one way to re-
solve the con   ict is to extract general characteristics of whole populations
without disclosing the private information of individuals.

in this paper, we consider di   erential privacy, one of the most popular
and powerful de   nitions of privacy. we explore the interplay between ma-
chine learning and di   erential privacy, namely privacy-preserving machine
learning algorithms and learning-based data release mechanisms. we also
describe some theoretical results that address what can be learned dif-
ferentially privately and upper bounds of id168s for di   erentially
private algorithms.

finally, we present some open questions, including how to incorpo-
rate public data, how to deal with missing data in private datasets, and
whether, as the number of observed samples grows arbitrarily large, di   er-
entially private machine learning algorithms can be achieved at no cost to
utility as compared to corresponding non-di   erentially private algorithms.

the objective of machine learning is to extract useful information from data,
such as how to classify data, how to predict a quantity, or how to    nd clusters of
similar samples. given a family of learning models, machine learning algorithms
select and output the best one based on some given data. the output model can
be used in either dealing with future data or interpreting the distribution of data.
although the output model typically far more compact than the underlying
dataset, it must capture some information describing the dataset. privacy, on
the other hand, concerns the protection of private data from leakage, especially
the information of individuals.1

1 [9] uses privacy to mean both population privacy and individual privacy. for example,
disclosing that members of some family are highly susceptible to a given genetic condition
might violate population privacy, while disclosing that some speci   c patient su   ers from this
condition would violate their individual privacy. however, even if one conceals all his/her

1

it would be reasonable to ask,    why is it insu   cient to anonymize data?   
one could remove names and other obviously identi   able information from
a database.
it might seem di   cult then, for an attacker to identify an in-
dividual. current hipaa guidelines promotes such an approach, listing 18
categories of personally identi   able information which must be redacted in
the case of research data for publication. unfortunately, this method can
leak information when the attacker already has some information about the
individuals in question.
in a well-known case, the personal health informa-
tion of massachusetts governor william weld was discovered in a supposedly
anonymized public database [51]. by merging overlapping records between the
health database and a voter registry, researchers were able to identify the per-
sonal health records of the governor, among others.

to combat such background attacks, some more robust de   nitions of pri-
vacy (such as k-anonymity[51], l-diversity[36] and t-closeness[34]) have been
proposed. in these approaches, samples are grouped if their sensitive features
are the same, and a group is published if the number of samples in that group
is large enough. intuitively, it should be di   cult for an attacker to distinguish
individual samples. however, even these de   nitions cannot prevent background
attacks, in which the attackers already know something about the information
contained in the dataset.
in an extreme case, the attacker might know the
contents of all but one of the rows in the set.

consider a database which holds the address and income of four people and
publishes private data using 3-anonymity. according to 3-anonymity, if any
three people live in the same city, the city and average income of the three
people is released. now suppose an attacker knows that two people in the
database live in los angeles and that a third lives in new york. if no data is
published, the attacker can easily infer that the fourth person does not live in
los angeles.

even if one released aggregated statistics, they might risk compromising
private information. recently [23], researchers demonstrated that an attacker
could infer whether an individual had participated in a genome study using
only publicly available aggregated genetic data. in such cases, aggregation is no
longer safe.

di   erential privacy [12, 13], which will be introduced in the next section,
uses random noise to ensure that the publicly visible information doesn   t change
much if one individual in the dataset changes.

as no individual sample can signi   cantly a   ect the output, attackers cannot
infer the private information corresponding to any individual sample con   dently.
this paper addresses the interplay between machine learning and di   erential
privacy.

although it seems that machine learning and privacy protection are in op-
position, it is often possible to reconcile them. researchers have designed many
mechanisms to build models that can capture the distributions corresponding to

information, it   s still possible to breach population privacy by collecting information from
other members of the subpopulation. as one cannot easily protect oneself from the disclosure
of this information, we will use privacy to refer only to individual privacy in this paper.

2

large datasets while guaranteeing di   erential privacy with respect to individual
examples. in order to achieve generalizability, machine learning models should
not depend heavily on any single sample. therefore, it is possible to hide the
e   ects of individual samples, simultaneously preserving privacy and providing
utility.

0.1 prior work

several recent surveys address di   erential privacy and data science [17, 47, 22].
some others ([17, 22]) mainly focus on statistical estimators, while [47] discusses
the high level interactions between di   erential privacy and machine learning.

our survey focuses speci   cally on methods by which machine learning al-
gorithms can be made di   erentially private. we study current di   erentially
private machine learning algorithms and organize them according to the funda-
mental machine learning tasks they address, including classi   cation, regression,
id91, and id84. we also describe some di   erentially
private data release mechanisms, both because their mechanisms involves dif-
ferential privacy, and because their output can be used to learn di   erentially
private machine learning models. we explain how all of these mechanisms work
and compare their theoretical guarantees. some general theoretical results and
discussion follow.

1 di   erential privacy

di   erential privacy is one of the most popular de   nitions of privacy today.
intuitively, it requires that the mechanism outputting information about an un-
derlying dataset is robust to any change of one sample, thus protecting privacy.
the following subsections mathematically de   ne di   erential privacy and in-

troduce some commonly used methods in di   erential privacy.

1.1 de   nition of di   erential privacy
de   nition 1: a mechanism   f is a random function that takes a dataset d as
input, and outputs a random variable   f (d).

for example, suppose d is a medical dataset, then the function that outputs
the number of patients in d plus noise from the standard normal distribution
is a mechanism.

de   nition 2: the distance of two datasets, d(d, d   ), denotes the minimum

number of sample changes that are required to change d into d   .

for example, if d and d    di   er on at most one individual, there is d(d, d   ) =

1. we also call such a pair of datasets neighbors.

the original de   nition of di   erential privacy de   nes as neighbors datasets
which    di   er on at most one individual   . this phrasing has given rise to two
di   erent understandings. some interpret this as the replacement of a sample,

3

while others also consider addition and deletion. although the second inter-
pretation is stronger than the    rst one, most of the mechanisms discussed in
this paper work for both de   nitions if slightly modi   ed. di   erent de   nitions of
distance usually lead to di   erent values of sensitivity, while both are bounded.
in order to make a mechanism designed for one de   nition of distance work for
another de   nition of distance, we only need to make slight changes to the scale
of noise. therefore we won   t distinguish them.

de   nition 3: a mechanism   f satis   es (  ,   )-di   erential privacy [12, 13] for
two non-negative numbers    and    i    for all neighbors d(d, d   ) = 1, and all
subset s of   f    s range, as long as the following probabilities are well-de   ned,
there holds

p (   f (d)     s)        + e  p (   f (d   )     s)

intuitively speaking, the number    represents the id203 that a mechanism   s
output varies by more than a factor of e   when applied to a dataset and any one
of its neighbors. a lower value of    signi   es greater con   dence and a smaller
value of    tightens the standard for privacy protection. the smaller    and    are,
the closer p (   f (d)     s) and p (   f (d   )     s) are, and the stronger protection is.
there is also a commonly used heuristic to choose   [20]: when there are n
samples in the dataset,        o(1/n). this is because a mechanism can satisfy
(0,   )-di   erential privacy but breach privacy with high id203 when    is
large. for each sample in the dataset, the mechanism releases it with id203
  , and the release of di   erent samples are independently. it   s easy to prove the
mechanism is di   erentially private. however by expectation, the mechanism
release n   samples from the dataset. to prevent such leakage,    must be smaller
than 1/n.

typically, (  , 0)-di   erential privacy is simpli   ed to   -di   erential privacy. with
(  ,   )-di   erential privacy, when    > 0, there is still a small chance that some in-
formation is leaked. when    = 0, the guarantee is not probabilistic. [11] shows
that in terms of mtutal information,   -di   erential privacy is much stronger than
(  ,   )-di   erential privacy.

in di   erential privacy, the number    is also called the privacy budget.

1.2 query

usually, we want the output of a mechanism to be both di   erentially private
and useful. by    useful   , we mean the output accurately answers some queries on
the dataset. de   nition 4 de   nes query below and in the following subsections,
some mechanisms that guarantee di   erential privacy will be introduced.

de   nition 4: a query f is a function that takes a dataset as input. the

answer to the query f is denoted f (d).

for example, if d is a medical dataset, then    how many patients were suc-
is a query since it takes d as input and outputs a number.
cessfully cured?   
the output of a query is not necessarily a number. however, some mechanisms,
notably the laplacian mechanism, assume that answers to queries are numer-
p but not categorical. a more sophisticated query
ical, or vectors f (d)     r

4

can be    a id28 model trained from the dataset   , which outputs a
classi   cation model.

1.3 the laplacian mechanism

the laplacian mechanism[15] is a popular   -di   erentially private mechanism for
p, in which sensitivity (de   nition 5) plays an
queries f with answers f (d)     r
important role.

de   nition 5: given a query f and a norm function k.k over the range of f ,

the sensitivity s(f,k.k) is de   ned as

s(f,k.k) = max

d(d,d   )=1kf (d)     f (d   )k
usually, the norm function k.k is either l1 or l2 norm.
the laplacian mechanism[15]: given a query f and a norm function over
the range of f , the random function   f (d) = f (d) +    satis   es   -di   erential
privacy. here    is a random variable whose id203 density function is
p(  )     e     k  k/s(f,k.k).
there is a variation of the laplacian mechanism, which replaces lapla-
cian noise with gaussian noise. on one side, this replacement greatly reduces
the id203 of very large noise; on the other side, it only preserves (  ,   )-
di   erential privacy for some    > 0, which is weaker than   -di   erential privacy.
variation of the laplacian mechanism: given a query f and a distance
function over the range of f , the random function   f (d) = f (d) +    satis-
   es (  ,   )-di   erential privacy. here    is a random variable from distribution
n (0, 2

  2 (s(f,k.k))2 log 2

   ) [2].

1.4 the exponential mechanism

the exponential mechanism[38] is an   -di   erentially private method to select
one element from a set. suppose the set to select from is a, and there exists a
score function h whose input is a dataset d and a potential answer a     a, and
whose output is a real number. given a dataset d, the exponential mechanism
selects the element a     a that has a large score h(d, a).

de   nition 6: the sensitivity of score function h is de   ned as

s(h,k.k) =

d(d,d   )=1,a   akh(d, a)     h(d   , a)k

max

the exponential mechanism: given a dataset d and a set of possible answers
a, if a random mechanism selects an answer based on the following id203,
then the mechanism is   -di   erentially private:

p (a     a is selected)     e  h(d,a)/2s(h,k.k)

the laplacian mechanism is related to the exponential mechanism. if f (d)
p h(d, a) = ka     f (d)k, then the output has
is a vector in r
exactly the same distribution as   f (d) in the laplacian mechanism with half
privacy budget.

p, and    a     r

5

1.5 the smooth sensitivity framework and the sample

and aggregate framework

smooth sensitivity [42] is a framework which allows one to publish an (  ,   )-
di   erentially private numerical answer to a query. the noise it adds is deter-
mined not only by the query but also by the database itself. by avoiding using
the worst-case sensitivity, this framework can enjoy much smaller noise, though
the de   nition of privacy is weaker in this framework compared to the laplacian
mechanism. two concepts, local and smooth sensitivities, are introduced.

de   nition 7: given a query function f , a norm function k.k and a dataset

d, the local sensitivity of f is de   ned as:

ls(f,k.k, d) =

d   :d(d,d   )=1kf (d)     f (d   )k

max

one intuitive mechanism would be to add noise to the answer f (d) pro-
portional to the local sensitivity given (f, d). however such a mechanism may
leak information. for example, assume d(d, d   ) = 1, if d has very small local
sensitivity and d    has large local sensitivity w.r.t. some query f , the answer
given by the mechanism on dataset d is very close to f (d). however, the an-
swer given d    might be far away from f (d). in that case, attackers can infer
whether the dataset is d or d    according to the distance between f (d) and the
output. to overcome this problem, the smooth sensitivity framework smooths
the scale of noise across neighboring datasets.

de   nition 8: given a query function f , a norm function k.k, a dataset d

and a number   , the    smooth sensitivity of f is de   ned as

s(f,k.k, d,   ) =

max

any dataset d   

(e     d(d,d   )ls(f,k.k, d   ))

  

s(f,k.k,d   ,  )

smooth sensitivity framework: given a query function f , the dimension d of
the sample space, and gaussian noise z     n (0, 1), the output   f (d) = f (d) +
z is (  ,   )-di   erentially private, provided that    =   /pln(1/  ) and
   =    (  /pd ln(1/  )).

the sample and aggregate framework [42] is a mechanism to respond to
queries whose answers can be approximated well with a small number of samples,
while ensuring (  ,   )-di   erential privacy. the algorithm consists of a sampling
step and an aggregating step. in the sampling step, the framework partitions
the private data set d into many subsets {d1, ..., dk}, and the answer f (di) is
estimated on each subset. given the assumption that f can be measured well
with small subsets, f (d1), ..., f (dk) are fairly accurate. however, we haven   t
yet placed a privacy constraint on the estimation, thus the estimates cannot be
released.

in the aggregating step, the framework    rst de   nes a quantity r(i) to de-
note the distance between f (di) and f (di)   s t-th nearest neighbor among
f (d1), ..., f (dk) while t     k/2. then the framework de   nes a function g(f (d1), ..., f (dk))
which outputs the f (di) with the smallest r(i). then the smooth sensitivity

6

framework is applied to the function g(f (d1), ..., f (dk)) to ensure di   erential
privacy.

as changing one sample a   ects only one estimate, the function g(f (d1), ..., f (dk))

has small local sensitivity. therefore the noise required is small. further-
more, as most of the estimates f (d1), ..., f (dk) are close to the true answer,
g(f (d1), ..., f (dk)) is accurate. together, these two properties ensure that the
output is accurate.

an e   cient aggregation function is provided in the paper [42]. given m
answers {f1, f2, ..., fm} and a constant t0, the function    rst computes a quantity
ri for each fi. the quantity ri is radius of the smallest ball that is centred at
fi and covers at least t0 answers in {f1, f2, ..., fm}. then the function outputs
the answer fi which has the smallest ri.

1.6 combination of di   erentially private mechanisms

sometimes we need to combine several di   erentially private mechanisms in data
processing, thus we need to know how the combination a   ects the privacy pro-
tection. in this subsection,   fi represents di   erentially private algorithms, d is
the dataset, and {di} is a partition of d. notation g() represents any function.
[37] provides the following two theorems.
if   fi is (  i,   i)-di   erentially private, then

sequential theorem[15, 13, ?]:

di   erentially private.

  f (d) = g(   f1(d),   f2(d,   f1(d)), ...,   fn(d,   f1(d),   f2(d), ...,   fn   1(d))) is (pn

intuitively, it means that we can split    among a sequence of di   erentially
private mechanisms and allow a mechanism in the sequence to use both the
dataset and the outputs of previous mechanisms, while the    nal output is still
di   erentially private. some more sophisticated forms of this theorem can be
   nd in [16, 43].

parallel theorem:

if each   fi is   -di   erentially private, given a partition
{di} of the dataset d, then   f (d =    di) = g(   f1(d1),   f2(d2), ...,   fn(dn)) is
  -di   erentially private.
if we apply   -di   erentially private mechanisms to each partition of the dataset,
the combined output is still   -di   erentially private. the partitioning here can
be either independent of private data or based on output of some other di   er-
entially private mechanism.

i=1   i)-

i=1   i,pn

both the sequential method and the parallel method have multiple outputs.
a natural question is whether it is always bene   cial to the utility of such privacy-
preserving mechanisms to average those outputs. the answer is no. for the
sequential method, the privacy budget has to be split among several steps; for
the parallel method, each partition has less samples than d. in both cases, the
ratio between the amount of noise applied and the accurate answer is larger than
the corresponding ratio for the original mechanism. therefore, simply averaging
them doesn   t necessarily lead to better performance.

7

2 machine learning

machine learning algorithms extract information about the distribution of data.
informally, a learning algorithm takes as input a set of samples called a training
set and outputs a model that captures some knowledge about the underlying
distribution. samples are also called examples. typically an individual as dis-
cussed in the context of di   erential privacy will correspond to a single sample in
the machine learning context. the set of all possible samples is called a sample
space, and all samples in the sample space have the same set of variables. these
variables can be either categorical or numerical.
in the following sections, if
there are variables whose values we would like to predict, that are known in
training, but unknown for future examples, that variable is denoted y . all
the other variables are denoted x. when we want to predict labels for new
examples, this task is called supervised learning. the task in which there are
no labels and we want to identify structure in the dataset is called unsupervised
learning.

the information extracted is represented by machine learning models, and
di   erent models are used for di   erent tasks. regression models predict a nu-
merical variable y given a set of variables x. classi   cation models predict
a categorical variable y given a set of variables x. id91 models group
unlabelled samples into several groups based on similarity. dimension reduction
models    nd a projection from the original sample space to a low-dimensional
space, which preserves the most useful information for further machine learn-
ing. feature selection techniques select the variables that are most informative
for further research. according to whether the learning task is supervised or
unsupervised, the training set is denoted either {(xi)}n
i=1, while
n is the number of training samples.

i=1 or {(xi, yi)}n

given a family of possible models and a dataset, a machine learning algo-
rithm selects one model that    ts the data best. the process of selection is called
training.

in the following section, we assume that there is only one variable to predict

in regression or classi   cation tasks. for binary classi   cations, y     {   1, 1}.
in all the following sections, we use the same notation for machine learning
tasks. usual capital letters x and y mean random variables while bold capital
letters x and y mean data matrices. the j-th component of x is denoted
xj. there are n samples in the dataset and the i-th sample is denoted xi for
unsupervised learning tasks or (xi, yi) for supervised ones. the j-th component
of xi is denoted xij . all constants are denoted by capital letter c.

2.1 performance measurement

many papers have analyzed the performance of their mechanisms and proven
that the private models they output are very close to the true models. however,
the analyses in these papers di   er in how they de   ne the true model, how
they de   ne the distance between two models, and given such a distance metric,
how they de   ne closeness. these di   erences can impede our e   orts to compare

8

di   erent mechanisms.

to assess the performance of a di   erentially private algorithm, it is necessary
to have some notion of a    true model    against which comparisons can be made.
some papers [50, 45, 6, 25, 26, 21, 31, 7, 53, 27] consider the    true model    to be
the output of a noiseless algorithm on training data. however, others [14, 33, 40]
consider the    true model    to mean the optimal model if the true distribution were
known.

they also di   er on how to de   ne the distance between two models. some
papers [6, 25, 53] use the di   erence of values of the target function. thus the
distance between the private model and the true model is the di   erence between
the values taken by the target functions corresponding to each of the two models.
some other papers [14, 50, 33, 40, 21, 31, 7, 27] use the distance of the parameters
in private and non-private models when the models are parametric and have the
same parameters. still others [45, 26] use the distance between the predictions
made by private and non-private models at certain points in the sample space.
finally, they di   er on the de   nition of    closeness   . given a measure of
distance between two models, some papers [14, 40] prove that as the num-
ber of training examples grows large, the output converges to the true model.
however they do not provide a guaranteed rate of convergence. other papers
[45, 50, 6, 26, 21, 31, 7, 53, 27] give bounds on how fast the output models
converge to true models. for those which prove bounds on the speed of con-
vergence, the convergence is usually measured by (  ,   )-usefulness [3]. if the
mechanism output   f (d) is an (  ,   )-useful answer to f on dataset d, then with
id203 1       , the di   erence between   f (d) and f (d) is less than   . such
mechanisms usually provide a relationship between data size, model settings,   
and   . a few papers [33] provide worst case guarantees on the distance, which
is equivalent to (  , 0)-usefulness. yet another paper [25] uses the expectation
of di   erence.

below, we will describe the utility analysis of various algorithms, but we
cannot always compare two mechanisms that di   er on some of these aspects.
furthermore, even if we can compare the utility of two mechanisms, one might
outperform in some situations while the second outperforms in others. suppose
one is (  ,   )-di   erentially private and the other is   -di   erentially private. if we
can tolerate a very small id203 that information is leaked, then the    rst
may be better; if we are opposed to taking any risk, the second may be better.
therefore, choice of mechanism can depend on speci   c applications.

2.2 general ideas of di   erentially private machine learn-

ing algorithms

many di   erentially private machine learning algorithms can be grouped accord-
ing to the basic approaches they use to compute a privacy-preserving model.
this applies both for supervised and unsupervised learning.

some approaches    rst learn a model on clean data, and then use either
the exponential mechanism or the laplacian mechanism to generate a noisy
model. for example, [52, 40, 46, 28, 30] use the laplacian mechanism, while

9

[7, 53] use the exponential mechanism. for some other approaches that have
many iterations or multiple steps, the laplacian mechanism and the exponen-
tial mechanism are applied to output parameters of each iteration/step. such
approaches include [21, 31, 24, 19, 25, 41, 55].

some mechanisms add noise to the target function and use the minimum/maximum

of the noisy function as the output model. these technique is called objective
perturbation. some examples include [56, 5, 6, 45].

some mechanisms use the idea of the sample and aggregate framework. they
are specially designed for queries that can be measured with a small number
of samples. first, they split the dataset into many small subsets. next, they
combine the results from all subsets to estimate a model, adding noise in this
aggregation step. mechanisms that employ this idea include [42, 27]. the linear
regression in [14] is partially based on this idea.

some mechanisms explore other ideas. for example, [33] partitions the sam-
ple space and uses counts in each partition to estimate the density function.
[26] interprets a model as a function and uses another function to approximate
it by iteratively minimizing the largest distance.

most output perturbation and objective perturbation mechanisms require a
bounded sample space. this is because unbounded sample space usually leads to
unbounded sensitivity. mechanisms based on the sample and aggregate frame-
work don   t have this limitation. however most of them use (  ,   )-di   erential
privacy.
in practice, if the sample space is unbounded and we want to use
  -di   erential privacy, we can simply truncate the values in pre-processing. if
the rule to truncate is independent of the private data, then the truncation is
privacy-safe.

in the next sections, some di   erentially private machine learning mecha-
nisms are introduced. we will brie   y introduce the learning models, describe
additional conditions assumed by various authors, explain how they design the
mechanisms, and provide some utility analysis. however, the computation of
sensitivity, the proof of di   erential privacy and some mechanism details won   t
be discussed here.

3 di   erentially private supervised learning

supervised machine learning describes the setting when labels are known for
training data, and the task is to train a model to predict accurate labels given
a new example. in this section we will describe di   erentially private versions of
commonly used supervised machine learning algorithms.

3.1 naive bayes model

the naive bayes model is a classi   er which predicts label y according to fea-
tures in x. given features x and a model, one can compute the conditional
id203 p (y |x) for all labels y and predict the label with largest con-
ditional id203. the naive bayes model is based on two assumptions.

10

the    rst assumption is that xj are conditionally independent given y , i.e.,
p (xj|y, x1, ..., xj   1) = p (xj|y ). this enables us to compute the coe   cient
of each feature independently. the second assumption is that for all numerical
features in x, p (x|y ) is a normal distribution.
bility is as follows:

based on the    rst assumption and bayes    theorem, the conditional proba-

p (y |x1, ..., xp)     p (y )

p

yj=1

p (xj|y )

to train the model, we need to estimate all the p (y ) and p (xj|y ). the
probabilities p (y ) can be estimated by the frequencies of samples with label y
in the training set. for conditional probabilities p (xj|y ), the training is based
on whether xj is categorical or numerical. if xj is a categorical feature, for
all values x and y, we have p (xj = x|y = y) = p (xj = x, y = y)/p (y =
y) = pi i[xij = x]i[yi = y]/pi i[yi = y]. thus we need counts pi i[yi = y]
and pi i[xij = x]i[yi = y] to compute the conditional probabilities. if xj is
numeric, then based on the second assumption, the normal distribution p(xj|y )
is decided by e[xj|y ] and v ar[xj|y ]. thus to compute the model we only
need the following information: pi i[yi = y], all pi i[xij = x]i[yi = y] for
categorical variables and all e[xj|y ] and v ar[xj|y ] for numerical variables.
an   -di   erentially private naive bayes model mechanism is introduced in
[52]. this mechanism relies on one additional assumption: all values for all
features in the dataset are bounded by some known number. if the bound cov-
ers most of the gaussian distribution, then both the bound assumption and
gaussian assumption hold approximately. therefore the sensitivity of the infor-
mation that is needed to compute the model can be calculated. the mechanism
then adds noise to this information according to the laplacian mechanism and
computes the model. although no analysis on utility is provided, it is easy to
see that the noise on the parameters is o(1/n  ).

sometimes the (non-private) naive bayes model is more accurate if we model
the continuous features with histograms instead of a gaussian distribution.
however, in this case, many histograms may lead to high sensitivity. thus
as long as the gaussian assumption is not far from the truth, there is no need
to use histograms. a good assumption about a distribution can result in good
performance.
if in extreme cases the assumption is too far from the truth,
we can represent those features with histograms in preprocessing. if the rule in
preprocessing is independent of the private dataset, the preprocessing is privacy-
free.

another question is whether we can use the logarithms of countspi i[xij =

x]i[yi = y] here, as we sometimes do in using the standard naive bayes model.
clearly, we can apply the laplacian mechanism to logarithms, however this
change is useless. for example, suppose the true count is c, the noisy count
is   c and we add noise to log(c + 1). according to the laplacian mechanism,
(c) = 1|c = 0) stays
the sensitivity of log(c) is log 2. thus the id203 p (
(c) = 9|c = 4) increases a lot. such
the same, while probabilities such as p (

  

  

11

transformation cannot reduce noise when the count is small, however it increases
noise a lot when the count is large. therefore, it is better to add noise to the
counts directly.

3.2 id75

id75 is a technique for predicting numerical values y in which the
value is modelled as a linear combination wt x of features x. here, the vector
w contains the weights corresponding to each feature and constitutes the set of
parameters which must be optimized during training. to train the model, w is

computed by minimizing square loss pi(yi     wt xi)2 over the training set.

[56] assumes bounded sample space and proposes a di   erentially private
mechanism for id75. as the id168 is analytic, the mecha-
nism expands the function with taylor expansion, approximates it with a low
order approximation, and adds noise to the coe   cients of the terms. the mech-
anism then    nds the w that minimizes the approximate id168. as the
sensitivities of the coe   cients are easy to compute, the laplacian mechanism
can ensure the di   erential privacy of the noisy approximation. since no pri-
vate information is used after adding noise, the output vector w here is also
  -di   erentially private. furthermore, the model that is decided by w is also
di   erentially private.

3.3 linear id166

linear id166 is a linear classi   er in which a vector w captures the model param-
eters. a linear id166 model outputs a score wt x for features x in a sample,
and usually uses sign(wt x) as the label y . the parameter w is computed

by minimizing cpi max(0, 1     yi(wt xi)) + wt w/2, while c > 0 is an input

parameter which sets the strength of prediction error.

under the assumption that the sample space is bounded, the linear id166
model satis   es the following two conditions. first, it computes w by minimizing
a strongly convex and di   erentiable id168 l(w). second, a change of one
sample results in bounded change in l   (w). for linear id166 and all other models
satisfying the two conditions, [5, 6] provide an output perturbation mechanism
and an objective perturbation mechanism. the output perturbation mechanism
   rst trains the model and then adds noise to it. the objective perturbation
mechanism introduces noise by adding a carefully designed linear perturbation
item to the original id168. the w computed from the perturbed loss
function is   -di   erentially private.

[6] also provides a performance analysis of the objective perturbation mech-
anism. to achieve (  ,   )-usefulness and   -di   erential privacy, the mechanism
needs o( log(1/  )
) samples. as de   ned earlier, (  ,   ) usefulness
provides a guarantee that with respect to the true id168, the performance
of the private model will be within a distance    of that achieved by the true
model with id203 greater than 1       .

     + log(1/  )

  2 + 1

    

12

3.4 id28

id28 is model for binary classi   cation. it makes prediction p (y =
1|x) = 1/(1 + e   wt x ) given features x in a sample. the parameters w are
trained by minimizing negative log-likelihood pi log(1 + exp(   yiwt xi)) over
the training set.

regularized id28 di   ers from standard id28 in
its w is computed by

that the id168 includes a id173 term.

minimizing pi log(1 + exp(   yiwt xi)) +   wt w over the training set {(xi, yi)}

while    > 0 is a hyperparameter which sets the strength of id173.

assuming that the sample space is bounded, the mechanism in [56] (see
section 3.2) can be applied to make both models   -di   erentially private. fur-
thermore, the output perturbation and objective perturbation mechanism in
[5, 6] (see section 3.3) can ensure   -di   erential privacy for regularized logistic
regression.

3.5 kernel id166

kernel id166 is a machine learning model that uses a id81 k(, ), which
takes two samples as input and outputs a real number. di   erent id81s
lead to di   erent id166 models. kernel id166 can be used both for classi   cation
and regression. when used to classify a sample with features x, kernel id166

predicts the label y = sign(pi wik(x, xi)); when used for regression, kernel
id166 predicts the quantity y = pi wik(x, xi). in both cases {(xi, yi)} are
training samples and wi are weights in the model to compute. note the model
includes the id81 k(, ), all training data and a vector of weights
{wi}n
i=1. although there exist many algorithms to train kernel id166, i will only
address those relevant to current di   erentially private versions.

unlike previous models, kernel id166s contain all the training data. therefore
the techniques required to make di   erentially private kernel id166 mechanisms
are di   erent from those we have already described. in [6, 45], an idea for private
kernel id166 is proposed. it works for all translation-invariant kernels, where
there exists some function g(x) such that k(x1, x2) = g(x1     x2)   x1, x2. for
example, radial basis function kernel is translation-invariant. the basic idea is to
approximate id81s in the original sample space with a linear kernel
in another space, so as to avoid publishing training data.
it    rst constructs
a space independent of private training data and then projects data from the
original sample space to that space. according to [44], the id81 of two
samples in the original sample space can be approximated by the inner product
of their projections in the new space. thus the kernel id166 model turns out to
be a linear id166 model in the new space and we can use private linear id166
mechanisms mentioned above. furthermore, the non-private projection can be
published, thus future data can be projected to the same space and then use
the parameters from private linear id166 to predict. in this way, the mechanism
transforms a kernel id166 model training problem into a linear id166 training
problem. to achieve both (  ,   )-usefulness w.r.t. to predictions on any sample

13

and   -di   erential privacy, this mechanism needs n = o( log1.5(1/    )

) samples.

    3

the previous mechanism can not be applied to id81s that are not
translation-invariant, such as polynomial kernel or sigmoid kernel. therefore
[26] proposes another private kernel id166 algorithm for all rkhs kernels. an
rkhs kernel means that there is some function   (x) that projects x onto an-
other space such that k(x1, x2) equals the inner product of   (x1) and   (x2).
this mechanism seems similar to the one previously described (where the pro-
jection can be seen as an approximate to   (x)), however here projection doesn   t
need to be explicit.

the test data-independent learner (ttdp) mechanism in [26] publishes
a private kernel id166 model satisfying (  ,   )-di   erential privacy as follows. in-
tuitively, it trains a non-private kernel id166 model f (x) from the private data
and then approximates it in a di   erentially private way. the private model g(x)
is trained iteratively. first, the mechanism sets g(x) = 0. then it computes a
non-private model f (x). next, it constructs a    nite set z from the unit sphere
that represents the sample space. each iteration consists of three steps. in the
   rst step, the mechanism selects a point z     z where g(z) and f (z) disagree the
most. this selection is based on the exponential mechanism, and |g(z)   f (z)| is
used as the score function. in the second step, a noisy di   erence |g(z)   f (z)|+  
is computed while    is laplacian noise. in the third step, the mechanism tests
whether the noisy di   erence exceeds some threshold.
if not, the mechanism
proceeds to the next iteration; if it does exceed the threshold, the mechanism
updates g(x) to be closer to f (x) at point z. after many iterations, g(x) may
approximate f (x). to achieve both (  ,   )-usefulness w.r.t. to predictions on
any sample in the unit ball and (  ,   )-di   erential privacy, this mechanism needs

n = o(

log3 1

   log1.5 1

  

  1.5  0.75

) samples.

3.6 decision tree learning

learning a decision tree classi   er involves partitioning the sample space and
assigning labels to each partition. the training algorithm for a decision tree
classi   er consists of a tree building process and a pruning process. in the tree
building process,    rst the entire sample space and all samples are put in the root
partition. then the algorithm iteratively selects an existing partition, selects
a variable based on the samples in that partition and a score function such as
information gain or gini index, and partitions the sample space (and the sam-
ples corresponding to that partition) according to the variable selected. if the
selected variable is categorical, usually each value of that variable corresponds
to a partition; if the variable is numerical, then some thresholds will be selected
and the partitioning is based on those thresholds. the partitioning process ends
when the spaces corresponding to all partitions are small enough or the numbers
of samples in each partition are too small. after building the tree, the pruning
process removes unnecessary partitions from the tree and merges their spaces
and samples to their parents.

[24] proposes an   -di   erentially private mechanism. the mechanism con-

14

structs n id90 and uses the ensemble to make classi   cation. when
constructing a decision tree ti, it randomly partitions the sample space into
partitions pi1, ..., pimi without using private data and computes countijy, noisy
counts of samples with each label y in partition pij . when predicting the la-
bel of a sample x, it looks for all the partitions p1a1 , ..., pn an from all trees
t1, ..., tn that include x, sums the counts of samples with each label from all
i=1 countiaiy, and computes the probabilities of label

those partitions sy = pn
y    by p (y = y   |x) = sy     /py sy.

[19] proposes another   -di   erentially private decision tree algorithm. the
mechanism is based on the assumption that all features are categorical, in order
to avoid selecting partition points for any feature.
in the partitioning pro-
cess, the mechanism uses the exponential mechanism to select the variable with
largest score (for example, information gain or gini index) di   erentially pri-
vately. each time a partition reaches a pre-determined depth, or the number of
samples in that partition is about the same scale as random noise, or the sample
space corresponding to that partition is too small, the mechanism stops operat-
ing on that partition. it then assigns to that partition a noisy count of samples
with each label. after the partitioning process has completed altogether, these
noisy counts are used to decide whether to remove those nodes without having
to consider privacy.

3.7 online convex programming

many machine learning techniques, such as id28 and id166, specify
optimization problems which must then be solved to    nd the optimal param-
eters. online algorithms, such as id119, which consider examples
one at a time, are widely used for this purpose. to that a machine learning
algorithm is di   erentially private, it is therefore important to demonstrate that
the optimization algorithm doesn   t leak information.

online convex programming (ocp) solves convex programming problems in
an online manner. the input to an ocp algorithm is a sequence of functions
(f1, ..., ft ) and the output is a sequence of points (w1, ..., wt ) from a convex set
c. the algorithm is iterative and starts from a random point w0. in the t-th
iteration, the algorithm receives the function ft and outputs a point wt+1     c
according to (f1, ..., ft) and (w1, ..., wt). the target of the ocp algorithm is to
minimize regret, which is de   ned as

r =

t

xt=1

ft(wt)     min

w   c

ft(w).

t

xt=1

note that the input here is a sequence of functions instead of samples.

there are many methods to    nd wt+1     c according to (f1, ..., ft) and
[25] provides (  ,   )-di   erentially private versions for two of them:
(w1, ..., wt).
the implicit id119 (igd) and the generalized in   nitesimal gradi-
ent ascent (giga) given all the functions are l-lipschitz continuous for some

15

constant l and   -strongly convex. igd    rst computes

  wt+1 = arg min

w   c(cid:18) 1

2kw     wtk2 +

1
  t

ft(w)(cid:19)

and projects   wt+1 onto c to get the output wt+1. giga    rst computes

  wt+1 = wt    

1
  t   ft(wt)

and then does the projection. both algorithms ensure bounded sensitivity of
wt+1 given wt. the private mechanism in [25] adds gaussian noise to every   wt
before it is projected to wt to preserve privacy, and then use the noisy wt for
the future computation. given t functions(samples), the expected regret by

this (  ,   )-di   erentially private mechanism is o(cid:16)q t

   ln2 t

  (cid:17).

4 di   erentially private unsupervised learning

unsupervised learning describes the setting when there are no labels associated
in the absence of labels, unsupervised machine
with each training example.
learning algorithms    nd structure in the dataset.
in id91, for example,
seeks to    nd distinct groups to which each datapoint belongs. it can be useful in
many contexts such as medical diagnosis, to know of an individuals member ship
in a group which shares certain speci   c characteristics. however, releasing the
high-level information about a group may inadvertently leak information about
the individuals in the dataset. therefore it is important to develop di   erentially
private unsupervised machine learning algorithms.

4.1 id116 id91

id116 is a commonly used model in id91. to train the model, the
algorithm starts with k randomly selected points which represent the k groups,
then iteratively clusters samples to the nearest point and updates the points by
the mean of the samples that are clustered to the points.

[42] proposes an (  ,   )-di   erentially private id116 id91 algorithm us-
ing the sample and aggregate framework. the mechanism is based on the as-
sumption that the data are well-separated.
   well separated    means that the
clusters can be estimated easily with a small number of samples. this is a
prerequisite of the sample and aggregate framework. the mechanism randomly
splits the training set into many subsets, runs the non-private id116 algorithm
on each subset to get many outputs, and then uses the smooth sensitivity frame-
work to publish the output from a dense region di   erentially privately. this step
preserves privacy while the underlying id116 algorithm is unchanged.

any modi   cations on the id116 id91 algorithm (such as id116++)
can be used in the sample step, with the sole restriction that such modi   cations
leave intact the property that the algorithm can be estimated with a small

16

number of samples. additionally, if the sample space is bounded and the number
of samples surpasses a threshold, there is a bound on the noise added. however
this bound is not directly related to the number of samples in the dataset.

5 di   erentially private dimensionality reduc-

tion

in machine learning contexts, when data is high dimensional, it is often desir-
able learn a low-dimensional representation. lower dimensional datasets yield
models with less degrees of freedom and tend to be less prone to over   tting.
from a di   erential privacy perspective, lower dimensional representations are
desirable because they tend to have lower sensitivity.

feature selection is one technique for id84, in which a
subset of features is kept from an original feature space. principal component
analysis (pca), on the other hand is a id105 technique in which
a linear projection of the original dataset into a low dimensional space is learned
such that the new representation explains as much of the variance in the original
dataset as possible.

5.1 feature selection

[53] proposes an   -di   erentially private feature selection, privatekd, for classi-
   cation. privatekd is based on the assumption that all features are categorical
and each feature has    nite possible values. for any set of features s, it de   nes
a function f (s) which tells how many pairs of samples from di   erent classes
can features in s distinguish. the set of selected features s    is initialized to    .
then a greedy algorithm adds new features one by one to s   . when selecting
a feature to add, the algorithm uses the exponential mechanism to select the
feature that can lead to the largest increase of f (s   ). the paper provides a
utility guarantee for the special case where the cardinality of sample space m
and the number of features d have the relation m = d     1. in that case, ex-
cept id203 o(1/poly(m)) (poly(m) means a polynomial expression of m),
f (s   )     (1     1/e)f (soptimal)     o(log m/  ).
[27] proposes an (  ,   )-di   erentially private algorithm for feature selection
when the target function is stable. unlike the previous paper, this paper doesn   t
explicitly state the algorithm for feature selection. instead, it only requires the
selection algorithm to be stable. by    stable   , we mean that either the value of
function as calculated on the input dataset doesn   t change when some samples
in the set change, or that the function can output the same result on a random
subset from the input dataset with high id203. for the    rst kind of func-
tions, the mechanism uses the smooth sensitivity framework in [42] to select
features. if adding or removing any log(1/    )
samples from the input dataset
doesn   t change the selection result, then the algorithm can output the correct
selection result with id203 1       .

  

17

for the second kind of functions, the mechanism uses an idea similar to the
sample and aggregate framework in [42]:
it creates some bootstrap sets from
the private dataset, selects features non-privately on each set, and counts the
frequencies of feature sets output by the algorithm. intuitively, if the number
of samples is large and the features set is not too large, there is high id203
that the correct output is far more frequent than any other one. thus, the
mechanism can release the most frequently selected set of features. if a ran-
dom subset of the input dataset with   /(32 log(1/  )) samples outputs the same
selection result with id203 at least 3/4, then the mechanism outputs the
correct solution with id203 at least 1       .

5.2 principal component analysis

principal component analysis (pca) is a popular method in dimension reduc-
tion.
it    nds k orthogonal directions on which the projections of data have
largest variance. the original data can then be represented by its projection
onto those k directions. usually k is much smaller than the dimension of sam-
ple space. thus the projection greatly reduces the dimensionality of the data.
it is well-known that this analysis is closely related to eigen-decomposition: if
we rank the eigenvectors of matrix a = v ar[x] according to the corresponding
eigenvalues   1       2     ...       p, then the    rst k eigenvectors are the k directions.
there are two di   erentially private mechanisms to select eigenvectors iter-
atively. the iterative methods are based on the spectral decomposition, which
ensures that if the components corresponding to the    rst i     1 eigenvectors of
a are subtracted from a, then the i-th largest eigenvector becomes the largest
eigenvector of what remains. therefore the process of selecting the largest k
eigenvectors can be replaced by repeatedly    nding the    rst eigenvector and re-
moving the component corresponding to the selected eigenvector. the following
two mechanisms both make use of this idea but di   er on how to select the    rst
eigenvector.

an (  ,   )-di   erentially private mechanism is proposed in [21]. the mecha-
nism uses the power method: anv/kanvk converges to the    rst eigenvector of
a if v is not orthogonal to the    rst eigenvector. it randomly starts with a unit-
length vector v, then iteratively updates v with (av +   i)/kav +   ik while
  i is gaussian noise in the i-th iteration. since it is exceedingly improbable
that a random vector is orthogonal to the    rst eigenvector, the vector v will
get close to the    rst eigenvector. however due to the noise, v cannot converge
with arbitrary accuracy. thus it outputs v after a    xed number of iterations
and proceed to    nd the next largest eigenvector. a utility guarantee is provided
on the power method, which outputs the    rst eigenvector. however there is
no direct guarantee on the k eigenvectors. for each eigenvector a output by
running the power method on matrix a, the distance from the    rst eigenvector

(kaak/kak       1 while   1 is the    rst eigenvalue) is o((plog(1/  ) log n)/  ).

[31] provides an   -di   erentially private mechanism for principal component
analysis. according to the property that the    rst eigenvector v of a is the unit-
length vector that maximizes vt av, the mechanism uses h(x, v) = vt av as

18

the score function in the exponential mechanism to select the    rst eigenvector
from the set {v : vt v = 1} di   erentially privately. the selection algorithm is
specially designed to be computable in reasonable time. this paper also provides
two proofs on utility of this mechanism. for any 0 <    < 1 and privacy budget
  , if the    rst eigenvalue of matrix a,   1 > o(ln(1/  )/(n    )), then the    rst
eigenvector v has the property e[vt av]     (1       )  1. for any 0 <    < 1 and
privacy budget   , if the    rst eigenvalue   1 > o(1/(n    6)), the k+1-th eigenvalue
is denoted   k+1, and the k-rank approximation matrix output is denoted ak,
then the largest eigenvalue of a     ak is smaller than   k+1 +     1 with large
id203.
not all di   erentially private approaches to pca rely on iterative algorithms.
[7] proposes an   -di   erentially private mechanism, ppca, to compute k largest
eigenvectors at the same time. the mechanism uses the property that the    rst k
eigenvectors of a are the columns of the p  k matrix v = arg maxv :v t v =i k tr(v t av ).
therefore, it uses h(x, v ) = tr(v t av ) as the score function and selects v
from the set of matrices {v : v t v = i k} according to the exponential mecha-
nism. a gibbs sampler method is used here to select the matrix v . this paper
provides a guarantee on the    rst eigenvector. for any 0 <   ,    < 1, if the sample

size n = o(cid:16) 1
eigenvector and true eigenvector is larger than    with id203 1       .

1     2(cid:17)(cid:17), then the inner product of output    rst

  (1     )(cid:16)log 1

   + log

1

6 statistical estimators

statistical estimators calculate approximations of quantities of interest based
upon the evidence in a given dataset. simple examples include the population
mean and variance. while estimators are clearly useful, they may potentially
leak information about the individuals contained in the dataset, especially when
the dataset is small or features are rare. therefore, to protect privacy, it is
necessary to develop di   erentially private estimators.

6.1 robust statistics estimator

[14] proposes an (  ,   )-di   erentially private mechanism for robust statistical es-
timators. roughly speaking, a statistical estimator produces an estimate of a
vector (such as the mean and variance of a gaussian distribution) based on the
input dataset. the estimator t can be seen as a function that maps a dataset
d to the output vector t (d). most statistical estimators converge when the
number of samples tends to in   nity and the samples are i.i.d. drawn from some
distribution p . when the estimator does converge, the limit lim|d|   +    t (d)
is denoted t (p ). the de   nition of robust estimator is based on the stability
of estimates. an estimator is robust if for any element x in the sample space,
the following limit exists limt   0(t ((1     t)p + t  x)     t (p ))/t. the distribution
(1     t)p + t  x means that with id203 1     t the sample is from p and with
id203 t the sample is x.

19

p. it divides r

the output of a robust estimator doesn   t change much if a small number
of samples change. based on the property, [14] comes up with a propose-
test-release framework. the framework is based on the assumption that the
p into small cubes, then computes the statistics
statistics are in r
t (d) from a dataset d and the number of sample changes needed to make
t (d) fall into another cube.
if the number is large, the statistics are stable
and thus the mechanism can add laplacian noise to the statistics to make it
private; if the number is small, then the mechanism outputs    , which means
it fails. when the number of samples tends to in   nity, and the samples are
i.i.d. drawn, the framework output is asymptotically equivalent to a non-private
robust estimator.

based on this framework, [14] proposes three mechanisms for interquartile
range estimation, trimmed mean and median, and id75, respectively.
when applying the framework to id75, the framework uses a robust
estimator to learn a model from the training set {(xi, yi)}

  w = arg min

w xi

|yi     wt xi|

kxik

instead of minimizing the mean square error. given n samples, the linear regres-
sion estimator can successfully output a model with id203 1     o(n   c ln n)
for some constant c. additionally, its output converges to the true linear regres-
sion parameter when n tends to in   nite.

[4] explores robust estimators in another way. they prove that if e   ect of
one sample is bounded by o(1/n), and the range of t (p ) is bounded, then the
smooth sensitivity framework provides bounded error.

however, if t (p ) is not bounded, and if for any value    in an in   nite range,
there exists some p such that t (p ) =    , then the error of any   -di   erentially
private mechanism cannot be upper bounded.

here, a bound on the e   ect of one sample means that there exists a uniform
upper bound m (p ) for distribution p , such that for all distributions p     satis-

to achieve both (  ,   )-di   erential privacy and (  ,   )-usefulness, the smooth

fying |p     p    |     o(p1/n), all x in the sample space, t ((1     1/n)p +   x/n)    
t (p ))     m (p )/n.
sensitivity framework requires o(cid:16) ln(1/  )

     + ln2(1/  ) ln2(    / ln(1/  ))

(cid:17) samples.

  2 ln(1/  )

6.2 point estimator

[49, 50] give a di   erentially private mechanism for point estimation. using the
notation in 6.1, the mechanism splits d into k subsets with equal size {d1..., dk}
randomly and estimates parameters {t (d1), t (d2), ..., t (dk)} on each subset.
then it uses a di   erentially private mean of all t (di) to approximate t (d). the
mean is computed in two steps. first, if the space of parameters is unbounded,
it computes two quantiles and truncates all estimates according to the quantiles
and the sample size. second, the mechanism adds laplacian noise to the mean
of truncated values and publishes the noisy mean. when the space of possible

20

parameters are bounded, then the mechanism only executes the second step
and becomes   -di   erentially private. when the space of possible parameters
is not bounded, then both steps have to be executed and the mechanism is
(  ,   )-di   erentially private.

the papers give su   cient conditions under which the mechanism is as accu-
rate as non-private estimators asymptotically. the conditions are listed below.
n is the number of samples in d and   p is a real number. for models that don   t
converge to a    xed point (for example, some em algorithms) or models with
varying numbers of features (such as kernel id166), there is no such guarantee.

t (d)     t (p )

  p /   n     n (0, 1) when n     +   

e[t (d)]     t (p ) = o(1/n)
(cid:19)3# = o(1)
e"(cid:18)|t (d)     t (p )|
  p /   n

6.3 m-estimator

[33] proposes an   -di   erentially private mechanism for m-estimator. unlike the
robust estimator above, the de   nition of an m-estimator depends on the function
from which the estimates come. m-estimation relies on a function m(, ), which
takes a sample and a parameter    as input and outputs a real number. an
m-estimator estimates the parameter    by computing

     = arg min

  

m(xi,   )

1

nxi

the mechanism in [33]    rst divides the sample space ([0, 1]d) into many small
cubes without using private data. then it adds laplacian noise to the counts
of samples in the cubes and computes the density function in each cube by
dividing the noisy count corresponding to that cube by the volume of the cube.
the noisy density function leads to a noisy target function in the minimization
problem above, and the noisy target function leads to a noisy minimum     . the
noisy minimum can be released as an estimate.
the output parameter will converge to the true parameter based on the
distribution of training data with speed o(n   1/2 + (   log n/n)2/(d+2)) under
some regularity conditions. here d is the number of features and n is the number
of samples.

7 learning in private data release

many di   erentially private data release mechanisms have been described, such
as [3, 1, 18, 10]. in this section, we focus on data release mechanisms that are
either useful for machine learning or based on machine learning algorithms.

21

many papers use partition based algorithms to release data.

[55] assumes
that the density function is smooth, [8, 57] assume that the data   s format per-
mits it to be organized in a tree, and [41] assumes that partitions can preserve
most important information for further data mining. all these assumptions
motivate partitioning the sample space and publishing counts in each partition.
with respect to the mechanism design, the mechanisms can be divided into two
groups. [41]    rst partitions the sample space using the exponential mechanism
and then adds noise to the counts using the laplacian mechanism. some others
([8, 57, 55]) generate noisy counts with the laplacian mechanism for each cell
and then partition according to the noisy counts.

some data release mechanisms don   t depend on partitioning. some of them
assume the private data is    t well by some family of models. they select an
optimal model from that family privately and then generate new data according
to the selected model. some others assume some property (like sparsity) of the
data, and propose mechanisms that can make use of that property.

[40, 46] publish a graph generator model based on the assumption that the
private data is    t well by some parametrized generative model. though the
two mechanisms use di   erent generative models, both train the model    rst, add
laplacian noise to the parameters of the model, and then use the noisy model
to generate a new graph.

[54] represents the network structure by using a statistical hierarchical ran-
dom graph model. unlike the two models above, the number of parameters
in this model is proportional to the number of nodes in the graph. thus we
will introduce too much noise if we use the laplacian mechanism to publish the
model. as the parameter space is very large and no score function exists, which
is both simple and meaningful, it is not easy to use the exponential mechanism
directly. to overcome this di   culty, the authors propose a mechanism based
on the id115 procedure. it uses the metropolishastings
algorithm to draw a model from the distribution in the exponential mechanism
given the score function is the likelihood function. though the likelihood func-
tion is complicated w.r.t. the whole space of parameters, it is simple w.r.t. one
parameter if all the others are    xed. thus the id115 pro-
cedure is possible. the graph can be reconstructed from the output model after
many iterations.

sometimes corresponding to a large private dataset of interest, there exists
a similarly structured but smaller publicly available dataset.
[28] makes use
of a public dataset, assigning weights to its examples to embed information
contained in the private data. the mechanism assumes the existence of such a
public dataset, but does not require that it is drawn from the same distribution
as the private data. first they use public/private datasets as positive/negative
samples in a id28 model. they train a noisy id28
model and assign weights based on the noisy model to all public samples. the
weighted public dataset can replace the private dataset in future data mining. if
the weighted set is used in measuring the expectation e[f (x)] for some function
f (x) while x is from the distribution of private data, the standard deviation
of the estimate is o(1/   n). if the assumption holds exactly, then the estimate

22

is asymptotically unbiased.

[30] assumes that samples have binary labels and that they are    t by a
id156 (lda) model. the idea is similar to the exponential
mechanism: the exponential mechanism computes the non-private lda model
from the private dataset d. then, it uses the distance between the non-private
model parameters and the lda parameters trained from another dataset d    as
the score function, and draws a dataset di   erentially privately.

the mechanism in this paper, however,    rst computes a private lda model
by the laplacian mechanism, and then draws a dataset that minimizes the
distance. such an output dataset can preserve the classi   cation information
from the private data.

[35] assumes that the data matrix is sparse, thus the mechanism can make
use of results from compressive sensing research. informally speaking, if we ran-
domly project the high-dimensional data matrix to a low-dimensional space, and
then attempt to recover the high-dimensional matrix using the low-dimensional
embedding and the constraint that the recovered matrix is sparse, there is high
id203 that the recovered matrix exactly matches the original one.
[35]
   rst randomly projects the data matrix, then adds noise to the compressed in-
formation, and reconstructs data from the noisy compressed information. as
the dimension of compressed information is much smaller than that of the orig-
inal data matrix, the scale of laplacian noise needed to preserve   -di   erential
privacy can be reduced from o(   n/  ) to o(log n/  ) given n samples.

8 theoretical results

[32] studies the general properties of private classi   ers, instead of individual
learning models. they    rst de   ne a problem to be learnable if there is an
algorithm that can output a highly accurate model with a large id203
given enough data. the accuracy here is measured as the percentage of samples
that are correctly classi   ed. they further claim that if a problem is learnable,
then it can be learned di   erentially privately. the mechanism they constructs
takes the number of correctly classi   ed samples as a score function and uses
the exponential mechanism to draw the best model, which it calls the best
hypothesis in a class.

[39] formulates di   erentially private learning in an information theoretic
framework. the paper uses a concept in id205, pac-bayesian
bound. this concept is for parametrized models that have bounded loss func-
tions and it uses the bayesian learning framework. pac-bayesian bound p ac(    ,   ,   ,   )
is a function of the posterior distribution      of model parameters, the prior dis-
tribution    of parameters, a positive number    representing the privacy budget,
and another number        (0, 1), which is related to the strength of the bound.
with id203 1        and prior distribution   , p ac(    ,   ,   ,   ) upper bounds
the expected loss on the true distribution if the model parameter is from the dis-
tribution     . this bound varies given di   erent   . according to [39], the output
of the exponential mechanism follows the posterior distribution that minimizes

23

pac-bayesian bound.

note that the pac-bayesian bound upper bounds the id168. it is
possible that other mechanisms have id168s that achieve a better bound.
therefore the conclusion in [39] doesn   t necessarily mean that the exponential
mechanism is the best.

9 discussion

the papers reviewed by this survey address the question of how to train a
di   erentially private model with as little noise as possible. to summarize, there
are generally four guiding principles for reducing the scale of noise. first, adding
noise only one time is usually better than adding noise many times. this is
because if we add noise many times, we have to split the privacy budget into
many smaller portions and let each noise addition procedure use one portion.
because the budget allocated to each procedure is small and the scale of noise
is inversely related to the privacy budget, the amount of noise added in each
procedure is large. furthermore, when we aggregate the outputs, the noise can
grow even larger. therefore one-time noise addition is usually better.

for example, when we train a id28 model, we can add noise
to the training process, the target function or the    nal model. adding noise
to the target function is a one-time procedure. this is also true for the    nal
model. however, as the training process is iterative, adding noise during training
requires adding noise many times. according to our experience, noise addition
in training process leads to signi   cantly worse performance.

second, lower global sensitivity (compared to the result) leads to smaller
noise. in one strategy to lower global sensitivity, some queries be approximated
by combining the results of other queries, each of which have far lower global
sensitivities than the original query. for example, [52] adds noise to the counts
that generate the naive bayes model instead of conditional probabilities of the
model directly. the global sensitivity of each id155 is 1, which
is too high to be useful. the global sensitivity of each underlying count is 1,
which is much lower compared to the counts. by adding noise to those counts,
we encounter lower global sensitivity.

another approach is to modify the model. for example, [45] transforms
kernel id166 to linear id166, and [14] uses a robust id75 model to
replace the commonly used model.

third, use of public data, when available, can reduce the noise in some cases.
for a private dataset, there is often a smaller public dataset drawn from a similar
population. this public dataset can be from a previous leak or by consent of
data owners. because di   erentially private mechanisms distort private data, the
smaller public dataset sometimes provides similar or better utility. according
to [28, 29], such a public dataset can enhance the performance of di   erentially
private mechanisms.

fourth, for some models, iterative noise addition may be reasonable. there
are some times when the sensitivity of output model parameters is very large

24

but the iterative algorithm has smaller sensitivity. this statement may seem
counterintuitive, as the sum of sensitivities of all iterations should be similar to
the sensitivity of the model parameters. however, in some cases, the sensitivity
of each iteration is determined by the parameter before that iteration. thus
the sum of sensitivities of those iterations in fact relies on the training path.
excepting some extreme cases, the sum can be much smaller than the sensitivity
of the model parameters. in this case, it seems necessary to add noise in the
iterations.

for those models, one can consider trying the mcmc-based algorithm as
in [48]. likelihood function or id168 can be used as score functions and
the metropolis hastings algorithm ensures that the output is from the same
distribution as that in the exponential mechanism. this idea is still not widely
used, however it seems possible that it improves learning performance.

in addition to these four ideas, some other issues warrant attention. for
example, most di   erentially private mechanisms use clean and complete data as
input, which is not always available in practice. furthermore, traditional meth-
ods for missing data or pre-processing may not satisfy di   erential privacy. thus
mechanisms that can deal with incomplete data are desired. such mechanisms
can either release data, or be combined with other di   erentially private learning
mechanisms.

when private data is discussed, medical data is typically o   ered as an exam-
ple application. however, medical datasets are often not relational. they may
be temporal, and sometimes structural. although we can transform such data,
the transformation may lose some important information and increase sensitiv-
ity. therefore, mechanisms specially designed for such data are required.

another important question is whether privacy can be free, i.e., achieved at
no cost to utility in di   erentially private learning. for privacy to be free, the
noise required to preserve privacy might need to be smaller than noise from sam-
ple randomness. in that case, it wouldn   t change the magnitude of noise to take
privacy into account. for example, [50] proves that (  ,   )-di   erential privacy is
free for learning models satisfying a certain set of conditions. the mechanism
in [6] ensures free   -di   erential privacy for regularized id28 models
and linear id166 models, where noise from sample randomness is o(1/   n) and
the noise to preserve privacy is o(1/n). the mechanism in [28] also proves that
the e   ect of noise brought by di   erential privacy is o(1/n), while the e   ect from
sample randomness is o(1/   n).

we should also consider the extent to which privacy is compatible with and
related to the idea of generalization in machine learning. intuitively, machine
learning algorithms seek to generalize patterns gleaned from a training set avoid-
ing the e   ects of sample randomness. ideally, these algorithms should be robust
to small changes in the empirical distribution of training data. a model which
   ts too heavily to individual examples loses generalizability and is said to over   t.
perhaps the goals of di   erential privacy and generalization are compatible.

25

acknowledgments

the authors would like to thank kamalika chaudhuri for her comments. the
authors are in part funded by nlm(r00lm011392).

references

[1] boaz barak, kamalika chaudhuri, cynthia dwork, satyen kale, frank
mcsherry, and kunal talwar. privacy, accuracy, and consistency too: a
holistic solution to contingency table release. in acm sigact-sigmod-
sigart symposium on principles of database systems, pages 273   282,
2007.

[2] avrim blum, cynthia dwork, frank mcsherry, and kobbi nissim. prac-
tical privacy: the sulq framework. in acm sigmod-sigact-sigart
symposium on principles of database systems, pages 128   138, 2005.

[3] avrim blum, katrina ligett, and aaron roth. a learning theory approach
to non-interactive database privacy. in acm sigact-sigmod-sigart
symposium on principles of database systems, pages 609   618, 2008.

[4] kamalika chaudhuri and daniel hsu. convergence rates for di   erentially

private statistical estimation. in icml, 2012.

[5] kamalika chaudhuri and claire monteleoni. privacy-preserving logistic
regression. in advances in neural information processing systems, pages
289   296, 2008.

[6] kamalika chaudhuri, claire monteleoni, and anand d. sarwate. di   eren-
tially private empirical risk minimization. in journal of machine learning
research, pages 1069   1109, 2011.

[7] kamalika chaudhuri, anand d. sarwate, and kaushik sinha. near-optimal
di   erentially private principal components. in advances in neural infor-
mation processing systems, pages 998   1006, 2012.

[8] rui chen, noman mohammed, benjamin c. m. fung, bipin c. desai, and
li xiong. publishing set-valued data via di   erential privacy. in interna-
tional conference on very large data bases, pages 1087   1098, 2011.

[9] graham cormode. personal privacy vs population privacy: learning to at-
tack anonymization. in international conference on knowledge discovery
and data mining, pages 1253   1261, 2011.

[10] graham cormode, cecilia m. procopiuc, divesh srivastava, and thanh
t. l. tran. di   erentially private summaries for sparse data. in interna-
tional conference on database theory, pages 299   311, 2012.

26

[11] anindya de. lower bounds in di   erential privacy. in theory of cryptog-

raphy, pages 321   338, 2012.

[12] cynthia dwork. di   erential privacy. in encyclopedia of cryptography and

security (2nd ed.), pages 338   340, 2011.

[13] cynthia dwork, krishnaram kenthapadi, frank mcsherry, ilya mironov,
and moni naor. our data, ourselves: privacy via distributed noise gen-
eration.
in international conference on the theory and applications of
cryptographic techniques, pages 486   503, 2006.

[14] cynthia dwork and jing lei. di   erential privacy and robust statistics. in
acm sigact-sigmod-sigart symposium on principles of database
systems, pages 371   380, 2009.

[15] cynthia dwork, frank mcsherry, kobbi nissim, and adam smith. cali-
brating noise to sensitivity in private data analysis. in theory of cryptog-
raphy conference, pages 265   284, 2006.

[16] cynthia dwork, guy n. rothblum, and salil p. vadhan. boosting and

di   erential privacy. in focs, pages 51   60, 2010.

[17] cynthia dwork and adam smith. di   erential privacy for statistics: what

we know and what we want to learn. 2008.

[18] chengfang fang and ee-chien chang. adaptive di   erentially private his-
togram of low-dimensional data. in privacy enhancing technologies, pages
160   179, 2012.

[19] arik friedman and assaf schuster. data mining with di   erential privacy. in
international conference on knowledge discovery and data mining, pages
493   502, 2010.

[20] srivatsava ranjit ganta, shiva prasad kasiviswanathan, and adam smith.
composition attacks and auxiliary information in data privacy. in kdd,
pages 265   273, 2008.

[21] moritz hardt and aaron roth. beyond worst-case analysis in private sin-

gular vector computation. in computing research repository, 2012.

[22] ori he   etz and katrina ligett. privacy and data-based research. 2013.

[23] nils homer, szabolcs szelinger, margot redman, david duggan, waibhav
tembe, jill muehling, john pearson, dietrich stephan, stanley nelson, and
david craig. resolving individuals contributing trace amounts of dna to
highly complex mixtures using high-density snp genotyping microarrays.
page 4(8):e1000167., 2008.

[24] geetha jagannathan, krishnan pillaipakkamnatt, and rebecca n. wright.
a practical di   erentially private random decision tree classi   er. in inter-
national conference on data mining workshops, pages 114   121, 2009.

27

[25] prateek jain, pravesh kothari, and abhradeep thakurta. di   erentially
in conference on learning theory, pages 24.1   

private online learning.
24.34, 2012.

[26] prateek jain and abhradeep thakurta. di   erentially private learning with
kernels. in international conference on machine learning, pages 118   126,
2013.

[27] j.czerniak and h.zarzycki. application of rough sets in the presumptive
diagnosis of urinary system diseases. in arti   cal inteligence and security in
computing systems, acs   2002 9th international conference proceedings,
pages 41   51, 2002.

[28] zhanglong ji and charles elkan. di   erential privacy based on importance

weighting. in machine learning, pages 163   183, 2013.

[29] zhanglong ji, xiaoqian jiang, shuang wang, li xiong, and lucila ohno-
machado. di   erentially private distributed id28 using private
and public data. page 7(suppl 1): s14., 2014.

[30] xiaoqian jiang, zhanglong ji, shuang wang, noman mohammed, samuel
cheng, and lucila ohno-machado. di   erential-private data publishing
through component analysis. pages 19   34, 2013.

[31] michael kapralov and kunal talwar. on di   erentially private low rank
approximation. in acm-siam symposium on discrete algorithms, pages
1395   1414, 2013.

[32] shiva prasad kasiviswanathan, homin k. lee, kobbi nissim, sofya
raskhodnikova, and adam smith. what can we learn privately? in ieee
symposium on foundations of computer science, pages 531   540, 2008.

[33] jing lei. di   erentially private m-estimators. in advances in neural infor-

mation processing systems, pages 361   369, 2011.

[34] ninghui li, tiancheng li, and suresh venkatasubramanian. t-closeness:
privacy beyond k-anonymity and l-diversity. in international conference
on data engineering, pages 106   115, 2007.

[35] yang d. li, zhenjie zhang, marianne winslett, and yin yang. compres-
sive mechanism: utilizing sparse representation in di   erential privacy. in
workshop on privacy in the electronic society, pages 177   182, 2011.

[36] ashwin machanavajjhala, johannes gehrke, daniel kifer, and muthu-
l-diversity: privacy beyond k-
in international conference on data engineering, page 24,

ramakrishnan venkitasubramaniam.
anonymity.
2006.

[37] frank mcsherry. privacy integrated queries: an extensible platform for
in sigmod conference, pages 19   30,

privacy-preserving data analysis.
2009.

28

[38] frank mcsherry and kunal talwar. mechanism design via di   erential pri-

vacy. in focs, pages 94   103, 2007.

[39] darakhshan j. mir. di   erentially-private learning and id205.
in international conference on extending database technology workshops,
pages 206   210, 2012.

[40] darakhshan j. mir and rebecca n. wright. a di   erentially private graph
estimator. in international conference on data mining workshops, pages
122   129, 2009.

[41] noman mohammed, rui chen, benjamin c. m. fung, and philip s. yu.
di   erentially private data release for data mining. in international con-
ference on knowledge discovery and data mining, pages 493   501, 2011.

[42] kobbi nissim, sofya raskhodnikova, and adam smith. smooth sensi-
tivity and sampling in private data analysis. in acm sigact-sigmod-
sigart symposium on principles of database systems, pages 75   84, 2007.

[43] sewoong oh and pramod viswanath. the composition theorem for di   er-

ential privacy. in computing research repository, 2013.

[44] ali rahimi and benjamin recht. random features for large-scale kernel

machines. in advances in neural information processing systems, 2007.

[45] benjamin i. p. rubinstein, peter l. bartlett, ling huang, and nina taft.
learning in a large function space: privacy-preserving mechanisms for id166
learning. in computing research repository, 2009.

[46] alessandra sala, xiaohan zhao, christo wilson, haitao zheng, and ben y.
zhao. sharing graphs using di   erentially private graph models. in internet
measurement conference, pages 81   98, 2011.

[47] anand d. sarwate and kamalika chaudhuri. signal processing and ma-
chine learning with di   erential privacy: algorithms and challenges for con-
tinuous data. pages 86   94, 2013.

[48] entong shen and ting yu. mining frequent graph patterns with di   erential

privacy. in kdd, pages 545   553, 2013.

[49] adam smith. e   cient, di   erentially private point estimators. in comput-

ing research repository, 2008.

[50] adam smith. privacy-preserving statistical estimation with optimal con-
vergence rates. in acm symposium on theory of computing, pages 813   
822, 2011.

[51] latanya sweeney. k-anonymity: a model for protecting privacy. in inter-
national journal of uncertainty, fuzziness and knowledge-based systems,
pages 557   570, 2002.

29

[52] jaideep vaidya, basit sha   q, anirban basu, and yuan hong. di   erentially
private naive bayes classi   cation. in web intelligence, pages 571   576, 2013.

[53] staal a. vinterbo. di   erentially private projected histograms: construc-
tion and use for prediction. in european conference on machine learning
(ecml) and conference on principles and practice of knowledge discov-
ery in databases, pages 19   34, 2012.

[54] qian xiao, rui chen, and kian-lee tan. di   erentially private network

data release via structural id136. in kdd, pages 911   920, 2014.

[55] yonghui xiao, li xiong, and chun yuan. di   erentially private data release
through multidimensional partitioning. in secure data management, pages
150   168, 2010.

[56] jun zhang, zhenjie zhang, xiaokui xiao, yin yang, and marianne
winslett. functional mechanism: regression analysis under di   erential
privacy.
in international conference on very large data bases, pages
1364   1375, 2012.

[57] xiaojian zhang, xiaofeng meng, and rui chen. di   erentially private set-
valued data release against incremental updates. in international confer-
ence on database systems for advanced applications, pages 392   406, 2013.

30

