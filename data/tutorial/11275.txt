canonical correlation id136 for mapping abstract scenes to text

nikos papasarantopoulos

school of informatics
university of edinburgh

helen jiang

department of computer science

stanford university

nikos.papasa@ed.ac.uk

helennn@stanford.edu

shay b. cohen
school of informatics
university of edinburgh

scohen@inf.ed.ac.uk

7
1
0
2

 

v
o
n
7
1

 

 
 
]
l
c
.
s
c
[
 
 

2
v
4
8
7
2
0

.

8
0
6
1
:
v
i
x
r
a

abstract

we describe a technique for id170, based on
canonical correlation analysis. our learning algorithm    nds
two projections for the input and the output spaces that aim
at projecting a given input and its correct output into points
close to each other. we demonstrate our technique on a
language-vision problem, namely the problem of giving a tex-
tual description to an    abstract scene.   

1

introduction

canonical correlation analysis (cca) is a method to re-
duce the dimensionality of multiview data, introduced by
hotelling (1935).
it takes two random vectors and com-
putes their corresponding empirical cross-covariance matrix.
it then applies singular value decomposition (svd) on this
matrix to get linear projections of the random vectors that
have maximal correlation.

in this paper, we investigate the idea of using cca for
a full-   edged id170 problem. more specif-
ically, we suggest a method in which we take a structured
prediction problem training set, and then project both the in-
puts and the outputs to low-dimensional space. the projec-
tion ensures that inputs and outputs that correspond to each
other are projected to close points in low-dimensional space.
decoding happens in the low-dimensional space. as such,
our training algorithm builds on previous work by udupa
and khapra (2010) and jagarlamudi and daum  e iii (2012)
who used cca for id68.

our approach of canonical correlation id136 is simple
to implement and does not require complex engineering tai-
lored to the task. it mainly needs two feature functions, one
for the input values and one for the output values and does
not require features combining the two. we also propose a
simple decoding algorithm when the output space is text.

we test our learning algorithm on the domain of language
and vision. we use the abstract scene dataset of zitnick and
parikh (2013), with the goal of mapping images (in the form
of clipart abstract scenes) to their corresponding image de-
scriptions. this problem has a strong relationship to recent
work in language and vision that has used neural networks
copyright c(cid:13) 2018, association for the advancement of arti   cial
intelligence (www.aaai.org). all rights reserved.

or other id161 techniques to solve a similar prob-
lem for real images (section 2.2). our work is most closely
related to the work by ortiz, wolff, and lapata (2015) who
used phrase-based machine translation to translate images to
corresponding descriptions.

2 background and notation

we give in this section some background information about
cca and the problem which we aim to solve with it.
2.1 canonical correlation analysis
canonical correlation analysis (cca; hotelling, 1935) is a
technique for multiview id84, related to
co-training (blum and mitchell 1998). cca assumes that
there are two views for a given set of data, where these two
views are represented by two random vectors x     rd and
y     rd(cid:48)

the procedure that cca follows    nds a projection of
the two views in a shared space of dimension m, such
that the correlation between the two views is maximized
at each coordinate, and there is minimal redundancy be-
tween the coordinates of each view. this means that cca
solves the following sequence of optimization problems for
j     {1, . . . , m} where aj     r1  d and bj     r1  d(cid:48)

:

.

arg max
aj ,bj
such that

corr(ajx(cid:62), bjy (cid:62))
corr(ajx(cid:62), akx(cid:62)) = 0,
corr(bjy (cid:62), bky (cid:62)) = 0,

k < j

k < j

where corr is a function that accepts two vectors and returns
the pearson correlation between the pairwise elements of the
two vectors. the problem of cca can be solved by applying
singular value decomposition (svd) on a cross-covariance
matrix between the two random vectors x and y , normal-
ized by the covariance matrices of x and y .

more speci   cally, cca is solved by applying thin singu-
lar value decomposition (svd) on the empirical version of
the following matrix:

(e[xx(cid:62)])    1

2 e[xy (cid:62)](e[y y (cid:62)])    1

2     u   v (cid:62),

where e[  ] is the expectation operator and    is a diagonal
matrix of size m    m for some small m. since this version

of cca requires inverting matrices of potentially large di-
mension (d   d and d(cid:48)    d(cid:48)), it is often the case that only the
diagonal elements from these matrices are used, as we see in
section 3.

cca and its variants have been used in various con-
texts in nlp. they were used to derive id27s
(dhillon, foster, and ungar 2015), derive multilingual em-
beddings (faruqui and dyer 2014; lu et al. 2015), build
bilingual lexicons (haghighi et al. 2008), encode prior
knowledge into embeddings (osborne, narayan, and co-
hen 2016), semantically analyze text (vinokourov, cristian-
ini, and shawe-taylor 2002) and reduce the dimensions
of text with many views (rastogi, van durme, and arora
2015). cca is also an important sub-routine in the family
of spectral algorithms for estimating structured models such
as latent-variable pid18s and id48s (cohen et al. 2012;
stratos, collins, and hsu 2016) or    nding word clusters
(stratos et al. 2014). variants of it have been developed,
such as deepcca (andrew et al. 2013).
2.2 describing images
image description, the task of assigning textual descriptions
to images, is a problem that has been thoroughly studied in
various setups and variances. usually, proposed methods
treat images as sets of objects identi   ed in them (bags of
regions), however there has been work that uses some kind
of structural image cues or relations. an excellent example
of such cues are visual dependency representations (elliott
and keller 2013), which can be used to outline what can be
described as the visual counterpart of dependency trees.

common is also the idea of solving a related but slightly
different problem, the one of matching sentences to images
using existing descriptions. the core of those approaches is
an information retrieval task, where for every novel image,
a set of similar images is retrieved and generation proceeds
using the descriptions of those images. search queries are
posed against a visual space (ordonez, kulkarni, and berg
2011; mason and charniak 2014) or a multimodal space,
where images and descriptions have been projected (farhadi
et al. 2010; hodosh, young, and hockenmaier 2013). in-
stead of whole sentences, phrases from existing human gen-
erated descriptions have also been used (kuznetsova et al.
2012).

approaches to image description generation have for a
long time relied on a set of prede   ned sentence templates
(kulkarni et al. 2011; elliott and keller 2013; yang et
al. 2011) or used syntactic trees (mitchell et al. 2012),
while more recently, methods that use neural models (kiros,
salakhutdinov, and zemel 2014; vinyals et al. 2015) have
appeared, that avoid the use of any kind of prede   ned pat-
tern. approaches like the latter follow the paradigm of tack-
ling the problem as an end-to-end optimization problem. or-
tiz, wolff, and lapata (2015) describe a two-step process: a
content selection phase, where the objects that need to be de-
scribed are picked, and then the text realization, where the
description is generated by employing a statistical machine
translation (mt) system.

while id161 advances have given an unprece-
dented potential to image description generation, vision per-

formance affects the generation process, as those two prob-
lems are commonly solved together in a pipeline or a joint
fashion. to countermeasure that, zitnick and parikh (2013)
introduced the notion of    abstract scenes   , that is abstract
images generated by stitching together clipart images. their
intuition is that working on abstract scenes can allow for
a more clean and isolated evaluation of caption generators
and also lead to relatively easy construction of datasets of
images with semantically similar content. an example of
such dataset is the abstract scenes dataset.1 this dataset
has been used for description generation (ortiz, wolff, and
lapata 2015), sentence-to-scene generation (zitnick, parikh,
and vanderwende 2013) and object dynamics prediction
(fouhey and zitnick 2014) so far.

3 learning and decoding

we now describe the learning algorithm, based on cca,
and the corresponding decoding algorithm when the output
space is text.

3.1 learning based on canonical correlation

analysis

we assume two structured spaces, an input space x and an
output space y. as usual in the supervised setting, we are
given a set of instances (x1, y1), . . . , (xn, yn)     x   y, and
the goal is to learn a decoder dec : x     y such that dec(x)
is the    correct    output as learned based on the training ex-
amples.
the basic idea in our learning procedure is to learn two
projection functions u : x     rm and v : y     rm for some
low-dimensional m (relatively to d and d(cid:48)). in addition, we
assume the existence of a similarity measure    : rm  rm    
r such that for any x and y, the better y    matches    the x
according to the training data, the larger   (u(x), v(y)) is.
the decoder dec(x) is then de   ned as:

dec(x) = arg max

y   y   (u(x), v(y)).

(1)

our key observation is that one can use canonical cor-
relation analysis to learn the two projections u and v.
this is similar to the observation made by udupa and
khapra (2010) and jagarlamudi and daum  e iii (2012) in
previous work about id68. the learning algorithm
assumes the existence of two feature functions    : x    
rd  1 and    : y     rd(cid:48)  1, where d and d(cid:48) could potentially
be large, and the feature functions could potentially lead to
sparse vectors.

we then apply a modi   ed version of canonical correla-
tion analysis on the two    views:    one view corresponds to
the input feature function and the other view corresponds to
the output feature function. this means we calculate the
following three matrices d1     rd  d, d2     rd(cid:48)  d(cid:48)
and
        rd  d(cid:48)

:

1https://vision.ece.vt.edu/clipart/

[  (yk)]i[  (yk)]i

where

(d2)ii =

k=1

dij =

inputs: set of examples (xi, yi)     x    y for i    
{1, . . . , n}. an integer m. two feature functions   (x) and
  (y).
data structures:
projection matrices u and v .
algorithm:
(cross-covariance estimation)
    calculate         rd  d(cid:48)

   ij =

[  (xk)]i[  (yk)]j

    calculate d1     rd  d such that (d1)ij = 0 for i (cid:54)= j

(d1)ii =

[  (xk)]i[  (xk)]i

    calculate d2     rd(cid:48)  d(cid:48)

such that (d2)ij = 0 for i (cid:54)= j

and

and

n(cid:88)

k=1

n(cid:88)

k=1

n(cid:88)

(singular value decomposition step)
calculate m-rank thin svd on d
. let u and
v be the two resulting projection matrices. return the
    1
1 u )(cid:62)  (x) and v(y) =
two functions u(x) = (d
(d

    1
    1
1    d
2

    1
2 v )(cid:62)  (y).

2

2

2

2

figure 1: the cca learning algorithm.

(cid:33)
(cid:33)

  (xi)(  (xi))(cid:62)

  (yi)(  (yi))(cid:62)

n(cid:88)
n(cid:88)

i=1

i=1

1
n

1
n

d1 = diag

d2 = diag

(cid:32)
(cid:32)
n(cid:88)

i=1

    =

1
n

  (xi)(  (yi))(cid:62)

where diag(a) for a square matrix a is a diagonal matrix
with the diagonal copied from a. we then apply thin singu-
   1/2
lar value decomposition on d
1

   1/2
   d
2

so that

   1/2
d
1

   1/2
   d
2

    u   v (cid:62),

with u     rd  m,        rm  m is a diagonal matrix of sin-
gular values and v     rd(cid:48)  m. the value of m should be
relatively small compared to d and d(cid:48). we then choose u
and v to be:

u(x) = (d

v(y) = (d

2

    1
1 u )(cid:62)  (x),
    1
2 v )(cid:62)  (y).

2

for the similarity metric, we use the cosine similarity:

(cid:80)m
(cid:112)(cid:80)m
i=1 ziz(cid:48)

i

(cid:112)(cid:80)m

i=1 z2
i
(cid:104)z, z(cid:48)(cid:105)
||z||    ||z(cid:48)|| .

i=1(z(cid:48)
i)2

  (z, z(cid:48)) =

=

figure 2 describes a sketch of our cca id136 algo-

rithm.

motivation what is the motivation behind this use of
cca and the chosen projection matrices and similarity met-
ric? osborne, narayan, and cohen (2016) showed that cca
maximizes the following objective:

n(cid:88)

dij     n

d2
ii,

(cid:88)
(cid:118)(cid:117)(cid:117)(cid:116) 1
(cid:32) m(cid:88)

i,j

2

k=1

i=1

(u(xi)     v(yj))2

(cid:33)

.

the objective is maximized with respect to the projections
that cca    nds, u and v. this means that cca    nds pro-
jections such that the euclidean distance between u(x) and
v(y) for matching x and y is minimized, while it is maxi-
mized for x and y that have a mismatch between them.

as such, it is well-motivated to use a similarity metric
  (u(x), v(y)) which is inversely monotone with respect to
the euclidean distance between u(x) and v(y). we next note
that for any two vectors z (denoting u(x)) and z(cid:48) (denoting
v(y)) it holds that (by simple algebraic manipulation):

(cid:0)||z     z(cid:48)||2     ||z||2     ||z(cid:48)||2(cid:1) .

   (cid:104)z, z(cid:48)(cid:105) =

1
2

(2)
this means that if the norms of z and z(cid:48) are constant,
then maximizing the cosine similarity between z and z(cid:48) is
the same as minimizing euclidean distance between z and
z(cid:48). in our case, the norms of u(x) and v(y) are not constant,
but we    nd that our algorithm is much more stable when the
cosine similarity is used instead of the euclidean distance.
we also note that in order to minimize the distance be-
tween z and z(cid:48) to follow cca, according to eq. 2, we need
to maximize the dot product between z and z(cid:48) while mini-
mizing the norm of z and z(cid:48). this is indeed the recipe that
the cosine similarity metric follows.

in section 3.2 we give an additional interpretation to the
use of cosine similarity, as    nding the maximum aposteriori
solution for a re-normalized von mises-fisher distribution.

3.2 when the output space is language
while our approach to mapping from an input space to an
output space through cca is rather abstract and general, in
this paper we focus in cases where the output space y          
is a set of strings over some alphabet   , for example, the

  

jenny is holding an owl.

figure 2: demonstration of cca id136. an object from the input space x (the image on the left x) is mapped to a unit
vector. then, we    nd the closest unit vector which has an embodiment in the output space, y. that embodiment is the text on
the right, y. it also also holds that   (u(x), v(y)) = cos   .

english language. for example, y could be the set of all n-
gram chains possible over some id165 set or the set of pos-
sible composition of atomic phrases, similar to phrase tables
in phrase-based machine translation (koehn et al. 2007).

for the language-vision problem we address in section 4,
we assume the existence of a phrase table p, such that every
y     y can be decomposed into a sequence of consecutive
phrases p1, . . . , pr     p.

the problem of decoding over this space is not trivial.
this is regardless of x     once x is given, it is mapped using
u(x) to a vector in rm, and at this point this is the infor-
mation we use to further decode into y     the structure of x
before this transformation does not change much the com-
plexity of the problem. we propose the following approx-
imate decoding algorithm dec(x) for eq. 1. the algorithm
is a metropolis-hastings (mh) algorithm that assumes the
existence of a blackbox sampler q(y | y(cid:48))     the proposal
distribution. this blackbox sampler randomly chooses two
endpoints k and (cid:96) in y(cid:48) and if possible, replaces all the words
(cid:96)) with a phrase p     p
in between these two words (y(cid:48)
such that in the training data, there is an occurrence of the
new phrase p after the word y(cid:48)
(cid:96)+1.
as such, we are required to create a probabilistic table of
the form q :       p           r that maps a pair of words
y, y(cid:48)        and a phrase p     p to the id203 q(p | y, y(cid:48)).
in our experiments, we use the phrase table used by ortiz,
wolff, and lapata (2015), extracted using moses, and use
relative frequency count to estimate q: we count the number
of times each phrase p appears between the context words y
and y(cid:48) and normalize.

k        y(cid:48)
k   1 and before the word y(cid:48)

since we are interested in maximizing the cosine similar-
ity between v(y) and u(x), after each sampling step, we
check whether the cosine similarity of the new y is higher
(regardless of whether it is being accepted or rejected by the
mh algorithm) than that of any y so far. we return the best
y sampled.

the    true    unnormalized distribution we use in the
accept-rejection step is the exponentiated value of the co-
sine similarity between u(x) and v(y). this means that for
a given x, the mh algorithm implicitly samples from the
following distribution p :

inputs: an input example x, a similarity metric   , two pro-
jection functions u and v, a probabilistic phrase table q, a
constant        0, a constant        (0, 1), a starting temperature
t .
algorithm:
(initialization)
    let y    be an arbitrary point in the output space.
    let y(cid:48) be y   .
    let t be t .
while the temperature t goes below a given value:
    choose uniformly two different integers between 1 and

|y(cid:48)|, the length of y(cid:48), i and j.

    choose randomly a phrase p from q(p | y(cid:48)
    let y = y(cid:48)
    if   (u(x), v(y)) +   |y|       (u(x), v(y   )) +   |y(cid:48)|, then

j+1        y(cid:48)

1        y(cid:48)

i   1py(cid:48)

i   1, y(cid:48)

j+1).

|y(cid:48)|.

set y    to be y.

    let

(cid:19)
(cid:19)

(cid:18) 1
(cid:18) 1

t

exp

  (u(x), v(y)) +   |y|

  0 =

(cid:48)

  (u(x), v(y
j | y(cid:48)
i        y(cid:48)

exp
t
|y|2q(y(cid:48)
j+1)
|y(cid:48)|2q(yi        yj | yi   1, yj+1)

)) +   |y
i   1, y(cid:48)

(cid:48)|

  1 =
   = {1,   0      1}.

    uniformly sample a number from [0, 1], and if it is smaller

than   , set y(cid:48) to be y.

    let t        t.
return y   .

figure 3: the cca decoding algorithm.

(cid:18) (cid:104)u(x), v(y)(cid:105)

||u(x)||    ||v(y)||

(cid:19)

z(x)

(3)

exp

p (y | x) =

where

(cid:18) (cid:104)u(x), v(y(cid:48))(cid:105)

||u(x)||    ||v(y(cid:48))||

(cid:19)

.

exp

(cid:88)

y(cid:48)   y

z(x) =

the id203 distribution p has a strong relationship
to the von mises-fisher distribution, which is de   ned over
vectors of unit vector. the von mises-fisher distribution has
a parametric density function f (z;   ) which is proportional
to the exponentiated dot product between the unit vector z
and some other unit vector    which serves as the param-
eter for the distribution. the main difference between the
von mises-fisher distribution and the distribution de   ned in
eq. 3 is that we do not allow any unit vector to be used as
v(y)
||v(y)||     only those which originate in some output struc-
ture y. as such, the distribution in eq. 3 is a re-normalized
version of the von-mises distribution, after elements from
its support are removed.

in a set of preliminary experiments, we found that while
our algorithm gives adequate descriptions to the images, it is
not unusual for it to give short descriptions that just mention
a single object in the image. this relates to the adequacy-
   uency tension that exists in machine translation problems.
to overcome this issue, we add to the cosine similarity a
term   |y| where    is some positive constant tuned on a de-
velopment set and |y| is the length of the sampled sentence.
this pushes the decoding algorithm to prefer textual descrip-
tions which are longer.

simulated annealing since we are not interested in sam-
pling from the distribution p (y | x), but actually    nd its
mode, we use simulated annealing with our mh sampler.
this means that we exponentiate by a 1
term the unnormal-
t
ized distribution we sample from, and decrease this temper-
ature t as the sampler advances. we start with a temperature
t = 10, 000, and multiply t by    = 0.995 at each step. the
idea is for the sampler to start with an exploratory phase,
where it is jumping from different parts of the search space
to others. as the temperature decreases, the sampler makes
smaller jumps with the hope that it has gotten closer to parts
of the search space where most of the id203 mass is
concentrated.

4 experiments

our experiments are performed on a language-vision
dataset, with the goal of taking a so-called    abstract scene   
(zitnick and parikh 2013) and    nding a suitable textual de-
scription. figure 2 gives a description of our cca algorithm
in the context of this problem.

the abstract scenes dataset consists of 10,020 scenes,
each represented as a set of clipart objects placed in different
positions and sizes in a background image (consisting of a
grassy area and sky). cliparts can appear in different ways,
for example, the boy and the girl (cliparts 18 and 19), can
be depicted sad, angry, sitting or running. the descriptions
were given using id104.

we use the same data split as ortiz, wolff, and lap-
ata (2015). we use 7,014 of the scenes as a training set,

y

waiting

with
pizza
trying
baseball
(cid:104)begin(cid:105)

is

is

mike

p
to
the
on

with

to get away from

playing near the

jenny is playing with a

surprised by the
and the bear are

y(cid:48)
get

bucket.

the
jenny
the

swings.
colorful

owl

standing

prob.
1.000
0.750
0.343
0.050
0.033
0.011
0.008
0.006
0.002

table 1: example of phrases and their probabilities learned
for the function q(p | y, y(cid:48)). the marker (cid:104)begin(cid:105) marks the
beginning of a sentence.

1,002 as a development set and 2,004 as a test set.2 each
scene is labeled with at most eight short captions. we use
all of these captions in the training set, leading to a total of
42,276 training instances. we also use these captions as ref-
erence captions for both the development and the test set.

the feature function   (x) for an image is based on the
   visual features    that come with the abstract scene dataset.
more speci   cally, there are binary features that    re for 11
object categories, 58 speci   c objects, co-occurrence of ob-
ject category pairs, co-occurrence of object instance pairs,
absolute location of object categories and instances, absolute
depth, relative location of objects, relative location with di-
rectionality the object is facing, a feature indicating whether
an object is near a child   s hand or a child   s head and at-
tributes of the children (pose and facial expression). the
total number of features for this    function is 7,149. see
more in the description of the abstract scene dataset.

the feature function   (y) for an image description is de-
   ned as a one-hot representation for all phrases from the
phrase table of ortiz, wolff, and lapata (2015) that    re
in the image (the phrase table is denoted by p in sec-
tion 3.2). this phrase table was obtained through the moses
toolkit (koehn et al. 2007). the total number of phrases in
this phrase table is 30,911. the size of the domain of q
(meaning, the size of the phrase table with context words)
is 120,019. table 1 gives a few example phrases and their
corresponding probabilities.

in our cca learning algorithm, we also need to decide on
the value of m. we varied m between 30 and 300 (in steps of
10) and tuned its value on the development set by maximiz-
ing id7 score against the set of references.3 interestingly
enough, the id7 scores did not change that much (they
usually were within one point of each other for suf   ciently
large m), pointing to a stability of the algorithm with respect
to the number of dimensions used.

ortiz, wolff, and lapata (2015) partially measure the suc-
cess of their system by comparing id7 and meteor
scores of their different systems while using the descriptions
given in the dataset as a reference set. the scores for their

2our

dataset

found

be
canonical-correlation-id136.html.

in

splits

and

can
http://cohort.inf.ed.ac.uk/

information

other

3we use the multeval package from https://github.

com/jhclark/multeval.

id7 meteor

.
l
a

t
e

z
i
t
r

o

system
lbl
mlbl
image
keyword
template
smt
cca

7.3
12.3
12.8
14.7
40.3
43.7
26.1

17.7
20.4
21.7
26.6
30.4
35.6
25.6

figure 4: scatter plot of smt (statistical machine transla-
tion) and cca id7 scores versus human ratings.

captions only.

of kiros, salakhutdinov, and zemel (2014).

different systems are given in table 2. they compare their
system (smt, based on phrase-based machine translation)
against several baselines:4
    lbl: a log-bilinear language model trained on the image
    mlbl: mutlimodal log-bilinear model, implementation
    image: a system that for every new image, queries the set
of training images for the most similar one, and returns a
random description of that training example.
    keyword: system that annotates every image with key-
words that most probably describe it and then do a search
query against all training data descriptions, returning the
description that is closest (in terms of tf-idf similarity)
to the keywords.
    template: system that uses templates inferred from de-
pendency parses of the training data descriptions. a set
of templates is discovered and a classi   er that associates
images with templates is trained.
    smt: ortiz et al. system    rst selects pairs of clipart ob-
jects that are important enough to be described by solving
an integer id135 problem, creates a    visual
encoding    using visual dependency grammar and    nally
uses a phrase-based smt engine to translate the latter to
proper sentences.
our system does not score as high as their machine trans-

lation system.

it is important to note that the descriptions given in the
dataset, as well as those generated by the different systems
are not    complete.    each one of them describes a speci   c

4we note that we also experimented with a neural network
model (id195 model), but it performed badly, giving a id7
score of 10.20 and a meteor score of 15.20 with inappropriate
captions. it seems like id195 models is un   t for this dataset,
perhaps because of its size. see also rastogi, cotterell, and eis-
ner (2016) for similar results.

table 2: scene description evaluation results on the test set,
comparing the systems from ortiz et al.
to our cca in-
ference algorithm (the    rst six results are reported from the
ortiz et al. paper). the cca result uses m = 120 and
   = 0.05, tuned on the development set. see text for details
about each of the    rst six baselines.

figure 5: an image with the following descriptions in the
dataset: (1) mike is kicking the soccer ball;
(2) mike is sitting on the cat;
(3) jenny
(4) jenny is
is standing next to the dog;
kicking the soccer ball;
(5) the sun is
behind jenny; (6) the soccer ball is under
the sun.

bit of information that is implied by the scene. figure 5
demonstrates this. as such, the calculation of smt evalua-
tion scores with respect to a reference set is not necessarily
the best mechanism to identify the correctness of a textual
description. to demonstrate this point, we measure id7
scores of one of the reference sentences while comparing it
to the other references in the set. we did that for each of the
eight batches of references available in the training set.

the average reference id7 score is 24.1 and the av-
erage meteor score is 20.0, a signi   cant drop compared
to the machine translation system of ortiz, wolff, and la-
pata (2015). we concluded from this result that the smt
system is not    creatively    mapping the images to their cor-
responding descriptions. it relies heavily on the training set
captions, and learns how to map images to sentences in a
manner which does not generalize very well outside of the
training set.

another indication that our system creates a more diverse
set of captions is that the number of unique captions it gen-
erates for the test set is signi   cantly larger than that of the
smt system by ortiz et al. the smt system generates 359
unique captions (out of 2,004 instances in the test set), while
cca generates 496 captions, an increase of 38.1%.

to test this hypothesis about caption diversity, we con-
ducted the following human experiment. we asked 12 sub-
jects to rate the captions of 300 abstract scenes with a score

smt jenny is waving at mike
cca mike and jenny are camping

jenny is wearing a baseball
mike is holding a bat

jenny is holding a frisbee
jenny is throwing the frisbee

smt jenny is kicking the soccer ball
cca mike is kicking a blass

jenny is holding a hot dog
jenny wants the bear

jenny is holding a hamburger
the rocket is behind mike

figure 6: examples of outputs from the machine translation system and from cca id136. the top three images give
examples where the cca id136 outputs were rated highly by human evaluations (4 or 5), and the smt ones were rated
poorly (1 or 2). the bottom three pictures give the reverse case.

slice

smt < 3
smt     3

cca < 3 cca     3
m:1.77
m:1.92
c:3.71
c:1.64
m:3.46
m:3.42
c:1.47
c:3.54

table 3: average ranking by human judges for cases in
which the caption has an average rank of 3 or higher (for
both cca and smt) and when its average rank is lower
than 3.    m    stands for smt average rank and    c    stands
for cca average rank.

between 1 to 5.5 each rater was presented with three cap-
tions: a reference caption (selected randomly from the gold-
standard captions), an smt caption and a caption from our
system (presented in a random order) and was asked to rate
the captions on adequacy level (on a scale of 1 to 5). most
images were rated exactly twice, with a few images getting
three raters. a score of 1 or 2 means that the caption likely
does not adequately describe the scene. a score of 3 usually
means that the caption describes some salient component in
the scene, but perhaps not the most important one. scores
of 4 and 5 usually denote good captions that adequately de-
scribe the corresponding scenes. this experiment is similar
to the one done by ortiz et al.

the ranking results are given in table 3. the results show
that our system tends to score higher for images which are
highly ranked (by both the smt system and cca), but tends
to score lower for images which are lower ranked.

in addition, we checked the mt evaluation scores for
highly ranked captions both for smt and cca (ranking
larger than 4). for smt, the id7 scores are 49.70 (me-

5the ratings can be found here: http://cohort.inf.ed.

ac.uk/canonical-correlation-id136.html.

teor 40.10) and for cca it is 41.80 (meteor 33.10).
this is not the result of images in smt being ranked higher,
as the average ranking among these images is 4.18 for the
smt system and 4.25 for cca. the lower cca score again
indicates that our system gives captions which are not nec-
essarily aligned with the references, but still correct. it also
highlights the    aw with using mt id74 for this
dataset. figure 4 also demonstrates that the correlation be-
tween id7 scores and human ranking is not high. more
speci   cally, in that plot, the correlation between the x-axis
(ranking) and y-axis (id7 scores) for cca is 0.3 and for
the smt system 0.31.

figure 6 describes six examples in which the human raters
rated the smt system highly and cca poorly and vice-
versa.

5 conclusion

we described a technique to predict structures from complex
input spaces to complex output spaces based on canonical
correlation analysis. our approach projects the input space
into a low-dimensional representation, and then converts it
back into an instance in the output space. we demonstrated
the use of our method on the id170 problem
of attaching textual captions to abstract scenes. human eval-
uation of these captions demonstrate that our approach is
promising for generating text from images.

acknowledgments

the authors would like to thank mirella lapata, luis mateos
and clemens wolff for their help with replicating the results
from their paper. thanks also to marco damonte for use-
ful comments. this research was supported by the h2020
project summa, under grant agreement 688139.

references

image description us-
in proceedings of

andrew, g.; arora, r.; bilmes, j. a.; and livescu, k. 2013.
in icml (3), 1247   
deep canonical correlation analysis.
1255.
blum, a., and mitchell, t. 1998. combining labeled and
unlabeled data with co-training. in proceedings of colt.
cohen, s. b.; stratos, k.; collins, m.; foster, d. f.; and
ungar, l. 2012. spectral learning of latent-variable pid18s.
in proceedings of acl.
dhillon, p. s.; foster, d. p.; and ungar, l. h. 2015. eigen-
words: spectral id27s. the journal of machine
learning research 16(1):3035   3078.
elliott, d., and keller, f. 2013.
ing visual dependency representations.
emnlp.
farhadi, a.; hejrati, m.; sadeghi, m. a.; young, p.;
rashtchian, c.; hockenmaier, j.; and forsyth, d. 2010. ev-
ery picture tells a story: generating sentences from images.
in eccv, 15   29. springer.
faruqui, m., and dyer, c. 2014. improving vector space
word representations using multilingual correlation. asso-
ciation for computational linguistics.
fouhey, d. f., and zitnick, c. l. 2014. predicting object
dynamics in scenes. in proceedings of the ieee conference
on id161 and pattern recognition, 2019   2026.
haghighi, a.; liang, p.; berg-kirkpatrick, t.; and klein,
d. 2008. learning bilingual lexicons from monolingual
corpora. in acl, volume 2008, 771   779.
hodosh, m.; young, p.; and hockenmaier, j. 2013. fram-
ing image description as a ranking task: data, models and
id74. jair 47:853   899.
hotelling, h. 1935. canonical correlation analysis (cca).
journal of educational psychology.
jagarlamudi, j., and daum  e iii, h. 2012. regularized in-
terlingual projections: evaluation on multilingual transliter-
ation. in proceedings of emnlp.
kiros, r.; salakhutdinov, r.; and zemel, r. s. 2014. multi-
modal neural language models. in proceedings of icml.
koehn, p.; hoang, h.; birch, a.; callison-burch, c.; fed-
erico, m.; bertoldi, n.; cowan, b.; shen, w.; moran, c.;
zens, r.; et al. 2007. moses: open source toolkit for statis-
tical machine translation. in proceedings of the 45th annual
meeting of the acl on interactive poster and demonstration
sessions, 177   180. association for computational linguis-
tics.
kulkarni, g.; premraj, v.; dhar, s.; li, s.; choi, y.; berg,
a. c.; and berg, t. l. 2011. baby talk: understanding and
generating image descriptions. in proceedings of the 24th
cvpr. citeseer.
kuznetsova, p.; ordonez, v.; berg, a. c.; berg, t. l.; and
choi, y. 2012. collective generation of natural image de-
scriptions. in proceedings of the 50th annual meeting of the
association for computational linguistics: long papers-
volume 1, 359   368. association for computational linguis-
tics.

lu, a.; wang, w.; bansal, m.; gimpel, k.; and livescu,
k. 2015. deep multilingual correlation for improved word
embeddings. in proceedings of naacl.
mason, r., and charniak, e. 2014. nonparametric method
for data-driven image captioning. in acl (2), 592   598.
mitchell, m.; han, x.; dodge, j.; mensch, a.; goyal,
a.; berg, a.; yamaguchi, k.; berg, t.; stratos, k.; and
daum  e iii, h. 2012. midge: generating image descrip-
in proceedings of
tions from id161 detections.
the 13th conference of the european chapter of the associ-
ation for computational linguistics, 747   756. association
for computational linguistics.
ordonez, v.; kulkarni, g.; and berg, t. l. 2011. im2text:
describing images using 1 million captioned photographs.
in advances in neural information processing systems,
1143   1151.
ortiz, l. g. m.; wolff, c.; and lapata, m. 2015. learning
to interpret and describe abstract scenes. in proceedings of
naacl.
osborne, d.; narayan, s.; and cohen, s. b. 2016. encoding
prior knowledge with eigenid27s. in transac-
tions of the association for computational linguistics.
rastogi, p.; cotterell, r.; and eisner, j. 2016. weighting
in proceed-
   nite-state transductions with neural context.
ings of naacl.
rastogi, p.; van durme, b.; and arora, r. 2015. multi-
view lsa: representation learning via generalized cca.
in
proceedings of naacl.
stratos, k.; kim, d.-k.; collins, m.; and hsu, d. 2014. a
spectral algorithm for learning class-based id165 models
of natural language. proceedings of uai.
stratos, k.; collins, m.; and hsu, d. 2016. unsupervised
part-of-speech tagging with anchor id48.
transactions of the association for computational linguis-
tics.
udupa, r., and khapra, m. m. 2010. id68 equiv-
in european
alence using canonical correlation analysis.
conference on information retrieval, 75   86. springer.
vinokourov, a.; cristianini, n.; and shawe-taylor, j. s.
2002. inferring a semantic representation of text via cross-
language correlation analysis. in proceedings of nips.
vinyals, o.; toshev, a.; bengio, s.; and erhan, d. 2015.
show and tell: a neural image caption generator. in pro-
ceedings of cvpr.
yang, y.; teo, c. l.; daum  e iii, h.; and aloimonos, y.
2011. corpus-guided sentence generation of natural images.
in proceedings of the conference on empirical methods in
natural language processing, 444   454. association for
computational linguistics.
zitnick, c. l., and parikh, d. 2013. bringing semantics into
focus using visual abstraction. in proceedings of the ieee
conference on id161 and pattern recognition,
3009   3016.
zitnick, c. l.; parikh, d.; and vanderwende, l. 2013.
learning the visual interpretation of sentences. in proceed-
ings of iccv.

