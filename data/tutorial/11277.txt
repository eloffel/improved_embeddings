simpler context-dependent logical forms via model projections

reginald long

stanford university

panupong pasupat
stanford university

percy liang

stanford university

reggylong@cs.stanford.edu

ppasupat@cs.stanford.edu

pliang@cs.stanford.edu

6
1
0
2

 

n
u
j
 

6
1

 
 
]
l
c
.
s
c
[
 
 

1
v
8
7
3
5
0

.

6
0
6
1
:
v
i
x
r
a

abstract

we consider the task of learning a context-
dependent mapping from utterances to de-
notations. with only denotations at train-
ing time, we must search over a combina-
torially large space of logical forms, which
is even larger with context-dependent ut-
terances. to cope with this challenge, we
perform successive projections of the full
model onto simpler models that operate
over equivalence classes of logical forms.
though less expressive, we    nd that these
simpler models are much faster and can
be surprisingly effective. moreover, they
can be used to bootstrap the full model.
finally, we collected three new context-
dependent id29 datasets, and
develop a new left-to-right parser.

introduction

1
suppose we are only told that a piece of text (a
command) in some context (state of the world) has
some denotation (the effect of the command)   see
figure 1 for an example. how can we build a sys-
tem to learn from examples like these with no ini-
tial knowledge about what any of the words mean?
we start with the classic paradigm of training
semantic parsers that map utterances to logical
forms, which are executed to produce the deno-
tation (zelle and mooney, 1996; zettlemoyer and
collins, 2005; wong and mooney, 2007; zettle-
moyer and collins, 2009; kwiatkowski et al.,
2010). more recent work learns directly from de-
notations (clarke et al., 2010; liang, 2013; be-
rant et al., 2013; artzi and zettlemoyer, 2013),
but in this setting, a constant struggle is to con-
tain the exponential explosion of possible logical
forms. with no initial lexicon and longer context-
dependent texts, our situation is exacerbated.

figure 1: our task is to learn to map a piece of
text in some context to a denotation. an exam-
ple from the alchemy dataset is shown. in this
paper, we ask: what intermediate logical form is
suitable for modeling this mapping?

in this paper, we propose projecting a full se-
mantic parsing model onto simpler models over
equivalence classes of logical form derivations. as
illustrated in figure 2, we consider the following
sequence of models:

    model a: our full model that derives logi-
cal forms (e.g., in figure 1, the last utter-
ance maps to mix(args[1][1])) compo-
sitionally from the text so that spans of the ut-
terance (e.g.,    it   ) align to parts of the logical
form (e.g., args[1][1], which retrieves an
argument from a previous logical form). this
is based on standard id29 (e.g.,
zettlemoyer and collins (2005)).

    model b: collapse all derivations with the
same logical form; we map utterances to full
logical forms, but without an alignment be-
tween the utterance and logical forms. this
      oating    approach was used in pasupat and
liang (2015) and wang et al. (2015).

    model c: further collapse all logical forms
whose top-level arguments have the same de-
notation. in other words, we map utterances

context:text:pourthelastgreenbeakerintobeaker2.thenintothe   rstbeaker.mixit.denotation:figure 2: derivations generated for the last ut-
terance in figure 1. all derivations above ex-
ecute to mix(beaker2). model a generates
anchored logical forms (derivations) where words
are aligned to predicates, which leads to multiple
derivations with the same logical form. model b
discards these alignments, and model c collapses
the arguments of the logical forms to denotations.

to    at logical forms (e.g., mix(beaker2)),
where the arguments of the top-level predi-
cate are objects in the world. this model is
in the spirit of yao et al. (2014) and bordes
et al. (2014), who directly predicted concrete
paths in a id13 for question an-
swering.

model a excels at credit assignment:
the latent
derivation explains how parts of the logical form
are triggered by parts of the utterance. the price
is an unmanageably large search space, given that
we do not have a seed lexicon. at the other end,
model c only considers a small set of logical
forms, but the mapping from text to the correct
logical form is more complex and harder to model.
we collected three new context-dependent se-
mantic parsing datasets using amazon mechanical
turk: alchemy (figure 1), scene (figure 3),
and tangrams (figure 4). along the way, we
develop a new parser which processes utterances
left-to-right but can construct logical forms with-
out an explicit alignment.

our empirical    ndings are as follows: first,
model c is surprisingly effective, mostly surpass-
ing the other two given bounded computational re-
sources (a    xed beam size). second, on a synthetic
dataset, with in   nite beam, model a outperforms
the other two models. third, we can bootstrap up
to model a from the projected models with    nite
beam.

figure 3: scene dataset: each person has a shirt
of some color and a hat of some color. they enter,
leave, move around on a stage, and trade hats.

figure 4: tangrams dataset: one can add    g-
ures, remove    gures, and swap the position of    g-
ures. all the    gures slide to the left.

2 task

in this section, we formalize the task and describe
the new datasets we created for the task.

2.1 setup

first, we will de   ne the context-dependent seman-
tic parsing task. de   ne w0 as the initial world
state, which consists of a set of entities (beakers
in alchemy) and properties (location, color(s),
and amount    lled). the text x is a sequence
of utterances x1, . . . , xl. for each utterance xi
(e.g.,    mix   ), we have a latent logical form zi
(e.g., mix(args[1][2])). de   ne the context
ci = (w0, z1:i   1) to include the initial world state
w0 and the history of past logical forms z1:i   1.
each logical form zi is executed on the context
ci to produce the next state: wi = exec(ci, zi)
for each i = 1, . . . , l. overloading notation, we
write wl = exec(w0, z), where z = (z1, . . . , zl).
the learning problem is: given a set of training
examples {(w0, x, wl)}, learn a mapping from the
text x to logical forms z = (z1, . . . , zl) that pro-
duces the correct    nal state (wl = exec(w0, z)).

mix         itmodel amixargs[1][1]mix(args[1][1])model bmix(args[1][1])mix itmodel cmix         itmixargs[1][1]mix(args[1][1])mix   itmixpos(2)mix(pos(2))mix     itmixpos(2)mix(pos(2))mix(pos(2))mix itmix(beaker2)mix itcontext:text:amaninaredshirtandorangehatleavestotheright,leavingbehindamaninablueshirtinthemiddle.hetakesasteptotheleft.denotation:context:text:deletethesecond   gure.bringitbackasthe   rst   gure.denotation:dataset
scene
alchemy
tangrams

# examples
4402
4560
4989

# train
3363
3661
4189

# test words/example
1039
56.2
39.9
899
800
27.2

utterances
   then one more   ,    he moves back   
   mix   ,    throw the rest out   
   undo   ,    replace it   ,    take it away   

table 1: we collected three datasets. the number of examples, train/test split, number of tokens per
example, along with interesting phenomena are shown for each dataset.

2.2 datasets

we created three new context-dependent datasets,
alchemy, scene, and tangrams (see table 1
for a summary), which aim to capture a diverse set
of context-dependent linguistic phenomena such
as ellipsis (e.g.,    mix    in alchemy), anaphora
on entities (e.g.,    he    in scene), and anaphora
on actions (e.g.,    repeat step 3   ,    bring it back   
in tangrams).

for each dataset, we have a set of properties
and actions. in alchemy, properties are color,
and amount; actions are pour, drain, and
mix.
in scene, properties are hat-color
and shirt-color; actions are enter, leave,
move, and trade-hats. in tangrams, there
is one property (shape), and actions are add,
remove, and swap. in addition, we include the
position property (pos) in each dataset. each ex-
ample has l = 5 utterances, each denoting some
transformation of the world state.

our datasets are unique in that

they are
grounded to a world state and have rich linguis-
tic context-dependence. in the context-dependent
atis dataset (dahl et al., 1994) used by zettle-
moyer and collins (2009), logical forms of utter-
ances depend on previous logical forms, though
there is no world state and the linguistic phenom-
ena is limited to nominal references. in the map
navigation dataset (chen and mooney, 2011), used
by artzi and zettlemoyer (2013), utterances only
reference the current world state. vlachos and
clark (2014) released a corpus of annotated di-
alogues, which has interesting linguistic context-
dependence, but there is no world state.

data collection. our strategy was to automati-
cally generate sequences of world states and ask
amazon mechanical turk (amt) workers to de-
scribe the successive transformations. speci   -
cally, we started with a random world state w0.
for each i = 1, . . . , l, we sample a valid
action and argument (e.g., pour(beaker1,
beaker2)). to encourage context-dependent
descriptions, we upweight
recently used ac-

tions and arguments (e.g.,
the next action is
more like to be drain(beaker2) rather than
drain(beaker5)). next, we presented an
amt worker with states w0, . . . , wl and asked the
worker to write a description in between each pair
of successive states.

in initial experiments, we found it rather non-
to obtain interesting linguistic context-
trivial
often a
dependence in these micro-domains:
context-independent utterance such as    beaker 2   
is just clearer and not much longer than a possi-
bly ambiguous    it   . we modi   ed the domains to
encourage more context. for example, in scene,
we removed any visual indication of absolute posi-
tion and allowed people to only move next to other
people. this way, workers would say    to the left
of the man in the red hat    rather than    to position
2   .

3 model

we now describe model a, our full context-
dependent id29 model. first, let z
denote the set of candidate logical forms (e.g.,
pour(color(green),color(red))).
each logical form consists of a top-level action
with arguments, which are either primitive values
(green, 3, etc.), or composed via selection and
superlative operations. see table 2 for a full
description. one notable feature of the logical
forms is the context dependency:
for example,
given some context (w0, z1:4),
the predicate
actions[2] refers to the action of z2 and
args[2][1] refers to    rst argument of z2.1

we use the term anchored logical forms (a.k.a.
derivations) to refer to logical forms augmented
with alignments between sub-logical forms of zi
and spans of the utterance xi.
in the example
above, color(green) might align with    green
beaker    from figure 1; see figure 2 for another
example.

1these special predicates play the role of references in
zettlemoyer and collins (2009). they perform context-
independent parsing and resolve references, whereas we re-
solve them jointly while parsing.

property[p] value[v]
set[s] property[p]
set[s] int[i]
action[a] value[v1] value[v2]     root[a(v1, v2)]

    set[p(v)]
    value[argmin/argmax(s, p)]
    value[s[i]]

all entities whose property p is v
element in s with smallest/largest p
i-th element of s
top-level action applied to arguments v1, v2

table 2: grammar that de   nes the space of candidate logical forms. values include numbers, colors,
as well as special tokens args[i][j] (for all i     {1, . . . , l} and j     {1, 2}) that refer to the j-th ar-
gument used in the i-th logical form. actions include the    xed domain-speci   c set plus special tokens
actions[i] (for all i     {1, . . . , l}), which refers to the i-th action in the context.

derivation condition
zi contains predicate r
property p of zi.bj is y
action zi.a is a and property p of zi.yj is y
properties p of zi.v1 is y and p(cid:48) of zi.v2 is y(cid:48)
arg zi.vj is one of zi   1   s args
action zi.a = zi   1.a
properties p of zi.yj is y and p(cid:48) of zi   1.yk is y(cid:48)
t1 < s2

example
(zi contains predicate pour,    pour   )
(color of arg 1 is green,    green   )
(action is pour and pos of arg 2 is 2,    pour, 2   )
(color of arg 1 is green and pos of arg 2 is 2,       rst green, 2   )
(arg reused,    it   )
(action reused,    pour   )
(pos of arg 1 is 2 and pos of prev. arg 2 is 2,    then   )
spans don   t overlap

(f1)
(f2)
(f3)
(f4)
(f5)
(f6)
(f7)
(f8)

table 3: features   (xi, ci, zi) for model a: the left hand side describes conditions under which the
system    res indicator features, and right hand side shows sample features for each condition. for each
derivation condition (f1)   (f7), we conjoin the condition with the span of the utterance that the referenced
actions and arguments align to. for condition (f8), we just    re the indicator by itself.

log-linear model. we place a conditional dis-
forms zi    
tribution over anchored logical
z given an utterance xi and context ci =
(w0, z1:i   1), which consists of the initial world
state w0 and the history of past logical forms
z1:i   1. we use a standard log-linear model:
p  (zi | xi, ci)     exp(  (xi, ci, zi)      ),

(1)

the exact features given in table 3, references the
   rst two utterances of figure 1 and the associated
logical forms below:
x1 =    pour the last green beaker into beaker 2.   
z1 = pour(argmin(color(green),pos),pos(2))
x2 =    then into the    rst beaker.   
z2 = actions[1](args[1][2],pos(3)).

l(cid:89)

where    is the feature mapping and    is the param-
eter vector (to be learned). chaining these distri-
butions together, we get a distribution over a se-
quence of logical forms z = (z1, . . . , zl) given
the whole text x:

p  (z | x, w0) =

p  (zi | xi, (w0, z1:i   1)). (2)

i=1

features. our feature mapping    consists of two
types of indicators:

1. for each derivation, we    re features based on

the structure of the logical form/spans.

2. for each span s (e.g.,    green beaker   )
aligned to a sub-logical
form z (e.g.,
color(green)), we    re features on uni-
grams, bigrams, and trigrams inside s con-
joined with various conditions of z.

we describe the notation we use for table 3, re-
stricting our discussion to actions that have two
or fewer arguments. our featurization scheme,
however, generalizes to an arbitrary number of
arguments. given a logical form zi, let zi.a be
its action and (zi.b1, zi.b2) be its arguments (e.g.,
color(green)). the    rst and second argu-
ments are anchored over spans [s1, t1] and [s2, t2],
respectively. each argument zi.bj has a corre-
sponding value zi.vj (e.g., beaker1), obtained
by executing zi.bj on the context ci. finally, let
j, k     {1, 2} be indices of the arguments. for ex-
ample, we would label the constituent parts of z1
(de   ned above) as follows:

    z1.a = pour
    z1.b1 = argmin(color(green),pos)
    z1.v1 = beaker3
    z1.b2 = pos(2)
    z1.v2 = beaker2

figure 5: suppose we have already constructed delete(pos(2)) for    delete the second    gure.   
continuing, we shift the utterance    repeat   . then, we build action[1] aligned to the word    repeat.   
followed by args[1][1], which is unaligned. finally, we combine the two logical forms.

4 left-to-right parsing

we describe a new parser suitable for learning
from denotations in the context-dependent setting.
like a shift-reduce parser, we proceed left to right,
but each shift operation advances an entire utter-
ance rather than one word. we then sit on the
utterance for a while, performing a sequence of
build operations, which either combine two logi-
cal forms on the stack (like the reduce operation)
or generate fresh logical forms, similar to what is
done in the    oating parser of pasupat and liang
(2015).

our parser has two desirable properties: first,
proceeding left-to-right allows us to build and
score logical forms zi that depend on the world
state wi   1, which is a function of the previous log-
ical forms. note that wi   1 is a random variable in
our setting, whereas it is    xed in zettlemoyer and
collins (2009). second, the build operation allows
us the    exibility to handle ellipsis (e.g.,    mix.   )
and anaphora on full logical forms (e.g.,    do it
again.   ), where there   s not a clear alignment be-
tween the words and the predicates generated.

the parser transitions through a sequence of hy-
potheses. each hypothesis is h = (i, b,   ), where i
is the index of the current utterance, where b is the
number of predicates constructed on utterance xi,
and    is a stack (list) of logical forms. the stack
includes both the previous logical forms z1:i   1 and
fragments of logical forms built on the current ut-
terance. when processing a particular hypothesis,
the parser can choose to perform either the shift or
build operation:

shift: the parser moves to the next utterance
by incrementing the utterance index i and resetting
b, which transitions a hypothesis from (i, b,   ) to
(i + 1, 0,   ).

build: the parser creates a new logical form
by combining zero or more logical forms on the
stack. there are four types of build operations:

1. create a predicate out of thin air (e.g.,
args[1][1] in figure 5). this is useful
when the utterance does not explicitly refer-
ence the arguments or action. for example,
in figure 5, we are able to generate the log-
ical form args[1][1] in the presence of
ellipsis.

2. create a predicate anchored to some span of
the utterance (e.g., actions[1] anchored
to    repeat   ). this allows us to do credit as-
signment and capture which part of the utter-
ance explains which part of the logical form.
3. pop z from the stack    and push z(cid:48) onto   ,
where z(cid:48) is created by applying a rule in ta-
ble 2 to z.

4. pop z, z(cid:48) from the stack    and push z(cid:48)(cid:48) onto   ,
where z(cid:48)(cid:48) is created by applying a rule in ta-
ble 2 to z, z(cid:48) (e.g., actions[1](args[1]
[1]) by the top-level root rule).

the build step stops once a maximum number of
predicates b have been constructed or when the
top-level rule is applied.

we have so far described the search space over
logical forms. in practice, we keep a beam of the
k hypotheses with the highest score under the cur-
rent log-linear model.

5 model projections
model a is ambitious, as it tries to learn from
scratch how each word aligns to part of the log-
ical form. for example, when model a parses
   mix it   , one derivation will correctly align    mix   

delete the second    gure. repeat.delete2pos(2)actions[1](args[1])delete(pos(2))delete(pos(2))actions[1](args[1][1])actions[1]args[1][1]delete the second    gure. repeat.delete2pos(2)args[1][1]actions[1]delete(pos(2))delete(pos(2))actions[1]args[1][1]delete the second    gure. repeat.delete2pos(2)actions[1]delete(pos(2))delete(pos(2))actions[1]delete the second    gure. repeat.delete2pos(2)delete(pos(2))delete(pos(2))(1)(2)(3)(4)to mix, but others will align    mix    to args[1]
[1],    mix    to pos(2), and so on (figure 2).

as we do not assume a seed lexicon that could
map    mix    to mix, the set of anchored logical
forms is exponentially large. for example, parsing
just the    rst sentence of figure 1 would generate
1,216,140 intermediate anchored logical forms.

how can we reduce the search space? the key
is that the space of logical forms is much smaller
than the space of anchored logical forms. even
though both grow exponentially, dealing directly
with logical forms allows us to generate pour
without the combinatorial choice over alignments.
we thus de   ne model b over the space of these
logical forms. figure 2 shows that the two an-
chored logical forms, which are treated differently
in model a are collapsed in model b. this dra-
matically reduces the search space; parsing the
   rst sentence of figure 1 generates 7,047 interme-
diate logical forms.

logical form if we evaluate all

we can go further and notice that many
compositional logical forms reduce to the same
   at
the argu-
ments. for example, in figure 2, mix(args[1]
[1]) and mix(pos(2)) are equivalent
to
mix(beaker2). we de   ne model c to be the
space of these    at logical forms which consist of
a top-level action plus primitive arguments. us-
ing model c, parsing the    rst sentence of figure 1
generates only 349 intermediate logical forms.

note that in the context-dependent setting, the
number of    at logical forms (model c) still in-
creases exponentially with the number of utter-
ances, but it is an overwhelming improvement
over model a. furthermore, unlike other forms
of relaxation, we are still generating logical forms
that can express any denotation as before. the
gains from model b to model c hinge on the fact
that in our world, the number of denotations is
much smaller than the number of logical forms.

projecting the features. while we have de   ned
the space over logical forms for models b and c,
we still need to de   ne a distribution over these
spaces to to complete the picture. to do this, we
propose projecting the features of the log-linear
model (1). de   ne   a   b to be a map from a
anchored logical form za (e.g., mix(pos(2)
) aligned to    mix   ) to an unanchored one zb
(e.g., mix(pos(2))), and de   ne   b   c to be
a map from zb to the    at logical form zc (e.g.,
mix(beaker2)).

(4)

(3)

we construct a log-linear model for model b
by constructing features   (zb) (omitting the de-
pendence on xi, ci for convenience) based on the
model a features   (za). speci   cally,   (zb) is
the component-wise maximum of   (za) over all
za that project down to zb;   (zc) is de   ned simi-
larly:
  (zb) def= max{  (za) :   a   b(za) = zb},
  (zc) def= max{  (zb) :   b   c(zb) = zc}.
concretely, model b   s features include indica-
tor features over lf conditions in table 3 con-
joined with every id165 of the entire utterance,
as there is no alignment. this is similar to the
model of pasupat and liang (2015). note that
most of the derivation conditions (f2)   (f7) al-
ready depend on properties of the denotations of
the arguments, so in model c, we can directly rea-
son over the space of    at logical forms zc (e.g.,
mix(beaker2)) rather than explicitly comput-
ing the max over more complex logical forms zb
(e.g., mix(color(red))).
expressivity.
in going from model a to model
c, we gain in computational ef   ciency, but we lose
in modeling expressivity. for example, for    sec-
ond green beaker    in figure 1, instead of predict-
ing color(green)[2], we would have to pre-
dict beaker3, which is not easily explained by
the words    second green beaker    using the simple
features in table 3.

at the same time, we found that simple fea-
tures can actually simulate some logical forms.
for example, color(green) can be explained
by the feature that looks at the color property of
beaker3. nailing color(green)[2], how-
ever, is not easy. surprisingly, model c can use
a conjunction of features to express superlatives
(e.g., argmax(color(red),pos)) by using
one feature that places more mass on selecting ob-
jects that are red and another feature that places
more mass on objects that have a greater position
value.

6 experiments
our experiments aim to explore the computation-
expressivity tradeoff in going from model a to
model b to model c. we would expect that un-
der the computational constraint of a    nite beam
size, model a will be hurt the most, but with an
in   nite beam, model a should perform better.

dataset

alchemy

scene

tangrams

model
b
c
b
c
b
c

3-acc
0.189
0.568
0.068
0.232
0.649
0.567

3-ora
0.258
0.925
0.118
0.431
0.910
0.899

5-acc
0.037
0.523
0.017
0.147
0.276
0.272

5-ora
0.055
0.809
0.031
0.253
0.513
0.698

table 4: test set accuracy and oracle accuracy for
examples containing l = 3 and l = 5 utterances.
model c surpasses model b in both accuracy and
oracle on alchemy and scene, whereas model
b does better in tangrams.

we evaluate all models on accuracy, the frac-
tion of examples that a model predicts correctly.
a predicted logical form z is deemed to be correct
for an example (w0, x, wl) if the predicted logi-
cal form z executes to the correct    nal world state
wl. we also measure the oracle accuracy, which
is the fraction of examples where at least one z
on the beam executes to wl. all experiments train
for 6 iterations using adagrad (duchi et al., 2010)
and l1 id173 with a coef   cient of 0.001.

6.1 real data experiments
setup. we use a beam size of 500 within each
utterance, and prune to the top 5 between utter-
ances. for the    rst two iterations, models b and
c train on only the    rst utterance of each example
(l = 1). in the remaining iterations, the models
train on two utterance examples. we then evaluate
on examples with l = 1, . . . , 5, which tests our
models ability to extrapolate to longer texts.

accuracy with    nite beam. we compare mod-
els b and c on the three real datasets for both
l = 3 and l = 5 utterances (model a was too ex-
pensive to use). table 4 shows that on 5 utterance
examples, the    atter model c achieves an average
accuracy of 20% higher than the more composi-
tional model b. similarly, the average oracle accu-
racy is 39% higher. this suggests that (i) the cor-
rect logical form often falls off the beam for model
b due to a larger search space, and (ii) the expres-
sivity of model c is suf   cient in many cases.

on the other hand, model b outperforms model
c on the tangrams dataset. this happens for
two reasons. the tangrams dataset has the
smallest search space, since all of the utterances
refer to objects using position only. addition-
ally, many utterances reference logical forms that

model c is unable to express, such as    repeat the
   rst step   , or    add it back   .

figure 6 shows how the models perform as the
number of utterances per example varies. when
the search space is small (fewer number of ut-
terances), model b outperforms or is competitive
with model c. however, as the search space in-
creases (tighter computational constraints), model
c does increasingly better.

overall, both models perform worse as l in-
creases, since to predict the    nal world state wl
correctly, a model essentially needs to predict an
entire sequence of logical forms z1, . . . , zl, and
errors cascade. furthermore, for larger l, the ut-
terances tend to have richer context-dependence.

6.2 arti   cial data experiments
setup. due to the large search space, running
model a on real data is impractical. in order feasi-
bly evaluate model a, we constructed an arti   cial
dataset. the worlds are created using the proce-
dure described in section 2.2. we use a simple
template to generate utterances (e.g.,    drain 1 from
the 2 green beaker   ).

to reduce the search space for model a, we
only allow actions (e.g., drain) to align to verbs
and property values (e.g., green) to align to ad-
jectives. using these linguistic constraints pro-
vides a slightly optimistic assessment of model
a   s performance.

we train on a dataset of 500 training examples
and evaluate on 500 test examples. we repeat this
procedure for varying beam sizes, from 40 to 260.
the model only uses features (f1) through (f3).

accuracy under in   nite beam. since model a
is more expressive, we would expect it to be more
powerful when we have no computational con-
straints. figure 7 shows that this is indeed the
case: when the beam size is greater than 250, all
models attain an oracle of 1, and model a out-
performs model b, which performs similarly to
model c. this is because the alignments provide a
powerful signal for constructing the logical forms.
without alignments, models b and c learn noisier
features, and accuracy suffers accordingly.

id64. model a performs the best with
unconstrained computation, and model c per-
forms the best with constrained computation. is
there some way to bridge the two? even though
model c has limited expressivity, it can still learn

(a) alchemy

(b) scene

(c) tangrams

figure 6: test results on our three datasets as we vary the number of utterances. the solid lines are
the accuracy, and the dashed line are the oracles: with    nite beam, model c signi   cantly outperforms
model b on alchemy and scene, but is slightly worse on tangrams.

figure 7: test results on our arti   cial dataset with
varying beam sizes. the solid lines are the accura-
cies, and the dashed line are the oracle accuracies.
model a is unable to learn anything with beam
size < 240. however, for beam sizes larger than
240, model a attains 100% accuracy. model c
does better than models a and b when the beam
size is small < 40, but otherwise performs compa-
rably to model b. id64 model a using
model c parameters outperforms all of the other
models and attains 100% even with smaller beams.

to associate words like    green    with their corre-
sponding predicate green. these should be use-
ful for model a too.

to operationalize this, we    rst train model c
and use the parameters to initialize model a. then
we train model a. figure 7 shows that although
model a and c predict different logical forms, the
initialization allows model c to a to perform well
in constrained beam settings. this id64
works here because model c is a projection of
model a, and thus they share the same features.

figure 8: predicted logical forms for this text: the
logical form add takes a    gure and position as in-
put. model b predicts the correct logical form.
model c does not understand that    back    refers to
position 5, and adds the cat    gure to position 1.

model
b
c

beam action
0.47
0.15

0.03
0.03

argument
0.17
0.25

context
0.23
0.5

noise
0.04
0.07

table 5: percentage of errors for model b and c:
model b suffers predominantly from computation
constraints, while model c suffers predominantly
from a lack of expressivity.

6.3 error analysis
we randomly sampled 20 incorrect predictions on
3 utterance examples from each of the three real
datasets for model b and model c. we catego-
rized each prediction error into one of the follow-
ing categories:
(i) logical forms falling off the
beam; (ii) choosing the wrong action (e.g., map-
ping    drain    to pour); (iii) choosing the wrong
argument due to misunderstanding the description
(e.g., mapping    third beaker    to pos(1)); (iv)
choosing the wrong action or argument due to mis-
understanding of context (see figure 8); (v) noise

delete the last    gure. add it back.text:context:model c:model b:denotation:z1=remove(pos(5))z2=add(cat,1)z1=remove(pos(5))z2=add(args[1][1],pos(args[1][1]))in the dataset. table 5 shows the fraction of each
error category.

7 related work and discussion
context-dependent id29. utter-
ances can depend on either linguistic context or
world state context.
zettlemoyer and collins
(2009) developed a model that handles references
to previous logical forms; artzi and zettlemoyer
(2013) developed a model that handles references
to the current world state. our system considers
both types of context, handling linguistic phenom-
ena such as ellipsis and anaphora that reference
both previous world states and logical forms.

logical form generation. traditional semantic
parsers generate logical forms by aligning each
part of the logical form to the utterance (zelle
and mooney, 1996; wong and mooney, 2007;
zettlemoyer and collins, 2007; kwiatkowski et
al., 2011).
in general, such systems rely on a
lexicon, which can be hand-engineered, extracted
(cai and yates, 2013; berant et al., 2013), or au-
tomatically learned from annotated logical forms
(kwiatkowski et al., 2010; chen, 2012).

recent work on learning from denotations has
moved away from anchored logical forms. pa-
supat and liang (2014) and wang et al. (2015)
proposed generating logical forms without align-
ments, similar to our model b. yao et al. (2014)
and bordes et al. (2014) have explored predicting
paths in a id13 directly, which is sim-
ilar to the    at logical forms of model c.

relaxation and id64. the idea of
   rst training a simpler model in order to work up to
a more complex one has been explored other con-
texts. in the unsupervised learning of generative
models, id64 can help escape local op-
tima and provide helpful id173 (och and
ney, 2003; liang et al., 2009). when it is dif   cult
to even    nd one logical form that reaches the de-
notation, one can use the relaxation technique of
steinhardt and liang (2015).

recall that projecting from model a to c cre-
ates a more computationally tractable model at
the cost of expressivity. however, this is because
model c used a linear model. one might imag-
ine that a non-linear model would be able to re-
cuperate some of the loss of expressivity.
in-
deed, neelakantan et al. (2016) use recurrent neu-
ral networks attempt to perform logical operations.

one could go one step further and bypass log-
ical forms altogether, performing all the logical
reasoning in a continuous space (bowman et al.,
2014; weston et al., 2015; guu et al., 2015; reed
and de freitas, 2016). this certainly avoids the
combinatorial explosion of logical forms in model
a, but could also present additional optimization
challenges. it would be worth exploring this av-
enue to completely understand the computation-
expressivity tradeoff.

reproducibility

data,

code,

our
available
worksheets.codalab.org/worksheets/
0xad3fc9f52f514e849b282a105b1e3f02/.

and
codalab

experiments

on

at

are
https://

acknowledgments

we thank the anonymous reviewers for their con-
structive feedback. the third author is supported
by a microsoft research faculty fellowship.

references
y. artzi and l. zettlemoyer. 2013. weakly supervised
learning of semantic parsers for mapping instruc-
tions to actions. transactions of the association for
computational linguistics (tacl), 1:49   62.

j. berant, a. chou, r. frostig, and p. liang. 2013.
id29 on freebase from question-answer
in empirical methods in natural language
pairs.
processing (emnlp).

a. bordes, s. chopra, and j. weston. 2014. ques-
tion answering with subgraph embeddings. in em-
pirical methods in natural language processing
(emnlp).

s. r. bowman, c. potts, and c. d. manning. 2014.
can recursive neural tensor networks learn logical
reasoning? in international conference on learn-
ing representations (iclr).

q. cai and a. yates. 2013. large-scale semantic pars-
ing via schema matching and lexicon extension. in
association for computational linguistics (acl).

d. l. chen and r. j. mooney. 2011. learning to in-
terpret natural language navigation instructions from
observations. in association for the advancement of
arti   cial intelligence (aaai), pages 859   865.

d. l. chen. 2012. fast online lexicon learning for
in association for

grounded id146.
computational linguistics (acl).

j. clarke, d. goldwasser, m. chang, and d. roth.
2010. driving id29 from the world   s re-
sponse. in computational natural language learn-
ing (conll), pages 18   27.

a. vlachos and s. clark. 2014. a new corpus and
imitation learning framework for context-dependent
id29. transactions of the association
for computational linguistics (tacl), 2:547   559.

y. wang, j. berant, and p. liang. 2015. building a
semantic parser overnight. in association for com-
putational linguistics (acl).

j. weston, a. bordes, s. chopra, and t. mikolov.
2015. towards ai-complete id53: a
set of prerequisite toy tasks. arxiv.

y. w. wong and r. j. mooney.

2007. learning
synchronous grammars for id29 with
id198. in association for computational
linguistics (acl), pages 960   967.

x. yao, j. berant, and b. van-durme. 2014. freebase
qa: information extraction or id29. in
workshop on id29.

m. zelle and r. j. mooney.

learning to
parse database queries using inductive logic pro-
in association for the advancement of
gramming.
arti   cial intelligence (aaai), pages 1050   1055.

1996.

l. s. zettlemoyer and m. collins. 2005. learning to
map sentences to logical form: structured classi   ca-
tion with probabilistic categorial grammars. in un-
certainty in arti   cial intelligence (uai), pages 658   
666.

l. s. zettlemoyer and m. collins. 2007. online learn-
ing of relaxed id35 grammars for parsing to log-
in empirical methods in natural lan-
ical form.
guage processing and computational natural lan-
guage learning (emnlp/conll), pages 678   687.

l. s. zettlemoyer and m. collins. 2009. learning
context-dependent mappings from sentences to logi-
cal form. in association for computational linguis-
tics and international joint conference on natural
language processing (acl-ijcnlp).

d. a. dahl, m. bates, m. brown, w. fisher,
k. hunicke-smith, d. pallett, c. pao, a. rudnicky,
and e. shriberg. 1994. expanding the scope of the
atis task: the atis-3 corpus. in workshop on hu-
man language technology, pages 43   48.

j. duchi, e. hazan, and y. singer. 2010. adaptive sub-
gradient methods for online learning and stochastic
in conference on learning theory
optimization.
(colt).

k. guu, j. miller, and p. liang.

2015. travers-
in em-
ing id13s in vector space.
pirical methods in natural language processing
(emnlp).

t. kwiatkowski, l. zettlemoyer, s. goldwater, and
m. steedman. 2010.
inducing probabilistic id35
grammars from logical form with higher-order uni-
   cation. in empirical methods in natural language
processing (emnlp), pages 1223   1233.

t. kwiatkowski, l. zettlemoyer, s. goldwater, and
2011. lexical generalization in
m. steedman.
in
id35 grammar induction for id29.
empirical methods in natural language processing
(emnlp), pages 1512   1523.

p. liang, m. i. jordan, and d. klein. 2009. learning
semantic correspondences with less supervision. in
association for computational linguistics and in-
ternational joint conference on natural language
processing (acl-ijcnlp), pages 91   99.

p. liang. 2013. lambda dependency-based composi-

tional semantics. arxiv.

a. neelakantan, q. v. le, and i. sutskever. 2016.
neural programmer: inducing latent programs with
in international conference on
id119.
learning representations (iclr).

f. j. och and h. ney. 2003. a systematic comparison
of various statistical alignment models. computa-
tional linguistics, 29:19   51.

p. pasupat and p. liang. 2014. zero-shot entity extrac-
tion from web pages. in association for computa-
tional linguistics (acl).

p. pasupat and p. liang. 2015. compositional semantic
parsing on semi-structured tables. in association for
computational linguistics (acl).

s. reed and n. de freitas. 2016. neural programmer-
interpreters. in international conference on learn-
ing representations (iclr).

j. steinhardt and p. liang. 2015. learning with re-
laxed supervision. in advances in neural informa-
tion processing systems (nips).

