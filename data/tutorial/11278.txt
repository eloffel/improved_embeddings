6
1
0
2

 

n
a
j
 

3

 
 
]
l
c
.
s
c
[
 
 

2
v
2
1
6
6
0

.

2
1
5
1
:
v
i
x
r
a

backward and forward id38 for constrained

sentence generation

lili mou,1 rui yan,2 ge li,1 lu zhang,1 zhi jin1

1software institute, peking university

key laboratory of high con   dence software technologies (peking university),

ministry of education, china

2baidu inc.

{doublepower.mou,rui.yan.peking}@gmail.com

{lige,zhanglu,zhijin}@sei.pku.edu.cn

abstract

recent language models, especially those based on recurrent neural networks (id56s),
make it possible to generate natural language from a learned id203. language gen-
eration has wide applications including machine translation, summarization, question an-
swering, conversation systems, etc. existing methods typically learn a joint id203 of
words conditioned on additional information, which is (either statically or dynamically)
fed to id56   s hidden layer. in many applications, we are likely to impose hard constraints
on the generated texts, i.e., a particular word must appear in the sentence. unfortunately,
existing approaches could not solve this problem. in this paper, we propose a novel back-
ward and forward language model. provided a speci   c word, we use id56s to generate
previous words and future words, either simultaneously or asynchronously, resulting in two
model variants. in this way, the given word could appear at any position in the sentence.
experimental results show that the generated texts are comparable to sequential lms in
quality.

1 introduction

id38 is aimed at minimizing the joint id203 of a corpus. it has long been
the core of natural language processing (nlp) [8], and has inspired a variety of other models,
e.g., the id165 model, smoothing techniques [4], as well as various neural networks for nlp
[2, 6, 17]. in particular, the renewed prosperity of neural models has made groundbreaking
improvement in many tasks, including id38 per se [2], part-of-speech tagging,
id39, id14 [6], etc.

the recurrent neural network (id56) is a prevailing class of language models; it is suitable
for modeling time-series data (e.g., a sequence of words) by its iterative nature. an id56
usually keeps one or a few hidden layers; at each time slot, it reads a word, and changes
its state accordingly. compared with traditional id165 models, id56s are more capable of
learning long range features   especially with long short term memory (lstm) units [7] or
id149 (gru) [5]   and hence are better at capturing the nature of sentences.
on such a basis, it is even possible to generate a sentence from an id56 language model, which
has wide applications in nlp, including machine translation [15], abstractive summarization
[13], id53 [19], and conversation systems [18]. the sentence generation process

1

is typically accomplished by choosing the most likely word at a time, conditioned on previous
words as well as additional information depending on the task (e.g., the vector representation
of the source sentence in a machine translation system [15]).

in many scenarios, however, we are likely to impose constraints on the generated sentences.
for example, a id53 system may involve analyzing the question and querying
an existing knowledge base, to the point of which, a candidate answer is at hand. a natural
language generator is then supposed to generate a sentence, coherent in semantics, containing
the candidate answer. unfortunately, using existing language models to generate a sentence
with a given word is non-trivial: adding additional information [16, 19] about a word does
not guarantee that the wanted word will appear; generic probabilistic samplers (e.g., markov
chain monte carlo methods) hardly applies to id56 language models1; setting an arbitrary
word to be the wanted word damages the    uency of a sentence; imposing the constraint on
the    rst word restricts the form of generated sentences.

in this paper, we propose a novel backward and forward (b/f) language model to tackle
the problem of constrained id86. rather than generate a sentence
from the    rst word to the last in sequence as in traditional models, we use id56s to generate
previous and subsequent words conditioned on the given word. the forward and backward
generation can be accomplished either simultaneously or asynchronously, resulting in two
variants, syn-b/f and asyn-b/f. in this way, our model is complete in theory for generating
a sentence with a wanted word, which can appear at an arbitrary position in the sentence.

the rest of this paper is organized as follows. section 2 reviews existing language models
and natural language generators. section 3 describes the proposed b/f language models in
detail. section 4 presents experimental results. finally, we have conclusion in section 5.

2 background and related work

2.1 id38
given a corpus w = w1,       , wm, id38 aims to minimize the joint distribution
of w, i.e. p(w).
inspired by the observation that people always say a sentence from the
beginning to the end, we would like to decompose the joint id203 as2

m(cid:89)

t=1

p(w) =

p(wt|wt   1

1

)

(1)

parameterizing by multinomial distributions, we need to further simplify the above equation in
order to estimate the parameters. imposing a markov assumption   a word is only dependent
on previous n    1 words and independent of its position   results in the classic id165 model,
where the joint id203 is given by

p(w)     m(cid:89)

p(cid:0)wt

(cid:12)(cid:12)wt   1

t   n+1

(cid:1)

to mitigate the data sparsity problem, a variety of smoothing methods have been proposed.
we refer interested readers to textbooks like [8] for id165 models and their variants.

t=1

1with recent e   orts in [3].
2 w1, w2,       , wt is denoted as wt

1 for short.

2

(2)

bengio et al. [2] propose to use feed-forward neural networks to estimate the id203
in equation 2. in their model, a word is    rst mapped to a small dimensional vector, known
as an embedding; then a feed-forward neural network propagates information to a softmax
output layer, which estimates the id203 of the next word.

a recurrent neural network (id56) can also be used in id38.

it keeps a
hidden state vector (ht at time t), dependent on the its previous state (ht   1) and the current
input vector x, the id27 of the current word. an output layer estimates the
id203 that each word occurs at this time slot. following are listed the formulas for a
vanilla id56.3

ht = id56(xt, ht   1)

p(wt|wt   1

0

= f (winxt + whidht   1)
)     softmax (woutht)

(3)

(4)

(5)

as is indicated from the equations, an id56 provides a means of direct parametrization of
equation 1, and hence has the ability to capture long term dependency, compared with n-
gram models. in practice, the vanilla id56 is di   cult to train due to the gradient vanishing
or exploding problem; long short term (lstm) units [7] and id149 (gru) [5]
are proposed to better balance between the previous state and the current input.

2.2 language generation

using id56s to model the joint id203 of language makes it feasible to generate new
sentences. an early attempt generates texts by a character-level id56 language model [14];
recently, id56-based language generation has made breakthroughs in several real applications.
the sequence to sequence machine translation model [15] uses an id56 to encode a source
sentence (in foreign language) into one or a few    xed-size vectors; another id56 then decodes
the vector(s) to the target sentence. such network can be viewed as a language model,
conditioned on the source sentence. at each time step, the id56 predicts the most likely
word as the output; the embedding of the word is fed to the input layer at next step. the
process continues until the id56 generates a special symbol <eos> indicating the end of the
sequence. id125 [15] or sampling methods [16] can be used to improve the quality and
diversity of generated texts.

if the source sentence is too long to    t into one or a few    xed-size vectors, an attention
mechanism [1] can be used to dynamically focus on di   erent parts of the source sentence dur-
ing target generation. in other studies, wen et al. use an id56 to generate a sentence based
on some abstract representations of semantics; they feed a one-hot vector, as additional infor-
mation, to the id56   s hidden layer [16]. in a id53 system, yin et al. leverage
a soft logistic switcher to either generate a word from the vocabulary or copy the candidate
answer [19].

3 the proposed b/f language model

in this part, we introduce our b/f language model in detail. our intuition is to seek a new
approach to decompose the joint id203 of a sentence (equation 1). if we know a priori

3w    s refer to weights; biases are omitted.

3

figure 1: synchronous foward/backward generation.

that a word ws should appear in the sentence (w = w1,       , wm, ws     w), it is natural to
design a id110 where ws is the root node, and other words are conditioned on ws.
following the spirit of    sequence    generation, ws split the sentence into two subsequences:

    backward sequence: ws, ws   1, ws   2,       , w1

(s words)

    forward sequence: ws, ws+1, ws+2,       , wn

(m     s + 1 words)

the id203 that the sentence w with the split word at position s decomposes as

follows.4

p

(cid:32)

(cid:33)

w1
s
wn
s

s(cid:89)

i=0

= p(ws)

p(bw)(ws   i|  )

m   s+1(cid:89)

i=0

p(fw)(ws+1|  )

(6)

to parametrize the equation, we propose two model variants. the    rst approach is to
generate previous and backward models simultaneously, and we call this syn-b/f language
model (figure 1).5 concretely, equation 6 takes the form

(cid:32)

p

w1
s
wn
s

(cid:33)

=

max{s,m   s+1}   1(cid:89)

(cid:32)

p

t=0

ws   t
ws+t

(cid:33)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ws   t+1

ws+t   1

s

s

(7)

where the factor p(=|=) refers to the id155 that current time step t generates
ws   t, ws+t in the forward and backward sequences, respectively, given the middle part of the
sentence, that is, ws   t+1        ws        ws+t   1. if one part has generated <eos>, we pad the special
symbol <eos> for this sequence until the other part also terminates.

  
=   ) denotes the id203 of a particular backward/forward sequence.

4p(
5previously called backbone lm.

4

ws  1ws+1w0  .........<eos><eos>backward  sequence(softmax)hidden  layerforward  sequence(softmax)<eos><eos>wmwswsfigure 2: asynchronous forward/backward generation.

following the studies introduced in section 2, we also leverage a recurrent neural network

(id56) . the factors in equation 7 are computed by

(cid:32)

(cid:104)

ws   t
ws+t

(cid:33)
(cid:17)    softmax

s

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ws   t+1
(cid:16)
(cid:105)

w (bw)

out ht

s

ws+t   1

p
=p(bw) (ws   t|ht)    p(fw) (ws+t|ht)
= softmax

(cid:16)

(cid:17)

w (fw)

out ht

(8)

(9)

(10)

here, ht is the hidden layer, which is dependent on the previous state ht   1 and current

input id27s   x =

. we use gru [5] in our model, given by

x(fw)

; x(bw)

t

t

r =   (wr   x + urht   1)
z =   (wz   x + uzht   1)

  h = tanh(cid:0)wx   x + ux(r     ht   1)(cid:1)

ht = (1     z)     ht   1 + z       h

(11)

(12)

(13)

(14)

where   (  ) =
candidate hidden state at the current step.

1+e(     ) ;     denotes element-wise product. r and z are known as gates,   h the

1

in the syn-b/f model, we design a single id56 to generate both chains in hope that each
is aware of the other   s state. besides, we also propose an asynchronous version, denoted as
asyn-b/f (figure 2). the idea is to    rst generate the backward sequence, and then feed the
obtained result to another forward id56 to generate future words. the detailed formulas are
not repeated.

it is important to notice that asyn-b/f   s id56 for backward sequence generation is dif-
ferent from a generic backward lm. the latter is presumed to model a sentence from the last
word to the    rst one, whereas our backward id56 is, in fact, a    half    lm, starting from ws.

5

ws  1wsw0  ...<eos><eos>backward  sequence  (softmax)wsws+1w0  ......<eos>forward  sequence  (softmax)wm  ...generated  backward  sequence  in  step  istep  istep  iiws  1training criteria. if we assume ws is always given, the training criterion shall be the cross-
id178 loss of all words in both chains except ws. we can alternatively penalize the split
word ws in addition, which will make it possible to generate an entire sentence without ws
being provided. we do not deem the two criteria di   er signi   cantly, and adopt the latter one
in our experiments.

both labeled and unlabeled datasets su   ce to train the b/f language model. if a sentence
is annotated with a specially interesting word ws, it is natural to use it as the split word. for
an unlabeled dataset, we can randomly choose a word as ws.

notice that equation 6 gives the joint id203 of a sentence w with a particular split
word ws. to compute the id203 of the sentence, we shall marginalize out di   erent split
words, i.e.,

(cid:32)

(cid:33)

w1
s
wn
s

m(cid:88)

s=1

p(w) =

p

(15)

in our scenarios, however, we always assume that ws is given in practice. hence, di   erent
from id38 in general, the joint id203 of a sentence is not the number one
concern in our model.

4 evaluation

4.1 the dataset and settings

to evaluate our b/f lms, we prefer a vertical domain corpus with interesting application
nuggets instead of using generic texts like wikipedia.
in particular, we chose to build a
language model upon scienti   c paper titles on arxiv.6 building a language model on paper
titles may help researchers when they are preparing their drafts. provided a topic (desig-
nated by a given word), constrained id86 could also acts as a way of
brainstorming.7
we crawled computer science-related paper titles from january 2014 to november 2015.8
each word was decapitalized, but no id30 was performed. rare words (    10 occurrences)
were grouped as a single token, <unk>, (referring to unknown). we removed non-english
titles, and those with more than three <unk>   s. we notice that <unk>   s may appear fre-
quently, but a large number of them refer to acronyms, and thus are mostly consistent in
semantics.

currently, we have 25,000 samples for training, 1455 for validation and another 1455 for
testing; our vocabulary size is 3380. the asyn-b/f has one hidden layer with 100 units; syn-
b/f has 200; this makes a fair comparison because syn-b/f should simultaneously learn im-
plicit forward and backward lms, which are completely di   erent. in our models, embeddings
are 50 dimensional, initialized randomly. to train the model, we used standard backpropa-
gation (batch size 50) with element-wise gradient clipping. following [9], we applied rmsprop
for optimization (embeddings excluded), which is more suitable for training id56s than na    ve
stochastic id119, and less sensitive to hyperparameters compared with momentum
methods. initial weights were uniformly sampled from [   0.08, 0.08]. initial learning rate was

6http://arxiv.org
7the title of this paper is not generated by our model.
8crawled from http://http://dblp.uni-trier.de/db/journals/corr/

6

method overall ppl first word   s ppl subsequent words    ppl

sequential lm
info-init
info-all
sep-b/f
sep-b/f (ws oracle)
syn-b/f
syn-b/f (ws oracle)
asyn-b/f
asyn-b/f (ws oracle)

152.2
148.7
125.4
192.4
99.2
185.4
97.5
177.2
89.8

416.2
371.5
328.0
556.1

   

592.7

   

584.5

   

134.8
133.3
121.8
169.9

   

162.9

   

153.7

   

table 1: perplexity (ppl) of our b/f lms and baselines.

0.002, with a multiplicative learning rate decay of 0.97, moving average decay 0.99, and a
damping term   = 10   8. as id27s are sparse in use [12], they were optimized
 .9
asynchronously by pure stochastic id119 with learning rate being divided by

   

4.2 results

we    rst use the perplexity measure to evaluate the learned language models. perplexity is
de   ned as 2   (cid:96), where (cid:96) is the log-likelihood (with base 2), averaged over each word.

m(cid:88)

i=1

(cid:96) =

1
m

log p(wi)

note that <eos> is not considered when we compute the perplexity.

we compare our models with several baselines:
    sequential lm: a pure lm, which is not applicable to constrained sentence generation.
    info-all: built upon sequential lm, info-all takes the wanted word   s embedding as

additional input at each time step during sequence generation, similar to [16].

    info-init: the wanted word   s embedding is only added at the    rst step (sequence to

sequence model [15]).

    sep-b/f: we train two separate forward and backward lms (both starting from the

split word).

table 1 summarizes the perplexity of our models and baselines. we further plot the

perplexity of a word with respect to its position when generation (figure 3).

from the experimental results, we have the following observations.
    all b/f variants yield a larger perplexity than a sequantial lm. this makes much sense
because randomly choosing a split word increases uncertainly. it should be also noticed
that, in our model, the perplexity re   ects the id203 of a sentence with a speci   c
split word, whereas the perplexity of the sequential lm assesses the id203 of a
sentence itself.

9the implementation was based on [10, 11].

7

figure 3: perplexity versus word position t, which is the distance between the current word
and the    rst / split word in sequential, b/f lms, respectively.

based

convolu-
neural
for unk
image

asyn-b/f
deep
tional
networks
-
segmentation
object tracking and
unk for visual recog-
nition
optimal control
for
unk systems with
unk - type ii : a unk
- unk approach
unk : a new method
for unk based on -
line counting on unk

syn-b/f
convolutional neu-
ral networks for unk
- unk

sep-b/f
deep
tional
networks

convolu-
neural

learning deep convo-
lutional features for
object tracking
formal
veri   ca-
tion of unk - unk
systems

a new ap-
unk :
proach for unk -
based unk

object tracking

optimal control
unk systems

for

unk : a survey

an approach to unk
the edge - preserving
problem

an approach to unk

an approach to unk
- unk

sequential lm
convolutional neu-
ral networks for unk
-based object detec-
tion

tracking - unk -
based social media

systems
synthesis
based diagnose

- based
for unk

to

a unk - based
:
approach
unk
- based deign of
unk -based image
retrieval
a unk
to unk :
- e   cient and scal-
able framework for
the unk of unk

info-all
convolutional neu-
ral networks for im-
age classi   cation

unk - based unk
detection for image
segmentation
a new approach for
the unk of the unk -
free problem

unk : a unk - based
approach for unk -
free grammar

a unk - based ap-
proach to unk for
unk

table 2: generated texts by the b/f and sequential lms, with the word in bold being
provided.

    randomly choosing a split word cannot make use of position information in sentences.
the titles of scienti   c papers, for example, oftentimes follow templates, which may begin
with    <unk> : an approach    or    <unk> - based approach.    therefore, sequential lm
yields low perplexity when generating the word at a particular position (t = 2), but such
information is smoothed out in our b/f lms because the split word is chosen randomly.
    when t is large (e.g., t     4), b/f models yield almost the same perplexity as sequential
lm. the long term behavior is similar to sequential lm, if we rule out the impact of
choosing random words. for syn-b/f, in particular, the result indicates that feeding
two words    embeddings to the hidden layer does not add to confusion.

8

sheet1page  1syn  b/fsequential  lminfo  allt=0584.58592.731556.08416.198328.049371.526t=1171.78213.357218.244173.756144.133165.266t=2157.49167.005167.7386.57877.647784.4156t=3165.31159.472163.92123.114108.86122.801t=4163.18166.353159.052144.323121.649135.437t>=5139.08136.188153.168141.325132.928142.817asyn  b/fsep  b/finfo  initt=0t=1t=2t=3t=4t>=50100200300400500600700asyn  b/fsyn  b/fsep  b/fsequential  lminfo  allinfo  init    in our applications, ws is always given, which indicates p(ws) = 1 (denoted as ws oracle
in table 1). this reduces the perplexity to less than 100, showing that our b/f lms
can well make use of such information that some word should appear in the generated
text. further, our syn-b/f is better than na    ve sep-b/f; asyn-b/f is further capable
of integrating information in backward and forward sequences.

we then generate new paper titles from the learned language model with a speci   c word
being given, which can be thought of, in the application, as a particular interest of research
topics. table 2 illustrates examples generated from b/f models and baselines. as we see,
for words that are common at the beginning of a paper title   like the adjective convolutional
and gerund tracking   sequential lm can generate reasonable results. for plural nouns like
systems and models, the titles generated by sequential lm are somewhat in   uent, but they
basically comply with grammar rules. for words that are unlikely to be the initial word,
sequential lm fails to generate grammatically correct sentences.

adding additional information does guide the network to generate sentences relevant to

the topic, but the wanted word may not appear. the problem is also addressed in [16].

by contrast, b/f lms have the ability to generate correct sentences. but the sep-b/f
model is too greedy in its each chain. as generating short and general texts is a known issue
with neural network-based lms, sep-b/f can hardly generate a sentence containing much
substance. syn-b/f is better, and asyn-b/f is able to generate sentences whose quality is
comparable with sequential lms.

5 conclusion

in this paper, we proposed a backward and forward language model (b/f lm) for constrained
id86. given a particular word, our model can generate previous words
and future words either synchronously or asynchronously. experiments show a similar per-
plexity to sequential lm, if we disregard the perplexity introduced by random splitting. our
case study demonstrates that the asynchronous b/f lm can generate sentences that contain
the given word and are comparable to sequential lm in quality.

references

[1] d. bahdanau, k. cho, and y. bengio. id4 by jointly learning to

align and translate. in international conference on learning representations, 2015.

[2] y. bengio, r. ducharme, p. vincent, and c. janvin. a neural probabilistic language

model. the journal of machine learning research, 3:1137   1155, 2003.

[3] m. berglund, t. raiko, m. honkala, l. k  arkk  ainen, a. vetek, and j. t. karhunen.
in advances in neural

id182 as generative models.
information processing systems, pages 856   864, 2015.

[4] s. f. chen and j. goodman. an empirical study of smoothing techniques for language
modeling. in proceedings of the 34th annual meeting on association for computational
linguistics, pages 310   318, 1996.

9

[5] k. cho, b. van merri  enboer, d. bahdanau, and y. bengio. on the properties of neural
machine translation: encoder-decoder approaches. in proceedings of eighth workshop
on syntax, semtnatics and structure in statistical translation.

[6] r. collobert and j. weston. a uni   ed architecture for natural language processing:
deep neural networks with multitask learning. in proceedings of the 25th international
conference on machine learning, pages 160   167, 2008.

[7] s. hochreiter and j. schmidhuber. long short-term memory. neural computation,

9(8):1735   1780, 1997.

[8] d. jurafsky and j. h. martin. speech and language processing. pearson, 2014.

[9] a. karpathy, j. johnson, and f.-f. li. visualizing and understanding recurrent networks.

arxiv preprint arxiv:1506.02078, 2015.

[10] l. mou, g. li, l. zhang, t. wang, and z. jin. convolutional neural networks over tree
structures for programming language processing. in proceedings of the thirtieth aaai
conference on arti   cial intelligence, 2016.

[11] l. mou, h. peng, g. li, y. xu, l. zhang, and z. jin. discriminative neural sentence
modeling by tree-based convolution. in proceedings of the 2015 conference on empirical
methods in natural language processing, pages 2315   2325, 2015.

[12] h. peng, l. mou, g. li, y. chen, y. lu, and z. jin. a comparative study on regu-
larization strategies for embedding-based neural networks. in proceedings of the 2015
conference on empirical methods in natural language processing, pages 2106   2111,
2015.

[13] a. m. rush, s. chopra, and j. weston. a neural attention model for abstractive sentence
summarization. in proceedings of the 2015 conference on empirical methods in natural
language processing, pages 379   389, 2015.

[14] i. sutskever, j. martens, and g. e. hinton. generating text with recurrent neural net-
works. in proceedings of the 28th international conference on machine learning, pages
1017   1024, 2011.

[15] i. sutskever, o. vinyals, and q. v. le. sequence to sequence learning with neural
in advances in neural information processing systems, pages 3104   3112,

networks.
2014.

[16] t.-h. wen, m. gasic, d. kim, n. mrksic, p.-h. su, d. vandyke, and s. young. stochas-
tic language generation in dialogue using recurrent neural networks with convolutional
sentence reranking. in proceedings of the 16th annual meeting of the special interest
group on discourse and dialogue, pages 275   284, 2015.

[17] y. xu, l. mou, g. li, y. chen, h. peng, and z. jin. classifying relations via long short
term memory networks along shortest dependency paths. in proceedings of conference
on empirical methods in natural language processing, 2015.

[18] k. yao, g. zweig, and b. peng. attention with intention for a neural network conversation

model. arxiv preprint arxiv:1510.08565 (nips workshop), 2015.

10

[19] j. yin, x. jiang, z. lu, l. shang, h. li, and x. li. neural generative id53.

arxiv preprint arxiv:1512.01337, 2015.

11

