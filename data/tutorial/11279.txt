optimizing spectral learning for parsing

shashi narayan and shay b. cohen

school of informatics
university of edinburgh
edinburgh, eh8 9le, uk

{snaraya2,scohen}@inf.ed.ac.uk

6
1
0
2

 

n
u
j
 

4
1

 
 
]
l
c
.
s
c
[
 
 

3
v
2
4
3
2
0

.

6
0
6
1
:
v
i
x
r
a

abstract

we describe a search algorithm for opti-
mizing the number of latent states when
estimating latent-variable pid18s with
id106. our results show that
contrary to the common belief that the
number of latent states for each nontermi-
nal in an l-pid18 can be decided in isola-
tion with id106, parsing results
signi   cantly improve if the number of la-
tent states for each nonterminal is globally
optimized, while taking into account in-
teractions between the different nontermi-
nals. in addition, we contribute an empiri-
cal analysis of spectral algorithms on eight
morphologically rich languages: basque,
french, german, hebrew, hungarian, ko-
rean, polish and swedish. our results
show that our estimation consistently per-
forms better or close to coarse-to-   ne
expectation-maximization techniques for
these languages.

1 introduction

latent-variable probabilistic context-free gram-
mars (l-pid18s) have been used in the natural lan-
guage processing community (nlp) for syntactic
parsing for over a decade. they were introduced
in the nlp community by matsuzaki et al. (2005)
and prescher (2005), with matsuzaki et al. us-
ing the id83
to estimate them. their performance on syntac-
tic parsing of english at that stage lagged behind
state-of-the-art parsers.

petrov et al. (2006) showed that one of the
reasons that the em algorithm does not estimate
state-of-the-art parsing models for english is that
the em algorithm does not control well for the
model size used in the parser     the number of la-

tent states associated with the various nontermi-
nals in the grammar. as such, they introduced a
coarse-to-   ne technique to estimate the grammar.
it splits and merges nonterminals (with latent state
information) with the aim to optimize the likeli-
hood of the training data. together with other
types of    ne tuning of the parsing model, this led
to state-of-the-art results for english parsing.

in more recent work, cohen et al. (2012) de-
scribed a different family of estimation algorithms
for l-pid18s. this so-called    spectral    family of
learning algorithms is compelling because it offers
a rigorous theoretical analysis of statistical conver-
gence, and sidesteps local maxima issues that arise
with the em algorithm.

while spectral algorithms for l-pid18s are
compelling from a theoretical perspective,
they
have been lagging behind in their empirical results
on the problem of parsing. in this paper we show
that one of the main reasons for that is that spectral
algorithms require a more careful tuning proce-
dure for the number of latent states than that which
has been advocated for until now. in a sense, the
relationship between our work and the work of
cohen et al. (2013) is analogous to the relation-
ship between the work by petrov et al. (2006) and
the work by matsuzaki et al. (2005): we suggest
a technique for optimizing the number of latent
states for spectral algorithms, and test it on eight
languages.

our results show that when the number of la-
tent states is optimized using our technique, the
parsing models the spectral algorithms yield per-
form signi   cantly better than the vanilla-estimated
models, and for most of the languages     better than
the berkeley parser of petrov et al. (2006).

as such, the contributions of this parser are two-

fold:

    we describe a search algorithm for optimiz-

ing the number of latent states for spectral
learning.

    we describe an analysis of spectral algo-
rithms on eight languages (until now the re-
sults of l-pid18 estimation with spectral al-
gorithms for parsing were known only for
english). our parsing algorithm is rather
language-generic, and does not require sig-
ni   cant linguistically-oriented adjustments.

in addition, we dispel the common wisdom that
more data is needed with spectral algorithms. our
models yield high performance on treebanks of
varying sizes from 5,000 sentences (hebrew and
swedish) to 40,472 sentences (german).

the rest of the paper is organized as follows.
in   2 we describe notation and background.   3
further investigates the need for an optimization
of the number of latent states in spectral learn-
ing and describes our optimization algorithm, a
search algorithm akin to id125. in   4 we de-
scribe our experiments with natural language pars-
ing for basque, french, german, hebrew, hungar-
ian, korean, polish and swedish. we conclude in
  5.

2 background and notation

we denote by [n] the set of integers {1, . . . , n}.
an l-pid18 is a 5-tuple (n , i, p, f, n) where:

    n is the set of nonterminal symbols in the
grammar. i     n is a    nite set of intermi-
nals. p     n is a    nite set of preterminals.
we assume that n = i     p, and i     p =    .
hence we have partitioned the set of nonter-
minals into two subsets.

    f : n     n is a function that maps each non-
terminal a to the number of latent states it
uses. the set [ma] includes the possible hid-
den states for nonterminal a.

    [n] is the set of possible words.

    for all a     i, b     n , c     n , h1     [ma],
h2     [mb], h3     [mc], we have a binary
context-free rule a(h1)     b(h2) c(h3).

    for all a     p, h     [ma], x     [n], we have a

lexical context-free rule a(h)     x.

the estimation of an l-pid18 requires an as-
signment of probabilities (or weights) to each of

the rules a(h1)     b(h2)
c(h3) and a(h)     x,
and also an assignment of starting probabilities for
each a(h), where a     i and h     [ma]. estima-
tion is usually assumed to be done from a set of
parse trees (a treebank), where the latent states are
not included in the data     only the    skeletal    trees
which consist of nonterminals in n .

l-pid18s, in their symbolic form, are related
to regular tree grammars, an old grammar formal-
ism, but they were introduced as statistical mod-
els for parsing with latent heads more recently
by matsuzaki et al. (2005) and prescher (2005).
earlier work about l-pid18s by matsuzaki et al.
(2005) used the expectation-maximization (em)
algorithm to estimate the grammar probabilities.
indeed, given that the latent states are not ob-
served, em is a good    t for l-pid18 estimation,
since it aims to do learning from incomplete data.
this work has been further extended by petrov et
al. (2006) to use em in a coarse-to-   ne fashion:
merging and splitting nonterminals using the la-
tent states to optimize the number of latent states
for each nonterminal.

cohen et al. (2012) presented a so-called spec-
tral algorithm to estimate l-pid18s. this algo-
rithm uses linear-algebraic procedures such as sin-
gular value decomposition (svd) during learning.
the spectral algorithm of cohen et al. builds on
an estimation algorithm for id48s by hsu et al.
(2009).1 cohen et al. (2013) experimented with
this spectral algorithm for parsing english. a dif-
ferent variant of a spectral learning algorithm for
l-pid18s was developed by cohen and collins
(2014). it breaks the problem of l-pid18 estima-
tion into multiple id76 problems
which are solved using em.

the family of l-pid18 spectral learning algo-
rithms was further extended by narayan and co-
hen (2015). they presented a simpli   ed version
of the algorithm of cohen et al. (2012) that es-
timates sparse grammars and assigns probabili-
ties (instead of weights) to the rules in the gram-
mar, and as such does not suffer from the prob-
lem of negative probabilities that arise with the
original spectral algorithm (see discussion in co-
hen et al., 2013). in this paper, we use the algo-
rithms by narayan and cohen (2015) and cohen

1a related algorithm for weighted tree automata (wta)
was developed by bailly et al. (2010). however, the con-
version from l-pid18s to wta is not straightforward, and
information is lost in this conversion. see also (rabusseau et
al., 2016).

vp

v

np

chased

d

n

the

cat

s

np

vp

d

n

the

mouse

figure 1: the inside tree (left) and outside tree
(right) for the nonterminal vp in the parse tree
(s (np (d the) (n mouse)) (vp (v
chased) (np (d the) (n cat)))) for
the sentence    the mouse chased the cat.   

et al. (2012), and we compare them against state-
of-the-art l-pid18 parsers such as the berkeley
parser (petrov et al., 2006). we also compare our
algorithms to other state-of-the-art parsers where
elaborate linguistically-motivated feature speci   -
cations (hall et al., 2014), annotations (crabb  e,
2015) and formalism conversions (fern  andez-
gonz  alez and martins, 2015) are used.

3 optimizing spectral estimation

in this section, we describe our optimization algo-
rithm and its motivation.

3.1 spectral learning of l-pid18s and

model size

the family of spectral algorithms for
latent-
variable pid18s rely on feature functions that are
de   ned for inside and outside trees. given a tree,
the inside tree for a node contains the entire sub-
tree below that node; the outside tree contains ev-
erything in the tree excluding the inside tree. fig-
ure 1 shows an example of inside and outside trees
for the nonterminal vp in the parse tree of the sen-
tence    the mouse chased the cat   .

with l-pid18s, the model dictates that an in-
side tree and an outside tree that are connected at
a node are statistically conditionally independent
of each other given the node label and the latent
state that is associated with it. as such, one can
identify the distribution over the latent states for a
given nonterminal a by using the cross-covariance
matrix of the inside and the outside trees,    a. for
more information on the de   nition of this cross-
covariance matrix, see cohen et al. (2012) and
narayan and cohen (2015).

the l-pid18 spectral algorithms use singular
value decomposition (svd) on    a to reduce the
dimensionality of the feature functions. if    a is
computed from the true l-pid18 distribution then

the rank of    a (the number of non-zero singular
values) gives the number of latent states according
to the model.

in the case of estimating    a from data gener-
ated from an l-pid18, the number of latent states
for each nonterminal can be exposed by capping
it when the singular values of    a are smaller than
some threshold value. this means that spectral al-
gorithms give a natural way for the selection of the
number of latent states for each nonterminal a in
the grammar.

however, when the data from which we esti-
mate an l-pid18 model are not drawn from an l-
pid18 (the model is    incorrect   ), the number of
non-zero singular values (or the number of singu-
lar values which are large) is no longer suf   cient
to determine the number of latent states for each
nonterminal. this is where our algorithm comes
into play: it optimizes the number of latent search
for each nonterminal by applying a search algo-
rithm akin to id125.

3.2 optimizing the number of latent states

as mentioned in the previous section, the number
of non-zero singular values of    a gives a criterion
to determine the number of latent states ma for a
given nonterminal a. in practice, we cap ma not
to include small singular values which are close to
0, because of estimation errors of    a.

this procedure does not take into account the
interactions that exist between choices of latent
state numbers for the various nonterminals.
in
principle, given the independence assumptions
that l-pid18s make, choosing the nonterminals
based only on the singular values is    statistically
correct.    however, because in practice the mod-
eling assumptions that we make (that natural lan-
guage parse trees are drawn from an l-pid18) do
not hold, we can improve further the accuracy of
the model by taking into account the nonterminal
interaction. another source of dif   culty in choos-
ing the number of latent states based the singu-
lar values of    a is sampling error: in practice, we
are using data to estimate    a, and as such, even
if the model is correct, the rank of the estimated
matrix does not have to correspond to the rank of
   a according to the true distribution. as a mat-
ter of fact, in addition to neglecting small singular
values, the id106 of cohen et al. (2013)
and narayan and cohen (2015) also cap the num-
ber of latent states for each nonterminal to an up-

inputs: an input treebank divided into training and devel-
opment set. a basic spectral estimation algorithm s with its
default setting. an integer k denoting the size of the beam.
an integer m denoting the upper bound on the number of
latent states.

algorithm:
(step 0: initialization)

    set q, a queue of size k, to be empty.

    estimate an l-pid18 gs : (n , i, p, fs , n) using s.

    initialize f = fs, a function that maps each nontermi-

nal a     n to the number of latent states.

    let l be a list of nonterminals (a1, . . . , am ) such that
ai     n for which to optimize the number of latent
states.

    let s be the f1 score for the above l-pid18 gs on the

development set.

    put in q the element (s, 1, f, coarse).

    the queue is ordered by s, the    rst element of tuples,

in the queue.

(step 1: search, repeat until termination happens)

    dequeue the queue into (s, j, f, t) where j is the index

in the input nonterminal list l.

    if j = (m + 1), return f.

    if t is coarse then for each m0     {1, 5, 10, . . . , m}:

    let f0 be such that    a 6= aj f0(a) = f (a) and

f0(aj) = m0.

    train an l-pid18 g0 using s but with f0.
    let s0 be the f1 score for g0 on the development

set.

    enqueue into q: (s0, j, f0, re   ne).

    if t is re   ne then for each m0     {f (a) +     |        

{   4,    3,    2,    1, 0, 1, 2, 3, 4}}:

    let f0 be such that    a 6= aj f0(a) = f (a) and

f0(aj) = m0.

    train an l-pid18 g0 using s but with f0.
    let s0 be the f1 score for g0 on the development

set.

    enqueue into q: (s0, j + 1, f0, coarse).

figure 2: a search algorithm for    nding the opti-
mal number of latent states.

per bound to keep the grammar size small.

petrov et al. (2006) improves over the estima-
tion described in matsuzaki et al. (2005) by taking
into account the interactions between the nonter-
minals and their latent state numbers in the train-
ing data. they use the em algorithm to split and
merge nonterminals using the latent states, and op-

timize the number of latent states for each nonter-
minal such that it maximizes the likelihood of a
training treebank. their re   ned grammar success-
fully splits nonterminals to various degrees to cap-
ture their complexity. we take the analogous step
with id106. we propose an algorithm
where we    rst compute    a on the training data
and then we optimize the number of latent states
for each nonterminal by optimizing the parse-
val metric (black et al., 1991) on a development
set.

our optimization algorithm appears in figure 2.
the input to the algorithm is training and develop-
ment data in the form of parse trees, a basic spec-
tral estimation algorithm s in its default setting,
an upper bound m on the number of latent states
that can be used for the different nonterminals and
a beam size k which gives a maximal queue size
for the beam. the algorithm aims to learn a func-
tion f that maps each nonterminal a to the number
of latent states. it initializes f by estimating a de-
fault grammar gs : (n , i, p, fs , n) using s and
setting f = fs. it then iterates over a     n , im-
proving f such that it optimizes the parseval
metric on the development set.

the state of the algorithm includes a queue that
consists of tuples of the form (s, j, f, t) where f
is an assignment of latent state numbers to each
nonterminal in the grammar, j is the index of a
nonterminal to be explored in the input nontermi-
nal list l, s is the f1 score on the development set
for a grammar that is estimated with f and t is a
tag that can either be coarse or re   ne.

the algorithm orders these tuples by s in the
queue, and iteratively dequeues elements from the
queue. then, depending on the label t, it either
makes a re   ned search for the number of latent
states for aj, or a more coarse search. as such,
the algorithm can be seen as a variant of a beam
search algorithm.

the search algorithm can be used with any
training algorithm for l-pid18s, including the al-
gorithms of cohen et al. (2013) and narayan and
cohen (2015). these methods, in their default set-
ting, use a function fs which maps each nonter-
minal a to a    xed number of latent states ma it
uses. in this case, s takes as input training data,
in the form of a treebank, decomposes into in-
side and outside trees at each node in each tree in
the training set; and reduces the dimensionality of
the inside and outside feature functions by running

lang.
sent.
tokens
lex. size
#nts
v sent.

n
i
a
r
t

e
d

tokens

t sent.

s
e
t

tokens

basque
7,577
96,565
25,136

112
948

13,893

946

11,477

french
14,759
443,113
27,470

222
1,235
38,820
2,541
75,216

german-n german-t hebrew
5,000
128,065
15,971

18,602
328,531
48,509

40,472
719,532
77,219

208
1,000
17,542
1,000
17,585

762
5,000
76,704
5,000
92,004

375
500

11,305

716

17,002

hungarian

8,146
170,221
40,775

112
1,051
30,010
1,009
19,913

korean
23,010
301,800
85,671

352
2,066
25,729
2,287
28,783

polish
6,578
66,814
21,793

198
821
8,391
822
8,336

swedish

5,000
76,332
14,097

148
494
9,339
666

10,675

table 1: statistics about the different datasets used in our experiments for the training (   train   ), development (   dev   ) and test
(   test   ) sets.    sent.    denotes the number of sentences in the dataset,    tokens    denotes the total number of words in the dataset,
   lex. size    denotes the vocabulary size in the training set and    #nts    denotes the number of nonterminals in the training set after
binarization.

svd on the cross-covariance matrix    a of the in-
side and the outside trees, for each nonterminal a.
cohen et al. (2013) estimate the parameters of the
l-pid18 up to a linear transformation using f (a)
non-zero singular values of    a, whereas narayan
and cohen (2015) use the feature representations
induced from the svd step to cluster instances of
nonterminal a in the training data into f (a) clus-
ters; these clusters are then treated as latent states
that are    observed.    finally, narayan and cohen
follow up with a simple frequency count maxi-
mum likelihood estimate to estimate the parame-
ters in the l-pid18 with these latent states.

an important point to make is that the learning
algorithms of narayan and cohen (2015) and co-
hen et al. (2013) are relatively fast,2 in comparison
to the em algorithm. they require only one iter-
ation over the data. in addition, the svd step of
s for these learning algorithms is computed just
once for a large m. the svd of a lower rank can
then be easily computed from that svd.

4 experiments

in this section, we describe our setup for parsing
experiments on a range of languages.

4.1 experimental setup
datasets we experiment with nine treebanks
consisting of eight different morphologically rich
languages: basque, french, german, hebrew,
hungarian, korean, polish and swedish. table 1
shows the statistics of 9 different treebanks with
their splits into training, development and test sets.
eight out of the nine datasets (basque, french,
german-t, hebrew, hungarian, korean, polish

2it has been documented in several papers that the fam-
ily of spectral estimation algorithms is faster than algorithms
such as em, not just for l-pid18s. see, for example, parikh
et al. (2012).

and swedish) are taken from the workshop on
statistical parsing of morphologically rich lan-
guages (spmrl; seddah et al., 2013). the ger-
man corpus in the spmrl workshop is taken from
the tiger corpus (german-t, brants et al., 2004).
we also experiment with another german cor-
pus, the negra corpus (german-n, skut et al.,
1997), in a standard evaluation split.3 words in
the spmrl datasets are annotated with their mor-
phological signatures, whereas the negra cor-
pus does not contain any morphological informa-
tion.

id174 and treatment of rare
words we convert all trees in the treebanks to a
binary form, train and run the parser in that form,
and then transform back the trees when doing eval-
uation using the parseval metric. in addition,
we collapse unary rules into unary chains, so that
our trees are fully binarized. the column    #nts   
in table 1 shows the number of nonterminals af-
ter binarization in the various treebanks. before
binarization, we also drop all functional informa-
tion from the nonterminals. we use    ne tags for
all languages except korean. this is in line with
bj  orkelund et al. (2013).4 for korean, there are
2,825 binarized nonterminals making it impracti-
cal to use our optimization algorithm, so we use
the coarse tags.

bj  orkelund et al. (2013) have shown that the
morphological signatures for rare words are useful
to improve the performance of the berkeley parser.

3we use the    rst 18,602 sentences as a training set, the
next 1,000 sentences as a development set and the last 1,000
sentences as a test set. this corresponds to an 80%-10%-10%
split of the treebank.

4in their experiments bj  orkelund et al. (2013) found that
   ne tags were not useful for basque also; they did not    nd a
proper explanation for that. in our experiments, however, we
found that    ne tags were useful for basque. to retrieve the
   ne tags, we concatenate coarse tags with their re   nement
feature (   azp   ) values.

in our preliminary experiments with na    ve spectral
estimation, we preprocess rare words in the train-
ing set in two ways: (i) we replace them with their
corresponding pos tags, and (ii) we replace them
with their corresponding pos+morphological sig-
natures. we follow bj  orkelund et al. (2013) and
consider a word to be rare if it occurs less than 20
times in the training data. we experimented both
with a version of the parser that does not ignore
and does ignore letter cases, and discovered that
the parser behaves better when case is not ignored.

spectral algorithms: subroutine choices the
latent state optimization algorithm will work
with either the id91 estimation algorithm of
narayan and cohen (2015) or the spectral algo-
rithm of cohen et al. (2013).
in our setup, we
   rst run the latent state optimization algorithm
with the id91 algorithm. we then run the
spectral algorithm once with the optimized f from
the id91 algorithm. we do that because the
id91 algorithm is signi   cantly faster to itera-
tively parse the development set, because it leads
to sparse estimates.

our optimization algorithm is sensitive to the
initialization of the number of latent states as-
signed to each nonterminals as it sequentially goes
through the list of nonterminals and chooses latent
state numbers for each nonterminal, keeping latent
state numbers for other nonterminals    xed. in our
setup, we start our search algorithm with the best
model from the id91 algorithm, controlling
for all hyperparameters; we tune f , the function
which maps each nonterminal to a    xed number
of latent states m, by running the vanilla version
with different values of m for different languages.
based on our preliminary experiments, we set m
to 4 for basque, hebrew, polish and swedish; 8
for german-n; 16 for german-t, hungarian and
korean; and 24 for french.

we use the same features for the spectral meth-
ods as in narayan and cohen (2015) for german-
n. for the spmrl datasets we do not use the head
features. these require linguistic understanding of
the datasets (because they require head rules for
propagating leaf nodes in the tree), and we discov-
ered that simple heuristics for constructing these
rules did not yield an increase in performance.

we use the kmeans function in matlab to
do the id91 for the spectral algorithm of
narayan and cohen (2015). we experimented
with several versions of id116, and discovered

that the version that works best in a set of prelimi-
nary experiments is hard id116.5

decoding and evaluation for ef   ciency, we
use a base pid18 without latent states to prune
marginals which receive a value less than 0.00005
in the id145 chart.
this is
just a bare-bones pid18 that is estimated using
id113 (with frequency
count). the parser takes part-of-speech tagged
sentences as input. we tag the german-n data us-
ing the turbo tagger (martins et al., 2010). for
the languages in the spmrl data we use the mar-
mot tagger of m  ueller et al. (2013) to jointly pre-
dict the pos and morphological tags.6 the parser
itself can assign different part-of-speech tags to
words to avoid parse failure. this is also particu-
larly important for constituency parsing with mor-
phologically rich languages. it helps mitigate the
problem of the taggers to assign correct tags when
long-distance dependencies are present.

for all results, we report

the f1 measure
of the parseval metric (black et al., 1991).
we use the evalb program7 with the parame-
ter    le collins.prm (collins, 1999) for the
german-n data and the spmrl parameter    le,
spmrl.prm, for the spmrl data (seddah et al.,
2013).

in this setup, the latent state optimization algo-
rithm terminates in few hours for all datasets ex-
cept french and german-t. the german-t data
has 762 nonterminals to tune over a large develop-
ment set consisting of 5,000 sentences, whereas,
the french data has a high average sentence length
of 31.43 in the development set.8

following narayan and cohen (2015), we fur-
ther improve our results by using multiple spec-
tral models where noise is added to the underlying
features in the training set before the estimation of

5to be more precise, we use the matlab function kmeans
while passing it the parameter    start   =   sample    to ran-
domly sample the initial centroid positions.
in our experi-
ments, we found that default initialization of centroids differs
in matlab14 (random) and in matlab15 (kmeans++). our es-
timation performs better with random initialization.

6see bj  orkelund et al. (2013) for the performance of the

marmot tagger on the spmrl datasets.

7we

use evalb from http://nlp.cs.nyu.
edu/evalb/ for
the german-n data and its modi-
   ed version from http://dokufarm.phil.hhu.
de/spmrl2014/doku.php?id=shared_task_
description for the spmrl datasets.

8to speed up tuning on the french data, we drop sentences
with length >46 from the development set, dropping its size
from 12,35 to 1,006.

lang.

basque

l

k van
b
rep
van (pos)
van (rep)
opt
p van
opt

c

s

bk multiple
cl multiple
hall et al.    14
crabb  e    15

69.2
84.3
69.8
78.6
81.2   
78.1
79.0
87.4
83.4
83.7
84.0

-

-
-

75.7

french german-n german-t hebrew hungarian korean
79.9
79.7
73.9
73.7
76.7
78.0
78.1   
82.5
79.9
79.4
80.9

81.7
82.7
78.3
78.8
81.7
82.0
82.9   
85.0
85.1
83.3
84.1

87.8
89.6
88.0
88.1
90.1
89.2
90.3   
90.5
90.6
88.1
90.7

83.9
89.1
81.3
84.7
87.2
87.7
87.8   
91.1
89.0
87.4
88.3

71.0
82.8
68.7
76.5
79.2
80.6
80.9   
84.6
80.8
81.9
83.1

77.8
77.6
79.0   

82.7

-
-

-

polish
84.1
87.1
90.3
90.4
92.0   
91.7
91.7
88.4
92.5
91.1
92.8

swedish

74.5
75.5
70.9
71.4
75.2
73.4
75.5   
79.5
78.3
76.0
77.9

table 2: results on the development datasets.    bk    makes use of the berkeley parser with its coarse-to-   ne mechanism to
optimize the number of latent states (petrov et al., 2006). for bk,    van    uses the vanilla treatment of rare words using signatures
de   ned by petrov et al. (2006), whereas    rep.    uses the morphological signatures instead.    cl    uses the algorithm of narayan
and cohen (2015) and    sp    uses the algorithm of cohen et al. (2013). in cl,    van (pos)    and    van (rep)    are vanilla estima-
tions (i.e., each nonterminal is mapped to    xed number of latent states) replacing rare words by pos or pos+morphological
signatures, respectively. the best of these two models is used with our optimization algorithm in    opt   . for sp,    van    uses
the best setting for unknown words as cl. best result in each column from the    rst seven rows is in bold. in addition, our
best performing models from rows 3-7 are marked with    .    bk multiple    shows the best results with the multiple models using
product-of-grammars procedure (petrov, 2010) and discriminative reranking (charniak and johnson, 2005).    cl multiple    gives
the results with multiple models generated using the noise induction and decoded using the hierarchical decoding (narayan and
cohen, 2015). bk results are not available on the development dataset for german-n. for others, we report bk results from
bj  orkelund et al. (2013). we also include results from hall et al. (2014) and crabb  e (2015).

lang.
bk
l van
c
opt
p van
opt

s

bk multiple
cl multiple
hall et al.    14

f&m    15
crabb  e    15

basque

74.7
79.6
81.4   
79.9
80.5
87.9
83.4
83.4
85.9
84.9

80.1
76.4
78.0
78.4
79.4   
84.5
82.7

french german-n german-t hebrew hungarian korean
80.4
74.3
75.6
78.7
79.1   
82.9
80.4
79.7
78.8
80.8

78.3
74.1
76.0
78.0
78.2   
81.3
80.4
78.4
78.7
79.3

87.0
86.3
87.2
87.8
89.0   
89.5
89.2
87.2
89.0
89.7

85.2
86.5
88.4
89.1
89.2   
91.9
89.9
88.3
88.2
90.1

78.6
76.5
78.4
80.3
80.0   
84.3
80.3
80.2
79.3
82.7

-
-
-

polish
86.8
90.5
91.2   
91.8
91.8
87.8
92.4
90.7
91.2
92.7

swedish

80.6
76.4
79.4
78.4
80.9   
84.9
82.8
82.0
82.8
83.2

table 3: results on the test datasets.    bk    denotes the best berkeley parser result reported by the shared task organizers
(seddah et al., 2013). for the german-n data, bk results are taken from petrov (2010).    cl van    shows the performance of the
best vanilla models from table 2 on the test set.    cl opt    and    sp opt    give the result of our algorithm on the test set. we also
include results from hall et al. (2014), crabb  e (2015) and fern  andez-gonz  alez and martins (2015).

each model.9 using the optimized f , we estimate
80 models for each of noise induction mechanisms
in narayan and cohen: dropout, gaussian (ad-
ditive) and gaussian (multiplicative). to decode
with multiple noisy models, we train the maxent
reranker of charniak and johnson (2005).10 hi-
erarchical decoding with    maximal tree coverage   
over maxent models, further improves our accu-
racy. see narayan and cohen (2015) for more de-

9we only use the algorithm of narayan and cohen (2015)
for the noisy model estimation. they have shown that de-
coding with noisy models performs better with their sparse
estimates than the dense estimates of cohen et al. (2013).

10implementation: https://github.com/bllip/
bllip-parser.
more speci   cally, we used the
programs extract-spfeatures, cvlm-lbfgs and
best-indices. extract-spfeatures uses head fea-
tures, we bypass this for the spmrl datasets by creating a
dummy heads.cc    le. cvlm-lbfgs was used with the
default hyperparameters from the make   le.

tails on the estimation of a diverse set of models,
and on decoding with them.

4.2 results

table 2 and table 3 give the results for the various
languages.11 our main focus is on comparing the
coarse-to-   ne berkeley parser (petrov et al., 2006)
to our method. however, for the sake of com-
pleteness, we also present results for other parsers,
such as parsers of hall et al. (2014), fern  andez-
gonz  alez and martins (2015) and crabb  e (2015).
in line with bj  orkelund et al. (2013), our pre-
liminary experiments with the treatment of rare
words suggest
that morphological features are
useful for all spmrl languages except french.

11see more in http://cohort.inf.ed.ac.uk/

lpid18/.

language
basque
french
german-n
german-t
hebrew
hungarian
korean
polish
swedish

preterminals

interminals

pi xi pi yi
419
311
715
839
567
425
890
1251
434
442
415
457
980
1077
311
252
191
284

div.
196
476
416
795
182
282
547
197
127

#nts pi xi pi yi
227
169
1279
108
578
109
1323
378
279
544
261
87
220
331
180
135
106
345

91
1145
323
1037
169
186
218
132
85

div.
152
906
361
738
393
129
150
86
266

all
#nts pi xi pi yi
646
31
1994
114
1145
99
2213
384
96
986
676
25
1200
21
491
63
42
629

402
1984
748
2288
603
643
1295
384
276

div.
348
1382
777
1533
575
411
697
283
393

#nts
200
222
208
762
375
112
352
198
148

table 4: a comparison of the number of latent states for the different nonterminals before and after running our latent state
number optimization algorithm. the index i ranges over preterminals and interminals, with xi denoting the number of latent
states for nonterminal i with the vanilla version of the estimation algorithm and yi denoting the number of latent states for
nonterminal i after running the optimization algorithm. the divergence    gure (   div.   ) is a calculation of pi |xi     yi|.

speci   cally, for basque, hungarian and korean,
improvements are signi   cantly large.

our results show that the optimization of the
number of latent states with the id91 and
spectral algorithms indeed improves these algo-
rithms performance, and these increases general-
ize to the test sets as well. this was a point
of concern, since the optimization algorithm goes
through many points in the hypothesis space of
parsing models, and identi   es one that behaves op-
timally on the development set     and as such it
could over   t to the development set. however, this
did not happen, and in some cases, the increase in
accuracy of the test set after running our optimiza-
tion algorithm is actually larger than the one for
the development set.

while the vanilla estimation algorithms (with-
out latent state optimization) lag behind the berke-
ley parser for many of the languages, once the
number of latent states is optimized, our parsing
models do better for basque, hebrew, hungar-
ian, korean, polish and swedish. for german-
t we perform close to the berkeley parser (78.2
vs. 78.3).
it is also interesting to compare the
id91 algorithm of narayan and cohen (2015)
to the spectral algorithm of cohen et al. (2013).
in the vanilla version, the spectral algorithm does
better in most cases. however, these differences
are narrowed, and in some cases, overcome, when
the number of latent states is optimized. decod-
ing with multiple models further improves our ac-
curacy. our    cl multiple    results lag behind    bk
multiple.    we believe this is the result of the need
of head features for the maxent models.12

12bj  orkelund et al. (2013) also use the maxent raranker
with multiple models of the berkeley parser, and in their case
also the performance after the raranking step is not always
signi   cantly better. see footnote 10 on how we create dummy
head-features for our maxent models.

our

results show that spectral

learning is
a viable alternative to the use of expectation-
maximization coarse-to-   ne techniques. as we
discuss later, further improvements have been in-
troduced to state-of-the-art parsers that are orthog-
onal to the use of a speci   c estimation algorithm.
some of them can be applied to our setup.

4.3 further analysis
in addition to the basic set of parsing results, we
also wanted to inspect the size of the parsing mod-
els when using the optimization algorithm in com-
parison to the vanilla models. table 4 gives this
analysis. in this table, we see that in most cases,
on average, the optimization algorithm chooses to
enlarge the number of latent states. however, for
german-t and korean, for example, the optimiza-
tion algorithm actually chooses a smaller model
than the original vanilla model.

we further

inspected the behavior of

the
optimization algorithm for the preterminals in
german-n, for which the optimal model chose (on
average) a larger number of latent states. table 5
describes this analysis. we see that in most cases,
the optimization algorithm chose to decrease the
number of latent states for the various pretermi-
nals, but in some cases signi   cantly increases the
number of latent states.13

our experiments dispel another    common wis-
dom    about spectral learning and training data
size.
it has been believed that spectral learning
do not behave very well when small amounts of
data are available (when compared to maximum
likelihood estimation algorithms such as em)    
however we see that our results do better than the

13interestingly, most of the punctuation symbols, such as
$   lrb   , $. and $,, drop their latent state number to a sig-
ni   cantly lower value indicating that their interactions with
other nonterminals in the tree are minimal.

preterminal
pwat
xy
np|nn
vminf
ptka
vp|vvinf
prelat
ap|adjd
appo
pws
kokom
vp|vvpp
pwav
apzr

freq.
64
135
88
177
162
409
94
178
89
361
800
844
689
134

b.
2
3
2
3
3
6
2
3
2
6
8
8
8
3

a.
2
1
1
5
1
2
1
1
2
1
37
5
1
2

preterminal
trunc
vapp
pds
avp|adv
fm
vvimp
koui
vainf
prels
card
ne
prf
pdat
proav

freq.
614
363
988
211
578
76
339
1,024
2,120
6,826
17,489
2,158
1,129
1,479

b.
8
6
8
4
8
2
5
8
8
8
8
8
8
8

a.
1
4
8
11
3
1
2
1
40
8
6
1
1
10

preterminal
pis
$*lrb*
adjd
kous
piat
np|pper
vvpp
pp|proav
vafin
ptkneg
ptkzu
vvizu
pposat
ptkvz

freq.
1,628
13,681
6,419
2,456
1,061
382
5,005
174
8,814
1,884
1,586
479
2,295
1,864

b.
8
8
8
8
8
6
8
3
8
8
8
7
8
8

a.
8
6
60
1
8
1
20
1
1
8
1
1
6
3

preterminal
kon
pper
$.
apprart
adja
appr
vvfin
$,
vvinf
art
adv
pidat
nn
vmfin

freq.
8,633
4,979
17,699
6,217
18,993
26,717
13,444
16,631
4,382
35,003
15,566
1,254
68,056
3,177

b.
8
8
8
8
8
8
8
8
8
8
8
8
8
8

a.
30
100
3
15
10
7
3
1
10
10
8
20
12
1

table 5: a comparison of the number of latent states for each preterminal for the german-n model, before (   b.   ) running the
latent state number optimization algorithm and after running it (   a.   ). note that some of the preterminals denote unary rules
that were collapsed (the nonterminals in the chain are separated by |). we do not show rare preterminals with b. and a. both
being 1.

berkeley parser for several languages with small
training datasets, such as basque, hebrew, pol-
ish and hungarian. the source of this common
wisdom is that ml estimators tend to be statis-
tically    ef   cient:    they extract more information
from the data than spectral learning algorithms do.
indeed, there is no reason to believe that spectral
algorithms are statistically ef   cient. however, it is
not clear that indeed for l-pid18s with the em
algorithm, the ml estimator is statistically ef   -
cient either. id113 is statistically ef   cient under
speci   c assumptions which are not clearly satis-
   ed with l-pid18 estimation. in addition, when
the model is    incorrect,    (i.e. when the data is
not sampled from l-pid18, as we would expect
from natural language treebank data), spectral al-
gorithms could yield better results because they
can mimic a higher order model. this can be
understood through id48s. when estimating an
id48 of a low order with data which was gener-
ated from a higher order model, em does quite
poorly. however, if the number of latent states
(and feature functions) is properly controlled with
spectral algorithms, a spectral algorithm would
learn a    product    id48, where the states in the
lower order model are the product of states of a
higher order.14

state-of-the-art parsers for the spmrl datasets
improve the berkeley parser in ways which are or-
thogonal to the use of the basic estimation algo-
rithm and the method for optimizing the number
of latent states. they include transformations of
the treebanks such as with unary rules (bj  orkelund
et al., 2013), a more careful handling of unknown
words and better use of morphological informa-

14for example, a trigram id48 can be reduced to a bigram
id48 where the states are products of the original trigram
id48.

tion such as decorating preterminals with such in-
formation (bj  orkelund et al., 2014; sz  ant  o and
farkas, 2014), with careful feature speci   cations
(hall et al., 2014) and head-annotations (crabb  e,
2015), and other techniques. some of these tech-
niques can be applied to our case.

5 conclusion

we demonstrated that a careful selection of the
number of latent states in a latent-variable pid18
with spectral estimation has a signi   cant effect
on the parsing accuracy of the l-pid18. we de-
scribed a search procedure to do this kind of
optimization, and described parsing results for
eight languages (with nine datasets). our results
demonstrate that when comparing the expectation-
maximization with coarse-to-   ne techniques to
our spectral algorithm with latent state optimiza-
tion, spectral learning performs better on six of the
datasets. our results are comparable to other state-
of-the-art results for these languages. using a di-
verse set of models to parse these datasets further
improves the results.

acknowledgments

the authors would like to thank david mcclosky
for his help with running the bllip parser and
his comments on the paper and also the three
anonymous reviewers for their helpful comments.
we also thank eugene charniak, dk choe and
geoff gordon for useful discussions.
finally,
thanks to djam  e seddah for providing us with
the spmrl datasets and to thomas m  uller and
anders bj  orkelund for providing us the marmot
models. this research was supported by an ep-
src grant (ep/l02411x/1) and an eu h2020
grant (688139/h2020-ict-2015; summa).

references
rapha  el bailly, amaury habrard, and franc  ois denis.
2010. a spectral approach for probabilistic gram-
matical id136 on trees. in proceedings of inter-
national conference on algorithmic learning the-
ory.

anders bj  orkelund,   ozlem c   etino  glu, rich  ard farkas,
thomas m  ueller, and wolfgang seeker.
2013.
(re)ranking meets morphosyntax: state-of-the-art
results from the spmrl 2013 shared task. in pro-
ceedings of the fourth workshop on statistical pars-
ing of morphologically-rich languages.

anders bj  orkelund,

  ozlem c   etino  glu, agnieszka
fale  nska, rich  ard farkas, thomas m  uller, wolf-
gang seeker, and zsolt sz  ant  o. 2014. introducing
the ims-wroc  aw-szeged-cis entry at the spmrl
2014 shared task: reranking and morphosyntax
meet unlabeled data.
in proceedings of the first
joint workshop on statistical parsing of morpho-
logically rich languages and syntactic analysis of
non-canonical languages.

ezra w. black, steven abney, daniel p. flickinger,
claudia gdaniec, ralph grishman, philip harri-
son, donald hindle, robert j. p. ingria, freder-
ick jelinek, judith l. klavans, mark y. liberman,
mitchell p. marcus, salim roukos, beatrice san-
torini, and tomek strzalkowski. 1991. a procedure
for quantitatively comparing the syntactic coverage
of english grammars.
in proceedings of darpa
workshop on speech and natural language.

sabine brants, stefanie dipper, peter eisenberg, sil-
via hansen-schirra, esther k  onig, wolfgang lezius,
christian rohrer, george smith, and hans uszko-
reit. 2004. tiger: linguistic interpretation of a
german corpus. research on language and com-
putation, 2(4):597   620.

eugene charniak and mark johnson. 2005. coarse-
to-   ne n-best parsing and maxent discriminative
reranking. in proceedings of acl.

shay b. cohen and michael collins. 2014. a prov-
ably correct learning algorithm for latent-variable
pid18s. in proceedings of acl.

shay b. cohen, karl stratos, michael collins, dean f.
foster, and lyle ungar. 2012. spectral learning of
latent-variable pid18s. in proceedings of acl.

shay b. cohen, karl stratos, michael collins, dean p.
foster, and lyle ungar. 2013. experiments with
spectral learning of latent-variable pid18s. in pro-
ceedings of naacl.

michael collins. 1999. head-driven statistical mod-
els for natural language parsing. ph.d. thesis,
university of pennsylvania.

benoit crabb  e. 2015. multilingual discriminative lex-
icalized phrase structure parsing. in proceedings of
emnlp.

daniel fern  andez-gonz  alez and andr  e f. t. martins.
2015. parsing as reduction. in proceedings of acl-
ijcnlp.

david hall, greg durrett, and dan klein. 2014. less

grammar, more features. in proceedings of acl.

daniel hsu, sham m. kakade, and tong zhang. 2009.
a spectral algorithm for learning hidden markov
models. in proceedings of colt.

andr  e f. t. martins, noah a. smith, eric p. xing,
m  ario a. t. figueiredo, and pedro m. q. aguiar.
2010. turboparsers: id33 by ap-
proximate variational id136. in proceedings of
emnlp.

takuya matsuzaki, yusuke miyao, and junichi tsujii.
2005. probabilistic id18 with latent annotations. in
proceedings of acl.

thomas m  ueller, helmut schmid,

and hinrich
sch  utze. 2013. ef   cient higher-order crfs for
morphological tagging. in proceedings of emnlp.

shashi narayan and shay b. cohen. 2015. diversity
in spectral learning for natural language parsing. in
proceedings of emnlp.

ankur p. parikh, le song, mariya ishteva, gabi
teodoru, and eric p. xing. 2012. a spectral al-
gorithm for latent junction trees. in proceedings of
the twenty-eighth conference on uncertainty in ar-
ti   cial intelligence.

slav petrov, leon barrett, romain thibaux, and dan
2006. learning accurate, compact, and
in proceedings of

klein.
interpretable tree annotation.
coling-acl.

slav petrov. 2010. products of random latent variable

grammars. in proceedings of hlt-naacl.

detlef prescher.

2005. head-driven pid18s with

latent-head statistics. in proceedings of iwpt.

guillaume rabusseau, borja balle, and shay b. cohen.
2016. low-rank approximation of weighted tree au-
tomata.
in proceedings of the 19th international
conference on arti   cial intelligence and statistics.

djam  e seddah, reut tsarfaty, sandra k  ubler, marie
candito, jinho d. choi, rich  ard farkas, jen-
nifer foster, iakes goenaga, koldo gojenola gal-
letebeitia, yoav goldberg, spence green, nizar
habash, marco kuhlmann, wolfgang maier, joakim
nivre, adam przepi  orkowski, ryan roth, wolfgang
seeker, yannick versley, veronika vincze, marcin
woli  nski, alina wr  oblewska, and eric villemonte
de la cl  ergerie. 2013. overview of the spmrl
2013 shared task: a cross-framework evaluation of
parsing morphologically rich languages.
in pro-
ceedings of the fourth workshop on statistical pars-
ing of morphologically-rich languages.

wojciech skut, brigitte krenn, thorsten brants, and
hans uszkoreit. 1997. an annotation scheme for
free word order languages. in proceedings of anlp.

zsolt sz  ant  o and rich  ard farkas. 2014. special tech-
niques for constituent parsing of morphologically
rich languages. in proceedings of eacl.

