relation schema induction using tensor factorization with side

information

madhav nimishakavi

indian institute of science

bangalore, india

uday singh saini

partha talukdar

indian institute of science

indian institute of science

bangalore, india

bangalore, india

6
1
0
2

 

v
o
n
6
1

 

 
 
]

r

i
.
s
c
[
 
 

3
v
7
2
2
4
0

.

5
0
6
1
:
v
i
x
r
a

madhav@csa.iisc.ernet.in

uday.s.saini@gmail.com

ppt@cds.iisc.ac.in

abstract

given a set of documents from a speci   c do-
main (e.g., medical research journals), how do
we automatically build a id13
(kg) for that domain? automatic identi   ca-
tion of relations and their schemas, i.e., type
signature of arguments of relations (e.g., un-
dergo(patient, surgery)), is an important    rst
step towards this goal. we refer to this prob-
lem as relation schema induction (rsi). in
this paper, we propose schema induction us-
ing coupled tensor factorization (sictf), a
novel tensor factorization method for relation
schema induction. sictf factorizes open
information extraction (openie) triples ex-
tracted from a domain corpus along with ad-
ditional side information in a principled way
to induce relation schemas. to the best of
our knowledge,
this is the    rst application
of tensor factorization for the rsi problem.
through extensive experiments on multiple
real-world datasets, we    nd that sictf is not
only more accurate than state-of-the-art base-
lines, but also signi   cantly faster (about 14x
faster).

introduction

1
over the last few years, several techniques to build
id13s (kgs) from large unstructured
text corpus have been proposed, examples include
nell (mitchell et al., 2015) and google knowl-
edge vault (dong et al., 2014). such kgs con-
sist of millions of entities (e.g., oslo, norway, etc.),

their types (e.g., isa(oslo, city), isa(norway, coun-
try)), and relationships among them (e.g., citylo-
catedincountry(oslo, norway)). these kg con-
struction techniques are called ontology-guided as
they require as input list of relations, their schemas
(i.e., their type signatures, e.g., citylocatedincoun-
try(city, country)), and seed instances of each such
relation. listing of such relations and their schemas
are usually prepared by human domain experts.

the reliance on domain expertise poses signif-
icant challenges when such ontology-guided kg
construction techniques are applied to domains
where domain experts are either not available or are
too expensive to employ. even when such a domain
expert may be available for a limited time, she may
be able to provide only a partial listing of relations
and their schemas relevant to that particular domain.
moreover, this expert-mediated model is not scal-
able when new data in the domain becomes avail-
able, bringing with it potential new relations of in-
terest.
in order to overcome these challenges, we
need automatic techniques which can discover rela-
tions and their schemas from unstructured text data
itself, without requiring extensive human input. we
refer to this problem as relation schema induction
(rsi).

in contrast to ontology-guided kg construction
techniques mentioned above, open information ex-
traction (openie) techniques (etzioni et al., 2011)
aim to extract surface-level triples from unstructured
text. such openie triples may provide a suitable
starting point for the rsi problem. in fact, kb-lda,

typed rescal (chang et
al., 2014a)
universal schema (singh
et al., 2015)
kb-lda
attias and cohen, 2015)
sictf (this paper)

(movshovitz-

target task

embedding

interpretable
latent factors?
no

can induce relation
schema?
no

can use np side
info?
yes

can use relation
side info?
no

link prediction

ontology induc-
tion
schema
tion

induc-

no

yes

yes

no

yes

yes

no

yes

yes

no

no

yes

table 1: comparison among sictf (this paper) and other related methods. kb-lda is the most related prior method which is
extensively compared against sictf in section 4

a id96-based method for inducing an on-
tology from svo (subject-verb-object) triples was
recently proposed in (movshovitz-attias and cohen,
2015). we note that ontology induction (velardi et
al., 2013) is a more general problem than rsi, as we
are primarily interested in identifying categories and
relations from a domain corpus, and not necessar-
ily any hierarchy over them. nonetheless, kb-lda
maybe used for the rsi problem and we use it as a
representative of the state-of-the-art of this area.

instead of a id96 approach, we take a
tensor factorization-based approach for rsi in this
paper. tensors are a higher order generalization
of matrices and they provide a natural way to rep-
resent openie triples. applying tensor factoriza-
tion methods over openie triples to identify relation
schemas is a natural approach, but one that has not
been explored so far. also, a tensor factorization-
based approach presents a    exible and principled
way to incorporate various types of side informa-
tion. moreover, as we shall see in section 4, com-
pared to state-of-the-art baselines such as kb-lda,
tensor factorization-based approach results in better
and faster solution for the rsi problem. in this pa-
per, we make the following contributions:

    we present schema induction using coupled
tensor factorization (sictf), a novel and
principled tensor factorization method which
jointly factorizes a tensor constructed out of
openie triples extracted from a domain corpus,
along with various types of additional side in-
formation for relation schema induction.

baselines, but also much faster. for example,
sictf achieves 14x speedup over kb-lda
(movshovitz-attias and cohen, 2015).

    we have made the data and code available 1.

2 related work

schema induction: properties of sictf and other
related methods are summarized in table 12. a
method for inducing (binary) relations and the cat-
egories they connect was proposed by (mohamed et
al., 2011). however, in that work, categories and
their instances were known a-priori.
in contrast,
in case of sictf, both categories and relations are
to be induced. a method for event schema induc-
tion, the task of learning high-level representations
of complex events and their entity roles from unla-
beled text, was proposed in (chambers, 2013). this
gives the schemas of slots per event, but our goal
is to    nd schemas of relations. (chen et al., 2013)
and (chen et al., 2015) deal with the problem of
   nding semantic slots for unsupervised spoken lan-
guage understanding, but we are interested in    nd-
ing schemas of relations relevant for a given domain.
methods for link prediction in the universal schema
setting using matrix and a combination of matrix
and tensor factorization are proposed in (riedel et
al., 2013) and (singh et al., 2015), respectively. in-
stead of link prediction where relation schemas are
assumed to be given, sictf focuses on discovering
such relation schemas. moreover, in contrast to such

    we compare sictf against state-of-the-art
baseline on various real-world datasets from
diverse domains. we observe that sictf is
not only signi   cantly more accurate than such

1https://github.com/malllabiisc/sictf
2please note that not all methods mentioned in the table are
directly comparable with sictf, the table only illustrates the
differences. kb-lda is the only method which is directly com-
parable.

figure 1: relation schema induction (rsi) by sictf, the proposed method. first, a tensor (x) is constructed to represent openie
triples extracted from a domain corpus. noun phrase side information in the form of (noun phrase, hypernym), and relation-relation
similarity side information are separately calculated and stored in two separate matrices (w and s, respectively). sictf then
performs coupled factorization of the tensor and the two side information matrices to identify relation schemas which are stored in
the core tensor (r) in the output. please see section 3 for details.

methods which assume access to existing kgs, the
setting in this paper is unsupervised.

tensor factorization: due to their    exibility
of representation and effectiveness, tensor factor-
ization methods have seen increased application in
id13 (kg) related problems over the
last few years. methods for decomposing ontolog-
ical kgs such as yago (suchanek et al., 2007)
were proposed in (nickel et al., 2012; chang et al.,
2014b; chang et al., 2014a). in these cases, rela-
tion schemas are known in advance, while we are
interested in inducing such relation schemas from
unstructured text. a parafac (harshman, 1970)
based method for jointly factorizing a matrix and
tensor for data fusion was proposed in (acar et al.,
2013). in such cases, the matrix is used to provide
auxiliary information (narita et al., 2012; erdos and
miettinen, 2013). similar parafac-based ideas
are explored in rubik (wang et al., 2015) to fac-
torize structured electronic health records. in con-
trast to such structured data sources, sictf aims
at inducing relation schemas from unstructured text
data. propstore, a tensor-based model for distribu-
tional semantics, a problem different from rsi, was
presented in (goyal et al., 2013). even though cou-
pled factorization of tensor and matrices constructed
out of unstructured text corpus provide a natural and

plausible approach for the rsi problem, they have
not yet been explored     we    ll this gap in this paper.
ontology induction: relation schema induc-
tion can be considered a sub problem of ontol-
ogy induction (velardi et al., 2013).
instead of
building a full-   edged hierarchy over categories
and relations as in ontology induction, we are par-
ticularly interested in    nding relations and their
schemas from unstructured text corpus. we consider
kb-lda3 (movshovitz-attias and cohen, 2015), a
topic-modeling based approach for ontology induc-
tion, as a representative of this area. among all prior
work, kb-lda is most related to sictf. while
both kb-lda and sictf make use of noun phrase
side information, sictf is also able to exploit rela-
tional side information in a principled manner. in
section 4, through experiments on multiple real-
world datasets, we observe that sictf is not only
more accurate than kb-lda but also signi   cantly
faster with a speedup of 14x.

a method for canonicalizing noun and relation
phrases in openie triples was recently proposed in
(gal  arraga et al., 2014). the main focus of this ap-
proach is to cluster lexical variants of a single entity
or relation. this is not directly relevant for rsi, as

3in this paper, whenever we refer to kb-lda, we only refer

to the part of it that learns relations from unstructured data.

we are interested in grouping multiple entities of the
same type into one cluster, and use that to induce
relation schema.

3 our approach: schema induction using
coupled tensor factorization (sictf)

+

3.1 overview
sictf poses the relation schema induction problem
as a coupled factorization of a tensor along with ma-
trices containing relevant side information. over-
all architecture of the sictf system is presented
in figure 1. first, a tensor x     rn  n  m
is con-
structed to store openie triples and their scores ex-
tracted from the text corpus4. here, n and m rep-
resent the number of nps and relation phrases, re-
spectively. following (movshovitz-attias and co-
hen, 2015), sictf makes use of noun phrase (np)
side information in the form of (noun phrase, hyper-
nym). additionally, sictf also exploits relation-
relation similarity side information. these two side
information are stored in matrices w     {0, 1}n  h
and s     {0, 1}m  m, where h is the number of hy-
pernyms extracted from the corpus. sictf then per-
forms collective non-negative factorization over x,
w , and s to output matrix a     rn  c
+ and the core
tensor r     rc  c  m
. each row in a corresponds
to an np, while each column corresponds to an in-
duced category (latent factor). for brevity, we shall
refer to the induced category corresponding to the
qth column of a as aq. each entry apq in the out-
put matrix provides a membership score for np p
in induced category aq. please note that each in-
duced category is represented using the nps partic-
ipating in it, with the nps ranked by their member-
ship scores in the induced category.
in figure 1,
a2 = [(john, 0.9), (sam, 0.8), . . .] is an induced cat-
egory.

+

each slice of the core tensor r is a matrix which
corresponds to a speci   c relation, e.g., the matrix
rundergo highlighted in figure 1 corresponds to the
relation undergo. each cell in this matrix corre-
sponds to an induced schema connecting two in-
duced categories (two columns of the a matrix),
with the cell value representing model   s score of
the induced schema. for example, in figure 1,
undergo(a2, a4) is an induced relation schema with

4r+ is the set of non-negative reals.

medline
(hypertension, disease), (hypertension, state), (hypertension,
disorder) , (neutrophil, blood element), (neutrophil, effector
cell), (neutrophil, cell type)
stackover   ow
(image, resource), (image, content), (image,    le), (perl, lan-
guage), (perl, script), (perl, programs)

table 2: noun phrase (np) side information in the form of
(noun phrase, hypernym) pairs extracted using hearst patterns
from two different datasets. please see section 3.2 for details.

medline
(evaluate, analyze), (evaluate,
examine), (indicate, con   rm),
(indicate, suggest)

stackover   ow
(provides,
vides, offers),
(allows, enables)

con   rms),
(allows,

(pro-
lets),

table 3: examples of relation similarity side information in the
form of automatically identi   ed similar relation pairs. please
see section 3.2 for details.

score 0.8 involving relation undergo and induced
categories a2 and a4.

in section 3.2, we present details of the side in-
formation used by sictf, and then in section 3.3
present details of the optimization problem solved
by sictf.

3.2 side information

    noun phrase side information: through this
type of side information, we would like to cap-
ture type information of as many noun phrases
(nps) as possible. we apply hearst patterns
(hearst, 1992), e.g.,    <hypernym> such as
<np>   , over the corpus to extract such (np,
hypernym) pairs. please note that neither hy-
pernyms nor nps are pre-speci   ed, and they are
all extracted from the data by the patterns. ex-
amples of a few such pairs extracted from two
different datasets are shown in table 2. these
extracted tuples are stored in a matrix wn  h
whose rows correspond to nps and columns
correspond to extracted hypernyms. we de   ne,

(cid:26)

wij =

1,
0,

if npi belongs to hypernymj
otherwise

.

please note that we don   t expect w to be a fully
speci   ed matrix, i.e., we don   t assume that we
know all possible hypernyms for a given np.

    relation side information: in addition to the
side information involving nps, we would also

like to take prior knowledge about textual rela-
tions into account during factorization. for ex-
ample, if we know two relations to be similar to
one another, then we also expect their induced
schemas to be similar as well. consider the
following sentences    mary purchased a stuffed
animal toy.    and    janet bought a toy car for
her son.   . from these we can say that both re-
lations purchase and buy have the schema (per-
son, item). even if one of these relations is
more abundant than the other in the corpus, we
still want to learn similar schemata for both the
relations. as mentioned before, s     rm  m
is
the relation similarity matrix, where m is the
number of textual relations. we de   ne,

+

sij =

1,
0, otherwise

if similarity(reli, relj)       

(cid:26)

where    is a threshold5. for the experiments
in this paper, we use cosine similarity over
id97 (mikolov et al., 2013) vector repre-
sentations of the relational phrases. examples
of a few similar relation pairs are shown in ta-
ble 3.

3.3 sictf model details
sictf performs coupled non-negative factorization
of the input triple tensor xn  n  m along with the
two side information matrices wn  h and sm  m by
solving the following optimization problem.

m(cid:88)

min
a,v,r

where,

f (xk, a, rk) + fnp(w, a, v ) + frel(s, r)

k=1

(1)

f (xk, a, rk) =(cid:107) x:,:,k     ar:,:,kat (cid:107)2
fnp(w, a, v ) =   np (cid:107) w     av (cid:107)2
+   v (cid:107) v (cid:107)2

f

f +  r (cid:107) r:,:,k (cid:107)2

f

f +  a (cid:107) a (cid:107)2

f

m(cid:88)

m(cid:88)

frel(s, r) =   rel
ai,j     0,vj,r     0, rp,q,k     0

j=1

i=1

sij (cid:107) r:,:,i     r:,:,j (cid:107)2

f

in the objective above, the    rst term f (xk, a, rk)
minimizes reconstruction error for the kth relation,
with additional id173 on the r:,:,k matrix6.
the second term, fnp(w, a, v ), factorizes the np
side information matrix wn  h into two matrices
an  c and vc  h, where c is the number of induced
categories. we also enforce a to be non-negative.
typically, we require c (cid:28) h to get a lower dimen-
sional embedding of each np (rows of a). finally,
the third term frel(s, r) enforces the requirement
that two similar relations as given by the matrix s
should have similar signatures (given by the corre-
sponding r matrix). additionally, we require v
and r to be non-negative, as marked by the (non-
negative) constraints. in this objective,   r,   np,   a,
  v , and   rel are all hyper-parameters.

we derive non-negative multiplicative updates for
a, rk and v following the rules proposed in (lee
and seung, 2000), which has the following general
form:

          c(  )   

   c(  )+

     i

        

     i

  i =   i

     i

and    c(  )   

here c(  ) represents the cost function of the non-
negative variables    and    c(  )   
are the
negative and positive parts of the derivative of c(  )
(m  rup et al., 2008). (lee and seung, 2000) proved
that for    = 1, the cost function c(  ) monotonically
decreases with the multiplicative updates 7. c(  ) for
sictf is given in equation (1). the above procedure
will give the following updates:

     i

(cid:80)

k

a     a    

(cid:88)

  b =

k

rk     rk    

(xkart

k + x t

k ark) +   npw v t

a(   b +   ai +   npv v t )

(rkat art

k + rt

k at ark)

m(cid:80)

j=1

at xka + 2   rel

rjskj

at arkat a +   d

m(cid:88)

  d = 2   rel rk

skj +   rrk

    1     i     n, 1     r     h,
1     j, p, q     c, 1     k     m

(non negative)

v     v    

j=1
  npat w

  npat av +   v v

5for the experiments in this paper, we set    = 0.7, a
relatively high value, to focus on highly similar relations and
thereby justifying the binary s matrix.

6for brevity, we also refer to r:,:,k as rk, and similarly

x:,:,k as xk

7we also use    = 1.

dataset
medline
stackover   ow
table 4: datasets used in the experiments.

# docs
50,216
5.5m

2,499
37,439

# triples

in the equations above,     is the hadamard or
element-wise product8. in all our experiments, we
   nd the iterative updates above to converge in about
10-20 iterations.

4 experiments
in this section, we evaluate performance of differ-
ent methods on the relation schema induction (rsi)
task. speci   cally, we address the following ques-
tions.

    which method is most effective on the rsi

task? (section 4.3.1)

    how important are the additional side informa-

tion for rsi? (section 4.3.2)

    what is the importance of non-negativity in
rsi with tensor factorization? (section 4.3.3)

4.1 experimental setup
datasets: we used two datasets for the experi-
ments in this paper, they are summarized in table 4.
for medline dataset, we used stanford corenlp
(manning et al., 2014) for coreference resolution
and open ie v4.09 for triple extraction. triples with
noun phrases that have hypernym information were
retained. we obtained the stackover   ow triples di-
rectly from the authors of (movshovitz-attias and
cohen, 2015), which were also prepared using a
very similar process. in both datasets, we use cor-
pus frequency of triples for constructing the tensor.
side information: seven hearst patterns such
as    <hypernym> such as <np>   ,    <np> or
other <hypernym>    etc., given in (hearst, 1992)
were used to extract np side information from the
medline documents. np side information for the
stackover   ow dataset was obtained from the au-
thors of (movshovitz-attias and cohen, 2015).

as described in section 3, id97 embeddings
of the relation phrases were used to extract relation-
similarity based side-information. this was done for

8(a     b)i,j = ai,j    bi,j
9open ie v4.0: http://knowitall.github.io/openie/

both datasets. cosine similarity threshold of    = 0.7
was used for the experiments in the paper.

samples of side information used in the experi-
ments are shown in table 2 and table 3. a total
of 2067 unique np-hypernym pairs were extracted
from medline data and 16,639 were from stack-
over   ow data. 25 unique pairs of relation phrases
out of 1172 were found to be similar in medline
data, whereas 280 unique pairs of relation phrases
out of approximately 3200 were found similar in
stackover   ow data.

hyperparameters were tuned using grid search
and the set which gives minimum reconstruction er-
ror for both x and w was chosen. we set   np =
  rel = 100 for stackover   ow, and   np = 0.05 and
  rel = 0.001 for medline and we use c = 50 for our
experiments. please note that our setting is unsuper-
vised, and hence there is no separate train, dev and
test sets.

4.2 evaluation protocol
in this section, we shall describe how the induced
schemas are presented to human annotators and how
   nal accuracies are calculated.
in factorizations
produced by sictf and other ablated versions of
sictf, we    rst select a few top relations with best
reconstruction score. the schemas induced for each
selected relation k is represented by the matrix slice
rk of the core tensor obtained after factorization
(see section 3). from each such matrix, we iden-
tify the indices (i, j) with highest values. the in-
dices i and j select columns of the matrix a. a
few top ranking nps from the columns ai and aj
along with the relation k are presented to the hu-
man annotator, who then evaluates whether the tuple
relationk(ai, aj) constitutes a valid schema for re-
lation k. examples of a few relation schemas in-
duced by sictf are presented in table 5. a human
annotator would see the    rst and second columns of
this table and then offer judgment as indicated in
the third column of the table. all such judgments
across all top-reconstructed relations are aggregated
to get the    nal accuracy score. this evaluation pro-
tocol was also used in (movshovitz-attias and co-
hen, 2015) to measure learned relation accuracy.

all evaluations were blind, i.e., the annotators
were not aware of the method that generated the
output they were evaluating. moreover, the anno-

relation schema

top 3 nps in induced categories which were presented to annotators

annotator judgment

stackove   ow

clicks(a0, a1)

refreshes(a19, a13)

can parse(a41, a17)

a0: users, client, person
a1: link, image, item
a19: browser, window, tab
a13: page, activity, app
a41: access, permission, ability
a17: image    le, header    le, zip    le

suffer from(a38, a40)

have undergo(a3, a37)

have discontinue(a41, a20)

medline
a38: patient,    rst patient, anesthetized patient
a40: viral disease, renal disease, von recklin ghausen   s disease
a3:    fth patient, third patient, sixth patient
a37: initial liver biopsy, gun biopsy, lymph node biopsy
a41: patient, group, no patient
a20: endemic area, this area,    ber area

valid

valid

invalid

valid

valid

invalid

table 5: examples of relation schemas induced by sictf from the stackover   ow and medline datasets. top nps from each of
the induced categories, along with human judgment of the induced schema are also shown. see section 4.3.1 for more details.

(a)

(b)

figure 2: (a) relation schema induction (rsi) accuracies of different methods on the two datasets. sictf, our proposed method,
signi   cantly outperforms state-of-the-art method kblda. this is the main result of the paper. results for kb-lda on stackove-
   ow are directly taken from the paper. please see section 4.3.1 for details. (b) runtime comparison between kb-lda and sictf.
we observe that sictf results in 14x speedup over kb-lda. please see section 4.3.1 (runtime comparison) for details.

tators are experts in software domain and has high-
school level knowledge in medical domain. though
recall is a desirable statistic to measure, it is very
challenging to calculate it in our setting due to the
non-availability of relation schema annotated text on
large scale.

4.3 results
4.3.1 effectiveness of sictf

experimental results comparing performance of
various methods on the rsi task in the two datasets
are presented in figure 2(a). rsi accuracy is cal-
culated based on the evaluation protocol described
in section 4.2. performance number of kb-lda
for stackove   ow dataset is taken directly from the
(movshovitz-attias and cohen, 2015) paper, we
used our implementation of kb-lda for the med-
line dataset. annotation accuracies from two an-
notators were averaged to get the    nal accuracy.

from figure 2(a), we observe that sictf outper-
forms kb-lda on the rsi task. please note that
the inter-annotator agreement for sictf is 88% and
97% for medline and stackover   ow datasets re-
spectively. this is the main result of the paper.

in addition to kb-lda, we also compared sictf
with parafac, a standard tensor factorization
method. parafac induced extremely poor and
small number of relation schemas, and hence we
didn   t consider it any further.

runtime comparison: runtimes of sictf and
kb-lda over both datasets are compared in fig-
ure 2(b). from this    gure, we    nd that sictf is
able to achieve a 14x speedup on average over kb-
lda10. in other words, sictf is not only able to

10runtime of kb-lda over the stackover   ow dataset was
obtained from the authors of (movshovitz-attias and cohen,
2015) through personal communication. our own implementa-
tion also resulted in similar runtime over this dataset.

ablation

sictf

sictf (  rel = 0)
sictf (  np = 0)

sictf (  rel=0,   np = 0)

sictf (  rel=0,   np = 0, and no non-negativity constraints )

medline

stackover   ow

a1
0.64
0.60
0.46
0.46
0.14

a2 avg a1
0.96
0.64
0.83
0.56
0.40
0.89
0.84
0.50
0.10
0.20

0.64
0.58
0.43
0.48
0.12

a2 avg
0.94
0.92
0.77
0.70
0.90
0.90
0.59
0.33
0.14
0.17

table 6: rsi accuracy comparison of sictf with its ablated versions when no relation side information is used (  rel = 0), when
no np side information is used (  np = 0), when no side information of any kind is used (  rel = 0,   np = 0), and when additionally
there are no non-negative constraints. from this, we observe that additional side information improves performance, validating one
of the central thesis of this paper. please see section 4.3.2 and section 4.3.3 for details.

induce better relation schemas, but also do so at a
signi   cantly faster speed.

4.3.2

importance of side information

one of the central hypothesis of our approach is
that coupled factorization through additional side in-
formation should result in better relation schema in-
duction. in order to evaluate this thesis further, we
compare performance of sictf with its ablated ver-
sions: (1) sictf (  rel = 0), which corresponds to
the setting when no relation side information is used,
(2) sictf (  np = 0), which corresponds to the set-
ting when no noun phrases side information is used,
and (3) sictf (  rel = 0,   np = 0), which corre-
sponds to the setting when no side information of
any kind is used. hyperparameters are separately
tuned for the variants of sictf. results are pre-
sented in the    rst four rows of table 6. from this,
we observe that additional coupling through the side
information signi   cantly helps improve sictf per-
formance. this further validates the central thesis of
our paper.

4.3.3

importance of non-negativity on
relation schema induction

in the last row of table 6, we also present an
ablated version of sictf when no side informa-
tion no non-negativity constraints are used. com-
paring the last two rows of this table, we observe
that non-negativity constraints over the a matrix
and core tensor r result in signi   cant improvement
in performance. we note that the last row in ta-
ble 6 is equivalent to rescal (nickel et al., 2011)
and the fourth row is equivalent to non-negative
rescal (krompa   et al., 2013), two tensor factor-

ization techniques. we also note that none of these
tensor factorization techniques have been previously
used for the relation schema induction problem.

the reason for this improved performance may be
explained by the fact that absence of non-negativity
constraint results in an under constrained factoriza-
tion problem where the model often overgenerates
incorrect triples, and then compensates for this over-
generation by using negative latent factor weights.
in contrast, imposition of non-negativity constraints
restricts the model further forcing it to commit to
speci   c semantics of the latent factors in a. this
improved interpretability also results in better rsi
accuracy as we have seen above. similar bene   ts of
non-negativity on interpretability have also been ob-
served in id105 (murphy et al., 2012).

5 conclusion
relation schema induction (rsi) is an important
   rst step towards building a id13
(kg) out of text corpus from a given domain.
while human domain experts have traditionally pre-
pared listing of relations and their schemas, this
expert-mediated model poses signi   cant challenges
in terms of scalability and coverage.
in order to
overcome these challenges, in this paper, we present
sictf, a novel non-negative coupled tensor ma-
trix factorization method for relation schema in-
duction.
sictf is    exible enough to incorpo-
rate various types of side information during fac-
torization. through extensive experiments on real-
world datasets, we    nd that sictf is not only more
accurate but also signi   cantly faster (about 11.8x
speedup) compared to state-of-the-art baselines. as
part of future work, we hope to analyze cntf and

its optimization further, assign labels to induced cat-
egories, and also apply the model to more domains.
we hope to make all code and datasets used in the
paper publicly available upon publication of the pa-
per.

acknowledgement

thanks to the members of mall lab, iisc who
read our drafts and gave valuable feedback and we
also thank the reviewers for their constructive re-
views. this research has been supported in part
by bosch engineering and business solutions and
google.

references

[acar et al.2013] evrim acar, morten arendt rasmussen,
francesco savorani, tormod ns, and rasmus bro.
2013. understanding data fusion within the frame-
work of coupled matrix and tensor factorizations.
chemometrics and intelligent laboratory systems,
129(complete):53   63.

[chambers2013] nathanael chambers.

event
schema induction with a probabilistic entity-driven
model. in emnlp, pages 1797   1807. acl.

2013.

[chang et al.2014a] kai-wei chang, wen tau yih, bis-
han yang, and christopher meek. 2014a. typed ten-
sor decomposition of knowledge bases for relation ex-
in proceedings of the 2014 conference on
traction.
empirical methods in natural language processing.
acl association for computational linguistics, oc-
tober.

[chang et al.2014b] kai-wei chang, wen-tau yih, bis-
han yang, and christopher meek. 2014b. typed ten-
sor decomposition of knowledge bases for relation ex-
in proceedings of the 2014 conference on
traction.
empirical methods in natural language processing
(emnlp), pages 1568   1579.

[chen et al.2013] yun-nung chen, william y. wang, and
alexander i. rudnicky. 2013. unsupervised induc-
tion and    lling of semantic slots for spoken dialogue
systems using frame-id29. in 2013 ieee
workshop on automatic id103 and un-
derstanding (asru), pages 120   125. ieee.

[chen et al.2015] yun-nung chen, william yang wang,
anatole gershman, and alexander i. rudnicky. 2015.
id105 with id13 propaga-
tion for unsupervised spoken language understanding.
in acl (1), pages 483   494. the association for com-
puter linguistics.

[dong et al.2014] xin dong,

evgeniy gabrilovich,
geremy heitz, wilko horn, ni lao, kevin murphy,
thomas strohmann, shaohua sun, and wei zhang.
2014. knowledge vault: a web-scale approach to
in proceedings of
probabilistic knowledge fusion.
the 20th acm sigkdd international conference
on knowledge discovery and data mining, pages
601   610. acm.

[erdos and miettinen2013] dora erdos and pauli miet-
2013. discovering facts with boolean ten-
tinen.
in proceedings of the
sor tucker decomposition.
22nd acm international conference on information
& knowledge management, cikm    13, pages 1569   
1572, new york, ny, usa. acm.

[etzioni et al.2011] oren etzioni, anthony fader, ja-
nara christensen, stephen soderland, and mausam
mausam. 2011. id10: the
second generation. in ijcai, volume 11, pages 3   10.
[gal  arraga et al.2014] luis gal  arraga, geremy heitz,
kevin murphy, and fabian suchanek. 2014. canoni-
calizing open knowledge bases. cikm.

[goyal et al.2013] kartik goyal, sujay kumar, jauhar
huiying, li mrinmaya, sachan shashank, and srivas-
tava eduard hovy. 2013. a structured distributional
semantic model: integrating structure with semantics.
[harshman1970] r. a. harshman. 1970. foundations of
the parafac procedure: models and conditions for
an    explanatory    multi-modal factor analysis. ucla
working papers in id102, 16(1):84.

[hearst1992] marti a. hearst. 1992. automatic acqui-
in in
sition of hyponyms from large text corpora.
proceedings of the 14th international conference on
computational linguistics, pages 539   545.

[krompa   et al.2013] denis krompa  , maximilian
nickel, xueyan jiang, and volker tresp.
2013.
non-negative tensor factorization with rescal. tensor
methods for machine learning, ecml workshop.

[lee and seung2000] daniel d. lee and h. sebastian se-
ung. 2000. algorithms for non-negative matrix fac-
torization. in in nips, pages 556   562. mit press.

john bauer,

[manning et al.2014] christopher d. manning, mihai
surdeanu,
jenny finkel, steven j.
bethard, and david mcclosky.
2014. the stan-
ford corenlp natural language processing toolkit. in
proceedings of 52nd annual meeting of the associa-
tion for computational linguistics: system demon-
strations, pages 55   60.

[mikolov et al.2013] tomas mikolov, ilya sutskever, kai
chen, greg s corrado, and jeff dean. 2013. dis-
tributed representations of words and phrases and
their compositionality.
in c.j.c. burges, l. bottou,
m. welling, z. ghahramani, and k.q. weinberger, ed-
itors, advances in neural information processing sys-
tems 26, pages 3111   3119. curran associates, inc.

[singh et al.2015] sameer singh, tim rockt  aschel, and
sebastian riedel. 2015. towards combined matrix
and tensor factorization for universal schema rela-
tion extraction. in naacl workshop on vector space
modeling for nlp (vsm).

[suchanek et al.2007] fabian m suchanek, gjergji kas-
neci, and gerhard weikum. 2007. yago: a core of
semantic knowledge. in proceedings of www.

[velardi et al.2013] paola velardi, stefano faralli, and
roberto navigli. 2013. ontolearn reloaded: a graph-
based algorithm for taxonomy induction. computa-
tional linguistics, 39(3):665   707.

[wang et al.2015] yichen wang, robert chen, joydeep
ghosh, joshua c. denny, abel n. kho, you chen,
bradley a. malin, and jimeng sun.
2015. ru-
bik: knowledge guided tensor factorization and com-
pletion for health data analytics.
in longbing cao,
chengqi zhang, thorsten joachims, geoffrey i. webb,
dragos d. margineantu, and graham williams, edi-
tors, kdd, pages 1265   1274. acm.

[mitchell et al.2015] t. mitchell, w. cohen, e. hruschka,
p. talukdar, j. betteridge, a. carlson, b. dalvi,
m. gardner, b. kisiel, j. krishnamurthy, n. lao,
k. mazaitis, t. mohamed, n. nakashole, e. platan-
ios, a. ritter, m. samadi, b. settles, r. wang, d. wi-
jaya, a. gupta, x. chen, a. saparov, m. greaves, and
j. welling. 2015. never-ending learning. in proceed-
ings of aaai.

[mohamed et al.2011] thahir p. mohamed, estevam r.
hruschka, jr., and tom m. mitchell. 2011. discover-
ing relations between noun categories. in proceedings
of the conference on empirical methods in natural
language processing, emnlp    11, pages 1447   1455,
stroudsburg, pa, usa. association for computational
linguistics.

[m  rup et al.2008] m. m  rup, l. k. hansen, and s. m.
arnfred. 2008. algorithms for sparse non-negative
tucker. neural computation, 20(8):2112   2131,
aug.

[movshovitz-attias and cohen2015] dana movshovitz-
attias and william w. cohen. 2015. kb-lda: jointly
learning a knowledge base of hierarchy, relations, and
facts. in proceedings of the 53rd annual meeting of
the association for computational linguistics. asso-
ciation for computational linguistics.

[murphy et al.2012] brian murphy, partha pratim taluk-
dar, and tom m mitchell. 2012. learning effective
and interpretable semantic models using non-negative
sparse embedding. in coling, pages 1933   1950.

[narita et al.2012] atsuhiro narita, kohei hayashi, ryota
tomioka, and hisashi kashima. 2012. tensor factor-
ization using auxiliary information. data mining and
knowledge discovery, 25(2):298   324.

[nickel et al.2011] maximilian nickel, volker tresp, and
hans-peter kriegel. 2011. a three-way model for
collective learning on multi-relational data.
in lise
getoor and tobias scheffer, editors, proceedings of
the 28th international conference on machine learn-
ing (icml-11), icml    11, pages 809   816, new york,
ny, usa, june. acm.

[nickel et al.2012] maximilian nickel, volker tresp, and
hans-peter kriegel. 2012. factorizing yago: scal-
in proceed-
able machine learning for linked data.
ings of the 21st international conference on world
wide web, www    12, pages 271   280, new york, ny,
usa. acm.

[riedel et al.2013] sebastian riedel, limin yao, andrew
mccallum, and benjamin m. marlin. 2013. rela-
tion extraction with id105 and universal
schemas. in human language technologies: confer-
ence of the north american chapter of the association
of computational linguistics, proceedings, june 9-14,
2013, westin peachtree plaza hotel, atlanta, georgia,
usa, pages 74   84.

