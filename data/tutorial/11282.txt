inferring logical forms from denotations

panupong pasupat

computer science department

stanford university

percy liang

computer science department

stanford university

ppasupat@cs.stanford.edu

pliang@cs.stanford.edu

6
1
0
2

 

v
o
n
5
1

 

 
 
]
l
c
.
s
c
[
 
 

2
v
0
0
9
6
0

.

6
0
6
1
:
v
i
x
r
a

abstract

a core problem in learning semantic
parsers from denotations is picking out
consistent logical forms   those that yield
the correct denotation   from a combina-
torially large space. to control the search
space, previous work relied on restricted
set of rules, which limits expressivity. in
this paper, we consider a much more ex-
pressive class of logical forms, and show
how to use id145 to ef   -
ciently represent the complete set of con-
sistent logical forms. expressivity also
introduces many more spurious logical
forms which are consistent with the cor-
rect denotation but do not represent the
meaning of the utterance.
to address
this, we generate    ctitious worlds and use
crowdsourced denotations on these worlds
to    lter out spurious logical forms. on
the wikitablequestions dataset, we
increase the coverage of answerable ques-
tions from 53.5% to 76%, and the ad-
ditional crowdsourced supervision lets us
rule out 92.1% of spurious logical forms.

introduction

1
consider the task of learning to answer com-
plex natural language questions (e.g.,    where did
the last 1st place    nish occur?   )
using only
question-answer pairs as supervision (clarke et
al., 2010; liang et al., 2011; berant et al.,
2013; artzi and zettlemoyer, 2013).
seman-
tic parsers map the question into a logical form
(e.g., r[venue].argmax(position.1st, index))
that can be executed on a knowledge source to ob-
tain the answer (denotation). logical forms are
very expressive since they can be recursively com-
posed, but this very expressivity makes it more

dif   cult to search over the space of logical forms.
previous work sidesteps this obstacle by restrict-
ing the set of possible logical form compositions,
but this is limiting. for instance, for the system
in pasupat and liang (2015), in only 53.5% of the
examples was the correct logical form even in the
set of generated logical forms.

the goal of this paper is to solve two main chal-
lenges that prevent us from generating more ex-
pressive logical forms. the    rst challenge is com-
putational: the number of logical forms grows ex-
ponentially as their size increases. directly enu-
merating over all logical forms becomes infeasi-
ble, and pruning techniques such as id125
can inadvertently prune out correct logical forms.
the second challenge is the large increase in
spurious logical forms   those that do not re   ect
the semantics of the question but coincidentally
execute to the correct denotation. for example,
while logical forms z1, . . . , z5 in figure 1 are all
consistent (they execute to the correct answer y),
the logical forms z4 and z5 are spurious and would
give incorrect answers if the table were to change.
we address these two challenges by solving two
interconnected tasks. the    rst task, which ad-
dresses the computational challenge, is to enumer-
ate the set z of all consistent logical forms given
a question x, a knowledge source w (   world   ),
and the target denotation y (section 4). observ-
ing that the space of possible denotations grows
much more slowly than the space of logical forms,
we perform id145 on denotations
(dpd) to make search feasible. our method is
guaranteed to    nd all consistent logical forms up
to some bounded size.

given the set z of consistent logical forms, the
second task is to    lter out spurious logical forms
from z (section 5). using the property that spuri-
ous logical forms ultimately give a wrong answer
when the data in the world w changes, we create

year
venue
2001 hungary
2003
finland
2005 germany
thailand
2007
2008
china

position event
400m
400m
400m
relay
relay

2nd
1st
11th
1st
7th

time
47.12
46.69
46.62
182.05
180.32

x:    where did the last 1st place    nish occur?   
y: thailand

consistent
correct

z1: r[venue].argmax(position.1st, index)

among rows with position = 1st, pick the one with
maximum index, then return the venue of that row.
z2: r[venue].index.max(r[index].position.1st)
find the maximum index of rows with position =
1st, then return the venue of the row with that index.

z3: r[venue].argmax(position.number.1,

r[  x.r[date].r[year].x])

among rows with position number 1, pick one with
latest date in the year column and return the venue.

spurious

z4: r[venue].argmax(position.number.1,

r[  x.r[number].r[time].x])
among rows with position number 1, pick the one
with maximum time number. return the venue.

z5: r[venue].year.number.(
r[number].r[year].argmax(type.row, index)   1)
subtract 1 from the year in the last row, then return
the venue of the row with that year.

inconsistent

  z: r[venue].argmin(position.1st, index)

among rows with position = 1st, pick the one with
minimum index, then return the venue. (= finland)

figure 1: six logical forms generated from the
question x. the    rst    ve are consistent: they ex-
ecute to the correct answer y. of those, correct
logical forms z1, z2, and z3 are different ways to
represent the semantics of x, while spurious logi-
cal forms z4 and z5 get the right answer y for the
wrong reasons.

   ctitious worlds to test the denotations of the logi-
cal forms in z. we use id104 to annotate
the correct denotations on a subset of the gener-
ated worlds. to reduce the amount of annotation
needed, we choose the subset that maximizes the
expected information gain. the pruned set of log-
ical forms would provide a stronger supervision
signal for training a semantic parser.

we test our methods on the wikitableques-
tions dataset of complex questions on wikipedia
tables. we de   ne a simple, general set of deduc-
tion rules (section 3), and use dpd to con   rm
that the rules generate a correct logical form in

figure 2: the table in figure 1 is converted into
a graph. the recursive execution of logical form
z1 is shown via the different colors and styles.

76% of the examples, up from the 53.5% in pa-
supat and liang (2015). moreover, unlike beam
search, dpd is guaranteed to    nd all consistent
logical forms up to a bounded size. finally, by us-
ing annotated data on    ctitious worlds, we are able
to prune out 92.1% of the spurious logical forms.

2 setup

the overarching motivation of this work is allow-
ing people to ask questions involving computa-
tion on semi-structured knowledge sources such
as tables from the web. this section introduces
how the knowledge source is represented, how the
computation is carried out using logical forms, and
our task of inferring correct logical forms.

worlds. we use the term world to refer to a col-
lection of entities and relations between entities.
one way to represent a world w is as a directed
graph with nodes for entities and directed edges
for relations. (for example, a world about geog-
raphy would contain a node europe with an edge
contains to another node germany.)

in this paper, we use data tables from the web
as knowledge sources, such as the one in figure 1.
we follow the construction in pasupat and liang
(2015) for converting a table into a directed graph
(see figure 2). rows and cells become nodes (e.g.,
r0 =    rst row and finland) while columns be-
come labeled directed edges between them (e.g.,
venue maps r1 to finland). the graph is aug-
mented with additional edges next (from each

...r1      1finland1str2      2germany11thr3      3thailand1st...1111nextnextnextnextindexindexindexvenuepositionvenuepositionvenuepositionnumbernumbernumberz1=r[venue].argmax(position.1st,index)row to the next) and index (from each row to its
index number).
in addition, we add normaliza-
tion edges to cell nodes, including number (from
the cell to the    rst number in the cell), num2 (the
second number), date (interpretation as a date),
and part (each list item if the cell represents a
list). for example, a cell with content    3-4    has
a number edge to the integer 3, a num2 edge to 4,
and a date edge to xx-03-04.

logical forms. we can perform computation on
a world w using a logical form z, a small program
that can be executed on the world, resulting in a

denotation(cid:74)z(cid:75)w.

we use lambda dcs (liang, 2013) as the lan-
guage of logical forms. as a demonstration, we
will use z1 in figure 2 as an example. the small-
est units of lambda dcs are entities (e.g., 1st) and
relations (e.g., position). larger logical forms
can be constructed using logical operations, and
the denotation of the new logical form can be com-
puted from denotations of its constituents. for ex-
ample, applying the join operation on position
and 1st gives position.1st, whose denotation
is the set of entities with relation position point-
ing to 1st. with the world in figure 2, the denota-

tion is(cid:74)position.1st(cid:75)w = {r1, r3}, which cor-

the entities in(cid:74)position.1st(cid:75)w = {r1, r3} us-

responds to the 2nd and 4th rows in the table. the
partial logical form position.1st is then used
to construct argmax(position.1st, index), the
denotation of which can be computed by mapping
ing the relation index ({r0 : 0, r1 : 1, . . .}), and
then picking the one with the largest mapped value
(r3, which is mapped to 3). the resulting logical
form is    nally combined with r[venue] with an-
other join operation. the relation r[venue] is the
reverse of venue, which corresponds to traversing
venue edges in the reverse direction.

id29. a semantic parser maps a
natural language utterance x (e.g.,    where did the
last 1st place    nish occur?   ) into a logical form z.
with denotations as supervision, a semantic parser
is trained to put high id203 on z   s that are
consistent   logical forms that execute to the cor-
rect denotation y (e.g., thailand). when the space
of logical forms is large, searching for consistent
logical forms z can become a challenge.

as illustrated in figure 1, consistent logical
forms can be divided into two groups: correct log-
ical forms represent valid ways for computing the

answer, while spurious logical forms accidentally
get the right answer for the wrong reasons (e.g., z4
picks the row with the maximum time but gets the
correct answer anyway).
tasks. denote by z and zc the sets of all con-
sistent and correct logical forms, respectively. the
   rst task is to ef   ciently compute z given an ut-
terance x, a world w, and the correct denotation y
(section 4). with the set z, the second task is to
infer zc by pruning spurious logical forms from z
(section 5).

3 deduction rules
the space of logical forms given an utterance x
and a world w is de   ned recursively by a set of de-
duction rules (table 1). in this setting, each con-
structed logical form belongs to a category (set,
rel, or map). these categories are used for type
checking in a similar fashion to categories in syn-
tactic parsing. each deduction rule speci   es the
categories of the arguments, category of the re-
sulting logical form, and how the logical form is
constructed from the arguments.

deduction rules are divided into base rules and
compositional rules. a base rule follows one of
the following templates:

tokenspan[span]     c [f (span)]

        c [f ()]

(1)
(2)

a rule of template 1 is triggered by a span of
tokens from x (e.g., to construct z1 in figure 2
from x in figure 1, rule b1 from table 1 con-
structs 1st of category set from the phrase    1st   ).
meanwhile, a rule of template 2 generates a log-
ical form without any trigger (e.g., rule b5 gen-
erates position of category rel from the graph
edge position without a speci   c trigger in x).

compositional rules then construct larger logi-

cal forms from smaller ones:

c1 [z1] + c2 [z2]     c [g(z1, z2)]

c1 [z1]     c [g(z1)]

(3)
(4)

a rule of template 3 combines partial logical
forms z1 and z2 of categories c1 and c2 into
g(z1, z2) of category c (e.g., rule c1 uses 1st of
category set and position of category rel to con-
struct position.1st of category set). template 4
works similarly.

most rules construct logical forms without re-
quiring a trigger from the utterance x. this is

rule

semantics

base rules

fuzzymatch(span)

b1
(entity fuzzily matching the text:    chinese        china)
b2
(interpreted value:    march 2015        2015-03-xx)

tokenspan     set
tokenspan     set
        set
(the set of all rows)
        set

type.row
c     closedclass

val(span)

b3

(any entity from a column with few unique entities)

(e.g., 400m or relay from the event column)

(any relation in the graph: venue, next, num2, . . . )

r     graphedges
!= | < | <= | > | >=
z2.z1 | r[z2].z1

        rel
        rel
compositional rules
set + rel     set
set     set
(a     {count, max, min, sum, avg})
set + set     set
(subtraction is only allowed on numbers)

compositional rules with maps

a(z1)
z1 (cid:117) z2 | z1 (cid:116) z2 | z1     z2

(r[z] is the reverse of z; i.e.,    ip the arrow direction)

b4

b5

b6

c1

c2

c3

initialization

operations on map

m1

m2
m3

set     map
map + rel     map
map     map
map + set     map
m4
m5 map + map     map

(identity map)

(z1, x)
(u1, z2.b1) | (u1, r[z2].b1)
(u1, a(b1))
(u1, b1 (cid:117) z2) | . . .
(u1, b1 (cid:117) b2) | . . .

(a     {count, max, min, sum, avg})

(allowed only when u1 = u2)

(rules m4 and m5 are repeated for (cid:116) and    )

m6

finalization

map     set

argmin(u1, r[  x.b1])

| argmax(u1, r[  x.b1])

table 1: deduction rules de   ne the space of log-
ical forms by specifying how partial logical forms
are constructed. the logical form of the i-th argu-
ment is denoted by zi (or (ui, bi) if the argument
is a map). the set of    nal logical forms contains
any logical form with category set.

crucial for generating implicit relations (e.g., gen-
erating year from    what   s the venue in 2000?   
without a trigger    year   ), and generating opera-
tions without a lexicon (e.g., generating argmax
from    where   s the longest competition   ). how-
ever, the downside is that the space of possible
logical forms becomes very large.

the map category. the technique in this paper
requires execution of partial logical forms. this
poses a challenge for argmin and argmax oper-
ations, which take a set and a binary relation as
arguments. the binary could be a complex func-
tion (e.g., in z3 from figure 1). while it is possible
to build the binary independently from the set, ex-
ecuting a complex binary is sometimes impossible
(e.g., the denotation of   x.count(x) is impossible
to write explicitly without knowledge of x).

we address this challenge with the map cat-
egory. a map is a pair (u, b) of a    nite set
u (unary) and a binary relation b. the deno-
w) where the binary

tation of (u, b) is ((cid:74)u(cid:75)w,(cid:74)b(cid:75)(cid:48)
(cid:74)b(cid:75)(cid:48)
w is(cid:74)b(cid:75)w with the domain restricted to the set
(cid:74)u(cid:75)w. for example, consider the construction of

argmax(position.1st, index). after construct-
ing position.1st with denotation {r1, r3}, rule
m1 initializes (position.1st, x) with denotation
({r1, r3},{r1 : {r1}, r3 : {r3}}). rule m2 is then
applied to generate (position.1st, r[index].x)
with denotation ({r1, r3},{r1 : {1}, r3 : {3}}).
finally, rule m6 converts the map into the desired
argmax logical form with denotation {r3}.
generality of deduction rules. using domain
knowledge, previous work restricted the space of
logical forms by manually de   ning the categories
c or the semantic functions f and g to    t the do-
main. for example, the category set might be di-
vided into records, values, and atomic when the
knowledge source is a table (pasupat and liang,
2015). another example is when a compositional
rule g (e.g., sum(z1)) must be triggered by some
phrase in a lexicon (e.g., words like    total    that
align to sum in the training data). such restrictions
make search more tractable but greatly limit the
scope of questions that can be answered.

here, we have increased the coverage of logi-
cal forms by making the deduction rules simple
and general, essentially following the syntax of
lambda dcs. the base rules only generates en-
tities that approximately match the utterance, but
all possible relations, and all possible further com-
binations.

id125. given the deduction rules, an ut-
terance x and a world w, we would like to generate
all derived logical forms z. we    rst present the
   oating parser (pasupat and liang, 2015), which
uses id125 to generate zb     z, a usually
incomplete subset. intuitively, the algorithm    rst
constructs base logical forms based on spans of
the utterance, and then builds larger logical forms
of increasing size in a       oating    fashion   without
requiring a trigger from the utterance.

formally, partial logical forms with category c
and size s are stored in a cell (c, s). the algorithm
   rst generates base logical forms from base deduc-
tion rules and store them in cells (c, 0) (e.g., the
cell (set, 0) contains 1st, type.row, and so on).
then for each size s = 1, . . . , smax, we populate

2003; liang et al., 2010; gulwani, 2011).

the main idea of dpd is to collapse logical
forms with the same denotation together. instead
of using cells (c, s) as in id125, we per-
form id145 using cells (c, s, d)
where d is a denotation. for instance, the logi-
cal form position.number.1 will now be stored
in cell (set, 2,{r1, r3}).

for dpd to work, each deduction rule must
have a denotationally invariant semantic function
g, meaning that the denotation of the resulting log-
ical form g(z1, z2) only depends on the denota-
tions of z1 and z2:

(cid:74)z1(cid:75)w =(cid:74)z(cid:48)

1(cid:75)w     (cid:74)z2(cid:75)w =(cid:74)z(cid:48)
2(cid:75)w
2)(cid:75)w
   (cid:74)g(z1, z2)(cid:75)w =(cid:74)g(z(cid:48)
1, z(cid:48)

all of our deduction rules in table 1 are de-
notationally invariant, but a rule that, for in-
stance, returns the argument with the larger log-
ical form size would not be. applying a de-
notationally invariant deduction rule on any pair
of logical forms from (c1, s1, d1) and (c2, s2, d2)
always results in a logical form with the same
denotation d in the same cell (c, s1 + s2 +
1, d).1 (for example, the cell (set, 4,{r3}) con-
tains z1 := argmax(position.1st, index) and
z(cid:48)
1 := argmin(event.relay, index). combin-
ing each of these with venue using rule c1 gives
r[venue].z1 and r[venue].z(cid:48)
1, which belong to
the same cell (set, 5,{thailand})).
algorithm. dpd proceeds
in two forward
passes. the    rst pass    nds the possible combi-
nations of cells (c, s, d) that lead to the correct de-
notation y, while the second pass enumerates the
logical forms in the cells found in the    rst pass.
figure 3 illustrates the dpd algorithm.

in the    rst pass, we are only concerned about
   nding relevant cell combinations and not the ac-
tual logical forms. therefore, any logical form
that belongs to a cell could be used as an argu-
ment of a deduction rule to generate further logical
forms. thus, we keep at most one logical form per
cell; subsequent logical forms that are generated
for that cell are discarded.

after populating all cells up to size smax, we
list all cells (set, s, y) with the correct denotation
y, and then note all possible rule combinations
(cell1, rule) or (cell1, cell2, rule) that lead to those

1semantic functions f with one argument work similarly.

figure 3: the    rst pass of dpd constructs cells
(c, s, d) (square nodes) using denotationally in-
variant semantic functions (circle nodes). the sec-
ond pass enumerates all logical forms along paths
that lead to the correct denotation y (solid lines).

the cells (c, s) by applying compositional rules on
partial logical forms with size less than s. for in-
stance, when s = 2, we can apply rule c1 on
logical forms number.1 from cell (set, s1 = 1)
and position from cell (rel, s2 = 0) to create
position.number.1 in cell (set, s0 +s1 +1 = 2).
after populating each cell (c, s), the list of logi-
cal forms in the cell is pruned based on the model
scores to a    xed beam size in order to control the
search space. finally, the set zb is formed by
collecting logical forms from all cells (set, s) for
s = 1, . . . , smax.

due to the generality of our deduction rules, the
number of logical forms grows quickly as the size
s increases. as such, partial logical forms that
are essential for building the desired logical forms
might fall off the beam early on. in the next sec-
tion, we present a new search method that com-
presses the search space using denotations.

4 id145 on denotations

our    rst step toward    nding all correct logical
forms is to represent all consistent logical forms
(those that execute to the correct denotation). for-
mally, given x, w, and y, we wish to generate the

set z of all logical forms z such that(cid:74)z(cid:75)w = y.

as mentioned in the previous section, beam
search does not recover the full set z due to prun-
ing. our key observation is that while the number
of logical forms explodes, the number of distinct
denotations of those logical forms is much more
controlled, as multiple logical forms can share the
same denotation. so instead of directly enumerat-
ing logical forms, we use id145
on denotations (dpd), which is inspired by sim-
ilar methods from program induction (lau et al.,

                                          (set,7,{thailand})(set,7,{finland})   nal cells, including the combinations that yielded
discarded logical forms.

the second pass retrieves the actual logical
forms that yield the correct denotation. to do this,
we simply populate the cells (c, s, d) with all log-
ical forms, using only rule combinations that lead
to    nal cells. this elimination of irrelevant rule
combinations effectively reduces the search space.
(in section 6.2, we empirically show that the num-
ber of cells considered is reduced by 98.7%.)

the parsing chart is represented as a hyper-
graph as in figure 3. after eliminating unused
rule combinations, each of the remaining hyper-
paths from base predicates to the target denotation
corresponds to a single logical form. making the
remaining parsing chart a compact implicit repre-
sentation of all consistent logical forms. this rep-
resentation is guaranteed to cover all possible log-
ical forms under the size limit smax that can be
constructed by the deduction rules.

in our experiments, we apply dpd on the de-
duction rules in table 1 and explicitly enumerate
the logical forms produced by the second pass. for
ef   ciency, we prune logical forms that are clearly
redundant (e.g., applying max on a set of size 1).
we also restrict a few rules that might otherwise
create too many denotations. for example, we re-
stricted the union operation ((cid:116)) except unions of
two entities (e.g., we allow germany (cid:116) finland
but not venue.hungary (cid:116) . . . ), subtraction when
building a map, and count on a set of size 1.2

5 fictitious worlds

after    nding the set z of all consistent logical
forms, we want to    lter out spurious logical forms.
to do so, we observe that semantically correct log-
ical forms should also give the correct denotation
in worlds w(cid:48) other than than w. in contrast, spu-
rious logical forms will fail to produce the correct
denotation on some other world.
generating    ctitious worlds. with the ob-
servation above, we generate    ctitious worlds
w1, w2, . . . , where each world wi is a slight alter-
ation of w. as we will be executing logical forms
z     z on wi, we should ensure that all entities and
relations in z     z appear in the    ctitious world wi
(e.g., z1 in figure 1 would be meaningless if the
entity 1st does not appear in wi). to this end, we
2while we technically can apply count on sets of size 1,
the number of spurious logical forms explodes as there are
too many sets of size 1 generated.

venue
year
finland
2001
2003 germany
2005
china
2007 hungary

time
position event
46.62
relay
400m 180.32
47.12
relay
relay
182.05

7th
1st
1st
7th

w

figure 4: from the example in figure 1, we gen-
erate a table for the    ctitious world w1.
      
      
q1
      
       } q2
      
q3

thailand
thailand
thailand
thailand germany
thailand
thailand

      (cid:41)
      (cid:111)

w1
china
china
china

w2

finland
finland
finland
china
china
china
...

china
china
...

...

z1
z2
z3
z4
z5
z6
...

figure 5: we execute consistent logical forms
zi     z on    ctitious worlds to get denotation tu-
ples. logical forms with the same denotation tuple
are grouped into the same equivalence class qj.

impose that all predicates present in the original
world w should also be present in wi as well.

in our case where the world w comes from a
data table t, we construct wi from a new table ti as
follows: we go through each column of t and re-
sample the cells in that column. the cells are sam-
pled using random draws without replacement if
the original cells are all distinct, and with replace-
ment otherwise. sorted columns are kept sorted.
to ensure that predicates in w exist in wi, we use
the same set of table columns and enforce that any
entity fuzzily matching a span in the question x
must be present in ti (e.g., for the example in fig-
ure 1, the generated ti must contain    1st   ). fig-
ure 4 shows an example    ctitious table generated
from the table in figure 1.

fictitious worlds are similar to test suites for
computer programs. however, unlike manually
designed test suites, we do not yet know the cor-
rect answer for each    ctitious world or whether a
world is helpful for    ltering out spurious logical
forms. the next subsections introduce our method
for choosing a subset of useful    ctitious worlds to
be annotated.

equivalence classes. let w = (w1, . . . , wk) be
the list of all possible    ctitious worlds. for each

z     z, we de   ne the denotation tuple(cid:74)z(cid:75)w =
((cid:74)z(cid:75)w1, . . . ,(cid:74)z(cid:75)wk ). we observe that some logi-

cal forms produce the same denotation across all

   ctitious worlds. this may be due to an algebraic
equivalence in logical forms (e.g., z1 and z2 in fig-
ure 1) or due to the constraints in the construction
of    ctitious worlds (e.g., z1 and z3 in figure 1 are
equivalent as long as the year column is sorted).
we group logical forms into equivalence classes
based on their denotation tuples, as illustrated in
figure 5. when the question is unambiguous, we
expect at most one equivalence class to contain
correct logical forms.

1, . . . , w(cid:48)

annotation. to pin down the correct equiva-
lence class, we acquire the correct answers to the
(cid:96))    
question x on some subset w (cid:48) = (w(cid:48)
w of (cid:96)    ctitious worlds, as it is impractical to ob-
tain annotations on all    ctitious worlds in w . we
compile equivalence classes that agree with the an-
notations into a set zc of correct logical forms.

we want to choose w (cid:48) that gives us the most
information about the correct equivalence class as
possible. this is analogous to standard practices
in active learning (settles, 2010).3 let q be the

set of all equivalence classes q, and let(cid:74)q(cid:75)w (cid:48) be
into partitions ft = {q     q :(cid:74)q(cid:75)w (cid:48) = t} based

the denotation tuple computed by executing an ar-
bitrary z     q on w (cid:48). the subset w (cid:48) divides q

on the denotation tuples t (e.g., from figure 5, if
w (cid:48) contains just w2, then q2 and q3 will be in the
same partition f(china)). the annotation t   , which
is also a denotation tuple, will mark one of these
partitions ft    as correct. thus, to prune out many
spurious equivalence classes, the partitions should
be as numerous and as small as possible.

more formally, we choose a subset w (cid:48) that
maximizes the expected information gain (or
equivalently,
the reduction in id178) about
the correct equivalence class given the annota-
tion. with random variables q     q represent-
ing the correct equivalence class and t    
w (cid:48) for
the annotation on worlds w (cid:48), we seek to    nd
arg minw (cid:48) h(q | t    
w (cid:48)). assuming a uniform
prior on q (p(q) = 1/|q|) and accurate annota-
tion (p(t    | q) = i[q     ft   ]):
(cid:88)

h(q | t    

w (cid:48)) =

p(q, t) log

p(t)
p(q, t)
|ft| log |ft|.

(cid:88)

t

q,t
1
|q|

=

(*)

3 the difference is that we are obtaining partial informa-
tion about an individual example rather than partial informa-
tion about the parameters.

we exhaustively search for w (cid:48) that minimizes
(cid:80)
(*). the objective value follows our intuition since
t |ft| log |ft| is small when the terms |ft| are

small and numerous.

in our experiments, we approximate the full
set w of    ctitious worlds by generating k =
30 worlds to compute equivalence classes. we
choose a subset of (cid:96) = 5 worlds to be annotated.

6 experiments

for the experiments, we use the training portion
of the wikitablequestions dataset (pasupat
and liang, 2015), which consists of 14,152 ques-
tions on 1,679 wikipedia tables gathered by crowd
workers. answering these complex questions re-
quires different types of operations. the same
operation can be phrased in different ways (e.g.,
   best   ,    top ranking   , or    lowest ranking num-
ber   ) and the interpretation of some phrases de-
pend on the context (e.g.,    number of     could be
a table lookup or a count operation). the lexical
content of the questions is also quite diverse: even
excluding numbers and symbols, the 14,152 train-
ing examples contain 9,671 unique words, only
10% of which appear more than 10 times.

we attempted to manually annotate the    rst 300
examples with lambda dcs logical forms. we
successfully constructed correct logical forms for
84% of these examples, which is a good number
considering the questions were created by humans
who could use the table however they wanted. the
remaining 16% re   ect limitations in our setup   
for example, non-canonical table layouts, answers
appearing in running text or images, and com-
mon sense reasoning (e.g., knowing that    quarter-
   nal    is better than    round of 16   ).

6.1 generality of deduction rules
we compare our set of deduction rules with the
one given in pasupat and liang (2015) (hence-
forth pl15). pl15 reported generating the anno-
tated logical form in 53.5% of the    rst 200 exam-
ples. with our more general deduction rules, we
use dpd to verify that the rules are able to gener-
ate the annotated logical form in 76% of the    rst
300 examples, within the logical form size limit
smax of 7. this is 90.5% of the examples that were
successfully annotated. figure 6 shows some ex-
amples of logical forms we cover that pl15 could
not. since dpd is guaranteed to    nd all consis-
tent logical forms, we can be sure that the logical

   which opponent has the most wins   
z = argmax(r[opponent].type.row,

r[  x.count(opponent.x (cid:117) result.lost])

   how long did ian armstrong serve?   
z = r[num2].r[term].member.ianarmstrong

    r[number].r[term].member.ianarmstrong

   which players came in a place before lukas bauer?   
z = r[name].index.<.r[index].name.lukasbauer
   which players played the same position as ardo kreek?   
z = r[player].position.r[position].player.ardo

(cid:117) !=.ardo

figure 6: several example logical forms our sys-
tem can generated that are not covered by the de-
duction rules from the previous work pl15.

forms not covered are due to limitations of the de-
duction rules. indeed, the remaining examples ei-
ther have logical forms with size larger than 7 or
require other operations such as addition, union of
arbitrary sets, etc.

6.2 id145 on denotations
search space. to demonstrate the savings
gained by collapsing logical forms with the same
denotation, we track the growth of the number of
unique logical forms and denotations as the log-
ical form size increases. the plot in figure 7
shows that the space of logical forms explodes
much more quickly than the space of denotations.
the use of denotations also saves us from con-
sidering a signi   cant amount of irrelevant partial
logical forms. on average over 14,152 training
examples, dpd generates approximately 25,000
consistent logical forms. the    rst pass of dpd
generates     153,000 cells (c, s, d), while the sec-
ond pass generates only     2,000 cells resulting
from     8,000 rule combinations, resulting in a
98.7% reduction in the number of cells that have
to be considered.
comparison with id125. we compare
dpd to id125 on the ability to generate (but
not rank) the annotated logical forms. we consider
two settings: when the id125 parameters
are uninitialized (i.e., the beams are pruned ran-
domly), and when the parameters are trained using
the system from pl15 (i.e., the beams are pruned
based on model scores). the plot in figure 8
shows that dpd generates more annotated logical
forms (76%) compared to id125 (53.7%),
even when id125 is guided heuristically by
learned parameters. note that dpd is an exact al-
gorithm and does not require a heuristic.

figure 7: the median of the number of logical
forms (dashed) and denotations (solid) as the for-
mula size increases. the space of logical forms
grows much faster than the space of denotations.

figure 8: the number of annotated logical forms
that can be generated by id125, both unini-
tialized (dashed) and initialized (solid), increases
with the number of candidates generated (con-
trolled by beam size), but lacks behind dpd (star).

6.3 fictitious worlds
we now explore how    ctitious worlds divide the
set of logical forms into equivalence classes, and
how the annotated denotations on the chosen
worlds help us prune spurious logical forms.

equivalence classes. using 30    ctitious worlds
per example, we produce an average of 1,237
equivalence classes. one possible concern with
using a limited number of    ctitious worlds is that
we may fail to distinguish some pairs of non-
equivalent logical forms. we verify the equiva-
lence classes against the ones computed using 300
   ctitious worlds. we found that only 5% of the
logical forms are split from the original equiva-
lence classes.

ideal annotation. after computing equivalence
classes, we choose a subset w (cid:48) of 5    ctitious
worlds to be annotated based on the information-
theoretic objective. for each of the 252 exam-
ples with an annotated logical form z   , we use

the denotation tuple t    =(cid:74)z   (cid:75)w (cid:48) as the annotated

answers on the chosen    ctitious worlds. we are
able to rule out 98.7% of the spurious equivalence
classes and 98.3% of spurious logical forms. fur-
thermore, we are able to    lter down to just one
equivalence class in 32.7% of the examples, and

01234567logicalformsize00.2k0.4k0.6k0.8k1.0kcountlogicalformsdenotations0500010000150002000025000numberof   nallfsproduced0.00.20.40.60.8annotatedlfscoverage(cid:63)at most three equivalence classes in 51.3% of the
examples.
if we choose 5    ctitious worlds ran-
domly instead of maximizing information gain,
then the above statistics are 22.6% and 36.5%,
respectively. when more than one equivalence
classes remain, usually only one class is a dom-
inant class with many equivalent logical forms,
while other classes are small and contain logical
forms with unusual patterns (e.g., z5 in figure 1).
the average size of the correct equivalence
class is     3,000 with the standard deviation of
    8,000. because we have an expressive logical
language, there are fundamentally many equiva-
lent ways of computing the same quantity.
crowdsourced annotation. data from crowd-
sourcing is more susceptible to errors. from the
252 annotated examples, we use 177 examples
where at least two crowd workers agree on the an-
swer of the original world w. when the crowd-
sourced data is used to rule out spurious logical
forms, the entire set z of consistent logical forms
is pruned out in 11.3% of the examples, and the
correct equivalent class is removed in 9% of the
examples. these issues are due to annotation er-
rors, inconsistent data (e.g., having date of death
before birth date), and different interpretations of
the question on the    ctitious worlds. for the re-
maining examples, we are able to prune out 92.1%
of spurious logical forms (or 92.6% of spurious
equivalence classes).

to prevent the entire z from being pruned, we
can relax our assumption and keep logical forms
z that disagree with the annotation in at most 1
   ctitious world. the number of times z is pruned
out is reduced to 3%, but the number of spurious
logical forms pruned also decreases to 78%.

7 related work and discussion
this work evolved from a long tradition of learn-
ing executable semantic parsers, initially from an-
notated logical forms (zelle and mooney, 1996;
kate et al., 2005; zettlemoyer and collins, 2005;
zettlemoyer and collins, 2007; kwiatkowski et
al., 2010), but more recently from denotations
(clarke et al., 2010; liang et al., 2011; berant
et al., 2013; kwiatkowski et al., 2013; pasupat
and liang, 2015). a central challenge in learn-
ing from denotations is    nding consistent logical
forms (those that execute to a given denotation).

as kwiatkowski et al.

(2013) and berant
and liang (2014) both noted, a chief dif   culty

with executable id29 is the    schema
mismatch      words in the utterance do not map
cleanly onto the predicates in the logical form.
this mismatch is especially pronounced in the
wikitablequestions of pasupat and liang
(2015). in the second example of figure 6,    how
long    is realized by a logical form that computes
a difference between two dates. the rami   cation
of this mismatch is that    nding consistent logi-
cal forms cannot solely proceed from the language
side. this paper is about using annotated denota-
tions to drive the search over logical forms.

this takes us into the realm of program in-
duction, where the goal is to infer a program
(logical form) from input-output pairs (for us,
world-denotation pairs). here, previous work
has also leveraged the idea of dynamic program-
ming on denotations (lau et al., 2003; liang et
al., 2010; gulwani, 2011), though for more con-
strained spaces of programs. continuing the pro-
gram analogy, generating    ctitious worlds is simi-
lar in spirit to fuzz testing for generating new test
cases (miller et al., 1990), but the goal there is
coverage in a single program rather than identi-
fying the correct (equivalence class of) programs.
this connection can potentially improve the    ow
of ideas between the two    elds.

finally, the effectiveness of dynamic program-
ming on denotations relies on having a manage-
able set of denotations. for more complex logi-
cal forms and larger id13s, there are
many possible angles worth exploring: performing
abstract interpretation to collapse denotations into
equivalence classes (cousot and cousot, 1977),
relaxing the notion of getting the correct denota-
tion (steinhardt and liang, 2015), or working in a
continuous space and relying on id119
(guu et al., 2015; neelakantan et al., 2016; yin et
al., 2016; reed and de freitas, 2016). this paper,
by virtue of exact id145, sets the
standard.

acknowledgments. we gratefully acknowledge
the support of the google natural language un-
derstanding focused program and the defense
advanced research projects agency (darpa)
deep exploration and filtering of text (deft)
program under air force research laboratory
(afrl) contract no. fa8750-13-2-0040. in addi-
tion, we would like to thank anonymous reviewers
for their helpful comments.

reproducibility. code and experiments
for
this paper are available on the codalab platform
at
https://worksheets.codalab.org/worksheets/
0x47cc64d9c8ba4a878807c7c35bb22a42/.

references
[artzi and zettlemoyer2013] y. artzi and l. zettle-
moyer. 2013. uw spf: the university of wash-
ington id29 framework. arxiv preprint
arxiv:1311.3011.

[berant and liang2014] j. berant and p. liang. 2014.
id29 via id141. in association
for computational linguistics (acl).

[berant et al.2013] j. berant, a. chou, r. frostig, and
p. liang.
2013. id29 on freebase
from question-answer pairs. in empirical methods
in natural language processing (emnlp).

[clarke et al.2010] j.

clarke,

goldwasser,
m. chang, and d. roth. 2010. driving semantic
in computa-
parsing from the world   s response.
tional natural language learning (conll), pages
18   27.

d.

[cousot and cousot1977] p. cousot and r. cousot.
1977. abstract interpretation:
a uni   ed lattice
model for static analysis of programs by construc-
tion or approximation of    xpoints. in principles of
programming languages (popl), pages 238   252.

[gulwani2011] s. gulwani. 2011. automating string
processing in spreadsheets using input-output exam-
ples. acm sigplan notices, 46(1):317   330.

[guu et al.2015] k. guu, j. miller, and p. liang. 2015.
traversing id13s in vector space.
in
empirical methods in natural language processing
(emnlp).

[kate et al.2005] r. j. kate, y. w. wong, and r. j.
mooney. 2005. learning to transform natural to for-
mal languages. in association for the advancement
of arti   cial intelligence (aaai), pages 1062   1068.

[kwiatkowski et al.2010] t. kwiatkowski, l. zettle-
moyer, s. goldwater, and m. steedman.
2010.
inducing probabilistic id35 grammars from logi-
in em-
cal form with higher-order uni   cation.
pirical methods in natural language processing
(emnlp), pages 1223   1233.

[kwiatkowski et al.2013] t. kwiatkowski, e. choi,
y. artzi, and l. zettlemoyer. 2013. scaling seman-
tic parsers with on-the-   y ontology matching.
in
empirical methods in natural language processing
(emnlp).

[lau et al.2003] t. lau, s. wolfman, p. domingos, and
d. s. weld. 2003. programming by demonstra-
tion using version space algebra. machine learning,
53:111   156.

[liang et al.2010] p. liang, m. i. jordan, and d. klein.
2010. learning programs: a hierarchical bayesian
approach. in international conference on machine
learning (icml), pages 639   646.

[liang et al.2011] p. liang, m. i. jordan, and d. klein.
2011. learning dependency-based compositional
in association for computational lin-
semantics.
guistics (acl), pages 590   599.

[liang2013] p. liang.

2013. lambda dependency-

based id152. arxiv.

[miller et al.1990] b. p. miller, l. fredriksen, and
b. so. 1990. an empirical study of the reliabil-
ity of unix utilities. communications of the acm,
33(12):32   44.

[neelakantan et al.2016] a. neelakantan, q. v. le, and
i. sutskever. 2016. neural programmer: induc-
ing latent programs with id119. in inter-
national conference on learning representations
(iclr).

[pasupat and liang2015] p. pasupat and p. liang.
2015. compositional id29 on semi-
structured tables. in association for computational
linguistics (acl).

[reed and de freitas2016] s. reed and n. de freitas.
in inter-
2016. neural programmer-interpreters.
national conference on learning representations
(iclr).

[settles2010] b. settles. 2010. active learning litera-
ture survey. technical report, university of wiscon-
sin, madison.

[steinhardt and liang2015] j. steinhardt and p. liang.
in ad-
2015. learning with relaxed supervision.
vances in neural information processing systems
(nips).

[yin et al.2016] p. yin, z. lu, h. li, and b. kao. 2016.
neural enquirer: learning to query tables with nat-
ural language. arxiv.

[zelle and mooney1996] m. zelle and r. j. mooney.
1996. learning to parse database queries using in-
ductive logic programming. in association for the
advancement of arti   cial intelligence (aaai), pages
1050   1055.

[zettlemoyer and collins2005] l. s. zettlemoyer and
m. collins. 2005. learning to map sentences to log-
ical form: structured classi   cation with probabilis-
tic categorial grammars. in uncertainty in arti   cial
intelligence (uai), pages 658   666.

[zettlemoyer and collins2007] l. s. zettlemoyer and
2007. online learning of relaxed
m. collins.
id35 grammars for parsing to logical form.
in
empirical methods in natural language process-
ing and computational natural language learning
(emnlp/conll), pages 678   687.

