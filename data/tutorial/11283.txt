the yahoo query treebank, v. 1.0

may 2016

yuval pinter
yahoo research

roi reichart

yahoo research & technion iit

idan szpektor
yahoo research

me@yuvalpinter.com

roiri@ie.technion.ac.il

idan@yahoo-inc.com

6
1
0
2

 

y
a
m
1
1

 

 
 
]
l
c
.
s
c
[
 
 

2
v
5
4
9
2
0

.

5
0
6
1
:
v
i
x
r
a

1 general

dataset

release

this
accompanies
pinter et al. (2016) which describes the moti-
vation and grammatical theory1. please cite that
paper when referencing the dataset.

the dataset may be accessed via the yahoo
webscope homepage2 under linguistic data as
dataset l-28. the description in section 2 is in-
cluded within the dataset as a readme.

the dataset is sure to have annotation errors
which are not covered by the special cases spec-
i   ed in this document. please approach the    rst
author for any corrections and they will appear in
the next release. see section 4 for known errors.

2 dataset description

user queries annotated for syntactic dependency
parsing, version 1.0. these queries were issued on
all search engines between 2012 and 2014 and led
the searcher to click on a result link to a question
page on the yahoo answers site3.

2.1 full description
this dataset contains two    les:

ydata-search-parsed-queries-dev-v1 0.txt

1,000 queries, 5,344 tokens

ydata-search-parsed-queries-test-v1 0.txt

4,000 queries, 26,015 tokens

these    les differ in their level of annotation,
but share the schema. they contain tab-delimited
lines, each representing a single token in a
web query. the tokens in each query are given
sequentially, and queries are given in order of an
arbitrarily-selected numeric id (with no empty
lines between queries).
the    eld schema is
detailed in table 1.

1this dataset is presented there in section 3.3.
2http://webscope.sandbox.yahoo.com
3http://answers.yahoo.com

183
183
183
183

-

nn 2
1 seg charter
nn 0
2
school
3 seg graduate vb 0
4
rb 3

early

-

nn
root
root
advmod

table 2: a sample query (#183 in test set).

root

root

nn

advmod

nn

rb
charter school graduate early

nn

vb

figure 1: query #183 from the test set, tagged and
parsed.

an example query is brought in table 2. these
lines represent a query, whose id in the    test    set is
183, and whose raw form is charter school gradu-
ate early. it is interpretable thus: the query is com-
posed of two syntactic segments: [charter school]
[graduate early]. in the    rst segment, the syntac-
tic root is the noun school, and the noun charter
modi   es it in a nominal compound modi   er rela-
tion. the second segment is rooted by the verb
graduate, modi   ed by the adverb early in an ad-
verbial modi   er relation. a dependency tree cor-
responding to this query is produced in figure 1.

2.2 linguistic pre-processing notes

all queries were tokenized using the clearnlp to-
kenizer for english (choi and mccallum, 2013) 4
and were not spell-corrected or    ltered for adult
content.
initial quotes
were replaced with backticks. when using off-
the-shelf processing tools, we recommend to re-
replace them in pre-processing.

for excel-friendliness,

4version 2.0.1, from http://www.clearnlp.com

field # field content
0
1
2
3
4
5
6

query-id
token-in-query (starting with 1)
segmentation marker (   seg    iff token starts a new segment,    -    otherwise)
token form
token part-of-speech    
index of syntactic head token, with 0 denoting root    
dependency relation of edge from head token to this token        

table 1: fields marked by     are not populated for the dev set in v 1.0;    elds with     are not fully populated
in the test set.

3 annotation guidelines

the parse tree annotations followed the segmen-
tation annotation. both phases included super-
vision over automatic part-of-speech tagging. in
general, parse tree annotators were instructed to
adhere to the stanford dependency guidelines
(de marneffe and manning, 2008)5 with the nec-
essary caveats that come with non-standard text
prone to errors.

following are some selected issues which can
be seen in the test dataset. section 3.1 describes is-
sues relating to segmentation tagging, and the sub-
sequent subsections address issues of dependency
edge attachment.

3.1 segmentation ambiguity
noun strings the most dif   cult segmentation
decisions have been in cases of long strings of
nouns, both common and proper. the guideline
is based on judgment, asking whether the phrase
can stand as a constituent in a coherent sen-
tence. for example, in query 116 words 1-3 are
big mac cost, a phrase considered clunky as op-
posed to the much-preferred cost of (a) big mac,
resulting in the decision to segment it as [big mac]
[cost]. a more clear-cut case is the segmenta-
tion decision in q.284: [   rst day of missed pe-
riod] [symptoms], where theoretically day could
be tagged as a modi   er phrase head of symptoms,
but no imaginable well-formed sentence would
use this formation as a constituent.

sometimes reasonable semantics forced us to
conclude towards a segmentation decision over
an unlikely (but syntactically well-formed) single-
segment constituent. for example, in q.271 [fan-
   ction] [guest reviews] we assume (and support
this decision by running our own search) that
the searcher was looking for guest reviews on

fan   ction website platforms rather than guest re-
views written about pieces of fan   ction. like-
wise, in q.3013 [questionnaire] [assisted suicide]
we treated assisted as an adjective relating to sui-
cide rather than a main verb in a sentence describ-
ing an unlikely scenario.

missing auxiliary verbs another issue was
certain constructions which may become a sen-
tence by the addition of a copula or auxiliary
verb. the guidelines stated that if the post-copular
sequence describes an attribute of the semantic
subject (pre-copula), it is considered the head of
an adjunct phrase and shares its segment (e.g.
q.2916 [paragraph describing a family], q.91.w.1-
4: [battery cables not tight]).
if it is construed
as a sentence missing an auxiliary, it is segmented
away from the subject (e.g. q.475 [lab] [throwing
up blood ?]). in a way, this is an explicit applica-
tion of the constituency test described earlier.

3.2 proper names    internal structure

the dataset contains many instances of product
names followed by numbers denoting model (e.g.
query 2347: how to replace crank sensor on 95
saab cs9000. the guidelines in these cases were
to place the phrase head as the last alphabetic
word (here, saab) and depend the post-modi   er
(cs9000) on it as an npadvmod.

the1 call of

query 3252:

long proper names with complex internal struc-
tures were left as their underlying structures.
e.g.
the2 wild is
parsed with call as the phrase head with a det
preceding and a prep following, further decom-
posed into pobj(of,wild) and det(wild,the2). how-
ever, more trivial internal structures were    attened
into the common proper name representation:
q.3260,
jeeves & wooster: nn(wooster,jeeves),
nn(wooster,&). nominal compounds were tagged
as successive nnps, e.g.
q.3413, what cam-

5version 3.5.2, from http://nlp.stanford.edu/software/dependencies_manual.pdf

eras do they use in planet nnp earth nnp:
nn(earth,planet).

3.3 truncated sentences
many of the queries in the dataset are in fact sen-
tences aborted mid-way. some because the query
is a sentence-completion question (e.g. query 970
a major result of the european age of exploration
was) and some for more opaque reasons (e.g.
q.3828 why are russians so). in both these types,
where the root of the sentence would normally lie
in the missing complement, the root was assigned
to the token nearest to it in the assumed full graph
(was and so, respectively), with the other depen-
dents collapsed unto it. the same goes for any
phrase which is truncated before its grammatical
head (e.g. q.3840) or mandatory complement (e.g.
q.1861).

3.4 foreign languages
the dataset contains several non-english queries
that are treated as nonce (part-of-speech tag =
   fw   , dependency relation =    dep   ). their parse
tree is, as a rule, a    at tree headed by the    nal
word (proper name convention). e.g. q.3299,
q.3319. other cases where foreign words are
tagged as    fw    is when they function meta-
linguistically. e.g. q.3472, what does baka fw
mean in japanese.

3.5 grammatical errors
by the nature of web queries being written in real
time by users possessing diverse pro   ciency and
competence, the dataset contains many grammat-
ical errors (as well as typos     see section 3.6).
these were not corrected during pre-processing in
order to maintain the authenticity of the data. the
guidelines in such cases, unless re-segmentation
was in order, called to retain as much of the in-
tended structure as possible.
in
query 3274, word 4 part is meant to be parts and
as such is tagged as a plural noun. in some cases,
different parts of the sentence were fused together
due to incorrect grammar. the solution was to rep-
resent the fused token by the head of the intended
phrase if it is fully contained within (e.g. q.3233,
dogs for dog    s), or ignore a dependent if there is
structural crossover (e.g.
in q.3590, my sister in
laws husband, the possessive    s was in effect ex-
cluded from the tree).

for example,

another common case was auxiliary deletion
common in esl writing, e.g. query 3949 why you

study in university. here we opted again for the
intended meaning as a sentence missing the aux-
iliary do rather than treating the full query as a
constituent (adverbial clause, clausal complement,
etc.) of a deleted governing predicate.

3.6 typographical errors

in general, obvious typos were treated as the in-
tended word whether they resulted in a legal en-
glish word or not. e.g. q.3 w.13, triaid86le nn, or
q.3786 w.5 if vbz (for the intended is).
sometimes, typos result in word merge, in which
case they were either pos-tagged as xx (e.g.
q.252 w.3) or by the head if they can be construed
as a coherent phrase (e.g. q.3115 w.1 search-
where, which is probably a mis-concatenation of a
meta-linguistic search with the    rst intended term
where, and so analyzed as if it were the latter
alone).

sometimes, extremely creative id121 is
employed by users. behold the glory that is query
1814: green chemistry.in day today life. we parsed
day today as if it were a noun phrase, and gave up
representing the preposition in altogether.

3.7 be-sentences

sentences with forms of the verb be are often am-
biguous between attributive sentences (where the
be-verb acts as copula to the head of the follow-
ing phrase) and proper essential statements where
be is the main verb. we tended to go for the for-
mer in case of ambiguity, excepting for very clear
cases of essence (e.g. query 3264 the free market
is a myth: root(root,is)) and, of course, where
the following can only be a clausal or prepositional
complement (e.g. q.799 trouble is you think you
have time, q.3593 what is on sheldons shirt in sea-
son 1 episode 4).

4 known annotation errors

4.1 segmentation errors

test set:

query id v 1.0 correct
194
304
325
362
425
847
911
3779
1147
1812
2784
2883
2912
3348
3366

1
1
1
1
1
1
1
1
1,3
1,4
1,2,3
1,7
1,5,7
1,2
1,2,3,4

1,3
1,3
1,3
1,3
1,4
1,3
1,4
1,5
1
1
1
1
1
1
1

4.2 attachment errors
test set:

query.token v 1.0 correct
3153.6
3153.7

2
6

7
2

credits

segmentation tagged by bettina bolla, avihai
mejer, yuval pinter, roi reichart, and idan
szpektor. parsing tagged by shir givoni and yu-
val pinter.

references
[choi and mccallum2013] jinho d choi and andrew
2013. transition-based dependency
in acl (1),

mccallum.
parsing with selectional branching.
pages 1052   1062.

[de marneffe and manning2008] marie-catherine

de marneffe and christopher d manning. 2008.
stanford typed dependencies manual. technical
report, technical report, stanford university.

[pinter et al.2016] yuval pinter, roi reichart, and idan
szpektor. 2016. syntactic parsing of web queries
with question intent.
in proceedings of naacl-
hlt, san diego, ca, june 12-17, 2016.

