id86 as planning under uncertainty for spoken

dialogue systems

verena rieser

school of informatics
university of edinburgh

oliver lemon

school of informatics
university of edinburgh

vrieser@inf.ed.ac.uk

olemon@inf.ed.ac.uk

abstract

we present and evaluate a new model for
id86 (id86) in
spoken dialogue systems, based on statis-
tical planning, given noisy feedback from
the current generation context (e.g. a user
and a surface realiser). we study its use in
a standard id86 problem: how to present
information (in this case a set of search re-
sults) to users, given the complex trade-
offs between utterance length, amount of
information conveyed, and cognitive load.
we set these trade-offs by analysing exist-
ing match data. we then train a id86 pol-
icy using id23 (rl),
which adapts its behaviour to noisy feed-
back from the current generation context.
this policy is compared to several base-
lines derived from previous work in this
area. the learned policy signi   cantly out-
performs all the prior approaches.
introduction

1
natural language allows us to achieve the same
communicative goal (   what to say   ) using many
different expressions (   how to say it   ). in a spo-
ken dialogue system (sds), an abstract commu-
nicative goal (cg) can be generated in many dif-
ferent ways. for example,
the cg to present
database results to the user can be realised as a
summary (polifroni and walker, 2008; demberg
and moore, 2006), or by comparing items (walker
et al., 2004), or by picking one item and recom-
mending it to the user (young et al., 2007).

previous work has shown that it is useful to
adapt the generated output to certain features of
the dialogue context, for example user prefer-
ences, e.g. (walker et al., 2004; demberg and
moore, 2006), user knowledge, e.g. (janarthanam
and lemon, 2008), or predicted tts quality, e.g.
(nakatsu and white, 2006).

in extending this previous work we treat id86
as a statistical sequential planning problem, anal-
ogously to current statistical approaches to dia-
logue management (dm), e.g. (singh et al., 2002;
henderson et al., 2008; rieser and lemon, 2008a)
and    conversation as action under uncertainty   
(paek and horvitz, 2000).
in id86 we have
similar trade-offs and unpredictability as in dm,
and in some systems the content planning and dm
tasks are overlapping. clearly, very long system
utterances with many actions in them are to be
avoided, because users may become confused or
impatient, but each individual id86 action will
convey some (potentially) useful information to
the user. there is therefore an optimisation prob-
lem to be solved. moreover, the user judgements
or next (most likely) action after each id86 action
are unpredictable, and the behaviour of the surface
realiser may also be variable (see section 6.2).

id86 could therefore fruitfully be approached
as a sequential statistical planning task, where
there are trade-offs and decisions to make, such as
whether to choose another id86 action (and which
one to choose) or to instead stop generating. re-
inforcement learning (rl) allows us to optimise
such trade-offs in the presence of uncertainty, i.e.
the chances of achieving a better state, while en-
gaging in the risk of choosing another action.

in this paper we present and evaluate a new
model for id86 in spoken dialogue systems as
planning under uncertainty. in section 2 we argue
for applying rl to id86 problems and explain the
overall framework. in section 3 we discuss chal-
lenges for id86 for information presentation. in
section 4 we present results from our analysis of
the match corpus (walker et al., 2004). in sec-
tion 5 we present a detailed example of our pro-
posed id86 method.
in section 6 we report on
experimental results using this framework for ex-
ploring information presentation policies. in sec-
tion 7 we conclude and discuss future directions.

6
1
0
2

 

n
u
j
 

5
1

 
 
]
l
c
.
s
c
[
 
 

1
v
6
8
6
4
0

.

6
0
6
1
:
v
i
x
r
a

2 id86 as planning under uncertainty
we adopt the general framework of id86 as plan-
ning under uncertainty (see (lemon, 2008) for the
initial version of this approach). some aspects of
id86 have been treated as planning, e.g. (koller
and stone, 2007; koller and petrick, 2008), but
never before as statistical planning.

id86 actions take place in a stochastic environ-
ment, for example consisting of a user, a realizer,
and a tts system, where the individual id86 ac-
tions have uncertain effects on the environment.
for example, presenting differing numbers of at-
tributes to the user, and making the user more or
less likely to choose an item, as shown by (rieser
and lemon, 2008b) for multi-modal interaction.

most sds employ    xed template-based gener-
ation. our goal, however, is to employ a stochas-
tic realizer for sds, see for example (stent et al.,
2004). this will introduce additional noise, which
higher level id86 decisions will need to react
to. in our framework, the id86 component must
achieve a high-level communicative goal from
the dialogue manager (e.g.
to present a number
of items) through planning a sequence of lower-
level generation steps or actions, for example    rst
to summarise all the items and then to recommend
the highest ranking one. each such action has un-
predictable effects due to the stochastic realizer.
for example the realizer might employ 6 attributes
when recommending item i4, but it might use only
2 (e.g. price and cuisine for restaurants), depend-
ing on its own processing constraints (see e.g. the
realizer used to collect the match project data).
likewise, the user may be likely to choose an item
after hearing a summary, or they may wish to hear
more. generating appropriate language in context
(e.g. attributes presented so far) thus has the fol-
lowing important features in general:
    id86 is goal driven behaviour
    id86 must plan a sequence of actions
    each action changes the environment state or

context

    the effect of each action is uncertain.
these facts make it clear that the problem of
planning how to generate an utterance falls nat-
urally into the class of statistical planning prob-
lems, rather than rule-based approaches such as
(moore et al., 2004; walker et al., 2004), or super-
vised learning as explored in previous work, such

as classi   er learning and re-ranking, e.g. (stent et
al., 2004; oh and rudnicky, 2002). supervised
approaches involve the ranking of a set of com-
pleted plans/utterances and as such cannot adapt
online to the context or the user. reinforcement
learning (rl) provides a principled, data-driven
optimisation framework for our type of planning
problem (sutton and barto, 1998).

3 the information presentation problem

we will tackle the well-studied problem of infor-
mation presentation in id86 to show the bene   ts
of this approach. the task here is to    nd the best
way to present a set of search results to a user
(e.g. some restaurants meeting a certain set of con-
straints). this is a task common to much prior
work in id86, e.g. (walker et al., 2004; demberg
and moore, 2006; polifroni and walker, 2008).

in this problem, there there are many decisions
available for exploration. for instance, which pre-
sentation strategy to apply (id86 strategy selec-
tion), how many attributes of each item to present
(attribute selection), how to rank the items and at-
tributes according to different models of user pref-
erences (attribute ordering), how many (speci   c)
items to tell them about (conciseness), how many
sentences to use when doing so (syntactic plan-
ning), and which words to use (lexical choice) etc.
all these parameters (and potentially many more)
can be varied, and ideally, jointly optimised based
on user judgements.

we had two corpora available to study some of
the regions of this decision space. we utilise the
match corpus (walker et al., 2004) to extract an
evaluation function (also known as    reward func-
tion   ) for rl. furthermore, we utilise the sparky
corpus (stent et al., 2004) to build a high quality
stochastic realizer. both corpora contain data from
   overhearer    experiments targeted to information
presentation in dialogues in the restaurant domain.
while we are ultimately interested in how hearers
engaged in dialogues judge different information
presentations, results from overhearers are still di-
rectly relevant to the task.

4 match corpus analysis

the match project made two data sets available,
see (stent et al., 2002) and (whittaker et al., 2003),
which we combine to de   ne an evaluation function
for different information presentation strategies.

strategy
summary

compare

example
   the 4 restaurants differ in food quality, and cost.   
(#attr = 2, #sentence = 1)
   among the selected restaurants, the following offer
exceptional overall value. aureole   s price is 71 dol-
lars. it has superb food quality, superb service and
superb decor. daniel   s price is 82 dollars. it has su-
perb food quality, superb service and superb decor.   
(#attr = 4, #sentence = 5)

recommend    le madeleine has the best overall value among the
selected restaurants. le madeleine   s price is 40 dol-
lars and it has very good food quality. it   s in mid-
town west.     (#attr = 3, #sentence = 3)

av.#attr
2.07  .63
3.2  1.5

av.#sentence

1.56  .5
5.5  3.11

2.4  .7

3.5  .53

table 1: id86 strategies present in the match corpus with average no. attributes and sentences as found
in the data.

the    rst data set, see (stent et al., 2002), com-
prises 1024 ratings by 16 subjects (where we only
use the speech-based half, n = 512) on the follow-
ing presentation strategies: recommend, com-
pare, summary. these strategies are realised
using templates as in table 2, and varying num-
bers of attributes. in this study the users rate the
individual presentation strategies as signi   cantly
different (f (2) = 1361, p < .001). we    nd that
summary is rated signi   cantly worse (p = .05
with bonferroni correction) than recommend
and compare, which are rated as equally good.

this suggests that one should never generate
a summary. however, summary has different
qualities from compare and recommend, as
it gives users a general overview of the domain,
and probably helps the user to feel more con   -
dent when choosing an item, especially when they
are unfamiliar with the domain, as shown by (po-
lifroni and walker, 2008).

in order to further describe the strategies, we ex-
tracted different surface features as present in the
data (e.g. number of attributes realised, number of
sentences, number of words, number of database
items talked about, etc.) and performed a step-
wise id75 to    nd the features which
were important to the overhearers (following the
paradise framework (walker et al., 2000)). we
discovered a trade-off between the length of the ut-
terance (#sentence) and the number of attributes
realised (#attr), i.e. its informativeness, where
overhearers like to hear as many attributes as pos-
sible in the most concise way, as indicated by
the regression model shown in equation 1 (r2 =

.34). 1
score = .775    #attr + (   .301)    #sentence;
(1)
the second match data set, see (whittaker et
al., 2003), comprises 1224 ratings by 17 subjects
on the id86 strategies recommend and com-
pare. the strategies realise varying numbers of
attributes according to different    conciseness    val-
ues: concise (1 or 2 attributes), average (3
or 4), and verbose (4, 5, or 6). overhearers
rate all conciseness levels as signi   cantly different
(f (2) = 198.3, p < .001), with verbose rated
highest and concise rated lowest, supporting
our    ndings in the    rst data set. however, the rela-
tion between number of attributes and user ratings
is not strictly linear: ratings drop for #attr = 6.
this suggests that there is an upper limit on how
many attributes users like to hear. we expect this
to be especially true for real users engaged in ac-
tual dialogue interaction, see (winterboer et al.,
2007). we therefore include    cognitive load    as a
variable when training the policy (see section 6).
in addition to the trade-off between length and
informativeness for single id86 strategies, we are
interested whether this trade-off will also hold for
generating sequences of id86 actions. (whittaker
et al., 2002), for example, generate a combined
strategy where    rst a summary is used to de-
scribe the retrieved subset and then they recom-
mend one speci   c item/restaurant. for example
   the 4 restaurants are all french, but differ in

1for comparison: (walker et al., 2000) report on r2 be-

tween .4 and .5 on a slightly lager data set.

figure 1: possible id86 policies (x=stop generation)

food quality, and cost. le madeleine has the best
overall value among the selected restaurants. le
madeleine   s price is 40 dollars and it has very
good food quality. it   s in midtown west.   

we therefore extend the set of possible strate-
gies present in the data for exploration: we allow
ordered combinations of the strategies, assuming
that only compare or recommend can follow a
summary, and that only recommend can fol-
low compare, resulting in 7 possible actions:

1. recommend
2. compare
3. summary
4. compare+recommend
5. summary+recommend
6. summary+compare
7. summary+compare+recommend

we then analytically solved the regression
model in equation 1 for the 7 possible strategies
using average values from the match data. this is
solved by a system of linear inequalities. accord-
ing to this model, the best ranking strategy is to
do all the presentation strategies in one sequence,
i.e. summary+compare+recommend. how-
ever, this analytic solution assumes a    one-shot   
generation strategy where there is no intermediate
feedback from the environment: users are simply
static overhearers (they cannot    barge-in    for ex-
ample), there is no variation in the behaviour of the
surface realizer, i.e. one would use    xed templates
as in match, and the user has unlimited cogni-
tive capabilities. these assumptions are not real-
istic, and must be relaxed. in the next section we

describe a worked through example of the overall
framework.

5 method: the rl-id86 model

for the reasons discussed above, we treat the
id86 module as a statistical planner, operat-
ing in a stochastic environment, and optimise
it using id23.
the in-
to the module is a communicative goal
put
supplied by the dialogue manager.
the cg
consists of a dialogue act
to be generated,
example present items(i1, i2, i5, i8),
for
and a system goal
(sysgoal) which is
to make the
the desired user
items
user
presented
(user choose one of(i1, i2, i5, i8)).
the
rl-id86 module must plan a sequence of lower-
level id86 actions that achieve the goal (at lowest
cost) in the current context. the context consists
of a user (who may remain silent, supply more
constraints, choose an item, or quit), and variation
from the sentence realizer described above.

reaction, e.g.

choose

one

the

of

here, we

shown in table 2.

now let us walk-through one simple ut-
terance plan as carried out by this model,
as
start
with the cg present items(i1, i2, i5, i8)&
user choose one of(i1, i2, i5, i8) from the
system   s dm. this initialises the id86 state (init).
the policy chooses the action summary and this
transitions us to state s1, where we observe that
4 attributes and 1 sentence have been generated,
and the user is predicted to remain silent. in this
state, the current id86 policy is to recommend
the top ranked item (i5, for this user), which takes
us to state s2, where 8 attributes have been gener-
ated in a total of 4 sentences, and the user chooses
an item. the policy holds that in states like s2 the

figure 2: example rl-id86 action sequence for table 4

state action
init sysgoal: present items(i1, i2, i5, i8)& user choose one of(i1, i2, i5, i8)
s1
s2
end rl-id86: stop

rl-id86: summary(i1, i2, i5, i8)
rl-id86: recommend(i5)

state change/effect

initialise state

att=4, sent=1, user=silent

att=8, sent=4, user=choose(i5)

calculate reward

table 2: example utterance planning sequence for figure 2

best thing to do is    stop    and pass the turn to the
user. this takes us to the state end, where the total
reward of this action sequence is computed (see
section 6.3), and used to update the id86 policy
in each of the visited state-action pairs via back-
propagation.

6 experiments
we now report on a proof-of-concept study where
we train our policy in a simulated learning envi-
ronment based on the results from the match cor-
pus analysis in section 4. simulation-based rl
allows to explore unseen actions which are not in
the data, and thus less initial data is needed (rieser
and lemon, 2008b). note, that we cannot directly
learn from the match data, as therefore we would
need data from an interactive dialogue. we are
currently collecting such data in a wizard-of-oz
experiment.

6.1 user simulation
user simulations are commonly used to train
strategies for dialogue management, see for ex-
ample (young et al., 2007). a user simulation for
id86 is very similar, in that it is a predictive model
of the most likely next user act. however, this user
act does not actually change the overall dialogue

state (e.g. by    lling slots) but it only changes the
generator state. in other words, the id86 user sim-
ulation tells us what the user is most likely to do
next, if we were to stop generating now. it also
tells us the id203 whether the user chooses
to    barge-in    after a system id86 action (by either
choosing an item or providing more information).
the user simulation for this study is a simple
bi-gram model, which relates the number of at-
tributes presented to the next likely user actions,
see table 3. the user can either follow the goal
provided by the dm (sysgoal), for example
choosing an item. the user can also do some-
thing else (userelse), e.g. providing another
constraint, or the user can quit (userquit).

for simpli   cation, we discretise the number of
attributes into concise-average-verbose,
re   ecting the conciseness values from the match
data, as described in section 4.
in addition, we
assume that the user   s cognitive abilities are lim-
ited (   cognitive load   ), based on the results from
the second match data set in section 4. once the
number of attributes is more than the    magic num-
ber 7    (re   ecting psychological results on short-
term memory) (baddeley, 2001) the user is more
likely to become confused and quit.

the probabilities in table 3 are currently man-

inits1s2summariserecommendendstopatts=4user=silentatts=8user=chooseenvironment:actions:goalrewardually set heuristics. we are currently conducting a
wizard-of-oz study in order to learn these proba-
bilities (and other user parameters) from real data.

concise
average
verbose

sysgoal
20.0
60.0
20.0

userelse
60.0
20.0
20.0

userquit
20.0
20.0
60.0

table 3: id86 bi-gram user simulation

6.2 realizer model
the sequential id86 model assumes a realizer,
which updates the context after each generation
step (i.e. after each single action). we estimate
the realiser   s parameters from the mean values we
found in the match data (see table 1). for this
study we    rst (randomly) vary the number of at-
tributes, whereas the number of sentences is    xed
(see table 4). in current work we replace the re-
alizer model with an implemented generator that
replicates the variation found in the sparky real-
izer (stent et al., 2004).

summary
compare
recommend

#attr #sentence
1 or 2
3 or 4
2 or 3

2
6
3

table 4: realizer parameters

6.3 reward function
the reward function de   nes the    nal goal of the
utterance generation sequence. in this experiment
the reward is a function of the various data-driven
trade-offs as identi   ed in the data analysis in sec-
tion 4: utterance length and number of provided
attributes, as weighted by the regression model
in equation 1, as well as the next predicted user
action. since we currently only have overhearer
data, we manually estimate the reward for the
next most likely user act, to supplement the data-
if in the end state the next most
driven model.
likely user act is userquit, the learner gets a
penalty of    100, userelse receives 0 reward,
and sysgoal gains +100 reward. again, these
hand coded scores need to be re   ned by a more
targeted data collection, but the other components
of the reward function are data-driven.

note that rl learns to    make compromises   
with respect to the different trade-offs. for ex-
ample, the user is less likely to choose an item

if there are more than 7 attributes, but the real-
izer can generate 9 attributes. however, in some
contexts it might be desirable to generate all 9 at-
tributes, e.g. if the generated utterance is short.
threshold-based approaches, in contrast, cannot
(easily) reason with respect to the current context.

6.4 state and action space
we now formulate the problem as a markov de-
cision process (mdp), relating states to actions.
each state-action pair is associated with a transi-
tion id203, which is the id203 of mov-
ing from state s at time t to state s(cid:48) at time t + 1 af-
ter having performed action a when in state s. this
transition id203 is computed by the environ-
ment model (i.e. user and realizer), and explic-
itly captures noise/uncertainty in the environment.
this is a major difference to other non-statistical
planning approaches. each transition is also as-
sociated with a reinforcement signal (or reward)
rt+1 describing how good the result of action a
was when performed in state s.

the state space comprises 9 binary features rep-
resenting the number of attributes, 2 binary fea-
tures representing the predicted user   s next ac-
tion to follow the system goal or quit, as well as
a discrete feature re   ecting the number of sen-
tences generated so far, as shown in figure 3.
this results in 211    6 = 12, 288 distinct genera-
tion states. we trained the policy using the well
known sarsa algorithm, using linear function ap-
proximation (sutton and barto, 1998). the policy
was trained for 3600 simulated id86 sequences.

in future work we plan to learn lower level deci-
sions, such as lexical adaptation based on the vo-
cabulary used by the user.

6.5 baselines
we derive the baseline policies from informa-
tion presentation strategies as deployed by cur-
rent dialogue systems. in total we utilise 7 differ-
ent baselines (b1-b7), which correspond to single
branches in our policy space (see figure 1):

b1: recommend only, e.g. (young et al., 2007)
b2: compare only, e.g. (henderson et al., 2008)
b3: summary only, e.g. (polifroni and walker,

2008)

b4: summary followed by recommend, e.g.

(whittaker et al., 2002)

                              action:

               summary

compare
recommend
end

               state:

                              

attributes | 1| -|9| :

0,1

(cid:110)
(cid:110)
(cid:110)

(cid:111)
(cid:111)
(cid:111)

sentence:

1-11

usergoal:

0,1

userquit:

0,1

(cid:110)

(cid:111)

                              
                              

figure 3: state-action space for rl-id86

b5: randomly choosing between compare and

recommend, e.g. (walker et al., 2007)

b6: randomly choosing between all 7 outputs
b7: always generating whole sequence,
i.e.
summary+compare+recommend,
as
suggested by the analytic solution (see
section 4).

6.6 results
we analyse the test runs (n=200) using an anova
with a post-hoc t-test (with bonferroni correc-
tion). rl signi   cantly (p < .001) outperforms all
baselines in terms of    nal reward, see table 5. rl
is the only policy which signi   cantly improves the
next most likely user action by adapting to features
in the current context. in contrast to conventional
approaches, rl learns to    control    its environment
according to the estimated transition probabilities
and the associated rewards.

the learnt policy can be described as follows:
it either starts with summary or compare af-
ter the init state, i.e. it learnt to never start with a
recommend. it stops generating after compare
if the usergoal is (probably) reached (e.g. the
user is most likely to choose an item in the next
turn, which depends on the number of attributes
generated), otherwise it goes on and generates a
recommend. if it starts with summary, it al-
ways generates a compare afterwards. again, it
stops if the usergoal is (probably) reached, oth-
erwise it generates the full sequence (which corre-
sponds to the analytic solution b7).

the analytic solution b7 performs second best,
and signi   cantly outperforms all the other base-
lines (p < .01). still, it is signi   cantly worse
(p < .001) than the learnt policy as this    one-shot-
strategy    cannot robustly and dynamically adopt to
noise or changes in the environment.

in general, generating sequences of id86 ac-
tions rates higher than generating single actions
only: b4 and b6 rate directly after rl and b7,

while b1, b2, b3, b5 are all equally bad given
our data-driven de   nition of reward and environ-
ment. furthermore, the simulated environment
allows us to replicate the results in the match
corpus (see section 4) when only comparing sin-
gle strategies: summary performs signi   cantly
worse, while recommend and compare per-
form equally well.

policy
b1
b2
b3
b4
b5
b6
b7
rl

reward
99.1
90.9
65.5
176.0
95.9
168.8
229.3
310.8

(  std)
(  129.6)
(  142.2)
(  137.3)
(  154.1)
(  144.9)
(  165.3)
(  157.1)
(  136.1)

table 5: evaluation results (p < .001 )

7 conclusion
we presented and evaluated a new model for nat-
ural language generation (id86) in spoken dia-
logue systems, based on statistical planning. after
motivating and presenting the model, we studied
its use in information presentation.

we derived a data-driven model predicting
users    judgements on different information pre-
sentation actions (reward function), via a regres-
sion analysis on match data. we used this re-
gression model to set weights in a reward func-
tion for id23, and so optimise
a context-adaptive presentation policy. the learnt
policy was compared to several baselines derived
from previous work in this area, where the learnt
policy signi   cantly outperforms all the baselines.
there are many possible extensions to this
model, e.g. using the same techniques to jointly
optimise choosing the number of attributes, aggre-
gation, word choice, referring expressions, and so

on, in a hierarchical manner.

we are currently collecting data in targeted
wizard-of-oz experiments, to derive a fully data-
driven training environment and test the learnt
policy with real users,
following (rieser and
lemon, 2008b).
the trained id86 strategy
will also be integrated in an end-to-end statis-
tical system within the classic project (www.
classic-project.org).

acknowledgments

for access to the
we thank marilyn walker
match corpus. the research leading to these
results has received funding from the euro-
pean community   s seventh framework pro-
gramme (fp7/2007-2013) under grant agreement
no.
216594 (classic project project: www.
classic-project.org) and from the ep-
src project no. ep/e019501/1.

references
[baddeley2001] a. baddeley. 2001. working memory
and language: an overview. journal of communica-
tion disorder, 36(3):189   208.

[demberg and moore2006] vera demberg and jo-
hanna d. moore. 2006. information presentation in
spoken dialogue systems. in proceedings of eacl.

[henderson et al.2008] james henderson,

oliver
lemon, and kallirroi georgila. 2008. hybrid rein-
forcement / supervised learning of dialogue policies
from    xed datasets. computational linguistics,
34:4.

[janarthanam and lemon2008] srinivasan

ja-
narthanam and oliver lemon.
user
simulations for online adaptation and knowledge-
alignment in troubleshooting dialogue systems. in
proc. of semdial.

2008.

[koller and petrick2008] alexander koller and ronald
petrick. 2008. experiences with planning for natu-
ral language generation. in icaps.

[koller and stone2007] alexander koller and matthew
stone. 2007. sentence generation as planning. in
proceedings of acl.

[lemon2008] oliver lemon. 2008. adaptive natural
language generation in dialogue using reinforce-
ment learning. in proceedings of semdial.

[moore et al.2004] johanna moore, mary ellen foster,
oliver lemon, and michael white. 2004. generat-
ing tailored, comparative descriptions in spoken di-
alogue. in proc. flairs.

[nakatsu and white2006] crystal nakatsu and michael
white. 2006. learning to say it well: reranking
realizations by predicted synthesis quality. in pro-
ceedings of acl.

[oh and rudnicky2002] alice oh and alexander rud-
nicky. 2002. stochastic natural language genera-
tion for spoken id71. computer, speech
& language, 16(3/4):387   407.

[paek and horvitz2000] tim paek and eric horvitz.
2000. conversation as action under uncertainty. in
proc. of the 16th conference on uncertainty in arti-
   cial intelligence.

[polifroni and walker2008] joseph polifroni and mari-
lyn walker. 2008.
intensional summaries as co-
operative responses in dialogue automation and
evaluation. in proceedings of acl.

[rieser and lemon2008a] verena rieser and oliver
lemon. 2008a. does this list contain what you were
searching for? learning adaptive dialogue strategies
for interactive id53. j. natural lan-
guage engineering, 15(1):55   72.

[rieser and lemon2008b] verena rieser and oliver
lemon. 2008b. learning effective multimodal di-
alogue strategies from wizard-of-oz data: boot-
strapping and evaluation. in proceedings of acl.

[singh et al.2002] s. singh, d. litman, m. kearns, and
m. walker. 2002. optimizing dialogue manage-
ment with id23: experiments
with the njfun system. jair, 16:105   133.

[stent et al.2002] amanda stent, marilyn walker, steve
whittaker, and preetam maloor.
2002. user-
tailored generation for spoken dialogue: an exper-
iment. in in proc. of icslp.

[stent et al.2004] amanda stent, rashmi prasad, and
marilyn walker. 2004. trainable sentence planning
for complex information presentation in spoken dia-
log systems. in association for computational lin-
guistics.

[sutton and barto1998] r. sutton and a. barto. 1998.

id23. mit press.

[walker et al.2000] marilyn a. walker, candace a.
kamm, and diane j. litman. 2000. towards devel-
oping general models of usability with paradise.
natural language engineering, 6(3).

[walker et al.2004] marilyn walker, s. whittaker,
a. stent, p. maloor, j. moore, m. johnston, and
g. vasireddy. 2004. user tailored generation in
the match multimodal dialogue system. cognitive
science, 28:811   840.

[walker et al.2007] marilyn walker, amanda stent,
franc  ois mairesse, and rashmi prasad. 2007. indi-
vidual and id20 in sentence planning
for dialogue. journal of arti   cial intelligence re-
search (jair), 30:413   456.

[whittaker et al.2002] steve whittaker,

marilyn
walker, and johanna moore. 2002. fish or fowl:
a wizard of oz evaluation of dialogue strategies in
the restaurant domain. in proc. of the international
conference on language resources and evaluation
(lrec).

[whittaker et al.2003] stephen whittaker, marilyn
walker, and preetam maloor.
should i
tell all? an experiment on conciseness in spoken
dialogue. in proc. european conference on speech
processing (eurospeech).

2003.

[winterboer et al.2007] andi winterboer, jiang hu, jo-
hanna d. moore, and clifford nass. 2007. the in-
   uence of user tailoring and cognitive load on user
performance in spoken dialogue systems. in proc.
of the 10th international conference of spoken lan-
guage processing (interspeech/icslp).

[young et al.2007] sj young, j schatzmann, k weil-
hammer, and h ye. 2007. the hidden information
state approach to dialog management. in icassp
2007.

