relations such as hypernymy: identifying and exploiting hearst patterns in

distributional vectors for lexical entailment

stephen roller

department of computer science
the university of texas at austin

katrin erk

department of linguistics

the university of texas at austin

6
1
0
2

 

p
e
s
3
2

 

 
 
]
l
c
.
s
c
[
 
 

2
v
3
3
4
5
0

.

5
0
6
1
:
v
i
x
r
a

roller@cs.utexas.edu

katrin.erk@mail.utexas.edu

abstract

we consider the task of predicting lexical
entailment using distributional vectors. we
perform a novel qualitative analysis of one
existing model which was previously shown
to only measure the prototypicality of word
pairs. we    nd that the model strongly learns
to identify hypernyms using hearst patterns,
which are well known to be predictive of lexi-
cal relations. we present a novel model which
exploits this behavior as a method of fea-
ture extraction in an iterative procedure sim-
ilar to principal component analysis. our
model combines the extracted features with
the strengths of other proposed models in the
literature, and matches or outperforms prior
work on multiple data sets.

1

introduction

as the    eld of natural language processing has de-
veloped, more ambitious semantic tasks are starting
to be addressed, such as id53 (qa)
and recognizing id123 (rte). these
systems often depend on the use of lexical resources
like id138 in order to infer entailments for indi-
vidual words, but these resources are expensive to
develop, and always have limited coverage.

to address these issues, many works have con-
sidered on how lexical entailments can be derived
automatically using id65. some
focus mostly on the use of unsupervised techniques,
and study measures which emphasize particular
word relations (baroni and lenci, 2011). many are
based on the distributional inclusion hypothesis,

which states that the contexts in which a hypernym
appears are a superset of its hyponyms    contexts
(zhitomirsky-geffet and dagan, 2005; kotlerman et
al., 2010). more recently, a great deal of work has
pushed toward using supervised methods (baroni et
al., 2012; roller et al., 2014; weeds et al., 2014;
levy et al., 2015; kruszewski et al., 2015), varying
by their experimental setup or proposed model.

yet the literature disagrees about which models
are strongest (weeds et al., 2014; roller et al.,
2014), or even if they work at all (levy et al., 2015).
indeed, levy et al. (2015) showed that two exist-
ing lexical entailment models fail to account for
similarity between the antecedent and consequent,
and conclude that such models are only learning
to predict prototypicality: that is, they predict that
cat entails animal because animal is usually en-
tailed, and therefore will also predict that sofa en-
tails animal. yet it remains unclear why such models
make for such strong baselines (weeds et al., 2014;
kruszewski et al., 2015; levy et al., 2015).

we present a novel qualitative analysis of one pro-
totypicality classi   er, giving new insight into why
prototypicality classi   ers perform strongly in the lit-
erature. we    nd the model overwhelmingly learns
to identify hypernyms using hearst patterns avail-
able in the distributional space, like    animals such
as cats    and    animals including cats.    these pat-
terns have long been used to identify lexical rela-
tions (hearst, 1992; snow et al., 2004).

we propose a novel model which exploits this be-
havior as a method of feature extraction, which we
call h-feature detectors. using an iterative proce-
dure similar to principal component analysis, our

model is able to extract and learn using multiple h-
feature detectors. our model also integrates overall
word similarity and distributional inclusion, bring-
ing together strengths of several models in the litera-
ture. our model matches or outperforms prior work
on multiple data sets. the code, data sets, and model
predictions are made available for future research.1

2 background

research on lexical entailment using distributional
semantics has now spanned more than a decade,
and has been approached using both unsupervised
(weeds et al., 2004; kotlerman et al., 2010; lenci
and benotto, 2012; santus, 2013) and supervised
techniques (baroni et al., 2012; fu et al., 2014;
roller et al., 2014; weeds et al., 2014; kruszewski
et al., 2015; levy et al., 2015; turney and mo-
hammad, 2015; santus et al., 2016). most of the
work in unsupervised methods is based on the dis-
tributional inclusion hypothesis (weeds et al., 2004;
zhitomirsky-geffet and dagan, 2005), which states
that the contexts in which a hypernym appear should
be a superset over its hyponyms    contexts.

this work focuses primarily on the supervised
works in the literature. formally, we consider meth-
ods which treat lexical entailment as a supervised
classi   cation problem, which take as input the dis-
tributional vectors for a pair of words, (h, w), and
predict on whether the antecedent w entails the con-
sequent h.2

one of the earliest supervised approaches was
concat (baroni et al., 2012). in this work, the con-
catenation of the pair (cid:104)h, w(cid:105) was used as input to an
off-the-shelf id166 classi   er. at the time, it was very
successful, but later works noted that it had major
problems with lexical memorization (roller et al.,
2014; weeds et al., 2014; levy et al., 2015). that is,
when the training and test sets were carefully con-
structed to ensure they were completely disjoint, it
performed extremely poorly. nonetheless, concat is
continually used as a strong baseline in more recent
work (kruszewski et al., 2015).

1http://github.com/stephenroller/

emnlp2016

2we use the notation w and h for word and hypernym.
these variables refer to either the lexical items, or their dis-
tributional vectors, depending on context.

in response to these issues of lexical memoriza-
tion, alternative models were proposed. of particu-
lar note are the diff (fu et al., 2014; weeds et al.,
2014) and asym classi   ers (roller et al., 2014). the
diff model takes the vector difference h     w as
input, while the asym model uses both the vector
difference and the squared vector difference as in-
put. weeds et al. (2014) found that concat moder-
ately outperformed diff, while roller et al. (2014)
found that asym outperformed concat. both diff
and asym can also be seen as a form of supervised
distributional inclusion hypothesis, with the vector
difference being analogous to the set-inclusion mea-
sures of some unsupervised techniques (roller et al.,
2014). all of these works focused exclusively on hy-
pernymy detection, rather than the more general task
of lexical entailment.

recently, other works have begun to analyze con-
cat and diff for their ability to go beyond just hyper-
nymy detection. vylomova et al. (2016) take an ex-
tensive look at diff   s ability to model a wide variety
of lexical relations and conclude it is generally ro-
bust, and kruszewski et al. (2015) have success with
a neural network model based on the distributional
inclusion hypothesis.

on the other hand, levy et al. (2015) analyze both
concat and diff in their ability to detect general lex-
ical entailment on    ve data sets: two consisting of
only hypernymy, and three covering a wide variety
of other entailing word relations. they    nd that both
concat and diff fail, and analytically show that they
are learning to predict the prototypicality of the con-
sequent h, rather than the relationship between the
antecedent and the consequent, and consider this a
form of lexical memorization. they propose a new
model, ksim, which addresses their concerns, but
lacks any notion of distributional inclusion. in par-
ticular, they argue for directly including the cosine
similarity of w and h as a term in a custom id166
kernel, in order to determine whether w and h are
related all. ultimately, levy et al. (2015) conclude
that distributional vectors may simply be the wrong
tool for the job.

3 data and resources

prior work on lexical entailment relied on a variety
of data sets, each constructed in a different manner.

we focus on four different data sets, each of which
has been used for evaluation in prior work. two data
sets contain only hypernymy relations, and two con-
sider general lexical entailment.

our    rst data set is leds, the lexical entail-
ment data set, originally created by baroni et al.
(2012).
the data set contains 1385 hyponym-
hypernym pairs extracted directly from id138,
forming a set of positive examples. negative exam-
ples were generated by randomly shuf   ing the orig-
inal set of 1385 pairs. as such, leds only contains
examples of hypernymy and random relations.

another major data set has been bless, the
baroni and lenci (2011) evaluation of semantic
spaces. the data set contains annotations of word
relations for 200 unambiguous, concrete nouns from
17 broad categories. each noun is annotated with its
co-hyponyms, meronyms, hypernym and some ran-
dom words.
in this work, we treat hypernymy as
positive, and other relations as negative.

these two data sets form our hypernymy data
sets, but we cannot overstate their important differ-
ences: leds is balanced, while bless contains
mostly negative examples; negatives in bless in-
clude both random pairs and pairs exhibiting other
strong semantic relations, while leds only contains
random pairs. furthermore, all of the negative ex-
amples in leds are the same lexical items as the
positive items, which has strong implications on the
prototypicality argument of levy et al. (2015).

the next data set we consider is medical (levy et
al., 2014). this data set contains high quality anno-
tations of subject-verb-object entailments extracted
from medical texts, and transformed into noun-noun
entailments by argument alignments. the data con-
tains 12,600 annotations, but only 945 positive ex-
amples encompassing various relations like hyper-
nymy, meronomy, synonymy and contextonymy.3
this makes it one of the most dif   cult data sets: it is
both domain speci   c and highly unbalanced.

the    nal data set we consider is tm14, a varia-
tion on the semeval 2012 shared task of identifying
the degree to which word pairs exhibit various rela-
tions. these relationships include a small amount
of hypernymy, but also many more uncommon rela-

3a term for entailments that occur in some contexts, but do

not cleanly    t in other categories; e.g. hospital entails doctor.

tions (agent-object, cause-effect, time-activity, etc).
relationships were binarized into (non-)entailing
pairs by turney and mohammad (2015). the data
set covers 2188 pairs, 1084 of which are entailing.

these two entailment data sets also contain im-
portant differences, especially in contrast to the hy-
pernymy data sets. neither contains any random
negative pairs, meaning general semantic similarity
measures should be less useful; and both exhibit a
variety of non-hypernymy relations, which are less
strictly de   ned and more dif   cult to model.

3.1 distributional vectors
in all experiments, we use a standard, count-based,
syntactic distributional vector space. we use a cor-
pus composed of the concatenation of gigaword,
wikipedia, bnc and ukwac. we preprocess the
corpus using stanford corenlp 3.5.2 (chen and
manning, 2014) for id121,
lemmatization,
pos-tagging and universal dependency parses. we
compute a syntactic distributional space for the 250k
most frequent lemmas by counting their dependency
neighbors across the corpus. we use only the top 1m
most frequent dependency attachments as contexts.
we use corenlp   s    collapsed dependencies   ,
in
which prepositional dependencies are collapsed e.g.
   go to the store    emits the tuples (go, prep:to+store)
and (store, prep:to   1+go). after collecting counts,
vectors are transformed using ppmi, svd reduced
to 300 dimensions, and normalized to unit length.
the use of collapsed dependencies is very important,
as we will see in section 4, but other parameters are
reasonably robust.

4 motivating analysis

as discussed in section 2, the concat classi   er is a
classi   er trained on the concatenation of the word
vectors, (cid:104)h, w(cid:105). as additional background, we
   rst review the    ndings of levy et al. (2015), who
showed that concat trained using a linear classi   er is
only able to capture notions of prototypicality; that
is, concat guesses that (animal, sofa) is a positive
example because animal looks like a hypernym.

formally, a linear classi   er like logistic regres-
sion or linear id166 learns a decision hyperplane
represented by a vector   p. data points are compared
to this plane with the inner product:
those above

the plane (positive inner product) are classi   ed as
entailing, and those below as non-entailing. cru-
cially, since the input features are the concatenation
of the pair vectors (cid:104)h, w(cid:105), the hyperplane   p vec-
tor can be decomposed into separate h and w com-
ponents. namely, if we rewrite the decision plane
  p = (cid:104)   h,   w(cid:105), we    nd that each pair (cid:104)h, w(cid:105) is classi-
   ed using:

  p(cid:62)(cid:104)h, w(cid:105)
= (cid:104)   h,   w(cid:105)(cid:62)(cid:104)h, w(cid:105)
=   h(cid:62)h +   w(cid:62)w.

(1)

this analysis shows that, when the hyperplane   p is
evaluated on a novel pair, it lacks any form of direct
interaction between h and w like the inner prod-
uct h(cid:62)w. without any interaction terms, the con-
cat classi   er has no way of estimating the relation-
ship between the two words, and instead only makes
predictions based on two independent terms,   h and
  w, the prototypicality vectors. furthermore, the diff
classi   er can be analyzed in the same fashion and
therefore has the same fatal property.

we agree with this prototypicality interpretation,
although we believe it is incomplete: while it places
a fundamental ceiling on the performance of these
classi   ers, it does not explain why others have found
them to persist as strong baselines (weeds et al.,
2014; roller et al., 2014; kruszewski et al., 2015;
vylomova et al., 2016). to approach this ques-
tion, we consider a baseline concat classi   er trained
using a linear model. this classi   er should most
strongly exhibit the prototypicality behavior accord-
ing to equation 1, making it the best choice for anal-
ysis. we    rst consider the most pessimistic hypothe-
sis: is it only learning to memorize which words are
hypernyms at all?

we train the baseline concat classi   er using lo-
gistic regression on each of the four data sets, and
extract the vocabulary words which are most simi-
lar to the   h half of the learned hyperplane   p. if the
classi   er is only learning to memorize the training
data, we would expect items from the data to dom-
inate this list of closest vocabulary terms. table 1
gives the    ve most similar words to the learned hy-
perplane, with bold words appearing directly in the
data set.

interestingly, we notice there are very few bold
words at all in the list. in leds, we actually see

leds
material
structure
object
process
activity

bless
goods
lifeform
item
equipment
herbivore

medical
item
unlockable
succor
team-up
non-essential

tm14
sensitiveness
tactility
palate
stiffness
content

table 1: most similar words to the prototype   h learned by the
concat model. bold items appear in the data set.

some hypernyms of data set items that do not even
appear in the data set, and the medical and tm14
words do not even appear related to the content of
the data sets. similar results were also found for
diff and asym, and both when using linear id166
and id28. these lists cannot explain
the success of the prototypicality classi   ers in prior
work. instead, we propose an alternative interpreta-
tion of the hyperplane: that of a feature detector for
hypernyms, or an h-feature detector.

4.1 h-feature detectors
recall that distributional vectors are derived from
a matrix m containing counts of how often words
co-occur with the different syntactic contexts. this
co-occurrence matrix is factorized using singular
value decomposition, producing both w , the ubiq-
uitous word-embedding matrix, and c, the context-
embedding matrix (levy and goldberg, 2014):

m     w c(cid:62)

since the word and context embeddings implicitly
live in the same vector space (melamud et al., 2015),
we can also compare concat   s hyperplane with the
context matrix c. under this interpretation, the
concat model does not learn what words are hy-
pernyms, but rather what contexts or features are in-
dicative of hypernymy. table 2 shows the syntactic
contexts with the highest cosine similarity to the   h
prototype for each of the different data sets.

this view of concat as an h-feature detector
produces a radically different perspective on the
classi   er   s hyperplane. nearly all of the features
learned take the form of hearst patterns (hearst,
1992; snow et al., 2004). the most recognizable
and common pattern learned is the    such as    pat-
tern, as in    animals such as cats   . these patterns
have been well known to be indicative of hyper-
nymy for over two decades. other interesting pat-

leds
nmod:such as+animal
acl:relcl+identi   able
nmod:of   1+determine
nmod:of   1+categorisation
compound+many
nmod:such as+pot
medical
nmod:such as+patch
nmod:such as+skin
nmod:including+skin
nmod:such as+tooth
nmod:such as+feather
nmod:including+   nger

bless
nmod:such as+submarine
nmod:such as+ship
nmod:such as+seal
nmod:such as+plane
nmod:such as+rack
nmod:such as+rope
tm14
amod+desire
amod+heighten
nsubj   1+disparate
nmod:such as+honey
nmod:with   1+body
nsubj   1+unconstrained

table 2: most similar contexts to the prototype   h learned by
the concat model.

terns are the    including    pattern (   animals includ-
ing cats   ) and    many    pattern (   many animals   ).
although we list only the six most similar context
items for the data sets, we    nd similar contexts con-
tinue to dominate the list for the next 30-50 items.
taken together, it is remarkable that the model iden-
ti   ed these patterns using only distributional vectors
and only the positive/negative example pairs. how-
ever, the reader should note these are not true hearst
patterns: hearst patterns explicitly relate a hyper-
nym and hyponym using an exact pattern match of
a single co-occurrence. on the other hand, these
h-features are aggregate indicators of hypernymy
across a large corpus.

these learned features are much more inter-
pretable than those found in the analysis of prior
work like roller et al. (2014) and levy et al. (2015).
roller et al. (2014) found no signals of h-features
in their analysis of one classi   er, but their model
was focused on bag-of-words distributional vectors,
which perform signi   cantly worse on the task. levy
et al. (2015) also performed an analysis of lexical
entailment classi   ers, and found weak signals like
   such    and    of    appearing as prominent contexts in
their classi   er, giving an early hint of h-feature de-
tectors, but not to such an overwhelming degree as
we see in this work. critically, their analysis fo-
cused on a classi   er trained on high-dimensional,
sparse vectors, rather than focusing on context em-
beddings as we do. by using these sparse vectors,
their model was unable to generalize across simi-

lar contexts. additionally, their model did not make
use of collapsed dependencies, making features like
   such    much weaker signals of entailment and there-
fore less dominant during analysis.
among these remarkable lists,

the leds and
tm14 data sets stand out for having much fewer
   such as    patterns compared to bless and medi-
cal. the reason for this is explained by the construc-
tion of the data sets: since leds contains the same
words used as both positive and negative examples,
the classi   er has a hard time picking out clear sig-
nal. the tm14 data set, however, does not contain
any such negative examples.

we hypothesize the tm14 data set contains too
many diverse and mutually exclusive forms of lex-
ical entailment, like instrument-goal (e.g.    honey   
       sweetness   ). to test this, we retrained the model
with only hypernymy as positive examples, and all
other relations as negative. we    nd that    such as   
type patterns become top features, but also some
interesting data speci   c features, like    retailer of
[clothes]   . examining the data shows it contains
many consumer goods, like    beverage    or    clothes   ,
which explains these features.

5 proposed model

as we saw in the previous section, concat only acts
as a sort of h-feature detector for whether h is a
prototypical hypernym, but does not actually infer
the relationship between h and w. nonetheless, this
is powerful behavior which should still be used in
combination with the insights of other models like
ksim and asym. to this end, we propose a novel
model which exploits concat   s h-feature detector
behavior, extends its modeling power, and adds two
other types of evidence proposed in the literature:
overall similarity, and distributional inclusion.

our model works through an iterative procedure
similar to principal component analysis (pca).
each iteration repeatedly trains a concat classi   er
under the assumption that it acts as an h-feature de-
tector, and then explicitly discards this information
from the distributional vectors. by training a new
h-feature detector on these modi   ed distributional
vectors, we can    nd additional features indicative of
entailment which were missed by the    rst classi   er.
the entire procedure is iteratively repeated similar

wish to classify the pairs without using any informa-
tion learned from the previous iteration.

this second classi   er must perform strictly worse
than the original, otherwise the    rst classi   er would
have learned this second hyperplane. nonetheless,
it will be able to learn new h-feature detectors
which the original classi   er was unable to capture.
by repeating this process, we can    nd several h-
feature detectors,   p1, . . . ,   pn. although the    rst,   p1
is the best possible single h-feature detector, each
additional h-feature detector increases the model   s
representational power (albeit with diminishing re-
turns).

this procedure alone does not address the main
concern of levy et al. (2015): that these linear clas-
si   ers never actually model any connection between
h and w. to address this, we explicitly compare
h and w by extracting additional information about
how h and w interact with respect to each of the
h-feature detectors. this additional information is
then used to train one    nal classi   er which makes
the    nal prediction.

concretely, in each iteration i of the procedure,
we generate a four-valued feature vector fi, based
on the h-feature detector   pi. each feature vector
contains (1) the similarity of hi and wi (before pro-
jection); (2) the feature   pi applied to hi; (3) the h-
feature detector   pi applied to wi; and (4) the differ-
ence of 2 and 3.

fi((cid:104)hi, wi(cid:105),   pi)

= (cid:104)h(cid:62)

i wi, h(cid:62)

i   pi, w(cid:62)

i   pi, (hi     wi)(cid:62)   pi(cid:105)

these four    meta   -features capture all the bene-
   ts of the h-feature detector (slots 2 and 3), while
still addressing concat   s issues with similarity argu-
ments (slot 1) and distributional inclusion (slot 4).
the    nal feature   s relation to the dih comes from
the observation of roller et al. (2014) that the vec-
tor difference intuitively captures whether the hyper-
nym includes the hyponym.

the union of all the feature vectors f1, . . . , fn
from repeated iteration form a 4n-dimensional fea-
ture vector which we use as input to one    nal classi-
   er which makes the ultimate decision. this classi-
   er is trained on the same training data as each of the
individual h-feature detectors, so our iterative pro-
cedure acts only as a method of feature extraction.

figure 1: a vector   p is used to break x into two orthogonal
components, its projection and the rejection over   p.

to how in principal component analysis, the second
principal component is computed after the    rst prin-
cipal component has been removed from the data.

the main insight is that after training some h-
feature detector using concat, we can remove this
prototype from the distributional vectors through the
use of vector projection. formally, the vector pro-
jection of x onto a vector   p, proj  p(x)    nds the com-
ponent of x which is in the direction of   p,

(cid:18) x(cid:62)   p

(cid:19)

(cid:107)  p(cid:107)

  p.

proj  p(x) =

figure 1 gives a geometric illustration of the vector
projection. if x forms the hypotenuse of a right tri-
angle, proj  p(x) forms a leg of the triangle. this also
gives rise to the vector rejection, which is the vec-
tor forming the third leg of the triangle. the vector
rejection is orthogonal to the projection, and intu-
itively, is the original vector after the projection has
been removed:

rej  p(x) = x     proj  p(x).

using the vector rejection, we take a learned h-
feature detector   p, and discard these features from
each of the word vectors. that is, for every data
point (cid:104)h, w(cid:105), we replace it by its vector rejection
and rescale it to unit magnitude:

hi+1 = rej  p(h)/(cid:107)rej  p(h)(cid:107)
wi+1 = rej  p(w)/(cid:107)rej  p(w)(cid:107)

a new classi   er trained on the (cid:104)hi+1, wi+1(cid:105) data
must now learn a different decision plane than   p, as
  p is no longer present in any data points. this repeti-
tion of the procedure is roughly analogous to learn-
ing the second principal component of the data; we

for our    nal classi   er, we use an id166 with an
rbf-kernel, though id90 and other non-
linear classi   ers also perform reasonably well. the
nonlinear    nal classi   er can be understood as do-
ing a form of logical reasoning about the four slots:
   animal    is a hypernym of    cat    because (1) they are
similar words where (2) animal looks like a hyper-
nym, but (3) cat does not, and (4) some    animal   
contexts are not good    cat    contexts.

6 experimental setup and evaluation

in our experiments, we use a variation of 20-fold
cross validation which accounts for lexical overlap.
to simplify explanation, we    rst explain how we
generate splits for training/testing, and then after-
wards introduce validation methodology.

we    rst pool all the words from the antecedent
(lhs) side of the data into a set, and split these lex-
ical items into 20 distinct cross-validation folds. for
each fold fi, we then use all pairs (w, h) where
w     fi as the test set pairs. that is, if    car    is in
the test set fold, then    car     vehicle    and    car (cid:57)
truck    will appear as test set pairs. the training set
will then be every pair which does not contain any
overlap with the test set; e.g. the training set will be
all pairs which do not contain    car   ,    truck    or    ve-
hicle    as either the antecedent or consequent. this
ensures that both (1) there is zero lexical overlap be-
tween training and testing and (2) every pair is used
as an item in a test fold exactly once. one quirk of
this setup is that all test sets are approximately the
same size, but training sizes vary dramatically.

this setup differs from those of previous works
like kruszewski et al. (2015) and levy et al. (2015),
who both use single,    xed train/test/val sets without
lexical overlap. we    nd our setup has several advan-
tages over    xed sets. first, we    nd there can be con-
siderable variance if the train/test set is regenerated
with a different random seed, indicating that multi-
ple trials are necessary. second,    xed setups con-
sistently discard roughly half the data as ineligible
for either training or test, as lexical items appear in
many pairs. our cv-like setup allows us to evaluate
performance over every item in the data set exactly
once, making a much more ef   cient and representa-
tive use of the original data set.

our performance metric is f1 score. this is more

model

leds bless medical tm14

cosine
concat
diff
asym
concat+diff
concat+asym

rbf
ksim
our model

linear models
.208
.787
.612
.794
.805
.440
.510
.865
.604
.801
.631
.843
nonlinear models
.779
.893
.901

.574
.488
.631

.168
.218
.195
.210
.224
.240

.215
.224
.260

.676
.693
.665
.671
.703
.701

.705
.707
.697

table 3: mean f1 scores for each model and data set.

representative than accuracy, as most of the data sets
are heavily unbalanced. we report the mean f1
scores across all cross validation folds.

6.1 hyperparameter optimization
in order to handle hyperparameter selection, we ac-
tually generate the test set using fold i, and use
fold i     1 as a validation set (removing pairs which
would overlap with test), and the remaining 18
folds as training (removing pairs which would over-
lap with test or validation). we select hyperpa-
rameters using grid search. for all models, we
optimize over the id173 parameter c    
{10   4, 10   3, . . . , 104}, and for our proposed model,
the number of iterations n     {1, . . . , 6}. all other
hyperparameters are left as defaults provided by
scikit-learn (pedregosa et al., 2011), except for us-
ing balanced class weights. without balanced class
weights, several of the baseline models learn degen-
erate functions (e.g. always guess non-entailing).

7 results

we compare our proposed model to several ex-
isting and alternative baselines from the literature.
namely, we include a baseline cosine classi   er,
which only learns a threshold which maximizes f1
score on the training set; three linear models of prior
work, concat, diff and asym; and the rbf and
ksim models found to be successful in kruszewski
et al. (2015) and levy et al. (2015). we also in-
clude two additional novel baselines, concat+diff
and concat+asym, which add a notion of distri-
butional inclusion into the concat baseline, but are
still linear models. we cannot include baselines like

model
no similarity
no detectors
no inclusion

leds bless medical tm14
.003
.028
.001

.099
-.008
.010

.061
.136
.031

.034
.018
.014

table 4: absolute decrease in mean f1 on the development
sets with the different feature types ablated. higher numbers
indicate greater feature importance.

ksim+asym, because ksim is based on a custom
id166 kernel which is not amenable to combinations.
table 3 the results across all four data sets for all
of the listed models. our proposed model improves
signi   cantly4 over concat in the leds, bless and
medical data sets, indicating the bene   ts of combin-
ing these aspects of similarity and distributional in-
clusion with the h-feature detectors of concat. the
concat+asym classi   er also improves over the con-
cat baseline, further emphasizing these bene   ts. our
model performs approximately the same as ksim
on the leds and tm14 data sets (no signi   cant
difference), while signi   cantly outperforming it on
bless and medical data sets.

7.1 ablation experiments
in order to evaluate how important each of the vari-
ous f features are to the model, we also performed
an ablation experiment where the classi   er is not
given the similarity (slot 1), prototype h-feature de-
tectors (slots 2 and 3) or the inclusion features (slot
4). to evaluate the importance of these features,
we    x the id173 parameter at c = 1, and
train all ablated classi   ers on each training fold with
number of iterations n = 1, . . . , 6. table 4 shows
the decrease (absolute difference) in performance
between the full and ablated models on the develop-
ment sets, so higher numbers indicate greater feature
importance.

we    nd the similarity feature is extremely impor-
tant in the leds, bless and medical data sets,
therefore reinforcing the    ndings of levy et al.
(2015). the similarity feature is especially impor-
tant in the leds and bless data sets, where neg-
ative examples include many random pairs. the
detector features are moderately important for the
medical and tm14 data sets, and critically impor-
tant on bless, where we found the strongest evi-

4bootstrap test, p < .01.

dence of hearst patterns in the h-feature detectors.
surprisingly, the detector features are moderately
detrimental on the leds data set, though this can
also be understood in the data set   s construction:
since the negative examples are randomly shuf   ed
positive examples, the same detector signal will ap-
pear in both positive and negative examples. finally,
we    nd the model performs somewhat robustly with-
out the inclusion feature, but still is moderately im-
pactful on three of the four data sets, lending further
evidence to the distributional inclusion hypothesis.
in general, we    nd all three components are valu-
able sources of information for identifying hyper-
nymy and lexical entailment.

7.2 analysis by number of iterations
in order to evaluate how the iterative feature extrac-
tion affects model performance, we    x the regular-
ization parameter at c = 1, and train our model
   xing the number of iterations to n = {1, . . . , 6}.
we then measure the mean f1 score across the de-
velopment folds and compare to a baseline which
uses only one iteration. figure 2 shows these results
across all four data sets, with the 0 line set at per-
formance of the n = 1 baseline. models above 0
bene   t from the additional iterations, while models
below do not.

in the    gure, we see that

the iterative pro-
cedure moderately improves performance leds,
while greatly improving the scores of bless and
tm14, but on the medical data set, additional it-
erations actually hurt performance. the differing
curves indicate that the optimal number of itera-
tions is very data set speci   c, and provides differing
amounts of improvement, and therefore should be
tuned carefully. the leds and bless curves indi-
cate a sort of    sweet spot    behavior, where further
iterations degrade performance.

to gain some additional insight into what is cap-
tured by the various iterations of the feature extrac-
tion procedure, we repeat the procedure from sec-
tion 4: we train our model on the entire bless
data set using a    xed four iterations and regular-
ization parameter. for each iteration, we compare
its learned h-feature detector to the context embed-
dings, and report the most similar contexts for each
iteration in table 5.

the    rst iteration is identical to the one in ta-

figure 2: performance of model on development folds by number of iterations. plots show the improvement (absolute difference)
in mean f1 over the model    xed at one iteration.

iteration 1
nmod:such as+submarine
nmod:such as+ship
nmod:such as+seal
nmod:such as+plane
nmod:such as+rack
nmod:such as+rope
nmod:such as+box

iteration 2
nmod:including+animal
nmod:including+snail
nmod:including+insect
nmod:such as+crustacean
nmod:such as+mollusc
nmod:such as+insect
nmod:such as+animal

iteration 3
amod+free-swimming
nmod:including   1+thing
nsubj   1+scarcer
nsubj   1+pupate
nmod:such as+mollusc
nmod:of   1+value
nmod:as   1+exhibit

iteration 4
advcl+crown
advcl+victorious
nsubj+eaters
nsubj+kaine
nmod:at+   nale
nsubj+gowen
nsubj+pillman

table 5: most similar contexts to the h-feature detector for each iteration of the pca-like procedure. this model was trained on all
data of bless. the    rst and second iterations contain clear hearst patterns, while the third and fourth contain some data-speci   c
and non-obvious signals.

ble 2, as expected. the second iteration includes
many h-features not picked up by the    rst itera-
tion, mostly those of the form    x including y   . the
third iteration picks up some data set speci   c signal,
like    free-swimming [animal]    and    value of [com-
puter]   , and so on. by the fourth iteration, the fea-
tures no longer exhibit any obvious hearst patterns,
perhaps exceeding the sweet spot we observed in
figure 2. nonetheless, we see how multiple iter-
ations of the procedure allows our model to capture
many more useful features than a single concat clas-
si   er on its own.

8 conclusion

we considered the task of detecting lexical entail-
ment using distributional vectors of word meaning.
motivated by the fact that the concat classi   er acts
as a strong baseline in the literature, we proposed a
novel interpretation of the model   s hyperplane. we
found the concat classi   er overwhelmingly acted
as a feature detector which automatically identi   es
hearst patterns in the distributional vectors.

we proposed a novel model that embraces these

h-feature detectors fully, and extends their model-
ing power through an iterative procedure similar to
principal component analysis. in each iteration of
the procedure, an h-feature detector is learned, and
then removed from the data, allowing us to iden-
tify several different kinds of hearst patterns in the
data. our    nal model combines these h-feature de-
tectors with measurements of general similarity and
distributional inclusion, in order to integrate the
strengths of different models in prior work. our
model matches or exceeds the performance of prior
work, both on hypernymy detection and general lex-
ical entailment.

acknowledgments
the authors would like to thank i. beltagy, vered
shwartz, subhashini venugopalan, and the review-
ers for their helpful comments and suggestions.
this research was supported by the nsf grant iis
1523637. we acknowledge the texas advanced
computing center for providing grid resources that
contributed to these results.

ledsblessmedicaltm14   0.02   0.010.000.010.020.030.040.050.06123456123456123456123456number of iterationsrelative f1references
[baroni and lenci2011] marco baroni and alessandro
lenci. 2011. how we blessed distributional se-
mantic evaluation. in proceedings of the gems 2011
workshop on geometrical models of natural lan-
guage semantics, pages 1   10, edinburgh, uk.

[baroni et al.2012] marco baroni, raffaella bernardi,
ngoc-quynh do, and chung-chieh shan. 2012. en-
tailment above the word level in distributional seman-
tics. in proceedings of the 2012 conference of the eu-
ropean chapter of the association for computational
linguists, pages 23   32, avignon, france.

[chen and manning2014] danqi chen and christopher
2014. a fast and accurate dependency
manning.
in proceedings of the
parser using neural networks.
2014 conference on empirical methods in natural
language processing, pages 740   750, doha, qatar.

[fu et al.2014] ruiji fu, jiang guo, bing qin, wanxiang
che, haifeng wang, and ting liu. 2014. learning
in pro-
semantic hierarchies via id27s.
ceedings of the 2014 annual meeting of the associa-
tion for computational linguistics, pages 1199   1209,
baltimore, maryland.

[hearst1992] marti a hearst. 1992. automatic acquisi-
tion of hyponyms from large text corpora. in proceed-
ings of the 1992 conference on computational lin-
guistics, pages 539   545, nantes, france.

[kotlerman et al.2010] lili kotlerman, ido dagan, idan
szpektor, and maayan zhitomirsky-geffet. 2010. di-
rectional distributional similarity for lexical id136.
natural language engineering, 16:359   389, 10.

[kruszewski et al.2015] germ  an kruszewski, denis pa-
perno, and marco baroni. 2015. deriving boolean
structures from distributional vectors. transactions of
the association for computational linguistics, 3:375   
388.

[lenci and benotto2012] alessandro lenci and giulia
benotto. 2012.
identifying hypernyms in distribu-
tional semantic spaces. in the first joint conference
on lexical and computational semantics, pages 75   
79, montr  eal, canada.

[levy and goldberg2014] omer levy and yoav gold-
berg. 2014. neural id27 as implicit ma-
trix factorization. in advances in neural information
processing systems, pages 2177   2185.

[levy et al.2014] omer levy,

ido dagan, and jacob
focused entailment graphs
goldberger.
in proceedings of the
for open ie propositions.
2014 conference on computational natural language
learning, pages 87   97, ann arbor, michigan.

2014.

[levy et al.2015] omer levy, steffen remus, chris bie-
mann, and ido dagan. 2015. do supervised distribu-
tional methods really learn lexical id136 relations?

in proceedings of the 2015 north american chapter
of the association for computational linguistics: hu-
man language technologies, pages 970   976, denver,
colorado.

[melamud et al.2015] oren melamud, omer levy, and
ido dagan. 2015. a simple id27 model
in proceedings of the first
for lexical substitution.
workshop on vector space modeling for natural lan-
guage processing, pages 1   7, denver, colorado.

[pedregosa et al.2011] f. pedregosa, g. varoquaux,
a. gramfort, v. michel, b. thirion, o. grisel,
m. blondel, p. prettenhofer, r. weiss, v. dubourg,
j. vanderplas, a. passos, d. cournapeau, m. brucher,
m. perrot, and e. duchesnay. 2011. scikit-learn:
journal of machine
machine learning in python.
learning research, 12:2825   2830.

[roller et al.2014] stephen roller, katrin erk,

and
gemma boleda. 2014. inclusive yet selective: super-
vised distributional hypernymy detection. in proceed-
ings of the 2014 international conference on com-
putational linguistics, pages 1025   1036, dublin, ire-
land.

[santus et al.2016] enrico santus, alessandro lenci, tin-
shing chiu, qin lu, and chu-ren huang. 2016. nine
features in a id79 to learn taxonomical se-
mantic relations. in proceedings of the tenth interna-
tional conference on language resources and evalu-
ation, paris, france.

[santus2013] enrico santus. 2013. slqs: an id178

measure. master   s thesis, university of pisa.

[snow et al.2004] rion snow, daniel jurafsky, and an-
drew y ng. 2004. learning syntactic patterns for au-
tomatic hypernym discovery. in advances in neural
information processing systems, pages 1297   1304.

[turney and mohammad2015] peter d turney

and
saif m mohammad. 2015. experiments with three
approaches to recognizing lexical entailment. natural
language engineering, 21(03):437   476.

[vylomova et al.2016] ekaterina vylomova,

laura
rimell, trevor cohn, and timothy baldwin. 2016.
take and took, gaggle and goose, book and read:
evaluating the utility of vector differences for lexical
in proceedings of the 54th an-
relation learning.
nual meeting of the association for computational
linguistics, pages 1671   1682, berlin, germany,
august.

[weeds et al.2004] julie weeds, david weir, and diana
mccarthy. 2004. characterising measures of lexical
in proceedings of the 2004
distributional similarity.
international conference on computational linguis-
tics, pages 1015   1021, geneva, switzerland.

[weeds et al.2014] julie weeds, daoud clarke, jeremy
ref   n, david weir, and bill keller. 2014. learn-
ing to distinguish hypernyms and co-hyponyms.
in

proceedings of the 2014 international conference on
computational linguistics, pages 2249   2259, dublin,
ireland.

[zhitomirsky-geffet and dagan2005] maayan

the
zhitomirsky-geffet and ido dagan.
distributional inclusion hypotheses and lexical entail-
ment. in proceedings of the 2005 annual meeting of
the association for computational linguistics, pages
107   114, ann arbor, michigan.

2005.

