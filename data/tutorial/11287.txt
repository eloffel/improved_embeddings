on improving informativity and grammaticality

for multi-sentence compression

elahe sha   ei mohammad ebrahimi

raymond wong

fang chen

university of new south wales, australia

atp laboratory, national ict, sydney, australia
{elahehs,mohammade,wong,fang}@cse.unsw.edu.au

technical report

unsw-cse-tr-201517

november 2015

school of computer science and engineering

the university of new south wales

sydney 2052, australia

6
1
0
2

 

y
a
m
7

 

 
 
]
l
c
.
s
c
[
 
 

1
v
0
5
1
2
0

.

5
0
6
1
:
v
i
x
r
a

theuniversityofnewsouthwalesabstract

multi sentence compression (msc) is of great value to many real world appli-
cations, such as guided microblog summarization, opinion summarization and
newswire summarization. recently, word graph-based approaches have been
proposed and become popular in msc. their key assumption is that redun-
dancy among a set of related sentences provides a reliable way to generate
informative and grammatical sentences. in this paper, we propose an e   ective
approach to enhance the word graph-based msc and tackle the issue that most
of the state-of-the-art msc approaches are confronted with: i.e., improving both
informativity and grammaticality at the same time. our approach consists of
three main components: (1) a merging method based on multiword expres-
sions (mwe); (2) a mapping strategy based on synonymy between words; (3)
a re-ranking step to identify the best compression candidates generated using
a pos-based language model (pos-lm). we demonstrate the e   ectiveness of
this novel approach using a dataset made of clusters of english newswire sen-
tences. the observed improvements on informativity and grammaticality of the
generated compressions show that our approach is superior to state-of-the-art
msc methods.

1

introduction

multi-sentence compression (msc) refers to the method of mapping a collec-
tion of related sentences to a sentence shorter than the average length of the
input sentences, while retaining the most important information that conveys
the gist of the content, and still remain grammatically correct [16, 4]. msc is
one of the challenging tasks in natural language processing that has recently
attracted increasing interest [4]. this is mostly because of its potential use in
various applications such as guided microblog summarization, opinion summa-
rization, newswire summarization, text simpli   cation for mobile devices and so
on. a standard way to generate summaries usually consists of the following
steps: ranking sentences by their importance, id91 them by similarity, and
selecting a sentence from the top ranked clusters [31].

traditionally, most of the msc approaches rely on syntactic parsers, e.g.
[10, 8]. as an alternative, some recent works in this    eld [9, 4] are based on
word graphs, which only require a part-of-speech (pos) tagger and a list of
stopwords. these approaches simply rely on the words of the sentences and
e   cient id145. they take advantage of the redundancy among
a set of related sentences to generate informative and grammatical sentences.

although the proposed approach in [9] introduces an elegant word graph
to msc, approximately half of their generated sentences are missing important
information about the set of related sentences [4]. afterwards, boudin and
morin (2013) enhanced their work and produced more informative sentences by
maximizing the range of topics they cover. however, they con   rmed that gram-
maticality scores are decreased, since their re-ranking algorithm produces longer
compressions to ameliorate informativity. therefore, grammaticality might be
sacri   ced while enhancing informativity and vice versa.

in this paper, we are motivated to tackle the main di   culty of the above
mentioned msc approaches which is to simultaneously improve both informativ-
ity and grammaticality of the compressed sentences. to this end, we propose a
novel enhanced word graph-based msc approach by employing signi   cant merg-
ing, mapping and re-ranking steps that favor more informative and grammatical
compressions. the contributions of the proposed method can be summarized
as follows: (1) we exploit multiword expressions (mwe) from the given sen-
tences and merge their words, constructing each mwe into a speci   c node in
the word graph to reduce the ambiguity of mapping, so that well-organized and
more informative compressions can be produced; (2) we take advantage of the
concept of synonymy in two ways:    rstly, we replace a merged mwe with its
one-word synonym if available, and secondly, we use the synonyms of an up-
coming single word to    nd the most proper nodes for mapping; (3) we employ
a 7-gram pos-based language model (pos-lm) to re-rank the k -shortest ob-
tained paths, and produce well-structured and more grammatical compressions.
to our knowledge, this paper presents the    rst attempt to use mwes, synonymy
and pos-lm to improve the quality of word graph-based msc. extensive ex-
periments on the released standard dataset demonstrate the e   ectiveness of our
proposed approach. figure 1.1 also depicts the overview of this approach.

the rest of this paper is organized as follows. section 2 summarizes the
related work. section 3 presents our proposed approach. the data preparation
process for evaluating our method is demonstrated in section 4, and section 5
reports the id74 and the performed experiments. finally, section

1

figure 1.1: overview of the proposed approach

6 concludes the paper.

2 related work

2.1 multi-sentence compression

state-of-the-art approaches in the    eld of msc are generally divided into su-
pervised [21, 11] and unsupervised groups [6]. msc methods traditionally use a
syntactic parser to generate grammatical compressions, and fall into two cate-
gories (based on their implementations): (1) tree-based approaches, which create
a compressed sentence by making edits to the syntactic tree of the original sen-

2

mwe detectionsynonym mappingid138jmweclusters of relevant sentencesdataset preparationpre-processinggraph constructionre-rankingpos-taggingtokenizingid30k-shortest pathscompression candidatespos-based re-rankingword-graph constructorsrilmpos-lmgoogle newspos-taggingnewswire corpusbest compression candidatesoutputtence [21, 11, 10, 8]; (2) sentence-based approaches, which generates strings
directly [6].

as an alternative, word graph-based approaches that only require a pos
tagger have recently been used in di   erent tasks, such as guided microblog
summarization [27], opinion summarization [12] and newswire summarization
[9, 4, 30]. in these approaches, a directed word graph is constructed in which
nodes represent words while edges between two nodes represent adjacency re-
lations between words in a sentence. hence, the task of sentence compression
is performed by    nding the k-shortest paths in the word graph. in particular,
our work is applied to newswire summarization. in this    eld, filippova (2010)
has introduced an elegant word graph-based msc approach that relies on the
redundancy among the set of related sentences. however, some important infor-
mation are missed from 48% to 60% of the generated sentences in their approach
[4]. thus, boudin and morin (2013) proposed an additional re-ranking scheme
to identify summarizations that contain key phrases. however, they mentioned
that grammaticality is sacri   ced to improve informativity in their work.

in our proposed approach, we utilize mwes and synonym words in sentences
to signi   cantly enhance the traditional word graph, and improve informativity.
then, we re-rank the generated compression candidates with a 7-gram pos-
lm that captures the syntactic information, and strengthens the compressed
sentences in terms of grammaticality.

2.2 multiword expressions

an mwe is a combination of words with lexical, syntactic or semantic idiosyn-
crasy [26, 3].
it is estimated that the number of mwes in the lexicon of a
native speaker of a language has the same order of magnitude as the number
of single words [15]. hence, explicit identi   cation of mwes has been shown to
be useful in various nlp applications. components of an mwe can be treated
as a single unit to improve the e   ectiveness of re-ranking steps in ir systems
[1].
in this paper, we identify mwes, merge their components, and replace
them with their available one-word synonyms, if applicable. these strategies
help to construct an improved word graph and enhance the informativity of the
compression candidates.

2.3 pos-based language model (pos-lm)

a language model assigns a id203 to a sequence of m words p (w1, ..., wm)
by means of a id203 distribution. language models are an essential el-
ement of natural language processing, in tasks ranging from spell-checking to
machine translation. given the increasing need to ensure grammatical sentences
in di   erent applications, pos-lm comes into play as a remedy. pos-lm de-
scribes the id203 of a sequence of m pos tags p (t1, ..., tm). pos-lms are
traditionally used for id103 problems [14] and statistical machine
translation systems [17, 23, 25] to capture syntactic information. in this paper,
we bene   t from pos-lms to capture the syntactic information of sentences and
improve the grammaticality of compression candidates.

3

3 proposed approach

3.1 word graph construction for msc
consider a set of related sentences s = {s1, s2, ..., sn}, a traditional word graph
is constructed by iteratively adding sentences to it. this directed graph is an
ordered pair g = (v, e) comprising of a set of vertices or words together with
a set of directed edges which shows the adjacency between corresponding nodes
[9, 4]. the graph is    rstly constructed by the    rst sentence and displays words
in a sentence as a sequence of connected nodes. the    rst node is the start node
and the last one is the end node. words are added to the graph in three steps
of the following order: (1) non-stopwords for which no candidate exists in the
graph; or for which an unambiguous mapping is possible (i.e. there is only one
node in the graph that refer to the same word/pos pair); (2) non-stopwords
for which there are either several possible candidates in the graph; or for which
they occur more than once in the sentence; (3) stopwords. for the last group,
same as boudin and morin (2013), we use the stopword list included in nltk1
extended with temporal nouns such as    yesterday   ,    friday   , and etc..

all msc approaches aim at producing condensed sentences that inherit the
most important information from the original content while remains syntac-
tically correct. however, gaining these goals at the same time remains still
di   cult. as a remedy, we believe that a better resolution to construct an im-
proved word graph can be obtained by using more sophisticated pre-processing
and re-ranking steps. thus, we focus on the notions of synonymy, mwe and
pos-lm re-ranking, which dramatically raise the informativity and grammat-
icality of compression candidates. in the following, we describe the details of
our proposed approach:

3.2 merging and mapping strategies

like many nlp applications, msc will bene   t from the identi   cation of mwes
and the concept of synonymy; and even more so when lexical diversity arises in a
collection of sentences. for example, consider a sentence that includes an mwe
(kick the bucket): it would be a sad thing to kick the bucket without having been
to alaska. to bene   t from this mwe that has 3 components/words, we propose
the merging strategy below:

firstly, after tokenizing the sentence and id30 the words, we detect the
mwe and its tuple pos with an mwe detector. this step has the advantage
of reducing the ambiguity of mapping upcoming words onto the existing words
with the same appearance in the graph. for example, the word kick above
has a di   erent meaning and pos (as an mwe component) from the identical
appearance word kick in isolation (in another sentence say they kick open the
door and entered the room.). so, mwe identi   cation can keep us from mapping
these two kick together and retain the important meaning of the content. to
detect mwes, we use the jmwe toolkit [19], which is a java-based library for
constructing and testing mwe token detectors.

secondly, we use version 3.0 of id138 [22] to obtain its available one-word
synonym with an appropriate pos and replace the n-words mwe with a shorter
synonym word. id138 groups all synonyms into a synset - a synonym set.

1http://nltk.org/

4

we only consider the most frequent one-word synonym in the id138 that
also appears in the other relevant sentences. if other relevant sentences contain
none of the one-word synonyms, the most frequent one is selected directly from
the id138 to help condense the sentence. three native speakers were asked
to investigate all the synonym mappings performed in our approach, and specify
whether each mapped synonym re   ects the meaning of the original word in the
sentence or not. based on this evaluation, the average rate of correct synonym
mappings is 88.21%. in case that no appropriate synonym is found for mwe,
the merged mwe itself was used as a back-o   . this can reduce the number
of graph nodes and, consequently, the ambiguity for further false mappings of
mwe components in the word graph. these steps are brie   y depicted in figure
3.1 (a).

figure 3.1: (a) example of mwe merging and mapping, (b) example of syn-
onym mapping

furthermore, we use the concept of synonymy for mapping upcoming sin-
gle words. for example, consider 3 di   erent sentences containing words bright,
smart and brilliant, which are synonyms of each other. assume each sentence
contains one of these synonyms respectively. without an appropriate mapping
based on a notion of synonymy, these 3 nodes will be added to the word graph
as separate nodes. with our approach, the word graph in this example is con-
structed with a single node containing a word as a representative of its synonyms
from the other sentences. the weight of the obtained node is computed by sum-
ming the frequency scores from the other nodes as shown in figure 3.1 (b) for
each pair of word/pos. the main purpose of this modi   cation is three fold: (i)
the ambiguity of mapping nodes is reduced; (ii) the number of total possible
paths (compression candidates) is decreased; and (iii) the weight of frequent
similar words with di   erent appearances in the content is better re   ected by
the notion of synonymy.

in the following example, we will demonstrate how we use the pre-processing
strategies to produce re   ned sentences, and generate an improved word graph.

5

kick        the          bucket       nndtvbmwe detector kick the bucket                vbid138 synonymdie           vbbright / freq=msmart / freq=nbrilliant / freq=pstudentsynonymybright / freq=m+n+pstudent(a)(b)among the underlined words, mwes are put into bracket, and synonyms are
identi   ed by the same superscript notations.

:::::::

[fast :::::
[junk :::::

teenagea boys are more :::::::::

food]c than girls.
food]c marketers    nd :::::

(1) :::::::
more ::::
(2) :::::
releasede by the cancer council shows.
(3) ::::::::::
survey.
(4) the survey, ::::::::
regular consumers of ::::

adolescenta boys :::[use:::up]d more ::::

[junk:::::

food]c.

interestedb in ::::

[junk:::::

food]c marketing and :::::::
consumed

younga boys more ::::::::

fascinatedb than girls, a survey

[fast :::::

food]c than girls, ::::::::

[according:::to] a new

publishede by the cancer council, observed :::::::

teenagea boys were

the word graph constructed for the above sentences are partially shown in
figure 3.2. some nodes, edge weights and punctuations are omitted from the
graph for more clarity.

figure 3.2: the generated word graph and a compression path

where mapping in the graph is ambiguous (i.e. there are two or more nodes
in the graph that refer to the same word/pos pair), we follow the instruction
stated by filippova (2010): the immediate context (the preceding and following
words in the sentence, and the neighboring nodes in the graph) or the frequency
(i.e. the node which has words mapped to it) is used to select the best candidate
node. a new node is created only if there is no suitable candidate to be mapped
to, in the graph.

in filippova (2010), edge weights are calculated using the weighting function

de   ned in equation 3.1 in which w

(ei,j) is given by equation 3.2.

(cid:48)

(cid:80)

w(ei,j) =

(cid:48)

w

(ei,j)

f req(i)    f req(j)

(cid:48)

w

(ei,j) =

f req(i) + f req(j)
s   s dif f (s, i, j)   1

(3.1)

(3.2)

where f req(i) is the number of words mapped to the node i. the function
dif f (s, i, j) refers to the distance between the o   set positions of words i and j

6

startteenageinterestedinmarketersareconsumethanmoregirlsfast~foodendboysfindmoremarketingandin sentence s.

algorithm 1 proposed msc word graph
1: input: a cluster of relevant sentences: s = {si}n
2: output: g = (v, e)
3: for i = 1 to n do
t     tokenize(si)
4:
st     id30(t)
5:
mwe-comp     mwe-detection(t, st)
6:
mwe-list     merge-mwe(m w e-comp)
sentsize     sizeof(t)
for j = 1 to sentsize do

i=1

label     tj
sid     i
pid     j
samen     getsamen odes(g, label)
if sizeof (samen )     1 then

else

vj     getbestsame(samen )
maplistvj     maplistvj     (sid, p id)
synn     getsynonymnodes(g, label)
if sizeof (synn )     1 then
vj     getbestsyn(synn )
maplistvj     maplistvj     (sid, p id)
esle if tj     mwe-list then
wnsyn     getbestwnsyn(label)
vj     creatnewnode(g, wnsyn)
maplistvj     (sid, p id)
vj     creatnewnode(g, label)
maplistvj     (sid, p id)

esle

end if

7:
8:
9:
10:
11:
12:

13:
14:
15:
16:
17:

18:
19:
20:
21:
22:
23:

24:
25:
26:
27:
28:
29:

end if
if not existedge(g, vj   1     vj) then

addedge(vj   1     vj, g)

end if

30:
31:
32:
33:
34:
35: end for

end for

algorithm 1 presents the steps to build our proposed msc word graph, g(v,
e). we start with a cluster of relevant sentences from a set of input newswire
clusters. each cluster is denoted as s = {si}n
i=1 where each si is a sentence
containing pos annotations. line 4-5: each si     s is split into a set of tokens,
where each token, tj consists of a word and its corresponding pos annotation
(e.g.   boys:nn   ). the tokens are also stemmed into a set of stemmed words, st.
line 6-7: for each sentence, mwe components, i.e., mwe-comp, are detected
using the set of tokens t and stems st. then, these mwe components are
merged in each sentence, and kept in a list of mwe-list. line 10-12: each

7

unique tj will form a node vj in the msc graph, with tj being the label. since
we only have one node per unique token, each node keeps track of all sentences
that include its token. so, each node keeps a list of sentence identi   er, (sid)
along with the position of token in that sentence, (pid). each node including a
single word or a merged mwe will thus carry a mapping list (maplist) which
is a list of {sid:pid} pairs representing the node   s membership in a sentence.
line 13-16: for mapping the token tj, we    rst explore the graph to    nd the
same node (i.e. node that refers to the same word/pos pair as tj). if two or
more same nodes are found, considering the aforementioned ambiguous mapping
criteria in section 3.2, the best candidate node is selected for mapping. then
the pair of (sid:pid) of tj will be added to the mapping list of the selected node,
i.e., maplistvj . line 18-21: if no same node exists in the graph, then we look
for the best synonym node in the graph (i.e.    nd the most frequent synonym
among the id138 synsets that was earlier added to the graph.). again, the
mapping list of the selected node, maplistvj will be updated to include the pair
of (sid:pid) of tj. line 22-28: if none of the above conditions are satis   ed, it
is time to create a new node in the graph. however as explained in section 3.2,
when tj is mwe, we extract the best id138 one-word synonym, and replace
the n-word mwe with this shorter synonym word. so, a shorter content node
will be added to the graph. line 31-33: the original structure of a sentence is
reordered with the use of directed edges.

a heuristic algorithm is then used to    nd the k -shortest paths from start to
end node in the graph. throughout our experiments, the appropriate value for
k is 150. by re-ranking this number of shortest paths, most of the potentially
good candidates are kept and a decline in performance is prevented. paths
shorter than eight words or do not contain a verb are    ltered before re-ranking.
the remaining paths are re-ranked and the path that has the lightest average
edge weight is eventually considered as the best compression. next, an accurate
re-ranking approach to identify the most informative grammatical compression
candidate is described.

3.3 re-ranking strategy (pos-lm)

boudin and morin (2013) have recently utilized textrank (mihalcea and tarau
2004) to re-rank the compression candidates.
in their approach, a word rec-
ommends other co-occurring words, and the strength of the recommendation is
recursively computed based on the importance of the words making the recom-
mendation. the score of a keyphrase k is computed by summing the salience
of the words it contains, normalized with its length + 1 to favor longer id165s
according to equation 3.3.

score(k) =

w   k t extrank(w)
length(k) + 1

finally, the paths are re-ranked and the score of a compression candidate c is
given by equation 3.4.

(cid:80)
length(c)   (cid:80)

i,j   path(c) w(ei,j)

k   c score(k)

score(c) =

(3.3)

(3.4)

(cid:80)

8

in our re-ranking step, we bene   t from the fact that pos tags capture the
syntactic roles of words in a sentence. we use a pos-lm to assign a grammat-
icality score to each generated compression. our hypothesis is that pos-lm
helps in identifying the most grammatical sentence among the k -most informa-
tive compressions. this strategy shall improve the grammaticality of msc, even
when the grammatical structures of the input sentences are completely di   erent.
word-based language models estimate the id203 of a string of m words by
equation 3.5, and pos-lms estimate the id203 of string of m pos tags
by equation 3.6 [23].

1 )     m(cid:89)
1 )     m(cid:89)

i   1

i   1

p(wm

p(tm

p(wi|wi   1

i   n+1)

p(ti|ti   1

i   n+1)

(3.5)

(3.6)

where, n is the order of the language model, and w/t refers to the sub-sequence
of words/tags from position i to j.

to build a pos-lm, we use the srilm toolkit with modi   ed kneser-ney
smoothing [28], and train the language model on our pos annotated corpus.
srilm collects id165 statistics from all id165s occurring in a corpus to
build a single global language model. to train our pos-lm, we need a pos-
annotated corpus. in this regard, we make use of the stanford pos tagger [29]
to annotate the afe sections of ldcs gigaword corpus (ldc2003t05) as a
large newswire corpus (   170 m-words). then, we remove all words from the
pairs of words/pos in the pos annotated corpus.

although the vocabulary of a pos-lm, which is usually ranging between
40 and 100 tags, is much smaller than the vocabulary of a word-based language
model, there is still a chance in some cases of unseen events. since modi   ed
kneser-ney discounting appears to be the most e   cient method in a systematic
description and comparison of the usual smoothing methods [13], we use this
type of smoothing to help our language model.

the compression candidates also need to be annotated with pos tags. so,
the score of each compression is estimated by the language model, based on
its sequence of pos tags. since factors like pos tags, are less sparse than
surface forms, it is possible to create a higher order language models for these
factors. this may encourage more syntactically correct output [18]. thus, in
our approach we use 7-gram id38 based on id52
to re-rank the k -best compressions generated by the word graph.

to re-rank the obtained paths, our pos-lm gives the perplexity score
(scorelm ) which is the geometric average of 1/id203 of each sentence,
normalized by the number of words. so, scorelm for each sequence of pos in
the k -best compressions is computed by equation 3.7.

scorelm (c) = 10

log prob(c)

#word

(3.7)

9

where prob(c) is the id203 of compression (c) including #w ord number
of words, computed by the 7-gram pos-lm.

as the estimated scores for each cluster of sentences fall into di   erent ranges,
we make use of unity-based id172 to bring the values of score(c) in
equation 4, and the scorelm into the range [0, 1]. the score of each compression
is    nally given by equation 3.8

scoref inal(c) =       score(c) + (1       )    scorelm (c)

(3.8)

in which the scaling factor    in our experiments has been set to 0.4, so as to
reach the best re-ranking results.

to better understand how pos-lm is used, consider the sentences below,
which have the same scores for informativity but are added into our re-ranking
contest to be investigated based on their grammaticality. the corresponding
pos sequences of these sentences are given to the trained language model to
clarify which one is more grammatical.

(1) boys more
nns rbr

consume
v bp

fast
jj

food
nn

than
in

girls.
nns

(cid:124)

(cid:123)(cid:122)

wrong pattern

(cid:125)

(2) boys
nns

consume more
v bp
jjr

fast
jj

food
nn

than
in

girls.
nns

as expected, the winner of this contest is the second pos sequence, which
has a better grammatical structure and gets a higher id203 score from the
pos-lm.

4 data preparation

many attempts have been made to release various kinds of datasets and evalua-
tion corpora for sentence compression and id54, such as the
one introduced in [5]. however, to our knowledge, there is no dataset available
to evaluate msc in an automatic way [4]. since the prepared dataset in boudin
and morin (2013) is also in french, we have followed the below instructions to
construct a standard english newswire dataset:

we have collected news articles in clusters on the australian1 and u.s.2
edition of google news over a period of    ve months (january 2015 - may 2015).
clusters composed of at least 15 news articles about one single news event, were
manually extracted from di   erent categories (i.e. top stories, world, business,
technology, entertainment, science, health, etc.). leading sentences in news
articles are known to provide a good summary of the article content and are
used as a baseline in summarization [7]. hence, to obtain the sets of related
sentences, we have extracted the    rst sentences from the articles in the cluster
and removed duplicates.

1http://news.google.com.au/
2http://news.google.com/

10

the released dataset contains 568 sentences spread over 46 clusters (each
is related to one single news event). the average number of sentences within
each cluster is 12, with a minimum of 7 and a maximum of 24. three native
english speakers were also asked to meticulously read the sentences provided in
the clusters, extract the most salient facts, summarize the set of sentences, and
generate three reference summaries for each cluster with as less new vocabularies
as possible.

in practice, along with the clusters of sentences with similar lexical and
grammatical structures (we refer to these clusters as normal ), it is likely to
have clusters of content-relevant sentences, but with di   erent (non-redundant)
appearances and grammatical structures (we consider these clusters as diverse).
in fact, the denser a word graph is, the more edges interconnect with vertices and
hence more paths pass through the same vertices. this results in low lexical and
syntactical diversity, and vice versa [30]. the density of a word graph generated
by sentences of a cluster g = (v, e) is given by equation 4.1.

density =

|e|

|v |(|v |     1)

(4.1)

thereupon, we have also identi   ed 15 diverse clusters among the 46 clusters to
demonstrate the e   ect of our approach on the normal and diverse groups. table
4.1 lists the properties of the evaluation dataset.

total #clusters

#normal clusters
#diverse clusters
total #sentences

avg #sentences/cluster
min #sentences/cluster
max #sentences/cluster

46
31
15
568
12
7
24

table 4.1: information about the constructed dataset

5 experiments

5.1 id74

we evaluate the proposed method over our constructed dataset (normal and
diverse clusters) using automatic and the manual evaluations. the quality of
the generated compressions was assessed automatically through version 2.0 1
of id8 [20] and the version 13a 2 of id7 [24]. these sets of metrics are
typically used for evaluating id54 and machine translation.
they compare an automatically produced summary against a reference or a set
of human-produced summaries.

1http://kavita-ganesan.com/content/id8-2.0
2ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13a.pl

11

for the manual investigation of the quality of the generated compressions,
three native english speakers were asked to rate the grammaticality and in-
formativity of the compressions based on the points scale de   ned in filippova
(2010). grammaticality: (i) if the compression is grammatically perfect    
point 2 ; (ii) if the compression requires some minor editing     point 1 ; (iii)
if the compression is ungrammatical     point 0. the lack of capitalization is
ignored by the raters. informativity: (i) if the compression conveys the gist of
the content and is mostly similar to the human-produced summary     point 2 ;
(ii) if the compression misses some important information     point 1 ; (iii) if the
compression contains none of the important contents     point 0 (table 5.1).
the k value for the agreement between raters falls into range (0.4     0.6)
through kappa   s id74, which indicates that the strength of this
agreement is moderate [2].

feature

grammaticality

(cid:40)

informativity

state of the compression

(cid:40) grammatically perfect

requires some minor editing
ungrammatical

conveys the gist of the content
misses some important information
contains none of the important contents

point

2
   

   

1
   

   

0

   

   

table 5.1: points scale de   ned in the agreement between raters

5.2 experiment results

two existing approaches, i.e., filippova (2010) and boudin and morin (2013)
are used as baseline1 and baseline2 respectively, for comparison purposes in our
experiments. to better understand the behavior of our system, we examined our
test dataset, and made the following observations. for the manual evaluation
(table 5.2), we observed a signi   cant improvement in the average grammati-
cality and informativity scores along with the compression ratio (compr) over
the normal and diverse clusters. the informativity of baseline1 is adversely
in   uenced by missing important information about the set of related sentences
[4]. however baseline2 enhanced the informativity, the grammaticality scores
are decreased due to the outputs of longer compressions. in our approach, the
remarkable improvement in the grammaticality scores is due to the adding of
the syntactic-based re-ranking step. using this re-ranking method, the most
grammatical sentences are picked among the k -best compression candidates.
furthermore, merging mwes, replacing them with their available one-word
synonyms and mapping words using synonymy all enhance the informativity
scores, and help to generate a denser word graph instead of a sparse one. given
that, the value of the compression ratio (   48%) is better than the best obtained
compression ratio on these two baselines (50%).

the average performance of the baseline methods and the proposed approach

12

method

normal

diverse

compr

info. gram.

info. gram.

baseline1
baseline2
proposed

1.44
1.68
1.68

1.67
1.60
1.68

1.17
1.30
1.36

1.19
1.12
1.47

50%
58%
48%

table 5.2: average scores over normal and diverse clusters separately given by
the raters; along with the estimated compression rate

over the normal and diverse clusters in terms of id8 and id7 scores are
also shown in table 5.3. id8 measures the concordance of candidate and
reference summaries by determining id165, word sequence, and word pair
matches. we used id8 f-measure for unigram, bigrams, and su4 (skip-
bigram with maximum gap length 4) to evaluate the compression candidates.
the id7 metric computes the scores for individual sentences; then averages
these scores over the whole corpus for a    nal score. we used id7 for 4-grams
to evaluate the results.

metric

baseline1 baseline2 proposed

id8-1
id8-2
id8-su4
id7-4

0.4912
0.3050
0.2867
0.4510

0.5093
0.3131
0.3002
0.5144

0.5841
0.4284
0.3950
0.6913

table 5.3: average scores by automatic evaluation over the normal and diverse
clusters

to make the candidate and reference summaries comparable, a process of
manual mwe detection is performed on the reference summaries and the mwe
components are merged by three native annotators. in details, automatic eval-
uation packages use id138 to compare the synonyms in candidate and refer-
ence summaries. id138 puts hyphenation on synonyms, e.g., kick-the-bucket,
so annotators hyphenate mwes in their summaries to be used in these pack-
ages. then, the synonym properties are set in these packages to consider the
synsets. thus, n-words mwes are linked to their one-word synonyms in the
candidate summary. the overall results support our hypothesis that using the
pos-lm for re-ranking the compression candidates, results in more grammat-
ical compressions, especially for diverse clusters. this issue is con   rmed by
4-grams id7, which shows the grammaticality enhancement rather than the
informativity. meanwhile, we try to simultaneously improve the informativity
by identifying and merging mwes along with mapping the synonyms.

furthermore, the e   ectiveness of id8 and id7 is studied using the
pearson   s correlation coe   cient. we found that id8 shows a better correla-
tion with informativity, while the id7 correlates better with grammaticality.
overall, the results in figure 5.1 show high correlation (0.5     1.0) between the

13

automatic evaluation results and human ratings for both id8 and id7.
the main reason may be the simulation of factors that humans usually con-
sider for summarization, such as merging and mapping strategies, along with
the syntactic criteria employed by pos-lm.

figure 5.1: the e   ectiveness of id8 and id7

to investigate the impact of each improvement separately, we have also
conducted separate experiments over the prepared dataset. the results are
shown in figure 5.2 and the related data are provided in table 5.4.
in our
work, merging and mapping strategies signi   cantly increase the informativity
of the compressions. so, their computed scores by id8 are higher than the
score of pos-lm. however, the combination of mwe merging and mapping
gets a slightly lower score from id8-su4. one reason may be that usage
of synonymy only for mwes and ignoring other one-word synonym mapping
causes a more diverse graph, which slightly decreases the informativity and
grammaticality of compressed sentences. meanwhile, pos-lm gets better scores
from id7-4, which indicates the grammaticality enhancement rather than the
informativity.

figure 5.2: the e   ects of the improvements separately

6 conclusions

in a nutshell, we have presented our attempt in using mwes, synonymy and
pos-based id38 to tackle one of the pain points of msc, which is
improving both informativity and grammaticality at the same time. by manual
and automatic (id8 and id7) evaluations, experiments using a constructed

14

id8-1id8-2id8-su4id7-4info.0.6110.5630.5880.549gram.0.4720.5290.5070.60500.10.20.30.40.50.60.700.10.20.30.40.50.60.70.8id8-1id8-2id8-su4id7-4synonymymerging	&	mappingpos-lmall	togethermetric

synonymy merg/map pos-lm all

id8-1
id8-2
id8-su4
id7-4

0.5659
0.3723
0.3508
0.5340

0.5820
0.4087
0.3254
0.5601

0.5381
0.3599
0.3629
0.6725

0.5841
0.4284
0.3950
0.6913

table 5.4: the e   ects of the improvements separately

english newswire dataset show that our approach outperforms the competitive
baselines. in particular, the proposed merging and mapping strategies, along
with the grammar-enhanced pos-lm re-ranking method, ameliorate both infor-
mativity and grammaticality of the compressions, with an improved compression
ratio.

bibliography

[1] otavio costa acosta, aline villavicencio, and viviane p moreira. iden-
ti   cation and treatment of multiword expressions applied to information
retrieval. in proceedings of the workshop on multiword expressions: from
parsing and generation to the real world, pages 101   109. association for
computational linguistics, 2011.

[2] ron artstein and massimo poesio.

inter-coder agreement for computa-

tional linguistics. computational linguistics, 34(4):555   596, 2008.

[3] timothy baldwin and su nam kim. multiword expressions. handbook of
natural language processing, second edition. morgan and claypool, 2010.

[4] florian boudin and emmanuel morin. keyphrase extraction for n-best
reranking in multi-sentence compression. in north american chapter of
the association for computational linguistics (naacl), 2013.

[5] james clarke and mirella lapata. models for sentence compression: a
comparison across domains, training requirements and evaluation mea-
sures.
in proceedings of the 21st international conference on compu-
tational linguistics and the 44th annual meeting of the association for
computational linguistics, pages 377   384. association for computational
linguistics, 2006.

[6] james clarke and mirella lapata. modelling compression with discourse

constraints. in emnlp-conll, pages 1   11, 2007.

[7] hoa trang dang. overview of duc 2005. in proceedings of the document

understanding conference, pages 1   12, 2005.

[8] micha elsner and deepak santhanam. learning to fuse disparate sentences.
in proceedings of the workshop on monolingual text-to-text generation,
pages 54   63. association for computational linguistics, 2011.

15

[9] katja filippova. multi-sentence compression:    nding shortest paths in
word graphs. in proceedings of the 23rd international conference on com-
putational linguistics, pages 322   330. association for computational lin-
guistics, 2010.

[10] katja filippova and michael strube. sentence fusion via dependency graph
compression.
in proceedings of the conference on empirical methods in
natural language processing, pages 177   185. association for computa-
tional linguistics, 2008.

[11] michel galley and kathleen mckeown. lexicalized markov grammars for

sentence compression. in hlt-naacl, pages 180   187, 2007.

[12] kavita ganesan, chengxiang zhai, and jiawei han. opinosis: a graph-
based approach to abstractive summarization of highly redundant opinions.
in proceedings of the 23rd international conference on computational lin-
guistics, pages 340   348. association for computational linguistics, 2010.

[13] joshua t goodman. a bit of progress in id38. computer

speech & language, 15(4):403   434, 2001.

[14] peter a heeman. id52 versus classes in id38.

in
proceedings of the 6th workshop on very large corpora, montreal, 1998.

[15] ray jackendo   . the architecture of the language faculty. number 28. mit

press, 1997.

[16] hongyan jing. sentence reduction for automatic text summarization. in
proceedings of the sixth conference on applied natural language processing,
pages 310   315. association for computational linguistics, 2000.

[17] philipp koehn, abhishek arun, and hieu hoang. towards better machine
translation quality for the german   english language pairs. in proceedings
of the third workshop on id151, pages 139   142.
association for computational linguistics, 2008.

[18] philipp koehn, hieu hoang, alexandra birch, chris callison-burch, mar-
cello federico, nicola bertoldi, brooke cowan, wade shen, christine
moran, richard zens, et al. moses: open source toolkit for statistical
machine translation. in proceedings of the 45th annual meeting of the acl
on interactive poster and demonstration sessions, pages 177   180. associa-
tion for computational linguistics, 2007.

[19] nidhi kulkarni and mark alan finlayson. jmwe: a java toolkit for detect-
ing multi-word expressions. in proceedings of the workshop on multiword
expressions: from parsing and generation to the real world, pages 122   
124. association for computational linguistics, 2011.

[20] chin-yew lin. id8: a package for automatic evaluation of summaries.
in text summarization branches out: proceedings of the acl-04 workshop,
volume 8, 2004.

[21] ryan t mcdonald. discriminative sentence compression with soft syntactic

evidence. in eacl, 2006.

16

[22] george a miller. id138: a lexical database for english. communications

of the acm, 38(11):39   41, 1995.

[23] christof monz. id151 with local language mod-
in proceedings of the conference on empirical methods in natural
els.
language processing, pages 869   879. association for computational lin-
guistics, 2011.

[24] kishore papineni, salim roukos, todd ward, and wei-jing zhu. id7: a
method for automatic evaluation of machine translation. in proceedings of
the 40th annual meeting on association for computational linguistics, pages
311   318. association for computational linguistics, 2002.

[25] maja popovi  c. morpheme-and pos-based ibm1 scores and language model
scores for translation quality estimation.
in proceedings of the seventh
workshop on id151, pages 133   137. association
for computational linguistics, 2012.

[26] ivan a sag, timothy baldwin, francis bond, ann copestake, and dan
flickinger. multiword expressions: a pain in the neck for nlp. in compu-
tational linguistics and intelligent text processing, pages 1   15. springer,
2002.

[27] beaux shari   , mark-anthony hutton, and jugal k kalita. experiments in
microblog summarization. in social computing (socialcom), 2010 ieee
second international conference on, pages 49   56. ieee, 2010.

[28] andreas stolcke et al. srilm-an extensible id38 toolkit. in

interspeech, 2002.

[29] kristina toutanova, dan klein, christopher d manning, and yoram
singer. feature-rich part-of-speech tagging with a cyclic dependency net-
work. in proceedings of the 2003 conference of the north american chap-
ter of the association for computational linguistics on human language
technology-volume 1, pages 173   180. association for computational lin-
guistics, 2003.

[30] emmanouil tzouridis, jamal abdul nasir, lums lahore, and ulf brefeld.
learning to summarise related sentences. in the 25th international con-
ference on computational linguistics (coling14), dublin, ireland, acl,
2014.

[31] dingding wang, tao li, shenghuo zhu, and chris ding. multi-document
summarization via sentence-level semantic analysis and symmetric matrix
factorization. in proceedings of the 31st annual international acm sigir
conference on research and development in information retrieval, pages
307   314. acm, 2008.

17

