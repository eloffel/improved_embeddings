6
1
0
2

 

g
u
a
6
1

 

 
 
]
l
c
.
s
c
[
 
 

1
v
5
6
4
4
0

.

8
0
6
1
:
v
i
x
r
a

fast,smallandexact:in   nite-orderlanguagemodellingwithcompressedsuf   xtreesehsanshareghi,[matthiaspetri,\gholamrezahaffari[andtrevorcohn\[facultyofinformationtechnology,monashuniversity\computingandinformationsystems,theuniversityofmelbournefirst.last@monash.edu,initial.last@unimelb.edu.auabstractef   cientmethodsforstoringandqueryingarecriticalforscalinghigh-orderm-gramlan-guagemodelstolargecorpora.weproposealanguagemodelbasedoncompressedsuf   xtrees,arepresentationthatishighlycompactandcanbeeasilyheldinmemory,whilesup-portingqueriesneededincomputinglanguagemodelprobabilitieson-the-   y.wepresentseveraloptimisationswhichimprovequeryruntimesupto2500  ,despiteonlyincurringamodestincreaseinconstructiontimeandmemoryusage.forlargecorporaandhighmarkovorders,ourmethodishighlycompeti-tivewiththestate-of-the-artkenlmpackage.itimposesmuchlowermemoryrequirements,oftenbyordersofmagnitude,andhasrun-timesthatareeithersimilar(fortraining)orcomparable(forquerying).1introductionlanguagemodels(lms)arefundamentaltomanynlptasks,includingmachinetranslationandspeechrecognition.statisticallmsareprobabilis-ticmodelsthatassignaid203toasequenceofwordswn1,indicatinghowlikelythesequenceisinthelanguage.m-gramlmsarepopular,andprovetobeaccuratewhenestimatedusinglargecorpora.intheselms,theid203ofm-gramsareoftenprecomputedandstoredexplicitly.althoughwidelysuccessful,currentm-gramlmapproachesareimpracticalforlearninghigh-orderlmsonlargecorpora,duetotheirpoorscalingprop-ertiesinbothtrainingandqueryphases.prevailingmethods(hea   eld,2011;stolckeetal.,2011)pre-computeallm-gramprobabilities,andconsequentlyneedtostoreandaccessasmanyasahundredofbil-lionsofm-gramsforatypicalmoderate-orderlm.recentresearchhasattemptedtotacklescalabil-ityissuesthroughtheuseofef   cientdatastructuressuchastriesandhash-tables(hea   eld,2011;stol-ckeetal.,2011),lossycompression(talbotandos-borne,2007;levenbergandosborne,2009;guthrieandhepple,2010;paulsandklein,2011;churchetal.,2007),compactdatastructures(germannetal.,2009;watanabeetal.,2009;sorensenandallauzen,2011),anddistributedcomputation(hea   eldetal.,2013;brantsetal.,2007).fundamentaltoallthewidelyusedmethodsistheprecomputationofallprobabilities,hencetheydonotprovideanadequatetrade-offbetweenspaceandtimeforhighm,bothduringtrainingandquerying.exceptionsareken-ningtonetal.(2012)andzhangandvogel(2006),whouseasuf   x-treeorsuf   x-arrayoverthetextforcomputingthesuf   cientstatisticson-the-   y.inourpreviouswork(shareghietal.,2015),weextendedthislineofresearchusingacompressedsuf   xtree(cst)(ohlebuschetal.,2010),whichprovidesaconsiderablymorecompactsearchablemeansofstoringthecorpusthananuncompressedsuf   xarrayorsuf   xtree.thisapproachshowedfavourablescalingpropertieswithmandhadonlyamodestmemoryrequirement.however,themethodonlysupportedkneser-neysmoothing,notitsmodi-   edvariant(chenandgoodman,1999)whichover-allperformsbetterandhasbecomethede-factostandard.additionally,queryingwassigni   cantlyslowerthanforleadinglmtoolkits,makingthemethodimpracticalforwidespreaduse.inthispaperweextendshareghietal.(2015)tosupportmodi   edkneser-neysmoothing,andpresentnewoptimisationmethodsforfastconstruc-tionandquerying.1criticaltoourapproachare:   precomputationofseveralmodi   edcounts,whichwouldbeveryexpensivetocomputeatthequerytime.toorchestratethis,asub-setofthecstnodesisselectedbasedonthecostofcomputingtheirmodi   edcounts(whichrelateswiththebranchingfactorofanode).theprecomputedcountsarethenstoredinacompresseddatastructuresupportingef   cientmemoryusageandlookup.   re-useofcstnodeswithinm-gramprobabil-itycomputationasasentencegetsscoredleft-to-right,thussavingmanyexpensivelookups.empiricalcomparisonagainstourearlierwork(shareghietal.,2015)showsthesigni   canceofeachoftheseoptimisations.thestrengthsofourmethodareapparentwhenappliedtoverylargetrainingdatasets(   16gib)andforhighordermod-els,m   5.inthissetting,whileourapproachismorememoryef   cientthantheleadingkenlmmodel,bothintheconstruction(training)andquery-ingphases(testing),wearehighlycompetitiveintermsofruntimesofbothphases.whenmemoryisalimitingfactoratquerytime,ourapproachisordersofmagnitudefasterthanthestateoftheart.moreover,ourmethodallowsforef   cientqueryingwithanunlimitedmarkovorder,m      ,withoutresortingtoapproximationsorheuristics.2modi   edkneser-neylanguagemodelinanm-gramlanguagemodel,theid203ofasentenceisdecomposedintoqni=1p(wi|wi   1i   m+1),wherep(wi|wi   1i   m+1)istheconditionalid203ofthenextwordgivenits   nitehistory.smooth-ingtechniquesareemployedtodealwithsparsitywhenestimatingtheparametersofp(wi|wi   1i   m+1).acomprehensivecomparisonofdifferentsmooth-ingtechniquesisprovidedin(chenandgoodman,1999).wefocusoninterpolatedmodi   edkneser-ney(mkn)smoothing,whichiswidelyregardedasastate-of-the-arttechniqueandissupportedbypopularlanguagemodellingtoolkits,e.g.srilm(stolcke,2002)andkenlm(hea   eld,2011).1https://github.com/eehsan/cstlmpm(w|ux)=[c(uxw)   dm(c(uxw))]+c(ux)+  m(ux)  pm   1(w|x)c(ux)  pk(w|ux)=[n1+(  uxw)   dk(n1+(  uxw))]+n1+(  ux  )+  k(ux)  pk   1(w|x)n1+(  ux  )  p0(w| )=[n1+(  w)   d1(n1+(  w))]+n1+(    )+  ( )n1+(    )  1    k(ux)=(pj   {1,2,3+}dk(j)nj(ux  ),ifk=mpj   {1,2,3+}dk(j)n0j(ux  ),ifk<mdk(j)=                                 0,ifj=01   2n2(k)n1(k).n1(k)n1(k)+2n2(k),ifj=12   3n3(k)n2(k).n1(k)n1(k)+2n2(k),ifj=23   4n4(k)n3(k).n1(k)n1(k)+2n2(k),ifj   3ni(k)=(|{  s.t.|  |=k,c(  )=i}|,ifk=m(cid:12)(cid:12)(cid:8)  s.t.|  |=k,n1+(    )=i(cid:9)(cid:12)(cid:12),ifk<mfigure1:thequantitiesandformulaneededformodi   edkneser-neysmoothing,wherexisak-gram,uandwarewords,and[a]+def=max{0,a}.weusemtorefertotheorderofthelanguagemodel,andk   [1,m]totheleveloid122oothing.therecursionstopsattheunigraid113vel  p0(w| )wheretheid203issmoothedbytheuniformdistributionoverthevocabulary1  .mknisarecursivesmoothingtechniquewhichuseslowerorderk-gramlanguagemodelstosmoothhigherordermodels.figure1describesthere-cursivesmoothingformulaemployedinmkn.itisdistinguishedfromkneser-ney(kn)smoothinginitsuseofadaptivediscountparameters(denotedasdk(j)infigure1)basedonthek-gramcounts.importantly,mknisbasedonnotjustm-gramfrequencies,c(x),butalsoseveralmodi   edcountsbasedonnumbersofuniquecontexts,namelyni+(    )=|{ws.t.c(  w)   i}|ni+(    )=|{ws.t.c(w  )   i}|ni+(      )=|{w1w2s.t.c(w1  w2)   i}|n0i+(    )=(cid:12)(cid:12)(cid:8)ws.t.n1+(    w)   i(cid:9)(cid:12)(cid:12).ni+(    )andni+(    )arethenumberofwordswithfrequencyatleastithatcomebeforeandafterapattern  ,respectively.ni+(      )isthenumberofword-pairswithfrequencyatleastiwhichsur-roundx.n0i+(    )isthenumberofwordscomingafter  toformapattern  wforwhichthenumberofuniqueleftcontextsisatleasti;itisspeci   ctomknandnotneededinkn.table1illustratestheknum.countdenom.count  4c(forceisstrongwith)c(forceisstrong)n{1,2,3+}(forceisstrong  )3n1+(  isstrongwith)n1+(  isstrong  )n0{1,2,3+}(isstrong  )2n1+(  strongwith)n1+(  strong  )n0{1,2,3+}(strong  )1n1+(  with)n1+(    )n0{1,2,3+}(   )table1:themainquantitiesrequiredforcomputingp(with|force,is,strong)undermkn.differenttypesofquantitiesrequiredforcomputinganexample4-grammknid203.ef   cientcomputationofthesequantitiesischal-lengingwithlimitedmemoryandtimeresources,particularlywhentheorderofthelanguagemodelmishighand/orthetrainingcorpusislarge.inthispaper,wemakeuseofadvanceddatastructurestoef   cientlyobtaintherequiredquantitiestoanswerprobabilisticqueriesastheyarrive.oursolutionin-volvesprecomputingandcachingexpensivequan-tities,n1+(      ),n1+(    ),n{1,2,3+}(    )andn0{1,2,3+}(    ),whichwewillexplainin  4.westartin  3byprovidingareviewoftheapproachinshareghietal.(2015)uponwhichwebaseourwork.3knwithcompressedsuf   xtrees3.1compresseddatastructuresshareghietal.(2015)proposedamethodforkneser-ney(kn)languagemodellingbasedonon-the-   yid203computationfromacompressedsuf   xtree(cst)(ohlebuschetal.,2010).thecstemulatesthefunctionalityofthesuf   xtree(st)(weiner,1973)usingsubstantiallylessspace.thesuf   xtreeisaclassicalsearchindexconsistingofarootedlabelledsearchtreeconstructedfromatexttoflengthndrawnfromanalphabetofsize  .eachroottoleafpathinthesuf   xtreecorrespondstoasuf   xoft.theleaves,consideredinleft-to-rightorderde   nethesuf   xarray(sa)(manberandmyers,1993)suchthatthesuf   xt[sa[i],n   1]islexicographicallysmallerthant[sa[i+1],n   1]fori   [0,n   2].searchingforapattern  oflengthmintcanbeachievedby   ndingthe   highest   nodevinthestsuchthatthepathfromtheroottovispre   xedby  .allleafnodesinthesubtreestart-ingatvcorrespondtothelocationsof  int.thisistranslatedto   ndingthespeci   crangesa[lb,rb]suchthatt[sa[j],sa[j+m   1]]=  forj   [lb,rb]asillustratedinthestandsaoffigure2(left).whilesearchingusingthestorthesaisef   -cientintheory,itrequireslargeamountsofmainmemory.acstreducesthespacerequirementsofstbyutilizingthecompressibilityoftheburrows-wheelertransform(bwt)(burrowsandwheeler,1994).thebwtcorrespondstoareversibleper-mutationofthetextusedindatacompressiontoolssuchasbzip2toincreasethecompressibilityoftheinput.thetransformisde   nedasbwt[i]=t[sa[i]   1modn](1)andisthecorecomponentofthefm-index(ferrag-inaandmanzini,2000)whichisasubcomponentofacsttoprovideef   cientsearchforlocatingar-bitrarylengthpatterns(m-grams),determiningoc-currencefrequenciesetc.thekeyfunctionalitypro-videdbythefm-indexistheabilitytoef   cientlydeterminetherangesa[lb,rb]matchingagivenpat-tern  describedabovewithouttheneedtostorethestorsaexplicitly.thisisachievedbyiterativelyprocessing  inreverseorderusingthebwt,whichisusuallyreferredtoasbackward-search.thebackward-searchprocedureutilizesthedual-itybetweenthebwtandsatoiterativelydeterminesa[lb,rb]forsuf   xesof  .supposesa[spj,epj]correspondstoallsuf   xesintmatching  [j,m   1].rangesa[spj   1,epj   1]matching  [j   1,m   1]withcdef=  [j   1]canbeexpressedasspj   1=c[c]+rank(bwt,spj,c)epj   1=c[c+1]+rank(bwt,epj+1,c)   1wherec[c]referstothestartingpositionofallsuf-   xespre   xedbycinsaandrank(bwt,spj,c)determinesthenumberofoccurrencesofsymbolcinbwt[0,spj].operationrank(bwt,i,c)(anditsinverseop-erationselect(bwt,i,c)2)canbeperformedef   -cientlyusingawavelettree(grossietal.,2003)rep-resentationofthebwt.awavelettreeisaver-satile,space-ef   cientrepresentationofasequencewhichcanef   cientlysupportavarietyofopera-tions(navarro,2014).thestructureofthewavelettreeisderivedbyrecursivelydecomposingtheal-phabetintosubsets.ateachlevelthealphabetis2select(bwt,i,c)returnsthepositionoftheithoccur-renceofsymbolcinbwt.$1471213036914#    108112501234567891011121314sa14130147101125812369bwt#c$#ccdbaaabbbb(a)suf   xtree,suf   xarray,andburrows-wheelertransform#c$#ccdbaaabbbb010011110001111#$#aaa000111cccdbbbbb000100000#$#101$bc0101#a1cccbbbbb0001111100101d(b)wavelettreeofbwtfigure2:(a)character-levelsuf   xtree,suf   xarray(sa),andburrows-wheelertransform(bwt)for   #abcab-cabdbbc#$   (asformulatedineq.1).(b)thewavelettreeofbwtandtherank(bwt,8,a).theorderedalphabetandtheircodewordsare{$:000,#:001,a:01,b:100,c:101,d:11},andsymbols   #   and   $   aretomarksentenceand   leboundaries.theredboundingboxesanddigitssignifythepathforcomputingrank(bwt,8,a).splitintotwosubsetsbasedonwhichsymbolsinthesequenceareassignedtotheleftandrightchildnodesrespectively.usingcompressedbitvectorrep-resentationsandhuffmancodestode   netheal-phabetpartitioning,thespaceusageofthewavelettreeandassociatedrankstructuresofthebwtisboundbyhk(t)n+o(nlog  )bits(grossietal.,2003).thusthespaceusageisproportionaltotheorderkid178(hk(t))ofthetext.figure2(right)showsasamplewavelettreerep-resentation.usingthewavelettreestructure,rankoverasequencedrawnfromanalphabetofsize  canbereducedtolog  binaryrankopera-tionswhichcanbeansweredef   cientlyinconstanttime(jacobson,1989).therangesa[lb,rb]cor-respondingtoapattern  ,canbedeterminedino(mlog  )timeusingawavelettreeofthebwt.inadditiontothefm-index,acstef   cientlystoresthetreetopologyofthesttoemulatetreeoperationssuchef   ciently(ohlebuschetal.,2010).3.2computingknmodi   edcountsshareghietal.(2015)showedhowthereq-uisitecountsforakn-lm,namelyc(  ),n1+(    ),n1+(      )andn1+(    ),canbecomputeddirectlyfromcst.considertheexampleinfigure2,thenumberofoccurrencesofbcorre-spondstocountingthenumberofleaves,size(v),inthesubtreerootedatv.thiscanbecomputedino(1)timebycomputingthesizeoftherange[lb,rb]implicitlyassociatedwitheachnode.thenumberofuniquerightcontextsofbcanbedeterminedusingdegree(v)(againo(1)butrequiresbitoperationsonthesuccincttreerepresentationofthest).thatis,n1+(b  )=degree(v)=3.determiningthenumberofleft-contextsandsur-roundingcontextsaremoreinvolved.computingn1+(    )reliesonthebwt.recallthatbwt[i]correspondstothesymbolprecedingthesuf   xstart-ingatsa[i].forexamplecomputingn1+(  b)   rstrequires   ndingtheintervalofsuf   xesstart-ingwithbinsa,namelylb=6andrb=10,andthencountingthenumberofuniquesymbolsinbwt[6,10]={d,b,a,a,a},i.e.,3.determin-ingalluniquesymbolsinbwt[i,j]canbeper-formedef   ciently(independentlyofthesizeoftherange)usingthewavelettreeencodingofthebwt.thesetofsymbolsprecedingpattern  ,denotedbyp(  )canbecomputedino(|p(  )|log  )byvis-itingalluniqueleafsinthewavelettreewhichcor-respondtosymbolsinbwt[i,j].thisisusuallyreferredtoastheinterval-symbols(schnattingeretal.,2010)procedureandusesrankoperationsto   ndthesetofsymbolss   p(  )andcorre-spondingrangesfors  insa.intheaboveex-ample,identifyingthesarangeofabrequiresto   ndthelb,rbinthesaforsuf   xesstartingwitha(sa[3,5])andthenadjustingtheboundstocoveronlythesuf   xesstartingwithab.thislaststepisdoneviacomputingtherankofthreeasymbols2358  010s20sn'123+(a .)n123+(a .)n1+(. a .)n1+(. a)n1+(a .)backward   searchon   the   flytime (sec)2358  04ms8msprecomputedm   gramtime (msec)figure3:timebreakdownforqueryingaverageper-sentence,shownwithoutruntimeprecomputationofex-pensivecontextualcounts(above)vs.withprecomputa-tion(below).theleftandrightbarineachgroupdenoteknandmkn,respectively.trainedonthegermanpor-tionoftheeuroparlcorpusandtestedoverthe   rst10ksentencesfromthenewscommentarycorpus.inbwt[8,10]usingthewavelettree,seefigure2(right)forrank(bwt,a,8).asillustrated,an-sweringrank(bwt,8,a)correspondstoprocess-ingthe   rstdigitofthecodewordattherootlevel,whichtranslatesintorank(wtroot,8,0)=4,fol-lowedbyarank(wt1,4,1)=1ontheleftbranch.oncetheranksarecomputedlb,rbarere   nedac-cordinglytosa[3+(1-1),3+(3-1)].finally,forn1+(      )allpatternswhichcanfollow  areenumerated,andforeachoftheseextendedpatterns,thenumberofprecedingsymbolsiscomputedusinginterval-symbols.thisprovedtobethemostexpen-siveoperationintheirapproach.giventhesequantities,shareghietal.(2015)showhowm-gramprobabilitiescanbecomputedondemandusinganiterativealgorithmtosearchformatchingnodesinthesuf   xtreeforthere-quiredk-gram(k   m)patternsinthenumeratoranddenominatorofknrecursiveequations,whicharethenusedtocomputetheprobabilities.were-ferthereadertoshareghietal.(2015)forfurtherdetails.overalltheirapproachshowedpromise,inthatitallowedforunlimitedorderkn-lmstobeevaluatedwithamodestmemoryfootprint,howeveritwasmanyordersofmagnitudeslowerforsmallermthanleadinglmtoolkits.toillustratethecostofquerying,seefigure3(top)whichshowsper-sentencequerytimeforkn,algorithm1n{1,2,3+}(    )orn0{1,2,3+}(    )1:functionn123pfront(t,v,  ,is-prime).tisacst,visthenodematchingpattern  2:n1,n2,n3+   03:foru   children(v)do4:ifis-primethen5:f   interval-symbols(t,[lb(u),rb(u)])6:else7:f   size(u)8:if1   f   2then9:nf   nf+110:n3+   degree(v)   n1   n211:retuid561,n2,n3+basedontheapproachofshareghietal.(2015)(alsoshownismkn,throughanextensionoftheirmethodasdescribedin  4).itisclearthattherun-timesforknismuchtooslowforpracticaluse   about5secondspersentence,withthemajorityofthistimespentcomputingn1+(      ).clearlyop-timisationiswarranted,andthegainsfromdoingsoarespectacular(seefigure3bottom,whichusestheprecomputationmethodasdescribedin  4.2).4extendingtomkn4.1computingmknmodi   edcountsacentralrequirementforextendingshareghietal.(2015)tosupportmknarealgorithmsforcomput-ingn{1,2,3+}(    )andn0{1,2,3+}(    ),whichwenowexpoundupon.algorithm1computesbothofthesequantities,takingasinputacstt,anodevmatchingthepattern  ,thepatternanda   agis-prime,denotingwhichofthenandn0variantsisrequired.thismethodenumeratesthechildrenofthenode(line3)andcalculateseitherthefrequencyofeachchild(line7)orthemodi   edcountn1+(    x),foreachchilduwherexisthe   rstsymbolontheedgevu(line5).lines8and9accumulatethenumberofthesevaluesequaltooneortwo,and   -nallyinline10,n3+iscomputedbythedifferencebetweenn1+(    )=degree(v)andthealreadycountedeventsn1+n2.forexample,computingn{1,2,3+}(b  )infig-ure2correspondstoenumeratingoveritsthreechil-dren.twoofv   schildrenareleafnodes{10,8},andonechildhasthreeleafdescendants{11,2,5},hencen1andn2are2and0respectively,andn3+is1.further,considercomputingn0{1,2,3+}(b  )infigure2,whichagainenumeratesoverchildnodes(whosepathlabelsstartwithsymbolsb,candd)andcomputesthenumberofprecedingsymbolsfortheextendedpatterns.3accordinglyn01(b  )=2,n02(b  )=1andn03+(b  )=0.whileroughlysimilarinapproach,comput-ingn0{1,2,3+}(    )isinpracticeslowerthann{1,2,3+}(    )sinceitrequirescallinginterval-symbols(line7)insteadofcallingtheconstanttimesizeoperation(line5).thisgivesrisetoatimecomplexityofo(d|p(  )|log  )forn0{1,2,3+}(    )wheredisthenumberofchildrenofv.asillustratedinfigure3(top),themodi   edcounts(  2)combinedareresponsiblefor99%ofthequerytime.moreoverthealreadyexpensiveruntimeofknisconsiderablyworsenedinmknduetotheadditionalcountsrequired.thesefactsmotivateop-timisation,whichweachievebyprecomputingval-ues,resultingina2500  speedupinquerytimeasshowninfigure3(bottom).4.2ef   cientprecomputationlanguagemodellingtoolkitssuchaskenlmandsrilmprecomputerealvaluedprobabilitiesandbackoff-weightsattrainingtime,suchthatqueryingbecomeslargelyaproblemofretrieval.wemightconsidertakingasimilarrouteinoptimisingourlan-guagemodel,howeverwewouldfacetheproblemthat   oatingpointnumberscannotbecompressedveryeffectively.evenwithquantisation,whichcanhaveadetrimentaleffectonmodellingperplexity,wewouldnotexpectgoodcompressionandthusthistechniquewouldlimitthescalingpotentialofourap-proach.forthesereasons,insteadwestorethemostex-pensivecountdata,targetingthosecountswhichhavethegreatesteffectonruntime(seefigure3top).weexpecttheseintegervaluestocompresswell:ashighlightedbyfigure4mostcountswillhavelowvalues,andaccordinglyavariablebytecompressionschemewillbeabletorealisehighcompressionrates.removingtheneedforcomput-ingthesevaluesatquerytimeleavesonlypatternsearchandafew   oatingpointoperationsinordertocomputelanguagemodelprobabilities(see  4.3)whichcanbedonecheaply.3thatisn1+(  bb)=1,n1+(  bc)=2,n1+(  bd)=1.25%50%75%100%0163248648096112128storedvalue%ofvaluessmallerorequalthan0%5%10%15%2345678910storagethreshold%oftotalspaceusagen1+(    )n1(    )n01(    )n2(    )n02(    )n1+(      )bvfigure4:left:distributionofvaluesprestoredforeu-roparlgerman;right:spaceusageofprestoredvaluesrelativetototalindexsizeforeuroparlgermanfordif-ferentstoragethresholds(  m).our   rstconsiderationishowtostructurethecache.giventhateachprecomputedvalueiscom-putedusingacstnode,v,(withthepatternasitspath-label),westructurethecacheasamappingbe-tweenuniquenodeidenti   ersid(v)andtheprecom-putedvalues.4nextweconsiderwhichvaluestocache:whileitispossibletoprecomputevaluesforeverynodeinthecst,manynodesareunlikelytobeaccessedatquerytime.moreover,theserarepat-ternsarelikelytobecheaptoprocessusingtheon-the-   ymethods,giventheyoccurinfewcontexts.consequentlyprecomputingthesevalueswillbringminimalspeedbene   ts,whilestillincurringamem-orycost.forthisreasonweprecomputetheval-uesonlyfornodescorrespondingtok-gramsuptolength  m(forourword-levelexperiments  m=10),whicharemostlikelytobeaccessedatruntime.5theprecomputationmethodisoutlinedinalgorithm2,showinghowacompressedcacheiscreatedforthequantitiesx   {n1+(    ),n1+(      ),n12(    ),n012(    )}.thealgorithmvisitsthesuf   xtreenodesindepth-   rst-search(dfs)order,andselectsasubsetofnodesforprecomputation(line7),suchthattheremainingnodesareeitherrareortrivialtohandle4eachnodecanuniquelybeidenti   edbytheorderwhichitisvisitedinadfstraversalofthesuf   xtree.thiscorrespondstotherankoftheopeningparenthesisofthenodeinthebal-ancedparenthesisrepresentationofthetreetopologyofthecstwhichcanbedeterminedino(1)time(ohlebuschetal.,2010).5wedidnottestotherselectioncriteria.othermethodsmaybemoreeffective,suchasselectingnodesforprecomputationbasedonthefrequencyoftheircorrespondingpatternsinthetrainingset.algorithm2precomputingexpensivecountsn{1,2}(    ),n1+(      ),n1+(    ),n0{1,2}(    ).1:functionprecompute(t,  m)2:bvl   0   l   [0,nodes(t)   1]3:i(x)l   0   l   [0,nodes(t)   1],x   counttypes4:j   05:forv   descendants(root(t))do.dfs6:d   string-depth(v)7:ifnotis-leaf(v)   d     mthen8:l   id(v).unique :bvl   110:calln1pfrontback1(t,v,  )11:calln123pfront(t,v,  ,0)12:calln123pfront(t,v,  ,1)13:i(x)j   countsfromabove,foreachoutputx14:j   j+115:bvrrr   compress-rrr(bv)16:i   compress-dac({i(x)   x})17:write-to-disk(bvrrr,i)on-the-   y(i.e.,leafnodes).anodeincludedinthecacheismarkedbystoringa1inthebitvectorbv(lines8-9)atindexl,wherelisthenodeidenti   er.foreachselectednodeweprecomputetheexpensivecountsinlines10-12,n1+(      ),n1+(    )via6n1pfrontback1(t,v,  ),n{1,2}(    )vian123pfront(t,v,  ,0),n0{1,2}(    )vian123pfront(t,v,  ,1),whicharestoredintointegervectorsi(x)foreachcounttypex(line13).theintegervectorsarestreamedtodiskandthencompressed(lines15-17)inordertolimitmemoryusage.the   nalstepsinlines15and16compresstheintegerandbit-vectors.theintegervectorsi(x)arecompressedusingavariablelengthencod-ing,namelydirectlyaddressablevariable-lengthcodes(dac;brisaboaetal.(2009))whichallowsforef   cientstorageofintegerswhileprovidingef   -cientrandomaccess.astheoverwhelmingmajorityofourprecomputedvaluesaresmall(seefigure4left),thisgivesrisetoadramaticcompressionrateofonly   5.2bitsperinteger.thebitvectorbvofsizeo(n)wherenisthenumberofnodesinthesuf-   xtree,iscompressedusingtheschemeoframanetal.(2002)whichsupportsconstanttimerankop-erationoververylargebitvectors.6thefunctionn1pfrontback1isde   nedasalgorithm5inshareghietal.(2015).thisencodingallowsforef   cientretrievaloftheprecomputedcountsatquerytime.thecompressedvectorsareloadedintomemoryandwhenanex-pensivecountisrequiredfornodev,theprecom-putedquantitiescanbefetchedinconstanttimevialookup(v,bv,i(x))=i(x)rank(bv,id(v),1).weuseranktodeterminethenumberof1sprecedingv   spositioninthebitvectorbv.thiscorrespondstov   sindexinthecompressedintegervectorsi(x),fromwhichitsprecomputedcountcanbefetched.thisstrategyonlyappliesforprecomputednodes;forothernodes,thevaluesarecomputedon-the-   y.figure3comparesthequerytimebreakdownforon-the-   ycountcomputation(top)versusprecom-putation(bottom),forbothknandmknandwithdifferentmarkovorders,m.notethatqueryspeedimprovesdramatically,byafactorofabout2500  ,forprecomputedcases.thisimprovementcomesatamodestcostinconstructionspace.precomputingforcstnodeswithm   10resultedin20%ofthenodesbeingselectedforprecomputation.thespaceusedbytheprecomputedvaluesaccountsfor20%ofthetotalspaceusage(seefigure4right).indexconstructiontimeincreasedby70%.4.3computingmknid203havingestablishedameansofcomputingthereq-uisitecountsformknandanef   cientprecomputa-tionstrategy,wenowturntothealgorithmforcom-putingthelanguagemodelid203.thisispre-sentedinalgorithm3,whichisbasedonshareghietal.(2015)   ssinglecstapproachforcomputingtheknid203(reportedintheirpaperasalgo-rithm4.)similartotheirmethod,ourapproachim-plementstherecursivem-gramid203formula-tionasaniterativeloop(hereusingmkn).thecoreofthealgorithmarethetwonodesvfullandvwhichcorrespondtonodesmatchingthefullk-gramandits(k   1)-gramcontext,respectively.althoughsimilartoshareghietal.(2015)   smethod,whichalsofeaturesasimilarright-to-leftpatternlookup,inadditionweoptimisethecompu-tationofafullsentenceid203byslidingawin-dowofwidthmoverthesequencefroid113ft-to-right,addingonenewwordatatime.7thisallowsforthe7paulsandklein(2011)proposeasimilaralgorithmfortrie-basedlms.algorithm3mknid203p(cid:0)wi|wi   1i   (m   1)(cid:1)1:functionprobmkn(t,wii   m+1,m,[vk]m   1k=0)2:assumption:vkisthematchingnodeforwi   1i   k3:vfull0   root(t).tracksmatchforwii   k4:p   1/|  |5:fork   1tomdo6:ifvk   1doesnotmatchthen7:breakoutofloop8:vfullk   back-search([lb(vfullk   1),rb(vfullk   1)],wi   k+1)9:dk(1),dk(2),dk(3+)   discountsfork-grams10:ifk=mthen11:c   size(vfullk)12:d   size(vk   1)13:n1,2,3+   n123pfront(t,vk   1,wi   1i   k+1,0)14:else15:c   n1pback1(t,vfullk,wi   1i   k+1)16:d   n1pfrontback1(t,vk   1,wi   1i   k+1)17:n1,2,3+   n123pfront(t,vk   1,wi   1i   k+1,1)18:if1   c   2then19:c   c   dk(c)20:else21:c   c   dk(3+)22:     dk(1)n1+dk(2)n2+dk(3+)n3+23:p   1d(c+  p)24:return(cid:16)p,(cid:2)vfullk(cid:3)m   1k=0(cid:17)re-useofnodesinonewindowmatchingthefullk-grams,vfull,asthenodesmatchingthecontextinthesubsequentwindow,denotedv.forexample,inthesentence   theforceisstrongwiththisone.   ,computingthe4-gramid203of   theforceisstrong   requiresmatchesintothecstfor   strong   ,   isstrong   ,etc.asillustratedintable1,forthenext4-gramresultingfromslid-ingthewindowtoinclude   with   ,thedenomina-tortermsrequireexactlythesenodes,seefigure5.practically,thisisachievedbystoringthematchingvfullnodescomputedinline8,andpassingthisvec-torastheinputargument[vk]m   1k=0tothenextcalltoprobmkn(line1).thissaveshalfthecallstobackward-search,which,asshowninfigure3,rep-resentasigni   cantfractionofthequeryingcost,re-sultingina30%improvementinqueryruntime.thealgorithmstartsbyconsideringtheunigramid203,andgrowsthecontexttoitsleftbyonewordatatimeuntilthem-gramisfullycovered(line5).thisbestsuitstheuseofbackward-searchinacst,whichproceedsfromright-to-leftoverthesearchpattern.ateachstagethesearchforsisfistfiswswiswfiswsisfisn1+(   w)n1+(      )n1+(   sw)n1+(   s   )n   123+(s   )n1+(   isw)n1+(   is   )n   123+(is    )c (fisw)c (fis)n123+(fis   )n   123+(     )root : [0,n]root : [0,n]figure5:examplemknid203computationfora4-gramlmappliedto   theforceisstrongwith   (eachwordabbreviatedtoits   rstcharacter),showinginthetwoleftcolumnsthesuf   xmatchesrequiredforthe4-gramfiswandelementswhichcanbereusedfrompre-vious4-gramcomputation(grayshading),tfis.ele-mentsontherightdenotethecountandoccurrencestatis-ticsderivedfromthesuf   xmatches,aslinkedbybluelines.vfullkusesthespanfromthepreviousmatch,vfullk   1,alongwiththebwttoef   cientlylocatethematch-ingnode.oncethenodesmatchingthefullse-quenceanditscontextareretrieved,theprocedureisfairlystraightforward:thediscountsareloadedonline9andappliedinlines18-21,whilethenu-merator,denominatorandsmoothingquantitiesasrequiredforcomputingpand  parecalculatedinlines10-13and15-17,respectively.8notethatthecallsforfunctionsn123pfront,n1pback1,n1pfrontback1areavoidedifthecorrespondingnodeisamongsttheselectednodesintheprecompu-tationstep;insteadthelookupfunctioniscalled.finally,thesmoothingweight  iscomputedinline22andtheconditionalid203computedonline23.theloopterminateswhenwereachthelengthlimitk=morwecannotmatchthecontext,i.e.,wi   1i   kisnotinthetrainingcorpus,inwhichcasetheid203valuepforthelongestmatchisreturned.wenowturntothediscountparameters,dk(j),k   m,j   1,2,3+,whicharefunctionofthecorpusstatisticsasoutlinedinfigure1.whilethesecouldbecomputedbasedonrawm-gramstatistics,thisapproachisveryinef   cientforlargem   5;insteadthesevaluescanbecomputedef   -cientlyfromthecompresseddatastructures.algo-rithm4outlineshowthedk(i)valuescanbecom-8n1pback1andn1pfrontback1arede   nedinshareghietal.(2015);seealso  3foranoverview.algorithm4computediscounts1:functioncomputediscounts(t,  m,bv0,sa)2:ni(k)   0,  ni(k)   0   i   [1,4],k   [1,  m]3:n1+(    )   04:forv   descendants(root(t))do.dfs5:dp   string-depth(parent(v))6:d   string-depth(v)7:ds   depth-next-sentinel(sa,bv0,lb(v))8:i   size(v).frequency9:c   interval-symbols(t,[lb(v),rb(v)]).leftocc.10:fork   dp+1tomin(d,  m,ds   1)do11:ifk=2then12:n1+(    )   n1+(    )+113:if1   i   4then14:ni(k)   ni(k)+115:if1   c   4then16:  nc(k)     nc(k)+117:dk(i)   computedusingformulainfigure118:returndk(i),k   [1,  m],i   {1,2,3+}puteddirectlyfromthecst.thismethoditeratesoverthenodesinthesuf   xtree,andforeachnodeconsidersthek-gramsencodedintheedgelabel,whereeachk-gramistakentostartattherootnode(toavoidduplicatecounting,weconsiderk-gramsonlycontainedonthegivenedgebutnotinthepar-entedges,i.e.,byboundingkbasedonthestringdepthoftheparentandcurrentnodes,dp   k   d).foreachk-gramwerecorditscount,i(line8),andthenumberofuniquesymbolstotheleft,c(line9),whichareaccumulatedinanarrayforeachk-gramsizeforvaluesbetween1and4(lines13-14and15-16,respectively).wealsorecordthenumberofuniquebigramsbyincrementingacounterduringthetraversal(lines11-12).specialcareisrequiredtoexcludeedgelabelsthatspansentenceboundaries,bydetectingspecialsen-tinelsymbols(line8)thatseparateeachsentenceorconcludethecorpus.thischeckcouldbedonebyrepeatedlycallingedge(v,k)to   ndthekthsymbolonthegivenedgetocheckforsentinels,howeverthisisaslowoperationasitrequiresmultipleback-wardsearchcalls.insteadweprecalculateabitvec-tor,bv0,ofsizeequaltothenumberoftokensinthecorpus,n,inwhichsentinellocationsinthetextaremarkedby1bits.coupledwiththis,weusethesuf-   xarraysa,suchthatdepth-next-sentinel(sa,bv0,   )=select(bv0,rank(bv0,sa   ,1)+1,1)   sa   ,wheresa   returnstheoffsetintothetextforindex   ,andthesaisstoreduncompressedtoavoidtheex-pensivecostofrecoveringthesevalues.9thisfunc-tioncanbeunderstoodas   ndingthe   rstoccurrenceofthepatterninthetext(usingsa   )then   ndingthelocationofthenext1inthebitvector,usingconstanttimerankandselectoperations.thislocatesthenextsentinelinthetext,afterwhichitcomputesthedistancetothestartofthepattern.usingthismethodinplaceofexplicitedgecallsimprovedthetrainingruntimesubstantiallyupto41  .weprecomputethediscountvaluesfork     m-grams.forqueryingwithm>  m(including   )wereusethediscountsforthelargest  m-grams.105experimentstoevaluateourapproachwemeasurememoryandtimeusage,alongwiththepredictiveperplexityscoreofword-levellmsonanumberofdifferentcorporavaryinginsizeanddomain.forallofourword-levellms,weuse  m,  m   10.wealsodemonstratethepositiveimpactofincreasingthesetlimiton  m,  mfrom10to50onimprovingcharacter-levellmperplexity.thesdsllibrary(gogetal.,2014)isusedtoimplementourdatastructures.thebenchmarkingexperimentswererunonasinglecoreofaintelxeone5-2687v33.10ghzserverwith500gibofram.inourword-levelexperiments,weusetheger-mansubsetoftheeuroparl(koehn,2005)asasmallcorpus,whichis382mibinsizemeasuringtherawuncompressedtext.wealsoevaluateonmuchlargercorpora,trainingon32gibsubsetsofthededupli-catedenglish,spanish,german,andfrenchcom-moncrawlcorpus(bucketal.,2014).astestsets,weusednewstest-2014foralllanguagesex-ceptspanish,forwhichweusednewstest-2013.11in9althoughthesacanbeverylarge,weneednotstoreitinmemory.thedfstraversalinalgorithm4(lines4   16)meansthatthecallstosa   occurinincreasingorderof   .hence,weuseon-diskstorageforthesawithasmallmemorymappedbuffer,therebyincurringanegligiblememoryoverhead.10itispossibletocomputethediscountsforallpatternsofthetextusingouralgorithmwiththecomplexitylinearinthelengthofthetext.however,thediscountsappeartoconvergebypatternlength  m=10.thislimitalsohelpstoavoidproblemsofwild   uctuationsindiscountsforverylongpatternsarisingfromnoiseforlowcountevents.11http://www.statmt.org/wmt{13,14}/test.tgzconstructionload+query101001k10k100k0.11.010.00.11.010.0memory[gib]time[sec]cston-the-   ycstprecomputekenlm(trie)kenlm(probing)srilmfigure6:memoryconsumptionandtotalruntimeforthecstwithandwithoutprecomputation,kenlm(trie),andsrilm(default)withm   [2,10],whilewealsoin-cludem=   forcstmethods.trainedontheeuroparlgermancorpusandtestedoverthebottom1msentencesfromgermancommoncrawlcorpus.ourbenchmarkingexperimentsweusedthebottom1msentences(notusedintraining)ofgermancom-mancrawlcorpus.weusedthepreprocessingscriptofbucketal.(2014),thenremovedsentenceswith   2words,andreplacedrarewords12c   9inthetrainingdatawithaspecialtoken.inourcharacter-levelexperiments,weusedthetrainingandtestdataofthebenchmark1-billion-wordscorpus(chelbaetal.,2013).smalldata:germaneuroparlfirst,wecom-parethetimeandmemoryconsumptionofboththesrilmandkenlmtoolkits,andthecstonthesmallgermancorpus.figure6showsthememoryusageforconstructionandqueryingforcst-basedmethodsw/oprecomputationisindependentofm,buttheygrowsubstantiallywithmforthesrilmandkenlmbenchmarks.tomakeourresultscom-parabletothosereportedin(shareghietal.,2015)forquerytimemeasurementswereportedtheload-ingandquerytimecombined.theconstructioncostismodest,requiringlessmemorythanthebench-marksystemsform   3,andrunninginasim-12runningwiththefullvocabularyincreasedthememoryre-quirementby40%forconstructionand5%forqueryingwithourmodel,and10%and30%,resp.forkenlm.constructiontimesforbothapproacheswere15%slower,butqueryruntimewas20%slowerforourmodelversus80%forkenlm.size(m)perplexitytokensm=2m=3m=5m=7m=10m=   en6470321.6183.8154.3152.7152.5152.3es6276231.3133.2111.7109.7109.3109.2fr6100215.8109.285.283.182.682.4de5540588.3336.6292.8288.1287.8287.8table2:perplexitiesonenglish,french,germannew-stests2014,andspanishnewstest2013whentrainedon32gibchunksofenglish,spanish,french,andgermancommoncrawlcorpus.ilartime13(despiteourmethodsupportingqueriesofunlimitedsize).precomputationaddstothecon-structiontime,whichrosefrom173to299seconds,butyieldedspeedimprovementsofseveralordersofmagnitudeforquerying(218kto98secondsfor10-gram).inquerying,thecst-precomputemethodis2-4  slowerthanbothsrilmandkenlmforlargem   5,withtheexceptionofm=10whereitoutperformssrilm.asubstantialfractionofthequerytimeisloadingthestructuresfromdisk;whenthiscostisexcluded,ourapproachisbetween8-13  slowerthanthebenchmarktoolkits.notethatperplexitycomputedbythecstcloselymatchedkenlm(differences   0.1).bigdata:commoncrawltable2reportstheperplexityresultsfortrainingon32gibsubsetsoftheenglish,spanish,french,andgermancommoncrawlcorpus.notethatwithsuchlargedatasets,perplexityimproveswithincreasingm,withsub-stantialgainsavailablemovingabovethewidelyusedm=5.thishighlightstheimportanceofourapproachbeingindependentfromm,inthatwecanevaluateforanym,including   ,atlowcost.heterogeneousdatatoillustratetheeffectsofdomainshift,corpussizeandlanguagemodelcapac-ityonmodellingaccuracy,wenowevaluatethesys-temusingavarietyofdifferenttrainingcorpora.ta-ble3reportstheperplexityforgermanwhentrain-ingoverdatasetsrangingfromthesmalleuroparlup13foralltimingsreportedinthepaperwemanually   ushedthesystemcachebetweeneachoperation(bothforconstructionandquerying)toremovetheeffectofcachingonruntime.toquerykenlm,weusedthespeedoptimisedpopulatemethod.(wealsocomparethememoryoptimisedlazymethodinfig-ure7.)totrainandquerysrilmweusedthedefaultmethodwhichisoptimisedforspeed,buthadslightlyworsememoryusagethanthecompactmethod.size(m)perplexitytrainingtokenssentsm=3m=5m=10europarl552.21004.8973.3971.4commentary60.2941.8928.6928.1ncrawl2007372.0514.8493.5488.9ncrawl20081266.8427.7404.8400.0ncrawl20091196.5433.4408.9404.7ncrawl2010543.0472.7450.9446.8ncrawl201129816.3362.6335.9327.9ncrawl201237720.9343.9315.6307.3ncrawl201364135.1268.9229.8225.6ncrawl201484546.3247.6195.2189.3allcombined2560139.3211.8158.9151.5ccrawl1g17314.2542.8515.5512.7ccrawl2g34628.5493.2462.3459.2ccrawl4g69256.2446.5412.2408.8ccrawl8g1390110.2402.1364.4360.9ccrawl16g2770216.7364.9323.9319.6ccrawl32g5540426.6336.6292.8287.8table3:perplexityofgermannewstest2014withdiffer-entdatasets(europarl,news-commentary,newscrawl2007-2014,commoncrawl1-32gibchunks)andm.to32gibofthecommoncrawlcorpus.notethatthetestsetisfromthesamedomainasthenewscrawl,whichexplainsthevastdifferenceinperplex-ities.thedomaineffectisstrongenoughtoelimi-natetheimpactofusingmuchlargercorpora,com-pare10-gramperplexitiesfortrainingonthesmallernewscrawl2007corpusversuseuroparl.however   bigdata   isstilluseful:inallcasestheperplexityimprovesasweprovidemoredatafromthesamesource.moreover,themagnitudeofthegaininper-plexitywhenincreasingmisin   uencedbythedatasize:withmoretrainingdatahigherorderm-gramsproviderichermodels;therefore,thescalabilityofourmethodtolargedatasetsiscruciallyimportant.benchmarkingagainstkenlmnextwecom-pareourmodelagainstthestate-of-the-artmethod,kenlmtrie.theperplexitydifferencebetweencstandkenlmwaslessthan0.003inallexperiments.constructioncost.figure7a)comparesthepeakmemoryusageofourcstmodelsandkenlm.kenlmisgivenatargetmemoryusageofthepeakusageofourcstmodels.14theconstructionphase14usingthememorybudgetoption,-s.notethatkenlmof-tenusedmorememorythanspeci   ed.allowingkenlmuseof80%oftheavailableramreducedtrainingtimebyafactorofbetween2and4.forthecstrequiredmoretimeforlowerordermod-els(seefigure7c)butwascomparableforlargerm,roughlymatchingkenlmform=10.15forthe32gibdataset,thecstmodeltook14hourstobuild,comparedtokenlm   s13.5and4hoursforthe10-gramand5-grammodels,respectively.querycost.asshowninfigure7b,themem-oryrequirementsforqueryingwiththecstmethodwereconsistentlylowerthankenlmform   4:form=10thememoryconsumptionofkenlmwas277gibcomparedtoour27gib,a10  im-provement.thiscloselymatchesthe   lesizesofthestoredmodelsondisk.figure7dreportsthequeryruntimes,showingthatkenlmgrowssubstantiallyslowerwithincreasingdatasetsizeandincreasinglanguagemodelorder.incontrast,theruntimeofourcstapproachismuchlessaffectedbydatasizeormodelorder.ourapproachisfasterthankenlmwiththememoryoptimisedlazyoptionform   3,oftenbyseveralordersofmagnitude.forthefasterkenlmpopulate,ourmodelisstillhighlycompeti-tive,growingto4  fasterforthelargestdatasize.16theloadingtimeisstillasigni   cantpartoftherun-time;withoutthiscost,ourmodelis5  slowerthankenlmpopulateform=10onthelargestdataset.runningourmodelwithm=   onthelargestdatasizedidnotchangethememoryusageandonlyhadaminoreffectonruntime,taking645s.character-levelmodellingtodemonstratethefullpotentialofourapproach,wenowconsidercharacterbasedlanguagemodelling,evaluatedonthelargebenchmark1-billion-wordslanguagemod-ellingcorpus,a3.9gib(training)datasetwith768mwordsand4billioncharacters.17table4showsthetestperplexityresultsforourmodels,usingthefulltrainingvocabulary.notethatperplexityimproveswithmforthecharacterbasedmodel,butplateausatm=10forthewordbasedmodel;onereasonforthisisthelimiteddiscountcomputation,  m   10,15thecstmethodusesasinglethreadforconstruction,whilekenlmusesseveralthreads.moststagesofconstructionforourmethodcouldbeeasilyparallelised.16kenlmbene   tssigni   cantlyfromcachingwhichcanoc-curbetweenrunsorasmorequeriesareissued(fromm-gramrepetitioninourlarge1millionsentencetestset),whereasthecstapproachdoesnotbene   tnoticeably(asitdoesnotincor-porateanycachingfunctionality).17http://www.statmt.org/lm-benchmark/constructionload+query11010014816321481632inputsize[gib]memory[gib]constructionload+query1001k10k14816321481632inputsize[gib]time[seconds]m2gram3gram4gram5gram8gram10grammethodken(pop.)ken(lazy)csta)b)c)d)figure7:memoryandruntimestatisticsforcstandkenlmforconstructionandqueryingwithdifferentamountsofgermancommoncrawltrainingdataanddifferentmarkovorders,m.wecomparethequeryruntimesagainsttheoptimisedversionofkenlmformemory(lazy)andspeed(populate).forclarity,inthe   gureweonlyshowcstnumbersform=10;theresultsforothersettingsofmareverysimilar.kenlmwastrainedtomatchtheconstructionmemoryrequirementsofthecst-precomputemethod.unittime(s)mem(gib)m=5m=10m=20m=   word81646.2973.4568.6668.7668.80character1793518.583.932.692.372.33table4:perplexityresultsforthe1billionwordbench-markcorpus,showingwordbasedandcharacterbasedmknmodels,fordifferentm.timingsandpeakmem-oryusagearereportedforconstruction.thewordmodelcomputeddiscountsandprecomputedcountsupto  m,  m=10,whilethecharactermodelusedthresholds  m,  m=50.timingsmeasuredonasinglecore.forthewordmodel,whichmaynotbeagoodpa-rameterisationform>  m.despitethecharacterbasedmodel(implicitly)havingamassiveparameterspace,estimatingthismodelwastractablewithourapproach:thecon-structiontimewasamodest5hours(and2.3hoursforthewordbasedmodel.)forthesamedataset,chelbaetal.(2013)reportthattrainingamkn5-grammodeltook3hoursusingaclusterof100cpus;ouralgorithmisfasterthanthis,despiteonlyusingasinglecpucore.18querieswerealsofast:0.72-0.87msand15mspersentenceforwordandcharacterbasedmodels,respectively.6conclusionsweproposedalanguagemodelbasedoncom-pressedsuf   xtrees,arepresentationthatishighly18chelbaetal.(2013)reportabetterperplexityof67.6,buttheyprunedthetrainingvocabulary,whereaswedidnot.alsoweuseastringenttreatmentofoov,followinghea   eld(2013).compactandcanbeeasilyheldinmemory,whilesupportingqueriesneededincomputinglanguagemodelprobabilitiesonthe   y.wepresentedseveraloptimisationstoacceleratethisprocess,withonlyamodestincreaseinconstructiontimeandmemoryusage,yetimprovingqueryruntimesupto2500  .inbenchmarkingagainstthestate-of-the-artkenlmpackageonlargecorpora,ourmethodhassuperiormemoryusageandhighlycompetitiveruntimesforbothqueryingandtraining.ourapproachallowseasyexperimentationwithhighorderlanguagemod-els,andourresultsprovideevidencethatsuchhighordersmostusefulwhenusinglargetrainingsets.wepositthatfurtherperplexitygainscanbere-alisedusingrichersmoothingtechniques,suchasanon-parametricbayesianprior(teh,2006;woodetal.,2011).ourongoingworkwillexplorethisav-enue,aswellasintegratingourlanguagemodelintothemosesmachinetranslationsystem,andimprov-ingthequeryingtimebycachingthelowerorderprobabilities(e.g.,m<4)whichwebelievecanimprovequerytimesubstantiallywhilemaintainingamodestmemoryfootprint.acknowledgementsthisresearchwassupportedbytheaustralianre-searchcouncil(ft130101105),nationalictaus-tralia(nicta)andagooglefacultyresearchaward.referencesthorstenbrants,ashokcpopat,pengxu,franzjoch,andjeffreydean.2007.largelanguagemodelsinmachinetranslation.inproceedingsofthejointconferenceonempiricalmethodsinnaturallan-guageprocessingandcomputationalnaturallan-guagelearning.nievesrbrisaboa,susanaladra,andgonzalonavarro.2009.directlyaddressablevariable-lengthcodes.instringprocessingandinformationretrieval,pages122   130.christianbuck,kennethhea   eld,andbasvanooyen.2014.id165countsandlanguagemodelsfromthecommoncrawl.inproceedingsofthelanguagere-sourcesandevaluationconference.michaelburrowsanddavidwheeler.1994.ablocksortinglosslessdatacompressionalgorithm.techni-calreport124,digitalequipmentcorporationsys-temsresearchcenter.ciprianchelba,tomasmikolov,mikeschuster,qige,thorstenbrants,phillippkoehn,andtonyrobin-son.2013.onebillionwordbenchmarkformeasur-ingprogressinstatisticallanguagemodeling.arxivpreprintarxiv:1312.3005.stanleyfchenandjoshuagoodman.1999.anempir-icalstudyoid122oothingtechniquesforlanguagemod-eling.computerspeech&language,13(4):359   393.kennethchurch,tedhart,andjianfenggao.2007.compressingtrigramlanguagemodelswithgolombcoding.inproceedingsoftheconferenceonempir-icalmethodsinnaturallanguageprocessing,pages199   207.paoloferraginaandgiovannimanzini.2000.oppor-tunisticdatastructureswithapplications.inproceed-ingsoftheannualsymposiumonfoundationsofcom-puterscience,pages390   398.ulrichgermann,ericjoanis,andsamuellarkin.2009.tightlypackedtries:howto   tlargemodelsintomemory,andmakethemloadfast,too.inproceedingsoftheworkshoponsoftwareengineering,testing,andqualityassurancefornaturallanguageprocessing,pages31   39.simongog,timobeller,alistairmoffat,andmatthiaspetri.2014.fromtheorytopractice:plugandplaywithsuccinctdatastructures.inproceedingsofthein-ternationalsymposiumonexperimentalalgorithms,pages326   337.r.grossi,a.gupta,andj.s.vitter.2003.high-orderid178-compressedtextindexes.inproceed-ingsoftheacm-siamsymposiumondiscretealgo-rithms,pages841   850.davidguthrieandmarkhepple.2010.storingthewebinmemory:spaceef   cientlanguagemodelswithcon-stanttimeretrieval.inproceedingsoftheconferenceonempiricalmethodsinnaturallanguageprocess-ing,pages262   272.kennethhea   eld,ivanpouzyrevsky,jonathanh.clark,andphilippkoehn.2013.scalablemodi   edkneser-neylanguagemodelestimation.inproceedingsofthe51stannualmeetingoftheassociationforcomputa-tionallinguistics,pages690   696.kennethhea   eld.2011.kenlm:fasterandsmallerlan-guagemodelqueries.inproceedingsoftheworkshoponstatisticalmachinetranslation.kennethhea   eld.2013.ef   cientlanguagemodelingalgorithmswithapplicationstostatisticalmachinetranslation.ph.d.thesis,carnegiemellonuniversity.guyjacobson.1989.space-ef   cientstatictreesandgraphs.inproceedingsoftheannualsymposiumonfoundationsofcomputerscience,pages549   554.caseyreddkennington,martinkay,andannemariefriedrich.2012.suf   xtreesaslanguagemodels.inproceedingsoftheconferenceonlanguagere-sourcesandevaluation,pages446   453.philippkoehn.2005.europarl:aparallelcorpusforstatisticalmachinetranslation.inmachinetransla-tionsummit,volume5,pages79   86.abbylevenbergandmilesosborne.2009.stream-basedrandomisedlanguagemodelsforsmt.inpro-ceedingsoftheconferenceonempiricalmethodsinnaturallanguageprocessing,pages756   764.udimanberandeugenew.myers.1993.suf   xarrays:anewmethodforon-linestringsearches.siamjour-naloncomputing,22(5):935   948.gonzalonavarro.2014.wavelettreesforall.journalofdiscretealgorithms,25:2   20.ennoohlebusch,johannesfischer,andsimongog.2010.cst++.inproceedingsoftheinternationalsymposiumonstringprocessingandinformationre-trieval,pages322   333.adampaulsanddanklein.2011.fasterandsmallerid165languagemodels.inproceedingsoftheannualmeetingoftheassociationforcomputationallinguis-tics:humanlanguagetechnologies.rajeevraman,venkateshraman,andssrinivasarao.2002.succinctindexabledictionarieswithapplica-tionstoencodingk-arytreesandmultisets.inpro-ceedingsofthethirteenthannualacm-siamsympo-siumondiscretealgorithms,pages233   242.thomasschnattinger,ennoohlebusch,andsimongog.2010.bidirectionalsearchinastringwithwavelettrees.inproceedingsoftheannualsymposiumoncombinatorialpatternmatching,pages40   50.ehsanshareghi,matthiaspetri,gholamrezahaffari,andtrevorcohn.2015.compact,ef   cientandunlimitedcapacity:languagemodelingwithcompressedsuf   xtrees.inproceedingsoftheconferenceonempiri-calmethodsinnaturallanguageprocessing,pages2409   2418.jeffreysorensenandcyrilallauzen.2011.unarydatastructuresforlanguagemodels.inproceedingsofin-terspeech,pages1425   1428.andreasstolcke,jingzheng,wenwang,andvictorabrash.2011.srilmatsixteen:updateandoutlook.inproceedingsofieeeautomaticspeechrecognitionandunderstandingworkshop.andreasstolcke.2002.srilm   anextensiblelanguagemodelingtoolkit.inproceedingsoftheinternationalconferenceofspokenlanguageprocessing.davidtalbotandmilesosborne.2007.randomisedlanguagemodellingforstatisticalmachinetranslation.inproceedingsoftheannualmeetingoftheassocia-tionforcomputationallinguistics.yeewhyeteh.2006.ahierarchicalbayesianlanguagemodelbasedonpitman-yorprocesses.inproceedingsoftheannualmeetingoftheassociationforcompu-tationallinguistics,pages985   992.tarowatanabe,hajimetsukada,andhidekiisozaki.2009.asuccinctid165languagemodel.inpro-ceedingsoftheannualmeetingoftheassociationforcomputationallinguistics,pages341   344.peterweiner.1973.linearpatternmatchingalgorithms.inproceedingsoftheannualsymposiumswitchingandautomatatheory,pages1   11.frankwood,jangasthaus,c  edricarchambeau,lancelotjames,andyeewhyeteh.2011.thesequencememoizer.communicationsoftheacm,54(2):91   98.yingzhangandstephanvogel.2006.suf   xarrayanditsapplicationsinempiricalnaturallanguageprocess-ing.technicalreport,cmu,pittsburghpa.