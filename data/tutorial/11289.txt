captioning images with diverse objects

subhashini venugopalan   
raymond mooney   

   ut austin

lisa anne hendricks   

trevor darrell   
   uc berkeley

{vsub,mooney}@cs.utexas.edu

{lisa anne, rohrbach, trevor}

marcus rohrbach   
kate saenko   
   boston univ.

saenko@bu.edu

@eecs.berkeley.edu

7
1
0
2

 
l
u
j
 

0
2

 
 
]

v
c
.
s
c
[
 
 

3
v
0
7
7
7
0

.

6
0
6
1
:
v
i
x
r
a

abstract

recent captioning models are limited in their ability to
scale and describe concepts unseen in paired image-text
corpora. we propose the novel object captioner (noc),
a deep visual semantic captioning model that can describe
a large number of object categories not present in exist-
ing image-caption datasets. our model takes advantage of
external sources     labeled images from object recognition
datasets, and semantic knowledge extracted from unanno-
tated text. we propose minimizing a joint objective which
can learn from these diverse data sources and leverage
distributional semantic embeddings, enabling the model to
generalize and describe novel objects outside of image-
caption datasets. we demonstrate that our model exploits
semantic information to generate captions for hundreds of
object categories in the id163 object recognition dataset
that are not observed in mscoco image-caption training
data, as well as many categories that are observed very
rarely. both automatic evaluations and human judgements
show that our model considerably outperforms prior work
in being able to describe many more categories of objects.

1. introduction

modern visual classi   ers [6, 22] can recognize thou-
sands of object categories, some of which are basic or entry-
level (e.g. television), and others that are    ne-grained and
task speci   c (e.g. dial-phone, cell-phone). however, recent
state-of-the-art visual captioning systems [2, 3, 8, 10, 15,
26] that learn directly from images and descriptions, rely
solely on paired image-caption data for supervision and fail
in their ability to generalize and describe this vast set of rec-
ognizable objects in context. while such systems could be
scaled by building larger image/video description datasets,
obtaining such captioned data would be expensive and la-
borious. furthermore, visual description is challenging be-
cause models have to not only correctly identify visual con-
cepts contained in an image, but must also compose these
concepts into a coherent sentence.

figure 1. we propose a model that learns simultaneously from
multiple data sources with auxiliary objectives to describe a va-
riety of objects unseen in paired image-caption data.

recent work [7] shows that,

to incorporate the vast
knowledge of current visual recognition networks with-
out explicit paired caption training data, caption models
can learn from external sources and learn to compose sen-
tences about visual concepts which are infrequent or non-
existent in image-description corpora. however, the pio-
neering dcc model from [7] is unwieldy in the sense that
the model requires explicit transfer (   copying   ) of learned
parameters from previously seen categories to novel cate-
gories. this not only prevents it from describing rare cate-
gories and limits the model   s ability to cover a wider variety
of objects but also makes it unable to be trained end-to-end.
we instead propose the novel object captioner (noc), a
network that can be trained end-to-end using a joint training
strategy to integrate knowledge from external visual recog-
nition datasets as well as semantic information from inde-
pendent unannotated text corpora to generate captions for a
diverse range of rare and novel objects (as in fig. 1).

speci   cally, we introduce auxiliary objectives which al-
low our network to learn a captioning model on image-
caption pairs simultaneously with a deep language model
and visual recognition system on unannotated text and la-
beled images. unlike previous work, the auxiliary objec-
tives allow the noc model to learn relevant information
from multiple data sources simultaneously in an end-to-end
fashion. furthermore, noc implicitly leverages pre-trained
distributional id27s enabling it to describe un-
seen and rare object categories. the main contributions of
our work are 1) an end-to-end model to describe objects
not present in paired image-caption data, 2) auxiliary/joint

1

visual classifiers. existing captioners.mscocoa okapi standing in the middle of a field.mscoco++noc (ours): jointly train on multiple sources with auxiliary objectives.okapiinit + trainvisual classifiersa horse standing in the dirt.training of the visual and language models on multiple data
sources, and 3) incorporating pre-trained semantic embed-
dings for the task. we demonstrate the effectiveness of our
model by performing extensive experiments on objects held
out from mscoco [13] as well as hundreds of objects from
id163 [21] unseen in caption datasets. our model sub-
stantially outperforms previous work [7] on both automated
as well as human evaluations.

2. related work
visual description. this area has seen many different ap-
proaches over the years [27, 11, 18], and more recently deep
models have gained popularity for both their performance
and potential for end-to-end training. deep visual descrip-
tion frameworks    rst encode an image into a    xed length
feature vector and then generate a description by either con-
ditioning text generation on image features [2, 8, 26] or em-
bedding image features and previously generated words into
a multimodal space [9, 10, 15] before predicting the next
word. though most models represent images with an inter-
mediate representation from a convolutional neural network
(such as fc7 activations from a id98), other models repre-
sent images as a vector of con   dences over a    xed number
of visual concepts [3, 7]. in almost all cases, the parameters
of the visual pipeline are initialized with weights trained on
the id163 classi   cation task. for id134,
recurrent networks (id56s) are a popular choice to model
language, but log bilinear models [9] and maximum id178
language models [3] have also been explored. our model is
similar to the id98-id56 frameworks in [7, 15] but neither
of these models can be trained end-to-end to describe ob-
jects unseen in image-caption pairs.
novel object captioning.
[16] proposed an approach that
extends a model   s capability to describe a small set of novel
concepts (e.g. quidditch, samisen) from a few paired train-
ing examples while retaining its ability to describe previ-
ously learned concepts. on the other hand, [7] introduce
a model that can describe many objects already existing
in english corpora and object recognition datasets (ima-
genet) but not in the caption corpora (e.g. pheasant, otter).
our focus is on the latter case.
[7] integrate information
from external text and visual sources, and explicitly trans-
fer (   copy   ) parameters from objects seen in image-caption
data to unseen id163 objects to caption these novel cate-
gories. while this works well for many id163 classes it
still limits coverage across diverse categories and cannot be
trained end-to-end. furthermore, their model cannot cap-
tion objects for which few paired training examples already
exist. our proposed framework integrates distributional se-
mantic embeddings implicitly, obviating the need for any
explicit transfer and making it end-to-end trainable. it also
extends directly to caption id163 objects with few or no
descriptions.

figure 2. our noc image caption network. during training,
the visual recognition network (left), the lstm-based language
model (right), and the caption model (center) are trained simul-
taneously on different sources with different objectives but with
shared parameters, thus enabling novel object captioning.
multi-modal and zero-shot learning. another closely
related line of research takes advantage of distributional
semantics to learn a joint embedding space using visual
and textual information for zero-shot labeling of novel
object categories [4, 19], as well as retrieval of images
with text [12, 23]. visual description itself can be cast
as a multimodal learning problem in which caption words
w0, ..., wn   1 and an image are projected into a joint em-
bedding space before the next word in a caption, wn, is gen-
erated [10, 15]. although our approach uses distributional
id27s, our model differs in the sense that it can
be trained with unpaired text and visual data but still com-
bine the semantic information at a later stage during cap-
tion generation. this is similar in spirit to works in natural
language processing that use monolingual data to improve
machine translation [5].
3. novel object captioner (noc)

our noc model is illustrated in fig. 2. it consists of a
language model that leverages distributional semantic em-
beddings trained on unannotated text and integrates it with a
visual recognition model. we introduce auxiliary loss func-
tions (objectives) and jointly train different components on
multiple data sources, to create a visual description model
which simultaneously learns an independent object recog-
nition model, as well as a language model.

we start by    rst training a lstm-based language model
(lm) [24] for sentence generation. our lm incorporates
dense representations for words from distributional embed-
dings (glove, [20]) pre-trained on external text corpora. si-
multaneously, we also train a state-of-the-art visual recog-
nition network to provide con   dences over words in the vo-
cabulary given an image. this decomposes our model into
discrete textual and visual pipelines which can be trained
exclusively using unpaired text and unpaired image data
(networks on left and right of fig. 2). to generate descrip-
tions conditioned on image content, we combine the pre-
dictions of our language and visual recognition networks
by summing (element-wise) textual and visual con   dences

jointtrainingshared parametersid98embedmscocoshared parameterselementwisesumid98embedlstm(wglove)twgloveembedjointtrainingjoint-objective lossimage-specific lossimage-text losstext-specific losslstm(wglove)twgloveembedover the vocabulary of words. during training, we introduce
auxiliary image-speci   c (lim), and text-speci   c (llm)
objectives along with the paired image-caption (lcm) loss.
these id168s, when trained jointly, in   uence our
model to not only produce reasonable image descriptions,
but also predict visual concepts as well as generate cohe-
sive text (id38). we    rst discuss the auxiliary
objectives and the joint training, and then discuss how we
leverage embeddings trained with external text to compose
descriptions about novel objects.
3.1. auxiliary training objectives

our motivation for introducing auxiliary objectives is to
learn how to describe images without losing the ability to
recognize more objects. typically, image-captioning mod-
els incorporate a visual classi   er pre-trained on a source do-
main (e.g. id163 dataset) and then tune it to the target
domain (the image-caption dataset). however, important
information from the source dataset can be suppressed if
similar information is not present when    ne-tuning, lead-
ing the network to forget (over-write weights) for objects
not present in the target domain. this is problematic in our
scenario in which the model relies on the source datasets
to learn a large variety of visual concepts not present in the
target dataset. however, with pre-training as well as the
complementary auxiliary objectives the model maintains its
ability to recognize a wider variety of objects and is encour-
aged to describe objects which are not present in the target
dataset at test time. for the ease of exposition, we abstract
away the details of the language and the visual models and
   rst describe the joint training objectives of the complete
model, i.e.
the text-speci   c loss, the image-speci   c loss,
and the image-caption loss. we will then describe the lan-
guage and the visual models.
3.1.1 image-speci   c loss
our visual recognition model (fig. 2, left) is a neural net-
work parametrized by   i and is trained on object recogni-
tion datasets. unlike typical visual recognition models that
are trained with a single label on a classi   cation task, for the
task of image captioning an image model that has high con-
   dence over multiple visual concepts occurring in an image
simultaneously would be preferable. hence, we choose to
train our model using multiple labels (more in sec. 5.1) with
a multi-label loss. if l denotes a label and zl denotes the bi-
nary ground-truth value for the label, then the objective for
the visual model is given by the cross-id178 loss (lim ):

lim(i;   i ) =    (cid:88)

(cid:104)

zl log(sl(fim (i;   i )))

l

+ (1     zl) log(1     sl(fim (i;   i )))

(cid:105)

(1)

where si(x) is the output of a softmax function over index
i and input x, and fim , is the activation of the    nal layer of
the visual recognition network.

3.1.2 text-speci   c loss
our language model (fig. 2, right) is based on lstm re-
current neural networks. we denote the parameters of this
network by   l, and the activation of the    nal layer of this
network by flm . the language model is trained to predict
the next word wt in a given sequence of words w0, ..., wt   1.
this is optimized using the softmax loss llm which is
equivalent to the maximum-likelihood:

llm(w0, ..., wt   1;   l) =

   (cid:88)

log(swt(flm (w0, ..., wt   1;   l)))

(2)

t

image-caption loss

3.1.3
the goal of the image captioning model (fig. 2, center) is
to generate a sentence conditioned on an image (i). noc
predicts the next word in a sequence, wt, conditioned on
previously generated words (w0, ..., wt   1) and an image
(i), by summing activations from the deep language model,
which operates over previous words, and the deep image
model, which operates over an image. we denote these    -
nal (summed) activations by fcm . then, the id203 of
predicting the next word is given by, p (wt|w0, ..., wt   1, i)

=swt(fcm (w0, ..., wt   1, i;   ))
=swt(flm (w0, ..., wt   1;   l) + fim (i;   i ))

(3)

given pairs of images and descriptions, the caption model
optimizes the parameters of the underlying language model
(  l) and image model (  i) by minimizing the caption model
loss lcm : lcm(w0, ., wt   1, i;   l,   i )

log(swt(fcm (w0, ., wt   1, i;   l,   i )))

(4)

=    (cid:88)

t

joint training with auxiliary losses

3.1.4
while many previous approaches have been successful on
image captioning by pre-training the image and language
models and tuning the caption model alone (eqn. 4), this
is insuf   cent to generate descriptions for objects outside of
the image-caption dataset since the model tends to    forget   
(over-write weights) for objects only seen in external data
sources. to remedy this, we propose to train the image
model, language model, and caption model simultaneously
on different data sources. the noc model   s    nal objective
simultaneously minimizes the three individual complemen-
tary objectives:

l = lcm + lim + llm

(5)

by sharing the weights of the caption model   s network with
the image network and the language network (as depicted
in fig. 2 (a)), the model can be trained simultaneously on
independent image-only data, unannotated text data, as well
as paired image-caption data. consequently, co-optimizing
different objectives aids the model in recognizing categories
outside of the paired image-sentence data.

3.2. language model with semantic embeddings

our language model consists of the following compo-
nents: a continuous lower dimensional embedding space
for words (wglove), a single recurrent (lstm) hidden layer,
and two linear transformation layers where the second layer
(w t
glove) maps the vectors to the size of the vocabulary.
finally a softmax activation function is used on the out-
put layer to produce a normalized id203 distribution.
the cross-id178 loss which is equivalent to the maximum-
likelihood is used as the training objective.

in addition to our joint objective (eqn.5), we also em-
ploy semantic embeddings in our language model to help
generate sentences when describing novel objects. speci   -
cally, the initial input embedding space (wglove) is used to
represent the input (one-hot) words into semantically mean-
ingful dense    xed-length vectors. while the    nal transfor-
mation layer (w t
glove) reverses the mapping [15, 25] of a
dense vector back to the full vocabulary with the help of a
softmax activation function. these distributional embed-
dings [17, 20] share the property that words that are seman-
tically similar have similar vector representations. the in-
tuitive reason for using these embeddings in the input and
output transformation layers is to help the language model
treat words unseen in the image-text corpus to (semanti-
cally) similar words that have previously been seen so as to
encourage compositional sentence generation i.e. encour-
age it to use new/rare word in a sentence description based
on the visual con   dence.
3.3. visual classi   er

the other main component of our model is the visual
classi   er.
identical to previous work [7], we employ the
vgg-16 [22] convolutional network as the visual recogni-
tion network. we modify the    nal layers of the network to
incorporate the multi-label loss (eqn. 1) to predict visual
con   dence over multiple labels in the full vocabulary. the
rest of the classi   cation network remains unchanged.

finally, we take an elementwise-sum of the visual and
language outputs, one can think of this as the language
model producing a smooth id203 distribution over
words (based on glove parameter sharing) and then the im-
age signal    selecting    among these based on the visual evi-
dence when summed with the language model beliefs.
4. datasets

in this section we describe the image description dataset
as well as the external text and image datasets used in our
experiments.
4.1. external text corpus (webcorpus)

we extract sentences from gigaword, the british na-
tional corpus (bnc), ukwac, and wikipedia. stanford
corenlp 3.4.2 [14] was used to extract id121s. this
dataset was used to train the lstm language model. for

the dense word representation in the network, we use glove
[20] pre-trained on 6b tokens of external corpora including
gigaword and wikipedia. to create our lm vocabulary we
identi   ed the 80,000 most frequent tokens from the com-
bined external corpora. we re   ne this vocabulary further to
a set of 72,700 words that also had glove embeddings.
4.2. image caption data

to empirically evaluate the ability of noc to describe
new objects we use the training and test set from [7].
this dataset is created from mscoco [13] by cluster-
ing the main 80 object categories using cosine distance on
id97 (of the object label) and selecting one object from
each cluster to hold out from training. the training set holds
out images and sentences of 8 objects (bottle, bus, couch,
microwave, pizza, racket, suitcase, zebra), which constitute
about 10% of the training image and caption pairs in the
mscoco dataset. our model is evaluated on how well it
can generate descriptions about images containing the eight
held-out objects.
4.3. image data

we also evaluate sentences generated by noc on ap-
proximately 700 different id163 [21] objects which are
not present in the mscoco dataset. we choose this set
by identifying objects that are present in both id163
and our language corpus (vocabulary), but not present in
mscoco. chosen words span a variety of categories in-
cluding    ne-grained categories (e.g.,    bloodhound    and
   chrysanthemum   ), adjectives (e.g.,    chiffon   ,    woollen   ),
and entry level words (e.g.,    toad   ). further, to study how
well our model can describe rare objects, we pick a sepa-
rate set of 52 objects which are in id163 but mentioned
infrequently in mscoco (52 mentions on average, with
median 27 mentions across all 400k training sentences).
5. experiments on mscoco

we perform the following experiments to compare
noc   s performance with previous work [7]: 1. we eval-
uate the model   s ability to caption objects that are held out
from mscoco during training (sec. 5.1). 2. to study the
effect of the data source on training, we report performance
of noc when the image and language networks are trained
on in-domain and out-of-domain sources (sec. 5.2). in addi-
tion to these, to understand our model better: 3. we perform
ablations to study how much each component of our model
(such as id27s, auxiliary objective, etc.) con-
tributes to the performance (sec. 5.3). 4. we also study if
the model   s performance remains consistent when holding
out a different subset of objects from mscoco (sec. 5.4).
5.1. empirical evaluation on mscoco

we empirically evaluate the ability of our proposed
model to describe novel objects by following the experi-
mental setup of [7]. we optimize each loss in our model
with the following datasets:
the caption model, which

model
dcc
noc (ours)

bottle
4.63
17.78

bus
29.79
68.79

couch microwave
45.87
25.55

28.09
24.72

pizza
64.59
69.33

racket
52.24
55.31

suitcase
13.16
39.86

zebra avg. f1 avg. meteor
79.88
89.02

39.78
48.79

21.00
21.32

table 1. mscoco captioning: f1 scores (in %) of noc (our model) and dcc [7] on held-out objects not seen jointly during image-
caption training, along with the average f1 and meteor scores of the generated captions across images containing these objects.

image

text model meteor

baseline

(no transfer)

image
net

coco

web
corpus

web
corpus

coco

coco

1

2

3

4

lrcn
dcc
dcc
noc

noc

dcc
noc

f1
0
0

34.94
36.50

19.33
19.90
20.66
17.56

19.18

41.74

21.00
21.32

39.78
48.79

figure 3. coco captioning: examples comparing captions by
noc (ours) and dcc [7] on held out objects from mscoco.
jointly learns the parameters   l and   i, is trained only
on the subset of mscoco without the 8 objects (see sec-
tion 4.2), the image model, which updates parameters   i, is
optimized using labeled images, and the language model
which updates parameters   l, is trained using the corre-
sponding descriptions. when training the visual network
on images from coco, we obtain multiple labels for each
image by considering all words in the associated captions as
labels after removing stop words. we    rst present evalua-
tions for the in-domain setting in which the image classi   er
is trained with all coco training images and the language
model is trained with all sentences. we use the meteor
metric [1] to evaluate description quality. however, me-
teor only captures    uency and does not account for the
mention (or lack) of speci   c words. hence, we also use f1
to ascertain that the model mentions the object name in the
description of the images containing the object. thus, the
metrics measure if the model can both identify the object
and use it    uently in a sentence.

coco heldout objects. table 1 compares the f1 score
achieved by noc to the previous best method, dcc [7] on
the 8 held-out coco objects. noc outperforms dcc (by
10% f1 on average) on all objects except    couch    and    mi-
crowave   . the higher f1 and meteor demonstrate that
noc is able to correctly recognize many more instances of
the unseen objects and also integrate the words into    uent
descriptions.

5.2. training data source

to study the effect of different data sources, we also eval-
uate our model in an out-of-domain setting where classi   ers

table 2. comparison with different training data sources on 8 held-
out coco objects. having in-domain data helps both the dcc [7]
and our noc model caption novel objects.

for held out objects are trained with images from id163
and the language model is trained on text mined from exter-
nal corpora. table 2 reports average scores across the eight
held-out objects. we compare our noc model to results
from [7] (dcc), as well as a competitive image caption-
ing model - lrcn [2] trained on the same split.
in the
out-of-domain setting (line 2), for the chosen set of 8 ob-
jects, noc performs slightly better on f1 and a bit lower on
meteor compared to dcc. however, as previously men-
tioned, dcc needs to explicitly identify a set of    seen    ob-
ject classes to transfer weights to the novel classes whereas
noc can be used for id136 directly. dcc   s transfer
mechanism also leads to peculiar descriptions. e.g., racket
in fig. 3.

with coco image training (line 3), f1 scores of noc
improves considerably even with the web corpus lm train-
ing. finally in the in-domain setting (line 4) noc outper-
forms dcc on f1 by around 10 points while also improving
meteor slightly. this suggests that noc is able to asso-
ciate the objects with captions better with in-domain train-
ing, and the auxiliary objectives and embedding help the
model to generalize and describe novel objects.

5.3. ablations

table 3 compares how different aspects of training im-
pact the overall performance. tuned vision contribution
the model that does not incorporate glove or lm pre-
training has poor performance (meteor 15.78, f1 14.41);
this ablation shows the contribution of the vision model
alone in recognizing and describing the held out objects.
lm & glove contribution: the model trained without the

glove lm pretrain tuned visual auxiliary
objective

classi   er

contributing factor

tuned vision

lm & embedding

lm & pre-trained vision

auxiliary objective

all

-
(cid:88)
(cid:88)
(cid:88)
(cid:88)

-
(cid:88)
(cid:88)
-
(cid:88)

(cid:88)
(cid:88)
fixed
(cid:88)
(cid:88)

(cid:88)
-
-
(cid:88)
(cid:88)

meteor

15.78
19.80
18.91
19.69
21.32

f1

14.41
25.38
39.70
47.02
48.79

table 3. ablations comparing the contributions of the glove embedding, lm pre-training, and auxiliary objectives, of the noc model.
our auxiliary objective along with glove have the largest impact in captioning novel objects.

model
noc (ours)

bed
53.31

book
18.58

carrot
20.69

elephant
85.35

spoon
02.70

toilet
73.61

truck
57.90

umbrella avg. f1 avg. meteor

54.23

45.80

20.04

table 4. mscoco captioning: f1 scores (in %) of noc (our model) on a different subset of the held-out objects not seen jointly during
image-caption training, along with the average f1 and meteor scores of the generated captions across images containing these objects.
noc is consistently able to caption different subsets of unseen object categories in mscoco.

auxiliary objective, performs better with f1 of 25.38 and
meteor of 19.80; this improvement comes largely from
the glove embeddings which help in captioning novel ob-
ject classes. lm & pre-trained vision: it   s interesting to
note that when we    x classi   er   s weights (pre-trained on
all objects), before tuning the lm on the image-caption
coco subset, the f1 increases substantially to 39.70 sug-
gesting that the visual model recognizes many objects but
can    forget    objects learned by the classi   er when    ne-
tuned on the image-caption data (without the 8 objects).
auxiliary objective: incorporating the auxiliary objectives,
f1 improves remarkably to 47.02. we note here that by
virtue of including auxiliary objectives the visual network
is tuned on all images thus retaining it   s ability to clas-
sify/recognize a wide range of objects. finally, incorporat-
ing all aspects gives noc the best performance (f1 48.79,
meteor 21.32), signi   cantly outperforming dcc.
5.4. validating on a different subset of coco

to show that our model is consistent across objects, we
create a different training/test split by holding out a differ-
ent set of eight objects from coco. the objects we hold out
are: bed, book, carrot, elephant, spoon, toilet, truck and um-
brella. images and sentences from these eight objects again
constitute about 10% of the mscoco training dataset. ta-
ble 4 presents the performance of the model on this subset.
we observe that the f1 and meteor scores, although a
bit lower, are consistent with numbers observed in table 1
con   rming that our model is able to generalize to different
subsets of objects.

6. experiments: scaling to id163

to demonstrate the scalability of noc, we describe ob-
jects in id163 for which no paired image-sentence data
exists. our experiments are performed on two subsets of

id163, (i) novel objects: a set of 638 objects which are
present in id163 as well as the model   s vocabulary but
are not mentioned in mscoco. (ii) rare objects: a set of
52 objects which are in id163 as well as the mscoco
vocabulary but are mentioned infrequently in the mscoco
captions (median of 27 mentions). for quantitative evalua-
tion, (i) we measure the percentage of objects for which the
model is able to describe at least one image of the object
(using the object label), (ii) we also report accuracy and f1
scores to compare across the entire set of images and objects
the model is able to describe. furthermore, we obtain hu-
man evaluations comparing our model with previous work
on whether the model is able to incorporate the object label
meaningfully in the description together with how well it
describes the image.

6.1. describing novel objects

table 5 compares models on 638 novel object categories
(identical to [7]) using the following metrics: (i) describing
novel objects (%) refers to the percentage of the selected
id163 objects mentioned in descriptions, i.e. for each
novel word (e.g.,    otter   ) the model should incorporate the
word (   otter   ) into at least one description about an ima-
genet image of the object (otter). while dcc is able to
recognize and describe 56.85% (363) of the selected ima-
genet objects in descriptions, noc recognizes several more
objects and is capable of describing 91.27% (582 of 638)
id163 objects.
(ii) accuracy refers to the percentage
of images from each category where the model is able to
correctly identify and describe the category. we report the
average accuracy across all categories. dcc incorporates
a new word correctly 11.08% of the time, in comparison,
noc improves this appreciably to 24.74%. (iii) f1 score is
computed based on precision and recall of mentioning the
object in the description. again, noc outperforms with av-

model desc. novel (%) acc (%)
dcc
11.08
24.74
noc

56.85
91.27

f1 (%)
14.47
33.76

table 5. id163: comparing our model against dcc [7] on
% of novel classes described, average accuracy of mentioning the
class in the description, and mean f1 scores for object mentions.

figure 5. id163 captioning: common types of errors observed
in the captions generated by the noc model.
6.2. describing rare objects/words

the selected rare words occur with varying frequency in
the mscoco training set, with about 52 mentions on aver-
age (median 27) across all training sentences. for example,
words such as    bonsai    only appear 5 times,   whisk    (11
annotations),    teapot    (30 annotations), and others such as
pumpkin appears 58 times,    swan    (60 annotations), and on
the higher side objects like scarf appear 144 times. when
tested on id163 images containing these concepts, a
model trained only with mscoco paired data incorporates
rare words into sentences 2.93% of the time with an av-
erage f1 score of 4.58%. in contrast, integrating outside
data, our noc model can incorporate rare words into de-
scriptions 35.15% of the time with an average f1 score of
47.58%. we do not compare this to dcc since dcc cannot
be applied directly to caption rare objects.
6.3. human evaluation

id163 images do not have accompanying captions
and this makes the task much more challenging to evalu-
ate. to compare the performance of noc and dcc we ob-
tain human judgements on captions generated by the mod-
els on several object categories. we select 3 images each
from about 580 object categories that at least one of the
two models, noc and dcc, can describe. (note that al-
though both models were trained on the same id163 ob-
ject categories, noc is able to describe almost all of the
object categories that have been described by dcc). when
selecting the images, for object categories that both models
can describe, we make sure to select at least two images for
which both models mention the object label in the descrip-
tion. each image is presented to three workers. we con-
ducted two human studies (sample interface is in the supple-
ment): given the image, the ground-truth object category
(and meaning), and the captions generated by the models,
we evaluate on:
word incorporation: we ask humans to choose which
sentence/caption incorporates the object label mean-

figure 4. id163 captioning: examples comparing captions by
noc (ours) and dcc [7] on objects from id163.

erage f1 33.76% to dcc   s 14.47%.

although noc and dcc [7] use the same id98, noc
is both able to describe more categories, and correctly inte-
grate new words into descriptions more frequently. dcc [7]
can fail either with respect to    nding a suitable object that
is both semantically and syntactically similar to the novel
object, or with regard to their language model composing a
sentence using the object name, in noc the former never
occurs (i.e. we don   t need to explicitly identify similar ob-
jects), reducing the overall sources of error.

fig. 4 and fig. 6 (column 3) show examples where noc
describes a large variety of objects from id163. fig. 4
compares our model with dcc. fig. 5 and fig. 6 (right)
outline some errors. failing to describe a new object is one
common error for noc. e.g. fig. 6 (top right), noc in-
correctly describes a man holding a    sitar    as a man hold-
ing a    baseball bat   . other common errors include generat-
ing noid165matical or nonsensical phrases (example with
   gladiator   ,    aardvark   ) and repeating a speci   c object (   a
barracuda ... with a barracuda   ,    tri   e cake   ).

moussaka (n07872593)dcc: a white plate topped with a sandwich and a moussaka.noc (ours): a moussaka with cheese and vegetables on a white plate.scythe (n04158250)dcc: a small child is holding a small child on a skateboard.noc (ours): a man is standing on a green field with a scythe.caribou (n02433925)dcc: a caribou is in a field with a small caribou.noc (ours): a caribou that is standing in the grass.circuitry (n03034405)dcc: a large white and black and white photo of a large building.noc (ours): a bunch of different types of circuitry on a table.warship (n04552696)dcc: a warship is sitting on the water.noc (ours): a large warship is on the water.newsstand (n03822656)dcc: a bunch of people are sitting on a newsstand.noc (ours): a extremely large newsstand with many different items on it.pharmacy (n03249342) [both models incorporate the word incorrectly]dcc: a white refrigerator freezer sitting on top of a pharmacy.noc (ours): a kitchen with a pharmacy and a refrigerator.woollen (n04599235)dcc: a red and white cat sitting on top of a red woollen.noc (ours): a red and blue woollen yarn sitting on a wooden table.gladiator (n10131815)       error: semanticsnoc: a man wearing a gladiator wearing a gladiator hat.taper (n13902793)           error: countingnoc: a group of three taper sitting on a table.trifle (n07613480)             error: repetitionnoc: a trifle cake with trifle cake on top of a trifle cake. lory (n01820348)              error: recognitionnoc: a bird sitting on a branch with a colorful bird           sitting on it.figure 6. descriptions produced by noc on a variety of objects, including    caddie   ,    saucepan   , and       ounder   . (right) noc makes
errors and (top right) fails to describe the new object (   sitar   ). more categories of images and objects are in the supplement.

ingfully in the description. the options provided are:
(i) sentence 1 incorporates the word better, (ii) sen-
tence 2 incorporates the word better, (iii) both sen-
tences incorporate the word equally well, or (iv) nei-
ther of them do well.

image description: we also ask humans to pick which of

the two sentences describes the image better.

word incorporation
intersection

objects subset     union
43.78
noc is better
25.74
dcc is better
6.10
both equally good
neither is good
24.37

34.61
34.12
9.35
21.91

image description
union
intersection
59.84
40.16

51.04
48.96

-
-

this allows us to compare both how well a model incorpo-
rates the novel object label in the sentence, as well as how
appropriate the description is to the image. the results are
presented in table 6. on the subset of images correspond-
ing to objects that both models can describe (intersection),
noc and dcc appear evenly matched, with noc only hav-
ing a slight edge. however, looking at all object categories
(union), noc is able to both incorporate the object label in
the sentence, and describe the image better than dcc.
7. conclusion

we present an end-to-end trainable architecture that in-
corporates auxiliary training objectives and distributional
semantics to generate descriptions for object classes unseen
in paired image-caption data. notably, noc   s architecture
and training strategy enables the visual recognition network
to retain its ability to recognize several hundred categories
of objects even as it learns to generate captions on a differ-
ent set of images and objects. we demonstrate our model   s
captioning capabilities on a held-out set of mscoco ob-
jects as well as several hundred id163 objects. both
human evaluations and quantitative assessments show that
our model is able to describe many more novel objects com-

table 6. id163: human judgements comparing our noc
model with dcc [7] on the ability to meaningfully incorporate the
novel object in the description (word incorporation) and describe
the image.    union    and    intersection    refer to the subset of objects
where atleast one model, and both models are able to incorporate
the object name in the description. all values in %.

pared to previous work. noc has a 10% higher f1 on un-
seen coco objects and 20% higher f1 on id163 ob-
jects compared to previous work, while also maintaining or
improving descriptive quality. we also present an analysis
of the contributions from different network modules, train-
ing objectives, and data sources. additionally, our model
directly extends to generate captions for id163 objects
mentioned rarely in the image-caption corpora. code is
available at: https://vsubhashini.github.io/noc.html
acknowledgements

we thank anonymous reviewers and saurabh gupta for
helpful suggestions. venugopalan is supported by a ut
scholarship, and hendricks was supported by a huawei fel-
lowship. darrell was supported in part by darpa; afrl;
dod muri award n000141110688; nsf awards iis-
1212798, iis-1427425, and iis-1536003, and the berke-

tennis player preparing to hit the ball with a racket. a white and red cockatoo standing in a field.a woodpecker sitting on a tree branch in the woods.a otter is sitting on a rock in the sun.a man holding a baseball bat standing in front of a buildinga cat is laying inside of a small white aardvark.a barracuda on a blue ocean with a barracuda. a man in a red and white shirt and a red and white octopus.a red trolley train sits on the tracks near a buildinga close up of a plate of food with a spatula.rare wordserrors (id163)novel objects (id163 images)novel objects (coco)a bus driving down a busy street with people standing around.a cat sitting on a suitcase next to a bag.a man is standing on a field with a caddie.a woman is holding a large megaphone in her hand.a orca is riding a small wave in the water.a table with a plate of sashimi and vegetables.a saucepan full of soup and a pot on a stove.a large flounder is resting on a rockley arti   cial intelligence research lab. mooney and
saenko are supported in part by darpa under afrl grant
fa8750-13-2-0026 and a google grant.

supplement

this supplement presents further qualitative results of
our novel object captioner (noc) model on id163 im-
ages (in sec. a), details pertaining to the quantitative results
on coco held-out objects (in sec. b), as well as the in-
terface used by mechanical turk workers comparing noc
with prior work (in sec. c).

a. id163 qualitative examples

we present additional examples of the noc model   s de-
scriptions on id163 images. we    rst present some ex-
amples where the model is able to generate descriptions of
an object in different contexts. then we present several ex-
amples to demonstrate the diversity of objects that noc can
describe. we then present examples where the model gen-
erates erroneous descriptions and categorize these errors.
a.1. context

fig. 7 shows images of eight objects, each in two differ-
ent settings from id163. images show objects in differ-
ent backgrounds (snowbird on a tree branch and on a rock,
hyena on a dirt path and near a building); actions (cari-
bou sitting vs lying down); and being acted upon differently
(flounder resting and a person holding the    sh, and lychees
in a bowl vs being held by a person). noc is able to capture
the context information correctly while describing the novel
objects (eartherware, caribou, warship, snowbird,    ounder,
lychee, verandah, and hyena).
a.2. object diversity

fig. 8 and fig. 9 present descriptions generated by noc
on a variety of object categories such as birds, animals, veg-
etable/fruits, food items, household objects, kitchen uten-
sils, items of clothing, musical instruments, indoor and out-
door scenes among others. while almost all novel words
(nouns in id163) correspond to objects, noc learns to
use some of them more appropriately as adjectives (   chif-
fon    dress in fig. 8,    brownstone    building and    tweed   
jacket in fig. 9 as well as    woollen    yarn in fig. 4.

comparison with prior work. additionally, for compar-
ison with the dcc model from [7], fig. 9 presents images
of objects that both models can describe, and captions gen-
erated by both dcc and noc.
a.3. categorizing errors

fig. 10 presents some of the errors that our model makes
when captioning id163 images. while noc improves

upon existing methods to describe a variety of object cate-
gories, it still makes a lot of errors. the most common error
is when it simply fails to recognize the object in the im-
age (e.g. image with    python   ) or describes it with a more
generic hyponym word (e.g. describing a bird species such
as    wren    or    warbler    in fig. 10 as just    bird   ). for objects
that the model is able to recognize, the most common er-
rors are when the model tends to repeat words or phrases
(e.g. descriptions of images with    balaclava   ,    mousse    and
   cashew   ), or just hallucinate other objects in the context
that may not be present in the image (e.g.
images with
   butte   ,    caldera   ,    lama   ,    timber   ). sometimes, the model
does get confused between images of other similar looking
objects (e.g.
it confuses    levee    with    train   ). apart from
these the model does make mistakes when identifying gen-
der of people (e.g.    gymnast   ), or just fails to create a co-
herent correct description even when it identi   es the object
and the context (e.g. images of    sunglass    and    cougar   ).

relevant but minor errors. fig. 11 presents more ex-
amples where noc generates very relevant descriptions but
makes some minor errors with respect to counting (e.g. im-
ages of    vulture    and    aardvark   ), age (e.g.
refers to boy
wearing    snorkel    as    man   ), confusing the main object cate-
gory (e.g.    macaque    with    bear    and person as    teddy bear   )
or makes minor word repetitions, and grammatical errors.

b. mscoco quantitative results

we present detailed quantitative results comparing dcc

and noc on the 8 held-out objects.
b.1. f1 and meteor

while table. 1 presents the f1 scores comparing the
dcc model [7] and our noc model for each of the eight
held-out objects in the test split, table. 7 supplements this
by also providing the individual meteor scores for the sen-
tences generated by the two models on these eight objects.
in case of noc, we sampled sentences (25) and picked
one with lowest log id203. using, id125 with
a beam width of 1 produces sentences with meteor score
20.69 and f1 of 50.51. in tables 2 and 3, all lines except
the last line corresponding to noc use id125 with a
beam-width of 1.
b.2. word-embedding for dcc and noc

one aspect of difference between noc and dcc is
that noc uses glove embeddings in it   s language model
whereas dcc uses id97 embeddings to select similar
objects for transfer. in order to make a fair comparison of
dcc with noc, it is also important to consider the set-
ting where both models use the same word-embedding. we
modify the transfer approach in dcc and replace id97

metric

f1

meteor

model
dcc
noc (ours)
dcc
noc (ours)

bottle
4.63
17.78
18.1
21.2

bus
29.79
68.79
21.6
20.4

couch microwave
45.87
25.55
23.1
21.4

28.09
24.72
22.1
21.5

pizza
64.59
69.33
22.2
21.8

racket
52.24
55.31
20.3
24.6

suitcase
13.16
39.86
18.3
18.0

zebra
79.88
89.02
22.3
21.8

avg.
39.78
48.79
21.00
21.32

table 7. mscoco captioning: f1 and meteor scores (in %) of noc (our model) and dcc [7] on the held-out objects not seen jointly
during image-caption training, along with the average scores of the generated captions across images containing these objects.

with glove embeddings. from table. 8 we note that the
difference in dcc is not signi   cant. thus, the embeddings
themselves do not play as signi   cant a role as the joint train-
ing approach.

model
dcc with id97
dcc with glove
noc (ours, uses glove)

f1 (%) meteor (%)
39.78
38.04
48.79

21.00
20.26
21.32

table 8. dcc and noc both using glove on mscoco dataset.

d. future directions

one interesting future direction would be to create a
model that can learn on new image-caption data after it has
already been trained. this would be akin to [16], where af-
ter an initial noc model has already been trained we might
want to add more objects to the vocabulary, and train it on
few image-caption pairs. the key novelty would be to im-
prove the captioning model by re-training only on the new
data instead of training on all the data from scratch.

b.3. joint training with auxiliary objectives

when performing joint

training and considering the
overall optimization objective as the sum of the image-
speci   c loss, the text-speci   c loss and image-caption loss,
we can de   ne the objective more generally as:
l = lcm +   lim +   llm

(6)

where    and    are hyper-parameters which determine the
weighting between different losses. in our experiments set-
ting    = 1 and    = 1 provided the best performance on
the validation set. other values of (  ,   )     {(1, 2), (2, 1)}
resulted in lower f1 and meteor scores.

c. mechanical turk interface

fig. 12 presents the interface used by mechanical turk
workers when comparing sentences generated by our model
and previous work. the workers are provided with the im-
age, the novel object category (word as well as meaning)
that is present in the image, and two sentences (one each
from our model and previous work). the sentence gener-
ated by the noc model is randomly chosen to be either
sentence 1 or sentence 2 for each image (with the other
sentence corresponding to the caption generated by previ-
ous work [7]). three workers look at each image and the
corresponding descriptions. the workers are asked to judge
the captions based on how well it incorporates the novel
object category in the description, and which sentence de-
scribes the image better.

figure 7. examples showing descriptions generated by noc for id163 images of eight objects, each in two different contexts. noc is
often able to generate descriptions incorporating both the novel object name as well as the background context correctly.

a couple of earthenware sitting on top of a wooden table.earthenwarea earthenware sitting on a table with a plate of food.cariboua caribou that is standing in the grass.a caribou that is laying in the grass.warshipa large warship is on the water.a group of people standing around a large white warship.snowbirda snowbird bird perched on a branch of a tree.a snowbird bird sitting on a rock in the middle of a small tree.floundera large flounder is resting on a rocka man is holding a large flounder on a beach.lycheea bowl filled with lots of lychee and lychee.a man holding a lychee and lychee tree.verandaha large building with a verandah and tropical plants in it.a table with a verandah area and chairs.hyenaa hyena dog walking across a dirt road.a hyena standing on a dirt area next to a building.figure 8. examples of sentences generated by our noc model on id163 images of objects belonging to a diverse variety of categories
including food, instruments, outdoor scenes, household equipment, and vehicles. the novel objects are in bold. the last row highlights
common errors where the model tends to repeat itself or hallucinate objects not present in the image.

birdsa osprey flying over a large grassy area.outdoorsa large glacier with a mountain in the background.a group of people are sitting in a baobab.a small pheasant is standing in a field.a table with a cauldron in the dark.a man is standing on a beach holding a snapper.a humpback is flying over a large body of water.a woman is posing for a picture with a chiffon dress.water animalsmiscfoodkitchena close up of a plate of food with a scone.a large colander with a piece of food on it.a dumpling sitting on top of a wooden tablea saucepan and a pot of food on a stove top.vehiclesinstrumentsland animalshouseholda large metal candelabra next to a wall.a black and white photo of a corkscrew and a corkscrew.a snowplow truck driving down a snowy road.a group of people standing around a large white warship.a okapi is in the grass with a okapi.a small brown and white jackal is standing in a field.a man holding a banjo in a park.a large chime hanging on a metal poleerrorsa chainsaw is sitting on a chainsaw near a chainsaw.a man is sitting on a bike in front of a waggon.a volcano view of a volcano in the sun.a trampoline with a trampoline in the middle of it.figure 9. examples comparing sentences generated by dcc [7] and our noc model on id163 images of object categories that both
models can describe including food, animals, vegetables/fruits, indoor and outdoor scenes, and clothing. the novel objects are in bold.

birdsnoc: a shorebird bird standing on a water pond.outdoorsnoc: a volcano view of a mountain with clouds in the background.noc: a brownstone building with a clock on the side of it.noc: a grouse is standing on a dirt ground.noc: a crocodile floats through the water edge of a body of water.noc: a swordfish sitting on a wooden bench in a city.water animalsfoodnoc: a plate of food with hollandaise sauce and vegetables.noc: a close up of a plate of food with falafel.scenesnoc: a woman standing in front of a cabaret with a large discotheque.noc: a parlour room with a table and chairs.noc: a small white and grey tarantula is sitting on a hill.noc: a dingo dog is laying in the grass.animalsdcc: a plate of food with a fork and a hollandaise.dcc: a plate of food with a fork and a falafel.dcc: a woman standing in a room with a red and white background.dcc: a large room with a large window and a table.dcc: a large crocodile in a body of water.dcc: a man is sitting on a bench in the water.dcc: a black and white photo of a person on a white surface.dcc: a dog laying on a wooden bench next to a fence.dcc: a shorebird bird standing in the water near a body of water.dcc: a man is sitting on a bench in the middle of a large volcano.dcc: a red and white brownstone in a city street.dcc: a grouse is standing in the middle of a small pond.vegetablesnoc: a tree with a bunch of papaya hanging on it.waternoc: a steamship boat is sailing in the water.noc: a man standing on a boat holding a snapper in his hand.noc: a bunch of yam are laying on a table.noc: a woman in corset posing for a picture.noc: a woman standing next to a woman holding a boa.clothingmisc.noc: a abacus sitting on a wooden shelf with a abacus.noc: a young child is holding a drumstick in a kitchen.misc.noc: a copier desk with a copier machine on top of it.noc: a spectrometer is sitting in a spectrometer room.noc: a man wearing a hat and  wearing topcoat.noc: a man wearing a suit and tie with a tweed jacket.clothingdcc: a abacus with a lot of different types of food.dcc: a little girl is drumstick with a toothbrush in the background.dcc: a laptop copier sitting on top of a table.dcc: a white and white photo of a white and black photo of a white.dcc: a woman holding a red and white corset on a woman.dcc: a man holding a pink umbrella in a pink boa.dcc: a man wearing a suit and tie in a suit.dcc: a man wearing a suit and tie in a suit.dcc: a papaya tree with a papaya tree.dcc: a boat is docked in the water.dcc: a man standing on a boat with a man in the background.dcc: a person holding a knife and a knife.figure 10. examples of images where the model makes errors when generating descriptions. the novel object is in bold and the errors
are underlined. noc often tends to repeat words in its description, or hallucinate objects not present in the image. the model sometime
misidenti   es gender, misrepresents the semantics of the novel object, or just makes grammatical errors when composing the sentence.

figure 11. some examples where noc makes minor errors when describing the image. the novel object is in bold and the word or
segment corresponding to the error is underlined. counting, repetitions, confusing object categories (e.g.    macaque   ,    bear   ), grammatical
errors, and hallucinating objects that are absent are some common errors that the model makes. however, the generated description is still
meaningful and relevant.

figure 12. interface used by mechanical turk workers when comparing captions/sentences generated by our noc model with previous
work (dcc [7]). the workers are asked to compare on both word incorporation i.e. how well each model incorporates the novel object in
the sentence, as well as image description i.e. which caption describes the image better.

from sentence descriptions of images.
10

in iccv, 2015. 2,

[17] t. mikolov, i. sutskever, k. chen, g. s. corrado, and
j. dean. distributed representations of words and phrases
and their compositionality. in nips, 2013. 4

[18] m. mitchell, j. dodge, a. goyal, k. yamaguchi, k. stratos,
x. han, a. mensch, a. c. berg, t. l. berg, and h. d. iii.
midge: generating image descriptions from id161
detections. in eacl, 2012. 2

[19] m. norouzi, t. mikolov, s. bengio, y. singer, j. shlens,
a. frome, g. s. corrado, and j. dean. zero-shot learning by
convex combination of semantic embeddings. arxiv preprint
arxiv:1312.5650, 2013. 2

[20] j. pennington, r. socher, and c. d. manning. glove: global
vectors for word representation. proceedings of the em-
piricial methods in natural language processing (emnlp
2014), 12:1532   1543, 2014. 2, 4

[21] o. russakovsky, j. deng, h. su, j. krause, s. satheesh,
s. ma, z. huang, a. karpathy, a. khosla, m. bernstein,
a. c. berg, and l. fei-fei. ilsvrc, 2014. 2, 4

[22] k. simonyan and a. zisserman.

very deep convolu-
tional networks for large-scale image recognition. corr,
abs/1409.1556, 2014. 1, 4

[23] r. socher, a. karpathy, q. v. le, c. d. manning, and a. y.
ng. grounded id152 for    nding and de-
scribing images with sentences. tacl, 2014. 2

[24] m. sundermeyer, r. schl  uter, and h. ney. lstm neural
networks for id38. in interspeech, 2012.
2

[25] s. venugopalan, l. a. hendricks, r. mooney,

and
k. saenko. improving lstm-based video description with
linguistic knowledge mined from text. in emnlp, 2016. 4
[26] o. vinyals, a. toshev, s. bengio, and d. erhan. show and
tell: a neural image caption generator. in cvpr, 2015. 1, 2
[27] y. yang, c. l. teo, h. daum  e iii, and y. aloimonos.
in

corpus-guided sentence generation of natural images.
emnlp, 2011. 2

references
[1] m. denkowski and a. lavie. meteor universal: language
speci   c translation evaluation for any target language.
in
proceedings of the ninth workshop on statistical machine
translation, 2014. 5

[2] j. donahue, l. a. hendricks, s. guadarrama, m. rohrbach,
s. venugopalan, k. saenko, and t. darrell. long-term recur-
rent convolutional networks for visual recognition and de-
scription. in cvpr, 2015. 1, 2, 5

[3] h. fang, s. gupta, f. n. iandola, r. srivastava, l. deng,
p. doll  ar, j. gao, x. he, m. mitchell, j. c. platt, c. l. zit-
nick, and g. zweig. from captions to visual concepts and
back. in cvpr, 2015. 1, 2

[4] a. frome, g. s. corrado, j. shlens, s. bengio, j. dean,
t. mikolov, et al. devise: a deep visual-semantic embed-
ding model. in advances in neural information processing
systems, pages 2121   2129, 2013. 2

[5] c. gulcehre, o. firat, k. xu, k. cho, l. barrault, h. lin,
f. bougares, h. schwenk, and y. bengio. on using mono-
lingual corpora in id4. arxiv preprint
arxiv:1503.03535, 2015. 2

[6] k. he, x. zhang, s. ren, and j. sun. deep residual learning

for image recognition. in cvpr, 2016. 1

[7] l. a. hendricks, s. venugopalan, m. rohrbach, r. mooney,
k. saenko, and t. darrell. deep compositional captioning:
describing novel object categories without paired training
data. in cvpr, 2016. 1, 2, 4, 5, 6, 7, 8, 9, 10, 13, 16

[8] a. karpathy and l. fei-fei. deep visual-semantic align-
in cvpr, 2015.

ments for generating image descriptions.
1, 2

[9] r. kiros, r. salakhutdinov, and r. zemel. multimodal neu-
in proceedings of the 31st interna-
ral language models.
tional conference on machine learning (icml-14), pages
595   603, 2014. 2

[10] r. kiros, r. salakhutdinov, and r. s. zemel. unifying
visual-semantic embeddings with multimodal neural lan-
guage models. tacl, 2015. 1, 2

[11] p. kuznetsova, v. ordonez, t. l. berg, u. c. hill, and
y. choi. treetalk: composition and compression of trees
for image descriptions. in tacl, 2014. 2

[12] a. lazaridou, e. bruni, and m. baroni. is this a wampimuk?
cross-modal mapping between id65 and
the visual world. in acl, 2014. 2

[13] t.-y. lin, m. maire, s. belongie, j. hays, p. perona, d. ra-
manan, p. doll  ar, and c. l. zitnick. microsoft coco: com-
mon objects in context. in eccv, 2014. 2, 4

[14] c. manning, m. surdeanu, j. bauer, j. finkel, s. j. bethard,
and d. mcclosky. the stanford corenlp natural language
in proceedings of 52nd annual meet-
processing toolkit.
ing of the association for computational linguistics: system
demonstrations, pages 55   60, 2014. 4

[15] j. mao, w. xu, y. yang, j. wang, z. huang, and a. yuille.
deep captioning with multimodal recurrent neural networks
(m-id56). in iclr, 2015. 1, 2, 4

[16] j. mao, w. xu, y. yang, j. wang, z. huang, and a. l. yuille.
learning like a child: fast novel visual concept learning

