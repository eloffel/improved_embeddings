generalizing to unseen entities and entity pairs with

row-less universal schema

patrick verga, arvind neelakantan, & andrew mccallum

college of information and computer sciences

university of massachusetts amherst

{pat, arvind, mccallum}@cs.umass.edu

7
1
0
2

 

n
a
j
 

9

 
 
]
l
c
.
s
c
[
 
 

2
v
4
0
8
5
0

.

6
0
6
1
:
v
i
x
r
a

abstract

universal schema predicts the types of enti-
ties and relations in a knowledge base (kb)
by jointly embedding the union of all avail-
able schema types   not only types from mul-
tiple structured databases (such as freebase
or wikipedia infoboxes), but also types ex-
pressed as textual patterns from raw text. this
prediction is typically modeled as a matrix
completion problem, with one type per col-
umn, and either one or two entities per row
(in the case of entity types or binary relation
types, respectively). factorizing this sparsely
observed matrix yields a learned vector em-
bedding for each row and each column. in this
paper we explore the problem of making pre-
dictions for entities or entity-pairs unseen at
training time (and hence without a pre-learned
row embedding). we propose an approach
having no per-row parameters at all; rather
we produce a row vector on the    y using a
learned aggregation function of the vectors of
the observed columns for that row. we exper-
iment with various aggregation functions, in-
cluding neural network id12. our
approach can be understood as a natural lan-
guage database, in that questions about kb
entities are answered by attending to textual
or database evidence. in experiments predict-
ing both relations and entity types, we demon-
strate that despite having an order of magni-
tude fewer parameters than traditional univer-
sal schema, we can match the accuracy of the
traditional model, and more importantly, we
can now make predictions about unseen rows
with nearly the same accuracy as rows avail-
able at training time.
introduction

1
automatic knowledge base construction (akbc) is the
task of building a structured knowledge base (kb) of
facts using raw text evidence, and often an initial seed
kb to be augmented (carlson et al., 2010; suchanek et
al., 2007; bollacker et al., 2008). kbs generally con-
tain entity type facts such as sundar pichai isa per-
son and relation facts such as ceo of(sundar pichai,

google). extracted facts about entities, and their types
and relations are useful for many downstream tasks
such as id53 (bordes et al., 2014) and
id29 (berant et al., 2013; kwiatkowski et
al., 2013).

an effective approach to akbc is universal schema,
which predicts the types of entities and relations in a
knowledge base (kb) by jointly embedding the union
of all available schema types   not only types from
multiple structured databases (such as freebase or
wikipedia infoboxes), but also types expressed as tex-
tual patterns from raw text. this prediction is typically
modeled as a matrix completion problem. in the stan-
dard formulation for id36 (riedel et al.,
2013), entity pairs and relations occupy the rows and
columns of the matrix respectively (figure 1a). analo-
gously in entity type prediction (yao et al., 2013), en-
tities and types occupy the rows and columns of the
matrix respectively (figure 1b). the row and column
entries are represented as learned vectors with compat-
ibility determined by a scoring function.

in its original form, universal schema can reason
only about row entries and column entries explicitly
seen during training. unseen rows and columns ob-
served at test time do not have a learned embedding.
this problem is referred to as the cold-start problem in
id126s (schein et al., 2002).

recently toutanova et al. (2015) and verga et al.
(2016) proposed    column-less    versions of universal
schema models that generalize to unseen column en-
tries. they learn compositional pattern encoders to pa-
rameterize the column matrix in place of individual col-
umn embeddings. however, these models still do not
generalize to unseen row entries.

in this work, we present a    row-less    extension of
universal schema that generalizes to unseen entities and
entity pairs. rather than representing each row entry
with an explicit dense vector, we encode each entity or
entity pair as aggregate functions over their observed
column entries. this is bene   cial because when new
entities are mentioned in text documents and subse-
quently added to the kb, we can directly reason on
the observed text evidence to infer new binary relations
and entity types for the new entities. this avoids the
cumbersome effort of re-training the whole model from
scratch to learn embeddings for the new entities.

to construct the row representation, we compare var-
ious aggregation functions in our experiments. we
consider query independent and dependent aggregation
functions. we    nd that query dependent attentional
models that selectively focus on relevant evidence out-
perform the query independent alternatives. the query
dependent attention mechanism also helps in provid-
ing a direct connection between the prediction and its
provenance. additionally, our models have a much
smaller memory footprint since they do not store ex-
plicit row representations.

it is important to note that our approach is different
from sentence level classi   ers that predict kb relations
and entity types using a single sentence as evidence.
first, we pool information from multiple pieces of ev-
idence coming from both text and annotated kb facts,
rather than considering a single sentence at test time.
second, our methods are not limited to a    xed schema
but instead predict a richer set of labels (kb types and
textual), enabling easier downstream processing closer
to natural language interaction with the kb. finally,
our model gains additional training signal from multi-
task learning of textual and kb types. since universal
schema leverages large amounts of unlabeled text we
desire the bene   ts of entity pair modeling, and row-less
universal schema facilitates learning entity pair repre-
sentations without the drawbacks of the traditional one-
embedding-per-pair approach.

the majority of current embedding methods for kb
entity type prediction operate with explicit entity rep-
resentations (yao et al., 2013; neelakantan and chang,
2015) and hence, cannot generalize to unseen entities.
in id36, entity-level models (nickel et
al., 2011; garc    a-dur  an et al., 2015; yang et al., 2015;
bordes et al., 2013; wang et al., 2014; lin et al., 2015;
socher et al., 2013) can handle unseen entity pairs at
test time. these models learn representations for the
entities instead of entity pairs. hence, these methods
still cannot generalize to predict relations between an
entity pair if even one of the entities is unseen. more-
over, toutanova et al. (2015) and riedel et al. (2013)
observe that the entity pair model outperforms entity
models in cases where the entity pair was seen at train-
ing time.

most similar to this work, neelakantan et al. (2015)
classify kb relations by    nding the maximum scoring
path between two entities. this model is also    row-
less    and does not directly model entities or entity pairs.
there are several important differences in this work.
neelakantan et al. (2015) learn per-relation classi   ers
to predict only a small set of kb relations, while we in-
stead predict all relations, including textual relations.
we also explore aggregation functions that pool ev-
idence from multiple paths while neelakantan et al.
(2015) only chose the maximum scoring path. ad-
ditionally, we demonstrate that our models can per-
form on par with those with explicit row representa-
tions while neelakantan et al. (2015) did not perform

this comparison.

in this paper we investigate universal schema mod-
els without explicit row representations on two tasks:
entity type prediction and id36. we use
entity type and relation facts from freebase (bollacker
et al., 2008) augmented with textual relations and types
from clueweb text (orr et al., 2013; gabrilovich et
al., 2013). we explore multiple aggregation functions
and    nd that an attention-based aggregation function
outperforms several simpler functions and matches a
model using explicit row representations with an order
of magnitude fewer parameters. more importantly, we
then demonstrate that our    row-less    models accurately
predict relations on unseen entity pairs and types on
unseen entities.

2 background: universal schema
universal schema (riedel et al., 2013; yao et al., 2013)
id36 and entity type prediction is typi-
cally modeled as a matrix completion task. in relation
extraction, entity pairs and relations occupy the rows
and columns of the matrix (figure 1-a), while in en-
tity type prediction, entities and types occupy the rows
and columns of the matrix (figure 1-b). during train-
ing, we observe some positive entries in the matrix and
at test time, we predict the missing cells in the matrix.
this is achieved by decomposing the observed matrix
into two low-rank matrices resulting in embeddings for
each column entry and each row entry. test time pre-
diction is performed using the learned low-rank column
and row representations.
let t be the training set consisting of examples of
the form (r, c), where row r     u and column c     v ,
denote an entity pair and relation type in the relation
extraction task, or entity and entity type in the entity
type prediction task. let v(r)     rd and v(c)     rd be
the vector representations or embeddings of row r    
u and column c     v that are learned during training.
given a positive example, (r, c)     t in training, the
id203 of observing the fact is given by,

p (yr,c = 1) =   (v(r).v(c))

(1)

where yr,c is a binary random variable that is equal to
1 when (r, c) is a fact and 0 otherwise, and    is the
sigmoid function. the embeddings are learned using
bayesian personalized ranking (bpr) (rendle et al.,
2009) in which the id203 of the observed triples
are ranked above unobserved triples.

3 model
in this section, we describe the model, discuss the dif-
ferent aggregation functions and give details on the
training objective.

   row-less    universal schema

3.1
while column-less universal schema addresses reason-
ing over arbitrary textual patterns, it is still limited to

entity for type prediction and an entity pair for rela-
tion extraction while a column contains a relation type
for id36 and an entity type for type pre-
diction. a learned row embedding can be seen as a
summarization of all columns observed with that par-
ticular row. instead of modeling this summarization as
a single embedding, we reconstruct a row representa-
tion from an aggregate of its column embeddings, es-
sentially learning a mixture model rather than a single
centroid.

figure 2: row-less universal schema for relation ex-
traction encodes an entity pair as an aggregation of its
observed relation types.

3.2 aggregation functions
in this work we examine four aggregation functions to
construct the representations for the row. let v(.) de-
note a function that returns the vector representation
for rows and columns. to model the id203 be-
  v (r)
tween row r and column c, we consider the set
which contains the set of column entries that are ob-
served with row r at training time, i.e.,
     c       v (r), (r,   c)     t

the    rst two aggregation functions create a single
representation for each row independent of the query.
mean pool creates a single centroid for the row by av-
eraging all of its column vectors,

v(r) =(cid:80)

  c      v (r) v(  c)

while this formulation intuitively makes sense as an
approximation for the explicit row representation, aver-
aging large numbers of embeddings can lead to a noisy
representation.

max pool also creates a single representation for the
row by taking a dimension-wise max over the observed
column vectors:

v(r)i = max  c      v (r) v(  c)i,   i     1, 2, . . . , d

figure 1: universal schema matrix. a: relation extrac-
tion. relation types are represented as columns and
entity pairs as rows of a matrix. both kb relation types
and textual patterns from raw text are jointly embed-
ded in the same space. b: entity type prediction. entity
types are represented as columns and entities as rows
of a matrix.

reasoning over row entries seen at training time. verga
et al. (2016) use column-less universal schema for rela-
tion extraction. they address the problem of unseen
row entries by using universal schema as a sentence
classi   er     directly comparing a textual relation to a
kb relation to perform id36. however,
this approach is unsatisfactory for two reasons. the
   rst is that this creates an inconsistency between train-
ing and testing. the model is trained to predict com-
patibility between rows and columns, but at test time it
predicts compatibility between relations directly. sec-
ond, it considers only a single piece of evidence in
making its prediction.

we address both of these concerns in our    row-less   
universal schema. rather than explicitly encoding each
row, we encode the row as a learned aggregation over
their observed columns (figure 2). a row contains an

per:spouse...per:born_inarg1    s wife arg2...arg1 was born in arg 2englishbarack obama / michelle obamamar  a m  nera /   juan m santosbarack obama / hawaiimar  a m  nera /            colombiabernie sanders / jane o'meara......11.921...1...per/actor...loc/countrylawyer...companybarack obama ruth b. ginsburgargentina1.891...1...new yorkbrad pittibm...1per:spouseper:co-workerarg1    s wife arg2arg1 co-founded the     foundation with arg2arg1 married arg 2...bill gates /melinda gates1...arg1    s wife arg2(bill gates / melinda gates)1arg1 co-founded the foundation with arg2arg1 married arg 2aggregation function............1where ai denotes the ith dimension of vector a. both
mean pool and max pool are query-independent and
form the same representation for the row regardless of
the query relation.

we also examine two query-speci   c aggregation
functions. these models are more expressive than a
single vector forced to to act as a centroid to all possible
columns observed with that particular row. for exam-
ple, the entity pair bill and melinda gates could hold
the relation    per:spouse    or    per:co-worker   . a query-
speci   c aggregation mechanism can produce separate
representations for this entity pair dependent on the
query.

the max relation aggregation function represents
the row as its most similar column to the query vector
of interest. given a query relation c,

cmax = argmax  c      v (r)v(  c).v(c)

v(r) = v(cmax)

a similar strategy has been successfully applied in pre-
vious work (weston et al., 2013; neelakantan et al.,
2014; neelakantan et al., 2015) for different tasks. this
model has the advantage of creating a query-speci   c
entity pair representation, but is more susceptible to
noisy training data as a single incorrect piece of evi-
dence could be used to form a prediction.

finally, we look at an attention aggregation func-
tion over columns (figure 3) which is similar to a
single-layer memory network (sukhbaatar et al., 2015).
the soft attention mechanism has been used to selec-
tively focus on relevant parts in many different mod-
els (bahdanau et al., 2014; graves et al., 2014; nee-
lakantan et al., 2016).

in this model the query is scored with an input rep-
resentation of each column embedding followed by a
softmax, giving a weighting over each relation type.
this output is then used to get a weighted sum over
a set of output representations for each column result-
ing in a query-speci   c vector representation of the row.
given a query relation c,

score  c = v(c).v(  c),     c       v (r)
(cid:80)
v(r) =(cid:80)
  c      v (r) exp(score  c) ,     c       v (r)

  c      v (r) p  c    v(  c)

exp(score  c)

p  c =

the model pools relevant information over the entire
set of observed columns and selects the most salient
aspects to the query.

model
entity embeddings
attention
mean pool/max pool/max relation

parameters

3.7 e6
3.1 e5
1.5 e5

table 1: number of parameters for the different models
on the entity type dataset.

3.3 training
the vector representation of the rows and the columns
are the parameters of the model. riedel et al. (2013)

use bayesian personalized ranking (bpr) (rendle et
al., 2009) to train their universal schema models. bpr
ranks the id203 of observed triples above unob-
served triples rather than explicitly modeling unob-
served edges as negative. each training example is an
(entity pair, relation type) or (entity, entity type) pair
observed in the training text corpora or kb.

rather than bpr, toutanova et al. (2015) use 200
negative samples to approximate the negative log like-
lihood1. in our experiments, we use the sampled ap-
proximate negative log likelihood which outperformed
bpr in early experiments.

each example in the training procedure consists of
a row-column pair observed in the training set. for a
positive example (r, c)     t , we construct the set
  v (r)
containing all the other column entries apart from c that
are observed with row r.

to make training faster and more robust, we add
   pattern dropout    for entity pairs with many mentions.
  v (r) to be m randomly sampled mentions for
we set
entity pairs with greater than m total mentions. in our
experiments we set m = 10 and at test time we use all
  v (r) to obtain the aggregated
mentions. we then use
row representation as discussed above.

we randomly sample 200 columns unobserved with
row r to act as the negative samples. all models are
implemented in torch2 and are trained using adam
(kingma and ba, 2015) with default momentum related
hyperparameters.

4 related work
id36 for kb completion has a long his-
tory. mintz et al. (2009) train per relation linear classi-
   ers using features derived from the sentences in which
the entity pair is mentioned. most of the embedding-
based methods learn representations for entities (nickel
et al., 2011; socher et al., 2013; bordes et al., 2013)
whereas riedel et al. (2013) use entity pair representa-
tions.

   column-less    versions of universal schema have
been proposed (toutanova et al., 2015; verga et al.,
2016). these models can generalize to column entries
unseen at training by learning compositional pattern
encoders to parameterize the column matrix in place
of embeddings. most of these models do not general-
ize to unseen entity pairs and none of them generalize
to unseen entities. recently, neelakantan et al. (2015)
introduced a multi-hop id36 model that is
   row-less    having no explicit parameters for entity pairs
and entities.

entity type prediction at the individual sentence level
has been studied extensively (pantel et al., 2012; ling

1many past papers restrict negative samples to be of the
same type as the positive example. we simply sample uni-
formly from the entire set of row entries

2data and code available at https://github.com/
patverga/torch-relation-extraction/tree/
rowless-updates

figure 3: example attention model in a row-less universal schema relation extractor. in the attention model, we
compute the dot product between the representation of the query relation and the representation of an entity pair   s
observed relation type followed by a softmax, giving a weighting over the observed relation types. this output
is then used to get a weighted sum over the set of representations of the observed relation types. the result is a
query-speci   c vector representation of the entity pair. the max relation model takes the most similar observed
relation   s representation.

and weld, 2012; shimaoka et al., 2016). more recently,
embedding-based methods for knowledge base entity
type prediction have been proposed (yao et al., 2013;
neelakantan and chang, 2015). these methods have
explicit entity representations, hence cannot generalize
to unseen entities.

the task of generalizing to unseen row and column
entries is referred to as the cold-start problem in recom-
mendation systems. methods proposed to tackle this
problem commonly use user and item content and at-
tributes (schein et al., 2002; park and chu, 2009).

multi-instance learning can be viewed as the rela-
tion classi   er analogy of rowless universal schema.
riedel et al. (2010) used a relaxation of distant super-
vision training where all sentences for an entity pair
(bag) are considered jointly and only the most relevant
sentence is treated as the single training example for
the bag   s label. surdeanu et al. (2012) extended this
idea with multi-instance multi-label learning (miml)
where each entity pair / bag can hold multiple relations
/ labels. recently lin et al. (2016) used a selective at-
tention over sentences in miml.

concurrent to our work, weissenborn (2016) pro-
poses a row-less method for id36 consid-
ering both a uniform and weighted average aggregation
function over columns. however, weissenborn (2016)
did not experiment with max and max-pool aggregation
functions or evaluate on entity-type prediction. they
also did not combine the rowless model with an lstm
column-less parameterization and did not compare to a
model with explicit entity-pair representations.

5 experimental results
in this section, we compare our models that have ag-
gregate row representations with models that have ex-
plicit row representations on entity type prediction and
id36 tasks. finally, we perform experi-
ments on a column-less universal schema model. ta-
ble 1 shows that the row-less models require far fewer
parameters since they do not explicitly store the row
representations.

5.1 entity type prediction
we    rst evaluate our models on an entity type predic-
tion task. we collect all entities along with their types
from a dump of freebase3. we then    lter all enti-
ties with less than    ve freebase types leaving a set of
844780 (entity, type) pairs. additionally, we collect
712072 textual (entity, type) pairs from clueweb. the
textual types are the 5000 most common appositives
extracted from sentences mentioning entities. this re-
sults in 140513 unique entities, 1120 freebase types,
and 5000 free text types.
all embeddings are 25 dimensions, randomly initial-
ized. we tune learning rates from {.01, .001}, (cid:96)2 from
{1e-8, 0}, batch size {512, 1024, 2048} and negative
samples from {2, 200}.

for evaluation, we split the freebase (entity, type)
pairs into 60% train, 20% validation, and 20% test. we
randomly generate 100 negative (entity, type) pairs for
each positive pair in our test set by selecting random
entity and type combinations. we    lter out false nega-
tives that were observed true (entity, type) pairs in our
complete data set. each model produces a score for
each positive and negative (entity, type) pair where the

3downloaded march 1, 2015.

(bill gates/melinda gates)output encoderper:spouseattention encoderinner product + softmaxweighted avg- arg1 married arg2- arg1    s wife arg2- arg1 co-founded the foundation  with arg 2{}inputoutputquery encodermodel
entity embeddings
mean pool
max pool
attention
max relation

(a)

model
entity embeddings
mean columns
max column
mean pool
max pool
attention
max relation

(b)

map
54.81
39.47
32.59
55.66
55.37

map
3.14
34.77
43.20
35.53
30.98
54.52
54.72

table 2: entity type prediction. entity embeddings
refers to the model with explicit row representations.
mean columns and max column are equivalent to
mean pool and max relation respectively (section 3.2)
but use the column embeddings learned during training
of the entity embeddings model. b: positive entities
are unseen at train time.

type is the query. we then rank these predictions, cal-
culate average precision for each of the types in our test
set, and then use those scores to calculate mean average
precision (map).

table 2a shows the results of this experiment. we
can see that the query dependent aggregation func-
tions (attention and max relation) performs better
than the query independent functions (mean pool and
max pool). the performance of models with query de-
pendent aggregation functions which have far fewer pa-
rameters match the performance of the model with ex-
plicit entity representations.

we additionally evaluate our model   s ability to pre-
dict types for entities unseen during training. for this
experiment, we randomly select 14000 entities and take
all (entity, type) pairs containing those entities. we re-
move these pairs from our training set and use them
as positive samples in our test set. we then select 100
negatives (entity, type) pairs per positive as above.

table 2b shows the results of the experiment with
unseen entities. there is very little performance drop
for models trained with query dependent aggregation
functions. the performance of the model with explicit
entity representations is close to random.
5.1.1 qualitative results
a query speci   c aggregation function is able to pick
out relevant columns to form a prediction. this is par-
ticularly important for rows that are not described eas-
ily by a single centroid such as an entity with several
very different careers or an entity pair with multiple

highly varied relations. for example, in the    rst row
in table 3, for the query /baseball/baseball player
the model needs to correctly focus on aspects like
/sports/pro athlete and ignore evidence information
like /tv/tv actor. a model that creates a single query-
independent centroid will be forced to try and merge
these disparate pieces of information together.

5.2 id36
we evaluate our models on a id36 task
using the fb15k-237 dataset from toutanova et al.
(2015). the data is composed of a small set of 237
freebase relations and approximately 4 million textual
patterns from clueweb with entities linked to freebase
(gabrilovich et al., 2013).
in past studies, for each
(subject, relation, object) test triple, negative examples
are generated by replacing the object with all other enti-
ties,    ltering out triples that are positive in the data set.
the positive triple is then ranked among the negatives.
in our experiments we limit the possible generated neg-
atives to those entity pairs that have textual mentions in
our training set. this way we can evaluate how well
the model classi   es textual mentions as freebase rela-
tions. we also    lter textual patterns with length greater
than 35. our    ltered data set contains 2740237 relation
types, 2014429 entity pairs, and 176476 tokens. we re-
port the percentage of positive triples ranked in the top
10 amongst their negatives as well as the mrr scaled
by 100.

models are tuned to maximize mean reciprocal rank
(mrr) on the validation set with early stopping. the
entity pair model used a batch size 1024, (cid:96)2 = 1e-
8,   = 1e-4, and learning rate 0.01. the aggregation
models all used batch size 4096, (cid:96)2 = 0,   = 1e-8,
and learning rate 0.01. each use 200 negative sam-
ples except for max pool which performed better with
two negative samples. the column vectors are initial-
ized with the columns learned by the entity pair model.
randomly initializing the query encoders and tying the
output and attention encoders performed better and all
results use this method. all models are trained with
embedding dimension 25.

our results are shown in table 4a. we can see that
the models with query speci   c aggregation functions
give the same results as models with explicit entity pair
representations. the max relation model performs
competitively with the attention model which is not
entirely surprising as it is a simpli   ed version of the
attention model. further, the attention model reduces
to the max relation model for entity pairs with only a
single observed relation type. in our data, 64.8% of en-
tity pairs have only a single observed relation type and
80.9% have 1 or 2 observed relation types.

we also explore the models    abilities to predict on
unseen entity pairs (table 4b). we remove all training
examples that contain a positive entity pair in either our
validation or test set. we use the same validation and
test set as in table 4a. the entity pair model predicts

query
/baseball/baseball player

/architecture/engineer
/baseball/baseball player
/computer/computer scientist
/business/board member

/education/academic

observed columns
/sports/pro athlete, /sports/sports award winner, /tv/tv actor, /people/measured person,
/award/award winner, /people/person
engineer, /book/author, /projects/project focus , /people/person , sir
baseman, /sports/pro athlete, /people/measured person, /people/person, dodgers, coach
/education/academic, /music/group member, /music/artist, /people/person
/organization/organization founder, /award/award winner, /computer/computer scientist,
/people/person, president, scientist
/astronomy/astronomer, /book/author

table 3: each row corresponds to a true query entity type (left column) and the observed entity types (right column)
for a particular entity. the maximum scoring observed entity type for each query entity type is indicated in bold.
the other types are in no particular order. it can be seen that the maximum scoring entity types are interpretable.

model
entity-pair embeddings
mean pool
max pool
attention
max relation

mrr hits@10
31.85
25.89
29.61
31.92
31.71

51.72
45.94
49.93
51.67
51.94

(a)

model
entity-pair embeddings
mean pool
max pool
attention
max relation

mrr hits@10
5.23
18.10
20.80
29.75
28.46

11.94
35.76
40.25
49.69
48.15

(b)

table 4: the percentage of positive triples ranked in
the top 10 amongst their negatives as well as the mean
reciprocal rank (mrr) scaled by 100 on a subset of
the fb15k-237 dataset. all positive entity pairs in the
evaluation set are unseen at train time. entity-pair em-
beddings refers to the model with explicit row repre-
sentations. b: predicting entity pairs that are not seen
at train time.

random relations as it is unable to make predictions on
unseen entity pairs. the query-independent aggrega-
tion functions, mean pool and max pool, perform bet-
ter than models with explicit entity pair representations.
again, query speci   c aggregation functions get the best
results, with the attention model performing slightly
better than the max relation model.

the two experiments indicate that we can train rela-
tion extraction models without explicit entity pair rep-
resentations that perform as well as models with ex-
plicit representations. we also    nd that models with
query speci   c aggregation functions accurately predict
relations for unseen entity pairs.

   column-less    universal schema

5.3
the original universal schema approach has two main
drawbacks: similar textual patterns do not share statis-
tics, and the model is unable to make predictions about

entities and textual patterns not explicitly seen at train
time.

recently,

   column-less    versions of universal
schema to address some of these issues (toutanova et
al., 2015; verga et al., 2016). these models learn com-
positional pattern encoders to parameterize the column
matrix in place of direct embeddings. compositional
universal schema facilitates more compact sharing of
statistics by composing similar patterns from the same
sequence of id27s     the text patterns    lives
in the city    and    lives in the city of    no longer exist as
distinct atomic units. more importantly, compositional
universal schema can thus generalize to all possible
textual patterns, facilitating reasoning over arbitrary
text at test time.

the column-less universal schema model general-
izes to all possible input textual relations and the row-
less model generalizes to all entities and entity pairs,
whether seen at train time or not. we can combine these
two approaches together to make an universal schema
model that generalizes to unseen rows and columns.

the parse path between the two entities in the sen-
tence is encoded with an lstm model. we use a single
layer model with 100 dimensional token embeddings
initialized randomly. to prevent exploding gradients,
we clip them to norm 10 while all the other hyperpa-
rameters are tuned the same way as before. we follow
the same evaluation protocol from 5.2.

the results of this experiment with observed rows
are shown in table 5a. while both the mrr and
hits@10 metrics increase for models with explicit row
representations, the row-less models show an improve-
ment only on the hits@10 metric. the mrr of the
query dependent row-less models is still competitive
with the model with explicit row representation even
though they have far fewer parameters to    t the data.
6 conclusion
in this paper we explore a row-less extension of uni-
versal schema that forgoes explicit row representations
for an aggregation function over its observed columns.
this extension allows prediction between all rows in
new textual mentions     whether seen at train time or not
    and also provides a natural connection to the prove-
nance supporting the prediction. our models also have

model
mrr hits@10
entity-pair embeddings
31.85
entity-pair embeddings-lstm 33.37
31.92
attention
30.00
attention-lstm
max relation
31.71
30.77
max relation-lstm

51.72
54.39
51.67
53.35
51.94
54.80

(a)

model
entity-pair embeddings
attention
attention-lstm
max relation
max relation-lstm

mrr hits@10
5.23
29.75
27.95
28.46
29.61

11.94
49.69
51.05
48.15
54.19

(b)

table 5: the percentage of positive triples ranked in
the top 10 amongst their negatives as well as the mean
reciprocal rank (mrr) scaled by 100 on a subset of the
fb15k-237 dataset. negative examples are restricted
to entity pairs that occurred in the kb or text portion
of the training set. models with the suf   x    -lstm   
are column-less. entity-pair embeddings refers to the
model with explicit row representations. b: predicting
entity pairs that are not seen at train time.

a smaller memory footprint.

in this work we show that an aggregation function
based on query-speci   c attention over relation types
outperforms query independent aggregations. we show
that aggregation models are able to predict on par with
models with explicit row representations on seen row
entries with far fewer parameters. more importantly,
aggregation models predict on unseen row entries with-
out much loss in accuracy. finally, we show that
in id36, we can combine row-less and
column-less models to train models that generalize to
both unseen rows and columns.

acknowledgments
we thank emma strubell, david belanger, and luke
vilnis for helpful discussions and edits. this work was
supported in part by the center for intelligent informa-
tion retrieval and the center for data science, and in
part by darpa under agreement number fa8750-13-
2-0020. the u.s. government is authorized to repro-
duce and distribute reprints for governmental purposes
notwithstanding any copyright notation thereon, in part
by defense advanced research agency (darpa) con-
tract number hr0011-15-2-0036, and in part by the
national science foundation (nsf) grant number iis-
1514053. any opinions,    ndings and conclusions or
recommendations expressed in this material are those
of the authors and do not necessarily re   ect those of the
sponsor. arvind neelakantan is supported by a google
phd fellowship in machine learning.

references
[bahdanau et al.2014] dzmitry bahdanau, kyunghyun
cho, and yoshua bengio. 2014. neural machine
translation by jointly learning to align and trans-
late. iclr 2015, pages 1   15.

[berant et al.2013] j. berant, a. chou, r. frostig, and
p. liang. 2013. id29 on freebase from
question-answer pairs. in empirical methods in nat-
ural language processing.

[bollacker et al.2008] kurt bollacker, colin evans,
praveen paritosh, tim sturge, and jamie taylor.
2008. freebase: a collaboratively created graph
database for structuring human knowledge. in pro-
ceedings of the acm sigmod international con-
ference on management of data.

[bordes et al.2013] antoine bordes, nicolas usunier,
alberto garc    a-dur  an, jason weston, and oksana
yakhnenko.
2013. translating embeddings for
modeling multi-relational data. in advances in neu-
ral information processing systems.

[bordes et al.2014] antoine bordes, sumit chopra, and
jason weston. 2014. id53 with sub-
graph embeddings. arxiv preprint arxiv:1406.3676.

[carlson et al.2010] andrew carlson, justin betteridge,
bryan kisiel, burr settles, estevam r. hruschka,
and a. 2010. toward an architecture for never-
ending language learning. in in aaai.

[gabrilovich et al.2013] evgeniy gabrilovich, michael
ringgaard, and amarnag subramanya.
2013.
facc1: freebase annotation of clueweb corpora,
version 1 (release date 2013-06-26, format version
1, correction level 0). note: http://lemurproject.
org/clueweb09/facc1/cited by, 5.

[garc    a-dur  an et al.2015] alberto garc    a-dur  an, an-
toine bordes, nicolas usunier, and yves grand-
valet. 2015. combining two and three-way embed-
dings models for link prediction in knowledge bases.
corr, abs/1506.00999.

[graves et al.2014] alex graves, greg wayne, and ivo
danihelka. 2014. id63s. arxiv
preprint arxiv:1410.5401.

[kingma and ba2015] diederik kingma and jimmy
ba. 2015. adam: a method for stochastic optimiza-
tion. in 3rd international conference for learning
representations (iclr).

[kwiatkowski et al.2013] tom kwiatkowski, eunsol
choi, yoav artzi, and luke. zettlemoyer. 2013.
scaling semantic parsers with on-the-   y ontology
in empirical methods in natural lan-
matching.
guage processing.

[lin et al.2015] yankai lin, zhiyuan liu, maosong
sun, yang liu, and xuan zhu. 2015. learning
entity and relation embeddings for id13
completion. in proceedings of aaai.

[lin et al.2016] yankai lin, shiqi shen, zhiyuan liu,
huanbo luan, and maosong sun. 2016. neural
id36 with selective attention over in-
stances. in acl.

[ling and weld2012] xiao ling and daniel s. weld.
2012. fine-grained entity recognition. in associa-
tion for the advancement of arti   cial intelligence.

[mintz et al.2009] mike mintz, steven bills, rion
snow, and dan jurafsky. 2009. distant supervision
for id36 without labeled data. in asso-
ciation for computational linguistics and interna-
tional joint conference on natural language pro-
cessing.

[neelakantan and chang2015] arvind neelakantan and
ming-wei chang. 2015.
inferring missing entity
type instances for knowledge base completion: new
dataset and methods. in north american chapter of
the association for computational linguistics.

[neelakantan et al.2014] arvind neelakantan, jeevan
shankar, alexandre passos, and andrew mccallum.
2014. ef   cient non-parametric estimation of multi-
ple embeddings per word in vector space. in empir-
ical methods in natural language processing.

[neelakantan et al.2015] arvind neelakantan, ben-
jamin roth, and andrew mccallum.
2015.
compositional vector space models for knowledge
base completion. proceedings of the 53rd annual
meeting of
the association for computational
linguistics.

[neelakantan et al.2016] arvind neelakantan, quoc v.
le, and ilya sutskever. 2016. neural programmer:
inducing latent programs with id119. in
iclr.

[nickel et al.2011] maximilian nickel, volker tresp,
and hans-peter kriegel. 2011. a three-way model
for collective learning on multi-relational data.
in
international conference on machine learning.

[orr et al.2013] dave orr, amarnag subramanya, ev-
geniy gabrilovich, and michael ringgaard. 2013.
11 billion clues in 800 million documents: a web
research corpus annotated with freebase concepts.
http://googleresearch.blogspot.com/2013/07/11-
billion-clues-in-800-million.html.

[pantel et al.2012] patrick pantel, thomas lin, and
michael gamon. 2012. mining entity types from
query logs via user intent modeling. in association
for computational linguistics.

[park and chu2009] seung-taek park and wei chu.
2009. pairwise preference regression for cold-start
recommendation. in recsys.

[rendle et al.2009] steffen rendle, christoph freuden-
thaler, zeno gantner, and lars schmidt-thieme.
2009. bpr: bayesian personalized ranking from im-
plicit feedback. in proceedings of the twenty-fifth
conference on uncertainty in arti   cial intelligence,
pages 452   461. auai press.

[riedel et al.2010] sebastian riedel, limin yao, and
andrew mccallum. 2010. modeling relations and
in machine
their mentions without labeled text.
learning and knowledge discovery in databases,
pages 148   163. springer.

[riedel et al.2013] sebastian riedel, limin yao, an-
drew mccallum, and benjamin m. marlin. 2013.
id36 with id105 and
universal schemas. in hlt-naacl.

[schein et al.2002] andrew i schein, alexandrin
popescul, lyle h ungar, and david m pennock.
2002. methods and metrics for cold-start recom-
in proceedings of the 25th annual
mendations.
international acm sigir conference on research
and development
in information retrieval, pages
253   260. acm.

[shimaoka et al.2016] sonse shimaoka, pontus stene-
torp, kentaro inui, and sebastian riedel. 2016. an
attentive neural architecture for    ne-grained entity
type classi   cation. in arxiv.

[socher et al.2013] richard socher, danqi chen,
christopher d manning, and andrew ng. 2013.
reasoning with neural tensor networks for knowl-
in advances in neural
edge base completion.
information processing systems.

[suchanek et al.2007] fabian m. suchanek, gjergji
kasneci, and gerhard weikum. 2007. yago: a core
of semantic knowledge. in proceedings of the 16th
international conference on world wide web.

[sukhbaatar et al.2015] sainbayar sukhbaatar,

jason
weston, rob fergus, et al. 2015. end-to-end mem-
in advances in neural information
ory networks.
processing systems, pages 2431   2439.

[surdeanu et al.2012] mihai surdeanu, julie tibshirani,
ramesh nallapati, and christopher d manning.
2012. multi-instance multi-label learning for rela-
in proceedings of the 2012 joint
tion extraction.
conference on empirical methods in natural lan-
guage processing and computational natural lan-
guage learning, pages 455   465. association for
computational linguistics.

[toutanova et al.2015] kristina

toutanova,

danqi
chen, patrick pantel, hoifung poon, pallavi choud-
hury, and michael gamon. 2015. representing text
for joint embedding of text and knowledge bases. in
empirical methods in natural language processing
(emnlp).

[verga et al.2016] patrick verga, david belanger,
emma strubell, benjamin roth,
and andrew
mccallum. 2016. multilingual id36
annual
using compositional universal schema.
conference of
the north american chapter of
the association for computational linguistics
(naacl).

[wang et al.2014] zhen wang, jianwen zhang, jianlin
feng, and zheng chen. 2014. id13

in pro-
embedding by translating on hyperplanes.
ceedings of the twenty-eighth aaai conference on
arti   cial intelligence, pages 1112   1119. citeseer.

[weissenborn2016] dirk weissenborn.

2016. em-
bedding entity pairs through observed relations for
knowledge base completion. in openreview.

[weston et al.2013] jason weston, ron weiss, and
hector yee. 2013. nonlinear latent factorization by
embedding multiple user interests. in acm interna-
tional conference on recommender systems.

[yang et al.2015] bishan yang, wen-tau yih, xiaodong
he, jianfeng gao, and li deng. 2015. embed-
ding entities and relations for learning and id136
international conference on
in knowledge bases.
learning representations 2014.

[yao et al.2013] limin yao, sebastian riedel, and an-
drew mccallum. 2013. universal schema for en-
in proceedings of the 2013
tity type prediction.
workshop on automated knowledge base construc-
tion, pages 79   84. acm.

