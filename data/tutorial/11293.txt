6
1
0
2

 
l
u
j
 

0
3

 
 
]

g
l
.
s
c
[
 
 

1
v
4
0
1
0
0

.

8
0
6
1
:
v
i
x
r
a

world knowledge as indirect supervision for document id91

chenguang wang, peking university
yangqiu song, hong kong university of science and technology
dan roth, university of illinois at urbana-champaign
ming zhang1, peking university
jiawei han, university of illinois at urbana-champaign

one of the key obstacles in making learning protocols realistic in applications is the need to supervise them, a costly process
that often requires hiring domain experts. we consider the framework to use the world knowledge as indirect supervision.
world knowledge is general-purpose knowledge, which is not designed for any speci   c domain. then the key challenges are
how to adapt the world knowledge to domains and how to represent it for learning. in this paper, we provide an example of
using world knowledge for domain dependent document id91. we provide three ways to specify the world knowledge
to domains by resolving the ambiguity of the entities and their types, and represent the data with world knowledge as a
heterogeneous information network. then we propose a id91 algorithm that can cluster multiple types and incorporate
the sub-type information as constraints. in the experiments, we use two existing knowledge bases as our sources of world
knowledge. one is freebase, which is collaboratively collected knowledge about entities and their organizations. the other
is yago2, a knowledge base automatically extracted from wikipedia and maps knowledge to the linguistic knowledge base,
id138. experimental results on two text benchmark datasets (20newsgroups and rcv1) show that incorporating world
knowledge as indirect supervision can signi   cantly outperform the state-of-the-art id91 algorithms as well as id91
algorithms enhanced with world knowledge features.

a preliminary version of this work appeared in the proceedings of kdd 2015 [wang et al. 2015a]. this journal version has
made several major improvements. first, we have proposed a new and general learning framework for machine learning with
world knowledge as indirect supervision, where document id91 is a special case in the original paper. second, in order to
make our unsupervised id29 method more understandable, we add several real cases from the original sentences
to the resulting logic forms with all the necessary information. third, we add details of the three semantic    ltering methods
and conduct deep analysis of the three semantic    lters, by using case studies to show why the conceptualization based
semantic    lter can produce more accurate indirect supervision. finally, in addition to the experiment on 20 newsgroup data
and freebase, we have extended the experiments on id91 results by using all the combinations of text (20 newsgroup,
mcat, ccat, ecat) and world knowledge sources (freebase, yago2).
general terms: data mining, algorithms, performance
additional key words and phrases: world knowledge; heterogeneous information network; document id91; knowl-
edge base; id13.

1. introduction
machine learning algorithms have become pervasive in multiple domains, impacting a wide variety
of applications. nonetheless, a key obstacle in making learning protocols realistic in applications is
the need to supervise them, a costly process that often requires hiring domain experts. in the past
decades, machine learning community has elaborated to reduce the labeling work done by human for
supervised machine learning algorithms or to improve unsupervised learning with only minimum
supervision. for example, semi-supervised learning [chapelle et al. 2006] is proposed to use only
partially labeled data and a lot of unlabeled data to perform learning with the hope that it can perform
as good as fully supervised learning. id21 [pan and yang 2010a] uses the labeled data
from other relevant domains to help the learning task in the target domain. however, there are still
many cases that neither semi-supervised learning nor id21 can help. for example, in the
era of big data, we can have a lot textual information from different web sites, e.g., blogs, forums,

1corresponding author: ming zhang

author   s addresses: c. wang and m. zhang, school of eecs, peking university; email: {wangchenguang,
mzhang cs}@pku.edu.cn; y. song, department of computer science and engineering, hong kong university of science
and technology; email: yqsong@cse.ust.hk; d. roth and j. han, department of computer science, university of illinois at
urbana-champaign; email: {danr, hanj}@illinois.edu.

2

c. wang et al.

fig. 1: heterogeneous information network example. the network g contains    ve entity types:
document, word, date, person and location, which are represented with gray rectangle, gray round,
green square, blue round, and yellow triangle, respectively.

mailing lists. it is impossible to ask human to annotate all the required tasks. it is also dif   cult
to    nd relevant labeled domains. recognizing that some domains can be very speci   c and really
need the domain experts to perform annotation, e.g., the medical domain publication classi   cation.
therefore, we should consider a more general approach to further reducing the labeling cost for
learning tasks in diverse domains.

fortunately, with the proliferation of general-purpose knowledge bases (or id13s),
e.g., cyc project [lenat and guha 1989], wikipedia, freebase [bollacker et al. 2008], know-
itall [etzioni et al. 2004], textrunner [banko et al. 2007], reverb [fader et al. 2011], ol-
lie [mausam et al. 2012], wikitaxonomy [ponzetto and strube 2007], probase [wu et al. 2012],
dbpedia [auer et al. 2007], yago [suchanek et al. 2007], nell [mitchell et al. 2015] and knowl-
edge vault [dong et al. 2014], we have an abundance of available world knowledge. we call these
knowledge bases world knowledge [gabrilovich and markovitch 2005], because they are universal
knowledge that are either collaboratively annotated by human labelers or automatically extracted
from big data. in general, world knowledge can be common sense knowledge, common knowledge,
or domain dependent knowledge. common sense knowledge is the facts that an ordinary person is
expected to know, but we seldom need to learn them on purpose. for example, we all know that a
dog is an animal. common knowledge is widely accessible knowledge. for example, the population
of illinois is around 12.8m. an ordinary person may not know the exact number of population, but
we would    nd the answer easily from numerous sources. domain knowledge can be very speci   c,
such as the complete set of species of animals, or the taxonomy classi   cation of trees.

when world knowledge is annotated or extracted, it is not collected for any speci   c domain.
however, because we believe the facts in world knowledge bases are very useful and of high qual-
ity, we propose using them as supervision for many machine learning problems. people have found
it useful to use world knowledge as distant supervision for entity and id36 and em-
bedding [mintz et al. 2009; wang et al. 2014b; xu et al. 2014]. this is a direct use of the facts in
world knowledge bases, where the entities in the knowledge bases are matched in the context re-
gardless the ambiguity. a more interesting question is can we use the world knowledge to indirectly
   supervise    more machine learning algorithms or applications? for example, if we can use world
knowledge as indirect supervision, then we can extend the knowledge about entities and relations to
more generic text analytics problems, e.g., categorization and information retrieval.

thus, we consider a general machine learning framework that can incorporate world knowledge
into machine learning algorithms. in general, there are three challenges of incorporating world

world knowledge as indirect supervision for document id91

3

knowledge into machine learning algorithms: (1) domain speci   cation, (2) knowledge represen-
tation, and (3) propagation of indirect supervision.

first, as mentioned, world knowledge is not designed for any speci   c domain. for example, when
we want to cluster the documents about entertainment or sports, then the world knowledge about
names of celebrities and athletes may help while the terms used in science and technology may not
be very useful. thus, a key issue is how we should adapt world knowledge to the domain speci   c
tasks. second, when we have the world knowledge, we should consider how we can represent it for
the domain dependent tasks. an intuitive way is to use knowledge bases to generate features for
machine learning algorithms [gabrilovich and markovitch 2005; song et al. 2015]. we call these
features       at    because they do not consider the link information in the knowledge bases. however,
most knowledge bases use a linked network to organize the knowledge. for example, a ceo is con-
nected to an it company, and the it company is a company. thus, the structure of the knowledge
also provides rich information about the connections of entities and relations. therefore, we should
also carefully consider the best way to represent the world knowledge for machine learning algo-
rithms. third, given the world knowledge about entities and their relations, as well as the types of
entities and relations, we should consider an effective algorithm that can propagate the knowledge
about entity and relation categories to the categories of data that contain the entities and relations.
this is a non-trivial task because we should consider both the data representation and the structural
representation of the world knowledge.

in this paper, we illustrate the framework of machine learning with world knowledge using a doc-
ument id91 problem. we select two knowledge bases, i.e., freebase, yago2, as the sources
of world knowledge. freebase [bollacker et al. 2008] is a collaboratively collected knowledge base
about entities and their organizations. yago2 [suchanek et al. 2007] is a knowledge base au-
tomatically extracted from wikipedia and maps the knowledge to the linguistic knowledge base,
id138 [fellbaum 1998]. to adapt the world knowledge to domain speci   c tasks, we    rst use
id29 to ground any text to the knowledge bases [berant et al. 2013]. we then apply en-
tity frequency, document frequency, and conceptualization [song et al. 2015] based semantic    lters
to resolve the ambiguity problem when adapting world knowledge to the domain tasks. after that,
we have the documents as well as the extracted entities and their relations. since the knowledge
bases provide the entity types, the resulting data naturally form a heterogeneous information net-
work (hin) [han et al. 2010]. we show an example of such hin in figure 1. the speci   ed world
knowledge, such as named entities (   bush   ,    obama   ) and their types (person), as well as the doc-
uments and the words form the hin. we then formulate the document id91 problem as an hin
partitioning problem, and provide a new algorithm to better perform id91 by incorporating the
rich structural information as constraints in the hin. for example, the hin builds a link (a must-
link constraint) between    obama    of sub-type politician in one document and    bush    of sub-type
politician in another document. such link and type information could be very useful if the target
id91 domain is    politics.    then we use the sub-type information as supervision information
for the entities, and propagate the information to the documents through the network. therefore we
call the sub-type information as indirect supervision of documents.
the main contributions of this work are highlighted as follows:

    we propose a new learning framework of machine learning with world knowledge as indirect
supervision. we give the general data mining and machine learning framework, and use a speci   c
problem of document id91 to illustrate the process.

    we propose to use id29 and semantic    ltering to specify world knowledge to the
domain dependent documents, and develop a new constrained hin id91 algorithm to make
better use of the structural information from the world knowledge for document id91 task.

    we conduct experiments on two benchmark datasets (20newsgroups and rcv1) to evaluate the
id91 algorithm using hin, compared with the state-of-the-art document id91 algo-
rithms and id91 with       at    world knowledge features. we show that our approach can be

4

c. wang et al.

13.3% better than the semi-supervised id91 algorithm incorporating 250k constraints which
are generated by ground-truth labels.

this paper is an extension of our previous work [wang et al. 2015a]. we make the detailed algo-
rithms clearer, and illustrate the effectiveness and ef   ciency of the algorithms with more extensive
experiments.

the remainder of the paper is organized as follows. section 2 introduces the general learning
framework of machine learning with world knowledge. section 3 presents our world knowledge
speci   cation approach. the representation of world knowledge is introduced in section 4. the
model for document id91 with world knowledge is shown in section 5. experiments and re-
sults are discussed in section 6. section 7 discusses the related work, and we conclude this study in
section 8.

2. machine learning with world knowledge framework
in this section, we discuss the general framework on how we enable world knowledge to indirectly
   supervise    machines, and give an overview on how to conduct document id91 in this frame-
work. in general, performing machine learning with world knowledge, we should follow four steps.

(1) knowledge acquisition. world knowledge acquisition is a challenging problem. there exist
some world knowledge bases. they are either collaboratively constructed by humans (such as
cyc project [lenat and guha 1989], wikipedia, freebase [bollacker et al. 2008]) or automati-
cally extracted from big data (such as knowitall [etzioni et al. 2004], textrunner [banko et al.
2007], reverb [fader et al. 2011], ollie [mausam et al. 2012], wikitaxonomy [ponzetto and
strube 2007], probase [wu et al. 2012], dbpedia [auer et al. 2007], yago [suchanek et al.
2007], nell [mitchell et al. 2015] and knowledge vault [dong et al. 2014]). since we assume
the world knowledge is given, we skip this step in this study. we select two knowledge bases in
this paper, which are freebase and yago2. different knowledge bases have different character-
istics. for example, freebase is collaboratively collected. it focuses on named entities and their
organizations. yago2 is automatically extracted from wikipedia and mapped to id138. it
will be interesting to compare the effects of using different world knowledge bases.

(2) data adaptation. given the world knowledge, it is not necessary to use the whole knowledge
base to perform id136, since not all the world knowledge is related to the speci   c domains.
therefore, we should consider specifying the world knowledge to the domain dependent data,
and adapting the world knowledge to better characterize the speci   c domains. moreover, since
the knowledge can be ambiguous without context, we should consider using domain dependent
data to    nd the best knowledge to use. for example, when a text mentions    apple,    it can refer
to a company or a fruit. in the knowledge base, we have both. therefore, we should choose
the right one to use. notice that the general data adaptation process contains the disambigua-
tion phase. for example, in section 3.2, we describe the way to data adaptation for documents
including the entity disambiguation phrase [li et al. 2013] (semantic    ltering procedure). the
traditional entity disambiguation problem is focusing on leveraging the purely context infor-
mation, such as the co-occurrence of words/phrases appearing in certain window of the entity
to disambiguate the entity, while we are considering to use more information from the world
knowledge base, such as the types of the near entities and relations to disambiguate the entities.
besides, we further explore the semantic context of the entity by considering the relations be-
tween entities in the world knowledge bases. thus, disambiguation is more on entity side in the
original raw documents, while data adaptation is more general.

(3) data and id99. traditionally, machine learning algorithms use feature vec-
tors to represent data. some interesting algorithms can represent data as trees or graphs, and
compute kernel based on trees and graphs for machine learning [collins and duffy 2002; vish-
wanathan et al. 2010]. given the speci   ed knowledge we have as well as the domain dependent
data, we should use a better representation which considers the structure information of the

world knowledge as indirect supervision for document id91

5

fig. 2: major steps of incorporating world knowledge into machine learning algorithms.

linked knowledge rather than just considering the knowledge as    at features. therefore, we
propose to use a typed graph, which is called hin to represent the data.

(4) learning. after we have the representation, we can design a learning algorithm for domain
dependent task. the learning algorithm is dependent to the problem as well as the data and
id99. we will show how to handle the hin for our id91 problem.
particularly, by representing the world knowledge as hin, we have type information for the
named entities we detected from the documents. moreover, the world knowledge also provides
the sub-type information. the coarse-grained type information is denser than the    ne-grained
sub-type information. for example, we can have much more entities annotated as person than
politician. thus, we use the coarse-grained type information to construct the hin, and use the
   ne-grained sub-type information as further supervision for the entities, which is then used as
indirect supervision for the documents.

the illustration of these four steps are shown in figure 2, where steps two and three are sometimes
dependent. for example, we can do id20 and id99 jointly. the
above four steps are general, which means they may apply to many applications. in the following
sections, we demonstrate how to select the right knowledge to use and to represent this knowledge
for the task of document id91. after that, we will introduce the learning algorithm to perform
better document id91 given the representation.

in figure 3, we show the general overview of the framework illustrating the major procedures of
how to adapt the framework to the document id91 task. notice that each module of the general
learning framework shown in figure 2 is directly speci   ed for the particular document id91
with world knowledge approach (figure 3), e.g., data adaptation is speci   ed as world knowledge
speci   cation. generally, we    rst assume the knowledge acquisition is done, i.e., the world knowl-
edge bases (e.g., freebase) are given. second, robust unsupervised id29 (section 3.1)
and three alternative semantic    lters (section 3.2) (e.g., conceptualization based semantic    ltering)
are introduced for data adaptation. we third generate the new representation of the documents by
modelling the unstructured texts in the heterogeneous information network (hin) with entities in-
cluding documents themselves, words in the document set and relevant named entities and relations
with proper types from the knowledge bases. finally, for the learning phase in the framework, based
on the new hin representation of the documents, we propose the constrained hin id91 model
to take the type and link information obtained from the knowledge bases into consideration via
constraints and probabilistic distributions. in simpler terms, the    rst three steps are performed to
construct the document-based hin. then new learning algorithms (e.g., hin based id91 algo-
rithms) are proposed to handle the document id91 in the new network representation and make
use of the relevant knowledge speci   ed from the world knowledge, to improve the performance of
the traditional id91 algorithms.

6

c. wang et al.

fig. 3: the overview of adapting machine learning with world knowledge framework to document
id91.

3. world knowledge specification
in this section, we propose a world knowledge speci   cation approach to generate speci   ed world
knowledge given a set of domain dependent documents. we    rst use id29 to ground
any text to the knowledge base, then provide three semantic    ltering approaches to avoid ambiguity
of the extracted information.

3.1. id29
id29 is the task of mapping a piece of natural language text to a formal meaning
representation [mooney 2007]. this can support id53 by querying a knowledge
base [kwiatkowski et al. 2011]. most previous id29 algorithms or tools developed are
for small scale problems but with complicated logical forms. more recently, large scale semantic
parsing grounding to world knowledge bases has been investigated, e.g., using freebase [krishna-
murthy and mitchell 2012; cai and yates 2013; kwiatkowski et al. 2013; berant et al. 2013; berant
and liang 2014; yao and durme 2014; reddy et al. 2014] or reverb [fader et al. 2013]. these
methods use id29 to answer questions with the world knowledge bases. intuitively, they
need to identify the candidate answers in the parsing results so that they can be ranked to answer
the questions. therefore, they either need the question-answer pairs as supervision, or need a large
amount of resources as well as the questions as distant/weak supervision [reddy et al. 2014]. simi-
lar to them, we are also working with very large scale world knowledge bases, but unlike them, we
do not match question and answers. we have already got all the entities in the document that can be
matched to the world knowledge bases. our task is then to ground the text to the knowledge base
entities and their relationships in the prescribed logical form. because we do not have and do not
necessarily have the question-answer pairs, our problem is a fully unsupervised problem. then the
remaining problems are two-folds. first, we need to identify the relations between entities to map
them to the knowledge base relations. second, we need to resolve the ambiguity of the entities and
relations.
we    rst introduce the problem formulation and then introduce how we perform unsupervised
id29. let e be a set of entities and r be a set of relations in the knowledge base. then
the knowledge base k consists of triplets in the form of (e1, r, e2), where e1, e2     e and r     r. we
follow [berant et al. 2013] to use a simple version of lambda dependency-based compositional
semantics (  -dcs) [liang 2013] as the logic language to query the knowledge base. we use   -dcs
because that it can generate logic forms simpler than id198 forms.   -dcs can reduce the
number of variables by making existential quanti   cation implicit. the logical form in simple   -dcs
is either in the form of unary (a subset of e) or binary (a subset of e    e). we brie   y introduce the
de   nition of basic   -dcs logical forms x and the corresponding denotations xk as below: (1) unary

world knowledge as indirect supervision for document id91

7

fig. 4: id29 example. the    gure shows a derivation d of the input text    obama is
president of united states.    and the sub-derivations. each is labeled with composition rule (in blue)
and logical form (in red). the derivation d ignores words    is    and    of.   

base: an entity e     e is a unary logic form (e.g., obama) with ek = {e}; (2) binary base: a relation
r     r is a binary logic form (e.g., presidentofcountry) with rk = {(e1, e2) : (e1, r, e2)     k};
(3) join: b.u is a unary logic form, denoting a join and projection, where b is a binary and e is a
unary. b.uk = {e1     e :    e2.(e1, e2)     bk     e2     uk} (e.g., presidentofcountry.obama); (4)
intersection: u1 (cid:117) u2 (u1 and u2 are both unaries) denotes set intersection: u1 (cid:117) u2k = u1k     u2k
(e.g., location.olympics (cid:117) presidentofcountry.obama).
we generally adopt the id29 framework proposed in [berant et al. 2013]. given a
piece of text s, the semantic parser produces a distribution over possible derivations d(s). each
derivation d     d(s) is a tree that indicates applying a certain set of composition rules that ends
in the root of the tree, i.e., the logical form d.z. we illustrate the id29 process with an
example as shown in figure 4. in simpler terms, the id29 process can be understood
as the following. first, given a piece of text    obama is president of united states of america,    it
maps the entities as well as the relation phrases in the text to knowledge base. so    obama    and
   united states of america    are mapped to knowledge base, resulting in two unary logic forms peo-
ple.barackobama and country.usa, where people and country are the type information in free-
base. the relation phrase    president    is mapped to a binary logic form presidentofcountry. notice
that, the mapping process skips the words    is    and    of.    the mapping dictionary is constructed by
aligning a large text corpus to the knowledge base. a phrase and a knowledge base entity or relation
can be aligned if they co-occur with many of the same entities. we select two knowledge bases, i.e.,
freebase and yago2. for freebase, we just use the mapping already existing in the released tool
shown in [berant et al. 2013]. for yago2, we follow [berant et al. 2013] and download a subset of
clueweb092 to    nd the new mapping for yago2 entities and relations. second, it uses some rules
(i.e., grammar) to combine the basic logic forms to generate the derivations, and rank the results
(i.e., derivations). in detail, the id29 framework constructs a set of derivations for each
span of the input text. first, for each span of the input text, it generates the single-predicate deriva-
tions based on the lexicon mapping from text to knowledge base predicates (e.g.,    president    maps
to presidentofcountry). according to the set of composition rules, given any logical form x1 and
x2 that are constructed over span [i1, j1] and [i2, j2], we then generate the logic forms based on the
span [min(i1, i2), max(j1, j2)] as the following: x1 (cid:117) x2 (intersection), x1.x2 (join), or x1 (cid:117) r.x2
(briging) for any relation r     r (bridging operation is de   ned to generate additional predicates
based on neighboring predicates). for the example shown in this paragraph, people.barackobama
(cid:117) president.usa is generated to represent its semantic meaning. notice that, president.usa is gen-

2http://www.lemurproject.org/clueweb09.php/

8

c. wang et al.

fig. 5: a real example of the input documents and output logic forms for 20 newsgroups dataset.
left: a set of given documents; middle: semantic parser; right: resulting candidate logic forms for
each document. we only show some examples of the parsed logic forms according to entities    john
smoltz,       braves,    and    bob horner    in the given documents for simplicity.

fig. 6: illustration of semantic    ltering results based on the example shown in figure 5. left: can-
didate logic forms according to each document; middle: semantic    lter; right:    ltered semantics.
   john smoltz    is actually a baseball player, and    braves    means the atlanta braves, which is a
baseball team playing in national league. by using cbsf, the entity ambiguation problem is re-
solved to some extent. for example,    bob horner    could help    john smoltz    disambiguate the
correct entity type to be baseball player from tv actor, by using the context information provided
in the    rst two documents. similarly, the context information in the    rst and the third documents
could help to disambiguate the entity type of    braves    to be baseball team.

erated by joining the unary country.usa with the binary presidentofcountry. figure 5 shows a
real example in 20 newsgroups dataset. given the documents on the left side, the id29
model is performed to generate the logic form candidates on the right side. notice that, we only
show some examples of the parsed logic forms according to entities    john smoltz,       braves,    and
   bob horner    in the given documents for simplicity.

when there are more than one candidate semantic meanings (i.e., derivations) for a sentence,
in [berant et al. 2013], they learn the ranks based on the annotated question-answer pairs. for our
task, this annotation is not available. therefore, instead of ranking or enumerating all the possi-

world knowledge as indirect supervision for document id91

9

ble logic forms (which is found to be not feasible in limited time), we constrain the entities to be
the maximum length spanning phrases recognized by a state-of-the-art id39
tool [ratinov and roth 2009]. we then perform the two steps introduced above by using the maxi-
mum length spanning noun phrase as entities, and use the phrase between them in the text as relation
phrase. we propose to use the following three semantic    ltering methods to resolve the ambiguation
problem.

3.2. semantic filtering
for each sentence in the given document, the output of id29 is a set of derivations that
represent the semantic meaning. however, the extracted entities (i.e., unaries in the resulting deriva-
tions) can be ambiguous. for example,    apple    may be associated with type company or fruit.
therefore, we should    lter out the noisy entities and their types to ensure that the knowledge we
have is good enough as indirect supervision for document id91. we assume that in the domain
speci   c tasks, given the context, the entities seldom have multiple mutually exclusive meanings.
given that we have the domain dependent corpus containing the documents to be clustered, we are
given a lot of evidence to disambiguate the entities. we propose the following three approaches to
select the best knowledge to use for further learning process.

frequency based semantic    lter (fbsf). for each entity ei in document dj, which can have
multiple types choosing from all the types t1, . . . , t   t where we assume there are in total   t types.
then we can use the frequency ndj (ei, tk) of a type tk for an entity ei appearing in a document dj
as the criterion to decide whether the entity should be extracted for the domain speci   c task in a
sentence. then we use a threshold to cut the entity types that appear less than the frequency. here
we assume that the most frequent type(s) of an entity appearing in the document are the correct
semantic meaning(s) in the context.

document frequency based semantic    lter (dfbsf). similar to the frequency based method, we
j idj (ei, tk) of a type tk of an entity ei as the criterion to    nd the most
likely semantic meaning, where idj (ei, tk) = 1 if ei in dj is with type tk, otherwise idj (ei, tk) = 0.
here we assume that if an entity appears in multiple documents with the same type, then the type
should be the correct semantic meaning in whole document collection.

conceptualization based semantic    lter (cbsf). motivated by the approaches of conceptualiza-
tion [song et al. 2011; song et al. 2015] and entity disambiguation [li et al. 2013], we represent each
entity with a feature vector ti = (t1, . . . , t   t )t of entity types, and use standard kmeans to cluster
the entities in a document. suppose in one cluster we have a set of entities e = {e1,       , ene}. we
then use the probabilistic conceptualization proposed in [song et al. 2011] to    nd the most likely
entity types for the entities in the cluster. we make the naive bayes assumption and use

use the document frequency(cid:80)

p (tk|e)     p (tk)

p (ei|tk)

(1)

i=1

as the score of entity type tk. here, p (ei|tk) = n(ei,tk)
n(tk) where n(ei, tk) is the co-occurrence count
of entity type tk and entity ei in the knowledge base, and n(tk) is the overall number of entities with
type tk in the knowledge base. besides, p (tk) = n(tk)(cid:80)   t
. note that, in this formulation, we have
also replace n(ei, tk) with the frequency ndj (ei, tk) or document frequency(cid:80)
n(ei, tk)     1 for freebase and yago, since the evidence in freebase and yago is deterministic.
we also want to leverage the information provided by the document or corpus. therefore, we can
j idj (ei, tk) used in
the previous two methods. in this case, we mean we construct a sub-knowledge base with the edges
being weighted by the evidence shown in the document or the corpus. the id203 p (tk|e) is
used to rank the entity types and the largest ones are selected. in this case, different entities in a
document can be used to disambiguate each other. for each cluster, only the common types are re-
tained, and concepts with con   icts are    ltered out. here we also assume that the type that can best    t

k n(tk)

ne(cid:89)

10

c. wang et al.

fig. 7: heterogeneous information network schema. the speci   ed knowledge is represented in the
form of heterogeneous information network. the schema contains multiple entity types: document
d, word w, named entities {e i}t

i=1, and the relation types connecting the entity types.

the context is the correct semantic meaning. different from the fbsf method, we also consider the
entity cluster information. therefore, cbsf will use more accurate context information about the
entity types. figure 6 shows the semantic    ltering results based on the example shown in figure 5.
   john smoltz    is actually a baseball player, and    braves    means the atlanta braves, which is a base-
ball team playing in national league. note that, based on cbsf, the entity disambiguation problem
is resolved to some extent. for example,    bob horner    could help    john smoltz    disambiguate the
correct entity type to be baseball player from tv actor, by using the context information provided
in the    rst two documents. similarly, the context information in the    rst and the third documents
could help to disambiguate the entity type of    braves    to be baseball team.

4. world id99
the output of id29 and semantic    ltering is then the document associated with the
entities, which are further associated with the types (or concepts, categories, the names can be
different for different knowledge bases and relations). for example, in freebase, we select the top
level named entity categories (i.e., domains) as the types, e.g., person, location, and organization.
in addition to the named entities, we also regard the document and word as two types. then we use
an hin to represent the data we get after id29 and semantic    ltering.

de   nition 4.1. a heterogeneous information network (hin) is a graph g = (v,e) with an
entity type mapping   : v     a and a relation type mapping   : e     r, where v denotes the entity
set and e denotes the link set, a denotes the entity type set and r denotes the relation type set, and
the number of entity types |a| > 1 or the number of relation types |r| > 1.

the network schema provides a high-level description of a given heterogeneous information net-

work.

de   nition 4.2. given an hin g = (v,e) with the entity type mapping   : v     a and the
relation type mapping   : e     r, the network schema for network g, denoted as tg = (a,r), is
a graph with nodes as entity types from a and edges as relation types from r.

then for our world knowledge dependent network, we use the network schema shown in figure 7
to represent the data. the network contains multiple entity types: document d, word w, named
entities {e i}t
i=1, and a few relation types connecting the entity types. notice that, we use    entity
type    to represent the node type in hin, as de   nition 4.1 showed. we use    named entity type    to
represent the type of the name mentioned in text (widely used in nlp community), e.g., person,
location, and organization names. the entities in hin do not have to be named entities, e.g., the
categories of animals or diseases. we denote the document set as d = {d1, d2, . . . , dm}, where
m is the size of d, the word set as w = {w1, w2, . . . , wn}, where n is the size of w, and the

world knowledge as indirect supervision for document id91

11

fig. 8: document based heterogeneous information network example. the network g contains    ve
entity types: document, word, date, person and location, which are represented with gray rectangle,
gray round, green square, blue round, and yellow triangle, respectively.

1, et

}, where vt is the size of e t. we have t = 1, ..., t where t is the
entity set as e t = {et
total number of named entity types we    nd in the knowledge base. note that if there are no named
entities, then the network reduces to a bipartite graph containing only documents and words.

2, . . . , et
vt

in figure 8, we show a real example of the world knowledge dependent network. from the    gure,
we can see that two documents represented as gray rectangles are modelled in the hin. besides
the two documents, we have words represented as gray rounds. the link between a word and the
corresponding document indicates that the document contains the word. we also have named entities
that associated with certain types (e.g., date, person, location), relevant to the documents speci   ed
from the knowledge base. the link between a document and a named entity means that the document
contains the named entity. the link between two named entities represents the relation between the
named entities in the knowledge base generated by world knowledge speci   cation. in summary,
after performing world knowledge speci   cation for documents, world id99
aims to construct a document based hin that contains the link and type information that are useful
to understand the documents, thus lead to better id111 performance.

5. document id91 with world knowledge
in this subsection, we present our id91 algorithm using hin, constructed from domain depen-
dent documents and the world knowledge. given the hin, it is natural to perform hin partitioning
to obtain the document clusters. in addition to the hin itself, let us revisit the structural information
in a typical world knowledge base, e.g., freebase. in the world knowledge base, the named entities
are often organized in a hierarchy of categories. although there are additional category information
for each entity, we only use the top level named entity types as the entity types in hin. for example,
   barack obama    is a person, where person is the top level category. in addition, he is the president
of the    united states,    a politician, a celebrity, etc.. another example is that    google    is a software
company, plus it has a ceo. this shows that the entities can have some attributes. we choose to use
top level entity types for the hin schema since then we will have a relatively dense graph for each
pairwise nodes in the network schema. the    ne-grained named entity sub-types or the attributes are
also very useful to identify the topics or the clusters of the documents. therefore, in this section, we
introduce how we incorporate the    ne-grained level of named entity types as constraints in the hin
id91 algorithm.

12

c. wang et al.

5.1. constrained id91 modeling
to formulate the id91 algorithm for the domain dependent documents, we denote latent label
sets of the documents as ld = {ld1 , ld2 , . . . , ldm}. we also denote lw = {lw1, lw2 , . . . , lwn}
} for the tth named entities set. in general, we follow
for words, and let = {let
the framework of information-theoretic co-id91 (itcc) [dhillon et al. 2003] and constrained
itcc [song et al. 2013; wang et al. 2015] to formulate our approach. instead of only performing
on the bipartite graph, we need to handle multi-type relational data, as well as more complicated
constraints.

, . . . , let
vt

the original itcc uses a variational function to approximate the joint id203 of documents

, let

1

2

and words, which is:

q(dm, wi) = p(   dkd ,   wkw )p(dm|   dkd )p(wi|   wkw ),

(2)
where   dkd and   wkw are cluster indicators to formulate the id155, and kd and kw
are the corresponding cluster indices. q(dm, wi) is used to approximate p(dm, wi) by minimizing
the kullback-leibler (kl) divergence:

dkl(p(d,w)||q(d,w))

= dkl(p(d,w,   d,   w)||q(d,w,   d,   w))

p(dm)dkl(p(w|dm)||p(w|   dkd ))
p(wi)dkl(p(d|wi)||p(d|   wkw )),

dm:ldm =kd

wi:lwi =kw

= (cid:80)kd
= (cid:80)kw

kd

kw

(cid:80)
(cid:80)

where   d and   w are the cluster sets, p(w|   dkd ) denotes a multinomial distribution based on the
probabilities

symmetrically, we have

p(w|   dkd) = (p(w1|   dkd ), . . . , p(wn|   dkd ))t ,
p(wi|   dkd ) = p(wi|   wkw )p(   wkw|   dkd ) and,

p(wi|   wkw ) = p(wi)/p(lwi =   wkw ).

p(d|   wkw ) = (p(d1|   wkw ), . . . , p(dm|   wkw ))t ,

p(di|   wkw ) = p(di|   dkd)p(   dkd|   wkw ) and,

p(di|   dkd ) = p(di)/p(ldi =   dkd ).

(cid:80)
moreover, p(   wkw|   dkd ) and p(   dkd|   wkw ) are computed based on the joint id203 q(   dkd ,   wkw ) =

(cid:80)

ldm =kd
motivated by itcc, according to the network schema shown in figure 7, our problem of hin

p(dm, wi).

lwi =kw

id91 is formulated as

+(cid:80)t
+(cid:80)t

jhinc = dkl(p(d,w)||q(d,w))

(cid:80)t
t=1 dkl(p(d,e t)||q(d,e t))
s=1 dkl(p(e t,e s)||q(e t,e s)),

t=1

where the equation aims to leverage the link and type information (documents, words, types of
named entities) in the hin shown in figure 7 to estimate the cluster label of each document. the
   rst part of the problem, dkl(p(d,w)||q(d,w)), according to eq. 3, is exactly the original itcc
on the document-word bipartite graph (the top portion of the figure 7 with two types of entities, doc-
ument and word). it aims to use a variational function q(d,w) to approximate the joint id203

(3)

(4)

in [dhillon et al. 2003]   s lemma 2.1. similarly, the second part(cid:80)t

world knowledge as indirect supervision for document id91
13
of documents and words p(d,w) measured by kl divergence, to minimize the mutual informa-
tion loss due to the mapping to generate the cluster indices of the documents and words as shown
t=1 dkl(p(d,e t)||q(d,e t))
means to use the variational function q(d,e t) to approximate the joint id203 of documents
and named entities belong to every type t. this equals to perform itcc on every document-named
entity of type t bipartite graph (the middle portion of the figure 7). the third part is also de   ned
similarly, which de   nes the variational function q(e t,e s) to approximate the joint id203 of
named entities of every type t and named entities belong to every type s. this equals to perform
itcc on every named entity of type t-named entity of type s bipartite graph (the bottom portion of
the figure 7). all the probabilities of the second and third parts can be similarly de   ned as the    rst
part, document-word bipartite graph. we omit the detailed de   nitions for brevity. a summary of the
notations is shown in table i.

table i: notations for id91 algorithm. the indicators are used for the id203 representa-
tion, while the indices are used as ids for the clusters.

meaning

cluster index

cluster indicator
data indicator

data indicator set

label

label indicator set

document word named entity

kd
  dkd
dm
d
ldm
ld

kw
  wkw
wi
w
lwi
lw

ket
  et
ket
et
e t
i
let
let

i

to incorporate the side information of the    ne-grained named entity sub-types or the attributes
as indirect supervision for document id91, we de   ne the constraints for the named entities we
   nd after id29. we take the tth entity label set e t as an example, and use must-links
and cannot-links as the constraints. we denote the must-link set associated with et
, and
the cannot-link set as cet
. the way how we build must-links and cannot-links is described in the
experiment (section 6.4.3). for must-links, the cost function is de   ned as

i as met

i

i

vm(et
i1

    met
= wmdkl(p(d|et

, et
i2

)

i1

)||p(d|et

))    ilet

i2

i1

(cid:54)=let

i2

i1

,

(5)

where wm is the weight for must-links, and p(d|et
) denotes a multinomial distribution based on
), . . . , p(dm|et
the probabilities (p(d1|et
))t , and itrue = 1, if alse = 0. the above must-link
i1
cost function means that if the label of et
i1 is not equal to the label of et
i2, then we should take
into account the cost function of how dissimilar the two entities et
i2 are. the dissimilarity is
computed based on the id203 of document d given the entities et
i2 as eq. (5). the more
dissimilar the two entities are, the larger cost is imposed. please refer to the experimental section
(section 6.4) for details about the weight setting (wm) for the must-links.

i1 and et

i1 and et

i1

i1

for cannot-links, the cost function is de   ned as

    cet

vc(et
i1
= wc(dt

, et
i2
max     dkl(p(d|et

i1

)

)||p(d|et

i2

)))    ilet

i1

(cid:54)=let

i2

,

(6)

i1

)||p(d|et

the
where wc is the weight for cannot-links, and dt
dkl(p(d|et
i1 is equal
i2, then we should take into account the cost function of how similar they are.
to the label of et
also, please refer to the experimental section (section 6.4) for details about how we set wc for the
cannot-links.

)). the cannot-link cost function means that if the label of et

max is the maximum value for all

i1

i2

14

c. wang et al.

algorithm 1: alternating optimization for chinc.

input: hin de   ned on documents d, words w, and entities
e t, t = 1, ..., t ; set maxiter and max  .
while iter < maxiter and    > max   do

d label update: minimize eq. (8) w.r.t. ld.
d model update: update q(dm, wi) and q(dm, et
i).
for t = 1, ..., t do
e t label update: minimize eq. (10) w.r.t. let.
e t model update: update q(dm, et
i) and q(es
j , et

end for
d label update: minimize eq. (8) w.r.t. ld.
d model update: update q(dm, wi) and q(dm, et
i).
w label update: minimize eq. (9) w.r.t. lw.
w model update: update q(dm, wi).
compute cost change    using eq. (7).

i).

end while

id91 is:

integrating the constraints for le1 , . . . ,let to eq. (4), the objective function of constrained hin

jchinc = dkl(p(d,w)||q(d,w))

+(cid:80)t
+(cid:80)t
+(cid:80)t
+(cid:80)t

(cid:80)t
t=1 dkl(p(d,e t)||q(d,e t))
(cid:80)vt
s=1 dkl(p(e t,e s)||q(e t,e s))
(cid:80)vt
, et
i2
    cet

vm(et
i1
, et
i2

i1
vc(et
i1

(cid:80)
(cid:80)

   met
   cet

et
i2

et
i1

t=1

t=1

t=1

=1

=1

et
i2

et
i1

    met
).

i1

i1

i1

)

(7)

from this objective function we can see that, the must-links and cannot-links are imposed to the
entities that the id29 detects. since the task is document id91, the sub-types of
entities serve as indirect supervision because they cannot directly affect the cluster labels of the
documents. however, the constraints can affect the labels of entities, and then the labels of entities
can be transferred to the document side to affect the labels of documents.

5.2. alternating optimization
since global optimization of all the latent labels as well as the approximate function q(  ,  ) is in-
tractable, we perform an alternating optimization shown in algorithm 1. we iterate the process to
optimize the labels of documents, words, and entities. meanwhile, we update the function q(  ,  ) for
the corresponding types.

for example, to    nd label ldm of document dm, we have:

ldm = arg min
ldm =kd

to    nd label lwi of word wi, we have:

(cid:80)t
dkl(p(w|dm)||p(w|   dkd ))+
t=1 dkl(p(e t|dm)||p(e t|   dkd )).

lwi = arg min
lwi =kw

dkl(p(d|wi)||p(d|   wkw )).

(8)

(9)

to    nd the label let

iteratively assign a label to the entity. we update one label let

, we use the iterated conditional mode (icm) algorithm [basu et al. 2004] to
at a time, and keep all the other labels

i

i

15

(10)

(11)

(12)

(13)

world knowledge as indirect supervision for document id91

   xed:

let

i

dkl(p(d|et

i)||p(d|   et

ket ))

i)||p(e s|   et

ket ))
wmdkl(p(d|et
i)||p(d|et

i(cid:48)))

= arg min
=ket

let
i

+(cid:80)t
+(cid:80)
+(cid:80)

i

;

s=1 dkl(p(e s|et
i(cid:48)     met
et
(cid:54)=let
ilet
i(cid:48)
i(cid:48)     cet
et
ilet
=let
i(cid:48)

;

wc (dt

i

i

i

max     dkl(p(d|et

i)||p(d|et

i(cid:48)))) .

to transfer the original objective function (7) to eq. (10), we should follow eq. (3) where we replace
the document and word notations to the entity notations. to understand why eq. (3) holds, we
suggest to refer to the original itcc for detailed derivation [dhillon et al. 2003].
then, with the labels ld, let and lw    xed, we update the model function q(dm, wi), q(dm, et
i),
i). the update of q is not in   uenced by the must-links and cannot-links. thus we can

and q(es
modify them the same as itcc [dhillon et al. 2003] and only show the update of q(dm, et

j, et

i) here:

(cid:88)

(cid:88)

q(   dkd ,   et

ket ) =

p(dm, et

i);

ldm =kd

let
i

=ket

q(dm|   dkd ) =

q(dm)

q(ldm = kd)

[q(dm|   dkd ) = 0 if ldm (cid:54)= kd];

where q(dm) = (cid:80)
(cid:80)

p(   dkd ,   et

ket ).

q(et

i|   et

ket ) =

p(dm, et

i), q(et

et
i

q(et
i)
= ket)

q(let

i) = (cid:80)

i

[q(et

ket ) = 0 if let

i|   et
i), q(   dkd ) = (cid:80)

i

p(dm, et

dm

(cid:54)= ket];

p(   dkd ,   et

ket ) and q(   et

ket ) =

ket

t=1

kd
algorithm 1 summarizes the main steps in the procedure. the objective function (7) with our
alternating update monotonically decreases to a local optimum. this is because the icm algorithm
decreases the non-negative objective function (7) to a local optimum given a    xed q function. then
the update of q is monotonically decreasing as guaranteed by the theorem proven in [song et al.
the time complexity of algorithm 1 is o(nd,w    (kd + kw) +(cid:80)t
2013]. besides, the original proof of the decrease of q is shown by theorem 4.1 in [dhillon et al.
(cid:80)t
(cid:80)t
2003].
t=1 nd,e t    (kd + ket) +
s=1(ne t,e s + (nc     itericm ))    (ket + kes ))    iterao, where n  ,   is the total number
of non-zero elements in the corresponding co-occurrence matrix, nc is the number of constraints,
itericm is the number of icm iterations, kd, kw and ket are the number of document clusters,
word clusters and entity clusters of type t, and iterao is the number of the alternating optimization
iterations.

discussion: the major factors contribute to the time complexity of algorithm 1 are the numbers
of non-zero elements in the corresponding matrices. we have about 20, 000 documents and 60, 000
words in 20ng. from figure 10, we    nd around 20, 000 entities speci   ed from freebase with
79 types of entities. in contrary, in 20ng, the number of document clusters, word clusters and
named entity clusters are empirically set to be 20, 40 and 79, according to the number of categories
of documents, twice the number of document clusters following [song et al. 2013] and the total
number of top level named entity types in freebase, respectively. compared to the number of non-
zero elements in the matrix (e.g., hundreds of thousands), the number of relevant clusters can be
ignored. in such situation, if the network schema contains more types of named entities, the running
time of the algorithm will not increase that much, compared to the impacts caused by the size of

16

c. wang et al.

document sets and vocabulary, as well as the named entity sets. there could be lots of ways to
improve the performance of machine learning algorithms in large-scale datasets, e.g., distributed
computation. however, this beyond the scope of this paper, we leave it for future work.

6. experiments
in this section, we show the experimental results to demonstrate the effectiveness and ef   ciency of
our approach on document id91 with world knowledge as indirect supervision.

6.1. datasets
we use the following two benchmark datasets to evaluate domain dependent document id91.
for both datasets we assume the numbers of document clusters are given.

20newsgroups (20ng): the 20newsgroups dataset [lang 1995] contains about 20,000 news-
groups documents evenly distributed across 20 newsgroups.3 we use all the 20 groups as 20 classes.
rcv1: the rcv1 dataset is a dataset containing manually labeled newswire stories from reuter
ltd [lewis et al. 2004]. the news documents are categorized with respect to three controlled vocab-
ularies: industries, topics and regions. there are 103 categories including all nodes except for root in
the hierarchy. the maximum depth is four, and 82 nodes are leaves. we select top categories mcat
(markets), ccat (corporate/industrial) and ecat (economics) in one portion of the test partition
to form three id91 tasks. the three id91 tasks are summarized in table ii. we use the
original source of this data, and use the leaf categories in each task as the ground-truth classes.

table ii: rcv1 dataset statistics. #(categories) is the number of all categories; #(leaf categories)
is the number of leaf categories; #(documents) is the number of documents.

mcat
ccat
ecat

#(categories)

9
31
23

#(leaf categories)

7
26
18

#(documents)

44,033
47,494
19,813

6.2. world knowledge bases
then we introduce the knowledge bases we use.

freebase: freebase4 is a publicly available knowledge base consisting of entities and relations
collaboratively collected by its community members. now, it contains over 2 billions relation ex-
pressions between 40 millions entities. we convert a logical form generated by our unsupervised se-
mantic parser of the world knowledge speci   cation approach introduced in section 3 into a sparql
query and execute it on our copy of freebase using the virtuoso engine.

yago2: yago25 is also a semantic knowledge base, derived from wikipedia, id138 and
geonames. currently, yago2 has knowledge of more than 10 million entities (like persons, orga-
nizations, cities, etc.) and contains more than 120 million facts about these entities. similar to free-
base, we also convert a logical form into a sparql query and execute it on our copy of yago2
using the virtuoso engine.

in table iii, we show some statistics about freebase and yago2.
note that in most knowledge bases, such as freebase and yago2, entities types are often or-
ganized in a hierarchical manner. for example, politician is a sub-type of person. university is a
sub-type of organization. all the types or attributes share a common root, called object. figure 9
depicts an example of hierarchy of types. in general, we use the highest level under the root object

3http://qwone.com/   jason/20newsgroups/
4https://developers.google.com/freebase/
5http://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/yago/

world knowledge as indirect supervision for document id91

17

table iii: statistics of freebase and yago2. #(entity types) is the number of entity types; #(entity
instances) is the number of entity instances; #(relation types) is the number of relation types;
#(relation instances) is the number of relation instances.
freebase
1,500a

yago2
350,000

#(entity types)

name

#(entity instances)
#(relation types)

#(relation instances)

40 millions

35,000
2 billions

10 millions

100

120 millions

a the number of 1,500 types is reported in [dong et al.
2014]. in our downloaded dump of freebase, we found 79
domains, 2,232 types, and 6,635 properties.

as the entity types (e.g., person) as speci   ed world knowledge incorporated in the hin, and the di-
rect children (e.g., politician) as entity constraints. in the following experiments, we select person,
organization, and location as the three entity types in the hin, because they are popular in both
freebase and yago2.

fig. 9: hierarchy of entity types.

discussion: in all the following experiments, we conduct experiments based on different com-
binations of world knowledge bases (i.e., freebase, yago2) introduced in this section with the
four datasets (i.e., 20ng, mcat, ccat and ecat) described in section 6.1. in total, we illustrate
eight different world knowledge base and document dataset combinations for both world knowl-
edge speci   cation and the id91 algorithms. for example, we show eight id91 results of
our id91 algorithm named chinc in table vi(b). as shown in the results, for instance, the
id91 result of chinc on 20ng dataset using freebase as world knowledge source is 0.631.

6.3. effectiveness of world knowledge speci   cation
before applying the speci   ed world knowledge to downstream text analytics tasks, such as doc-
ument id91 in our case, we need to evaluate whether our world knowledge speci   cation ap-
proach could produce the correct speci   ed world knowledge.

in order to test the effectiveness of our world knowledge speci   cation approach, we    rst sample
200 documents from 20 newsgroups, i.e., 10 documents from each category. second, we split the
documents into sentences. after post-processing, 3,232 sentences are generated for human evalu-
ation. third, we use our world knowledge speci   cation approach in section 3 with three different
semantic    ltering modules to generate the speci   ed world knowledge for each sentence, which con-
sists of relation triplets in the form of (e1, r, e2) with the type information. notice that we use
freebase as the world knowledge source. for fbsf, in each document, we decide the type of an
entity in that document by choosing the one with the largest frequency in the document according
to section 3.2; for dfbsf, similar to that of fbsf, the type with the largest document frequency
is selected as the correct type of the entity in all the documents; for cbsf, we set the number of
entity type clusters as 79, which is the number of top types (in freebase, i.e., domains) as shown

18

c. wang et al.

in table iii, since we assume the document set including all the possible entity types in the world
knowledge base. afterwards, we ask three annotators to label the speci   ed world knowledge ac-
cording two criterion: (1) whether the boundaries of e1 and e2 are correctly recognized or not; (2)
whether the entity type of e1 and e2 are correct or not. it is annotated as correct if both (1) and
(2) are satis   ed. we check the mutual agreement of the human annotation, which is around 91.3%
accuracy.

table iv: precision of different semantic    ltering results. fbsf represents frequency based semantic
   lter; dfbsf represents document frequency based semantic    lter; cbsf represents conceptual-
ization based semantic    lter.

semantic filter

precision

fbsf dfbsf cbsf
0.751
0.916

0.890

table v: error analysis of speci   ed world knowledge generated by the world knowledge speci   ca-
tion approach with three different semantic    lters. fbsf represents frequency based semantic    lter;
dfbsf represents document frequency based semantic    lter; cbsf represents conceptualization
based semantic    lter.

type of error

example sentence

entity
recognition

entity
disambiguation

subordinate
clause

   einstein    s theory of rela-
tivity explained mercury    s
motion.   
   bill said all this to make
the point that christianity is
eminently.   
   bruce s. winters, worked
at united states tech-
nologies research center,
bought a ford.   

number and percentage of errors

fbsf (805)
179 (22.2%)

dfbsf (359)
129 (35.9%)

cbsf (272)
105 (38.6%)

537 (66.7%)

182 (50.7%)

130 (47.8%)

89 (11.1%)

48 (13.4%)

37 (13.6%)

we then test the precision of three different speci   ed world knowledge generated by the corre-
sponding semantic    ltering method. the results are shown in table iv. from the results we can
see that, cbsf outpeforms the other two ways to generate the correct semantic meaning. the main
reason is that, conceptualization based method is able to use the context information to help judge
the real semantic of the text rather than only taking the statistics of the data into account. here we
only care about precision because we wish to use world knowledge as indirect supervision. the
recall will not be very important.

error analysis. to further investigate what triggers the errors in our id29 and seman-
tic    ltering pipelines, we analyze the cause of errors for the incorrect speci   ed world knowledge.
the errors are collected from the error cases based on the annotation generated by the three anno-
tators. then we ask the annotators to try to classify the errors. finally, we summarize the following
three error categories as shown in table v.

entity recognition: in id29, entities can be extracted incorrectly. long entities are
composed of multiple simple entities. for example,    einstein    s theory of relativity    may be ex-
tracted as    einstein    and    theory of relativity.    id141 and misspelling entities cause their
textual expressions to deviate from any knowledge base entries. idiomatic expressions are incor-
rectly picked up as entities. using a larger mapping from text to knowledge base phrases, or para-
phrasing techniques will help avoid some errors. however, this is out of the scope of this article.

world knowledge as indirect supervision for document id91

19

table vi: performance of different id91 algorithms on 20ng and rcv1 data. chinc is our
proposed method. bow, fb (freebase), or yg (yago2) represent bag of word features, the en-
tities generated by our world knowledge speci   cation approach based on freebase or yago2, re-
spectively. we compared all the numbers of hinc and chinc with citcc, which is the strongest
baseline. the percentage in the brackets are the relative number compared to citcc. citcc uses
250k constraints generated based on ground-truth labels of documents.

(a) performance of kmeans, itcc, and citcc with different features.

citcc
features bow bow bow bow bow bow bow

kmeans

itcc

data
20ng
mcat
ccat
ecat

0.429
0.549
0.403
0.417

+fb
0.447
0.575
0.419
0.436

+yg
0.437
0.559
0.410
0.424

0.501
0.604
0.472
0.493

+fb
0.525
0.630
0.494
0.516

+yg
0.513
0.619
0.481
0.505

0.569
0.652
0.535
0.562

(b) performance of hinc and chinc with different world knowledge sources.

hinc

fb

yg

fb

chinc

yg

0.571 (+0.4%)
0.645 (   1.1%)
0.542 (+1.3%)
0.561 (   0.2%)

0.541 (   4.9%)
0.625 (   4.1%)
0.515 (   3.7%)
0.530 (   5.7%)

0.631 (+10.9%)
0.698 (+7.1%)
0.606 (+13.3%)
0.624 (+11.0%)

0.600 (+5.5%)
0.685 (+5.1%)
0.574 (+7.3%)
0.588 (+4.6%)

features

data
20ng
mcat
ccat
ecat

entity disambiguation: selecting an incorrect entity out of multiple matching candidates causes
this error, e.g.,    bill    in our example sentence can be    bill clinton    or    bill gates.    primarily due
to two reasons:    rst, entity disambiguation is a tough research problem in nlp community. second,
the type information of relations are not suf   cient to futher prune out mismatching entities during
semantic    ltering process. notice that, entity disambiguation is the major cause of the errors. by
using cbsf, the number of incorrect entities caused by disambiguation can be dramatically reduced.
subordinate clause: id29 sometimes produces wrong relation phrases in the sub-
ordinate clauses. for example, in the example wrong case shown in table v, it takes the relation
phrase    worked at    meaning the working place of    bruce s. winters,    ignores the phrase    bought,   
which could be more informative for the target id91 domain. this could be resolved by adding
more concrete rules in the id29 grammar.

discussion: notice that both 20ng and rcv1 have relatively larger number of named entities
speci   ed from knowledge bases, thus we show more signi   cantly improved id91 results. when
encountering the text with little named entities, our algorithm would be very similar to that of
the original citcc, since the schema of the document based hin constructed in section 3 and
section 4 would tend to be similar as the schema with only two types of entities, i.e., documents
and entities. besides, citcc only uses the constraints built upon the words and documents to cluster
the documents, while our approach explores the constraints built based on not only documents and
words, but also the named entities from the knowledge base.

in the following experiments, we use the world knowledge speci   cation approach with cbsf,

because it performs the best among the three semantic    ltering methods.

6.4. id91 result
in this experiment, we compare the performance of our model, constrained heterogeneous informa-
tion network id91 (chinc), with several representative id91 algorithms such as kmeans,

20

c. wang et al.

fig. 10: statistics of the number of entities in different document datasets with different world
knowledge sources.

1(cid:80)t
itcc [dhillon et al. 2003] and citcc [song et al. 2013]. the parameters used in chinc to con-
trol the constraints are wm and wc. we set them as
t=1 |e t| following the rules tested in [song
et al. 2013], where |e t| is the number of entities in e t.6 we also denote our algorithm without
constraints as hinc. for both chinc and hinc, we set the numbers of document clusters, word
clusters and named entity clusters according to the numbers of categories of the document set,
twice the number of document clusters following [song et al. 2013] and the total number of top
level named entity types in the world knowledge base, respectively. the number of constraints used
in each combination of document set and knowledge base are the same, which equals to 3     108.
the constraints are randomly selected from all the constraints generated by the method introduced
in section 6.4.3.    fb    and    yg    represent two different world knowledge sources, freebase and
yago2, respectively. we re-implement all the above id91 algorithms. notice that, for citcc,
we follow [song et al. 2013] to generate and add constraints for documents and words. we also use
the speci   ed world knowledge as features to enhance the kmeans and itcc. the feature settings
are de   ned as below:
    bow: traditional bag-of-words model with the tf-idf weighting mechanism.
    bow+fb: bow integrated with additional features from entities in speci   ed world knowledge

of freebase.

    bow+yg: bow integrated with additional features from entities in speci   ed world knowledge

of yago2.
we employ the widely-used normalized mutual information (nmi) [strehl and ghosh 2003] as
the evaluation measure. the nmi score is 1 if the id91 results match the category labels per-
fectly and 0 if the clusters are obtained from a random partition. in general, the larger the scores are,
the better the id91 results are.

in table vi, we show the performance of all the id91 algorithms with different experimental
settings. the nmi is the average nmi of    ve random trials per experiment setting. overall, among
all the methods we test, chinc consistently performs the best among all the id91 methods.
we can see that hinc+fb and hinc+yg perform better than itcc with bow+fb or bow+yg
features, respectively. this means that by using the structural information provided by the world
knowledge, we can further improve the id91 results. in addition, the algorithms with freebase

6in [song et al. 2013], the experiment shows the parameter study on the weights of the constraints varying from 1e     8 to
100, and conclude this is one of the best settings.

world knowledge as indirect supervision for document id91

21

(a)    chinc + freebase    for 20ng.

(b)    chinc + freebase    for mcat.

(c)    chinc + freebase    for ccat.

(d)    chinc + freebase    for ecat.

fig. 11: effect of number of entity clusters of each entity type on document id91 on different
datasets with freebase as world knowledge source.

consistently outperform the ones with yago2, since freebase has much more facts compared with
yago2 as shown in table iii; besides, one can see in figure 10 that freebase could consistently
specify more entities than yago2 does from all of the document datasets. citcc is the strongest
baseline id91 algorithm, because it uses the ground-truth constraints derived from category
labels based on the human knowledge. we use 250k constraints to perform citcc. as shown in
table vi, hinc performs competitive with the citcc. chinc signi   cantly outperforms citcc.
this shows that by automatically using world knowledge, it has the potential to perform better than
the algorithm with the speci   c domain knowledge.

discussion: based on the results, we can see that, when performing existing id91 algorithms
with world knowledge, it is better to choose the world knowledge sources including relatively larger
number of instances of entities and relations. since, in general, the more entities and relations one
knowledge base has, the bigger possibility that more useful entities and relations could be parsed
out, and thus impact the    nal id91 result. table vi shows such difference between freebase
and yago2 on the document id91 task, which marches the difference between the number
of speci   ed entities in figure 10. besides, it will be interesting to explore the id91 validation

22

c. wang et al.

(a)    chinc + yago2    for 20ng.

(b)    chinc + yago2    for mcat.

(c)    chinc + yago2    for ccat.

(d)    chinc + yago2    for ecat.

fig. 12: effect of number of entity clusters of each entity type on document id91 on different
datasets with yago2 as world knowledge source.

methods [luo et al. 2009; wu et al. 2009; liu et al. 2013] to further demonstrate the reliability of
the id91 results.

6.4.1. analysis of number of entity clusters. we also evaluate the effect of varying the number
of entity clusters of each entity type in chinc on the document id91 task. figure 11a shows
the results of id91 with different numbers of entity clusters of each entity type on    chinc +
freebase    for the 20ng dataset. the number of entity clusters varies from 2 to 128. the default
number of iterations is set as 20, which will be discussed in section 6.4.2. when testing the effect of
the number of entity clusters of one entity type, the numbers of entity clusters of the other two entity
types are    xed as twice as the number of document clusters, which are 40 and 40 in 20ng, respec-
tively. it is shown that for this dataset, more entity clusters may not result in improved document
id91 results when a suf   cient number of entity clusters is reached. for example, as shown in
figure 11a, after reaching 32, the nmi scores of chinc actually decrease when the numbers of
entity clusters further increase. one can also    nd the effects of the numbers of entity clusters on the
id91 performance with the other document dataset and knowledge base combinations in fig-

world knowledge as indirect supervision for document id91

23

ures 11b   11d for freebase and figures 12a   12d for yago2. from the results, we can conclude that,
there exist certain values of the number of entity clusters leading to the best id91 peformance.
6.4.2. analysis of number of iterations in alternating optimization. we evaluate the impact of the
number of iterations of the alternating optimization (algorithm 1) on chinc in relation to the
execution time of the optimization algorithm as well as the id91 performance. we increase the
number of iterations from 1 to 80. for example, for each number of iterations, we run chinc    ve
trials, and the average execution time and nmi are summarized in figures 13   14. from the result,
one can conclude that the larger number of iterations is, the more signi   cant the improvement on
id91 performance. this improvement eventually drops, tapers out, and becomes stable. the
reason is that, with the increase of the number of iterations, the alternating optimization algorithm
comes to covergence. however, the execution time still increase in a nearly linear manner. for
example, as shown in figure 13a, after reaching 20, the performance stays stable. thus, we set the
number of iterations as 20 in the remaining experiments with the consideration of both performance
and ef   ciency. as shown in figure 13b   14d, we set the number of iterations as 20 when conducting
experiments on the other combinations of document datasets and world knowledge bases.

(a)    chinc + freebase    for 20ng.

(b)    chinc + freebase    for mcat.

(c)    chinc + freebase    for ccat.

(d)    chinc + freebase    for ecat.

fig. 13: analysis of # of iterations in alternating optimization algorithm on different datasets with
freebase as world knowledge source. left y-axis: average nmi; right y-axis: average execution
time (ms).

6.4.3. analysis of speci   ed world knowledge based constraints. rather than using human
knowledge as constraints, we use the speci   ed world knowledge automatically generated by our
approach as constraints in chinc. based on the speci   ed world knowledge, it is straightforward to
design constraints for entities.

24

c. wang et al.

(a)    chinc + yago2    for 20ng.

(b)    chinc + yago2    for mcat.

(c)    chinc + yago2    for ccat.

(d)    chinc + yago2    for ecat.

fig. 14: analysis of # of iterations in alternating optimization algorithm on different datasets with
yago2 as world knowledge source. left y-axis: average nmi; right y-axis: average execution
time (ms).

entity constraints. (1) must-links. if two entities belong to the same entity sub-type, we add a
must-link. for example, the entity sub-types of    obama    and    bush    are both politician, we thus
build a must-link between them. (2) cannot-links. if two entities belong to different entity sub-
types, we add a cannot-link. for example, the entity sub-types of    obama    and    united states    are
politician and country respectively. in this case, we add a cannot-link to them.

we then test the performance of our proposed chinc by using the speci   ed world knowledge
as constraints described above. we show the experiments on all of the different combinations of
datasets and world knowledge sources in figure 15   16. each x-axis represents the number of entity
type constraints used in each experiment, and y-axis is the average nmi of    ve random trials. for
example, the constraints derived from entity type #1, #2, and #3 are eventually added to chinc
as shown in figure 15a, figure 15b and figure 15c respectively, when using freebase as world
knowledge and testing on 20ng dataset. we can see that chinc outperforms the best id91
algorithm with the human knowledge as shown in table vi (citcc: 0.569) with even no constraints
(hinc: 0.571). by adding more and more constraints, the id91 result of chinc is signi   cantly
better. so chinc is able to use information in world knowledge speci   ed in the hin, and the
entity sub-type information can be transferred to the document side. the results show the power
of modeling data as heterogeneous information networks, as well as the high quality of constraints
derived from world knowledge.

from figures 15   16, by increasing the number of constraints, we    nd that the average execution
time of    ve trials increases linearly, and the id91 performance measured by nmi is increasing
as mentioned before. for example, figure 15c shows the effects of the constraints of all the three
entity types on the id91 performance as well as the execution time, when freebase is used

world knowledge as indirect supervision for document id91

25

as world knowledge and chinc is tested on 20ng dataset. after the number of constraints reach
around 50m, the increase of performance drops and stays stable. at this point, the execution time
is around 1.2m (ms). in figure 15d   16l, we can see the similar results on the other combinations
of document datasets and knowledge bases. we also    nd that the average execution time of our
algorithm with freebase as world knowledge source is greater than that with yago2. as shown in
figure 10, the reason is that each document datasets with freebase could be speci   ed much more
entities than that with yago2. from the results, we can see that our algorithm is scalable to use the
large scale speci   ed world knowledge as constraints, and cluster large amounts of documents.

7. related work
in this section, we review the related work on document id91, machine learning with domain
and world knowledge, as well as heterogeneous information networks.

7.1. document id91
document id91 has been studied for many years. we can use traditional one-dimensional clus-
tering algorithms (e.g., kmeans) to cluster the documents. if we treat the document and correspond-
ing words as a bipartite graph, we can use co-id91 algorithms [dhillon et al. 2003] to cluster the
documents. moreover, with the help of labeled seed documents, semi-supervised id91 can be
used [basu et al. 2002]. when the seeds are not available, we can use side information as constraints
to guide id91 algorithms [basu et al. 2004]. when the supervision from target domain is not
available, we can also perform id21 to transfer the labeled information from other do-
mains to the target domain [dai et al. 2007a; wang et al. 2009]. all the above id91 algorithms
with supervision need domain or relevant domain knowledge. when there are diverse domains and
the supervision is needed, they will still be very costly to ask a lot of different domain experts to
label.

7.2. machine learning with domain knowledge
the general idea of incorporating domain knowledge into machine learning algorithms has al-
ready been studied extensively in natural language processing community. chang, ratinov and
roth [chang et al. 2012] presented constrained conditional models (ccms) that allow to inject
high-level knowledge as soft constraints into linear models, such as id48 and
structured id88, for lots of natural language processing tasks, including information extrac-
tion [roth and yih 2004; roth and yih 2007], id14 [roth and yih 2005; pun-
yakanok et al. 2008], summarization [clarke and lapata 2006], and co-reference resolution [denis
et al. 2007]. posterior id173 (pr) [ganchev et al. 2010] works on incorporating indirect su-
pervision via constraints on posterior distributions of probabilistic models with latent variables. the
key difference between ccm based learning and pr is that ccms allows the use of hard constraints,
while pr uses expectation constraints. samdani et al. [samdani et al. 2012] have proposed an uni-
   ed expectation-maximization algorithm that can cover ccm based learning and pr. besides, lots
of models have been studied to incorporate the domain knowledge for better performance. markov
logic network (mln) [richardson and domingos 2006] is proposed to integrate    rst-order logic
with markov random field. a combination of the id110 model with a collection of de-
terministic constraints is presented in [dechter and mateescu 2004]. however, all of the mentioned
work use domain knowledge to improve the performance of the corresponding machine learning
algorithms. different from their work, we explore a more general learning framework with world
knowledge. given domain-dependent data, we aim to automatically generate domain knowledge
by specifying the world knowledge, and represent the domain knowledge in an uni   ed format for
more general machine learning. it will be interesting to use our learning framework in more learn-
ing models, to conduct empirical experiments to compare to ccm and pr based models in various
domain-dependent tasks.

besides, id21 [pan and yang 2010b] is another direction on leveraging domain knowl-
edge for better machine learning. the main idea of id21 is to leverage the domain knowl-

26

c. wang et al.

(a) constraints of type #1 of    chinc + free-
base    for 20ng.

(b) constraints of types #1+#2 of    chinc +
freebase    for 20ng.

(c) constraints of types #1+#2+#3 of    chinc
+ freebase    for 20ng.

(d) constraints of type #1 of    chinc + free-
base    for mcat.

(e) constraints of types #1+#2 of    chinc +
freebase    for mcat.

(f) constraints of types #1+#2+#3 of    chinc
+ freebase    for mcat.

(g) constraints of type #1 of    chinc + free-
base    for ccat.

(h) constraints of types #1+#2 of    chinc +
freebase    for ccat.

(i) constraints of types #1+#2+#3 of    chinc
+ freebase    for ccat.

(j) constraints of type #1 of    chinc + free-
base    for ecat.
fig. 15: effects of entity constraints and world knowledge source (freebase). left y-axis: average
nmi; right y-axis: average execution time (ms).

(k) constraints of types #1+#2 of    chinc +
freebase    for ecat.

(l) constraints of types #1+#2+#3 of    chinc
+ freebase    for ecat.

world knowledge as indirect supervision for document id91

27

(a) constraints of type #1 of    chinc +
yago2    for 20ng.

(b) constraints of types #1+#2 of    chinc +
yago2    for 20ng.

(c) constraints of types #1+#2+#3 of    chinc
+ yago2    for 20ng.

(d) constraints of type #1 of    chinc +
yago2    for mcat.

(e) constraints of types #1+#2 of    chinc +
yago2    for mcat.

(f) constraints of types #1+#2+#3 of    chinc
+ yago2    for mcat.

(g) constraints of type #1 of    chinc +
yago2    for ccat.

(h) constraints of types #1+#2 of    chinc +
yago2    for ccat.

(i) constraints of types #1+#2+#3 of    chinc
+ yago2    for ccat.

(j) constraints of type #1 of    chinc +
yago2    for ecat.
fig. 16: effects of entity constraints and world knowledge source (yago2). left y-axis: average
nmi; right y-axis: average execution time (ms).

(k) constraints of types #1+#2 of    chinc +
yago2    for ecat.

(l) constraints of types #1+#2+#3 of    chinc
+ yago2    for ecat.

28

c. wang et al.

fig. 17: analysis of the ef   ciency of our algorithm on different document datasets with different
world knowledge sources.

edge in source domain to help the learning tasks in the target domain. the key intuition is that the
labeled data is relatively easier to collect in the source domain than in the target domain. unsuper-
vised id21 [dai et al. 2008] has been adapted for developing new id91 algorithms.
self-taught id91 [dai et al. 2008] is an instance of unsupervised id21, which aims
at id91 a small collection of unlabeled data in the target domain with the help of a large amount
of unlabeled data in the source domain. besides unsupervised id21, there are inductive
id21 [dai et al. 2007b; mihalkova et al. 2007] and transductive id21 [arnold
et al. 2007; ling et al. 2008] for the problem of regression and classi   cation. recently, source-free
id21 [lu et al. 2014] and selective id21 [lu et al. 2013] have been proposed
to address the text classi   cation and cross domain recommendation problems, and shown improved
results respectively. lifelong learning [eaton and ruvolo 2013; chen and liu 2014] is also a frame-
work of machine learning with domain knowledge. lifelong learning test [li and yang 2015] has
been proposed to take both the current performance and the performance growth rate into consid-
eration. however, both id21 and lifelong learning are focusing on leveraging domain
knowledge rather than world knowledge, whereas our framework is focusing on    rst how to specify
the world knowledge to automatically generate domain knowledge and then modeling the learning
task(s) with the help of the speci   ed world knowledge. again, it will be of great interests to build an
end-to-end machine learning system by using the automatically generated domain knowledge while
conducting id21 or lifelong learning.

7.3. machine learning with world knowledge
most of the existing usage of world knowledge is to enrich the features beyond bag-of-words rep-
resentation of documents. for example, by using the linguistic knowledge base id138 to re-
solve synonyms and introduce id138 concepts, the quality of document id91 can be im-
proved [hotho et al. 2003]. the    rst paper using the term    world knowledge    [gabrilovich and
markovitch 2005] extends the bag-of-words features with the categories in open directory project
(odp), and shows that it can help improve text classi   cation with additional knowledge. following
this, by mapping the text to the semantic space provided by wikipedia pages or other ontologies,
it has been proven to be useful for short text classi   cation [gabrilovich and markovitch 2006;
gabrilovich and markovitch 2007; gabrilovich and markovitch 2009], id91 [hu et al. 2008;
hu et al. 2009a; hu et al. 2009b; fodeh et al. 2011], and information retrieval [egozi et al. 2011].
after that, a bunch of research also uses another knowledge base of taxonomy, probase [wu et al.
2012], to enrich the features of short text or keywords for classi   cation [wang et al. 2014a], clus-
tering [song et al. 2011; song et al. 2015], information retrieval [hua et al. 2013; song et al. 2014;

world knowledge as indirect supervision for document id91

29

wang et al. 2016], or mines knowledge from text for information retrieval [wang et al. 2013]. all
of the above approaches just consider to use world knowledge as a source of features. however, the
knowledge in the knowledge bases indeed has annotations of types, categories, etc.. thus, it can
be more effective to consider this information as    supervision    to supervise other machine learning
algorithms and tasks. along this research direction, recent work [wang et al. 2015b; wang et al.
2016] apply our world knowledge enabled machine learning framework for the document similarity
computation and classi   cation tasks.

distant supervision uses the knowledge of entities and their relationships from world knowledge
bases, e.g., freebase, as supervision for the task of entity and id36 [mintz et al. 2009;
wang et al. 2014b; xu et al. 2014]. it considers to use knowledge supervision to extract more entities
and relations from new text or to generate a better embedding of entities and relations. thus, the
application of direct supervision is limited to entities and relations themselves.

song et al. [song et al. 2013] consider using fully unsupervised method to generate constraints
of words using an external general-purpose knowledge base, id138. this can be regarded as an
initial attempt to use general knowledge as indirect supervision to help id91. however, the
knowledge from id138 is mostly linguistically related. it lacks of the information about named
entities and their types. moreover, their approach is still a simple application of constrained co-
id91, where it misses the rich structural information in the knowledge base.

7.4. heterogeneous information network
a heterogeneous information network (hin) is de   ned as a graph of multi-typed entities and rela-
tions [han et al. 2010; sun and han 2012]. different from traditional graphs, hin incorporates the
type information which can be useful to identify the semantic meaning of the paths in the graph [sun
et al. 2011b]. this is a good property to perform graph search and matching [he and singh 2006;
yan et al. 2004; zhang et al. 2007]. original hins are developed for the applications of scien-
ti   c publication network analysis [zhao et al. 2009; sun et al. 2011a; sun et al. 2011b; sun et al.
2012]. then social network analysis also leverages this representation for user similarity and link
prediction [kong et al. 2013; zhang et al. 2013; zhang et al. 2014]. seaid113ssly, we can see that
the knowledge in world knowledge bases, e.g., freebase and yago2, can be naturally represented
as an hin, since the entities and relations in the knowledge base are all typed. we introduce this
representation to knowledge based analysis, and show that it can be very useful for our document
id91 task. note that there is also a series of methods called multi-type relational data clus-
tering [long et al. 2006; long et al. 2007] and collective id105 [singh and gordon
2008; nickel et al. 2011; bouchard et al. 2013; klami et al. 2013]. while they require the data to
be structural beforehand (e.g., providing information of authors, co-authors, etc.), our method only
needs the input of raw documents. in addition to the multi-type relational information, we also in-
corporate the type information provided by the knowledge base as constraints to further improve the
id91 results.

8. conclusion
in this paper, we study a novel problem of machine learning with world knowledge. particularly, we
take document id91 as an example and show how to use world knowledge as indirect supervi-
sion to improve the id91 results. to use the world knowledge, we show how to adapt the world
knowledge to domain dependent tasks by using id29 and semantic    ltering. then we
represent the data as a heterogeneous information network, and use a constrained network id91
algorithm to obtain the document clusters. we demonstrate the effectiveness and ef   ciency of our
approach on two real datasets along with two popular knowledge bases. in the future, we plan to
use world knowledge to help more id111 and text analytics tasks, such as text classi   cation
and information retrieval.

30

c. wang et al.

acknowledgments
chenguang wang gratefully acknowledges the support by the national natural science foundation
of china (nsfc grant number 61472006) and the national basic research program (973 program
no. 2014cb340405). the research is also partially supported by the army research laboratory
(arl) under agreement w911nf-09-2-0053, and by darpa under agreement number fa8750-
13-2-0008. research is also partially sponsored by china national 973 project 2014cb340304,
u.s. national science foundation iis-1320617, iis-1354329 and iis 16-18481, hdtra1-10-1-
0120, and grant 1u54gm114838 awarded by nigms through funds provided by the trans-nih
big data to knowledge (bd2k) initiative (www.bd2k.nih.gov), and mias, a dhs-ids center
for multimodal information access and synthesis at uiuc. the views and conclusions contained
herein are those of the authors and should not be interpreted as necessarily representing the of   cial
policies or endorsements, either expressed or implied by these agencies or the u.s. government.

references
andrew arnold, ramesh nallapati, and william w cohen. 2007. a comparative study of methods for transductive transfer

learning. in icdm workshops. 77   82.

s  oren auer, christian bizer, georgi kobilarov, jens lehmann, richard cyganiak, and zachary ives. 2007. dbpedia: a

nucleus for a web of open data. springer.

michele banko, michael j. cafarella, stephen soderland, matthew broadhead, and oren etzioni. 2007. open information

extraction from the web. in ijcai. 2670   2676.

sugato basu, arindam banerjee, and raymond j. mooney. 2002. semi-supervised id91 by seeding. in icml. 27   34.
sugato basu, mikhail bilenko, and raymond j mooney. 2004. a probabilistic framework for semi-supervised id91. in

kdd. 59   68.

jonathan berant, andrew chou, roy frostig, and percy liang. 2013. id29 on freebase from question-answer

pairs. in emnlp. 1533   1544.

jonathan berant and percy liang. 2014. id29 via id141. in acl. 1415   1425.
kurt d. bollacker, colin evans, praveen paritosh, tim sturge, and jamie taylor. 2008. freebase: a collaboratively created

graph database for structuring human knowledge. in sigmod. 1247   1250.

guillaume bouchard, dawei yin, and shengbo guo. 2013. convex collective id105. in aistats. 144   152.
qingqing cai and alexander yates. 2013. large-scale id29 via schema matching and lexicon extension. in

acl. 423   433.

ming-wei chang, lev ratinov, and dan roth. 2012. structured learning with constrained conditional models. machine

learning 88, 3 (2012), 399   431.

o. chapelle, b. sch  olkopf, and a. zien (eds.). 2006. semi-supervised learning. mit press.
zhiyuan chen and bing liu. 2014. id96 using topics from many domains, lifelong learning and big data. in icml.

703   711.

james clarke and mirella lapata. 2006. constraint-based sentence compression an integer programming approach. in acl.

144   151.

michael collins and nigel duffy. 2002. convolution kernels for natural language. in nips, t.g. dietterich, s. becker, and

z. ghahramani (eds.). 625   632.

wenyuan dai, gui-rong xue, qiang yang, and yong yu. 2007a. co-id91 based classi   cation for out-of-domain

documents. in kdd. 210   219.

wenyuan dai, qiang yang, gui-rong xue, and yong yu. 2007b. boosting for id21. in proceedings of the 24th

international conference on machine learning. acm, 193   200.

wenyuan dai, qiang yang, gui-rong xue, and yong yu. 2008. self-taught id91. in icml. 200   207.
rina dechter and robert mateescu. 2004. mixtures of deterministic-probabilistic networks and their and/or search space.

in auai. 120   129.

pascal denis, jason baldridge, and others. 2007. joint determination of anaphoricity and coreference resolution using

integer programming.. in naacl. 236   243.

inderjit s dhillon, subramanyam mallela, and dharmendra s modha. 2003. information-theoretic co-id91. in kdd.

89   98.

xin dong, evgeniy gabrilovich, geremy heitz, wilko horn, ni lao, kevin murphy, thomas strohmann, shaohua sun, and

wei zhang. 2014. knowledge vault: a web-scale approach to probabilistic knowledge fusion. in kdd. 601   610.

eric eaton and paul l ruvolo. 2013. ella: an ef   cient lifelong learning algorithm. in proceedings of the 30th international

conference on machine learning (icml-13). 507   515.

world knowledge as indirect supervision for document id91

31

ofer egozi, shaul markovitch, and evgeniy gabrilovich. 2011. concept-based information retrieval using explicit seman-

tic analysis. acm trans. inf. syst. (tois) 29, 2 (2011), 8.

oren etzioni, michael cafarella, and doug downey. 2004. webscale information extraction in knowitall (preliminary

results). in www. 100   110.

anthony fader, stephen soderland, and oren etzioni. 2011. identifying relations for id10. in

emnlp. 1535   1545.

anthony fader, luke s. zettlemoyer, and oren etzioni. 2013. paraphrase-driven learning for open id53.

in acls. 1608   1618.

christiane fellbaum (ed.). 1998. id138: an electronic lexical database. mit press.
samah fodeh, bill punch, and pang-ning tan. 2011. on ontology-driven document id91 using core semantic fea-

tures. knowl. inf. syst. 28, 2 (2011), 395   421.

evgeniy gabrilovich and shaul markovitch. 2005. feature generation for text categorization using world knowledge. in

ijcai. 1048   1053.

evgeniy gabrilovich and shaul markovitch. 2006. overcoming the brittleness bottleneck using wikipedia: enhancing text

categorization with encyclopedic knowledge. in aaai. 1301   1306.

evgeniy gabrilovich and shaul markovitch. 2007. computing semantic relatedness using wikipedia-based explicit se-

mantic analysis.. in ijcai. 1606   1611.

evgeniy gabrilovich and shaul markovitch. 2009. wikipedia-based semantic interpretation for natural language process-

ing. (jair) 34 (2009), 443   498.

kuzman ganchev, joao grac  a, jennifer gillenwater, and ben taskar. 2010. posterior id173 for structured latent

variable models. jmlr 11 (2010), 2001   2049.

jiawei han, yizhou sun, xifeng yan, and philip s. yu. 2010. mining knowledge from databases: an information network

analysis approach. in sigmod. 1251   1252.

huahai he and ambuj k singh. 2006. closure-tree: an index structure for graph queries. in icde. ieee, 38   38.
andreas hotho, steffen staab, and gerd stumme. 2003. ontologies improve text document id91. in icdm. 541   544.
jian hu, lujun fang, yang cao, hua-jun zeng, hua li, qiang yang, and zheng chen. 2008. enhancing text id91 by

leveraging wikipedia semantics. in sigir. 179   186.

xia hu, nan sun, chao zhang, and tat-seng chua. 2009a. exploiting internal and external semantics for the id91 of

short texts using world knowledge. in cikm. 919   928.

xiaohua hu, xiaodan zhang, caimei lu, e. k. park, and xiaohua zhou. 2009b. exploiting wikipedia as external knowledge

for document id91. in kdd. 389   396.

wen hua, yangqiu song, haixun wang, and xiaofang zhou. 2013. identifying users    topical tasks in web search. in

wsdm. 93   102.

arto klami, guillaume bouchard, and abhishek tripathi. 2013. group-sparse embeddings in collective id105.

arxiv (2013).

xiangnan kong, jiawei zhang, and philip s. yu. 2013. inferring anchor links across multiple heterogeneous social networks.

in cikm. 179   188.

jayant krishnamurthy and tom m. mitchell. 2012. weakly supervised training of semantic parsers. in emnlp-conll.

754   765.

tom kwiatkowski, eunsol choi, yoav artzi, and luke s. zettlemoyer. 2013. scaling semantic parsers with on-the-fly

ontology matching. in emnlp. 1545   1556.

tom kwiatkowski, luke s. zettlemoyer, sharon goldwater, and mark steedman. 2011. lexical generalization in id35

grammar induction for id29. in emnlp. 1512   1523.

ken lang. 1995. newsweeder: learning to    lter netnews. in icml. 331   339.
douglas b. lenat and r. v. guha. 1989. building large knowledge-based systems: representation and id136 in the cyc

project. addison-wesley.

david d lewis, yiming yang, tony g rose, and fan li. 2004. rcv1: a new benchmark collection for text categorization

research. jmlr 5 (2004), 361   397.

lianghao li and qiang yang. 2015. lifelong machine learning test.. in aaai workshop.
yang li, chi wang, fangqiu han, jiawei han, dan roth, and xifeng yan. 2013. mining evidences for named entity disam-

biguation. in kdd. 1070   1078.

percy liang. 2013. lambda dependency-based id152. arxiv (2013).
xiao ling, wenyuan dai, gui-rong xue, qiang yang, and yong yu. 2008. spectral domain-id21. in kdd.

488   496.

yanchi liu, zhongmou li, hui xiong, xuedong gao, junjie wu, and sen wu. 2013. understanding and enhancement of

internal id91 validation measures. ieee transactions on cybernetics 43, 3 (2013), 982   994.

32

c. wang et al.

bo long, zhongfei (mark) zhang, xiaoyun w  u, and philip s. yu. 2006. spectral id91 for multi-type relational data.

in icml. 585   592.

bo long, zhongfei mark zhang, and philip s. yu. 2007. a probabilistic framework for relational id91. in kdd.

470   479.

zhongqi lu, weike pan, evan wei xiang, qiang yang, lili zhao, and erheng zhong. 2013. selective id21 for

cross domain recommendation.. in sdm. 641   649.

zhongqi lu, yin zhu, sinno jialin pan, evan wei xiang, yujing wang, and qiang yang. 2014. source free id21

for text classi   cation.. in aaai. 122   128.

ping luo, hui xiong, guoxing zhan, junjie wu, and zhongzhi shi. 2009. information-theoretic distance measures for

id91 validation: generalization and id172. ieee tkde 21, 9 (2009), 1249   1262.

mausam, michael schmitz, stephen soderland, robert bart, and oren etzioni. 2012. open language learning for informa-

tion extraction. in emnlp-conll. 523   534.

lilyana mihalkova, tuyen huynh, and raymond j mooney. 2007. mapping and revising markov logic networks for transfer

learning. in aaai, vol. 7. 608   614.

mike mintz, steven bills, rion snow, and dan jurafsky. 2009. distant supervision for id36 without labeled

data. in acl/afnlp. 1003   1011.

tom m. mitchell, william w. cohen, estevam r. hruschka jr., partha pratim talukdar, justin betteridge, andrew carlson,
bhavana dalvi mishra, matthew gardner, bryan kisiel, jayant krishnamurthy, ni lao, kathryn mazaitis, thahir mo-
hamed, ndapandula nakashole, emmanouil antonios platanios, alan ritter, mehdi samadi, burr settles, richard c.
wang, derry tanti wijaya, abhinav gupta, xinlei chen, abulhair saparov, malcolm greaves, and joel welling. 2015.
never-ending learning. in aaai. 2302   2310.

raymond j. mooney. 2007. learning for id29. in cicling. 311   324.
maximilian nickel, volker tresp, and hans-peter kriegel. 2011. a three-way model for collective learning on multi-

relational data. in icml. 809   816.

sinno jialin pan and qiang yang. 2010a. a survey on id21. ieee tkde 22, 10 (2010), 1345   1359.
sinno jialin pan and qiang yang. 2010b. a survey on id21. tkde 22, 10 (2010), 1345   1359.
simone paolo ponzetto and michael strube. 2007. deriving a large-scale taxonomy from wikipedia. in aaai. 1440   1445.
vasin punyakanok, dan roth, and wen-tau yih. 2008. the importance of syntactic parsing and id136 in semantic role

labeling. computational linguistics 34, 2 (2008), 257   287.

lev ratinov and dan roth. 2009. design challenges and misconceptions in id39. in conll. 147   155.
siva reddy, mirella lapata, and mark steedman. 2014. large-scale id29 without question-answer pairs. tacl

2 (2014), 377   392.

matthew richardson and pedro domingos. 2006. markov logic networks. machine learning 62, 1-2 (2006), 107   136.
dan roth and wen-tau yih. 2004. a id135 formulation for global id136 in natural language tasks. (2004),

1   8.

dan roth and wen-tau yih. 2005. integer id135 id136 for conditional random    elds. in icml. 736   743.
dan roth and wen-tau yih. 2007. global id136 for entity and relation identi   cation via a id135 formula-

tion. introduction to statistical relational learning (2007), 553   580.

rajhans samdani, ming-wei chang, and dan roth. 2012. uni   ed expectation maximization. in naacl. 688   698.
ajit p singh and geoffrey j gordon. 2008. relational learning via collective id105. in kdd. 650   658.
yangqiu song, shixia liu, xueqing liu, and haixun wang. 2015. automatic taxonomy construction from keywords via

scalable bayesian rose trees. tkde 27, 7 (2015), 1861   1874.

yangqiu song, shimei pan, shixia liu, furu wei, m.x. zhou, and weihong qian. 2013. constrained text coid91 with

supervised and unsupervised constraints. ieee tkde 25, 6 (2013), 1227   1239.

yangqiu song, haixun wang, weizhu chen, and shusen wang. 2014. transfer understanding from head queries to tail

queries. in cikm. 1299   1308.

yangqiu song, haixun wang, zhongyuan wang, hongsong li, and weizhu chen. 2011. short text conceptualization using

a probabilistic knowledgebase. in ijcai. 2330   2336.

yangqiu song, shusen wang, and haixun wang. 2015. open domain short text conceptualization: a generative + de-

scriptive modeling approach. in ijcai. 3820   3826.

alexander strehl and joydeep ghosh. 2003. cluster ensembles   a knowledge reuse framework for combining multiple

partitions. jmlr 3 (2003), 583   617.

fabian m suchanek, gjergji kasneci, and gerhard weikum. 2007. yago: a core of semantic knowledge. in www. 697   706.
yizhou sun, rick barber, manish gupta, charu c aggarwal, and jiawei han. 2011a. co-author relationship prediction in

heterogeneous bibliographic networks. in asonam. ieee, 121   128.

world knowledge as indirect supervision for document id91

33

yizhou sun and jiawei han. 2012. mining heterogeneous information networks: principles and methodologies. synthesis

lectures on data mining and knowledge discovery 3, 2 (2012), 1   159.

yizhou sun, jiawei han, xifeng yan, philip s. yu, and tianyi wu. 2011b. pathsim: meta path-based top-k similarity

search in heterogeneous information networks. pvldb (2011), 992   1003.

yizhou sun, brandon norick, jiawei han, xifeng yan, philip s. yu, and xiao yu. 2012. integrating meta-path selection with

user-guided object id91 in heterogeneous information networks. in kdd. 1348   1356.

s. v. n. vishwanathan, nicol n. schraudolph, risi kondor, and karsten m. borgwardt. 2010. graph kernels. jmlr 11

(aug. 2010), 1201   1242.

chenguang wang, nan duan, ming zhou, and ming zhang. 2013. id141 adaptation for web search ranking.. in

acl. 41   46.

chenguang wang, yangqiu song, ahmed el-kishky, dan roth, ming zhang, and jiawei han. 2015a. incorporating world

knowledge to document id91 via heterogeneous information networks. in kdd. 1215   1224.

chenguang wang, yangqiu song, haoran li, ming zhang, and jiawei han. 2015b. knowsim: a document similarity

measure on structured heterogeneous information networks. in icdm. 1015   1020.

chenguang wang, yangqiu song, haoran li, ming zhang, and jiawei han. 2016. text classi   cation with heterogeneous

information network kernels. in thirtieth aaai conference on arti   cial intelligence.

chenguang wang, yangqiu song, dan roth, chi wang, jiawei han, heng ji, and ming zhang. 2015. constrained

information-theoretic tripartite graph id91 to identify semantically similar relations. in ijcai. 3882   3889.

chenguang wang, yizhou sun, yanglei song, jiawei han, yangqiu song, lidan wang, and ming zhang. 2016. relsim:

relation similarity search in schema-rich heterogeneous information networks. (2016).

zheng wang, yangqiu song, and changshui zhang. 2009. knowledge transfer on hybrid graph. in ijcai. 1291   1296.
zhongyuan wang, fang wang, ji-rong wen, and zhoujun li. 2014a. concept-based short text classi   cation and ranking.

in cikm. 1069   1078.

zhen wang, jianwen zhang, jianlin feng, and zheng chen. 2014b. id13 and text jointly embedding. in emnlp.

1591   1601.

junjie wu, hui xiong, and jian chen. 2009. adapting the right measures for id116 id91. in kdd. 877   886.
wentao wu, hongsong li, haixun wang, and kenny q. zhu. 2012. probase: a probabilistic taxonomy for text under-

standing. in sigmod. 481   492.

chang xu, yalong bai, jiang bian, bin gao, gang wang, xiaoguang liu, and tie-yan liu. 2014. rc-net: a general frame-

work for incorporating knowledge into word representations. in cikm. 1219   1228.

xifeng yan, philip s yu, and jiawei han. 2004. graph indexing: a frequent structure-based approach. in sigmod. acm,

335   346.

xuchen yao and benjamin van durme. 2014. information extraction over structured data: id53 with free-

base. in acl. 956   966.

jiawei zhang, xiangnan kong, and philip s. yu. 2013. predicting social links for new users across aligned heterogeneous

social networks. in icdm. 1289   1294.

jiawei zhang, xiangnan kong, and philip s. yu. 2014. transferring heterogeneous links across location-based social net-

works. in wsdm. 303   312.

shijie zhang, meng hu, and jiong yang. 2007. treepi: a novel graph indexing method. in icde. 966   975.
peixiang zhao, jiawei han, and yizhou sun. 2009. p-rank: a comprehensive structural similarity measure over information

networks. in cikm. 553   562.

