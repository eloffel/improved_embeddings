id33 as head selection

xingxing zhang, jianpeng cheng and mirella lapata

institute for language, cognition and computation

school of informatics, university of edinburgh

{x.zhang,jianpeng.cheng}@ed.ac.uk, mlap@inf.ed.ac.uk

10 crichton street, edinburgh eh8 9ab

6
1
0
2
 
c
e
d
2
2

 

 
 
]
l
c
.
s
c
[
 
 

4
v
0
8
2
1
0

.

6
0
6
1
:
v
i
x
r
a

abstract

conventional graph-based dependency
parsers guarantee a tree structure both
during training and id136. instead, we
formalize id33 as the prob-
lem of independently selecting the head
of each word in a sentence. our model
which we call dense (as shorthand for
dependency neural selection) produces
a distribution over possible heads for
each word using features obtained from
a bidirectional recurrent neural network.
without enforcing structural constraints
during training, dense generates (at
id136 time) trees for the overwhelm-
ing majority of sentences, while non-tree
outputs can be adjusted with a maximum
spanning tree algorithm. we evaluate
dense on four languages (english, chi-
nese, czech, and german) with varying
degrees of non-projectivity. despite the
simplicity of the approach, our parsers are
on par with the state of the art.1

1

introduction

id33 plays an important role in
many natural language applications, such as re-
lation extraction (fundel et al., 2007), machine
translation (carreras and collins, 2009), language
modeling (chelba et al., 1997; zhang et al., 2016)
and ontology construction (snow et al., 2005). de-
pendency parsers represent syntactic information
as a set of head-dependent relational arcs, typi-
cally constrained to form a tree. practically all
models proposed for id33 in recent
years can be described as graph-based (mcdon-

1our

code

is

available

at

http://github.com/

xingxingzhang/dense_parser.

ald et al., 2005a) or transition-based (yamada and
matsumoto, 2003; nivre et al., 2006b).

graph-based dependency parsers are typically
arc-factored, where the score of a tree is de   ned
as the sum of the scores of all its arcs. an arc
is scored with a set of local features and a lin-
ear model, the parameters of which can be effec-
tively learned with online algorithms (crammer
and singer, 2001; crammer and singer, 2003; fre-
und and schapire, 1999; collins, 2002).
in or-
der to ef   ciently    nd the best scoring tree during
training and decoding, various maximization algo-
rithms have been developed (eisner, 1996; eisner,
2000; mcdonald et al., 2005b). in general, graph-
based methods are optimized globally, using fea-
tures of single arcs in order to make the learn-
ing and id136 tractable. transition-based algo-
rithms factorize a tree into a set of parsing actions.
at each transition state, the parser scores a candi-
date action conditioned on the state of the transi-
tion system and the parsing history, and greedily
selects the highest-scoring action to execute. this
score is typically obtained with a classi   er based
on non-local features de   ned over a rich history of
parsing decisions (yamada and matsumoto, 2003;
zhang and nivre, 2011).

the

regardless of

algorithm used, most
well-known dependency parsers,
such as the
mst-parser (mcdonald et al., 2005b) and the
maltpaser (nivre et al., 2006a), rely on exten-
sive feature engineering. feature templates are
typically manually designed and aim at captur-
ing head-dependent relationships which are no-
toriously sparse and dif   cult to estimate. more
recently, a few approaches (chen and manning,
2014; lei et al., 2014a; kiperwasser and gold-
berg, 2016) apply neural networks for learning
dense feature representations. the learned fea-
tures are subsequently used in a conventional
graph- or transition-based parser, or better de-

signed variants (dyer et al., 2015).

in this work, we propose a simple neural
network-based model which learns to select the
head for each word in a sentence without enforc-
ing tree structured output. our model which we
call dense (as shorthand for dependency neural
selection) employs bidirectional recurrent neural
networks to learn feature representations for words
in a sentence. these features are subsequently
used to predict the head of each word. although
there is nothing inherent in the model to enforce
tree-structured output, when tested on an english
dataset, it is able to generate trees for 95% of the
sentences, 87% of which are projective. the re-
maining non-tree (or non-projective) outputs are
post-processed with the chu-liu-edmond (or eis-
ner) algorithm. dense uses the head selection
procedure to estimate arc weights during training.
during testing, it essentially reduces to a standard
graph-based parser when it fails to produce tree (or
projective) output.

we evaluate our model on benchmark depen-
dency parsing corpora,
representing four lan-
guages (english, chinese, czech, and german)
with varying degrees of non-projectivity. despite
the simplicity of our approach, experiments show
that the resulting parsers are on par with the state
of the art.

2 related work

graph-based parsing graph-based
depen-
dency parsers employ a model
scoring
for
possible dependency graphs for a given sen-
tence. the graphs are typically factored into
their component arcs and the score of a tree is
de   ned as the sum of its arcs. this factorization
enables tractable search for the highest scoring
graph structure which is commonly formulated
as the search for the maximum spanning tree
(mst). the chu-liu-edmonds algorithm (chu
and liu, 1965; edmonds, 1967; mcdonald et
al., 2005b) is often used to extract the mst in
the case of non-projective trees, and the eisner
algorithm (eisner, 1996; eisner, 2000) in the
case of projective trees. during training, weight
parameters of the scoring function can be learned
with margin-based algorithms (crammer and
singer, 2001; crammer and singer, 2003) or
the structured id88 (freund and schapire,
1999; collins, 2002). beyond basic    rst-order
models, the literature offers a few examples of

higher-order models involving sibling and grand
parent relations (carreras, 2007; koo et al., 2010;
zhang and mcdonald, 2012). although more
expressive, such models render both training and
id136 more challenging.

transition-based parsing as the term implies,
transition-based parsers conceptualize the process
of transforming a sentence into a dependency tree
as a sequence of transitions. a transition sys-
tem typically includes a stack for storing partially
processed tokens, a buffer containing the remain-
ing input, and a set of arcs containing all depen-
dencies between tokens that have been added so
far (nivre, 2003; nivre et al., 2006b). a de-
pendency tree is constructed by manipulating the
stack and buffer, and appending arcs with prede-
termined operations. most popular parsers em-
ploy an arc-standard (yamada and matsumoto,
2003; nivre, 2004) or arc-eager transition system
(nivre, 2008). extensions of the latter include the
use of non-local training methods to avoid greedy
error propagation (zhang and clark, 2008; huang
and sagae, 2010; zhang and nivre, 2011; gold-
berg and nivre, 2012).

neural network-based features neural net-
work representations have a long history in syn-
tactic parsing (mayberry and miikkulainen, 1999;
henderson, 2004; titov and henderson, 2007).
recent work uses neural networks in lieu of
the linear classi   ers typically employed in con-
ventional transition- or graph-based dependency
parsers. for example, chen and manning (2014)
use a feed forward neural network to learn fea-
tures for a transition-based parser, whereas lei
et al. (2014a) do the same for a graph-based
parser. lei et al. (2014b) apply tensor decompo-
sition to obtain id27s in their syntac-
tic roles, which they subsequently use in a graph-
based parser. dyer et al. (2015) redesign com-
ponents of a transition-based system where the
buffer, stack, and action sequences are modeled
separately with stack long short-term memory net-
works. the hidden states of these lstms are con-
catenated and used as features to a    nal transi-
tion classi   er. kiperwasser and goldberg (2016)
use bidirectional lstms to extract features for a
transition- and graph-based parser, whereas cross
and huang (2016) build a greedy arc-standard
parser using similar features.

in our work, we formalize id33

as the task of    nding for each word in a sentence
its most probable head. both head selection and
the features it is based on are learned using neu-
ral networks. the idea of modeling child-parent
relations independently dates back to hall (2007)
who use an edge-factored model to generate k-best
parse trees which are subsequently reranked us-
ing a model based on rich global features. later
smith (2010) show that a head selection variant
of their loopy belief propagation parser performs
worse than a model which incorporates tree struc-
ture constraints. our parser is conceptually sim-
pler: we rely on head selection to do most of
the work and decode the best tree directly with-
out using a reranker. in common with recent neu-
ral network-based dependency parsers, we aim to
alleviate the need for hand-crafting feature com-
binations. beyond id171, we further
show that it is possible to simplify the training of
a graph-based dependency parser in the context of
id182.

3 id33 as head selection

in this section we present our parsing model,
dense, which tries to predict the head of each
word in a sentence. speci   cally, the model takes
as input a sentence of length n and outputs n
(cid:104)head, dependent(cid:105) arcs. we describe the model
focusing on unlabeled dependencies and then dis-
cuss how it can be straightforwardly extended to
the labeled setting. we begin by explaining how
words are represented in our model and then give
details on how dense makes predictions based
on these learned representations. since there is
no guarantee that the outputs of dense are trees
(although they mostly are), we also discuss how
to extend dense in order to enforce projective
and non-projective tree outputs. throughout this
paper, lowercase boldface letters denote vectors
(e.g., v or vi), uppercase boldface letters denote
matrices (e.g., m or mb), and lowercase letters
denote scalars (e.g., w or wi).

3.1 word representation
let s = (w0, w1, . . . , wn ) denote a sentence of
length n; following common practice in the de-
pendency parsing literature (k  ubler et al., 2009),
we add an arti   cial root token represented by w0.
analogously, let a = (a0, a1, . . . , an ) denote the
representation of sentence s, with ai representing
(0     i     n ). besides encoding infor-
word wi

phead(root|love, s)

phead(candy|love, s)

phead(kids|love, s)

root

kids

love

candy

figure 1: dense estimates the id203 a word
being the head of another word based on bidirec-
tional lstm representations for the two words.
phead(root|love, s) is the id203 of root
being the head of love (dotted arcs denote candi-
date heads; the solid arc is the goldstandard).

mation about each wi in isolation (e.g., its lexical
meaning or pos tag), ai must also encode wi   s
positional information within the sentence. such
information has been shown to be important in de-
pendency parsing (mcdonald et al., 2005a). for
example, in the following sentence:

root a dog is chasing a cat

the head of the    rst a is dog, whereas the head of
the second a is cat. without considering positional
information, a model cannot easily decide which a
(nearer or farther) to assign to dog.

id137 (hochreiter
and schmidhuber, 1997; lstms), a type of re-
current neural network with a more complex com-
putational unit, have proven effective at capturing
long-term dependencies. in our case lstms al-
low to represent each word on its own and within
a sequence leveraging long-range contextual infor-
mation. as shown in figure 1, we    rst use a for-
ward lstm (lstmf ) to read the sentence from
left to right and then a backward lstm (lstmb)
to read the sentence from right to left, so that the
entire sentence serves as context for each word:2

(1)

i   1)

i   1, cf
i+1, cb

i = lstmf (xi, hf
i = lstmb(xi, hb

hf
i , cf
i , cb
hb
(2)
i    
where xi is the feature vector of word wi, hf
i     rd are the hidden states and mem-
rd and cf
ory cells for the ith word wi in lstmf and d is
2for more detail on id137, see e.g., graves

i+1)

(2012) or goldberg (2016).

the hidden unit size. hf
is also the representa-
i
tion for w0:i (wi and its left neighboring words)
and cf
is an internal state maintained by lstmf .
i
i     rd and cb
i     rd are the hidden states and
hb
memory cells for the backward lstmb. each to-
ken wi is represented by xi, the concatenation of
two vectors corresponding to wi   s lexical and pos
tag embeddings:

xi = [we    e(wi); wt    e(ti)]

(3)

where e(wi) and e(ti) are one-hot vector repre-
sentations of token wi and its pos tag ti; we    
rs  |v | and wt     rq  |t| are the word and pos
tag embedding matrices, where |v | is the vocab-
ulary size, s is the id27 size, |t| is
the pos tag set size, and q the tag embedding
size. the hidden states of the forward and back-
ward lstms are concatenated to obtain ai, the    -
nal representation of wi:

ai = [hf

i ; hb
i ]

i     [0, n ]

(4)

note that bidirectional lstms are one of many
possible ways of representing word wi. alterna-
tive representations include embeddings obtained
from feed-forward neural networks (chen and
manning, 2014; lei et al., 2014a), character-based
embeddings (ballesteros et al., 2015), and more
conventional features such as those introduced in
mcdonald et al. (2005a).

3.2 head selection
we now move on to discuss our formalization of
id33 as head selection. we begin
with unlabeled dependencies and then explain how
the model can be extended to predict labeled ones.
in a dependency tree, a head can have multiple
dependents, whereas a dependent can have only
one head. based on this fact, dependency pars-
ing can be formalized as follows. given a sen-
tence s = (w0, w1, . . . , wn ), we aim to    nd for
each word wi     {w1, w2, . . . , wn} the most prob-
able head wj     {w0, w1, . . . , wn}. for example,
in figure 1, to    nd the head for the token love,
we calculate probabilities phead(root|love, s),
phead(kids|love, s),
and phead(candy|love, s),
and select the highest. more formally, we estimate
the id203 of token wj being the head of to-
ken wi in sentence s as:

phead(wj|wi, s) =

exp(g(aj, ai))
k=0 exp(g(ak, ai))

(5)

(cid:80)n

a    tanh(ua    aj + wa    ai)

where ai and aj are vector-based representations
of wi and wj, respectively (described in sec-
tion 3.1); g(aj, ai) is a neural network with a
single hidden layer that computes the associative
score between representations ai and aj:
g(aj, ai) = v(cid:62)
(6)
where va     r2d, ua     r2d  2d, and wa    
r2d  2d are weight matrices of g. note that the
candidate head wj can be the root, while the de-
pendent wi cannot. equations (5) and (6) com-
pute the id203 of adding an arc between two
words, in a fashion similar to the neural atten-
tion mechanism in sequence-to-sequence models
(bahdanau et al., 2015).
we train our model by minimizing the neg-
ative log likelihood of the gold standard (cid:104)head,
dependent(cid:105) arcs in all training sentences:

(cid:88)

ns(cid:88)

s   t

i=1

j(  ) =     1
|t |

log phead(h(wi)|wi, s) (7)

where t is the training set, h(wi) is wi   s gold
standard head3 within sentence s, and ns the
number of words in s (excluding root). dur-
ing id136, for each word wi (i     [1, ns])
in s, we greedily choose the most likely head
wj (j     [0, ns]):

wj = arg max
wj :j   [0,ns ]

phead(wj|wi, s)

(8)

note that the prediction for each word wi is made
independently of the other words in the sentence.
given our greedy id136 method, there is no
guarantee that predicted (cid:104)head, dependent(cid:105) arcs
form a tree (maybe there are cycles). however,
we empirically observed that most outputs during
id136 are indeed trees. for instance, on an en-
glish dataset, 95% of the arcs predicted on the de-
velopment set are trees, and 87% of them are pro-
jective, whereas on a chinese dataset, 87% of the
arcs form trees, 73% of which are projective. this
indicates that although the model does not explic-
itly model tree structure during training, it is able
to    gure out from the data (which consists of trees)
that it should predict them.

so far we have focused on unlabeled depen-
dencies, however it is relatively straightforward
to extend dense to produce labeled dependen-
cies. we basically train an additional classi   er

3note that h(wi) can be root.

to predict labels for the arcs which have been
already identi   ed. the classi   er takes as input
features [ai; aj; xi; xj] representing properties of
the arc (cid:104)wj, wi(cid:105). these consist of ai and aj,
the lstm-based representations for wi and wj
(see equation (4)), and their word and part-of-
speech embeddings, xi and xj (see equation (3)).
speci   cally, we use a trained dense model to go
through the training corpus and generate features
and corresponding dependency labels as training
data. we employ a two-layer recti   er network
(glorot et al., 2011) for the classi   cation task.

3.3 maximum spanning tree algorithms
as mentioned earlier, greedy id136 may not
produce well-formed trees. in this case, the out-
put of dense can be adjusted with a maximum
spanning tree algorithm. we use the chu-liu-
edmonds algorithm (chu and liu, 1965; ed-
monds, 1967) for building non-projective trees and
the eisner (1996) algorithm for projective ones.

following mcdonald et al. (2005b), we view
a sentence s = (w0 = root, w1, . . . , wn ) as a
graph gs = (cid:104)vs, es(cid:105) with the sentence words
and the dummy root symbol as vertices and a di-
rected edge between every pair of distinct words
and from the root symbol to every word. the di-
rected graph gs is de   ned as:

vs = {w0 = root, w1, . . . , wn}
es = {(cid:104)i, j(cid:105) : i (cid:54)= j,(cid:104)i, j(cid:105)     [0, n ]    [1, n ]}

s(i, j) = phead(wi|wj, s)

(cid:104)i, j(cid:105)     es

a

is

build

the weight of edge (cid:104)i, j(cid:105)
where s(i, j)
and phead(wi|wj, s) is known. the problem of
id33 now boils down to    nding the
tree with the highest score which is equivalent to
   nding a mst in gs (mcdonald et al., 2005b).
non-projective parsing to
non-
projective parser, we solve the mst problem with
the chu-liu-edmonds algorithm (chu and liu,
1965; edmonds, 1967). the algorithm selects for
each vertex (excluding root) the in-coming edge
with the highest weight. if a tree results, it must
be the maximum spanning tree and the algorithm
terminates. otherwise,
there must be a cycle
which the algorithm identi   es, contracts into a
single vertex and recalculates edge weights going
into and out of the cycle. the greedy id136
strategy described in equation (8)) is essentially a
sub-procedure in the chu-liu-edmonds algorithm

with the algorithm terminating after the    rst
iteration.
in implementation, we only run the
chu-liu-edmonds algorithm through graphs with
cycles, i.e., non-tree outputs.

projective parsing for projective parsing, we
solve the mst problem with the eisner (1996) al-
gorithm. the time complexity of the eisner al-
gorithm is o(n 3), while checking if a tree is
projective can be done reasonably faster, with a
o(n log n ) algorithm. therefore, we only apply
the eisner algorithm to the non-projective output
of our greedy id136 strategy. finally, it should
be noted that the training of our model does not
rely on the chu-liu-edmonds or eisner algorithm,
or any other graph-based algorithm. mst algo-
rithms are only used at test time to correct non-tree
outputs which are a minority; dense acquires
underlying tree structure constraints from the data
without an explicit learning algorithm.

4 experiments

we evaluated our parser in a projective and non-
projective setting. in the following, we describe
the datasets we used and provide training details
for our models. we also present comparisons
against multiple previous systems and analyze the
parser   s output.

4.1 datasets
in the projective setting, we assessed the perfor-
mance of our parser on the english id32
(ptb) and the chinese treebank 5.1 (ctb). our
experimental setup closely follows chen and man-
ning (2014) and dyer et al. (2015).

for english, we adopted the stanford basic de-
pendencies (sd) representation (de marneffe et
al., 2006).4 we follow the standard splits of
ptb, sections 2   21 were used for training, sec-
tion 22 for development, and section 23 for test-
ing. pos tags were assigned using the stanford
tagger (toutanova et al., 2003) with an accuracy
of 97.3%. for chinese, we follow the same split
of ctb5 introduced in zhang and clark (2008). in
particular, we used sections 001   815, 1001   1136
for training, sections 886   931, 1148   1151 for
development, and sections 816   885, 1137   1147
for testing. the original constituency trees in
ctb were converted to dependency trees with the

4we obtained sd representations using the stanford

parser v.3.3.0.

# sentences

(%) projective

dev

test

dataset
english
chinese
czech
german

39,832
16,091
72,319
38,845

99.9
100.0
76.9
72.2

table 1: projective statistics on four datasets.
number of sentences and percentage of projective
trees are calculated on the training set.

penn2malt tool.5 we used gold segmentation and
gold pos tags as in chen and manning (2014) and
dyer et al. (2015).

in the non-projective setting, we assessed the
performance of our parser on czech and german,
the largest non-projective datasets released as part
of the conll 2006 multilingual dependency pars-
ing shared task. since there is no of   cial develop-
ment set in either dataset, we used the last 374/367
sentences in the czech/german training set as de-
velopment data.6 projective statistics of the four
datasets are summarized in table 1.

4.2 training details
we trained our models on an nvidia gpu card;
training takes one to two hours. model parameters
were uniformly initialized to [   0.1, 0.1]. we used
adam (kingma and ba, 2014) to optimize our
models with hyper-parameters recommended by
the authors (i.e., learning rate 0.001,    rst momen-
tum coef   cient 0.9, and second momentum coef-
   cient 0.999). to alleviate the gradient exploding
problem, we rescaled the gradient when its norm
exceeded 5 (pascanu et al., 2013). dropout (sri-
vastava et al., 2014) was applied to our model
with the strategy recommended in the literature
(zaremba et al., 2014; semeniuta et al., 2016).
on all datasets, we used two-layer lstms and set
d = s = 300, where d is the hidden unit size and
s is the id27 size.

as in previous neural id33 work
(chen and manning, 2014; dyer et al., 2015),
we used pre-trained word vectors to initialize our
id27 matrix we. for the ptb ex-
periments, we used 300 dimensional pre-trained
glove7 vectors (pennington et al., 2014). for
the ctb experiments, we trained 300 dimensional

5

http://stp.lingfil.uu.se/  nivre/research/

penn2malt.html

6we make the number of sentences in the development

and test sets comparable.

7http://nlp.stanford.edu/projects/glove/

92.00
93.20

uas
   
   
   
   

parser
bohnet10
martins13
z&m14
z&n11
c&m14
dyer15
   
weiss15
andor16
   
k&g16 graph    
k&g16 trans
   
dense-pei
90.77
dense-pei+e 91.39
94.17
dense
94.30
dense+e

89.70
90.90

las uas
    92.88
    92.89
    93.22
    93.00
91.80
93.10
    93.99
    94.61
    93.10
    93.90
90.39
91.00
94.02
94.10

88.35
88.94
91.82
91.95

las
90.71
90.55
91.02
90.95
89.60
90.90
92.05
92.79
91.00
91.90
88.05
88.61
91.84
91.90

table 2: results on english dataset (ptb with
stanford dependencies).
+e: we post-process
non-projective output with the eisner algorithm.

glove vectors on the chinese gigaword corpus
which we segmented with the stanford chinese
segmenter (tseng et al., 2005). for czech and
german, we did not use pre-trained word vectors.
the pos tag embedding size was set to q = 30
in the english experiments, q = 50 in the chinese
experiments and q = 40 in both czech and ger-
man experiments.

4.3 results
for both english and chinese experiments, we
report unlabeled (uas) and labeled attachment
scores (las) on the development and test sets; fol-
lowing chen and manning (2014) punctuation is
excluded from the evaluation.

experimental results on ptb are shown in ta-
ble 2. we compared our model with several recent
papers following the same evaluation protocol and
experimental settings. the    rst block in the table
contains mostly graph-based parsers which do not
use neural networks: bohnet10 (bohnet, 2010),
martins13 (martins et al., 2013), and z&m14
(zhang and mcdonald, 2014). z&n11 (zhang
and nivre, 2011) is a transition-based parser with
non-local features. accuracy results for all four
parsers are reported in weiss et al. (2015).

the second block in table 2 presents re-
sults obtained from neural network-based parsers.
c&m14 (chen and manning, 2014) is a transition-
based parser using features learned with a feed

dev

test

parser

uas las uas las
        86.00 84.40
z&n11
        87.96 86.34
z&m14
84.00 82.40
83.90 82.40
c&m14
87.20 85.90
dyer15
87.20 85.70
k&g16 graph         86.60 85.10
k&g16 trans
        87.60 86.10
dense-pei
82.38 80.55
82.50 80.74
83.46 81.65
dense-pei+e 83.40 81.63
87.63 85.94
87.27 85.73
dense
87.35 85.85
dense+e
87.84 86.15

czech

german

parser

uas las uas las
mst-1st
86.18     89.54    
mst-2nd
87.30     90.14    
turbo-1st
87.66     90.52    
90.32     92.41    
turbo-3rd
rbg-1st
87.90     90.24    
90.50     91.97    
rbg-3rd
dense-pei
86.00 77.92 89.42 86.48
dense-pei+cle 86.52 78.42 89.52 86.58
89.60 81.70 92.15 89.58
dense
dense+cle
89.68 81.72 92.19 89.60

table 3: results on chinese dataset (ctb).
+e: we post-process non-projective outputs with
the eisner algorithm.

table 5: non-projective results on the conll
2006 dataset. +cle: we post-process non-tree
outputs with the chu-liu-edmonds algorithm.

ptb

ctb

dev
parser
43.35
c&m14
51.94
dyer15
dense
51.24
dense+e 52.47

test
40.93
50.70
49.34
50.79

dev
32.75
39.72
34.74
36.49

test
32.20
37.23
33.66
35.13

table 4: uem results on ptb and ctb.

forward neural network. although very fast,
its performance is inferior compared to graph-
based parsers or strong non-neural transition based
parsers (e.g., z&n11). dyer15 (dyer et al., 2015)
uses (stack) lstms to model the states of the
buffer, the stack, and the action sequence of a tran-
sition system. weiss15 (weiss et al., 2015) is an-
other transition-based parser, with a more elabo-
rate training procedure. features are learned with
a neural network model similar to c&m14, but
much larger with two layers. the hidden states of
the neural network are then used to train a struc-
tured id88 for better id125 decod-
ing. andor16 (andor et al., 2016) is similar to
weiss15, but uses a globally normalized training
algorithm instead.

unlike all models above, dense does not use
any kind of transition- or graph-based algorithm
during training and id136. nonetheless, it ob-
tains a uas of 94.02%. around 95% of the
model   s outputs after id136 are trees, 87% of
which are projective. when we post-process the
remaining 13% of non-projective outputs with the
eisner algorithm (dense+e), we obtain a slight
improvement on uas (94.10%).

kiperwasser and goldberg (2016) extract fea-

tures from bidirectional lstms and feed them
to a graph- (k&g16 graph) and transition-based
parser (k&g16 trans). their lstms are jointly
trained with the parser objective. dense yields
very similar performance to their transition-based
parser while it outperforms k&g16 graph. a key
difference between dense and k&g16 lies in the
training objective. the objective of dense is log-
likelihood based without tree structure constraints
(the model is trained to produce a distribution over
possible heads for each word, where each head
selection is independent), while k&g16 employ
a max-margin objective with tree structure con-
straints. although our probabilistic objective is
non-structured, it is perhaps easier to train com-
pared to a margin-based one.

we also assessed the importance of the bidi-
rectional lstm on its own by replacing our
lstm-based features with those obtained from
a feed-forward network. speci   cally, we used
the 1-order-atomic features introduced in lei et
al. (2014a) which represent pos-tags, modi   ers,
heads, and their relative positions. as can be seen
in table 2 (dense-pei), these features are less
effective compared to lstm-based ones and the
contribution of the mst algorithm (eisner) during
decoding is more pronounced (dense-pei+e).
we observe similar trends in the chinese, german,
and czech datasets (see tables 3 and 5).

results on ctb follow a similar pattern. as
shown in table 3, dense outperforms all previ-
ous neural models (see the test set columns) on
uas and las. dense performs competitively
with z&m14, a non-neural model with a com-

a.

b.

figure 2: uas against sentence length on ptb and ctb (development set). sentences are sorted by
length in ascending order and divided equally into 10 bins. the horizontal axis is the length of the last
sentence in each bin.

plex high order decoding algorithm involving cube
pruning and strategies for encouraging diversity.
post-processing the output of the parser with the
eisner algorithm generally improves performance
(by 0.21%; see last row in table 3). again we
observe that 1-order-atomic features (lei et al.,
2014a) are inferior compared to the lstm. ta-
ble 4 reports unlabeled sentence level exact match
(uem) in table 4 for english and chinese.
in-
terestingly, even when using the greedy id136
strategy, dense yields a uem comparable to
dyer15 on ptb. finally, in figure 2 we analyze
the performance of our parser on sentences of dif-
ferent length. on both ptb and ctb, dense
has an advantage on long sentences compared to
c&m14 and dyer15.

for czech and german, we closely follow the
evaluation setup of conll 2006. we report
both uas and las, although most previous work
has focused on uas. our results are summarized
in table 5. we compare dense against three
non-projective graph-based dependency parsers:
the mst parser (mcdonald et al., 2005b), the
turbo parser (martins et al., 2013), and the rbg
parser (lei et al., 2014b). we show the per-
formance of these parsers in the    rst order set-
ting (e.g., mst-1st) and in higher order settings
(e.g., turbo-3rd). the results of mst-1st, mst-
2nd, rbg-1st and rbg-3rd are reported in lei et
al. (2014b) and the results of turbo-1st and turbo-
3rd are reported in martins et al. (2013). we show
results for our parser with greedy id136 (see
dense in the table) and when we use the chu-

dataset
ptb
ctb
czech
german

before mst
proj
86.6
73.1
65.5
67.3

#sent tree
95.1
1,700
87.0
803
87.7
374
367
96.7

after mst
tree
proj
100.0
100.0
100.0
100.0
72.7
100.0
100.0
68.1

table 6: percentage of trees and projective trees
on the development set before and after dense
uses a mst algorithm. on ptb and ctb, we use
the eisner algorithm and on czech and german,
we use the chu-liu-edmonds algorithm.

liu-edmonds algorithm to post-process non-tree
outputs (dense+cle).

as can been seen, dense outperforms all other
   rst (and second) order parsers on both german
and czech. as in the projective experiments, we
observe slight a improvement (on both uas and
las) when using a mst algorithm. on german,
dense is comparable with the best third-order
parser (turbo-3rd), while on czech it lags behind
turbo-3rd and rbg-3rd. this is not surprising
considering that dense is a    rst-order parser and
only uses words and pos tags as features. com-
parison systems use a plethora of hand-crafted fea-
tures and more sophisticated high-order decoding
algorithms. finally, note that a version of dense
with features in (lei et al., 2014a) is consistently
worse (see the second block in table 5).

our experimental results demonstrate that us-
ing a mst algorithm during id136 can slightly
improve the model   s performance. we further ex-

111417202326283238118ptb sentence length8990919293949596uas (%)c&m14dense+edyer155914182226303749116ptb sentence length8081828384858687888990919293uas (%)c&m14dense+edyer15ctbctbamined the extent to which the mst algorithm is
necessary for producing dependency trees. table 6
shows the percentage of trees before and after the
application of the mst algorithm across the four
languages. in the majority of cases dense out-
puts trees (ranging from 87.0% to 96.7%) and a
signi   cant proportion of them are projective (rang-
ing from 65.5% to 86.6%). therefore, only a small
proportion of outputs (14.0% on average) need
to be post-processed with the eisner or chu-liu-
edmonds algorithm.

5 conclusions
in this work we presented dense, a neural de-
pendency parser which we train without a tran-
sition system or graph-based algorithm. experi-
mental results show that dense achieves compet-
itive performance across four different languages
and can seaid113ssly transfer from a projective to a
non-projective parser simply by changing the post-
processing mst algorithm during id136.
in
the future, we plan to increase the coverage of our
parser by using tri-training techniques (li et al.,
2014) and id72 (luong et al., 2015).
acknowledgments we would like to thank
adam lopez and frank keller for their valuable
feedback. we acknowledge the    nancial support
of the european research council (erc; award
number 681760).

references
[andor et al.2016] daniel andor, chris alberti, david
weiss, aliaksei severyn, alessandro presta, kuz-
man ganchev, slav petrov, and michael collins.
2016. globally normalized transition-based neural
networks. in proceedings of the 54th annual meet-
ing of the association for computational linguistics
(volume 1: long papers), pages 2442   2452, berlin,
germany.

[bahdanau et al.2015] dzmitry bahdanau, kyunghyun
cho, and yoshua bengio. 2015. neural machine
translation by jointly learning to align and translate.
in proceedings of the 3rd international conference
on learing representations, san diego, california.

[ballesteros et al.2015] miguel ballesteros, chris dyer,
and noah a. smith. 2015.
improved transition-
based parsing by modeling characters instead of
words with lstms. in proceedings of the 2015 con-
ference on empirical methods in natural language
processing, pages 349   359, lisbon, portugal.

[bohnet2010] bernd bohnet. 2010. top accuracy and
in

fast id33 is not a contradiction.

proceedings of the 23rd international conference on
computational linguistics (coling 2010), pages 89   
97, beijing, china.

carreras

[carreras and collins2009] xavier

and
michael collins.
2009. non-projective parsing
in proceedings
for id151.
of the 2009 conference on empirical methods in
natural language processing, pages 200   209,
singapore.

[carreras2007] xavier carreras. 2007. experiments
with a higher-order projective dependency parser.
in proceedings of the conll shared task session
of emnlp-conll 2007, pages 957   961, prague,
czech republic.

[chelba et al.1997] ciprian chelba, david engle, fred-
erick jelinek, victor jimenez, sanjeev khudanpur,
lidia mangu, harry printz, eric ristad, ronald
rosenfeld, andreas stolcke, and dekai wu. 1997.
structure and performance of a dependency lan-
in fifth european conference
guage model.
on speech communication and technology, eu-
rospeech 1997, rhodes, greece.

[chen and manning2014] danqi chen and christopher
manning. 2014. a fast and accurate dependency
in proceedings of
parser using neural networks.
the 2014 conference on empirical methods in natu-
ral language processing (emnlp), pages 740   750,
doha, qatar.

[chu and liu1965] yoeng-jin chu and tseng-hong
liu. 1965. on shortest arborescence of a directed
graph. scientia sinica, 14(10):1396.

[collins2002] michael collins. 2002. discriminative
training methods for id48: the-
ory and experiments with id88 algorithms. in
proceedings of the 2002 conference on empirical
methods in natural language processing, pages 1   
8.

[crammer and singer2001] koby crammer and yoram
singer. 2001. on the algorithmic implementation of
multiclass kernel-based vector machines. journal of
machine learning research, 2:265   292.

[crammer and singer2003] koby crammer and yoram
singer. 2003. ultraconservative online algorithms
for multiclass problems. journal of machine learn-
ing research, 3:951   991.

[cross and huang2016] james cross and liang huang.
2016. incremental parsing with minimal features us-
ing bi-directional lstm. in proceedings of the 54th
annual meeting of the association for computa-
tional linguistics (volume 2: short papers), pages
32   37, berlin, germany.

[de marneffe et al.2006] marie-catherine de marn-
effe, bill maccartney, christopher d manning, et al.
2006. generating typed dependency parses from
in proceedings of lrec,
phrase structure parses.
volume 6, pages 449   454, genoa, italy.

[dyer et al.2015] chris dyer, miguel ballesteros,
wang ling, austin matthews, and noah a. smith.
2015. transition-based id33 with
in proceedings of
stack long short-term memory.
the 53rd annual meeting of the association for
computational linguistics and the 7th international
joint conference on natural language processing
(volume 1: long papers), pages 334   343, beijing,
china.

[edmonds1967] jack edmonds.

optimum
branchings. journal of research of the national bu-
reau of standards b, 71(4):233   240.

1967.

[eisner1996] jason eisner. 1996. ef   cient normal-
form parsing for id35.
in proceedings of the 34th annual meeting of the as-
sociation for computational linguistics, pages 79   
86, santa cruz, california, usa.

[eisner2000] jason eisner. 2000. bilexical grammars
in ad-
and their cubic-time parsing algorithms.
vances in probabilistic and other parsing technolo-
gies, pages 29   61. springer.

[freund and schapire1999] yoav freund and robert e
schapire. 1999. large margin classi   cation us-
ing the id88 algorithm. machine learning,
37(3):277   296.

[fundel et al.2007] katrin fundel, robert k  uffner, and
ralf zimmer. 2007. id36 using de-
pendency parse trees. bioinformatics, 23(3):365   
371.

[glorot et al.2011] xavier glorot, antoine bordes, and
yoshua bengio. 2011. deep sparse recti   er neural
networks. in international conference on arti   cial
intelligence and statistics, pages 315   323, cadiz,
spain.

[goldberg and nivre2012] yoav goldberg and joakim
nivre. 2012. a dynamic oracle for arc-eager depen-
in proceedings of coling 2012,
dency parsing.
pages 959   976, mumbai, india.

[goldberg2016] yoav goldberg. 2016. a primer on
neural network models for natural language pro-
cessing. journal of arti   cial intelligence research,
57:345   420.

[graves2012] alex graves.

supervised se-
quence labelling with recurrent neural networks.
studies in computational intelligence. springer.

2012.

[hall2007] keith hall. 2007. k-best spanning tree
in proceedings of the 45th annual meet-
parsing.
ing of the association of computational linguistics,
pages 392   399, prague, czech republic.

[henderson2004] james henderson. 2004. discrimi-
native training of a neural network statistical parser.
in proceedings of the 42nd meeting of the associa-
tion for computational linguistics (acl   04), main
volume, pages 95   102, barcelona, spain.

[huang and sagae2010] liang huang and kenji sagae.
2010. id145 for linear-time incre-
mental parsing. in proceedings of the 48th annual
meeting of the association for computational lin-
guistics, pages 1077   1086, uppsala, sweden.

[kingma and ba2014] diederik kingma and jimmy
ba. 2014. adam: a method for stochastic opti-
mization. arxiv preprint arxiv:1412.6980.

[kiperwasser and goldberg2016] eliyahu kiperwasser
and yoav goldberg. 2016. simple and accurate de-
pendency parsing using bidirectional lstm feature
representations. transactions of the association for
computational linguistics, 4:313   327.

[koo et al.2010] terry koo, alexander m. rush,
michael collins, tommi jaakkola, and david son-
tag. 2010. id209 for parsing with
in proceedings of
non-projective head automata.
the 2010 conference on empirical methods in nat-
ural language processing, pages 1288   1298, cam-
bridge, ma.

[k  ubler et al.2009] sandra k  ubler, ryan mcdonald,
joakim nivre, and graeme hirst. 2009. depen-
dency parsing. morgan and claypool publishers.

[lei et al.2014a] tao lei, yu xin, yuan zhang, regina
barzilay, and tommi jaakkola. 2014a. low-rank
tensors for scoring dependency structures. in pro-
ceedings of the 52nd annual meeting of the associa-
tion for computational linguistics (volume 1: long
papers), pages 1381   1391, baltimore, maryland.

[lei et al.2014b] tao lei, yuan zhang, regina barzi-
lay, and tommi jaakkola. 2014b. low-rank tensors
for scoring dependency structures. in proceedings
of the acl. baltimore, maryland.

[li et al.2014] zhenghua li, min zhang, and wenliang
chen. 2014. ambiguity-aware ensemble training
in pro-
for semi-supervised id33.
ceedings of the 52nd annual meeting of the associa-
tion for computational linguistics (volume 1: long
papers), pages 457   467, baltimore, maryland.

[luong et al.2015] minh-thang luong, quoc v le,
ilya sutskever, oriol vinyals, and lukasz kaiser.
2015. multi-task sequence to sequence learning. in
proceedings of the 4th international conference on
learning representations, san juan, puerto rico.

[martins et al.2013] andre martins, miguel almeida,
and noah a. smith. 2013. turning on the turbo:
fast third-order non-projective turbo parsers.
in
proceedings of the 51st annual meeting of the as-
sociation for computational linguistics (volume 2:
short papers), pages 617   622, so   a, bulgaria.

[mayberry and miikkulainen1999] marshall r. may-
berry and risto miikkulainen. 1999. sardsrn: a
neural network shift-reduce parser. in in proceed-
ings of the 16th international joint conference on
arti   cial intelligence, pages 820   825, stockholm,
sweden.

[mcdonald et al.2005a] ryan mcdonald, koby cram-
mer, and fernando pereira. 2005a. online large-
margin training of dependency parsers. in proceed-
ings of the 43rd annual meeting of the association
for computational linguistics, pages 91   98, ann
arbor, michigan.

[mcdonald et al.2005b] ryan mcdonald, fernando
pereira, kiril ribarov, and jan hajic. 2005b. non-
projective id33 using spanning tree
in proceedings of human language
algorithms.
technology conference and conference on empiri-
cal methods in natural language processing, pages
523   530, vancouver, british columbia, canada.

[nivre et al.2006a] joakim nivre, johan hall, and jens
nilsson. 2006a. maltparser: a data-driven parser-
in proceedings
generator for id33.
of lrec, volume 6, genoa, italy.

[nivre et al.2006b] joakim nivre, johan hall, jens
nilsson, g  uls  en eryi  git, and svetoslav marinov.
2006b. labeled pseudo-projective dependency pars-
in proceedings
ing with support vector machines.
of the tenth conference on computational natu-
ral language learning (conll-x), pages 221   225,
new york city.

[nivre2003] joakim nivre. 2003. an ef   cient algo-
in pro-
rithm for projective id33.
ceedings of the 8th international workshop on pars-
ing technologies, pages 149   160, nancy, france.

[nivre2004] joakim nivre. 2004.

incrementality in
deterministic id33. in frank keller,
stephen clark, matthew crocker, and mark steed-
man, editors, proceedings of the acl workshop in-
cremental parsing: bringing engineering and cog-
nition together, pages 50   57, barcelona, spain.

[nivre2008] joakim nivre. 2008. algorithms for de-
terministic incremental id33. com-
putational linguistics, 34(4):513   553.

[pascanu et al.2013] razvan pascanu, tomas mikolov,
and yoshua bengio. 2013. on the dif   culty of train-
ing recurrent neural networks. in proceedings of the
30th international conference on machine learn-
ing, pages 1310   1318, atlanta, georgia.

[pennington et al.2014] jeffrey pennington, richard
socher, and christopher manning. 2014. glove:
global vectors for word representation. in proceed-
ings of the 2014 conference on empirical methods
in natural language processing (emnlp), pages
1532   1543, doha, qatar.

[smith2010] david arthur smith. 2010. ef   cient in-
ference for trees and alignments: modeling mono-
lingual and bilingual syntax with hard and soft con-
straints and latent variables. johns hopkins uni-
versity.

[snow et al.2005] rion snow, daniel jurafsky, and an-
drew y ng. 2005. learning syntactic patterns for
automatic hypernym discovery. in advances in neu-
ral information processing systems 17, pages 1297   
1304, vancouver, british columbia.

[srivastava et al.2014] nitish srivastava, geoffrey hin-
ton, alex krizhevsky, ilya sutskever, and ruslan
salakhutdinov. 2014. dropout: a simple way to
prevent neural networks from over   tting. the jour-
nal of machine learning research, 15(1):1929   
1958.

[titov and henderson2007] ivan titov and james hen-
derson. 2007. constituent parsing with incremental
sigmoid belief networks. in proceedings of the 45th
annual meeting of the association of computational
linguistics, pages 632   639, prague, czech repub-
lic.

[toutanova et al.2003] kristina toutanova, dan klein,
christopher d manning, and yoram singer. 2003.
feature-rich part-of-speech tagging with a cyclic de-
pendency network. in proceedings of hlt-naacl
2003, pages 173   180, edmonton, canada.

[tseng et al.2005] huihsin tseng, pichuan chang,
galen andrew, daniel jurafsky, and christopher
manning. 2005. a conditional random    eld word
segmenter for sighan bakeoff 2005. in proceedings
of the 4th sighan workshop on chinese language
processing, pages 168   171, jeju island, korea.

[weiss et al.2015] david weiss, chris alberti, michael
collins, and slav petrov. 2015. structured training
for neural network transition-based parsing. in pro-
ceedings of the 53rd annual meeting of the associ-
ation for computational linguistics and the 7th in-
ternational joint conference on natural language
processing (volume 1: long papers), pages 323   
333, beijing, china.

[yamada and matsumoto2003] hiroyasu yamada and
yuji matsumoto. 2003. statistical dependency anal-
ysis with support vector machines. in proceedings
of the 8th workshop on parsing technologies, pages
195   206, nancy, france.

[zaremba et al.2014] wojciech

ilya
sutskever, and oriol vinyals.
recur-
rent neural network id173. arxiv preprint
arxiv:1409.2329.

zaremba,
2014.

[semeniuta et al.2016] stanislau semeniuta, aliaksei
2016. recurrent
severyn, and erhardt barth.
in proceedings of
dropout without memory loss.
coling 2016, the 26th international conference
on computational linguistics: technical papers,
pages 1757   1766, osaka, japan.

[zhang and clark2008] yue zhang and stephen clark.
2008. a tale of two parsers: investigating and com-
bining graph-based and transition-based dependency
parsing. in proceedings of the 2008 conference on
empirical methods in natural language process-
ing, pages 562   571, honolulu, hawaii.

[zhang and mcdonald2012] hao zhang and ryan mc-
donald. 2012. generalized higher-order depen-
in proceedings
dency parsing with cube pruning.
of the 2012 joint conference on empirical meth-
ods in natural language processing and computa-
tional natural language learning, pages 320   331,
jeju island, korea.

[zhang and mcdonald2014] hao zhang and ryan mc-
2014. enforcing structural diversity in
donald.
in proceedings
cube-pruned id33.
of the 52nd annual meeting of the association for
computational linguistics (volume 2: short pa-
pers), pages 656   661, baltimore, maryland.

[zhang and nivre2011] yue zhang and joakim nivre.
2011. transition-based id33 with
rich non-local features. in proceedings of the 49th
annual meeting of the association for computa-
tional linguistics: human language technologies,
pages 188   193, portland, oregon, usa.

[zhang et al.2016] xingxing zhang, liang lu, and
mirella lapata. 2016. top-down tree long short-
term memory networks. in proceedings of the 2016
conference of the north american chapter of the
association for computational linguistics: human
language technologies, pages 310   320, san diego,
california.

