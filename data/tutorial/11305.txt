proceedings of the 54th annual meeting of the association for computational linguistics, pages 1780   1790,

berlin, germany, august 7-12, 2016. c(cid:13)2016 association for computational linguistics

1780

cross-lingualimagecaptiongenerationtakashimiyazaki   yahoojapancorporationtokyo,japantakmiyaz@yahoo-corp.jpnobuyukishimizu   yahoojapancorporationtokyo,japannobushim@yahoo-corp.jpabstractautomaticallygeneratinganaturallan-guagedescriptionofanimageisafun-damentalprobleminarti   cialintelligence.thistaskinvolvesbothcomputervisionandnaturallanguageprocessingandiscalled   imagecaptiongeneration.   re-searchonimagecaptiongenerationhastypicallyfocusedontakinginanimageandgeneratingacaptioninenglishasex-istingimagecaptioncorporaaremostlyinenglish.thelackofcorporainlanguagesotherthanenglishisanissue,especiallyformorphologicallyrichlanguagessuchasjapanese.thereisthusaneedforcor-porasuf   cientlylargeforimagecaption-inginotherlanguages.wehavedevelopedajapaneseversionofthemscococap-tiondatasetandagenerativemodelbasedonadeeprecurrentarchitecturethattakesinanimageandusesthisjapanesever-sionofthedatasettogenerateacaptioninjapanese.asthejapaneseportionofthecorpusissmall,ourmodelwasde-signedtotransfertheknowledgerepresen-tationobtainedfromtheenglishportionintothejapaneseportion.experimentsshowedthattheresultingbilingualcompa-rablecorpushasbetterperformancethanamonolingualcorpus,indicatingthatimageunderstandingusingaresource-richlan-guagebene   tsaresource-poorlanguage.1introductionautomaticallygeneratingimagecaptionsbyde-scribingthecontentofanimageusingnaturallan-guagesentencesisachallengingtask.itises-peciallychallengingforlanguagesotherthanen-      bothauthorscontributedequallytothiswork.glishduetothesparsityofannotatedresourcesinthetargetlanguage.apromisingsolutiontothisproblemistocreateacomparablecorpus.tosupporttheimagecaptiongenerationtaskinjapanese,wehaveannotatedimagestakenfromthemscococaptiondataset(chenetal.,2015b)withjapanesecaptions.wecallourcorpusthe   yjcaptions26kdataset.   whilethesizeofourdatasetiscomparativelylargewith131,740captions,itgreatlytrailsthe1,026,459captionsinthemscocodataset.wewerethusmoti-vatedtotransfertheresourcesinenglish(sourcelanguage)tojapaneseandtherebyimproveim-agecaptiongenerationinjapanese(targetlan-guage).innaturallanguageprocessing,ataskin-volvingtransferringinformationacrosslanguagesisknownasacross-lingualnaturallanguagetask,andwellknowntasksincludecross-lingualsenti-mentanalysis(chenetal.,2015a),cross-lingualnamedentityrecognition(ziriklyandhagiwara,2015),cross-lingualdependencyparsing(guoetal.,2015),andcross-lingualinformationretrieval(funakiandnakayama,2015).existingworkinthecross-lingualsettingisusu-allyformulatedasfollows.first,toovercomethelanguagebarrier,createaconnectionbetweenthesourceandtargetlanguages,generallybyusingadictionaryorparallelcorpus.second,developanappropriateknowledgetransferapproachtolever-agetheannotateddatafromthesourcelanguageforuseintrainingamodelinthetargetlanguage,usuallysupervisedorsemi-supervised.thesetwostepstypicallyamounttoautomaticallygenerat-ingandexpandingthepseudo-trainingdataforthetargetlanguagebyexploitingtheknowledgeob-tainedfromthesourcelanguage.weproposeaverysimpleapproachtocross-lingualimagecaptiongeneration:exploittheen-glishcorpustoimprovetheperformanceofimagecaptiongenerationinanotherlanguage.inthisap-1781

proach,noresourcesbesidestheimagesfoundinthecorpusareusedtoconnectthelanguages,andweconsiderourdatasettobeacomparablecor-pus.pairedtextsinacomparablecorpusdescribethesametopic,inthiscaseanimage,butunlikeaparallelcorpus,thetextsarenotexacttranslationsofeachother.thisunrestrictivesettingenablesthemodeltobeusedtocreateimagecaptionre-sourcesinotherlanguages.moreover,thismodelscalesbetterthancreatingaparallelcorpuswithexacttranslationsofthedescriptions.ourtransfermodelisverysimple.westartwithaneuralimagecaptionmodel(vinyalsetal.,2015)andpretrainitusingtheenglishportionofthecorpus.wethenremoveallofthetrainedneu-ralnetworklayersexceptforonecruciallayer,theoneclosesttothevisionsystem.nextweattachanuntrainedjapanesegenerationmodelandtrainitusingthejapaneseportionofthecorpus.thisresultsinimprovedgenerationinjapanesecom-paredtousingonlythejapaneseportionofthecorpus.tothebestofourknowledge,thisisthe   rstpapertoaddresstheproblemofcross-lingualimagecaptiongeneration.ourcontributionistwofold.first,wehavecre-atedandplantoreleasethe   rsteversigni   cantlylargecorpusforimagecaptiongenerationforthejapaneselanguage,formingacomparablecorpuswithexistingenglishdatasets.second,wehavecreatedaverysimplemodelbasedonneuralim-agecaptiongenerationforjapanesethatcanex-ploittheenglishportionofthedataset.again,wearethe   rsttoreportresultsincross-lingualim-agecaptiongeneration,andoursurprisinglysim-plemethodimprovestheevaluationmetricssignif-icantly.thismethodiswellsuitedasabaselineforfutureworkoncross-lingualimagecaptiongener-ation.thepaperisorganizedasfollows.inthenextsection,wedescriberelatedworkinimagecap-tiongenerationandlistthecorporacurrentlyavail-ableforcaptiongeneration.theninsection3wepresentthestatisticsforourcorpusandexplainhowweobtainedthem.wethenexplainourmodelinsection4andpresenttheresultsofourexperi-mentalevaluationinsection5.wediscussthere-sultsinsection6,andconcludeinsection7withasummaryofthekeypoints.2relatedworkrecentadvancesincomputervisionresearchhaveledtohalvingtheerrorratebetween2012and2014atthelargescalevisualrecognitionchal-lenge(russakovskyetal.,2015),largelydrivenbytheadoptionofdeepneuralnetworks(krizhevskyetal.,2012;simonyanandzisserman,2014;don-ahueetal.,2014;sharifrazavianetal.,2014).similarly,wehaveseenincreasedadaptationofdeepneuralnetworksfornaturallanguagepro-cessing.inparticular,sequence-to-sequencetrain-ingusingrecurrentneuralnetworkshasbeensuc-cessfullyappliedtomachinetranslation(choetal.,2014;bahdanauetal.,2015;sutskeveretal.,2014;kalchbrennerandblunsom,2013).thesedevelopmentsoverthepastfewyearshaveledtorenewedinterestinconnectingvisionandlanguage.theencoder-decoderframework(choetal.,2014)inspiredthedevelopmentofmanymethodsforgeneratingimagecaptionssincegeneratinganimagecaptionisanalogoustotrans-latinganimageintoasentence.since2014,manyresearchgroupshavere-portedasigni   cantimprovementinimagecaptiongenerationduetousingamethodthatcombinesaconvolutionalneuralnetworkwitharecurrentneuralnetwork.vinyalsetal.usedaconvolu-tionalneuralnetwork(id98)withinceptionmod-ulesforvisualrecognitionandlongshort-termmemory(lstm)forlanguagemodeling(vinyalsetal.,2015).xuetal.introducedanattentionmechanismthatalignsvisualinformationandsen-tencegenerationforimprovingcaptionsandun-derstandingofmodelbehavior(xuetal.,2015).theinterestedreadercanobtainfurtherinforma-tionelsewhere(bernardietal.,2016).thesedevelopmentsweremadepossibleduetoanumberofavailablecorpora.thefollowingisalistofavailablecorporathatalignimageswithcrowd-sourcedcaptions.acomprehensivelistofotherkindsofcorporaconnectingvisionandlan-guage,e.g.,visualquestionanswering,isavailableelsewhere(ferraroetal.,2015).1.uiucpascaldataset(farhadietal.,2010)includes1,000imageswith5sentencesperimage;probablyoneofthe   rstdatasets.2.abstractscenesdataset(clipart)(zitnicketal.,2013)contains10,020imagesofchildrenplayingoutdoorsassociatedwith60,396de-scriptions.1782

3.flickr30kimages(youngetal.,2014)ex-tendsflickrdatasets(rashtchianetal.,2010)andcontains31,783imagesofpeoplein-volvedineverydayactivities.4.microsoftcocodataset(mscoco)(linetal.,2014;chenetal.,2015b)includesabout328,000imagesofcomplexeverydaysceneswithcommonobjectsinnaturallyoc-curringcontexts.eachimageispairedwith   vecaptions.5.japaneseuiucpascaldataset(funakiandnakayama,2015)isajapanesetranslationoftheuiucpascaldataset.tothebestofourknowledge,therearenolargedatasetsforimagecaptiongenerationexceptforenglish.withthereleaseoftheyjcaptions26kdataset,weaimtoremedythissituationandtherebyexpandtheresearchhorizonbyexploitingtheavailabilityofbilingualimagecaptioncorpora.3statisticsfordatasetinthissectionwedescribethedatastatisticsandhowwegathereddatafortheyjcaptions26kdataset.forimages,weusedthemicrosoftcocodataset(chenetal.,2015b).theimagesinthisdatasetweregatheredbysearchingforpairsof80objectcategoriesandvariousscenetypesonflickr.theythustendedtocontainmultipleob-jectsintheirnaturalcontext.objectsinthescenewerelabeledusingper-instancesegmentations.thisdatasetcontainspicturesof91basicobjecttypeswith2.5millionlabeledinstances.tocollectjapanesedescriptionsoftheimages,weusedya-hoo!id1041,amicrotaskid104serviceoperatedbyyahoojapancorporation.given26,500imagestakenfromthetrain-ingpartofthemscocodataset,wecollected131,740captionsintotal.theimageshadonav-erage4.97captions;themaximumnumberwas5andtheminimumwas3.onaverage,eachcaptionhad23.23japanesecharacters.weplantoreleasetheyjcaptions26kdataset2.3.1id104procedureourcaptionswerehumangeneratedusingyahoo!id104.asthisid104platformisoperatedinjapan,signingupfortheserviceandparticipatingrequirejapanesepro   ciency.thus,1http://id104.yahoo.co.jp2http://research-lab.yahoo.co.jp/software/index.htmlfigure1:userinterfaceweassumedthattheparticipantswere   uentinjapanese.first,wepostedapilottaskthataskedthepar-ticipantstodescribeanimage.wethenexam-inedtheresultsandselectedpromisingpartici-pants(comprisinga   whitelist   )forfuturetaskre-quests.thatis,onlytheparticipantsonthewhitelistcouldseethenexttask.thisselectionpro-cesswasrepeated,andthe   nalwhitelistincludedabout600participants.about150ofthemregu-larlyparticipatedintheactualimagecaptioncol-lectiontask.wemodi   edthetaskrequestpageanduserinterfaceonthebasisofourexperiencewiththepilottask.inordertopreventtheirfa-tigue,thetasksweregiveninsmallbatchessothattheparticipantswereunabletoworkoverlonghours.inourinitialtrials,wetriedadirecttranslationoftheinstructionsusedinthems-cocoenglishcaptions.thishoweverdidnotproducejapanesecaptionscomparabletothoseinenglish.thisisbecausepeopledescribewhatappearsunfamiliartothemanddonotdescribethingstheytakeforgranted.ourexaminationoftheresultsfromthepilottasksrevealedthattheparticipantsgenerallythoughtthatthepicturescontainednon-japanesepeopleandforeignplacessincetheimagesorigi-natedfromflickrandnosceneryfromjapanwasincludedintheimagedataset.whenjapanese1783

crowdsareshownpictureswithsceneryintheusoreuropeinms-cocodataset,thescenesthem-selvesappearexoticandwordssuchas   foreign   and   oversea   wouldbeeverywhereinthedescrip-tions.assuchwordsarenotcommonintheorig-inaldataset,andtomakethecorpusnicercomple-menttotheenglishdatasetandtoreducetheef-fectsofsuchculturalbias,wemodi   edtheinstruc-tions:   2.pleasegiveonlyfactualstatements   ;   3.pleasedonotspecifyplacenamesornation-alities.   wealsostrengthenedtwosectionsinthetaskrequestpageandaddedmoreexamples.theinterfaceisshowninfigure1.theinstruc-tionsintheuserinterfacecanbetranslatedintoenglishas   pleaseexplaintheimageusing16ormorejapanesecharacters.writeasinglesentenceasifyouwerewritinganexamplesentencetobeincludedinatextbookforlearningjapanese.de-scribealltheimportantpartsofthescene;donotdescribeunimportantdetails.usecorrectpunctu-ation.writeasinglesentence,notmultiplesen-tencesoraphrase.   potentialparticipantsareshowntaskrequestpages,andtheparticipantsselectwhichcrowd-sourcingtask(s)toperform.thetaskrequestpageforourtaskhadthefollowinginstructions(en-glishtranslation):1.pleaseexplainanimageusing16ormorejapanesecharacters.pleasewriteasinglesentenceasifyouwerewritinganexamplesentencetobeincludedinatextbookforlearningjapanese.(a)donotuseincorrectjapanese.(b)useapolitestyleofspeech(desu/masustyle)aswellascorrectpunctuation.(c)writeasinglecompletesentencethatendswithaperiod.donotwritejustaphraseormultiplesentences.2.pleasegiveonlyfactualstatements.(a)donotwriteaboutthingsthatmighthavehap-penedormighthappeninthefuture.donotwriteaboutsounds.(b)donotspeculate.donotwriteaboutsomethingaboutwhichyoufeeluncertain.(c)donotstateyourfeelingsaboutthesceneinthepicture.donotuseanoverlypoeticstyle.(d)donotuseademonstrativepronounsuchas   this   or   here.   3.pleasedonotspecifyplacenamesornationalities.(a)pleasedonotgivepropernames.4.pleasedescribealltheimportantpartsofthescene;donotdescribeunimportantdetails.togetherwiththeinstructions,weprovided15examples(1goodexample;14badexamples).uponexaminingthecollecteddata,manualchecksof   rst100imagescontaining500captionsrevealedthat9captionswereclearlybad,and12captionshadminorproblemsindescriptions.inordertofurtherimprovethequalityofthecorpus,wecrowdsourcedanewdata-cleaningtask.weshowedeachparticipantanimageand   vecap-tionsthatdescribetheimageandaskedto   xthem.thefollowingistheinstructions(englishtrans-lation)forthetaskrequestpageforourdata-cleaningtask.1.thereare   vesentencesaboutahyper-linkedimage,andseveralsentencesrequire   xesinordertosatisfytheconditionsbelow.please   xthesentences,andwhiledoingso,tickacheckboxoftheitem(condition)being   xed.2.theconditionsthatrequire   xesare:(a)please   xtypographicalerrors,omissionsandinput-method-editorconversionmisses.(b)pleaseremoveorrephraseexpressionssuchas   oversea   ,   foreign   and   foreigner.   (c)pleaseremoveorrephraseexpressionssuchas   image   ,   picture   and   photographed.   (d)please   xthedescriptionifitdoesnotmatchthecontentsoftheimage.(e)pleaseremoveorrephrasesubjectiveexpressionsandpersonalimpressions.(f)ifthestatementisdividedintoseveralsentences,pleasemakeitonesentence.(g)ifthesentenceisinaquestionform,pleasemakeitadeclarativesentence.(h)pleaserewritetheentiresentenceifmeetingallaboveconditionsrequiresextensivemodi   ca-tions.(i)iftherearelessthan16characters,pleasepro-videadditionaldescriptionssothatthesentencewillbelongerthan16characters.foreachcondition,weprovidedapairofexam-ples(1badexampleand1   xedexample).togatherparticipantsforthedata-cleaningtask,wecrowdsourcedapreliminaryuserquali   cationtaskthatexplainedeachconditionrequiring   xesinthe   rsthalf,thenquizzedtheparticipantsinthesecondhalf.thistimeweobtainedover900quali   edparticipants.wepostedthedata-cleaningtasktothesequali   edparticipants.theinterfaceisshowninfigure2.theinstruc-tionsintheuserinterfaceareverysimilartothetaskrequestpage,exceptthatwehaveanaddi-tionalcheckbox:(j)allconditionsaresatis   edandno   xeswerenecessary.weprovidedthesecheckboxestobeusedasachecklist,soastoreducefailurebycompensatingforpotentiallimitsofparticipants   memoryandat-tention,andtoensureconsistencyandcomplete-nessincarryingoutthedata-cleaningtask.forthisdata-cleaningtask,wehad26,500im-agestotaling132,500captionscheckedby267participants.thenumberof   xedcaptionsare1784

figure2:datacleaningtaskuserinterface45,909.tooursurprise,arelativelylargepor-tionofthecaptionswere   xedbytheparticipants.wesuspectthatinourdata-cleaningtask,thecon-dition(e)wasespeciallyambiguousforthepar-ticipants,andtheyerroredonthecautiousside,   xing   alivingroom   tojust   aroom   ,thinkingthataroomthatlookslikealivingroommaynotbealivingroomforthefamilywhooccupiesthehouse,forexample.anotherexampleincludes   x-ing   beautiful   owers   tojust      owers   becausebeautyisintheeyeofthebeholderandthoughttobesubjective.thepercentageofthetickedcheckboxesisasfollows:(a)27.2%,(b)5.0%,(c)12.3%,(d)34.1%,(e)28.4%,(f)3.9%,(g)0.3%,(h)11.6%,(i)18.5%,and(j)24.0%.notethatacheckboxistickedifthereisatleastonesentenceoutof   vethatmeetsthecondition.inmachinelearning,thissettingiscalledmultiple-instancemultiple-labelproblem(zhouetal.,2012).wecannotdirectlyinferhowmanycaptionscorre-spondtoaconditiontickedbytheparticipants.afterthisdata-cleaningtask,wefurtherre-movedafewmorebadcaptionsthatcametoourattention.theresultingcorpus   nallycontains131,740captionsasnotedintheprevioussection.4methodology!"#$!!"#$!%&   (%)(%*(+,,-*.(/01!%)(!"#$!%*(%)(23!23!*4*5623.!*4*5623.!&7!7628*)(2   93:(423:;2:*7!figure3:modeloverview4.1modeloverviewfigure3showsanoverviewofourmodel.follow-ingtheapproachofvinyalsetal.(vinyalsetal.,2015),weusedadiscriminativemodelthatmax-imizestheid203ofthecorrectdescriptiongiventheimage.ourmodelisformulatedas     =argmax     (i,s)n   t=0logp(st|i,s0,...,st   1;  ),(1)wherethe   rstsummationisoverpairsofanim-ageianditscorrecttranscriptions.forthesec-ondsummation,thesumisoverallwordsstins,andnisthelengthofs.  representsthemodelparameters.notethatthesecondsummationrep-resentstheid203ofthesentencewithrespecttothejointid203ofitswords.wemodeledp(st|i,s0,...,st   1;  )byusingarecurrentneuralnetwork(id56).tomodelthese-quencesintheid56,weleta   xedlengthhiddenstateormemoryhtexpressthevariablenumberofwordstobeconditioneduptot   1.theht1785

isupdatedafterobtaininganewinputxtusinganon-linearfunctionf,sothatht+1=f(ht,xt).sinceanlstmnetworkhasstate-of-theartper-formanceinsequencemodelingsuchasmachinetranslation,weuseoneforf,whichweexplaininthenextsection.acombinationoflstmandid98areusedtomodelp(st|i,s0,...,st   1;  ).x   1=wimid98(i)(2)xt=west,t   {0...n   1}(3)pt+1=softmax(wdlstm(xt)),t   {0...n   1}(4)wherewimisanimagefeatureencodingmatrix,weisawordembeddingmatrix,andwdisaworddecodingmatrix.4.2lstm-basedlanguagemodelanlstmisanid56thataddressesthevanish-ingandexplodinggradientsproblemandthathan-dleslongerdependencieswell.anlstmhasamemorycellandvariousgatestocontrolthein-put,theoutput,andthememorybehaviors.weuseanlstmwithinputgateit,inputmodulationgategt,outputgateot,andforgettinggateft.thenumberofhiddenunitshtis256.ateachtimestept,thelstmstatect,htisasfollows:it=  (wixxt+wihht   1+bi)(5)ft=  (wfxxt+wfhht   1+bf)(6)ot=  (woxxt+wohht   1+bo)(7)gt=  (wcxxt+wchht   1+bc)(8)ct=ft   ct   1+it   gt(9)ht=ot     (ct),(10)where  (x)=(1+e   x)   1isasigmoidfunction,  (x)=(ex   e   x)/(ex+e   x)isahyperbolictangentfunction,and   denotestheelement-wiseproductoftwovectors.wandbareparameterstobelearned.fromthevaluesofthehiddenunitsht,theid203distributionofwordsiscalculatedaspt+1=softmax(wdht).(11)weuseasimplegreedysearchtogeneratecap-tionsasasequenceofwords,and,ateachtimestept,thepredictedwordisobtainedusingst=argmaxspt.4.3imagefeatureextractionwithdeepconvolutionalneuralnetworktheimagerecognitionperformanceofdeepcon-volutionalneuralnetworkmodelshasrapidlyad-vancedinrecentyears,andtheyarenowwidelyusedforvariousimagerecognitiontasks.weuseda16-layervggnet(simonyanandzisser-man,2014),whichwasatopperformerattheim-agenetlargescalevisualrecognitionchallengein2014.a16-layervggnetiscomposedof13convolutionallayershavingsmall3x3   lterker-nelsand3fullyconnectedlayers.animagefea-tureisextractedasa4096-dimensionalvectorofthevggnet   sfc7layer,whichisthesecondfullyconnectedlayerfromtheoutputlayer.vggnetwaspretrainedusingtheilsvrc2014subsetoftheid163dataset,anditsweightswerenotup-datedthroughtraining.4.4datasetsplitbecauseourcaptiondatasetisannotatedforonly26,500imagesofthemscocotrainingset,wereorganizedthedatasetsplitforourexperi-ments.trainingandvalidationsetimagesofthemscocodatasetweremixedandsplitintofourblocks,andtheseblockswereassignedtotraining,validation,andtestingasshownintable1.allblockswereusedfortheenglishcaptiondataset.blocksb,c,anddwereusedforthejapanesecaptiondataset.blockno.ofimagessplitlanguagea96,787trainenb22,500trainen,jac2,000valen,jad2,000testen,jatotal123,287table1:datasetsplit4.5trainingthemodelsweretrainedusingminibatchstochas-ticgradientdescent,andthegradientswerecom-putedbyid26throughtime.parame-teroptimizationwasdoneusingthermspropal-gorithm(tielemanandhinton,2012)withanini-tiallearningrateof0.001,adecayrateof0.999,and  of1.0   8.eachimageminibatchcontained100imagefeatures,andthecorrespondingcap-tionminibatchcontainedonesampledcaptionperimage.toevaluatetheeffectivenessofjapanese1786

imagecaptiongeneration,weusedthreelearningschemes.monolinguallearningthiswasthebase-linemethod.themodelhadonlyonelstmforjapanesecaptiongeneration,andonlythejapanesecaptioncorpuswasusedfortraining.alternatelearninginthisscheme,amodelhadtwolstms,oneforenglishandoneforjapanese.thetrainingbatchesforcaptionscontainedeitherenglishorjapanese,andthebatcheswerefedintothemodelalternatingbetweenenglishandjapanese.transferlearningamodelwithonelstmwastrainedcompletelyfortheenglishdataset.thetrainedlstmwasthenremoved,andanotherlstmwasaddedforjapanesecaptiongenera-tion.wimwassharedbetweentheenglishandjapanesetraining.thesemodelswereimplementedusingthechainerneuralnetworkframework(tokuietal.,2015).weconsultedneuraltalk(karpathy,2014),anopensourceimplemenationofneuralnetworkbasedimagecaptiongenerationsystem,fortrainingparametersanddatasetpreprocessing.trainingtookaboutonedayusingnvidiati-tanx/teslam40gpus.5evaluation01000020000300004000050000no. of iterations0.10.20.30.40.50.60.7cidertransfermonolingualalternatefigure4:learningcurverepresentedbyciderscore5.1evaluationmetricsweusedsixstandardmetricsforevaluatingthequalityofthegeneratedjapanesesentences:id7-1,id7-2,id7-3,id7-4(papinenietal.,2002),id8-l(lin,2004),andcider-d(vedantametal.,2014).weusedthecococap-tionevaluationtool(chenetal.,2015b)tocom-putethemetrics.id7(papinenietal.,2002)wasoriginallydesignedforautomaticmachinetranslation.bycountingid165co-occurrences,itratesthequalityofatranslatedsentencegivensev-eralreferencesentences.toapplyid7,wecon-sideredthatgeneratingimagecaptionsisthesameastranslatingimagesintosentences.id8(lin,2004)isanevaluationmetricdesignedbyadaptingid7toevaluateautomatictextsum-marizationalgorithms.id8isbasedonthelongestcommonsubsequencesinsteadofid165s.cider(vedantametal.,2014)isametricdevel-opedspeci   callyforevaluatingimagecaptions.itmeasuresconsensusinimagecaptionsbyper-formingaterm-frequencyinversedocumentfre-quency(tf-idf)weightingforeachid165.weusedarobustvariantofcidercalledcider-d.forallevaluationmetrics,higherscoresarebetter.inadditiontothesemetrics,mscococaptionevaluation(chenetal.,2015b)usesmeteor(lavie,2014),anothermetricforevaluatingauto-maticmachinetranslation.althoughmeteorisagoodmetric,itusesanenglishthesaurus.itwasnotusedinourstudyduetothelackofathesaurusforthejapaneselanguage.theciderandmeteormetricsperformwellintermsofcorrelationwithhumanjudgment(bernardietal.,2016).althoughid7isunabletosuf   cientlydiscriminatebetweenjudgments,wereporttheid7   guresaswellsincetheiruseinliteratureiswidespread.inthenextsection,wefocusouranalysisoncider.0500010000150002000025000no. of images (ja)0.400.450.500.550.600.65cidertransfermonolingualfigure5:ciderscorevs.japanesedatasetsize5.2resultstable2showstheevaluationmetricsforvarioussettingsofcross-lingualtransferlearning.allval-ueswerecalculatedforjapanesecaptionsgener-1787

no.ofimagesmetricsenjaid7-1id7-2id7-3id7-4id8-lcider-dmonolingual022,5000.7150.5730.4680.3790.6160.580alternate119,28722,5000.7090.5650.4600.3700.6110.568transfer119,28722,5000.7170.5740.4690.3800.6190.625table2:evaluationmetricsatedfortestsetimages.ourproposedmodelisla-beled   transfer.   asyoucansee,itoutperformedtheothertwomodelsforeverymetric.inpar-ticular,thecider-dscorewasabout4%higherthanthatforthemonolingualbaseline.theper-formanceofamodeltrainedusingtheenglishandjapanesecorporaalternatelyisshownonthelinelabel   alternate.   surprisingly,thismodelhadlowerperformancethanthebaselinemodel.infigure4,weplotthelearningcurvesrep-resentedbytheciderscoreforthejapanesecaptionsgeneratedforthevalidationsetimages.transferlearningfromenglishtojapanesecon-vergedfasterthanlearningfromthejapanesedatasetorlearningbytrainingfrombothlan-guagesalternately.figure5showstherelation-shipbetweentheciderscoreandthejapanesedatasetsize(numberofimages).themodelspretrainedusingenglishcaptions(blueline)out-performedtheonestrainedusingonlyjapanesecaptionsforalltrainingdatasetsizes.ascanbeseenbycomparingthecaseof4,000im-ageswiththatof20,000images,theimprovementduetocross-lingualtransferwaslargerwhenthejapanesedatasetwassmaller.theseresultsshowthatpretrainingthemodelwithallavailableen-glishcaptionsisroughlyequivalenttotrainingthemodelwithcaptionsfor10,000additionalimagesinjapanese.this,inourcase,nearlyhalvesthecostofbuildingthecorpus.examplesofmachine-generatedcaptionsalongwiththecrowd-writtengroundtruthcaptions(en-glishtranslations)areshowninfigure6.6discussiondespiteourinitialbelief,trainingbyalternatingenglishandjapaneseinputbatchdataforlearningbothlanguagesdidnotworkwellforeitherlan-guage.asjapaneseisamorphologicallyrichlan-guageandwordorderingissubject-object-verb,itisoneofmostdistantlanguagesfromenglish.wesuspectthatthealternatingbatchtraininginter-feredwithlearningthesyntaxofeitherlanguage.moreover,whenwetriedcharacter-basedmodelsforbothlanguages,theperformancewassignif-icantlylower.thiswasnotsurprisingbecauseonewordinenglishisroughlytwocharactersinjapanese,andpresumablydifferencesinthelan-guageunitshouldaffectperformance.perhapsnotsurprisingly,cross-lingualtransferwasmoreef-fectivewhentheresourcesinthetargetlanguagearepoor.convergencewasfasterwiththesameamountofdatainthetargetlanguagewhenpre-traininginthesourcelanguagewasdoneaheadoftime.thesetwo   ndingseasetheburdenofdevel-opingalargecorpusinaresourcepoorlanguage.7conclusionwehavecreatedanimagecaptiondatasetforthejapaneselanguagebycollecting131,740captionsfor26,500imagesusingtheyahoo!crowdsourc-ingserviceinjapan.weshowedthatpretraininganeuralimagecaptionmodelwiththeenglishpor-tionofthecorpusimprovestheperformanceofajapanesecaptiongenerationmodelsubsequentlytrainedusingjapanesedata.pretrainingthemodelusingtheenglishcaptionsof119,287imageswasroughlyequivalenttotrainingthemodelusingthecaptionsof10,000additionalimagesinjapanese.this,inourcase,nearlyhalvesthecostofbuildingacorpus.sincethisperformancegainisobtainedwithoutmodifyingtheoriginalmonolingualimagecaptiongenerator,theproposedmodelcanserveasastrongbaselineforfutureresearchinthisarea.wehopethatourdatasetandproposedmethodkickstartstudiesoncross-lingualimagecaptiongenerationandthatmanyothersfollowourlead.referencesdzmitrybahdanau,kyunghyuncho,andyoshuaben-gio.2015.neuralmachinetranslationbyjointlylearningtoalignandtranslate.ininternationalcon-ferenceonlearningrepresentation(iclr).raffaellabernardi,ruketcakici,desmondelliott,aykuterdem,erkuterdem,nazliikizler-cinbis,frankkeller,adrianmuscat,andbarbaraplank.1788

figure6:imagecaptiongenerationexamples1789

2016.automaticdescriptiongenerationfromim-ages:asurveyofmodels,datasets,andevaluationmeasures.arxivpreprintarxiv:1601.03896.qiangchen,wenjieli,yulei,xuleliu,andyanxi-anghe.2015a.learningtoadaptcredibleknowl-edgeincross-lingualsentimentanalysis.inpro-ceedingsofthe53rdannualmeetingoftheassoci-ationforcomputationallinguisticsandthe7thin-ternationaljointconferenceonnaturallanguageprocessing(volume1:longpapers),pages419   429,beijing,china,july.associationforcomputa-tionallinguistics.xinleichen,tsung-yilinhaofang,ramakr-ishnavedantam,saurabhgupta,piotrdollr,andc.lawrencezitnick.2015b.microsoftcococap-tions:datacollectionandevaluationserver.arxivpreprintarxiv:1504.00325.kyunghyuncho,bartvanmerrienboer,caglargul-cehre,dzmitrybahdanau,fethibougares,holgerschwenk,andyoshuabengio.2014.learningphraserepresentationsusingid56encoder   decoderforstatisticalmachinetranslation.inproceedingsofthe2014conferenceonempiricalmethodsinnat-urallanguageprocessing(emnlp),pages1724   1734,doha,qatar,october.associationforcom-putationallinguistics.jeffdonahue,yangqingjia,oriolvinyals,judyhoff-man,ningzhang,erictzeng,andtrevordarrell.2014.decaf:adeepconvolutionalactivationfea-tureforgenericvisualrecognition.ininternationalconferenceinmachinelearning(icml).alifarhadi,mohsenhejrati,mohammadaminsadeghi,peteryoung,cyrusrashtchian,juliahockenmaier,anddavidforsyth.2010.everypic-turetellsastory:generatingsentencesfromimages.inproceedingsofthe11theuropeanconferenceoncomputervision:partiv,eccv   10,pages15   29,berlin,heidelberg.springer-verlag.francisferraro,nasrinmostafazadeh,ting-haohuang,lucyvanderwende,jacobdevlin,michelgalley,andmargaretmitchell.2015.asurveyofcurrentdatasetsforvisionandlanguageresearch.inproceedingsofthe2015conferenceonempiri-calmethodsinnaturallanguageprocessing,pages207   213,lisbon,portugal,september.associationforcomputationallinguistics.rukafunakiandhidekinakayama.2015.image-mediatedlearningforzero-shotcross-lingualdoc-umentretrieval.inproceedingsofthe2015con-ferenceonempiricalmethodsinnaturallan-guageprocessing,pages585   590,lisbon,portugal,september.associationforcomputationallinguis-tics.jiangguo,wanxiangche,davidyarowsky,haifengwang,andtingliu.2015.cross-lingualdepen-dencyparsingbasedondistributedrepresentations.inproceedingsofthe53rdannualmeetingoftheassociationforcomputationallinguisticsandthe7thinternationaljointconferenceonnaturallan-guageprocessing(volume1:longpapers),pages1234   1244,beijing,china,july.associationforcomputationallinguistics.nalkalchbrennerandphilblunsom.2013.recurrentcontinuoustranslationmodels.inproceedingsofthe2013conferenceonempiricalmethodsinnatu-rallanguageprocessing,pages1700   1709,seattle,washington,usa,october.associationforcompu-tationallinguistics.andrejkarpathy.2014.neuraltalk.https://github.com/karpathy/neuraltalk.alexkrizhevsky,ilyasutskever,andgeoffreye.hin-ton.2012.id163classi   cationwithdeepcon-volutionalneuralnetworks.inf.pereira,c.j.c.burges,l.bottou,andk.q.weinberger,editors,advancesinneuralinformationprocessingsystems25,pages1097   1105.curranassociates,inc.michaeldenkowskialonlavie.2014.meteoruniver-sal:languagespeci   ctranslationevaluationforanytargetlanguage.acl2014,page376.tsung-yilin,michaelmaire,sergebelongie,jameshays,pietroperona,devaramanan,piotrdoll  ar,andc.lawrencezitnick,2014.computervision   eccv2014:13theuropeanconference,zurich,switzerland,september6-12,2014,proceedings,partv,chaptermicrosoftcoco:commonobjectsincontext,pages740   755.springerinternationalpublishing,cham.chin-yewlin.2004.id8:apackageforauto-maticevaluationofsummaries.textsummarizationbranchesout:proceedingsoftheacl-04workshop,8.kishorepapineni,salimroukos,toddward,andwei-jingzhu.2002.id7:amethodforautomaticevaluationofmachinetranslation.inproceedingsofthe40thannualmeetingonassociationforcom-putationallinguistics,acl   02,pages311   318,stroudsburg,pa,usa.associationforcomputa-tionallinguistics.cyrusrashtchian,peteryoung,micahhodosh,andjuliahockenmaier.2010.collectingimageannota-tionsusingamazon   smechanicalturk.inproceed-ingsofthenaaclhlt2010workshoponcreatingspeechandlanguagedatawithamazon   smechan-icalturk,csldamt   10,pages139   147,strouds-burg,pa,usa.associationforcomputationallin-guistics.olgarussakovsky,jiadeng,haosu,jonathankrause,sanjeevsatheesh,seanma,zhihenghuang,an-drejkarpathy,adityakhosla,michaelbernstein,alexanderc.berg,andlifei-fei.2015.ima-genetlargescalevisualrecognitionchallenge.internationaljournalofcomputervision(ijcv),115(3):211   252.1790

alisharifrazavian,hosseinazizpour,josephinesul-livan,andstefancarlsson.2014.id98featuresoff-the-shelf:anastoundingbaselineforrecognition.intheieeeconferenceoncomputervisionandpatternrecognition(cvpr)workshops,june.k.simonyananda.zisserman.2014.verydeepcon-volutionalnetworksforlarge-scaleimagerecogni-tion.corr,abs/1409.1556.ilyasutskever,oriolvinyals,andquocvvle.2014.sequencetosequencelearningwithneuralnet-works.inadvancesinneuralinformationprocess-ingsystems,pages3104   3112.t.tielemanandg.hinton.2012.lecture6.5   rmsprop:dividethegradientbyarunningaverageofitsrecentmagnitude.coursera:neuralnet-worksformachinelearning.seiyatokui,kentaoono,shoheihido,andjustinclayton.2015.chainer:anext-generationopensourceframeworkfordeeplearning.inproceedingsofworkshoponmachinelearningsystems(learn-ingsys)inthetwenty-ninthannualconferenceonneuralinformationprocessingsystems(nips).ramakrishnavedantam,clawrencezitnick,anddeviparikh.2014.cider:consensus-basedimagedescriptionevaluation.arxivpreprintarxiv:1411.5726.oriolvinyals,alexandertoshev,samybengio,anddumitruerhan.2015.showandtell:aneuralim-agecaptiongenerator.intheieeeconferenceoncomputervisionandpatternrecognition(cvpr),june.kelvinxu,jimmyba,ryankiros,kyunghyuncho,aaroncourville,ruslansalakhutdinov,richardzemel,andyoshuabengio.2015.show,attendandtell:neuralimagecaptiongenerationwithvisualat-tention.arxivpreprintarxiv:1502.03044.peteryoung,alicelai,micahhodosh,andjuliahockenmaier.2014.fromimagedescriptionstovisualdenotations:newsimilaritymetricsforse-manticid136overeventdescriptions.transac-tionsoftheassociationforcomputationallinguis-tics,2:67   78.zhi-huazhou,min-lingzhang,sheng-junhuang,andyu-fengli.2012.multi-instancemulti-labellearning.arti   cialintelligence,176(1):2291   2320.ayahziriklyandmasatohagiwara.2015.cross-lingualtransferofnamedentityrecognizerswithoutparallelcorpora.inproceedingsofthe53rdannualmeetingoftheassociationforcomputationallin-guisticsandthe7thinternationaljointconferenceonnaturallanguageprocessing(volume2:shortpapers),pages390   396,beijing,china,july.asso-ciationforcomputationallinguistics.c.l.zitnick,d.parikh,andl.vanderwende.2013.learningthevisualinterpretationofsentences.incomputervision(iccv),2013ieeeinternationalconferenceon,pages1681   1688,dec.