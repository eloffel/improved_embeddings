proceedings of the 54th annual meeting of the association for computational linguistics, pages 1802   1813,

berlin, germany, august 7-12, 2016. c(cid:13)2016 association for computational linguistics

1802

generatingnaturalquestionsaboutanimagenasrinmostafazadeh1,ishanmisra2,jacobdevlin3,margaretmitchell3xiaodonghe3,lucyvanderwende31universityofrochester,2carnegiemellonuniversity,3microsoftresearchnasrinm@cs.rochester.edu,lucyv@microsoft.comabstracttherehasbeenanexplosionofworkinthevision&languagecommunityduringthepastfewyearsfromimagecaptioningtovideotranscription,andansweringques-tionsaboutimages.thesetaskshavefo-cusedonliteraldescriptionsoftheimage.tomovebeyondtheliteral,wechoosetoexplorehowquestionsaboutanimageareoftendirectedatcommonsenseid136andtheabstracteventsevokedbyobjectsintheimage.inthispaper,weintroducethenoveltaskofvisualquestiongener-ation(vqg),wherethesystemistaskedwithaskinganaturalandengagingques-tionwhenshownanimage.weprovidethreedatasetswhichcoveravarietyofim-agesfromobject-centrictoevent-centric,withconsiderablymoreabstracttrainingdatathanprovidedtostate-of-the-artcap-tioningsystemsthusfar.wetrainandtestseveralgenerativeandretrievalmod-elstotacklethetaskofvqg.evaluationresultsshowthatwhilesuchmodelsaskreasonablequestionsforavarietyofim-ages,thereisstillawidegapwithhumanperformancewhichmotivatesfurtherworkonconnectingimageswithcommonsenseknowledgeandpragmatics.ourproposedtaskoffersanewchallengetothecommu-nitywhichwehopefurthersinterestinex-ploringdeeperconnectionsbetweenvision&language.1introductionwearewitnessingarenewedinterestininterdis-ciplinaryairesearchinvision&language,fromdescriptionsofthevisualinputsuchasimagecap-tioning(chenetal.,2015;fangetal.,2014;don-ahueetal.,2014;chenetal.,2015)andvideonaturalquestions:-wasanyoneinjuredinthecrash?-isthemotorcyclistalive?-whatcausedthisaccident?generatedcaption:-amanstandingnexttoamotorcycle.figure1:exampleimagealongwithitsnaturalquestionsandautomaticallygeneratedcaption.transcription(rohrbachetal.,2012;venugopalanetal.,2015),totestingcomputerunderstandingofanimagethroughquestionanswering(antoletal.,2015;malinowskiandfritz,2014).themostestablishedworkinthevision&languagecom-munityis   imagecaptioning   ,wherethetaskistoproducealiteraldescriptionoftheimage.ithasbeenshown(devlinetal.,2015;fangetal.,2014;donahueetal.,2014)thatareasonablelanguagemodelingpairedwithdeepvisualfeaturestrainedonlargeenoughdatasetspromiseagoodperfor-manceonimagecaptioning,makingitalesschal-lengingtaskfromlanguagelearningperspective.furthermore,althoughthistaskhasagreatvalueforcommunitiesofpeoplewhoarelow-sightedorcannotseeinallorsomeenvironments,foroth-ers,thedescriptiondoesnotaddanythingtowhatapersonhasalreadyperceived.thepopularityoftheimagesharingapplica-tionsinsocialmediaanduserengagementaroundimagesisevidencethatcommentingonpicturesisaverynaturaltask.apersonmightrespondtoanimagewithashortcommentsuchas   cool   ,   nicepic   oraskaquestion.imaginesomeonehassharedtheimageinfigure1.whatisthevery   rstquestionthatcomestomind?yourquestionismostprobablyverysimilartothequestionslistednexttotheimage,expressingconcernaboutthemotorcyclist(whoisnotevenpresentintheim-age).asyoucantell,naturalquestionsarenot1803

aboutwhatisseen,thepolicemenorthemotorcy-cle,butratheraboutwhatisinferredgiventheseobjects,e.g.,anaccidentorinjury.assuch,ques-tionsareoftenaboutabstractconcepts,i.e.,eventsorstates,incontrasttotheconcreteterms1usedinimagecaptioning.itisclearthatthecorrespond-ingautomaticallygeneratedcaption2forfigure1presentsonlyaliteraldescriptionofobjects.tomovebeyondtheliteraldescriptionofim-agecontent,weintroducethenoveltaskofvisualquestiongeneration(vqg),wheregivenanim-age,thesystemshould   askanaturalandengag-ingquestion   .askingaquestionthatcanbean-sweredsimplybylookingattheimagewouldbeofinteresttothecomputervisioncommunity,butsuchquestionsareneithernaturalnorengagingforapersontoanswerandsoarenotofinterestforthetaskofvqg.learningtoaskquestionsisanimportanttaskinnlpandismorethanasyntactictransformationofadeclarativesentence(vanderwende,2008).decidingwhattoaskaboutdemonstratesunder-standingandassuch,questiongenerationprovidesanindicationofmachineunderstanding,justassomeeducationalmethodsassessstudents   under-standingbytheirabilitytoaskrelevantquestions3.furthermore,trainingasystemtoaskagoodques-tion(notonlyansweraquestion)mayimbuethesystemwithwhatappearstobeacognitiveabilityuniquetohumansamongotherprimates(jordania,2006).developingtheabilitytoaskrelevantandto-the-pointquestionscanbeanessentialcompo-nentofanydynamiclearnerwhichseeksinforma-tion.suchanabilitycanbeanintegralcompo-nentofanyconversationalagent,eithertoengagetheuserinstartingaconversationortoelicittask-speci   cinformation.thecontributionsofthispapercanbesum-marizedasfollows:(1)inordertoenablethevqgresearch,wecarefullycreatedthreedatasetswithatotalof75,000questions,whichrangefromobject-toevent-centricimages,whereweshowthatvqgcoversawiderangeofabstracttermsincludingeventsandstates(section3).(2)wecollected25,000goldcaptionsforourevent-centricdatasetandshowthatthisdatasetpresents1concretetermsaretheonesthatcanbeexperiencedwith   vesenses.abstracttermsrefertointangiblethings,suchasfeelings,concepts,andqualities2throughoutthispaperweusethestate-of-the-artcap-tioningsystem(fangetal.,2014),henceforthmsrcaption-ingsystemhttps://www.captionbot.ai/,togener-atecaptions.3http://rightquestion.org/challengestothestate-of-the-artimagecaption-ingmodels(section3.3).(3)weperformanalysisofvariousgenerativeandretrievalapproachesandconcludethatend-to-enddeepneuralmodelsout-performotherapproachesonourmost-challengingdataset(section4).(4)weprovideasystematicevaluationmethodologyforthistask,whereweshowthattheautomaticmetric   id7stronglycorrelateswithhumanjudgments(section5.3).theresultsshowthatwhileourmodelslearntogeneratepromisingquestions,thereisstillalargegaptomatchhumanperformance,makingthegenerationofrelevantandnaturalquestionsanin-terestingandpromisingnewchallengetothecom-munity.2relatedworkforthetaskofimagecaptioning,datasetshavepri-marilyfocusedonobjects,e.g.pascalvoc(ever-inghametal.,2010)andmicrosoftcommonob-jectsincontext(mscoco)(linetal.,2014).mscoco,forexample,includescomplexevery-daysceneswith91basicobjectsin328kimages,eachpairedwith5captions.eventdetectionisthefocusinvideoprocessingandactiondetection,butthesedonotincludeatextualdescriptionoftheevent(yaoetal.,2011b;andrilukaetal.,2014;chaoetal.,2015;xiongetal.,2015).thenum-berofactionsineachofthesedatasetsisstillrel-ativelysmall,rangingfrom40(yaoetal.,2011a)to600(chaoetal.,2015)andallinvolvehuman-orientedactivity(e.g.   cooking   ,   gardening   ,   rid-ingabike   ).inourwork,wearefocusedongen-eratingquestionsforstaticimagesofevents,suchas      re   ,   explosion   or   snowing   ,whichhavenotyetbeeninvestigatedinanyoftheabovedatasets.visualquestionansweringisarelativelynewtaskwherethesystemprovidesananswertoaquestionabouttheimagecontent.themostno-table,visualquestionanswering(vqa)(antoletal.,2015),isanopen-ended(free-form)dataset,inwhichboththequestionsandtheanswersarecrowd-sourced,withworkerspromptedtoaskavisuallyveri   ablequestionwhichwill   stumpasmartrobot   .gaoetal.(2015)usedsimilarmethodologytocreateavisualquestionansweringdatasetinchinese.coco-qa(cqa)(renetal.,2015),incontrast,doesnotusehuman-authoredquestions,butgeneratesquestionsautomaticallyfromimagecaptionsofthemscocodatasetbyapplyingasetoftransformationrulestogeneratethewh-question.theexpectedanswersincqa1804

figure2:examplerightandwrongquestionsforthetaskofvqg.arebydesignlimitedtoobjects,numbers,colors,orlocations.amorein-depthanalysisofvqaandcqadatasetswillbepresentedinsection3.1.inthiswork,wefocusonquestionswhichareinterestingforapersontoanswer,notques-tionsdesignedtoevaluatecomputervision.are-centlypublishedworkonvqa,visual7w(zhuetal.,2016),establishesagroundinglinkontheobjectregionscorrespondingtothetextualan-swer.thissetupenablesasystemtoansweraquestionwithvisualanswers(inadditiontotex-tualanswers).theycollectasetof327,9397wmultiple-choiceqapairs,wheretheypointoutthat   where   ,   when   and   why   questionsoftenrequirehigh-levelcommonsensereasoning,goingbeyondspatialreasoningrequiredfor   which   or   who   questions.thisismoreinlinewiththetypeofquestionsthatvqgcaptures,however,thema-jorityofthequestionsinvisual7waredesignedtobeanswerablebyonlytheimage,makingthemunnaturalforaskingahuman.thus,learningtogeneratethequestionsinvqataskisnotause-fulsub-task,astheintersectionbetweenvqgandanyvqaquestionsisbyde   nitionminimal.previousworkonquestiongenerationfromtex-tualinputhasfocusedontwoaspects:thegram-maticality(wolfe,1976;mitkovandha,2003;heilmanandsmith,2010)andthecontentfocusofquestiongeneration,i.e.,   whattoaskabout   .forthelatter,severalmethodshavebeenexplored:(beckeretal.,2012)create   ll-in-the-blankques-tions,(mazidiandnielsen,2014)and(lindbergetal.,2013)usemanuallyconstructedquestiontemplates,while(labutovetal.,2015)usecrowd-sourcingtocollectasetoftemplatesandthenrankthepotentiallyrelevanttemplatesfortheselectedcontent.toourknowledge,neitheraretrievalmodelnoradeeprepresentationoftextualinput,presentedinourwork,haveyetbeenusedtogen-eratequestions.3datacollectionmethodologytaskde   nition:givenanimage,thetaskistogenerateanaturalquestionwhichcanpotentiallyengageahumaninstartingaconversation.ques-tionsthatarevisuallyveri   able,i.e.,thatcanbeansweredbylookingatonlytheimage,areout-sidethescopeofthistask.forinstance,infigure2,aquestionaboutthenumberofhorses(appear-inginthevqadataset)orthecolorofthe   eldisnotofinterest.althoughinthispaperwefocusonaskingaquestionaboutanimageinisolation,addingpriorcontextorhistoryofconversationisthenaturalnextstepinthisproject.wecollectedthevqgquestionsbycrowd-sourcingthetaskonamazonmechanicalturk(amt).weprovidedetailsonthepromptandthespeci   cinstructionsforalltheid104tasksinthispaperinthesupplementarymaterial.ourpromptwasverysuccessfulatcapturingnon-literalquestions,asthegoodquestioninfigure2demonstrates.inthefollowingsections,wede-scribeourprocessforselectingtheimagestobeincludedinthevqgdataset.westartwithimagesfrommscoco,whichenablesmeaningfulcom-parisonwithvqaandcqaquestions.giventhatitismorenaturalforpeopletoaskquestionsaboutevent-centricimages,weexploresourcingevent-fulimagesfromflickrandfromqueryinganim-agesearchengine.eachdatasourceisrepresentedby5,000images,with5questionsperimage.3.1vqgcoco   5000andvqgflickr   5000asour   rstdataset,wecollectedvqgques-tionsforasampleofimagesfromthemscocodataset4.inordertoenablecomparisonswithre-lateddatasets,wesampled5,000imagesofmscocowhichwerealsoannotatedbythecqadataset(renetal.,2015)andbyvqa(antoletal.,2015).wenamethisdatasetvqgcoco   5000.table1showsasamplemscocoimagealongwithannotationsinthevariousdatasets.asthecqaquestionsaregeneratedbyruleapplicationfromcaptions,theyarenotalwayscoherent.thevqaquestionsarewrittentoevaluatethedetailedvisualunderstandingofarobot,sotheirquestionsaremainlyvisuallygroundedandliteral.theta-bledemonstrateshowdifferentvqgquestionsarefromvqaandcqaquestions.infigure3weprovidestatisticsforthevariousannotationsonthatportionofthemscocoim-ageswhicharerepresentedinthevqgcoco   50004http://mscoco.org/1805

datasetannotationscoco-amanholdingaboxwithalargechocolatecovereddonut.cqa-whatisthemanholdingwithalargechocolate-covereddoughnutinit?vqa-isthisalargedoughnut?vqg-whyisthedonutsolarge?-isthatforaspeci   ccelebration?-haveyouevereatenadonutthatlargebefore?-isthatabigdonutoracake?-wheredidyougetthat?table1:datasetannotationsontheaboveimage.dataset.infigure3(a)wecomparethepercent-ageofobject-mentionsineachoftheannota-tions.object-mentionsarewordsassociatedwiththegold-annotatedobjectboundaryboxes5aspro-videdwiththemscocodataset.naturally,cococaptions(greenbars)havethehighestper-centageoftheseliteralobjects.sinceobject-mentionsareoftentheanswertovqaandcqaquestions,thosequestionsnaturallycontainob-jectslessfrequently.hence,weseethatvqgquestionsincludethementionofmoreofthoselit-eralobjects.figure3(b)showsthatcococap-tionshavealargervocabularysize,whichre   ectstheirlongerandmoredescriptivesentences.vqgshowsarelativelylargevocabularysizeaswell,indicatinggreaterdiversityinquestionformula-tionthanvqaandcqa.moreover,figure3(c)showsthattheverbpartofspeechisrepresentedwithhighfrequencyinourdataset.figure3(d)depictsthepercentageofabstracttermssuchas   think   or   win   inthevocabulary.followingferraroetal.(2015),weusealistofmostcommonabstracttermsinenglish(van-derwendeetal.,2015),andcountalltheotherwordsexceptasetoffunctionwordsasconcrete.this   guresupportsourexpectationthatvqgcoversmoreabstractconcepts.furthermore,fig-ure3(e)showsinter-annotationtextualsimilarityaccordingtotheid7metric(papinenietal.,2002).interestingly,vqgshowsthehighestinter-annotatortextualsimilarity,whichre   ectsontheexistenceofconsensusamonghumanforasking5notethatmscocoannotatesonly91objectcategories.anaturalquestion,evenforobject-centricimagesliketheonesinmscoco.figure3:comparisonofvariousannotationsonthemscocodataset.(a)percentageofgoldob-jectsusedinannotations.(b)vocabularysize(c)percentageofverbpos(d)percentageofabstractterms(e)inter-annotationtextualsimilarityscore.themscocodatasetislimitedintermsoftheconceptsitcovers,duetoitspre-speci   edsetofobjectcategories.wordfrequencyinvqgcoco   5000dataset,asdemonstratedinfigure4,bearsthisout,withthewords   cat   and   dog   thefourthand   fthmostfrequentwordsinthedataset.notshowninthefrequencygraphisthatwordssuchas   wedding   ,   injured   ,or   accident   areattheverybottomoffrequencyrankinglist.thisobservationmotivatedthecollectionofthevqgflickr   5000dataset,withimagesappearingasthemiddlephotoinastory-fullphotoalbum(huangetal.,2016)onflickr6.thedetailsaboutthisdatasetcanbefoundinthesupplementaryma-terial.3.2vqgbing   5000toobtainamorerepresentativevisualizationofspeci   ceventtypes,wequeriedasearchengine7with1,200event-centricquerytermswhichwereobtainedasfollows:weaggregatedall   event   and   process   hyponymsinid138(miller,1995),1,000mostfrequenttimebankevents(puste-jovskyetal.,2003)andasetofmanuallycurated30stereotypicalevents,fromwhichweselectedthetop1,200queriesbasedonprojectgutenbergwordfrequencies.foreachquery,wecollectedthe   rstfourto   veimagesretrieved,foratotal6http://www.flickr.com7https://datamarket.azure.com/dataset/bing/search1806

figure4:frequencygraphoftop40wordsinvqgcoco   5000dataset.figure5:averageannotationlengthofthethreevqgdatasets.of5,000images,having   rstusedid104to   lteroutimagesdepictinggraphicsandcar-toons.asimilarwordfrequencyanalysisshowsthatthevqgbing   5000datasetindeedcontainsmorewordsaskingaboutevents:happen,work,causeappearintop40words,whichwasouraimincreatingthebingdataset.statistics:ourthreedatasetstogethercoverawiderangeofvisualconceptsandevents,total-ing15,000imageswith75,000questions.fig-ure5drawsthehistogramofnumberoftokensinvqgquestions,wheretheaveragequestionlengthis6tokens.figure6visualizestheid165distribution(withn=6)ofquestionsinthethreevqgdatasets8.table2showsthestatisticsoftheid104task.3.3captionsbing   5000thewordfrequenciesofquestionsaboutthevqgbing   5000datasetindicatethatthisdataset8pleaserefertoourwebpageonhttp://research.microsoft.com/en-us/downloadstogetalinktoadynamicvisualizationandstatisticsofallid165sequences.#allimages15,000#questionsperimage5#allworkersparticipated308max#questionswrittenbyoneworker6,368averageworktimeperworker(sec)106.5medianworktimeperworker(sec)23.0averagepaymentperquestion(cents)6.0table2:statisticsofid104task,aggre-gatingallthreedatasets.issubstantiallydifferentfromthemscocodataset.humanevaluationresultsofare-centwork(tranetal.,2016)furthercon   rmsthesigni   cantimagecaptioningqualitydegra-dationonout-of-domaindata.tofurtherex-plorethisdifference,wecrowdsourced5cap-tionsforeachimageinthevqgbing   5000datasetusingthesamepromptasusedtosourcethemscococaptions.wecallthisnewdatasetcaptionsbing   5000.table3showstheresultsoftestingthestate-of-the-artmsrcaptioningsys-temonthecaptionsbing   5000datasetascom-paredtothemscocodataset,measuredbythestandardid7(papinenietal.,2002)andme-teor(denkowskiandlavie,2014)metrics.thewidegapintheresultsfurthercon   rmsthatin-deedthevqgbing   5000datasetcoversanewclassofimages;wehopetheavailabilityofthisnewdatasetwillencourageincludingmorediversedomainsforimagecaptioning.id7meteorbingmscocobingmscoco0.1010.2910.1510.247table3:imagecaptioningresultstogetherwiththispaperwearereleasinganex-tendedsetofvqgdatasettothecommunity.wehopethattheavailabilityofthisdatasetwillen-couragetheresearchcommunitytotacklemoreend-goalorientedvision&languagetasks.4modelsinthissectionwepresentseveralgenerativeandretrievalmodelsfortacklingthetaskofvqg.foralltheforthcomingmodelsweusethevggnet(simonyanandzisserman,2014)architectureforcomputingdeepconvolutionalimagefeatures.weprimarilyusethe4096-dimensionaloutputthelastfullyconnectedlayer(fc7)astheinputtothegen-erativemodels.1807

figure6:vqgid165sequences.   end   tokendistinguishesnaturalendingwithid165cut-off.figure7:threedifferentgenerativemodelsfortacklingthetaskofvqg.4.1generativemodelsfigure7representsanoverviewofourthreegen-erativemodels.themelmmodel(fangetal.,2014)isapipelinestartingfromasetofcandi-datewordprobabilitieswhicharedirectlytrainedonimages,whichthengoesthroughamaximumid178(me)languagemodel.themtmodelisasequence2sequencetranslationmodel(choetal.,2014;sutskeveretal.,2014)whichdirectlytranslatesadescriptionofanimageintoaques-tion,whereweusedthemscococaptionsandcaptionsbing   5000asthesourceoftranslation.thesetwomodelstendedtogeneratelesscoher-entsentences,detailsofwhichcanbefoundinthesupplementarymaterial.weobtainedthebestre-sultsbyusinganend-to-endneuralmodel,gid56,asfollows.gatedrecurrentneuralnetwork(gid56):thisgenerationmodelisbasedonthestate-of-the-artmultimodalrecurrentneuralnetworkmodelusedforimagecaptioning(devlinetal.,2015;vinyalsetal.,2015).first,wetransformthefc7vectortoa500-dimensionalvectorwhichservesastheinitialrecurrentstatetoa500-dimensionalgatedrecurrentunit(gru).weproducetheout-putquestiononewordatatimeusingthegru,untilwehittheend-of-sentencetoken.wetrainthegruandthetransformationmatrixjointly,butwedonotback-propagatetheid98duetothesizeofthetrainingdata.theneuralnetworkistrainedusingstochasticgradientdescentwithearlystop-ping,anddecodedusingabeamsearchofsize8.thevocabularyconsistsofallwordsseen3ormoretimesinthetraining,whichamountsto1942uniquetokensinthefulltrainingset.unknownwordsaremappedtotoan<unk>tokenduringtraining,butwedonotallowthedecodertopro-ducethistokenattesttime.4.2retrievalmethodsretrievalmodelsusethecaptionofanearestneighbortrainingimagetolabelthetestimage(hodoshetal.,2013;devlinetal.,2015;farhadietal.,2010;ordonezetal.,2011).forthetaskofimagecaptioning,ithasbeenshownthatupto80%ofthecaptionsgeneratedattesttimebyanearstate-of-the-artgenerationapproach(vinyalsetal.,2015)wereexactlyidenticaltothetrainingsetcaptions,whichsuggeststhatreusingtrainingannotationscanachievegoodresults.moreover,basicnearestneighborapproachestoimagecap-tioningonthemscocodatasetareshowntoout-performgenerationmodelsaccordingtoautomaticmetrics(devlinetal.,2015).theperformanceofretrievalmodelsofcoursedependsonthediversityofthedataset.weimplementedseveralretrievalmodelscus-tomizedforthetaskofvqg.asthe   rststep,wecomputeknearestneighborimagesforeachtestimageusingthefc7featurestogetacandidatepool.weobtainedthemostcompetitiveresultsbysettingkdynamically,asopposedtotheearlier1808

q.explosionhurricaneraincloudcaraccidenthuman-whatcausedthisexplosion?-wasthisexplosionanaccident?-whatcausedthedamagetothiscity?-whathappenedtothisplace?-arethoserainclouds?-diditrain?-didthedriversofthisaccidentlivethroughit?-howfastweretheygoing?gid56-howmuchdidthe   recost?-whatisbeingburnedhere?-whathappenedtothecity?-whatcausedthefall?-whatkindofcloudsarethese?-wasthereabadstorm?-howdidthecarcrash?-whathappenedtothetrailer?knn-whatcausedthis   re?-whatstatewasthisearthquakein?-diditrain?-wasanybodyhurtinthisaccident?caption-atrainwithsmokecomingfromit.-apileofdirt.-somecloudsinacloudyday.-amanstandingnexttoamotorcycle.table4:samplegenerationsbydifferentsystemsonvqgbing   5000,inorder:humanconsensusandhumanrandom,gid56bingandgid56all,knn+minid7   all,msrcaptions.qisthequery-term.workswhich   xkthroughoutthetesting.weob-servedthatcandidateimagesbeyondacertaindis-tancemadethepoolnoisy,hence,weestablishaparametercalledmax-distancewhichisanupperboundforincludinganeighborimageinthepool.moreover,ourexperimentsshowedthatifthereexistsaverysimilarimagetothetestimage,thecandidatepoolcanbeignoredandthattestimageshouldbecometheonlycandidate9.foraddress-ingthis,wesetamin-distanceparameter.alltheseparametersweretunedonthecorrespond-ingvalidationsetsusingthesmoothed-id7(linandoch,2004)metricagainstthehumanrefer-encequestions.giventhateachimageinthepoolhas   veques-tions,wede   netheone-bestquestiontobethequestionwiththehighestsemanticsimilarity10totheotherfourquestions.thisresultsinapoolofkcandidatequestions.thefollowingsettingswereusedforour   nalretrievalmodels:   1-nn:setk=1,whichretrievestheclosestim-ageandemitsitsone-best.   id92+min:setk=30withmax-distance=0.35,andmin-distance=0.1.amongthe309attesttime,thefrequencyof   ndingatrainsetimagewithdistance   0.1is7.68%,8.4%and3.0%incoco,flickrandbingdatasetsrespectively.10weuseid7tocomputetextualsimilarity.thispro-cesseliminatesoutlierquestionsperimage.candidatequestions(one-bestofeachimage),   ndthequestionwiththehighestsimilaritytotherestofthepoolandemitthat:wecomputethetextualsimilarityaccordingthetwometrics,smoothed-id7andaverage-id97(gensim)11.table4showsafewexampleimagesalongwiththegenerationsofourbestperformingsystems.formoreexamplespleaserefertothewebpageoftheproject.5evaluationwhileinvqgthesetofpossiblequestionsisnotlimited,thereisconsensusamongthenaturalquestions(discussedinsection3.1)whichenablesmeaningfulevaluation.althoughhumanevalua-tionistheidealformofevaluation,itisimpor-tantto   ndanautomaticmetricthatstronglycorre-lateswithhumanjudgmentinordertobenchmarkprogressonthetask.5.1humanevaluationthequalityoftheevaluationisinpartdeterminedbyhowtheevaluationispresented.forinstance,11average-id97referstothesentence-leveltextualsimilaritymetricwherewecomputethecosinesimilaritybe-tweentwosentencesbyaveragingtheirword-levelid97(mikolovetal.,2013)vectorrepresentations.hereweusethegensimsoftwareframework(  reh  u  rekandsojka,2010).1809

itisimportantforthehumanjudgestoseevar-ioussystemhypothesesatthesametimeinor-dertogiveacalibratedrating.wecrowdsourcedourhumanevaluationonamt,askingthreecrowdworkerstoeachratethequalityofcandidateques-tionsonathree-pointsemanticscale.5.2automaticevaluationthegoalofautomaticevaluationistomeasurethesimilarityofsystem-generatedquestionhypothe-sesandthecrowdsourcedquestionreferences.tocaptureid165overlapandtextualsimilaritybe-tweenhypothesesandreferences,weusestandardmachinetranslationmetrics,id7(papinenietal.,2002)andmeteor(denkowskiandlavie,2014).weuseid7withequalweightsupto4-gramsanddefaultsettingofmeteorversion1.5.additionallyweuse   id7(galleyetal.,2015)whichisspeci   callytailoredtowardsgen-erationtaskswithdiversereferences,suchascon-versations.   id7requiresratingperreference,distinguishingbetweenthequalityoftherefer-ences.forthispurpose,wecrowd-sourcedthreehumanratings(onascaleof1-3)perreferenceandusedthemajorityrating.thepairwisecorrelationalanalysisofhumanandautomaticmetricsispresentedintable6,wherewereportonpearson   sr,spearman   s  andkendall   s  correlationcoef   cients.asthistablereveals,   id7stronglycorrelateswithhumanjudgmentandwesuggestitasthemainevaluationmetricfortestingavqgsystem.itisimportanttonotethatid7isalsoverycompetitivewith   id7,showingstrongcorrelationswithhumanjudgment.hence,werecommendusingid7foranyfurtherbenchmarkingandoptimizationpur-poses.id7canalsobeusedasaproxyfor   id7forevaluationpurposeswheneverratingperreferencearenotavailable.5.3resultsinthissection,wepresentthehumanandauto-maticmetricevaluationresultsofthemodelsin-troducedearlier.werandomlydividedeachvqg-5000datasetintotrain(50%),val(25%)andtest(25%)sets.inordertoshedsomelightondiffer-encesbetweenourthreedatasets,wepresenttheevaluationresultsseparatelyoneachdatasetinta-ble5.eachmodel(section4.2)isoncetrainedonalltrainsets,andoncetrainedonlyonitscor-respondingtrainset(representedasxinthere-sultstable).forqualitycontrolandfurtherinsightonthetask,weincludetwohumanannotationsamongourmodels:   humanconsensus   (thesameasone-best)whichindicatestheconsensushumanannotationonthetestimageand   humanrandom   whichisarandomlychosenannotationamongthe   vehumanannotations.itisquiteinterestingtoseethatamongthehu-manannotations,humanconsensusachievescon-sistentlyhigherscoresthanhumanrandom.thisfurtherveri   esthatthereisindeedacommonintuitionaboutwhatisthemostnaturalques-tiontoaskaboutagivenimage.asthere-sultsofhumanevaluationintable5shows,gid56allperformsthebestascomparedwithalltheothermodelsin2/3ofruns.allthemod-elsachievetheirbestscoreonvqgcoco   5000,whichwasexpectedgiventhelessdiversesetofimages.usingautomaticmetrics,thegid56xmodeloutperformsothermodelsaccordingtoallthreemetricsonthevqgbing   5000dataset.amongretrievalmodels,themostcompetitiveisid92+minid7all,whichperformsthebestonvqgcoco   5000andvqgflickr   5000datasetsaccordingtoid7and   id7score.thisfur-thercon   rmsoureffectiveretrievalmethodologyforincludingmin-distanceandid165overlapsimilaritymeasures.furthermore,theboostfrom1-nntoid92modelsisconsiderableaccordingtobothhumanandautomaticmetrics.itisimpor-tanttonotethatnoneoftheretrievalmodelsbeatthegid56modelonthebingdataset.thisaddi-tionallyshowsthatourbingdatasetisinfactmoredemanding,makingitameaningfulchallengeforthecommunity.6discussionweintroducedthenoveltaskof   visualques-tiongeneration   ,wheregivenanimage,thesys-temistaskedwithaskinganaturalquestion.weprovidethreedistinctdatasets,eachcoveringavarietyofimages.themostchallengingisthebingdataset,requiringsystemstogenerateques-tionswithevent-centricconceptssuchas   cause   ,   event   ,   happen   ,etc.,fromthevisualinput.fur-thermore,weshowthatourbingdatasetpresentschallengingimagestothestate-of-the-artcaption-ingsystems.weencouragethecommunitytore-porttheirsystemresultsonthebingtestdatasetandaccordingtothe   id7automaticmetric.allthedatasetswillbereleasedtothepublic12.thisworkfocusesondevelopingthecapabil-12please   ndvisualquestiongenerationunderhttp://research.microsoft.com/en-us/downloads.1810

humanconsensushumanrandomgid56xgid56all1-nnid7   x1-nngensim   xid92+minid7   xid92+mingensim   x1-nnid7   all1-nngensim   allid92+minid7   allid92+mingensim   allhumanevaluationbing2.492.381.351.761.721.721.691.571.721.731.751.58coco2.492.381.661.941.811.821.881.641.821.821.961.74flickr2.342.261.241.571.441.441.541.281.461.461.521.30automaticevaluationid7bing87.183.712.311.19.09.011.27.99.09.011.87.9coco86.083.513.914.211.011.019.111.510.710.719.211.2flickr84.483.69.99.97.47.410.95.97.67.611.75.8met.bing62.258.816.215.814.714.715.414.714.714.715.514.7coco60.858.318.518.516.216.219.717,415.915.919.517.5flickr59.958.614.314.912.312.313.612.612.612.614.613.0   id7bing63.3857.2511.610.88.288.2810.247.118.438.4311.017.59coco60.8156.7912.4512.469.859.8516.149.969.789.7816.299.96flickr62.3757.349.369.556.476.479.495.376.736.739.85.26table5:resultsofevaluatingvariousmodelsaccordingtodifferentmetrics.xrepresentstrainingonthecorrespondingdatasetintherow.humanscorepermodeliscomputedbyaveraginghumanscoreacrossmultipleimages,wherehumanscoreperimageisthemedianratingacrossthethreeraters.meteorid7   id7r0.916(4.8e-27)0.915(4.6e-27)0.915(5.8e-27)  0.628(1.5e-08)0.67(7.0e-10)0.702(5.0e-11)  0.476(1.6e-08)0.51(7.9e-10)0.557(3.5e-11)table6:correlationsofautomaticmetricsagainsthumanjudgments,withp-valuesinparentheses.itytoaskrelevantandto-the-pointquestions,akeyintelligentbehaviorthatanaisystemshoulddemonstrate.webelievethatvqgisonesteptowardsbuildingsuchasystem,whereanengag-ingquestioncannaturallystartaconversation.tocontinueprogressonthistask,itispossibletoin-creasethesizeofthetrainingdata,butwealsoex-pecttodevelopmodelsthatwilllearntogeneralizetounseenconcepts.forinstance,considertheex-amplesofsystemerrorsintable7,wherevisualfeaturescanbeenoughfordetectingthespeci   csetofobjectsineachimage,butthesystemcannotmakesenseofthecombinationofpreviouslyun-seenconcepts.anothernaturalfutureextensionofthisworkistoincludequestiongenerationwithinaconversationalsystem(sordonietal.,2015;lietal.,2016),wherethecontextandconversationhistoryaffectthetypesofquestionsbeingasked.human-howlongdidittaketomakethaticesculpture?-isthedoglookingtotakeashower?gid56-howlonghashebeenhiking?-isthisinahotelroom?knn-howdeepwasthesnow?-doyouenjoythelightinthisbathroom?table7:examplesoferrorsingeneration.therowsarehumanconsensus,gid56all,andknn+minid7   all.acknowledgmentwewouldliketothanktheanonymousreviewersfortheirinvaluablecomments.wethanklarryzitnickanddeviparikhfortheirhelpfuldiscus-sionsregardingthiswork,rebeccahansonforhergreathelpindatacollection,michelgalleyforhisguidelinesonevaluation,andbilldolanforhisvaluablefeedbackthroughoutthiswork.1811

referencesmykhayloandriluka,leonidpishchulin,petergehler,andberntschiele.2014.2dhumanposeestima-tion:newbenchmarkandstateoftheartanalysis.inieeeconferenceoncomputervisionandpat-ternrecognition(cvpr),june.stanislawantol,aishwaryaagrawal,jiasenlu,mar-garetmitchell,dhruvbatra,c.lawrencezitnick,anddeviparikh.2015.vqa:visualquestionan-swering.ininternationalconferenceoncomputervision(iccv).leebecker,sumitbasu,andlucyvanderwende.2012.mindthegap:learningtochoosegapsforquestiongeneration.inproceedingsofthe2012conferenceofthenorthamericanchapteroftheassociationforcomputationallinguistics:humanlanguagetechnologies,pages742   751,montr  eal,canada,june.associationforcomputationallin-guistics.yu-weichao,zhanwang,yugenghe,jiaxuanwang,andjiadeng.2015.hico:abenchmarkforrecog-nizinghuman-objectinteractionsinimages.inpro-ceedingsoftheieeeinternationalconferenceoncomputervision.jianfuchen,polinakuznetsova,davidwarren,andyejinchoi.2015.d  ej`aimage-captions:acor-pusofexpressivedescriptionsinrepetition.inpro-ceedingsofthe2015conferenceofthenorthamer-icanchapteroftheassociationforcomputationallinguistics:humanlanguagetechnologies,pages504   514,denver,colorado,may   june.associationforcomputationallinguistics.kyunghyuncho,bartvanmerri  enboer,caglargul-cehre,dzmitrybahdanau,fethibougares,holgerschwenk,andyoshuabengio.2014.learningphraserepresentationsusingid56encoder-decoderforstatisticalmachinetranslation.arxivpreprintarxiv:1406.1078.michaeldenkowskiandalonlavie.2014.meteoruniversal:languagespeci   ctranslationevaluationforanytargetlanguage.inproceedingsoftheeacl2014workshoponstatisticalmachinetranslation.jacobdevlin,haocheng,haofang,saurabhgupta,lideng,xiaodonghe,geoffreyzweig,andmar-garetmitchell.2015.languagemodelsforimagecaptioning:thequirksandwhatworks.inproceed-ingsofthe53rdannualmeetingoftheassociationforcomputationallinguisticsandthe7thinterna-tionaljointconferenceonnaturallanguagepro-cessing(volume2:shortpapers),pages100   105,beijing,china,july.associationforcomputationallinguistics.jeffdonahue,lisaannehendricks,sergioguadar-rama,marcusrohrbach,subhashinivenugopalan,katesaenko,andtrevordarrell.2014.long-termrecurrentconvolutionalnetworksforvisualrecogni-tionanddescription.corr,abs/1411.4389.markeveringham,lucgool,christopherk.williams,johnwinn,andandrewzisserman.2010.thepascalvisualobjectclasses(voc)challenge.int.j.comput.vision,88(2):303   338,june.haofang,saurabhgupta,forrestn.iandola,ru-peshsrivastava,lideng,piotrdoll  ar,jianfenggao,xiaodonghe,margaretmitchell,johnc.platt,c.lawrencezitnick,andgeoffreyzweig.2014.fromcaptionstovisualconceptsandback.corr,abs/1411.4952.alifarhadi,mohsenhejrati,mohammadaminsadeghi,peteryoung,cyrusrashtchian,juliahockenmaier,anddavidforsyth.2010.everypic-turetellsastory:generatingsentencesfromimages.inproceedingsofthe11theuropeanconferenceoncomputervision:partiv,eccv   10,pages15   29,berlin,heidelberg.springer-verlag.francisferraro,nasrinmostafazadeh,ting-haohuang,lucyvanderwende,jacobdevlin,michelgalley,andmargaretmitchell.2015.asurveyofcurrentdatasetsforvisionandlanguageresearch.inproceedingsofthe2015conferenceonempiri-calmethodsinnaturallanguageprocessing,pages207   213,lisbon,portugal,september.associationforcomputationallinguistics.michelgalley,chrisbrockett,alessandrosordoni,yangfengji,michaelauli,chrisquirk,margaretmitchell,jianfenggao,andbilldolan.2015.deltaid7:adiscriminativemetricforgenerationtaskswithintrinsicallydiversetargets.inproceed-ingsofthe53rdannualmeetingoftheassociationforcomputationallinguisticsandthe7thinterna-tionaljointconferenceonnaturallanguagepro-cessing(volume2:shortpapers),pages445   450,beijing,china,july.associationforcomputationallinguistics.haoyuangao,junhuamao,jiezhou,zhihenghuang,leiwang,andweixu.2015.areyoutalkingtoamachine?datasetandmethodsformultilingualim-agequestionanswering.corr,abs/1505.05612.michaelheilmanandnoaha.smith.2010.goodquestion!statisticalrankingforquestiongenera-tion.inhumanlanguagetechnologies:the2010annualconferenceofthenorthamericanchap-teroftheassociationforcomputationallinguistics,pages609   617,losangeles,california,june.as-sociationforcomputationallinguistics.micahhodosh,peteryoung,andjuliahockenmaier.2013.framingimagedescriptionasarankingtask:data,modelsandevaluationmetrics.j.artif.int.res.,47(1):853   899,may.ting-haohuang,francisferraro,nasrinmostafazadeh,ishanmisra,aishwaryaagrawal,jacobdevlin,rossb.girshick,xiaodonghe,pushmeetkohli,dhruvbatra,c.lawrencezitnick,deviparikh,lucyvanderwende,michelgalley,andmargaretmitchell.2016.visualstorytelling.1812

inproceedingsofnaacl2016.associationforcomputationallinguistics.josephjordania.2006.whoaskedthefirstquestion?theoriginsofhumanchoralsinging,intelligence,languageandspeech.logos.igorlabutov,sumitbasu,andlucyvanderwende.2015.deepquestionswithoutdeepunderstanding.inproceedingsofthe53rdannualmeetingoftheassociationforcomputationallinguisticsandthe7thinternationaljointconferenceonnaturallan-guageprocessing(volume1:longpapers).jiweili,michelgalley,chrisbrockett,jianfenggao,andbilldolan.2016.apersona-basedneuralcon-versationmodel.inproceedingsofthe2015con-ferenceofthenorthamericanchapteroftheasso-ciationforcomputationallinguistics:humanlan-guagetechnologies.associationforcomputationallinguistics.chin-yewlinandfranzjosefoch.2004.auto-maticevaluationofmachinetranslationqualityus-inglongestcommonsubsequenceandskip-bigramstatistics.inproceedingsofthe42ndannualmeet-ingonassociationforcomputationallinguistics,acl   04,stroudsburg,pa,usa.associationforcomputationallinguistics.tsung-yilin,michaelmaire,sergebelongie,jameshays,pietroperona,devaramanan,piotrdollar,andc.lawrencezitnick.2014.microsoftcoco:commonobjectsincontext.corr,abs/1405.0312.davidlindberg,fredpopowich,johnnesbit,andphilwinne.2013.generatingnaturallanguageques-tionstosupportlearningon-line.inproceedingsofthe14theuropeanworkshoponnaturallanguagegeneration,pages105   114,so   a,bulgaria,au-gust.associationforcomputationallinguistics.mateuszmalinowskiandmariofritz.2014.amulti-worldapproachtoquestionansweringaboutreal-worldscenesbasedonuncertaininput.inadvancesinneuralinformationprocessingsystems27,pages1682   1690.karenmazidiandrodneyd.nielsen.2014.linguis-ticconsiderationsinautomaticquestiongeneration.inproceedingsofthe52ndannualmeetingoftheassociationforcomputationallinguistics(volume2:shortpapers),pages321   326,baltimore,mary-land,june.associationforcomputationallinguis-tics.tomasmikolov,ilyasutskever,kaichen,gregorys.corrado,andjeffreydean.2013.distributedrep-resentationsofwordsandphrasesandtheircom-positionality.inadvancesinneuralinformationprocessingsystems26:27thannualconferenceonneuralinformationprocessingsystems2013.pro-ceedingsofameetinghelddecember5-8,2013,laketahoe,nevada,unitedstates.,pages3111   3119.georgea.miller.1995.id138:alexicaldatabaseforenglish.commun.acm,38(11):39   41,novem-ber.ruslanmitkovandleanha.2003.computer-aidedgenerationofmultiple-choicetests.injillbursteinandclaudialeacock,editors,proceedingsofthehlt-naacl03workshoponbuildingeducationalapplicationsusingnaturallanguageprocessing,pages17   22.vicenteordonez,girishkulkarni,andtamaral.berg.2011.im2text:describingimagesusing1millioncaptionedphotographs.inneuralinformationpro-cessingsystems(nips).kishorepapineni,salimroukos,toddward,andwei-jingzhu.2002.id7:amethodforautomaticevaluationofmachinetranslation.inproceedingsofthe40thannualmeetingonassociationforcom-putationallinguistics,acl   02,pages311   318,stroudsburg,pa,usa.associationforcomputa-tionallinguistics.j.pustejovsky,p.hanks,r.sauri,a.see,r.gaizauskas,a.setzer,d.radev,b.sund-heim,d.day,l.ferro,andm.lazo.2003.thetimebankcorpus.inproceedingsofcorpuslinguistics2003,pages647   656,lancaster,march.radim  reh  u  rekandpetrsojka.2010.softwareframeworkfortopicmodellingwithlargecor-pora.inproceedingsofthelrec2010workshoponnewchallengesfornlpframeworks,pages45   50,valletta,malta,may.elra.http://is.muni.cz/publication/884893/en.mengyeren,ryankiros,andrichardzemel.2015.questionansweringaboutimagesusingvisualse-manticembeddings.indeeplearningworkshop,icml2015.marcusrohrbach,sikandaramin,mykhayloan-driluka,andberntschiele.2012.adatabasefor   negrainedactivitydetectionofcookingactivities.inieeeconferenceoncomputervisionandpatternrecognition(cvpr).ieee,ieee,june.k.simonyananda.zisserman.2014.verydeepcon-volutionalnetworksforlarge-scaleimagerecogni-tion.corr,abs/1409.1556.alessandrosordoni,michelgalley,michaelauli,chrisbrockett,yangfengji,margaretmitchell,jian-yunnie,jianfenggao,andbilldolan.2015.aneuralnetworkapproachtocontext-sensitivegen-erationofconversationalresponses.inproceed-ingsofthe2015conferenceofthenorthameri-canchapteroftheassociationforcomputationallinguistics:humanlanguagetechnologies,pages196   205,denver,colorado,may   june.associationforcomputationallinguistics.1813

ilyasutskever,oriolvinyals,andquocv.le.2014.sequencetosequencelearningwithneuralnet-works.inadvancesinneuralinformationprocess-ingsystems27:annualconferenceonneuralin-formationprocessingsystems2014,december8-132014,montreal,quebec,canada,pages3104   3112.kennethtran,xiaodonghe,leizhang,jiansun,cor-neliacarapcea,christhrasher,chrisbuehler,andchrissienkiewicz.2016.richimagecaptioninginthewild.inproceedingsofdeepvisionworkshopatcvpr2016.ieee,june.lucyvanderwende,arulmenezes,andchrisquirk.2015.anamrparserforenglish,french,german,spanishandjapaneseandanewamr-annotatedcor-pus.proceedingsofnaacl2015,june.lucyvanderwende.2008.theimportanceofbeingimportant:questiongeneration.ininworkshoponthequestiongenerationsharedtaskandevalua-tionchallenge,arlington,va.subhashinivenugopalan,huijuanxu,jeffdonahue,marcusrohrbach,raymondmooney,andkatesaenko.2015.translatingvideostonaturallan-guageusingdeeprecurrentneuralnetworks.inproceedingsthe2015conferenceofthenorthamericanchapteroftheassociationforcompu-tationallinguistics   humanlanguagetechnolo-gies(naaclhlt2015),pages1494   1504,denver,colorado,june.oriolvinyals,alexandertoshev,samybengio,anddumitruerhan.2015.showandtell:aneuralim-agecaptiongenerator.incomputervisionandpat-ternrecognition.johnhwolfe.1976.automaticquestiongener-ationfromtext-anaidtoindependentstudy.inacmsigcueoutlook,volume10,pages104   112.acm.yuanjunxiong,kaizhu,dahualin,andxiaooutang.2015.recognizecomplexeventsfromstaticim-agesbyfusingdeepchannels.intheieeeconfer-enceoncomputervisionandpatternrecognition(cvpr),june.bangpengyao,xiaoyejiang,adityakhosla,andylailin,leonidasj.guibas,andlifei-fei.2011a.actionrecognitionbylearningbasesofactionat-tributesandparts.ininternationalconferenceoncomputervision(iccv),barcelona,spain,novem-ber.bangpengyao,xiaoyejiang,adityakhosla,andylailin,leonidasj.guibas,andlifei-fei.2011b.hu-manactionrecognitionbylearningbasesofactionattributesandparts.ininternationalconferenceoncomputervision(iccv),barcelona,spain,novem-ber.yukezhu,olivergroth,michaels.bernstein,andlifei-fei.2016.visual7w:groundedquestionansweringinimages.inieeeconferenceoncom-putervisionandpatternrecognition(cvpr).ieee,ieee.