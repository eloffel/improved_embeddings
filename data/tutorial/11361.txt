6
1
0
2

 

v
o
n
6
1

 

 
 
]

g
l
.
s
c
[
 
 

1
v
7
9
3
5
0

.

1
1
6
1
:
v
i
x
r
a

id23 with unsupervised
auxiliary tasks

max jaderberg   , volodymyr mnih*, wojciech marian czarnecki*
tom schaul, joel z leibo, david silver & koray kavukcuoglu
deepmind
london, uk
{jaderberg,vmnih,lejlot,schaul,jzl,davidsilver,korayk}@google.com

abstract

deep id23 agents have achieved state-of-the-art results by di-
rectly maximising cumulative reward. however, environments contain a much
wider variety of possible training signals. in this paper, we introduce an agent
that also maximises many other pseudo-reward functions simultaneously by rein-
forcement learning. all of these tasks share a common representation that, like
unsupervised learning, continues to develop in the absence of extrinsic rewards.
we also introduce a novel mechanism for focusing this representation upon ex-
trinsic rewards, so that learning can rapidly adapt to the most relevant aspects
of the actual task. our agent signi   cantly outperforms the previous state-of-the-
art on atari, averaging 880% expert human performance, and a challenging suite
of    rst-person, three-dimensional labyrinth tasks leading to a mean speedup in
learning of 10   and averaging 87% expert human performance on labyrinth.

natural and arti   cial agents live in a stream of sensorimotor data. at each time step t, the agent
receives observations ot and executes actions at. these actions in   uence the future course of the
sensorimotor stream. in this paper we develop agents that learn to predict and control this stream,
by solving a host of id23 problems, each focusing on a distinct feature of the
sensorimotor stream. our hypothesis is that an agent that can    exibly control its future experiences
will also be able to achieve any goal with which it is presented, such as maximising its future
rewards.
the classic id23 paradigm focuses on the maximisation of extrinsic reward. how-
ever, in many interesting domains, extrinsic rewards are only rarely observed. this raises questions
of what and how to learn in their absence. even if extrinsic rewards are frequent, the sensorimotor
stream contains an abundance of other possible learning targets. traditionally, unsupervised learn-
ing attempts to reconstruct these targets, such as the pixels in the current or subsequent frame. it
is typically used to accelerate the acquisition of a useful representation. in contrast, our learning
objective is to predict and control features of the sensorimotor stream, by treating them as pseudo-
rewards for id23. intuitively, this set of tasks is more closely matched with the
agent   s long-term goals, potentially leading to more useful representations.
consider a baby that learns to maximise the cumulative amount of red that it observes. to correctly
predict the optimal value, the baby must understand how to increase    redness    by various means,
including manipulation (bringing a red object closer to the eyes); locomotion (moving in front of a
red object); and communication (crying until the parents bring a red object). these behaviours are
likely to recur for many other goals that the baby may subsequently encounter. no understanding of
these behaviours is required to simply reconstruct the redness of current or subsequent images.
our architecture uses id23 to approximate both the optimal policy and optimal
value function for many different pseudo-rewards. it also makes other auxiliary predictions that
serve to focus the agent on important aspects of the task. these include the long-term goal of
predicting cumulative extrinsic reward as well as short-term predictions of extrinsic reward. to
learn more ef   ciently, our agents use an experience replay mechanism to provide additional updates

   joint    rst authors. ordered alphabetically by    rst name.

1

figure 1: overview of the unreal agent. (a) the base agent is a id98-lstm agent trained on-policy with
the a3c loss (mnih et al., 2016). observations, rewards, and actions are stored in a small replay buffer which
encapsulates a short history of agent experience. this experience is used by auxiliary learning tasks. (b) pixel
control     auxiliary policies qaux are trained to maximise change in pixel intensity of different regions of the
input. the agent id98 and lstm are used for this task along with an auxiliary deconvolution network. this
auxiliary control task requires the agent to learn how to control the environment. (c) reward prediction     given
three recent frames, the network must predict the reward that will be obtained in the next unobserved timestep.
this task network uses instances of the agent id98, and is trained on reward biased sequences to remove the
perceptual sparsity of rewards. (d) value function replay     further training of the value function using the
agent network is performed to promote faster value iteration. further visualisation of the agent can be found in
https://youtu.be/uz-zgyryeja

to the critics. just as animals dream about positively or negatively rewarding events more frequently
(schacter et al., 2012), our agents preferentially replay sequences containing rewarding events.
importantly, both the auxiliary control and auxiliary prediction tasks share the convolutional neural
network and lstm that the base agent uses to act. by using this jointly learned representation,
the base agent learns to optimise extrinsic reward much faster and, in many cases, achieves better
policies at the end of training.
this paper brings together the state-of-the-art asynchronous advantage actor-critic (a3c) frame-
work (mnih et al., 2016), outlined in section 2, with auxiliary control tasks and auxiliary reward
tasks, de   ned in sections section 3.1 and section 3.2 respectively. these auxiliary tasks do not re-
quire any extra supervision or signals from the environment than the vanilla a3c agent. the result
is our unsupervised reinforcement and auxiliary learning (unreal) agent (section 3.4)
in section 4 we apply our unreal agent to a challenging set of 3d-vision based domains known
as the labyrinth (mnih et al., 2016), learning solely from the raw rgb pixels of a    rst-person view.
our agent signi   cantly outperforms the baseline agent using vanilla a3c, even when the baseline
was augmented with an unsupervised reconstruction loss, in terms of speed of learning, robustness
to hyperparameters, and    nal performance. the result is an agent which on average achieves 87% of
expert human-normalised score, compared to 54% with a3c, and on average 10   faster than a3c.
our unreal agent also signi   cantly outperforms the previous state-of-the-art in the atari domain.

1 related work

a variety of id23 architectures have focused on learning temporal abstractions,
such as options (sutton et al., 1999b), with policies that may maximise pseudo-rewards (konidaris
& barreto, 2009; silver & ciosek, 2012). the emphasis here has typically been on the development
of temporal abstractions that facilitate high-level learning and planning. in contrast, our agents do
not make any direct use of the pseudo-reward maximising policies that they learn (although this is

2

   v   v   v   v000+1(c) reward predictionrt}replay buffervvv+rrr(d) value function replayt   t   +1t   +2t   +3t    3t    2t    1r   skewedsampling(b) pixel controlqauxv   environment      otagent lstmagent convnetaux deconvnetaux fc net(a) base a3c agentan interesting direction for future research). instead, they are used solely as auxiliary objectives for
developing a more effective representation.
the horde architecture (sutton et al., 2011) also applied id23 to identify value
functions for a multitude of distinct pseudo-rewards. however, this architecture was not used for
representation learning; instead each value function was trained separately using distinct weights.
the uvfa architecture (schaul et al., 2015a) is a factored representation of a continuous set of
optimal value functions, combining features of the state with an embedding of the pseudo-reward
function. initial work on uvfas focused primarily on architectural choices and learning rules for
these continuous embeddings. a pre-trained uvfa representation was successfully transferred to
novel pseudo-rewards in a simple task.
similarly, the successor representation (dayan, 1993; barreto et al., 2016; kulkarni et al., 2016)
factors a continuous set of expected value functions for a    xed policy, by combining an expectation
over features of the state with an embedding of the pseudo-reward function. successor representa-
tions have been used to transfer representations from one pseudo-reward to another (barreto et al.,
2016) or to different scales of reward (kulkarni et al., 2016).
another, related line of work involves learning models of the environment (schmidhuber, 2010;
xie et al., 2015; oh et al., 2015). although learning environment models as auxiliary tasks could
improve rl agents (e.g. lin & mitchell (1992); li et al. (2015)), this has not yet been shown to
work in rich visual environments.
more recently, auxiliary predictions tasks have been studied in 3d id23 environ-
ments. lample & chaplot (2016) showed that predicting internal features of the emulator, such
as the presence of an enemy on the screen, is bene   cial. mirowski et al. (2016) study auxiliary
prediction of depth in the context of navigation.

2 background
we assume the standard id23 setting where an agent interacts with an environment
over a number of discrete time steps. at time t the agent receives an observation ot along with a
sum of rewards, rt:t+n = (cid:80)n
reward rt and produces an action at. the agent   s state st is a function of its experience up until
time t, st = f (o1, r1, a1, ..., ot, rt). the n-step return rt:t+n at time t is de   ned as the discounted
i=1   irt+i. the value function is the expected return from state s,
v   (s) = e [rt:   |st = s,   ], when actions are selected accorded to a policy   (a|s). the action-
value function q  (s, a) = e [rt:   |st = s, at = a,   ] is the expected return following action a
from state s.
value-based id23 algorithms, such as id24 (watkins, 1989), or its deep
learning instantiations id25 (mnih et al., 2015) and asynchronous id24 (mnih et al., 2016),
approximate the action-value function q(s, a;   ) using parameters   , and then update parameters
to minimise the mean-squared error, for example by optimising an n-step lookahead loss (peng
; where       are

& williams, 1996), lq = e(cid:104)

(rt:t+n +   n maxa(cid:48) q(s(cid:48), a(cid:48);      )     q(s, a;   ))2(cid:105)
= e(cid:2)    

      log   (a|s)(q  (s, a)     v   (s))(cid:3) (watkins, 1989; sutton

previous parameters and the optimisation is with respect to   .
policy gradient algorithms adjust the policy to maximise the expected reward, l   =    es      [r1:   ],
using the gradient    es     [r1:   ]
et al., 1999a); in practice the true value functions q   and v    are substituted with approxima-
tions. the asynchronous advantage actor-critic (a3c) algorithm (mnih et al., 2016) constructs
an approximation to both the policy   (a|s,   ) and the value function v (s,   ) using parameters   .
both policy and value are adjusted towards an n-step lookahead value, rt:t+n +   nv (st+n+1,   ),
using an id178 regularisation penalty, la3c     lvr + l       es      [  h(  (s,  ,   )], where
lvr = es     
in a3c many instances of the agent interact in parallel with many instances of the environment,
which both accelerates and stabilises learning. the a3c agent architecture we build on uses an
lstm to jointly approximate both policy    and value function v , given the entire history of expe-
rience as inputs (see figure 1 (a)).

(cid:104)
(rt:t+n +   nv (st+n+1,      )     v (st,   ))2(cid:105)

     

.

3

3 auxiliary tasks for id23

in this section we incorporate auxiliary tasks into the id23 framework in order
to promote faster training, more robust learning, and ultimately higher performance for our agents.
section 3.1 introduces the use of auxiliary control tasks, section 3.2 describes the addition of reward
focussed auxiliary tasks, and section 3.4 describes the complete unreal agent combining these
auxiliary tasks.

3.1 auxiliary control tasks

t:t+n =(cid:80)n

the auxiliary control tasks we consider are de   ned as additional pseudo-reward functions in the
environment the agent is interacting with. we formally de   ne an auxiliary control task c by a reward
function r(c) : s    a     r, where s is the space of possible states and a is the space of available
actions. the underlying state space s includes both the history of observations and rewards as well
as the state of the agent itself, i.e. the activations of the hidden units of the network.
given a set of auxiliary control tasks c, let   (c) be the agent   s policy for each auxiliary task c     c and
let    be the agent   s policy on the base task. the overall objective is to maximise total performance
across all these auxiliary tasks,

arg max

  

e  [r1:   ] +   c

e  c[r(c)

1:   ],

(1)

(cid:88)

c   c

t

k=1   kr(c)

where, r(c)
is the discounted return for auxiliary reward r(c), and    is the set of
parameters of    and all   (c)   s. by sharing some of the parameters of    and all   (c) the agent must
balance improving its performance with respect to the global reward rt with improving performance
on the auxiliary tasks.
in principle, any id23 method could be applied to maximise these objectives.
however, to ef   ciently learn to maximise many different pseudo-rewards simultaneously in par-
allel from a single stream of experience, it is necessary to use off-policy reinforcement learn-
ing. we focus on value-based rl methods that approximate the optimal action-values by q-
learning. speci   cally, for each control task c we optimise an n-step id24 loss l(c)
q =
, as described in mnih et al. (2016).

e(cid:104)(cid:0)rt:t+n +   n maxa(cid:48) q(c)(s(cid:48), a(cid:48),      )     q(c)(s, a,   )(cid:1)2(cid:105)

while many types of auxiliary reward functions can be de   ned from these quantities we focus on
two speci   c types:

    pixel changes - changes in the perceptual stream often correspond to important events in
an environment. we train agents that learn a separate policy for maximally changing the
pixels in each cell of an n    n non-overlapping grid placed over the input image. we refer
to these auxiliary tasks as pixel control. see section 4 for a complete description.
    network features - since the policy or value networks of an agent learn to extract task-
relevant high-level features of the environment (mnih et al., 2015; zahavy et al., 2016;
silver et al., 2016) they can be useful quantities for the agent to learn to control. hence, the
activation of any hidden unit of the agent   s neural network can itself be an auxiliary reward.
we train agents that learn a separate policy for maximally activating each of the units in a
speci   c hidden layer. we refer to these tasks as feature control.

the figure 1 (b) shows an a3c agent architecture augmented with a set of auxiliary pixel control
tasks. in this case, the base policy    shares both the convolutional visual stream and the lstm with
the auxiliary policies. the output of the auxiliary network head is an nact    n    n tensor qaux
where qaux(a, i, j) represents the network   s current estimate of the optimal discounted expected
change in cell (i, j) of the input after taking action a. we exploit the spatial nature of the auxiliary
tasks by using a deconvolutional neural network to produce the auxiliary values qaux.

3.2 auxiliary reward tasks

in addition to learning generally about the dynamics of the environment, an agent must learn to
maximise the global reward stream. to learn a policy to maximise rewards, an agent requires features

4

figure 2: the raw rgb frame from the environment is the observation that is given as input to the
agent, along with the last action and reward. this observation is shown for a sample of a maze from the
nav maze all random 02 level in labyrinth. the agent must navigate this unseen maze and pick up apples
giving +1 reward and reach the goal giving +10 reward, after which it will respawn. top down views of samples
from this maze generator show the variety of mazes procedurally created. a video showing the agent playing
labyrinth levels can be viewed at https://youtu.be/uz-zgyryeja

that recognise states that lead to high reward and value. an agent with a good representation of
rewarding states, will allow the learning of good value functions, and in turn should allow the easy
learning of a policy.
however, in many interesting environments reward is encountered very sparsely, meaning that it
can take a long time to train feature extractors adept at recognising states which signify the onset
of reward. we want to remove the perceptual sparsity of rewards and rewarding states to aid the
training of an agent, but to do so in a way which does not introduce bias to the agent   s policy.
to do this, we introduce the auxiliary task of reward prediction     that of predicting the onset of
immediate reward given some historical context. this task consists of processing a sequence of
consecutive observations, and requiring the agent to predict the reward picked up in the subsequent
unseen frame. this is similar to value learning focused on immediate reward (   = 0).
unlike learning a value function, which is used to estimate returns and as a baseline while learning
a policy, the reward predictor is not used for anything other than shaping the features of the agent.
this keeps us free to bias the data distribution, therefore biasing the reward predictor and feature
shaping, without biasing the value function or policy.
we train the reward prediction task on sequences s   = (s     k, s     k+1, . . . , s     1) to predict the
reward r   , and sample s   from the experience of our policy    in a skewed manner so as to over-
represent rewarding events (presuming rewards are sparse within the environment). speci   cally,
we sample such that zero rewards and non-zero rewards are equally represented, i.e. the predicted
id203 of a non-zero reward is p (r   (cid:54)= 0) = 0.5. the reward prediction is trained to minimise
a loss lrp. in our experiments we use a multiclass cross-id178 classi   cation loss across three
classes (zero, positive, or negative reward), although a mean-squared error loss is also feasible.
the auxiliary reward predictions may use a different architecture to the agent   s main policy. rather
than simply    hanging    the auxiliary predictions off the lstm, we use a simpler feedforward net-
work that concatenates a stack of states s   after being encoded by the agent   s id98, see figure 1 (c).
the idea is to simplify the temporal aspects of the prediction task in both the future direction (focus-
ing only on immediate reward prediction rather than long-term returns) and past direction (focusing
only on immediate predecessor states rather than the complete history); the features discovered in
this manner is shared with the primary lstm (via shared weights in the convolutional encoder) to
enable the policy to be learned more ef   ciently.

3.3 experience replay

experience replay has proven to be an effective mechanism for improving both the data ef   ciency
and stability of deep id23 algorithms (mnih et al., 2015). the main idea is to store
transitions in a replay buffer, and then apply learning updates to sampled transitions from this buffer.
experience replay provides a natural mechanism for skewing the distribution of reward predic-
tion samples towards rewarding events: we simply split the replay buffer into rewarding and non-
rewarding subsets, and replay equally from both subsets. the skewed sampling of transitions from

5

a replay buffer means that rare rewarding states will be oversampled, and learnt from far more fre-
quently than if we sampled sequences directly from the behaviour policy. this approach can be
viewed as a simple form of prioritised replay (schaul et al., 2015b).
in addition to reward prediction, we also use the replay buffer to perform value function replay.
this amounts to resampling recent historical sequences from the behaviour policy distribution and
performing extra value function regression in addition to the on-policy value function regression
in a3c. by resampling previous experience, and randomly varying the temporal position of the
truncation window over which the n-step return is computed, value function replay performs value
iteration and exploits newly discovered features shaped by reward prediction. we do not skew the
distribution for this case.
experience replay is also used to increase the ef   ciency and stability of the auxiliary control tasks.
id24 updates are applied to sampled experiences that are drawn from the replay buffer, allow-
ing features to be developed extremely ef   ciently.

3.4 unreal agent

the unreal algorithm combines the bene   ts of two separate, state-of-the-art approaches to deep
id23. the primary policy is trained with a3c (mnih et al., 2016): it learns from
parallel streams of experience to gain ef   ciency and stability; it is updated online using policy gra-
dient methods; and it uses a recurrent neural network to encode the complete history of experience.
this allows the agent to learn effectively in partially observed environments.
the auxiliary tasks are trained on very recent sequences of experience that are stored and randomly
sampled; these sequences may be prioritised (in our case according to immediate rewards) (schaul
et al., 2015b); these targets are trained off-policy by id24; and they may use simpler feedfor-
ward architectures. this allows the representation to be trained with maximum ef   ciency.
the unreal algorithm optimises a single combined id168 with respect to the joint param-
(cid:88)
eters of the agent,   , that combines the a3c loss la3c together with an auxiliary control loss lpc,
auxiliary reward prediction loss lrp and replayed value loss lvr,
l(c)
q +   rplrp

lunreal(  ) = la3c +   vrlvr +   pc

(2)

c

where   vr,   pc,   rp are weighting terms on the individual loss components.
in practice, the loss is broken down into separate components that are applied either on-policy,
directly from experience; or off-policy, on replayed transitions. speci   cally, the a3c loss la3c is
minimised on-policy; while the value function loss lvr is optimised from replayed data, in addition
to the a3c loss (of which it is one component, see section 2). the auxiliary control loss lpc is
optimised off-policy from replayed data, by n-step id24. finally, the reward loss lrp is
optimised from rebalanced replay data.

4 experiments

in this section we give the results of experiments performed on the 3d environment labyrinth in
section 4.1 and atari in section 4.2.
in all our experiments we used an a3c id98-lstm agent as our baseline and the unreal agent
along with its ablated variants added auxiliary outputs and losses to this base agent. the agent is
trained on-policy with 20-step returns and the auxiliary tasks are performed every 20 environment
steps, corresponding to every update of the base a3c agent. the replay buffer stores the most recent
2k observations, actions, and rewards taken by the base agent. in labyrinth we use the same set of
17 discrete actions for all games and on atari the action set is game dependent (between 3 and 18
discrete actions). the full implementation details can be found in section b.

4.1 labyrinth results

labyrinth is a    rst-person 3d game platform extended from openarena (contributors, 2005), which
is itself based on quake3 (id software, 1999). labyrinth is comparable to other    rst-person 3d game

6

labyrinth performance

labyrinth robustness

atari performance

atari robustness

figure 3: an overview of performance averaged across all levels on labyrinth (top) and atari (bottom). in
the ablated versions rp is reward prediction, vr is value function replay, and pc is pixel control, with the
unreal agent being the combination of all. left: the mean human-normalised performance over last 100
episodes of the top-3 jobs at every point in training. we achieve an average of 87% human-normalised score,
with every element of the agent improving upon the 54% human-normalised score of vanilla a3c. right: the
   nal human-normalised score of every job in our hyperparameter sweep, sorted by score. on both labyrinth
and atari, the unreal agent increases the robustness to the hyperparameters (namely learning rate and id178
cost).
platforms for ai research like vizdoom (kempka et al., 2016) or minecraft (tessler et al., 2016).
however, in comparison, labyrinth has considerably richer visuals and more realistic physics. tex-
tures in labyrinth are often dynamic (animated) so as to convey a game world where walls and    oors
shimmer and pulse, adding signi   cant complexity to the perceptual task. the action space allows
for    ne-grained pointing in a fully 3d world. unlike in vizdoom, agents can look up to the sky or
down to the ground. labyrinth also supports continuous motion unlike the minecraft platform of
(oh et al., 2016), which is a 3d grid world.
we evaluated agent performance on 13 labyrinth levels that tested a range of different agent abilities.
a top-down visualization showing the layout of each level can be found in figure 7 of the appendix.
a gallery of example images from the    rst-person perspective of the agent are in figure 8 of the
appendix. the levels can be divided into four categories:

1. simple

2. navigation levels with a

a

fruit

gathering

static map

levels with

static map layout

(seekavoid arena 01

and
stairway to melon 01). the goal of these levels is to collect apples (small positive
reward) and melons (large positive reward) while avoiding lemons (small negative reward).
(nav maze static 0{1, 2, 3} and
the agent   s ability to    nd
nav maze random goal 0{1, 2, 3}).
their way to a goal in a    xed maze that remains the same across episodes. the starting
location is random. in this case, agents could encode the structure of the maze in network
weights. in the random goal variant, the location of the goal changes in every episode.
the optimal policy is to    nd the goal   s location at the start of each episode and then use
long-term knowledge of the maze layout to return to it as quickly as possible from any
location. the static variant is simpler in that the goal location is always    xed for all
episodes and only the agent   s starting location changes so the optimal policy does not
require the    rst step of exploring to    nd the current goal location.

these levels test

3. procedurally-generated navigation levels requiring effective exploration of a new maze
generated on-the-   y at the start of each episode (nav maze all random 0{1, 2, 3}). these
levels test the agent   s ability to effectively explore a totally new environment. the optimal

7

0.00.51.01.52.02.5steps  1070%10%20%30%40%50%60%70%80%90%humannormalisedperformance.87%unreal.81%a3c+pc.79%a3c+rp+vr.72%a3c+rp.57%a3c+vr.54%a3cavg.top3agents0%20%40%60%80%100%percentageofagentsinpopulation0%20%40%60%80%100%humannormalisedperformanceunreala3c+pca3c+rp+vra3c+rpa3c+vra3c0.00.51.01.52.02.5steps  1080%100%200%300%400%500%600%700%800%900%humannormalisedperformance.880%unreal.861%a3c+rp+vr.853%a3c592%prior.duelclipid25373%duelclipid25228%id25avg.top3agents0%20%40%60%80%100%percentageofagentsinpopulation0%200%400%600%800%1000%1200%humannormalisedperformanceunreala3c+rp+vra3cpolicy would begin by exploring the maze to rapidly learn its layout and then exploit that
knowledge to repeatedly return to the goal as many times as possible before the end of the
episode (between 60 and 300 seconds).

4. laser-tag levels requiring agents to wield laser-like science    ction gadgets to tag bots con-
trolled by the game   s in-built ai (lt horse shoe color and lt hallway slope). a reward
of 1 is delivered whenever the agent tags a bot by reducing its shield to 0. these levels
approximate the default openarena/quake3 gameplay mode. in lt hallway slope there is
a sloped arena, requiring the agent to look up and down. in lt horse shoe color, the colors
and textures of the bots are randomly generated at the start of each episode. this prevents
agents from relying on color for bot detection. these levels test aspects of    ne-control
(for aiming), planning (to anticipate where bots are likely to move), strategy (to control
key areas of the map such as gadget spawn points), and robustness to the substantial vi-
sual complexity arising from the large numbers of independently moving objects (gadget
projectiles and bots).

4.1.1 results

we compared the full unreal agent to a basic a3c lstm agent along with several ablated
versions of unreal with different components turned off. a video of the    nal agent perfor-
mance, as well as visualisations of the activations and auxiliary task outputs can be viewed at
https://youtu.be/uz-zgyryeja.
figure 3 (right) shows curves of mean human-normalised scores over the 13 labyrinth levels.
adding each of our proposed auxiliary tasks to an a3c agent substantially improves the perfor-
mance. combining different auxiliary tasks leads to further improvements over the individual auxil-
iary tasks. the unreal agent, which combines all three auxiliary tasks, achieves more than twice
the    nal human-normalised mean performance of a3c, increasing from 54% to 87% (45% to 92%
for median performance). this includes a human-normalised score of 116% on lt hallway slope
and 100% on nav maze random goal 02.
perhaps of equal importance, aside from    nal performance on the games, unreal is signi   cantly
faster at learning and therefore more data ef   cient, achieving a mean speedup of the number of
steps to reach a3c best performance of 10   (median 11  ) across all levels and up to 18   on
nav maze random goal 02. this translates in a drastic improvement in the data ef   ciency of un-
real over a3c, requiring less than 10% of the data to reach the    nal performance of a3c. we can
also measure the robustness of our learning algorithms to hyperparameters by measuring the perfor-
mance over all hyperparameters (namely learning rate and id178 cost). this is shown in figure 3
top: every auxiliary task in our agent improves robustness. a breakdown of the performance of
a3c, unreal and unreal without pixel control on the individual labyrinth levels is shown in
figure 4.

unsupervised id23 in order to better understand the bene   ts of auxiliary
control tasks we compared it to two simple baselines on three labyrinth levels. the    rst baseline
was a3c augmented with a pixel reconstruction loss, which has been shown to improve performance
on 3d environments (kulkarni et al., 2016). the second baseline was a3c augmented with an input
change prediction loss, which can be seen as simply predicting the immediate auxiliary reward
instead of learning to control. finally, we include preliminary results for a3c augmented with the
feature control auxiliary task on one of the levels. we retuned the hyperparameters of all methods
(including learning rate and the weight placed on the auxiliary loss) for each of the three labyrinth
levels. figure 5 shows the learning curves for the top 5 hyperparameter settings on three labyrinth
navigation levels. the results show that learning to control pixel changes is indeed better than simply
predicting immediate pixel changes, which in turn is better than simply learning to reconstruct the
input. in fact, learning to reconstruct only led to faster initial learning and actually made the    nal
scores worse when compared to vanilla a3c. our hypothesis is that input reconstruction hurts    nal
performance because it puts too much focus on reconstructing irrelevant parts of the visual input
instead of visual cues for rewards, which rewarding objects are rarely visible. encouragingly, we
saw an improvement from including the feature control auxiliary task. combining feature control
with other auxiliary tasks is a promising future direction.

8

figure 4: a breakdown of the improvement over a3c due to our auxiliary tasks for each level on labyrinth.
the values for a3c+rp+vr (reward prediction and value function replay) and unreal (reward prediction,
value function replay and pixel control) are normalised by the a3c value. auc performance gives the robust-
ness to hyperparameters (area under the robustness curve figure 3 right). data ef   ciency is area under the
mean learning curve for the top-5 jobs, and top5 speedup is the speedup for the mean of the top-5 jobs to reach
the maximum top-5 mean score set by a3c. speedup is not de   ned for stairway to melon as a3c did not
learn throughout training.

figure 5: comparison of various forms of self-supervised learning on random maze navigation. adding an
input reconstruction loss to the objective leads to faster learning compared to an a3c baseline. predicting
changes in the inputs works better than simple image reconstruction. learning to control changes leads to the
best results.

4.2 atari

we applied the unreal agent as well as unreal without pixel control to 57 atari games from
the arcade learning environment (bellemare et al., 2012) domain. we use the same evaluation
protocol as for our labyrinth experiments where we evaluate 50 different random hyper parameter
settings (learning rate and id178 cost) on each game. the results are shown in the bottom row of
figure 3. the left side shows the average performance curves of the top 3 agents for all three meth-
ods the right half shows sorted average human-normalised scores for each hyperparameter setting.
more detailed learning curves for individual levels can be found in figure 7. we see that unreal
surpasses the current state-of-the-art agents, i.e. a3c and prioritized dueling id25 (wang et al.,
2016), across all levels attaining 880% mean and 250% median performance. notably, unreal is
also substantially more robust to hyper parameter settings than a3c.

5 conclusion

we have shown how augmenting a deep id23 agent with auxiliary control and re-
ward prediction tasks can drastically improve both data ef   ciency and robustness to hyperparameter
settings. most notably, our proposed unreal architecture more than doubled the previous state-
of-the-art results on the challenging set of 3d labyrinth levels, bringing the average scores to over
87% of human scores. the same unreal architecture also signi   cantly improved both the learning
speed and the robustness of a3c over 57 atari games.

9

medianmeanstairwaytomelon01seekavoidarena01navmazestatic03navmazestatic02navmazestatic01navmazerandomgoal03navmazerandomgoal02navmazerandomgoal01navmazeallrandom03navmazeallrandom02navmazeallrandom01lthorseshoecolorlthallwayslopelevel210%243%129%115%146%210%222%315%509%197%203%230%251%154%481%407%495%114%109%141%1345%388%715%1164%202%407%570%471%187%627%aucperformance240%239%360%115%142%275%195%296%383%240%213%259%249%171%214%366%543%223%102%162%1797%267%923%1217%366%561%577%397%195%270%datae   ciency3x3x5x2x2x4x3x3x2x3x3x4x3x5x11x10x1x4x8x13x13x18x6x12x13x16x4x10xtop5speedupunreala3c+rp+vr020406080training steps in millions010203040506070average scorenav_maze_all_random_01a3ca3c + input reconstructiona3c + input change predictiona3c + pixel control010203040506070training steps in millions0102030405060708090average scorenav_maze_random_goal_01a3ca3c + input reconstructiona3c + input change predictiona3c + pixel control020406080training steps in millions010203040506070average scorenav_maze_all_random_01a3ca3c + feature controla3c + pixel controlacknowledgements

we thank charles beattie, julian schrittwieser, marcus wainwright, and stig petersen for environ-
ment design and development, and amir sadik and sarah york for expert human game testing. we
also thank joseph modayil, andrea banino, hubert soyer, razvan pascanu, and raia hadsell for
many helpful discussions.

references
andr  e barreto, r  emi munos, tom schaul, and david silver. successor features for transfer in

id23. arxiv preprint arxiv:1606.05312, 2016.

marc g bellemare, yavar naddaf, joel veness, and michael bowling. the arcade learning envi-
ronment: an evaluation platform for general agents. journal of arti   cial intelligence research,
2012.

openarena contributors. the openarena manual. 2005. url http://openarena.wikia.

com/wiki/manual.

peter dayan. improving generalization for temporal difference learning: the successor representa-

tion. neural computation, 5(4):613   624, 1993.

felix a gers, j  urgen schmidhuber, and fred cummins. learning to forget: continual prediction

with lstm. neural computation, 12(10):2451   2471, 2000.

id software.

quake3.

quake-iii-arena.

1999.

url https://github.com/id-software/

micha   kempka, marek wydmuch, grzegorz runc, jakub toczek, and wojciech ja  skowski. viz-
doom: a doom-based ai research platform for visual id23. arxiv preprint
arxiv:1605.02097, 2016.

george konidaris and andre s barreto. skill discovery in continuous id23 do-
mains using skill chaining. in advances in neural information processing systems, pp. 1015   
1023, 2009.

tejas d kulkarni, ardavan saeedi, simanta gautam, and samuel j gershman. deep successor

id23. arxiv preprint arxiv:1606.02396, 2016.

guillaume lample and devendra singh chaplot. playing fps games with deep reinforcement learn-

ing. corr, abs/1609.05521, 2016.

xiujun li, lihong li, jianfeng gao, xiaodong he, jianshu chen, li deng, and ji he. recurrent

id23: a hybrid approach. arxiv preprint arxiv:1509.03044, 2015.

long-ji lin and tom m mitchell. memory approaches to id23 in non-markovian

domains. technical report, carnegie mellon university, school of computer science, 1992.

piotr mirowski, razvan pascanu, fabio viola, andrea banino, hubert soyer, andy ballard, misha
denil, ross goroshin, laurent sifre, koray kavukcuoglu, dharshan kumaran, and raia hadsell.
learning to navigate in complex environments. 2016.

volodymyr mnih, koray kavukcuoglu, david silver, alex graves, ioannis antonoglou, daan wier-
stra, and martin riedmiller. playing atari with deep id23. in nips deep learn-
ing workshop. 2013.

volodymyr mnih, koray kavukcuoglu, david silver, andrei a. rusu, joel veness, marc g.
bellemare, alex graves, martin riedmiller, andreas k. fidjeland, georg ostrovski, stig pe-
tersen, charles beattie, amir sadik, ioannis antonoglou, helen king, dharshan kumaran, daan
wierstra, shane legg, and demis hassabis. human-level control through deep reinforcement
learning. nature, 518(7540):529   533, 02 2015. url http://dx.doi.org/10.1038/
nature14236.

10

volodymyr mnih, adri`a puigdom`enech badia, mehdi mirza, alex graves, timothy p. lillicrap, tim
harley, david silver, and koray kavukcuoglu. asynchronous methods for deep reinforcement
learning. in proceedings of the 33rd international conference on machine learning (icml), pp.
1928   1937, 2016.

junhyuk oh, xiaoxiao guo, honglak lee, richard l lewis, and satinder singh. action-conditional
video prediction using deep networks in atari games. in advances in neural information process-
ing systems, pp. 2863   2871, 2015.

junhyuk oh, valliappa chockalingam, satinder singh, and honglak lee. control of memory, active

perception, and action in minecraft. arxiv preprint arxiv:1605.09128, 2016.

jing peng and ronald j williams. incremental multi-step id24. machine learning, 22(1-3):

283   290, 1996.

daniel l schacter, donna rose addis, demis hassabis, victoria c martin, r nathan spreng, and
karl k szpunar. the future of memory: remembering, imagining, and the brain. neuron, 76(4):
677   694, 2012.

tom schaul, daniel horgan, karol gregor, and david silver. universal value function approxima-
tors. in proceedings of the 32nd international conference on machine learning (icml-15), pp.
1312   1320, 2015a.

tom schaul, john quan, ioannis antonoglou, and david silver. prioritized experience replay. arxiv

preprint arxiv:1511.05952, 2015b.

j  urgen schmidhuber. formal theory of creativity, fun, and intrinsic motivation (1990   2010). ieee

transactions on autonomous mental development, 2(3):230   247, 2010.

david silver and kamil ciosek. compositional planning using optimal option models. arxiv

preprint arxiv:1206.6473, 2012.

david silver, aja huang, chris j maddison, arthur guez, laurent sifre, george van den driessche,
julian schrittwieser, ioannis antonoglou, veda panneershelvam, marc lanctot, et al. mastering
the game of go with deep neural networks and tree search. nature, 529(7587):484   489, 2016.

richard s sutton, david a mcallester, satinder p singh, yishay mansour, et al. policy gradient
methods for id23 with function approximation. in nips, volume 99, pp. 1057   
1063, 1999a.

richard s sutton, doina precup, and satinder singh. between mdps and semi-mdps: a framework

for temporal abstraction in id23. arti   cial intelligence, 1999b.

richard s sutton, joseph modayil, michael delp, thomas degris, patrick m pilarski, adam white,
and doina precup. horde: a scalable real-time architecture for learning knowledge from unsuper-
vised sensorimotor interaction. in the 10th international conference on autonomous agents and
multiagent systems-volume 2, pp. 761   768. international foundation for autonomous agents
and multiagent systems, 2011.

chen tessler, shahar givony, tom zahavy, daniel j mankowitz, and shie mannor. a deep hierar-

chical approach to lifelong learning in minecraft. arxiv preprint arxiv:1604.07255, 2016.

z. wang, n. de freitas, and m. lanctot. dueling network architectures for deep reinforcement
learning. in proceedings of the 33rd international conference on machine learning (icml),
2016.

christopher john cornish hellaby watkins. learning from delayed rewards. phd thesis, university

of cambridge england, 1989.

christopher xie, sachin patil, teodor mihai moldovan, sergey levine, and pieter abbeel. model-
based id23 with parametrized physical models and optimism-driven explo-
ration. corr, abs/1509.06824, 2015.

tom zahavy, nir ben zrihem, and shie mannor. graying the black box: understanding id25s. in

proceedings of the 33rd international conference on machine learning, 2016.

11

a atari games

figure 6: learning curves for three example atari games. semi-transparent lines are agents with
different seeds and hyperparameters, the bold line is a mean over population and dotted line is the
best agent (in terms of    nal performance).

b implementation details

the input to the agent at each timestep was an 84    84 rgb image. all agents processed the input
with the convolutional neural network (id98) originally used for atari by mnih et al. (2013). the
network consists of two convolutional layers. the    rst one has 16 8    8    lters applied with stride 4,
while the second one has 32 4    4    lters with stride 2. this is followed by a fully connected layer
with 256 units. all three layers are followed by a relu non-linearity. all agents used an lstm
with forget gates (gers et al., 2000) with 256 cells which take in the id98-encoded observation
concatenated with the previous action taken and curren:t reward. the policy and value function are
linear projections of the lstm output. the agent is trained with 20-step unrolls. the action space
of the agent in the environment is game dependent for atari (between 3 and 18 discrete actions), and
17 discrete actions for labyrinth. labyrinth runs at 60 frames-per-second. we use an action repeat
of four, meaning that each action is repeated four times, with the agent receiving the    nal fourth
frame as input to the next processing step.
for the pixel control auxiliary tasks we trained policies to control the central 80    80 crop of the
inputs. the cropped region was subdivided into a 20    20 grid of non-overlapping 4    4 cells. the
instantaneous reward in each cell was de   ned as the average absolute difference from the previous
frame, where the average is taken over both pixels and channels in the cell. the output tensor of
auxiliary values, qaux, is produced from the lstm outputs by a deconvolutional network. the
lstm outputs are    rst mapped to a 32    7    7 spatial feature map with a linear layer followed by a
relu. deconvolution layers with 1 and nact    lters of size 4   4 and stride 2 map the 32   7   7 into
a value tensor and an advantage tensor respectively. the spatial map is then decoded into q-values
using the dueling parametrization (wang et al., 2016) producing the nact    20    20 output qaux.
the architecture for feature control was similar. we learned to control the second hidden layer,
which is a spatial feature map with size 32    9    9. similarly to pixel control, we exploit the spatial
structure in the data and used a deconvolutional network to produce qaux from the lstm outputs.
further details are included in the supplementary materials.
the reward prediction task is performed on a sequence of three observations, which are fed through
three instances of the agent   s id98. the three encoded id98 outputs are concatenated and fed
through a fully connected layer of 128 units with relu activations, followed by a    nal linear three-
class classi   er and softmax. the reward is predicted as one of three classes: positive, negative, or
zero and trained with a task weight   rp = 1. the value function replay is performed on a sequence
of length 20 with a task weight   vr = 1.
the auxiliary tasks are performed every 20 environment steps, corresponding to every update of the
base a3c agent, once the replay buffer has    lled with agent experience. the replay buffer stores the
most recent 2k observations, actions, and rewards taken by the base agent.
the agents are optimised over 32 asynchronous threads with shared rmsprop (mnih et al., 2016).
the learning rates are sampled from a log-uniform distribution between 0.0001 and 0.005. the
id178 costs are sampled from the log-uniform distribution between 0.0005 and 0.01. task weight
  pc is sampled from log-uniform distribution between 0.01 and 0.1 for labyrinth and 0.0001 and
0.01 for atari (since atari games are not homogeneous in terms of pixel intensities changes, thus
we need to    t this id172 factor).

12

0.00.51.01.52.02.5steps  107050010001500200025003000rewardmontezumarevengeunreala3c+rp+vra3c0.00.51.01.52.02.5steps  1070100020003000400050006000rewardseaquestunreala3c+rp+vra3c0.00.51.01.52.02.5steps  107020000400006000080000100000rewardchoppercommandunreala3c+rp+vra3cc labyrinth levels

stairway to melon

seekavoid arena 01

nav maze     01

nav maze     02

nav maze     03

lt horse shoe color

lt hallway slope

figure 7: top-down renderings of each labyrinth level. the nav maze     0{1, 2, 3} levels show
one example maze layout. in the all random case, a new maze was randomly generated at the start
of each episode.

13

figure 8: example images from the agent   s egocentric viewpoint for each labyrinth level.

14

