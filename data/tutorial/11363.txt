6
1
0
2

 

v
o
n
9

 

 
 
]
l
m

.
t
a
t
s
[
 
 

3
v
0
2
7
0
0

.

7
0
5
1
:
v
i
x
r
a

correlated random measures

rajesh ranganath

department of computer science

princeton university

rajeshr@cs.princeton.edu

david m. blei

departments of computer science and statistics

columbia university

david.blei@columbia.edu

abstract

we develop correlated random measures, random measures where the atom weights can ex-
hibit a    exible pattern of dependence, and use them to develop powerful hierarchical bayesian
nonparametric models. hierarchical bayesian nonparametric models are usually built from
completely random measures, a poisson-process based construction in which the atom weights
are independent. completely random measures imply strong independence assumptions in the
corresponding hierarchical model, and these assumptions are often misplaced in real-world set-
tings. correlated random measures address this limitation. they model correlation within the
measure by using a gaussian process in concert with the poisson process. with correlated ran-
dom measures, for example, we can develop a latent feature model for which we can infer both
the properties of the latent features and their dependency pattern. we develop several other
examples as well. we study a correlated random measure model of pairwise count data. we
derive an ef   cient variational id136 algorithm and show improved predictive performance
on large data sets of documents, web clicks, and electronic health records.

1.

introduction

hierarchical bayesian nonparametric models (teh and jordan, 2010) have emerged as a power-
ful approach to analyzing complex data (williamson et al., 2010; fox et al., 2011; zhou et al.,
2012). these models assume there are a set of patterns, or components, that underlie the observed
data; each data point exhibits each component with different non-negative weight; the number of
components is unknown and new data can exhibit still unseen components. given observed data,
the posterior distribution reveals the components (including how many there are), reveals how
each data point exhibits them, and allows for this representation to grow as more data are seen.
these kinds of assumptions describe many of the most common hierarchical bayesian nonpara-
metric models, such as the hierarchical dirichlet process (teh et al., 2006), the gamma-poisson
process (titsias, 2008), the beta-bernoulli process (thibaux and jordan, 2007), and others.

1

for example, in section 6 we analyze patient data from a large hospital; each patient is de-
scribed by the set of diagnostic codes on her chart. potentially, the full data set re   ects patterns in
diagnostic codes, each pattern a set of diagnoses that often occurs together. further, some patients
will exhibit multiple patterns   they simultaneously suffer from different clusters of symptoms.
with these data, a bayesian nonparametric model can uncover and characterize the underlying
pattern and describe each patient in terms of which patterns she exhibits. recent innovations in
approximate posterior id136 let us analyze such data at large scale, uncovering useful charac-
terizations of disease and injury for both exploration and prediction. in our study on medical data,
we discover components that summarize conditions such as congestive heart failure, diabetes, and
depression (table 1).

but there is a limitation to the current state of the art in bayesian nonparametric models. to
continue with the example, each patient is represented as an in   nite vector of non-negative weights,
one per component.
(there is a countably in   nite number of components.) most hierarchical
bayesian nonparametric models assume that these weights are uncorrelated   that is, the presence
of one component is unrelated to the presence (or absence) of the others. but this assumption is
usually unfounded. for example, in the medical data we    nd that type 2 diabetes is related to
congestive heart failure (table 4).

in this paper we solve this problem. we develop correlated random measures, a general-
purpose construction for infusing covariance into the distribution of weights of both random mea-
sures and hierarchical bayesian nonparametric models. our approach can capture that a large
positive weight for one component might covary with a large positive weight in another, a type
of pattern that is out of reach for most hierarchical bayesian nonparametric models. we demon-
strate that bringing such correlations into the model both improves prediction and reveals richer
exploratory structures. correlated random measures can be used as a model for a collection of
observed weighted point processes and can be adapted to a wide variety of proven bayesian non-
parametric settings, such as id38 (teh, 2006), time series analysis (fox et al., 2011),
dictionary learning (zhou et al., 2009), and nested models (paisley et al., 2015).

how do we achieve this? most bayesian nonparametric models are built on completely random
measures (kingman, 1967) and the independence of the weights is an artifact of this construction.
to create correlated random measures, we infuse a gaussian process (rasmussen and williams,
2005) into the construction with a latent kernel between components. this lets us relax the strict
independence assumptions. the details involve showing how to use the gaussian process in con-
cert with the poisson process, and without sacri   cing the technicalities needed to de   ne a proper
random measure. as a result of the general construction, we can build correlated variants of many
hierarchical bayesian nonparametric models.

we will describe four correlated random measures. the    rst is a correlated nonparametric
version of poisson factorization (canny, 2004; gopalan et al., 2014). this is a model of count
data, organized in a matrix, and it will be the model on which we focus our study. we show how to
derive an ef   cient variational id136 algorithm to approximate the posterior and use it to analyze
both medical data and text data. we also describe a correlated analog of the beta process (hjort,
1990) and two correlated binary latent feature models, each expanding on the hierarchical beta-
bernoulli process (grif   ths and ghahramani, 2006; thibaux and jordan, 2007). we note that the
discrete in   nite logistic normal model in paisley et al. (2012) is a normalized correlated random
measure, a correlated adaptation of the hierarchical dirichlet process (teh et al., 2006).

2

related work. correlated random measures (corrrms) can capture general covariance between
the measure of two sets, while also being atomic and extendible to id187 that share
atoms. in the bayesian nonparametric literature, researchers have proposed several other random
measures with covariance. we survey this work.

cox and isham (1980) introduced the cox process, a poisson random measure whose mean
measure is also stochastic. cox processes can capture covariance if the stochastic mean measure
exhibits covariance. unlike corrrms, however, cox processes do not allow for noninteger atom
weights. furthermore, most common examples of cox processes, such as the log-gaussian cox
process (m  ller et al., 1998), do not allow for atom sharing. we note that corrrms share the
doubly stochastic construction of the cox process; in appendix a.1 we show that corrrms can be
alternatively viewed as a stochastic transformation of a poisson process.

determinantal point processes (borodin, 2009) are point processes where the number of points
is related to the determinant of a id81. determinantal point processes exhibit    anti-
clumping    (i.e., negative correlation) because atoms that are close together will not appear together.
in contrast, hierarchical correlated random measures do not rely on the atom values to determine
their correlation, and can capture both negative and positive correlation among their weights.

doshi-velez and ghahramani (2009) present a speci   c correlated feature model by positing a
higher level grouping of features. in their model, observations exhibit correlation through these
groups. however, groups only contain features and thus these feature can only express positive
correlation. in the latent feature models based on corrrms, the latent locations of two features can
induce a negative correlation between their co-occurrence.

ammann et al. (1978) study in   nitely divisible random measures that are not completely ran-
dom. these measures are referred to as having    aftereffects" in that every atom in the random
measure has more effect on the measure than just its size. correlated random measures are random
measures with aftereffects, but they are not necessarily in   nitely divisible.

as we have mentioned, the discrete in   nite logistic normal (diln) (paisley et al., 2012), a
bayesian nonparametric topic model, is a normalized instance of a correlated random measure.
diln    rst generates top level shared atoms from a dirichlet process, along with latent locations
for each. it then draws each document with a gamma process from those atoms and a gaussian
process evaluated at their locations. finally, it convolves these processes and normalizes to form a
id203 measure. we discuss diln in detail in section 4.3.

finally, there has been a lot of research in bayesian nonparametrics about dependent random
measures, originating from the work of maceachern (1999), broadly surveyed in foti et al. (2015),
and used in applications such as for dynamic ordinal data (deyoreo and kottas, 2015), neuron
spikes (gasthaus et al., 2009), and images (sudderth and jordan, 2009). dependent random mea-
sures select atoms for each observation through a priori covariates, such as a timestamp associated
with the observation. atoms are correlated, but only though these observed covariates. the main
ideas behind correlated random measures and dependent random measures are different. corre-
lations in corrrms are not based on side information, but rather are recovered through a random
function associated with each observation. one dependent random measure that is close in con-
struction to correlated random measures is the dependent poisson process thinning measure of foti
et al. (2013). this measure can be reinterpreted as a type of correlated random measure; we dis-
cuss this connection with technical details in section 3. another construction, compound random
measures (grif   n and leisen, 2014), builds dependent random measures by using a score func-
tions to generate a set of measures conditional on a shared poisson process. compound random

3

measures and the dependent poisson process thinning measure share with our approach the idea of
separating out the atom generation from the independence breaking portion.

2. background: completely random measures

in this section we review random measures (kingman, 1967; cinlar, 2011). we describe the pois-
son random measure, completely random measures, and normalized random measures. this sets
the stage for our construction of the correlated random measure in section 3.
a random measure m is a stochastic process that is indexed by a sigma algebra. let (e,e) be
a measurable space, for example e is the real line and e are the borel sets. a random measure is
a collection of random variables m (a)     [0,   ], one for each set a     e. the expectation of a
random measure is called the mean measure, which we denote   (a) (cid:44) e[m (a)].
one subclass of random measures is the class of completely random measures (kingman,
1967). a completely random measure is a random measure m (  ) such that for any disjoint    -
nite collection of sets a1, a2, . . . , an, the corresponding realizations of the measure on those sets
m (a1), m (a2), . . . , m (an) are independent random variables. completely random measures en-
compass many of the constructions in bayesian nonparametric statistics. some examples include
the poisson process, the beta process (hjort, 1990), the bernoulli process (thibaux and jordan,
2007), and the gamma process (ferguson, 1973).

we begin by describing the simplest example of a completely random measure, the poisson
random measure. the poisson random measure is constructed from a poisson process. it is char-
acterized solely by its mean measure   (  ) : e     [0,   ], which is an arbitrary measure on (e,e).
the complete characterization of a poisson random measure m (  ) is that the marginal distribution
of m (a) is a poisson with rate   (a).

we represent a poisson random measure with a set of atoms ai in e and a sum of delta measures

on those atoms (cinlar, 2011),

   (cid:88)

m (a) =

  ai(a).

the delta measure   ai(a) equals one when ai     a and zero otherwise. note there can be a
countably in   nite set of atoms, but only if   (e) =    .1 the distribution of the atoms comes from
the mean measure   (  ). for each    nite measurable set a, the atoms in a are distributed according
to   (  )/  (a).

i=1

we now expand the simple poisson random measures to construct more general completely
random measures. consider a poisson process on the cross product of e and the positive reals,
e    r+. it is represented by a set {(ai, wi)}   
i=1; each pair contains an atom ai and corresponding
weight wi     r+. the completely random measure is

m (a) =

wi  ai(a).

(1)

i=1

this poisson process is characterized by its mean measure, called the levy measure, which is
de   ned on the corresponding cross product of sigma algebras,
  (  ,  ) : e    b(r+)     [0,   ].

1this fact follows from the marginal distribution m (e)     poisson(  (e)) and that a poisson random variable with

rate equal to     is     almost surely.

4

   (cid:88)

we note that completely random measures also have    xed components, where the atoms are    xed
in advance and the weights are random. but we will not consider    xed components here.

we call the process homogenous when the levy measure factorizes,   (a, r) = h(a)    (r); we
call h the base measure. for example, in a nonparametric mixture of gaussians the base measure
is a distribution on the mixture locations (escobar and west, 1995); in a nonparametric model of
text, the base distribution is a dirichlet over distributions of words (teh et al., 2006).
we con   rm that m (  ) in equation 1 is a measure. first, m (   ) = 0. second, m (a)     0
for any a. finally, m (  ) satis   es countable additivity. de   ne a to be the union of disjoint sets

{a1, a2, . . .}. then m (a) =(cid:80)
   (cid:88)

   (cid:88)

   (cid:88)

m (ak) =

k m (ak). this follows from a simple argument,

   (cid:88)

   (cid:88)

   (cid:88)

wi  ai(ak) =

wi

  ai(ak) =

wi  ai(a) = m (a).

k=1

k=1

i=1

i=1

k=1

i=1

we used tonelli   s theorem to interchange the summations.

one example of a completely random measure is the gamma process (ferguson, 1973). it has

levy measure

  (da, dw) (cid:44) h(da)e   cw/wdw.

this is called the gamma process because if m     gamma-process(h, c) the random measure
m (a) on any set a     e is gamma distributed m (a)     gamma(h(a), c), where h(a) is the
shape and c is the rate (cinlar, 2011). the gamma process has an in   nite number of atoms   its
levy measure integrates to in   nity   but the weights of the atoms are summable when the base
. finally, when m (e) <    , we can
measure is    nite (h(e) <    ) because e[m (e)] = h(e)
normalize a completely random measure to obtain a random id203 measure. for example, we
construct the dirichlet process (ferguson, 1973) by normalizing the gamma process.

c

3. correlated random measures

the main limitation of a completely random measure is articulated in its de   nition   the random
variables m (ai) are independent. (because they are normalized, random id203 measures ex-
hibit some negative correlation between the m (ai), but cannot capture other types of relationships
between the probabilities.) this limitation comes to the fore particularly when we see repeated
draws of a random measure, such as in hierarchical bayesian nonparametric models (teh and jor-
dan, 2010). in these settings, we may want to capture and infer a correlation structure among
m (ai) but cannot do so with the existing methods (e.g., the hierarchical dirichlet process). to
this end, we construct correlated random measures. correlated random measures build on com-
pletely random measures to capture rich correlation structure between the measure at disjoint sets,
and this structure can be estimated from data.

we built completely random measures from a poisson process by extending the space from
simple atoms (in the poisson process) to the space of atoms and weights (in a completely random
measure). we build correlated random measures from completely random measures by extending
the space again. as for a completely random measure, there is a set of atoms and uncorrelated
weights. we now further supply each tuple with a    location   , a vector in rd, and extend the mean
measure of the poisson process appropriately. a correlated random measure is built from a poisson
process on the extended space of tuples {(ai, wi, (cid:96)i)}   
i=1.

5

   (cid:88)

in the completely random measure of equation 1, the uncorrelated weights wi give the measure
at each atom. in a correlated random measure there is an additional layer of variables xi, called
the transformed weights. these transformed weights depend on both the uncorrelated weights wi
and a random function on the locations f ((cid:96)i). in the random measure, they are used in place of
the uncorrelated weights,

m (a) =

xi  ai(a).

(2)

i=1

it is through the random function f (  ), which is drawn from a gaussian process (rasmussen and
williams, 2005), that the weights exhibit correlation.
we    rst review the gaussian process (gp) and then describe how to construct the transformed
weights. a gaussian process is a random function f ((cid:96)i) from rd     r.
it is speci   ed by a
positive-de   nite id812 k((cid:96)i, (cid:96)j) and mean function   ((cid:96)i). the de   ning characteristic of
a gp is that each joint distribution of a collection of values is distributed as a multivariate normal,

(f ((cid:96)1), . . . , f ((cid:96)n))     n (m,   ),

where mi =   ((cid:96)i) and   ij = k((cid:96)i, (cid:96)j).

in a correlated random measure, we draw a random function from a gp, evaluate it at the
locations of the tuples (cid:96)i, and use these values to de   ne the transformed weights. we specify
the transformation distribution of xi     r+ denoted t (xi | wi, f ((cid:96)i)).
it depends on both the
uncorrelated weights wi and the gp evaluated at (cid:96)i. for example, one transformation distribution
we will consider below is the gamma,

xi     gamma(wi, exp{   f ((cid:96)i)}).

(3)

but we will consider other transformation distributions as well. what is important is that the xi are
positive random variables, one for each atom, that are correlated through their dependence on the
gp f .

we have now fully de   ned the distribution of the transformed weights xi that are used in the
correlated random measure of equation 2. we emphasize that in a completely random measure the
weights are independent. the arguments that m (  ) is a measure, however, only relied on its form,
and not on the independence of the weights. (see equation 2.)

in summary, we build a correlated random measure by specifying the following: the mean
measure of a poisson process on atoms, weights, and locations   (da, dw, d(cid:96)); a id81
k((cid:96)i, (cid:96)j) between latent locations and a mean function m((cid:96)i); and the conditional transformation
distribution t (  | wi, f ((cid:96)i)) over positive values. with these elements, we draw a correlated random
measure as follows:

{(ai, wi, (cid:96)i)}   

i=1     poisson-process(  )
f     gaussian-process(m, k)
xi     t (  | wi, f ((cid:96)i)).

(4)
(5)
(6)

the random measure m (  ) is in equation 2.

2this means that for a    nite collection of inputs, the kernel produces a positive de   nite matrix.

6

before turning to some concrete examples, we set up some useful notation for correlated ran-

dom measures. we denote the in   nite set of tuples from the poisson process (equation 4) with

c (cid:44) {(ai, wi, (cid:96)i)}   
i=1.

given these tuples, the process for generating a correlated random measure    rst draws from a gaus-
sian process (equation 5), then transforms the weights (equation 6), and    nally constructs the mea-
sure from an in   nite sum (equation 2). we shorthand this process with m     corrrm(c, k,   , t ).3
we note that correlated random measures generalize completely random measures. speci   -
cally, we can construct a completely random measure from a correlated random measure by setting
the mean measure   (dx, dw, d(cid:96)) to match the corresponding completely random measure mean
measure   (dx, dw) (i.e., the location distribution does not matter) and asserting that xi = wi with
id203 one.

with the full power of the correlated random measure, we can construct correlated versions of
common random measures such as the gamma process, the beta process, and normalized measures
such as the dirichlet process. we give two examples below.

example: correlated gamma process. we discussed the gamma process as an example of a
completely random measure. we now extend the gamma process to a correlated gamma process.
first, we must extend the mean measure to produce atoms, weights, and locations. we specify an
additional distribution of locations l((cid:96))   we typically use a multivariate gaussian   and expand
the mean measure of the gamma process to a product,

  (da, dw, d(cid:96)) = l(d(cid:96))h(da)e   cw/wdw.

second, for the transformation distribution, we choose the gamma in equation 3. finally, we de   ne
the gp parameters, the kernel k((cid:96)i, (cid:96)j) = (cid:96)(cid:62)
i (cid:96)j and a zero mean   ((cid:96)i) = 0. with these components
in place, we draw from equation 4, equation 5, and equation 6. this is one example of a correlated
random measure.

example: correlated dirichlet process. we can normalize the measure to construct a cor-
related random id203 measure from a correlated random measure. if m (  ) is a correlated
random measure, then

g(a) = m (a)/m (e)

is a correlated random id203 measure. as we discussed in section 2, the dirichlet process is a
normalized gamma process (ferguson, 1973). when we normalize the correlated gamma process,
we obtain a correlated dirichlet process.
this construction requires that m (e) <    . proposition 2 in appendix a.2 describes condi-
tions for well-de   ned id172 (i.e., m (e) <    ) in correlated random measures. roughly,
these conditions require    niteness of the expected value of the transformed weights against the
product of the transformation distribution and the levy measure. this condition plays a central
role in constructing useful bayesian nonparametric models.

3as in the construction of completely random measures, correlated random measure can also include    xed com-

ponents, where the tuples are    xed, but the xi are random.

7

   (cid:88)

   (cid:88)

the correlation structure. finally, we calculate the correlation structure of a correlated ran-
dom measure. consider a measure, m     corrrm(c, k, m, t ). to understand the nature of the
correlation, we compute cov(m (a), m (b)) for two sets a and b.

we express this covariance in terms of the covariance between atom weights,

cov(m (a), m (b)) =

cov(xi, xj)  ai(a)  aj (b).

i=1

j=1

in other words, the covariance between the measure of two sets is the sum of the covariances
between the transformed weights of the atoms in the two sets. in a completely random measure,
the atom covariance is zero except when ai = aj. thus its covariance depends on the overlap
between sets, and nothing else. in a correlated random measure, however, there may be non-zero
covariance between the transformed weights.
for now we are holding the underlying poisson process c    xed, i.e., the atoms, untransformed

weights, and locations. the covariance between transformed weights is
cov(xi, xj |c) = e[xixj]     e[xi]e[xj].

(7)

these expectations are driven by two sources of randomness. first there is a gaussian process f ,
a random function evaluated at the    xed locations. second there is the transformation distribution
t . this is a distribution of an atom   s transformed weight, conditional on its untransformed weight
and the value of the gaussian process at its location (see equation 6).
using iterated expectation, we write the conditional covariance in equation 7 in terms of the
conditional mean of the transformed weights,   i (cid:44) e[xi | f ((cid:96)i), wi]. this is a function of the
gaussian process f . we rewrite the conditional covariance,

cov(xi, xj |c) = e[  i  j]     e[  i]e[  j],

(8)

where the expectations are taken with respect to the gaussian process. for the    rst term, the
distribution is governed by the distribution of the pair (f ((cid:96)i), f ((cid:96)j)), which is a bivariate normal.
for the second term, the marginals are governed by f ((cid:96)i) and f ((cid:96)j), which are univariate normal
distributions.

4. hierarchical correlated random measures

a correlated random measure takes us from a set of tuples to a random measure by way of a
gaussian process and a transformation distribution. when used in a downstream model of data,
we can infer the latent correlation structure from repeated realizations of measures from the same
set of tuples. it is thus natural to build hierarchical correlated random measures. hierarchical
correlated random measures are the central use of this new construction.
in a hierarchical correlated random measure, we    rst produce a set of tuples {(ai, wi, (cid:96)i)}   
i=1
from a poisson process and then re-use that set in multiple realizations of a correlated random
measure. in each realization, we    x the tuples (weights, atoms, and locations) but draw from the
gaussian process anew; thus we redraw the transformed weights for each realization.
as for the simple correlated random measure, we    rst specify the mean measure of the poisson
process   (  ), the kernel and mean for the gaussian process k(  ,  ), and the conditional transfor-
mation distribution t (  | wi, g((cid:96)i)). we then draw n hierarchical correlated random measures as

8

(cid:88)

follows:

c     poisson-process(  )

mj(a)     corrrm(c, m, k, t ).

this is a hierarchical bayesian nonparametric model (teh and jordan, 2010). there are mul-
tiple random measures mj. each shares the same set of atoms, locations, and weights, but each
is distinguished by its own set of transformed weights.4 the correlation structure of these trans-
formed weights is shared across measures. we note that this construction generalizes the discrete
in   nite logistic normal (paisley et al., 2012), which is an instance of a normalized correlated ran-
dom measure.

we use this construction in a model of groups of observations yj, for which we must construct a
likelihood conditional on the correlated rm. to construct a likelihood, many hierarchical bayesian
nonparametric models in the research literature use the integral with respect to the random measure.

(this is akin to an unnormalized    expectation.   ). de   ne m a (cid:44) (cid:82) am (da), and note that in a

discrete random measure this integral is an in   nite sum,

m a =

xiai.

(9)

i

the jth observations are drawn from a distribution parameterized from this sum, yj     p(  | m a).
for example, we will study models where m a is a collection of rates for independent poisson
distributions.

we present several examples of hierarchical correlated random measures. first, we develop
correlated nonparametric poisson factorization (cnpf) for factorizing matrices of discrete data.
this is the example we focus on for posterior id136 (section 5) and our empirical study (sec-
tion 6). we then illustrate the breadth of correlated random measures with two other examples, both
of which are latent feature models that build correlations into the class of models introduced by
grif   ths and ghahramani (2006). finally, we discuss the discrete in   nite logistic normal (diln)
of paisley et al. (2012). we show that diln is a type of normalized correlated random measure.

4.1. correlated nonparametric poisson factorization
bayesian nonparametric poisson id105 (gopalan et al., 2014) combines gamma pro-
cesses (ferguson, 1973) with poisson likelihoods to factorize discrete data organized in a matrix.
the number of factors is unknown and is inferred as a consequence of the bayesian nonparametric
nature of the model.

for concreteness we will use the language of patients getting diagnoses (e.g., patients going to
the hospital and getting marked for medical conditions). in these data, each cell of the matrix yuj is
the number of times patient u was marked for diagnosis j. the goal is to factorize users into their
latent    health statuses    and factorize items into their latent    condition groups   . these id136s
then let us form predictions about which unseen codes a patient might have. though we focus
our attention here on patients getting diagnoses, we emphasize that discrete matrices are widely

4in our empirical study of section 6, we will also endow each with its own mean function to the gaussian process,

mj(  ). here we omit this detail to keep the notation clean.

9

found in modern data analysis problems. in our empirical study, we will also examine matrices of
documents (rows) organized into word counts (columns) from a    xed vocabulary and user (rows)
clicks over a    xed collection of items (columns).

we will use a hierarchical correlated random measure to model these data, where each group
is a patient and the group-speci   c data are her vector of per-diagnosis counts. an atom ai is
a vector of positive weights for each diagnosis, drawn from independent gamma distributions,
h(a) = gamma(  ,   ). when the posterior of these atoms is estimated from diagnosis counts,
they will represent semantic groups of conditions such as    diabetes,"    heart disease," or    cancer.   
table 1 displays some of the atoms inferred from a set of patients from the mayo clinic.

in using a correlated random measure, the idea is that patients    expression for these conditions
are represented by the per-group weights xi. intuitively, these exhibit correlation. a patient who
has    heart    conditions is more likely to also have    vascular    conditions than    cancer.    (to be
clear, these groupings are the latent components of the model. there are an unbounded number
of them, they are discovered in posterior id136, and their labels are not known.) using these
correlations, and based on her history, a correlated model should better predict which diagnoses a
patient will have.

we now set up the model. we set the mean measure for the shared atoms to be
l )e   cw/wdw.

  (da, dw, d(cid:96)) (cid:44) gamma(da,   ,   )normal(d(cid:96), 0, id  2

we de   ne the gp mean function to be a per-patient constant mu((cid:96)i) =   u, where   u     n (0,   2
m).
these per-patient gp means account for data where some patients tend to be sicker than others.
we de   ne the gp id81 to be k((cid:96)i, (cid:96)j) = (cid:96)(cid:62)

finally, we consider two different transformation distributions. the    rst transformation distri-

i (cid:96)j.

butions is as in equation 3,

the second is

xi     gamma(wi, exp{   f ((cid:96)i)}).

(cid:18)

xi     gamma

1

wi,

log(1 + exp{f ((cid:96)i)}

(cid:19)

,

where log(1 + exp(  )) is known as the softplus function. with these de   nitions, we can compute
the conditional covariance for xi and xj using equation 8. table 4 displays some of positive
correlations between atoms found on patient diagnosis counts. these correlations are captured by
the locations, which are shared across patients, associated with each atom. atoms with positive
covariance in this model will have inferred locations that have a large inner product.

with these components in place, correlated nonparametric poisson factorization is

the distribution of yu is a collection of poisson variables, one for each diagnosis j, where

yuj     poisson

xuiaij

.

(10)

c     poisson-process(  )
  u     n (0,   2
m)
mu     corrrm(c,   u, k, t )
yu     p(  | mua).

(cid:33)

(cid:32)    (cid:88)

i=1

10

recall that the atoms ai are each a vector of gamma variables, one per diagnosis, and so aij is the
value of atom i for diagnosis j. for this model to be well de   ned each rate in equation 10 must be
   nite. using proposition 2 in appendix a.2, it is    nite almost surely if   l < 1.

the sum that de   nes the rate of yui is an in   nite sum of patient weights and condition weights.
thus, this model amounts to a factorization distribution for yui. given observed data, the posterior
distribution of the atoms ai and transformed patient weights xi gives a mechanism to form predic-
tions. note that the atoms ai are shared across patients, but through xi each patient exhibits them
to different degree. we discuss how to approximate this posterior distribution in section 5.

4.2. correlated latent feature models
mixture models, such at the dirichlet process mixture (antoniak, 1974), are the most commonly
used bayesian nonparametric model. in mixture models, each observation exhibits only a single
class. many data, such as images of multiple objects, are better characterized as belonging to mul-
tiple classes. latent feature models posit that each observation is associated with some number
of latent classes, taken from a set of features shared by all observations. for each observation, its
likelihood depends on parameters attached to its active features (e.g., a sum of those parameters).
examples of latent feature models include factorial mixtures (ghahramani, 1995) and factorial hid-
den markov models (ghahramani and jordan, 1997). (latent feature models are closely connected
to spike and slab priors (ishwaran and rao, 2005).)

bayesian nonparametric latent feature models allow the number of features to be unbounded.
as an example, consider analyzing a large data set of images. latent features could correspond
to image patches, such as recurring objects that appear in the images. in advance, we might not
know how many objects will appear in the data set. bnp latent feature models attempt to solve
this problem. bnp latent feature models have been used in many domains such as image denoising
(zhou et al., 2011) and link prediction in graphs (miller et al., 2009).

the most popular bnp latent feature model is the hierarchical beta-bernoulli process (thibaux
and jordan, 2007). this process was originally developed as the indian buffet process, which
marginalized out the beta process (grif   ths and ghahramani, 2006). before developing the corre-
lated version, we review the beta-bernoulli process.

the beta process is a completely random measure with atom weights in the unit interval (0,1).

its levy measure is

  (da, dw) = h(da)  w   1(1     w)     1.

we use the beta process in concert with the bernoulli process, which is a completely random
measure parameterized by a random measure with weights in the unit interval, i.e., a collection
of atoms and corresponding weights. a draw from a bernoulli process selects each atom with
id203 equal to its weight. this forms a random measure on the underlying space, where each
weight is one or zero (i.e., where only a subset of the atoms are activated). returning to latent
feature models, the beta-bernoulli process is

b     beta-process(h,   )
bn     bernoulli-process(b)
yn     p(  | bn)

11

the beta process generates the feature atoms; the bernoulli processes chooses which features are
active in each observation.

this model is built on completely random measures. thus, the appearances of features in
each observation are independent of one other. correlated random measures relax this assumption.
consider a latent feature model of household images with image patch features. the completely
random assumption here implies that the appearance of a spoon is independent of the appearance
of a fork. our construction can account for such dependencies between the latent features. below
we will give two examples of correlated nonparametric latent feature models, one based on the
beta process and the other based on the gamma process.

one method to develop a correlated beta-bernoulli process is to de   ne transformed weights at

the bernoulli process level. we de   ne the transformation distribution to be

xni     bernoulli(  (     1(wi) + f ((cid:96)i))),

where    is the sigmoid function   (x) = 1/(1 + exp{   x}). thus the beta-bernoulli correlated
latent feature model is

c     poisson-process(h(da)l(d(cid:96))  w   1(1     w)     1)
mn     corrrm(c,   , k, t ).

(we defer de   ning    and k, as they will be application speci   c.)

we do not need to use the beta process to de   ne a correlated latent feature model; what is
important is that the per-observation weights are either one or zero. for example, if the top level
process is a gamma process, which produces positive weights, then we can de   ne the transforma-
tion distribution to be

(cid:18) wi exp(f ((cid:96)i))

1 + wi exp(f ((cid:96)i))

(cid:19)

.

xni     bernoulli

the resulting gamma-bernoulli correlated latent feature model is

c     poisson-process(h(da)l(d(cid:96))e   cw/wdw)
mn     corrrm(c,   , k, t ).

the beta-bernoulli process uses only a    nite number of features to generate a    nite number of
observations. in appendix a.3, we give some conditions under which the correlated latent feature
models do the same.

4.3. discrete in   nite logistic normal
the correlated random measure construction that we developed generalizes the discrete in   nite
logistic normal (diln) (paisley et al., 2012). diln is an example of a normalized hierarchical
correlated random measure; its atom weights come from a normalized gamma random measure,
i.e., a dirichlet process.

diln was developed as a bayesian nonparametric mixed-membership model, or topic model,
of documents. in diln, each document mixes a set of latent topics (distributions over terms),
where the per-document topic proportions can exhibit arbitrary correlation structure. this is in

12

contrast to a hierarchical dirichlet process topic model (teh et al., 2006), where the topic propor-
tions are nearly independent.
categorical variables, i.e., word j in document u. set the kernel k((cid:96)i, (cid:96)j) = (cid:96)(cid:62)
to be positive hyperparameters. set the transformation distribution to be

we will express diln in terms of a correlated random measure. the observations wuj are
i (cid:96)j, and set    and   

xi     gamma(  wi, exp{   f ((cid:96)i)}).

with our construction, diln is

c     dirichlet-process(  , h(da)    n (d(cid:96), 0,   2
mu     normalized-corrrm(c, 0, k, t )
zuj     mu
wuj     zuj.

l id))

note that the shared tuples come from a dirichlet process, i.e., a normalized gamma process. when
modeling documents, the base distribution over atoms h(da) is a dirichlet distribution over the
vocabulary.

this is a mixed-membership model   there is an additional layer of hidden variables zuj, drawn
from the random measure, before drawing observations wuj. these hidden variables zui will be
atoms, i.e., distributions over the vocabulary, from the set of shared tuples and drawn with prob-
ability according to the per-document transformed weights. each observation wuj is drawn from
the distribution over terms given in its atom zuj.

paisley et al. (2012) show the id172 step is well de   ned when   l < 1. viewing diln
through the lens of correlated random measures makes clear what can be changed. for example,
the top level choice of the dirichlet process is not critical. it could be any random measure that
places    nite total mass, such as a gamma process or a beta process.

4.4. connection to dependent random measures
finally, we discuss the detailed connection between correlated random measures and dependent
random measures maceachern (1999). dependent random measures are a collection of measures
indexed by covariates. a broad class of dependent random measures can be created by thinning a
poisson process (foti et al., 2013). given a draw from a poisson process (ai, wi, (cid:96)i)   
i=1, where a
are atoms, (cid:96) are locations in the covariate space, and w are weights, the thinned dependent random
measure b for user u with covariate   u is

   (cid:88)

bu(a) =

xui  ai(a)

i=1

xui     wibernoulli(k(  u, (cid:96)i)),

where k is a function from t    l     [0, 1]. this construction is related to corrrms. consider the
correlated random measure

xui     wibernoulli(  (f ((cid:96)i)))
mu(a)     corrrm((ai, wi, (cid:96)i)   

i=1, m, k, t ).

13

from, this we can see that bu(a) d= mu(a) when   (fu((cid:96)i)) = k(  u, (cid:96)i). in other words, thinned
dependent random measures are equivalent to a type of correlated random measure where the
random function, fu associated with each user is known and given by the covariate.

we note that dependent random measures map from covariates to measures. thus they can be
viewed as a type of measure-valued regression. in parallel, correlated random measures use latent
covariates. in this sense, they can be viewed as measure-valued

5. variational id136 for correlated
nonparametric poisson factorization

computing the posterior is the central computational problem in bayesian nonparametric model-
ing. however, computing the posterior exactly is intractable. to approximate it, we use variational
id136 (jordan et al., 1999; wainwright and jordan, 2008), an alternative to markov chain monte
carlo. variational id136 has been used to approximate the posterior in many bayesian nonpara-
metric models (kurihara et al., 2007; doshi-velez et al., 2009; paisley and carin, 2009; wang and
blei, 2012) and has been of general interest in statistics (braun and mcauliffe, 2007; faes et al.,
2011; ormerod and wand, 2012). here we develop a variational algorithm for correlated nonpara-
metric poisson id105 (section 4.1).
variational id136 turns approximate posterior computation into optimization. we set up
a family of distributions over the latent variables q = {q(  )} and then    nd the member that
minimizes the kl divergence to the exact posterior. minimizing the kl divergence to the posterior
is equivalent to maximizing the evidence lower bound (elbo),

q   (  ) = arg max

q   q eq  (  )[log p(y,   )     log q(  )],

(11)

where    are the latent variables and y are the observations. in this paper we work with the mean-
   eld family, where the approximating distribution fully factorizes. each latent variable is indepen-
dently governed by its own variational parameter.

to develop a variational method for cnpf, we give a constructive de   nition of the gamma pro-
cess and introduce auxiliary variables for the gaussian process. we then de   ne the corresponding
mean-   eld family and show how to optimize the corresponding elbo.

additional latent variables. we    rst give a constructive de   nition of a homogeneous gamma
process. we scale the stick breaking construction of sethuraman (1994) as used in gopalan et al.
(2014); zhou and carin (2015). we de   ne stick lengths vk from a beta distribution and a scaling s
from a gamma distribution. the weights of the gamma process wk are from the following process,

s     gamma(  , c)
vk     beta(1,   )
wk = s

(cid:17)
(cid:81)k   1
j=1 (1     vj)

(cid:16)

vi

.

we treat the gamma shape    and rate c as latent variables (with gamma priors).

we adapt the auxiliary variable representation of zero-mean gaussian processes with linear
kernels (paisley et al., 2012) to more general gaussian processes. suppose gn is a gaussian

14

process with mean   n and a linear kernel. let d be a standard gaussian vector with same dimension
as (cid:96)k. we can write the process as

this lets us evaluate likelihoods without matrix inversion.

g((cid:96)k) d= (cid:96)(cid:62)

k d +   n((cid:96)k).

the mean-   eld family. with the latent variables for the gamma and gaussian processes in hand,
we now de   ne the mean-   eld variational distribution. we use the following approximating family
for each latent variable

q(xku) = gamma(  x
q(aki) = gamma(  a

ku,   x
ku)
ki,   a
ki)

q(s)q(vk)q((cid:96)k) =     s     vk
     (cid:96)k
       u
q(  )q(c) =            c,

q(du)q(  u) =      du

where   r represents a point mass at r. as in prior work on variational id136 for bayesian
nonparametrics, we use delta distributions in the top level stick components, scaling, and hyperpa-
rameters for analytic tractability (liang et al., 2007; paisley et al., 2012; gopalan et al., 2014).5

bayesian nonparametric models contain an in   nite number of latent variables. following blei
and jordan (2005), we truncate the variational approximation of the sticks vk and associated tuples
to t . in practice it is straightforward to recognize if the truncation level is too small because all of
the components will be populated in the    tted variational distribution. in our studies t = 200 was
suf   cient (section 6).

the goal of variational id136 is to    nd the variational parameters   the free parameters of
q, such as   (cid:96)k   that maximize the evidence lower bound. in appendix a.4, we describe how to
optimize the elbo (equation 11) with stochastic variational id136 (hoffman et al., 2013).
code will be made available on github.

we have derived a variational id136 algorithm for one example of a correlated random
measure model. deriving algorithms for other examples follows a similar recipe. in general, we
can handle id136 for covariance functions with inducing variables (titsias, 2009) and subsam-
pling (hensman et al., 2013). further, we can address models with intractable expectations   e.g.,
those arising from different transformation distributions or levy measures   with recent methods
for generic and nonconjugate variational id136 (salimans et al., 2013; ranganath et al., 2014;
wang and blei, 2013).

6. empirical study

we study correlated nonparametric poisson factorization (cnpf) and compare to its uncorrelated
counterpart on a large text data set and a large data set of medical diagnosis codes. quantitatively,
we    nd that the correlated model gives better predictive performance. we also    nd that it reveals
interesting visualizations of the posterior components and their relationships.

5this corresponds to variational expectation-maximization, where the e step computes variational expectations

and the m step takes map estimates of the latent variables with delta factors (beal, 2003).

15

6.1. study details
before giving the results, we describe the baseline models, the evaluation metric, and the hyperpa-
rameter settings.

baseline models. as a baseline, we compare against the uncorrelated variant of bayesian non-
parametric poisson factorization. as we mentioned in section 3, uncorrelated random measures
can be cast in the correlated random measure framework by setting a transformation distribution
that does not depend on the gaussian process.

recall that xik is the weight for data point i on component k. in the simplest bayesian non-

parametric poisson factorization model, the transformation distribution is

xik     gamma(wk, 1).

this is a two-layer hierarchical gamma process, and we abbreviate this model hgp. the    rst
layer contains shared atoms and weights. the second layer is a gamma process for each data point
(e.g., patient or document), with base measure given by the    rst layer   s measure.

the second uncorrelated model places further hierarchy on the log of the scale parameter of

the gamma,

xik     gamma (wk, exp(   mi)) .

here mi     normal(a, b), which captures variants in the row sums for each data point (i.e., how
many total diagnoses for a patient or how many words for a document). we call this model the
scaled hgp.

appendix a.5 gives id136 details for both uncorrelated models.

evaluation metric. we compare models with held out perplexity, a standard metric from infor-
mation retrieval that relates to held out predictive likelihood (geisser, 1975). we use the partial
observation scenario that is now common in id96 (wallach et al., 2009). the idea is to
uncover components from most of the data, and then evaluate how well those components can help
predict held out portions of new partially-observed data.

for each data set, we hold out 1,000 examples (i.e., rows of the matrix). from the remaining
examples we run approximate posterior id136, resulting in approximate posterior components
e[ak] that describe the data. with the 1,000 held out examples, we then split each observation (i.e.,
columns) randomly into two parts, 90% in one part (ytest) and 10% in the other (yobs). we condition
on the yobs (and that there is a test word) and calculate the conditional perplexity on ytest. a better
model will assign the true observations a higher id203 and thus lower perplexity. formally,
perplexity is de   ned as

(cid:18)   (cid:80)

(cid:80)

y   held out

w   ytest log p(w | yobs)

nheld out words

(cid:19)

.

perplexity = exp

perplexity measures the average surprise of the test observations. the exponent is the average
number of nats (base e bits) needed to encode the test sample.

16

1
7

figure 1: a graph of the latent component correlations found on the new york times. this    gure is best viewed on a computer with
zoom. the sizes of the components are related to their frequency. the correlation structure consists of several tightly connected groups
with sparse links between them.

chinachinesehongkongasianlifemansaysbookknowmoneypayplancostpercentcasefederalcourtlawjudgecenturysmallwineplacewhitebusinessexecutivecompaniescomputerchiefmusicdanceoperafestivalconcertstudyresearchdiseasefoundscientistsstockpercentmarketinvestorssharesfoodrestaurantchickensaucefreshstreetavenueparkeastwesthealthchildrencaremedicalhospitalgiantscoachbasketballjohnsonknicksroombedroombathtaxesmarketartmuseumgalleryartistsexhibitionislandwaterbeachriverseawaterplantgardenplanttreesisraelisraelipalestinianjewishpeaceincnetsharereportsearnsrockmusicbandjonesalbuid113agueplayerssportsbaseballteamcomhotelsitewebwwwcourtlawsupremejusticejudgetheaterbroadwayplayshowproductioncontractlosangelesleaguechicagochurchcatholicreligiousromangaysanmexicofranciscowilsonmexicanunionsovietmoscowrussiangorbachevspacekimkochmoonnasapageeditorletterfrontarticleindianatlanticcasinotrumplasfrenchfranceparistennisopenarticlecorrectionmisstatedcolumnaugbuildinghousingbuildingssquareprojectpercentbillionpricesoileconomyrangershockeygoaldevilsgamegameteamseasonsecondplayleagueplayerssportsbaseballteamyankeesmetsgamerunrunsnucleareuropeaneuropegermanyrussiagovernmentpoliticalcountrynationsministerwariraqmilitaryarmyiraqibushadministrationclintono   cialshouserepublicancampaignsenatebilldemocratic1: mammogram, routine medical exam, lumbago, cervical cancer screening, hypothyroidism
2: hypertension, hyperlipidemia, coronary atherosclerosis, prostate cancer screening, vaccine for in   uenza
3: acute pharyngitis, cough, myopia, vaccine for in   uenza, joint pain-shlder
4: child exam, vaccine for    u, otitis media, upper respiratory infection, pharyngitis
5: long-term anticoagulants, atrial    brillation, hypertension, congestive heart failure, chronic airway obstruction
6: normal pregnancy, normal    rst pregnancy, cervical cancer screening, delivery, conditions antepartum
7: diabetes-2, hypertension, hyperlipidemia, uncontrolled diabetes-2, diabetes-2 with ophthalmic manifestations
8: depression, dysthymia, anxiety state, generalized anxiety disorder, major depressive affective disorder
9: joint pain lower leg, arthritis lower leg, local arthritis lower leg, post-procedural status, follow-up surgery
10: allergic rhinitis, desensitization to allergens, asthma, chronic allergic conjunctivitis, chronic sinusitis
11: heart valve replacement, prostate cancer, lung and bronchus cancer, secondary bone cancer, other lung disease
12: morbid obesity, obesity, obstructive sleep apnea, sleep apnea, intestinal bypass status
13: acne, convulsions, abnormal involuntary movements, cerebral palsy, long-term use meds
14: abnormality of gait, personality change, persistent mental disorders, lack of coordination, debility
15: attention disorder w hyperactivity, attention disorder w/o hyperactivity, adjustment disorder, opposition de   ant disorder, conduct disturbance
16: diseases of nail, corns and callosities, dermatophytosis of nail, ingrowing nail, other states following surgery of eye and adnexa
17: alcohol dependence, tobacco use disorder, alcohol abuse, other alcohol dependence-in remission, other alcohol dependence-continuous
18: schizophrenia-paranoid, long-term use meds, schizophrenia, schizophrenia-paranoid-chronic, drug monitor
19: female breast cancer, personal history of breast cancer, lymph cancer, carcinoma in situ of breast, lymphedema
20: child health exam, vaccination for disease, vaccinations against pneumonia, need for prophylactic vaccination against viral hepatitis, procedure

1
8

table 1: the top twenty components on the mayo clinic data. we    nd that each factor forms a medically meaningful grouping of
diagnosis codes. for example, there are allergy, pregnancy, and alcohol dependence components.

data
nyt

mayo clinic

arxiv

hgp scaled hgp cnpf softplus-cnpf
3570
1251
5713

2755
779
2107

3283
877
4076

2768
780
2120

table 2: a summary of the predictive results on the new york times, the mayo clinic and arxiv
clicks. the correlated models outperform both the uncorrelated models. adding per observation
scalings improves predictions.

for the models we analyze, we compute this metric as follows. for each held out data point
we hold the components    xed (i.e., eq[ak]) and use the 10% of observed columns to form a vari-
ational expectation of the per-data point weights eq[xik]. in all models, we compute the held out
id203 of unobserved columns by using the multinomial conditioning property of poissons.
conditional on there being a test observation, it is assigned to a particular column (e.g., a word or
a diagnostic code) with id203 equal to that column   s normalized poisson rates. formally,

(cid:80)
(cid:80)
(cid:80)

j

p(yi = j) =

k eq[xik]eq[akj]

k eq[xik]eq[akj]

.

we measure the id203 of the ytest columns under this distribution. this evaluates how well
the discovered components can form predictions in new and partially observed observations.

hyperparameters. we set the hyperparameters on the base distribution to have shape .01 and
rate 10.0. we set the truncation level t to be 200, and found that none of the studies required more
than this. we set the dimensionality of the latent locations to be 25 and the prior variance to be 1
250.
we keep these hyperparameters    xed for all data.
in the algorithm, we use robbins monro learning rates, (50 + t)   .9 for the text data and (100 +

t)   .9 for the medical codes, and click data.

6.2. results
we evaluate our posterior    ts on text, medical diagnosis data, and click data.

the new york times. we study a large collection of text from the new york times. rows are
documents; columns are vocabulary words; the cell yij is the number of times term j appeared in
document i. after preprocessing, the data contains 100,000 documents over a vocabulary of 8,000
words. analyzing text data with a poisson factorization model is a type of id96 (blei,
2012).

table 2 summarizes the held-out perplexity. we    nd that the correlated model outperforms
both of the uncorrelated models. note that even in the uncorrelated model, adding a per-document
scale parameter improves predictions.

the model also provides new ways to explore and summarize the data. figure 1 is a graph of
the positive correlation structure in the posterior for the top    fty components, sorted by frequency;
table 3 contains a list of the top ten negative correlations. to explore the data, we compute corre-
lations between components by using their latent locations through the covariance function of the

19

israel, israeli, palestinian, jewish, peace
league, players, sports, baseball, team
room, bedroom, bath, taxes, market
news, book, magazine, editor, books

war, iraq, military, army, iraqi
space, kim, koch, moon, nasa
rock, music, band, jones, album
family, tax, board, paid, friend
water, plant, garden, plants, trees

union, soviet, moscow, russian, gorbachev

island, water, beach, river, sea

theater, broadway, play, show, production
building, housing, buildings, square, project

news, book, magazine, editor, books

bush, administration, clinton, of   cials, house

space, kim, koch, moon, nasa

room, bedroom, bath, taxes, market
indian, atlantic, casino, trump, las
century, small, wine, place, white

contract, los, angeles, league, chicago

table 3: the top ten pairs of negatively correlated components inferred from the new york times.
each pair of components are highly unlikely to cooccur in an article.
gaussian process. for these    ts, the covariance between (cid:96)i and (cid:96)j is (cid:96)(cid:62)
two components is thus

i (cid:96)j; the correlation between

k (cid:96)m(cid:112)(cid:96)(cid:62)

(cid:96)(cid:62)
k (cid:96)k (cid:96)(cid:62)

.

m(cid:96)m

  km =

we    nd the correlation structures contains highly connected groups, connected to each other by
   glue   , individual components that bridge larger groups. for example the bottom left connected
group of    international politics" is glued together with the top left group of       nance" through the
   political parties" component and the    law    component.

as we said above, we set the truncation level of the approximate posterior to 200 components.
figure 2 plots the atom weights of these 200 components, ordered by size. the posterior uses
about 75 components; the truncation level is appropriate.

medical history from the mayo clinic. we study medical code data from the mayo clinic.
this data set contains of all the international classi   cation of diseases 9 (icd-9) diagnosis codes
(also called billing codes) for a collection of patients over three years. the diagnosis codes mark
medical conditions, such as chronic ischemic heart disease, pure hypercholesterolemia, and type

20

figure 2: the weights in each tuple on the new york times ordered by magnitude. around 75
components are used.

figure 3: the weights in each tuple ordered by magnitude. around 50 of components are used.
though similar in size to the nyt data set, fewer components are used. the component usage has
a steeper decline.

21

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.000.050.100.150.20050100150200indexweightslllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.000.050.100.150.20050100150200indexweights    long-term anticoagulants, atrial    brillation, hypertension, congestive heart failure, chronic airway obstruction
    heart valve replacement, prostate cancer, lung and bronchus cancer, secondary bone cancer, other lung disease
    abnormality of gait, personality change, persistent mental disorders, lack of coordination, debility
    schizophrenia-paranoid, long-term use meds, schizophrenia, schizophrenia-paranoid-chronic, drug monitor
    attention de   cit disorder with hyperactivity, attention de   cit disorder without hyperactivity, adjustment
disorder with disturbance of emotions and conduct, opposition de   ant disorder, conduct disturbance

    schizophrenia-paranoid, long-term use meds, schizophrenia, schizophrenia-paranoid-chronic, drug monitor
    depression, dysthymia, anxiety state, generalized anxiety disorder, major depressive affective disorder
    alcohol dependence, tobacco use disorder, alcohol abuse, other alcohol dependence-in remission,

other alcohol dependence-continuous

    long-term anticoagulants, atrial    brillation, hypertension, congestive heart failure, chronic airway obstruction
    diabetes-2, hypertension, hyperlipidemia, uncontrolled diabetes-2, diabetes-2 with ophthalmic manifestations
    hypertension, hyperlipidemia, coronary atherosclerosis, prostate cancer screening, vaccine for in   uenza
    long-term anticoagulants, atrial    brillation, hypertension, congestive heart failure, chronic airway obstruction
    heart valve replacement, prostate cancer, lung and bronchus cancer, secondary bone cancer, other lung disease
    female breast cancer, personal history of breast cancer, lymph cancer, carcinoma in situ of breast, lymphedema
    depression, dysthymia, anxiety state, generalized anxiety disorder, major depressive affective disorder
    schizophrenia-paranoid, long-term use meds, schizophrenia, schizophrenia-paranoid-chronic, drug monitor
    diabetes-2, hypertension, hyperlipidemia, uncontrolled diabetes-2, diabetes-2 with ophthalmic manifestations
    schizophrenia-paranoid, long-term use meds, schizophrenia, schizophrenia-paranoid-chronic, drug monitor
    mammogram, routine medical exam, lumbago, cervical cancer screening, hypothyroidism
    female breast cancer, personal history of breast cancer, lymph cancer, carcinoma in situ of breast, lymphedema

table 4: the top ten correlations among the heavily used components in the mayo clinic data.
we    nd several medically meaningful relationships between latent components. for example, the
relationships between obesity and type 2 diabetes is well established.

2 diabetes. the entire collection contains 142,297 patients and 11,102 codes. patients are rows in
the matrix; codes are columns; each cell marks how many times the patient was assigned to the
code.

table 2 summarizes the held-out perplexity. again, the correlated model does best. further, as
for text modeling, it is important to allow a patient-speci   c scale parameter to capture their relative
health. figure 3 plots the posterior sticks, ordered by size. the approximate posterior uses about
50 components, using the    rst 20 more heavily.

table 1 contains the 20 most commonly used components. the components correspond to med-
ically meaningful groups of conditions, such as obesity (12), type 2 diabetes (7), and breast malig-
nancy (19). the top positive correlations are in table 4. there are several meaningful correlations,
such as depression & alcohol dependency, and using anticoagulants & hypertension/lipidemia.
note that the relationship between schizophrenia and type 2 diabetes is an active area of research
in medicine (suvisaari et al., 2008; liu et al., 2013).

22

arxiv click data. finally, we examine user click data from the arxiv, an online repository of
research articles. the arxiv initially focused on physics articles but has now expanded to many
other domains, including statistics. this data set contains the number of times each user clicked
on an article; it spans 50,000 users and 20,000 articles. building models of such data is useful, for
example, to develop id126s that    nd interesting articles to arxiv readers.

as for the other data, we hold out some of the clicks and try to predict them. table 2 sum-
marizes the results. we    nd similar results as on our other two data sets. the correlated models
outperform the uncorrelated models on predicting unseen clicks for new users. we    nd that the
standard cnpf model outperforms the softplus cnpf on all of our data sets.

7. discussion

we present correlated random measures. correlated random measures enable us to construct ver-
sions of bayesian nonparametric models that capture correlations between their components. we
construct several examples of such models, and develop the id136 algorithm in detail for one of
them, correlated nonparametric poisson factorization. with this model, we    nd that the correlated
random measure improves predictions and produces interesting interpretable results.

random id203 measures such as the dirichlet process are consistent for density estima-
tion, so why might one prefer a correlated random measure over a completely random measure?
we conjecture that correlated random measures make more ef   cient use of the data. one promis-
ing avenue of future research is to study the rates of correlated random measures versus completely
random measures.

correlated random measures model latent correlations in the data, while dependent random
measures model correlations based on observed covariates. combining these two ideas to incorpo-
rate correlations both observed and latent yields a broader class of random measures that can model
many real world phenomena. another avenue of future research is to study this construction both
methodologically and practically.

we de   ne correlated random measures by combining poisson and gaussian processes. how-
ever, we note that other processes can also be used for the source of tuples (ai, wi, (cid:96)i) and the
random function. for example, the diln model of section 4.3 uses a dirichlet process to form its
tuples; another way to generate tuples would be through the pitman-yor process (pitman and yor,
1997; teh et al., 2006).

similarly, though we used a gaussian process to de   ne a random function from latent locations
to real values, there are other possibilities. for example we can replace the gp with the student-t
process (shah et al., 2014). or, if we restrict the latent locations to be positive then we can use
them to subordinate, as an index to, another stochastic process, such as brownian motion. also, we
could use discrete random functions to form feature groups. we leave these extensions for possible
future research.

acknowledgements. this work is supported by nsf iis-1247664, onr n00014-11-1-0651,
darpa fa8750-14-2-0009, darpa n66001-15-c-4032, adobe, ndseg fellowship, porter og-
den jacobus fellowship, seibel foundation, and the sloan foundation. the authors would like to
thank the reviewers for their helpful feedback and comments.

23

references

ammann, l. p., thall, p. f., et al. (1978). random measures with aftereffects. the annals of

id203, 6(2):216   230.

antoniak, c. (1974). mixtures of dirichlet processes with applications to bayesian nonparametric

problems. the annals of statistics, 2(6):1152   1174.

beal, m. (2003). variational algorithms for approximate bayesian id136. phd thesis, gatsby

computational neuroscience unit, university college london.

bishop, c. (2006). pattern recognition and machine learning. springer new york.

blei, d. (2012). probabilistic topic models. communications of the acm, 55(4):77   84.

blei, d. and jordan, m. (2005). variational id136 for dirichlet process mixtures. journal of

bayesian analysis, 1(1):121   144.

borodin, a. (2009). determinantal point processes. arxiv preprint arxiv:0911.1153.

braun, m. and mcauliffe, j. (2007). variational id136 for large-scale models of discrete choice.

journal of american statistical association, 105(489).

canny, j. (2004). gap: a factor model for discrete data.

in proceedings of the 27th annual
international acm sigir conference on research and development in information retrieval.

cinlar, e. (2011). id203 and stochastics. springer.

cox, d. r. and isham, v. (1980). point processes. chapman hall.

deyoreo, m. and kottas, a. (2015). modeling for dynamic ordinal regression relationships: an

application to estimating maturity of rock   sh in california. arxiv preprint arxiv:1507.01242.

doshi-velez, f. and ghahramani, z. (2009). correlated non-parametric latent feature models.
in proceedings of the twenty-fifth conference on uncertainty in arti   cial intelligence, pages
143   150. auai press.

doshi-velez, f., miller, k., van gael, j., and teh, y. (2009). variational id136 for the indian
buffet process. in proceedings of the intl. conf. on arti   cial intelligence and statistics, pages
137   144.

dunson, d. b. and herring, a. h. (2005). bayesian latent variable models for mixed discrete

outcomes. biostatistics, 6(1):11   25.

escobar, m. and west, m. (1995). bayesian density estimation and id136 using mixtures.

journal of the american statistical association, 90:577   588.

faes, c., ormerod, j. t., and wand, m. p. (2011). id58ian id136 for parametric
and nonparametric regression with missing data. journal of the american statistical association,
106(495).

24

ferguson, t. (1973). a bayesian analysis of some nonparametric problems. the annals of statis-

tics, 1:209   230.

foti, n. j., futoma, j. d., rockmore, d. n., and williamson, s. (2013). a unifying representation
for a class of dependent random measures. in international conference on arti   cal intelligence
and statistics.

foti, n. j., williamson, s., et al. (2015). a survey of non-exchangeable priors for bayesian
nonparametric models. pattern analysis and machine intelligence, ieee transactions on,
37(2):359   371.

fox, e. b., sudderth, e. b., jordan, m. i., and willsky, a. s. (2011). a sticky hdp-id48 with

application to speaker diarization. the annals of applied statistics, pages 1020   1056.

gasthaus, j., wood, f., gorur, d., and teh, y. w. (2009). dependent dirichlet process spike sorting.

in advances in neural information processing systems, pages 497   504.

geisser, s. (1975). the predictive sample reuse method with applications. journal of the american

statistical association, 70(350):320   328.

ghahramani, z. (1995). factorial learning and the em algorithm. in advances in neural informa-

tion processing systems, pages 617   624.

ghahramani, z. and jordan, m. (1997). factorial id48. machine learning,

31(1).

gopalan, p., ruiz, f. j., ranganath, r., and blei, d. m. (2014). bayesian nonparametric poisson
factorization for id126s. in international conference on arti   cial intelligence
and statistics.

grif   n, j. e. and leisen, f. (2014). compound random measures and their use in bayesian non-

parametrics. arxiv preprint arxiv:1410.0611.

grif   ths, t. and ghahramani, z. (2006). in   nite latent feature models and the indian buffet process.

in advances in neural information processing systems (nips).

hensman, j., fusi, n., and lawrence, n. d. (2013). gaussian processes for big data. in conference

on uncertainty in arti   cial intelligence.

hjort, n. (1990). nonparametric bayes estimators based on beta processes in models for life history

data. the annals of statistics, 18(3).

hoffman, m., blei, d., wang, c., and paisley, j. (2013). stochastic variational id136. journal

of machine learning research, 14(1303   1347).

honkela, a., tornio, m., raiko, t., and karhunen, j. (2008). natural conjugate gradient in varia-

tional id136. in neural information processing.

ishwaran, h. and rao, j. s. (2005). spike and slab variable selection: frequentist and bayesian

strategies. the annals of statistics, 33(2):730   773.

25

jordan, m., ghahramani, z., jaakkola, t., and saul, l. (1999). introduction to variational methods

for id114. machine learning, 37:183   233.

kingman, j. (1967). completely random measures. paci   c journal of mathematics, 21(1).

kingman, j. (1993). poisson processes. oxford university press, usa.

kurihara, k., welling, m., and teh, y. (2007). collapsed variational dirichlet process mixture

models. in international joint conferences on arti   cial intelligence (ijcai).

liang, p., petrov, s., klein, d., and jordan, m. (2007). the in   nite pid18 using hierarchical

dirichlet processes. in empirical methods in natural language processing.

liu, y., li, z., zhang, m., deng, y., yi, z., and shi, t. (2013). exploring the pathogenetic asso-
ciation between schizophrenia and type 2 diabetes mellitus diseases based on pathway analysis.
bmc medical genomics, 6(suppl 1):s17.

maceachern, s. (1999). dependent nonparametric processes. in asa proceedings of the section

on bayesian statistical science.

miller, k., grif   ths, t., and jordan, m. (2009). nonparametric latent feature models for link
in bengio, y., schuurmans, d., lafferty, j., williams, c. k. i., and culotta, a.,

prediction.
editors, advances in neural information processing systems 22, pages 1276   1284.

m  ller, j., syversveen, a. r., and waagepetersen, r. p. (1998). log gaussian cox processes.

scandinavian journal of statistics, 25(3):451   482.

ormerod, j. t. and wand, m. (2012). gaussian variational approximate id136 for generalized

linear mixed models. journal of computational and graphical statistics, 21(1):2   17.

paisley, b. and carin, l. (2009). nonparametric factor analysis with beta process priors.

international conference on machine learning.

in

paisley, j., wang, c., and blei, d. (2012). the discrete in   nite logistic normal distribution.

bayesian analysis, 7(2):235   272.

paisley, j., wang, c., blei, d. m., jordan, m., et al. (2015). nested hierarchical dirichlet processes.

pattern analysis and machine intelligence, ieee transactions on, 37(2):256   270.

pitman, j. and yor, m. (1997). the two-parameter poisson-dirichlet distribution derived from a

stable subordinator. the annals of id203, pages 855   900.

ranganath, r., gerrish, s., and blei, d. (2014). {black box variational id136}. in proceedings
of the seventeenth international conference on arti   cial intelligence and statistics, pages 814   
822.

rasmussen, c. e. and williams, c. k. i. (2005). gaussian processes for machine learning. the

mit press.

26

robbins, h. and monro, s. (1951). a stochastic approximation method. the annals of mathemat-

ical statistics, 22(3):pp. 400   407.

salimans, t., knowles, d. a., et al. (2013). fixed-form variational posterior approximation

through stochastic id75. bayesian analysis, 8(4):837   882.

sato, m. (2001). online model selection based on the id58. neural computation,

13(7):1649   1681.

sethuraman, j. (1994). a constructive de   nition of dirichlet priors. statistica sinica, 4:639   650.

shah, a., wilson, a. g., and ghahramani, z. (2014). student-t processes as alternatives to gaussian

processes. arxiv preprint arxiv:1402.4306.

sudderth, e. b. and jordan, m. i. (2009). shared segmentation of natural scenes using dependent
in advances in neural information processing systems, pages 1585   

pitman-yor processes.
1592.

suvisaari, j., per  l  , j., saarni, s. i., h  rk  nen, t., pirkola, s., joukamaa, m., koskinen, s., l  n-
nqvist, j., and reunanen, a. (2008). type 2 diabetes among persons with schizophrenia and
other psychotic disorders in a general population survey. european archives of psychiatry and
clinical neuroscience, 258(3):129   136.

teh, y. (2006). a hierarchical bayesian language model based on pitman-yor processes.

proceedings of the association of computational linguistics.

in

teh, y. w., jordan, m., beal, m. j., and blei, d. m. (2006). hierarchical dirichlet processes.

journal of the american statistical association, 101:1566   1581.

teh, y. w. and jordan, m. i. (2010). hierarchical bayesian nonparametric models with applica-
tions. in hjort, n., holmes, c., m  ller, p., and walker, s., editors, bayesian nonparametrics:
principles and practice. cambridge university press.

thibaux, r. and jordan, m. (2007). hierarchical beta processes and the indian buffet process. in

11th conference on arti   cial intelligence and statistics.

tieleman, t. and hinton, g. (2012). lecture 6.5-rmsprop: divide the gradient by a running average

of its recent magnitude. in coursera: neural networks for machine learning.

titsias, m. k. (2008). the in   nite gamma-poisson feature model. in advances in neural informa-

tion processing systems, pages 1513   1520.

titsias, m. k. (2009). variational learning of inducing variables in sparse gaussian processes. in

international conference on arti   cial intelligence and statistics, pages 567   574.

wainwright, m. and jordan, m. (2008). id114, exponential families, and variational

id136. foundations and trends in machine learning, 1(1   2):1   305.

wallach, h., murray, i., salakhutdinov, r., and mimno, d. (2009). evaluation methods for topic

models. in international conference on machine learning (icml).

27

wang, c. and blei, d. m. (2012). truncation-free stochastic variational id136 for bayesian

nonparametric models. in advances in neural information processing systems (nips).

wang, c. and blei, d. m. (2013). variational id136 for nonconjutate models. jmlr.

williamson, s., wang, c., heller, k. a., and blei, d. m. (2010). the ibp compound dirichlet
process and its application to focused id96. in proceedings of the 27th international
conference on machine learning (icml-10), pages 1151   1158.

zhou, m. and carin, l. (2015). negative binomial process count and mixture modeling. pattern

analysis and machine intelligence.

zhou, m., chen, h., paisley, j., ren, l., sapiro, g., and carin, l. (2009). non-parametric bayesian
dictionary learning for sparse image representations. in bengio, y., schuurmans, d., lafferty, j.,
williams, c. k. i., and culotta, a., editors, advances in neural information processing systems
22, pages 2295   2303.

zhou, m., hannah, l., dunson, d., and carin, l. (2012). beta negative binomial process and

poisson factor analysis. in international conference on arti   cial intelligence and statistics.

zhou, m., yang, h., sapiro, g., dunson, d. b., and carin, l. (2011). dependent hierarchical
in international conference on arti   cial

beta process for image interpolation and denoising.
intelligence and statistics, pages 883   891.

a. appendix

in the appendix we describe the laplace transform of corrrms, establish conditions for integra-
bility, derive id136 for correlated nonparametric poisson factorization, and show the changes
to id136 needed for variational id136 in our comparison models.

a.1. laplace transform
we can characterize the laplace functional of a poisson driven correlated random measure in terms
of a gaussian expectation.

proposition 1. let m be drawn from a correlated random measure with poisson mean measure   ,
gaussian process parameters m and k with gaussian process draw f , transformation distribution
p(x|  ), and let g be a positive, real valued, e measurable function, then the laplace functional
e[em g] = ef [e       (1   e   rg(a)x)], where      =   (da, dw, d(cid:96))p(dx|f ((cid:96)i), wi).

the laplace functional can be used for analytic computation of properties of correlated random

measures as it characterizes all moments of integrals with respect to this random measure.

28

proof of proposition 1. let (ai, wi, (cid:96)i)i   i be the atoms of the poisson random measure drawn
with mean   . then, conditional on f by the transformation property of poisson random measures
(ai, wi, (cid:96)i, xi) is a poisson random measure. this follows as given f, wi, (cid:96)i, the xi are conditionally
independent (kingman, 1993). the mean measure of this poisson random measure given f is

     =   (da, dw, d(cid:96))p(dx|f ((cid:96)i), wi).

(12)

the correlated random measure m can be written as integral with respect to the poisson random
measure n with mean      as n f (a)x. thus,

e[e   rm g] = e[e[e   rm g|f ]] = e[e[e   rn g(a)x|f ]] = e[e       (1   e   rg(a)x)],

where the last step follows from the laplace functional of a poisson process.

integrability

a.2.
establishing conditions for integrability with respect to the random measure aids in the construc-
tion of models (consider the aforementioned correlated id203 measures). here we provide
a proposition (using the notation x     y is the smaller of x and y) that completely characterizes
integrability of positive functions with respect to a poisson driven correlated random measure.
proposition 2. let m be drawn from a correlated random measure with poisson mean measure
  , gaussian process parameters m and k with gaussian process draw f , transformation distri-
bution p(x|  ), and let g be a positive, real valued, e measurable function. then, m g is    nite with
id203 pf (    g(a)x     1 <    ) , where      =   (da, dw, d(cid:96))p(dx|f ((cid:96)i), wi).

note the id203 is a gaussian expectation. this proposition parallels the integrability
conditions based on the mean measure for poisson random measures (cinlar, 2011). we use this
proposition to establish    niteness in our examples.

proof of proposition 2. we    rst begin by noting that

p(m g <    ) = lim
r   0

ee   rm g = lim
r   0

e[e       (1   e   rg(a)x)] = e[lim
r   0

e       (1   e   rg(a)x)],

by proposition 1 and where the last equality follows from the dominated convergence theorem and
the positivity of rg(a)x.
the function g(a)x     1 dominates (1     e   rg(a)x) for r < 1, thus when     g(a)x     1 <    , then
limr   0 e       (1   e   rg(a)x) = 1. similarly (1     e   rg(a)x) dominates (1     e   1)(g(a)x     1), thus when
    g(a)x     1 =    , limr   0 e       (1   e   rg(a)x) = 0. putting this all together gives

e       (1   e   rg(a)x)] = e[  (    g(a)x     1 <    )] = pf (    g(a)x     1 <    ).

e[lim
r   0

we can use proposition 2    niteness of m (e) by letting g equal to 1 everywhere. that is,

both propositions naturally extend to the hierarchical case when the shared tuples come from

thus p(m g <    ) = pf (    g(a)x     1 <    ).
p(m (e) <    ) = pf (    x     1 <    ).

a poisson process.

29

a.3. finiteness
finiteness of the corrrm in correlated nonparametric poisson factorization. the poisson
rate is an inner product between iid gamma variables and a draw from a correlated random measure.
thus the poisson rate is    nite almost surely if draws from the correlated random measure produce
   nite measures almost surely. we show this using proposition 2.

in this case the mean of the conditional poisson random measure is

    (d(cid:96), da, dw, dx) = l(d(cid:96))h(da)e   cw/wdw

e   f ((cid:96))w
  (w)

xw   1e   xe   f ((cid:96))dx.

we seek to integrate x     1 with respect to this measure. using a change of variables, this integral
is equal to

(cid:90)

x     1l(d(cid:96))h(da)e   cw/wdw

xw   1e   xe   f ((cid:96))dx

   

l(d(cid:96))h(da)e   cw/wdwx

xw   1e   xe   f ((cid:96))dx

e   f ((cid:96))w
  (w)

e   f ((cid:96))w
  (w)

= h(e)

l(d(cid:96))e   cwef ((cid:96))dw

(cid:90)
(cid:90)

h(e)

thus the poisson rate is    nite when(cid:82) ef ((cid:96))l(d(cid:96)) <    . by tonelli   s theorem and assuming g has

ef ((cid:96))l(d(cid:96)).

=

c

a linear kernel and a constant mean    which has a mean zero and   2

   variance prior,

e[

ef ((cid:96))l(d(cid:96))] =

1

2 k((cid:96),(cid:96))l(d(cid:96)) + exp

e

(cid:18)   2

(cid:19)

  
2

.

if k((cid:96), (cid:96)) is bounded, then the measure is    nite almost surely regardless of the density l(d(cid:96)). the
linear covariance function is unbounded. in this case, from the above equality we have

(cid:90)

d/2(cid:90)

e[

ef ((cid:96))l(d(cid:96))] =

1
2  

1

2 (1   1/  2)(cid:96)(cid:62)(cid:96)d(cid:96) <    ,

e

for   2 < 1. putting this all together means that the poisson rate is almost surely    nite for linear
kernel when the locations are drawn from an isotropic gaussian with variance less than 1. the
same conditions transfer to the softplus variant as exp(x)     log(1 + exp(x)).

finiteness of the beta-bernoulli correlated latent feature model. we use proposition 2 to
establish    niteness of this measure. we note that    niteness in the number of features follows
from summability of the id203 that each feature is on. the mean of the conditional random
measure of the probabilities is

    (d(cid:96), da, dw, dx) = l(d(cid:96))h(da)  w   1(1     w)     11{xi =   (     1(w) + f ((cid:96)))}dwdx.

30

(cid:90)

(cid:90)

(cid:90)

(cid:90)

the integral of x     1 is the same as x, as x is bounded by 1 due to the logistic function. thus,

xl(d(cid:96))h(da)  w   1(1     w)     11{xi =   (     1(w) + f ((cid:96)))}dwdx

= h(e)

  w   1(1     w)     1  (     1(w) + f ((cid:96)))dwl(d(cid:96)).

(cid:90)

we assume that h(e) is a    nite, like in a id203 distribution. thus the    niteness of this
quantity only depends on the interior integral. we can split this integral over each half of the unit
interval. the integral of the second half is

this means    niteness only depends on the integral with respect to the    rst part of the unit interval.
the integral over the    rst half is

(cid:90) (cid:90) 1

(cid:90) 1

1
2

1
2

    2

(cid:90) (cid:90) 1

2

2

(cid:90) (cid:90) 1
(cid:90) (cid:90) 1
(cid:90)

0

0

2

0

=

=

   

  w   1(1     w)     1  (     1(w) + f ((cid:96)))dwl(d(cid:96))

  (1     w)     1dw =

1

2(       1)

<    .

  w   1(1     w)     1  (     1(w) + f (l))dwl(d(cid:96))

  w   1(1     w)     1

1
1 + 1   w
w e   f (l)
1

dwl(d(cid:96))

  (1     w)     2

(cid:90) 1

2

w

(cid:90)
1   w + e   f (l) dwl(d(cid:96))

  (1     w)     2dw = c

ef ((cid:96))l(dl)

ef (l)l(d(cid:96)),

   nite when(cid:82) ef ((cid:96))l(d(cid:96)) is    nite. the measure is almost surely    nite for a linear kernel when the

for some constant c. following the same argument for cnpf above, this means the measure is

0

locations are drawn from an isotropic gaussian with variance less than 1 and for bounded variance
covariance functions.

finiteness of the gamma-bernoulli correlated latent feature model. the sum of the activation
probabilities can be given as

   (cid:88)
   (cid:88)

i=1

z (cid:44)

   

wi exp(f ((cid:96)i))

wi exp(f ((cid:96)i)) + 1

wi exp(f ((cid:96)i)).

(13)

i=1

this is the same as the    nite measure condition in cnpf. thus, the measure is almost surely    nite
for a linear kernel when the locations are drawn from an isotropic gaussian with variance less than
1 and for bounded variance covariance functions.

31

a.4. variational id136 for cnpf
variational id136 for cnpf maximizes the evidence lower bound (elbo). the full elbo
for cnpf is

l =eq[log p(  )] + eq[log p(c)] + eq[log p(s|  , c)] +

eq[log p(vk|  )]

t(cid:88)

k=1

t(cid:88)

i(cid:88)

+ eq[log p((cid:96)k)] +

eq[log p(aki)     log q(aki)]

k=1

i=1

eq[log p(du)] + eq[log p(  u)] +

t(cid:88)

eq[log p(xku|du, vk, s, (cid:96)k)     log q(xku)]

eq[log p(  yuik|xku, aki, yui)     log q(yuik)],

k=1

(14)

u(cid:88)
i(cid:88)

u=1

i=1

+

+

where the elbo for cnpf is augmented with an auxiliary variable   yui to allow for analytic updates
similar to dunson and herring (2005), zhou et al. (2012), and gopalan et al. (2014). we now
describe its role. in variational id136, the update for a latent variable depends on the variational
expectation of terms in the joint distribution where that variable appears (bishop, 2006). the
update of the components of the base measure depends on the observation log-likelihood

eq[   (cid:88)

xkuaik + yui log(

xkuaik)     log yui!].

the second term,

k

yuieq[log(

xkuaik)],

(15)

(cid:88)
(cid:88)

k

k

does not have analytic form when xku and aik are gamma distributed. to address this, we de-
compose the poisson observation into a sum of poisson variables. from the additivity of poisson
random variables, the poisson observation in cnpf is equivalent to

   (cid:88)

yui =

  yuik,   yuik     poisson(xkuaik),

with the auxiliary   yuik marginalized out. the rate of these auxiliary poisson is no longer a sum, so
the variational expectations are tractable.

k=1

in mean    eld variational id136, the update to the approximating family of a latent variable
depends on the distribution of that latent variable conditional on everything else (bishop, 2006).
conditional on yui, a, z, the vector   yui is multinomially distributed (zhou et al., 2012) as the
following

(cid:80)   

xkuaik
k=1 zkuaik

).

  yui|yui, a, x     mult(

32

we introduce these auxiliary variables for only those observations that are nonzero as eq.15 is zero
for zero observations.

in cnpf, there are global latent variables which are the set of global tuples, and local latent
variables which are the correlated random measure associated with each patient. we de   ne an
equivalent objective in terms of just the global latent variables g by maximizing over the per-
patient latent variables.

lg = max  l(g,   ) = l(g,      (g)),

where      (g) is the setting of the per-patient parameters that maximizes the elbo given the global
parameters g. to maximize lg we need to compute its gradient. by the chain rule,

   lg
   g

   l
   g

=

(g,      (g)) +

   l
     

(g,      (g))

        (g)

   g

(g) =

   l
   g

(g,      (g)).

thus in words, the gradient of the objective parameterized by just the global variables is the
gradient of the original objective evaluated at the maximizing per-patient parameters given the
global variables. this yields a mixed coordinate ascent/gradient ascent maximization for this ob-
jective that allows for parallel computation across patients. we will now detail all of the global
gradients followed by the local coordinate updates.

global gradients. given the optimal variational parameters for each of the patients, we give
the gradients of the variational parameters shared across patients. the global gradients may be
prescaled by a positive de   nite matrix (preconditioner) for ef   ciency.

ki and   a

ki. the variational approximation for aki, the positive condition
natural gradient of   a
weight in each component, is the same family as the prior, the gamma distribution. here   a
ki and
ki represent the shape and rate of the approximation respectively. we compute natural gradients,
  a
which are gradients scaled by the inverse fisher information matrix of the variational approxima-
tion. these gradients have been shown to have good computation properties (sato, 2001; honkela
et al., 2008; hoffman et al., 2013). the natural gradient with respect to   a

ki and   a

ki are

   l
     a
ki
   l
     a
ki

=  a

h +

=  a

h +

u(cid:88)
u(cid:88)

u=1

u=1

  uikyui

  x
ku
  x
ku

.

from this equation computing the gradients require iterating over the entire observation matrix. for
large, sparse observation matrices, this is inef   cient. we rewrite the gradient in terms of nonzero
yui as

   l
     a
ki
   l
     a
ki

=  a

h +

=  a

h +

  uikyui

(cid:88)
u(cid:88)

u:yui>0

  x
ku
  x
ku

,

u=1

33

where we note(cid:80)u

u=1

  x
ku
  x
ku

is the same across all i.

gradient of   s and   vk. we de   ne the following quantity that will be useful in writing the gradient
for both   s and   vk.

(cid:62)   (cid:96)k         u.

dwku =      (wk) +   (  x

ku)       du
ku)     log(  x
t(cid:88)
given this, the gradient of the rate of the gamma process is given by
dwku     t(cid:88)

    c  s     u(cid:88)
u(cid:88)

and the gradient of the sticks is

  s     1

+ wk/   vk

   l
     s

wk
  s

dwku

dwju

u=1

k=1

=

=

  s

,

   l
      vk

1       s
1       vk

u=1

j>k

wj
1       vk

.

positivity constraints are handled by transforming to the inverse softplus (log(1 + exp(x))) space
where the parameters are unconstrained. the gradient in this space follows directly from the
previous equations and the chain rule. we handle all future positivity constraints in a similar
manner. we handle the unit interval constraint on vk with the inverse logistic transformation.

gradient of   (cid:96)k. the gradient of the locations that de   ne the correlations is given by

(cid:33)

.

(cid:32)

u(cid:88)

u=1

   wk +

du

   l
      (cid:96)k

=     1
  2
l

  (cid:96)k +

   l2
      (cid:96)k      (cid:96)k

=     1
  l

id     u(cid:88)

u=1

dud(cid:62)

u

  x
ku
(cid:62)   (cid:96)k +     u)
ku exp(   du
  x
(cid:33)

  x
ku
(cid:62)   (cid:96)k +     u)
ku exp(   du
  x

(cid:32)

we use the negative hessian as the predconditioner matrix. the hessian of the elbo with respect
to locations is

.

(16)

gradient of     . the gradient of the base mass of the gamma process is given by
   l
        

= log(  c)     (t + 1)  (    ) + t   (         1) + log(  s) +

log(1       vk) + (a       1) log(    )     b  ,

t(cid:88)

k=1

where a   and b   are respectively the shape and rate of the hyperprior. we set a   to 1 and b   to
0.01.

gradient of   c. the gradient for the point estimate of the gamma process rate is given by

   l
     c

=

    
c

      s + (ac     1) log(  c)     bc

where ac and bc are respectively the shape and rate of the hyperprior. we set both parameters to
the same values as the prior on     .

34

coordinate updates. to    nd the optimal per patient variational parameters, we iterate between
coordinate updates.

coordinate update of   yui. the variational distribution on vector of auxiliary variables   yui is
the multinomial distribution. the vector   ui is the vector of probabilities to this multinomial
distribution. the optimal variational parameters given the rest of the model is given by

uik     exp(  (  x
     

ku)     log(  x

ku) +   (  a

ki)     log(  a

ki)).

we again note that we only introduce auxiliary variables for yui that are nonzero.

coordinate update of xku. we let the variational distribution over xku be the gamma distribu-
tion. the coordinate updates for the shape of this variational family is

(cid:88)

   

  x
ku

= wk +

xui  uik,

and the rate is

i:xui>0

   

  x
ku

= exp(   du

(cid:62)   (cid:96)k +     u) +

i(cid:88)

i=1

  a
ki
  a
ki

.

we note that the sum over conditions can be computed once and shared across all patients.

coordinate update of du and   u. there is no simple closed form solution for the coordinate
update of   du. instead, we use gradient ascent. the gradient of the elbo with respect to   du is
given by

   l
      du

=       du +

  (cid:96)k

   wk +

  x
ku
(cid:62)   (cid:96)k +     u)
ku exp(   du
  x

(cid:33)

.

and the gradient for the shared gaussian process mean is

   l
        u

=         u
  2
m

+

   wk +

  x
ku
(cid:62)   (cid:96)k +     u)
ku exp(   du
  x

.

(cid:32)

t(cid:88)

k=1

t(cid:88)

k=1

we terminate the procedure when the change in   du between steps falls below a threshold or a
maximum number of iterations is reached.

variational id136. algorithm 1 presents a variational id136 algorithm using the gra-
dients and coordinate maximization procedures derived in the previous section. for the global
gradients without preconditioners, we use rmsprop (tieleman and hinton, 2012). rmsprop is

35

algorithm 1: variational id136 for cnpf

input: data x.
initialize g randomly, t = 1.
repeat

for in parallel u = 1 to u do

optimize   u given g.

end for
follow preconditioned update for g.

until validation perplexity stops improving.

a per-component learning rate, which can be viewed as multiplication by a diagonal matrix. for-
mally, if gt is the gradient at iteration t,    is a number in the unit interval, and   t is a scalar, then
the rms preconditioner   t can be computed as
t = (1        )g2
  t(cid:113)
  g2
  t =

t   1 +   diag(gtg(cid:62)
t )

.

  g2

t

intuitively, rmsprop accounts for length scales and in the noisy setting takes smaller steps along
noisier coordinates.

as the optimization problem for each   u is independent given the global parameters g, we can
parallelize this step. in all of our experiments we parallelize the maximization step across forty
cores. we assess convergence using predictive perplexity on a held out collection of patients.

stochastic variational id136. the variational id136 algorithm presented in algorithm 1
computes the optimal local variational parameters for each patient before updating the variational
parameters for the random variables shared across patients at each iteration. as the number of
patients grows large, this computational cost of this becomes prohibitive. to remedy this malady,
we turn to stochastic variational id136 (hoffman et al., 2013).

stochastic variational id136 works by performing stochastic optimization (robbins and
monro, 1951) on the variational objective. stochastic optimization maximizes an objective by
following a noisy gradient which is unbiased (in expectation is the true gradient).

in stochastic variational id136, the noise stems from subsampling datapoints. this leads to
quicker updates as the noisy gradients are based on a fraction of the entire objective. consider a

36

algorithm 2: stochastic variational id136 for cnpf

input: data x
initialize g randomly, t = 1.
repeat

draw d     unif(1, u ).
optimize   d given g.
follow preconditioned update for g with stochastic gradients.

until validation perplexity stops improving.

patient u, then de   ne the following objective

lu(g) =eq[log p(  )] + eq[log p(c)] + eq[log p(s|  , c)] +

t(cid:88)

k=1

eq[log p(vk|  )]

+ eq[log p((cid:96)k)] +

eq[log p(aki)     log q(aki)]

t(cid:88)

i(cid:88)

k=1

i=1

t(cid:88)

+ u (eq[log p(du)] + eq[log p(  u)] +

eq[log p(xku|du, vk, s, (cid:96)k)     log q(xku)]

eq[log p(yuik|xku, aki)     log q(yuik)]).

k=1

(17)

i(cid:88)

+

i=1

if we let u     unif(1, u ), then ed[lu] = l. thus, gradient of lu where u is uniformly drawn
from 1 to u is an unbiased gradient. the gradient of lu is computed by    nding the local optimal
parameter for the patient u and scaling it according to the total number of patients. this objective
and noisy gradient generalizes in a straightforward manner to drawing small batches of patients.

computationally, stochastic variational id136 provides an advantage over algorithm 1, as
the slow part of algorithm 1 for large datasets is computing the optimal local parameters for every
single datum. algorithm 2 summarizes stochastic variational id136 for the cnpf model.

a.5. stochastic variational id136 for baselines
both the hgp and the uncorrelated models are restrictions of cnpf. the hgp is a restriction of
cnpf when the locations l and scalings   u are set to zero, while the uncorrelated model only
restricts the locations to be zero. this means the only change required in id136 is to    x the
respective parameters to zero depending on whether inferring the hgp or the scaled hgp.

a.6. stochastic variational id136 for softplus cnpf
the transformation distribution for softplus cnpf is

(cid:18)

xui     gamma

(cid:19)

.

wi,

1

log(1 + exp{f ((cid:96)i)})

37

this means the variational updates for w, (cid:96), x, d, and m will be different. de   ne guk to be   du
    u.

(cid:62)   (cid:96)k +

dwku =      (wk) +   (  x

softplus cnpf dw. the gradient of the weights is
ku)     log(  x
ku)     log(log(1 + exp(guk))).
(cid:18)

gradient of   (cid:96)k. recall    is the logistic function. the gradient of the locations is given by

(cid:19)

  (guk)

   wk +

  x
ku

log(1 + exp(guk))

  x
ku log(1 + exp(guk))

.

   l
      (cid:96)k

=     1
  2
l

  (cid:96)k +

u(cid:88)

u=1

du

(cid:88)

(cid:18)

(cid:18)

coordinate update of xku. the coordinate updates for the shape of xku is

   

  x
ku

= wk +

xui  uik,

and the rate is

i:xui>0

   

  x
ku

= log(1 + exp(guk)) +

i(cid:88)

i=1

  a
ki
  a
ki

.

coordinate update of du and   u. the gradient of the elbo with respect to   du is given by

t(cid:88)

k=1

t(cid:88)

   l
      du

=       du +

  (cid:96)k

  (guk)

log(1 + exp(guk))

   wk +

  x
ku

  x
ku log(1 + exp(guk))

and the gradient for the shared gaussian process mean is

   l
        u

=         u
  2
m

+

  (guk)

   wk +

log(1 + exp(guk))

k=1

(cid:19)

  x
ku

  x
ku log(1 + exp(guk))

(cid:19)

,

.

we terminate the procedure when the change in   du between steps falls below a threshold.

38

