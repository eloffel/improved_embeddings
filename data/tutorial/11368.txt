dependency language models for transition-based id33

juntao yu

university of birmingham

birmingham, uk

bernd bohnet

google

london, uk

j.yu.1@cs.bham.ac.uk

bohnetbd@google.com

7
1
0
2

 

g
u
a
0
3

 

 
 
]
l
c
.
s
c
[
 
 

2
v
2
8
9
4
0

.

7
0
6
1
:
v
i
x
r
a

abstract

in this paper, we present an approach
to improve the accuracy of a strong
transition-based dependency parser by ex-
ploiting dependency language models that
are extracted from a large parsed corpus.
we integrated a small number of features
based on the dependency language mod-
els into the parser. to demonstrate the
effectiveness of the proposed approach,
we evaluate our parser on standard en-
glish and chinese data where the base
parser could achieve competitive accuracy
scores. our enhanced parser achieved
state-of-the-art accuracy on chinese data
and competitive results on english data.
we gained a large absolute improvement
of one point (uas) on chinese and 0.5
points for english.

1 introduction

in recent years, using unlabeled data to improve
natural
language parsing has seen a surge of
interest as the data can easy and inexpensively
(sarkar, 2001; steedman et al.,
be obtained, cf.
2003; mcclosky et al., 2006; koo et al., 2008;
s  gaard and rish  j, 2010; petrov and mcdonald,
2012; chen et al., 2013; weiss et al., 2015). this
is in stark contrast to the high costs of manually
labeling new data. some of the techniques such
as
and
co-training (sarkar, 2001) use auto-parsed data
as additional
this enables the
parser to learn from its own or other parser   s
annotations.
techniques include word
id91 (koo et al., 2008) and id27
(bengio et al., 2003) which are generated from a
large amount of unannotated data. the outputs
can be used as features or inputs for parsers.

self-training (mcclosky et al., 2006)

training data.

other

both groups of techniques have been shown
effective on syntactic parsing tasks (zhou and li,
2005; reichart and rappoport,
2007; sagae,
2010; s  gaard and rish  j, 2010; yu et al., 2015;
weiss et al., 2015). however, most word cluster-
ing and the id27 approaches do not
consider the syntactic structures and most self-
/co-training approaches can use only a relatively
small additional training data as training parsers
on a large corpus might be time-consuming
or even intractable on a corpus of millions of
sentences.

language

dependency

(dlm)
models
(shen et al., 2008) are variants of
language
models based on dependency structures. an
id165 dlm is able to predict the next child
when given n-1 immediate previous children and
their head. chen et al. (2012) integrated    rst a
high-order dlm into a second-order graph-based
parser. the dlm allows the parser to explore
high-order features but not increasing the time
complexity.
following chen et al. (2012), we
adapted the dlm to transition-based depen-
dency parsing. our approach is different from
chen et al. (2012)   s in a number of important
aspects:

1. we applied the dlm to a strong parser that

on its own has a competitive performance.

2. we revised their feature templates to inte-
grate the dlms with a transition-based sys-
tem and labeled parsing.

3. we used dlms in joint tagging and parsing,

and gained up to 0.4% on tagging accuracy.

4. our approach could use not only single dlm

but also multiple dlms during parsing.

5. we evaluated additionally with dlms ex-
tracted from higher quality parsed data which

two parsers assigned the same annotations.

overall, our approach improved upon a compet-
itive baseline by 0.51% for english and achieved
state-of-the-art accuracy for chinese.

2 related work

previous studies using unlabeled text could be
classi   ed into two groups by how unlabeled data
is used for training.

training data.

the    rst group uses unlabeled data (usually
parsed data) directly in the training process as
additional
the most common
approaches in this group are self-/co-training.
mcclosky et al. (2006) applied    rst self-training
to a constituency parser. this was later adapted to
id33 by kawahara and uchimoto
(2008) and yu et al. (2015). compared to the
self-training approach used by mcclosky et al.
(2006), both self-training approaches for depen-
dency parsing need an additional selection step
to predict high-quality parsed sentences for re-
training. the basic idea behind this is similar
to sagae and tsujii (2007)   s co-training approach.
instead of using a separately trained classi   er
(kawahara and uchimoto, 2008) or con   dence-
based methods (yu et al., 2015), sagae and tsujii
(2007) used two different parsers to obtain the
additional training data. sagae and tsujii (2007)
shows that when two parsers assign the same syn-
tactic analysis to sentences then the parse trees
have usually a higher parsing accuracy.
tri-
training (zhou and li, 2005; s  gaard and rish  j,
2010) is a variant of co-training which involves a
third parser. the base parser is retrained on addi-
tional parse trees that the other two parsers agreed
on.

the second group uses the unlabeled data indi-
rectly. koo et al. (2008) used word clusters built
from unlabeled data to train a parser. chen et al.
(2008) used features extracted from short dis-
tance relations of a parsed corpus to improve a
id33 model. suzuki et al. (2009)
used features of generative models estimated from
large unlabelled data to improve a second order
dependency parser. their enhanced models im-
proved upon the second order baseline models by
0.65% and 0.15% for english and czech respec-
tively. mirroshandel et al. (2012) used the rela-
tive frequencies of nine manually selected head-
dependent patterns calculated from parsed french

corpora to rescore the n-best parses. their ap-
proach gained a labeled improvement of 0.8%
over the baseline. chen et al. (2013) combined
meta features based on frequencies with the ba-
sic    rst-/second-order features. the meta features
are extracted from parsed annotations by counting
the frequencies of basic feature representations in
a large corpus. with the help of meta features,
the parser achieved the state-of-the-art accuracy
on chinese. kiperwasser and goldberg (2015)
added features based on the statistics learned from
unlabeled data to a weak    rst-order parser and
they achieved 0.7% improvement on the english
data. id27s that represent words
as high dimensional vectors are mostly used in
neural network parsers (chen and manning, 2014;
weiss et al., 2015) and play an important role in
those parsers. the approach most close to ours is
reported by chen et al. (2012) who applied a high-
order dlm to a second-order graph-based parser
for unlabeled parsing. their dlms are extracted
from an english corpus that contains 43 million
words (charniak, 2000) and a 311 million word
corpus of chinese (huang et al., 2009) parsed by
a parser. from a relatively weak baseline, addi-
tional dlm-based features gained 0.6% uas for
english and an impressive 2.9% for chinese.

3 our approach

dependency language models were introduced by
shen et al. (2008) to capture long distance rela-
tions in syntactic structures. an id165 dlm
predicts the next child based on n-1 immediate
previous children and their head. we integrate
dlms extracted from a large parsed corpus into
the mate parser (bohnet et al., 2013). we    rst
extract dlms from a corpus parsed by the base
model. we then retrain the parser with additional
dlm-based features.

further, we experimented with techniques to
improve the quality of the syntactic annotations
which we use to build the dlms. we parse the
sentences with two different parsers and then se-
lect the annotations which both parsers agree on.
the method is similar to co-training except that we
do not train the parser directly on these sentences.
we build the dlms with the method of
chen et al. (2012). for each child xch, we gain
the id203 distribution pu(xch|h), where h
refers n     1 immediate previous children and
their head xh. the previous children for xch

< n odlm ,   (pu(s0)),   (pu(s1)), label >
< n odlm ,   (pu(s0)),   (pu(s1)), label, s0 pos >
< n odlm ,   (pu(s0)),   (pu(s1)), label, s0 word >
< n odlm ,   (pu(s0)),   (pu(s1)), label, s1 pos >
< n odlm ,   (pu(s0)),   (pu(s1)), label, s1 word >
< n odlm ,   (pu(s0)),   (pu(s1)), label, s0 pos, s1 pos >
< n odlm ,   (pu(s0)),   (pu(s1)), label, s0 word, s1 word >

table 1: feature templates which we use in the
parser.

are those who share the same head with xch
but closer to the head word according to the
word sequence in the sentence. let   s consider
the left side child xlk in the dependency rela-
tions (xlk...xl1, xh, xr1...xrm) as an example,
the n-1 immediate previous children for xlk are
xlk   1..xlk   n +1.
in our approach, we estimate
pu(xch|h) by the relative frequency:

pu(xch|h) =

count(xch, h)

px   

ch

count(x   

ch, h)

(1)

by their probabilities, the id165s are sorted in
a descending order. we then used the thresholds
of chen et al. (2012) to replace the probabilities
with one of the three classes (p h, p m, p l) ac-
cording to their position in the sorted list, i.e. the
id165s whose id203 has a rank in the    rst
10% receives the tag p h, p m refers probabilities
ranked between 10% and 30%, probabilities that
ranked below 30% are replaced with p l. during
parsing, we use an additional class p o for rela-
tions not presented in the dlm. in the preliminary
experiments, the p h class is mainly    lled by un-
usual relations that only appeared a few times in
the parsed text. to avoid this, we con   gured the
dlms to only use elements which have a mini-
mum frequency of three, i.e. count(xch, h)    
3. table 1 shows our feature templates, where
n odlm is an index which allows dlms distin-
guish from each other, s0, s1 are the top and the
second top of the stack,   (pu(s0/s1)) refers the
coarse label of probabilities pu(xs0/s1|h) (one of
the p h, p m, p l, p o), s0/s1 pos, s0/s1 word
refer to the part-of-speech tag, word form of
s0/s1, and label is the dependency label between
the s0 and the s1.

4 experimental set-up

ptb
ctb5

train
2-21
001-815,
1001-1136

dev
22
886-931,
1148-1151

test
23
816-885,
1137-1147

table 2: our data splits for english and chinese

used stanford parser 1 v3.3.0 to convert the con-
stituency trees into stanford style dependencies
(de marneffe et al., 2006). for chinese, we fol-
low the splits of zhang and nivre (2011), the con-
stituency trees are converted to dependency re-
lations by penn2malt2 tool using head rules of
zhang and clark (2008). table 2 shows the splits
of our data. we used gold segmentation for chi-
nese tests to make our work comparable with pre-
vious work. we used predicted part-of-speech tags
for both languages in all evaluations. tags are as-
signed by base parser   s internal joint tagger trained
on the training set. we report labeled (las) and
unlabeled (uas) attachment scores, punctuation
marks are excluded from the evaluation.

for the english unlabeled data, we used the
data of chelba et al. (2013) which contains around
30 million sentences (800 million words) from
the news domain. for chinese, we used xin-
hua portion of chinese gigaword 3 version 5.0
(ldc2011t13). the chinese unlabeled data we
used consists of 20 million sentences which is
roughly 450 million words after being segmented
by zpar4 v0.7.5. the word segmentor is trained
on the ctb5 training set. in most of our exper-
iments, the dlms are extracted from data anno-
tated by our base parser. for the evaluation on
higher quality dlms, the unlabeled data is ad-
ditionally tagged and parsed by berkeley parser
(petrov and klein, 2007) and is converted to de-
pendency trees with the same tools as for gold
data.

we used mate transition-based parser with its

default setting and a beam of 40 as our baseline.

5 results and discussion

combining different id165 dlms. we    rst
evaluated the effects of adding different number
of dlms. let m be the dlms we used in the ex-
periments, e.g. m=1-3 refers all three (unigram,

for our experiments, we used the penn en-
glish treebank (ptb) (marcus et al., 1993) and
chinese treebank 5 (ctb5) (xue et al., 2005).
for english, we follow the standard splits and

1http://nlp.stanford.edu/software/lex-parser.shtml
2http://stp.ling   l.uu.se/ nivre/research/penn2malt.html
3we excluded the sentences of ctb5 from chinese giga-

word corpus.

4https://github.com/frcchang/zpar

bigram and trigram) dlms are used. we evaluate
with both single and multiple dlms that extracted
from 5 million sentences for both languages. we
started from only using unigram dlm (m=1) and
then increasing the m until the accuracy drops. ta-
ble 3 shows the results with different dlm set-
tings. the unigram dlm is most effective for
english, which improves above the baseline by
0.38%. for chinese, our approach gained a large
improvement of 1.16% with an m of 1-3. thus,
we use m=1 for english and m=1-3 for chinese
in the rest of our experiments.

exploring dlms built from corpora of dif-
ferent size and quality. to evaluate the in   u-
ence of the size and quality of the input corpus for
building the dlms, we experiment with corpora
of different size and quality.

we    rst evaluate with dlms extracted from the
different number of single-parsed sentences. we
extracted dlms start from a 5 million sentences
corpus and increase the size of the corpus in step
until all of the auto-parsed sentences are used. ta-
ble 4 shows our results on english and chinese de-
velopment sets. for english, the highest accuracy
is still achieved by dlm extracted from 5 million
sentences. while for chinese, we gain the largest
improvement of 1.2% with dlms extracted from
10 million sentences.

we further evaluate the in   uence of dlms ex-
tracted from higher quality data.
the higher
quality corpora are prepared by parsing unlabeled
sentences with the mate parser and the berke-
ley parser and adding the sentences to the corpus
where both parsers agree. for chinese, only 1
million sentences that consist of 5 tokens in av-
erage had the same syntactic structures assigned
by the two parsers. unfortunately, this amount is
not suf   cient for the experiments as their average
sentence length is in stark contrast with the train-
ing data (27.1 tokens). for english, we obtained 7
million sentences with an average sentence length
of 16.9 tokens.

to get a    rst impression of the quality, we
parsed the development set with the two parsers.
when the parsers agree, the parse trees have an
accuracy of 97% las, while the labeled scores
of both parsers are around 91%. this indicates
that parse trees where both parsers return the same
tree have a higher accuracy. the dlm extracted
from 7 million higher quality english sentences
achieved a higher accuracy of 91.56% which out-

m
english
chinese

0
91.05
78.95

1
91.43
79.85

2
91.14
79.42

3
91.22
79.06

1-2
91.27
79.97

1-3
1-4
91.26 n/a
80.11

79.73

table 3: effects (las) of different number of
dlms for english and chinese. m = 0 refers the
baseline.

size
english
chinese

0
91.05
78.95

5
91.43
80.11

10
91.38
80.15

30
91.28

20
91.13
79.72 n/a

table 4: effects (las) of dlms extracted from
different size (in million sentences) of corpus.
size = 0 refers the baseline.

perform the baseline by 0.51%.

main results on test sets. we applied the
best settings tuned on the development sets to the
test sets. the best setting for english is the un-
igram dlm derived from the double parsed sen-
tences. table 5 presents our results and top per-
forming dependency parsers which were evaluated
on the same english data set. our approach with
40 beams surpasses our baseline by 0.46/0.51%
(las/uas) 5 and is only lower than the few
best neural network systems. when we enlarge
the beam, our enhanced models achieved similar
improvements. our semi-supervised result with
150 beams are more competitive when compared
with the state-of-the-art. we cannot directly com-
pare our results with that of chen et al. (2012) as
they evaluated on an old yamada and matsumoto
(2003) format.
in order to have an idea of the
accuracy difference between our baseline and the
second-order graph-based parser they used, we
include our baseline on yamada and matsumoto
(2003) conversion. as shown in table 5 our base-
line is 0.62% higher than their semi-supervised re-
sult and this is 1.28% higher than their baseline.
this con   rms our claim that our baseline is much
stronger.

for chinese, we extracting the dlms from 10
million sentences parsed by the mate parser and
using the unigram, bigram and the trigram dlms
together. table 6 shows the results of our approach
and a number of best chinese parsers. our system
gained a large improvement of 0.93/0.98% 6 for la-
beled and unlabeled attachment scores when using
a beam of 40. when larger beams are used our ap-
proach achieved even larger improvement of more

5signi   cant in dan bikel   s test (p < 10   3).
6signi   cant in dan bikel   s test (p < 10   5).

beam pos
system
97.44
32
zhang and nivre (2011)
80
97.44
bohnet and kuhn (2012)
97.44
martins et al. (2013)
n/a
97.44
zhang and mcdonald (2014) n/a
chen and manning (2014)   
n/a
dyer et al. (2015)   
97.30
weiss et al. (2015)   
97.44
andor et al. (2016)   
97.44
dozat and manning (2017)   
n/a
liu and zhang (2017)   
n/a
n/a
chen et al. (2012) baseline *
chen et al. (2012) dlm *
n/a
97.33
our baseline *
97.36
our baseline
97.34
97.34
97.38
97.39
97.42

1
1
8
32
n/a
n/a
8
8
40
40
80
150
40
80
150

our dlm

las uas
93.00
90.95
91.19
93.27
92.89
90.55
93.22
91.02
91.80
89.60
93.10
90.90
92.05
93.99
94.61
92.79
95.74
94.08
96.20
95.20
92.10
n/a
n/a
92.76
93.38
92.44
93.08
90.95
93.28
91.05
93.29
91.05
93.59
91.41
91.47
93.65
93.74
91.56

table 5: comparing with top performing parsers
on english.
(* means results that are evaluated
on yamada and matsumoto (2003) conversion.    
means neural network-based parsers)

system
hatori et al. (2011)
li et al. (2012)
chen et al. (2013)
chen et al. (2015)
our baseline

our dlm

beam pos
64
n/a
n/a
n/a
40
80
150
40
80
150

las uas
81.33
81.67
83.08
82.94
81.52
81.58
82.11
82.51
82.79
83.28

93.94 n/a
79.01
94.60
n/a
n/a
93.61 n/a
93.99
94.02
93.98
94.27
94.39
94.40

78.49
78.48
78.96
79.42
79.79
80.21

table 6: comparing with top performing parsers
on chinese.

than one percentage point for both labeled and un-
labeled accuracy when compared to the respec-
tive baselines. our scores with the default beam
size (40) are competitive and are 0.2% higher than
the best reported result (chen et al., 2013) when
increasing the beam size to 150. moreover, we
gained improvements up to 0.42% for part-of-
speech tagging on chinese tests.

6 conclusion

in this paper, we applied dependency language
models (dlm) extracted from a large parsed cor-
pus to a strong transition-based parser. we in-
tegrated a small number of dlm-based features
into the parser. we demonstrate the effectiveness
of our dlm-based approach by applying our ap-
proach to english and chinese. we achieved sta-
tistically signi   cant improvements on labeled and
unlabeled scores of both languages. our parsing

system improved by dlms outperforms most of
the systems on english and is competitive. for
chinese, we gained a large improvement of one
point and our accuracy is 0.2% higher than the best
reported result. in addition to that, our approach
gained an improvement of 0.4% on chinese part-
of-speech tagging.

references

daniel andor, chris alberti, david weiss, aliaksei
severyn, alessandro presta, kuzman ganchev, slav
petrov, and michael collins. 2016. globally nor-
malized transition-based neural networks.
in pro-
ceedings of the 54th annual meeting of the asso-
ciation for computational linguistics. pages 2442   
2452.

yoshua

bengio,

r  ejean

ducharme,
janvin.

pas-
2003.

and christian

cal vincent,
a neural probabilistic language model.
j.
http://dl.acm.org/citation.cfm?id=944919.944966.

learn.

mach.

res.

3:1137   1155.

bernd bohnet and jonas kuhn. 2012. the best of
both worlds     a graph-based completion model for
transition-based parsers. in proceedings of the 13th
conference of the european chpater of the associ-
ation for computational linguistics (eacl). pages
77   87.

bernd bohnet,

joakim nivre,

igor boguslavsky,
rich  ard farkas filip ginter, and jan hajic. 2013.
joint morphological and syntactic analysis for richly
in   ected languages. transactions of the associta-
tion for computational linguistics 1.

eugene charniak. 2000. a maximum-id178-inspired
parser.
in proceedings of the first meeting of
the north american chapter of the association for
computational linguistics (naacl). pages 132   
139.

ciprian chelba, tomas mikolov, mike schuster, qi ge,
thorsten brants, and philipp koehn. 2013. one
billion word benchmark for measuring progress in
statistical id38. computing research
repository (corr) abs/1312.3005:1   6.

danqi chen and christopher d manning. 2014. a fast
and accurate dependency parser using neural net-
works. in empirical methods in natural language
processing (emnlp).

wenliang chen, youzheng wu, and hitoshi isahara.
2008. learning reliable information for depen-
dency parsing adaptation.
in proceedings of the
22nd international conference on computational
linguistics-volume 1. association for computa-
tional linguistics, pages 113   120.

wenliang chen, min zhang, and haizhou li. 2012.
utilizing dependency language models for graph-
based id33 models. in proceedings

of the 50th annual meeting of the association for
computational linguistics: long papers-volume 1.
association for computational linguistics, pages
213   222.

wenliang chen, min zhang, and yue zhang. 2013.

terry koo, xavier carreras, and michael collins. 2008.
in
simple semi-supervised id33.
proceedings of the 46th annual meeting of the asso-
ciation for computational linguistics (acl). pages
595   603.

semi-supervised feature transformation for id33.
in proceedings of the 2013 conference on empir-
iang
ical methods in natural language processing.
a separately passive-aggressive training algorithm for joint id52 and id33.
association for computational linguistics, pages
in
1303   1313. http://aclweb.org/anthology/d13-1129.
coling
tee, mumbai,
http://www.aclweb.org/anthology/c12-1103.

coling 2012.
organizing
pages

the
commit-
1681   1698.

wenliang chen, min zhang, and yue zhang. 2015.

wanx-
2012.

proceedings

zhenghua

zhang,

india,

2012

che,

ting

min

liu.

and

li,

of

distributed feature representations for id33.
ieee/acm
and
https://doi.org/10.1109/taslp.2014.2365359.

speech
23(3):451   460.

audio,

trans.

lang.

proc.

marie-catherine de marneffe, bill maccartney, and
christopher d. manning. 2006. generating typed
dependency parses from phrase structure parses. in
proceedings of the 5th international conference on
language resources and evaluation (lrec).

timothy dozat and christopher manning. 2017.

deep biaf   ne attention for neural id33.
in
conference
https://openreview.net/pdf?id=hk95pk9le.

international
representations.

proceedings
on

learning

5th

the

of

chris dyer, miguel ballesteros, wang ling,
austin matthews, and noah a. smith. 2015.
transition-based id33 with stack long short-term memory.
in proceedings of
the 53rd annual meeting
of
the association for computational linguis-
tics and the 7th international joint conference
on natural language processing (volume 1:
long papers). association for computational
linguistics, beijing, china,
pages 334   343.
http://www.aclweb.org/anthology/p15-1033.

seyed

and

jun

hatori,

takuya matsuzaki,

yusuke
2011.

and

tsujii.

jun   ichi

miyao,
incremental joint id52 and id33 in chinese.
in proceedings of 5th international joint confer-
ence on natural language processing. asian
federation
process-
ing, chiang mai, thailand, pages 1216   1224.
http://www.aclweb.org/anthology/i11-1136.

of natural

language

liang huang, wenbin jiang, and qun liu. 2009.
bilingually-constrained (monolingual) shift-reduce
parsing. in proceedings of the conference on em-
pirical methods in natural language processing
(emnlp). pages 1222   1231.

daisuke kawahara and kiyotaka uchimoto. 2008.
learning reliability of parses for id20
of id33. in ijcnlp. volume 8.

eliyahu kiperwasser and yoav goldberg. 2015.

j. liu and y. zhang. 2017. in-order transition-based

constituent parsing. arxiv e-prints .

mitchell p. marcus, beatrice santorini, and mary ann
marcinkiewicz. 1993. building a large annotated
corpus of english: the id32. computa-
tional linguistics 19:313   330.

a. martins, m. almeida, and n. a. smith. 2013.    turn-
ing on the turbo: fast third-order non-projective
turbo parsers   . in annual meeting of the associa-
tion for computational linguistics - acl. volume -,
pages 617     622.

david mcclosky, eugene charniak, and mark john-
son. 2006. effective self-training for parsing.
in
proceedings of the human language technology
conference of the naacl, main conference. pages
152   159.

alexis
2012.

abolghasem

mirroshandel,

le

roux.

joseph

the association

nasr,
semi-supervised id33 using lexical af   nities.
in proceedings of
the 50th annual meeting
for computational lin-
of
long papers - volume 1. associa-
guistics:
strouds-
tion for computational linguistics,
burg, pa, usa, acl    12,
pages 777   785.
http://dl.acm.org/citation.cfm?id=2390524.2390634.

slav petrov and dan klein. 2007. improved id136
for unlexicalized parsing.
in proceedings of hu-
man language technologies: the annual confer-
ence of the north american chapter of the associ-
ation for computational linguistics (naacl hlt).
pages 404   411.

slav petrov and ryan mcdonald. 2012. overview of
the 2012 shared task on parsing the web. in notes
of the first workshop on syntactic analysis of non-
canonical language (sancl).

roi reichart and ari rappoport. 2007. self-training
for enhancement and id20 of statistical
parsers trained on small datasets. in acl. volume 7,
pages 616   623.

semi-supervised id33 using bilexical contextual features from auto-parsed data.
in proceedings of
the 2015 conference on
empirical methods in natural language pro-
cessing. association
for computational lin-
guistics, lisbon, portugal, pages 1348   1353.
http://aclweb.org/anthology/d15-1158.

kenji sagae. 2010. self-training without reranking for
parser id20 and its impact on semantic
role labeling. in proceedings of the 2010 workshop
on id20 for natural language pro-
cessing. association for computational linguistics,
pages 37   44.

kenji sagae and jun   ichi tsujii. 2007. dependency
parsing and id20 with lr models and
parser ensembles.
in proceedings of the conll
shared task of emnlp-conll 2007. pages 1044   
1050.

anoop sarkar. 2001. applying co-training methods
to statistical parsing. in proceedings of the second
meeting of the north american chapter of the as-
sociation for computational linguistics (naacl).
pages 175   182.

association for computational linguistics (volume
2: short papers). association for computational
linguistics, baltimore, maryland, pages 656   661.
http://www.aclweb.org/anthology/p/p14/p14-2107.

yue zhang and stephen clark. 2008. a tale of two
parsers: investigating and combining graph-based
and transition-based id33.
in pro-
ceedings of the conference on empirical methods
in natural language processing (emnlp). pages
562   571.

libin shen, jinxi xu, and ralph weischedel. 2008.
a new string-to-dependency machine translation al-
gorithm with a target dependency language model.
acl-08: hlt page 577.

yue zhang and joakim nivre. 2011. transition-based
parsing with rich non-local features. in proceedings
of the 49th annual meeting of the association for
computational linguistics (acl).

anders s  gaard

and christian rish  j.

2010.

semi-supervised id33 using generalized tri-training.
in proceedings of the 23rd international confer-
ence on computational linguistics. association
stroudsburg,
for computational linguistics,
pa, usa, coling    10,
pages 1065   1073.
http://dl.acm.org/citation.cfm?id=1873781.1873901.

zhi-hua zhou and ming li. 2005.

tri-training:
exploiting unlabeled data using three classi   ers.
knowledge and data engineering, ieee transac-
tions on 17(11):1529   1541.

mark steedman, rebecca hwa, miles osborne, and
anoop sarkar. 2003. corrected co-training for sta-
tistical parsers. in proceedings of the international
conference on machine learning (icml). pages
95   102.

jun

suzuki,

hideki

isozaki,

xavier car-
2009.

and

collins.

michael

reras,
an empirical study of semi-supervised structured conditional models for id33.
in proceedings of
the 2009 conference on
language
empirical methods
in
computational
processing. association
linguistics,
551   560.
singapore,
http://www.aclweb.org/anthology/d/d09/d09-1058.

natural
for

pages

david weiss, chris alberti, michael collins, and slav
petrov. 2015. structured training for neural network
transition-based parsing.
in proceedings of acl
2015. pages 323   333.

naiwen xue, fei xia, fu-dong chiou, and martha
palmer. 2005. the penn chinese treebank: phase
structure annotation of a large corpus. journal of
natural language engineering 11:207   238.

hiroyasu yamada and yuji matsumoto. 2003. statis-
tical dependency analysis with support vector ma-
chines.
in proceedings of the 8th international
workshop on parsing technologies (iwpt). pages
195   206.

juntao yu, mohab elkaref, and bernd bohnet. 2015.

id20 for id33 via self-training.
in proceedings of the 14th international conference
on parsing technologies. association for compu-
tational linguistics, bilbao, spain, pages 1   10.
http://www.aclweb.org/anthology/w15-2201.

hao

zhang

and

ryan mcdonald.

2014.

enforcing structural diversity in cube-pruned id33.
in proceedings of the 52nd annual meeting of the

