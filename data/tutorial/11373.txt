a thorough examination of the

id98/daily mail reading comprehension task

danqi chen and jason bolton and christopher d. manning

computer science stanford university

{danqi,jebolton,manning}@cs.stanford.edu

stanford, ca 94305-9020, usa

6
1
0
2

 

g
u
a
8

 

 
 
]
l
c
.
s
c
[
 
 

2
v
8
5
8
2
0

.

6
0
6
1
:
v
i
x
r
a

abstract

enabling a computer to understand a docu-
ment so that it can answer comprehension
questions is a central, yet unsolved goal
of nlp. a key factor impeding its solu-
tion by machine learned systems is the lim-
ited availability of human-annotated data.
hermann et al. (2015) seek to solve this
problem by creating over a million training
examples by pairing id98 and daily mail
news articles with their summarized bullet
points, and show that a neural network can
then be trained to give good performance
on this task. in this paper, we conduct a
thorough examination of this new reading
comprehension task. our primary aim is to
understand what depth of language under-
standing is required to do well on this task.
we approach this from one side by doing a
careful hand-analysis of a small subset of
the problems and from the other by show-
ing that simple, carefully designed systems
can obtain accuracies of 73.6% and 76.6%
on these two datasets, exceeding current
state-of-the-art results by 7   10% and ap-
proaching what we believe is the ceiling
for performance on this task.1

1

introduction

reading comprehension (rc) is the ability to read
text, process it, and understand its meaning.2 how
to endow computers with this capacity has been an
elusive challenge and a long-standing goal of arti-
   cial intelligence (e.g., (norvig, 1978)). genuine
reading comprehension involves interpretation of

1our code is available at https://github.com/

danqi/rc-id98-dailymail.

2https://en.wikipedia.org/wiki/

reading_comprehension

the text and making complex id136s. human
reading comprehension is often tested by asking
questions that require interpretive understanding
of a passage, and the same approach has been sug-
gested for testing computers (burges, 2013).

in recent years, there have been several strands
of work which attempt to collect human-labeled
data for this task     in the form of document, ques-
tion and answer triples     and to learn machine
learning models directly from it (richardson et
al., 2013; berant et al., 2014; wang et al., 2015).
however, these datasets consist of only hundreds of
documents, as the labeled examples usually require
considerable expertise and neat design, making
the annotation process quite expensive. the sub-
sequent scarcity of labeled examples prevents us
from training powerful statistical models, such as
deep learning models, and would seem to prevent
a system from learning complex textual reasoning
capacities.

recently, researchers at deepmind (hermann
et al., 2015) had the appealing, original idea of
exploiting the fact that the abundant news articles
of id98 and daily mail are accompanied by bullet
point summaries in order to heuristically create
large-scale supervised training data for the reading
comprehension task. figure 1 gives an example.
their idea is that a bullet point usually summarizes
one or several aspects of the article. if the computer
understands the content of the article, it should be
able to infer the missing entity in the bullet point.
this is a clever way of creating supervised data
cheaply and holds promise for making progress on
training rc models; however, it is unclear what
level of reading comprehension is actually needed
to solve this somewhat arti   cial task and, indeed,
what statistical models that do reasonably well on
this task have actually learned.

in this paper, our aim is to provide an in-depth
and thoughtful analysis of this dataset and what

an example4: it consists of a passage p, a ques-
tion q and an answer a, where the passage is a
news article, the question is a cloze-style task, in
which one of the article   s bullet points has had one
entity replaced by a placeholder, and the answer
is this questioned entity. the goal is to infer the
missing entity (answer a) from all the possible en-
tities which appear in the passage. a news article
is usually associated with a few (e.g., 3   5) bullet
points and each of them highlights one aspect of its
content.

the text has been run through a google nlp
pipeline. it it tokenized, lowercased, and named
entity recognition and coreference resolution have
been run. for each coreference chain containing at
least one named entity, all items in the chain are re-
placed by an @entityn marker, for a distinct index
n. hermann et al. (2015) argue convincingly that
such a strategy is necessary to ensure that systems
approach this task by understanding the passage in
front of them, rather than by using world knowl-
edge or a language model to answer questions with-
out needing to understand the passage. however,
this also gives the task a somewhat arti   cial charac-
ter. on the one hand, systems are greatly helped by
entity recognition and coreference having already
been performed; on the other, they suffer when ei-
ther of these modules fail, as they do (in figure 1,
   the character    should probably be coreferent with
@entity14; clearer examples of failure appear later
on in our data analysis). moreover, this inability
to use world knowledge also makes it much more
dif   cult for a human to do this task     occasionally
it is very dif   cult or impossible for a human to de-
termine the correct answer when presented with an
item anonymized in this way.

the creation of the datasets bene   ts from the
sheer volume of news articles available online, so
they offer a large and realistic testing ground for
statistical models. table 1 provides some statis-
tics on the two datasets: there are 380k and 879k
training examples for id98 and daily mail respec-
tively. the passages are around 30 sentences and
800 tokens on average, while each question con-
tains around 12   14 tokens.

in the following sections, we seek to more deeply
understand the nature of this dataset. we    rst build
some straightforward systems in order to get a bet-
ter idea of a lower-bound for the performance of

4the original

article

can be

found at http:

//www.id98.com/2015/03/10/entertainment/
feat-star-wars-gay-character/.

figure 1: an example item from dataset id98.

level of natural language understanding is needed
to do well on it. we demonstrate that simple, care-
fully designed systems can obtain high, state-of-
the-art accuracies of 73.6% and 76.6% on id98
and daily mail respectively. we do a careful
hand-analysis of a small subset of the problems
to provide data on their dif   culty and what kinds
of language understanding are needed to be suc-
cessful and we try to diagnose what is learned by
the systems that we have built. we conclude that:
(i) this dataset is easier than previously realized,
(ii) straightforward, conventional nlp systems can
do much better on it than previously suggested,
(iii) the distributed representations of deep learn-
ing systems are very effective at recognizing para-
phrases, (iv) partly because of the nature of the
questions, current systems much more have the
nature of single-sentence id36 sys-
tems than larger-discourse-context text understand-
ing systems, (v) the systems that we present here
are close to the ceiling of performance for single-
sentence and unambiguous cases of this dataset,
and (vi) the prospects for getting the    nal 20% of
questions correct appear poor, since most of them
involve issues in the data preparation which under-
mine the chances of answering the question (coref-
erence errors or anonymization of entities making
understanding too dif   cult).

2 the reading comprehension task

the rc datasets introduced in (hermann et al.,
2015) are made from articles on the news websites
id98 and daily mail, utilizing articles and their
bullet point summaries.3 figure 1 demonstrates

3the datasets are available at https://github.com/

deepmind/rc-data.

( @entity4 ) if you feel a ripple in the force today , it may be the news that the official @entity6 is getting its first gay character . according to the sci-fi website @entity9 , the upcoming novel " @entity11 " will feature a capable but flawed @entity13 official named @entity14 who " also happens to be a lesbian . " the character is the first gay figure in the official @entity6 -- the movies , television shows , comics and books approved by @entity6 franchise owner @entity22 -- according to @entity24 , editor of " @entity6 " books at @entity28 imprint @entity26 . passagequestioncharacters in " @placeholder " movies have gradually become more diverseanswer@entity6# train
# dev
# test
passage: avg. tokens
passage: avg. sentences
question: avg. tokens
avg. # entities

id98 daily mail
879,450
64,835
53,182
813.1
28.9
14.3
26.2

380,298
3,924
3,198
761.8
32.3
12.5
26.2

table 1: data statistics of the id98 and daily mail
datasets. the avg. tokens and sentences in the pas-
sage, the avg. tokens in the query, and the number
of entities are based on statistics from the training
set, but they are similar on the development and
test sets.

current nlp systems. then we turn to data analysis
of a sample of the items to examine their nature
and an upper bound on performance.

3 our systems

in this section, we describe two systems we im-
plemented     a conventional entity-centric classi   er
and an end-to-end neural network. while hermann
et al. (2015) do provide several baselines for per-
formance on the rc task, we suspect that their
baselines are not that strong. they attempt to use
a frame-semantic parser, and we feel that the poor
coverage of that parser undermines the results, and
is not representative of what a straightforward nlp
system     based on standard approaches to factoid
id53 and id36 devel-
oped over the last 15 years     can achieve. indeed,
their frame-semantic model is markedly inferior
to another baseline they provide, a heuristic word
distance model. at present just two papers are
available presenting results on this rc task, both
presenting neural network approaches: (hermann
et al., 2015) and (hill et al., 2016). while the latter
is wrapped in the language of end-to-end mem-
ory networks, it actually presents a fairly simple
window-based neural network classi   er running on
the id98 data. its success again raises questions
about the true nature and complexity of the rc
task provided by this dataset, which we seek to
clarify by building a simple attention-based neural
net classi   er.
given the (passage, question, answer) triple
(p, q, a), p = {p1, . . . , pm} and q = {q1, . . . , ql}
are sequences of tokens for the passage and

question sentence, with q containing exactly one
   @placeholder    token. the goal is to infer the cor-
rect entity a     p     e that the placeholder corre-
sponds to, where e is the set of all abstract entity
markers. note that the correct answer entity must
appear in the passage p.

3.1 entity-centric classi   er
we    rst build a conventional feature-based classi-
   er, aiming to explore what features are effective
for this task. this is similar in spirit to (wang et al.,
2015), which at present has very competitive per-
formance on the mctest rc dataset (richardson
et al., 2013). the setup of this system is to design
a feature vector fp,q(e) for each candidate entity e,
and to learn a weight vector    such that the correct
answer a is expected to rank higher than all other
candidate entities:
(cid:124)

fp,q(e),   e     e     p \ {a}
we employ the following feature templates:

fp,q(a) >   

(1)

  

(cid:124)

1. whether entity e occurs in the passage.
2. whether entity e occurs in the question.
3. the frequency of entity e in the passage.
4. the    rst position of occurence of entity e in the

passage.

5. id165 exact match: whether there is an exact
match between the text surrounding the place-
holder and the text surrounding entity e. we
have features for all combinations of matching
left and/or right one or two words.

6. word distance: we align the placeholder with
each occurrence of entity e, and compute the av-
erage minimum distance of each non-stop ques-
tion word from the entity in the passage.

7. sentence co-occurrence: whether entity e co-
occurs with another entity or verb that appears
in the question, in some sentence of the passage.
8. dependency parse match: we dependency parse
both the question and all the sentences in the
passage, and extract an indicator feature of
whether w r       @placeholder and w r       e are
both found; similar features are constructed for
@placeholder r       w and e r       w.
3.2 end-to-end neural network
our neural network system is based on the atten-
tivereader model proposed by (hermann et al.,
2015). the framework can be described in the
following three steps (see figure 2):

figure 2: our neural network architecture for the reading comprehension task.

encoding: first, all the words are mapped to d-
dimensional vectors via an embedding ma-
trix e     rd  |v|;
therefore we have p:
p1, . . . , pm     rd and q : q1, . . . , ql     rd.
next we use a shallow bi-directional recur-
rent neural network (id56) with hidden size   h
to encode contextual embeddings   pi of each
word in the passage,
      
      
h i = id56(
h i   1, pi), i = 1, . . . , m
      
      
h i = id56(
h i+1, pi), i = m, . . . , 1
      
      
h i)     rh, where h =
and   pi = concat(
h i,
2  h. meanwhile, we use another bi-directional
id56 to map the question q1, . . . , ql to an
embedding q     rh. we choose to use gated
recurrent unit (gru) (cho et al., 2014) in
our experiments because it performs similarly
but is computationally cheaper than lstm.
attention: in this step, the goal is to compare the
question embedding and all the contextual em-
beddings, and select the pieces of information
that are relevant to the question. we compute
a id203 distribution    depending on the
degree of relevance between word pi (in its
context) and the question q and then produce
an output vector o which is a weighted com-
bination of all contextual embeddings {  pi}:
(2)
(3)
ws     rh  h is used in a bilinear term, which
allows us to compute a similarity between
q and   pi more    exibly than with just a dot
product.

  i = softmaxi q
o =

(cid:88)

ws   pi

  i   pi

(cid:124)

i

prediction: using the output vector o, the system

outputs the most likely answer using:

a = arg maxa   p   e w

(cid:124)
a o

(4)

finally, the system adds a softmax function
(cid:124)
on top of w
a o and adopts a negative log-
likelihood objective for training.

differences from (hermann et al., 2015). our
model basically follows the attentivereader. how-
ever, to our surprise, our experiments observed
nearly 7    10% improvement over the original atten-
tivereader results on id98 and daily mail datasets
(discussed in sec. 4). concretely, our model has
the following differences:

    we use a bilinear term, instead of a tanh layer
to compute the relevance (attention) between
question and contextual embeddings. the ef-
fectiveness of the simple bilinear attention
function has been shown previously for neural
machine translation by (luong et al., 2015).
    after obtaining the weighted contextual em-
beddings o, we use o for direct prediction. in
contrast, the original model in (hermann et
al., 2015) combined o and the question em-
bedding q via another non-linear layer before
making    nal predictions. we found that we
could remove this layer without harming per-
formance. we believe it is suf   cient for the
model to learn to return the entity to which it
maximally gives attention.

    the original model considers all the words
from the vocabulary v in making predictions.
we think this is unnecessary, and only predict
among entities which appear in the passage.

( @entity4 ) if you feel a ripple in the force today , it may be the news that the official @entity6 is getting its first gay character . according to the sci-fi website @entity9 , the upcoming novel " @entity11 " will feature a capable but flawed @entity13 official named @entity14 who " also happens to be a lesbian . " the character is the first gay figure in the official @entity6 -- the movies , television shows , comics and books approved by @entity6 franchise owner @entity22 -- according to @entity24 , editor of " @entity6 " books at @entity28 imprint @entity26 . passagequestioncharacters in " @placeholder " movies have gradually become more diverseanswer@entity6         characters in " @placeholder " movies have gradually become more diversepassagequestionentity6answerof these changes, only the    rst seems important;
the other two just aim at keeping the model simple.

window-based memn2ns (hill et al., 2016).
another recent neural network approach proposed
by (hill et al., 2016) is based on a memory net-
work architecture (weston et al., 2015). we think
it is highly similar in spirit. the biggest difference
is their way of encoding passages: they demon-
strate that it is most effective to only use a 5-word
context window when evaluating a candidate en-
tity and they use a positional unigram approach to
encode the contextual embeddings: if a window
consists of 5 words x1, . . . , x5, then it is encoded
i=1 ei(xi), resulting in 5 separate embedding
matrices to learn. they encode the 5-word window
surrounding the placeholder in a similar way and
all other words in the question text are ignored. in
addition, they simply use a dot product to compute
the    relevance    between the question and a contex-
tual embedding. this simple model nevertheless
works well, showing the extent to which this rc
task can be done by very local context matching.

as(cid:80)5

4 experiments
4.1 training details
for training our conventional classi   er, we use the
implementation of lambdamart (wu et al., 2010)
in the ranklib package.5 we use this ranking al-
gorithm since our problem is naturally a ranking
problem and forests of boosted id90 have
been very successful lately (as seen, e.g., in many
recent kaggle competitions). we do not use all the
features of lambdamart since we are only scor-
ing 1/0 loss on the    rst ranked proposal, rather than
using an ir-style metric to score ranked results. we
use stanford   s neural network dependency parser
(chen and manning, 2014) to parse all our docu-
ment and question text, and all other features can
be extracted without additional tools.
for training our neural networks, we only keep
the most frequent |v| = 50k words (including
entity and placeholder markers), and map all other
words to an   unk   token. we choose word embed-
ding size d = 100, and use the 100-dimensional
pre-trained glove id27s (pennington
et al., 2014) for initialization. the attention and
output parameters are initialized from a uniform
distribution between (   0.01, 0.01), and the gru
5https://sourceforge.net/p/lemur/wiki/

ranklib/.

weights are initialized from a gaussian distribution
n (0, 0.1).

we use hidden size h = 128 for id98 and 256
for daily mail. optimization is carried out using
vanilla stochastic id119 (sgd), with a
   xed learning rate of 0.1. we sort all the examples
by the length of its passage, and randomly sample
a mini-batch of size 32 for each update. we also
apply dropout with id203 0.2 to the embed-
ding layer and gradient clipping when the norm of
gradients exceeds 10.

additionally, we think the original indices of
entity markers are generated arbitrarily. we attempt
to relabel the entity markers based on their    rst
occurrence in the passage and question 6 and    nd
that this step can make training converge faster as
well bring slight gains. we report both results (with
and without relabeling) for future reference.

all of our models are run on a single gpu
(geforce gtx titan x), with roughly a runtime
of 3 hours per epoch for id98, and 12 hours per
epoch for daily mail. we run all the models up to
30 epochs and select the model that achieves the
best accuracy on the development set.

we run our models 5 times independently with
different random seeds and report average perfor-
mance across the runs. we also report ensemble
results which average the prediction probabilities
of the 5 models.

4.2 main results
table 2 presents our main results. the conven-
tional feature-based classi   er obtains 67.9% ac-
curacy on the id98 test set. not only does this
signi   cantly outperform any of the symbolic ap-
proaches reported in (hermann et al., 2015), it also
outperforms all the neural network systems from
their paper and the best single-system result re-
ported so far from (hill et al., 2016). this suggests
that the task might not be as dif   cult as suggested,
and a simple feature set can cover many of the
cases. table 3 presents a feature ablation analysis
of our entity-centric classi   er on the development
portion of the id98 dataset. it shows that id165
match and frequency of entities are the two most
important classes of features.

more dramatically, our single-model neural net-
work surpasses the previous results by a large mar-
gin (over 5%). the relabeling process further im-

6the    rst occurring entity is relabeled as @entity1, and

the second one is relabeled as @entity2, and so on.

model
frame-semantic model    
word distance model    
deep lstm reader    
attentive reader    
impatient reader    
memnns (window memory)    
memnns (window memory + self-sup.)    
memnns (ensemble)    
ours: classi   er
ours: neural net
ours: neural net (ensemble)
ours: neural net (relabeling)
ours: neural net (relabeling, ensemble)

dev
36.3
50.5
55.0
61.6
61.8
58.0
63.4
66.2   
67.1
72.5
76.2   
73.8
77.2   

id98

daily mail

test
40.2
50.9
57.0
63.0
63.8
60.6
66.8
69.4   
67.9
72.7
76.5   
73.6
77.6   

dev
35.5
56.4
63.3
70.5
69.0
n/a
n/a
n/a
69.1
76.9
79.5   
77.6
80.2   

test
35.5
55.5
62.2
69.0
68.0
n/a
n/a
n/a
68.3
76.0
78.7   
76.6
79.2   

table 2: accuracy of all models on the id98 and daily mail datasets. results marked     are from
(hermann et al., 2015) and results marked     are from (hill et al., 2016). classi   er and neural net denote
our entity-centric classi   er and neural network systems respectively. the numbers marked with     indicate
that the results are from ensemble models.

features
full model
    whether e is in the passage
    whether e is in the question
    frequency of e
    position of e
    id165 match
    word distance
    sentence co-occurrence
    dependency parse match

accuracy

67.1
67.1
67.0
63.7
65.9
60.5
65.4
66.0
65.6

table 3: feature ablation analysis of our entity-
centric classi   er on the development portion of the
id98 dataset. the numbers denote the accuracy
after we exclude each feature from the full system,
so a low number indicates an important feature.

proves the results by 0.6% and 0.9%, pushing up
the state-of-the-art accuracies to 73.6% and 76.6%
on the two datasets respectively. the ensembles of
5 models consistently bring further 2     4% gains.
concurrently with our paper, kadlec et al. (2016)
and kobayashi et al. (2016) also experiment on
these two datasets and report competitive results.
however, our model not only still outperforms
theirs, but also appears to be structurally simpler.
all these recent efforts converge to similar num-

bers, and we believe that they are approaching the
ceiling performance of this task, as we will indicate
in the next section.

5 data analysis
so far, we have good results via either of our sys-
tems. in this section, we aim to conduct an in-
depth analysis and answer the following questions:
(i) since the dataset was created in an automatic
and heuristic way, how many of the questions are
trivial to answer, and how many are noisy and not
answerable? (ii) what have these models learned?
what are the prospects for further improving them?
to study this, we randomly sampled 100 exam-
ples from the dev portion of the id98 dataset for
analysis (see more details in appendix a).

5.1 breakdown of the examples
after carefully analyzing these 100 examples, we
roughly classify them into the following categories
(if an example satis   es more than one category, we
classify it into the earliest one):
exact match the nearest words around the place-
holder are also found in the passage sur-
rounding an entity marker; the answer is self-
evident.

sentence-level id141 the question text

category question
exact
match

it    s clear @entity0 is leaning to-
ward @placeholder , says an ex-
pert who monitors @entity0

para-
phrase

partial
clue

multiple
sent.

coref.
error

hard

@placeholder says he under-
stands why @entity0 wo n   t play
at his tournament
a tv movie based on @entity2    s
book @placeholder casts a @en-
tity76 actor as @entity5
he    s doing a his - and - her duet
all by himself , @entity6 said of
@placeholder

rapper @placeholder     disgusted ,
    cancels upcoming show for @en-
tity280
pilot error and snow were reasons
stated for @placeholder plane
crash

passage
. . . @entity116 , who follows @entity0    s operations
and propaganda closely , recently told @entity3 , it    s
clear @entity0 is leaning toward @entity60 in terms of
doctrine , ideology and an emphasis on holding territory
after operations . . . .
. . . @entity0 called me personally to let me know that
he would n   t be playing here at @entity23 ,     @entity3
said on his @entity21 event    s website . . . .
. . . to @entity12 @entity2 professed that his @entity11
is not a religious book . . . .

. . . we got some groundbreaking performances , here too
, tonight , @entity6 said . we got @entity17 , who will
be doing some musical performances . he    s doing a his
- and - her duet all by himself . . . .
. . . with hip - hop star @entity246 saying on @entity247
that he was canceling an upcoming show for the @en-
tity249 . . . . (but @entity249 = @entity280 = saes)
. . . a small aircraft carrying @entity5 , @entity6 and
@entity7 the @entity12 @entity3 crashed a few miles
from @entity9 , near @entity10 , @entity11 . . . .

table 4: some representative examples from each category.

is entailed/rephrased by exactly one sentence
in the passage, so the answer can de   nitely be
identi   ed from that sentence.

partial clue in many cases, even though we can-
not    nd a complete semantic match between
the question text and some sentence, we are
still able to infer the answer through partial
clues, such as some word/concept overlap.

multiple sentences it requires processing multi-

ple sentences to infer the correct answer.

coreference errors it is unavoidable that there
are many coreference errors in the dataset.
this category includes those examples with
critical coreference errors for the answer en-
tity or key entities appearing in the question.
basically we treat this category as    not an-
swerable   .

ambiguous or very hard this category includes
examples for which we think humans are not
able to obtain the correct answer (con   dently).

no. category
1
2
3
4
5
6

exact match
id141
partial clue
multiple sentences
coreference errors
ambiguous / hard

(%)
13
41
19
2
8
17

table 5: an estimate of the breakdown of the
dataset into classes, based on the analysis of our
sampled 100 examples from the id98 dataset.

table 5 provides our estimate of the percentage
for each category, and table 4 presents one repre-
sentative example from each category. to our sur-
prise,    coreference errors    and    ambiguous/hard   
cases account for 25% of this sample set, based on
our manual analysis, and this certainly will be a
barrier for training models with an accuracy much
above 75% (although, of course, a model can some-
times make a lucky guess). additionally, only 2
examples require multiple sentences for id136    

category
exact match
id141
partial clue
multiple sentences
coreference errors
ambiguous / hard
all

classi   er

13 (100.0%)
32 (78.1%)
14 (73.7%)
1 (50.0%)
4 (50.0%)
2 (11.8%)
66 (66.0%)

neural net
13 (100.0%)
39 (95.1%)
17 (89.5%)
1 (50.0%)
3 (37.5%)
1
(5.9%)
74 (74.0%)

table 6: the per-category performance of our two
systems.

this is a lower rate than we expected and hermann
et al. (2015) suggest. therefore, we hypothesize
that in most of the    answerable    cases, the goal is
to identify the most relevant (single) sentence, and
then to infer the answer based upon it.

5.2 per-category performance
now, we further analyze the predictions of our two
systems, based on the above categorization.

as seen in table 6, we have the following obser-
vations: (i) the exact-match cases are quite sim-
ple and both systems get 100% correct. (ii) for
the ambiguous/hard and entity-linking-error cases,
meeting our expectations, both of the systems per-
form poorly. (iii) the two systems mainly differ in
id141 cases, and some of the    partial clue   
cases. this clearly shows how neural networks are
better capable of learning semantic matches involv-
ing id141 or lexical variation between the
two sentences. (iv) we believe that the neural-net
system already achieves near-optimal performance
on all the single-sentence and unambiguous cases.
there does not seem to be much useful headroom
for exploring more sophisticated natural language
understanding approaches on this dataset.

6 related tasks
we brie   y survey other tasks related to reading
comprehension.

mctest (richardson et al., 2013) is an open-
domain reading comprehension task, in the form
of    ctional short stories, accompanied by multiple-
choice questions. it was carefully created using
crowd sourcing, and aims at a 7-year-old reading
comprehension level.

on the one hand, this dataset has a high de-
mand on various reasoning capacities: over 50% of
the questions require multiple sentences to answer

and also the questions come in assorted categories
(what, why, how, whose, which, etc). on the other
hand, the full dataset has only 660 paragraphs in to-
tal (each paragraph is associated with 4 questions),
which renders training statistical models (especially
complex ones) very dif   cult.

up to now, the best solutions (sachan et al.,
2015; wang et al., 2015) are still heavily relying
on manually curated syntactic/semantic features,
with the aid of additional knowledge (e.g., word
embeddings, lexical/paragraph databases).

children book test (hill et al., 2016) was de-
veloped in a similar spirit to the id98/daily mail
datasets.
it takes any consecutive 21 sentences
from a children   s book     the    rst 20 sentences
are used as the passage, and the goal is to infer
a missing word in the 21st sentence (question and
answer). the questions are also categorized by the
type of the missing word: named entity, common
noun, preposition or verb. according to the    rst
study on this dataset (hill et al., 2016), a language
model (an id165 model or a recurrent neural net-
work) with local context is suf   cient for predicting
verbs or prepositions; however, for named entities
or common nouns, it improves performance to scan
through the whole paragraph to make predictions.
so far, the best published results are reported by
window-based memory networks.

babi (weston et al., 2016) is a collection of
arti   cial datasets, consisting of 20 different reason-
ing types. it encourages the development of mod-
els with the ability to chain reasoning, induction/
deduction, etc., so that they can answer a question
like    the football is in the playground    after read-
ing a sequence of sentences    john is in the play-
ground; bob is in the of   ce; john picked up the
football; bob went to the kitchen.    various types of
memory networks (sukhbaatar et al., 2015; kumar
et al., 2016) have been shown effective on these
tasks, and lee et al. (2016) show that vector space
models based on extensive problem analysis can
obtain near-perfect accuracies on all the categories.
despite these promising results, this dataset is lim-
ited to a small vocabulary (only 100   200 words)
and simple language variations, so there is still a
huge gap from real-world datasets that we need to
   ll in.

7 conclusion

in this paper, we carefully examined the recent
id98/daily mail reading comprehension task. our

systems demonstrated state-of-the-art results, but
more importantly, we performed a careful analysis
of the dataset by hand.

overall, we think the id98/daily mail datasets
are valuable datasets, which provide a promising
avenue for training effective statistical models for
reading comprehension tasks. nevertheless, we
argue that: (i) this dataset is still quite noisy due to
its method of data creation and coreference errors;
(ii) current neural networks have almost reached a
performance ceiling on this dataset; and (iii) the re-
quired reasoning and id136 level of this dataset
is still quite simple.

as future work, we need to consider how we can
utilize these datasets (and the models trained upon
them) to help solve more complex rc reasoning
tasks (with less annotated data).
acknowledgments
we thank the anonymous reviewers for their
thoughtful feedback. stanford university gratefully
acknowledges the support of the defense advanced
research projects agency (darpa) deep explo-
ration and filtering of text (deft) program under
air force research laboratory (afrl) contract
no. fa8750-13-2-0040. any opinions,    ndings,
and conclusion or recommendations expressed in
this material are those of the authors and do not
necessarily re   ect the view of the darpa, afrl,
or the us government.

references
jonathan berant, vivek srikumar, pei-chun chen, abby
vander linden, brittany harding, brad huang, peter
clark, and christopher d. manning. 2014. modeling
biological processes for reading comprehension. in
empirical methods in natural language processing
(emnlp), pages 1499   1510.

christopher j.c. burges. 2013. towards the machine
comprehension of text: an essay. technical report,
microsoft research technical report msr-tr-2013-
125.

danqi chen and christopher manning. 2014. a fast and
accurate dependency parser using neural networks.
in empirical methods in natural language process-
ing (emnlp), pages 740   750.

kyunghyun cho, bart van merrienboer, caglar gul-
cehre, dzmitry bahdanau, fethi bougares, holger
schwenk, and yoshua bengio. 2014. learning
phrase representations using id56 encoder   decoder for
id151. in empirical methods
in natural language processing (emnlp), pages
1724   1734.

karl moritz hermann, tomas kocisky, edward grefen-
stette, lasse espeholt, will kay, mustafa suleyman,
and phil blunsom. 2015. teaching machines to read
and comprehend. in advances in neural information
processing systems (nips), pages 1684   1692.

felix hill, antoine bordes, sumit chopra, and jason
weston. 2016. the goldilocks principle: reading
children   s books with explicit memory representa-
tions. in international conference on learning rep-
resentations (iclr).

rudolf kadlec, martin schmid, ondrej bajgar, and jan
kleindienst. 2016. text understanding with the at-
tention sum reader network. in association for com-
putational linguistics (acl).

sosuke kobayashi, ran tian, naoaki okazaki, and ken-
taro inui. 2016. dynamic entity representation with
max-pooling improves machine reading. in north
american association for computational linguistics
(naacl).

ankit kumar, ozan irsoy, peter ondruska, mohit iyyer,
james bradbury, ishaan gulrajani, victor zhong, ro-
main paulus, and richard socher. 2016. ask me
anything: dynamic memory networks for natural lan-
guage processing. in international conference on
machine learning (icml).

moontae lee, xiaodong he, wen-tau yih, jianfeng gao,
li deng, and paul smolensky. 2016. reasoning
in vector space: an exploratory study of question
answering. in international conference on learning
representations (iclr).

thang luong, hieu pham, and christopher d. man-
ning. 2015. effective approaches to attention-based
id4. in empirical methods
in natural language processing (emnlp), pages
1412   1421.

peter norvig. 1978. a uni   ed theory of id136
for text understanding. ph.d. thesis, university of
california, berkeley.

jeffrey pennington, richard socher, and christopher
manning. 2014. glove: global vectors for word
in empirical methods in natural
representation.
language processing (emnlp), pages 1532   1543.

matthew richardson, christopher j.c. burges, and erin
renshaw. 2013. mctest: a challenge dataset for
the open-domain machine comprehension of text. in
empirical methods in natural language processing
(emnlp), pages 193   203.

mrinmaya sachan, kumar dubey, eric xing, and
matthew richardson.
2015. learning answer-
entailing structures for machine comprehension. in
association for computational linguistics and in-
ternational joint conference on natural language
processing (acl/ijcnlp), pages 239   249.

sainbayar sukhbaatar, arthur szlam, jason weston, and
rob fergus. 2015. end-to-end memory networks. in
advances in neural information processing systems
(nips), pages 2431   2439.

hai wang, mohit bansal, kevin gimpel, and david
mcallester. 2015. machine comprehension with
in association
syntax, frames, and semantics.
for computational linguistics and international
joint conference on natural language processing
(acl/ijcnlp), pages 700   706.

jason weston, sumit chopra, and antoine bordes.
2015. memory networks. in international confer-
ence on learning representations (iclr).

jason weston, antoine bordes, sumit chopra, and
tomas mikolov. 2016. towards ai-complete ques-
tion answering: a set of prerequisite toy tasks. in
international conference on learning representa-
tions (iclr).

qiang wu, christopher j. burges, krysta m. svore, and
jianfeng gao. 2010. adapting boosting for informa-
tion retrieval measures. information retrieval, pages
254   270.

a samples and labeled categories from

the id98 dataset

for the analysis in section 5, we uniformly sam-
pled 100 examples from the development set of
the id98 dataset. table 8 provides a full index list
of our samples and table 7 presents our labeled
categories.

category
exact match (13)
sentence-level id141 (41)

partial clues (19)

multiple sentences (2)
coreference errors (8)
ambiguous or very hard (17)

sample ids
8, 11, 23, 27, 28, 32, 43, 57, 63, 72, 86, 87, 99
0, 2, 7, 9, 12, 14, 16, 18, 19, 20, 29, 30, 31, 34, 36,
37, 39, 41, 42, 44, 47, 48, 52, 54, 58, 64, 65, 66, 69,
73, 74, 78, 80, 81, 82, 84, 85, 90, 92, 95, 96
4, 17, 21, 24, 35, 38, 45, 53, 55, 56, 61, 62, 75, 83,
88, 89, 91, 97, 98
5, 76
6, 22, 40, 46, 51, 60, 68, 94
1, 3, 10, 13, 15, 25, 26, 33, 49, 50, 59, 67, 70, 71, 77,
79, 93

table 7: our labeled categories of the 100 samples.

id filename

id filename

0
2
4
6
8
10
12
14
16
18
20
22
24
26
28
30
32
34
36
38
40
42
44
46
48
50
52
54
56
58
60
62
64
66
68
70
72
74
76
78
80
82
84
86
88
90
92
94
96
98

ddb1e746f88a22fee654ecde8f018e7586595045.question
38c702812a874f983e9890c32ba832841a327351.question
417cbffd5e6275b3c42cb88be222a9f6c7d415f1.question
b4e157a6a34bf11a03e0b5cd55065c0f39ac8d60.question
223c8e3aeddc3f65fee1964df17bb72f89b723e4.question
378fd418b8ec18dff406be07ec225e6bf53659f5.question
80529c792d3a368861b404c1ce4d7ad3c12e552a.question
3cf6fb2c0d09927a12add82b4a3f248da740d0de.question
f0abf359d71f7896abd09ff7b3319c70f2ded81e.question
881ab3139c34e9d9f29eb11601321a234d096272.question
f83a70d469fa667f0952959346b496fbf3cdb35c.question
02664d5e3af321afbaf4ee351ba1f24643746451.question
42c25a01801228a863c508f9d9e95399ea5f37a4.question
b6636e525ad58ffdc9a7c18187fb3412660d2cdd.question
262b855e2f24e1b2e4e0ba01ace81a1f214d729e.question
be813e58ae9387a9fdaf771656c8e1122794e515.question
9534c3907f1cd917d24a9e4f2afc5b38b82d9fca.question
6efa2d6bad587bde65ca22d10eca83cf0176d84f.question
0c44d6ef109d33543cfbd26c95c9c3f6fe33a995.question
fb4dd20e0f464423b6407fd0d21cc4384905cf26.question
f7133f844967483519dbf632e2f3fb90c5625a4c.question
8ea6ad57c1c5eb1950f50ea47231a5b3f32dd639.question
7f11f0b4f6bb9aaa3bdc74bffaed5c869b26be97.question
57fc2b7ffcfbd1068fbc33b95d5786e2bff24698.question
d857700721b5835c3472ba73ef7abfad0c9c499f.question
4c488f41622ad48977a60c2283910f15a736417e.question
addd9cebe24c96b4a3c8e9a50cd2a57905b6defb.question
3f7ac912a75e4ef7a56987bff37440ffa14770c6.question
d9c2e9bfc71045be2ecd959676016599e4637ed1.question
f5c2753703b66d26f43bafe7f157803dc96eedbc.question
e5bb1c27d07f1591929bf0283075ad1bc1fc0b50.question
58c4c046654af52a3cb8f6890411a41c0dd0063b.question
ece6f4e047856d5a84811a67ac9780d48044e69a.question
ddf3f2b06353fe8a9b50043f926eb3ab318e91b2.question
e86d3fa2a74625620bcae0003dfbe13416ee29cf.question
ee694cb968ae99aea36f910355bf73da417274c0.question
91e3cdd46a70d6dfbe917c6241eab907da4b1562.question
f3737e4de9864f083d6697293be650e02505768c.question
fb3eadd07b9f1df1f8a7a6b136ad6d06f4981442.question
54b6396669bdb2e30715085745d4f98d058269ef.question
d5eb4f98551d23810bfeb0e5b8a94037bcf58b0d.question
12f32c770c86083ff21b25de7626505c06440018.question
1c2a14f525fa3802b8da52aebaa9abd2091f9215.question
adcf5881856bcbaf1ad93d06a3c5431f6a0319ba.question
773066c39bb3b593f676caf03f7e7370a8cd2a43.question
b66ebaaefb844f1216fd3d28eb160b08f42cde62.question
e27ca3104a596171940db8501c4868ed2fbc8cea.question
83ff109c6ccd512abdf317220337b98ef551d94a.question
7a2a9a7fbb44b0e51512c61502ce2292170400c1.question
0c2e28b7f373f29f3796d29047556766cc1dd709.question

1
3
5
7
9
11
13
15
17
19
21
23
25
27
29
31
33
35
37
39
41
43
45
47
49
51
53
55
57
59
61
63
65
67
69
71
73
75
77
79
81
83
85
87
89
91
93
95
97
99

2bef8ec21b10a3294b1496d9a86f29f0592d2300.question
636857045cf266dd69b67b1e53617bed5253dc33.question
ef96409c707a699e4055a1d0684eecdb6e115c16.question
1d75e7c59978c7c06f3aecaf52bc35b8919eee17.question
13d33b8c86375b0f5fdc856116e91a7355c6fc5a.question
d8253b7f22662911c19ec4468f81b9db29df1746.question
728e7b365e941d814676168c78c9c4f38892a550.question
04b827f84e60659258e19806afe9f8d10b764db1.question
b6696e0f2166a75fcefbe4f28d0ad06e420eef23.question
66f5208d62b543ee41accb7a560d63ff40413bac.question
1853813a80f83a1661dd3f6695559674c749525e.question
20417b5efb836530846ddf677d1bd0bbc831643c.question
70a3ba822770abcaf64dd131c85ec964d172c312.question
6147c9f2b3d1cc6fbc57c2137f0356513f49bf46.question
d7211f4d21f40461bb59954e53360eeb4bb6c664.question
ad39c5217042f36e4c1458e9397b4a588bbf8cf9.question
3fbe4bfb721a6e1aa60502089c46240d5c332c05.question
436aa25e28d3a026c4fcd658a852b6a24fc6935e.question
8472b859c5a8d18454644d9acdb5edd1db175eb5.question
a192ddbcecf2b00260ae4c7c3c20df4d5ce47a85.question
29b274958eb057e8f1688f02ef8dbc1c6d06c954.question
1e43f2349b17dac6d1b3143f8c5556e2257be92c.question
8e6d8d984e51adb5071aad22680419854185eaea.question
57b773478955811a8077c98840d85af03e1b4f05.question
f8eedded53c96e0cb98e2e95623714d2737f29da.question
39680fd0bff53f2ca02f632eabbc024d698f979e.question
50317f7a626e23628e4bfd190e987ad5af7d283e.question
610012ef561027623f4b4e3b8310c1c41dc819cc.question
848c068db210e0b255f83c4f8b01d2d421fb9c94.question
4f76379f1c7b1d4acc5a4c82ced64af6313698dd.question
33b911f9074c80eb18a57f657ad01393582059be.question
7b03f730fda1b247e9f124b692e3298859785ef3.question
35565dc6aecc0f1203842ef13aede0a14a8cf075.question
e248e59739c9c013a2b1b7385d881e0f879b341d.question
176bf03c9c19951a8ae5197505a568454a6d4526.question
7a666f78590edbaf7c4d73c4ea641c545295a513.question
e54d9bdcb478ecc490608459d3405571979ef3f2.question
1fc7488755d24696a4ed1aabc0a21b8b9755d8c6.question
1406bdad74b3f932342718d5d5d0946a906d73e2.question
0a53102673f2bebc36ce74bf71db1b42a0187052.question
370de4ffe0f2f9691e4bd456ff344a6a337e0edf.question
9f6b5cff3ce146e21e323a1462c3eff8fca3d4a0.question
f2416e14d89d40562284ba2d15f7d5cc59c7e602.question
097d34b804c4c052591984d51444c4a97a3c41ac.question
598cf5ff08ea75dcedda31ac1300e49cdf90893a.question
535a44842decdc23c11bae50d9393b923897187e.question
bb07799b4193cffa90792f92a8c14d591754a7f3.question
5ede07a1e4ac56a0155d852df0f5bb6bde3cb507.question
9dcdc052682b041cdbf2fadc8e55f1bafc88fe61.question
2bdf1696bfd2579bb719402e9a6fa99cb8dbf587.question

table 8: a full index list of our samples.

