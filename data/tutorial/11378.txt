7
1
0
2

 

y
a
m
7

 

 
 
]

v
c
.
s
c
[
 
 

2
v
8
1
1
5
0

.

1
1
6
1
:
v
i
x
r
a

the amazing mysteries of the gutter:

drawing id136s between panels in comic book narratives

mohit iyyer   1 varun manjunatha   1 anupam guha1 yogarshi vyas1

jordan boyd-graber2 hal daum  e iii1 larry davis1
2university of colorado, boulder
1university of maryland, college park

{miyyer,varunm,aguha,yogarshi,hal,lsd}@umiacs.umd.edu

jordan.boyd.graber@colorado.edu

abstract

visual narrative is often a combination of explicit infor-
mation and judicious omissions, relying on the viewer to
supply missing details. in comics, most movements in time
and space are hidden in the    gutters    between panels. to
follow the story, readers logically connect panels together
by inferring unseen actions through a process called    clo-
sure   . while computers can now describe what is explic-
itly depicted in natural images, in this paper we examine
whether they can understand the closure-driven narratives
conveyed by stylized artwork and dialogue in comic book
panels. we construct a dataset, comics, that consists of
over 1.2 million panels (120 gb) paired with automatic
textbox transcriptions. an in-depth analysis of comics
demonstrates that neither text nor image alone can tell a
comic book story, so a computer must understand both
modalities to keep up with the plot. we introduce three
cloze-style tasks that ask models to predict narrative and
character-centric aspects of a panel given n preceding pan-
els as context. various deep neural architectures under-
perform human baselines on these tasks, suggesting that
comics contains fundamental challenges for both vision
and language.

1. introduction

comics are fragmented scenes forged into full-   edged
stories by the imagination of their readers. a comics creator
can condense anything from a centuries-long intergalactic
war to an ordinary family dinner into a single panel. but it
is what the creator hides from their pages that makes comics
truly interesting:
the unspoken conversations and unseen
actions that lurk in the spaces (or gutters) between adja-
cent panels. for example, the dialogue in figure 1 suggests
that between the second and third panels, gilda commands
her snakes to chase after a frightened michael in some

    authors contributed equally

figure 1. where did the snake in the last panel come from? why
is it biting the man? is the man in the second panel the same as
the man in the    rst panel? to answer these questions, readers form
a larger meaning out of the narration boxes, speech bubbles, and
artwork by applying closure across panels.

sort of strange cult initiation. through a process called
closure [40], which involves (1) understanding individual
panels and (2) making connective id136s across panels,
readers form coherent storylines from seemingly disparate
panels such as these. in this paper, we study whether com-
puters can do the same by collecting a dataset of comic
books (comics) and designing several tasks that require
closure to solve.
section 2 describes how we create comics,1 which
contains    1.2 million panels drawn from almost 4,000
publicly-available comic books published during the
   golden age    of american comics (1938   1954). comics
is challenging in both style and content compared to natural
images (e.g., photographs), which are the focus of most ex-
isting datasets and methods [32, 56, 55]. much like painters,
comic artists can render a single object or concept in mul-
tiple artistic styles to evoke different emotional responses
from the reader. for example, the lions in figure 2 are
drawn with varying degrees of realism: the more cartoon-

1data, code, and annotations to be made available after blind review.

1

# books
# pages
# panels
# textboxes
text cloze instances
visual cloze instances
char. coherence instances

3,948
198,657
1,229,664
2,498,657
89,412
587,797
72,313

table 1. statistics describing dataset size (top) and the number of
total instances for each of our three tasks (bottom).

we build our own dataset, comics, by (1) downloading
comics in the public domain, (2) segmenting each page into
panels, (3) extracting textbox locations from panels, and (4)
running ocr on textboxes and post-processing the output.
table 1 summarizes the contents of comics. the rest of
this section describes each step of our data creation pipeline.
2.1. where do our comics come from?

the    golden age of comics    began during america   s
great depression and lasted through world war ii, ending
in the mid-1950s with the passage of strict censorship reg-
ulations. in contrast to the long, world-building story arcs
popular in later eras, golden age comics tend to be small
and self-contained; a single book usually contains multi-
ple different stories sharing a common theme (e.g., crime
or mystery). while the best-selling golden age comics
tell of american superheroes triumphing over german and
japanese villains, a variety of other genres (such as ro-
mance, humor, and horror) also enjoyed popularity [18].
the digital comics museum (dcm)2 hosts user-uploaded
scans of many comics by lesser-known golden age pub-
lishers that are now in the public domain due to copyright
expiration. to avoid off-square images and missing pages,
as the scans vary in resolution and quality, we download the
4,000 highest-rated comic books from dcm.3
2.2. breaking comics into their basic elements

the dcm comics are distributed as compressed archives
of jpeg page scans. to analyze closure, which occurs from
panel-to-panel, we    rst extract panels from the page images.
next, we extract textboxes from the panels, as both location
and content of textboxes are important for character and nar-
rative understanding.
panel segmentation:
previous work on panel segmenta-
tion uses heuristics [34] or algorithms such as density gra-
dients and recursive cuts [52, 43, 48] that rely on pages
with uniformly white backgrounds and clean gutters. un-
fortunately, scanned images of eighty-year old comics do

2http://digitalcomicmuseum.com/
3some of the panels in comics contain offensive caricatures and opin-

ions re   ective of that period in american history.

figure 2. different artistic renderings of lions taken from the
comics dataset. the left-facing lions are more cartoonish (and
humorous) than the ones facing right, which come from action and
adventure comics that rely on realism to provide thrills.

ish lions, from humorous comics, take on human expres-
sions (e.g., surprise, nastiness), while those from adventure
comics are more photorealistic.

comics are not just visual: creators push their stories for-
ward through text   speech balloons, thought clouds, and
narrative boxes   which we identify and transcribe using
id42 (ocr). together, text and im-
age are often intricately woven together to tell a story that
neither could tell on its own (section 3). to understand a
story, readers must connect dialogue and narration to char-
acters and environments; furthermore, the text must be read
in the proper order, as panels often depict long scenes rather
than individual moments [10]. text plays a much larger role
in comics than it does for existing datasets of visual sto-
ries [25].

to test machines    ability to perform closure, we present
three novel cloze-style tasks in section 4 that require a deep
understanding of narrative and character to solve. in sec-
tion 5, we design four neural architectures to examine the
impact of multimodality and contextual understanding via
closure. all of these models perform signi   cantly worse
than humans on our tasks; we conclude with an error anal-
ysis (section 6) that suggests future avenues for improve-
ment.

2. creating a dataset of comic books

comics, de   ned by cartoonist will eisner as sequential
art [13], tell their stories in sequences of panels, or sin-
gle frames that can contain both images and text. existing
comics datasets [19, 39] are too small to train data-hungry
machine learning models for narrative understanding; addi-
tionally, they lack diversity in visual style and genres. thus,

not particularly adhere to these standards; furthermore,
many dcm comics have non-standard panel layouts and/or
textboxes that extend across gutters to multiple panels.

after our attempts to use existing panel segmentation
software failed, we turned to deep learning. we annotate
500 randomly-selected pages from our dataset with rect-
angular bounding boxes for panels. each bounding box
encloses both the panel artwork and the textboxes within
the panel; in cases where a textbox spans multiple pan-
els, we necessarily also include portions of the neighbor-
ing panel. after annotation, we train a region-based con-
volutional neural network to automatically detect panels.
in particular, we use faster r-id98 [45] initialized with a
pretrained vgg id98 m 1024 model [9] and alternatingly
optimize the region proposal network and the detection net-
work. in western comics, panels are usually read left-to-
right, top-to-bottom, so we also have to properly order all
of the panels within a page after extraction. we compute
the midpoint of each panel and sort them using morton or-
der [41], which gives incorrect orderings only for rare and
complicated panel layouts.
textbox segmentation: since we are particularly inter-
ested in modeling the interplay between text and artwork,
we need to also convert the text in each panel to a machine-
readable format.4 as with panel segmentation, existing
comic textbox detection algorithms [22, 47] could not ac-
curately localize textboxes for our data. thus, we re-
sort again to faster r-id98: we annotate 1,500 panels for
textboxes,5 train a faster-r-id98, and sort the extracted
textboxes within each panel using morton order.
2.3. ocr

the    nal step of our data creation pipeline is applying
ocr to the extracted textbox images. we unsuccessfully
experimented with two trainable open-source ocr systems,
tesseract [50] and ocular [6], as well as abbyy   s consumer-
grade finereader.6 the ineffectiveness of these systems is
likely due to the considerable variation in comic fonts as
well as domain mismatches with pretrained language mod-
els (comics text is always capitalized, and dialogue phe-
nomena such as dialects may not be adequately represented
in training data). google   s cloud vision ocr7 performs
much better on comics than any other system we tried.
while it sometimes struggles to detect short words or punc-
tuation marks, the quality of the transcriptions is good con-

4alternatively, modules for text spotting and recognition [27] could be
built into architectures for our downstream tasks, but since comic dialogues
can be quite lengthy, these modules would likely perform poorly.

5we make a distinction between narration and dialogue; the former
usually occurs in strictly rectangular boxes at the top of each panel and
contains text describing or introducing a new scene, while the latter is usu-
ally found in speech balloons or thought clouds.

6http://www.abbyy.com
7http://cloud.google.com/vision

sidering the image domain and quality. we use the cloud
vision api to run ocr on all 2.5 million textboxes for a cost
of $3,000. we post-process the transcriptions by removing
systematic spelling errors (e.g., failing to recognize the    rst
letter of a word). finally, each book in our dataset contains
three or four full-page product advertisements; since they
are irrelevant for our purposes, we train a classi   er on the
transcriptions to remove them.8

3. data analysis

in this section, we explore what makes understanding
narratives in comics dif   cult, focusing speci   cally on in-
trapanel behavior (how images and text interact within a
panel) and interpanel transitions (how the narrative ad-
vances from one panel to the next). we characterize panels
and transitions using a modi   ed version of the annotation
scheme in scott mccloud   s    understanding comics    [40].
over 90% of panels rely on both text and image to con-
vey information, as opposed to just using a single modal-
ity. closure is also important:
to understand most tran-
sitions between panels, readers must make complex infer-
ences that often require common sense (e.g., connecting
jumps in space and/or time, recognizing when new char-
acters have been introduced to an existing scene). we con-
clude that any model trained to understand narrative    ow
in comics will have to effectively tie together multimodal
inputs through closure.

to perform our analysis, we manually annotate
250 randomly-selected pairs of consecutive panels from
comics. each panel of a pair is annotated for intrapanel
behavior, while an interpanel annotation is assigned to the
transition between the panels. two annotators indepen-
dently categorize each pair, and a third annotator makes
the    nal decision when they disagree. we use four intra-
panel categories (de   nitions from mccloud, percentages
from our annotations):

1. word-speci   c, 4.4%: the pictures illustrate, but do

not signi   cantly add to a largely complete text.

2. picture-speci   c, 2.8%: the words do little more than

add a soundtrack to a visually-told sequence.

3. parallel, 0.6%: words and pictures seem to follow

very different courses without intersecting.

4. interdependent, 92.1%: words and pictures go hand-
in-hand to convey an idea that neither could convey
alone.

we group interpanel transitions into    ve categories:
1. moment-to-moment, 0.4%: almost no time passes
between panels, much like adjacent frames in a video.
2. action-to-action, 34.6%: the same subjects progress

through an action within the same scene.

8see supplementary material for speci   cs about our post-processing.

figure 3. five example panel sequences from comics, one for each type of interpanel transition. individual panel borders are color-coded
to match their intrapanel categories (legend in bottom-left). moment-to-moment transitions unfold like frames in a movie, while scene-to-
scene transitions are loosely strung together by narrative boxes. percentages are the relative prevalance of the transition or panel type in an
annotated subset of comics.

3. subject-to-subject, 32.7%: new subjects are intro-

duced while staying within the same scene or idea.

4. scene-to-scene, 13.8%: signi   cant changes in time

or space between the two panels.

5. continued conversation, 17.7%: subjects continue a
conversation across panels without any other changes.
the two annotators agree on 96% of the intrapanel an-
notations (cohen   s    = 0.657), which is unsurprising be-
cause almost every panel is interdependent. the interpanel
task is signi   cantly harder: agreement is only 68% (co-
hen   s    = 0.605). panel transitions are more diverse, as
all types except moment-to-moment are relatively common
(figure 3); interestingly, moment-to-moment transitions re-
quire the least amount of closure as there is almost no
change in time or space between the panels. multiple tran-
sition types may occur in the same panel, such as simultane-
ous changes in subjects and actions, which also contributes
to the lower interpanel agreement.

4. tasks that test closure

to explore closure in comics, we design three novel
tasks (text cloze, visual cloze, and character coherence) that
test a model   s ability to understand narratives and characters
given a few panels of context. as shown in the previous
section   s analysis, a high percentage of panel transitions re-
quire non-trivial id136s from the reader; to successfully
solve our proposed tasks, a model must be able to make the
same kinds of connections.

while their objectives are different, all

three tasks

same

format:

given preceding panels
follow the
pi   1, pi   2, . . . , pi   n as context, a model
is asked to
predict some aspect of panel pi. while previous work
on visual storytelling focuses on generating text given
the dialogue-heavy text in comics
some context [24],
makes evaluation dif   cult
(e.g., dialects, grammatical
variations, many rare words). we want our evaluations to
focus speci   cally on closure, not generated text quality,
so we instead use a cloze-style framework [53]: given c
candidates   with a single correct option   models must use
the context panels to rank the correct candidate higher than
the others. the rest of this section describes each of the
three tasks in detail; table 1 provides the total instances of
each task with the number of context panels n = 3.
text cloze:
in the text cloze task, we ask the model to
predict what text out of a set of candidates belongs in a par-
ticular textbox, given both context panels (text and image)
as well as the current panel image. while initially we did
not put any constraints on the task design, we quickly no-
ticed two major issues. first, since the panel images include
textboxes, any model trained on this task could in princi-
ple learn to crudely imitate ocr by matching text candi-
dates to the actual image of the text. to solve this problem,
we    black out    the rectangle given by the bounding boxes
for each textbox in a panel (see figure 4).9 second, pan-
els often have multiple textboxes (e.g., conversations be-
tween characters); to focus on interpanel transitions rather

9to reduce the chance of models trivially correlating candidate length

to textbox size, we remove very short and very long candidates.

figure 4. in the character coherence task (top), a model must order the dialogues in the    nal panel, while visual cloze (bottom) requires
choosing the image of the panel that follows the given context. for visualization purposes, we show the original context panels; during
model training and evaluation, textboxes are blacked out in every panel.

than intrapanel complexity, we restrict pi to panels that con-
tain only a single textbox. thus, nothing from the current
panel matters other than the artwork; the majority of the
predictive information comes from previous panels.

visual cloze: we know from section 3 that in most cases,
text and image work interdependently to tell a story. in the
visual cloze task, we follow the same set-up as in text cloze,
but our candidates are images instead of text. a key differ-
ence is that models are not given text from the    nal panel;
in text cloze, models are allowed to look at the    nal panel   s
artwork. this design is motivated by eyetracking studies in
single-panel cartoons, which show that readers look at art-
work before reading the text [7], although atypical font style
and text length can invert this order [16].

character coherence: while the previous two tasks fo-
cus mainly on narrative structure, our third task attempts to
isolate character understanding through a re-ordering task.
given a jumbled set of text from the textboxes in panel pi, a
model must learn to match each candidate to its correspond-
ing textbox. we restrict this task to panels that contain ex-
actly two dialogue boxes (narration boxes are excluded to
focus the task on characters). while it is often easy to order
the text based on the language alone (e.g.,    how   s it going   
always comes before       ne, how about you?   ), many cases
require inferring which character is likely to utter a partic-
ular bit of dialogue based on both their previous utterances
and their appearance (e.g., figure 4, top).

4.1. task dif   culty

for text cloze and visual cloze, we have two dif   culty
settings that vary in how cloze candidates are chosen. in the
easy setting, we sample textboxes (or panel images) from
the entire comics dataset at random. most incorrect can-
didates in the easy setting have no relation to the provided
context, as they come from completely different books and
genres. this setting is thus easier for models to    cheat    on
by relying on stylistic indicators instead of contextual in-
formation. with that said, the task is still non-trivial; for
example, many bits of short dialogue can be applicable in a
variety of scenarios. in the hard case, the candidates come
from nearby pages, so models must rely on the context to
perform well. for text cloze, all candidates are likely to
mention the same character names and entities, while color
schemes and textures become much less distinguishing for
visual cloze.

5. models & experiments

to measure the dif   culty of these tasks for deep learn-
ing models, we adapt strong baselines for multimodal lan-
guage and vision understanding tasks to the comics do-
main. we evaluate four different neural models, variants
of which were also used to benchmark the visual ques-
tion answering dataset [2] and encode context for visual
storytelling [25]: text-only, image-only, and two image-text
models. our best-performing model encodes panels with a
hierarchical lstm architecture (see figure 5).

figure 5. the image-text architecture applied to an instance of the text cloze task. pretrained image features are combined with learned
text features in a hierarchical lstm architecture to form a context representation, which is then used to score text candidates.

on text cloze, accuracy increases when models are given
images (in the form of pretrained vgg-16 features) in addi-
tion to text; on the other tasks, incorporating both modali-
ties is less important. additionally, for the text cloze and vi-
sual cloze tasks, models perform far worse on the hard set-
ting than the easy setting, con   rming our intuition that these
tasks are non-trivial when we control for stylistic dissimilar-
ities between candidates. finally, none of the architectures
outperform human baselines, which demonstrates the dif   -
culty of understanding comics: image features obtained
from models trained on natural images cannot capture the
vast variation in artistic styles, and textual models strug-
gle with the richness and ambiguity of colloquial dialogue
highly dependent on visual contexts. in the rest of this sec-
tion, we    rst introduce a shared notation and then use it to
specify all of our models.
5.1. model de   nitions

in all of our tasks, we are asked to make a prediction
about a particular panel given the preceding n panels as
context.10 each panel consists of three distinct elements:
image, text (ocr output), and textbox bounding box co-
ordinates. for any panel pi, the corresponding image is
zi. since there can be multiple textboxes per panel, we
refer to individual textbox contents and bounding boxes
as tix and bix, respectively. each of our tasks has a dif-
text cloze has three
ferent set of answer candidates a:
text candidates ta1...3, visual cloze has three image candi-
dates za1...3, and character coherence has two combina-

10test and validation instances for all tasks come from comic books that

are unseen during training.

tions of text / bounding box pairs, {ta1/ba1 , ta2/ba2} and
{ta1 /ba2 , ta2 /ba1}. our architectures differ mainly in the
encoding function g that converts a sequence of context pan-
els pi   1, pi   2, . . . , pi   n into a    xed-length vector c. we
score the answer candidates by taking their inner product
with c and normalizing with the softmax function,

s = softmax(at c),

(1)

and we minimize the cross-id178 loss against the ground-
truth labels.11
text-only: the text-only baseline only has access to the
text tix within each panel. our g function encodes this text
on multiple levels: we    rst compute a representation for
each tix with a id27 sum12 and then combine
multiple textboxes within the same panel using an intra-
panel lstm [23]. finally, we feed the panel-level represen-
tations to an interpanel lstm and take its    nal hidden state
as the context representation (figure 5). for text cloze, the
answer candidates are also encoded with a id27
sum; for visual cloze, we project the 4096-d fc7 layer of
vgg-16 down to the id27 dimensionality with
a fully-connected layer.13

11performance falters slightly on a development set with contrastive

max-margin id168s [51] in place of our softmax alternative.

12as in previous work for visual id53 [57], we observe no
noticeable improvement with more sophisticated encoding architectures.
13for training and testing, we use three panels of context and three can-
didates. we use a vocabulary size of 30,000 words, restrict the maximum
number of textboxes per panel to three, and set the dimensionality of word
embeddings and lstm hidden states to 256. models are optimized using
adam [29] for ten epochs, after which we select the best-performing model
on the dev set.

model

random
text-only
image-only

nc-image-text

image-text

human

text cloze
hard
easy
33.3
33.3
52.9
63.4
51.7
49.4
59.6
63.1
61.0
68.6
   
84

visual cloze
hard
easy
33.3
33.3
48.4
55.9
85.7
63.2

-

81.3

   

-

59.1
88

char. coheren.

50.0
68.2
70.9
65.2
69.3
87

table 2. combining image and text in neural architectures im-
proves their ability to predict the next image or dialogue in
comics narratives. the contextual information present in pre-
ceding panels is useful for all tasks: the model that only looks at
a single panel (nc-image-text) always underperforms its context-
aware counterpart. however, even the best performing models lag
well behind humans.

image-only: the image-only baseline is even simpler:
we feed the fc7 features of each context panel to an lstm
and use the same objective function as before to score
candidates. for visual cloze, we project both the context
and answer representations to 512-d with additional fully-
connected layers before scoring. while the comics dataset
is certainly large, we do not attempt learning visual fea-
tures from scratch as our task-speci   c signals are far more
complicated than simple image classi   cation. we also try
   ne-tuning the lower-level layers of vgg-16 [4]; however,
this substantially lowers task accuracy even with very small
learning rates for the    ne-tuned layers.

image-text: we combine the previous two models by
concatenating the output of the intrapanel lstm with the
fc7 representation of the image and passing the result
through a fully-connected layer before feeding it to the in-
terpanel lstm (figure 5). for text cloze and character co-
herence, we also experiment with a variant of the image-
text baseline that has no access to the context panels, which
we dub nc-image-text. in this model, the scoring function
computes inner products between the image features of pi
and the text candidates.14

6. error analysis

table 2 contains our full experimental results, which we
brie   y summarize here. on text cloze, the image-text model
dominates those trained on a single modality. however, text
is much less helpful for visual cloze than it is for text cloze,
suggesting that visual similarity dominates the former task.
having the context of the preceding panels helps across the
board, although the improvements are lower in the hard set-
ting. there is more variation across the models in the easy

14we cannot apply this model to visual cloze because we are not allowed

access to the artwork in panel pi.

setting; we hypothesize that the hard case requires mov-
ing away from pretrained image features, and transfer learn-
ing methods may prove effective here. differences between
models on character coherence are minor; we suspect that
more complicated attentional architectures that leverage the
bounding box locations bix are necessary to    follow    speech
bubble tails to the characters who speak them.

we also compare all models to a human baseline, for
which the authors manually solve one hundred instances of
each task (in the hard setting) given the same preprocessed
input that is fed to the neural architectures. most human
errors are the result of poor ocr quality (e.g., misspelled
words) or low image resolution. humans comfortably out-
perform all models, making it worthwhile to look at where
computers fail but humans succeed.

the top row in figure 6 demonstrates an instance (from
easy text cloze where the image helps the model make the
correct prediction. the text-only model has no idea that
an airplane (referred to here as a    ship   ) is present in the
panel sequence, as the dialogue in the context panels make
no mention of it. in contrast, the image-text model is able
to use the artwork to rule out the two incorrect candidates.
the bottom two rows in figure 6 show hard text cloze
instances in which the image-text model is deceived by the
artwork in the    nal panel. while the    nal panel of the mid-
dle row does contain what looks to be a creek,    cat   sh creek
jail    is more suited for a narrative box than a speech bubble,
while the meaning of the correct candidate is obscured by
the dialect and out-of-vocabulary token. similarly, a camera
   lms a    ght scene in the last row; the model selects a candi-
date that describes a    ght instead of focusing on the context
in which the scene occurs. these examples suggest that
the contextual information is overridden by strong associa-
tions between text and image, motivating architectures that
go beyond similarity by leveraging external world knowl-
edge to determine whether an utterance is truly appropriate
in a given situation.

7. related work

our work is related to three main areas: (1) multimodal
tasks that require language and vision understanding, (2)
computational methods that focus on non-natural images,
and (3) models that characterize language-based narratives.
deep learning has renewed interest in jointly reasoning
about vision and language. datasets such as ms coco [35]
and visual genome [31] have enabled image caption-
ing [54, 28, 56] and visual id53 [37, 36].
similar to our character coherence task, researchers have
built models that match tv show characters with their vi-
sual attributes [15] and speech patterns [21].

closest to our own comic book setting is the visual sto-
rytelling task, in which systems must generate [24] or re-
order [1] stories given a dataset (sind) of photos from

mantic scene properties from a clip art dataset featuring
characters and objects in a limited variety of settings. ap-
plications of deep learning to paintings include tasks such
as detecting objects in oil paintings [11, 12] and answering
questions about artwork [20]. previous computational work
on comics focuses primarily on extracting elements such as
panels and textboxes [46]; in addition to the references in
section 2, there is a large body of segmentation research on
manga [3, 44, 38, 30].

to the best of our knowledge, we are the    rst to com-
putationally model content in comic books as opposed to
just extracting their elements. we follow previous work
in language-based narrative understanding; very similar to
our text cloze task is the    story cloze test    [42], in which
models must predict the ending to a short (four sentences
long) story. just like our tasks, the story cloze test proves
dif   cult for computers and motivates future research into
commonsense knowledge acquisition. others have studied
characters [14, 5, 26] and narrative structure [49, 33, 8] in
novels.

8. conclusion & future work

we present the comics dataset, which contains over 1.2
million panels from    golden age    comic books. we de-
sign three cloze-style tasks on comics to explore closure,
or how readers connect disparate panels into coherent sto-
ries. experiments with different neural architectures, along
with a manual data analysis, con   rm the importance of mul-
timodal models that combine text and image for comics un-
derstanding. we additionally show that context is crucial for
predicting narrative or character-centric aspects of panels.
however, for computers to reach human performance,
they will need to become better at leveraging context. read-
ers rely on commonsense knowledge to make sense of dra-
matic scene and camera changes; how can we inject such
knowledge into our models? another potentially intrigu-
ing direction, especially given recent advances in generative
adversarial networks [17], is generating artwork given dia-
logue (or vice versa). finally, comics presents a golden
opportunity for id21; can we train models that
generalize across natural and non-natural images much like
humans do?

9. acknowledgments

we thank the anonymous reviewers for their insight-
ful comments and the umiacs and google cloud sup-
port staff for their help with ocr. manjunatha and
davis were supported by of   ce of naval research grant
n000141612713, while iyyer, boyd-graber, and daum  e
were supported by nsf grant iis-1320538. any opinions,
   ndings, or conclusions expressed here are those of the au-
thors and do not necessarily re   ect the view of the sponsor.

figure 6. three text cloze examples from the development set,
shown with a single panel of context (boxed candidates are predic-
tions by the text-image model). the airplane artwork in the top
row helps the image-text model choose the correct answer, while
the text-only model fails because the dialogue lacks contextual in-
formation. conversely, the bottom two rows show the image-text
model ignoring the context in favor of choosing a candidate that
mentions something visually present in the last panel.

flikr galleries of    storyable    events such as weddings and
birthday parties. sind   s images are fundamentally differ-
ent from comics in that they lack coherent characters and
accompanying dialogue. comics are created by skilled pro-
fessionals, not crowdsourced workers, and they offer a far
greater variety of character-centric stories that depend on
dialogue to further the narrative; with that said, the text in
comics is less suited for generation because of ocr errors.
we build here on previous work that attempts to under-
stand non-natural images. zitnick et al. [58] discover se-

references
[1] h. agrawal, a. chandrasekaran, d. batra, d. parikh, and
m. bansal. sort story: sorting jumbled images and captions
into stories. in proceedings of empirical methods in natural
language processing, 2016. 7

[2] s. antol, a. agrawal, j. lu, m. mitchell, d. batra,
c. lawrence zitnick, and d. parikh. vqa: visual question
answering. in international conference on id161,
2015. 5

[3] y. aramaki, y. matsui, t. yamasaki, and k. aizawa. inter-
active segmentation for manga. in special interest group on
computer graphics and interactive techniques conference,
2014. 8

[4] y. aytar, l. castrejon, c. vondrick, h. pirsiavash, and

a. torralba. cross-modal scene networks. arxiv, 2016. 7

[5] d. bamman, t. underwood, and n. a. smith. a bayesian
mixed effects model of literary character. in proceedings of
the association for computational linguistics, 2014. 8

[6] t. berg-kirkpatrick, g. durrett, and d. klein. unsupervised
transcription of historical documents. in proceedings of the
association for computational linguistics, 2013. 3

[7] p. j. carroll, j. r. young, and m. s. guertin. visual analysis
of cartoons: a view from the far side. in eye movements and
visual cognition. springer, 1992. 5

[8] n. chambers and d. jurafsky. unsupervised learning of nar-
rative schemas and their participants. in proceedings of the
association for computational linguistics, 2009. 8

[9] k. chat   eld, k. simonyan, a. vedaldi, and a. zisserman.
return of the devil in the details: delving deep into convo-
lutional nets. in british machine vision conference, 2014.
3

[10] n. cohn. the limits of time and transitions: challenges
to theories of sequential image comprehension. studies in
comics, 1(1), 2010. 2

[11] e. crowley and a. zisserman. the state of the art: object
retrieval in paintings using discriminative regions. in british
machine vision conference, 2014. 8

[12] e. j. crowley, o. m. parkhi, and a. zisserman. face paint-
in british machine vision

ing: querying art with photos.
conference, 2015. 8

[13] w. eisner. comics & sequential art. poorhouse press, 1990.

2

[14] d. k. elson, n. dames, and k. r. mckeown. extracting
social networks from literary    ction. in proceedings of the
association for computational linguistics, 2010. 8

[15] m. everingham, j. sivic, and a. zisserman. hello! my name
is... buffy        automatic naming of characters in tv video. in
proceedings of the british machine vision conference, 2006.
7

[16] t. foulsham, d. wybrow, and n. cohn. reading with-
out words: eye movements in the comprehension of comic
strips. applied cognitive psychology, 30, 2016. 5

[17] i. goodfellow,

j. pouget-abadie, m. mirza, b. xu,
d. warde-farley, s. ozair, a. courville, and y. bengio. gen-
erative adversarial nets. in proceedings of advances in neu-
ral information processing systems, 2014. 8

[18] r. goulart. comic book encyclopedia: the ultimate guide
to characters, graphic novels, writers, and artists in the
comic book universe. harpercollins, 2004. 2

[19] c. gu  erin, c. rigaud, a. mercier, f. ammar-boudjelal,
k. bertet, a. bouju, j.-c. burie, g. louis, j.-m. ogier,
and a. revel.
ebdtheque: a representative database of
comics. in international conference on document analysis
and recognition, 2013. 2

[20] a. guha, m. iyyer, and j. boyd-graber. a distorted skull
lies in the bottom center: identifying paintings from text de-
scriptions. in naacl human-computer question answer-
ing workshop, 2016. 8

[21] m. haurilet, m. tapaswi, z. al-halah, and r. stiefelhagen.
naming tv characters by watching and analyzing dialogs.
in ieee winter conference on applications of computer vi-
sion, 2016. 7

[22] a. k. n. ho, j.-c. burie, and j.-m. ogier. panel and speech
balloon extraction from comic books. in iapr international
workshop on document analysis systems, 2012. 3

[23] s. hochreiter and j. schmidhuber. long short-term memory.

neural computation, 1997. 6

[24] t.-h. k. huang, f. ferraro, n. mostafazadeh, i. misra,
a. agrawal, j. devlin, r. girshick, x. he, p. kohli, d. batra,
et al. visual storytelling. in conference of the north amer-
ican chapter of the association for computational linguis-
tics, 2016. 4, 7

[25] t. k. huang, f. ferraro, n. mostafazadeh,

i. misra,
a. agrawal, j. devlin, r. b. girshick, x. he, p. kohli, d. ba-
tra, c. l. zitnick, d. parikh, l. vanderwende, m. galley, and
m. mitchell. visual storytelling. in conference of the north
american chapter of the association for computational lin-
guistics, 2016. 2, 5

[26] m. iyyer, a. guha, s. chaturvedi, j. boyd-graber, and
h. daum  e iii. feuding families and former friends: un-
supervised learning for dynamic    ctional relationships.
in
conference of the north american chapter of the associa-
tion for computational linguistics, 2016. 8

[27] m. jaderberg, k. simonyan, a. vedaldi, and a. zisserman.
reading text in the wild with convolutional neural networks.
international journal of id161, 116(1), 2016. 3

[28] a. karpathy and f. li. deep visual-semantic alignments for
generating image descriptions. in ieee conference on com-
puter vision and pattern recognition, cvpr 2015, boston,
ma, usa, june 7-12, 2015, 2015. 7

[29] d. kingma and j. ba. adam: a method for stochastic opti-
mization. in proceedings of the international conference on
learning representations, 2014. 6

[30] s. kovanen and k. aizawa. a layered method for deter-
in international

mining manga text bubble reading order.
conference on image processing, 2015. 8

[31] r. krishna, y. zhu, o. groth, j. johnson, k. hata, j. kravitz,
s. chen, y. kalantidis, l.-j. li, d. a. shamma, m. bern-
stein, and l. fei-fei. visual genome: connecting language
and vision using crowdsourced dense image annotations.
2016. 7

[32] a. krizhevsky, i. sutskever, and g. e. hinton.

classi   cation with deep convolutional neural networks.

id163
in

[50] r. smith. an overview of the tesseract ocr engine. in inter-
national conference on document analysis and recognition,
2007. 3

[51] r. socher, q. v. le, c. d. manning, and a. y. ng. grounded
id152 for    nding and describing images
with sentences. transactions of the association for compu-
tational linguistics, 2014. 6

[52] t. tanaka, k. shoji, f. toyama, and j. miyamichi. lay-
out analysis of tree-structured scene frames in comic images.
in international joint conference on arti   cial intelligence,
2007. 2

[53] w. l. taylor. cloze procedure: a new tool for measuring
readability. journalism and mass communication quarterly,
30(4), 1953. 4

[54] o. vinyals, a. toshev, s. bengio, and d. erhan. show and
tell: a neural image caption generator. in id161
and pattern recognition, 2015. 7

[55] c. xiong, s. merity, and r. socher. dynamic memory net-
works for visual and textual id53. in proceed-
ings of the international conference of machine learning,
2016. 1

[56] k. xu, j. ba, r. kiros, k. cho, a. courville, r. salakhutdi-
nov, r. zemel, and y. bengio. show, attend and tell: neural
image id134 with visual attention. in proceed-
ings of the international conference of machine learning,
2015. 1, 7

[57] b. zhou, y. tian, s. sukhbaatar, a. szlam, and r. fer-
gus. simple baseline for visual id53. arxiv
preprint arxiv:1512.02167, 2015. 6

[58] c. l. zitnick, r. vedantam, and d. parikh. adopting abstract
images for semantic scene understanding. ieee trans. pat-
tern anal. mach. intell., 38(4):627   638, 2016. 8

proceedings of advances in neural information processing
systems, 2012. 1

[33] w. g. lehnert. plot units and narrative summarization. cog-

nitive science, 5(4), 1981. 8

[34] l. li, y. wang, z. tang, and l. gao. automatic comic page
segmentation based on polygon detection. multimedia tools
and applications, 69(1), 2014. 2

[35] t. lin, m. maire, s. j. belongie, l. d. bourdev, r. b. gir-
shick, j. hays, p. perona, d. ramanan, p. doll  ar, and c. l.
zitnick. microsoft coco: common objects in context. 2014.
7

[36] j. lu, j. yang, d. batra, and d. parikh. hierarchical
question-image co-attention for visual id53,
2016. 7

[37] m. malinowski, m. rohrbach, and m. fritz. ask your neu-
rons: a neural-based approach to answering questions about
images. in id161 and pattern recognition, 2015.
7

[38] y. matsui. challenge for manga processing: sketch-based
in proceedings of the 23rd annual acm

manga retrieval.
conference on multimedia, 2015. 8

[39] y. matsui, k. ito, y. aramaki, t. yamasaki, and k. aizawa.
sketch-based manga retrieval using manga109 dataset. arxiv
preprint arxiv:1510.04389, 2015. 2

[40] s. mccloud. understanding comics. harpercollins, 1994.

1, 3

[41] g. m. morton. a computer oriented geodetic data base and
international business

a new technique in    le sequencing.
machines co, 1966. 3

[42] n. mostafazadeh, n. chambers, x. he, d. parikh, d. ba-
tra, l. vanderwende, p. kohli, and j. allen. a corpus and
cloze evaluation for deeper understanding of commonsense
stories. in conference of the north american chapter of the
association for computational linguistics, 2016. 8

[43] x. pang, y. cao, r. w. lau, and a. b. chan. a robust panel
in proceedings of the acm

extraction method for manga.
international conference on multimedia, 2014. 2

[44] x. pang, y. cao, r. w. h. lau, and a. b. chan. a robust
in proceedings of the

panel extraction method for manga.
acm international conference on multimedia, 2014. 8

[45] s. ren, k. he, r. girshick, and j. sun. faster r-id98: to-
wards real-time id164 with region proposal net-
works. in proceedings of advances in neural information
processing systems, 2015. 3

[46] c. rigaud. segmentation and indexation of complex objects
in comic book images. phd thesis, university of la rochelle,
france, 2014. 8

[47] c. rigaud, j.-c. burie, j.-m. ogier, d. karatzas, and
j. van de weijer. an active contour model for speech bal-
in international conference on
loon detection in comics.
document analysis and recognition, 2013. 3

[48] c. rigaud, c. gu  erin, d. karatzas, j.-c. burie, and j.-m.
ogier. knowledge-driven understanding of images in comic
international journal on document analysis and
books.
recognition, 18(3), 2015. 2

[49] r. schank and r. abelson. scripts, plans, goals and un-
derstanding: an inquiry into human knowledge structures.
l. erlbaum, 1977. 8

