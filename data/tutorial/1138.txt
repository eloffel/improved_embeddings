id135 decoders in nlp:

integer programming, message passing, id209

andr  e martins

emnlp 2014: tutorials, doha, qatar, october 25, 2014

slides online at http://tiny.cc/lpdnlp.

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

1 / 149

id170 and nlp

id170: a machine learning framework for predicting
structured, constrained, and interdependent outputs

nlp deals with structured and ambiguous textual data (smith, 2011):

machine translation
id103
syntactic parsing
id29
information extraction
...

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

2 / 149

id33

map sentences to their syntactic structure.

a lexicalized syntactic formalism
grammar functions represented as lexical relationships (dependencies)

(eisner, 1996; mcdonald et al., 2005; nivre et al., 2006; koo et al., 2007)

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

3 / 149

*logicplaysaminimalrolehereid57

map a set of related documents to a brief summary.

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

4 / 149

id57

map a set of related documents to a brief summary.

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

4 / 149

current state of a   airs

1 id192 can deal with rich histories, but they are

suboptimal and su   er from error propagation

2 simple, tractable models permit exact decoding, but they make too

stringent factorization assumptions

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

5 / 149

current state of a   airs

1 id192 can deal with rich histories, but they are

suboptimal and su   er from error propagation

2 simple, tractable models permit exact decoding, but they make too

stringent factorization assumptions

we   d like fast predictors with global features and constraints, but how?

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

5 / 149

related recent tutorials

id209 and lagrangian relaxation for id136 in nlp
(rush & collins acl 2011)
id170s in nlp: constrained conditional models and
integer id135 (srikumar, goldwasser & roth naacl
2012)
variational id136 in structured nlp models (burkett & klein
naacl 2012)
structured belief propagation for nlp (groid113y & eisner acl 2014)

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

6 / 149

this tutorial: id135 decoders

we   ll provide a uni   ed view over these approaches (ilps, message-passing,
id209)
we   ll focus on map decoding, but touch brie   y on marginal decoding

we   ll illustrate with three applications:

1 turbo parsing
2 compressive summarization
3 joint coreference resolution and quote attribution

(companion software: ad3 toolkit)

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

7 / 149

outline

1 id170 and factor graphs

2 integer id135

3 message-passing algorithms

sum-product
max-product

4 id209

5 applications

6 conclusions

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

8 / 149

outline

1 id170 and factor graphs

2 integer id135

3 message-passing algorithms

sum-product
max-product

4 id209

5 applications

6 conclusions

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

9 / 149

id170

input set x
for each x     x: a large set of candidate outputs y(x )
a compatibility function fw (x , y ) induced by a model w
(linear model: fw (x , y ) = w>f (x , y ))

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

10 / 149

id170

input set x
for each x     x: a large set of candidate outputs y(x )
a compatibility function fw (x , y ) induced by a model w
(linear model: fw (x , y ) = w>f (x , y ))
training problem: learn the model w from data {hxi , yii}m
i=1
decoding problem (our focus):

by = arg max

y   y(x )

fw (x , y )

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

10 / 149

id170

input set x
for each x     x: a large set of candidate outputs y(x )
a compatibility function fw (x , y ) induced by a model w
(linear model: fw (x , y ) = w>f (x , y ))
training problem: learn the model w from data {hxi , yii}m
i=1
decoding problem (our focus):

by = arg max

y   y(x )

fw (x , y )

key assumption: fw decomposes into (overlapping) parts

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

10 / 149

three important questions

representation?
decoding/id136?
learning the parameters?

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

11 / 149

fw is a log-id203, factoring over emissions and transitions.

recap: id48
y

p(x , y ) =

| {z }
p(xi|yi )
emissions

}
{z
|
p(yi|yi   1)
transitions

i

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

12 / 149

fw is a log-id203, factoring over emissions and transitions.

recap: id48
y

p(x , y ) =

| {z }
p(xi|yi )
emissions

}
{z
|
p(yi|yi   1)
transitions

i

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

12 / 149

fw is a log-id203, factoring over emissions and transitions.

recap: id48
y

p(x , y ) =

| {z }
p(xi|yi )
emissions

}
{z
|
p(yi|yi   1)
transitions

i

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

12 / 149

fw is a log-id203, factoring over emissions and transitions.

recap: id48
y

y

y

p(x , y ) =

| {z }
p(xi|yi )
emissions

}
{z
|
p(yi|yi   1)
transitions

i

=

i

i

  i (yi )

  i,i   1(yi , yi   1)

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

12 / 149

recap: id48

representation? directed sequence model.
decoding/id136? viterbi/forward-backward algorithms.
learning the parameters? maximum likelihood (count and
normalize).

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

13 / 149

recap: id49

same factorization, but globally normalized.

p(y|x ) =

1

z (w, x )

exp

(cid:18)p

+p

|
}
i w>f i,i   1(x , yi , yi   1)

{z

edges

(cid:19)

|
}
i w>f i (x , yi )

{z

nodes

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

14 / 149

recap: id49

same factorization, but globally normalized.

p(y|x ) =

1

z (w, x )

exp

(cid:18)p

+p

|
}
i w>f i,i   1(x , yi , yi   1)

{z

edges

(cid:19)

|
}
i w>f i (x , yi )

{z

nodes

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

14 / 149

recap: id49

same factorization, but globally normalized.

p(y|x ) =

1

z (w, x )

exp

(cid:18)p

+p

|
}
i w>f i,i   1(x , yi , yi   1)

{z

edges

(cid:19)

|
}
i w>f i (x , yi )

{z

nodes

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

14 / 149

recap: id49

same factorization, but globally normalized.

p(y|x ) =

1

z (w, x )

  i (yi )

    y

i

exp

y

i

(cid:18)p

|
}
i w>f i (x , yi )

{z

nodes
  i,i   1(yi , yi   1)

+p

|
}
i w>f i,i   1(x , yi , yi   1)

{z

edges

(cid:19)

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

14 / 149

recap: id49

representation? undirected sequence model.
decoding/id136? viterbi/forward-backward algorithms.
learning the parameters? maximum conditional likelihood
(id76).

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

15 / 149

id114

id48s and crfs are two instances of id114.
in general, id114 come in two    avours:

directed (id110s)
undirected (markov networks)

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

16 / 149

id110s

useful to express causality relations.
factors are id155 tables.

y

i

p(y ) =

p(yi|parents(yi ))

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

17 / 149

id110s

useful to express causality relations.
factors are id155 tables.

y

i

p(y ) =

p(yi|parents(yi ))

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

17 / 149

markov networks
useful to express correlations between variables.
factors correspond to cliques of the graph.

p(y )     y

s   cliques(g)

  s (ys )

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

18 / 149

markov networks
useful to express correlations between variables.
factors correspond to cliques of the graph.

p(y )     y

s   cliques(g)

  s (ys )

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

18 / 149

conditional independence

id114 are a great tool for modeling conditional independence
they link properties of the id203 distribution with properties of the
graph (reachability, d-separation, etc.)
lots of literature about this: pearl (1988); lauritzen (1996); koller and
friedman (2009)

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

19 / 149

an intermediate representation: factor graph

a bipartite graph with variable nodes and factor nodes
it makes explicit the factors of the distribution

p(y )     y
|

i

  i (yi )

{z

}

   y
|

s

  s (ys )

{z

}

unary potentials

higher-order potentials

with unary potentials only, all variables would be independent
higher-order potentials can model correlations, impose soft/hard
constraints, etc.

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

20 / 149

example: low-density parity check codes

a message is transmitted through a noisy channel, corrupting some bits
redundancy can help decoding the message, e.g. via additional parity
check bits that can detect/correct errors (error-correcting codes)
high-level idea: increase redundancy to build more accurate decoders

(adapted from mackay 2003.)

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

21 / 149

id136/decoding

   y
|

i

   y
|

s

1

two decoding problems:

{z

p  (y|x ) =

{z

  i (yi )

  s (ys )

z (  , x )

unary potentials

}
map decoding: compute by = arg maxy
marginal decoding: compute every p  (yi|x ) and p  (ys|x ); and
evaluate the partition function z (  , x )

higher-order potentials

p  (y|x )

}

sometimes easy, in general intractable...

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

22 / 149

when is decoding easy?

independent variables (trivial)
sequence models (viterbi, forward-backward)
id114 without cycles (variable elimination, belief
propagation)
id114 with low treewidth (junction tree algorithm)

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

23 / 149

when is decoding easy?

independent variables (trivial)
sequence models (viterbi, forward-backward)
id114 without cycles (variable elimination, belief
propagation)
id114 with low treewidth (junction tree algorithm)

in general, for graphs with cycles, map decoding is np-hard and
marginal decoding is #p-hard

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

23 / 149

when is decoding easy?

independent variables (trivial)
sequence models (viterbi, forward-backward)
id114 without cycles (variable elimination, belief
propagation)
id114 with low treewidth (junction tree algorithm)

in general, for graphs with cycles, map decoding is np-hard and
marginal decoding is #p-hard
note: tractability depends not only on the topology, but also on the
potentials

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

23 / 149

example: ising and potts models

ising/potts grid

ernst ising, 1900   1998

ren potts, 1925   2005

all factors are pairwise, variables are binary (ising) or multi-class (potts)

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

24 / 149

example: ising and potts models

ising/potts grid

ernst ising, 1900   1998

ren potts, 1925   2005

all factors are pairwise, variables are binary (ising) or multi-class (potts)
map decoding is tractable for attractive ising models (i.e. ising models
with supermodular log-potentials):

log   ij (1, 1) + log   ij (0, 0)     log   ij (0, 1) + log   ij (1, 0)

good approximations for attractive potts models

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

24 / 149

example: ising and potts models

ising/potts grid

ernst ising, 1900   1998

ren potts, 1925   2005

all factors are pairwise, variables are binary (ising) or multi-class (potts)
map decoding is tractable for attractive ising models (i.e. ising models
with supermodular log-potentials):

log   ij (1, 1) + log   ij (0, 0)     log   ij (0, 1) + log   ij (1, 0)

good approximations for attractive potts models
... but the general case is np-hard and hard to approximate
andr  e martins (priberam/it)
http://tiny.cc/lpdnlp

lp decoders in nlp

24 / 149

example: skip-chain crfs

skip-chain crfs are useful to model long-range dependencies

skip-chains introduce cycles, making decoding more expensive
we could write this information in the    state    and still decode with
id145, but that would blow up the number of states

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

25 / 149

beyond id114

some nlp problems (e.g. parsing) require representations beyond
id114
id145 algorithms (cky, inside-outside) still work for
those representations
example: case-factor diagrams (mcallester et al., 2008)
other problems (e.g. matching, spanning trees) can be solved with
combinatorial algorithms not related with id145
all these can still be represented as gms by    generalizing    the
notion of factor

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

26 / 149

factors as machines

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

27 / 149

factors as machines

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

27 / 149

three kinds of factors

let n(s) denote the set of variables that are neighbors of factor s.
(its cardinality |n(s)| is called the degree of s.)
1 dense factors:   s (ys ) has all o(exp(|n(s)|)) degrees of freedom
2 structured factors:   s (ys ) has internal structure
3 hard constraint factors:

( 1,

  s (ys ) :=

if ys     ys
0, otherwise.

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

28 / 149

examples of structured factors

a factor for bipartite matching (duchi et al., 2007)
combining a sequential model (pos tagger) with a pid18 (rush
et al., 2010)
combining id35 parsing and id55 (auli and lopez, 2011)
id33 with head automata (smith and eisner, 2008;
koo et al., 2010)
handling string-valued variables with factors that are    nite state
transducers (dreyer and eisner, 2009)
inversion transduction grammar constraint (burkett and klein, 2012)

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

29 / 149

examples of hard constraint factors

logic factors: can express arbitrary fol constraints

applications: markov logic networks (richardson and domingos,
2006), constrained conditional models (roth and yih, 2004)

knapsack factors: can express budget constraints

applications: summarization, diversity problems,...

(martins et al., 2011b; almeida and martins, 2013; martins et al., 2014)

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

30 / 149

xororor-outknapsackandr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

31 / 149

approximate decoding

what to do when exact decoding is intractable?

sampling methods (mcmc, etc.)
mean    eld algorithms
lp relaxations
message-passing
id209

we   ll highlight connections between several of these methods.

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

32 / 149

approximate decoding

what to do when exact decoding is intractable?

sampling methods (mcmc, etc.)
mean    eld algorithms
lp relaxations
message-passing
id209

we   ll highlight connections between several of these methods.

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

32 / 149

global/local decoding

   local    denotes independent problems within the scope of each factor
   global    involves a global assignment of variables, consistent across factors
key idea:    glue    the local evidence at the factors to obtain a global
assignment
our assumption: local decoding is easy, for every factor
we want to build a good (approximate) global decoder by invoking
the local decoders.

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

33 / 149

what kind of local decoding do we need?

algorithm
sum-prod. bp (pearl, 1988)
trbp (wainwright et al., 2005)
norm-product bp (hazan and shashua, 2010)
max-prod. bp (pearl, 1988)
trw-s (kolmogorov, 2006)
mplp (globerson and jaakkola, 2008)
psdd (komodakis et al., 2007)
accelerated dd (jojic et al., 2010)
ad3 (martins et al., 2011a)

local operation

marginals
marginals
marginals

max-marginals
max-marginals
max-marginals

map

marginals
qp/map

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

34 / 149

outline

1 id170 and factor graphs

2 integer id135

3 message-passing algorithms

sum-product
max-product

4 id209

5 applications

6 conclusions

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

35 / 149

id135

(kantorovich, 1940; dantzig, 1947)

maxz
s.t.

s>z
linear objective
>z     bi , i = 1, . . . , n. linear constraints
ai

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

36 / 149

id135

(kantorovich, 1940; dantzig, 1947)

maxz
s.t.

s>z
linear objective
>z     bi , i = 1, . . . , n. linear constraints
ai

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

36 / 149

id135

(kantorovich, 1940; dantzig, 1947)

maxz
s.t.

s>z
linear objective
>z     bi , i = 1, . . . , n. linear constraints
ai

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

36 / 149

id135

(kantorovich, 1940; dantzig, 1947)

maxz
s.t.

s>z
linear objective
>z     bi , i = 1, . . . , n. linear constraints
ai

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

36 / 149

id135

(kantorovich, 1940; dantzig, 1947)

maxz
s.t.

s>z
linear objective
>z     bi , i = 1, . . . , n. linear constraints
ai

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

36 / 149

id135

(kantorovich, 1940; dantzig, 1947)

if feasible and bounded, the solution is always attained at a vertex
can be solved in polynomial time (khachiyan, 1980)
lots of o   -the-shelf solvers (cplex, gurobi, glpk, lp solve, etc.)

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

37 / 149

integer id135

maxz
s.t.

s>z
linear objective
>z     bi , i = 1, . . . , n, linear constraints
ai
z integer.

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

38 / 149

integer id135

maxz
s.t.

s>z
linear objective
>z     bi , i = 1, . . . , n, linear constraints
ai
z integer.

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

38 / 149

integer id135

maxz
s.t.

s>z
linear objective
>z     bi , i = 1, . . . , n, linear constraints
ai
z integer.

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

38 / 149

integer id135

maxz
s.t.

s>z
linear objective
>z     bi , i = 1, . . . , n, linear constraints
ai
z integer.

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

38 / 149

integer id135

maxz
s.t.

s>z
linear objective
>z     bi , i = 1, . . . , n, linear constraints
ai
z integer.

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

38 / 149

integer id135

in general, np-hard (karp, 1972)
existing solvers are e   ective for small instances, but don   t scale
lp relaxation: drops the integer constraints

gives an upper bound of the solution of the ilp
a common    rst step in exact algorithms (branch-and-bound, cutting
plane, branch-and-cut)

here   s a very simple approximate algorithm:

1 solve the lp relaxation
2 if the solution is integer, then it is the solution of the ilp
3 otherwise, apply a rounding heuristic (problem-dependent)

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

39 / 149

two representations of polytopes

intersection of half-spaces (h-representation) or convex hull of a set of
vertices (v-representation)

to call a solver, we need to specify a concise h-representation
however, it may be di   cult or impossible to obtain one if all we have is a
v-representation
we next show how this relates to map decoding...

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

40 / 149

structured outputs as bit-vectors

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

41 / 149

structured outputs as bit-vectors

one indicator pi (yi ) per each variable state

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

41 / 149

structured outputs as bit-vectors

one indicator pi (yi ) per each variable state

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

41 / 149

structured outputs as bit-vectors

one indicator pi (yi ) per each variable state
one indicator qs (ys ) per each factor con   guration

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

41 / 149

structured outputs as bit-vectors

one indicator pi (yi ) per each variable state
one indicator qs (ys ) per each factor con   guration

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

41 / 149

structured outputs as bit-vectors

one indicator pi (yi ) per each variable state
one indicator qs (ys ) per each factor con   guration

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

41 / 149

structured outputs as bit-vectors

one indicator pi (yi ) per each variable state
one indicator qs (ys ) per each factor con   guration
overall: each global output y     y(x ) is mapped to a bit-vector

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

41 / 149

structured outputs as bit-vectors

one indicator pi (yi ) per each variable state
one indicator qs (ys ) per each factor con   guration
overall: each global output y     y(x ) is mapped to a bit-vector

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

41 / 149

structured outputs as bit-vectors

one indicator pi (yi ) per each variable state
one indicator qs (ys ) per each factor con   guration
overall: each global output y     y(x ) is mapped to a bit-vector
note: not all bit vectors are valid (they must be consistent)

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

41 / 149

marginal polytope (wainwright and jordan, 2008)

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

42 / 149

marginal polytope (wainwright and jordan, 2008)

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

42 / 149

marginal polytope (wainwright and jordan, 2008)

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

42 / 149

marginal polytope (wainwright and jordan, 2008)

vertices of marg(g) correspond to outputs y(x )
points of marg(g) correspond to realizable marginals (more later)

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

42 / 149

marginal polytope (wainwright and jordan, 2008)

vertices of marg(g) correspond to outputs y(x )
points of marg(g) correspond to realizable marginals (more later)

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

42 / 149

marginal polytope (wainwright and jordan, 2008)

vertices of marg(g) correspond to outputs y(x )
points of marg(g) correspond to realizable marginals (more later)
this is a v-representation, what about an h-representation?

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

42 / 149

h-representation with integer constraints

x

ys

in general, there   s no concise h-representation for marg(g)
... but we can represent its vertices if integer constraints are permitted:

qs (ys ) = 1,

pi (yi ) =

qs (ys )     0,
x

qs (ys ),

ys   yi

   ys     ys
   i     n(s)

(id172)

(marginalization)

q is integer

(integer constraints)

this will open the door for formulating map decoding as an ilp.

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

43 / 149

map decoding as an ilp

recall the map decoding problem:

by = arg max

y   y(x )

y

p  (y|x )
  1
 
x
z (  , x )
 
i
  i (yi ) +

y

s

  i (yi )

x

s

  s (ys ),

  s (ys )

= arg max

y   y(x )  

= arg max
y   y(x )

i

where   i (yi ) := log   i (yi ) and   s (ys ) := log   s (ys )
we can rewrite this as an ilp:

maximize x

x

i

yi

subject to (p, q)     marg(g)

x

x

s

ys

  i (yi )pi (yi ) +

  s (ys )qs (ys )

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

44 / 149

local polytope

obtained by relaxing the integer constraints
regard pi and qs as id203 distributions that must be locally

consistent:x

qs (ys ) = 1,

ys

pi (yi ) =

qs (ys )     0,
x

qs (ys ),

ys   yi

   ys     ys
   i     n(s)

(id172)

(marginalization)

q is integer

(integer constraints)

the feasible points are pseudo-marginals (not necessarily valid marginals)

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

45 / 149

local and marginal polytopes

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

46 / 149

local and marginal polytopes

local(g) is an outer bound of marg(g)
it contains all the integer vertices of marg(g), plus spurious
fractional vertices
if the graph has no cycles, then local(g) = marg(g)

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

46 / 149

lp-map decoding

solves a linear relaxation of map decoding, replacing marg(g) by
local(g):

maximize x

x

  i (yi )pi (yi ) +
subject to (p, q)     local(g)

yi

i

x

x

s

ys

  s (ys )qs (ys )

if the solution is integer, we solved the problem exactly; otherwise, apply a
rounding heuristic
runtime is polynomial, but the procedure is only approximate.

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

47 / 149

what about hard constraint factors?

logic and knapsack/budget constraints can also be expressed linearly

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

48 / 149

xororor-outknapsacklogic/budget constraints

assume z1, z2, . . .     {0, 1} (binary variables)

condition
implication
negation
or
xor
or-out

and-out

budget
knapsack

statement
if z1 then z2
z1 i      z2
z1 or z2 or z3
z1 xor z2 xor z3
z12 = z1     z2
z12 = z1     z2

at most b active units p
at most b total weight p

constraint
z1     z2
z1 = 1     z2
z1 + z2 + z3     1
z1 + z2 + z3 = 1
z12     z1, z12     z2,
z12     z1 + z2
z12     z1, z12     z2,
z12     z1 + z2     1
i zi     b
i wizi     b

more complex expressions via composition and de morgan   s laws

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

49 / 149

summing up ilps

map decoding can be expressed as an integer linear program (ilp)
logic constraints can be incorporated easily
structured factors are harder (they need to be disassembled)
the ilp can be relaxed for approximate decoding (lp-map)
geometrically: an outer bound of the marginal polytope
the relaxation is tight if the graph g does not have cycles
disadvantage: an o   -the-shelf lp solver won   t exploit the
modularity of the problem
algorithms that exploit the structure of the lp will be the topic
of the remaining sections

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

50 / 149

outline

1 id170 and factor graphs

2 integer id135

3 message-passing algorithms

sum-product
max-product

4 id209

5 applications

6 conclusions

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

51 / 149

motivating example: counting soldiers

(adapted from mackay 2003 and gorid113y & eisner acl   14 tutorial.)

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

52 / 149

motivating example: counting soldiers

(adapted from mackay 2003 and gorid113y & eisner acl   14 tutorial.)

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

52 / 149

motivating example: counting soldiers

(adapted from mackay 2003 and gorid113y & eisner acl   14 tutorial.)

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

52 / 149

motivating example: counting soldiers

(adapted from mackay 2003 and gorid113y & eisner acl   14 tutorial.)

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

52 / 149

motivating example: counting soldiers

(adapted from mackay 2003 and gorid113y & eisner acl   14 tutorial.)

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

52 / 149

motivating example: counting soldiers

(adapted from mackay 2003 and gorid113y & eisner acl   14 tutorial.)

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

52 / 149

motivating example: counting soldiers

(adapted from mackay 2003 and gorid113y & eisner acl   14 tutorial.)

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

52 / 149

outline

1 id170 and factor graphs

2 integer id135

3 message-passing algorithms

sum-product
max-product

4 id209

5 applications

6 conclusions

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

53 / 149

sum-product belief propagation

recall that p  (y|x ) :=
alternate between computing two kinds of messages:

z (  , x )

i

  s (ys )

  y

1

  i (yi )   y
y

s

variable-to-factor: mi   s (yi ) =   i (yi )

ns0   i (yi )

factor-to-variable: ns   i (yi ) =

y

s0   n(i)\{s}

  s (ys )

x

ys   yi

j   n(s)\{i}

mj   s (yj )

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

54 / 149

beliefs

given the messages, we compute local beliefs:

variable beliefs:
pi (yi )       i (yi )

y

s   n(i)

ns   i (yi )

factor beliefs:
qs (ys )       s (ys )

y

i   n(s)

mi   s (yi )

if the graph has no cycles, these beliefs converge to the true marginals

pi (yi )     p  (yi|x ),

qs (ys )     p  (ys|x )

otherwise: loopy bp (later)

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

55 / 149

belief propagation as calibration

y

y

variable-to-factor messages:

mi   s (yi ) =   i (yi )

ns0   i (yi ) =

pi (yi )
ns   i (yi )

s0   n(i)\{s}

factor-to-variable messages:

ns   i (yi ) =

  s (ys )

x

ys   yi

p
ys   yi qs (ys )
mi   s (yi )

mj   s (yj ) =

calibration equations (attained at convergence):

j   n(s)\{i}

x

ys   yi

pi (yi ) =

qs (ys )

punchline: to run sum-product bp, we only need local marginals

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

56 / 149

what kind of local decoding do we need?

algorithm
sum-prod. bp (pearl, 1988)
trbp (wainwright et al., 2005)
norm-product bp (hazan and shashua, 2010)
max-prod. bp (pearl, 1988)
trw-s (kolmogorov, 2006)
mplp (globerson and jaakkola, 2008)
psdd (komodakis et al., 2007)
accelerated dd (jojic et al., 2010)
ad3 (martins et al., 2011a)

local operation

marginals
marginals
marginals

max-marginals
max-marginals
max-marginals

map

marginals
qp/map

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

57 / 149

loopy belief propagation

what if the graph has cycles?

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

58 / 149

loopy belief propagation

what if the graph has cycles?
we   ll see that marginal decoding corresponds to optimizing a free energy
objective over the marginal polytope
sum-product    loopy    bp entails two approximations:

1 replaces marg(g) by local(g)
2 optimizes a bethe free energy approximation

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

58 / 149

step #1: dual parametrization

for any   , there are marginals p, q in marg(g) that parametrize p  
e.g. if the graph has no cycles:

p  (y|x ) =

1

y

  i (yi )   y
pi (yi )1   |n(i)|   y

i

z (  , x )

y

  s (ys )

s
qs (ys )

s

=
:= pp,q(y|x )

i

(* next slide)

therefore: a distribution can be represented as a point in marg(g)
   := log(  ) are called canonical parameters, and (p, q) mean parameters

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

59 / 149

(*) derivation of dual parametrization

assume a tree-shaped bayes net (each variable i has a single parent   i)

p(y ) = p(y0)

= p(y0)

i6=0

i6=0

y
p(yi|y  i )
y
p(yi , y  i )
p(y0)q
p(y  i )
q
s p(ys)
p(y0)q
j p(yj )|i:j=  i|
p(y0)|n(0)|q
q
q
s p(ys)
pi (yi )1   |n(i)|   y
y
j p(yj )|n(j)|   1

i

=

=

=

=

s p(ys)
j6=0 p(yj )|n(j)   1|

qs (ys ).

s

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

60 / 149

id178 of a id203 distribution: h(p) =    x

step #2: id178 and log-partition function
p(y ) log p(y )

y

de   nition: the fenchel dual of a convex function f : rd     r     {+   } is
the convex function f ? : rd     r     {+   } de   ned pointwise as
f ?(v) := sup
u

(cid:16)v>u     f (u)
(cid:17)

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

61 / 149

id178 of a id203 distribution: h(p) =    x

step #2: id178 and log-partition function
p(y ) log p(y )

y

(cid:16)v>u     f (u)
(cid:17)

de   nition: the fenchel dual of a convex function f : rd     r     {+   } is
the convex function f ? : rd     r     {+   } de   ned pointwise as
f ?(v) := sup
u
theorem (i): the log-partition function log z (  ) and the negative
id178    h(pp,q) are fenchel dual:
x
|

>qs + h(pp,q)
}
,

(p,q)   marg(g)

log z (  ) =

>pi +

x

{z

max

  s

  i

s

i

(negative) variational free energy

this underlies the well-known duality between maximum likelihood in
id148 and maximum id178.

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

61 / 149

step #3: loopy bp as variational id136

theorem (ii): the maximizers (p   , q   ) are the true marginals of p  :
>qs + h(pp,q)

(p   , q   ) = arg max(p,q)   marg(g)

>pi +

x

x

  s

  i

i

s

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

62 / 149

step #3: loopy bp as variational id136

theorem (ii): the maximizers (p   , q   ) are the true marginals of p  :
>qs + h(pp,q)

(p   , q   ) = arg max(p,q)   marg(g)

>pi +

x

x

  s

  i

i

s

drawback: in general, marg(g) and h(pp,q) are both intractable

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

62 / 149

step #3: loopy bp as variational id136

theorem (ii): the maximizers (p   , q   ) are the true marginals of p  :
>qs + h(pp,q)

(p   , q   ) = arg max(p,q)   marg(g)

>pi +

x

x

  s

  i

i

s

drawback: in general, marg(g) and h(pp,q) are both intractable
yedidia et al. (2001) showed that loopy bp entails two approximations:

1 replace marg(g) by local(g)
2 approximate h(pp,q) by the bethe id178 hbethe(pp,q)

both are exact when the graph does not have cycles

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

62 / 149

bethe id178 approximation

derived by    pretending    the graph has no cycles
we have seen

p  (y|x )    y

pi (yi )1   |n(i)|   y

qs (ys )

i

s

from which we can derive
h(pp,q)     hbethe(pp,q)

x
(1     |n(i)|)hi (pi ) +
hi (pi ) =    x

s
a linear combination of local entropies:

pi (yi ) log pi (yi ), hs (qs ) =    x

x

=

i

yi

hs (qs )

hans bethe, 1906   2005

qs (ys ) log qs (ys )

ys

not concave in general!

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

63 / 149

geometric illustration

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

64 / 149

geometric illustration

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

64 / 149

geometric illustration

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

64 / 149

geometric illustration

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

64 / 149

geometric illustration

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

64 / 149

geometric illustration

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

64 / 149

geometric illustration

if loopy bp converges, it reaches a stationary point of the
approximate variational problem
hbethe(pp,q) is non-concave in general     local minima

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

64 / 149

summary of loopy bp

advantages:

simple to implement
handles structured and logic factors (only need to compute local
marginals)
often works well in practice (if cycles are not very in   uential)
often yields a reasonable approximation of log z and h

disadvantages:

doesn   t give an upper/lower bound of log z and h
id178 approximation is not concave (local minima)
may not converge
the    nal beliefs may not be realizable marginals

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

65 / 149

tree reweighted bp (wainwright et al., 2005)

key idea: cover the graph with a set of trees

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

66 / 149

tree reweighted bp (wainwright et al., 2005)

key idea: cover the graph with a set of trees

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

66 / 149

tree reweighted bp (wainwright et al., 2005)

key idea: cover the graph with a set of trees

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

66 / 149

tree reweighted bp (wainwright et al., 2005)

key idea: cover the graph with a set of trees

count the appearance id203 cis > 0 of each edge

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

66 / 149

tree reweighted bp (wainwright et al., 2005)

key idea: cover the graph with a set of trees

count the appearance id203 cis > 0 of each edge

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

66 / 149

tree reweighted bp (wainwright et al., 2005)

key idea: cover the graph with a set of trees

count the appearance id203 cis > 0 of each edge
this results in a convex upper bound of    h and log z:
x

x
(1    p

htrbp(pp,q) =

s   n(i) cis )hi (pi ) +

i

hs (qs )

s

(note: if all cis = 1 this would revert to the bethe approximation)

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

66 / 149

trbp messages

variable-to-factor messages:

factor-to-variable messages:

mi   s (yi ) =

ns   i (yi ) =

x

ys   yi

s0   n(i) ncis0 (yi )
s0   i
ns   i (yi )

j   n(s) mcjs
mi   s (yi )

j   s (yj )

variable beliefs:

factor beliefs:

pi (yi )       i (yi )

nciss   i (yi )

qs (ys )       s (ys )

mcisi   s (yi )

  i (yi )q
  s (ys )q
y
y

s   n(i)

i   n(s)

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

67 / 149

summary of trbp

advantages:

still simple to implement
id178 approximation is concave (no local minima)
gives an upper bound on    h and log z
lots of knobs (the appearance probabilities)

disadvantages:

lots of knobs (the appearance probabilities)
typically it   s a very loose bound
may not converge (but in practice always does, with dampening)
the    nal beliefs may not be realizable marginals

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

68 / 149

what kind of local decoding do we need?

algorithm
sum-prod. bp (pearl, 1988)
trbp (wainwright et al., 2005)
norm-product bp (hazan and shashua, 2010)
max-prod. bp (pearl, 1988)
trw-s (kolmogorov, 2006)
mplp (globerson and jaakkola, 2008)
psdd (komodakis et al., 2007)
accelerated dd (jojic et al., 2010)
ad3 (martins et al., 2011a)

local operation

marginals
marginals
marginals

max-marginals
max-marginals
max-marginals

map

marginals
qp/map

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

69 / 149

norm-product bp (hazan and shashua, 2010)

subsumes loopy bp and trbp
relies on a convex approximation to the id178 using counting numbers
ci     0 and cs > 0 (in its simpler variant)

hnpbp(pp,q) =

cihi (pi ) +

cshs (qs )

x

i

x

s

!1/p

 x

i

|xi|p

messages will become norms
recall the de   nition of    p-norm: kxkp =

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

70 / 149

variable-to-factor messages:

mi   s (yi ) =

factor-to-variable messages:

ns   i (yi ) =

variable beliefs:

pi (yi )    

factor beliefs:

(cid:17)cs /(ci +p

y

s0   n(i) ns0   i (yi )
ns   i (yi )

npbp messages
(cid:16)
  i (yi )q
         x
        s (ys )
        i (yi )
y
        s (ys )

ns   i (yi )

j   n(s)\{i}

y

s   n(i)

ys   yi

i   n(s)

mj   s (yj )

      1/(ci +p
      cs

mi   s (yi )

qs (ys )    

s0   n(i)

c0
s )

      1/cs         cs

s0   n(i)

c0
s )

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

71 / 149

summary of npbp

advantages:

still simple to implement
id178 approximation is concave (no local minima)
always converges (primal-dual block ascent)
lots of knobs (the counting numbers)

disadvantages:

lots of knobs (the counting numbers)
messages are not computed in parallel (otherwise, may not converge)
the    nal beliefs may not be realizable marginals

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

72 / 149

what kind of local decoding do we need?

algorithm
sum-prod. bp (pearl, 1988)
trbp (wainwright et al., 2005)
norm-product bp (hazan and shashua, 2010)
max-prod. bp (pearl, 1988)
trw-s (kolmogorov, 2006)
mplp (globerson and jaakkola, 2008)
psdd (komodakis et al., 2007)
accelerated dd (jojic et al., 2010)
ad3 (martins et al., 2011a)

local operation

marginals
marginals
marginals

max-marginals
max-marginals
max-marginals

map

marginals
qp/map

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

73 / 149

outline

1 id170 and factor graphs

2 integer id135

3 message-passing algorithms

sum-product
max-product

4 id209

5 applications

6 conclusions

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

74 / 149

zero-limit temperature

de   ne z  where   is a temperature parameter:

       x

y   y(x )

i

y

  i (yi )1/ y

s

       

  s (ys )1/ 

z (  , x ) =

if   = 1, this becomes the partition function z (  , x )
if       0, this becomes the mode of p  (y|x )
note that z (  , x ) = z (  1/ , x )  for any  , i.e., z  can be computed by
the same means as the partition function by scaling the potentials
by choosing a small enough  , any sum-product message-passing
algorithm can be used to approximate the map
there is a trade-o    between precision and numerical stability

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

75 / 149

max-product belief propagation

for map decoding instead of marginal decoding

only change: factor-to-variable messages (max instead ofx)

        s (ys )

y

ns   i (yi ) = max
ys   yi

mj   s (yj )

j   n(s)\{i}

       =

maxys   yi qs (ys )

mi   s (yi )

if the graph has no cycles, beliefs will converge to max-marginals:

pi (yi )     maxy   yi

p  (y|x ),

qs (ys )     max
y   ys

p  (y|x )

decoding the best max-marginal at each variable node gives the map
with cycles: not guaranteed to converge, and even if it does, no
relation with lp-map

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

76 / 149

what kind of local decoding do we need?

algorithm
sum-prod. bp (pearl, 1988)
trbp (wainwright et al., 2005)
norm-product bp (hazan and shashua, 2010)
max-prod. bp (pearl, 1988)
trw-s (kolmogorov, 2006)
mplp (globerson and jaakkola, 2008)
psdd (komodakis et al., 2007)
accelerated dd (jojic et al., 2010)
ad3 (martins et al., 2011a)

local operation

marginals
marginals
marginals

max-marginals
max-marginals
max-marginals

map

marginals
qp/map

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

77 / 149

trw-s (kolmogorov, 2006)

same rationale as sum-product trbp: cover the graph with spanning
trees, and compute messages using edge appearance probabilities
only di   erences:

replacex with max

messages need to be computed sequentially for convergence

as max-product loopy bp, all is required is to compute local max-marginals
under mild assumptions, gives the solution of lp-map

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

78 / 149

what kind of local decoding do we need?

algorithm
sum-prod. bp (pearl, 1988)
trbp (wainwright et al., 2005)
norm-product bp (hazan and shashua, 2010)
max-prod. bp (pearl, 1988)
trw-s (kolmogorov, 2006)
mplp (globerson and jaakkola, 2008)
psdd (komodakis et al., 2007)
accelerated dd (jojic et al., 2010)
ad3 (martins et al., 2011a)

local operation

marginals
marginals
marginals

max-marginals
max-marginals
max-marginals

map

marginals
qp/map

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

79 / 149

max-product lp (globerson and jaakkola, 2008)

derived by writing the dual of lp-map, and solving it with a block
coordinate descent algorithm
the message updates need to be computed in a sequential schedule
progress in the dual objective is monotonic
drawback: since the dual is non-smooth, we may get stuck at a
suboptimal point

(from bertsekas et al. (1999))

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

80 / 149

mplp messages

variable-to-factor messages:

mi   s (yi ) =   i (yi )

ns0   i (yi )

s0   n(i)\{s}

y
        s (ys )1/|n(s)| y

j   n(s)
mi   s (yi )

factor-to-variable messages:

max
ys   yi

ns   i (yi ) =

      

mj   s (yj )1/|n(s)|

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

81 / 149

summary of mplp

advantages:

very simple to implement
handles structured and logic factors (only need to compute local
max-marginals)
monotonically improves the dual
no parameters to tune

disadvantages:

can get stuck at a suboptimal solution (general problem with
nonsmooth coordinate ascent)
messages are not computed in parallel (otherwise, may not converge)

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

82 / 149

what kind of local decoding do we need?

algorithm
sum-prod. bp (pearl, 1988)
trbp (wainwright et al., 2005)
norm-product bp (hazan and shashua, 2010)
max-prod. bp (pearl, 1988)
trw-s (kolmogorov, 2006)
mplp (globerson and jaakkola, 2008)
psdd (komodakis et al., 2007)
accelerated dd (jojic et al., 2010)
ad3 (martins et al., 2011a)

local operation

marginals
marginals
marginals

max-marginals
max-marginals
max-marginals

map

marginals
qp/map

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

83 / 149

summing up message-passing

bp algorithms and their variants can be used both for map and
marginal decoding
they need to compute local marginals (sum-product) or
max-marginals (max-product)
always exact if the graph has no cycles; approximate otherwise
they correspond to minimizing an energy approximation over the
local polytope
some variants do convex approximations or compute upper bounds
two views of map decoding: (1) the near-zero temperature limit of
marginal decoding; (2) a non-smooth optimization problem

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

84 / 149

outline

1 id170 and factor graphs

2 integer id135

3 message-passing algorithms

sum-product
max-product

4 id209

5 applications

6 conclusions

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

85 / 149

id209

old idea in optimization (dantzig and wolfe, 1960; everett iii, 1963)
first proposed by komodakis et al. (2007) in id161
introduced in nlp by rush et al. (2010) for model combination
successful in syntax, semantics, mt: koo et al. (2010); chang and
collins (2011); martins et al. (2011b); almeida et al. (2014); martins
and almeida (2014), and many others.

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

86 / 149

id209

old idea in optimization (dantzig and wolfe, 1960; everett iii, 1963)
first proposed by komodakis et al. (2007) in id161
introduced in nlp by rush et al. (2010) for model combination
successful in syntax, semantics, mt: koo et al. (2010); chang and
collins (2011); martins et al. (2011b); almeida et al. (2014); martins
and almeida (2014), and many others.

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

86 / 149

id209

old idea in optimization (dantzig and wolfe, 1960; everett iii, 1963)
first proposed by komodakis et al. (2007) in id161
introduced in nlp by rush et al. (2010) for model combination
successful in syntax, semantics, mt: koo et al. (2010); chang and
collins (2011); martins et al. (2011b); almeida et al. (2014); martins
and almeida (2014), and many others.

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

86 / 149

id209

old idea in optimization (dantzig and wolfe, 1960; everett iii, 1963)
first proposed by komodakis et al. (2007) in id161
introduced in nlp by rush et al. (2010) for model combination
successful in syntax, semantics, mt: koo et al. (2010); chang and
collins (2011); martins et al. (2011b); almeida et al. (2014); martins
and almeida (2014), and many others.

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

86 / 149

recap: lp-map

recall the lp-map problem:

maximize x

x
( qs        |ys|,    s

>pi +

  i

s

i

subject to

pi = misqs ,    i, s.

>qs

  s

(local polytope)

x

ys   yi

qs (ys )

matrix mis     {0, 1}|yi|  |ys| represents the constraints pi (yi ) =

we   ll reformulate this problem by:

1 introducing copy variables qis = pi
2 de   ning   is :=   i /|n(i)|

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

87 / 149

the problem becomes:

reformulation of lp-map
(cid:17)

i   n(s)   is

>qis

maximize x

s

  s

(cid:16)
>qs +p
                qs        |ys|,    s
(cid:16)
>qs +p

  s

qis = misqs ,    i, s
qis = pi ,    i, s.

i   n(s)   is

subject to

(local polytope)

by introducing lagrange multipliers for the last constraints, we get the
following lagrangian function:

(cid:17)

x

is

>qis

+

>(pi     qis )

  is

l(p, q,   ) =

x

s

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

88 / 149

the dual problem is

minimize x

gs (  )

s

where the gs (  ) are local subproblems,

gs (  )

:= max
  qs   qs
= max
ys   ys

dual of lp-map

           

(cid:12)(cid:12)(cid:12)(cid:12) x

s   n(i)

         

  is = 0

subject to           :=
(cid:16)
>qs +p
(cid:16)
  s (ys ) +p

  s

( qs        |ys|

(cid:17)

>qis

i   n(s) (  is +   is )
i   n(s)(  is (yi ) +   is (yi ))

(cid:17)

and   qs     qs encodes the constraints
a subgradient can be computed by solving these local subproblems

qis = misqs ,    i     n(s).

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

89 / 149

projected subgradient (komodakis et al., 2007)

initialize penalties    to zero
repeat

until consensus (all qis = pi) or maximum number of iterations reached

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

90 / 149

projected subgradient (komodakis et al., 2007)

initialize penalties    to zero
repeat

for each factor s do
  s

  qs     arg max
  qs   qs

>qs +

end for

x

i   n(s)

(  is +   is )

>qis

until consensus (all qis = pi) or maximum number of iterations reached

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

90 / 149

projected subgradient (komodakis et al., 2007)

initialize penalties    to zero
repeat

for each factor s do
  s

  qs     arg max
  qs   qs
x

end for
pi     1
|n(i)|

qis

s   n(i)

x

i   n(s)

(  is +   is )

>qis

>qs +

until consensus (all qis = pi) or maximum number of iterations reached

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

90 / 149

projected subgradient (komodakis et al., 2007)

x

i   n(s)

(  is +   is )

>qis

initialize penalties    to zero
repeat

>qs +

for each factor s do
  s

  qs     arg max
  qs   qs
x

end for
pi     1
|n(i)|
  is       is       (qis     pi )

s   n(i)

qis

until consensus (all qis = pi) or maximum number of iterations reached

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

90 / 149

projected subgradient (komodakis et al., 2007)

x

i   n(s)

(  is +   is )

>qis

initialize penalties    to zero
repeat

>qs +

for each factor s do
  s

  qs     arg max
  qs   qs
x

end for
pi     1
|n(i)|
  is       is       (qis     pi )

s   n(i)

qis

until consensus (all qis = pi) or maximum number of iterations reached

guaranteed to converge to an  -accurate solution after at most
o(1/ 2) iterations
problem: too slow when there are many factors (martins et al.,
2011b)

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

90 / 149

what kind of local decoding do we need?

algorithm
sum-prod. bp (pearl, 1988)
trbp (wainwright et al., 2005)
norm-product bp (hazan and shashua, 2010)
max-prod. bp (pearl, 1988)
trw-s (kolmogorov, 2006)
mplp (globerson and jaakkola, 2008)
psdd (komodakis et al., 2007)
accelerated dd (jojic et al., 2010)
ad3 (martins et al., 2011a)

local operation

marginals
marginals
marginals

max-marginals
max-marginals
max-marginals

map

marginals
qp/map

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

91 / 149

accelerating consensus

two fundamental problems with the subgradient algorithm:

1 the dual objectivex

gs (  ) is non-smooth

s

2 consensus is promoted only through updating    (no memory about

past updates)

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

92 / 149

accelerating consensus

two fundamental problems with the subgradient algorithm:

1 the dual objectivex

gs (  ) is non-smooth

s

2 consensus is promoted only through updating    (no memory about

past updates)

how can id209 be accelerated?

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

92 / 149

accelerating consensus

two fundamental problems with the subgradient algorithm:

1 the dual objectivex

gs (  ) is non-smooth

s

2 consensus is promoted only through updating    (no memory about

past updates)

how can id209 be accelerated?

jojic et al. (2010) smooth the objective and use gradient methods
martins et al. (2011a): augmented lagrangian

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

92 / 149

accelerating consensus

two fundamental problems with the subgradient algorithm:

1 the dual objectivex

gs (  ) is non-smooth

s

2 consensus is promoted only through updating    (no memory about

past updates)

how can id209 be accelerated?

jojic et al. (2010) smooth the objective and use gradient methods
martins et al. (2011a): augmented lagrangian

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

92 / 149

accelerated gradient (jojic et al., 2010)

basic idea: make the dual objective smooth by adding an entropic
perturbation with a near-zero   temperature (also johnson (2008))
the subproblems become local marginal computations instead of
maximizations
with nesterov   s accelerated gradient method (nesterov, 2005), the
iteration bound goes from o(1/ 2) to o(1/ )

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

93 / 149

accelerated gradient (jojic et al., 2010)

basic idea: make the dual objective smooth by adding an entropic
perturbation with a near-zero   temperature (also johnson (2008))
the subproblems become local marginal computations instead of
maximizations
with nesterov   s accelerated gradient method (nesterov, 2005), the
iteration bound goes from o(1/ 2) to o(1/ )
however: very sensitive to the temperature parameter
with low temperatures, may face numerical issues (in particular for
some hard-constraint factors)
in practice, quite slow to take o    (we   ll see some plots later)

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

93 / 149

what kind of local decoding do we need?

algorithm
sum-prod. bp (pearl, 1988)
trbp (wainwright et al., 2005)
norm-product bp (hazan and shashua, 2010)
max-prod. bp (pearl, 1988)
trw-s (kolmogorov, 2006)
mplp (globerson and jaakkola, 2008)
psdd (komodakis et al., 2007)
accelerated dd (jojic et al., 2010)
ad3 (martins et al., 2011a)

local operation

marginals
marginals
marginals

max-marginals
max-marginals
max-marginals

map

marginals
qp/map

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

94 / 149

accelerating consensus

two fundamental problems with the subgradient algorithm:

1 the dual objectivex

gs (  ) is non-smooth

s

2 consensus is promoted only through updating    (no memory about

past updates)

how can id209 be accelerated?

jojic et al. (2010) smooth the objective and use gradient methods (cid:88)
martins et al. (2011a): augmented lagrangian

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

95 / 149

accelerating consensus

two fundamental problems with the subgradient algorithm:

1 the dual objectivex

gs (  ) is non-smooth

s

2 consensus is promoted only through updating    (no memory about

past updates)

how can id209 be accelerated?

jojic et al. (2010) smooth the objective and use gradient methods (cid:88)
martins et al. (2011a): augmented lagrangian

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

95 / 149

alternating directions id209 (ad3)

based on the alternating direction method of multipliers (admm):
an old method in optimization inspired by augmented lagrangians
(gabay and mercier, 1976; glowinski and marroco, 1975)
a natural    t to consensus problems
a natural    upgrade    of the subgradient algorithm (boyd et al., 2011)

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

96 / 149

augmented lagrangian and admm

basic idea: augment the lagrangian function with a quadratic penalty
>(pi     qis )

>qs +p

l  (p, q,   ) =

i   n(s)   is

>qis

  is

x

+

(cid:17)

kqis     pik2

is

x
s
      
2

(cid:16)
x

  s

is

method of multipliers (super-linear convergence):

1 maximize l  (p, q,   ) jointly w.r.t. p and q (challenging)
2 multiplier update:   is       is       (qis     pi )
alternating direction method of multipliers: replace step 1 by separate
maximizations (   rst w.r.t. q, then p)

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

97 / 149

from subgradient to ad3 (martins et al., 2011a)

x

i   n(s)

(  is +   is )

>qis

initialize penalties    to zero
repeat

for each factor s = 1, . . . , s do

>qs +

  s

  qs     arg max
  qs   qs
x

end for
pi     1
|n(i)|
  is       is       (qis     pi )

s   n(i)

qis

until consensus (all qis = pi) or maximum number of iterations reached

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

98 / 149

from subgradient to ad3 (martins et al., 2011a)

x

i   n(s)

(  is +   is )

>qis       
2

x

i   n(s)

kqis    pik2

initialize penalties    to zero
repeat

for each factor s = 1, . . . , s do

>qs +

  s

  qs     arg max
  qs   qs
x

end for
pi     1
|n(i)|
  is       is       (qis     pi )

s   n(i)

qis

until consensus (all qis = pi) or maximum number of iterations reached

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

98 / 149

from subgradient to ad3 (martins et al., 2011a)

x

i   n(s)

(  is +   is )

>qis       
2

x

i   n(s)

kqis    pik2

initialize penalties    to zero
repeat

for each factor s = 1, . . . , s do

>qs +

  s

  qs     arg max
  qs   qs
x

end for
pi     1
|n(i)|
  is       is       (qis     pi )

s   n(i)

qis

until consensus (all qis = pi) or maximum number of iterations reached

faster consensus: regularize q-step towards average votes in p

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

98 / 149

from subgradient to ad3 (martins et al., 2011a)

x

i   n(s)

(  is +   is )

>qis       
2

x

i   n(s)

kqis    pik2

initialize penalties    to zero
repeat

for each factor s = 1, . . . , s do

>qs +

  s

  qs     arg max
  qs   qs
x

end for
pi     1
|n(i)|
  is       is       (qis     pi )

s   n(i)

qis

until consensus (all qis = pi) or maximum number of iterations reached

faster consensus: regularize q-step towards average votes in p
better stopping conditions: keeps track of primal and dual residuals

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

98 / 149

theoretical guarantees of ad3

convergent in primal and dual (glowinski and le tallec, 1989)
iteration bound: o(1/ ) (cf. o(1/ 2) for projected subgradient)
inexact ad3 subproblems: still convergent if residuals are summable
(eckstein and bertsekas, 1992)
always dual feasible: can compute upper bounds and embed in
branch-and-bound toward exact decoding (das et al., 2012)

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

99 / 149

theoretical guarantees of ad3

convergent in primal and dual (glowinski and le tallec, 1989)
iteration bound: o(1/ ) (cf. o(1/ 2) for projected subgradient)
inexact ad3 subproblems: still convergent if residuals are summable
(eckstein and bertsekas, 1992)
always dual feasible: can compute upper bounds and embed in
branch-and-bound toward exact decoding (das et al., 2012)

but: ad3 local subproblems are quadratic (more involved than in
projected subgradient)

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

99 / 149

theoretical guarantees of ad3

convergent in primal and dual (glowinski and le tallec, 1989)
iteration bound: o(1/ ) (cf. o(1/ 2) for projected subgradient)
inexact ad3 subproblems: still convergent if residuals are summable
(eckstein and bertsekas, 1992)
always dual feasible: can compute upper bounds and embed in
branch-and-bound toward exact decoding (das et al., 2012)

but: ad3 local subproblems are quadratic (more involved than in
projected subgradient)
still   very easy and e   cient for logic and knapsack factors!

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

99 / 149

projecting onto hard constraint polytopes

martins et al. (2011a): logic factors can be solved in o(k ) time
almeida and martins (2013): same for knapsack factors!

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

100 / 149

xororor-outknapsackstructured factors

what about structured factors?

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

101 / 149

structured factors

what about structured factors?

projected subgradient handles these quite well

combinatorial machinery (viterbi, chu-liu-edmonds, fulkerson-ford,
floyd-warshall,...)

we cannot solve the ad3 subproblems with that machinery...

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

101 / 149

structured factors

what about structured factors?

projected subgradient handles these quite well

combinatorial machinery (viterbi, chu-liu-edmonds, fulkerson-ford,
floyd-warshall,...)

we cannot solve the ad3 subproblems with that machinery...
or can we?

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

101 / 149

structured factors

what about structured factors?

projected subgradient handles these quite well

combinatorial machinery (viterbi, chu-liu-edmonds, fulkerson-ford,
floyd-warshall,...)

we cannot solve the ad3 subproblems with that machinery...
or can we?
active set method: seek the support of the solution by adding/removing
components; very suitable for warm-starting (nocedal and wright, 1999)

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

101 / 149

an active set method for the ad3 subproblem

        s

>qs +

x

i   n(s)

  qs     arg max
  qs   qs

(  is +   is )

>qis       
2

      

kqis     pik2

x

i   n(s)

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

102 / 149

an active set method for the ad3 subproblem

        s

>qs +

x

i   n(s)

  qs     arg max
  qs   qs

(  is +   is )

>qis       
2

      

kqis     pik2

x

i   n(s)

too many possible assignments: dimension of qs is o(exp(|n(s)|))

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

102 / 149

an active set method for the ad3 subproblem

        s

>qs +

x

i   n(s)

  qs     arg max
  qs   qs

(  is +   is )

>qis       
2

      

kqis     pik2

x

i   n(s)

too many possible assignments: dimension of qs is o(exp(|n(s)|))
key result: there   s a sparse solution (only o(|n(s)|) nonzeros)

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

102 / 149

an active set method for the ad3 subproblem

        s

>qs +

x

i   n(s)

  qs     arg max
  qs   qs

(  is +   is )

>qis       
2

      

kqis     pik2

x

i   n(s)

too many possible assignments: dimension of qs is o(exp(|n(s)|))
key result: there   s a sparse solution (only o(|n(s)|) nonzeros)
active set methods: seek the support of the solution by adding/removing
components; very suitable for warm-starting (nocedal and wright, 1999)
only requirement: a local-max oracle (as in projected subgradient)

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

102 / 149

an active set method for the ad3 subproblem

        s

>qs +

x

i   n(s)

  qs     arg max
  qs   qs

(  is +   is )

>qis       
2

      

kqis     pik2

x

i   n(s)

too many possible assignments: dimension of qs is o(exp(|n(s)|))
key result: there   s a sparse solution (only o(|n(s)|) nonzeros)
active set methods: seek the support of the solution by adding/removing
components; very suitable for warm-starting (nocedal and wright, 1999)
only requirement: a local-max oracle (as in projected subgradient)
more info: martins et al. (2014)

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

102 / 149

runtime of ad3 vs psdd (parsing)

caching and warm-starting the subproblems reduces drastically the
number of oracle calls   huge speed-ups!!
ad3 faster to achieve consensus (due to the quadratic penalty)

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

103 / 149

020040060080010001200140016001800# iterations 02004006008001000# oracle calls (normalized)ad3 (full)subgrad. (full)01020304050sentencelength(words)0.000.100.20averageruntime(sec.)ad3subgrad.what kind of local decoding do we need?

algorithm
sum-prod. bp (pearl, 1988)
trbp (wainwright et al., 2005)
norm-product bp (hazan and shashua, 2010)
max-prod. bp (pearl, 1988)
trw-s (kolmogorov, 2006)
mplp (globerson and jaakkola, 2008)
psdd (komodakis et al., 2007)
accelerated dd (jojic et al., 2010)
ad3 (martins et al., 2011a)

local operation

marginals
marginals
marginals

max-marginals
max-marginals
max-marginals

map

marginals
qp/map

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

104 / 149

example: potts grid (20    20, 8 states)

a. martins, m. figueiredo, p. aguiar, n. smith, e. xing (2014).
ad3: alternating directions id209 for map id136 in id114.
journal of machine learning research (to appear).

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

105 / 149

example: frame-id29

embedded in a branch-and-bound procedure for exact decoding
d. das, a. martins, n. smith.
   an exact dd algorithm for shallow id29 with constraints.   
*sem workshop, 2012.

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

106 / 149

200400600800100025.825.92626.1numberofiterationsdualobjectiveframe   semanticparsing020040060080042.1242.1442.1642.1810020030040050020.120.220.320.4200400600800100060.660.86161.261.4200400600800100063646566mplpsubgrad.ad3try it yourself: ad3 toolkit

freely available at: http://www.ark.cs.cmu.edu/ad3
implemented in c++, includes a python wrapper (thanks to andy
mueller)
implements mplp, psdd, ad3 for arbitrary factor graphs
many built-in factors: logic, knapsack, dense, and some structured
factors
you can implement your own factor (only need to write a local map
decoder!)
toy examples included (parsing, coreference, potts models)

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

107 / 149

summing up id209

id209 is a general optimization technique that splits the
dual into several subproblems (one per factor) that must agree on
overlaps
this can be used to solve lp-map
we discussed three variants: subgradient (psdd), accelerated
gradient (add), and alternating directions (ad3)
the algorithms are convergent and retrieve the true map if the graph
has no cycles; they also give certi   cates when the solution of
lp-map equals the map
for psdd and ad3 only local maximizations are necessary; add
requires computing marginals

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

108 / 149

outline

1 id170 and factor graphs

2 integer id135

3 message-passing algorithms

sum-product
max-product

4 id209

5 applications

6 conclusions

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

109 / 149

applications

we   ll discuss three applications:

turbo parsing
compressive summarization
joint coreference resolution and quotation attribution

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

110 / 149

applications

we   ll discuss three applications:

turbo parsing
compressive summarization
joint coreference resolution and quotation attribution

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

110 / 149

what is a turbo parser?

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

111 / 149

what is a turbo parser?

a parser that runs id136 in factor graphs, ignoring global
e   ects caused by loops (martins et al., 2010)
name inspired from turbo decoders (berrou et al., 1993)

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

111 / 149

what is a turbo parser?

a parser that runs id136 in factor graphs, ignoring global
e   ects caused by loops (martins et al., 2010)
name inspired from turbo decoders (berrou et al., 1993)
next: we speed up turbo parsers via ad3 w/ active set

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

111 / 149

recent paper

andr  e f. t. martins, miguel b. almeida, noah a. smith.
   turning on the turbo: fast third-order non-projective turbo parsers.   
acl 2013 short paper.

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

112 / 149

an important distinction

a projective tree:

a non-projective tree:

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

113 / 149

*logicplaysaminimalrolehere*welearnedalessonin1987aboutvolatilityan important distinction

a projective tree:

a non-projective tree:

this talk: we allow non-projective trees.
suitable for languages with    exible word order (dutch, german, czech,...)

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

113 / 149

*logicplaysaminimalrolehere*welearnedalessonin1987aboutvolatilityfirst-order scores for arcs

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

114 / 149

*welearnedalessonin1987aboutvolatilitysecond-order scores for consecutive siblings

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

114 / 149

*welearnedalessonin1987aboutvolatilitysecond-order scores for grandparents

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

114 / 149

*welearnedalessonin1987aboutvolatilityscores for arbitrary siblings

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

114 / 149

*welearnedalessonin1987aboutvolatilityscores for head bigrams

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

114 / 149

*welearnedalessonin1987aboutvolatilitythird-order scores for grand-siblings

used by koo and collins (2010) for projective parsing.

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

114 / 149

*welearnedalessonin1987aboutvolatilitythird-order scores for tri-siblings

used by koo and collins (2010) for projective parsing.

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

114 / 149

*welearnedalessonin1987aboutvolatilitydecoding

how to deal with all these parts?

id145 only available for the projective case...

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

115 / 149

decoding

how to deal with all these parts?

id145 only available for the projective case...
beyond arc-factored models, non-projective parsing is np-hard
(mcdonald and satta, 2007)
need to embrace approximations!

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

115 / 149

approximate dependency parsers

mcdonald et al. (2006)
smith et al. (2008)
martins et al. (2010)
koo et al. (2010)
martins et al. (2011)
martins et al. (2013)

parser

projective + greedy

loopy bp
lp solver

dual decomp.

ad3

ad3 & active set

af
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

cs
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

g
(cid:88)
(cid:88)

(cid:88)
(cid:88)

as
(cid:88)
(cid:88)

(cid:88)
(cid:88)

dp

hb

npa

gs

ts

(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)

(cid:88)

(cid:88)

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

116 / 149

arcconsecutive siblingsgrandparentall siblingsdirected pathhead bigramnonprojective arcgrand-siblingstri-siblingsfactor graph representation

variables nodes for dependency arcs, linked to a tree constraint
head automata for consecutive siblings and grandparents (as in
smith and eisner (2008); koo et al. (2010))
pairwise factors for arbitrary siblings (as martins et al. (2011b))
third-order head automata for grand-siblings and tri-siblings
sequence model for head bigrams

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

117 / 149

factor graph representation

variables nodes for dependency arcs, linked to a tree constraint
head automata for consecutive siblings and grandparents (as in
smith and eisner (2008); koo et al. (2010))
pairwise factors for arbitrary siblings (as martins et al. (2011b))
third-order head automata for grand-siblings and tri-siblings
sequence model for head bigrams

we solve the lp-map relaxation with ad3.

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

117 / 149

parsing accuracies/runtimes

sota accuracies for the largest non-projective datasets (conll-2006 and
conll-2008):

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

118 / 149

extension: broad-coverage id29

same idea applied to id14.

best results in the semeval 2014 shared task:

andr  e f. t. martins and mariana s. c. almeida.
   priberam: a turbo semantic parser with second order features.   
semeval 2014.

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

119 / 149

applications

we   ll discuss three applications:

turbo parsing
compressive summarization
joint coreference resolution and quotation attribution

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

120 / 149

applications

we   ll discuss three applications:

turbo parsing
compressive summarization
joint coreference resolution and quotation attribution

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

120 / 149

recent paper

miguel b. almeida and andr  e f. t. martins.
   fast and robust compressive summarization with dual
decomposition and id72.   
acl 2013.

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

121 / 149

id57

map a set of related documents to a brief summary.

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

122 / 149

id57

map a set of related documents to a brief summary.

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

122 / 149

what makes a good summary?

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

123 / 149

what makes a good summary?

1 conciseness

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

123 / 149

what makes a good summary?

1 conciseness
2 informativeness

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

123 / 149

what makes a good summary?

1 conciseness
2 informativeness
3 grammaticality

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

123 / 149

extractive summarization

just extract the most salient sentences.

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

124 / 149

extractive summarization

just extract the most salient sentences.

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

124 / 149

extractive summarization

just extract the most salient sentences.

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

124 / 149

what we do: compressive summarization

jointly extract and compress sentences.

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

125 / 149

what we do: compressive summarization

jointly extract and compress sentences.

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

125 / 149

what we do: compressive summarization

jointly extract and compress sentences.

for given summary size, easier to be informative, but harder to be
grammatical.

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

125 / 149

compressive summarization as global optimization

indicator variables for every word of the nth sentence, zn := hzn,   iln
   =1

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

126 / 149

compressive summarization as global optimization

indicator variables for every word of the nth sentence, zn := hzn,   iln
   =1
summary length must not exceed the budget (b words)
quality function rewards global informativeness (through g(z))...
... but also local grammaticality (through hn(zn)):

nx

maximize

s.t.

g(z) +

nx

lnx

n=1

   =1

hn(zn)

n=1
zn,        b.

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

126 / 149

grammaticality: sentence compression model

inspired by knight and marcu (2000)   s word deletion model

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

127 / 149

$theleaderofmoderatekashmiriseparatistswarnedthursdaythat...grammaticality: sentence compression model

inspired by knight and marcu (2000)   s word deletion model
our model factors over dependency arcs:

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

127 / 149

$theleaderofmoderatekashmiriseparatistswarnedthursdaythat...grammaticality: sentence compression model

inspired by knight and marcu (2000)   s word deletion model
our model factors over dependency arcs:

goal: maximize sum of arc scores, allowing only deletion of subtrees.

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

127 / 149

$theleaderofmoderatekashmiriseparatistswarnedthursdaythat...grammaticality: sentence compression model

inspired by knight and marcu (2000)   s word deletion model
our model factors over dependency arcs:

goal: maximize sum of arc scores, allowing only deletion of subtrees.
a structured factor, locally decodable with id145.

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

127 / 149

$theleaderofmoderatekashmiriseparatistswarnedthursdaythat...informativeness: coverage model

inspired by extractive max-coverage models (filatova and hatzivassiloglou,
2004; yih et al., 2007; gillick et al., 2008; lin and bilmes, 2010)

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

128 / 149

informativeness: coverage model

inspired by extractive max-coverage models (filatova and hatzivassiloglou,
2004; yih et al., 2007; gillick et al., 2008; lin and bilmes, 2010)

extract a list of concepts from the original documents
de   ne relevance scores for each concept (linear feature-based
model)

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

128 / 149

informativeness: coverage model

inspired by extractive max-coverage models (filatova and hatzivassiloglou,
2004; yih et al., 2007; gillick et al., 2008; lin and bilmes, 2010)

extract a list of concepts from the original documents
de   ne relevance scores for each concept (linear feature-based
model)
de   ne g(z) as sum of scores for each concept in the summary

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

128 / 149

graphical model for our compressive summarizer

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

129 / 149

budgetgraphical model for our compressive summarizer

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

129 / 149

sentences$     the      leader    of   moderate  kashmiri   separatists   warned   thursday   that ...$     talks    with   kashmiri    separatists  began    last       year ...budgetgraphical model for our compressive summarizer

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

129 / 149

sentences$     the      leader    of   moderate  kashmiri  separatists warned   thursday   that ...$     talks    with   kashmiri  separatists began    last       year ...budgetgraphical model for our compressive summarizer

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

129 / 149

sentences$     the      leader    of   moderate  kashmiri  separatists warned   thursday   that ...$     talks    with   kashmiri  separatists began    last       year ...budgetconcept tokensgraphical model for our compressive summarizer

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

129 / 149

sentences$     the      leader    of   moderate  kashmiri  separatists warned   thursday   that ...$     talks    with   kashmiri  separatists began    last       year ..."kashmiri separatists"budgetconcept tokensconcept typegraphical model for our compressive summarizer

1 we use id209 (ad3) for solving a linear relaxation
2 we apply a fast rounding procedure to obtain a valid summary

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

129 / 149

sentences$     the      leader    of   moderate  kashmiri  separatists warned   thursday   that ...$     talks    with   kashmiri  separatists began    last       year ..."kashmiri separatists"budgetconcept tokensconcept typegraphical model for our compressive summarizer

1 we use id209 (ad3) for solving a linear relaxation
2 we apply a fast rounding procedure to obtain a valid summary

id72: user-generated data (simple english wikipedia)
along with manual abstracts and compressive summaries

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

129 / 149

sentences$     the      leader    of   moderate  kashmiri  separatists warned   thursday   that ...$     talks    with   kashmiri  separatists began    last       year ..."kashmiri separatists"budgetconcept tokensconcept typeresults on tac-2008 dataset

better informativeness (without sacri   cing grammaticality):

gillick et al. (2008)
berg-kirkpatrick et al. (2011)
woodsend and lapata (2012)
single-task ad3
multi-task ad3

11.03

11.37

11.71

11.88

id8-2 recall

12.30

averaged runtimes per summarization problem (10 documents):

solver
ilp exact, glpk
lp-relax., glpk
ad3 (1,000 its.)
extractive (ilp)

runtime (sec.)

10.394
2.265
0.406
0.265

id8-2

12.40
12.38
12.30
11.16

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

130 / 149

applications

we   ll discuss three applications:

turbo parsing
compressive summarization
joint coreference resolution and quotation attribution

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

131 / 149

applications

we   ll discuss three applications:

turbo parsing
compressive summarization
joint coreference resolution and quotation attribution

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

131 / 149

recent paper

mariana s. c. almeida, miguel b. almeida and andr  e f. t. martins.
   a joint model for quotation attribution and coreference
resolution.   
eacl 2014.

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

132 / 149

why jointly?

coreference resolution and quotation attribution may bene   t from
being treated as a joint task.

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

133 / 149

why jointly?

coreference resolution and quotation attribution may bene   t from
being treated as a joint task.
a speaker doesn   t refer to himself as he:

rivals carp at    the principle of pilson,    as nbc   s arthur watson once
put it        he   s always expounding that rights are too high, then he   s
going crazy.    but the 49-year-old mr. pilson is hardly a man to ignore
the numbers.

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

133 / 149

why jointly?

coreference resolution and quotation attribution may bene   t from
being treated as a joint task.
a speaker doesn   t refer to himself as he:

rivals carp at    the principle of pilson,    as nbc   s arthur watson once
put it        he   s always expounding that rights are too high, then he   s
going crazy.    but the 49-year-old mr. pilson is hardly a man to ignore
the numbers.

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

133 / 149

why jointly?

coreference resolution and quotation attribution may bene   t from
being treated as a joint task.
a speaker doesn   t refer to himself as he:

rivals carp at    the principle of pilson,    as nbc   s arthur watson once
put it        he   s always expounding that rights are too high, then he   s
going crazy.    but the 49-year-old mr. pilson is hardly a man to ignore
the numbers.

two consecutive quotes are often from co-referent speakers:

english novelist dorothy l. sayers described ringing as a    passion that
   nds its satisfaction in mathematical completeness and mechanical
perfection.   
ringers, she added, are       lled with the solemn intoxication that comes
of intricate ritual faultlessly performed.   

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

133 / 149

why jointly?

coreference resolution and quotation attribution may bene   t from
being treated as a joint task.
a speaker doesn   t refer to himself as he:

rivals carp at    the principle of pilson,    as nbc   s arthur watson once
put it        he   s always expounding that rights are too high, then he   s
going crazy.    but the 49-year-old mr. pilson is hardly a man to ignore
the numbers.

two consecutive quotes are often from co-referent speakers:

english novelist dorothy l. sayers described ringing as a    passion that
   nds its satisfaction in mathematical completeness and mechanical
perfection.   
ringers, she added, are       lled with the solemn intoxication that comes
of intricate ritual faultlessly performed.   

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

133 / 149

coreference tree (denis and baldridge, 2008;
fernandes et al., 2012; durrett and klein, 2013)

clusters of co-referent mentions (entities) correspond to subtrees
coming out from the root node.

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

134 / 149

coreference tree (denis and baldridge, 2008;
fernandes et al., 2012; durrett and klein, 2013)

clusters of co-referent mentions (entities) correspond to subtrees
coming out from the root node.

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

134 / 149

from coreference to quotation-coreference trees

(almeida et al., 2014)

include mention nodes and quotation nodes
quotation nodes have to be leaves
subtrees coming out from the root induce entity clusters along with
their quotes: entity-based quotation attribution

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

135 / 149

from coreference to quotation-coreference trees

(almeida et al., 2014)

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

136 / 149

from coreference to quotation-coreference trees

(almeida et al., 2014)

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

136 / 149

beyond arc scores

the simplest coreference models (e.g., the surface model of durrett and
klein (2013)) are arc-factored

exact decoding can be performed in a greedy manner

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

137 / 149

beyond arc scores

the simplest coreference models (e.g., the surface model of durrett and
klein (2013)) are arc-factored

exact decoding can be performed in a greedy manner

however: in our approach, an arc factored model would be equivalent to
do coreference resolution and quotation attribution independently...

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

137 / 149

beyond arc scores

the simplest coreference models (e.g., the surface model of durrett and
klein (2013)) are arc-factored

exact decoding can be performed in a greedy manner

however: in our approach, an arc factored model would be equivalent to
do coreference resolution and quotation attribution independently...

to do things jointly, we add extra scores for:

a speaker being mentioned inside a quotation
consecutive quotes having the same speakers

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

137 / 149

beyond arc scores

the simplest coreference models (e.g., the surface model of durrett and
klein (2013)) are arc-factored

exact decoding can be performed in a greedy manner

however: in our approach, an arc factored model would be equivalent to
do coreference resolution and quotation attribution independently...

to do things jointly, we add extra scores for:

a speaker being mentioned inside a quotation
consecutive quotes having the same speakers

these scores require knowing if pairs of nodes are in the same subtree.

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

137 / 149

arc variables: each node (except the root) has exactly one parent

logic program

ai   j = 1,

   j 6= 0

j   1x

i=0

_

i6=0

path variables: paths propagate through arcs

  i      i = 1,

   i,

  i      k =

(ai   j       j      k ),

   i, k

_

i<j   k

pair variables: nodes k and     are in the same subtree if they have a
common ancestor which is not the root

pk,    =

(  i      k       i         ),

   k, l.

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

138 / 149

experiments

datasets:

wsj portion of the ontonotes (597 documents); same splits as
conll 2011 shared task
quotation annotations of the parc dataset (pareti, 2012; o   keefe
et al., 2012)

coreference id74: average between muc, b3, ceafe
quotation id74:

representative speaker match (rsm): # matches to
representative (non-pronominal) mention of the gold speaker   s entity
entity cluster f1 (ecf1): f1 score between the predicted and gold
speaker entity (sets of mentions)

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

139 / 149

results

coreference resolution:

durrett and klein (2013) (surface)

quote/coref independent

joint system

muc f1
58.87
57.89
58.78

bcub f1

62.74
62.50
63.79

ceafe f1

45.46
45.48
45.50

avg.
55.7
55.3
56.0

quotation attribution:

quoteonly
quoteaftercoref
quote/coref independent
joint system

rsm
ecf1
49.4% 41.2%
64.6% 70.0%
74.7% 73.7%
76.6% 74.1%

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

140 / 149

outline

1 id170 and factor graphs

2 integer id135

3 message-passing algorithms

sum-product
max-product

4 id209

5 applications

6 conclusions

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

141 / 149

conclusions

many structured problems in nlp are np-hard or expensive
(constrained models, diversity, combination of structured models)
often they can be approximately decoded via id135
(e.g., by relaxing an ilp)
the structure inherent to these problems can be represented with a
factor graph
message-passing and id209 algorithms can solve these
lps e   ciently, exploiting the structure of the graph
conceptually: approximate global decoding by invoking only local
decoders (local maximizations, marginals, max-marginals, qps, ...)
ad3 is faster than the subgradient algorithm both in theory and in
practice, and requires the same local decoders
sota results in several applications (turbo parsing, summarization,
joint coref and quotation attribution)

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

142 / 149

thank you!

the syntactic/semantic parser and ad3 are freely available at:

http://www.ark.cs.cmu.edu/turboparser
http://www.ark.cs.cmu.edu/ad3

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

143 / 149

ltiacknowledgments
fundac    ao para a ci  encia e tecnologia, grants
pest-oe/eei/la0008/2011 and ptdc/eei-sii/2312/2012.
fundac    ao para a ci  encia e tecnologia and information and
communication technologies institute (portugal/usa), through the
cmu-portugal program.
priberam: qren/por lisboa (portugal), eu/feder programme,
discooperio project, contract 2011/18501.
priberam: qren/por lisboa (portugal), eu/feder programme,
intelligo project, contract 2012/24803.

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

144 / 149

references i

almeida, m. b. and martins, a. f. t. (2013). fast and robust compressive summarization with id209 and

id72. in proc. of the annual meeting of the association for computational linguistics.

almeida, m. s. c., almeida, m. b., and martins, a. f. t. (2014). a joint model for quotation attribution and coreference
resolution. in proc. of the annual meeting of the european chapter of the association for computational linguistics.

auli, m. and lopez, a. (2011). a comparison of loopy belief propagation and id209 for integrated id35

id55 and parsing. in proc. of annual meeting of the association for computational linguistics.

berg-kirkpatrick, t., gillick, d., and klein, d. (2011). jointly learning to extract and compress. in proc. of annual meeting of

the association for computational linguistics.

berrou, c., glavieux, a., and thitimajshima, p. (1993). near shannon limit error-correcting coding and decoding. in proc. of

international conference on communications, volume 93, pages 1064   1070.

bertsekas, d., hager, w., and mangasarian, o. (1999). nonid135. athena scienti   c.
boyd, s., parikh, n., chu, e., peleato, b., and eckstein, j. (2011). distributed optimization and statistical learning via the

alternating direction method of multipliers. now publishers.

burkett, d. and klein, d. (2012). fast id136 in phrase extraction models with belief propagation. in proc. of the north

american chapter of the association for computational linguistics, pages 29   38. association for computational linguistics.

chang, y.-w. and collins, m. (2011). exact decoding of phrase-based translation models through lagrangian relaxation. in

proc. of empirical methods for natural language processing.

clarke, j. and lapata, m. (2008). global id136 for sentence compression an integer id135 approach.

journal of arti   cial intelligence research, 31:399   429.

dantzig, g. and wolfe, p. (1960). decomposition principle for linear programs. operations research, 8(1):101   111.
dantzig, g. b. (1947). maximization of a linear function of variables subject to linear inequalities. published in t.c. koopmans

(ed.): activity analysis of production and allocation, new york-london 1951, pages 339   347.

das, d., martins, a. f. t., and smith, n. a. (2012). an exact id209 algorithm for shallow id29

with constraints. in proc. of first joint conference on lexical and computational semantics (*sem).

denis, p. and baldridge, j. (2008). specialized models and ranking for coreference resolution. in proceedings of the conference

on empirical methods in natural language processing, pages 660   669. association for computational linguistics.

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

145 / 149

dreyer, m. and eisner, j. (2009). id114 over multiple strings. in proc. of empirical methods in natural language

processing, pages 101   110. association for computational linguistics.

references ii

duchi, j., tarlow, d., elidan, g., and koller, d. (2007). using combinatorial optimization within max-product belief

propagation. advances in neural information processing systems, 19.

durrett, g. and klein, d. (2013). easy victories and uphill battles in coreference resolution. in proceedings of the 2013

conference on empirical methods in natural language processing.

eckstein, j. and bertsekas, d. (1992). on the douglas-rachford splitting method and the proximal point algorithm for maximal

monotone operators. mathematical programming, 55(1):293   318.

eisner, j. (1996). three new probabilistic models for id33: an exploration. in proc. of international conference

on computational linguistics, pages 340   345.

everett iii, h. (1963). generalized lagrange multiplier method for solving problems of optimum allocation of resources.

operations research, 11(3):399   417.

fernandes, e. r., dos santos, c. n., and milidi  u, r. l. (2012). latent structure id88 with feature induction for

unrestricted coreference resolution. in joint conference on emnlp and conll-shared task, pages 41   48. association for
computational linguistics.

filatova, e. and hatzivassiloglou, v. (2004). a formal model for information selection in multi-sentence text extraction. in proc.

of international conference on computational linguistics.

gabay, d. and mercier, b. (1976). a dual algorithm for the solution of nonlinear variational problems via    nite element

approximation. computers and mathematics with applications, 2(1):17   40.

gillick, d., favre, b., and hakkani-tur, d. (2008). the icsi summarization system at tac 2008. in proc. of text understanding

conference.

globerson, a. and jaakkola, t. (2008). fixing max-product: convergent message passing algorithms for map lp-relaxations.

neural information processing systems, 20.

glowinski, r. and le tallec, p. (1989). augmented lagrangian and operator-splitting methods in nonlinear mechanics. society

for industrial mathematics.

glowinski, r. and marroco, a. (1975). sur l   approximation, par   el  ements    nis d   ordre un, et la r  esolution, par

penalisation-dualit  e, d   une classe de probl`emes de dirichlet non lin  eaires. rev. franc. automat. inform. rech. operat.,
9:41   76.

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

146 / 149

references iii

hazan, t. and shashua, a. (2010). norm-product belief propagation: primal-dual message-passing for approximate id136.

ieee transactions on id205, 56(12):6294   6316.

johnson, j. (2008). equivalence of id178 id173 and relative-id178 proximal method. unpublished manuscript.
jojic, v., gould, s., and koller, d. (2010). accelerated id209 for map id136. in international conference of

machine learning.

kantorovich, l. v. (1940). a new method of solving of some classes of extremal problems. in dokl. akad. nauk sssr,

volume 28, pages 211   214.

karp, r. m. (1972). reducibility among combinatorial problems. springer.
khachiyan, l. g. (1980). polynomial algorithms in id135. ussr computational mathematics and mathematical

physics, 20(1):53   72.

knight, k. and marcu, d. (2000). statistics-based summarization   step one: sentence compression. in aaai/iaai.
koller, d. and friedman, n. (2009). probabilistic id114: principles and techniques. the mit press.
kolmogorov, v. (2006). convergent tree-reweighted message passing for energy minimization. ieee transactions on pattern

analysis and machine intelligence, 28:1568   1583.

komodakis, n., paragios, n., and tziritas, g. (2007). mrf optimization via id209: message-passing revisited. in

proc. of international conference on id161.

koo, t. and collins, m. (2010). e   cient third-order dependency parsers. in proc. of annual meeting of the association for

computational linguistics, pages 1   11.

koo, t., globerson, a., carreras, x., and collins, m. (2007). id170 models via the matrix-tree theorem. in

empirical methods for natural language processing.

koo, t., rush, a. m., collins, m., jaakkola, t., and sontag, d. (2010). id209 for parsing with non-projective

head automata. in proc. of empirical methods for natural language processing.

lauritzen, s. (1996). id114. clarendon press, oxford.
lin, h. and bilmes, j. (2010). id57 via budgeted maximization of submodular functions. in proc. of

annual meeting of the north american chapter of the association for computational linguistics.

mackay, d. (2003). id205, id136, and learning algorithms, volume 7. cambridge university press.

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

147 / 149

references iv

martins, a. f. t. and almeida, m. s. c. (2014). priberam: a turbo semantic parser with second order features. in proc. of the

international workshop on semantic evaluations (semeval); task 8: broad-coverage semantic id33.

martins, a. f. t., figueiredo, m. a. t., aguiar, p. m. q., smith, n. a., and xing, e. p. (2011a). an augmented lagrangian

approach to constrained map id136. in proc. of international conference on machine learning.

martins, a. f. t., figueiredo, m. a. t., aguiar, p. m. q., smith, n. a., and xing, e. p. (2014). ad3: alternating directions

id209 for map id136 in id114. journal of machine learning research (to appear).

martins, a. f. t. and smith, n. a. (2009). summarization with a joint model for sentence extraction and compression. in
north american chapter of the association for computational linguistics: workshop on integer id135 for
nlp.

martins, a. f. t., smith, n. a., aguiar, p. m. q., and figueiredo, m. a. t. (2011b). id209 with many

overlapping components. in proc. of empirical methods for natural language processing.

martins, a. f. t., smith, n. a., xing, e. p., figueiredo, m. a. t., and aguiar, p. m. q. (2010). turbo parsers: dependency

parsing by approximate variational id136. in proc. of empirical methods for natural language processing.

mcallester, d., collins, m., and pereira, f. (2008). case-factor diagrams for structured probabilistic modeling. journal of

computer and system sciences, 74(1):84   96.

mcdonald, r. (2006). discriminative sentence compression with soft syntactic constraints. in proc. of annual meeting of the

european chapter of the association for computational linguistics.

mcdonald, r. and satta, g. (2007). on the complexity of non-projective data-driven id33. in proc. of

international conference on parsing technologies.

mcdonald, r. t., pereira, f., ribarov, k., and hajic, j. (2005). non-projective id33 using spanning tree

algorithms. in proc. of empirical methods for natural language processing.

nesterov, y. (2005). smooth minimization of non-smooth functions. mathematical programming, 103(1):127   152.
nivre, j., hall, j., nilsson, j., eryi  git, g., and marinov, s. (2006). labeled pseudo-projective id33 with support

vector machines. in procs. of international conference on natural language learning.

nocedal, j. and wright, s. (1999). numerical optimization. springer verlag.

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

148 / 149

references v

o   keefe, t., pareti, s., curran, j. r., koprinska, i., and honnibal, m. (2012). a sequence labelling approach to quote

attribution. in proceedings of the 2012 joint conference on empirical methods in natural language processing and
computational natural language learning, pages 790   799. association for computational linguistics.

pareti, s. (2012). a database of attribution relations. in lrec, pages 3213   3217.
pearl, j. (1988). probabilistic reasoning in intelligent systems: networks of plausible id136. morgan kaufmann.
richardson, m. and domingos, p. (2006). markov logic networks. machine learning, 62(1):107   136.
roth, d. and yih, w. (2004). a id135 formulation for global id136 in natural language tasks. in international

conference on natural language learning.

rush, a. and collins, m. (2012). a tutorial on id209 and lagrangian relaxation for id136 in natural

language processing. journal of arti   cial intelligence research, 45:305   362.

rush, a., sontag, d., collins, m., and jaakkola, t. (2010). on id209 and id135 relaxations for

natural language processing. in proc. of empirical methods for natural language processing.

rush, a. m. and collins, m. (2011). exact decoding of syntactic translation models through lagrangian relaxation. in proc. of

annual meeting on association for computational linguistics.

smith, d. and eisner, j. (2008). id33 by belief propagation. in proc. of empirical methods for natural language

processing.

smith, n. a. (2011). linguistic structure prediction, volume 13 of synthesis lectures on human language technologies.

morgan and claypool.

wainwright, m. and jordan, m. (2008). id114, exponential families, and variational id136. now publishers.
wainwright, m. j., jaakkola, t., and willsky, a. (2005). a new class of upper bounds on the log partition function. ieee

transactions on id205, 51(7):2313   2335.

woodsend, k. and lapata, m. (2012). multiple aspect summarization using integer id135. in proc. of empirical

methods in natural language processing.

yedidia, j. s., freeman, w. t., and weiss, y. (2001). generalized belief propagation. in neural information processing systems.
yih, w.-t., goodman, j., vanderwende, l., and suzuki, h. (2007). id57 by maximizing informative

content-words. in proc. of international joint conference on arti   cal intelligence.

andr  e martins (priberam/it)

lp decoders in nlp

http://tiny.cc/lpdnlp

149 / 149

