the more you know: using id13s for image classi   cation

kenneth marino, ruslan salakhutdinov, abhinav gupta

carnegie mellon university

5000 forbes ave, pittsburgh, pa 15213

{kdmarino, rsalakhu, abhinavg}@cs.cmu.edu

7
1
0
2

 
r
p
a
2
2

 

 
 
]

v
c
.
s
c
[
 
 

2
v
4
4
8
4
0

.

2
1
6
1
:
v
i
x
r
a

abstract

one characteristic that sets humans apart from modern
learning-based id161 algorithms is the ability to
acquire knowledge about the world and use that knowledge
to reason about the visual world. humans can learn about
the characteristics of objects and the relationships that oc-
cur between them to learn a large variety of visual con-
cepts, often with few examples. this paper investigates the
use of structured prior knowledge in the form of knowledge
graphs and shows that using this knowledge improves per-
formance on image classi   cation. we build on recent work
on end-to-end learning on graphs, introducing the graph
search neural network as a way of ef   ciently incorporating
large id13s into a vision classi   cation pipeline.
we show in a number of experiments that our method out-
performs standard neural network baselines for multi-label
classi   cation.

1. introduction

our world contains millions of visual concepts under-
stood by humans. these often are ambiguous (tomatoes
can be red or green), overlap (vehicles includes both cars
and planes) and have dozens or hundreds of subcategories
(thousands of speci   c kinds of insects). while some vi-
sual concepts are very common such as person or car, most
categories have many fewer examples, forming a long-tail
distribution [37]. and yet, even when only shown a few
or even one example, humans have the remarkable ability
to recognize these categories with high accuracy. in con-
trast, while modern learning-based approaches can recog-
nize some categories with high accuracy, it usually requires
thousands of labeled examples for each of these categories.
given how large, complex and dynamic the space of visual
concepts is, this approach of building large datasets for ev-
ery concept is unscalable. therefore, we need to ask what
humans have that current approaches do not.

one possible answer to this is structured knowledge and
reasoning. humans are not merely appearance-based classi-
   ers; we gain knowledge of the world from experience and

figure 1. example of how semantic knowledge about the world
aids classi   cation. here we see an elephant shrew. humans are
able to make the correct classi   cation based on what we know
about the elephant shrew and other similar animals.

language. we use this knowledge in our everyday lives to
recognize objects. for instance, we might have read in a
book about the    elephant shrew    (maybe even seen an ex-
ample) and will have gained knowledge that is useful for
recognizing one. figure 1 illustrates how we might use our
knowledge about the world in this problem. we might know
that an elephant shrew looks like a mouse, has a trunk and
a tail, is native to africa, and is often found in bushes. with
this information, we could probably identify the elephant
shrew if we saw one in the wild. we do this by    rst rec-
ognizing (we see a small mouse-like object with a trunk
in a bush), recalling knowledge (we think of animals we
have heard of and their parts, habitat, and characteristics)
and then reasoning (it is an elephant shrew because it has
a trunk and a tail, and looks like a mouse while mice and
elephants do not have all these characteristics). with this
information, even if we have only seen one or two pictures
of this animal, we would be able to classify it.

there has been a lot of work in end-to-end learning on
graphs or neural network trained on graphs [31, 2, 6, 11,
25, 22, 9, 21]. most of these approaches either extract fea-
tures from the graph or they learn a propagation model that
transfers evidence between nodes conditional on the type of
edge. an example of this is the gated graph neural net-
work [18] which takes an arbitrary graph as input. given

1

eleph-antlargebushdetectionhashas attributelooks likefound inmousetrunktailprediction: elephant shrew elp. shrewsmallsome initialization speci   c to the task, it learns how to prop-
agate information and predict the output for every node in
the graph. this approach has been shown to solve basic
logical tasks as well as program veri   cation.

our work improves on this model and adapts end-to-end
graph neural networks to multi-label image classi   cation.
we introduce the graph search neural network (gsnn)
which uses features from the image to ef   ciently anno-
tate the graph, select a relevant subset of the input graph
and predict outputs on nodes representing visual concepts.
these output states are then used to classify the objects in
the image. gsnn learns a propagation model which rea-
sons about different types of relationships and concepts to
produce outputs on the nodes which are then used for im-
age classi   cation. our new architecture mitigates the com-
putational issues with the gated graph neural networks
for large graphs which allows our model to be ef   ciently
trained for image tasks using large id13s. we
show how our model is effective at reasoning about con-
cepts to improve image classi   cation tasks.
importantly,
our gsnn model is also able to provide explanations on
classi   cations by following how the information is propa-
gated in the graph.

the major contributions of this work are (a) the introduc-
tion of the gsnn as a way of incorporating potentially large
id13s into an end-to-end learning system that is
computationally feasible for large graphs; (b) a framework
for using noisy id13s for image classi   cation;
and (c) the ability to explain our image classi   cations by
using the propagation model. our method signi   cantly out-
performs baselines for multi-label classi   cation.

2. related work

learning id13s [4, 3, 30] and using graphs
for visual reasoning [37, 20] has recently been of interest
to the vision community. for reasoning on graphs, several
approaches have been studied. for example, [38] collects
a knowledge base and then queries this knowledge base to
do    rst-order probabilistic reasoning to predict affordances.
[20] builds a graph of exemplars for different categories and
uses the spatial relationships to perform contextual reason-
ing. approaches such as [17] use id93 on the
graphs to learn patterns of edges while performing the walk
and predict new edges in the id13. there has
also been some work using a knowledge base for image
retrieval [12] or answering visual queries [39], but these
works are focused on building and then querying knowl-
edge bases rather than using existing knowledge bases as
side information for some vision task.

however, none of these approaches have been learned
in an end-to-end manner and the propagation model on the
graph is mostly hand-crafted. more recently, learning from
id13s using neural networks and other end-to-

end learning systems to perform reasoning has become an
active area of research. several works treat graphs as a spe-
cial case of a convolutional input where, instead of pixel
inputs connected to pixels in a grid, we de   ne the inputs as
connected by an input graph, relying on either some global
graph structure or doing some sort of pre-processing on
graph edges [2, 6, 11, 25]. however, most of these ap-
proaches have been tried on smaller, cleaner graphs such as
molecular datasets. in vision problems, these graphs encode
contextual and common-sense relationships and are signi   -
cantly larger and noisier.

li and zemel present graph gated neural networks
(ggnn) [18] which uses neural networks on graph struc-
tured data. this paper (an extension of graph neural
networks [31]) serves as the foundation for our graph
search neural network (gsnn). several papers have found
success using variants of graph neural networks applied
to various simple domains such as quantitative structure-
property relationship (qspr) analysis in chemistry [22]
and subgraph matching and other graph problems on toy
datasets [9]. ggnn is a fully end-to-end network that takes
as input a directed graph and outputs either a classi   ca-
tion over the entire graph or an output for each node. for
instance, for the problem of graph reachability, ggnn is
given a graph, a start node and end node, and the ggnn
will have to output whether the end node is reachable from
the start node. they show results for logical tasks on graphs
and more complex tasks such as program veri   cation.

there is also a substantial amount of work on vari-
ous types of kernels de   ned for graphs [36] such as diffu-
sion kernels [14], graphlet kernels [33], weisfeiler-lehman
graph kernels [32], deep graph kernels [27], graph invari-
ant kernels [26] and shortest-path kernels [1]. the methods
have various ways of exploiting common graph structures,
however, these approaches are only helpful for kernel-based
approaches such as id166s which do not compare well with
neural network architectures in vision.

our work is also related to attribute approaches [8] to
vision such as [16] which uses a    xed set of binary at-
tributes to do zero-shot prediction, [34] which uses at-
tributes shared across categories to prevent semantic drift
in semi-supervised learning and [5] which automatically
discovers attributes and uses them for    ne-grained classi-
   cation. our work also uses attribute relationships that ap-
pear in our id13s, but also uses relationships
between objects and reasons directly on graphs rather than
using object-attribute pairs directly.

3. methodology
3.1. graph gated neural network

the idea of ggnn is that given a graph with n nodes,
we want to produce some output which can either be an
output for every graph node o1, o2, ...on or a global output

2

og. this is done by learning a propagation model similar
to an lstm. for each node in the graph v, we have a hid-
den state representation h(t)
v at every time step t. we start
at t = 0 with initial hidden states xv that depends on the
problem. for instance, for learning graph reachability, this
might be a two bit vector that indicates whether a node is
the source or destination node. in case of visual knowledge
graph reasoning, xv can be a one bit activation represent-
ing the con   dence of a category being present based on an
object detector or classi   er.

1

n

]t + b

v , 0]t
...h(t   1)

next, we use the structure of our graph, encoded in a ma-
trix a which serves to retrieve the hidden states of adjacent
nodes based on the edge types between them. the hidden
states are then updated by a gated update module similar to
an lstm. the basic recurrence for this propagation net-
work is
(1)
(2)
(3)
(4)
(5)
(6)

h(1)
v = [xt
v [h(t   1)
a(t)
v = at
zt
v =   (w za(t)
(cid:102)ht
rt
v =   (w ra(t)
v = tanh(w a(t)
v = (1     zt
h(t)
where h(t)
is the hidden state for node v at time step t, xv is
v
the problem speci   c annotation, av is the adjacency matrix
of the graph for node v, and w and u are learned param-
eters. eq 1 is the initialization of the hidden state with xv
and empty dimensions. eq 2 shows the propagation updates
from adjacent nodes. eq (3-6) combine the information
from adjacent nodes and current hidden state of the nodes
to compute the next hidden state.

v + u zh(t   1)
v + u rh(t   1)
v + u (rt
v) (cid:12) h(t   1)

v (cid:12)(cid:102)ht
v (cid:12) h(t   1)
+ zt

)
)

))

v

v

v

v

v

after t time steps, we have our    nal hidden states. the

node level outputs can then just be computed as

ov = g(h(t )

(7)
where g is a fully connected network, the output network,
and xv is the original annotation for the node.
3.2. graph search neural network

, xv)

v

the biggest problem in adapting ggnn for image tasks
is computational scalability. neil [4] for example has over
2000 concepts, and nell [3] has over 2m con   dent be-
liefs. even after pruning to our task, these graphs would
still be huge. forward propagation on the standard ggnn
is o(n 2) to the number of nodes n and backward propaga-
tion is o(n t ) where t is the number of propagation steps.
we perform simple experiments on ggnns on synthetic
graphs and    nd that after more than about 500 nodes, a for-
ward and backward pass takes over 1 second on a single
instance, even when making generous parameter assump-
tions. on 2,000 nodes, it takes well over a minute for a
single image. using ggnn out of the box is infeasible.

our solution to this problem is the graph search neu-
ral network (gsnn). as the name might imply, the idea is

3

that rather than performing our recurrent update over all of
the nodes of the graph at once, we start with some initial
nodes based on our input and only choose to expand nodes
which are useful for the    nal output. thus, we only com-
pute the update steps over a subset of the graph. so how do
we select which subset of nodes to initialize the graph with?
during training and testing, we determine initial nodes in
the graph based on likelihood of the concept being present
as determined by an object detector or classi   er. for our
experiments, we use faster r-id98 [28] for each of the 80
coco categories. for scores over some chosen threshold,
we choose the corresponding nodes in the graph as our ini-
tial set of active nodes.

once we have initial nodes, we also add the nodes adja-
cent to the initial nodes to the active set. given our initial
nodes, we want to    rst propagate the beliefs about our ini-
tial nodes to all of the adjacent nodes. after the    rst time
step, however, we need a way of deciding which nodes to
expand next. we therefore learn a per-node scoring func-
tion that estimates how    important    that node is. after each
propagation step, for every node in our current graph, we
predict an importance score

i(t)
v = gi(hv, xv)

(8)

where gi is a learned network, the importance network.

once we have values of iv, we take the top p scoring
nodes that have never been expanded and add them to our
expanded set, and add all nodes adjacent to those nodes
to our active set. figure 2 illustrates this expansion. at
t = 1 only the detected nodes are expanded. at t = 2 we
expand chosen nodes based on importance values and add
their neighbors to the graph. at the    nal time step t we
compute the per-node-output and re-order and zero-pad the
outputs into the    nal classi   cation net.

to train the importance net, we assign target importance
value to each node in the graph for a given image. nodes
corresponding to ground-truth concepts in an image are as-
signed an importance value of 1. the neighbors of these
nodes are assigned a value of   . nodes which are two-hop
away have value   2 and so on. the idea is that nodes closest
to the    nal output are the most important to expand.

we now have an end-to-end network which takes as input
a set of initial nodes and annotations and outputs a per-node
output for each of the active nodes in the graph. it consists
of three sets of networks: the propagation net, the impor-
tance net, and the output net. the    nal loss from the image
problem can be backpropagated from the    nal output of the
pipeline back through the output net and the importance loss
is backpropagated through each of the importance outputs.
see figure 3 to see the gsnn architecture. first xinit, the
detection con   dences initialize h(1)
init, the hidden states of
the initially detected nodes. we then initialize h(1)
adj1, the

takes in the hidden state and initial annotation of the node v
to compute its output. in a certain sense it is agnostic to the
meaning of the node. that is, at train or test time, gsnn
takes in a graph it has perhaps never seen before, and some
initial annotations xv for each node. it then uses the struc-
ture of the graph to propagate those annotations through the
network and then compute an output. the nodes of the
graph could have represented anything from human rela-
tionships to a computer program. however, in our graph
network, the fact that a particular node represents    horse   
or    cat    will probably be relevant, and we can also constrain
ourselves to a static graph over image concepts. hence we
introduce node bias terms that, for every node in our graph,
has some learned values. our output equations are now
g(h(t )
, xv, nv) where nv is a bias term that is tied to a par-
ticular node v in the overall graph. this value is stored in a
table and its value are updated by id26.
3.3. image pipeline and baselines

v

another problem we face adapting graph networks for
vision problems is how to incorporate the graph network
into an image pipeline. for classi   cation, this is fairly
straightforward. we take the output of the graph network,
reorder it so that nodes always appear in the same order
into the    nal network, and zero pad any nodes that were
not expanded. therefore, if we have a graph with 316 node
outputs, and each node predicts a 5-dim hidden variable, we
create a 1580-dim feature vector from the graph. we also
concatenate this feature vector with fc7 layer (4096-dim) of
a    ne-tuned vgg-16 network [35] and top-score for each
coco category predicted by faster r-id98 (80-dim). this
5756-dim feature vector is then fed into 1-layer    nal classi-
   cation network trained with dropout.

for baselines, we compare to: (1) vgg baseline - feed
just fc7 into    nal classi   cation net; (2) detection baseline -
feed fc7 and top coco scores into    nal classi   cation net.

4. results
4.1. datasets

for our experiments, we wanted to test on a dataset that
represents the complex, noisy visual world with its many
different kinds of objects, where labels are potentially am-
biguous and overlapping, and categories fall into a long-tail
distribution [37]. humans do well in this setting, but vision
algorithms still struggle with it. to this end, we chose the
visual genome dataset [15] v1.0.

visual genome contains over 100,000 natural images
from the internet. each image is labeled with objects, at-
tributes and relationships between objects entered by human
annotators. annotators could enter any object in the image
rather than from a prede   ned list, so as a result there are
thousands of object labels with some being more common

4

figure 2. graph search neural network expansion. starts with
detected nodes and expands neighbors. adds nodes adjacent to
expand nodes predicted by importance net.

figure 3. graph search neural network diagram. shows initial-
ization of hidden states, addition of new nodes as graph is ex-
panded and the    ow of losses through the output, propagation and
importance nets.

hidden states of the adjacent nodes, with 0. we then up-
date the hidden states using the propagation net. the values
of h(2) are then used to predict the importance scores i(1),
which are used to pick the next nodes to add adj2. these
nodes are then initialized with h(2)
adj2 = 0 and the hidden
states are updated again through the propagation net. after
t steps, we then take all of the accumulated hidden states
ht to predict the gsnn outputs for all the active nodes.
during id26, the binary cross id178 (bce)
loss is fed backward through the output layer, and the im-
portance losses are fed through the importance networks to
update the network parameters.

one    nal detail is the addition of a    node bias    into
, xv)

gsnn. in ggnn, the per-node output function g(h(t )

v

coco detections:personcarbicycledetected nodesnodes expanded by importancet=tpropagation stepoutput network00to classification netreorder and zero-padpropagation stept=2t=1   xinit0h(1)inith(1)adj1h(2)inith(2)adj1i(1)adj2top pimport. net0h(2)adj2h(2)adj1h(2)initi(2)adj3top ph(2)adj1h(2)inith(3)adj2h(3)adj1h(3)init   h(t-1)adjnh(t-1)adj1h(t-1)init   prop.neth(t)gsnn outxinit0h(1)inith(1)adj1h(2)inith(2)adj1i(1)adj2h(2)adj2i(2)adj3h(2)adj1h(2)inith(3)adj2h(3)adj1h(3)inith(t-1)adjnh(t-1)adj1prop.netprop.netclassification lossgtlabelsimportance lossimportance losspredicted labelsbce lossfinal classificationoutput netimport. netand most having many fewer examples. there are on aver-
age 21 labeled objects in an image, so compared to datasets
such as id163 [29] or pascal [7], the scenes we are
considering are far more complex. visual genome is also
labeled with object-object relationships and object-attribute
relationships which we use for gsnn.

in our experiments, we create a subset from visual
genome which we call visual genome multi-label dataset
or vgml. in vgml, we take the 200 most common ob-
jects in the dataset and the 100 most common attributes and
also add any coco categories not in those 300 for a total
of 316 visual concepts. our task is then multi-label clas-
si   cation: for each image predict which subset of the 316
total categories appear in the scene. we randomly split the
images into a roughly 80-20 train/test split. since we used
pre-trained detectors from coco, we ensure none of our
test images overlap with our detector   s training images.

we also evaluate out method on the more standard
coco dataset [19] to show that our approach is useful
on multiple datasets and that our method does not rely on
graphs built speci   cally for our datasets. we train and test
in the multi-label setting [24], and evaluate on the minival
set [28].

4.2. building the id13

we also use visual genome as a source for our knowl-
edge graph. using only the train split, we build a knowl-
edge graph connecting the concepts using the most com-
mon object-attribute and object-object relationships in the
dataset. speci   cally, we counted how often an object/object
relationship or object/attribute pair occurred in the training
set, and pruned any edges that had fewer than 200 instances.
this leaves us with a graph over all of the images with
each edge being a common relationship. the idea is that
we would get very common relationships (such as grass is
green or person wears clothes) but not relationships that are
rare and only occur in single images (such as person rides
zebra).

the visual genome graphs are useful for our problem
because they contain scene-level relationships between ob-
jects, e.g. person wears pants or    re hydrant is red and thus
allow the graph network to reason about what is in a scene.
however, it does not contain useful semantic relationships.
for instance, it might be helpful to know that dog is an ani-
mal if our visual system sees a dog and one of our labels is
animal. to address this, we also create a version of graph
by fusing the visual genome graphs with id138 [23].
using the subset of id138 from [10], we    rst collect new
nodes in id138 not in our output label by including those
which directly connect to our output labels and thus likely
to be relevant and add them to a combined graph. we then
take all of the id138 edges between these nodes and add
them to our combined graph.

4.3. training details

we jointly train all parts of the pipeline (except for the
detectors). all models are trained with stochastic gradient
descent, except gsnn which is trained using adam [13].
we use an initial learning rate of 0.05, 0.005 for the vgg
net before f c7, decreasing by a factor of 0.1 every 10
epochs, an l2 penalty of 1e   6 and a momentum of 0.5. we
set our gsnn hidden state size to 10, importance discount
factor    to 0.3, number of time steps t to 3, initial con   -
dence threshold to 0.5 and our expand number p to 5. our
gsnn importance and output networks are single layer net-
works with sigmoid activations. all networks were trained
for 20 epochs with a batch size of 16.
4.4. quantitative evaluation

table 1 shows the result of our method on visual
genome multi-label classi   cation. in this experiment, the
combined visual genome, id138 graph outperforms the
visual genome graph. this suggests that including the out-
side semantic knowledge from id138 and performing ex-
plicit reasoning on a id13 allows our model to
learn better representations compared to the other models.
we also perform experiments to test the effect of lim-
iting the size of the training dataset has on performance.
figure 4 shows the results of this experiment on visual
genome, varying the training set size from the entire train-
ing set (approximately 80,000), all the way down to 500
examples. choosing the subsets of examples for these ex-
periments is done randomly, but each training set is a subset
of the larger ones   e.g. all of the examples in the 1,000
set are also in the 2,000 set. we see that, until the 1,000
sample set, the gsnn-based methods all outperform base-
lines. at 1,000 and 500 examples, all of the methods per-
form equally. given the long-tail nature of visual genome,
it is likely that for fewer than 2,000 samples, many cate-
gories do not have enough examples for any method to learn
well. this experiment indicates that our method is able to
improve even in the low-data case up to a point.

in table 2, we show results on the coco multi-label
dataset. we can see that the boost from using graph knowl-
edge is more signi   cant than it was on visual genome. one
possible explanation is that the visual genome knowledge
graph provides signi   cant information which helps improve
the performance on the coco dataset itself. in the previ-
ous visual genome experiment, much of the graph informa-
tion is contained in the labels and images themselves. one
other interesting result is that the visual genome graph out-
performs the combined graph for coco, though both out-
perform baselines. one possible reason is that the original
vgml graph is smaller, cleaner, and contains more rele-
vant information than the combined graph. furthermore, in
the vgml experiment, id138 is new outside informa-
tion for the algorithm helping boost the performance.

5

table 1. mean average precision for multi-label classi   cation on
visual genome multi-label dataset. numbers for vgg baseline,
vgg baseline with detections, gsnn using visual genome graph
and gsnn using a combined visual genome and id138 graph.

method
vgg
vgg+det
gsnn-vg
gsnn-vg+wn 33

map
30.57
31.4
32.83

formly on all categories, but rather does better on some cat-
egories and worse on others. figure 5 shows the differences
in average precision for each category between our gsnn
model with the combined graph and the detection baseline
for the vgml experiment. figure 6 shows the same for
our coco experiment. performance on some classes im-
proves greatly, such as    fork    in our vgml experiment and
   scissors    in our coco experiment. these and other good
results on    knife    and    toothbrush    seem to indicate that
the graph reasoning helps especially with small objects in
the image. in the next section, we analyze our gsnn mod-
els on several examples to try to gain a better intuition as
to what the gsnn model is doing and why it does well or
poorly on certain examples.

4.5. qualitative evaluation

figure 4. mean average precision on visual genome in the low
data setting. shows performance for all methods for the full
dataset, 40,000, 20,000, 10,000, 5,000, 2,000, 1,000, and 500
training examples.

table 2. mean average precision for multi-label classi   cation on
coco. numbers for vgg baseline, vgg baseline with detec-
tions, gsnn using visual genome graph and gsnn using com-
bined visual genome and id138 graph.

map
method
69.86
vgg
73.93
vgg+det
77.57
gsnn-vg
gsnn-vg+wn 75.73

table 3. mean average precision for multi-label classi   cation on
coco, using only odd and even detectors.

method
vgg+det
gsnn-vg
gsnn-vg+wn 73.59

even map
71.87
73

odd map
71.73
73.43
73.97

one possible concern is the over dependence of the graph
reasoning on the set of 80 coco detectors and initial de-
tections. therefore, we performed an ablation experiment
to see how sensitive our method is to having all of the ini-
tial detections. we reran the coco experiments with both
graphs using two different subsets of coco detectors. the
   rst subset is just the even coco categories and the second
subset is just the odd categories. we see from table 3 that
gsnn methods again outperform the baselines.

as one might suspect, our method does not perform uni-

6

one way to analyse the gsnn is to look at the sensitiv-
ities of parameters in our model with respect to a particular
output. given a single image i, and a single label of in-
terest yi that appears in the image, we would like to know
how information travels through the gsnn and what nodes
and edges it uses. we examined the sensitivity of the out-
put to hidden states and detections by computing the partial
derivatives    yi
with respect to the category of
   h(1)
interest. these values tell us how a small change in the hid-
den state of a particular node affects a particular output. we
would expect to see, for instance, that for labeling elephant,
we see a high sensitivity for the hidden states corresponding
to grey and trunk.

   yi
   h(2)

   xdet

   yi

in this section, we show the sensitivity analysis for the
gsnn combined graph model on the vgml experiment
and the visual genome graph on the coco experiments.
in particular, we examine some classes that performed well
under gsnn compared to the detection baseline and a few
that performed poorly to try to get a better intuition into why
some categories improve more.

figure 7 shows the graph sensitivity analysis for the ex-
periments with vgml on the left and coco on the right,
showing four examples where gsnn does better and two
where it does worse. each example shows the image, the
ground truth output we are analyzing and the sensitivities
of the concept of interest with respect to the hidden states
of the graph or detections. for convenience, we display the
names of the top detections or hidden states. we also show
part of the graph that was expanded, to see what relation-
ships gsnn was using.

for the vgml experiment, the top left of figure 7 shows
that using the detection for person, gsnn is able to reason
that jeans are more likely since jeans are usually on people
in images using the    wearing    edge. it is also sensitive to
skateboard and horse, and each of these has a second or-
der connection to jeans through person, so it is likely able
to capture the fact that people tend to wear jeans while on

05101520253035full40,00020,00010,0005,0002,0001,000500maptraining set sizevisual genome low data experimentbaselinedetectgraph vgonlygraph combinedfigure 5. difference in average precision for each of the 316 labels in vgml between our gsnn combined graph model and detection
baseline for the visual genome experiment. top categories: scissors, donut, frisbee, microwave, fork. bottom categories: stacked, tiled,
light brown, ocean, grassy.

figure 6. difference in average precision for each of the 80 labels in coco between our gsnn vg graph model and detection baseline
for the coco experiment. top categories: fork, donut, cup, apple, microwave. bottom categories: hairdryer, parking meter, bear, kite, and
giraffe.

horses and skateboards. note that the sensitivities are not
the same as the actual detections, so it is not contradictory
that horse has high sensitivity. the second row on the left
shows a successful example for bicycle, using detections
from person and skateboard and the fact that people tend
to be    on    bicycles and skateboards. the last row shows
a failure case for windshield.
it correctly correlates with
bus, but because the id13 lacks a connection
between bus and windshield, the graph network is unable
to do better than the detection baseline. on the right, for
the coco experiment, the top example shows that fork is
highly correlated with the detection for fork, which should
not be surprising. however, it is able to reinforce this de-
tection with the connections between broccoli and dining
table, which are both two step connections to fork on the
graph. similarly, the middle example shows that the graph
connections for pizza, bowl, and bottle being    on    dining ta-
ble reinforce the detection of dining table. the bottom right
shows another failure case. it is able to get the connection
between the detection for toilet and hair dryer (both found
in the bathroom), but the lack of good connections in the
graph prevent the gsnn from improving over the baseline.

5. conclusion

in this paper, we present the graph search neural net-
work (gsnn) as a way of ef   ciently using knowledge

graphs as extra information to improve image classi   ca-
tion. we provide analysis that examines the    ow of infor-
mation through the gsnn and provides insights into why
our model improves performance. we hope that this work
provides a step towards bringing symbolic reasoning into
traditional feed-forward id161 frameworks.

the gsnn and the framework we use for vision prob-
lems is completely general. our next steps will be to apply
the gsnn to other vision tasks, such as detection, visual
id53, and image captioning. another inter-
esting direction would be to combine the procedure of this
work with a system such as neil [4] to create a system
which builds id13s and then prunes them to get
a more accurate, useful graph for image tasks.

acknowledgements: we would like to thank everyone who took time to
review this work and provide helpful comments. this research is based
upon work supported in part by the of   ce of the director of national
intelligence (odni), intelligence advanced research projects activity
(iarpa). the views and conclusions contained herein are those of the au-
thors and should not be interpreted as necessarily representing the of   cial
policies, either expressed or implied of odni, iarpa, or the us govern-
ment. the us government is authorized to reproduce and distribute the
reprints for governmental purposed notwithstanding any copyright annota-
tion therein. this material is based upon work supported by the national
science foundation graduate research fellowship under grant no. dge-
1252522 and onr muri n000141612007.

7

-0.1-0.0500.050.10.150.20.250.30.35stackedwindshieldtilehillgreytowerearlegtailgreencoatdarkskiingoutsidewheelfloordistantherecounterrockslargelitceilingontopwalldoorwatertowelwoodenshoesbasketwheelsheadlightpantstreescloudtrackssilvercalmgrazingsinkboatkiteplantbirdwine glasspizzacell phonebackpackbicyclefire hydrantmicrowaveap  improvementcategory-0.02-0.0100.010.020.030.040.050.060.070.08hair drierbeargiraffesurfboardzebrabedpersonfire hydrantelephantskissinkairplanetruckorangebenchtielaptopstop signchairbookrefrigeratorhot dogtvsuitcasetoasterfrisbeebroccolibackpackmousepotted plantbicyclecarsheepspoonteddy bearknifebottletoothbrushappledonutap improvementcategoryfigure 7. sensitivity analysis of gsnn in vgml experiment (left) and coco experiment (right) with the combined graph and visual
genome graphs respectively. each example shows the image, part of the id13 expanded during the classi   cation, and the
sensitivity values of the initial detections, and the hidden states at time steps 2 and 3 with respect to the output class listed. the top
detections and hidden state nodes are printed for convenience since the x-axis is too large to list every class. the top and middle rows
show the results for images and classes where the gsnn signi   cantly outperforms the detection baseline to get an intuition for when our
method is working. the bottom row shows images and classes where gsnn does worse than the detection baseline to get an idea of when
our method fails and why.

8

motorcyclewhiteskateboardpersonbusdetectionhidden t=2hidden t=3streetpersonhorsedetectionhidden t=2hidden t=3blackskateboardjeanscapoldpersonbicycleblackparkeddetectionhidden t=2hidden t=3vgml top:parkedoldstop signblackpersontop:bicycleskateboardhorseumbrellafrisbeetop:busskateboardmotorcyclehorseclocktop:personblackrunningoldoceantop:horseskateboardpersoncapfoottop:personskateboardhorsebirdbenchtop:parkedbicycleskateboardhorseshirttop:buswhiterunningpersonblacktop:streetmotorcyclehorseclockshirtpersonsurfboardbaseball gloveumbrellaremoteblackdetectionhidden t=2hidden t=3bluebroccolipizzaforkdetectionhidden t=2hidden t=3dining tablebookplategreenbottlebowlbookpizzadining tablerounddetectionhidden t=2hidden t=3cocotop:bookroundlargevisiblecookedtop:dining tablepizzabowlcakebottletop:toiletremotepersonbaseball glovebackpacktop:pizzabroccolibookgreenplatetop:forkgreenpizzabroccolibooktop:forkbroccolidining tablezebramousetop:dining tablepizzabookbowlcaketop:blackbaseball gloveumbrellasurfboardpersontop:toiletblackblueremotebaseball glovegraph legendhas attributeonhaswearingholdingfootskateboardreferences
[1] k. m. borgwardt and h.-p. kriegel. shortest-path kernels on

graphs. icdm, 2005.

[2] j. bruna, w. zaremba, a. szlam, and y. lecun. spectral
networks and locally connected networks on graphs. arxiv
preprint arxiv:1312.6203, 2013.

[3] a. carlson, j. betteridge, b. kisiel, b. settles, e. r. hr-
uschka, and t. m. mitchell. toward an architecture for never-
ending language learning. aaai, 2010.

[4] x. chen, a. shrivastava, and a. gupta. neil: extracting

visual knowledge from web data. cvpr, 2013.

[5] k. duan, d. parikh, d. crandall, and k. grauman. discover-
ing localized attributes for    ne-grained recognition. cvpr,
2012.

[6] d. k. duvenaud, d. maclaurin, j. iparraguirre, r. bom-
barell, t. hirzel, a. aspuru-guzik, and r. p. adams. con-
volutional networks on graphs for learning molecular    nger-
prints. nips, 2015.

[7] m. everingham, l. van gool, c. k. i. williams, j. winn,
and a. zisserman. the pascal visual object classes
challenge 2012 (voc2012) results.
http://www.pascal-
network.org/challenges/voc/voc2012/workshop/index.html.
[8] a. farhadi, i. endres, d. hoiem, and d. forsyth. describing

objects by their attributes. cvpr, 2009.

[9] m. gori, g. monfardini, and f. scarselli. a new model for
learning in graph domains. ieee international joint confer-
ence on neural networks, 2, 2005.

[10] k. guu, j. miller, and p. liang. traversing id13s
in vector space. in empirical methods in natural language
processing (emnlp), 2015.

[11] m. henaff, j. bruna, and y. lecun.

deep convolu-
tional networks on graph-structured data. arxiv preprint
arxiv:1506.05163, 2015.

[12] j. johnson, r. krishna, m. stark, l.-j. li, d. a. shamma,
m. s. bernstein, and l. fei-fei. id162 using scene
graphs. cvpr, 2015.

[13] d. p. kingma and j. l. ba. adam: a method for stochastic

optimization. iclr, 2015.

[14] r. i. kondor and j. lafferty. diffusion kernels on graphs and

other discrete input spaces. icml, 2, 2002.

[15] r. krishna, y. zhu, o. groth, j. johnson, k. hata, j. kravitz,
s. chen, y. kalantidis, l.-j. li, d. a. shamma, m. bern-
stein, and l. fei-fei. visual genome: connecting language
and vision using crowdsourced dense image annotations.
2016.

[16] c. h. lampert, h. nickisch, and s. harmeling. attribute-
based classi   cation for zero-shot visual object categoriza-
tion. tpami, 2014.

[17] n. lao, t. mitchell, and w. w. cohen. random walk in-
ference and learning in a large scale knowledge base. nips,
2011.

[18] y. li and r. zemel. gated graph sequence neural networks.

iclr, 2016.

[19] t. lin, m. maire, s. j. belongie, r. b. girshick, j. hays,
p. perona, d. ramanan, p. doll  ar, and c. l. zitnick. mi-
crosoft coco: common objects in context. eccv, 2014.

9

[20] t. malisiewicz and a. efros. beyond categories: the vi-
sual memex model for reasoning about object relationships.
nips, 2009.

[21] v. d. massa, g. monfardini, l. sarti, f. scarselli, m. mag-
gini, and m. gori. a comparison between recursive neu-
ral networks and graph neural networks. ieee international
joint conference on neural network proceedings, 2006.

[22] a. micheli. neural network for graphs: a contextual con-
structive approach. ieee transactions on neural networks,
2009.

[23] g. a. miller. id138: a lexical database for english. acm,

38, 1995.

[24] i. misra, c. l. zitnick, m. mitchell, and r. girshick. seeing
through the human reporting bias: visual classi   ers from
noisy human-centric labels. in cvpr, 2016.

[25] m. niepert, m. ahmed, and k. kutzkov. learning con-
arxiv preprint

volutional neural networks for graphs.
arxiv:1605.05273, 2016.

[26] f. orsini, p. frasconi, and l. d. raedt. graph invariant ker-

nels. ijcai, 2015.

[27] pinar, yanardag, and s. v. n. vishwanathan. deep graph

kernels. kddm, 2015.

[28] s. ren, k. he, r. girshick, and j. sun. faster r-id98: to-
wards real-time id164 with region proposal net-
works. nips, 2015.

[29] o. russakovsky, j. deng, h. su, j. krause, s. satheesh,
s. ma, z. huang, a. karpathy, a. khosla, m. bernstein,
a. c. berg, and l. fei-fei.
id163 large scale visual
recognition challenge. ijcv, 115(3):211   252, 2015.

[30] f. sadeghi, s. k. divvala, and a. farhadi. viske: visual
knowledge extraction and id53 by visual ver-
i   cation of relation phrases. cvpr, 2015.

[31] f. scarselli, m. gori, a. c. tsoi, and g. monfardini. the
graph neural network model. ieee transactions on neural
networks, 2009.

[32] n. shervashidze, p. schweitzer, e. j. van leeuwen,
k. mehlhorn, and k. m. borgwardt. weisfeiler-lehman
graph kernels. jmlr, 2011.

[33] n. shervashidze, s. v. n. vishwanathan, t. h. petri,
k. mehlhorn, and k. m. borgwardt. ef   cient graphlet ker-
nels for large graph comparison. aistats, 5, 2009.

[34] a. shrivastava, s. singh, and a. gupta. constrained semi-
supervised learning using attributes and comparative at-
tributes. eccv, 2012.

[35] k. simonyan and a. zisserman. very deep convolutional
networks for large-scale image recognition. arxiv preprint
arxiv:1409.1556, 2014.

[36] s. v. n. vishwanathan, n. n. schraudolph, r. kondor, and

k. m. borgwardt. graph kernels. jmlr, 2010.

[37] x. zhu, d. anguelov, and d. ramanan. capturing long-tail

distributions of object subcategories. cvpr, 2014.

[38] y. zhu, a. fathi, and l. fei-fei. reasoning about object af-
fordances in a knowledge base representation. in european
conference on id161, 2014.

[39] y. zhu, c. zhang, c. r, and l. fei-fei. building a large-
scale multimodal knowledge base system for answering vi-
sual queries. arxiv preprint arxiv:1507.05670, 2015.

