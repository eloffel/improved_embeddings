automated phrase mining from massive text corpora

jingbo shang1, jialu liu2, meng jiang1, xiang ren1, clare r voss3, jiawei han1

1computer science department, university of illinois at urbana-champaign, il, usa

2google research, new york city, ny, usa

3computational & information sciences directorate, army research laboratory
2jialu@google.com

1{shang7, mjiang89, xren7, hanj}@illinois.edu

3clare.r.voss.civ@mail.mil

7
1
0
2

 
r
a

 

m
1
1

 
 
]
l
c
.
s
c
[
 
 

2
v
7
5
4
4
0

.

2
0
7
1
:
v
i
x
r
a

abstract
as one of the fundamental tasks in text analysis, phrase
mining aims at extracting quality phrases from a text corpus.
phrase mining is important in various tasks such as informa-
tion extraction/retrieval, taxonomy construction, and topic
modeling. most existing methods rely on complex, trained
linguistic analyzers, and thus likely have unsatisfactory per-
formance on text corpora of new domains and genres without
extra but expensive adaption. recently, a few data-driven
methods have been developed successfully for extraction of
phrases from massive domain-speci   c text. however, none
of the state-of-the-art models is fully automated because
they require human experts for designing rules or labeling
phrases.

since one can easily obtain many quality phrases from
public knowledge bases to a scale that is much larger than
that produced by human experts, in this paper, we propose
a novel framework for automated phrase mining, autophrase,
which leverages this large amount of high-quality phrases in
an e   ective way and achieves better performance compared
to limited human labeled phrases. in addition, we develop a
pos-guided phrasal segmentation model, which incorporates
the shallow syntactic information in part-of-speech (pos)
tags to further enhance the performance, when a pos tagger
is available. note that, autophrase can support any language
as long as a general knowledge base (e.g., wikipedia) in that
language is available, while bene   ting from, but not requiring,
a pos tagger. compared to the state-of-the-art methods,
the new method has shown signi   cant improvements in e   ec-
tiveness on    ve real-world datasets across di   erent domains
and languages.

1.

introduction

phrase mining refers to the process of automatic extrac-
tion of high-quality phrases (e.g., scienti   c terms and general
entity names) in a given corpus (e.g., research papers and
news). representing the text with quality phrases instead

of id165s can improve computational models for applica-
tions such as information extraction/retrieval, taxonomy
construction, and id96.

almost all the state-of-the-art methods, however, require
human experts at certain levels. most existing methods [9,
20, 25] rely on complex, trained linguistic analyzers (e.g.,
dependency parsers) to locate phrase mentions, and thus
may have unsatisfactory performance on text corpora of new
domains and genres without extra but expensive adaption.
our latest domain-independent method segphrase [13] out-
performs many other approaches [9, 20, 25, 5, 19, 6], but
still needs domain experts to    rst carefully select hundreds
of varying-quality phrases from millions of candidates, and
then annotate them with binary labels.

such reliance on manual e   orts by domain and linguis-
tic experts becomes an impediment for timely analysis of
massive, emerging text corpora in speci   c domains. an
ideal automated phrase mining method is supposed to be
domain-independent, with minimal human e   ort or reliance
on linguistic analyzers1. bearing this in mind, we propose
a novel automated phrase mining framework autophrase in
this paper, going beyond segphrase, to further get rid of ad-
ditional manual labeling e   ort and enhance the performance,
mainly using the following two new techniques.
1. robust positive-only distant training.

in fact, many
high-quality phrases are freely available in general knowl-
edge bases, and they can be easily obtained to a scale
that is much larger than that produced by human ex-
perts. domain-speci   c corpora usually contain some qual-
ity phrases also encoded in general knowledge bases, even
when there may be no other domain-speci   c knowledge
bases. therefore, for distant training, we leverage the
existing high-quality phrases, as available from general
knowledge bases, such as wikipedia and freebase, to get
rid of additional manual labeling e   ort. we independently
build samples of positive labels from general knowledge
bases and negative labels from the given domain corpora,
and train a number of base classi   ers. we then aggregate
the predictions from these classi   ers, whose independence
helps reduce the noise from negative labels.

2. pos-guided phrasal segmentation. there is a trade-
o    between the performance and domain-independence
when incorporating linguistic processors in the phrase
mining method. on the domain independence side, the

acm isbn 978-1-4503-2138-9.
doi: 10.1145/1235

1the phrase    minimal human e   ort    indicates using only
existing general knowledge bases without any other human
e   ort.

accuracy might be limited without linguistic knowledge. it
is di   cult to support multiple languages, if the method is
completely language-blind. on the accuracy side, relying
on complex, trained linguistic analyzers may hurt the
domain-independence of the phrase mining method. for
example, it is expensive to adapt dependency parsers to
special domains like clinical reports. as a compromise, we
propose to incorporate a pre-trained part-of-speech (pos)
tagger to further enhance the performance, when it is
available for the language of the document collection. the
pos-guided phrasal segmentation leverages the shallow
syntactic information in pos tags to guide the phrasal
segmentation model locating the boundaries of phrases
more accurately.
in principle, autophrase can support any language as long
as a general knowledge base in that language is available. in
fact, at least 58 languages have more than 100,000 articles
in wikipedia as of feb, 20172. moreover, since pre-trained
part-of-speech (pos) taggers are widely available in many
languages (e.g., more than 20 languages in treetagger [22]3),
the pos-guided phrasal segmentation can be applied in many
scenarios. it is worth mentioning that for domain-speci   c
knowledge bases (e.g., mesh terms in the biomedical domain)
and trained pos taggers, the same paradigm applies. in
this study, without loss of generality, we only assume the
availability of a general knowledge base together with a
pre-trained pos tagger.

and analyze its major challenges as above.

our main contributions are highlighted as follows:

as demonstrated in our experiments, autophrase not only
works e   ectively in multiple domains like scienti   c papers,
business reviews, and wikipedia articles, but also supports
multiple languages, such as english, spanish, and chinese.
    we study an important problem, automated phrase mining,
    we propose a robust positive-only distant training method
for phrase quality estimation to minimize the human e   ort.
    we develop a novel phrasal segmentation model to leverage
pos tags to achieve further improvement, when a pos
tagger is available.
    we demonstrate the robustness and accuracy of our method
and show improvements over prior methods, with results
of experiments conducted on    ve real-world datasets in
di   erent domains (scienti   c papers, business reviews, and
wikipedia articles) and di   erent languages (english, span-
ish, and chinese).

the rest of the paper is organized as follows. section 2
positions our work relative to existing works. section 3
de   nes basic concepts including four requirements of phrases.
the details of our method are covered in section 4. extensive
experiments and case studies are presented in section 5. we
conclude the study in section 7.

has conducted extensive studies typically referred to as au-
tomatic term recognition [9, 20, 25], for the computational
task of extracting terms (such as technical phrases). this
topic also attracts attention in the information retrieval (ir)
community [7, 19] since selecting appropriate indexing terms
is critical to the improvement of search engines where the
ideal indexing units represent the main concepts in a corpus,
not just literal bag-of-words.

text indexing algorithms typically    lter out stop words and
restrict candidate terms to noun phrases. with pre-de   ned
part-of-speech (pos) rules, one can identify noun phrases
as term candidates in pos-tagged documents. supervised
noun phrase chunking techniques [21, 24, 3] exploit such
tagged documents to automatically learn rules for identifying
noun phrase boundaries. other methods may utilize more
sophisticated nlp technologies such as id33
to further enhance the precision [11, 16]. with candidate
terms collected, the next step is to leverage certain statistical
measures derived from the corpus to estimate phrase quality.
some methods rely on other reference corpora for the calibra-
tion of    termhood    [25]. the dependency on these various
kinds of linguistic analyzers, domain-dependent language
rules, and expensive human labeling, makes it challenging to
extend these approaches to emerging, big, and unrestricted
corpora, which may include many di   erent domains, topics,
and languages.

to overcome this limitation, data-driven approaches opt
instead to make use of frequency statistics in the corpus to
address both candidate generation and quality estimation [5,
19, 6, 13]. they do not rely on complex linguistic feature
generation, domain-speci   c rules or extensive labeling e   orts.
instead, they rely on large corpora containing hundreds
of thousands of documents to help deliver superior perfor-
mance [13]. in [19], several indicators, including frequency
and comparison to super/sub-sequences, were proposed to
extract id165s that reliably indicate frequent, concise con-
cepts. deane [5] proposed a heuristic metric over frequency
distribution based on zip   an ranks, to measure lexical associ-
ation for phrase candidates. as a preprocessing step towards
topical phrase extraction, el-kishky et al. [6] proposed to
mine signi   cant phrases based on frequency as well as doc-
ument context following a bottom-up fashion, which only
considers a part of quality phrase criteria, popularity and
concordance. our previous work [13] succeeded at integrat-
ing phrase quality estimation with phrasal segmentation to
further rectify the initial set of statistical features, based on
local occurrence context. unlike previous methods which
are purely unsupervised,
[13] required a small set of phrase
labels to train its phrase quality estimator. it is worth men-
tioning that all these approaches still depend on the human
e   ort (e.g., setting domain-sensitive thresholds). therefore,
extending them to work automatically is challenging.

2. related work

identifying quality phrases e   ciently has become ever
more central and critical for e   ective handling of massively
increasing-size text datasets. in contrast to keyphrase ex-
traction [17, 23, 14], this task goes beyond the scope of
single documents and provides useful cross-document sig-
nals. the natural language processing (nlp) community
2https://meta.wikimedia.org/wiki/list_of_wikipedias
3http://www.cis.uni-muenchen.de/~schmid/tools/
treetagger/

3. preliminaries

the goal of this paper is to develop an automated phrase
mining method to extract quality phrases from a large collec-
tion of documents without human labeling e   ort, and with
only limited, shallow linguistic analysis. the main input to
the automated phrase mining task is a corpus and a knowl-
edge base. the input corpus is a textual word sequence
in a particular language and a speci   c domain, of arbitrary
length. the output is a ranked list of phrases with decreasing
quality.

figure 1: the overview of autophrase. the two novel techniques developed in this paper are highlighted.
the autophrase framework is shown in figure 1. the
work    ow is completely di   erent form our previous domain-
independent phrase mining method requiring human ef-
fort [13], although the phrase candidates and the features
used during phrase quality (re-)estimation are the same. in
this paper, we propose a robust positive-only distant train-
ing to minimize the human e   ort and develop a pos-guided
phrasal segmentation model to improve the model perfor-
mance. in this section, we brie   y introduce basic concepts
and components as preliminaries.

the minimum support threshold    (e.g., 30) in the corpus.
here, this threshold refers to raw frequency of the id165s
calculated by string matching.
in practice, one can also
set a phrase length threshold (e.g., n     6) to restrict the
number of words in any phrase. given a phrase candidate
w1w2 . . . wn, its phrase quality is:

q(w1w2 . . . wn) = p(dw1w2 . . . wnc|w1w2 . . . wn)     [0, 1]

frequency in the given document collection.

a phrase is de   ned as a sequence of words that appear
consecutively in the text, forming a complete semantic unit
in certain contexts of the given documents [8]. the phrase
quality is de   ned to be the id203 of a word sequence
being a complete semantic unit, meeting the following crite-
ria [13]:
    popularity: quality phrases should occur with su   cient
    concordance: the collocation of tokens in quality phrases
occurs with signi   cantly higher id203 than expected
due to chance.
    informativeness: a phrase is informative if it is indica-
    completeness: long frequent phrases and their subse-
quences within those phrases may both satisfy the 3 criteria
above. a phrase is deemed complete when it can be inter-
preted as a complete semantic unit in some given document
context. note that a phrase and a subphrase contained
within it, may both be deemed complete, depending on
the context in which they appear. for example,    relational
database system   ,    relational database    and    database sys-
tem    can all be valid in certain context.

tive of a speci   c topic or concept.

autophrase will estimate the phrase quality based on the
positive and negative pools twice, once before and once
after the pos-guided phrasal segmentation. that is, the
pos-guided phrasal segmentation requires an initial set of
phrase quality scores; we estimate the scores based on raw
frequencies beforehand; and then once the feature values
have been recti   ed, we re-estimate the scores.

only the phrases satisfying all above requirements are

recognized as quality phrases.

example 1.    strong tea    is a quality phrase while    heavy
tea    fails to be due to concordance.    this paper    is a popular
and concordant phrase, but is not informative in research
publication corpus.    np-complete in the strong sense    is a
quality phrase while    np-complete in the strong    fails to be
due to completeness. (cid:3)

to automatically mine these quality phrases, the    rst phase
of autophrase (see leftmost box in figure 1) establishes the
set of phrase candidates that contains all id165s over

where dw1w2 . . . wnc refers to the event that these words
constitute a phrase. q(  ), also known as the phrase quality
estimator, is initially learned from data based on statistical
features4, such as point-wise mutual information, point-wise
kl divergence, and inverse document frequency, designed
to model concordance and informativeness mentioned above.
note the phrase quality estimator is computed independent
of pos tags. for unigrams, we simply set their phrase quality
as 1.

example 2. a good quality estimator will return q(this paper)    

0 and q(relational database system)     1. (cid:3)

then, to address the completeness criterion, the phrasal
segmentation    nds the best segmentation for each sentence.
example 3. ideal phrasal segmentation results are as fol-

lows.

... / the / great firewall / is / ...

#1:
#2: this / is / a / great /    rewall software/ .
#3: the / discriminative classi   er / id166 / is / ...

(cid:3)

during the phrase quality re-estimation, related sta-
tistical features will be re-computed based on the recti   ed
frequency of phrases, which means the number of times that
a phrase becomes a complete semantic unit in the identi   ed
segmentation. after incorporating the recti   ed frequency, the
phrase quality estimator q(  ) also models the completeness
in addition to concordance and informativeness.

example 4. continuing the previous example, the raw
frequency of the phrase    great    rewall    is 2, but its recti   ed
frequency is 1. both the raw frequency and the recti   ed
frequency of the phrase       rewall software    are 1. the raw
frequency of the phrase    classi   er id166    is 1, but its recti   ed
frequency is 0. (cid:3)

4. methodology

in this section, we focus on introducing our two new tech-

niques.
4see https://github.com/shangjingbo1226/autophrase for
further details

phrase candidates (frequent     -gram)speaksat a townbarackobamaanderson cooperobama administrationthisisan exampleus president   robustpositive-only distant training   0.99 us president0.98 anderson cooper0.98barackobama   0.85 obamaadministration   0.3 speaksat 0.2 a town   pos-guided phrasalsegmentationus president/ barack obama/ speaks/ at / a/ townhallmeeting/ with/ id98 / 's /andersoncooper/    the / obamaadministration/ may / be/ winding / down/ but/first lady/ michelle obama/is / keeping /       positivepoolbarackobamaanderson cooperus president   massive textcorporanoisy negative poolspeak ata townobama administration   phrase qualityre-estimationknowledge basesfigure 2: the illustration of each base classi   er. in
each base classi   er, we    rst randomly sample k pos-
itive and negative labels from the pools respectively.
there might be    quality phrases among the k neg-
ative labels. an unpruned decision tree is trained
based on this perturbed training set.
4.1 robust positive-only distant training

to estimate the phrase quality score for each phrase can-
didate, our previous work [13] required domain experts to
   rst carefully select hundreds of varying-quality phrases from
millions of candidates, and then annotate them with binary
labels. for example, for computer science papers, our domain
experts provided hundreds of positive labels (e.g.,    spanning
tree    and    computer science   ) and negative labels (e.g.,    pa-
per focuses    and    important form of    ). however, creating
such a label set is expensive, especially in specialized do-
mains like clinical reports and business reviews, because this
approach provides no clues for how to identify the phrase
candidates to be labeled.
in this paper, we introduce a
method that only utilizes existing general knowledge bases
without any other human e   ort.
4.1.1 label pools
public knowledge bases (e.g., wikipedia) usually encode
a considerable number of high-quality phrases in the titles,
keywords, and internal links of pages. for example, by ana-
lyzing the internal links and synonyms5 in english wikipedia,
more than a hundred thousand high-quality phrases were
discovered. as a result, we place these phrases in a positive
pool.

knowledge bases, however, rarely, if ever, identify phrases
that fail to meet our criteria, what we call inferior phrases.
an important observation is that the number of phrase can-
didates, based on id165s (recall leftmost box of figure 1),
is huge and the majority of them are actually of of inferior
quality (e.g.,    francisco opera and   ).
in practice, based
on our experiments, among millions of phrase candidates,
usually, only about 10% are in good quality. therefore,
phrase candidates that are derived from the given corpus
but that fail to match any high-quality phrase derived from
the given knowledge base, are used to populate a large but
noisy negative pool.
4.1.2 noise reduction
directly training a classi   er based on the noisy label pools
is not a wise choice: some phrases of high quality from
the given corpus may have been missed (i.e., inaccurately
binned into the negative pool) simply because they were
not present in the knowledge base.
instead, we propose
to utilize an ensemble classi   er that averages the results
of t independently trained base classi   ers. as shown in
figure 2, for each base classi   er, we randomly draw k phrase
candidates with replacement from the positive pool and the
negative pool respectively (considering a canonical balanced
classi   cation scenario). this size-2k subset of the full set of
5https://github.com/kno10/wikipediaentities

  

  

all phrase candidates is called a perturbed training set [2],
because the labels of some (   in the    gure) quality phrases
are switched from positive to negative.
in order for the
ensemble classi   er to alleviate the e   ect of such noise, we
need to use base classi   ers with the lowest possible training
errors. we grow an unpruned decision tree to the point of
separating all phrases to meet this requirement.
in fact,
such decision tree will always reach 100% training accuracy
when no two positive and negative phrases share identical
feature values in the perturbed training set. in this case,
its ideal error is
2k , which approximately equals to the
proportion of switched labels among all phrase candidates
2k     10%). therefore, the value of k is not sensitive
(i.e.,
to the accuracy of the unpruned decision tree and is    xed as
100 in our implementation. assuming the extracted features
are distinguishable between quality and inferior phrases, the
empirical error evaluated on all phrase candidates, p, should
be relatively small as well.

an interesting property of this sampling procedure is that
the random selection of phrase candidates for building per-
turbed training sets creates classi   ers that have statistically
independent errors and similar erring id203 [2, 15].
therefore, we naturally adopt id79 [10], which is
veri   ed, in the statistics literature, to be robust and e   cient.
the phrase quality score of a particular phrase is computed
as the proportion of all id90 that predict that phrase
is a quality phrase. suppose there are t trees in the random
forest, the ensemble error can be estimated as the id203
of having more than half of the classi   ers misclassifying a
given phrase candidate as follows.

ensemble_ error(t ) =

pt(1     p)t   t

tx

(cid:18)t

(cid:19)

t=b1+t /2c

t

from figure 3, one can
easily observe that the en-
semble error is approach-
ing 0 when t grows. in
practice, t needs to be
set larger due to the ad-
ditional error brought by
model bias. empirical
studies can be found in
figure 7.
4.2 pos-guided phrasal segmentation

figure 3: ensemble er-
rors of di   erent p   s vary-
ing t.

phrasal segmentation addresses the challenge of measuring
completeness (our fourth criterion) by locating all phrase
mentions in the corpus and rectifying their frequencies ob-
tained originally via string matching.

the corpus is processed as a length-n pos-tagged word
sequence     =    1   2 . . .    n, where    i refers to a pair con-
sisting of a word and its pos tag hwi, tii. a pos-guided
phrasal segmentation is a partition of this sequence into
m segments induced by a boundary index sequence b =
{b1, b2, . . . , bm+1} satisfying 1 = b1 < b2 < . . . < bm+1 =
n+1. the i-th segment refers to    bi   bi+1 . . .    bi+1   1.
compared to the phrasal segmentation in our previous
work [13], the pos-guided phrasal segmentation addresses
the completeness requirement in a context-aware way, instead
of equivalently penalizing phrase candidates of the same
length. in addition, pos tags provide shallow, language-
speci   c knowledge, which may help boost phrase detection

sampling   labels            ideal error =    2       noise in the negative pool    10%. empirical error     should be similar.+labels    negative poolpositive poolall phrase candidatesa size-2kperturbed training setsamplingquality phrasesinferior phrases(           )    t100101102103ensemble error00.10.20.30.4p=0.05p=0.1p=0.2p=0.4algorithm 1: pos-guided phrasal segmentation (pgps)
input: corpus     =    1   2 . . .    n, phrase quality q,
parameters   u and   (tx, ty).
output: optimal boundary index sequence b.
// hi     maxb p(   1   2 . . .    i   1, b|q,   ,   )
h1     1, hi     0 (1 < i     n + 1)
for i = 1 to n do

for j = i + 1 to min(i + length threshold, n + 1) do

// efficiently implemented via trie.
if there is no phrase starting with w[i,j) then

break

// in practice, log and addition are used
if hi    p(j,dw[i,j)c|i, t[i,j)) > hj then

to avoid underflow.
hj     hi    p(j,dw[i,j)c|i, t[i,j))
gj     i
j     n + 1, m     0
while j > 1 do
m     m + 1
bm     j
j     gj

return b     1, bm, bm   1, . . . , b1

accuracy, especially at syntactic constituent boundaries for
that language.

given the pos tag sequence for the full n-length corpus
is t = t1t2 . . . tn, containing the tag subsequence tl . . . tr   1
(denote as t[l,r) for clarity), the pos quality score for that
tag subsequence is de   ned to be the id155 of
its corresponding word sequence being a complete semantic
unit. formally, we have

t (t[l,r)) = p(dwl . . . wrc|t)     [0, 1]

the pos quality score t (  ) is designed to reward the phrases
with their correctly identi   ed pos sequences, as follows.

example 5. suppose the whole pos tag sequence is    nn
nn nn vb dt nn   . a good pos sequence quality estima-
tor might return t (nn nn nn)     1 and t (nn vb)     0,
where nn refers to singular or mass noun (e.g., database),
vb means verb in the base form (e.g., is), and dt is for
determiner (e.g., the). (cid:3)

the particular form of t (  ) we have chosen is:

t (t[l,r)) = (1       (tbr   1, tbr))   

  (tj   1, tj)

j=l+1

where,   (tx, ty) is the id203 that the pos tag tx is
adjacent and precedes pos tag ty within a phrase in the given
document collection. in this formula, the    rst term indicates
the id203 that there is a phrase boundary between the
words indexed r     1 and r, while the latter product indicates
the id203 that all pos tags within t[l,r) are in the
same phrase. this pos quality score can naturally counter
the bias to longer segments because    i > 1, exactly one of
  (ti   1, ti) and (1      (ti   1, ti)) is always multiplied no matter
how the corpus is segmented. note that the length penalty
model in our previous work [13] is a special case when all
values of   (tx, ty) are the same.

mathematically,   (tx, ty) is de   ned as:
  (tx, ty) = p(d. . . w1w2 . . .c|   , tag(w1) = tx     tag(w2) = ty)

r   1y

as it depends on how documents are segmented into phrases,
  (tx, ty) is initialized uniformly and will be learned during
the phrasal segmentation.
now, after we have both phrase quality q(  ) and pos
quality t (  ) ready, we are able to formally de   ne the pos-
guided phrasal segmentation model. the joint id203 of
a pos tagged sequence     and a boundary index sequence
b = {b1, b2, . . . , bm+1} is factorized as:

(cid:16)

bi+1,dw[bi,bi+1)c(cid:12)(cid:12)(cid:12)bi, t

(cid:17)

my

i=1

p(   , b) =

p

where p(bi+1,dw[bi,bi+1)c|bi, t) is the id203 of observing
a word sequence w[bi,bi+1) as the i-th quality segment given
the previous boundary index bi and the whole pos tag
sequence t.

since the phrase segments function as a constituent in the
syntax of a sentence, they usually have weak dependence on
each other [8, 13]. as a result, we assume these segments in
the word sequence are generated one by one for the sake of
both e   ciency and simplicity.

for each segment, given the pos tag sequence t and the
start index bi, the generative process is de   ned as follows.
1. generate the end index bi+1, according to its pos quality

p(bi+1|bi, t) = t (t[bi,bi+1))

2. given the two ends bi and bi+1, generate the word se-
quence w[bi,bi+1) according to a multinomial distribution
over all segments of length-(bi+1     bi).

p(w[bi,bi+1)|bi, bi+1) = p(w[bi,bi+1)|bi+1     bi)

3. finally, we generate an indicator whether w[bi,bi+1) forms

a quality segment according to its quality

p(dw[bi,bi+1)c|w[bi,bi+1)) = q(w[bi,bi+1))

we denote p(w[bi,bi+1)|bi+1   bi) as   w[bi,bi+1) for convenience.
integrating the above three generative steps together, we
have the following probabilistic factorization:

p(bi+1,dw[bi,bi+1)c|bi, t)
=p(bi+1|bi, t)p(w[bi,bi+1)|bi, bi+1)p(dw[bi,bi+1)c|w[bi,bi+1))
=t (t[bi,bi+1))  w[bi,bi+1) q(w[bi,bi+1))
therefore, there are three subproblems:

1. learn   u for each word and phrase candidate u;
2. learn   (tx, ty) for every pos tag pair; and
3. infer b when   u and   (tx, ty) are    xed.

we employ the maximum a posterior principle and maxi-

mize the joint log likelihood:

(cid:16)

bi+1,dw[bi,bi+1)c(cid:12)(cid:12)(cid:12)bt, t

(cid:17)

(1)

log p(   , b) =

log p

mx

i=1

given   u and   (tx, ty), to    nd the best segmentation that
maximizes equation (1), we develop an e   cient dynamic
programming algorithm for the pos-guided phrasal segmen-
tation as shown in algorithm 1.

when the segmentation s and the parameter    are    xed,

the closed-form solution of   (tx, ty) is:

pm

i=1

pbi+1   2
pn   1

  (tx, ty) =

1(tj = tx     tj+1 = ty)

j=bi
i=1 1(ti = tx     ti+1 = ty)

(2)

algorithm 2: viterbi training (vt)
input: corpus     and phrase quality q.
output:   u and   (tx, ty).
initialize    with normalized raw frequencies in the corpus
while   u does not converge do

while   (tx, ty) does not converge do
b     best segmentation via alg. 1
update   (tx, ty) using b according to eq. (2)

b     best segmentation via alg. 1
update   u using b according to eq. (3)

return   u and   (tx, ty)

pm
pm

where 1(  ) denotes the identity indicator.   (tx, ty) is the
unsegmented ratio among all htx, tyi pairs in the given corpus.
similarly, once the segmentation s and the parameter   
are    xed, the closed-form solution of   u can be derived as:

  u =

i=1 1(w[bi,bi+1) = u)
i=1 1(bi+1     bi = |u|)

(3)

we can see that   u is the times that u becomes a com-
plete segment normalized by the number of the length-|u|
segments.

as shown in algorithm 2, we choose viterbi training, or
hard em in literature [1] to iteratively optimize parameters,
because viterbi training converges fast and results in sparse
and simple models for hidden markov model-like tasks [1].
4.3 complexity analysis

the time complexity of the most time consuming compo-
nents in our framework, such as frequent id165, feature
extraction, pos-guided phrasal segmentation, are all o(|   |)
with the assumption that the maximum number of words in
a phrase is a small constant (e.g., n     6), where |   | is the
total number of words in the corpus. therefore, autophrase
is linear to the corpus size and thus being very e   cient and
scalable. meanwhile, every component can be parallelized
in an almost lock-free way grouping by either phrases or
sentences.

5. experiments

in this section, we will apply the proposed method to mine
quality phrases from    ve massive text corpora across three
domains (scienti   c papers, business reviews, and wikipedia
articles) and in three languages (english, spanish, and chi-
nese). we compare the proposed method with many other
methods to demonstrate its high performance. then we
explore the robustness of the proposed positive-only distant
training and its performance against expert labeling. the
advantage of incorporating pos tags in phrasal segmentation
will also be proved. in the end, we present case studies.
5.1 datasets

to validate that the proposed positive-only distant train-
ing can e   ectively work in di   erent domains and the pos-
guided phrasal segmentation can support multiple languages
e   ectively, we have    ve large collections of text in di   erent
domains and languages, as shown in table 1: abstracts
of english computer science papers from dblp6, english

6https://aminer.org/citation

table 1: five real-world massive text corpora in dif-
|   | is the
ferent domains and multiple languages.
total number of words. sizep is the size of positive
pool.

dataset
dblp
yelp
en
es
cn

domain

scienti   c paper
business review
wikipedia article
wikipedia article
wikipedia article

language
english
english
english
spanish
chinese

|   |
file size
sizep
29k
91.6m 618mb
145.1m 749mb
22k
808.0m 3.94gb 184k
65k
791.2m 4.06gb
371.9m 1.56gb
29k

business reviews from yelp7, wikipedia articles8 in english
(en), spanish (es), and chinese (cn). from the exist-
ing general knowledge base wikipedia, we extract popular
mentions of entities by analyzing intra-wiki citations within
wiki content9. on each dataset, the intersection between the
extracted popular mentions and the generated phrase candi-
dates forms the positive pool. therefore, the size of positive
pool may vary in di   erent datasets of the same language.
5.2 compared methods

we compare autophrase with three lines of methods as

follows. every method returns a ranked list of phrases.
segphrase10/wrapsegphrae11: in english domain-speci   c
text corpora, our latest work segphrase outperformed phrase
mining [6], keyphrase extraction [23, 19], and noun phrase
chunking methods. wrapsegphrase extends segphrase to dif-
ferent languages by adding an encoding preprocessing to
   rst transform non-english corpus using english characters
and punctuation as well as a decoding postprocessing to
later translate them back to the original language. both
methods require domain expert labors. for each dataset, we
ask domain experts to annotate a representative set of 300
quality/interior phrases.
parser-based phrase extraction: using complicated lin-
guistic processors, such as parsers, we can extract minimum
phrase units (e.g., np) from the parsing trees as phrase
candidates. parsers of all three languages are available in
stanford nlp tools [18, 4, 12]. two ranking heuristics are
considered:
    tf-idf ranks the extracted phrases by their term fre-
quency and inverse document frequency in the given docu-
ments;
    textrank: an unsupervised graph-based ranking model

for keyword extraction [17].

pre-trained chinese segmentation models: di   erent
from english and spanish, phrasal segmentation in chi-
nese has been intensively studied because there is no space
between chinese words. the most e   ective and popular
segmentation methods are:
    ansjseg12 is a popular text segmentation algorithm for
chinese corpus. it ensembles statistical modeling methods
of id49 (crf) and hidden markov
models (id48s) based on the id165 setting;
    jiebapseg13 is a chinese text segmentation method im-
7https://www.yelp.com/dataset_challenge
8https://dumps.wikimedia.org/
9https://github.com/kno10/wikipediaentities
10https://github.com/shangjingbo1226/segphrase
11https://github.com/remenberl/segphrase-multilingual
12https://github.com/nlpchina/ansj_seg
13https://github.com/fxsjy/jieba

(a) dblp

(b) yelp

(c) en

(d) es

(e) cn

figure 4: overall performance evaluation: precision-recall curves of all methods evaluated by human anno-
tation.

plemented in python. it builds a directed acyclic graph
for all possible phrase combinations based on a pre   x
dictionary structure to achieve e   cient phrase graph scan-
ning. then it uses id145 to    nd the most
probable combination based on the phrase frequency. for
unknown phrases, an id48-based model is used with the
viterbi algorithm.

note that all parser-based phrase extraction and chinese
segmentation models are pre-trained based on general corpus.
to study the e   ectiveness of the pos-guided segmenta-
tion, autosegphrase adopts the length penalty instead of
  (tx, ty), while other components are the same as autophrase.
autosegphrase is useful when there is no pos tagger.
5.3 experimental settings
implementation. the preprocessing includes tokenizers
from lucene and stanford nlp as well as the pos tagger
from treetagger. our documented code package has been
released and maintained in github14.
default parameters. we set the minimum support thresh-
old    as 30. the maximum number of words in a phrase is
set as 6 according to labels from domain experts. these are
two parameters required by all methods. other parameters
required by compared methods were set according to the
open-source tools or the original papers.
human annotation. we rely on human evaluators to
judge the quality of the phrases which cannot be identi   ed
through any knowledge base. more speci   cally, on each
dataset, we randomly sample 500 such phrases from the
predicted phrases of each method in the experiments. these
selected phrases are shu   ed in a shared pool and evaluated
by 3 reviewers independently. we allow reviewers to use
search engines when unfamiliar phrases encountered. by the
rule of majority voting, phrases in this pool received at least
two positive annotations are quality phrases. the intra-class
correlations (iccs) are all more than 0.9 on all    ve datasets,
which shows the agreement.
id74. for a list of phrases, precision is
de   ned as the number of true quality phrases divided by the
number of predicted quality phrases; recall is de   ned as the
number of true quality phrases divided by the total number
of quality phrases. we retrieve the ranked list of the pool
from the outcome of each method. when a new true quality
phrase encountered, we evaluate the precision and recall of
this ranked list.
in the end, we plot the precision-recall
curves. in addition, area under the curve (auc) is adopted
as another quantitative measure. auc in this paper refers
to the area under the precision-recall curve.

14https://github.com/shangjingbo1226/autophrase

5.4 overall performance

figures 4 presents the precision-recall curves of all com-
pared methods evaluated by human annotation on    ve datasets.
overall, autophrase performs the best, in terms of both pre-
cision and recall. signi   cant advantages can be observed,
especially on two non-english datasets es and cn. for
example, on the es dataset, the recall of autophrase is
about 20% higher than the second best method (segphrase)
in absolute value. meanwhile, there is a visible precision
gap between autophrase and the best baseline. the phrase
chunking-based methods tf-idf and textrank work poorly,
because the extraction and ranking are modeled separately
and the pre-trained complex linguistic analyzers fail to ex-
tend to domain-speci   c corpora. textrank usually starts
with a higher precision than tf-idf, but its recall is very
low because of the sparsity of the constructed co-occurrence
graph. tf-idf achieves a reasonable recall but unsatisfac-
tory precision. on the cn dataset, the pre-trained chinese
segmentation models, jiebaseg and ansjseg, are very com-
petitive, because they not only leverage training data for
segmentations, but also exhaust the engineering work, in-
cluding a huge dictionary for popular chinese entity names
and speci   c rules for certain types of entities. as a conse-
quence, they can easily extract tons of well-known terms
and people/location names. outperforming such a strong
baseline further con   rms the e   ectiveness of autophrase.

the comparison among the english datasets across three

domains (i.e., scienti   c papers, business reviews, and wikipedia
articles) demonstrates that autophrase is reasonably domain-
independent. the performance of parser-based methods
tf-idf and textrank depends on the rigorous degree of
the documents. for example, it works well on the dblp
dataset but poorly on the yelp dataset. however, without
any human e   ort, autophrase can work e   ectively on domain-
speci   c datasets, and even outperforms segphrase, which is
supervised by the domain experts.

the comparison among the wikipedia article datasets in
three languages (i.e., en, es, and cn) shows that,    rst
of all, autophrase supports multiple languages. secondly,
the advantage of autophrase over segphrase/wrapsegphrase
is more obvious on two non-english datasets es and cn
than the en dataset, which proves that the helpfulness of
introducing the pos tagger.

as conclusions, autophrase is able to support di   erent
domains and support multiple languages with minimal human
e   ort.
5.5 distant training exploration

to compare the distant training and domain expert label-
ing, we experiment with the domain-speci   c datasets dblp

recall00.51precision0.40.60.81autophrasesegphrasetf-idftextrankrecall00.51precision0.20.40.60.81autophrasesegphrasetf-idftextrankrecall00.51precision00.51autophrasesegphrasetf-idftextrank00.20.40.60.8recall00.51precisionautophrasewrapsegphrasetf-idftextrank00.51recall00.51precisionautophrasewrapsegphrasetf-idftextrankjiebapsegansjseg(a) dblp

(b) yelp

(a) dblp

(b) yelp

figure 5: auc curves of four variants when we have
enough positive labels in the positive pool ep.

figure 6: auc curves of four variants after we ex-
haust positive labels in the positive pool ep.

and yelp. to be fair, all the con   gurations in the classi   ers
are the same except for the label selection process. more
speci   cally, we come up with four training pools:
1. ep means that domain experts give the positive pool.
2. dp means that a sampled subset from existing general

knowledge forms the positive pool.

3. en means that domain experts give the negative pool.
4. dn means that all unlabeled (i.e., not in the positive

pool) phrase candidates form the negative pool.

by combining any pair of the positive and negative pools,
we have four variants, epen (in segphrase), dpdn (in
autophrase), epdn, and dpen.

first of all, we evaluate the performance di   erence in the
two positive pools. compared to epen, dpen adopts a
positive pool sampled from knowledge bases instead of the
well-designed positive pool given by domain experts. the
negative pool en is shared. as shown in figure 5, we vary the
size of the positive pool and plot their auc curves. we can
   nd that epen outperforms dpen and the trends of curves
on both datasets are similar. therefore, we conclude that the
positive pool generated from knowledge bases has reasonable
quality, although its corresponding quality estimator works
slightly worse.

secondly, we verify that whether the proposed noise re-
duction mechanism works properly. compared to epen,
epdn adopts a negative pool of all unlabeled phrase can-
didates instead of the well-designed negative pool given by
domain experts. the positive pool ep is shared. in fig-
ure 5, the clear gap between them and the similar trends on
both datasets show that the noisy negative pool is slightly
worse than the well-designed negative pool, but it still works
e   ectively.

as illustrated in figure 5, dpdn has the worst perfor-
mance when the size of positive pools are limited. however,
distant training can generate much larger positive pools,
which may signi   cantly beyond the ability of domain experts
considering the high expense of labeling. consequently, we
are curious whether the distant training can    nally beat do-
main experts when positive pool sizes become large enough.
we call the size at this tipping point as the ideal number.
we increase positive pool sizes and plot auc curves of
dpen and dpdn, while epen and epdn are degenerated
as dashed lines due to the limited domain expert abilities.
as shown in figure 6, with a large enough positive pool,
distant training is able to beat expert labeling. on the
dblp dataset, the ideal number is about 700, while on the
yelp dataset, it becomes around 1600. our guess is that the
ideal training size is proportional to the number of words

(a) en

(b) es

(c) cn

figure 8: precision-recall curves of autophrase and
autosegphrase.

(e.g., 91.6m in dblp and 145.1m in yelp). we notice that
compared to the corpus size, the ideal number is relatively
small, which implies the distant training should be e   ective
in many domain-speci   c corpora as if they overlap with
wikipedia.

besides, figure 6 shows that when the positive pool size
continues growing, the auc score increases but the slope
becomes smaller. the performance of distant training will
be    nally stable when a relatively large number of quality
phrases were fed.

we are curious how
many trees (i.e., t ) is
enough for dpdn. we
increase t and plot auc
curves of dpdn.
as
shown in figure 7, on
both datasets,
as t
grows, the auc scores
   rst increase rapidly and
later
the speed slows
down gradually, which is
consistent with the theoretical analysis in section 4.1.2.
5.6 pos-guided phrasal segmentation

figure 7: auc curves of
dpdn varying t.

we are also interested in how much performance gain we
can obtain from incorporating pos tags in this segmentation
model, especially for di   erent languages. we select wikipedia
article datasets in three di   erent languages: en, es, and
cn.

figure 8 compares the results of autophrase and autosegphrase,

with the best baseline methods as references. autophrase
outperforms autosegphrase even on the english dataset en,
though it has been shown the length penalty works rea-
sonably well in english [13]. the spanish dataset es has
similar observation. moreover, the advantage of autophrase
becomes more signi   cant on the cn dataset, indicating the
poor generality of length penalty.

510152025303540455055606570positive pool size0.600.650.700.750.800.850.90aucepen (in segphrase)dpenepdndpdn (in autophrase)152025303540455055606570positive pool size0.700.750.800.85aucepen (in segphrase)dpenepdndpdn (in autophrase)100600110016000.820.840.860.880.90auc2900429154293040.820.840.860.880.90epen (in segphrase)epdndpendpdn (in autophrase)positive pool size100600110016000.810.820.830.840.85auc2171821868220180.810.820.830.840.85epen (in segphrase)epdndpendpdn (in autophrase)positive pool size00.51recall0.40.60.81precisionautophraseautosegphrasesegphrase00.20.40.60.8recall0.20.40.60.81precisionautophraseautosegphrasewrapsegphrase00.51recall0.40.60.81precisionautophraseautosegphrasejiebasegt100101102103auc0.60.70.80.9dblpyelp(a) running time (b) peak memory (c) multi-threading

figure 9: e   ciency of autophrase.

in summary, thanks to the extra context information and
syntactic information for the particular language, incorpo-
rating pos tags during the phrasal segmentation can work
better than equally penalizing phrases of the same length.
5.7 case study

we present a case study about the extracted phrases
as shown in table 2. the top ranked phrases are mostly
named entities, which makes sense for the wikipedia article
datasets. even in the long tail part, there are still many high-
quality phrases. for example, we have the dgreat spotted
woodpeckerc (a type of birds) and d                      c (i.e.,
computer science and technology) ranked about 100,000.
in fact, we have more than 345k and 116k phrases with a
phrase quality higher than 0.5 on the en and cn datasets
respectively.
5.8 ef   ciency evaluation

to study both time and memory e   ciency, we choose the

three largest datasets: en, es, and cn.

figures 9(a) and 9(b) evaluate the running time and the
peak memory usage of autophrase using 10 threads on dif-
ferent proportions of three datasets respectively. both time
and memory are linear to the size of text corpora. moreover,
autophrase can also be parallelized in an almost lock-free
way and shows a linear speedup in figure 10(c).

besides, compared to the previous state-of-the-art phrase
mining method segphrase and its variants wrapsegphrase
on three datasets, as shown in table 3, autophrase achieves
about 8 to 11 times speedup and about 5 to 7 times memory
usage improvement. these improvements are made by a
more e   cient indexing and a more thorough parallelization.

6. single-word phrases

autophrase can be extended to model single-word phrases,
which can gain about 10% to 30% recall improvements on
di   erent datasets. to study the e   ect of modeling quality
single-word phrases, we choose the three wikipedia article
datasets in di   erent languages: en, es, and cn.
6.1 quality estimation

in the paper, the de   nition of quality phrases and the
evaluation only focus on multi-word phrases. in linguistic
analysis, however, a phrase is not only a group of multiple
words, but also possibly a single word, as long as it functions
as a constituent in the syntax of a sentence [8]. as a great
portion (ranging from 10% to 30% on di   erent datasets based
on our experiments) of high-quality phrases, we should take
single-word phrases (e.g., duiucc, dillinoisc, and dusac)
into consideration as well as multi-word phrases to achieve a
high recall in phrase mining.

considering the criteria of quality phrases, because single-
word phrases cannot be decomposed into two or more parts,
the concordance and completeness are no longer de   nable.
therefore, we revise the requirements for quality single-
word phrases as below.
    popularity: quality phrases should occur with su   cient
    informativeness: a phrase is informative if it is indica-
    independence: a quality single-word phrase is more likely

frequency in the given document collection.

tive of a speci   c topic or concept.

a complete semantic unit in the given documents.

only single-word phrases satisfying all popularity, indepen-
dence, and informativeness requirements are recognized as
quality single-word phrases.

example 6.    uiuc    is a quality single-word phrase.    this   
is not a quality phrase due to its low informativeness.    united   ,
usually occurring within other quality multi-word phrases such
as    united states   ,    united kingdom   ,    united airlines   ,
and    united parcel service   , is not a quality single-word
phrase, because its independence is not enough.

after the phrasal segmentation, in replacement of con-
cordance features, the independence feature is added for
single-word phrases. formally, it is the ratio of the recti   ed
frequency of a single-word phrase given the phrasal segmen-
tation over its raw frequency. quality single-word phrases
are expected to have large values. for example,    united    is
likely to an almost zero ratio.

we use autophrase+ to denote the extended autophrase

with quality single-word phrase estimation.
6.2 experiments

we have a similar human annotation as that in the pa-
per. di   erently, we randomly sampled 500 wiki-uncovered
phrases from the returned phrases (both single-word and
multi-word phrases) of each method in experiments of the
paper. therefore, we have new pools on the en, es, and cn
datasets. the intra-class correlations (iccs) are all more
than 0.9, which shows the agreement.

figure 10 compare all methods based these new pools.
note that all methods except for segphrase/wrapsegphrase
extract single-word phrases as well.

signi   cant recall advantages can be always observed on
all en, es, and cn datasets. the recall di   erences be-
tween autophrase+ and autophrase, ranging from 10% to
30% sheds light on the importance of modeling single-word
phrases. across two latin language datasets, en and es,
autophrase+ and autophrase overlaps in the beginning, but
later, the precision of autophrase drops earlier and has a
lower recall due to the lack of single-word phrases. on the
cn dataset, autophrase+ and autophrase has a clear gap
even in the very beginning, which is di   erent from the trends
on the en and es datasets, which re   ects that single-word
phrases are more important in chinese. the major reason
behind is that there are a considerable number of high-quality
phrases (e.g., person names) in chinese have only one token
after id121.

7. conclusions

in this paper, we present an automated phrase mining
framework with two novel techniques: the robust positive-
only distant training and the pos-guided phrasal segmenta-
tion incorporating part-of-speech (pos) tags, for the develop-

0.20.40.60.810204060portion of datarunning time (mins)  enescn0.20.40.60.8105101520portion of datapeak memory (gb)  enescn5101551015the number of threadsspeedup  enescntable 2: the results of autophrase on the en and cn datasets, with translations and explanations for chinese
phrases. the whitespaces on the cn dataset are inserted by the chinese tokenizer.

en

cn

phrase
elf aquitaine
arnold sommerfeld
eugene wigner
tarpon springs
sean astin
. . .

rank
1
2
3
4
5
. . .
20,001 ecac hockey
20,002
sacramento bee
20,003 bering strait
20,004
jacknife lee
20,005 wxyz-tv
. . .
99,994
99,995 white-tailed eagle
99,996

translation (explanation)
(the name of a soccer team)
absinthe
(the name of a novel/tv-series)
notebook computer, laptop
secretary of party committee
. . .
african countries
the left (german: die linke)
fraser valley
hippocampus
mitsuki saiga (a voice actress)
. . .

phrase
              
           
              
                 
             
. . .
             
          
              
          
             
. . .
                       computer science and technology
          
                     the vice president of writers
          
great spotted woodpecker           b
             
. . .

association of china
vitamin b
controlled guidance of the media
. . .

99,997
99,998 david manners
. . .

rhombic dodecahedron

fonterra (a company)

. . .
john gregson

. . .

table 3: e   ciency comparison between autophrase and segphrase/wrapsegphrase utilizing 10 threads.

en

time memory
(gb)
(mins)
32.77
13.77
97.72
369.53
11.27
86%

es

time memory
(gb)
(mins)
54.05
16.42
92.47
452.85
8.37
82%

cn

time memory
(gb)
(mins)
9.43
5.74
35.38
108.58
11.50
83%

autophrase
(wrap)segphrase
speedup/saving

(a) en

(b) es

(c) cn

figure 10: precision-recall curves evaluated by human annotation with both single-word and multi-word
phrases in pools.

ment of an automated phrase mining framework autophrase.
our extensive experiments show that autophrase is domain-
independent, outperforms other phrase mining methods, and
supports multiple languages (e.g., english, spanish, and
chinese) e   ectively, with minimal human e   ort.
besides, the inclusion of quality single-word phrases (e.g.,
duiucc and dusac) which leads to about 10% to 30% in-
creased recall and the exploration of better indexing strate-
gies and more thorough parallelization, which leads to about
8 to 11 times running time speedup and about 80% to 86%

memory usage saving over segphrase. interested readers may
try our released code at github.

for future work, it is interesting to (1) re   ne quality
phrases to entity mentions, (2) apply autophrase to more lan-
guages, such as japanese, and (3) for those languages without
general knowledge bases, seek an unsupervised method to
generate the positive pool from the corpus, even with some
noise.
8. references
[1] a. allahverdyan and a. galstyan. comparative

00.20.40.60.81recall00.20.40.60.81precisionautophrase+autophrasesegphrasetf-idftextrank00.20.40.60.8recall00.20.40.60.81precisionautophrase+autophrasewrapsegphrasetf-idftextrank00.20.40.60.81recall00.20.40.60.81precisionautophrase+autophrasejiebapsegansjsegtf-idfwrapsegphrasetextranka. rajaraman. towards the web of concepts:
extracting concepts from large datasets. proceedings of
the very large data bases conference (vldb),
3((1-2)), september 2010.

[20] y. park, r. j. byrd, and b. k. boguraev. automatic
glossary extraction: beyond terminology identi   cation.
in coling, 2002.

[21] v. punyakanok and d. roth. the use of classi   ers in

sequential id136. in nips, 2001.

[22] h. schmid. treetagger| a language independent
part-of-speech tagger. institut f  r maschinelle
sprachverarbeitung, universit  t stuttgart, 43:28, 1995.
[23] i. h. witten, g. w. paynter, e. frank, c. gutwin, and

c. g. nevill-manning. kea: practical automatic
keyphrase extraction. in proceedings of the fourth acm
conference on digital libraries, pages 254   255. acm,
1999.

[24] e. xun, c. huang, and m. zhou. a uni   ed statistical
model for the identi   cation of english basenp. in acl,
2000.

[25] z. zhang, j. iria, c. a. brewster, and f. ciravegna. a
comparative evaluation of term recognition algorithms.
lrec, 2008.

analysis of viterbi training and maximum likelihood
estimation for id48s. in nips, pages 1674   1682, 2011.

[2] l. breiman. randomizing outputs to increase

prediction accuracy. machine learning, 40(3):229   242,
2000.

[3] k.-h. chen and h.-h. chen. extracting noun phrases

from large-scale texts: a hybrid approach and its
automatic evaluation. in acl, 1994.

[4] m.-c. de marne   e, b. maccartney, c. d. manning,

et al. generating typed dependency parses from phrase
structure parses. in proceedings of lrec, volume 6,
pages 449   454, 2006.

[5] p. deane. a nonparametric method for extraction of

candidate phrasal terms. in acl, 2005.

[6] a. el-kishky, y. song, c. wang, c. r. voss, and
j. han. scalable topical phrase mining from text
corpora. vldb, 8(3), aug. 2015.

[7] d. a. evans and c. zhai. noun-phrase analysis in

unrestricted text for information retrieval. in
proceedings of the 34th annual meeting on association
for computational linguistics, pages 17   24.
association for computational linguistics, 1996.

[8] g. finch. linguistic terms and concepts. macmillan

press limited, 2000.

[9] k. frantzi, s. ananiadou, and h. mima. automatic

recognition of multi-word terms:. the c-value/nc-value
method. jodl, 3(2):115   130, 2000.

[10] p. geurts, d. ernst, and l. wehenkel. extremely

randomized trees. machine learning, 63(1):3   42, 2006.

[11] t. koo, x. carreras, and m. collins. simple

semi-supervised id33. acl-hlt, 2008.
[12] r. levy and c. manning. is it harder to parse chinese,

or the chinese treebank? in proceedings of the 41st
annual meeting on association for computational
linguistics-volume 1, pages 439   446. association for
computational linguistics, 2003.

[13] j. liu, j. shang, c. wang, x. ren, and j. han. mining

quality phrases from massive text corpora. in
proceedings of 2015 acm sigmod international
conference on management of data, 2015.

[14] z. liu, x. chen, y. zheng, and m. sun. automatic
keyphrase extraction by bridging vocabulary gap. in
proceedings of the fifteenth conference on
computational natural language learning, pages
135   144. association for computational linguistics,
2011.

[15] g. mart  nez-mu  oz and a. su  rez. switching class
labels to generate classi   cation ensembles. pattern
recognition, 38(10):1483   1494, 2005.

[16] r. mcdonald, f. pereira, k. ribarov, and j. haji  .

non-projective id33 using spanning tree
algorithms. in emnlp, 2005.

[17] r. mihalcea and p. tarau. textrank: bringing order

into texts. in acl, 2004.

[18] j. nivre, m.-c. de marne   e, f. ginter, y. goldberg,

j. hajic, c. d. manning, r. mcdonald, s. petrov,
s. pyysalo, n. silveira, et al. universal dependencies
v1: a multilingual treebank collection. in proceedings
of the 10th international conference on language
resources and evaluation (lrec 2016), 2016.

[19] a. parameswaran, h. garcia-molina, and

