real-time on-demand crowd-powered entity extraction

(supplementary technical report)

ting-hao (kenneth) huang

carnegie mellon university

pittsburgh, pa, usa

tinghaoh@cs.cmu.edu

yun-nung (vivian) chen

national taiwan university

taipei, taiwan

yvchen@csie.ntu.edu.tw

jeffrey p. bigham

carnegie mellon university

pittsburgh, pa, usa

jbigham@cs.cmu.edu

7
1
0
2
 
c
e
d
6

 

 
 
]

c
h
.
s
c
[
 
 

2
v
7
2
6
3
0

.

4
0
7
1
:
v
i
x
r
a

[note] this is the supplementary technical report of our
paper    real-time on-demand crowd-powered entity
extraction,    which was published at the 5th edition of
the collective intelligence conference (ci 2017) as an
oral presentation (huang et al. 2017). the original paper
was only 3-page long, so we decided to share extra technical
details in this report.

abstract

modern id71 rely on accurate entity extraction
to understand user utterances. however, entity extraction is
brittle due to data scarcity, language variability, and out-of-
vocabulary entities. to bridge this gap, we propose a real-
time id104 solution based on the esp game for im-
age labeling. when multiple players agree, entities can be re-
liably extracted from an utterance. this approach is advanta-
geous because it does not require training data. further, it is
robust to unexpected input and capable of recognizing new
entities. our approach achieves better f1-scores than that of
the automated baseline for complex queries with a reason-
able response time. the proposed method is also evaluated
via google hangouts    text chat and demonstrates the feasi-
bility of real-time crowd-powered entity extraction.

introduction

to understand user utterances, modern id71 rely
heavily on entity extraction, known as the core task of slot
   lling in many dialog system frameworks such as olym-
pus (bohus et al. 2007). the goal of slot    lling is to identify
from a running dialog different slots, which correspond to
different parameters of the user   s query. for instance, when
a user queries for nearby restaurants, key slots for location
and preferred food are required for a dialog system to re-
trieve the appropriate information. thus, the main challenge
in the slot-   lling task is to extract the target entity.

id71 face three key challenges in entity ex-
traction. due to data scarcity, labeled training data, which
many existing technologies require to identify entities such
as id49 (crf) (raymond and ric-
cardi 2007; xu and sarikaya 2014) and recurrent neural
networks (mesnil et al. 2015a), are often unavailable for
the wide variety of dialog system tasks. furthermore, it is

copyright    2016, association for the advancement of arti   cial
intelligence (www.aaai.org). all rights reserved.

figure 1: the crowd-powered entity extraction with a multi-
player dialog esp game. by aggregating input answers
from all players, our approach is able to provide good qual-
ity results in seconds.

more dif   cult to acquire the complicated conversational data
required by other alternative dialog technologies, such as
statistical dialog management (young 2006) or state track-
ing (williams et al. 2013). second, existing entity extrac-
tion technologies are not robust enough to identify out-of-
vocabulary entities. even when labeled training data for
the targeted slot could be collected, state-of-the-art super-
vised learning approaches are brittle in extracting unseen
entities. (xu and sarikaya 2014)    nd that the crf-based
entity extractor performed signi   cantly worse when dictio-
nary features were not used. third, challenges are also posed
by language variability. successful applications process di-
verse input languages where potential entities are unlimited.
therefore, to robustly serve arbitrary input, id71
must collect new sources of entities and update accordingly.
research on id71 has focused on utilizing
the internet resource to extract entities such as movie
names (wang, heck, and hakkani-tur 2014); unsupervised
slot-   lling approaches have also been developed in recent
years (chen, hakkani-t  ur, and tur 2014; heck, hakkani-
t  ur, and t  ur 2013). however, these methods are still under-
developed.

to address these challenges, we propose to use real-time
id104 as an entity extractor in id71. to
the best of our knowledge, few previous works have at-
tempted to use id104 to extract entities from a run-
ning conversation. (wang et al. 2012), for example, studied
various methods to acquire natural language sentences for a
given semantic form by the crowd. (lasecki, kamar, and bo-
hus 2013) utilized id104 to collect dialog data, and

1

sunday flights from new york to las vegas answer aggregate destination: las vegas dialog esp game user a dialog1. the esp game was originally proposed as a
id104 mechanism to acquire quality image la-
bels (von ahn and dabbish 2004). the original game ran-
domly pairs two players and presents them with the same
image. each player guesses the labels that the other player
would answer. if the players match labels, each is awarded
1000 points. our approach replaces the image in the esp
game with a dialog chat log and players answer the re-
quired entity name within a short time. we also relax the
constraints of player numbers to increase game speed. as
figure 1 shows, by aggregating input answers from all play-
ers, the dialog esp game is able to provide high quality
results in seconds.

figure 2 shows the worker   s interface. when input dialog
utterances reach the crowd-powered entity extraction com-
ponent, workers are recruited from id104 platforms
such as amazon mechanical turk (mturk). the timer be-
gins counting down when the input utterance arrives, and
the worker sees the remaining time on the top right corner
of the interface (figure 2). when two workers match an-
swers, a feedback noti   cation is displayed, and the workers
earn 1000 points. when the time is up, the task automatically
closes.

to recruit crowd workers quickly, many approaches have
been used in real-time crowd-powered systems such as
vizwiz (bigham et al. 2010) and chorus (lasecki et al.
2013). the quikturkit toolkit (quikturkit.googlecode.com)
attracts workers by posting tasks and using old tasks to
queue workers. similarly, the retainer model maintains a
retaining pool of workers-in-waiting, who receive a signal
when tasks become available. prior research shows that the
retainer model is able to recall 75% of workers within 3
seconds (bernstein et al. 2011). in experiment 1, we    rst fo-
cus on the speed and performance of the dialog esp game
itself instead of recruiting time. in experiment 2, we pro-
pose a novel approach to recruit workers within 60 seconds
and discuss details of the end-to-end response speed.

experiment 1:

applying dialog esp game

on atis dataset

to evaluate the dialog esp game for entity extraction, we
conducted experiments on mturk to extract names of desti-
nation cities from a    ight schedule query dialog dataset, the
airline travel information system (atis) dataset.

atis dataset the atis dataset contains a set of    ight
schedule query sessions, each of which consists of a se-
quence of spoken queries (utterances). each query contains
automatic speech recognized transcripts and a set of corre-
sponding sql queries. all queries in the data set are anno-
tated with the query category: a, d, or x. class a queries
are context-independent, answerable, and formed mostly in
a single sentence; however, real-world queries are more

1the source code of worker interface and the data collected in

experiment 2 are available at:
https://github.com/windx0303/dialogue-esp-game

2

figure 2: the dialog esp game interface is designed to
encourage quick and correct entity identi   cation by crowd
workers. workers are shown the complete dialog and a de-
scription of the entity they should identify.

illustrated crowdparse, a system that uses the crowd to parse
dialogs into semantic frames. recently, (huang, lasecki,
and bigham 2015) presented a crowd-powered dialog sys-
tem called guardian that uses the crowd to extract informa-
tion from input utterances. however, none of these works
conducted formal studies on crowd-powered entity extrac-
tion in real-time.

inspired by the esp game for image labeling (von ahn
and dabbish 2004), we propose a dialog esp game to en-
courage crowd workers to accurately and quickly perform
entity extraction. the esp game matches answers among
different workers to ensure label quality, and we use a timer
on the interface (figure 2) to ensure input speed. our method
offers three main advantages: 1) it does not require training
data; 2) it is robust to unexpected input; and 3) it is capable
of recognizing new entities. furthermore, answers submit-
ted from the crowd can be used as training data to bootstrap
automatic entity extraction algorithms. in this paper, we con-
duct experiments on a standard dialog dataset and user ex-
periments with 10 users via google hangouts    text chatting
interface. detailed experiments demonstrate that our crowd-
powered approach is robust, effective, and fast.

in sum, the contributions of our work are as follows:

1. we propose an esp-game-based real-time id104
approach for entity extraction in id71, which
enables accurate entity extraction for a wide variety of
tasks.

2. to strive for real-time id71, we present detailed
experiments to understand the trade-offs between entity
extraction accuracy and time delay.

3. we demonstrate the feasibility of

real-time crowd-
powered entity extraction in instant messaging applica-
tions.

real-time dialog esp game

we utilize real-time id104 with a multi-player di-
alog esp game setting to extract the targeted entity from

complex. in the atis data set, 32.2% queries are context-
dependent (class d) and 24.0% of the queries are cannot
be evaluated (class x) (hirschman 1992). the    context-
dependent    class d queries require information from pre-
vious queries to form a complete sql query. for instance,
in one atis session, the    rst query is    from montreal to las
vegas    (class a). the second query in the session is    satur-
day,    which requires the destination and departure city name
from the    rst query, and is thus annotated as class d. class
x is of all the problematic queries, e.g., hopelessly-vague or
unanswerable.

data pre-processing & experiment setting for class
a, we obtain the preprocessed data used in many slot    ll-
ing works (xu and sarikaya 2014; mesnil et al. 2015a;
he and young 2003; raymond and riccardi 2007; tur,
hakkani-tur, and heck 2010), which contain 4,978 queries
for training, 893 queries for testing, and 491 queries for
developing. 200 queries are randomly extracted from the
developing set for our study; for class d and x, we ob-
tain the original training set of atis-3 data (dahl et al.
1994), which contains 364 sessions and 3,235 queries. 200
class-d queries are randomly selected from 200 distinct ses-
sions. for each extracted query, all previous queries before
it within the same session are also obtained and displayed in
the worker   s interface (figure 2). the same process is used
to extract 150 class-x queries for the experiments. note that
in this work we focus only on the toloc.city name slot
(name of destination city), which is the most frequent slot
type in atis. for each extracted query of class d and x, we
de   ne the last-mentioned destination city name of the    ight
in the query history (including the extracted query) as the
gold-standard slot value.
understanding accuracy and speed trade-offs
in order to design an effective crowd-powered real-time en-
tity extraction system, it is crucial to understand trade-offs
between accuracy and speed. these trade-offs correspond to
the three main variables in our system: the number of play-
ers recruited to answer each query in the dialog esp game,
the time constraint that each player has to answer a query,
and the method to aggregate input answers. we have 3
ways to aggregate the input answers from the esp game:
    esp only: return the    rst matched answer. if no answers

match within the given time, return an empty label.

    ith only: return the ith input answer (i = 1, 2, ...). for

example, i = 1 means to return the    rst input answer.

    esp + ith: return the    rst matched answers of the esp
game. if no answers match within the given time, return
the ith answer.
we recruit 10 players for each esp game, and randomly
select player results to simulate the conditions of various
player numbers. all results reported in experiment 1 are the
averages of 20 rounds of this random-pick simulation pro-
cess. after empirically testing the interface, we run two sets
of studies with time constraints set at 20 and 15 seconds,
respectively. different methods to aggregate input answers

time
const. aggregate

#

player

20s

15s

esp+
1st

1st
only

esp
only

esp+
1st

1st
only

esp
only

10
5
10
5
10
5
10
5
10
5
10
5

avg.
resp.
time
7.837s
11.160s
5.590s
6.924s
7.837s
11.160s
8.129s
10.628s
5.895s
7.136s
8.129s
10.628s

p

r

f1

.867
.828
.713
.730
.867
.856
.837
.799
.739
.729
.860
.872

.916
.877
.753
.769
.916
.797
.893
.798
.764
.726
.865
.637

.891
.852
.732
.749
.891
.826
.864
.798
.751
.727
.863
.736

table 1: dialog esp game results in class a given different
settings of number of players, time constraint (time const.),
and the method to aggregate input answers.

could result in different response speed and output quality.
note that if there are not any input answers, the methods
above will wait until the time constraint and return an empty
label. in the actual experiments, 5 dialog esp games for 5
different class-a queries are aggregated in one task, with an
extra scripted game at the beginning as a tutorial. when the
   rst game ends, the timer of the second esp game starts and
a browser alert informs the worker. all experiments are run
on mturk; 800 human intelligence tasks (hits) are posted,
and 588 unique workers participate in this study.

table 1 shows the results on class a queries. with 10
players and a 20-second time constraint, the dialog esp
game achieves a best f1-score of 0.891 by the    esp+1st   
setting, and achieves the fastest average response time of
5.590 seconds by the    1st    setting. the esp+1st setting
achieves the best f1-score, and the 1st only setting has
the shortest response time. in most cases, tightening the
time constraint provides a faster response but reduces out-
put quality.

we also analyze the relations among worker numbers,
performance, and response time. first, figure 4 shows out-
put quality with respect to answer   s input order. on aver-
age, earlier input answers are of better quality, unless 10
or more players participate in the game. however, with 10
players, almost all esp games have at least one matched an-
swer pair so that the ith answer is not solely used. there-
fore, for the following experiments, we set i as 1. second,
in figure 3(a) we observe the relations between the num-
ber of players and average response time. adding players
reduces the average response time for all settings. third, the
relations between number of players and output quality are
also analyzed. figure 3(b) shows that the f1-scores increase
when adding more players, even with the    1st only    setting.

3

figure 3: trade-off curves between accuracy, average response time and number of players.

language understanding model, implemented by an recur-
rent neural network (id56) with gru cells (mesnil et al.
2015b; chen et al. 2016), on the crowd-labelled or expert-
labelled 200 atis conversations used in our study, and use
the standard atis class-a testing set to compare the per-
formance in terms of the number of training samples. fig-
ure 5 illustrates the performance curves of models trained
on crowd-labelled and expert-labelled data. the result shows
that crowd-generated labels can be used to train machine-
learning models for id64 in new domains, and
achieve comparable performance to expert-labelled data.

evaluation on complex queries
based on the study above, for class d and x queries, we use
the dialog esp game of 10 players with    esp+1st    and    1st
only    settings to measure the best f1-score and speed. the
time constraint is set to 20 seconds. the experiments are run
on mturk and all settings are identical as the previous sec-
tion. 76 distinct workers participate in class d experiments,
and 68 distinct workers participate in class x experiments.
experimental results are shown in table 2. an automated
crf model is implemented as a baseline.2 the crf model
is trained on the class-a training set mentioned above by us-
ing neighbor words (window size = 2) and pos tag features.
the crf model is decoded and timed on a laptop with in-
tel i5-4200u cpu (@1.60ghz) and 8gb ram. as a result,
the proposed crowd-powered approach largely outperforms
the crf baseline in terms of f1-score on both class d and
x queries . although the crf approach is well-developed
on class a data, it is not effective on the remaining data.

surprisingly, we    nd similar average response times in
each query category. note that the text length is different for
each category: the average token number of class-a queries
is 11.47, of class-d queries (including the query history)
is 48.64, and of class-x queries is 67.72. studies showed
that eyes    warm-up time (inhoff and rayner 1986) and word
frequency in   uence speed of text comprehension (rayner
and duffy 1986; healy 1976). these factors might reduce
the effect of text length to the reading speed of crowd works.
we also conduct an error analysis on the result of
   esp+1st    setting, which achieves our best f1-score. the
distribution of error types are shown in table 3. the
   fromloc.city name    type indicates that the crowd ex-
tracts the departure city, rather than destination city; in    in-

2implemented with crf++: http://taku910.github.io/crfpp/

figure 4: f1-score of the    ith only    setting. earlier input
answers are generally of better quality (unless #players (cid:62)
10, where almost all esp games have at least one matched
answer and the ith answer might not be solely used.)

figure 5: the performance of the extracted destination city
predicted by the id56-gru trained on crowd-labelled and
expert-labelled data. crowd-labelled labels can be used to
train machine-learning models and achieve comparable per-
formance to expert-labelled data.

finally, figure 3(c) demonstrates the trade-offs between per-
formance and speed. for a    xed number of players, different
input aggregate methods have different response times and
f1-scores. the esp game requires more time for input an-
swer matching, but in return output quality increases.

id64 automated system performance
once our crowd-powered system starts extracting entities,
the collected annotations can serve as training data for
the automated system. in order to see whether the crowd-
annotated data is good enough for training a machine learn-
ing model, and how many of text instances are required to
achieve reasonable performance, we train a state-of-the-art

4

481216200246810avg. resp. time (sec) # player esp + first (20 sec)esp + first (15 sec)first (20 sec)first (15 sec)0.600.700.800.901.000246810f1-score # player esp + 1st (20 sec)esp + 1st (15 sec)1st (20 sec)1st (15 sec)0.600.700.800.9056789101112f1-score avg. response time (sec) 10 players9 player8 players7 players6 players5 playersesp + 1st (20 sec) 1st only (20 sec) (a) (b) (c) 0.600.700.8001234567f1-score input order (i) 10 players4 players2 players01020304050607080020406080100120140160180200f1 (%)#training samples for id56crowd-labelledexpert-labelledquery category

methods

crf baseline

1st only
esp + 1st

class d (context-dependent)

class x (unevaluable)

resp. time

0.043s
5.460s
7.118s

p
.776
.658
.814

r
.307
.641
.797

f1
.440
.649
.805

resp. time

0.061s
6.342s
8.301s

p
.636
.563
.654

r
.285
.577
.675

f1
.393
.570
.664

resp. time

class a (context-independent)
f1
.986
.732
.891

0.019s
5.590s
7.837s

p
.985
.713
.867

r
.987
.753
.916

table 2: result for class d, x and a. crowd-powered entity extraction outperforms the crf baseline in terms of f1-score on
both class d and x queries. although the crf baseline is well-developed on class a, it is not effective on complex queries.

error type

fromloc.city name
false negative
incorrect city
correct city & soft match
false positive

class d class x class a
39.53% 16.67%
18.60% 26.67%
16.28% 18.33%
16.28%
5.00%
9.30% 33.33%

40.00%
0.00%
8.00%
12.00%
40.00%

table 3: error analysis for class d, x and a.

correct city    type, the crowd extracts an incorrect city from
the query history (but not the departure city);    correct city
& soft match    type means the extracted city name is se-
mantically correct but does not match the gold-standard city
name (e.g.,    washington    and    washington dc   ). from the
error analysis, we conclude two directions to improve per-
formance: 1) treat the cases of absent slot more carefully,
and 2) use domain knowledge if available. first, 28% of er-
rors in class d and 50% in class x occur when either the
gold-standard label or the predicted label does not exist. it
suggests that a more reliable step to recognize the existence
of the targeted entity might be required. second, 16.28% of
class-d queries and 5% of class-x queries are of the    soft
match    cases. by introducing domain knowledge like a list
of city names, a post-processor that    nds the most similar
city name of the predicted label can    x this type of error.

experiments 2:

user experiment via a real-world instant

messaging interface

to examine the feasibility of real-time crowd-powered entity
extraction in an actual system, we conduct lab-based user
experiments via google hangouts    instant messaging inter-
face. our proposed method has a task completion time of
5-8 seconds, per experiment 1. in this section, we demon-
strate our approach is robust and fast enough to support a
real-world instant messaging application, where the average
time gap between conversational turns is 24 seconds (isaacs
et al. 2002).
system implementation
we implemented a google hangouts chatbot by using the
hangupsbot3 framework. users are able to send text chats
to our chatbot via google hangouts. the chatbot recruits
crowd workers on mturk in real-time to perform the dialog
esp game task upon receiving the chat. figure 6 shows the
overview of our system. we record all answers submitted by

3https://github.com/hangoutsbot/hangoutsbot

recruited workers and log the timestamps of following ac-
tivities: 1) users    and workers    keyboard typing, 2) workers   
task arrival, and 3) the workers    answer submissions.

to recruit crowd workers, we introduce    eeting task, a re-
cruiting practice inspired by quikturkit (bigham et al. 2010).
this approach achieves low latency by posting hundreds of
short lifetime tasks, which increases task visibility. its short
lifetime (e.g., 60 seconds) encourages workers to complete
tasks quickly. a core bene   t of the    eeting task approach is
its ease in implementation: the method bypasses the com-
mon practices of pre-recruiting workers and maintaining
a waiting pool (bigham et al. 2010; lasecki et al. 2013;
bernstein et al. 2011). in a system deployed at scale, a re-
tainer or push model is likely to work as well.

user experiment setup
we conduct lab-based user experiments to evaluate the pro-
posed technology on extracting    food    entities. ten google
hangouts users enter our lab with their own laptops. we    rst
ask them to arbitrarily create a list 9 foods, 3 drinks, and 3
countries based on their own preferences. then we explain
the purpose of the experiments, and introduce    ve scenarios
of using instant messaging:
1. eat: you discuss with your friend about what to eat later.
2. drink: you discuss with an employee a coffee place, bar,

or restaurant to order something to drink.

3. cook: you plan to cook later. you discuss the details with

your friend who knows how to cook.

4. chat: you are chatting with your friend.
5. no food: you are chatting with your friend. you do not

mention food. instead, you mention a country name.
we also list three types of conversational acts which could

emerge in each scenario:
1. question: ask a question.
2. answer: answer a question that could be asked under the

current scenario.

3. mentioning: naturally converse without asking or an-

swering any speci   c questions.
using their laptops, users send one text chat for each com-
bination of [scenario, conversational act] to our chatbot, i.e.,
15 chats in total. in the eat, cook, and chat scenarios, users
must mention one of the foods they listed earlier; in the
drink scenario, they must mention one of the drinks they
listed. in the no food scenario, users must mention one of

5

figure 6: timeline of the real-time crowd-powered entity extraction system. on average, the    rst worker takes 30.83 seconds
to reach, the    rst answer is received at 37.14 seconds, and the    rst matched answer occurs at 40.95 seconds. a user on average
spends 27.05 seconds to type a chat line, i.e., the perceived response time to users falls within 10-14 seconds.

acc (%)

1st only
esp only
esp + 1st
1st worker reached time (s)

77.33%
81.33%
84.00%

user type time (s)

response time (s)

mean (stdev)
37.14 (14.70)
40.95 (13.56)
40.95 (13.56)
30.83 (16.86)
27.05 (25.28)

table 4: result of user experiment. a trade-off between
time and output quality can be observed.

the countries they listed, and no food names can be men-
tioned. in total, we collect 150 chat inputs from 10 user ex-
periments. correspondingly, instructions on the workers    in-
terface (figure 2) is modi   ed as    what is the food name
in this dialog?   , and the explanation of food name is mod-
i   ed as    food name. the full name of the food. including
any drinks or beverages.    in the experiments, our chatbot
post 120 hits with a lifetime of 60 seconds to mturk upon
receiving a text chat. the price of each hit is $0.1. we use
the interface shown in figure 2 with a time constraint of 20
seconds.

experimental results
results are shown in table 4. the    esp+1st    setting
achieves the best accuracy of 84% with an average response
time of 40.95 seconds. the    1st only    setting has the short-
est average response time of 37.14 seconds with an accuracy
of 77.33%.4 a trade-off between time and output quality can
be observed.this trade-off is similar to the results of exper-
iment 1 (shown in figure 3(c)). on average, 14.45 mturk
workers participated in each trial and submitted 33.81 an-
swers.

1st

esp + 1st

entity
type

conv.
act

food5
drink
none

question
answer
mention

avg.

avg.

time(s)
36.64
37.43
38.33
34.26
39.90
37.26
37.14

avg.

acc.
(%)

time(s)
40.19
70.00%
41.37
80.00%
42.83
96.67%
37.94
82.00%
43.88
68.00%
82.00%
41.04
77.33% 40.95

acc.
(%)

78.89%
83.33%
100.00%
90.00%
78.00%
84.00%
84.00%

table 5: results of user experiment for each scenario and
conversational act.

knowledge-base, our id104 approach achieves an
accuracy of 78.89% in extracting food entities and 83.33%
in extracting drink entities. despite the signi   cant variety of
the input entities6, our approach extracts most entities cor-
rectly. furthermore, our method is effective in identifying
the absence of entities; table 5 also shows the robustness
of the proposed method under various linguistic conditions.
the    esp+1st    setting achieves accuracies of 90.00% in ex-
tracting entities from questions, 78.00% in extracting from
answers, and 84.00% in extracting from regular conversa-
tions. qualitatively, our approach can handle complex input,
such as strange restaurant names and beverage names, which
are essentially confusing for automated approaches. for ex-
ample,    have you ever tried bibimbap at green pepper?   
and    i usually have magic hat #9   , where green pepper
and magic hat #9 are names of a restaurant and beverage,
respectively.

robustness in out-of-vocabulary entities & language
variability the results over each entity type are shown
in table 5. without using any training data or pre-de   ned

4we only consider the answers submitted within 60 seconds.
5including the results from food, cook, and chat scenarios.

6 the food entities arbitrarily created by our users are quite di-
verse: from a generic category (e.g., thai food) to a speci   c en-
try (e.g., magic hat #9), and from a simple food (e.g., cherry) to
a complex food (e.g., sausage muf   n with egg). the list covers
the food of many other countries (e.g., okonomiyaki, bibimbap,
samosa.)

6

-0.5-0.3-0.10.10.30.5051015202530354045 30.83 sec 37.14 sec 40.95 sec avg user typing time = 27.05s chatbot task routing time task completion time worker searching time task posting time fleeting task method post 120 fleeting tasks tasks become available to workers 1st worker arrives 1st response 1st matched response user error analysis table 6 shows the errors in the user exper-
iments (   esp+1st    setting). 45.83% of errors are caused by
absence of answers, mainly due to the task routing latency
of the mturk platform. we discuss this in more detail below.
37.50% of errors are due to various system problems such
as the string encoding issues. more interestingly, 12.50% of
incorrect answers are sub-spans of the correct answers. for
instance, the crowd extracts    rice    for    stew pork over rice   ,
and    tea    for    bubble tea   . this type of error is similar to
the    soft match    error in experiment 1. finally, 4.17% of
errors are caused by user typos (e.g., latter for latte), which
the crowd tends to exclude in their answers.

error type
no answers received
system problem
substring of a multi-token entity
typo

%

45.83%
37.50%
12.50%
4.17%

table 6: error analysis for user experiment.

response speed table 4 shows the average response time
in the user experiment. on average, the    rst worker takes
30.83 seconds to reach to our dialog esp game, the    rst
answer is received at 37.14 seconds, and the    rst matched
answer occurs at 40.95 seconds. for comparison, we illus-
trate the timeline of our system in figure 6. in the user ex-
periments, a user on average spends 27.05 seconds to type
a chat line. if we align the user typing time along with
the system timeline, the theoretical perceived response time
to users falls within 10-14 seconds, while the average re-
sponse time in instant messaging is 24 seconds (isaacs et
al. 2002). (baron 2010) reports that 24.5% of instant mes-
sages get responses within 11-30 seconds, and 8.2% of mes-
sages have even longer response times. the proposed tech-
nology proves to be fast enough to support instant messag-
ing applications. the main bottleneck of the end-to-end re-
sponse speed is the task routing time in figure 6, which
approximately ranges from 5-40 seconds and changes over
time. the task routing time also causes the major errors in
table 6. the task lifetime begins when a task reaches the
mturk server instead of when it becomes visible to work-
ers. when the task routing time is longer than a task   s life-
time, the task could expire before it is selected by work-
ers. because mturk requesters can not effectively reduce
the task routing time, pre-recruiting and queuing workers
seems inevitable for applications which require a response
time sharply shorter than 30 seconds.

discussion

incorporating domain-speci   c knowledge is a major obsta-
cle in generalization of id104 technologies (huang,
lasecki, and bigham 2015). we think that automation helps
resolve this challenge. one most common errors in our sys-
tem are the soft match, where the crowd extracts a sub-string
of the target entity instead of the complete string. domain

7

knowledge can help to    x this type of errors. however, un-
like automated technology, we do not have a generic method
to update human workers with new knowledge. thus, our
next step is to incorporate automated components. it is easy
to replace some workers with automated annotators in our
multi-player esp game. despite fragility in extracting un-
seen entities, automated approaches are robust in identify-
ing known entities and can be easily updated if new data
is collected. we will develop a hybrid approach, which we
believe will be robust in unexpected input and easily incor-
porate new knowledge.

conclusion and future work

we have explored using real-time id104 to extract
entities for id71. by using an esp game setting,
our approach is absolute 36.5% and 27.1% better than the
crf baseline in terms of f1-score for class d and x queries
in the atis dataset, respectively. the timing cost is about 8
seconds, which is slower than machines but still reasonable
given the large gains in accuracy. the proposed method also
has been evaluated via google hangouts    text chat with 10
users. the results demonstrate the robustness and feasibility
of our approach in real-world systems. in the future, we will
generalize our approach by adding automated components,
and also explore the possibility of using audio input.

appendix: list of food and drinks used in the

experiment 2

the followings are the lists of 9 food and 3 drinks created
by 10 participants in our user experiment.

food
1. spaghetti, burger, vindaloo lamb, makhani chicken, kim-
chee, wheat bread pizza, cornish pasty, mushroom soup

2. burger, french fries, scallion cake, okonomiyaki, oy-

akodon, gyudon, fried rice, wings, salad

3. stinky tofu, acai berry bowl, tuna onigiri, rice burger,
seared salmon, milk   sh soup, mapo tofu, beef pho,
scallion pancake

4. pizza, fried rice, waf   e, alcohol drink, chocolate pie,

cookie, dimsum, burger, milk shake

5. pho, bbq, thai food, beef noodles, steak, tomato soup,

spicy hot pot, soup dumplings, ramen

6. chocolate, donut, cheesecake, pad thai, seafood pancake,

   sh    llets in hot chili, hot pot, bibimbap, japchae
fried    sh,

7. chocolate, pancakes,

strawberries,

chicken, sausages, gulaab jamun, paneer tika, samosa

fried

8. dumplings, noodle, stew pork over rice, sandwich, pasta,
hot pot, potato slices with green peppers, chinese bbq,
pancakes

9. stinky tofu, stew pork over rice, yakitori, baked cinnamon
apple, apple pie, stew pork with potato and apple, teppa-
nyaki, okonomiyaki, crab hotpot

10. hot pot, cherry, chinese cabbage, pumpkin risotto,
tomato risotto, boeuf bourguignon, stinky tofu, sausage
muf   n with egg (mcdonald), eggplant with basil

drink
1. tea, coke, latte
2. green tea latte, bubble tea, root beer
3. medium latte with non-fat milk, green tea latte,

soymilk

4. water, pepsi, tea
5. latte with nonfat milk, magic hat #9, old fashion
6. vanilla latte, strawberry smoothie, iced tea
7. coffee, milk shake, beer
8. mocha coffee, beers, orange juice
9. caramel frappuccino, caramel macchiato, coffee with co-

conut milk

10. ice tea, macha, apple juice

references

[baron 2010] baron, n. s. 2010. discourse structures in in-
stant messaging: the case of utterance breaks. language@
internet 7(4):1   32.
[bernstein et al. 2011] bernstein, m. s.; brandt, j.; miller,
r. c.; and karger, d. r. 2011. crowds in two seconds:
enabling realtime crowd-powered interfaces. in uist, 33   
42. acm.
[bigham et al. 2010] bigham, j. p.; jayant, c.; ji, h.; lit-
tle, g.; miller, a.; miller, r. c.; miller, r.; tatarowicz, a.;
white, b.; white, s.; et al. 2010. vizwiz: nearly real-time
answers to visual questions. in uist, 333   342. acm.
[bohus et al. 2007] bohus, d.; raux, a.; harris, t. k.; es-
kenazi, m.; and rudnicky, a. i. 2007. olympus: an open-
source framework for conversational spoken language inter-
face research. in proceedings of the workshop on bridging
the gap: academic and industrial research in dialog tech-
nologies, 32   39. association for computational linguistics.
[chen et al. 2016] chen, y.-n.; hakkani-tur, d.; tur, g.;
celikyilmaz, a.; gao, j.; and deng, l. 2016. knowledge as
a teacher: knowledge-guided structural attention networks.
arxiv preprint arxiv:1609.03286.
[chen, hakkani-t  ur, and tur 2014] chen, y.-n.; hakkani-
t  ur, d.; and tur, g. 2014. deriving local relational sur-
face forms from dependency-based entity embeddings for
unsupervised spoken language understanding. proceedings
of slt.
[dahl et al. 1994] dahl, d. a.; bates, m.; brown, m.; fisher,
w.; hunicke-smith, k.; pallett, d.; pao, c.; rudnicky, a.;
and shriberg, e. 1994. expanding the scope of the atis
in hlt, 43   48. association for
task: the atis-3 corpus.
computational linguistics.
[he and young 2003] he, y., and young, s. 2003. a data-
driven spoken language understanding system. in asru   03,
583   588. ieee.
[healy 1976] healy, a. f. 1976. detection errors on the word
the: evidence for reading units larger than letters. journal
of experimental psychology: human perception and perfor-
mance 2(2):235.

8

t.-h.

k.;
2015. guardian: a
in

[heck, hakkani-t  ur, and t  ur 2013] heck, l. p.; hakkani-
t  ur, d.; and t  ur, g. 2013. leveraging id13s
in inter-
for web-scale unsupervised id29.
speech, 1594   1598.
[hirschman 1992] hirschman, l. 1992. multi-site data col-
lection for a spoken language corpus. in proceedings of the
workshop on speech and natural language, hlt    91, 7   
14. stroudsburg, pa, usa: association for computational
linguistics.
[huang et al. 2017] huang, t.-h. k.; chen, y.-n.; bigham,
j. p.; et al. 2017. real-time on-demand crowd-powered en-
tity extraction. in in proceedings of the 5th edition of the
collective intelligence conference (ci 2017, oral presenta-
tion).
[huang, lasecki, and bigham 2015] huang,
lasecki, w. s.; and bigham, j. p.
crowd-powered spoken dialog system for web apis.
hcomp.
[inhoff and rayner 1986] inhoff, a. w., and rayner, k.
1986. parafoveal word processing during eye    xations in
reading: effects of word frequency. perception & psy-
chophysics 40(6):431   439.
[isaacs et al. 2002] isaacs, e.; walendowski, a.; whittaker,
s.; schiano, d. j.; and kamm, c. 2002. the character,
functions, and styles of instant messaging in the workplace.
in proceedings of the 2002 acm cscw, 11   20. acm.
[lasecki et al. 2013] lasecki, w. s.; wesley, r.; nichols, j.;
kulkarni, a.; allen, j. f.; and bigham, j. p. 2013. chorus: a
crowd-powered conversational assistant. in uist    13, uist
   13, 151   162. new york, ny, usa: acm.
[lasecki, kamar, and bohus 2013] lasecki, w. s.; kamar,
e.; and bohus, d. 2013. conversations in the crowd: col-
lecting data for task-oriented dialog learning. in hcomp.
[mesnil et al. 2015a] mesnil, g.; dauphin, y.; yao, k.; ben-
gio, y.; deng, l.; hakkani-tur, d.; he, x.; heck, l.; tur,
g.; yu, d.; et al. 2015a. using recurrent neural networks
for slot    lling in spoken language understanding. audio,
speech, and language processing, ieee/acm transactions
on 23(3):530   539.
[mesnil et al. 2015b] mesnil, g.; dauphin, y.; yao, k.; ben-
gio, y.; deng, l.; hakkani-tur, d.; he, x.; heck, l.; tur,
g.; yu, d.; et al. 2015b. using recurrent neural networks for
ieee/acm
slot    lling in spoken language understanding.
transactions on audio, speech, and language processing
23(3):530   539.
[raymond and riccardi 2007] raymond, c., and riccardi,
g.
2007. generative and discriminative algorithms for
spoken language understanding. in interspeech, 1605   
1608.
[rayner and duffy 1986] rayner, k., and duffy, s. a. 1986.
lexical complexity and    xation times in reading: effects
of word frequency, verb complexity, and lexical ambiguity.
memory & cognition 14(3):191   201.
[tur, hakkani-tur, and heck 2010] tur, g.; hakkani-tur,
d.; and heck, l. 2010. what is left to be understood in
atis? in slt, 2010 ieee, 19   24. ieee.

[von ahn and dabbish 2004] von ahn, l., and dabbish, l.
2004. labeling images with a computer game. in proceed-
ings of the sigchi conference on human factors in com-
puting systems, 319   326. acm.
[wang et al. 2012] wang, w. y.; bohus, d.; kamar, e.; and
horvitz, e. 2012. id104 the acquisition of natural
language corpora: methods and observations. in slt 2012,
73   78. ieee.
[wang, heck, and hakkani-tur 2014] wang, l.; heck, l.;
and hakkani-tur, d. 2014. leveraging semantic web search
and browse sessions for multi-turn spoken id71.
in icassp 2014, 4082   4086. ieee.
[williams et al. 2013] williams, j.; raux, a.; ramachan-
dran, d.; and black, a. 2013. the dialog state tracking
challenge. in proceedings of the sigdial 2013 conference,
404   413.
[xu and sarikaya 2014] xu, p., and sarikaya, r. 2014. tar-
geted feature dropout for robust slot    lling in natural lan-
guage understanding. in fifteenth annual conference of the
international speech communication association.
[young 2006] young, s. 2006. using pomdps for dialog
management. in slt, 8   13.

9

