searchqa: a new q&a dataset

augmented with context from a search engine

matt dunn

center for data science, nyu

levent sagun

courant institute, nyu

mike higgins

center for data science, nyu

v. u  gur g  uney

senior data scientist, driversiti

volkan cirik

school of computer science, cmu

kyunghyun cho

courant institute and center for data science, nyu

abstract

we publicly release a new large-scale
dataset, called searchqa, for machine
comprehension, or question-answering.
unlike recently released datasets, such as
deepmind id98/dailymail and squad,
the proposed searchqa was constructed
to re   ect a full pipeline of general
question-answering. that is, we start not
from an existing article and generate a
question-answer pair, but start from an ex-
isting question-answer pair, crawled from
j! archive, and augment it with text snip-
pets retrieved by google. following this
approach, we built searchqa, which con-
sists of more than 140k question-answer
pairs with each pair having 49.6 snippets
on average. each question-answer-context
tuple of the searchqa comes with ad-
ditional meta-data such as the snippet   s
url, which we believe will be valuable
resources for future research. we conduct
human evaluation as well as test two base-
line methods, one simple word selection
and the other deep learning based, on the
searchqa. we show that there is a mean-
ingful gap between the human and ma-
chine performances. this suggests that
the proposed dataset could well serve as
a benchmark for question-answering.

introduction

1
one of the driving forces behind the recent suc-
cess of deep learning in challenging tasks, such
as object recognition (krizhevsky et al., 2012),
id103 (xiong et al., 2016) and ma-
chine translation (bahdanau et al., 2014), has been
the increasing availability of large-scale annotated
data.

this observation has also led to the interest
in building a large-scale annotated dataset for
question-answering. in 2015, bordes et al. (2015)
released a large-scale dataset of 100k open-world
question-answer pairs constructed from freebase,
and hermann et al. (2015) released two datasets,
each consisting of closed-world question-answer
pairs automatically generated from news articles.
the latter was followed by hill et al. (2015), ra-
jpurkar et al. (2016) and onishi et al. (2016), each
of which has released a set of large-scale closed-
world question-answer pairs focused on a speci   c
aspect of question-answering.

let us    rst take a step back, and ask what a full
end-to-end pipeline for question-answering would
look like. a general question-answering system
would be able to answer a question about any do-
main, based on the world knowledge. this system
would consist of three stages. a given question is
read and reformulated in the    rst stage, followed
by information retrieval via a search engine. an
answer is then synthesized based on the query and
a set of retrieved documents.

we notice a gap between the existing closed-
world question-answering data sets and this con-
ceptual picture of a general question-answering
system. the general question-answering system
must deal with a noisy set of retrieved documents,
which likely consist of many irrelevant docu-
ments as well as semantically and syntactically ill-
formed documents. on the other hand, most of the
existing closed-world question-answering datasets
were constructed in a way that the context pro-
vided for each question is guaranteed relevant and
well-written. this guarantee comes from the fact
that each question-answer-context tuple was gen-
erated starting from the context from which the
question and answer were extracted.

in this paper, we build a new closed-world
question-answering dataset that narrows this gap.

7
1
0
2

 

n
u
j
 

1
1

 
 
]
l
c
.
s
c
[
 
 

3
v
9
7
1
5
0

.

4
0
7
1
:
v
i
x
r
a

unlike most of the existing work, we start by
building a set of question-answer pairs from jeop-
ardy!. we augment each question-answer pair,
which does not have any context attached to it,
by querying google with the question. this pro-
cess enables us to retrieve a realistic set of rel-
evant/irrelevant documents, or more speci   cally
their snippets. we    lter out those questions whose
answers could not be found within the retrieved
snippets and those with less than forty web pages
returned by google. we end up with 140k+
question-answer pairs, and in total 6.9m snippets.1
we evaluate this new dataset, to which we re-
fer as searchqa, with a variant of recently pro-
posed attention sum reader (kadlec et al., 2016)
and with human volunteers. the evaluation shows
that the proposed searchqa is a challenging task
both for humans and machines but there is still a
signi   cant gap between them. this suggests that
the new dataset would be a valuable resource for
further research and advance our ability to build a
better automated question-answering system.

2 searchqa
collection a major goal of the new dataset is to
build and provide to the public a machine compre-
hension dataset that better re   ects a noisy informa-
tion retrieval system. in order to achieve this goal,
we need to introduce a natural, realistic noise to
the context of each question-answer pair. we use
a production-level search engine    google    for this
purpose.

we crawled the entire set of question-answer
pairs from j! archive2 which has archived all the
question-answer pairs from the popular television
show jeopardy!. we used the question from each
pair to query google in order to retrieve a set of
relevant web page snippets. the relevancy in this
case was fully determined by an unknown, but in-
production, algorithm underlying google   s search
engine, making it much closer to a realistic sce-
nario of question-answering.
cleaning because we do not have any control
over the internals of google search engine, we
extensively cleaned up the entire set of question-
answer-context tuples. first, we removed any
snippet returned that included the air-date of the
jeopardy! episode, the exact copy of the question,

1 the dataset can be found at https://github.com/

nyu-dl/searchqa.

2http://j-archive.com

figure 1: one example in .json format.

or a term    jeopardy!   ,    quiz    or    trivia   , to en-
sure that the answer could not be found trivially by
a process of word/phrase matching. furthermore,
we manually checked any url, from which these
removed snippets were taken, that occurs more
than 50 times and removed any that explicitly con-
tains jeopardy! question-answer pairs.

among the remaining question-answer-context
tuples, we removed any tuple whose context did
not include the answer. this was done mainly for
computational ef   ciency in building a question-
answering system using the proposed dataset. we
kept only those tuples whose answers were three
or less words long.

basic statistics after all these processes, we
have ended up with 140,461 question-answer
pairs. each pair is coupled with a set of 49.6  2.10
snippets on average. each snippet is 37.3  11.7
tokens long on average. answers are on aver-
age 1.47  0.58 tokens long. there are 1,257,327
unique tokens.

meta-data we collected for each question-
answer-context
tuple additional metadata from
jeopardy! and returned by google. more speci   -
cally, from jeopardy! we have the category, dollar
value, show number and air date for each ques-
tion. from google, we have the url, title and
a set of related links (often none) for each snip-
pet. although we do not use them in this paper,
these items are included in the public release of
searchqa and may be used in the future. an ex-
ample of one question-answer pair with just one
snippet is presented in fig. 1.

training, validation and test sets
in order to
maximize its reusability and reproducibility, we
provide a prede   ned split of the dataset into train-
ing, validation and test sets. one of the most im-
portant aspects in question-answering is whether
a question-answering machine would generalize
to unseen questions from the future. we thus
ensure that these three sets consist of question-
answer pairs from non-overlapping years, and that

the validation and test question-answer pairs are
from years later than the training set   s pairs. the
training, validation and test sets consist of 99,820,
13,393 and 27,248 examples, respectively. among
these, examples with unigram answers are respec-
tively 55,648, 8,672 and 17,056.

3 related work
open-world question-answering an open-
world question-answering dataset consists of a
set of question-answer pairs and the knowledge
database. it does not come with an explicit link be-
tween each question-answer pair and any speci   c
entry in the knowledge database. a representative
example of such a dataset is simpleqa by (bordes
et al., 2015). simpleqa consists of 100k question-
answer pairs, and uses freebase as a knowledge
database. the major limitation of this dataset is
that all the questions are simple in that all of them
are in the form of (subject, relationship, ?).
closed-world question-answering although
we use open-world snippets, the    nal searchqa
is a closed-world question-answering dataset since
each question can be answered entirely based
on the associated snippets. one family of such
datasets includes children   s book dataset (hill
et al., 2015), id98 and dailymail (hermann et al.,
2015). each question-answer-context tuple in
these datasets was constructed by    rst selecting
the context article and then creating a question-
answer pair, where the question is a sentence
with a missing word and the answer is the miss-
ing word. this family differs from searchqa in
two aspects. first, in searchqa we start from a
question-answer pair, and, second, our question is
not necessarily of a    ll-in-a-word type.

this

family of datasets.

another family is an extension of the for-
mer
family in-
cludes squad (rajpurkar et al., 2016) and
newsqa (trischler et al., 2016). unlike the
   rst family, answers in this family are often multi-
word phrases, and they do not necessarily appear
as they are in the corresponding context. in con-
trast, in searchqa we ensure that all multi-word
phrase answers appear in their corresponding con-
text. answers, often as well as questions, are thus
often crowd-sourced in this family of datasets.
nonetheless, each tuple in these datasets was how-
ever also constructed starting from a correspond-
ing context article, making them less realistic than
the proposed searchqa.

answer

per-question average

per-user average
per-user std. dev.

f1 score (for id165)

unigram id165
66.97% 42.86%
64.85% 43.85%
10.43%
8.16%
57.62 %

-

table 1: the accuracies achieved by the volun-
teers.

ms marco (nguyen et al., 2016)   the most
recently released dataset to our knowledge    is
perhaps most similar to the proposed searchqa.
nguyen et al. (2016) selected a subset of actual
user-generated queries to microsoft bing that cor-
respond to questions. these questions are aug-
mented with a manually selected subset of snip-
pets returned by bing. the question is then an-
swered by a human. two major differences be-
tween ms marco and searchqa are the choice
of questions and search engine. we believe the
comparison between ms marco and the pro-
posed searchqa would be valuable for expand-
ing our understanding on how the choice of search
engines as well as types of questions impact
question-answering systems in the future.

4 experiments and results

as a part of our release of searchqa, we provide
a set of baseline performances against which other
researchers may compare their future approaches.
unlike most of the previous datasets, searchqa
augments each question-answer pair with a noisy,
real context retrieved from the largest search en-
gine in the world. this implies that the human per-
formance is not necessarily the upper-bound but
we nevertheless provide it as a guideline.

4.1 human evaluation
we designed a web interface that displays a query
and retrieved snippets and lets a user select an an-
swer by clicking words on the screen. a user is
given up to 40 minutes to answer as many ques-
tions as possible. we randomly select question-
answer-context pairs from the test set.

we recruited thirteen volunteers from the mas-
ter   s program in the center for data science at
nyu. they were uniform-randomly split into two
groups. the    rst group was presented with ques-
tions that have single-word (unigram) answers
only, and the other group with questions that have
either single-word or multi-word (id165) an-
swers. on average, each participant answers 47.23
questions with the standard deviation of 30.42.

we report the average and standard deviation of
the accuracy achieved by the volunteers in table 1.
we notice the signi   cant gap between the accura-
cies by the    rst and second groups, suggesting that
the dif   culty of question-answering grows as the
length of the answer increases. also, according
to the f1 scores, we observe a large gap between
the asr and humans. this suggests the potential
for the proposed searchqa as a benchmark for ad-
vancing question-answering research. overall, we
found the performance of human volunteers much
lower than expected and suspect the following un-
derlying reasons. first, snippets are noisy, as they
are often excerpts not full sentences. second, hu-
man volunteers may have become exhausted over
the trial. we leave more detailed analysis of the
performance of human subjects on the proposed
searchqa for the future.

4.2 machine baselines
tf-idf max an interesting property of the
proposed searchqa is that the context of each
question-answer pair was retrieved by google
with the question as a query. this implies that
the information about the question itself may be
implicitly embedded in the snippets. we therefore
test a naive strategy (tf-idf max) of selecting the
word with the highest tf-idf score in the context
as an answer. note that this can only be used for
the questions with a unigram answer.

attention sum reader attention sum reader
(asr, kadlec et al., 2016) is a variant of a pointer
network (vinyals et al., 2015) that was speci   -
cally constructed to solve a cloze-style question-
answering task. asr consists of two encoding
recurrent networks. the    rst network encodes a
given context c, which is the concatenation of all
the snippets in the case of searchqa, into a set of
hidden vectors {hc
j}, and the second network en-
codes a question q into a single vector hq. the
dot product between each hidden vector from the
context and the question vector is exponentiated
to form word scores   j = exp(hq(cid:62)hc
j). asr then
pulls these word scores by summing the scores of
the same word, resulting in a set of unique word
scores   (cid:48)
  j, where di indicates where
the word i appears in the context. these unique-
word scores are normalized, and we obtain an an-
swer distribution p(i|c, q) =   (cid:48)
i(cid:48). the
asr is trained to maximize this (log-)id203
of the correct answer word in the context.

i =(cid:80)

i/(cid:80)

i(cid:48)   (cid:48)

j   di

model
set
tf-idf valid
test
valid
test

asr

max

unigram

id165

acc acc@5
13.0
12.7
43.9
41.3

49.3
49.0
67.3
65.1

f1
   
   

24.2
22.8

table 2:
the accuracies on the validation and
test sets using the non-trainable baseline (tf-idf
max) and the trainable baseline (asr). we report
top-1/5 accuracies for unigram answers, and oth-
erwise, f1 scores.

this vanilla asr only works with a unigram
answer and is not suitable for an id165 answer.
we avoid this issue by introducing another recur-
rent network which encodes the previous answer
words (  a1, . . . ,   al   1) into a vector ha. this vector
is added to the question vectors, i.e., hq     hq+ha.
during training, we use the correct previou an-
swer words, while we let the model, called id165
asr, predict one answer at a time until it predicts
(cid:104)answer(cid:105). this special token, appended to the con-
text, indicates the end of the answer.

we try both the vanilla and id165 asr   s on
the unigram-answer-only subset and on the whole
set, respectively. we use recurrent networks with
100 id149 (gru, cho et al., 2014)
for both unigram and id165 models, respec-
tively. we use adam (kingma and ba, 2014) and
dropout (srivastava et al., 2014) for training.

result we report the results in table 2. we see
that the attention sum reader is below human eval-
uation, albeit by a rather small margin. also, tf-
idf max scores are not on par when compared to
asr which is perhaps not surprising. given the
unstructured nature of searchqa, we believe im-
provements on the benchmarks presented are cru-
cial for developing a real-world q&a system.

5 conclusion

for question-
we constructed a new dataset
answering research, called searchqa. it was built
using an in-production, commercial search engine.
it closely re   ects the full pipeline of a (hypothet-
ical) general question-answering system, which
consists of information retrieval and answer syn-
thesis. we conducted human evaluation as well
as machine evaluation. using the latest tech-
nique, asr, we show that there is a meaningful
gap between humans and machines, which sug-
gests the potential of searchqa as a benchmark

takeshi onishi, hai wang, mohit bansal, kevin gim-
pel, and david mcallester. 2016. who did what:
a large-scale person-centered cloze dataset. arxiv
preprint arxiv:1608.05457 .

pranav rajpurkar, jian zhang, konstantin lopyrev, and
percy liang. 2016.
squad: 100,000+ questions
for machine comprehension of text. arxiv preprint
arxiv:1606.05250 .

nitish srivastava, geoffrey e hinton, alex krizhevsky,
ilya sutskever, and ruslan salakhutdinov. 2014.
dropout: a simple way to prevent neural networks
from over   tting. journal of machine learning re-
search 15(1):1929   1958.

adam trischler, tong wang, xingdi yuan, justin har-
ris, alessandro sordoni, philip bachman, and ka-
heer suleman. 2016. newsqa: a machine compre-
hension dataset. arxiv preprint arxiv:1611.09830 .

oriol vinyals, meire fortunato, and navdeep jaitly.
2015. id193. in advances in neural in-
formation processing systems. pages 2692   2700.

wayne xiong, jasha droppo, xuedong huang, frank
seide, mike seltzer, andreas stolcke, dong yu, and
geoffrey zweig. 2016. achieving human parity in
conversational id103. arxiv preprint
arxiv:1610.05256 .

task for question-answering research. we release
searchqa publicly, including our own implemen-
tation of asr and id165 asr in pytorch.3

acknowledgments
kc thanks support by google, nvidia, ebay and
facebook. md conducted this work as a part of
ds-ga 1010: independent study in data science
at the center for data science, new york univer-
sity.

references
dzmitry bahdanau, kyunghyun cho, and yoshua ben-
gio. 2014. id4 by jointly
arxiv preprint
learning to align and translate.
arxiv:1409.0473 .

antoine bordes, nicolas usunier, sumit chopra, and
jason weston. 2015. large-scale simple question
answering with memory networks. arxiv preprint
arxiv:1506.02075 .

kyunghyun cho, bart van merrienboer, caglar gul-
cehre, fethi bougares, holger schwenk, and yoshua
bengio. 2014.
learning phrase representations
using id56 encoder-decoder for statistical machine
translation. in conference on empirical methods in
natural language processing (emnlp 2014).

karl moritz hermann, tomas kocisky, edward
grefenstette, lasse espeholt, will kay, mustafa su-
leyman, and phil blunsom. 2015. teaching ma-
chines to read and comprehend. in advances in neu-
ral information processing systems. pages 1693   
1701.

felix hill, antoine bordes, sumit chopra, and jason
weston. 2015. the goldilocks principle: reading
children   s books with explicit memory representa-
tions. arxiv preprint arxiv:1511.02301 .

rudolf kadlec, martin schmid, ondrej bajgar, and
jan kleindienst. 2016. text understanding with
the attention sum reader network. arxiv preprint
arxiv:1603.01547 .

diederik kingma and jimmy ba. 2014. adam: a
method for stochastic optimization. arxiv preprint
arxiv:1412.6980 .

alex krizhevsky, ilya sutskever, and geoffrey e hin-
ton. 2012.
id163 classi   cation with deep con-
volutional neural networks. in advances in neural
information processing systems. pages 1097   1105.

tri nguyen, mir rosenberg, xia song, jianfeng gao,
saurabh tiwary, rangan majumder, and li deng.
2016. ms marco: a human generated machine
arxiv preprint
reading comprehension dataset.
arxiv:1611.09268 .
3http://pytorch.org/

