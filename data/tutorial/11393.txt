punny captions: witty wordplay in image descriptions

arjun chandrasekaran1

devi parikh1,2

mohit bansal3

1georgia institute of technology

2facebook ai research

3unc chapel hill

{carjun, parikh}@gatech.edu

mbansal@cs.unc.edu

8
1
0
2

 

y
a
m
1
3

 

 
 
]
l
c
.
s
c
[
 
 

2
v
4
2
2
8
0

.

4
0
7
1
:
v
i
x
r
a

abstract

wit is a form of rich interaction that is often
grounded in a speci   c situation (e.g., a com-
ment in response to an event). in this work,
we attempt to build computational models that
can produce witty descriptions for a given im-
age.
inspired by a cognitive account of hu-
mor appreciation, we employ linguistic word-
play, speci   cally puns, in image descriptions.
we develop two approaches which involve re-
trieving witty descriptions for a given image
from a large corpus of sentences, or generat-
ing them via an encoder-decoder neural net-
work architecture. we compare our approach
against meaningful baseline approaches via
human studies and show substantial improve-
ments. we    nd that when a human is sub-
ject to similar constraints as the model regard-
ing word usage and style, people vote the im-
age descriptions generated by our model to be
slightly wittier than human-written witty de-
scriptions. unsurprisingly, humans are almost
always wittier than the model when they are
free to choose the vocabulary, style, etc.
introduction
   wit is the sudden marriage of ideas
which before their union were not per-
ceived to have any relation.   

1

witty remarks are often contextual,

    mark twain
i.e.,
grounded in a speci   c situation. developing com-
putational models that can emulate rich forms of
interaction like contextual humor, is a crucial step
towards making human-ai interaction more nat-
ural and more engaging (yu et al., 2016). e.g.,
witty chatbots could help relieve stress and in-
crease user engagement by being more personable
and human-like. bots could automatically post
witty comments (or suggest witty responses) on
social media, chat, or messaging.

(a) generated: a poll (pole)
on a city street at night.
retrieved:
(night) chuckled.
human:
the knight (night)
in shining armor drove away.

the light knight

(b) generated: a bare (bear)
black bear walking through a
forest.
retrieved: another reporter is
standing in a bare (bear) brown
   eld.
human:
the bear killed the
lion with its bare (bear) hands.

figure 1: sample images and witty descriptions from 2
models, and a human. the words inside    ()    (e.g., pole
and bear) are the puns associated with the image, i.e.,
the source of the unexpected puns used in the caption
(e.g., poll and bare).

the absence of large scale corpora of witty cap-
tions and the prohibitive cost of collecting such a
dataset (being witty is harder than just describing
an image) makes the problem of producing con-
textually witty image descriptions challenging.

in this work, we attempt to tackle the challeng-
ing task of producing witty (pun-based) remarks
for a given (possibly boring) image. our approach
is inspired by a two-stage cognitive account of hu-
mor appreciation (suls, 1972) which states that a
perceiver experiences humor when a stimulus such
as a joke, captioned cartoon, etc., causes an incon-
gruity, which is shortly followed by resolution.

we introduce an incongruity in the perceiver   s
mind while describing an image by using an un-
expected word that is phonetically similar (pun)
to a concept related to the image. e.g., in fig. 1b,
the expectations of a perceiver regarding the image
(bear, stones, etc.)
is momentarily discon   rmed
by the (phonetically similar) word    bare   . this

incongruity is resolved when the perceiver parses
the entire image description. the incongruity fol-
lowed by resolution can be perceived to be witty.1
we build two computational models based on
this approach to produce witty descriptions for an
image. first, a model that retrieves sentences con-
taining a pun that are relevant to the image from a
large corpus of stories (zhu et al., 2015). second,
a model that generates witty descriptions for an
image using a modi   ed id136 procedure dur-
ing image captioning which includes the speci   ed
pun word in the description.

our paper makes the following contributions:
to the best of our knowledge, this is the    rst work
that tackles the challenging problem of produc-
ing a witty natural language remark in an every-
day (boring) context. we present two novel mod-
els to produce witty (pun-based) captions for a
novel (likely boring) image. our models rely on
linguistic wordplay. they use an unexpected pun
in an image description during id136/retrieval.
thus, they do not require to be trained with witty
captions. humans vote the descriptions from the
top-ranked generated captions    wittier    than three
baseline approaches. moreover, in a turing test-
style evaluation, our model   s best image descrip-
tion is found to be wittier than a witty human-
written caption2 55% of the time when the human
is subject to the same constraints as the machine
regarding word usage and style.

2 related work

humor theory. general theory of verbal hu-
mor (attardo and raskin, 1991) characterizes lin-
guistic stimuli that induce humor but implement-
ing computational models of it requires severely
restricting its assumptions (binsted, 1996).
puns. zwicky and zwicky (1986) classify puns as
perfect (pronounced exactly the same) or imper-
fect (pronounced differently). similarly, pepicello
and green (1984) categorize riddles based on the
linguistic ambiguity that they exploit     phonologi-
cal, morphological or syntactic. kao et al. (2013)
formalize the notion of incongruity in puns and
use a probabilistic model to evaluate the funniness
of a sentence. jaech et al. (2016) learn phone-edit
distances to predict the counterpart, given a pun by

1indeed, a perceiver may fail to appreciate wit if the pro-
cess of    solving    (resolution) is trivial (the joke is obvious) or
too complex (they do not    get    the joke).

2 this data is available on the author   s webpage.

drawing from automatic id103 tech-
niques. in contrast, we augment a web-scraped list
of puns using an existing model of pronunciation
similarity.
generating textual humor. jape (binsted and
ritchie, 1997) also uses phonological ambiguity
to generate pun-based riddles. while our task in-
volves producing free-form responses to a novel
stimulus, jape produces stand-alone    canned   
jokes. hahacronym (stock and strapparava,
2005) generates a funny expansion of a given
acronym. unlike our work, hahacronym oper-
ates on text, and is limited to producing sets of
words.
petrovic and matthews (2013) develop
an unsupervised model that produces jokes of the
form,    i like my x like i like my y, z    .
generating multi-modal humor. wang and wen
(2015) predict a meme   s text based on a given
funny image.
similarly, shahaf et al. (2015)
and radev et al. (2015) learn to rank cartoon cap-
tions based on their funniness. unlike typical, bor-
ing images in our task, memes and cartoons are
images that are already funny or atypical. e.g.,
   lol-cats    (funny cat photos),    bieber-memes   
(modi   ed pictures of justin bieber), cartoons with
talking animals, etc. chandrasekaran et al. (2016)
alter an abstract scene to make it more funny. in
comparison, our task is to generate witty natural
language remarks for a novel image.
poetry generation. although our tasks are differ-
ent, our generation approach is conceptually simi-
lar to ghazvininejad et al. (2016) who produce po-
etry, given a topic. while they also generate and
score a set of candidates, their approach involves
many more constraints and utilizes a    nite state
acceptor unlike our approach which enforces con-
straints during id125 of the id56 decoder.

3 approach

extracting tags. the    rst step in producing a con-
textually witty remark is to identify concepts that
are relevant to the context (image). at times, these
concepts are directly available as e.g., tags posted
on social media. we consider the general case
where such tags are unavailable, and automatically
extract tags associated with an image.

we extract

the top-5 object categories pre-
dicted by a state-of-the-art inception-resnet-v2
model (szegedy et al., 2017) trained for image
classi   cation on id163 (deng et al., 2009). we
also consider the words from a (boring) image de-

figure 2: our models for generating and retrieving image descriptions containing a pun (see sec. 3).

scription (generated from vinyals et al. (2016)).
we combine the classi   er object labels and words
from the caption (ignoring stopwords) to produce
a set of tags associated with an image, as shown in
fig. 2. we then identify concepts from this collec-
tion that can potentially induce wit.
identifying puns. we attempt to induce an incon-
gruity by using a pun in the image description. we
identify candidate words for linguistic wordplay
by comparing image tags against a list of puns.

we construct the list of puns by mining the web
for differently spelled words that sound exactly the
same (heterographic homophones). we increase
coverage by also considering pairs of words with 0
edit-distance, according to a metric based on    ne-
grained articulatory representations (ar) of word
pronunciations (jyothi and livescu, 2014). our
list of puns has a total of 1067 unique words (931
from the web and 136 from the ar-based model).
the pun list yields a set of puns that are as-
sociated with a given image and their phonolog-
ically identical counterparts, which together form
the pun vocabulary for the image. we evaluate our
approach on the subset of images that have non-
empty pun vocabularies (about 2 in 5 images).
generating punny image captions. we intro-
duce an incongruity by forcing a vanilla image
captioning model (vinyals et al., 2016) to decode
a phonological counterpart of a pun word associ-
ated with the image, at a speci   c time-step dur-
ing id136 (e.g.,    sell    or    sighed   , showed in
orange in fig. 2). we achieve this by limiting the
vocabulary of the decoder at that time-step to only
contain counterparts of image-puns. in following
time-steps, the decoder generates new words con-
ditioned on all previously decoded words. thus,
the decoder attempts to generate sentences that
   ow well based on previously uttered words.

we train two models that decode an image de-

scription in forward (start to end) and reverse
(end to start) directions, depicted as    fid56    and
   rid56    in fig. 2 respectively. the fid56 can de-
code words after accounting for the incongruity
that occurs early in the sentence and the rid56 is
able to decode the early words in the sentence af-
ter accounting for the incongruity that can occur
later. the forward id56 and reverse id56 gener-
ate sentences in which the pun appears in each of
the    rst t and last t positions, respectively.3
retrieving punny image captions. as an al-
ternative to our approach of generating witty re-
marks for the given image, we also attempt to
leverage natural, human-written sentences which
are relevant (yet unexpected) in the given con-
text. concretely, we retrieve natural language
sentences4 from a combination of the book cor-
pus (zhu et al., 2015) and corpora from the nltk
toolkit (loper and bird, 2002). the retrieved
sentences each (a) contains an incongruity (pun)
whose counterpart is associated with the image,
and (b) has support in the image (contains an im-
age tag). this yields a pool of candidate captions
that are perfectly grammatical, a little unexpected,
and somewhat relevant to the image (see sec. d).
ranking. we rank captions in the candidate pools
from both generation and retrieval models, accord-
ing to their log-id203 score under the image
captioning model. we observe that the higher-
ranked descriptions are more relevant to the image
and grammatically correct. we then perform non-
maximal suppression, i.e., eliminate captions that
are similar5 to a higher-ranked caption to reduce
3for an image, we choose t = {1, 2, ..., 5} and beam
size = 6 for each decoder. this generates a pool of 5 (t)     6
(beam size)     2 (forward + reverse decoder) = 60 candidates.
4to prevent the context of the sentence from distracting
the perceiver, we consider sentences with < 15 words. over-
all, we are left with a corpus of about 13.5 million sentences.
5two sentences are similar if the cosine similarity be-

classifiermancell phoneagroupofstandingcelltagsinputid98fid56fid56fid56fid56fid56sellpeoplesidewiththeatside (sighed)fid56selloutdoorfid56phonesfid56sell<stop>rid56phonescellrid56rid56theirrid56fid56peoplerid56ofgrouprid56rid56arid56sell<stop>theiratpeoplesell (cell)aneventrid56filter punssellsellthe street signs read thirty-eighth and eighth avenue.    i have decided to sell the group to you .       you sell  phones ?   retrieval corpusphonesimage  captioning  modelgenerationsentence reads this waythe pool to a smaller, more diverse set. we report
results on the 3 top-ranked captions. we describe
the effect of design choices in the supplementary.

4 results
data. we evaluate witty captions from our ap-
proach via human studies. 100 random images
(having associated puns) are sampled from the val-
idation set of coco (lin et al., 2014).
baselines. we compare the wittiness of descrip-
tions generated by our model against 3 qualita-
tively different baselines, and a human-written
witty description of an image. each of these evalu-
ates a different component of our approach. reg-
ular id136 generates a    uent caption that is
relevant to the image but is not attempting to be
witty. witty mismatch is a human-written witty
caption, but for a different image from the one be-
ing evaluated. this baseline results in a caption
that is intended to be witty, but does not attempt to
be relevant to the image. ambiguous is a    punny   
caption where a pun word in the boring (regular)
caption is replaced by its counterpart. this cap-
tion is likely to contain content that is relevant to
the image, and it contains a pun. however, the pun
is not being used in a    uent manner.

we evaluate the image-relevance of the top
witty caption by comparing against a boring ma-
chine caption and a random caption (see supple-
mentary).
evaluating annotations. our task is to gener-
ate captions that a layperson might    nd witty. to
evaluate performance on this task, we ask people
on amazon mechanical turk (amt) to vote for
the wittier among the given pair of captions for
an image. we collect annotations from 9 unique
workers for each relative choice and take the ma-
jority vote as ground-truth. for each image, we
compare each of the generated 3 top-ranked and
1 low-ranked caption against 3 baseline captions
and 1 human-written witty caption.6
constrained human-written witty captions. we
evaluate the ability of humans and automatic
methods to use the given context and pun words
to produce a caption that is perceived as witty. we

tween the average of the id97 (mikolov et al., 2013)
representations of words in each sentence is     0.8.
6this results in a total of 4 (captions)    2 (generation +
retrieval)    4 (baselines + human caption) = 32 comparisons
of our approach against baselines. we also compare the wit-
tiness of the 4 generated captions against the 4 retrieved cap-
tions (see supplementary) for an image (16 comparisons). in
total, we perform 48 comparisons per image, for 100 images.

figure 3: wittiness of top-3 generated captions vs.
other approaches. y-axis measures the % images for
which at least one of k captions from our approach
is rated wittier than other approaches. recall steadily
increases with the number of generated captions (k).

ask subjects on amt to describe a given image
in a witty manner. to prevent observable struc-
tural differences between machine and human-
written captions, we ensure consistent pun vocab-
ulary (utilization of pre-speci   ed puns for a given
image). we also ask people to avoid    rst person
accounts or quote characters in the image.
metric. 3, we report performance of the gener-
ation approach using the recall@k metric. for
k = 1, 2, 3, we plot the percentage of images
for which at least one of the k    best    descriptions
from our model outperformed another approach.
generated captions vs. baselines. as we see in
fig. 3, the top generated image description (top-
1g) is perceived as wittier compared to all base-
line approaches more often than not (the vote is
>50% at k = 1). we observe that as k in-
creases, the recall steadily increases, i.e., when we
consider the top k generated captions, increas-
ingly often, humans    nd at least one of them to
be wittier than captions produced by baseline ap-
proaches. people    nd the top-1g for a given image
to be wittier than mismatched human-written im-
age captions, about 95% of the time. the top-1g is
also wittier than a naive approach that introduces
ambiguity about 54.2% of the time. when com-
pared to a typical, boring caption, the generated
captions are wittier 68% of the time. further, in a
head-to-head comparison, the generated captions
are wittier than the retrieved captions 67.7% of the
time. we also validate our choice of ranking cap-
tions based on the image captioning model score.
we observe that a    bad    caption, i.e., one ranked
lower by our model, is signi   cantly less witty than
the top 3 output captions.

surprisingly, when the human is constrained to

(a) generated:
a bored
(board) bench sits in front of
a window.
retrieved: wedge sits on
the bench opposite berry,
bored (board).
human: could you please
make your pleas (please)!

a

(b) generated:
loop
(loupe) of    owers in a glass
vase.
retrieved: the    our (   ower)
inside teemed with worms.
human: piece required for
peace (piece).

(c) generated: a woman sell
(cell) her cell phone in a city.
retrieved: wright (right)
slammed down the phone.
human:
a woman sighed
(side) as she regretted the
sell.

(d) generated: a bear that is
bare (bear) in the water.
retrieved: water glistened
off her bare (bear) breast.
human: you won   t hear a
creak (creek) when the bear
is feasting.

a

(e) generated:
loop
(loupe) of scissors and a pair
of scissors.
retrieved:
continued
slicing my pear (pair) on the
cutting board.
human:
the scissors were
near, but not clothes (close).

i

(f) generated: a female ten-
nis player caught (court) in
mid swing.
retrieved:
thieves on the roof top.
human:
the man made a
loud bawl (ball) when she
threw the ball.

i caught (court)

(g) generated:
a bored
(board) living room with a
large window.
retrieved: anya sat on the
couch, feeling bored (board).
human:
the sealing (ceil-
ing) on the envelope resem-
bled that in the ceiling.

(h) generated:
a parking
meter with rode (road) in the
background.
retrieved:
sighed (side).
human: a nitting of color
didn   t make the poll (pole)
less black.

smoke speaker

figure 4: the top row contains selected examples of human-written witty captions, and witty captions generated
and retrieved from our models. the examples in the bottom row are randomly picked.

use the same words and style as the model, the
generated descriptions from the model are found
to be wittier for 55% of the images. note that in a
turing test, a machine would equal human perfor-
mance at 50%7. this led us to speculate if the con-
straints placed on language and style might be re-
stricting people   s ability to be witty. we con   rmed
this by evaluating free-form human captions.
free-form human-written witty captions. we
ask people on amt to describe an image (using
any vocabulary) in a manner that would be per-
ceived as funny. as expected, when compared
against automatic captions from our approach, hu-
man evaluators    nd free-form human captions to
be wittier about 90% of the time compared to 45%
in the case of constrained human witty captions.
clearly, human-level creative language with un-
constrained sentence length, style, choice of puns,

7recall that this compares how a witty description is con-
structed, given the image and speci   c pun words. a turing
test-style evaluation that compares the overall wittiness of a
machine and a human would refrain from constraining the
human in any way.

etc., makes a signi   cant difference in the witti-
ness of a description. in contrast, our automatic
approach is constrained by caption-like language,
length, and a word-based pun list. training mod-
els to intelligently navigate this creative freedom
is an exciting open challenge.
qualitative analysis. the generated witty cap-
tions exhibit interesting features like alliteration
(   a bare black bear ...   )
in fig. 1b and 6c. at
times, both the original pun (pole) and its counter-
part (poll) make sense for the image (fig. 1a). oc-
casionally, a pun is naively replaced by its counter-
part (fig. 6b) or rare puns are used (fig. 6d). on
the other hand, some descriptions (fig. 4e and 4h)
that are forced to utilize puns do not make sense.
see supplementary for analysis of retrieval model.

5 conclusion
we presented novel computational models in-
spired by cognitive accounts to address the chal-
lenging task of producing contextually witty de-
scriptions for a given image. we evaluate the mod-

els via human-studies, in which they signi   cantly
outperform meaningful baseline approaches.

acknowledgements
we thank shubham toshniwal for his advice regard-
ing the automatic id103 model. this
work was supported in part by: a nsf career
award, onr yip award, onr grant n00014-14-
12713, pga family foundation award, google fra,
amazon ara, darpa xai grant to dp and nvidia
gpu donations, google fra, ibm faculty award, and
bloomberg data science research grant to mb.

references
salvatore attardo and victor raskin. 1991. script the-
ory revis (it) ed: joke similarity and joke representa-
tion model. humor-international journal of humor
research 4(3-4):293   348.

kim binsted. 1996. machine humour: an imple-
mented model of puns. phd thesis, university of
edinburgh .

kim binsted and graeme ritchie. 1997. computa-
tional rules for generating punning riddles. humor:
international journal of humor research .

arjun chandrasekaran, ashwin kalyan, stanislaw an-
tol, mohit bansal, dhruv batra, c. lawrence zit-
nick, and devi parikh. 2016. we are humor be-
ings: understanding and predicting visual humor. in
cvpr.

jia deng, wei dong, richard socher, li-jia li, kai li,
and li fei-fei. 2009.
id163: a large-scale hi-
erarchical image database. in id161 and
pattern recognition, 2009. cvpr 2009. ieee con-
ference on. ieee, pages 248   255.

marjan ghazvininejad, xing shi, yejin choi, and
kevin knight. 2016. generating topical poetry. in
emnlp.

aaron jaech, rik koncel-kedziorski, and mari osten-
dorf. 2016. phonological pun-derstanding. in hlt-
naacl.

preethi jyothi and karen livescu. 2014. revisiting
word neighborhoods for id103. acl
2014 page 1.

edward loper and steven bird. 2002. nltk: the natu-
ral language toolkit. in proceedings of the acl-02
workshop on effective tools and methodologies for
teaching natural language processing and com-
putational linguistics - volume 1. association for
computational linguistics, etmtnlp    02.

tomas mikolov, ilya sutskever, kai chen, greg s cor-
rado, and jeff dean. 2013. distributed representa-
tions of words and phrases and their compositional-
in advances in neural information processing
ity.
systems.

william j pepicello and thomas a green. 1984. lan-
guage of riddles: new perspectives. the ohio state
university press.

sasa petrovic and david matthews. 2013. unsuper-

vised joke generation from big data. in acl.

dragomir radev, amanda stent, joel tetreault, aa-
sish pappu, aikaterini iliakopoulou, agustin chan-
freau, paloma de juan, jordi vallmitjana, alejandro
jaimes, rahul jha, et al. 2015. humor in collective
discourse: unsupervised funniness detection in the
new yorker cartoon caption contest. arxiv preprint
arxiv:1506.08126 .

dafna shahaf, eric horvitz, and robert mankoff.
2015.
inside jokes: identifying humorous cartoon
captions. in proceedings of the 21th acm sigkdd
international conference on knowledge discovery
and data mining. acm, pages 1065   1074.

oliviero stock and carlo strapparava. 2005. ha-
in

hacronym: a computational humor system.
acl.

jerry m suls. 1972. a two-stage model for the ap-
preciation of jokes and cartoons: an information-
processing analysis. the psychology of humor:
theoretical perspectives and empirical issues .

christian szegedy, sergey ioffe, vincent vanhoucke,
and alexander a alemi. 2017.
inception-v4,
inception-resnet and the impact of residual connec-
tions on learning. in aaai. pages 4278   4284.

oriol vinyals, alexander toshev, samy bengio, and
dumitru erhan. 2016.
show and tell: lessons
learned from the 2015 mscoco image captioning
ieee transactions on pattern analysis
challenge.
and machine intelligence .

justine t kao, roger levy, and noah d goodman.
2013. the funny thing about incongruity: a com-
putational model of humor in puns. in proceedings
of the annual meeting of the cognitive science so-
ciety.

william yang wang and miaomiao wen. 2015.

i
can has cheezburger? a nonparanormal approach to
combining textual and visual information for pre-
dicting and generating popular meme descriptions.
in naacl.

tsung-yi lin, michael maire, serge belongie, james
hays, pietro perona, deva ramanan, piotr doll  ar,
and c lawrence zitnick. 2014. microsoft coco:
in european confer-
common objects in context.
ence on id161. springer, pages 740   755.

zhou yu, leah nicolich-henkin, alan w black, and
alex i rudnicky. 2016. a wizard-of-oz study on a
non-task-oriented id71 that reacts to user
engagement. in 17th annual meeting of the special
interest group on discourse and dialogue. page 55.

yukun zhu, ryan kiros, rich zemel, ruslan salakhut-
dinov, raquel urtasun, antonio torralba, and sanja
fidler. 2015. aligning books and movies: towards
story-like visual explanations by watching movies
and reading books. in proceedings of the ieee in-
ternational conference on id161. pages
19   27.

arnold zwicky and elizabeth zwicky. 1986.

imper-
fect puns, markedness, and phonological similarity:
with fronds like these, who needs anemones. folia
linguistica 20(3-4):493   503.

appendix
we    rst present details and results for the addi-
tional experiments that evaluate the relevance of
the top generated caption from the model to the
image, and compare the retrieved captions to base-
line approaches. we then brie   y discuss the ratio-
nales for the design choices that we made in our
approach. subsequently, we brie   y describe some
characteristics of pun words and follow it up with
brief qualitative analysis of the output from the re-
trieval model. finally, we present the annotation
and evaluation interfaces that the human subjects
interacted with.
a additional experiments
a.1 relevance of witty caption to image
we compared the relative relevance of the top
witty caption from our generation approach
against a machine generated boring caption (ei-
ther for the same image or for a different, ran-
domly chosen image) in a pairwise comparison.
we showed turkers an image and a pair of cap-
tions, and asked them to choose the more relevant
caption for the image. we see that on average, the
generated witty caption is considered more rele-
vant than a machine generated boring caption for
the same image 37.5% of the time. people found
the generated witty caption to be more relevant
than a random caption 97.2% of the time. this
shows that in an effort to generate witty content,
our approach produces descriptions that are a lit-
tle less relevant compared to a boring description
for the image. but our witty caption is clearly still
relevant to the image (almost always more relevant
than an unrelated caption).

a.2 retrieved captions vs. baselines
humans evaluate the wittiness of each of the 3
top-ranked retrieved captions against baseline ap-
proaches and a human witty caption. as we see

figure 5: comparison of wittiness of the top 3 cap-
tions from our retrieval approach vs. other ap-
proaches. the y-axis measures the % images for
which at least one of k captions from our ap-
proach is rated wittier than other approaches. as
we increase the number of retrieved captions (k),
recall steadily increases.

in fig. 5, at k = 1, the top retrieved description
is found to be wittier than only a human-written
witty caption that is mismatched with the given
image (witty mismatch) 83.8% of the time. the
top retrieved caption is found less witty than even
a typical caption (regular id136) about 63.4%
of the time. similarly, the retrieved caption is also
found to be less witty than a naive method that pro-
duces punny captions (ambiguous) about 62% of
the time. we observe the trend that as k increases,
recall also increases. on average, at least one of
the top 3 retrieved captions is wittier than the (con-
strained) human witty caption about 61.6% of the
time, compared to generated captions which are
wittier 84.0% of the time.

poor performance of retrieved captions could be
due to the fact that they are often not perfectly apt
for the given image since they are retrieved from
story-based corpora. please see sec. d for exam-
ples, and a more detailed discussion. as we will
see in the next section, these issues do not extend
to the generation approach which exhibits strong
performance against baseline approaches, human-
written witty captions and the retrieval approach.
while these captions might evoke a sense of in-
congruity, it is likely hard for the viewer to resolve
the alternate interpretation of the retrieved caption
as being applicable to the image.

b design choices
in this section, we describe how our architecture
design and parameter choices in the architectures
in   uence witty descriptions. during the design of
our model, we made choices of parameters based
on observations from qualitative results. for in-
stance, we experimented with different beam sizes
to generate a set of high precision captions with
few false positives. we found that a beam size
of 6 resulted in a suf   cient number of candidate
sentences which were reasonably accurate. we
extract image tags from the top-k predictions of
an image classi   er. we experimented with differ-
ent values of k, where k     {1, 5, 10}. we also
tried using a score threshold, where classes pre-
dicted with a score above the threshold were con-
sidered valid image tags. we found that k = 5 re-
sults in reasonable predictions. determining a rea-
sonable threshold on the other hand was dif   cult
because for most images, class prediction scores
are extremely peaky. we also experimented with
the different positions that a pun counterpart can
be forced to appear in. based on qualitative ex-
amples, we found that the model generated witty
descriptions that were somewhat sensible when a
pun word appeared at any of the    rst or last 5
positions of a sentence. we also experimented
with a number of different methods to re-rank the
candidate of witty captions, e.g., language model
score (jozefowicz et al., 2016), image-sentence
similarity score (kiros et al., 2014), semantic sim-
ilarity (using id97 (mikolov et al., 2013)) of
the pun counterpart to the sentence, a priori prob-
ability of the pun counterpart in a large corpus of
english sentences to avoid rare / unfamiliar words,
likelihood of the tag (under the image captioning
model or the classi   er as applicable). etc. we
qualitatively found that re-ranking using log. prob.
score of the image captioning model, while being
the simplest, resulted in the best set of candidate
witty captions.

c pun list
recall that we construct a list of puns by min-
ing the web and based on automatic methods that
measure the similarity of pronunciation of words.
upon inspecting our list of puns, we observe that it
contains puns of many frequently used words and
some pun words that are rarely used in everyday
language, e.g.,    wight    (which is the counterpart of
   white   ). since a rare pun word can be distracting

to a perceiver, the corresponding caption might be
harder to resolve, making it less likely to be per-
ceived as witty. thus, we see limited bene   t in in-
creasing the size of our pun list further to include
words that are used even less frequently.
d qualitative analysis of retrieved

descriptions

the retrieved witty descriptions are retrieved from
story-based corpora. they often contain sentences
that describe a very speci   c situation or instance.
although these sentences are grounded in objects
that are also present in the image, the entire sen-
tence often contains a few words that are irrelevant
for a given image, as we see in fig. 6b, fig. 6d
and fig. 6e. this is a likely reason for why a re-
trieved sentence containing a pun is perceived as
less witty when compared with witty descriptions
generated for the image.
e interface for    be witty!   
we ask people on amazon mechanical turk
(amt) to create witty descriptions for the given
image. we also ask them to utilize one of the given
pun words associated with the image. we show
them a few good and bad examples to illustrate the
task better. fig. 7 shows the interface that we used
to collect these human-written witty descriptions
for an image.
f interface for    which is wittier?   
we showed people on amt two descriptions for
a given image and asked them to click on the de-
scription that was wittier for the image. the web
interface that we used to collect this data is shown
in fig. 8.

references
rafal jozefowicz, oriol vinyals, mike schuster, noam
exploring
arxiv preprint

shazeer, and yonghui wu. 2016.
the limits of id38.
arxiv:1602.02410 .

ryan kiros, ruslan salakhutdinov, and richard s
zemel. 2014. unifying visual-semantic embeddings
arxiv
with multimodal neural language models.
preprint arxiv:1411.2539 .

tomas mikolov, ilya sutskever, kai chen, greg s cor-
rado, and jeff dean. 2013. distributed representa-
tions of words and phrases and their compositional-
in advances in neural information processing
ity.
systems.

(a) generated: a read (red) stop sign on a
street corner.
retrieved: the sign read (red) :
human: i sighed (side) when we got off
the main street.

(b) generated: a bored (board) bench sits
in front of a window.
retrieved: wedge sits on the bench oppo-
site berry, bored (board).
human:
pleas (please)!

could you please make your

(c) generated: a man wearing a loop
(loupe) and a bow tie.
retrieved: meanwhile the man   s head
turned back to beau (bow) with his eyes
wide.
human: he could be my beau (bow) with
that style.

(d) generated: a loop (loupe) of    owers
in a glass vase.
retrieved:
teemed with worms.
human: piece required for peace (piece).

the    our

(   ower)

inside

(e) generated: a female tennis player
caught (court) in mid swing.
retrieved: my shirt caught (court)    re.
human: the woman   s hand caught (court)
in the center.

(f) generated: broccoli and meet (meat)
on a plate with a fork.
retrieved:    you mean white folk (fork)   .
human: the folk (fork) enjoyed the food
with a fork.

figure 6: sample images and witty descriptions from our generation model, retrieval model and a hu-
man. the puns (counterparts) that are used in captions (a) to (f) are read/sighed, bored, loop/beau,
loop/   our/peace, caught and meet/folk respectively. the word in the parenthesis following each coun-
terpart is the pun associated with the image. it is provided as a reference to the source of the unexpected
pun which is used in the caption.

figure 7: amt web interface for the    be witty!    task.

good examples        witty sentence with a pun:witty sentence with a pun:witty sentence with a pun:emotional wedding where the cake is in tiers.                  a woman at a dine and whine.a cat is pressing pause on the phone.bad examples                          a bridesmaid is in tiers at a wedding.                    she will always whine after wine.sleepy cat said, "dance to the music without pause".[pun word should make sense! this caption makes[no personal viewpoints.[shouldn't be what a character in the picture might say!]sense for the *original* word but not for the pun.]no    rst person accounts!]        keyboard shortcutsprevious    left  arrownext    right  arrow>hi, my name is (f)punky. i am an arti   cial intelligence (ai). i am learning how to be witty. please write a caption about this image thatcontains a pun.  task: write a witty sentence about the image containing one of the puns listed beside the image.please see a few good examples (green font) and bad examples (red font) below.hide examplesif you don't follow these instructions, your work will be rejected.task    1/5previousnextlist of puns: waul (wall), wight (white), stile (style), poll (pole), sine (sign) write a caption about this image using waul, wight, stile, poll, or sine. remember: the caption should be relevant to the image, and the sentence should make sense for: waul,wight, stile, poll, or sine.   witty sentence here ...figure 8: amt web interface for the    which is wittier?    task.

hi, my name is (f)punky. i am an arti   cial intelligence (ai). i'm trying to learn to be witty by using puns while describing images. i'm not very good yet, and i'd like tolearn so i can slowly get better.  please tell me which of the following two captions are wittier for this image. to give you a sense for what pun iwas going for -- i'll also show you in parenthesis what i saw in the image which i then made a pun around.   even if both captions seem not all that witty, please indicate the one that seems (ever so slightly) wittier.   i will bene   t from this positive feedback! thanks :)task    5/15which of the two captions for the image is wittier?previousnextcaught (court) a tennis player hitting the ball . the teddy bear were bare (bear)  keyboard shortcutstop captionctrl+jbottomcaptionctrl+kpreviousctrl+dnextctrl+h