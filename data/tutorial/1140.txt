id31 of social media texts 
saif m. mohammad and xiaodan zhu 
{saif.mohammad,xiaodan.zhu}@nrc-cnrc.gc.ca  
national research council canada 

 
acknowledgment: thanks to svetlana kiritchenko for helpful suggestions. 
a tutorial presented at the 2014 conference on empirical methods on natural language processing, october 
2014, doha, qatar. 

introduction 

2 

id31 
        

is a given piece of text positive, negative, or neutral?"
       the text may be a sentence, a tweet, an sms message, a 

customer review, a document, and so on."

"

3 

id31 
        

is a given piece of text positive, negative, or neutral?"
       the text may be a sentence, a tweet, an sms message, a 

customer review, a document, and so on."

emotion analysis 
"
         what emotion is being expressed in a given piece of text?"

       basic emotions: joy, sadness, fear, anger,   "
       other emotions: guilt, pride, optimism, frustration,   "

"

4 

id31 
        

is a given piece of text positive, negative, or neutral?"
       the text may be a sentence, a tweet, an sms message, a 

customer review, a document, and so on."

emotion analysis 
"
         what emotion is being expressed in a given piece of text?"

not in the scope of this tutorial. 

       basic emotions: joy, sadness, fear, anger,   "
       other emotions: guilt, pride, optimism, frustration,   "

"

5 

id31: tasks 
        

is a given piece of text positive, negative, or neutral?"
       the text may be a sentence, a tweet, an sms message, a 

customer review, a document, and so on."

6 

id31: tasks 
        

is a given piece of text positive, negative, or neutral?"
       the text may be a sentence, a tweet, an sms message, a 

customer review, a document, and so on."

        

is a word within a sentence positive, negative, or neutral? "
       unpredictable movie plot vs. unpredictable steering "

         what is the sentiment towards speci   c aspects of a product?"

       sentiment towards the food and sentiment towards the 

service in a customer review of a restaurant"

         what is the sentiment towards an entity such as a politician, 

government policy, company, or product?"
       stance detection: favorable or unfavorable "
       framing: focusing on speci   c dimensions"

"

7 

id31: tasks (continued) 
         what is the sentiment of the speaker/writer?"

       is the speaker explicitly expressing sentiment?"
         what sentiment is evoked in the listener/reader?"
         what is the sentiment of an entity mentioned in the text?"
"
consider the above questions with the examples below:"

general tapioca was ruthlessly executed today.
mass-murdered general tapioca    nally killed in battle. 
general tapioca was killed in an explosion.
may god help the people being persecuted by general tapioca. 


"

8 

id31: domains 

         newspaper texts 
       financial news 
       entertainment news 

         legal texts 
         novels 
         e-mails 
         sms messages 
         customer reviews 
         blog posts 
         tweets 
         facebook posts 
            and so on. 

9 

id31: domains 

         newspaper texts 
       financial news 
       entertainment news 

         legal texts 
         novels 
         e-mails 
         sms messages 
         customer reviews 
         blog posts 
         tweets 
         facebook posts 
            and so on. 

short informal pieces of text     often 
called social media texts. 

10 

quirks of social media texts 
        
         short 

informal 

       140 characters for tweets and sms messages 

         abbreviations and shortenings 
         wide array of topics and large vocabulary 
         spelling mistakes and creative spellings 
         special strings 

       hashtags, emoticons, conjoined words 

         high volume 

       500 million tweets posted every day 

         often come with meta-information 

       date, links, likes, location 

         often express sentiment 
 

11 

example applications of id31 
and emotion detection 

improving customer relation models"
identifying what evokes strong emotions in people"

         tracking sentiment towards politicians, movies, products"
        
        
         detecting happiness and well-being"
         measuring the impact of activist movements through text 

generated in social media."
improving automatic dialogue systems"
improving automatic tutoring systems"

        
        
         detecting how people use emotion-bearing-words and 

metaphors to persuade and coerce others"

12 

not in the scope of this tutorial 

         sentiment in formal writing such as news, academic 

publications, etc.  

         application-specific analysis 

       for example, for predicting stocks, election results, public 

health, machine translation, etc.    

         id31 in resource-poor languages 

       porting sentiment resources from one language to another 

         detecting sentiment of reader 
         stance detection  
         visualizations of sentiment 
         emotion analysis 

13 

semeval sentiment tasks 
       semeval-2013 task 2 
       semeval-2014 task 9 
       semeval-2014 task 4  

14 

semeval-2013, task 2: id31 in twitter  

       is a given message positive, negative, or neutral? 
       is a given term within a message positive, negative, or 

         tasks 

neutral? 

         data 

       test set 

         tweets  
         sms messages  

       training set 

         tweets 

       sources of data 

         tweets taken from twitter api 
         search was for certain named entities 
         tweets had to have some words from sentiid138 

15 

examples: message-level sentiment 

tweet: the new star trek movie is visually spectacular.   

 

tweet: the new star trek movie does not have much of  a story. 

 
 

tweet: spock is from planet vulcan.  
 

16 

 
 
examples: message-level sentiment 

tweet: the new star trek movie is visually spectacular.   

tweet: the new star trek movie does not have much of  a story. 

positive  

negative  

 

 
 

tweet: spock is from planet vulcan.  
 

neutral  

17 

 
 
examples: message-level sentiment 

tweet: the new star trek movie is visually spectacular.   

tweet: the new star trek movie does not have much of  a story. 

positive  

negative  

 

 
 

neutral  

tweet: spock is from planet vulcan.  
 
 
 
when creating annotated data:  
       using labels indeterminate and both positive and negative may be 

helpful. 

18 

 
 
examples: term-level sentiment 

tweet: the new star trek does not have much of  a story, but it is visually 
spectacular.   
        target 

 

tweet: the movie was so slow it felt like a documentary. 

 

 

tweet: spock is watching a documentary. 
 

       target 

         target 

19 

 
 
examples: term-level sentiment 

tweet: the new star trek does not have much of  a story, but it is visually 
spectacular.   
target is positive  

 

tweet: the movie was so slow it felt like a documentary. 
target is negative  

 

 

tweet: spock is watching a documentary. 
target is neutral  
 

20 

 
 
evaluation metric 

         macro-averaged f-score: 
 
 

where fpos and fneg are the f-scores of the positive and 
negative sentiment classes, respectively; i.e., 

 
so, the two classes are given the same weight in evaluation. 

21 

semeval-2014, task 9: id31 in twitter  
(repeat of 2013 task) 
         tasks 

       is a given message positive, negative, or neutral? 
       is a given term within a message positive, negative, or 

neutral? 

         2014: tweets set, sarcastic tweets set, blog posts set 
         2013: tweets set, sms set  

         data 

       test set 

       training set 

         2013 tweets set 

       sources of data 

         tweets taken from twitter api 
         search was for certain named entities 
         tweet had to have some words from sentiid138 

22 

semeval-2014, task 4: aspect based id31 

         tasks 

       in a restaurant or laptop review, identify: 

         aspect terms 
         aspect categories 
         sentiment towards aspect terms 
         sentiment towards aspect categories 

 

 

the lasagna was great, but we had to wait 20 minutes to be seated.
aspect terms: lasagna (positive)
aspect categories: food (positive), service (negative)

 

restaurant domain had five pre-defined aspect categories: 
        
"

food, service, price, ambience, anecdotes/miscellaneous

23 

 
semeval-2014, task 4: aspect based id31 

         data 

       test set 

         restaurant reviews 
         laptop reviews 

       training set 

         restaurant reviews 
         laptop reviews 

       source of data 

york 

         restaurant data: customer reviews from citysearch new 

         laptop data: customer reviews from amazon.com 

24 

 
nrc-canada ranks in semeval-2013, task 2  

         message-level task (44 teams) 

       tweets set: 1st 
       sms set: 1st 

         term-level task (23 teams) 

       tweets set: 1st 
       sms set: 2nd 

25 

id31 competition 
classify tweets 

f-score 

0.8 

0.7 

0.6 

0.5 

0.4 

0.3 

0.2 

0.1 

0 

teams 

26 

id31 competition 
classify sms 

f-score 

0.8 

0.7 

0.6 

0.5 

0.4 

0.3 

0.2 

0.1 

0 

teams 

27 

nrc-canada ranks in semeval-2013, task 2  

         message-level task (44 teams) 

         term-level task (23 teams) 

       tweets set: 1st 
       sms set: 1st 

       tweets set: 1st 
       sms set: 2nd 

released description of features. 
released resources created (tweet-specific sentiment 
lexicons). 
www.purl.com/net/sentimentoftweets 
 
nrc-canada: building the state-of-the-art in id31 of 
tweets, saif m. mohammad, svetlana kiritchenko, and xiaodan zhu, in 
proceedings of the seventh international workshop on semantic evaluation 
exercises (semeval-2013), june 2013, atlanta, usa."

28 

2

2

29 

2

2

submissions: 30+ 

30 

semeval-2015, sentiment tasks 

         task 12: aspect based id31 

       repeat of 2014 task 
       new subtask on id20 

         task 11: id31 of figurative language in twitter 

       metaphoric and ironic tweets 
       intensity of sentiment 

         task 10: id31 in twitter 

       repeat of 2013 and 2014 task 
       more subtasks 

         task 9: clipeval implicit polarity of events 

       identify polarity and event class 
 

31 

 
id31 features 

32 

id31 features 
features!
sentiment lexicon!

examples!
#positive: 3, scorepositive: 2.2; maxpositive: 1.3; last: 
0.6, scorenegative: 0.8, scorepositive_neg: 0.4
spectacular, like documentary
spect, docu, visua
#n: 5, #v: 2, #a:1
#neg: 1; ngram:perfect     ngram:perfect_neg, 
polarity:positive     polarity:positive_neg
probably, de   nitely, def
yes, cool
#!+: 1, #?+: 0, #!?+: 0
:d, >:(
soooo, yaayyy

word id165s!
char id165s!
part of speech!
negation!

word clusters!
all-caps!
punctuation!
emoticons!
elongated words!

33 

feature contributions (on tweets) 

f-scores!

0.7"

0.6"

0.5"

0.4"

0.3"

34 

id165s 

         word ngrams 

         w1 * w3 
       features:  

       unigrams, bigrams, trigrams, fourgrams  
       skip ngrams 

         whether ngram present or not 

 
         character ngrams 

       3-gram, 4-gram, 5-gram  
       features:  

         whether ngram present or not 

35 

sentiment lexicons 

lists of positive and negative words. 

 spectacular positive 0.91 
 okay positive 0.3 
 lousy negative 0.84 
 unpredictable negative 0.17 

positive
spectacular  
okay  

negative
lousy  
unpredictable  

 

36 

sentiment lexicons: manually created 

         general inquirer (stone, dunphy, smith, ogilvie, & associates, 1966): ~3,600 words 
         mpqa (wilson, wiebe, & hoffmann, 2005): ~8,000 words 
         hu and liu lexicon (hu and liu, 2004): ~6,800 words  
         nrc emotion lexicon (mohammad & turney, 2010): ~14,000 words and 

~25,000 word senses 
       senses are based on categories in a thesaurus 
       has emotion associations in addition to sentiment 

         afinn (by finn   rup nielsen in 2009-2011): ~2400 words 
         maxdiff sentiment lexicon (kiritchenko, zhu, and mohammad, 2014): about 

1,500 terms  
       has intensity scores 

37 

sentiment lexicons 

lists of positive and negative words. 

 spectacular positive 0.91 
 okay positive 0.3 
 lousy negative 0.84 
 unpredictable negative 0.17 

positive
spectacular  
okay  

negative
lousy  
unpredictable  

 

38 

sentiment lexicons 

lists of positive and negative words, with scores indicating the 
degree of association 

 spectacular positive 0.91 
 okay positive 0.3 
 lousy negative 0.84 
 unpredictable negative 0.17 

positive
spectacular 0.91 
okay 0.30 

negative
lousy -0.84 
unpredictable -0.17 

 

39 

sentiment lexicons 

lists of positive and negative words, with scores indicating the 
degree of association 

 spectacular positive 0.91 
 okay positive 0.3 
 lousy negative 0.84 
 unpredictable negative 0.17 

positive
spectacular 0.91 
okay 0.30 

negative
lousy -0.84 
unpredictable -0.17 

spectacular positive 0.91 
okay positive 0.30 
lousy negative 0.84 
unpredictable negative 0.17 

 

40 

how to create sentiment lexicons with 
intensity values? 
         humans are not good at giving real-valued scores? 
       hard to be consistent across multiple annotations 
       difficult to maintain consistency across annotators 

         0.8 for annotator may be 0.7 for another 

         humans are much better at comparisons 

       questions such as: is one word more positive than 

another? 

       large number of annotations needed. 

need a method that preserves the comparison aspect, without 
greatly increasing the number of annotations needed.  

41 

maxdiff 

         the annotator is presented with four words (say, a, b, c, and 

d) and asked:  
       which word is the most positive 
       which is the least positive  
 

         by answering just these two questions, five out of the six 
inequalities are known 

       for e.g.:  

         if a is most positive  
         and d is least positive, then we know: 
             a > b, a > c, a > d, b > d, c > d 
 

maximum difference scaling: improved measures of importance and 
preference for segmentation. cohen, steve. sawtooth software 
conference proceedings, sawtooth software, inc. vol. 530. 2003. 

42 

maxdiff 

         each of these maxdiff questions can be presented to multiple 

annotators.  

         the responses to the maxdiff questions can then be easily 

        

translated into: 
       a ranking of all the terms 
       a real-valued score for all the terms (orme, 2009)  
if two words have very different degrees of association (for 
example, a >> d), then: 
       a will be chosen as most positive much more often than d 
       d will be chosen as least positive much more often than a.  
this will eventually lead to a ranked list such that a and d are 
significantly farther apart, and their real-valued association 
scores will also be significantly different. 

43 

dataset of sentiment scores  
(kiritchenko, zhu, and mohammad, 2014) 

         selected ~1,500 terms from tweets 

       regular english words: peace, jumpy
       tweet-specific terms 

         hashtags and conjoined words: #inspiring, #happytweet, 

#needsleep

         creative spellings: amazzing, goooood

       negated terms: not nice, nothing better, not sad

         generated 3,000 maxdiff questions 
         each question annotated by 10 annotators on crowdflower 
         answers converted to real-valued scores (0 to 1) and to a full 

ranking of terms using the counting procedure (orme, 2009)  

44 

examples of sentiment scores from the 
maxdiff annotations 

term 

awesomeness
#happygirl
cant waitttt
don't worry
not true
cold
#getagrip
#sickening

sentiment score  
0 (most negative) to 1 (most positive) 
0.9133 
0.8125 
0.8000 
0.5750 
0.3871 
0.2750 
0.2063 
0.1389 

45 

robustness of the annotations 

         divided the maxdiff responses into two equal halves 
         generated scores and ranking based on each set individually 
         the two sets produced very similar results: 

       average difference in scores was 0.04 
       spearman   s rank coefficient between the two rankings 

was 0.97 

dataset will be used as test set for subtask e in task 10 of 
semeval-2015: determining prior id203. 
 
trial data already available: 
http://alt.qcri.org/semeval2015/task10/index.php?id=data-and-tools 
(full dataset to be released after the shared task competition in dec., 2014.) 

46 

sentiment lexicons: automatically created 

         turney and littman lexicon (turney and littman, 2003) 
         sentiid138 (esuli & sebastiani, 2006): id138 synsets 
         msol (mohammad, dunne, and dorr, 2009): ~60,000 words 
         hashtag sentiment lexicon (mohammad, kiritchenko, and zhu, 2013): 

~220,000 unigrams and bigrams 

         sentiment140 sentiment lexicon (mohammad, kiritchenko, and zhu, 2013): 

~330,000 unigrams and bigrams 

 

47 

turney and littman (2003) method  

         pointwise mutual information (pmi)  based measure 
         pmi between two words, w1  and w2 (church and hanks 1989): 






pmi(w1,w2) = log2(p(w1 and w2)/p(w1)p(w2))

p(w1 and w2) is id203 of how often w1 and w2 co-occur 
p(w1) is id203 of occurrence of w1
p(w2) is id203 of occurrence of w2 
 
if pmi > 1, then w1 and w2 co-occur more often than chance 
if pmi < 1, then w1 and w2 co-occur less often than chance 

 

 

48 

turney and littman (2003) method (continued)  

         created a list of seed sentiment words: 

       positive seeds (pwords):  good, nice, excellent, positive,  

                                              fortunate, correct, superior
       negative seeds (nwords): bad, nasty, poor, negative,  

                                               unfortunate, wrong, inferior

         polled the altavista advanced search engine for number of 

documents that had both a target word and a seed word 
within a small window 
       positive seed is assumed to be positive label for co-

       negative seed is assumed to be negative label for co-

occurring word w

occurring word w

 

49 

turney and littman (2003) method (continued)  

         for every word w a sentiment association score is generated: 

 score(w) = pmi(w,positive)     pmi(w,negative)

pmi = pointwise mutual information 

 pmi(w,positive) =                   pmi(w,pword)


if score(w) > 0, then word w is positive 
if score(w) < 0, then word w is negative 
 

50 

hashtagged tweets 

         hashtagged words are good labels of sentiments and 

emotions 

 

can   t wait to have my own google glasses #awesome  
 some jerk just stole my photo on #tumblr. #grr #anger 

 

         hashtags are not always good labels: 

       hashtag used sarcastically 

 the reviewers want me to re-annotate the data. #joy 

       hashtagged emotion not in the rest of the message 

 mika used my photo on tumblr. #anger 

 

#emotional tweets, saif mohammad, in proceedings of the first joint conference on 
lexical and computational semantics (*sem), june 2012, montreal, canada.  

51 

mohammad, kiritchenko, and zhu method 

         created a list of seed sentiment words by looking up 

synonyms of excellent, good, bad, and terrible: 
       30 positive words 
       46 negative words 

         polled the twitter api for tweets with seed-word hashtags 

       a set of 775,000 tweets was compiled from april to 

december 2012 

52 

automatically generated new lexicons 

         sentiment lexicons can be generated from sentiment-labeled 

data 
       emoticons and hashtag words can be used as labels 

 

         for every word w in the set of millions tweets, an association 

score is generated: 

 score(w) = pmi(w,positive)     pmi(w,negative)

pmi = pointwise mutual information 
if score(w) > 0, then word w is positive 
if score(w) < 0, then word w is negative 
 

53 

pmi method based lexicons 

         hashtag sentiment lexicon 

       created from a large collection of hashtagged tweets 
       has entries for ~215,000 unigrams and bigrams 

         sentiment140 lexicon 

       created from a large collection of tweets with emoticons 

         sentiment140 corpus (alec go, richa bhayani, and lei huang, 2009) 
    http://help.sentiment140.com/for-students/ 

       has entries for ~330,000 unigrams and bigrams 

54 

features of the twitter lexicon 

         connotation and not necessarily denotation 

        

       tears, party, vacation 
large vocabulary 
       cover wide variety of topics 
       lots of informal words 
       twitter-specific words 

         creative spellings, hashtags, conjoined words 

         seed hashtags have varying effectiveness 

       study on sentiment predictability of different hashtags 

(kunneman, f.a., liebrecht, c.c., van den bosch, a.p.j., 2014) 

55 

negation 

         a grammatical category that allows the changing of the truth 

value of a proposition (morante and sporleder, 2012)  

         often expressed through the use of negative signals or 

negators 
       words like isn   t and never 

         can significantly affect the sentiment of its scope  
         examples: 

 people do not like change. 
 jack never hated the plan, he just has other priorities. 

 

    the negator is shown in blue. 
    the scope is shown by underline. 

56 

conventional methods to handle negation 

reversing hypothesis: 

 s(n,w) = -s(w) 

where, s(w) is the sentiment of word (or phrase) w, 
       s(n,w) is the sentiment of the expression formed by the 
concatenation of the negator n and word w. 

         for example, if s(honest) = 0.9, then s(not, honest) = -0.9 

but how good is this hypothesis? 
what about: 

 the movie is not terrible.

if s(terrible) = -0.9. what is the appropriate value for s(not, 
terrible) in this context? 
 

57 

negation 

an empirical study on the effect of negation words on sentiment. xiaodan zhu, hongyu guo, 
saif mohammad and svetlana kiritchenko. in proceedings of the 52nd annual meeting of the 
association for computational linguistics, june 2014, baltimore, md. 

58 

negation 

jack was not thrilled at the prospect of  working weekends l    
 
 
 
 
 
 
the bill is not garbage, but we need a more focused effort l     

59 

negation 

negator 

jack was not thrilled at the prospect of  working weekends l    
 
 
 
 
 
 
the bill is not garbage, but we need a more focused effort l     

need to determine this word   s 
sentiment when negated  

sentiment 

label: negative 

negator 

sentiment 

label: negative 

need to determine this word   s 
sentiment when negated  

60 

handling negation 

in the list of 
negators 

jack was not thrilled at the prospect of  working weekends l    
 
 
 
 
 
 
the bill is not garbage, but we need a more focused effort l     

scope of negation 

sentiment 

label: negative 

in the list of 
negators 

scope of negation 

sentiment 

label: negative 

scope of negation: from negator till a punctuation (or end of sentence) 

61 

negative label 

positive label 

tweets or sentences 

62 

affirmative contexts 
(in light grey) 

negated contexts  
(in dark grey) 

63 

all the affirmative contexts 

all the negated contexts 

generate sentiment lexicon for 
words in affirmative context 

generate sentiment lexicon for 
words in negated context 

64 

improving customer relation models!
identifying what evokes strong emotions in people!

applications of id31 and 
emotion detection 
applications of id31 and 
!    tracking sentiment towards politicians, movies, products!
emotion detection 
!    detecting happiness and well-being!
!    tracking sentiment towards politicians, movies, products!
!    measuring the impact of activist movements through text 
improving customer relation models!
generated in social media.!
improving automatic dialogue systems!
identifying what evokes strong emotions in people!
improving automatic tutoring systems!

applications of id31 and 
emotion detection 
!    detecting happiness and well-being!
applications of id31 and 
!    detecting how people use emotion-bearing-words and 
!    measuring the impact of activist movements through text 
!    tracking sentiment towards politicians, movies, products!
emotion detection 
!    detecting happiness and well-being!
!    tracking sentiment towards politicians, movies, products!
!    measuring the impact of activist movements through text 
!    detecting how people use emotion-bearing-words and 

metaphors to persuade and coerce others!
generated in social media.!
improving customer relation models!
improving automatic dialogue systems!
identifying what evokes strong emotions in people!
improving automatic tutoring systems!
generated in social media.!
improving customer relation models!
metaphors to persuade and coerce others!
improving automatic dialogue systems!
identifying what evokes strong emotions in people!
improving automatic tutoring systems!
metaphors to persuade and coerce others!

!    detecting how people use emotion-bearing-words and 
!    detecting happiness and well-being!
!    measuring the impact of activist movements through text 

generated in social media.!
improving automatic dialogue systems!
improving automatic tutoring systems!

!    detecting how people use emotion-bearing-words and 

metaphors to persuade and coerce others!

8 

8 

8 

65 

(cid:7)(cid:131)(cid:150)(cid:131)(cid:3)(cid:25)(cid:139)(cid:149)(cid:151)(cid:131)(cid:142)(cid:139)(cid:156)(cid:131)(cid:150)(cid:139)(cid:145)(cid:144)(cid:3)(cid:145)(cid:136)(cid:3)(cid:22)(cid:135)(cid:144)(cid:150)(cid:139)(cid:143)(cid:135)(cid:144)(cid:150)(cid:149)(cid:3)(cid:131)(cid:144)(cid:134)(cid:3)
(cid:8)(cid:143)(cid:145)(cid:150)(cid:139)(cid:145)(cid:144)(cid:149)(cid:3)(cid:139)(cid:144)(cid:3)(cid:17)(cid:145)(cid:152)(cid:135)(cid:142)(cid:149)(cid:3)(cid:131)(cid:144)(cid:134)(cid:3)(cid:23)(cid:153)(cid:135)(cid:135)(cid:150)(cid:149)

table 4: the number of positive and negative entries in the sentiment lexicons.

lexicon
nrc emotion lexicon
bing liu   s lexicon
mpqa subjectivity lexicon

hashtag sentiment lexicons (hs)

hs base lexicon

(cid:4)(cid:410)(cid:3)(cid:100)(cid:346)(cid:286)(cid:3)(cid:69)(cid:286)(cid:449)(cid:3)(cid:122)(cid:381)(cid:396)(cid:364)(cid:3)(cid:100)(cid:349)(cid:373)(cid:286)(cid:400)(cid:853)(cid:3)(cid:449)(cid:286)(cid:3)(cid:400)(cid:410)(cid:396)(cid:381)(cid:374)(cid:336)(cid:367)(cid:455)(cid:3)(cid:271)(cid:286)(cid:367)(cid:349)(cid:286)(cid:448)(cid:286)(cid:3)(cid:410)(cid:346)(cid:258)(cid:410)(cid:3)(cid:448)(cid:349)(cid:400)(cid:437)(cid:258)(cid:367)(cid:349)(cid:460)(cid:258)(cid:410)(cid:349)(cid:381)(cid:374)(cid:3)(cid:349)(cid:400)(cid:3)(cid:396)(cid:286)(cid:393)(cid:381)(cid:396)(cid:410)(cid:349)(cid:374)(cid:336)(cid:853)(cid:3)(cid:449)(cid:349)(cid:410)(cid:346)(cid:3)(cid:373)(cid:258)(cid:374)(cid:455)(cid:3)(cid:381)(cid:296)(cid:3)(cid:410)(cid:346)(cid:286)(cid:3)(cid:400)(cid:258)(cid:373)(cid:286)(cid:3)
(cid:286)(cid:367)(cid:286)(cid:373)(cid:286)(cid:374)(cid:410)(cid:400)(cid:3)(cid:410)(cid:346)(cid:258)(cid:410)(cid:3)(cid:449)(cid:381)(cid:437)(cid:367)(cid:282)(cid:3)(cid:373)(cid:258)(cid:364)(cid:286)(cid:3)(cid:258)(cid:3)(cid:410)(cid:396)(cid:258)(cid:282)(cid:349)(cid:410)(cid:349)(cid:381)(cid:374)(cid:258)(cid:367)(cid:3)(cid:400)(cid:410)(cid:381)(cid:396)(cid:455)(cid:3)(cid:286)(cid:296)(cid:296)(cid:286)(cid:272)(cid:410)(cid:349)(cid:448)(cid:286)(cid:855)(cid:3)(cid:258)(cid:3)(cid:374)(cid:258)(cid:396)(cid:396)(cid:258)(cid:410)(cid:349)(cid:448)(cid:286)(cid:3)(cid:410)(cid:346)(cid:258)(cid:410)(cid:3)(cid:393)(cid:258)(cid:396)(cid:286)(cid:400)(cid:3)(cid:258)(cid:449)(cid:258)(cid:455)(cid:3)(cid:286)(cid:454)(cid:410)(cid:396)(cid:258)(cid:374)(cid:286)(cid:381)(cid:437)(cid:400)(cid:3)
(cid:349)(cid:374)(cid:296)(cid:381)(cid:396)(cid:373)(cid:258)(cid:410)(cid:349)(cid:381)(cid:374)(cid:3)(cid:410)(cid:381)(cid:3)(cid:296)(cid:349)(cid:374)(cid:282)(cid:3)(cid:258)(cid:3)(cid:400)(cid:410)(cid:381)(cid:396)(cid:455)(cid:3)(cid:349)(cid:374)(cid:3)(cid:410)(cid:346)(cid:286)(cid:3)(cid:282)(cid:258)(cid:410)(cid:258)(cid:854)(cid:3)(cid:272)(cid:381)(cid:374)(cid:410)(cid:286)(cid:454)(cid:410)(cid:3)(cid:410)(cid:381)(cid:3)(cid:346)(cid:286)(cid:367)(cid:393)(cid:3)(cid:410)(cid:346)(cid:286)(cid:3)(cid:396)(cid:286)(cid:258)(cid:282)(cid:286)(cid:396)(cid:3)(cid:437)(cid:374)(cid:282)(cid:286)(cid:396)(cid:400)(cid:410)(cid:258)(cid:374)(cid:282)(cid:3)(cid:410)(cid:346)(cid:286)(cid:3)(cid:271)(cid:258)(cid:400)(cid:349)(cid:272)(cid:400)(cid:3)(cid:381)(cid:296)(cid:3)(cid:410)(cid:346)(cid:286)(cid:3)
(cid:400)(cid:437)(cid:271)(cid:361)(cid:286)(cid:272)(cid:410)(cid:854)(cid:3)(cid:349)(cid:374)(cid:410)(cid:286)(cid:396)(cid:448)(cid:349)(cid:286)(cid:449)(cid:349)(cid:374)(cid:336)(cid:3)(cid:410)(cid:346)(cid:286)(cid:3)(cid:282)(cid:258)(cid:410)(cid:258)(cid:3)(cid:410)(cid:381)(cid:3)(cid:296)(cid:349)(cid:374)(cid:282)(cid:3)(cid:349)(cid:410)(cid:400)(cid:3)(cid:296)(cid:367)(cid:258)(cid:449)(cid:400)(cid:3)(cid:258)(cid:374)(cid:282)(cid:3)(cid:271)(cid:286)(cid:3)(cid:400)(cid:437)(cid:396)(cid:286)(cid:3)(cid:381)(cid:296)(cid:3)(cid:381)(cid:437)(cid:396)(cid:3)(cid:272)(cid:381)(cid:374)(cid:272)(cid:367)(cid:437)(cid:400)(cid:349)(cid:381)(cid:374)(cid:400)(cid:856)(cid:3)(cid:87)(cid:396)(cid:286)(cid:410)(cid:410)(cid:349)(cid:374)(cid:286)(cid:400)(cid:400)(cid:3)(cid:349)(cid:400)(cid:3)(cid:258)(cid:3)(cid:271)(cid:381)(cid:374)(cid:437)(cid:400)(cid:854)(cid:3)
(cid:349)(cid:296)(cid:3)(cid:349)(cid:410)(cid:3)(cid:381)(cid:271)(cid:367)(cid:349)(cid:410)(cid:286)(cid:396)(cid:258)(cid:410)(cid:286)(cid:400)(cid:3)(cid:410)(cid:346)(cid:286)(cid:3)(cid:258)(cid:271)(cid:349)(cid:367)(cid:349)(cid:410)(cid:455)(cid:3)(cid:410)(cid:381)(cid:3)(cid:396)(cid:286)(cid:258)(cid:282)(cid:3)(cid:410)(cid:346)(cid:286)(cid:3)(cid:400)(cid:410)(cid:381)(cid:396)(cid:455)(cid:3)(cid:381)(cid:296)(cid:3)(cid:410)(cid:346)(cid:286)(cid:3)(cid:448)(cid:349)(cid:400)(cid:437)(cid:258)(cid:367)(cid:349)(cid:460)(cid:258)(cid:410)(cid:349)(cid:381)(cid:374)(cid:853)(cid:3)(cid:349)(cid:410)(cid:859)(cid:400)(cid:3)(cid:374)(cid:381)(cid:410)(cid:3)(cid:449)(cid:381)(cid:396)(cid:410)(cid:346)(cid:3)(cid:258)(cid:282)(cid:282)(cid:349)(cid:374)(cid:336)(cid:3)(cid:400)(cid:381)(cid:373)(cid:286)(cid:3)(cid:449)(cid:349)(cid:367)(cid:282)(cid:3)(cid:374)(cid:286)(cid:449)(cid:3)

- unigrams
- bigrams
hs a   lex

(cid:7)(cid:131)(cid:150)(cid:131)(cid:3)(cid:25)(cid:139)(cid:149)(cid:151)(cid:131)(cid:142)(cid:139)(cid:156)(cid:131)(cid:150)(cid:139)(cid:145)(cid:144)(cid:3)(cid:145)(cid:136)(cid:3)(cid:22)(cid:135)(cid:144)(cid:150)(cid:139)(cid:143)(cid:135)(cid:144)(cid:150)(cid:149)(cid:3)(cid:131)(cid:144)(cid:134)(cid:3)
(cid:7)(cid:131)(cid:150)(cid:131)(cid:3)(cid:25)(cid:139)(cid:149)(cid:151)(cid:131)(cid:142)(cid:139)(cid:156)(cid:131)(cid:150)(cid:139)(cid:145)(cid:144)(cid:3)(cid:145)(cid:136)(cid:3)(cid:22)(cid:135)(cid:144)(cid:150)(cid:139)(cid:143)(cid:135)(cid:144)(cid:150)(cid:149)(cid:3)(cid:131)(cid:144)(cid:134)(cid:3)
(cid:8)(cid:143)(cid:145)(cid:150)(cid:139)(cid:145)(cid:144)(cid:149)(cid:3)(cid:139)(cid:144)(cid:3)(cid:17)(cid:145)(cid:152)(cid:135)(cid:142)(cid:149)(cid:3)(cid:131)(cid:144)(cid:134)(cid:3)(cid:23)(cid:153)(cid:135)(cid:135)(cid:150)(cid:149)
(cid:8)(cid:143)(cid:145)(cid:150)(cid:139)(cid:145)(cid:144)(cid:149)(cid:3)(cid:139)(cid:144)(cid:3)(cid:17)(cid:145)(cid:152)(cid:135)(cid:142)(cid:149)(cid:3)(cid:131)(cid:144)(cid:134)(cid:3)(cid:23)(cid:153)(cid:135)(cid:135)(cid:150)(cid:149)

- unigrams
- bigrams
hs neglex
- unigrams
- bigrams

sentiment140 lexicons (s140)

s140 base lexicon

(cid:894)(cid:400)(cid:381)(cid:437)(cid:396)(cid:272)(cid:286)(cid:855)(cid:3)(cid:346)(cid:410)(cid:410)(cid:393)(cid:855)(cid:876)(cid:876)(cid:449)(cid:449)(cid:449)(cid:856)(cid:374)(cid:349)(cid:286)(cid:373)(cid:258)(cid:374)(cid:367)(cid:258)(cid:271)(cid:856)(cid:381)(cid:396)(cid:336)(cid:876)(cid:1006)(cid:1004)(cid:1005)(cid:1005)(cid:876)(cid:1005)(cid:1004)(cid:876)(cid:449)(cid:381)(cid:396)(cid:282)(cid:882)(cid:272)(cid:367)(cid:381)(cid:437)(cid:282)(cid:400)(cid:882)(cid:272)(cid:381)(cid:374)(cid:400)(cid:349)(cid:282)(cid:286)(cid:396)(cid:286)(cid:282)(cid:882)(cid:346)(cid:258)(cid:396)(cid:373)(cid:296)(cid:437)(cid:367)(cid:876)(cid:895)

(cid:100)(cid:346)(cid:349)(cid:400)(cid:3)(cid:349)(cid:400)(cid:3)(cid:258)(cid:3)(cid:393)(cid:396)(cid:381)(cid:393)(cid:381)(cid:400)(cid:258)(cid:367)(cid:3)(cid:296)(cid:381)(cid:396)(cid:3)(cid:448)(cid:349)(cid:400)(cid:437)(cid:258)(cid:367)(cid:349)(cid:460)(cid:258)(cid:410)(cid:349)(cid:381)(cid:374)(cid:3)(cid:381)(cid:296)(cid:3)(cid:286)(cid:373)(cid:381)(cid:410)(cid:349)(cid:381)(cid:374)(cid:400)(cid:3)(cid:258)(cid:374)(cid:282)(cid:3)(cid:400)(cid:286)(cid:374)(cid:410)(cid:349)(cid:373)(cid:286)(cid:374)(cid:410)(cid:400)(cid:3)(cid:349)(cid:374)(cid:3)(cid:410)(cid:286)(cid:454)(cid:410)(cid:856)(cid:3)(cid:47)(cid:410)(cid:3)(cid:393)(cid:396)(cid:286)(cid:400)(cid:437)(cid:373)(cid:286)(cid:400)(cid:3)(cid:410)(cid:346)(cid:258)(cid:410)(cid:3)(cid:410)(cid:346)(cid:286)(cid:3)(cid:286)(cid:373)(cid:381)(cid:410)(cid:349)(cid:381)(cid:374)(cid:3)(cid:258)(cid:374)(cid:282)(cid:3)
(cid:400)(cid:286)(cid:374)(cid:410)(cid:349)(cid:373)(cid:286)(cid:374)(cid:410)(cid:3)(cid:258)(cid:374)(cid:258)(cid:367)(cid:455)(cid:400)(cid:349)(cid:400)(cid:3)(cid:346)(cid:258)(cid:282)(cid:3)(cid:258)(cid:367)(cid:396)(cid:286)(cid:258)(cid:282)(cid:455)(cid:3)(cid:271)(cid:286)(cid:286)(cid:374)(cid:3)(cid:282)(cid:381)(cid:374)(cid:286)(cid:853)(cid:3)(cid:258)(cid:374)(cid:282)(cid:3)(cid:410)(cid:346)(cid:286)(cid:3)(cid:258)(cid:367)(cid:367)(cid:3)(cid:410)(cid:346)(cid:286)(cid:3)(cid:282)(cid:258)(cid:410)(cid:258)(cid:3)(cid:349)(cid:400)(cid:3)(cid:258)(cid:448)(cid:258)(cid:349)(cid:367)(cid:258)(cid:271)(cid:367)(cid:286)(cid:3)(cid:271)(cid:286)(cid:296)(cid:381)(cid:396)(cid:286)(cid:346)(cid:258)(cid:374)(cid:282)(cid:3)(cid:894)(cid:449)(cid:286)(cid:3)(cid:282)(cid:286)(cid:272)(cid:349)(cid:282)(cid:286)(cid:282)(cid:3)(cid:410)(cid:381)(cid:3)
(cid:400)(cid:286)(cid:393)(cid:258)(cid:396)(cid:258)(cid:410)(cid:286)(cid:3)(cid:448)(cid:349)(cid:400)(cid:437)(cid:258)(cid:367)(cid:349)(cid:460)(cid:258)(cid:410)(cid:349)(cid:381)(cid:374)(cid:3)(cid:258)(cid:400)(cid:393)(cid:286)(cid:272)(cid:410)(cid:3)(cid:296)(cid:396)(cid:381)(cid:373)(cid:3)(cid:282)(cid:258)(cid:410)(cid:258)(cid:3)(cid:286)(cid:454)(cid:410)(cid:396)(cid:258)(cid:272)(cid:410)(cid:349)(cid:381)(cid:374)(cid:3)(cid:258)(cid:400)(cid:393)(cid:286)(cid:272)(cid:410)(cid:3)(cid:258)(cid:400)(cid:3)(cid:258)(cid:361)(cid:258)(cid:454)(cid:3)(cid:272)(cid:258)(cid:367)(cid:367)(cid:400)(cid:3)(cid:258)(cid:396)(cid:286)(cid:3)(cid:272)(cid:381)(cid:400)(cid:410)(cid:367)(cid:455)(cid:3)(cid:258)(cid:374)(cid:282)(cid:3)(cid:410)(cid:346)(cid:286)(cid:3)(cid:258)(cid:374)(cid:258)(cid:367)(cid:455)(cid:400)(cid:349)(cid:400)(cid:3)(cid:349)(cid:400)(cid:3)(cid:400)(cid:410)(cid:349)(cid:367)(cid:367)(cid:3)

(cid:24)(cid:258)(cid:410)(cid:258)(cid:3)(cid:448)(cid:349)(cid:400)(cid:437)(cid:258)(cid:367)(cid:349)(cid:460)(cid:258)(cid:410)(cid:349)(cid:381)(cid:374)(cid:3)(cid:349)(cid:400)(cid:3)(cid:374)(cid:381)(cid:410)(cid:3)(cid:258)(cid:374)(cid:3)(cid:286)(cid:258)(cid:400)(cid:455)(cid:3)(cid:410)(cid:258)(cid:400)(cid:364)(cid:855)(cid:3)
(cid:24)(cid:258)(cid:410)(cid:258)(cid:3)(cid:448)(cid:349)(cid:400)(cid:437)(cid:258)(cid:367)(cid:349)(cid:460)(cid:258)(cid:410)(cid:349)(cid:381)(cid:374)(cid:3)(cid:349)(cid:400)(cid:3)(cid:374)(cid:381)(cid:410)(cid:3)(cid:258)(cid:374)(cid:3)(cid:286)(cid:258)(cid:400)(cid:455)(cid:3)(cid:410)(cid:258)(cid:400)(cid:364)(cid:855)(cid:3)

(cid:38)(cid:381)(cid:396)(cid:3)(cid:410)(cid:346)(cid:349)(cid:400)(cid:3)(cid:286)(cid:454)(cid:286)(cid:396)(cid:272)(cid:349)(cid:400)(cid:286)(cid:3)(cid:449)(cid:286)(cid:3)(cid:449)(cid:349)(cid:367)(cid:367)(cid:3)(cid:271)(cid:286)(cid:3)(cid:296)(cid:381)(cid:272)(cid:437)(cid:400)(cid:349)(cid:374)(cid:336)(cid:3)(cid:381)(cid:374)(cid:3)(cid:374)(cid:381)(cid:448)(cid:286)(cid:367)(cid:400)(cid:3)(cid:258)(cid:374)(cid:282)(cid:3)(cid:410)(cid:449)(cid:286)(cid:286)(cid:410)(cid:400)(cid:853)(cid:3)(cid:271)(cid:437)(cid:410)(cid:3)(cid:258)(cid:374)(cid:455)(cid:3)(cid:393)(cid:349)(cid:286)(cid:272)(cid:286)(cid:3)(cid:381)(cid:296)(cid:3)(cid:410)(cid:286)(cid:454)(cid:410)(cid:3)(cid:272)(cid:258)(cid:374)(cid:3)(cid:271)(cid:286)(cid:3)(cid:258)(cid:374)(cid:258)(cid:367)(cid:455)(cid:460)(cid:286)(cid:282)(cid:3)(cid:296)(cid:381)(cid:396)(cid:3)

(cid:4)(cid:410)(cid:3)(cid:100)(cid:346)(cid:286)(cid:3)(cid:69)(cid:286)(cid:449)(cid:3)(cid:122)(cid:381)(cid:396)(cid:364)(cid:3)(cid:100)(cid:349)(cid:373)(cid:286)(cid:400)(cid:853)(cid:3)(cid:449)(cid:286)(cid:3)(cid:400)(cid:410)(cid:396)(cid:381)(cid:374)(cid:336)(cid:367)(cid:455)(cid:3)(cid:271)(cid:286)(cid:367)(cid:349)(cid:286)(cid:448)(cid:286)(cid:3)(cid:410)(cid:346)(cid:258)(cid:410)(cid:3)(cid:448)(cid:349)(cid:400)(cid:437)(cid:258)(cid:367)(cid:349)(cid:460)(cid:258)(cid:410)(cid:349)(cid:381)(cid:374)(cid:3)(cid:349)(cid:400)(cid:3)(cid:396)(cid:286)(cid:393)(cid:381)(cid:396)(cid:410)(cid:349)(cid:374)(cid:336)(cid:853)(cid:3)(cid:449)(cid:349)(cid:410)(cid:346)(cid:3)(cid:373)(cid:258)(cid:374)(cid:455)(cid:3)(cid:381)(cid:296)(cid:3)(cid:410)(cid:346)(cid:286)(cid:3)(cid:400)(cid:258)(cid:373)(cid:286)(cid:3)
(cid:4)(cid:410)(cid:3)(cid:100)(cid:346)(cid:286)(cid:3)(cid:69)(cid:286)(cid:449)(cid:3)(cid:122)(cid:381)(cid:396)(cid:364)(cid:3)(cid:100)(cid:349)(cid:373)(cid:286)(cid:400)(cid:853)(cid:3)(cid:449)(cid:286)(cid:3)(cid:400)(cid:410)(cid:396)(cid:381)(cid:374)(cid:336)(cid:367)(cid:455)(cid:3)(cid:271)(cid:286)(cid:367)(cid:349)(cid:286)(cid:448)(cid:286)(cid:3)(cid:410)(cid:346)(cid:258)(cid:410)(cid:3)(cid:448)(cid:349)(cid:400)(cid:437)(cid:258)(cid:367)(cid:349)(cid:460)(cid:258)(cid:410)(cid:349)(cid:381)(cid:374)(cid:3)(cid:349)(cid:400)(cid:3)(cid:396)(cid:286)(cid:393)(cid:381)(cid:396)(cid:410)(cid:349)(cid:374)(cid:336)(cid:853)(cid:3)(cid:449)(cid:349)(cid:410)(cid:346)(cid:3)(cid:373)(cid:258)(cid:374)(cid:455)(cid:3)(cid:381)(cid:296)(cid:3)(cid:410)(cid:346)(cid:286)(cid:3)(cid:400)(cid:258)(cid:373)(cid:286)(cid:3)
(cid:286)(cid:367)(cid:286)(cid:373)(cid:286)(cid:374)(cid:410)(cid:400)(cid:3)(cid:410)(cid:346)(cid:258)(cid:410)(cid:3)(cid:449)(cid:381)(cid:437)(cid:367)(cid:282)(cid:3)(cid:373)(cid:258)(cid:364)(cid:286)(cid:3)(cid:258)(cid:3)(cid:410)(cid:396)(cid:258)(cid:282)(cid:349)(cid:410)(cid:349)(cid:381)(cid:374)(cid:258)(cid:367)(cid:3)(cid:400)(cid:410)(cid:381)(cid:396)(cid:455)(cid:3)(cid:286)(cid:296)(cid:296)(cid:286)(cid:272)(cid:410)(cid:349)(cid:448)(cid:286)(cid:855)(cid:3)(cid:258)(cid:3)(cid:374)(cid:258)(cid:396)(cid:396)(cid:258)(cid:410)(cid:349)(cid:448)(cid:286)(cid:3)(cid:410)(cid:346)(cid:258)(cid:410)(cid:3)(cid:393)(cid:258)(cid:396)(cid:286)(cid:400)(cid:3)(cid:258)(cid:449)(cid:258)(cid:455)(cid:3)(cid:286)(cid:454)(cid:410)(cid:396)(cid:258)(cid:374)(cid:286)(cid:381)(cid:437)(cid:400)(cid:3)
(cid:286)(cid:367)(cid:286)(cid:373)(cid:286)(cid:374)(cid:410)(cid:400)(cid:3)(cid:410)(cid:346)(cid:258)(cid:410)(cid:3)(cid:449)(cid:381)(cid:437)(cid:367)(cid:282)(cid:3)(cid:373)(cid:258)(cid:364)(cid:286)(cid:3)(cid:258)(cid:3)(cid:410)(cid:396)(cid:258)(cid:282)(cid:349)(cid:410)(cid:349)(cid:381)(cid:374)(cid:258)(cid:367)(cid:3)(cid:400)(cid:410)(cid:381)(cid:396)(cid:455)(cid:3)(cid:286)(cid:296)(cid:296)(cid:286)(cid:272)(cid:410)(cid:349)(cid:448)(cid:286)(cid:855)(cid:3)(cid:258)(cid:3)(cid:374)(cid:258)(cid:396)(cid:396)(cid:258)(cid:410)(cid:349)(cid:448)(cid:286)(cid:3)(cid:410)(cid:346)(cid:258)(cid:410)(cid:3)(cid:393)(cid:258)(cid:396)(cid:286)(cid:400)(cid:3)(cid:258)(cid:449)(cid:258)(cid:455)(cid:3)(cid:286)(cid:454)(cid:410)(cid:396)(cid:258)(cid:374)(cid:286)(cid:381)(cid:437)(cid:400)(cid:3)
(cid:349)(cid:374)(cid:296)(cid:381)(cid:396)(cid:373)(cid:258)(cid:410)(cid:349)(cid:381)(cid:374)(cid:3)(cid:410)(cid:381)(cid:3)(cid:296)(cid:349)(cid:374)(cid:282)(cid:3)(cid:258)(cid:3)(cid:400)(cid:410)(cid:381)(cid:396)(cid:455)(cid:3)(cid:349)(cid:374)(cid:3)(cid:410)(cid:346)(cid:286)(cid:3)(cid:282)(cid:258)(cid:410)(cid:258)(cid:854)(cid:3)(cid:272)(cid:381)(cid:374)(cid:410)(cid:286)(cid:454)(cid:410)(cid:3)(cid:410)(cid:381)(cid:3)(cid:346)(cid:286)(cid:367)(cid:393)(cid:3)(cid:410)(cid:346)(cid:286)(cid:3)(cid:396)(cid:286)(cid:258)(cid:282)(cid:286)(cid:396)(cid:3)(cid:437)(cid:374)(cid:282)(cid:286)(cid:396)(cid:400)(cid:410)(cid:258)(cid:374)(cid:282)(cid:3)(cid:410)(cid:346)(cid:286)(cid:3)(cid:271)(cid:258)(cid:400)(cid:349)(cid:272)(cid:400)(cid:3)(cid:381)(cid:296)(cid:3)(cid:410)(cid:346)(cid:286)(cid:3)
(cid:349)(cid:374)(cid:296)(cid:381)(cid:396)(cid:373)(cid:258)(cid:410)(cid:349)(cid:381)(cid:374)(cid:3)(cid:410)(cid:381)(cid:3)(cid:296)(cid:349)(cid:374)(cid:282)(cid:3)(cid:258)(cid:3)(cid:400)(cid:410)(cid:381)(cid:396)(cid:455)(cid:3)(cid:349)(cid:374)(cid:3)(cid:410)(cid:346)(cid:286)(cid:3)(cid:282)(cid:258)(cid:410)(cid:258)(cid:854)(cid:3)(cid:272)(cid:381)(cid:374)(cid:410)(cid:286)(cid:454)(cid:410)(cid:3)(cid:410)(cid:381)(cid:3)(cid:346)(cid:286)(cid:367)(cid:393)(cid:3)(cid:410)(cid:346)(cid:286)(cid:3)(cid:396)(cid:286)(cid:258)(cid:282)(cid:286)(cid:396)(cid:3)(cid:437)(cid:374)(cid:282)(cid:286)(cid:396)(cid:400)(cid:410)(cid:258)(cid:374)(cid:282)(cid:3)(cid:410)(cid:346)(cid:286)(cid:3)(cid:271)(cid:258)(cid:400)(cid:349)(cid:272)(cid:400)(cid:3)(cid:381)(cid:296)(cid:3)(cid:410)(cid:346)(cid:286)(cid:3)
(cid:400)(cid:437)(cid:271)(cid:361)(cid:286)(cid:272)(cid:410)(cid:854)(cid:3)(cid:349)(cid:374)(cid:410)(cid:286)(cid:396)(cid:448)(cid:349)(cid:286)(cid:449)(cid:349)(cid:374)(cid:336)(cid:3)(cid:410)(cid:346)(cid:286)(cid:3)(cid:282)(cid:258)(cid:410)(cid:258)(cid:3)(cid:410)(cid:381)(cid:3)(cid:296)(cid:349)(cid:374)(cid:282)(cid:3)(cid:349)(cid:410)(cid:400)(cid:3)(cid:296)(cid:367)(cid:258)(cid:449)(cid:400)(cid:3)(cid:258)(cid:374)(cid:282)(cid:3)(cid:271)(cid:286)(cid:3)(cid:400)(cid:437)(cid:396)(cid:286)(cid:3)(cid:381)(cid:296)(cid:3)(cid:381)(cid:437)(cid:396)(cid:3)(cid:272)(cid:381)(cid:374)(cid:272)(cid:367)(cid:437)(cid:400)(cid:349)(cid:381)(cid:374)(cid:400)(cid:856)(cid:3)(cid:87)(cid:396)(cid:286)(cid:410)(cid:410)(cid:349)(cid:374)(cid:286)(cid:400)(cid:400)(cid:3)(cid:349)(cid:400)(cid:3)(cid:258)(cid:3)(cid:271)(cid:381)(cid:374)(cid:437)(cid:400)(cid:854)(cid:3)
(cid:400)(cid:437)(cid:271)(cid:361)(cid:286)(cid:272)(cid:410)(cid:854)(cid:3)(cid:349)(cid:374)(cid:410)(cid:286)(cid:396)(cid:448)(cid:349)(cid:286)(cid:449)(cid:349)(cid:374)(cid:336)(cid:3)(cid:410)(cid:346)(cid:286)(cid:3)(cid:282)(cid:258)(cid:410)(cid:258)(cid:3)(cid:410)(cid:381)(cid:3)(cid:296)(cid:349)(cid:374)(cid:282)(cid:3)(cid:349)(cid:410)(cid:400)(cid:3)(cid:296)(cid:367)(cid:258)(cid:449)(cid:400)(cid:3)(cid:258)(cid:374)(cid:282)(cid:3)(cid:271)(cid:286)(cid:3)(cid:400)(cid:437)(cid:396)(cid:286)(cid:3)(cid:381)(cid:296)(cid:3)(cid:381)(cid:437)(cid:396)(cid:3)(cid:272)(cid:381)(cid:374)(cid:272)(cid:367)(cid:437)(cid:400)(cid:349)(cid:381)(cid:374)(cid:400)(cid:856)(cid:3)(cid:87)(cid:396)(cid:286)(cid:410)(cid:410)(cid:349)(cid:374)(cid:286)(cid:400)(cid:400)(cid:3)(cid:349)(cid:400)(cid:3)(cid:258)(cid:3)(cid:271)(cid:381)(cid:374)(cid:437)(cid:400)(cid:854)(cid:3)
(cid:349)(cid:296)(cid:3)(cid:349)(cid:410)(cid:3)(cid:381)(cid:271)(cid:367)(cid:349)(cid:410)(cid:286)(cid:396)(cid:258)(cid:410)(cid:286)(cid:400)(cid:3)(cid:410)(cid:346)(cid:286)(cid:3)(cid:258)(cid:271)(cid:349)(cid:367)(cid:349)(cid:410)(cid:455)(cid:3)(cid:410)(cid:381)(cid:3)(cid:396)(cid:286)(cid:258)(cid:282)(cid:3)(cid:410)(cid:346)(cid:286)(cid:3)(cid:400)(cid:410)(cid:381)(cid:396)(cid:455)(cid:3)(cid:381)(cid:296)(cid:3)(cid:410)(cid:346)(cid:286)(cid:3)(cid:448)(cid:349)(cid:400)(cid:437)(cid:258)(cid:367)(cid:349)(cid:460)(cid:258)(cid:410)(cid:349)(cid:381)(cid:374)(cid:853)(cid:3)(cid:349)(cid:410)(cid:859)(cid:400)(cid:3)(cid:374)(cid:381)(cid:410)(cid:3)(cid:449)(cid:381)(cid:396)(cid:410)(cid:346)(cid:3)(cid:258)(cid:282)(cid:282)(cid:349)(cid:374)(cid:336)(cid:3)(cid:400)(cid:381)(cid:373)(cid:286)(cid:3)(cid:449)(cid:349)(cid:367)(cid:282)(cid:3)(cid:374)(cid:286)(cid:449)(cid:3)
(cid:349)(cid:296)(cid:3)(cid:349)(cid:410)(cid:3)(cid:381)(cid:271)(cid:367)(cid:349)(cid:410)(cid:286)(cid:396)(cid:258)(cid:410)(cid:286)(cid:400)(cid:3)(cid:410)(cid:346)(cid:286)(cid:3)(cid:258)(cid:271)(cid:349)(cid:367)(cid:349)(cid:410)(cid:455)(cid:3)(cid:410)(cid:381)(cid:3)(cid:396)(cid:286)(cid:258)(cid:282)(cid:3)(cid:410)(cid:346)(cid:286)(cid:3)(cid:400)(cid:410)(cid:381)(cid:396)(cid:455)(cid:3)(cid:381)(cid:296)(cid:3)(cid:410)(cid:346)(cid:286)(cid:3)(cid:448)(cid:349)(cid:400)(cid:437)(cid:258)(cid:367)(cid:349)(cid:460)(cid:258)(cid:410)(cid:349)(cid:381)(cid:374)(cid:853)(cid:3)(cid:349)(cid:410)(cid:859)(cid:400)(cid:3)(cid:374)(cid:381)(cid:410)(cid:3)(cid:449)(cid:381)(cid:396)(cid:410)(cid:346)(cid:3)(cid:258)(cid:282)(cid:282)(cid:349)(cid:374)(cid:336)(cid:3)(cid:400)(cid:381)(cid:373)(cid:286)(cid:3)(cid:449)(cid:349)(cid:367)(cid:282)(cid:3)(cid:374)(cid:286)(cid:449)(cid:3)
(cid:448)(cid:349)(cid:400)(cid:437)(cid:258)(cid:367)(cid:349)(cid:460)(cid:258)(cid:410)(cid:349)(cid:381)(cid:374)(cid:3)(cid:400)(cid:410)(cid:455)(cid:367)(cid:286)(cid:3)(cid:381)(cid:396)(cid:3)(cid:400)(cid:410)(cid:396)(cid:258)(cid:374)(cid:336)(cid:286)(cid:3)(cid:349)(cid:374)(cid:410)(cid:286)(cid:396)(cid:296)(cid:258)(cid:272)(cid:286)(cid:856)
(cid:448)(cid:349)(cid:400)(cid:437)(cid:258)(cid:367)(cid:349)(cid:460)(cid:258)(cid:410)(cid:349)(cid:381)(cid:374)(cid:3)(cid:400)(cid:410)(cid:455)(cid:367)(cid:286)(cid:3)(cid:381)(cid:396)(cid:3)(cid:400)(cid:410)(cid:396)(cid:258)(cid:374)(cid:336)(cid:286)(cid:3)(cid:349)(cid:374)(cid:410)(cid:286)(cid:396)(cid:296)(cid:258)(cid:272)(cid:286)(cid:856)

(cid:882)(cid:3)(cid:58)(cid:258)(cid:272)(cid:381)(cid:271)(cid:3)(cid:44)(cid:258)(cid:396)(cid:396)(cid:349)(cid:400)(cid:853)(cid:3)(cid:400)(cid:286)(cid:374)(cid:349)(cid:381)(cid:396)(cid:3)(cid:400)(cid:381)(cid:296)(cid:410)(cid:449)(cid:258)(cid:396)(cid:286)(cid:3)(cid:258)(cid:396)(cid:272)(cid:346)(cid:349)(cid:410)(cid:286)(cid:272)(cid:410)(cid:3)(cid:258)(cid:410)(cid:3)(cid:100)(cid:346)(cid:286)(cid:3)(cid:69)(cid:286)(cid:449)(cid:3)(cid:122)(cid:381)(cid:396)(cid:364)(cid:3)(cid:100)(cid:349)(cid:373)(cid:286)(cid:400)(cid:3)
(cid:882)(cid:3)(cid:58)(cid:258)(cid:272)(cid:381)(cid:271)(cid:3)(cid:44)(cid:258)(cid:396)(cid:396)(cid:349)(cid:400)(cid:853)(cid:3)(cid:400)(cid:286)(cid:374)(cid:349)(cid:381)(cid:396)(cid:3)(cid:400)(cid:381)(cid:296)(cid:410)(cid:449)(cid:258)(cid:396)(cid:286)(cid:3)(cid:258)(cid:396)(cid:272)(cid:346)(cid:349)(cid:410)(cid:286)(cid:272)(cid:410)(cid:3)(cid:258)(cid:410)(cid:3)(cid:100)(cid:346)(cid:286)(cid:3)(cid:69)(cid:286)(cid:449)(cid:3)(cid:122)(cid:381)(cid:396)(cid:364)(cid:3)(cid:100)(cid:349)(cid:373)(cid:286)(cid:400)(cid:3)

- unigrams
- bigrams

s140 a   lex
- unigrams
- bigrams

s140 neglex
- unigrams
- bigrams

positive
2,312 (41%)
2,006 (30%)
2,718 (36%)

negative
3,324 (59%)
4,783 (70%)
4,911 (64%)

total
5,636
6,789
7,629

19,121 (49%)
69,337 (39%)

20,292 (51%)
109,514 (61%)

39,413
178,851

19,344 (51%)
67,070 (42%)

18,905 (49%)
90,788 (58%)

38,249
157,858

936 (14%)
3,954 (15%)

5,536 (86%)
22,258 (85%)

6,472
26,212

39,979 (61%)
135,280 (51%)

25,382 (39%)
131,230 (49%)

65,361
266,510

40,422 (63%)
133,242 (55%)

23,382 (37%)
107,206 (45%)

63,804
240,448

1,038 (12%)
5,913 (16%)

7,315 (88%)
32,128 (84%)

8,353
38,041

66 

more data: restaurant reviews 

         yelp phoenix academic dataset  

       230,000 customer reviews posted on yelp 
       500 business categories 

         multiple categories assigned for a business 

         for e.g.,    restaurant, deli, and bakery    

         yelp restaurant reviews corpus 

       58 business categories related to the restaurant domain 
       183,935 customer reviews 
       generated sentiment lexicons using the star ratings as 

labels 

67 

more data: laptop reviews 

         amazon customer reviews dataset (mcauley and leskovec, 2013)  

       34,686,770 customer reviews posted on amazon from 

1995 to 2013 (11gb of data) 

       6,643,669 users  
       2,441,053 products

  
 

  

         amazon laptop reviews corpus 

       searched for mentions of laptop or notepad in the 

electronics reviews subset 
       124,712 customer reviews 
       generated sentiment lexicons using the star ratings as 

labels 

68 

other recent approaches  
to creating sentiment lexicons 
         using neural networks and deep learning techniques 

       duyu tang, furu wei, bing qin, ming zhou and ting liu (2014)  
         constructing domain-specific sentiment  

       sheng huanga, zhendong niua, and chongyang shi (2014) 
      

ilia chetviorkin and natalia loukachevitch (2014) 

         others: 

       hassan saif, miriam fernandez, yulan he, and harith alani (2014): senticircles for contextual and 

conceptual semantic id31 of twitter.  

       shi feng, kaisong song, daling wang, ge yu (2014): a word-emoticon mutual reinforcement 
ranking model for building sentiment lexicon from massive collection of microblogs.  
       raheleh makki, stephen brooks and evangelos e. milios (2014): context-specific sentiment 

lexicon expansion via minimal user interaction.  

       yanqing chen and steven skiena (2014): building sentiment lexicons for all major languages 
       bandhakavi et al. (2014): generating a word-emotion lexicon from #emotional tweets -- 

em with mixture of classes model. 

69 

id31 features 
features!
sentiment lexicon!

examples!
#positive: 3, scorepositive: 2.2; maxpositive: 1.3; last: 
0.6, scorenegative: 0.8, scorepositive_neg: 0.4
spectacular, like documentary
spect, docu, visua
#n: 5, #v: 2, #a:1
#neg: 1; ngram:perfect     ngram:perfect_neg, 
polarity:positive     polarity:positive_neg
probably, de   nitely, def
yes, cool
#!+: 1, #?+: 0, #!?+: 0
:d, >:(
soooo, yaayyy

word id165s!
char id165s!
part of speech!
negation!

word clusters!
all-caps!
punctuation!
emoticons!
elongated words!

id31 of short informal texts. svetlana kiritchenko, xiaodan zhu and saif 
mohammad. journal of artificial intelligence research, 50, august 2014. 

70 

word clusters 

         the cmu twitter nlp tool provides 1000 token clusters  

       produced with the brown id91 algorithm on 56 million 

english-language tweets 

       alternative representation of tweet content 
         reducing the sparcity of the token space 

         feature: 

clusters. 

       the presence or absence of tokens from each of the 1000 

71 

other features 

         punctuation: 

       the number of contiguous sequences of exclamation 

marks, question marks, and both exclamation and question 
marks, for example, !!!! 

       whether the last token contains an exclamation or question 

mark 

         emoticons 

       presence or absence of emoticons at any position in the 

tweet, for example, :) 

       whether the last token is a positive or negative emoticon 

         elongated words 

       the number of words with one character repeated more 

than two times, for example, yesssss 

72 

id31 features 
features!
sentiment lexicon!

examples!
#positive: 3, scorepositive: 2.2; maxpositive: 1.3; last: 
0.6, scorenegative: 0.8, scorepositive_neg: 0.4
spectacular, like documentary
spect, docu, visua
#n: 5, #v: 2, #a:1
#neg: 1; ngram:perfect     ngram:perfect_neg, 
polarity:positive     polarity:positive_neg
probably, de   nitely, def
yes, cool
#!+: 1, #?+: 0, #!?+: 0
:d, >:(
soooo, yaayyy

word id165s!
char id165s!
part of speech!
negation!

word clusters!
all-caps!
punctuation!
emoticons!
elongated words!

id31 of short informal texts. svetlana kiritchenko, xiaodan zhu and saif 
mohammad. journal of artificial intelligence research, 50, august 2014. 

73 

overview of sentiment 
analysis systems 
       rule-based systems 
       conventional statistical systems 
       deep-learning-based models 

74 

overview of sentiment 
analysis systems 
       rule-based systems 
       conventional statistical systems 
       deep-learning-based models 

75 

teragram: a rule-based system 
(reckman et al., 2013) 

         develop lexicalized hand-written rules: each rule is a pattern 

that matches words or sequences of words. 
       examples: 

 negative: 

positive: 

         background data: use blogs, forums, news, and tweets to 

develop the rules.  

         performance:  

       ranked 3rd on the tweet test data in message-level task 
(semeval-2013 task 2), but ranked 15th on the term-level 
task. 

 

76 

remarks 

         carefully developed rule-based systems can sometimes 

achieve completive performance on the data/domains they 
are created for. 

         advantages: explicit id99, so intuitive to 

develop and maintain. 

         problems 

       coverage: hand-written rules often have limited coverage, 

so recall is often low. this can impact the overall 
performance (as observed in teragram). 

       extensibility: not easy to be extended to new data/domains; 
rule-based models have inherent difficulty in automatically 
acquiring knowledge. 
       modeling capability. 

   

77 

remarks (continued) 

         the main stream is statistical approaches, which achieve top 

performance across different tasks and data sets. 
       note that knowledge acquired by applying rules can often 

be easily incorporated as features into statistical 
approaches. 

   

 

78 

overview of sentiment 
analysis systems 
       rule-based systems 
       conventional statistical systems 

       nrc-canada systems 
       key ideas from other top teams 

       deep-learning-based models 

79 

detailed description of the nrc-canada 
systems  
         message-level sentiment (of tweets, blogs, sms):  

       semeval-2013 task 2, semeval-2014 task 9 
         term-level sentiment (within tweets, blogs, sms) 
       semeval-2013 task 2, semeval-2014 task 9 
         aspect-level sentiment (in customer reviews):  

       semeval-2014 task 4 

80 

detailed description of the nrc-canada 
systems  
         message-level sentiment (of tweets, blogs, sms):  

       semeval-2013 task 2, semeval-2014 task 9 
         term-level sentiment (within tweets, blogs, sms) 
       semeval-2013 task 2, semeval-2014 task 9 
         aspect-level sentiment (in customer reviews):  

       semeval-2014 task 4 

81 

message-level sentiment: the task 

tweet: happy birthday, hank williams. in honor if  the hank turning 88, we'll 
play 88 hank songs in a row tonite @the_zoo_bar. #honkytonk






positive 


tweet: #londonriots is trending 3rd worldwide ..... this is not something to 
be proud of  united kingdom!!! sort it out!!!!






negative 


tweet: on the night hank williams came to town.
 

 neutral 

 

 

82 

message-level sentiment : the 
approach 
         pre-processing 

       url -> http://someurl 
       userid -> @someuser 
       id121 and part-of-speech (pos) tagging  
 (cmu twitter nlp tool) 

         classifier 

       linear id166 (an in-house implementation by colin cherry) 

         evaluation 

       macro-averaged f-pos and f-neg 

83 

message-level sentiment : the features 

features!
id165s 
char id165s 
emoticons 
hashtags 
capitalization 
part of speech 
negation 
word clusters 
lexicons 

examples!
happy, am_very_happy, am_*_happy 
un, unh, unha, unhap 
:d, >:( 
#excited, #nowplaying 
yes, cool 
n: 5, v: 2, a:1 
neg:1 
probably, definitely, def, possibly, prob, ... 
count: 3; score: 2.45; max: 1.3; last: 0.6 

84 

sentiment lexicons 

         manual lexicons: 

       nrc emotion lexicon 
       mpqa sentiment lexicon 
       bing liu   s opinion lexicon 

         automatically created lexicons: 
       hashtag sentiment lexicon 
       sentiment140 lexicon 

85 

message-level sentiment : the data 
(semeval-2013 task 2) 
         training: ~ 10,000 labeled tweets 

       positive: 40% 
       negative: 15% 
       neutral: 45% 

         test: 

       tweets: ~ 4,000 
       sms: ~ 2,000 

86 

of   cial performance/rankings 

         tweets  

       macro-averaged f: 69.02 
       1st place 

         sms 

       macro-averaged f: 68.42 
       1st place 

87 

detailed results on tweets 

f-score 

69.02!

best unconstrained"

0.8 

0.7 

0.6 

0.5 

0.4 

0.3 

0.2 

0.1 

0 

teams 

unigrams!

majority!

88 

feature contributions on tweets 

70"

68"

66"

64"

62"

60"

58"

56"

all"

all - 

lexicons"

all - 

manual 
lexicons"

all - auto 
lexicons"

all - 

ngrams"

all - word 
ngrams"

all - char 
ngrams"

all - 

all - pos" all - 

negation"

clusters"

all - 
twitter 
spec."

89 

detailed results on tweets (continued) 

4th!

8th!

no auto lexicons!
no any lexicons!

f-score 

0.8 

0.7 

0.6 

0.5 

0.4 

0.3 

0.2 

0.1 

0 

teams 

90 

f-score 

68.46!

0.8 

0.7 

0.6 

0.5 

0.4 

0.3 

0.2 

0.1 

0 

detailed results on sms 

unigrams"

majority"

91 

teams 

feature contributions (on sms) 

70"

68"

66"

64"

62"

60"

58"

56"

54"

all"

all - 

lexicons"

all - 

manual 
lexicons"

all - auto 
lexicons"

all - 

ngrams"

all - word 
ngrams"

all - char 
ngrams"

all - 

negation"

all - pos"

all - 

clusters"

all - twitter 

spec."

92 

improving our systems for 
semeval-2014 task 9  

key idea: 
improving sentiment lexicons to better cope with 
negation. 

93 

complex effect of negation 
         why negation? negation often significantly affects the 

sentiment of its scopes. 

not  very good

negator  

argument 

         this complex effect has recently been studied in stanford 

sentiment tree bank (zhu et al., 2014; socher et al., 2013) 
       non-lexicalized assumptions 

       reversing 
       shifting, polarity-based shifting 
       simple lexicalized assumptions 

       negator-based shifting  
       combined shifting 

       sentiment composition 

       recursive-neural-network-based 

composition 

94 

improving the systems for 
semeval-2014 task 9  
        

in our semeval-2014 system, we adopted a lexicon-based 
approach (kiritchenko et al., 2014) to determine the sentiment 
of words in affirmative and negated context. 

95 

message-level sentiment : the data 
(semeval-2014 task 9) 
         training (same as in semeval-2013): ~ 10,000 labeled tweets 

       positive: 40% 
       negative: 15% 
       neutral: 45% 

         test 

       official 2014 data: 
         tweets: ~ 2,000 
         sarcastic tweets: ~ 100 
         livejournal blogs (sentences): ~ 1,000 

       progress (semeval-2013 test data): 

         tweets: ~ 4,000 
         sms: ~ 2,000 

96 

of   cial performance/rankings 

         1st on micro-averaged f-score over all 5 test sets 
         details 

our rankings: 

97 

of   cial performance/rankings 

98 

ablation effects of features 

99 

message-level sentiment : summary 

         significant improvement over nrc-canada-2013 system 
         best micro- and macro-averaged results on all 5 datasets; 

best results on 3 out of 5 datasets 

         system trained on tweets showed similar performance on 

sms and livejournal blog sentences 

         strong performance on sarcastic tweets 
         most useful features on all datasets:  

       sentiment lexicons, especially automatic tweet-specific 

lexicons 

         id165s are very helpful for in-domain data (tweets), less 

helpful for out-of-domain data (sms and livejournal) 

100 

key ideas from other top systems 

         coooolll 

       use sentiment-specific id27s (details will be 

discussed later) 

         teamx  

       parameters are fine-tuned towards the tweet datasets  

         this may explain why the system achieved the best results on 

the tweet sets but showed worse performance on the out-of-
domain sets. 

         rtrgo 

       use random subspace learning (s  gaard and johannsen, 

2012)  
         train a classifier on a concatenation of 25 corrupted 
copies of the training set (each feature is randomly 
disabled with prob=0.2) 

101 

key ideas from other top systems 

         other ideas 

       id147 
       careful id172 (e.g., for the elongated words) 
       term/cluster weighting (e.g., tf-idf) 

102 

detailed description of the nrc-canada 
systems  
         message-level sentiment (of tweets, blogs, sms):  

       semeval-2013 task 2, semeval-2014 task 9 
         term-level sentiment (within tweets, blogs, sms) 
       semeval-2013 task 2, semeval-2014 task 9 
         aspect-level sentiment (in customer reviews):  

       semeval-2014 task 4 

103 

term-level sentiment : the problem 

 
tweet: the new star trek does not have much of  a story, but it is visually 
spectacular.   
target is positive  
 
tweet: the new star trek does not have much of  a story, but it is visually 
spectacular.   
 
tweet: spock displays more emotions in this star trek than the original 
series.  
 

target is negative  

target is neutral  

104 

 
 
further clari   cation of the problem 

         the task is not de   ned as a sequential labeling problem:"

"
tweet:   w1 w2 w3 w4 w5 w6 w7 w8 w9 .
                         obj     pos  neu  obj     neg   

        no boundary detection is required"
        no need to label all expressions in a tweet."

        

it is an independent classi   cation problem for each sentiment 
term."

tweet:   w1 w2 w3 w4 w5 w6 w7 w8 w9 .
                                  pos     neu             neg   

105 

   
basic feature categories 

features!
term features"

context features"

description!

extracted from the target terms, 
including all the features discussed 
above."

extracted from a window of words 
around a target term or the entire 
tweet, depending on features."

106 

detailed features 
features!
ngrams"
    word ngrams"
    char. ngrams"
encodings"
     emoticons"
     hashtags"
     punctuations"
     elongated word"
lexical"
      manual lexicons"
      automatic lexicons"
negation"
      negations words"
      interaction w. others"

description!

   f-word    +    good   "
dis-   un-"

:-)   d:<   :@   :-||"
#biggestdayoftheyear"
?!   !!!"
sooooo"

mpqa, nrc-emo, liu   s, turney & littman's "
in-house, osgood"

can   t  n   t  cant  isnt"
negating lexical words that follow"

107 

classi   er and evaluation 

         classifier: 

       linear id166 (libid166)"

id166 has performed better than id28 (lr) on 
this task (trick: the latter is much faster and corresponds 
well with id166, so we used lr to quickly test ideas.)"

         evaluation: 

       macro-averaged f-measure"
    (same as in the tweet-level task)"

108 

of   cial performance/rankings 

         tweets  

       macro-averaged f: 89.10 
       1st place 

         sms 

       macro-averaged f: 88.34 
       2st place 

109 

detailed results on tweets 

no lexicons!
6th!

no ngrams!

8th!

1 

0.9 

0.8 

0.7 

0.6 

0.5 

0.4 

0.3 

0.2 

0.1 

0 

110 

term features vs. context features 

         are contexts helpful? how much?"

       by large, sentiment of terms can be judged by the target 

terms themselves. 

       the contextual features can additionally yield 2-4 points 

improvement on f-scores. 

111 

discussion 

performance in the term-level task (~0.9) markedly higher than 
in message-level task (~0.7) 

 

what does this mean? 

 

        

is it harder for humans to determine sentiment of whole 
message? 
       inter-annotator agreement scores will be helpful. 

         does the task set-up favors the term-level task? 

       about 85% of the target terms seen in training data 
       about 81% of the instances of a word in the training and 

test data have the same polarity 

112 

key ideas from other top systems 

         gu-mlt-lt 

       use on-line classifiers (stochastic gradient decent). 
       careful id172: all numbers are normalized to 0; 

repeated letters are also collapsed (liiike->like) 

         avaya 

       dependency parse features: 

         edges that contain at least one target words 
         paths between the head of term and the root of the 

entire message 

         bounce 

likely to be neutral) 

       use term length features (intuition: longer terms are more 

         teragram 

       use hand-written rules (as discussed earlier) 

113 

improving the systems for 
semeval-2014 task 9  
        

improving sentiment lexicons (as in message-level models) 
       using a lexicon-based approach (kiritchenko et al., 2014) to 

determining the sentiment of words in af   rmative and 
negated context."

         discriminating negation words 

       different negation words, e.g. never and didn   t, can affect 

sentiment differently (zhu et al., 2014)."

       we made a simple, lexicalized modi   cation to our system"

 

 this is never acceptable 

"the word acceptable is marked as acceptable_not in our 
old system but as acceptable_benever in our new system."

 

114 

term-level sentiment : the data 
(semeval-2014 task 9) 
         training (same as in semeval-2013): 8,891 terms 

       positive: 62%; negative: 35%; neutral: 3% 

         test 

       official 2014 data: 

         tweets: 2,473 terms 
         sarcastic tweets: 124  
         livejournal blogs: 1,315 

       progress (semeval-2013 test data): 

         tweets: 4,435 
         sms: 2,334 

115 

of   cial performance/rankings 

         1st on micro-averaged f-score over all 5 test sets 
         details 

our rankings: 

116 

of   cial performance/rankings 

117 

ablation effects of features 

118 

summary 

         significant improvement over nrc-canada-2013 system. 
         best micro-averaged results on all 5 datasets; best results on 

2 out of 5 datasets. 

         effect of lexicon features 

       sentiment lexicons automatically built from tweets are 

particularly effective in our models.!
         better handling of negation is helpful. 

119 

key ideas from other top systems 

         sentiklue 

       use message-level polarity, which is the 3rd most 

important feature category (following bag-of-words and 
sentiment lexicon features.) 

         cmuq-hybrid 

       rbf kernel found to be the best kernel in the models(so 

different systems may still need to try different kernels 
during development.) 

120 

key ideas from other top systems 

         cmuq@qatar 

       careful preprocessing (5.2% gain was observed.) 

         tokenizing also words that stick together (no space 

between them). 

         for any acronym, keeping both the acronym and the 

expanded version (e.g., lol -> laugh out loudly) 

         think-positive: 

       apply a deep convolution neural network approach. 

121 

detailed description of the nrc-canada 
systems  
         message-level sentiment (of tweets, blogs, sms):  

       semeval-2013 task 2, semeval-2014 task 9 
         term-level sentiment (within tweets, blogs, sms) 
       semeval-2013 task 2, semeval-2014 task 9 
         aspect-level sentiment (in customer reviews):  

       semeval-2014 task 4 

122 

aspect-level sentiment 

         sub-task 1: aspect term extraction 

       find terms in a given sentence that are related to aspects 

of the products. 

         sub-task 2: aspect term polarity 

       determine whether the polarity of each aspect term is 

positive, negative, neutral or conflict. 

         sub-task 3: aspect category detection  

       identify aspect categories discussed in a given sentence 

(e.g., food, service) 

         sub-task 4: aspect category polarity  

       determine the polarity of each aspect category. 

123 

sub-task 1: aspect term extraction 

         find terms in a given sentence that are related to aspects of the 

products under review 

       i charge it at night and skip taking the cord with me because 

of the good battery life. 

       the service is outstanding and my crab-cake eggs benedict 

could not have been better. 

 

 

 

  

124 

 
aspect term extraction: the approach 

         semi-markov discriminative tagger 

       tags phrases, not tokens, can memorize    fish and chips    
       trained with mira using a basic feature set 
       for each token included in a phrase being tagged: 

         word identity (cased & lowercased) in a 2-word window 
         prefixes and suffixes up to 3 characters 

       for each phrase being tagged 

         phrase identity (cased and lowercased) 

         our current system does not use any word clusters, 

embeddings or gazetteers/lexicons. 

125 

aspect term extraction: the data 

         restaurants 
       sentences: 
       term tokens: 
       term types: 

 3,041 
 3,693 
 1,212 

         laptops 

       sentences: 
       term tokens: 
       term types: 

 3,045 
 2,358 
 0,955 

 

376 
food 
238 
service 
65 
prices 
64 
place 
57 
menu 
57 
staff 
56 
dinner 
pizza 
51 
atmosphere  49 
42 
price 

screen 
price 
battery life 
use 
keyboard 
battery 
programs 
features 
software 
warranty 

64 
58 
55 
53 
52 
48 
37 
35 
34 
31 

126 

aspect term extraction: results 
restaurants 
dlirec 
xrce 
nrc-canada 
unitor 
ihs_rd 
18 other teams    

precision  recall 
85.4 
86.2 
84.4 
82.4 
86.1 

f1 
84.0 
84.0 
80.2 
80.1 
79.6 

82.7 
81.8 
76.4 
77.9 
74.1 

laptops 
ihs_rd 
dlirec 
nrc-canada 
unitor 
xrce 
19 other teams    

precision  recall 
84.8 
82.5 
78.8 
77.4 
69.7 

66.5 
67.1 
60.7 
57.6 
65.0 

f1 
74.6 
74.0 
68.6 
68.0 
67.2 

127 

key ideas from other top systems 

         dlirec 

data.  

       clusters (brown/id97) built on amazon and yelp 

       entity list harvested from "double propagation     

         start with sentiment words, find noun phrases in unsupervised 
data that are modified by those sentiment words, declare those 
noun phrases entities, i.e.: "the rice is amazing" extracts "rice   . 

       syntactic heads (from stanford parser) are important 

features. 

         his_rd 

helpful.  

       some domain-independent word lists appeared to be 

128 

key ideas from other top systems 

         unitor 

       word vectors built using word co-occurrence + lsa on 

opinosis (laptop) and tripadvisor datasets 

         xrce 

parser 

       rule-based post-processing of output from a syntactic 

       parser's lexicon augmented with terms from training data, 

id138 synonyms, and food terms list from wikipedia 
"food portal   .  

129 

aspect-level sentiment: sub-tasks  

         sub-task 1: aspect term extraction 

       find terms in a given sentence that are related to aspects 

of the products. 

         sub-task 2: aspect term polarity 

       determine whether the polarity of each aspect term is 

positive, negative, neutral or conflict. 

         sub-task 3: aspect category detection  

       identify aspect categories discussed in a given sentence 

(e.g., food, service) 

         sub-task 4: aspect category polarity  

       determine the polarity of each aspect category. 

130 

aspect term polarity: the task 

  
 the asian salad of great asian is barely eatable. 
 
task: in the sentence above, what   s the sentiment 
expressed towards the target term    asian salad   ? 

 

131 

aspect term polarity: the task 

         this is different from the    term-level    problem in task 9 

 the asian salad of  great asian is barely eatable.
task 4:  aspect terms 

task 9:  sentiment terms 



 
task 9: phrase-level id31 
task 4: sentiment towards a target 
 

         system concerns 

-    the task-9 systems do not consider syntactic features, but 

task-4 systems should. 

-    task-9 depends mainly on the sentiment terms themselves, 

while task 4 relies more on context. 

-    a task-9 model can be a component of task 4. 

132 

aspect term polarity: the features 

         surface features 

       unigrams 
       contex-target bigrams (formed by a word from the surface 

context and a word from the target term itself) 

         lexicon features 

       number of positive/negative tokens  
       sum/maximum of the tokens    sentiment scores 

133 

aspect term polarity: the features 

         syntactic features 

       consider long-distance sentiment phrases 

the ma-po tofu, though not as spicy as what we had last 
time, is actually great too. 
  

       consider local syntax
  a serious sushi lover 

       word- and pos-ngrams in the parse context 
       context-target bigrams, i.e., bigrams composed of a word 

from the parse context and a word from the target term 

       all paths that start or end with the root of the target terms 
       sentiment terms in parse context 

 

134 

aspect term polarity: the data 

         customer reviews  

       laptop data 

         training: 2358 terms 
         test: 654 terms 
       restaurant data 

         training: 3693 target terms 
         test: 1134 terms 

         pre-processing 

       we tokenized and parsed the provided data with stanford 
corenlp toolkits to obtain (collapsed) typed dependency 
parse trees (de marneffe et al., 2006). 

135 

aspect term polarity: set-up 

         classifier 

       libid166 with the linear kernel 

         evaluation metric 

       accuracy 

136 

aspect term polarity: results 

         laptop reviews 

       accuracy: 70.49 
       1st among 32 submissions from 29 teams 

         restaurant reviews 
       accuracy: 80.16 
       2nd among 36 submissions from 29 teams 

137 

aspect term polarity: contributions of 
features 

138 

aspect term polarity: summary 

         our systems achieve an accuracy of 70.49 on the laptop 

reviews and 80.16 on the restaurant data, ranking 1st and 2nd, 
respectively.  

         the sentiment lexicon and parse features are critical to help 

us achieve the performance.  

         carefully designed features are also important: distance-

weighted features, id172 & id121, etc.  

139 

key ideas from other top systems 

         dcu 

       sentiment scores of a word are reweighted by the distance 

between the word and the target aspect terms. 
         different types of distances are calculated: surface, the number 

of discourse chunks, dependency parse path length. 

       multiple manual sentiment lexicons are combined together 

       manually built domain-specific lexicons:    mouthwatering   , 

before being used. 

   watering   . 

       careful id172 & id121: spelling check, 
multiword processing (e.g., word with the form x-y) 

140 

key ideas from other top systems 

         szte-nlp 

features. 

       syntax-based features showed to be the best category of 

       if a sentence contains multiple aspect terms, identifying the 

range associated with each target aspect. 

       each id165 feature is weighted by the distance of the n-

gram to the target aspect term.  

       dependency parse trees are used to select the words 

around aspect terms. 

       use aspect categories as features. 

         uwb 

       each id165 feature is weighted by the distance of the n-

gram to the target aspect term (using a gaussian 
distribution.) 

141 

key ideas from other top systems 

         xrce 

       the entire system is built around sentiment-oriented 

dependency parser 
         parse trees were annotated with sentiment information. 

       rules are used to link sentiment on terms based on the 

parse. 

       hybridizing rule based parse with machine learning. 

         ubham 

       detect sentiment of text using lexicon-based methods, then 

assign that to different clauses using dependency parse 
trees. 
 

142 

aspect-level sentiment: sub-tasks  

         sub-task 1: aspect term extraction 

       find terms in a given sentence that are related to aspects 

of the products. 

         sub-task 2: aspect term polarity 

       determine whether the polarity of each aspect term is 

positive, negative, neutral or conflict. 

         sub-task 3: aspect category detection  

       identify aspect categories discussed in a given sentence 

(e.g., food, service) 

         sub-task 4: aspect category polarity  

       determine the polarity of each aspect category. 

143 

aspect category detection: the task 

        

identify aspect categories discussed in a given sentence 
extracted from a restaurant review 
 
to be completely fair, the only redeeming factor was the food, which was above 
average, but couldn't make up for all the other de   ciencies of  teodora.

aspect categories: food, miscellaneous 

         a second stage will assign sentiment to each of these categories 

144 

 
aspect category detection: the approach 

         pre-processing 

       id121 (cmu twitter nlp tool) 
       id30 (porter stemmer) 

         classifier 

category 

       id166 with linear kernel (colin   s implementation) 
       five binary classifiers (one-vs-all), one for each aspect 

         evaluation 

       micro-averaged f1-score 

145 

aspect category detection: the approach 

         features 
       ngrams 
       stemmed ngrams 
       character ngrams 
       word cluster ngrams 
       yelp restaurant word     aspect association lexicon features 

         post-processing 

       if no category is assigned,  

         cmax = argmaxc p(c|d) 
         assign cmax if p(cmax|d)     0.4 

146 

aspect category detection: the data 
         training:  

       3044 sentences with at least one aspect category 
       574 sentences (19%) with more than one aspect category 

 

1400"
1200"
1000"
800"
600"
400"
200"
0"

1233!

1133!

597!

432!

319!

food!

miscellaneous!

service!

ambience!

price!

         test: 

       800 sentences with at least one aspect category 
       189 sentences (24%) with more than one aspect category  

147 

aspect category detection: results 
f-score 
100 

88.58!

unconstrained"

90 

80 

70 

60 

50 

40 

30 

20 

10 

0 

148 

aspect category detection: feature 
contributions 
f-score 

90"

88"

86"

84"

82"

80"

78"

all"

all     lexicons     clusters"

all     lexicons"

all     word clusters"

all - post-processing"

149 

aspect category detection: summary 

         the system ranked first among 18 teams 
         most useful features:  

       in-domain word     aspect association lexicon 

150 

aspect-level sentiment: sub-tasks  

         sub-task 1: aspect term extraction 

       find terms in a given sentence that are related to aspects 

of the products. 

         sub-task 2: aspect term polarity 

       determine whether the polarity of each aspect term is 

positive, negative, neutral or conflict. 

         sub-task 3: aspect category detection  

       identify aspect categories discussed in a given sentence 

(e.g., food, service) 

         sub-task 4: aspect category polarity  

       determine the polarity of each aspect category. 

151 

aspect category polarity: the task 

         determine the polarity (positive, negative, neutral, or conflict) of 
each aspect category discussed in a given sentence extracted 
from a restaurant review 
 
to be completely fair, the only redeeming factor was the food, which was above 
average, but couldn't make up for all the other de   ciencies of  teodora.
aspect categories: food (positive), miscellaneous (negative) 

152 

aspect category polarity: the approach 

         pre-processing 

       id121 and part-of-speech (pos) tagging  
 (cmu twitter nlp tool) 

         classifier 

       id166 with linear kernel (colin   s implementation) 
       one classifier for all aspect categories 
       2 copies of each feature: general and category-specific 
         e.g.,    delicious    ->    delicious_general    and    delicious_food     

food 

service 

the pizza was delicious, but the waiter was rude. 

         evaluation 
       accuracy 

153 

aspect category polarity: the features 

         standard features 

       ngrams, character ngrams 
       word cluster ngrams 
       sentiment lexicon features 
       negation 

         task-specific features 

       find terms associated with a given aspect category using 

yelp restaurant word     aspect association lexicon 
       add standard features generated just for those terms 

food 

service 

the pizza was delicious, but the waiter was rude. 

154 

aspect category polarity: sentiment 
lexicons 
         manual lexicons 

       nrc emotion lexicon 
       mpqa sentiment lexicon 
       bing liu   s opinion lexicon 
         automatically created lexicons 

       yelp restaurant sentiment lexicons: afflex and 

neglex 

       hashtag sentiment lexicons: afflex and neglex 
       sentiment140 lexicons: afflex and neglex 

155 

aspect category polarity: the data 

         training 

       3044 sentences with at least one aspect category 
       574 sentences (19%) with more than one aspect category 

         167 sentences (5%) with aspect categories having different polarities 

319!

432!

597!

 

price 

ambience 

service 

miscellaneous 

food 

positive 
negative 
neutral 
conflict 

1131!

1233!

0 

200 

400 

600 

800 

1000 

1200 

1400 

number of examples!

         test 

       800 sentences with at least one aspect category 
       189 sentences (24%) with more than one aspect category 

         42 sentences (5%) with aspect categories having different polarities 

156 

aspect category polarity: results 

accuracy 

90  82.93!
80 

unconstrained"

70 

60 

50 

40 

30 

20 

10 

0 

157 

aspect category polarity: feature 
contributions 
accuracy 
86 

84 

82 

80 

78 

76 

74 

72 

70 

68 

all 

all     lexicons - 

clusters 

all - clusters  all - lexicons  all     manual 

lexicons 

all     tweet 
lexicons 

all     yelp 
lexicons 

all - aspect 
term features 

158 

aspect category polarity: summary 

         the system ranked first among 20 teams 
         most useful features:  

       sentiment lexicons, especially in-domain automatic  lexicon 

159 

key ideas from other top systems 

        

 xrce  
       built around sentiment-oriented dependency parser 

         parse trees were annotated with sentiment information. 

       rules are used to link sentiment on terms based on the 

parse. 

       hybridizing rule based parse with machine learning. 

         unitor 

       linear combination of different kernels  
       lsa features obtained on word-context matrix derived from 

a large-scale in-domain unlabeled corpus 

         uwb 

       use topic-modeling features obtained with latent dirichlet 

allocation (lda)     

 

160 

overview of sentiment 
analysis systems 
       rule-based systems 
       conventional statistical systems 
       deep-learning-based models 

       sentiment id27 
       sentiment composition 

161 

general id27 

         id27: representation of lexical items as points in 

a real-valued (low-dimensional) vector space. 
it is often computed by compressing a larger matrix to smaller 
one. 

        

new 
old 
1 
good  1 
bad 
2 
    

6 

2 

2 

3 

1 

1 

1 

6 

4 

1 

1 

2 

9 

4 

7 

1 

3 

3      

2      

    

    

    

-0.03  0.5 

-0.04  0.3 

1.4 

1.3 

0 

0 

0 

0 

2.5 

3.6 

new 
old 
good 
bad 
    

       keep (semantically or syntactically) close items in the 

original matrix/space to be close in the embedding space.  

162 

general id27 

         id27: representation of lexical items as points in 

a real-valued (low-dimensional) vector space. 
it is often computed by compressing a larger matrix to smaller 
one. 

        

new 
old 
1 
good  1 
bad 
2 
    

6 

2 

2 

3 

1 

1 

1 

6 

4 

1 

1 

2 

9 

4 

7 

1 

3 

3      

2      

    

    

    

-0.03  0.5 

-0.04  0.3 

1.4 

1.3 

0 

0 

0 

0 

2.5 

3.6 

new 
old 
good 
bad 
    

there are many ways to construct this matrix,  
e.g., using word-context or word-document counts.  

163 

general id27 

         id27: representation of lexical items as points in 

a real-valued (low-dimensional) vector space. 
it is often computed by compressing a larger matrix to smaller 
one. 

        

new 
old 
1 
good  1 
bad 
2 
    

6 

2 

2 

3 

1 

1 

1 

6 

4 

1 

1 

2 

9 

4 

7 

1 

3 

3      

2      

    

    

    

-0.03  0.5 

-0.04  0.3 

1.4 

1.3 

0 

0 

0 

0 

2.5 

3.6 

new 
old 
good 
bad 
    

also, there are many ways to compress the matrix,  
e.g., pca, lle, sne, c&w, and id97.  

164 

general id27 

165 

sentiment id27 

         coooolll (tang et al., 2014): adapt syntactic/semantic word 

embedding to consider sentiment information. 
       motivation: word new and old often have similar syntactic/
semantic embedding but should have different sentiment 
embedding. 

       approach: a linear modification of the objective functions. 

166 

results 

macro-f scores on five test sets. t1 is livejournal2014, 
t2 is sms2013, t3 is twitter2013, t4 is twitter2014, 
and t5 is twitter2014sarcasm. 

167 

sentiment composition 

        

in addition to obtaining sentiment embedding, composing 
word sentiment to analyze larger pieces of text (e.g., 
sentences) is another important problem. 

         most work we have discussed so far is based on bag-of-

words or bag-of-ngrams assumption. 

         more principled models    

168 

sentiment composition 

         socher et al. (2013) proposed a id56 to 

compose sentiment of a sentence. 

169 

sentiment composition 

         tensors are critical in capturing interaction between two 
words/phrases being composed (e.g., a negator and the 
phrase it modifies.) 

         standard forward/backward propagation was adapted to 
learn the weights/parameters; main difference lies in the 
tensor part (v in the figure.) 

170 

results 

accuracy for fine grained (5-class) and binary predictions 
at the sentence level (root) and for all nodes. 

171 

overview of sentiment 
analysis systems 
       rule-based systems 
       conventional statistical systems 
       deep-learning-based models 

172 

summary 

173 

summary 

         id31 subsumes several different problems 

       important to be aware of the problem pertinent to your task, 

and the annotation instructions used to create the data 

         id31 relevant to many domains 

       not just customer reviews 

         several shared tasks exist 

       source of data and benchmarks 

         statistical machine learning methods quite popular 
         term-sentiment associations are a key source of information 

       can obtain this from training data (ngrams) 
       more can be obtained from quasi-annotations, such as 

from emoticons and hashtags 

174 

summary (continued) 

         other significant features include those from: 

       handling negation 
       handling semantic composition 

         building a competition system involves: 

       careful evaluation of usefulness of features 
       trying various parameters pertaining to both the learning 

algorithm and the features 

       keep features that obtain improvements consistently on 

many datasets 

       more labeled data trumps smarter algorithms 

 

175 

future directions 

         semantic roles of sentiment 

         sentiment embedding and composition 

         sarcasm, irony, and metaphor 

         id31 in non-english languages 

         detecting stance, framing, and spin 

         detecting trends and significant changes in sentiment 

distributions 

176 

future directions (continued) 

         detecting intensity of sentiment 

 

         developing better sentiment models for negation, intensifiers, 

and modality 

         developing better emotion models 

         developing applications for public health, business, social 

welfare, and for literary and social scientists 

177 

semeval-2015, sentiment tasks 

         task 9: clipeval implicit polarity of events 

       explicit and implicit, pleasant and unpleasant, events  

         task 10: id31 in twitter 

       repeat of 2013 and 2014 task 
       more subtasks 

         task 11: id31 of figurative language in twitter 

       metaphoric and ironic tweets 
       intensity of sentiment 

         task 12: aspect based id31 

       repeat of 2014 task 
       id20 task 
 

178 

 
task 9: clipeval implicit polarity of events 

         explicit pleasant event   

yesterday i met a beautiful woman

         explicit  unpleasant event 

i ate a bad mcrib this week

implicit pleasant event    

last night i    nished the sewing project

implicit unpleasant event   

        

        

today, i lost a bet with my grandma



a dataset of first person sentences annotated as instantiations of 
psychologically grounded pleasant and unpleasant events 
(macphillamy and lewinsohn 1982): 


after that, i started to color my hair and polish my nails.
 positive, personal_care 

when  swedish security police saepo arrested me in 2003  

i was asked questions about this man.
 negative, legal_issue 

  



 

179 

task 10: id31 in twitter  

         subtask a: contextual polarity disambiguation  

       given a message containing a marked instance of a word or phrase, 
determine whether that instance is positive, negative or neutral in that 
context. 

         subtask b: message polarity classification 

       given a message, classify whether the message is of positive, negative, 

or neutral sentiment.  

         subtask cnew: topic-based message polarity classification 

       given a message and a topic, classify whether the message is of positive, 

negative, or neutral sentiment towards the given topic.  

         subtask dnew: detecting trends towards a topic 

       given a set of messages on a given topic from the same period of time, 

determine whether the dominant sentiment towards the target topic in 
these messages is (a) strongly positive, (b) weakly positive, (c) neutral, (d) 
weakly negative, or (e) strongly negative. 

         subtask enew: determining degree of prior polarity 

       given a word or a phrase, provide a score between 0 and 1 that is 

indicative of its strength of association with positive sentiment.  

180 

task 11: id31 of figurative 
language in twitter 
         twitter is rife with ironic, sarcastic and figurative language.  
         how does this creativity impact the perceived affect? 
         do conventional sentiment techniques need special 
augmentations to cope with this non-literal content?   
       this is not an irony detection task per se, but a sentiment 

analysis task in the presence of irony. 

         task 11 will test the capability of sentiment systems on a 

collection of tweets that have a high concentration of 
sarcasm, irony and metaphor.  
       tweets are hand-tagged on a sentiment scale ranging from 

-5 (very negative meaning) to +5 (very positive).  

181 

task 12: aspect based id31 

         subtask 1 

       a set of quintuples has to be extracted from a collection of 

opinionated documents 
         opinion target 
         target category 
         target polarity 
             from    and    to    that indicate the opinion target   s starting 

and ending offset in the text 

         subtask 2 

       same as subtask 1, but on new unseen domain 
       no training data from the target domain 

182 

other sentiment challenges 

         kaggle competition on id31 on movie reviews 
       website: http://www.kaggle.com/c/sentiment-analysis-on-movie-

reviews 

       deadline: 11:59 pm, saturday 28 february 2015 utc 
       # of teams: 395 
       the sentiment labels are: 

         0 - negative 
         1 - somewhat negative 
         2 - neutral 
         3 - somewhat positive 
         4 - positive 

183 

email 

saif m. mohammad  
saif.mohammad@nrc-cnrc.gc.ca  

 

xiaodan zhu 
xiaodan.zhu@nrc-cnrc.gc.ca  

 

svetlana kiritchenko 
svetlana.kiritchenko@nrc-cnrc.gc.ca  
 

184 

references 

         agarwal, a., xie, b., vovsha, i., rambow, o., & passonneau, r. (2011). id31 of twitter 
data. in proceedings of the workshop on languages in social media, lsm'11, pp. 30--38, portland, 
oregon. 

         aisopos, f., papadakis, g., tserpes, k., & varvarigou, t. (2012). textual and contextual patterns for 
id31 over microblogs. in proceedings of the 21st international conference on world 
wide web companion, www '12 companion, pp. 453--454, new york, ny, usa. 

         almquist, e., & lee, j. (2009). what do customers really want? harvard business review. 
         baccianella, s., esuli, a., & sebastiani, f. (2010). sentiid138 3.0: an enhanced lexical resource for 
id31 and opinion mining. in proceeding of the 7th international conference on language 
resources and evaluation, vol. 10 of lrec '10, pp. 2200--2204. 

         bakliwal, a., arora, p., madhappan, s., kapre, n., singh, m., & varma, v. (2012). mining sentiments 
from tweets. in proceedings of the 3rd workshop on computational approaches to subjectivity and 
id31, wassa '12, pp. 11--18, jeju, republic of korea. 

         becker, l., erhart, g., skiba, d., & matula, v. (2013). avaya: id31 on twitter with self-

training and polarity lexicon expansion. in proceedings of the 7th international workshop on semantic 
evaluation (semeval 2013), pp. 333{340, atlanta, georgia, usa. 

         bellegarda, j. (2010). emotion analysis using latent aective folding and embedding. in proceedings of 
the naacl-hlt 2010 workshop on computational approaches to analysis and generation of emotion 
in text, los angeles, california. 

         anil bandhakavi; nirmalie wiratunga; deepak p; stewart massie. generating a word-emotion lexicon 

from #emotional tweets. semeval-2014. 

         boucher, j. d., & osgood, c. e. (1969). the pollyanna hypothesis. journal of verbal learning and 

verbal behaviour, 8, 1--8. 

185 

         boucouvalas, a. c. (2002). real time text-to-emotion engine for expressive internet communication. 

emerging communication: studies on new technologies and practices in communication, 5, 305-318. 

         brody, s., & diakopoulos, n. (2011). cooooooooooooooollllllllllllll!!!!!!!!!!!!!!: using word lengthening to 

detect sentiment in microblogs. in proceedings of the conference on empirical methods in natural 
language processing, emnlp '11, pp. 562--570, stroudsburg, pa, usa. 

         chang, c.-c., & lin, c.-j. (2011). libid166: a library for support vector machines. acm transactions on 

intelligent systems and technology, 2 (3), 27:1--27:27. 
ilia chetviorkin, natalia loukachevitch. two-step model for sentiment lexicon extraction from twitter 
streams. proceedings of the 5th workshop on computational approaches to subjectivity, sentiment 
and social media analysis, 2014, pages 67-72, baltimore, maryland. 

        

         chen, yanqing, and steven skiena. "building sentiment lexicons for all major languages." malay 

2934: 0-39. 

         choi, y., & cardie, c. (2008). learning with id152 as structural id136   for 

subsentential id31. in proceedings of the conference on empirical   methods in natural 
language processing, emnlp '08, pp. 793--801. 

         choi, y., & cardie, c. (2010). hierarchical sequential learning for extracting opinions and their 

attributes. in proceedings of the annual meeting of the association for computational linguistics, acl 
'10, pp. 269--274. 

         davidov, d., tsur, o., & rappoport, a. (2010). enhanced sentiment learning using twitter   hashtags 

and smileys. in proceedings of the 23rd international conference on computational linguistics: 
posters, coling '10, pp. 241--249, beijing, china. 

         esuli, a., & sebastiani, f. (2006). sentiid138: a publicly available lexical resource   for opinion 
mining. in in proceedings of the 5th conference on language resources   and evaluation, lrec '06, 
pp. 417--422. 
francisco, v., & gervas, p. (2006). automated mark up of a ective information in english texts. in 
sojka, p., kopecek, i., & pala, k. (eds.), text, speech and dialogue, vol. 4188 of lecture notes in 
computer science, pp. 375{382. springer berlin / heidelberg.  

        

         genereux, m., & evans, r. p. (2006). distinguishing a ective states in weblogs. in pro-ceedings of the 

aaai spring symposium on computational approaches to analysing weblogs, pp. 27{29, stanford, 
california. 

186 

         gimpel, k., schneider, n., o'connor, b., das, d., mills, d., eisenstein, j., heilman, m., yogatama, d., 

flanigan, j., & smith, n. a. (2011). part-of-speech tagging for twitter: annotation, features, and 
experiments. in proceedings of the annual meeting of the association for computational linguistics, 
acl '11. 

         go, a., bhayani, r., & huang, l. (2009). twitter sentiment classi cation using distant supervision. tech. 

rep., stanford university. 

         hanley, j., & mcneil, b. (1982). the meaning and use of the area under a receiver oper-ating 

characteristic (roc) curve. radiology, 143, 29-36.  

         hatzivassiloglou, v., & mckeown, k. r. (1997). predicting the semantic orientation of adjectives. in 

proceedings of the 8th conference of european chapter of the association for computational 
linguistics, eacl '97, pp. 174{181, madrid, spain. 

        

        

         hu, m., & liu, b. (2004). mining and summarizing customer reviews. in proceedings of the 10th acm 
sigkdd international conference on knowledge discovery and data mining, kdd '04, pp. 168-177, 
new york, ny, usa. acm. 
jia, l., yu, c., & meng, w. (2009). the e ect of negation on id31 and retrieval e 
ectiveness. in proceedings of the 18th acm conference on information and knowledge management, 
cikm '09, pp. 1827-1830, new york, ny, usa. acm. 
jiang, l., yu, m., zhou, m., liu, x., & zhao, t. (2011). target-dependent twitter senti-ment classi 
cation. in proceedings of the 49th annual meeting of the association for computational linguistics, 
acl '11, pp. 151-160. 
johansson, r., & moschitti, a. (2013). relational features in  ne-grained opinion analysis. 
computational linguistics, 39 (3), 473-509. 
john, d., boucouvalas, a. c., & xu, z. (2006). representing emotional momentum within expressive 
internet communication. in proceedings of the 24th international conference on internet and 
multimedia systems and applications, pp. 183-188, anaheim, ca. acta press.  
jurgens, d., mohammad, s. m., turney, p., & holyoak, k. (2012). semeval-2012 task 2: measuring 
degrees of relational similarity. in proceedings of the 6th international workshop on semantic 
evaluation, semeval '12, pp. 356{364, montreal, canada. association for computational linguistics. 

        

        

        

187 

         kennedy, a., & inkpen, d. (2005). sentiment classi cation of movie and product reviews using 

contextual valence shifters. in proceedings of the workshop on the analysis of informal and formal 
information exchange during negotiations, ottawa, ontario, canada. 

         kennedy, a., & inkpen, d. (2006). sentiment classi cation of movie reviews using contextual valence 

shifters. computational intelligence, 22 (2), 110-125. 

         kiritchenko, s., zhu, x., cherry, c., & mohammad, s. (2014). nrc-canada-2014: detecting aspects 

and sentiment in customer reviews. in proceedings of the international workshop on semantic 
evaluation, semeval '14, dublin, ireland. 

         kunneman, f. a., c. c. liebrecht, and a. p. j. van den bosch. the (un) predictability of emotional 

hashtags in twitter. 2014. 

         svetlana kiritchenko, xiaodan zhu and saif mohammad. id31 of short informal texts. 

journal of artificial intelligence research, volume 50, pages 723-762. 

         kouloumpis, e., wilson, t., & moore, j. (2011). twitter id31: the good the bad and the 

        

        

        

        

omg!. in proceedings of the 5th international aaai conference on weblogs and social media. 
lapponi, e., read, j., & ovrelid, l. (2012). representing and resolving negation for senti-ment 
analysis. in vreeken, j., ling, c., zaki, m. j., siebes, a., yu, j. x., goethals, b., webb, g. i., & wu, x. 
(eds.), icdm workshops, pp. 687-692. ieee computer society. 
li, j., zhou, g., wang, h., & zhu, q. (2010). learning the scope of negation via shallow se-mantic 
parsing. in proceedings of the 23rd international conference on computational linguistics, coling 
'10, pp. 671-679, beijing, china. 
liu, b., & zhang, l. (2012). a survey of opinion mining and id31. in aggarwal, c. c., & 
zhai, c. (eds.), mining text data, pp. 415-463. springer us. 
liu, h., lieberman, h., & selker, t. (2003). a model of textual a ect sensing using real-world 
knowledge. in proceedings of the 8th international conference on intelligent user interfaces, iui '03, 
pp. 125-132, new york, ny. acm. 
louviere, j. j. (1991). best-worst scaling: a model for the largest di erence judgments. working paper. 

        
         raheleh makki, stephen brooks and evangelos e. milios, context-specific sentiment lexicon 

expansion via minimal user interaction. 2014. 

188 

         mart nez-camara, e., mart n-valdivia, m. t., ure~nalopez, l. a., & montejoraez, a. r. (2012). 

id31 in twitter. natural language engineering, 1-28. 

         mihalcea, r., & liu, h. (2006). a corpus-based approach to nding happiness. in pro-ceedings of the 

aaai spring symposium on computational approaches to analysing weblogs, pp. 139-144. aaai 
press. 

         mohammad, s. (2012). #emotional tweets. in proceedings of the first joint conference on lexical and 

computational semantics, *sem '12, pp. 246-255, montreal, canada. association for computational 
linguistics. 

         mohammad, s., dunne, c., & dorr, b. (2009). generating high-coverage semantic ori-entation lexicons 
from overtly marked words and a thesaurus. in proceedings of the conference on empirical methods in 
natural language processing: volume 2, emnlp '09, pp. 599-608. 

         mohammad, s. m., & kiritchenko, s. (2014). using hashtags to capture ne emotion categories from 

tweets. to appear in computational intelligence. 

         mohammad, s. m., kiritchenko, s., & martin, j. (2013). identifying purpose behind elec-toral tweets. in 
proceedings of the 2nd international workshop on issues of sentiment discovery and opinion mining, 
wisdom '13, pp. 1-9. 

         mohammad, s. m., & turney, p. d. (2010). emotions evoked by common words and phrases: using 

mechanical turk to create an emotion lexicon. in proceedings of the naacl-hlt workshop on 
computational approaches to analysis and generation of emotion in text, la, california. 

         mohammad, s. m., & yang, t. w. (2011). tracking sentiment in mail: how genders di er on emotional 

axes. in proceedings of the acl workshop on computational approaches to subjectivity and 
id31, wassa '11, portland, or, usa. 

         neviarouskaya, a., prendinger, h., & ishizuka, m. (2011). a ect analysis model: novel rule-based 

approach to a ect sensing from text. natural language engineering, 17, 95-135. 

         orme, b. (2009). maxdi analysis: simple counting, individual-level logit, and hb. saw-tooth software, 

inc. 

         pak, a., & paroubek, p. (2010). twitter as a corpus for id31 and opinion mining. in 

proceedings of the 7th conference on international language resources and evaluation, lrec '10, 
valletta, malta. european language resources association (elra). 

189 

         pang, b., & lee, l. (2005). seeing stars: exploiting class relationships for sentiment cate-gorization 

with respect to rating scales. in proceedings of the annual meeting of the association for 
computational linguistics, acl '05, pp. 115-124. 

         pang, b., & lee, l. (2008). opinion mining and id31. foundations and trends in 

information retrieval, 2 (1-2), 1-135. 

         pang, b., lee, l., & vaithyanathan, s. (2002). thumbs up?: sentiment classi cation using machine 
learning techniques. in proceedings of the conference on empirical methods in natural language 
processing, emnlp '02, pp. 79-86, philadelphia, pa. 

         polanyi, l., & zaenen, a. (2004). contextual valence shifters. in exploring attitude and a ect in text: 

theories and applications (aaai spring symposium series). 

         porter, m. (1980). an algorithm for su  x stripping. program, 3, 130-137. 
         proisl, t., greiner, p., evert, s., & kabashi, b. (2013). klue: simple and robust methods for polarity 
classi cation. in proceedings of the 7th international workshop on se-mantic evaluation (semeval 
2013), pp. 395-401, atlanta, georgia, usa. association for computational linguistics. 

         reckman, h., baird, c., crawford, j., crowell, r., micciulla, l., sethi, s., & veress, f. (2013). teragram: 

rule-based detection of sentiment phrases using sas id31. in proceedings of the 7th 
international workshop on semantic evaluation (semeval 2013), pp. 513-519, atlanta, georgia, usa. 
association for computational linguistics. 

         hassan saif, miriam fernandez, yulan he, and harith alani (2014). senticircles for contextual and 

conceptual semantic id31 of twitter. in proceeding of the 11th extended semantic web 
conference (eswc), crete, greece. 

         sauper, c., & barzilay, r. (2013). automatic aggregation by joint modeling of aspects and values. 

journal of arti cial intelligence research, 46, 89-127. 

         sheng huang and zhendong niu and chongyang shi. automatic construction of domain-specific 
sentiment lexicon based on constrained label propagation. knowledge-based systems, 2014, pp.
191-200.  

         shi feng, kaisong song, daling wang, ge yu. a word-emoticon mutual reinforcement ranking model 
for building sentiment lexicon from massive collection of microblogs. world wide web. pg 1-19. 2014. 

190 

        

        

        

         socher, r., huval, b., manning, c. d., & ng, a. y. (2012). semantic compositionality through recursive 

matrix-vector spaces. in proceedings of the conference on empirical methods in natural language 
processing, emnlp '12. association for computational linguistics. 

         socher, r., perelygin, a., wu, j. y., chuang, j., manning, c. d., ng, a. y., & potts, c. (2013). 

recursive deep models for semantic compositionality over a sentiment treebank. in proceedings of the 
conference on empirical methods in natural language processing, emnlp '13. association for     
computational linguistics. 

         stone, p., dunphy, d. c., smith, m. s., ogilvie, d. m., & associates (1966). the general inquirer: a 

computer approach to content analysis. the mit press. 
taboada, m., brooke, j., to loski, m., voll, k., & stede, m. (2011). lexicon-based methods for 
id31. computational linguistics, 37 (2), 267-307. 

         duyu tang, furu wei, bing qin, ming zhou, ting liu. building large-scale twitter-specific sentiment 

lexicon : a representation learning approach, coling 2014.  

         duyu tang, furu wei, nan yang, bing qin, ting liu, ming zhou: proceedings of the 8th international 

workshop on semantic evaluation (semeval 2014), pages 208   212, 

         dublin, ireland, august 23-24, 2014. 
        

thelwall, m., buckley, k., & paltoglou, g. (2011). sentiment in twitter events. journal of the american 
society for information science and technology, 62 (2), 406-418. 
turney, p. (2001).  mining the web for synonyms: pmi-ir versus lsa on toefl.  in proceedings of the 
twelfth european conference on machine learning, pp. 491-502, freiburg, germany. 
turney, p., & littman, m. l. (2003). measuring praise and criticism: id136 of semantic orientation 
from association. acm transactions on information systems, 21 (4). 

         wiegand, m., balahur, a., roth, b., klakow, d., & montoyo, a. (2010). a survey on the role of negation 

in id31. in proceedings of the workshop on negation and speculation in natural 
language processing, nesp-nlp '10, pp. 60-68, stroudsburg, pa, usa. association for computational 
linguistics. 

         wilson, t., kozareva, z., nakov, p., rosenthal, s., stoyanov, v., & ritter, a. (2013). semeval-2013 

task 2: id31 in twitter. in proceedings of the interna-tional workshop on semantic 
evaluation, semeval '13, atlanta, georgia, usa. 

191 

         wilson, t., wiebe, j., & ho mann, p. (2005). recognizing contextual polarity in phrase-level sentiment 
analysis. in proceedings of the conference on human language tech-nology and empirical methods 
in natural language processing, hlt '05, pp. 347-354, stroudsburg, pa, usa. association for 
computational linguistics. 

         yang, b., & cardie, c. (2013).  joint id136 for  ne-grained opinion extraction.  in proceedings of the 

annual meeting of the association for computational linguistics, acl '13. 

         yeh, a. (2000). more accurate tests for the statistical signi cance of result di erences. in proceedings of 

the 18th conference on computational linguistics - volume 2, coling '00, pp. 947-953, stroudsburg, 
pa, usa. association for computational linguistics.  

         xiaodan zhu, hongyu guo, saif mohammad and svetlana kiritchenko. an empirical study on the 

effect of negation words on sentiment. in proceedings of the 52nd annual meeting of the association 
for computational linguistics, june 2014, baltimore, md. 

 
semeval proceedings 

 

         semeval-2013 proceedings are available here: 
       http://www.aclweb.org/anthology/siglex.html#2013_1 

 

         semeval-2014 proceedings are available here: 
       http://alt.qcri.org/semeval2014/cdrom/ 
 

192 

