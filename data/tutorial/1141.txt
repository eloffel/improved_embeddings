embedding methods for nlp

part 1: unsupervised and supervised embeddings

jason weston & antoine bordes

facebook ai research

emnlp tutorial     october 29, 2014

1 / 69

what is a id27?

suppose you have a dictionary of words.

the i th word in the dictionary is represented by an embedding:

wi 2 rd

i.e. a d-dimensional vector, which is learnt!

d typically in the range 50 to 1000.
similar words should have similar embeddings (share latent features).
embeddings can also be applied to symbols as well as words (e.g.
freebase nodes and edges).
discuss later: can also have embeddings of phrases, sentences,
documents, or even other modalities such as images.

2 / 69

learning an embedding space

example of embedding of 115 countries (bordes et al.,    11)

3 / 69

main methods we highlight, ordered by date.

id45 (deerwester et al.,    88).
neural net language models (nn-lms) (bengio et al.,    06)
convolutional nets for tagging (senna) (collobert & weston,    08).
supervised semantic indexing (bai et al,    09).
wsabie (weston et al.,    10).
recurrent nn-lms (mikolov et al.,    10).
recursive nns (socher et al.,    11).
id97 (mikolov et al.,    13).
paragraph vector (le & mikolov,    14).
overview of recent applications.

4 / 69

main methods we highlight, ordered by topic.
embeddings for ranking and retrieval:

id45 (deerwester et al.,    88).
supervised semantic indexing (bai et al,    09).
wsabie (weston et al.,    10).

embeddings for id38 (useful for speech, translation, . . . ):

neural net language models (nn-lms) (bengio et al.,    06)
recurrent nn-lms (mikolov et al.,    10).
id97 (mikolov et al.,    13).

embeddings for supervised prediction tasks (pos, chunk, ner, srl,
sentiment, etc.):

convolutional nets for tagging (senna) (collobert & weston,    08).
recursive nns (socher et al.,    11).
paragraph vector (le & mikolov,    14).

5 / 69

main methods we highlight, ordered by topic.

embeddings for ranking and retrieval:

id45 (deerwester et al.,    88).
supervised semantic indexing (bai et al,    09).
wsabie (weston et al.,    10).

embeddings for id38 (useful for speech, translation, . . . ):

neural net language models (nn-lms) (bengio et al.,    06)
recurrent nn-lms (mikolov et al.,    10).
id97 (mikolov et al.,    13).

embeddings for supervised prediction tasks (pos, chunk, ner, srl,
sentiment, etc.):

convolutional nets for tagging (senna) (collobert & weston,    08).
recursive nns (socher et al.,    11).
paragraph vector (le & mikolov,    14).

6 / 69

ranking and retrieval: the goal

we want to learn to match a query (text) to a target (text).

many classical supervised ranking methods use hand-coded features.

methods like lsi that learn from words are unsupervised.

supervised semantic indexing (ssi) uses supervised learning from text

only:

bai et al, learning to rank with (a lot of) word features. journal of

information retrieval,    09.

outperforms existing methods (on words) like tfidf, lsi or a

(supervised) margin ranking id88 baseline.

7 / 69

basic bag-o   -words

bag-of-words + cosine similarity:
each doc. {dt}n
similarity with query q is: f (q, d) = q>d

t=1     rd is a normalized bag-of-words.

doesn   t deal with synonyms: bag vectors can be orthogonal

no machine learning at all

8 / 69

id45 (lsi)

learn a linear embedding  (di ) = udi via a reconstruction objective.
rank with: f (q, d) = q>u>ud =  (q)> (di ) 1.

uses    synonyms   : low-dimensional latent    concepts   .

unsupervised machine learning: useful for goal?

1f (q, d) = q>(u>u +    i )d gives better results.

also, usually normalize this ! cosine similarity.

9 / 69

supervised semantic indexing (ssi )

basic model: rank with

f (q, d) = q>wd = pdi,j=1 qi wij dj

i.e. learn weights of polynomial terms between documents.
learn w 2 rd   d (huge!) with click-through data or other labels.

uses    synonyms   

supervised machine learning: targeted for goal

too big/slow?! solution = constrain w :

low rank ! embedding model!

10 / 69

ssi: why is this a good model?

classical bag-of-words doesnt work when there are few matching terms:
q=(kitten, vet, nyc)
d=(cat, veterinarian, new, york)

method q>wd learns that e.g. kitten and cat are highly related.

e.g. if i is the index of kitten and j is the index of cat, then wij > 0
after training.

11 / 69

ssi: why the basic model sucks

w is big : 3.4gb if d = 30000, 14.5tb if d = 2.5m.
slow: q>wd computation has mn computations qj wij di , where q
and d have m and n nonzero terms.

or one computes v = q>w once, and then vd for each document.
classical speed where query has d terms, assuming w is dense !
still slow.
one could minimize ||w||1 and attempt to make w sparse. then at
most mp times slower than classical model (with p nonzeros in a
column.)

12 / 69

ssi improved model: low rank w

constrain w :

w = u>v + i .

u and v are n    d matrices ! smaller
low dimensional    latent concept    space like lsi (same speed).
di   erences: supervised, asymmetric, learns with i .

variants:

w = i : bag-of-words again.
w = d, reweighted bag-of-words related to [grangier and bengio,
2005].
w = u>u + i : symmetric.

13 / 69

ssi: training via maximizing auc

given a set of tuples r with a query q, a related document d + and
an unrelated (or lower ranked) document d .
we would like f (q, d +) > f (q, d ).
minimize margin ranking loss [herbrich et al., 2000]:

x(q,d +,d )2r

max(0, 1   f (q, d +) + f (q, d )).

learning algorithm stochastic id119: fast & scalable.

iterate

sample a triplet (q, d +, d ),
update w   w     @

@w max(0, 1   f (q, d +) + f (q, d )).

other options: batch gradient, parallel sgd (hogwild), adagrad . . .

14 / 69

training: setting hyperparameters

the following hyperparameters can be tuned for training:

the initial random weights of the embedding vectors:

e.g. use (mean 0, variance 1pd

) .

the learning rate (typically: 0.0001, 0.001, 0.01, 0.1, . . . ).

the value of the margin (e.g.: 1, 0.5, 0.2, 0.1, . . . ).

restricting the norm of embeddings:

||ui||     c , ||vi||     c (e.g.: c=1).

all these parameters are relative to each other, e.g. a larger margin might
need larger initial weights and learning rate.
typically, we    x the initialization and norm, and try di   erent values of
margin and learning rate. this can make big di   erences in performance.

15 / 69

prior work: summary of learning to rank

[grangier & bengio,    06] used similar methods to basic ssi for retrieving
images.
[goel, langord & strehl,    08] used hash kernels (vowpal wabbit) for advert
placement.
main di   erence: ssi uses low rank on w .
id166 [joachims, 2002] and nn ranking methods [burges, 2005] .
use hand-coded features: title, body, url, search rankings,. . . (don   t use
words)
(e.g. burges uses 569 features in all).
in contrast ssi uses only the words and trains on huge feature sets.
several works on optimizing di   erent id168s (map, roc, ndcg):
[cao, 2008], [yu, 2007], [qin, 2006],. . . .

lots of stu    for    metric learning    problem as well..

one could also add features + new loss to this method ..

16 / 69

experimental comparison

wikipedia

1,828,645 documents. 24,667,286 links.
split into 70% train, 30% test.

pick random doc. as query, then rank other docs.
docs that are linked to it should be highly ranked.
two setups:

(i) whole document is used as query;
(ii) 5,10 or 20 words are picked to mimic keyword search.

17 / 69

wikipedia experiments: document retrieval performance
experiments on wikipedia, which contains 1.8m documents: retrieval task
using the link structure and separated the data into 70% for training and
30% for test.

document based retrieval:

algorithm
tfidf
   lsi + (1      )tfidf
linear id166 ranker
hash kernels +    i
ssi

rank-loss

0.842%
0.721%
0.410%
0.322%
0.158%

map
0.432  0.012
0.433
0.477
0.492
0.547  0.012

p10
0.193
0.193
0.212
0.215
0.239  0.008

k-keywords based retrieval:

k = 5: algorithm

tfidf
   lsi + (1      )tfidf
ssi

params

0

200d+1
400d

rank
map
21.6% 0.047
14.2% 0.049
4.37% 0.166

p@10
0.023
0.023
0.083

18 / 69

scatter plots: ssi vs. tfidf and lsi

(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:2)(cid:8)(cid:3)(cid:9)(cid:2)(cid:5)(cid:10)(cid:8)(cid:2)(cid:11)(cid:12)(cid:13)(cid:12)(cid:14)(cid:4)(cid:5)(cid:15)(cid:14)(cid:8)(cid:5)(cid:16)(cid:17)(cid:18)(cid:19)(cid:17)(cid:5)(cid:3)(cid:4)(cid:20)(cid:5)(cid:21)(cid:21)(cid:18)(cid:22)(cid:23)(cid:24)(cid:25)(cid:26)(cid:18)

(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:2)(cid:8)(cid:3)(cid:9)(cid:2)(cid:5)(cid:10)(cid:8)(cid:2)(cid:11)(cid:12)(cid:13)(cid:12)(cid:14)(cid:4)(cid:5)(cid:15)(cid:14)(cid:8)(cid:5)(cid:16)(cid:17)(cid:18)(cid:5)(cid:3)(cid:4)(cid:19)(cid:5)(cid:17)(cid:17)(cid:18)(cid:20)(cid:21)(cid:22)(cid:23)(cid:24)(cid:18)

(cid:31)

(cid:24)(cid:27)(cid:30)(cid:29)

(cid:17)
(cid:19)
(cid:18)
(cid:17)
(cid:16)

(cid:24)(cid:27)(cid:29)

(cid:24)(cid:27)(cid:28)(cid:29)

(cid:24)

(cid:24)

(cid:24)(cid:27)(cid:28)(cid:29)

(cid:24)(cid:27)(cid:29)

(cid:24)(cid:27)(cid:30)(cid:29)

(cid:31)

(cid:21)(cid:21)(cid:18)(cid:22)(cid:23)(cid:24)(cid:25)(cid:26)(cid:18)

(a)

(cid:37)

(cid:25)

(cid:1)
(cid:38)
(cid:36)
(cid:20)
(cid:34)
(cid:35)
(cid:18)
(cid:34)
(cid:33)
(cid:28)
(cid:32)
(cid:3)
(cid:27)
(cid:26)
(cid:3)
(cid:31)
(cid:30)
(cid:29)
(cid:24)
(cid:23)
(cid:22)
(cid:21)
(cid:20)
(cid:18)
(cid:17)
(cid:16)
(cid:28)
(cid:3)
(cid:27)
(cid:26)
(cid:3)

(cid:25)

(cid:30)

(cid:22)(cid:37)(cid:39)(cid:38)

(cid:22)(cid:37)(cid:38)

(cid:22)(cid:37)(cid:36)(cid:38)

(cid:22)

(cid:22)

(cid:22)(cid:37)(cid:36)(cid:38)

(cid:22)(cid:37)(cid:38)

(cid:22)(cid:37)(cid:39)(cid:38)

(cid:30)

(cid:17)(cid:17)(cid:18)(cid:20)(cid:21)(cid:22)(cid:23)(cid:24)(cid:18)

(b)

figure : scatter plots of average precision for 500 documents:
(a) ssi vs. tfidf, (b) ssi vs.    lsi + (1      ) tfidf.

19 / 69

experiments: cross-language retrieval
retrieval experiments using a query document in japanese, where the task
is to retrieve documents in english (using link structure as ground truth).

ssi can do this without doing a translation step    rst as it learns to map
the two languages together in the embedding space.

algorithm
tfidfengeng (google translated queries)
   lsiengeng +(1      )tfidfengeng
   cl-lsijapeng +(1      )tfidfengeng
ssiengeng
ssijapeng
   ssijapeng + (1      )tfidfengeng
   ssijapeng + (1      )ssiengeng

rank-loss

4.78%
3.71%
3.31%
1.72%
0.96%
0.75%
0.63%

map
0.319
0.300
0.275
0.399
0.438
0.493
0.524

p10
0.259
0.253
0.212
0.325
0.351
0.377
0.386

some recent related translation-based embeddings:
(hermann & blunsom, iclr    14) and (mikolov et al.,    13).

20 / 69

wsabie (weston, bengio & usunier,    10)

extension to ssi, also embeds objects other than text, e.g. images.
warp id168 that optimizes precision@k.

21 / 69

joint item-item embedding model
l.h.s: image, query string or user pro   le (depending on the task)

 lhs (x) = u x (x) : rdx ! r100.

r.h.s: document, image, video or annotation (depending on the task)

 rhs (y ) = v  y (y ) : rdy ! r100.

this model again compares the degree of match between the l.h.s and
r.h.s in the embedding space:

fy (x) = sim( lhs (x),  rhs (y )) =  x (x)>u>v  y (y )

also constrain the weights (regularize):

||ui||2     c ,

i = 1, . . . , dx ,

||vi||2     c ,

i = 1, . . . , dy .

22 / 69

ranking annotations: auc is suboptimal

classical approach to learning to rank is maximize auc by minimizing:

xx xy x  y6=y

max(0, 1 + f  y (x)   fy (x))

problem: all pairwise errors are considered the same, it counts the
number of ranking violations.
example:
hellofunction 1: true annotations ranked 1st and 101st.
hellofunction 2: true annotations ranked 50th and 52nd.
helloauc prefers these equally as both have 100    violations   .

we want to optimize the top of the ranked list!

23 / 69

rank weighted loss [usunier et al.    09]

replace classical auc optimization:

xx xy x  y6=y

max(0, 1 + f  y (x)   fy (x))

with weighted version:

xx xy x  y6=y

l(ranky (x)) max(0, 1 + f  y (x)   fy (x))

where ranky (f (x)) is the rank of the true label:

ranky (f (x)) =x  y6=y

i (f  y (x)   fy (x))

and l(   ) converts the rank to a weight, e.g. l(   ) =p   

i=1 1/   .

24 / 69

weighted approximate-rank pairwise (warp) loss

problem: we would like to apply sgd:

weighting l(ranky (f (x))),

ranky (f (x)) =x  y6=y

i (f  y (x) + 1   fy (x))

. . . too expensive to compute per (x, y ) sample as y 2y is large.

solution: approximate by sampling fi (x) until we    nd a violating label   y

ranky (f (x))     |y|   1
n    

where n is the number of trials in the sampling step.

25 / 69

online warp loss

input: labeled data (xi , yi ), yi 2{ 1, . . . , y}.
repeat

pick a random labeled example (xi , yi )
set n = 0.
repeat

pick a random annotation   y 2{ 1, . . . , y} \ yi .
n = n + 1.

until f  y (x) > fyi (x)   1 or n > y   1
if f  y (x) > fyi (x)   1 then
make a gradient step to minimize:

l(    y 1

n    )|1   fy (x) + f  y (x)|+

end if

until validation error does not improve.

26 / 69

image annotation performance

algorithm
nearest means
one-vs-all id166s 1+:1-
one-vs-all id166s
auc id166 ranker
wsabie

16k id163

22k id163

97k web data

4.4%
4.1%
9.4%
4.7%
11.9%

2.7%
3.5%
8.2%
5.1%
10.5%

2.3%
1.6%
6.8%
3.1%
8.3%

training time: warp vs. owpc-sgd & auc

training time warp vs auc vs owpc-sgd on id163

 0.014

0
1
p
o
t
@
n
o
s
c
e
r
p

i

i

 
t
s
e
t

 0.012

 0.01

 0.008

 0.006

 0.004

 0.002

 0

 0

warp
auc
owpc-sgd

 5

 10

 15

 20

hours

 25

 30

 35

27 / 69

learned annotation embedding (on web data)

neighboring annotations
annotation
barack obama
barak obama, obama, barack, barrack obama, bow wow
david beckham beckham, david beckam, alessandro del piero, del piero
santa
dolphin
cows
rose
pine tree
mount fuji
ei   el tower
ipod
f18

santa claus, papa noel, pere noel, santa clause, joyeux noel
delphin, dauphin, whale, del   n, del   ni, baleine, blue whale
cattle, shire, dairy cows, kuh, horse, cow, shire horse, kone
rosen, hibiscus, rose    ower, rosa, roze, pink rose, red rose
abies alba, abies, araucaria, pine, neem tree, oak tree
mt fuji, fuji, fujisan, fujiyama, mountain, zugspitze
ei   el, tour ei   el, la tour ei   el, big ben, paris, blue mosque
i pod, ipod nano, apple ipod, ipod apple, new ipod
f 18, euro   ghter, f14,    ghter jet, tomcat, mig 21, f 16

28 / 69

summary

conclusion

powerful: supervised methods for ranking.
outperform classical methods
e cient low-rank models ! learn hidden representations.
embeddings good for generalization, but can    blur    too much e.g.
for exact word matches.

extensions

nonlinear extensions     e.g. convolutional net instead.

e.g. devise (frome et al., nips    13)

29 / 69

main methods we highlight, ordered by topic.
embeddings for ranking and retrieval:

id45 (deerwester et al.,    88).
supervised semantic indexing (bai et al,    09).
wsabie (weston et al.,    10).

embeddings for id38 (useful for speech, translation)

neural net language models (nn-lms) (bengio et al.,    06)
recurrent nn-lms (mikolov et al.,    10).
id97 (mikolov et al.,    13).

embeddings for supervised prediction tasks (pos, chunk, ner, srl,
sentiment, etc.):

convolutional nets for tagging (senna) (collobert & weston,    08).
recursive nns (socher et al.,    11).
paragraph vector (le & mikolov,    14).

30 / 69

id38

task: given a sequence of words, predict the next word.

the cat sat on the ??

id165 models are a strong baseline on this task.
a variety of embedding models have been tried, they can improve
results.
the embeddings learnt from this unsupervised task can also be used
to transfer to and improve a supervised task.

31 / 69

neural network language models

bengio, y., schwenk, h., sencal, j. s., morin, f., & gauvain, j. l. (2006).
neural probabilistic language models. in innovations in machine learning (pp.
137-186). springer berlin heidelberg.

32 / 69

neural network language models:
hierarchical soft max trick (morin & bengio    05)

predicting the id203 of each next word is slow in nnlms because the
output layer of the network is the size of the dictionary.

can predict via a tree instead:

1 cluster the dictionary either according to semantics (similar words in
the same cluster) or frequency (common words in the same cluster).
this gives a two-layer tree, but a binary tree is another possibility.

2 the internal nodes explicitly model the id203 of its child nodes.
3 the cost of predicting the id203 of the true word is now:

traversal to the child, plus id172 via the internal nodes and
children in the same node.

this idea is used in id97 and id56 models as well.

33 / 69

recurrent neural network language models

key idea: input to predict next word is current word plus context fed-back
from previous word (i.e. remembers the past with recurrent connection).

recurrent neural network based language model. mikolov et al., interspeech,    10.

34 / 69

nnlms vs. id56s: id32 results (mikolov)

recent uses of nnlms and id56s to improve machine translation:
fast and robust nn joint models for machine translation, devlin et al, acl    14.
also (kalchbrenner    13), (sutskever et al.,    14), (cho et al.,    14).

35 / 69

id97 : very simple lm, works well

tomas mikolov, ilya sutskever, kai chen, greg corrado, and je   rey dean.
distributed representations of words and phrases and their compositionality.
nips, 2013.

36 / 69

id97: compositionality

code: https://code.google.com/p/id97/

37 / 69

main methods we highlight, ordered by topic.
embeddings for ranking and retrieval:

id45 (deerwester et al.,    88).
supervised semantic indexing (bai et al,    09).
wsabie (weston et al.,    10).

embeddings for id38 (useful for speech, translation, . . . ):

neural net language models (nn-lms) (bengio et al.,    06)
recurrent nn-lms (mikolov et al.,    10).
id97 (mikolov et al.,    13).

embeddings for supervised prediction tasks (pos, chunk, ner,
srl, sentiment, etc.):

convolutional nets for tagging (senna) (collobert & weston,    08).
recursive nns (socher et al.,    11).
paragraph vector (le & mikolov,    14).

38 / 69

nlp tasks

part-of-speech tagging (pos): syntactic roles (noun, adverb...)
chunking: syntactic constituents (noun phrase, verb phrase...)
name entity recognition (ner): person/company/location...
id14 (srl):

[john]arg 0 [ate]rel [the apple]arg 1 [in the garden]argm loc

39 / 69

the large scale feature engineering way

extract hand-made features e.g. from the parse tree
disjoint: all tasks trained separately, cascade features
feed these features to a shallow classi   er like id166

40 / 69

assert: many hand built features for srl (pradhan et al,    04)

problems:
1) features rely on other solutions (parsing, named entity, word-sense)
2) technology task-transfer is di cult
- choose some good hand-crafted features

predicate and pos tag of predicate
phrase type: adverbial phrase, prepositional phrase, . . .
head word and pos tag of the head word
path: traversal from predicate to constituent
word-sense disambiguation of the verb
length of the target constituent (number of words)
partial path: lowest common ancestor in path
first and last words and pos in constituents
constituent tree distance
dynamic class context: previous node labels
constituent relative features: head word
constituent relative features: siblings

voice: active or passive (hand-built rules)
governing category: parent node   s phrase type(s)
position: left or right of verb
predicted named entity class
verb id91
neg feature: whether the verb chunk has a    not   
head word replacement in prepopositional phrases
ordinal position from predicate + constituent type
temporal cue words (hand-built rules)
constituent relative features: phrase type
constituent relative features: head word pos
number of pirates existing in the world. . .

- feed them to a shallow classi   er like id166

41 / 69

the suboptimal (?) cascade

(or, the opposing view is the above is a smart use of prior knowledge..)

42 / 69

43 / 69

44 / 69

the    deep learning    way
neural nets attempt to propose a radically? di   erent end-to-end approach:

avoid building a parse tree. humans don   t need this to talk.
try to avoid all hand-built features ! monolithic systems.
humans implicitly learn these features. neural networks can too. . . ?

45 / 69

46 / 69

the big picture
a uni   ed architecture for all nlp (labeling) tasks:

sentence:

pos:

chunk:

ner:
srl:

felix
nnp
np
per
arg1

sat
vbd
vp
-

rel

on
in
pp
-

the
dt
np
-

mat
nn
np-i

-

arg2 arg2-i arg2-i

.
.
.
-
-

47 / 69

48 / 69

the lookup tables

each word/element in dictionary maps to a vector in rd .

we learn these vectors.
lookuptable: input of i th word is

x = (0, 0, . . . , 1, 0, . . . , 0)

1 at position i

in the original space words are orthogonal.

cat = (0,0,0,0,0,0,0,0,0,1,0,0,0,0, . . . )
kitten = (0,0,1,0,0,0,0,0,0,0,0,0,0,0, . . . )
to get the rd embedding vector for the word we multiply wx where
w is a d     n vector with n words in the dictionary.

49 / 69

50 / 69

51 / 69

52 / 69

deep srl

 

l

h
a
b
h
a
b
h
a
b

l

l

 

input sentence

text

word of interest
verb of interest
the cat sat on the mat

indices
pos w.rt. word
pos w.r.t. verb

s(1) s(2) s(3) s(4) s(5) s(6)
 -1      0      1       2      3     4
 -2     -1     0       1      2     3

lookup tables

i

g
n
d
d
e
b
m
e

ltw

ltpw
ltpv

l
o
c
a

l
 
f
e
a
t
u
r
e
s

l

g
o
b
a

l
 
f
e
a
t
u
r
e
s

convolution layer

.
.
.

max over time

...

hardtanh

linear

hardtanh

linear

softmax

tags

this is the network for a single window. we train/test predicting the entire

sentence of tags (   structured outputs   ) using viterbi approach, similar to other

nlp methods.

53 / 69

removing the time dimension (1/2)

54 / 69

(cid:1)(cid:2)(cid:3)(cid:4)(cid:2)(cid:5)(cid:6)(cid:7)(cid:1)(cid:8)(cid:7)(cid:9)(cid:4)(cid:2)(cid:5)(cid:10)(cid:11)(cid:12)(cid:5)(cid:13)(cid:3)(cid:13)(cid:9)(cid:4)(cid:14)(cid:13)(cid:15)(cid:16)(cid:17)(cid:4)(cid:16)(cid:13)(cid:13)(cid:16)(cid:18)(cid:2)(cid:8)(cid:4)(cid:17)(cid:2)(cid:6)(cid:13)(cid:18)(cid:18)(cid:7)(cid:5)(cid:19)(cid:2)(cid:20)(cid:4)(cid:6)(cid:13)(cid:19)(cid:20)(cid:15)(cid:20)(cid:6)(cid:2)(cid:5)(cid:17)(cid:7)(cid:18)(cid:9)(cid:7)(cid:2)(cid:15)(cid:5)(cid:13)(cid:7)(cid:20)(cid:6)(cid:4)(cid:17)(cid:2)(cid:9)(cid:11)(cid:3)(cid:17)(cid:10)(cid:7)(cid:5)(cid:21)(cid:2)(cid:4)(cid:2)(cid:22)(cid:23)(cid:18)(cid:13)(cid:6)(cid:2)(cid:6)(cid:24)removing the time dimension (2/2)

55 / 69

(cid:1)(cid:2)(cid:3)(cid:4)(cid:2)(cid:5)(cid:6)(cid:7)(cid:1)(cid:8)(cid:7)(cid:9)(cid:4)(cid:2)(cid:5)(cid:10)(cid:11)(cid:12)(cid:5)(cid:13)(cid:3)(cid:13)(cid:9)(cid:4)(cid:14)(cid:13)(cid:15)(cid:16)(cid:17)(cid:4)(cid:16)(cid:13)(cid:13)(cid:16)(cid:18)(cid:2)(cid:8)(cid:4)(cid:17)(cid:2)(cid:6)(cid:13)(cid:18)(cid:18)(cid:7)(cid:5)(cid:19)(cid:2)(cid:20)(cid:4)(cid:6)(cid:13)(cid:19)(cid:20)(cid:15)(cid:20)(cid:6)(cid:2)(cid:5)(cid:17)(cid:7)(cid:18)(cid:9)(cid:7)(cid:2)(cid:15)(cid:5)(cid:13)(cid:7)(cid:20)(cid:6)(cid:4)(cid:17)(cid:2)(cid:9)(cid:11)(cid:3)(cid:17)(cid:10)(cid:7)(cid:5)(cid:21)(cid:2)(cid:4)(cid:2)(cid:22)(cid:23)(cid:18)(cid:13)(cid:6)(cid:2)(cid:6)(cid:24).. we can train directly for that (word tag likelihood) or we could train in a
structured way by predicting the entire sentence   s tags.

that should be useful because tags are not independent.

56 / 69

57 / 69

58 / 69

59 / 69

improving id27

rare words are not trained properly
sentences with similar words should be tagged in the same way:

the cat sat on the mat
the feline sat on the mat

word

sat

word

feline

lookup table

wlt

lookup table

wlt

only 1m wsj not enough     let   s use lots of unsupervised data!

60 / 69

semi-supervised: mtl with unlabeled text

* syntax

language model:    is a sentence actually english or not?   
implicitly captures:
bengio & ducharme (2001) id203 of next word given previous
words. overcomplicated     we do not need probabilities here
english sentence windows: wikipedia (    631m words)
non-english sentence windows: middle word randomly replaced

* semantics

the champion federer wins wimbledon

vs. the champion saucepan wins wimbledon
multi-class margin cost:

xs2s xw2d

max (0, 1   f (s, w ?

s ) + f (s, w ))

s: sentence windows d: dictionary

s : true middle word in s

w ?

f (s, w ): network score for sentence s and middle word w

61 / 69

language model: embedding

nearest neighbors in 100-dim. embedding space:

france

454
spain
italy
russia
poland
england
denmark
germany
portugal
sweden
austria

jesus
1973
christ

god

resurrection

prayer
yahweh
josephus

moses

sin

heaven

salvation

xbox
6909

reddish
11724

yellowish
playstation
dreamcast
greenish
psnumber brownish

snes
wii
nes

nintendo
gamecube

psp

amiga

bluish
creamy
whitish
blackish
silvery
greyish
paler

scratched

29869

smashed
ripped
brushed
hurled
grabbed
tossed

squeezed
blasted
tangled
slashed

(even fairly rare words are embedded well.)

62 / 69

results

algorithm
baselines

nn + wtl
nn + stl

nn + lm + stl
nn + . . . + tricks

pos (pwa) chunk (f1) ner (f1) srl (f1)

97.24

[toutanova    03]

96.31
96.37
97.22
97.29

[+su x]

94.29

[sha    03]
89.13
90.33
94.10
94.32

[+pos]

89.31

[ando    05]
79.53
81.47
88.67
89.95

77.92

[koomen    05]
55.40
70.99
74.15
76.03

[+gazetteer]

[+parse trees]

notes:
    didn   t compare to benchmarks that used external labeled data.
    [ando    05] uses external unlabeled data.
    [koomen    05] uses 4 parse trees not provided by the challenge. using
only 1 tree it gets 74.76.

63 / 69

software

code for tagging with pos, ner, chunk, srl + parse trees:
http://ml.nec-labs.com/senna/

see also torch: http://www.torch.ch

64 / 69

recursive nns for parsing, sentiment, ... and more!
(socher et al., icml    13), (socher et al., emnlp,    13))
build sentence representations using the parse tree to compose
embeddings via a nonlinear function taking pairs (c1, c2) and output p.

65 / 69

paragraph vector
(le & mikolov,    14)

a paragraph vector (a vector that represents a paragraph/doc) learned by:

1) predicting the words in a doc;

2) predict id165s in the doc:

at test time, for a new document, one needs to learn its vector, this can

encode word order via the id165 prediction approach.

66 / 69

comparison of id98, id56 & pv (kim    14)

67 / 69

some more recent work

compositionality approaches by marco baroni   s group:
words are combined with linear matrices dependendent on the p.o.s.:
g. dinu and m. baroni. how to make words with vectors: phrase
generation in id65. acl    14.

id194 by phil blunson   s group:
variants of convolutional networks for text:
kalchbrenner et al. a convolutional neural network for modelling
sentences. acl    14

good tutorial slides from these teams covering multiple topics:

new directions in vector space models of meaning

http://www.cs.ox.ac.uk/   les/6605/aclvectortutorial.pdf

68 / 69

summary

generic end-to-end deep learning system for nlp tasks.

id27s combined to form sentence or document
embeddings can perform well on supervised tasks.

previous common belief in nlp: engineering syntactic features
necessary for semantic tasks.
one can do well by engineering a model/algorithm rather than
features.

attitude is changing in recent years... let   s see what happens!

69 / 69

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

embedding methods for nlp

part 2: embeddings for multi-relational data

antoine bordes & jason weston

facebook ai research

emnlp tutorial     october 29, 2014

1 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

menu     part 2

1 embeddings for multi-relational data

multi-relational data
link prediction in kbs
embeddings for information extraction
id53

2 pros and cons of embedding models

3 future of embedding models

4 resources

2 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

menu     part 2

1 embeddings for multi-relational data

multi-relational data
link prediction in kbs
embeddings for information extraction
id53

2 pros and cons of embedding models

3 future of embedding models

4 resources

3 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

multi-relational data

data is structured as a graph
each node = an entity
each edge = a relation/fact
a relation = (sub, rel, obj):

sub =subject,
rel = relation type,
obj = object.

nodes w/o features.

in this talk, we focus on knowledge bases (kbs).

4 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

example of kb: id138

id138: dictionary where each entity is a sense (synset).

popular in nlp.
statistics:

117k entities;
20 relation types;
500k facts.

examples:
(car nn 1, has part, wheel nn 1)
(score nn 1,
(score nn 2,

is a,
is a, sheet music nn 1)

rating nn 1)

5 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

example of kb: freebase

freebase: huge collaborative (hence noisy) kb.

part of the google id13.
statistics:

80m of entities;
20k relation types;
1.2b facts.

examples:
(barack obama, place of birth, hawai)
(albert einstein,
follows diet, veganism)
(san francisco, contains, telegraph hill)

6 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

modeling knowledge bases

why kbs?

kbs: semantic search, connect people and things
kbs   text: information extraction
kbs ! text: text interpretation, summary, q&a

main issue: kbs are hard to manipulate

large dimensions: 105/108 entities, 104/106 rel. types
sparse: few valid links
noisy/incomplete: missing/wrong relations/entities

how?

1 encode kbs into low-dimensional vector spaces
2 use these representations:

to complete/visualize kbs
as kb data in text applications

7 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

menu     part 2

1 embeddings for multi-relational data

multi-relational data
link prediction in kbs
embeddings for information extraction
id53

2 pros and cons of embedding models

3 future of embedding models

4 resources

8 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

link prediction
add new facts without requiring extra knowledge

from known information, assess the
validity of an unknown fact

! collective classi   cation
! towards reasoning in embedding
spaces

9 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

link prediction
add new facts without requiring extra knowledge

from known information, assess the
validity of an unknown fact

! collective classi   cation
! reasoning in embedding spaces

10 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

statistical relational learning

framework:

ns subjects {subi}i2[1;ns ]
nr relation types {relk}k2[1;nr ]
no objects {objj}j2[1;no ]

! for us, ns = no = ne and 8i 2 [1; ne] , subi = obji .

a fact exists for (subi , relk, objj ) if relk(subi , objj ) = 1

goal: we want to model, from data,

p[relk(subi , objj) = 1]

(eq. to approximate the binary tensor x 2 {0, 1}ns   no   nr )

11 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

previous work

tensor factorization (harshman et al.,    94)
probabilistic relational learning (friedman et al.,    99)
relational markov networks (taskar et al.,    02)
markov-logic networks (kok et al.,    07)
extension of sbms (kemp et al.,    06) (sutskever et al.,    10)
spectral id91 (undirected graphs) (dong et al.,    12)
ranking of id93 (lao et al.,    11)
collective id105 (nickel et al.,    11)
embedding models (bordes et al.,    11,    13) (jenatton et al.,    12)
(socher et al.,    13) (wang et al.,    14) (garc    a-dur  an et al.,    14)

12 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

collective id105 (nickel et al.,    11)

rescal: 8k 2 [1; nr ] , rk 2 rd   d and a 2 rne   d
(close from dedicom (harshman,    78)).

a & r learned by reconstruction (alternating least-squares):

1

2 xk

min
a,r

||xk   arka>||2

f! +  a||a||2

f +  rxk

||rk||2

f

13 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

scalability

method nb of parameters on freebase15k
rescal o(ned + nr d 2)
88m (d = 250)

freebase15k: ne = 15k, nr = 1.3k.

rescal involves many parameters.
bad scalability w.r.t. nr .
reconstruction criterion does not    t well for binary data..

14 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

embedding models

two main ideas:

1 models based on low-dimensional continuous vector embeddings

for entities and relation types, directly trained to de   ne a
similarity criterion.

2 stochastic training based on ranking loss with sub-sampling of

unknown relations.

15 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

embedding models for kbs

subjects and objects are represented by vectors in rd.

{subi}i2[1;ns ] ! [s1, . . . , sns ] 2 rd   ns
{objj}j2[1;no ] ! [o1, . . . , ono ] 2 rd   no
for us, ns = no = ne and 8i 2 [1; ne] , si = oi .

rel. types = similarity operators between subj/obj.

{relk}k2[1;nr ] ! operators {rk}k2[1;nr ]

learning similarities depending on rel ! d(sub, rel, obj),
parameterized by s, r and o.

16 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

structured embeddings (bordes et al.,    11)

intuition: sub and obj are projected using rel
in a space where they are similar

d(sub, rel, obj) =  ||rlefts>   rrighto>||1
- entities: s and o 2 rd
- projection: rleft and rright 2 rd   d
rleft 6= rright because of asymmetry
- similarity: l1 distance

17 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

stochastic training

learning by stochastic id119: one training fact after
the other

for each relation from the training set:
sub-sample unobserved facts (false?)

1
2 check if the similarity of the true fact is lower
3

if not, update parameters of the considered facts

stopping criterion: performance on a validation set

18 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

scalability

method nb of parameters on freebase15k
rescal o(ned + nr d 2)
88m (d = 250)
o(ned + 2nr d 2)
se
8m (d = 50)

freebase15k: ne = 15k, nr = 1.3k.

se also involves many parameters.
bad scalability w.r.t. nr .
potential training problems for se (over   tting).

19 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

neural tensor networks (socher et al.,    13)

in ntn, a relationship is represented by a tensor, 2 matrices and
2 vectors + a non-linearity (tanh).

d(sub, rel, obj) = u>r tanh h>wr t + v1

r h + v2

r t + br 

neural tensor layer:

very powerful model with high capacity for each relation.

20 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

scalability

method nb of parameters on freebase15k
rescal o(ned + nr d 2)
88m (d = 250)
o(ned + 2nr d 2)
se
8m (d = 50)
o(ned + nr d 3)
165m (d = 50)
ntn

freebase15k: ne = 15k, nr = 1.3k.

very high modeling capacity.
involves many parameters.
bad scalability w.r.t. nr (over   tting if few triples).

21 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

modeling relations as translations (bordes et al.    13)

intuition: we want s + r     o.

the similarity measure is de   ned as:

d(h, r , t) =  ||h + r   t||2

2

we learn s,r and o that verify that.

22 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

modeling relations as translations (nips13)

intuition: we would like that s + r     o.

the similarity measure is de   ned as:

d(h, r , t) =  ||h + r   t||2

2

we learn s,r and o that verify that.

23 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

modeling relations as translations (bordes et al.    13)

intuition: we want s + r     o.

the similarity measure is de   ned as:

d(sub, rel, obj) = ||s + r   o||2
s,r and o are learned to verify that.

2

24 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

learning transe

for training, a margin ranking criterion is minimized:

xpos xneg2s0     + ||s + r   o||2

2   +
2   ||s0 + r   o0||2

where [x]+ is the positive part of x,   > 0 is a margin, and:

s0 = (sub0,rel,obj)|sub0 2 e  [ (sub,rel,obj0)|obj0 2 e 

25 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

learning transe

1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:

input: training set s = {(sub,rel,obj)}, margin  , learning rate  
initialize r   uniform(  6pk

) for each rel

, 6pk

loop

r   `/ k `k for each `
e   uniform(  6pj , 6pk
e   e/ k ek for each entity ent
sbatch   sample(s, b) //sample minibatch of size b
tbatch   ; //initialize set of pairs
for (sub,rel,obj) 2 sbatch do

) for each entity ent(sub or obj)

end for

(sub0,rel,obj0)  sample(s0(sub,rel,obj)) //sample negative triplet
tbatch   tbatch [  (sub,rel,obj), (sub0,rel,obj0)  
update embeddings w.r.t. xtbatch
2   +
r     + ||s + r   o||2
2   ||s0 + r   o0||2

14: end loop

26 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

motivations of a translation-based model

natural representation for hierarchical relationships.

recent work on id27s (mikolov et al.,    13):
there may exist embedding spaces in which relationships among
concepts are represented by translations.

27 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

scalability

method nb of parameters on freebase15k
rescal o(ned + nr d 2)
88m (d = 250)
o(ned + 2nr d 2)
se
8m (d = 50)
o(ned + nr d 3)
165m (d = 50)
ntn
transe
o(ned + nr d)
0.8m (d = 50)

freebase15k: ne = 15k, nr = 1.3k.

transe is a special case of se and ntn.
transe obtains better training errors: less over   tting.
much better scalability.

28 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

chunks of freebase

data statistics:

fb13
fb15k
fb1m

entities (ne) rel. (nr ) train. ex. valid. ex. test ex.
23,733
59,071
177,404

5,908
50,000
50,000

1,345
23,382

13

316,232
483,142
17.5   106

75,043
14,951
1   106

training times for transe:
embedding dimension: 50.
training time:

on freebase15k:    2h (on 1 core),
on freebase1m:    1d (on 16 cores).

29 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

visualization of 1,000 entities

30 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

visualization of 1,000 entities - zoom 1

31 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

visualization of 1,000 entities - zoom 2

32 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

visualization of 1,000 entities - zoom 3

33 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

example

   who in   uenced j.k. rowling?   

j. k. rowling

influenced by ?

c. s. lewis
lloyd alexander
terry pratchett
roald dahl
jorge luis borges
stephen king
ian fleming

34 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

example

   who in   uenced j.k. rowling?   

j. k. rowling

influenced by g. k. chesterton
j. r. r. tolkien
c. s. lewis
lloyd alexander
terry pratchett
roald dahl
jorge luis borges
stephen king
ian fleming

35 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

example

   which genre is the movie wall-e?   

wall-e

has genre ?

computer animation
comedy    lm
adventure    lm
science fiction
fantasy
stop motion
satire
drama

36 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

example

   which genre is the movie wall-e?   

wall-e

has genre animation

computer animation
comedy    lm
adventure    lm
science fiction
fantasy
stop motion
satire
drama

37 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

benchmarking

ranking on fb15k

classi   cation on fb13

on fb1m,transe predicts 34% in the top-10 (se only 17.5%).
results extracted from (bordes et al.,    13) and (wang et al.,    14)

38 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

re   ning transe

tatec (garc    a-dur  an et al.,    14) supplements transe with a
trigram term for encoding complex relationships:

d(sub, rel, obj) =

trigram
s>1 ro1 +

z }| {

z

bigrams   transe

s>2 r + o>2 r0 + s>2 do2,

}|

{

with s1 6= s2 and o1 6= o2.
transh (wang et al.,    14) adds an orthogonal projection to the
translation of transe:

d(sub, rel, obj) = ||(s r>p srp) + rt   (o r>p orp)||2
2,

with rp ? rt.

39 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

benchmarking

ranking on fb15k

results extracted from (garc    a-dur  an et al.,    14) and (wang et al.,    14)

40 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

menu     part 2

1 embeddings for multi-relational data

multi-relational data
link prediction in kbs
embeddings for information extraction
id53

2 pros and cons of embedding models

3 future of embedding models

4 resources

41 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

information extraction

information extraction: populate kbs with new facts using text
usually two steps:

entity linking: identify mentions of entities in text
id36: extract facts about them

previous works include rule-based models, classi   ers with
features from parsers, id114, etc.
embedding models exist for both steps.

42 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

entity linking as wsd

id51 $ id138 entity linking

towards open-text id29:

43 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

embeddings of text and id138 (bordes et al.,    12)

text is converted into relations (sub,rel,obj).
joint learning of embeddings for all symbols: words, entities and
relation types from id138.
this system can label 37,141 words with 40,943 synsets.

id138
wikipedia
conceptnet
ext. id138
unamb. wikip.
total

train. ex. test ex.
5,000
10,000
0
5,000
0
20,000

146,442
2,146,131
11,332
42,957
981,841
3,328,703

symbol
labeled?
synsets
no
words
no
non
words
yes words+synsets
yes words+synsets
-

-

44 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

benchmarking on extended id138
f1-score on 5,000 test sentences to disambiguate.

results extracted from (bordes et al.,    12)

45 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

id138 is enriched through text

similarities among senses beyond id138

   what does an army attack?   

army nn 1 attack vb 1 ?

armed service nn 1
ship nn 1
territory nn 1
military unit nn 1

46 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

id138 is enriched through text

similarities among senses beyond original id138 data

   what does an army attack?   

army nn 1 attack vb 1 troop nn 4

armed service nn 1
ship nn 1
territory nn 1
military unit nn 1

47 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

id138 is enriched through text

similarities among senses beyond id138

   who or what earns money   

?

earn vb 1 money nn 1

business    rm nn 1

family nn 1
payo    nn 3

card game nn 1

48 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

id138 is enriched through text

similarities among senses beyond original id138 data

   who or what earns money   

earn vb 1 money nn 1

person nn 1
business    rm nn 1
family nn 1
payo    nn 3
card game nn 1

49 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

id36

given a bunch of sentences.

50 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

id36

given a bunch of sentences concerning the same pair of entities.

51 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

id36

goal: identify if there is a relation between them to add to the kb.

52 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

id36

and from which type, to enrich an existing kb.

53 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

embeddings of text and freebase (weston et al.,    13)

standard method: an embedding-based classi   er is trained to
predict the relation type, given text mentions m and (sub, obj):

r (m, sub, obj) = arg max

sm2r (m, rel0)

rel0 xm2m

classi   er based on wsabie (weston et al.,    11).

54 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

embeddings of text and freebase (weston et al.,    13)

idea: improve extraction by using both text + available
knowledge (= current kb).

a model of the kb is used in a re-ranking setting to force
extracted relations to agree with it:

r0(m, sub, obj) = arg max

rel0   xm2m

sm2r (m, rel0) dkb(sub, rel0, obj) 

with dkb(sub, rel0, obj) = ||s + r0   o||2

2 (trained separately)

55 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

benchmarking on nyt+freebase
exp. on ny times papers linked with freebase (riedel et al.,    10)

precision/recall curve for predicting relations

results extracted from (weston et al.,    13)

56 / 83

(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:5)(cid:6)(cid:1)(cid:2)(cid:3)(cid:7)(cid:8)(cid:7)(cid:9)(cid:10)(cid:11)(cid:11)(cid:12)(cid:11)(cid:13)(cid:11)(cid:12)(cid:11)(cid:14)(cid:11)(cid:12)(cid:11)(cid:15)(cid:11)(cid:12)(cid:11)(cid:16)(cid:11)(cid:12)(cid:11)(cid:17)(cid:11)(cid:12)(cid:11)(cid:18)(cid:11)(cid:12)(cid:11)(cid:19)(cid:11)(cid:12)(cid:11)(cid:20)(cid:11)(cid:12)(cid:11)(cid:21)(cid:11)(cid:12)(cid:13)(cid:11)(cid:12)(cid:16)(cid:11)(cid:12)(cid:17)(cid:11)(cid:12)(cid:18)(cid:11)(cid:12)(cid:19)(cid:11)(cid:12)(cid:20)(cid:11)(cid:12)(cid:21)(cid:22)(cid:8)(cid:4)(cid:23)(cid:7)(cid:2)(cid:24)(cid:25)(cid:14)(cid:26)(cid:27)(cid:28)(cid:29)(cid:25)(cid:30)(cid:25)(cid:31)(cid:26)(cid:32)(cid:33)(cid:9)(cid:34)(cid:34)(cid:35)(cid:4)(cid:10)(cid:10)(cid:22)(cid:8)(cid:4)(cid:23)(cid:7)(cid:2)(cid:24)(cid:25)(cid:14)(cid:26)(cid:26)(cid:7)(cid:2)(cid:36)(cid:2)(cid:5)(cid:25)(cid:7)(cid:10)(cid:37)(cid:38)embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

universal schemas (riedel et al.,    13)

join in a single learning problem:

id36
link prediction

the same model score triples:

made of text mentions
from a kb

57 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

universal schemas (riedel et al.,    13)

relation prediction using the score:

r0(m, sub, obj) = arg maxrel0  pm2m sm2r (m, rel0)
+ sneighbors(sub, rel0, obj) 

+ skb(sub, rel0, obj)

all scores are de   ned using embeddings:

sm2r (m, rel0) = f(m)>r0
skb(sub, rel0, obj) = s>r0s + o>r0o

sneighbors(sub, rel0, obj) = x(sub,rel00,obj)

rel006=rel0

w rel0
rel00

training by ranking observed facts versus other and updating
using sgd.

58 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

benchmarking on nyt+freebase
exp. on ny times papers linked with freebase (riedel et al.,    10)

mean averaged precision for predicting relations

results extracted from (riedel et al.,    13)

59 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

menu     part 2

1 embeddings for multi-relational data

multi-relational data
link prediction in kbs
embeddings for information extraction
id53

2 pros and cons of embedding models

3 future of embedding models

4 resources

60 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

link prediction as q&a

   who in   uenced j.k. rowling?   

j. k. rowling

influenced by g. k. chesterton
j. r. r. tolkien
c. s. lewis
lloyd alexander
terry pratchett
roald dahl
jorge luis borges

can we go beyond such rigid structure?

61 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

open-domain id53

open-domain q&a: answer question on any topic
 ! query a kb with natural language

examples
   what is cher   s son   s name ?   
   what are dollars called in spain ?   
   what is henry clay known for ?   
   who did georges clooney marry in 1987 ?   

elijah blue allman
peseta
lawyer
kelly preston

recent e   ort with id29 (kwiatkowski et al.    13)
(berant et al.    13,    14) (fader et al.,    13,    14) (reddy et al.,    14)
models with embeddings as well (bordes et al.,    14)

62 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

subgraph embeddings (bordes et al.,    14)

model learns embeddings of questions and (candidate) answers
answers are represented by entity and its neighboring subgraph

embedding model 

embedding%of%
the%ques6on%

score 

how%the%candidate%answer%

   ts%the%ques6on%

embedding%of%the%

subgraph%

word%embedding%lookup%table%

dot%product%

freebase%embedding%lookup%table%

binary%encoding%
of%the%ques6on%

binary%encoding%
of%the%subgraph%

ques%on(

   who did clooney marry in 1987?    

freebase subgraph 

g. clooney 

k. preston 

honolulu 

1987 

model 

detec6on%of%freebase%
en6ty%in%the%ques6on%

subgraph%of%a%candidate%
answer%(here%k.%preston)%

j. travolta 

63 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

training data

freebase is automatically converted into q&a pairs
closer to expected language structure than triples

examples of freebase data
(sikkim, location.in state.judicial capital, gangtok)
what is the judicial capital of the in state sikkim ?     gangtok

(brighouse, location.location.people born here, edward barber)
who is born in the location brighouse ?     edward barber

(sepsis, medicine.disease.symptoms, skin discoloration)
what are the symptoms of the disease sepsis ?     skin discoloration

64 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

training data

all freebase questions have rigid and similar structures
supplemented by pairs from clusters of paraphrase questions
multitask training: similar questions $ similar embeddings

examples of paraphrase clusters
what are two reason to get a 404 ?
what is error 404 ?
how do you correct error 404 ?

what is the term for a teacher of islamic law ?
what is the name of the religious book islam use ?
who is chief of islamic religious authority ?

what country is bueno aire in ?
what countrie is buenos aires in ?
what country is bueno are in ?

65 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

benchmarking on webquestions
experiments on webquestions (berant et al.,    13)

f1-score for answering test questions

results extracted from (berant et al.,    14) and (bordes et al.,    14)

66 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

menu     part 2

1 embeddings for multi-relational data

multi-relational data
link prediction in kbs
embeddings for information extraction
id53

2 pros and cons of embedding models

3 future of embedding models

4 resources

67 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

advantages

e cient features for many tasks in practice
training with sgd scales & parallelizable (niu et al.,    11)
flexible to various tasks: id72 of embeddings
supervised or unsupervised training
allow to use extra-knowledge in other applications

68 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

issues

must train all embeddings together (no parallel 1-vs-rest)
low-dimensional vector  ! compression, blurring
sequential models su   er from long-term memory
embeddings need quite some updates to be good     not 1-shot
negative example sampling can be une cient

69 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

menu     part 2

1 embeddings for multi-relational data

multi-relational data
link prediction in kbs
embeddings for information extraction
id53

2 pros and cons of embedding models

3 future of embedding models

4 resources

70 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

fix current limitations

compression: improve the memory capacity of embeddings and
allows for id62 of new symbols
long-term memory: encode longer dependencies in sequential
models like id56s
training: faster and better sampling of examples
beyond linear: most supervised labeling problems are well
tackled by simple linear models. non-linearity should help more.

71 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

explore new directions

compositionality (baroni et al.    10) (grefenstette, 13)
multimodality (bruni et al., 12) (kiros et al.,    14)
grounding language into actions (bordes et al., 10)

step 0:

step 4:

step 5:

he

cooks

the

rice

?

?

?

<kitchen>

<gina>

<rice>

x:

y:

u:

?

n

a ti o

c

o

l

<john>

<mark>

x:

y:
(2)

u:

he

cooks

the

rice

?

?
<cook>

?

?
<rice>

<kitchen>

(1)

<john>

<gina>

x:

y:

u:

<garden>

<cook>

<mark>

<garden>

he

cooks

the

rice

?
<john>

?
<cook>

?

?
<rice>

<kitchen>

<gina>

<mark>

<garden>

72 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

at emnlp
modeling interestingness with deep neural networks
jianfeng gao, patrick pantel, michael gamon, xiaodong he and li deng

translation modeling with id182
martin sundermeyer, tamer alkhouli, joern wuebker and hermann ney

learning image embeddings using convolutional neural networks for improved
multi-modal semantics
douwe kiela and l  eon bottou

learning abstract concept embeddings from multi-modal data: since you
probably can   t see what i mean
felix hill and anna korhonen

incorporating vector space similarity in random walk id136 over knowledge
bases
matt gardner, partha talukdar, jayant krishnamurthy and tom mitchell

composition of word representations improves semantic role labelling
michael roth and kristian woodsend

73 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

at emnlp

a neural network for factoid id53 over paragraphs
mohit iyyer, jordan boyd-graber, leonardo claudino, richard socher and hal
daum  e iii

joint relational embeddings for knowledge-based id53
min-chul yang, nan duan, ming zhou and hae-chang rim

evaluating neural word representations in tensor-based compositional settings
dmitrijs milajevs, dimitri kartsaklis, mehrnoosh sadrzadeh and matthew purver

opinion mining with deep recurrent neural networks
ozan irsoy and claire cardie

the inside-outside id56 model for id33
phong le and willem zuidema

a fast and accurate dependency parser using neural networks
danqi chen and christopher manning

74 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

at emnlp

reducing dimensions of tensors in type-driven id65
tamara polajnar, luana fagarasan and stephen clark

word semantic representations using bayesian probabilistic tensor factorization
jingwei zhang, jeremy salwen, michael glass and al   o gliozzo

glove: global vectors for word representation
je   rey pennington, richard socher and christopher manning

jointly learning word representations and composition functions using
predicate-argument structures
kazuma hashimoto, pontus stenetorp, makoto miwa and yoshimasa tsuruoka

typed tensor decomposition of knowledge bases for id36
kai-wei chang, wen-tau yih, bishan yang and christopher meek

id13 and text jointly embedding
zhen wang, jianwen zhang, jianlin feng and zheng chen

75 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

at emnlp

id53 with subgraph embeddings
antoine bordes, sumit chopra and jason weston

word translation prediction for morphologically rich languages with bilingual
neural networks
ke m. tran, arianna bisazza and christof monz

learning phrase representations using id56 encoder   decoder for statistical
machine translation
kyunghyun cho, bart van merrienboer, caglar gulcehre, dzmitry bahdanau,
fethi bougares, holger schwenk and yoshua bengio

convolutional neural networks for sentence classi   cation
yoon kim

#tagspace: semantic embeddings from hashtags
jason weston, sumit chopra and keith adams

76 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

menu     part 2

1 embeddings for multi-relational data

multi-relational data
link prediction in kbs
embeddings for information extraction
id53

2 pros and cons of embedding models

3 future of embedding models

4 resources

78 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

code

torch: www.torch.ch
senna: ronan.collobert.com/senna
id56lm: www.fit.vutbr.cz/~imikolov/id56lm
id97: code.google.com/p/id97
recursive nn: nlp.stanford.edu/sentiment
sme (multi-relational data): github.com/glorotxa/sme

79 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

references

id29 on freebase from question-answer pairs
j. berant, a. chou, r. frostig & p. liang. emnlp, 2013

id29 via id141
j. berant & p. liang. acl, 2013

learning structured embeddings of knowledge bases
a. bordes, j. weston, r. collobert & y. bengio. aaai, 2011

joint learning of words and meaning rep. for open-text id29
a. bordes, x. glorot, j. weston & y. bengio. aistats, 2012

a semantic matching energy function for learning with multi-relational data
a. bordes, x. glorot, j. weston & y. bengio. mlj, 2013

translating embeddings for modeling multi-relational data
a. bordes, n. usunier, a. garc    a-dur  an, j. weston & o. yakhnenko. nips, 2013

80 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

references

id53 with subgraph embeddings
a. bordes, s. chopra & j. weston. emnlp, 2014

id91 with multi-layer graphs: a spectral perspective
x. dong, p. frossard, p. vandergheynst & n. nefedov. ieee tsp, 2013

paraphrase-driven learning for open id53
a. fader, l. zettlemoyer & o. etzioni. acl, 2013

open id53 over curated and extracted knowledge bases
a. fader, l. zettlemoyer & o. etzioni. kdd, 2014

learning probabilistic relational models
n. friedman, l. getoor, d. koller & a. pfe   er. ijcai, 1999

e   ective blending of two and three-way interactions for modeling
multi-relational data
a. garc    a-dur  an, a. bordes & n. usunier. ecml-pkdd, 2014

81 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

references

models for the analysis of asymmetrical relationships among n objects or
stimuli
r. harshman. joint symposium of psych. and mathematical societies, 1978.

parafac: parallel factor analysis
r. harshman & m. lundy. comp. statistics and data analysis, 1994

a latent factor model for highly multi-relational data
r. jenatton, n. le roux, a. bordes & g. obozinski. nips, 2012.

learning systems of concepts with an in   nite relational model
c. kemp, j. tenenbaum, t. gri ths, t. yamada & n. ueda. aaai, 2006.

statistical predicate invention
s. kok, p. domingos. icml, 2007

scaling semantic parsers with on-the-   y ontology matching
t. kwiatkowski, e. choi, y. artzi & l. zettlemoyer. emnlp, 2013

82 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

references

random walk id136 and learning in a large scale knowledge base
n. lao, t. mitchell & w. cohen. emnlp, 2011.

distributed representations of words and phrases and their compositionality
t. mikolov, i. sutskever, k. chen, g. corrado & j. dean. nips, 2013.

a three-way model for collective learning on multi-relational data
m. nickel, v. tresp & h.-p. kriegel. icml, 2011.

large-scale id29 without question-answer pairs
s. reddy, m. lapata & m. steedman. tacl, 2014.

modeling relations and their mentions without labeled text
s. riedel, l. yao and a. mccallum. ecml-pkdd, 2010

id36 with id105 and universal schemas
s. riedel, l. yao, b. marlin and a. mccallum. hlt-naacl, 2013

83 / 83

embeddings for multi-relational data

pros and cons of embedding models

future of embedding models

resources

references

reasoning with neural tensor networks for knowledge base completion
r. socher, d. chen, c. manning & a. ng nips, 2013.

modelling relational data using bayesian clustered tensor factorization
i. sutskever, r. salakhutdinov & j. tenenbaum. nips, 2009.

discriminative probabilistic models for relational data
b. taskar, p. abbeel & d. koller. uai, 2002.

id13 embedding by translating on hyperplanes
z. wang, j. zhang, j. feng & z. chen. aaai, 2014.

wsabie: scaling up to large vocabulary image annotation
j. weston, s. bengio & n. usunier. ijcai, 2011

connecting language and knowledge bases with embedding models for relation
extraction.
j. weston, a. bordes, o. yakhnenko & n. usunier. emnlp, 2013

84 / 83

