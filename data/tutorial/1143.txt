spectral learning techniques for weighted automata,

transducers, and grammars

borja balle   

ariadna quattoni   

xavier carreras   

p   q mcgill university

p   q xerox research centre europe

tutorial @ emnlp 2014

latest version available from:

http://www.lsi.upc.edu/~bballe/slides/tutorial-emnlp14.pdf

outline

1. weighted automata and hankel matrices

2. spectral learning of probabilistic automata

3. id106 for transducers and grammars

sequence tagging
finite-state transductions
tree automata

4. hankel matrices with missing entries

5. conclusion

6. references

compositional functions and bilinear operators

   compositional functions de   ned in terms of recurrence relations
   consider a sequence abaccb

fpabaccbq       fpabq      fpaccbq

      fpabq    aa      fpccbq
      fpabaq    ac      fpcbq

where

   n is the dimension of the model
     f maps pre   xes to rn
     f maps su   xes to rn
   aa is a bilinear operator in rn  n

problem

how to estimate   f,   f and aa, ab, . . . from    samples    of f?

weighted finite automata (wfa)

an algebraic model

for compositional functions on strings

weighted finite automata (wfa)
example with 2 states and alphabet        ta, bu

operator representation

   
   

1.0
0.0

0.0
0.6

   
   
   
   

0.4 0.2
0.1 0.1

0.1 0.3
0.1 0.1

   
   

a 0.1
b 0.1

0.6

q1

a 0.4
b 0.1

q0

a 0.1
b 0.1

a 0.2
b 0.3

  0    
  8    
aa    
ab    

fpabq       j

0 aaab  8

weighted finite automata (wfa)
notation:

     : alphabet        nite set
   n: number of states     positive integer
     0: initial weights     vector in rn
     8:    nal weights     vector in rn
   a  : transition weights     matrix in rn  n (@   p   )

(features of empty pre   x)

(features of empty su   x)

de   nition: wfa with n states over   

a     x  0,   8,ta  uy

compositional function: every wfa a de   nes a function fa :          r

fapxq     fapx1 . . . xtq       j

0 ax1        axt   8       j

0 ax  8

example     hidden markov model

   assigns probabilities to strings fpxq     prxs
   emission and transition are conditionally independent given state

0     r0.3 0.3 0.4s
  j
  j8     r1 1 1s
aa     oa    t
t    

0.7

0.3
0 0.75 0.25
0
0.6

0.4

      0
      0.3

0
0

0
0.9
0

0
0
0.5

oa    

      
      

0.30.40.750.250.70.6a,0.5b,0.5a,0.3b,0.7a,0.9b,0.1example     probabilistic transducer

          x    y, where x input alphabet and y output alphabet
   assigns conditional probabilities fpx, yq     pry|xs to pairs px, yq p      

x     ta, bu
y     ta, bu
0     r0.3 0 0.7s
  j
  j8     r1 1 1s
b    
ab

      0.2

0.4
0

0
1
0.75 0

0
0

      

a~a,0.1 a~b,0.9b~a,0.25 b~b,0.75 a~b,0.15a~a,0.75a~b,0.25 b~b,1b~b,0.4b~a,0.4a~b,0.85b~b,0.20.30.7other examples of wfa

automata-theoretic:

   probabilistic finite automata (pfa)
   deterministic finite automata (dfa)

dynamical systems:

   observable operator models (oom)
   predictive state representations (psr)

disclaimer: all weights in r with usual addition and multiplication (no
semi-rings!)

applications of wfa

wfa can model:

   id203 distributions fapxq     prxs
   binary classi   ers gpxq     signpfapxq `   q
   real predictors fapxq
   sequence predictors gpxq     argmaxy fapx, yq (with        x    y)

used in several applications:

   id103 [mohri, pereira, and riley    08]
   image processing [albert and kari    09]
   ocr systems [knight and may    09]
   system testing [baier, gr  osser, and ciesinski    09]
   etc.

useful intuitions about fa
0 ax1        axt   8       j
fapxq     fapx1 . . . xtq       j
  
  
t  
   sum-product: fapxq is a sum   product computation

  

0 ax  8

axtpit  1, itq

  8pitq

i0,i1,...,itprns

t   1

   forward-backward: fapxq is dot product between forward and
backward vectors

fappsq    

  j
0 ap

   pas  8q       p      s

  0pi0q
`
`

  
  

   compositional features: fapxq is a linear model

fapxq    

  j
0 ax

     8       pxq      8

where    :          rn compositional features (i.e.   px  q       pxqa  )

forward   backward equations for a  
any wfa a de   nes forward and backward maps
  a,   a :          rn

such that for any splitting x     p    s one has

0 ap1        apt
  appq       j
  apsq     as1        ast1   8
fapxq       appq      apsq

example

   in id48 and pfa one has for every i p rns

r  appqsi     prp , h`1     is
r  apsqsi     prs | h     is

forward   backward equations for a  
any wfa a de   nes forward and backward maps
  a,   a :          rn

such that for any splitting x     p    s one has

  appq       j
0 ap1        apt
  apsq     as1        ast1   8
fapxq       appq      apsq

key observation

if fapp  sq,   appq, and   apsq were known for many p, s, then a   could

be recovered from equations of the form

fapp  sq       appq    a        apsq

hankel matrices help organize these equations!

the hankel matrix

two equivalent representations

   functional: f :          r
   matricial: hf p r            

, the hankel matrix of f
de   nition: p pre   x, s su   x    hfpp, sq     fpp    sq

properties

   |x| ` 1 entries for fpxq
   depends on ordering of      
   captures structure

hf    

 

a

b

aa

...

  a b aa
0 1 0
2
3
1 2 1
2
0 1 0
2 3 2
4
...

                    

                     

      
      

. . .

hfp , aaq     hfpa, aq     hfpaa,  q     2

a fundamental theorem about wfa

relates the rank of hf

and the number of states of wfa computing f

theorem [carlyle and paz    71, fliess    74]
let f :          r be any function
1. if f     fa for some wfa a with n states    rankphfq    n
2. if rankphfq     n    exists wfa a with n states s.t. f     fa

why fundamental?

because proof of (2) gives an algorithm for    recovering    a from the hankel
matrix of fa
example: can    recover    an id48 from the probabilities it assigns to se-
quences of observations

structure of low-rank hankel matrices

                          

p

hf p r            

s

...
...
...
          
...

      

      

      

                               

p

                 

fpp1        pt    s1        st 1q       j

p p r       n

                  

  
  
  
  

  
  
  
  
  
  
           
  
  
loooooooomoooooooon
0 ap1        apt

  appq

s p rn       

  
  
  

s

         
         
         

  
  
  

      

     

loooooooomoooooooon
as1        ast 1   8

  apsq

  appq     ppp,  q

  apsq     sp  , sq

hankel factorizations and operators

                 

p

h   p r            

s  
  
  
   
  

  

  

  

  

                      

p

fpp1        pt          s1        st 1q    

                 

p p r       n

                  

a   p rn  n

s p rn       

s

  
  
  
   
  

         
  
  
  
  
  
  
   
   
  
  
loooooooomoooooooon
0 ap1        apt
  j

            
   
   
   
loooooooomoooooooon
   a      as1        ast 1   8

   
   
   

   
   
   

   
   

  
  
  

  
  
  

  
  
  

  
  
  

      

  appq

  apsq

h       p a   s =   a       p` h   s`

note: works with    nite sub-blocks as well (assuming rankppq     rankpsq     n)

general learning algorithm for wfa

key idea: the hankel trick

1. learn a low-rank hankel matrix that implicitly induces

   latent    states

2. recover the states from a decomposition of the

hankel matrix

datahankelmatrixwfalow-rank matrixestimationfactorization andid202limitations of wfa
invariance under change of basis
for any invertible matrix q the following wfa are equivalent:

   a     x  0,   8,ta  uy
   b     xqj  0, q  1  8,tq  1a  quy

0 ax1        axt   8
fapxq       j
    p  j
0 qqpq  1ax1qq      pq  1axt qqpq  1  8q     fbpxq
   
   

   

   

   

0.5 0.1
0.2 0.3

q    

0

1  1 0

q  1aaq    

0.3   0.2
  0.1
0.5

example
aa    

   

consequences

   there is no unique parametrization for wfa
   given a it is undecidable whether @x fapxq    0
   cannot expect to recover a probabilistic parametrization

outline

1. weighted automata and hankel matrices

2. spectral learning of probabilistic automata

3. id106 for transducers and grammars

sequence tagging
finite-state transductions
tree automata

4. hankel matrices with missing entries

5. conclusion

6. references

spectral learning of probabilistic automata

basic setup:

   data are strings sampled from id203 distribution on      
   hankel matrix is estimated by empiricial probabilities
   factorization and low-rank approximation is computed using svd

datahankelmatrixwfalow-rank matrixestimationfactorization andid202the empirical hankel matrix
suppose s     px1, . . . , xnq is a sample of n i.i.d. strings
empirical distribution:

n  

i   1

  fspxq     1

n

irxi     xs

empirical hankel matrix:

  hspp, sq       fsppsq

example:

$      &      % aa, b, bab, a,

b, a, ab, aa,
ba, b, aa, a,
aa, bab, b, aa

,//.//-

s    

            a

.19
.31
.06
.00

 

a

b

ba

            

b
.25
.06
.00
.13

     

  h    

(hankel with rows p     t , a, b, bau and columns s     ta, bu)

finite sub-blocks of hankel matrices
parameters:

   set of rows (pre   xes) p         
   set of columns (su   xes) s         

   h for    nding p and s
   h   for    nding a  
   h  ,s for    nding   0
   hp,   for    nding   8

    abaaab...  10.30.70.050.25...a0.30.050.250.020.03...b0.70.60.10.030.2...aa0.050.020.030.0170.003...ab0.250.230.020.110.12........................    abaaab...  10.30.70.050.25...a0.30.050.250.020.03...b0.70.60.10.030.2...aa0.050.020.030.0170.003...ab0.250.230.020.110.12........................hha    abaaab...  10.30.70.050.25...a0.30.050.250.020.03...b0.70.60.10.030.2...aa0.050.020.030.0170.003...ab0.250.230.020.110.12........................hhalow-rank approximation and factorization

parameters:

   desired number of states n
   block h p rp  s of the empirical hankel matrix

low-rank approximation: compute truncated svd of rank n

hloomoon

p  s

   unloomoon

  nloomoon

p  n

n  n

factorization: h    ps already given by svd
p`         1
s`     vn

p     un  n
s     vj

  
  

n

n uj

n

vj
n  s

nloomoon
`
    phvnq`  

computing the wfa

parameters:

   factorization h    pu  qvj
   hankel blocks h  , h  ,s, hp,  

`
a           1ujh  v
`
  0     vjh  ,s
    phvq`hp,  
  8         1ujhp,  

    phvq`h  v
  

  

computational and statistical complexity

running time:

   empirical hankel matrix: op|ps|    nq
   svd and id202: op|p|    |s|    nq

statistical consistency:

   by law of large numbers,   hs    erhs when n    8
   if erhs is hankel of some wfa a, then   a    a
   works for data coming from pfa and id48

pac analysis: (assuming data from a with n states)

   with high id203, }   hs    h}    opn  1{2q
   when n    opn|  |2t 4{  2snphq4q, then

  

|x|  t

|fapxq    f   apxq|      

proofs can be found in [hsu, kakade, and zhang    09, bailly    11, balle    13]

practical considerations

basic setup:

   data are strings sampled from id203 distribution on      
   hankel matrix is estimated by empiricial probabilities
   factorization and low-rank approximation is computed using svd

advanced implementations:

   choice of parameters p and s
   scalable estimation and factorization of hankel matrices
   smoothing and variance id172
   use of pre   x and substring statistics

datahankelmatrixwfalow-rank matrixestimationfactorization andid202choosing the basis
de   nition: the pair pp, sq de   ning the sub-block is called a basis

intuitions:

   basis should be choosen such that erhs has full rank
   p must contain strings reaching each possible state of the wfa
   s must contain string producing di   erent outcomes for each pair of

states in the wfa

popular approaches:

   set p     s         k for some k    1 [hsu, kakade, and zhang    09]
   choose p and s to contain the k most frequent pre   xes and su   xes

in the sample [balle, quattoni, and carreras    12]

   take all pre   xes and su   xes appearing in the sample [bailly, denis, and

ralaivola    09]

scalable implementations

problem: when |  | is large, even the simplest basis become huge

hankel matrix representation:

   use hash functions to map p (s) to row (column) indices
   use sparse matrix data structures because statistics are usually sparse
   never store the full hankel matrix in memory

e   cient svd computation:

   svd for sparse matrices [berry    92]
   approximate randomized svd [halko, matrinsson, and tropp    11]
   on-line svd with rank 1 updates [brand    06]

re   ning the statistics in the hankel matrix

smoothing the estimates

   empirical probabilities   fspxq tend to be sparse
   like in id165 models, smoothing can help when    is large
   should take into account that strings in ps have di   erent lengths
   open problem: how to smooth empirical hankels properly

row and column weighting

   more frequent pre   xes (su   xes) have better estimated rows

(columns)

   can scale rows and columns to re   ect that
   will lead to more reliable svd decompositions
   see [cohen, stratos, collins, foster, and ungar    13] for details

substring statistics
problem: if the sample contains strings with wide range of lengths, small
basis will ignore most of the examples

string statistics (occurence id203):

$      &      %

s    

aa, b, bab, a,

bbab, abb, babba, abbb,

ab, a, aabba, baa,
abbab, baba, bb, a

            a

.19
.06
.00
.06

 

a

b

ba

            

b
.06
.06
.06
.06

     

  h    

substring statistics (expected number of occurences as substring):

$      &      %

s    

empirical expectation     1
n

aa, b, bab, a,

bbab, abb, babba, abbb,

ab, a, aabba, baa,
abbab, baba, bb, a

rnumber of occurences of x in xis

            a

1.31
.19
.56
.06

            

b

1.56
.62
.50
.31

     

  h    

 

a

b

ba

,//.//-

n  

i   1

,//.//-

substring statistics

theorem [balle, carreras, luque, and quattoni    14]
if a id203 distribution f is computed by a wfa with n states, then
the corresponding substring statistics are also computed by a wfa with n
states

learning from substring statistics

   can work with smaller hankel matrices
   but estimating the matrix takes longer

experiment: pos-tag sequence models

   ptb sequences of simpli   ed pos tags [petrov, das, and mcdonald 2012]
   con   guration: expectations on frequent substrings
   metric: error rate on predicting next symbol in test sequences

 60 62 64 66 68 70 72 74 0 10 20 30 40 50word error rate (%)number of statesspectral,    basisspectral, basis k=25spectral, basis k=50spectral, basis k=100spectral, basis k=300spectral, basis k=500unigrambigramexperiment: pos-tag sequence models

   comparison with a bigram baseline and em
   metric: error rate on predicting next symbol in test sequences
   at training, the spectral method is    100 faster than em

 58 60 62 64 66 68 70 0 10 20 30 40 50word error rate (%)number of statesspectral,    basisspectral, basis k=500emunigrambigramoutline

1. weighted automata and hankel matrices

2. spectral learning of probabilistic automata

3. id106 for transducers and grammars

sequence tagging
finite-state transductions
tree automata

4. hankel matrices with missing entries

5. conclusion

6. references

sequence tagging and transduction

   many applications involve pairs of input-output sequences:

   sequence tagging (one output tag per input token)

e.g.: id52
output:
input:

nnp nnp vbz nnp .
ms. haag plays elianti
.

   transductions (sequence lenghts might di   er)

e.g.: id147
output:
input:

a p p l e
a p l e

   finite-state automata are classic methods to model these relations.

id106 apply naturally to this setting.

sequence tagging

   notation:

   input alphabet x
   output alphabet y
   joint alphabet        x    y

   goal: map input sequences to output sequences of the same length
   approach: learn a function

f : px    yq       r

then, given an input x p xt return

fpx, yq

argmax
ypyt

(note: this maximization is not tractable in general)

weighted finite tagger

   notation:

   x    y: joint alphabet        nite set
   n: number of states     positive integer
     0: initial weights     vector in rn
     8:    nal weights     vector in rn
   ab

a: transition weights     matrix in rn  n (@a p x, b p y)

   de   nition: wftagger with n states over x    y

(features of empty pre   x)
(features of empty su   x)

a     x  0,   8,tab
auy

   compositional function: every wftagger de   nes a function
fa : px    yq       r

fapx1 . . . xt , y1 . . . ytq       j

x1        ayt

xt

0 ay1

  8       j

0 ay

x   8

the spectral method for wftaggers

   assume fpx, yq     ppx, yq

   same mechanics as for wfa, with        x    y
   in a nutshell:

1. choose set of pre   xes and su   xes to de   ne hankel

   in this case they are bistrings

2. estimate hankel with pre   x-su   x training statistics
3. factorize hankel using svd
4. compute    and    projections,

and compute operators x  0,   8,ta  uy

   other cases:

   fapx, yq     ppy | xq     see [balle et al., 2011]
   fapx, yq non-probabilistic     see [quattoni et al., 2014]

datahankelmatrixwfalow-rank matrixestimationfactorization andid202prediction with wftaggers

   assume fapx, yq     ppx, yq
   given x1:t , compute most likely output tag at position t:

argmax

  pt, aq

apy

  
where
  
  pt, aq     ppyt     a | xq    
y   y1...a...yt
     
  
   
y   y1...a...yt
loooooooooooomoooooooooooon
      j
  
    

ppx, yq
  j
0 ay

y1...yt  1
apx1:t  1q
     

ay1:t  1
x1:t  1

x   8

0

apx1:tq          
     

apx1:t  1q

apxt:tq    
     

aa
xt

ab
xt

bpy

     
  
loooooooooooomoooooooooooon
    

yt`1...yt
apxt`1:tq
     

ayi`1:t
xt`1:t

  

  8

apxt`1:tq
     

ab
xt

bpy

prediction with wftaggers (ii)

   assume fapx, yq     ppx, yq
   given x1:t , compute most likely output bigram ab at position t:

  pt, a, bq

argmax
a,bpy

where

  pt, a, bq     ppyt     a, yt`1     b | xq
xt`1     
ab

apx1:t  1qaa

         

xt

apxt`2:tq

   compute most likely full sequence y     intractable

in practice, use minimum bayes-risk decoding:
  pt, yt, yt`1q

argmax
ypyt

t

  

finite state transducers

c

d

e

a-c

 -d

b-e

a

b

(ab,cde)

   a wftransducer evaluates aligned strings,

using the empty symbol   to produce one-to-one alignments:

fpc

a

d
 

bq       j

e

0 ac

aad

 ae
b

   then, a function can be de   ned on unaligned strings by aggregating

alignments

gpab, cdeq    

  

fp  q

  p  pab,cdeq

finite state transducers: main problems

   id136: given an fst a, how to . . .
   compute gpx, yq for unaligned strings?
   using edit-distance recursions
   compute marginal quantities   pedgeq     ppedge | xq?
   also using edit-distance recursions
   compute most-likely y for given x?
   use mbr-decoding with marginal scores

   unsupervised learning: learn an fst from pairs of unaligned strings
   unlike with em, the spectral method can not recover latent structure

such as alignments
(recall: alignments are needed to estimate hankel entries)

   see [bailly et al., 2013b] for a solution based on hankel matrix

completion

spectral learning of tree automata and grammars

s

np

vp

noun

verb

np

mary

plays

det

noun

the

guitar

some references:

   tree series: [bailly et al., 2010, bailly et al., 2010]
   latent-annotated pid18: [cohen et al., 2012, cohen et al., 2013b]
   id33: [luque et al., 2012, dhillon et al., 2012]
   unsupervised learning of wid18: [bailly et al., 2013a, parikh et al., 2014]
   synchronous grammars: [saluja et al., 2014]

compositional functions over trees

       a

b

c

b

f

a

b

             f

c

    f

    f

c

b

       a
       a
       a

b

b

b

b

c

c

b

a

b

a

b

a

b

    
  

a

b

c

  

b

     
  

c

c

aa

  a

b

b

c

              a
  
              a
  
              a
  

c

b

b

c

a

b

   

  a

  j
  j
  j

   

a

a

a   

c

ac p  ap bq b   ap bqq

  

b   ap cq

inside-outside composition of trees

b

   

a

c

a

c

b

a

   

c

d

b

a

c

b

t     to d ti

note: i-o composition generalizes the notion of concatenation in strings,

i.e., outside trees are pre   xes, inside trees are su   xes

weighted finite tree automata (wfta)

an algebraic model for compositional functions on trees

wfta notation (i)

labeled trees

   t  ku     t  0,   1, . . . ,   ru     ranked alphabet
   t     space of labeled trees over some ranked alphabet

tree:

   t p t     xv, e, lpvqy: a labeled tree
   v     t1, . . . , mu: the set of vertices
   e     txi, jyu: the set of edges forming a tree
   lpvq    t  ku: returns the label of v     (i.e. a symbol in t  ku)

wfta notation (ii)

labeled trees

   t  ku     t  0,   1, . . . ,   ru     ranked alphabet
   t     space of labeled trees over some ranked alphabet

leaf trees and inside compositions:

unary composition
   p   1, t1 p t

binary composition
   p   2, t1, t2 p t

leaf tree
   p   0

  

  

t1

t       

t       rt1s

  

t1
t2
t       rt1, t2s

wfta notation (iii)

labeled trees

   t  ku     t  0,   1, . . . ,   ru     ranked alphabet
   t     space of labeled trees over some ranked alphabet

useful functions (to access the nodes of a tree t):

   rptq: returns the root node of t
   ppt, vq: returns the parent of v
   apt, vq: returns the arity of v (number of children of v)
   cpt, vq: returns the children of v

   if cpt, vq     rv1, . . . vks we use cipt, vq for i-th child
   children are assumed to be ordered from left to right

notation for wfta (iv): tensors

kronecker product:

   for v1 p rn and v2 p rn:
   v1 b v2 p rn2
   example:
   v1     ra, bs
   v2     rc, ds
   v1 b v2     rac, ad, bc, bds

contains all products between elements of v1 and v2

simplifying assumption:

   we consider trees with apt, vq    2
   i.e. tensors of order 3 (two children per parent)

weighted finite tree automata (wfta)

       t  0,   1,   2u: ranked alphabet of order 2        nite set

de   nition: wfta with n states over   

a     x     ,t    u,ta1

  u,ta2

  uy

   n: number of states     positive integer
         p rn: root weights
        p rn: leaf weights     (@   p   0)
   a1
   a2
   note: a2

   p rn  n: transition weights     (@   p   1)
: transition weights     (@   p   2)
   p rn  n2

   is a tensor in rn  n  n packed as a matrix

wfta: inside function

de   nition: any wfta a de   nes an inside function:

  a : t    rn     maps a tree to a vector in rn

   if t is a leaf:
  apt       q         

   if t results from a unary composition:
  apt       rt1sq     a1

    apt1q

   if t results from a binary composition:
  apt       rt1, t2sq     a2

   p  apt1q b   apt2qq

t    

t       

t    

  

t1

  

t1

t2

wfta function:

every wfta a de   nes a function

computed as:

fa : t    r

faptq       j      aptq

weighted finite tree automaton (wfta)

example of inside computation:

loooooooooooooooooomoooooooooooooooooon
apa1
cp  bq b   cqq
ap  b b a2
  j    a2
ap   bloomoonb a2
loooooooooomoooooooooonq
apa1
cp  bq b   cq
looomooonb   cloomoonq
apa1
cp  bq
a2

a2

a

a

b

  b

cp   bloomoonq

a1

c

c

  c

b

  b

useful intuition: latent-variable models as wfta

faptq       t     aptq

  

h0,h1,...,h|v|prns

   each labeled node v is decorated with a latent variable hv p rns
         ph0q
   faptq is a sum   product computation:
  
  
  
faptq     n  

  lpvqrhvs
vpv:apvq   0
lpvqrhv, hcpt,vqs
a1

   faptq is a linear model in the latent space de   ned by   a : t    rn

lpvqrhv, hc1pt,vq, hc2pt,vqs
a2

     ris   aptqris

vpv:apvq   2

vpv:apvq   1

     

  

  

i   1

inside/outside decomposition

v

v

   

tree t

inside tree trvs

outside tree tzv

consider a tree t and one node v:

   inside tree trvs: the subtree of t rooted at v
   outside tree tzv: the rest of t when removing trvs

   trvs p t

   t   : the space of outside trees, i.e. tzv p t   
   foot node    : a tree insertion point (a special symbol     r t  ku)
   an outside tree has exactly one foot node in the leaves

inside/outside composition

b

   

a

c

a

c

b

a

   

c

d

b

a

c

b

   a tree is formed by composing an outside tree with an inside tree
   generalizes pre   x/su   x concatenation in strings
   multiple ways to decompose a full tree into inside/outside trees
   as many as nodes in a tree

outside trees

   outside trees t    p t    are de   ned recursively using compositions:

foot node

unary composition
to p t   ,    p   1

binary composition
to p t   ,    p   2, ti p t

   

   to d
   

  

t           

t        to d   r   s

   to d
   
t        to d   r   , tis

ti

  

   to d

  

   
t        to d   rti,   s

ti

wfta: outside function

de   nition: any wfta a de   nes an outside function:

   if t    is a foot node:
  apt           q       0

  a : t       rn     maps an outside tree to a vector in rn
t           
   to d
   
   to d

   p  aptiq b 1nq

t       

  

  

t       

   if t    results from a unary composition:
  apt        to d   r   sq       aptoqja1

   if t    results from a binary composition:
  apt       tod  rti,   sq       aptoqja2

(note: similar expression for t        to d   r   , tis)

  

   

ti

wfta are fully compositional

a

   

c

d

b

b

   

a

c

a

c

b

for any inside-outside decomposition of a tree:

faptq       aptoqj  aptiq

      aptoqja2

  p  apt1q b   apt2qq

consequences:

   we can isolate the   a and   a vector spaces
   given   a and   a, we can isolate the operators ak
  

a

c

b

plet t     to d tiq
plet ti       rt1, t2sq

hankel matrices of functions over labeled trees

two equivalent representations

   functional: fa : t    r
   matricial: hfa p r|t   |  |t|

                                                  

   

a   
b   
a    
a   
...

b

b

a

b

a
a

a

b

a

a

b

1   1

0
  1

2

2
1   1

3
      

6

1

4
2
0   1   3   7

...

3
...

      

...

                                                   

(the hankel matrix of fa)

   de   nition:
hpto, tiq     fpto d tiq
   sublock for   :
h  pto,   rt1, t2sq     fpto d   rt1, t2sq
   properties:

   |v| ` 1 entries for fptq
   depends on ordering of t    and t
   captures structure

a fundamental theorem about wfta

relates the rank of hf

and the number of states of wfta computing f

let f : t    r be any function over labeled trees.
1. if f     fa for some wfta a with n states    rankphfq    n
2. if rankphfq     n    exists wfta a with n states s.t. f     fa

why fundamental?

proof of (2) gives an algorithm for    recovering    a from the hankel matrix
of fa

structure of low-rank hankel matrices

                          

to

      

      

hf p rt     t

ti

...
...
...
   
...

      

      

                               

to

                 

o p rt     n

  
  
  
  
  
  
           
  
  

  
  
  
  

                  

i p rn  t

     

      

  
  
  

  
  
  

ti

   
   
   

  
  
  

  
  
  

fpto d tiq       aptoqj  aptiq

  aptoq     opto,  q

  aptiq     ip  , tiq

hankel factorizations and operators

                 

  

to

                 

to

  
  
  
   
  

h   p rt     t
  rt1,t2s

                      
looomooon
fpto d   rt1, t2sq

   

  

  

  

ti

o p rt     n a2
   

                  

  
  
  
  
  
  
       
  
  

   p rn  n2

               
               

                 

i p rn  t

i p rn  t

   

   

b

  
  

  
  

  
  

t1

   
   

  
  

  
  

t2

   
   

  
  

  
  

  
  

               

loooooooooooomoooooooooooon
  p  apt1q b   apt2qq

  aptoqj a2

  aptiq

hankel factorizations and operators

h       o a2

   ri b is =   a2

       o` h   ri b is`

note: works with    nite sub-blocks as well

(assuming rankpoq     rankpiq     n)

wfta: application to parsing

s

np

vp

noun

verb

np

mary

plays

det

noun

the

guitar

some intuitions:

   derivation = labeled tree
   learning compositional functions over derivations

=   learning functions over trees

   we are interested in functions computed by wfta

wfta for parsing: key questions

   what is the latent state representing?

   for example: latent real valued embeddings of words and phrases

   what form of supervision do we get?

   full derivations (labeled trees)

i.e., supervised learning of latent-variable grammars

   derivation skeletons (unlabeled trees)

e.g. [pereira and schabes, 1992]

   yields from the grammar (only the leaves)

i.e., grammar induction

parsing and tree automaton

0 asranpr  marys b avpr  plays b anpr  the b   guitarsss
  j

s

asranpr  marys b avpr  plays b anpr  the b   guitarsss

np

anpr  marys

vp

avpr  plays b anpr  the b   guitarss

mary
plays
  mary   plays

np

anpr  the b   guitars

the guitar
  the   guitar

phrase embeddings using wfta

assume a wid18 in chomsky normal form

   n     number of states; i.e. dimensionality of the embedding.
   ranked alphabet:

     0     tthe, mary, plays, . . .u     terminal words
     1     tnoun, verb, det, np, vp, . . .u     unary non-terminals
     2     ts, np, vp, . . .u     binary non-terminals

                nal weights
   t  wu for all w p   0     id27s
   ta1
   ta2

u for all n1 p   1     computes phrase embedding
u for all n2 p   2     computes phrase embedding

n1

n2

phrase embeddings using wfta

n1

u,ta2

uy     wfta
   a     x     ,t  wu,ta1
   faptq       t     apt, sq     scores a derivation
     apt, sq     is the n-dimensional embedding of derivation t

n2

spectral learning algorithm for wfta

assume a is stochastic     i.e. it computes a distribution over derivations

   general algorithm:

   chose a basis     i.e. a set of inside and outside trees
   estimate their empirical probabilities from a sample of derivations
   compute h and thnu
   thnu     one for each non-terminal
   perform svd on h
   recover parameters of a using the wfta theorem
   note: we can also use sub-tree statistics

spectral learning of tree automata

   wfta are a general algebraic framework for compositional functions

   wfta can exploit real-valued embeddings

   there are simple algorithms for learning wftas from samples

outline

1. weighted automata and hankel matrices

2. spectral learning of probabilistic automata

3. id106 for transducers and grammars

sequence tagging
finite-state transductions
tree automata

4. hankel matrices with missing entries

5. conclusion

6. references

learning wfa in more general settings

question: how do we use these approach to learn f :          r where fpxq
does not have a probabilistic interpretation?

examples:

   classi   cation f :          t`1,  1u
   unconstrained real-valued predictions f :          r
   general scoring functions for tagging: f : p         q       r

datahankelmatrixwfalow-rank matrixestimationfactorization andid202example: hankel matrices with missing entries
when learning probabilistic functions. . .
entries in hf are estimated from empirical counts, e.g. fpxq     prxs

but in a general regression setting...
entries in hf are labels observed in the sample, and many may be missing

b, a, ab, aa,
ba, b, aa, a,
aa, bab, b, aa

$      &      % aa, b, bab, a,
,//////////.//////////-
$                              &                              %

(bab,1)
(bbb,0)
(aaa,3)
(a,1)
(ab,1)
(aa,2)
(aba,2)
(bb,0)

,//.//-

            a

.19
.31
.06
.00

            

b
.25
.06
.00
.13

     

 

a

b

ba

                       

                        

 
1
?
2
1
?
0

a
2
?
3
2
?
?

b
1
0
?
?
1
0

     

a

b

aa

ab

ba

bb

id136 of hankel matrices
goal: learn a hankel matrix h p rp  s from partial information, then
apply the hankel trick

information models:

   subset of entries: thpp, sq|pp, sq p iu
   linear measurements: thv|v p vu
   bilinear measurements: tujhv|u p u, v p vu
   constraints between entries: thpp, sq    hpp1, s1q|pp, s, p1, s1q p iu
   noisy versions of all the above

constraints and inductive bias:

   hankel constraints hpp, sq     hpp1, s1q if ps     p1s1
   constraints on entries |hpp, sq|    c
   low-rank constraints/id173 rankphq

empirical risk minimization approach
data: tpxi, yiqun
parameters:

i   1, xi p      , yi p r

   rows and columns p, s         
   (convex) id168 (cid:96) : r    r    r`
   id173 parameter    / rank bound r

optimization (constrained formulation):

n  

i   1

argmin
hprp  s

1
n

optimization (regularized formulation):

(cid:96)pyi, hpxiqq subject to rankphq    r
n  

(cid:96)pyi, hpxiqq `    rankphq

argmin
hprp  s

1
n

i   1

note: these optimization problems are non-convex!

nuclear norm relaxation

nuclear norm: matrix m, }m}      

  

sipmq

in machine learning, minimizing the nuclear norm is a commonly used
convex surrogate for minimizing the rank

id76 for hankel matrix estimation

n  

i   1

argmin
hprp  s

1
n

(cid:96)pyi, hpxiqq `   }h}  

optimization algorithms for hankel matrix
estimation

optimizing the nuclear norm surrogate

   projected/proximal sub-gradient (e.g. [duchi and singer    09])
   frank   wolfe [jaggi and sulovsk    10]
   singular value thresholding [cai, cand`es, and shen    10]

non-convex    heuristics   

   alternating minimization (e.g. [jain, netrapalli, and sanghavi    13])

applications of hankel matrix estimation

   max-margin taggers [quattoni, balle, carreras, and globerson    14]
   unsupervised transducers [bailly, quattoni, and carreras    13]
   unsupervised wid18 [bailly, carreras, luque, quattoni    13]

outline

1. weighted automata and hankel matrices

2. spectral learning of probabilistic automata

3. id106 for transducers and grammars

sequence tagging
finite-state transductions
tree automata

4. hankel matrices with missing entries

5. conclusion

6. references

conclusion

   id106 provide new tools to learn compositional functions

by means of algebraic operations

   key result:

forward-backward recursions    low-rank hankel matrices

   applicable to a wide range of compositional formalisms:

   nite-state automata and transducers, context-free grammars, . . .

   relation to loss-regularized methods, by means of matrix-completion

techniques

spectral learning techniques for weighted automata,

transducers, and grammars

borja balle   

ariadna quattoni   

xavier carreras   

p   q mcgill university

p   q xerox research centre europe

tutorial @ emnlp 2014

outline

1. weighted automata and hankel matrices

2. spectral learning of probabilistic automata

3. id106 for transducers and grammars

sequence tagging
finite-state transductions
tree automata

4. hankel matrices with missing entries

5. conclusion

6. references

anandkumar, a., chaudhuri, k., hsu, d., kakade, s. m., song, l.,
and zhang, t. (2011).
id106 for learning multivariate latent tree structure.
in nips.

anandkumar, a., foster, d. p., hsu, d., kakade, s. m., and liu, y.
(2012a).
a spectral algorithm for id44.
in nips.

anandkumar, a., ge, r., hsu, d., and kakade, s. (2013a).
a tensor spectral approach to learning mixed membership community
models.
in colt.

anandkumar, a., ge, r., hsu, d., kakade, s. m., and telgarsky, m.
(2012b).
tensor decompositions for learning latent variable models.
corr, abs/1210.7559.

anandkumar, a., hsu, d., huang, f., and kakade, s. m. (2012c).
learning mixtures of tree id114.

in nips.

anandkumar, a., hsu, d., and kakade, s. m. (2012d).
a method of moments for mixture models and id48.
in 25th annual conference on learning theory.

anandkumar, a., hsu, d., and kakade, s. m. (2012e).
a method of moments for mixture models and id48.
in colt.

anandkumar, a., hsu, d., and kakade, s. m. (2013b).
tutorial: tensor decomposition algorithms for latent variable model
estimation.
in icml.

bailly, r. (2011).
quadratic weighted automata: spectral algorithm and likelihood
maximization.
in acml.

bailly, r., carreras, x., luque, f., and quattoni, a. (2013a).
unsupervised spectral learning of wid18 as low-rank matrix
completion.

in emnlp.

bailly, r., carreras, x., and quattoni, a. (2013b).
unsupervised spectral learning of    nite state transducers.
in nips.

bailly, r., denis, f., and ralaivola, l. (2009).
grammatical id136 as a principal component analysis problem.
in icml.

bailly, r., habrard, a., and denis, f. (2010).
a spectral approach for probabilistic grammatical id136 on trees.
in alt.

balle, b. (2013).
learning id122: algorithmic and statistical aspects.
phd thesis, universitat polit`ecnica de catalunya.

balle, b., carreras, x., luque, f., and quattoni, a. (2013).
spectral learning of weighted automata: a forward-backward
perspective.
machine learning.

balle, b. and mohri, m. (2012).
spectral learning of general weighted automata via constrained matrix
completion.
in nips.

balle, b., quattoni, a., and carreras, x. (2011).
a spectral learning algorithm for    nite state transducers.
in ecml-pkdd.

balle, b., quattoni, a., and carreras, x. (2012).
local loss optimization in operator models: a new insight into
spectral learning.
in icml.

berry, m. w. (1992).
large-scale sparse singular value computations.

boots, b. and gordon, g. (2011).
an online spectral learning algorithm for partially observable
dynamical systems.
in association for the advancement of arti   cial intelligence.

boots, b., gretton, a., and gordon, g. (2013).
hilbert space embeddings of predictive state representations.
in uai.

boots, b., siddiqi, s., and gordon, g. (2009).
closing the learning-planning loop with predictive state
representations.
in proceedings of robotics: science and systems vi.

boots, b., siddiqi, s., and gordon, g. (2011).
closing the learning planning loop with predictive state
representations.
international journal of robotics research.

boyd, s., parikh, n., chu, e., peleato, b., and eckstein, j. (2011).
distributed optimization and statistical learning via the alternating
direction method of multipliers.
foundations and trends in machine learning.

brand, m. (2006).
fast low-rank modi   cations of the thin singular value decomposition.
id202 and its applications, 415(1):20   30.

carlyle, j. w. and paz, a. (1971).
realizations by stochastic    nite automata.
journal of computer systems science.

chaganty, a. t. and liang, p. (2013).
spectral experts for estimating mixtures of id75s.
in icml.

cohen, s. b., collins, m., foster, d. p., stratos, k., and ungar, l.
(2013a).
tutorial: spectral learning algorithms for natural language processing.
in naacl.

cohen, s. b., stratos, k., collins, m., foster, d. p., and ungar, l.
(2012).
spectral learning of latent-variable pid18s.
acl.

cohen, s. b., stratos, k., collins, m., foster, d. p., and ungar, l.
(2013b).
experiments with spectral learning of latent-variable pid18s.
in naacl-hlt.

cohen, s. b., stratos, k., collins, m., foster, d. p., and ungar, l.
(2014).
spectral learning of latent-variable pid18s: algorithms and sample
complexity.
journal of machine learning research.

dempster, a. p., laird, n. m., and rubin, d. b. (1977).
maximum likelihood from incomplete data via the em algorithm.
journal of the royal statistical society.

denis, f. and esposito, y. (2008).
on rational stochastic languages.
fundamenta informaticae.

dhillon, p. s., rodu, j., collins, m., foster, d. p., and ungar, l. h.
(2012).
spectral id33 with latent variables.
in emnlp-conll.

dupont, p., denis, f., and esposito, y. (2005).
links between probabilistic automata and id48:
id203 distributions, learning models and induction algorithms.

pattern recognition.

fliess, m. (1974).
matrices de hankel.
journal de math  ematiques pures et appliqu  ees.

foster, d. p., rodu, j., and ungar, l. h. (2012).
spectral id84 for id48s.
corr, abs/1203.6130.

gordon, g. j., song, l., and boots, b. (2012).
tutorial: spectral approaches to learning latent variable models.
in icml.

halko, n., martinsson, p., and tropp, j. (2011).
finding structure with randomness: probabilistic algorithms for
constructing approximate matrix decompositions.
siam review, 53(2):217   288.

hamilton, w., fard, m., and pineau, j. (2013a).
modelling sparse dynamical systems with compressed predictive state
representations.

in icml.

hamilton, w. l., fard, m. m., and pineau, j. (2013b).
modelling sparse dynamical systems with compressed predictive state
representations.
in proceedings of the 30th international conference on machine
learning.

hsu, d. and kakade, s. m. (2013).
learning mixtures of spherical gaussians: moment methods and
spectral decompositions.
in itcs.

hsu, d., kakade, s. m., and zhang, t. (2009).
a spectral algorithm for learning id48.
in colt.

hulden, m. (2012).
treba: e   cient numerically stable em for pfa.
in icgi.

jaeger, h. (2000).
observable operator models for discrete stochastic time series.

neural computation, 12(6):1371   1398.

littman, m., sutton, r. s., and singh, s. (2002).
predictive representations of state.
in advances in neural information processing systems.

luque, f., quattoni, a., balle, b., and carreras, x. (2012).
spectral learning in non-deterministic id33.
in eacl.

marcus, m., marcinkiewicz, m., and santorini, b. (1993).
building a large annotated corpus of english: the id32.
computational linguistics, 19(2):313   330.

parikh, a., song, l., ishteva, m., teodoru, g., and xing, e. (2012).
a spectral algorithm for latent junction trees.
in uai.

parikh, a. p., cohen, s. b., and xing, e. (2014).
spectral unsupervised parsing with additive tree metrics.
in proceedings of acl.

parikh, a. p., song, l., and xing, e. (2011).

a spectral algorithm for latent tree id114.
in icml.

pereira, f. and schabes, y. (1992).
inside-outside reestimation from partially bracketed corpora.
in proceedings of the 30th annual meeting on association for
computational linguistics, pages 128   135. association for
computational linguistics.

quattoni, a., balle, b., carreras, x., and globerson, a. (2014).
spectral id173 for max-margin sequence tagging.
in icml.

rabiner, l. r. (1990).
a tutorial on id48 and selected applications in
id103.
in waibel, a. and lee, k., editors, readings in id103,
pages 267   296.

recasens, a. and quattoni, a. (2013).
spectral learning of sequence taggers over continuous sequences.
in ecml-pkdd.

rosencrantz, m., gordon, g., and thrun, s. (2004).
learning low dimensional predictive representations.
in proceedings of the 21st international conference on machine
learning.

saluja, a., dyer, c., and cohen, s. b. (2014).
latent-variable synchronous id18s for hierarchical translation.
proceedings of emnlp.

siddiqi, s. m., boots, b., and gordon, g. (2010).
reduced-rank id48.
in aistats.

singh, s., james, m., and rudary, m. (2004).
predictive state representations: a new theory for modeling dynamical
systems.
in proceedings of the 20th conference on uncertainty in arti   cial
intelligence.

song, l., boots, b., siddiqi, s., gordon, g., and smola, a. (2010).
hilbert space embeddings of id48.

in icml.

song, l., ishteva, m., parikh, a., xing, e., and park, h. (2013).
hierarchical tensor decomposition of latent tree id114.
in icml.

stratos, k., rush, a. m., cohen, s. b., and collins, m. (2013).
spectral learning of re   nement id48s.
in conll.

verwer, s., eyraud, r., and higuera, c. (2012).
results of the pautomac probabilistic automaton learning
competition.
in icgi.

wiewiora, e. (2007).
modeling id203 distributions with predictive state
representations.
phd thesis, university of california at san diego.

