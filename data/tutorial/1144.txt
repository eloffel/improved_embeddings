learning semantic
relations from text

preslav nakov1,
diarmuid    s  aghdha2,
vivi nastase3,
stan szpakowicz4

1 qatar computing research institute, hbku

2 vocal iq

3 fondazione bruno kessler

4 university of ottawa

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

outline

1

introduction

2 semantic relations

3 features

4 supervised methods

5 unsupervised methods

6 embeddings

7 wrap-up

learning semantic relations from text

2 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

outline

1

introduction

2 semantic relations

3 features

4 supervised methods

5 unsupervised methods

6 embeddings

7 wrap-up

learning semantic relations from text

3 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

motivation

the connection is indispensable to the expression of
thought. without the connection, we would not be able
to express any continuous thought, and we could only
list a succession of images and ideas isolated from
each other and without any link between them.
[tesni  re, 1959]

learning semantic relations from text

4 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

what is it all about?

learning semantic relations from text

5 / 97

opportunity and curiosity find similar rocks on mars.introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

what is it all about?

learning semantic relations from text

5 / 97

opportunity and curiosity find similar rocks on mars.mars roveris_ais_alocated_onexplorer_ofintroduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

what is it all about? (1)

semantic relations

matter a lot
connect up entities in a text
together with entities make up a good chunk of the
meaning of that text
are not terribly hard to recognize

learning semantic relations from text

6 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

what is it all about? (2)

semantic relations between nominals

matter even more in practice
are the target for knowledge acquisition
are key to reaching the meaning of a text
their recognition is fairly feasible

learning semantic relations from text

6 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

historical overview (1)

capturing and describing world knowledge

artistotle   s organon

includes a treatise on categories

objects in the natural world are put into categories called
   `                    (ta legomena, things which are said)
organization based on the class inclusion relation

then, for 20 centuries:
other philosophers
some botanists, zoologists

in the 1970s: realization that a robust arti   cial intelligence
(ai) system needs the same kind of knowledge

capture and represent knowledge: machine-friendly
intersection with language: inevitable

learning semantic relations from text

7 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

historical overview (2)

indian linguistic tradition

p  an. ini   s as.t.  adhy  ay    

rules describing the process of generating a sanskrit
sentence from a semantic representation
semantics is conceptualized in terms of k  arakas, semantic
relations between events and participants, similar to
semantic roles
covers noun-noun compounds comprehensively from the
perspective of word formation, but not semantics
later, commentators such as k  aty  ayana and pata  jali:
compounding is only supported by the presence of a
semantic relation between entities

learning semantic relations from text

7 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

historical overview (3)

ferdinand de saussure

course in general linguistics [de saussure, 1959]

taught 1906-1911; published in 1916

learning semantic relations from text

7 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

historical overview (4)

ferdinand de saussure

course in general linguistics: two types of relations which
   correspond to two different forms of mental activity, both
indispensable to the workings of language   

syntagmatic relations

hold in context

associative (paradigmatic) relations

come from accumulated experience

but no explicit list of relations was proposed

learning semantic relations from text

7 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

historical overview (5)

ferdinand de saussure

syntagmatic relations hold between two or more terms in a
sequence in praesentia, in a particular context:    words as
used in discourse, strung together one after the other,
enter into relations based on the linear character of
languages     words must be arranged consecutively in a
spoken sequence. combinations based on sequentiality
may be called syntagmas.   

learning semantic relations from text

7 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

historical overview (6)

ferdinand de saussure

associative (paradigmatic) relations come from
accumulated experience and hold in absentia:    outside the
context of discourse, words having something in common
are associated together in the memory. [. . . ] all these
words have something or other linking them. this kind of
connection is not based on linear sequence. it is a
connection in the brain. such connections are part of that
accumulated store which is the form the language takes in
an individual   s brain.   

learning semantic relations from text

7 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

historical overview (7)

syntagmatic vs. paradigmatic relations

[harris, 1987]: frequently occurring instances of
syntagmatic relations may become part of our memory,
thus becoming paradigmatic
[gardin, 1965]: instances of paradigmatic relations are
derived from accumulated syntagmatic data
this re   ects current thinking on id36 from
open texts.

learning semantic relations from text

7 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

historical overview (8)

predicate logic [frege, 1879]

inherently relational formalism
e.g., the sentence    google buys youtube.    is represented as

buy(google, youtube)

learning semantic relations from text

7 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

historical overview (9)

neo-davidsonian logic representation

additional variables represent the event or relation
it can thus be explicitly modi   ed and subject to
quanti   cation
   e instanceofbuying(e)     agent(e, google)     patient(e, youtube)
or perhaps
   e instanceof(e, buying)     agent(e, google)     patient(e, youtube)

existential graphs [peirce, 1909]

learning semantic relations from text

7 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

historical overview (10)

the dual nature of semantic relations

in logic: predicates

used in ai to support knowledge-based agents and
id136

in graphs: arcs connecting concepts

used in nlp to represent factual knowledge
thus, mostly binary relations

in ontologies
as the target in ie
...

learning semantic relations from text

7 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

historical overview (11)

the rise of reasoning systems

[mccarthy, 1958]: logic-based reasoning, no language
early nlp systems with semantic knowledge

[winograd, 1972]: interactive english dialogue system
[charniak, 1972]: understanding children   s stories
conceptual shift from the    shallow    architecture of primitive
conversation systems such as eliza [weizenbaum, 1966]

large-scale hand-crafted ontologies

cyc
openmind common sense
mindpixel
freebase     truly large-scale

learning semantic relations from text

7 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

historical overview (12)

at the cross-roads between knowledge and language

[sp  rck-jones, 1964]: lexical relations found in a dictionary
can be learned automatically from text
[quillian, 1962]: semantic network

a graph in which meaning is modelled by labelled
associations between words

vertices are concepts onto which words in a text are mapped
connections     relations between such concepts

id138 [fellbaum, 1998]

155,000 words (nouns, verbs, adjectives, adverbs)
a dozen semantic relations, e.g., synonymy, antonymy,
hypernymy, meronymy

learning semantic relations from text

7 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

historical overview (13)

automating knowledge acquisition

learning ontological relations

is-a [hearst, 1992]
part-of [berland & charniak, 1999]

id64 [patwardhan & riloff, 2007; ravichandran & hovy, 2002]
open id36

no pre-speci   ed list/type of relations
learn patterns about how relations are expressed, e.g.,

pos [fader&al., 2011]
paths in a syntactic tree [ciaramita&al., 2005]
sequences of high-frequency words [davidov & rappoport, 2008]

hard to map to    canonical    relations

learning semantic relations from text

7 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

why should we care about semantic relations?

relation learning/extraction can help

building knowledge repositories
text analysis
nlp applications

information extraction
information retrieval
text summarization
machine translation
id53
id141
recognizing id123
thesaurus construction
semantic network construction
word-sense disambiguation
language modelling

learning semantic relations from text

8 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

example application: information retrieval

[cafarella&al., 2006]

list all x such that x causes cancer
list all x such that x is part of an automobile engine
list all x such that x is material for making a submarine   s
hull
list all x such that x is a type of transportation
list all x such that x is produced from cork trees

learning semantic relations from text

9 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

example application: id151

[nakov, 2008]

if the smt system knows that

oil price hikes is translated to portuguese as
aumento nos pre  os do petr  leo

note: this is hard to get word-for-word!

if we further interpret/paraphrase oil price hikes as

hikes in oil prices
hikes in the prices of oil
...

then we can use the same    uent portuguese translation for
the paraphrases

learning semantic relations from text

10 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

outline

1

introduction

2 semantic relations

3 features

4 supervised methods

5 unsupervised methods

6 embeddings

7 wrap-up

learning semantic relations from text

11 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

two perspectives on semantic relations

relations between concepts
. . . arise from, and capture, knowledge about the world

relations between nominals
. . . arise from, and capture, particular events/situations expressed in texts

learning semantic relations from text

12 / 97

opportunity and curiosity find similar rocks on mars.mars roveris_ais_alocated_onexplorer_ofintroduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

two perspectives on semantic relations

relations between concepts
. . . arise from, and capture, knowledge about the world
. . . can be found in texts!

relations between nominals
. . . arise from, and capture, particular events/situations expressed in texts
. . . can be found using information from knowledge bases

learning semantic relations from text

12 / 97

opportunity and curiosity find similar rocks on mars.mars roveris_ais_alocated_onexplorer_ofintroduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

[casagrande & hale, 1967]

speakers

of
asked
an exotic
language
to give de   nitions for
a given list of words,
then
13
relations from these
de   nitions.

extracted

relation
attributive
function
operational
exempli   cation
synonymy
provenience
circularity
contingency
spatial
comparison
class inclusion
antonymy
grading

example
toad - small
ear - hearing
shirt - wear
circular - wheel
thousand - ten hundred
milk - cow
x is de   ned as x
lightning - rain
tongue - mouth
wolf - coyote
bee - insect
low - high
monday - sunday

learning semantic relations from text

13 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

[chaf   n & hermann, 1984]

asked humans to group instances of 31 semantic relations.
found    ve coarser classes.
relation
constrasts
similars
class inclusion
part-whole
case relations     agent, instrument

example
night - day
car - auto
vehicle - car
airplane - wing

learning semantic relations from text

13 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

semantic relations in noun compounds (1)

noun compounds (ncs)

de   nition: sequences of two or more nouns that function
as a single noun, e.g.,

silkworm
olive oil
healthcare reform
plastic water bottle
colon cancer tumor suppressor protein

learning semantic relations from text

14 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

semantic relations in noun compounds (2)

properties of noun compounds

encode implicit relations: hard to interpret

taxi driver is    a driver who drives a taxi   
embassy driver is    a driver who is employed by/drives for an
embassy   
embassy building is    a building which houses, or belongs to,
an embassy   

abundant: cannot be ignored

cover 4% of the tokens in the reuters corpus

highly productive: cannot be listed in a dictionary

60% of the ncs in bnc occur just once

learning semantic relations from text

14 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

semantic relations in noun compounds (3)

noun compounds as a microcosm: representation issues
re   ect those for general semantic relations

voluminous literature on their semantics

www.cl.cam.ac.uk/~do242/resources/compound_bibliography.html

two complementary perspectives

linguistic:    nd the most comprehensive explanatory
representation
nlp: select the most useful representation for a particular
application

computationally tractable
giving informative output to downstream systems

learning semantic relations from text

14 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

semantic relations in noun compounds (4)

do the relations in noun compounds come from a small
closed inventory?

is there a (reasonably)
in other words,
small set of relations which could cover
completely what occurs in texts in the
vicinity of (simple) noun phrases?

af   rmative: most linguists

early descriptive work [grimm, 1826; jespersen, 1942; noreen, 1904]
generative linguistics [levi, 1978; li, 1971; warren, 1978]

negative: some linguists e.g., [downing, 1977]

learning semantic relations from text

14 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

[warren, 1978] (1)

relations arising from a comprehensive study of the brown corpus:

a four-level hierarchy of relations
six major semantic relations

relation
possession
location
purpose
activity-actor
resemblance
constitute

example
family estate
water polo
water bucket
crime syndicate
cherry bomb
clay bird

learning semantic relations from text

15 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

[warren, 1978] (2)

a four-level hierarchy of relations

l1: constitute

l2: source-result
l2: result-source
l2: copula

l3: adjective-like_modi   er
l3: subsumptive
l3: attributive

l4: animate_head (e.g., girl friend)
l4: inanimate_head (e.g., house boat)

learning semantic relations from text

15 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

[levi, 1978] (1)

relations (recoverable deletable predicates) which underlie all
compositional non-nominalized compounds in english

rdp
cause1
cause2
have1
have2
make1
make2
use
be
in
for
from
about

example
tear gas
drug deaths
apple cake
lemon peel
silkworm
snowball
steam iron
soldier ant
   eld mouse
horse doctor
olive oil
price war

role
object
subject
object
subject
object
subject
object
object
object
object
object
object

traditional name
causative
causative
possessive/dative
possessive/dative
productive/composit.
productive/composit.
instrumental
essive/appositional
locative
purposive/benefactive
source/ablative
topic

learning semantic relations from text

15 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

[levi, 1978] (2)

nominalizations

act
product
agent
patient

subjective
parental refusal
clerical errors

   

student inventions

objective
dream analysis
musical critique
city planner

   

multi-modi   er
city land acquisition
student course ratings

   
   

problem: spurious ambiguity
horse doctor is for (rdp)
horse healer is agent (nominalization)

learning semantic relations from text

15 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

[vanderwende, 1994]

question
relation
who/what?
subject
whom/what?
object
where?
locative
when?
time
possessive whose?
whole-part what is it part of?
part-whole what are its parts?
equative
instrument how?
purpose
material
causes
caused-by what causes it?

what for?
made of what?
what does it cause?

what kind of?

example
press report
accident report
   eld mouse
night attack
family estate
duck foot
daisy chain
   ounder    sh
paraf   n cooker
bird sanctuary
alligator shoe
disease germ
drug death

learning semantic relations from text

15 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

desiderata for building a relation inventory

1

2

3

4

5

6

the inventory should have good coverage
relations should be disjoint, and should each describe a
coherent concept
the class distribution should not be overly skewed or sparse
the concepts underlying the relations should generalize to other
linguistic phenomena
the guidelines should make the annotation process as simple
as possible
the categories should provide useful semantic information

(adapted from [   s  aghdha, 2007])

learning semantic relations from text

15 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

[   s  aghdha, 2007]

be (identity, substance-form, similarity)
have (possession, condition-experiencer, property-object,
part-whole, group-member)
in (spatially located object, spatially located event,
temporarily located object, temporarily located event)
actor (participant-event, participant-participant)
inst (participant-event, participant-participant)
about (topic-object, topic-collection, focus-mental activity,
commodity-charge)

e.g., tax law is topic-object, crime investigation is focus-mental activity,
and they both are also about.

learning semantic relations from text

15 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

[barker & szpakowicz, 1998]

an inventory of 20 semantic relations.

relation example
possessor
product
property
purpose
result
source
time
topic

company car
automobile factory
blue car
concert hall
cold virus
north wind
morning class
safety standard

relation example
student protest
agent
student price
bene   ciary
exam anxiety
cause
printer tray
container
paper tray
content
destination game bus
equative
instrument
located
location
material
object

player coach
laser printer
home town
lab printer
water vapor
horse doctor

learning semantic relations from text

15 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

[nastase & szpakowicz, 2003]

a two-level hierarchy of 31 semantic relations

causal

participant

(4 relations)
cause:    u virus,
effect: exam anxiety, . . .
(12 relations)
agent: student protest,
instrument: laser printer, . . .

quality (8 relations)

spatial

temporal

manner: stylish writing,
measure: expensive book, . . .
(4 relations)
direction: outgoing mail,
location: home town, . . .
(3 relations)
frequency: daily experience,
time_at: morning exercise, . . .

learning semantic relations from text

15 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

[girju, 2005]

a list of 21 noun compound semantic relations: a subset of the
35 general semantic relations of [moldovan&al.,2004].

relation
manner
means
experiencer
recipient
measure
theme
result

example
style performance
bus service
disease victim
worker fatalities
session day
car salesman
combustion gas

relation
possession
attribute-holder
agent
temporal
depiction-depicted
part-whole
is-a
cause
make/produce
instrument
location/space
purpose
source
topic

example
family estate
quality sound
crew investigation
night    ight
image team
girl mouth
dallas city
malaria mosquito
shoe factory
pump drainage
texas university
migraine drug
olive oil
art museum

learning semantic relations from text

15 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

[tratz & hovy, 2010]

[tratz&hovy, 2010]

new inventory
43 relations in 10 categories
developed through an iterative crowd-sourcing
maximize agreement between annotators

analysis: all previous inventories have commonalities

e.g., have categories for locative, possessive, purpose, etc.
cover essentially the same semantic space

but differ in the exact way of partitioning that space

learning semantic relations from text

15 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

[rosario, 2001]: biomedical relations (1)

18 biomedical noun compound relations (initially 38).

relation
subtype
activity/physical_process
produce_genetically
cause
characteristic
defect
person_af   icted
attribute_of_clinical_study
procedure
frequency/time_of
measure_of
instrument
...

example
headaches migraine
virus reproduction
polyomavirus genome
heat shock
drug toxicity
hormone de   ciency
aids patient
headache parameter
genotype diagnosis
in   uenza season
relief rate
laser irradiation
...

learning semantic relations from text

15 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

[rosario, 2001]: biomedical relations (2)

18 biomedical noun compound relations (initially 38).

relation
...
object
purpose
topic
location
material
defect_in_location

example
...
bowel transplantation
headache drugs
headache questionnaire
brain artery
aloe gel
lung abscess

learning semantic relations from text

15 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

the opposite view: no small set of semantic
relations

much opposition to the previous work

[zimmer, 1971]: so much variety of relations that it is
simpler to categorize the semantic relations that cannot
be encoded in compounds
[downing, 1977]

plate length (   what your hair is when it drags in your food   )
   the existence of numerous novel compounds like these
guarantees the futility of any attempt to enumerate an
absolute and    nite class of compounding relationships.   

learning semantic relations from text

16 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

noun compounds: using lexical paraphrases (1)

lexical items instead of abstract relations

the hidden relation in a noun compound can be made explicit
in a paraphrase.

e.g., weather report

abstract
topic

lexical

report about the weather
report forecasting the weather

learning semantic relations from text

17 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

noun compounds: using lexical paraphrases (2)

using prepositions: the idea

[lauer, 1995] used just eight prepositions

of, for, in, at, on, from, with, about

olive oil is    oil from olives   
night    ight is       ight at night   
odor spray is    spray for odors   

easy to extract from text or the web [lapata & keller, 2004]

[srikumar&roth, 2013] 32 relations / 34 prepositions

good at boxing     activity
opened by annie     agent
travel by road     journey
...

learning semantic relations from text

17 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

noun compounds: using lexical paraphrases (3)

using prepositions: the issues

prepositions are polysemous, e.g., different of

school of music
theory of computation
bell of (the) church

unnecessary distinctions, e.g., in vs. on vs. at

prayer in (the) morning
prayer at night
prayer on (a) feast day

some compounds cannot be paraphrased with
prepositions

woman driver

strange paraphrases

honey bee     is it    bee for honey   ?

learning semantic relations from text

17 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

noun compounds: using lexical paraphrases (4)

using id141 verbs

[nakov, 2008]: a relation is represented as a distribution
over verbs and prepositions which occur in texts

e.g., olive oil is    oil that is extracted from olives    or    oil that
is squeezed from olives   
rich representation, close to what downing [1977]
demanded
allows comparisons, e.g., olive oil vs. sea salt

similar: both match the paraphrase    n1 is extracted from n2   
different: salt is not squeezed from the sea

learning semantic relations from text

17 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

noun compounds: using lexical paraphrases (5)

abstract relations vs. prepositions vs. verbs

abstract relations [nastase & szpakowicz, 2003; kim & baldwin, 2005; girju, 2007;   
s  aghdha & copestake, 2007]

malaria mosquito: cause
olive oil: source

prepositions [lauer, 1995]

malaria mosquito: with
olive oil: from

verbs [finin, 1980; vanderwende, 1994; kim & baldwin 2006; butnariu & veale 2008; nakov & hearst
2008]

malaria mosquito: carries, spreads, causes, transmits, brings, has
olive oil: comes from, is made from, is derived from

learning semantic relations from text

17 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

noun compounds: using lexical paraphrases (6)

note 1 on id141 verbs

can paraphrase a noun compound

chocolate bar: be made of, contain, be composed of, taste like

can also express an abstract relation

make2: be made of, be composed of, consist of, be manufactured
from

... but can also be nc-speci   c
orange juice: be squeezed from
bacon pizza: be topped with
chocolate bar: taste like

learning semantic relations from text

17 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

noun compounds: using lexical paraphrases (7)

note 2 on id141 verbs

single verb

malaria mosquito: cause
olive oil: be extracted from

multiple verbs

malaria mosquito: cause, carry, spread, transmit, bring, ...
olive oil: be extracted from, come from, be made from, ...

distribution over verbs (semeval-2010 task 9)

malaria mosquito: carry (23), spread (16), cause (12), transmit (9),
bring (7), be infected with (3), infect with (3), give (2), ...
olive oil: come from (33), be made from (27), be derived from (10), be
made of (7), be pressed from (6), be extracted from (5), ...

learning semantic relations from text

17 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

noun compounds: using lexical paraphrases (8)

free paraphrases at semeval-2013 task 4 [hendrickx & al., 2013]

e.g., for onion tears
tears from onions
tears due to cutting onion
tears induced when cutting onions
tears that onions induce
tears that come from chopping onions
tears that sometimes    ow when onions are chopped
tears that raw onions give you
...

learning semantic relations from text

17 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

relations between concepts:
semantic relations in ontologies

the easy ones:

is-a
part-of

the backbone of any ontology.

learning semantic relations from text

18 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

relations between concepts:
semantic relations in ontologies

the easy ones?

is-a

chocolate is-a food     class inclusion
toblerone is-a chocolate     class membership

and also [wierzbicka, 1984]

chicken is-a bird     taxonomic (is-a-kind-of)
adornment is-a decoration     functional
(is-used-as-a-kind-of)
. . .

part-of

learning semantic relations from text

18 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

relations between concepts:
semantic relations in ontologies

the easy ones?

is-a
part-of [winston & al., 1987]

relation
component-integral object
member-collection
portion-mass
stuff-object
feature-activity
place-area

example
pedal - bike
ship -    eet
slice - pie
steel - car
paying - shopping
everglades - florida

learning semantic relations from text

18 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

relations between concepts:
semantic relations in ontologies

the easy ones?

is-a
part-of [winston & al., 1987]

motivation: lack of transitivity

1 simpson   s arm is part of simpson(   s body).
2 simpson is part of the philosophy department.
3

*simpson   s arm is part of the philosophy department.
component-object is incompatible with member-collection

learning semantic relations from text

18 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

relations in id138

relation
synonym
antonym
hypernym
hyponym
member-of holonym
has-member meronym
part-of holonym
has-part meronym
substance-of holonym
has-substance meronym
domain - topic
domain - usage
domain member - topic
attribute
derived form
derived form

example
day (sense 2) / time
day (sense 4) / night
berry (sense 2) / fruit
fruit (sense 1) / berry
germany / nato
germany / sorbian
germany / europe
germany / mannheim
wood (sense 1) / lumber
lumber (sense 1) / wood
line (sense 7) / military
line (sense 21) / channel
ship / porthole
speed (sense 2) / fast
speed (sense 2) / quick
speed (sense 2) / accelerate

learning semantic relations from text

19 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

conclusions

no consensus on a comprehensive list of relations    t for all
purposes and all domains.
some shared properties of relations, and of relation
schemata.

learning semantic relations from text

20 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

properties of relations (1)

useful distinctions

ontological vs. idiosyncratic
binary vs. n-ary
targeted vs. emergent
first-order vs. higher-order
general vs. domain-speci   c

learning semantic relations from text

21 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

properties of relations (2)

ontological vs. idiosyncratic

ontological

come up practically the same in numerous contexts

e.g., is-a(apple, fruit)

can be extracted with both supervised and unsupervised
methods
idiosyncratic

highly sensitive to the context

e.g., content-container(apple, basket)
best extracted with supervised methods

note: parallel to paradigmatic vs. syntagmatic relations in the
course in general linguistics [de saussure, 1959].

learning semantic relations from text

21 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

properties of relations (3)

binary vs. n-ary

binary

most relations
our focus here

n-ary

good for verbs that can take multiple arguments, e.g., sell
can be represented as frames

e.g., a selling event can invoke a frame covering relations
between a buyer, a seller, an object_bought and price_paid

learning semantic relations from text

21 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

properties of relations (4)

targeted vs. emergent

targeted

coming from a    xed inventory
e.g., {cause, source, target, time, location}

emergent

not    xed in advance
can be extracted using patterns over parts-of-speech
e.g., (v | v (n | adj | adv | pron | det)* pp)
can extract invented, is located in or made a deal with
could also use id91 to group similar relations

but then naming the clusters is hard

learning semantic relations from text

21 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

properties of relations (5)

first-order vs. higher-order

first-order

e.g., is-a(apple, fruit)
most relations

higher-order

e.g., believes(john, is-a(apple, fruit))
can be expressed as conceptual graphs [sowa, 1984]
important in id29 [liang & al., 2011; lu & al., 2008]
also in biomedical event extraction [kim & al., 2009]
e.g.,    in this study we hypothesized that the
phosphorylation of traf2 inhibits binding to the cd40
cytoplasmic domain.   

e1: phosphorylation(theme:traf2),
e2: binding(theme1:traf2, theme2:cd40,
site:cytoplasmic domain),
e3: negative_regulation(theme:e2, cause:e1).

learning semantic relations from text

21 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

properties of relations (6)

general vs. domain-speci   c

general

likely to be useful in processing all kinds of text or in
representing knowledge in any domain
e.g., location, possession, causation, is-a, or part-of

domain-speci   c

only relevant to a speci   c text genre or to a narrow domain
e.g., inhibits, activates, phosphorylates for gene/protein events

learning semantic relations from text

21 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

properties of relation schemata (1)

useful distinctions

coarse-grained vs. fine-grained
flat vs. hierarchical
closed vs. open

learning semantic relations from text

22 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

properties of relation schemata (2)

coarse-grained vs. fine-grained

coarse-grained

e.g., 5 relations

fine-grained

e.g., 30 relations

in   nite, in the extreme

every interaction between entities is a distinct relation with
unique properties
not very practical as there is no generalization
however, a distribution over paraphrases is useful

learning semantic relations from text

22 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

properties of relation schemata (3)

flat vs. hierarchical

flat

most inventories

hierarchical

e.g., nastase & szpakowicz   s [2003] schema has 5
top-level and 30 second-level relations
e.g., warren   s [1978] schema has four levels:
e.g., possessor-legal belonging is a subrelation of
possessor-belonging, which is a subrelation of whole-part
under the top-level relation possession

learning semantic relations from text

22 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

properties of relation schemata (4)

closed vs. open

closed

most inventories

open

used for the web

re   ects the distinction between targeted and emergent
relations.

learning semantic relations from text

22 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

the focus of this tutorial

our focus

relations between entities mentioned in the same sentence
expressed linguistically as nominals

terminology

relation type

e.g., hyponymy, meronymy, container, product, location

relation instance

e.g.,    chocolate contains caffeine   

learning semantic relations from text

23 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

nominal (1)

the standard de   nition

a phrase that behaves syntactically like a noun or a noun
phrase [quirk & al., 1985]

learning semantic relations from text

24 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

nominal (2)

our narrower de   nition

a common noun (chocolate, food)
a proper noun (godiva, belgium)
a multi-word proper name (united nations)
a deverbal noun (cultivation, roasting)
a deadjectival noun ([the] rich)
a base noun phrase built of a head noun with optional
premodi   ers (processed food, delicious milk
chocolate)
(recursively) a sequence of nominals (cacao tree,
cacao tree growing conditions)

learning semantic relations from text

24 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

some clues for extracting semantic relations (1)

explicit clue

a phrase linking the entity mentions in a sentence

e.g.,    chocolate is a raw or processed food produced from the seed of
the tropical theobroma cacao tree.   
issue 1: ambiguity

in may indicate a temporal relation (chocolate in the 20th
century)
but also a spatial relation (chocolate in belgium)

issue 2: over-speci   cation

the relation between chocolate and cultures in    chocolate was
prized as a health food and a divine gift by the mayan and aztec
cultures.   

learning semantic relations from text

25 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

some clues for extracting semantic relations (2)

implicit clue

the relation can be implicit
e.g., in noun compounds

clues come from knowledge about the entities
e.g., cacao tree: cacao are seeds produced by a tree

learning semantic relations from text

25 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

some clues for extracting semantic relations (3)

implicit clue
when an entity is an occurrence (event, activity, state)
expressed by a deverbal noun such as cultivation

the relation mirrors that between the underlying verb and
its arguments

e.g., in    the ancient mayans cultivated chocolate   , chocolate is the
theme

thus, a theme relation in chocolate cultivation

we do not treat nominalizations separately: typically, they
can be also analyzed as normal nominals

but they are treated differently

in some linguistic theories [levi, 1978]
in some computational linguistics work [lapata, 2002]

learning semantic relations from text

25 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

our assumptions

entities are given

no entity identi   cation
no entity disambiguation

entities in the same sentence, no coreference, no ellipsis

not of direct interest: existing ontologies, knowledge bases
and other repositories

though useful as seed examples or training data

learning semantic relations from text

26 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

outline

1

introduction

2 semantic relations

3 features

4 supervised methods

5 unsupervised methods

6 embeddings

7 wrap-up

learning semantic relations from text

27 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

learning relations

methods of learning semantic relations

supervised

pros: perform better
cons: require labeled data and feature representation

unsupervised

pros: scalable, suitable for id10
cons: perform worse

learning semantic relations from text

28 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

learning relations: features

purpose: map a pair of terms to a vector
entity features and relational features [turney, 2006]

learning semantic relations from text

29 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

features

entity features
. . . capture some representation of the meaning of an entity    
the arguments of a relation

relational features
. . . directly characterize the relation     the interaction between its
arguments

learning semantic relations from text

30 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

entity features (1)

basic entity features

the string value of the argument (possibly lemmatized or
stemmed)
examples:

string value
individual words/stems/lemmata

pros: often informative enough for good relation assignment
cons: too sparse

learning semantic relations from text

31 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

entity features (2)

background entity features

syntactic information, e.g., grammatical role
semantic information, e.g., semantic class
can use task-speci   c inventories, e.g.,

ace entity types
id138 features

pros: solve the data sparseness problem
cons: manual resources required

learning semantic relations from text

31 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

entity features (3)

background entity features

clusters as semantic class information

brown clusters [brown&al., 1992]
id91 by committee [pantel & lin, 2002]
id44 [blei&al., 2003]

learning semantic relations from text

31 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

entity features (4)

background entity features

direct representation of co-occurrences in feature space

coordination (and/or) [   s  aghdha & copestake, 2008], e.g., dog and cat
distributional representation
relational-semantic representation

id27s [nguyen & grishman, 2014; hashimoto&al., 2015]

learning semantic relations from text

31 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

entity features (5)

background entity features

distributional representation

learning semantic relations from text

31 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

entity features (6)

background entity features

distributional representation for the noun paper

what a paper can do: propose, say
what one can do with a paper: read, publish
typical adjectival modi   ers: white, recycled
noun modi   ers: toilet, consultation
nouns connected via prepositions: on environment, for
meeting, with a title

pros: captures word meaning by aggregating all
interactions (found in a large collection of texts)
cons: lumps together different senses

ink refers to the medium for writing
propose refers to writing/publication/document

learning semantic relations from text

31 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

entity features (7)

background entity features

relational-semantic representation:
it uses related concepts from a semantic network or a
formal ontology

pros: based on word senses, not on words
cons: word-sense disambiguation required

learning semantic relations from text

31 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

entity features (8)

background entity features

determining the semantic class of relation arguments

id91
the descent of hierarchy
iterative semantic specialization
semantic scattering

learning semantic relations from text

31 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

entity features (9)

background entity features

the descent of hierarchy [rosario & hearst, 2002]:
the same relation is assumed for all compounds from the
same hierarchies

e.g., the    rst noun denotes a body region, the second
noun denotes a cardiovascular system:
limb vein, scalp arteries,    nger capillary, forearm
microcirculation
generalization at levels 1-3 in the mesh hierarchy
generalization done manually
90% accuracy

learning semantic relations from text

31 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

entity features (10)

background entity features

iterative semantic specialization [girju & al., 2003]

fully automated
applied to part-whole
given positive and negative examples

1 generalize up in id138 from each example
2 specialize so that there are no ambiguities
3 produce rules

semantic scattering [moldovan & al., 2004]

learns a boundary (a cut)

learning semantic relations from text

31 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

relational features (1)

relational features

characterize the relation directly
(as opposed to characterizing each argument in isolation)

learning semantic relations from text

32 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

relational features (2)

basic relational features

model the context

words between the two arguments
words from a    xed window on either side of the arguments
a dependency path linking the arguments
an entire dependency graph
the smallest dominant subtree

learning semantic relations from text

32 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

relational features (3)

basic relational features: examples

learning semantic relations from text

32 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

relational features (4)

background relational features

encode knowledge about how entities typically interact in
texts beyond the immediate context, e.g.,

paraphrases which characterize a relation
patterns with placeholders
id91 to    nd similar contexts

learning semantic relations from text

32 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

relational features (5)

background relational features

characterizing noun compounds using paraphrases
nakov & hearst [2007] extract from the web verbs,
prepositions and coordinators connecting the arguments

   x that * y   
   y that * x   
   x * y   
   y * x   

butnariu & veale [2008] use the google web 1t id165s

learning semantic relations from text

32 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

relational features (6)

background relational features

[nakov & hearst, 2007]: example for committee member

learning semantic relations from text

32 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

relational features (7)

background relational features

using features with placeholders: turney [2006] mines
from the web patterns like

   y * causes x    for cause (e.g., cold virus)
   y in * early x    for temporal (e.g., morning frost).

learning semantic relations from text

32 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

relational features (8)

background relational features

can be distributional

turney & littman [2005] characterize the relation between
two words as a vector with coordinates corresponding to
the web frequencies of 128    xed phrases like    x for y   
and    y for x    (for is one of a    xed set of 64 joining
terms: such as, not the, is *, etc. etc. )

can be used directly, or
in singular value decomposition [turney, 2006]

learning semantic relations from text

32 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

outline

1

introduction

2 semantic relations

3 features

4 supervised methods

5 unsupervised methods

6 embeddings

7 wrap-up

learning semantic relations from text

33 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

supervised methods

supervised id36: setup

task: given a piece of text,    nd instances of semantic
relations
subtasks

argument identi   cation (often ignored)
relation classi   cation (core subtask)

needed

an inventory of possible semantic relations
annotated positive/negative examples: for training, tuning
and evaluation

learning semantic relations from text

34 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

data

annotated data for learning semantic relations

small-scale / large-scale
general-purpose / domain-speci   c
arguments marked / not marked
additional information about the arguments (e.g., senses)
/ no additional information

learning semantic relations from text

35 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

data: muc and ace

part-whole

personal-social

relation type
physical

subtypes
located
near
geographical
subsidiary
business
family
lasting-personal
employment
ownership
founder
student-alum
sports-af   liation
investor-shareholder
membership
agent-artifact
user-owner-inventor-manufacturer
general af   liation citizen-resident-religion-ethnicity

organization-
af   liation

organization-location-origin

learning semantic relations from text

36 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

data: muc and ace

the arguments of relations are tagged for type!

employment(person, organization):
<per>he</per> had previously worked at <org>nbc
entertainment</org>.

near(person, facility):
<per>muslim youths</per> recently staged a half dozen
rallies in front of <fac>the embassy</fac>.

citizen-resident-religion-ethnicity(person,
entity):
some <gpe>missouri</gpe> <per>voters</per>. . .

geo-political

part-whole

personal-social

relation type
physical

subtypes
located
near
geographical
subsidiary
business
family
lasting-personal
employment
ownership
founder
student-alum
sports-af   liation
investor-shareholder
membership
agent-artifact
user-owner-inventor-manufacturer
general af   liation citizen-resident-religion-ethnicity

organization-
af   liation

organization-location-origin

learning semantic relations from text

36 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

data: semeval

a small number of relations
annotated entities
additional entity information (id138 senses)
sentential context + mining patterns

learning semantic relations from text

37 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

semeval-2007 task 4 (1)

semantic relations between nominals: inventory

learning semantic relations from text

38 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

semeval-2007 task 4 (2)

semantic relations between nominals: examples

learning semantic relations from text

38 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

semeval-2010 task 8 (1)

multi-way semantic relations between nominals: inventory

learning semantic relations from text

39 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

semeval-2010 task 8 (2)

multi-way semantic relations between nominals: examples

learning semantic relations from text

39 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

algorithms for relation learning (1)

pretty much any machine learning algorithm can work, but
some are better for relation learning.

classi   cation with kernels is appropriate because relational

features (in particular) may have complex
structures.

neural networks are appropriate for capturing complex

interactions and compositionality

sequential labelling methods are appropriate because the

arguments of a relation have variable span.

learning semantic relations from text

40 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

algorithms for relation learning (2)

classi   cation with kernels: overview

idea: the similarity of two instances can be computed in a
high-dimensional feature space without the need to
enumerate the dimensions of that space (e.g., using
id145)
convolution kernels: easy to combine features, e.g., entity
and relational
kernelizable classi   ers: id166, id28, knn,
na  ve bayes

learning semantic relations from text

40 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

algorithms for relation learning (3)

kernels for linguistic structures

string sequencies [cancedda & al., 2003]
dependency paths [bunescu & mooney, 2005]
shallow parse trees [zelenko & al., 2003]
constituent parse trees [collins & duffy, 2001]
dependency parse trees [moschitti, 2006]
feature-enriched/semantic tree kernel [plank & moschitti,
2013; sun & han, 2014]
directed acyclic graphs [suzuki & al., 2003]

learning semantic relations from text

40 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

algorithms for relation learning (4)

tree kernels

similarity between two trees is the (normalized) sum of
similarities between their subtrees
similarity between subtrees based on similarities between
roots and children (leaf nodes or subtrees)
similarity between leaf (word) nodes can be 0/1 or based
on semantic similarity using e.g., clusters or word
embeddings [plank & moschitti, 2013; nguyen & al., 2015]

learning semantic relations from text

40 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

algorithms for relation learning (5)

sequential labelling methods

id48s / memms / crfs
[bikel & al., 1999; lafferty & al., 2001; mccallum & li, 2003]
useful for

argument identi   cation

e.g., born-in holds between person and location

id36

argument order matters for some relations

learning semantic relations from text

40 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

algorithms for relation learning (6)

sequential labelling: argument identi   cation

words: individual words, previous/following two words, word
substrings (pre   xes, suf   xes of various lengths), capitalization, digit
patterns, manual lexicons (e.g., of days, months, honori   cs, stopwords,
lists of known countries, cities, companies, and so on)
labels: individual labels, previous/following two labels
combinations of words and labels

learning semantic relations from text

40 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

algorithms for relation learning (7)

sequential labelling: id36

when one argument is known: the task becomes argument
identi   cation

e.g., this generif is about cox-2

cox-2 expression is signi   cantly more common in
endometrial adenocarcinoma and ovarian serous
cystadenocarcinoma, but not in cervical squamous
carcinoma, compared with normal tissue.

some relations come in order

e.g., party, job and father below

learning semantic relations from text

40 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

algorithms for relation learning (8)

sequential labelling: id36

id48s, crfs [culotta & al., 2006; bundschus & al., 2008]
dynamic graphical model [rosario & hearst, 2004]

learning semantic relations from text

40 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

algorithms for relation learning (9)

neural networks for representing contexts
recursive networks create a bottom-up representation for a

tree context by recursively combining
representations of siblings [socher & al., 2012]

convolutional networks create a representation by sliding a

window over the context and pooling the
representations at each step [zeng & al., 2014]

recurrent networks create a representation for a sequence

context by processing each item in the sequence
and updating the representation at each step [li & al.,
2015]

context representation can be augmented with traditional entity
features.

learning semantic relations from text

40 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

algorithms for relation learning (10)

id56s [socher & al., 2012]

prediction

o o o o o o

o o o o o o

o o o o o o

smoking

o o o o o o

o o o o o o

causes

cancer

word vectors (can be pretrained)

compositional vectors (id56):
vparent = f (wlvl + wr vr + b)

compositional vectors and matrices (mv-id56):
vparent = f (wvlmr vl + wvr mlvr + b)
mparent = wmlml + wmr mr

learning semantic relations from text

40 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

algorithms for relation learning (11)

convolutional neural networks [zeng & al., 2014, liu &
al., 2015, dos santos & al., 2015]

o
o

o
o

o
o

o

o

o

o

o

o

o
o

o

o

o

o

o

o

o
o

o

o

o

o

o

o

o
o

word vectors (can be pretrained)

position vectors

vt,win =(cid:80)length

i

window vector (length = 3) at word t:

wi,wordvt,i,word + wi,positionvt,i,position + b

sentence vector (max pooling):
vsen[i] = max0   t<|t| vt,win[i]

semantics

doesn   t

cause

cancer

learning semantic relations from text

40 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

beyond binary relations (1)

non-binary relations

some relations are not binary

purchase (purchaser, purchased_entity, price, seller)

previous methods generally apply
but there are some issues

features: not easy to use the words between entity
mentions, or the dependency path between mentions, or
the least common subtree
partial mentions

sparks ltd. bought 500 tons of steel from steel ltd.
steel ltd. bought 200 tons of coal.

learning semantic relations from text

41 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

beyond binary relations (2)

non-binary relations

coping with partial mentions

treat partial mentions as negatives
ignore partial mentions
train a separate model for each combination of arguments
mcdonald & al. (2005)

1 predict whether two entities are related to each other
2 use strong argument typing and graph-based global

optimization to compose n-ary predictions
many solutions for id14
[palmer & al., 2010]

learning semantic relations from text

41 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

supervised methods: practical considerations (1)

some very general advice

favour high-performing algorithms such as id166, logistic
regression or crf
(crf only if it makes sense as a sequence-labelling problem)
entity and relational features are almost always useful
the value of background features varies across tasks

e.g., for noun compounds, background knowledge is key,
while context is not very useful

learning semantic relations from text

42 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

supervised methods: practical considerations (2)

performance depends on a number of factors
the number and nature of the relations used
the distribution of those relations in data
the source of data for training and testing
the annotation procedure for data
the amount of training data available
. . .

conservative conclusion: state-of-the-art systems perform
well above random or majority-class baseline.

learning semantic relations from text

42 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

supervised methods: practical considerations (3)

performance at semeval
semeval-2007 task 4

winning system: f=72.4%, acc=76.3%, using resources
such as id138
[beamer & al., 2007]
later: similar performance, using corpus data only
[davidov & rappoport, 2008;    s  aghdha & copestake, 2008; nakov & kozareva, 2011]

semeval-2010 task 8

winning system: f=82.2%, acc=77.9%, using many manual
resources
[rink & harabagiu, 2010]
later: improvement f=84.1%, neural network with corpus
data only
[dos santos & al., 2015]

learning semantic relations from text

42 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

supervised methods: practical considerations (4)

performance at ace

different task

full documents rather than single sentences
relations between speci   c classes of named entities

f-score

low-to-mid 70s [jiang & zhai, 2007; zhou & al., 2007, 2009]

granularity matters

moving from <10 ace relation types to >20 relation
subtypes (on the same data!) decreases f1 by about 20%

learning semantic relations from text

42 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

outline

1

introduction

2 semantic relations

3 features

4 supervised methods

5 unsupervised methods

6 embeddings

7 wrap-up

learning semantic relations from text

43 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

mining very large corpora (1)

very large corpora

examples

gigaword (news texts)
pubmed (scienti   c articles)
world-wide web

contain massive amounts of data

cannot all be encoded to train a supervised model

learning semantic relations from text

44 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

mining very large corpora (2)

very large corpora

suitable for unsupervised relation mining
useful in extracting relational knowledge

taxonomic

e.g., what kinds of animals exist?

ontological

e.g., which cities are located in the united kingdom?

event

e.g., which companies have bought which other companies?

needed because manual knowledge bases are inherently
incomplete, e.g., cyc and freebase

learning semantic relations from text

44 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

mining very large corpora (3)

example

swanson [1987] discovered a connection between
migraines and magnesium
swanson linking

publication 1: illness a is caused by chemical b
publication 2: drug c reduces chemical b in the body
linking: connection between illness a and drug c

learning semantic relations from text

44 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

mining very large corpora (4)

challenges

a lot of irrelevant information
high precision is key
a supervised model might not be feasible

new relations, not seen in training
deep features too expensive

learning semantic relations from text

44 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

mining very large corpora (5)

historically important: crafted patterns

very high precision
low recall

not a problem because of the scale of corpora

low coverage

cover only a small number of relations

learning semantic relations from text

44 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

mining very large corpora (6)

brief history

pioneered by hearst (1992)
initially, taxonomic relations     the backbone of any
taxonomy or ontology

is-a: hyponymy/hypernymy
part-of: meronymy/holonymy

gradually expanded

more relations
larger scale of corpora     web-scale now within reach

the never-ending language learner project
the machine reading project

learning semantic relations from text

44 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

early work: mining dictionaries (1)

extracting taxonomic relations from dictionaries

popular in 1980s

[ahlswede & evens, 1988; alshawi, 1987; amsler, 1981; chodorow & al., 1985; ide & al., 1992;

klavans & al., 1992]

focus on is-a

hypenymy/hyponymy
subclass/superclass

used dictionaries such as merriam-webster
pattern-based

learning semantic relations from text

45 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

early work: mining dictionaries (2)

merriam-webster: group and related concepts
[amsler, 1981]

group 1.0a     a number of individuals related by a common factor (as physical association, community of
interests, or blood)
class 1.1a     a group of the same general status or nature
type 1.4a     a class, kind, or group set apart by common characteristics
kind 1.2a     a group united by common traits or interests
kind 1.2b     category
category .0a     a division used in classi   cation
category .0b     class, group, kind
division .2a     one of the parts, sections, or groupings into which a whole is divided
*grouping <== w7     a set of objects combined in a group
set 3.5a     a group of persons or things of the same kind or having a common characteristic usu. classed
together
sort 1.1a     a group of persons or things that have similar characteristics
sort 1.1b - class
species .ia     sort, kind
species .ib     a taxonomic group comprising closely related organisms potentially able to breed with one
another

learning semantic relations from text

45 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

early work: mining dictionaries (3)

merriam-webster: group and related concepts
[amsler, 1981]

learning semantic relations from text

45 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

early work: mining dictionaries (4)

mining dictionaries: summary

pros

short, focused de   nitions
standard language
limited vocabulary

cons

circularity
hard to identify the key terms

group of persons
number of individuals

limited coverage

learning semantic relations from text

45 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

mining relations with patterns (1)

relation mining patterns

when matched against a text fragment, identify relation
instances
can involve

lexical items
wildcards
parts of speech
syntactic relations
   exible rules, e.g., as in id157
...

learning semantic relations from text

46 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

mining relations with patterns (2)

hearst   s (1992) lexico-syntactic patterns

np such as {np,}    {(or|and)} np
   . . . bow lute, such as bambara ndang . . .    
    (bow lute, bambara ndang)
such np as {np,}    {(or|and)} np
   . . . works by such authors as herrick, goldsmith, and shakespeare   
    (authors, herrick); (authors, goldsmith); (authors, shakespeare)
np {, np}    {,} (or|and) other np
   . . . temples, treasuries, and other important civic buildings . . .    
    (important civic buildings, temples); (important civic buildings, treasuries)
np{,} (including|especially) {np,}    (or|and) np
   . . . most european countries, especially france, england and spain . . .    
    (european countries, france); (european countries, england); (european
countries, spain)

learning semantic relations from text

46 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

mining relations with patterns (3)

hearst   s (1992) lexico-syntactic patterns

designed for very high precision, but low recall
only cover is-a
later, extended to other relations, e.g.,

part-of [berland & charniak, 1999]
protein-protein interactions
[blaschke & al., 1999; pustejovsky & al., 2002]

n1 inhibits n2
n2 is inhibited by n1
inhibition of n2 by n1

unclear if such patterns can be designed for all relations

learning semantic relations from text

46 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

mining relations with patterns (4)

hearst   s (1992) lexico-syntactic patterns

ran on grolier   s american academic encyclopedia

small by today   s standards
still, large enough: 8.6 million tokens

very low recall

extracted just 152 examples (but with very high precision)

increase recall

id64

learning semantic relations from text

46 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

id64 (1)

learning semantic relations from text

47 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

id64 (2)

learning semantic relations from text

47 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

id64 (3)

id64
initialization

few seed examples
e.g., for is-a

cat-animal
car-vehicle
banana-fruit

expansion

new patterns
new instances
several iterations
main dif   culty

semantic drift

learning semantic relations from text

47 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

id64 (4)

id64

context-dependency

not good for context-dependent relations

in one newspaper:    lokomotiv defeated porto.   
in a few months:    porto defeated lokomotiv moscow.   

speci   city

good for speci   c relations such as birthdate
cannot distinguish between    ne-grained relations
e.g., different kinds of part-whole     maybe
component-integral_object, member-collection,
portion-mass, stuff-object, feature-activity and place-area
    would share the same patterns

learning semantic relations from text

47 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

tackling semantic drift (1)

example of semantic drift

seeds

london
paris

new york

   

patterns

mayor of x
lives in x

...

   

added examples

california
europe

...

learning semantic relations from text

48 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

tackling semantic drift (2)

example: euler diagram for four people-relations [krause&al.,2012]

learning semantic relations from text

48 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

tackling semantic drift (3)

some strategies

limit the number of iterations
select a small number of patterns/examples per iteration
use semantic types, e.g., the snowball system

(cid:104)organization(cid:105)   s headquarters in (cid:104)location(cid:105)
(cid:104)location(cid:105)-based (cid:104)organization(cid:105)
(cid:104)organization(cid:105), (cid:104)location(cid:105)

learning semantic relations from text

48 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

tackling semantic drift (4)

more strategies

scoring patterns/instances

speci   city: prefer patterns that match less contexts
con   dence: prefer patterns with higher precision
reliability: based on pmi

argument type checking
coupled training

learning semantic relations from text

48 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

tackling semantic drift (5)

coupled training [carlson & al., 2010]

used in the never-ending language learner

learning semantic relations from text

48 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

distant supervision (1)

distant supervision

issue with id64: starts with a small number of
seeds
distant supervision uses a huge number
[craven & kumlien, 1999]

1 get huge seed sets, e.g., from id138, cyc, wikipedia

infoboxes, freebase

2 find contexts where they occur
3 use these contexts to train a classi   er

learning semantic relations from text

49 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

distant supervision (2)

example: experiments of mintz & al. [2009]

102 relations from freebase, 17,000 seed instances
mapped them to wikipedia article texts
extracted

1.8 million instances
connecting 940,000 entities

assumption: all co-occurrences of a pair of entities
express the same relation

riedel & al. [2010] assume that at least one context
expresses the target relation (rather than all)
ling & al. [2013] assume that a certain percentage (which
can vary by relation) of the contexts are true positives

learning semantic relations from text

49 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

distant supervision (3)

training sentences

1 positive: with the relation
2 negative: without the

relation

train a two-stage classi   er:
identify the sentences
with a relation instance

1

2 extract relations from

these sentences

learning semantic relations from text

49 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

distant supervision (4)

false negatives

knowledge bases used to provide distant supervision are
incomplete

1 avoid false negatives [min&al. 2013]
2    ll in gaps [xu&al. 2013]

learning semantic relations from text

49 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

distant supervision (5)

distant and partial supervision

choose representative and useful training examples to
maximize performance

1 active learning [angeli&al. 2014]
2
3 semantic consistency [han & sun, 2014]

infusion of labeled data [pershina&al. 2014]

learning semantic relations from text

49 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

unsupervised id36

other issues with id64

uses multiple passes over a corpus

often undesirable/unfeasible, e.g., on the web

if we want to extract all relations

no seeds for all of them

possible solution

unsupervised id36
no pre-speci   ed list of relations, seeds or patterns

learning semantic relations from text

50 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

extracting is-a relations (1)

pantel & ravichandran [2004]

cluster nouns using cooccurrence as in [pantel & lin, 2002]

apple, google, ibm, oracle, sun microsystems, ...

extract hypernyms using patterns

apposition (n:appo:n), e.g., . . . oracle, a company known
for its progressive employment policies . . .
nominal subject (-n:subj:n), e.g., . . . apple was a hot
young company, with steve jobs in charge . . .
such as (-n:such as:n), e.g., . . . companies such as ibm
must be weary . . .
like (-n:like:n), e.g., . . . companies like sun
microsystems do not shy away from such challenges . . .
is-a between the hypernym and each noun in the cluster

learning semantic relations from text

51 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

extracting is-a relations (2)

[kozareva & al., 2008]

uses a doubly-anchored pattern (dap)

   sem-class such as term1 and *   

similar to the hearst pattern

np0 such as {np1, np2, . . ., (and | or)} npn

but different

exactly two arguments after such as
and is obligatory

prevents sense mixing
cats   jaguar   puma
predators   jaguar   leopard
cars   jaguar   ferrari

learning semantic relations from text

51 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

extracting is-a relations (3)

[kozareva & hovy, 2010]: daps can yield a taxonomy

learning semantic relations from text

51 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

extracting is-a relations (4)

[kozareva & hovy, 2010]: daps can yield a taxonomy

learning semantic relations from text

51 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

emergent relations (1)

emergent relations in open id36

no    xed set of relations
need to identify novel relations

use verbs, prepositions

different verbs, same relation: shot against the    u, shot to
prevent the    u
verb, but no relation:    it rains.    or    i do.   
no verb, but relation:    u shot

use id91

string similarity
distributional similarity

learning semantic relations from text

52 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

emergent relations (2)

id91 with distributional similarity

using paraphrases from dependency parses
[lin & pantel, 2001; pasca, 2007]

e.g., dirt for x solves y

y is solved by x, x resolves y, x    nds a solution to y, x tries to solve y, x deals with y, y is
resolved by x, x addresses y, x seeks a solution to y, x does something about y, x
solution to y, y is resolved in x, y is solved through x, x recti   es y, x copes with y, x
overcomes y, x eases y, x tackles y, x alleviates y, x corrects y, x is a solution to y, x
makes worse y, x irons out y

extracted shared property model
[yates & etzioni, 2007]

e.g., if (lacks, mars, ozone layer) and (lacks, red planet,
ozone layer), then mars and red planet share the property
(lacks, *, ozone layer)

learning semantic relations from text

52 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

emergent relations (3)

[davidov & rappoport, 2008]

prefix cw1 infix cw2 postfix

label
(pets, dogs)
(phone, charger)

patterns
{ such x as y, x such as y, y and other x }
{ buy y accessory for x!, shipping y for x,
y is available for x, y are available for x,
y are available for x systems, y for x }

these (cw1, cw2) clusters are ef   cient as background
features for supervised models.

learning semantic relations from text

52 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

self-supervised id36 (1)

self-supervision

algorithm

1 parse a small corpus
2 extract and annotate relation instances: e.g., based on

heuristics and the connecting path between entity mentions
train relation extractors on these instances

3

not guided by or assigned to any particular relation type
features: shallow lexical and pos, dependency path

applicable on the web
used in the machine reading project at u washington

learning semantic relations from text

53 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

self-supervised id36 (2)

self-supervision

issues with the extracted relations

not coherent

e.g., the mark 14 was central to the torpedo scandal of the
   eet.     was central torpedo

uninformative

e.g., . . . is the author of . . .     is

too speci   c

e.g., is offering only modest greenhouse gas reductions
targets at

learning semantic relations from text

53 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

self-supervised id36 (3)

self-supervision

improving the relation quality

constraints: syntactic, positional and frequency [fader & al., 2011]
focus on functional relations, e.g., birthplace [lin & al., 2010]
use redundancy: the    knowitall hypothesis    [downey & al., 2005,
2010]     extractions from more distinct sentences in a corpus
are more likely to be correct

high frequency is not enough though:
"elvis killed jfk" yields 1,360 hits (on september 17, 2015)
still, "oswald killed jfk" had 7,310 hits

learning semantic relations from text

53 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

web-scale id36 (1)

two major large-scale knowledge acquisition projects that
harvest the web continuously

never-ending language learner (nell)

at carnegie-mellon university
http://rtw.ml.cmu.edu/rtw/

machine reading

at the university of washington
http://ai.cs.washington.edu/projects/
open-information-extraction

learning semantic relations from text

54 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

web-scale id36 (2)

never-ending language learner [mohamed & al., 2011]

starting with a seed ontology

600 categories and relations
each with 20 seed examples

learns

new concepts
new concept instances
new instances of the existing relations
novel relations

approach: id64, coupled learning, manual
intervention, id91
learned (as of september 17, 2015)

50 million con   dence-scored relations (beliefs)
2,575,848 with high con   dence scores

learning semantic relations from text

54 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

web-scale id36 (3)

machine reading at u washington

knowitall [etzioni & al., 2005]     id64 using hearst patterns
textrunner [banko & al., 2007]     self-supervised, speci   c relation
models from a small corpus, applied to a large corpus
kylin [wu & weld, 2007] and wpe [hoffmann & al., 2010] id64
starting with wikipedia infoboxes and associated articles
woe [wu & weld, 2010] extends kylin to open information
extraction, using part-of-speech or dependency patterns
reverb [fader & al., 2011]     lexical and syntactic constraints on
potential relation expressions
ollie [mausam & al., 2012]     extends woe with better patterns
and dependencies (e.g., some relations are true for some
period of time, or are contingent upon external conditions)

learning semantic relations from text

54 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

other large-scale knowledge acquisition projects (1)

yago-naga [hoffart&al., 2015]

harvest, search, and rank knowledge from the web
large-scale, highly accurate, machine-processible
integration with wikipedia and id138
started in 2016, several subprojects

learning semantic relations from text

55 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

other large-scale knowledge acquisition projects (2)

babelnet [navigli&ponzetto, 2012]

multilingual semantic network
integrates several knowledge sources
no additional web mining (just integration)

learning semantic relations from text

55 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

unsupervised methods: summary

unsupervised id36

good for

large text collections or the web
context-independent relations

methods

id64 (but semantic drift)
coupled learning
distant supervision
semi-supervision
self-supervision

applications

continuous id10

nell
machine reading

learning semantic relations from text

56 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

outline

1

introduction

2 semantic relations

3 features

4 supervised methods

5 unsupervised methods

6 embeddings

7 wrap-up

learning semantic relations from text

57 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

id27s (1)

id27

what is it?

mapping words to vectors of real numbers in a low
dimensional space

how is it done?

neural networks (e.g., cbow, skip-gram) [mikolov&al.2013a]
id84 (e.g., lsa, lda, pca)
explicit representation (words in the context)

why should we care?

useful for a number of nlp tasks
. . . including semantic relations

learning semantic relations from text

58 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

id27s (2)

id27s from a neural lm [bengio &al.2003]

learning semantic relations from text

58 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

id27s (3)

continuous bag of words (   predict word   ) [mikolov &al.2013a]

learning semantic relations from text

58 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

id27s (4)

skip-gram (   predict context   ) [mikolov &al.2013a]

learning semantic relations from text

58 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

id27s (5)

skip-gram: projection with pca

learning semantic relations from text

58 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

id27s (6)

skip-gram: properties [mikolov&al.2013a]

id27s have linear structure that enables
analogies with vector arithmetics
due to training objective: input and output (before softmax)
are in a linear relationship

learning semantic relations from text

58 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

id27s (7)

skip-gram: vector arithmetics

inspired by analogy problems

learning semantic relations from text

58 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

id27s (8)

recurrent neural network language model (id56lm)
[mikolov&al.2013b]

learning semantic relations from text

58 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

id27s (9)

id56lm: beyond semantic relations [mikolov&al.2013b]

gender, number, etc.

learning semantic relations from text

58 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

syntactic id27s (1)

dependency-based embeddings [levy&goldberg,2014a]

learning semantic relations from text

59 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

syntactic id27s (2)

dependency- vs. word-based embeddings [levy&goldberg,2014a]

words: topical
dependencies: functional

also true for explicit representations [lin,1998; pad  &lapata,2007]

example: turing

words: nondeterministic, non-deterministic, computability,
deterministic,    nite-state
dependencies: pauling, hotelling, heting, lessing,
hamming

learning semantic relations from text

59 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

id27s: should we care?

embeddings vs. explicit representations

embeddings are better across many tasks [baroni&al., 2014]

semantic relatedness
synonym detection
concept categorization
selectional preferences
analogy

but explicit representation can be as good on analogies,
with a better objective [levy&goldberg,2014b]

learning semantic relations from text

60 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

embeddings for id36 (1)

id56s (id56) [socher&al., 2012]

prediction

o o o o o o

o o o o o o

o o o o o o

smoking

o o o o o o

o o o o o o

causes

cancer

word vectors (can be pretrained)

compositional vectors (id56):
vparent = f (wlvl + wr vr + b)

compositional vectors and matrices (mv-id56):
vparent = f (wvlmr vl + wvr mlvr + b)
mparent = wmlml + wmr mr

learning semantic relations from text

61 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

embeddings for id36 (2)

mv-id56: matrix-vector id56 [socher&al., 2012]

vectors: for compositionality
matrices: for operator semantics

learning semantic relations from text

61 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

embeddings for id36 (3)

mv-id56 for relation classi   cation [socher&al., 2012]

learning semantic relations from text

61 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

embeddings for id36 (4)

id98: convolutional deep neural network [zeng&al., 2014]

learning semantic relations from text

61 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

embeddings for id36 (5)

id98 (sentence level features) [zeng&al., 2014]

wf: word vectors; pf: position vectors (distance to e1, e2)

learning semantic relations from text

61 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

embeddings for id36 (6)

fcm: factor-based compositional embed. model [yu&al., 2014]

extension of the model coming at emnlp   2015 [gorid113y&al., 2015]

learning semantic relations from text

61 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

embeddings for id36 (7)

fcm (continued) [yu&al., 2014]

extension of the model at emnlp   2015! [gorid113y&al., 2015]

learning semantic relations from text

61 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

embeddings for id36 (8)

cr-id98: classi   cation by ranking id98 [dos santos&al., 2015]

pairwise ranking loss
word, class, position, sentence embeddings

learning semantic relations from text

61 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

embeddings for id36 (9)

sdp-lstm: shortest dependency path lstm [yan xu&al., 2015]

to be presented at emnlp   2015!

learning semantic relations from text

61 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

embeddings for id36 (10)

deplid98: dependency id98 (w/ neg. sampling) [kun xu&al., 2015]

to be presented at emnlp   2015!

learning semantic relations from text

61 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

embeddings for id36 (11)

comparison on semeval-2010 task 8 [kun xu&al., 2015]

learning semantic relations from text

61 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

outline

1

introduction

2 semantic relations

3 features

4 supervised methods

5 unsupervised methods

6 embeddings

7 wrap-up

learning semantic relations from text

62 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

lessons learned

semantic relations

are an open class
just like concepts, they can be organized hierarchically
some are ontological, some idiosyncratic
the way we work with them depends on

the application
the method

learning semantic relations from text

63 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

lessons learned

learning to identify or discover relations

investigate many detailed features in a (small)
fully-supervised setting, and try to port them into an open
id36 setting
set an inventory of targeted relations, or allow them to
emerge from the analyzed data
use (more or less) annotated data to bootstrap the learning
process
exploit resources created for different purposes for our own
ends (wikipedia!)

learning semantic relations from text

63 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

extracting relational knowledge from text

the bigger picture: nlp    nds knowledge in a lot of text
and then gets the deeper meaning of a little text

manual construction of knowledge bases

pros: accurate (insofar as people who do it do not make mistakes)
cons: costly, inherently limited in scope

automated knowledge acquisition
pros: scalable, e.g., to the web
cons: inaccurate, e.g., due to semantic drift or
inaccuracies in the analyzed text

learning relations

pros: reasonably accurate
cons: needs relation inventory and annotated training
data, does not scale to large corpora

learning semantic relations from text

64 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

the future

hot research topics and future directions

embeddings, deep learning
web-scale relation mining
continuous, never-ending learning
distant supervision
use of large knowledge sources such as wikipedia,
dbpedia
semi-supervised methods
combining symbolic and statistical methods

e.g., ontology acquisition using statistics

learning semantic relations from text

65 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

relevant literature is huge! (1)

relevant papers at emnlp   2015

[li&al., 2015] compare recursive (based on syntactic trees)
vs. recurrent (inspired by lms) neural networks on four
tasks, including semantic id36
[kun xu&al., 2015] learn robust relation representations
from shortest dependency paths through a convolution
neural network using simple negative sampling
[yan xu&al., 2015] use long short term memory networks
along shortest dependency paths for relation classi   cation
[gorid113y&al., 2015] propose a compositional embedding
model for id36 that combines (unlexicalized)
hand-crafted features with learned id27s

learning semantic relations from text

66 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

relevant literature is huge! (2)

relevant papers at emnlp   2015

[zeng&al., 2015] propose piecewise convolutional neural
networks for id36 using distant supervision
[batista&al., 2015] use id27s and
id64 for id36
[li&jurafsky, 2015] propose a multi-sense embedding
model based on chinese restaurant processes,applied to
a number of tasks including semantic relation identi   cation
[d   souza&ng, 2015] use expanding parse trees with
sieves for spatial id36

learning semantic relations from text

66 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

relevant literature is huge! (3)

relevant papers at emnlp   2015

[grycner&al., 2015] mine relational phrases and their
hypernyms
[kloetzer&al., 2015] acquire entailment pairs of binary
relations on a large-scale
[gupta&al., 2015] use distributional vectors for    ne-grained
semantic attribute extraction
[su&al., 2015] use bilingual correspondence recursive
autoencoder to model bilingual phrases in translation
[qiu&al., 2015] compare syntactic and id165 based word
embeddings for chinese analogy detection and mining

learning semantic relations from text

66 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

relevant literature is huge! (4)

relevant papers at emnlp   2015

[luo&al, 2015] infer binary relation schemas for open
information extraction
[petroni&al., 2015] propose context-aware open relation
extraction with factorization machines
[augenstein&al., 2015] extract relations between
non-standard entities using distant supervision and
imitation learning
[tuan&al., 2015] use trustiness and collective
synonym/contrastive evidence into taxonomy construction

learning semantic relations from text

66 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

relevant literature is huge! (5)

relevant papers at emnlp   2015

[bovi&al., 2015] perform knowledge base relation
uni   cation via sense embeddings and disambiguation
[garcia-duran&al.,2015] perform link prediction in
knowledge bases by composing relationships with
translations in the embedding space
[zhong&al., 2015] perform link predictions in kbs and
relational fact extraction by aligning knowledge and text
embeddings by entity descriptions
[gardner&mitchell, 2015] extract relations using subgraph
feature selection for knowledge base completion

learning semantic relations from text

66 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

relevant literature is huge! (6)

relevant papers at emnlp   2015

[toutanova&al., 2015] learn joint embeddings of text and
knowledge bases for knowledge base completion
[luo&al., 2015] present context-dependent knowledge
graph embedding for link prediction and triple classi   cation
[kotnis&al., 2015] extend knowledge bases with missing
relations, using bridging entities
[lin&al., 2015] embed entities and relations using a
path-based representation for knowledge base completion
and id36

learning semantic relations from text

66 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

relevant literature is huge! (7)

relevant papers at emnlp   2015

[mitra&baral, 2015] extract relations to automatically solve
logic grid puzzles
[seo&al., 2015] extract relations from text and visual
diagrams to solve geometry problems
[li&clark, 2015] use semantic relations for background
knowledge construction for answering elementary science
questions

learning semantic relations from text

66 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

relevant literature is huge! (8)

relevant papers at emnlp   2015

28 out of the 312 papers at emnlp   2015, or 9%, are about
id36

topics: embeddings, various neural network types and
architectures
applications: knowledge base and taxonomy enrichment,
id53, problem solving (e.g., math), machine
translation

we probably miss some relevant emnlp   2015 papers...
... and there is much more recent work beyong
emnlp   2015

learning semantic relations from text

66 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

read the book!

doi:10.2200/s00489ed1v01y201303hlt019

learning semantic relations from text

67 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

                                                                           -

learning semantic relations from text

68 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

thank you!

questions?

learning semantic relations from text

68 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

bibliography i

thomas ahlswede and martha evens.
parsing vs. text processing in the analysis of dictionary de   nitions.
in proc. 26th annual meeting of the association for computational linguistics, buffalo, ny, usa, pages
217   224, 1988.

hiyan alshawi.
processing dictionary de   nitions with phrasal pattern hierarchies.
americal journal of computational linguistics, 13(3):195   202, 1987.

robert amsler.
a taxonomy for english nouns and verbs.
in proc. 19th annual meeting of the association for computational linguistics, stanford university, stanford,
ca, usa, pages 133   138, 1981.

gabor angeli, julie tibshirani, jean wu, and christopher d. manning.
combining distant and partial supervision for id36.
in proceedings of the 2014 conference on empirical methods in natural language processing (emnlp),
pages 1556   1567, doha, qatar, october 2014. association for computational linguistics.
url http://www.aclweb.org/anthology/d14-1164.

isabelle augenstein, andreas vlachos, and diana maynard.
extracting relations between non-standard entities using distant supervision and imitation learning.
in proc. conference on empirical methods in natural language processing, pages 747   757, 2015.

learning semantic relations from text

69 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

bibliography ii

michele banko, michael cafarella, stephen sonderland, matt broadhead, and oren etzioni.
id10 from the web.
in proc. 22nd conference on the advancement of arti   cial intelligence, vancouver, bc, canada, pages
2670   2676, 2007.

ken barker and stan szpakowicz.
semi-automatic recognition of noun modi   er relationships.
in proc. 36th annual meeting of the association for computational linguistics, montr  al, canada, pages
96   102, 1998.

marco baroni, georgiana dinu, and germ  n kruszewski.
don   t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors.
in proc. of the annual meeting of the association for computational linguistics, pages 238   247, 2014.

david s. batista, bruno martins, and m  rio j. silva.
semi-supervised id64 of relationship extractors with id65.
in proc. conference on empirical methods in natural language processing, pages 499   504, 2015.

brandon beamer, suma bhat, brant chee, andrew fister, alla rozovskaya, and roxana girju.
uiuc: a knowledge-rich approach to identifying semantic relations between nominals.
in proc. 4th international workshop on semantic evaluations (semeval-1), prague, czech republic, pages
386   389, 2007.

yoshua bengio, r  jean ducharme, pascal vincent, and christian janvin.
a neural probabilistic language model.
j. mach. learn. res., 3:1137   1155, march 2003.

learning semantic relations from text

70 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

bibliography iii

matthew berland and eugene charniak.
finding parts in very large corpora.
in proc. 37th annual meeting of the association for computational linguistics, college park, md, usa,
pages 57   64, 1999.

daniel m. bikel, richard schwartz, and ralph m. weischedel.
an algorithm that learns what   s in a name.
machine learning, 34(1-3):211   231, february 1999.
url http://dx.doi.org/10.1023/a:1007558221122.

christian blaschke, miguel a. andrade, christos ouzounis, and alfonso valencia.
automatic extraction of biological information from scienti   c text: protein-protein interactions.
in proc. 7th international conference on intelligent systems for molecular biology (ismb-99), heidelberg,
germany, 1999.

david m. blei, andrew y. ng, and michael i. jordan.
id44.
journal of machine learning research, 3:993   1022, 2003.

peter f. brown, peter v. desouza, robert l. mercer, vincent j. della pietra, and jenifer c. lai.
class-based id165 models of natural language.
computational linguistics, 18:467   479, 1992.

learning semantic relations from text

71 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

bibliography iv

razvan bunescu and raymond j. mooney.
a shortest path dependency kernel for id36.
in human language technology conference and conference on empirical methods in natural language
processing (hlt-emnlp-05), vancouver, canada, 2005.

cristina butnariu and tony veale.
a concept-centered approach to noun-compound interpretation.
in proc. 22nd international conference on computational linguistics, pages 81   88, manchester, uk, 2008.

michael cafarella, michele banko, and oren etzioni.
relational web search.
technical report 2006-04-02, university of washington, department of computer science and engineering,
2006.

nicola cancedda, eric gaussier, cyril goutte, and jean-michel renders.
word-sequence kernels.
journal of machine learning research, 3:1059   1082, 2003.
url http://jmlr.csail.mit.edu/papers/v3/cancedda03a.html.

andrew carlson, justin betteridge, richard c. wang, estevam r. hruschka jr., and tom m. mitchell.
coupled semi-supervised learning for information extraction.
in proc. third acm international conference on web search and data mining (wsdm 2010), 2010.

learning semantic relations from text

72 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

bibliography v

joseph b. casagrande and kenneth hale.
semantic relationships in papago folk-de   nition.
in dell h. hymes and william e. bittleolo, editors, studies in southwestern ethnolinguistics, pages 165   193.
mouton, the hague and paris, 1967.

roger chaf   n and douglas j. herrmann.
the similarity and diversity of semantic relations.
memory & cognition, 12(2):134   141, 1984.

eugene charniak.
toward a model of children   s story comprehension.
technical report aitr-266 (hdl.handle.net/1721.1/6892), massachusetts institute of technology, 1972.

martin s. chodorow, roy byrd, and george heidorn.
extracting semantic hierarchies from a large on-line dictionary.
in proc. 23th annual meeting of the association for computational linguistics, chicago, il, usa, pages
299   304, 1985.

massimiliano ciaramita, aldo gangemi, esther ratsch, jasmin   ari  c, and isabel rojas.
unsupervised learning of semantic relations between concepts of a molecular biology ontology.
in proc. 19th international joint conference on arti   cial intelligence, edinburgh, scotland, pages 659   664,
2005.

learning semantic relations from text

73 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

bibliography vi

michael collins and nigel duffy.
convolution kernels for natural language.
in proc. 15th conference on neural information processing systems (nips-01), vancouver, canada, 2001.
url http://books.nips.cc/papers/files/nips14/aa58.pdf.

m. craven and j. kumlien.
constructing biological knowledge bases by extracting information from text sources.
in proc. seventh international conference on intelligent systems for molecular biology, pages 77   86, 1999.

dmitry davidov and ari rappoport.
classi   cation of semantic relationships between nominals using pattern clusters.
in proc. 46th annual meeting of the association for computational linguistics: human language
technologies, columbus, oh, usa, pages 227   235, 2008.

ferdinand de saussure.
course in general linguistics.
philosophical library, new york, 1959.
edited by charles bally and albert sechehaye. translated from the french by wade baskin.

claudio delli bovi, luis espinosa anke, and roberto navigli.
knowledge base uni   cation via sense embeddings and disambiguation.
in proc. conference on empirical methods in natural language processing, pages 726   736, 2015.

c  cero nogueira dos santos, bing xiang, and bowen zhou.
classifying relations by ranking with convolutional neural networks.
in proceedings of acl-15, beijing, china, 2015.

learning semantic relations from text

74 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

bibliography vii

doug downey, oren etzioni, and stephen soderland.
a probabilistic model of redundancy in information extraction.
in proc. 9th international joint conference on arti   cial intelligence, edinburgh, uk, pages 1034   1041, 2005.

doug downey, oren etzioni, and stephen soderland.
analysis of a probabilistic model of redundancy in unsupervised information extraction.
arti   cial intelligence, 174(11):726   748, 2010.

pamela downing.
on the creation and use of english noun compounds.
language, 53(4):810   842, 1977.

jennifer d   souza and vincent ng.
sieve-based spatial id36 with expanding parse trees.
in proc. conference on empirical methods in natural language processing, pages 758   768, 2015.

oren etzioni, michael cafarella, doug downey, ana-maria popescu, tal shaked, stephen soderland,
daniel s. weld, and alexander yates.
unsupervised named-entity extraction from the web: an experimental study.
arti   cial intelligence, 165(1):91   134, june 2005.
issn 0004-3702.

anthony fader, stephen soderland, and oren etzioni.
identifying relations for id10.
in proc. conference of empirical methods in natural language processing (emnlp    11), edinburgh,
scotland, uk, july 27-31 2011.

learning semantic relations from text

75 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

bibliography viii

christiane fellbaum, editor.
id138     an electronic lexical database.
mit press, 1998.

timothy finin.
the semantic interpretation of nominal compounds.
in proc. 1st national conference on arti   cial intelligence, stanford, ca, usa, 1980.

gottlob frege.
begriffschrift.
louis nebert, halle, 1879.

alberto garcia-duran, antoine bordes, and nicolas usunier.
composing relationships with translations.
in proc. conference on empirical methods in natural language processing, pages 286   290, 2015.

jean claude gardin.
syntol.
graduate school of library service, rutgers, the state university (rutgers series on systems for the
intellectual organization of information, susan artandi, ed.), new brunswick, new jersey, 1965.

matt gardner and tom mitchell.
ef   cient and expressive knowledge base completion using subgraph feature extraction.
in proc. conference on empirical methods in natural language processing. association for computational
linguistics, 2015.

learning semantic relations from text

76 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

bibliography ix

roxana girju.
improving the interpretation of noun phrases with cross-linguistic information.
in proc. 45th annual meeting of the association for computational linguistics, prague, czech republic,
pages 568   575, 2007.

roxana girju, adriana badulescu, and dan moldovan.
learning semantic constraints for the automatic discovery of part-whole relations.
in proc. human language technology conference of the north american chapter of the association for
computational linguistics, edmonton, alberta, canada, 2003.

roxana girju, dan moldovan, marta tatu, and daniel antohe.
on the semantics of noun compounds.
computer speech and language, 19:479   496, 2005.

matthew r. gorid113y, mo yu, and mark dredze.
improved id36 with feature-rich compositional embedding models.
in proc. conference on empirical methods in natural language processing, pages 1774   1784, 2015.

adam grycner, gerhard weikum, jay pujara, james foulds, and lise getoor.
relly: inferring hypernym relationships between relational phrases.
in proc. conference on empirical methods in natural language processing, pages 971   981, 2015.

abhijeet gupta, gemma boleda, marco baroni, and sebastian pad  .
distributional vectors encode referential attributes.
in proc. conference on empirical methods in natural language processing, pages 12   21, 2015.

learning semantic relations from text

77 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

bibliography x

xianpei han and le sun.
semantic consistency: a local subspace based method for distant supervised id36.
in proceedings of the 52nd annual meeting of the association for computational linguistics (volume 2:
short papers), pages 718   724, baltimore, maryland, june 2014. association for computational linguistics.
url http://www.aclweb.org/anthology/p14-2117.

roy harris.
reading saussure: a critical commentary on the cours le linquistique generale.
open court, la salle, ill., 1987.

kazuma hashimoto, pontus stenetorp, makoto miwa, and yoshimasa tsuruoka.
task-oriented learning of id27s for semantic relation classi   cation.
arxiv preprint arxiv:1503.00095, 2015.

marti hearst.
automatic acquisition of hyponyms from large text corpora.
in proc. 15th international conference on computational linguistics, nantes, france, pages 539   545, 1992.

johannes hoffart, fabian m. suchanek, klaus berberich, and gerhard weikum.
yago2: a spatially and temporally enhanced knowledge base from wikipedia.
artif. intell., 194:28   61, january 2013.

raphael hoffmann, congle zhang, and daniel weld.
learning 5000 relational extractors.
in proc. 48th annual meeting of the association for computational linguistics, uppsala, sweden, pages
286   295, 2010.

learning semantic relations from text

78 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

bibliography xi

nancy ide, jean veronis, susan warwick-armstrong, and nicoletta calzolari.
principles for encoding machine-readable dictionaries.
in fifth euralex international congress, pages 239   246, university of tampere, finland, 1992.

jing jiang and chengxiang zhai.
instance weighting for id20 in nlp.
in proc. 45th annual meeting of the association for computational linguistics, acl    07, pages 264   271,
prague, czech republic, 2007.
url http://www.aclweb.org/anthology/p07-1034.

karen sp  rck jones.
synonymy and semantic classi   cation.
phd thesis, university of cambridge, 1964.

su nam kim and timothy baldwin.
automatic interpretation of noun compounds using id138::similarity.
in proc. 2nd international joint conference on natural language processing, jeju island, south korea,
pages 945   956, 2005.

su nam kim and timothy baldwin.
interpreting semantic relations in noun compounds via verb semantics.
in proc. 21st international conference on computational linguistics and 44th annual meeting of the
association for computational linguistics, sydney, australia, pages 491   498, 2006.

learning semantic relations from text

79 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

bibliography xii

judith l. klavans, martin s. chodorow, and nina wacholder.
building a knowledge base from parsed de   nitions.
in george heidorn, karen jensen, and steve richardson, editors, natural language processing: the
plnlp approach. kluwer, new york, ny, usa, 1992.

julien kloetzer, kentaro torisawa, chikara hashimoto, and jong-hoon oh.
large-scale acquisition of entailment pattern pairs by exploiting transitivity.
in proc. conference on empirical methods in natural language processing, pages 1649   1655, 2015.

bhushan kotnis, pradeep bansal, and partha p. talukdar.
knowledge base id136 using bridging entities.
in proc. conference on empirical methods in natural language processing, pages 2038   2043, 2015.

zornitsa kozareva and eduard hovy.
a semi-supervised method to learn and construct taxonomies using the web.
in proc. 2010 conference on empirical methods in natural language processing, cambridge, ma, usa,
pages 1110   1118, 2010.

zornitsa kozareva, ellen riloff, and eduard hovy.
semantic class learning from the web with hyponym pattern linkage graphs.
in proc. 46th annual meeting of the association for computational linguistics acl-08: hlt, pages
1048   1056, 2008.

sebastian krause, hong li, hans uszkoreit, and feiyu xu.
large-scale learning of relation-extraction rules with distant supervision from the web.
in proc. international conference on the semantic web, pages 263   278, 2012.

learning semantic relations from text

80 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

bibliography xiii

john d. lafferty, andrew mccallum, and fernando c. n. pereira.
conditional random    elds: probabilistic models for segmenting and labeling sequence data.
in proc. eighteenth international conference on machine learning, icml    01, pages 282   289, san
francisco, ca, usa, 2001. morgan kaufmann publishers inc.
isbn 1-55860-778-1.
url http://dl.acm.org/citation.cfm?id=645530.655813.

maria lapata.
the disambiguation of nominalizations.
computational linguistics, 28(3):357   388, 2002.

mirella lapata and frank keller.
the web as a baseline: evaluating the performance of unsupervised web-based models for a range of nlp
tasks.
in proc. human language technology conference and conference on empirical methods in natural
language processing, pages 121   128, boston, usa, 2004.

mark lauer.
designing statistical language learners: experiments on noun compounds.
phd thesis, macquarie university, 1995.

judith n. levi.
the syntax and semantics of complex nominals.
academic press, new york, 1978.

learning semantic relations from text

81 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

bibliography xiv

omer levy and yoav goldberg.
dependency-based id27s.
in proc. 52nd annual meeting of the association for computational linguistics, pages 302   308, 2014a.

omer levy and yoav goldberg.
linguistic regularities in sparse and explicit word representations.
in proc. conference on computational natural language learning, pages 171   180, 2014b.

jiwei li and dan jurafsky.
do multi-sense embeddings improve natural language understanding?
in proc. conference on empirical methods in natural language processing, pages 1722   1732, 2015.

jiwei li, thang luong, dan jurafsky, and eduard hovy.
when are tree structures necessary for deep learning of representations?
in proc. conference on empirical methods in natural language processing, pages 2304   2314, 2015.

yang li and peter clark.
answering elementary science questions by constructing coherent scenes using background knowledge.
in proc. conference on empirical methods in natural language processing, pages 2007   2012, 2015.

dekang lin.
an information-theoretic de   nition of similarity.
in proc. international conference on machine learning, pages 296   304, 1998.

learning semantic relations from text

82 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

bibliography xv

dekang lin and patrick pantel.
discovery of id136 rules for question-answering.
natural language engineering, 7(4):343   360, 2001.
issn 1351-3249.

thomas lin, mausam, and oren etzioni.
identifying functional relations in web text.
in proc. 2010 conference on empirical methods in natural language processing, pages 1266   1276,
cambridge, ma, october 2010.

yankai lin, zhiyuan liu, huanbo luan, maosong sun, siwei rao, and song liu.
modeling relation paths for representation learning of knowledge bases.
in proc. conference on empirical methods in natural language processing, pages 705   714, 2015.

xiao ling, peter clark, and daniel s. weld.
extracting meronyms for a biology knowledge base using distant supervision.
in proceedings of automated knowledge base construction (akbc) 2013: the 3rd workshop on
knowledge extraction at cikm 2013, san francisco, ca, october 27-28 2013.

kangqi luo, xusheng luo, and kenny zhu.
inferring binary relation schemas for id10.
in proc. conference on empirical methods in natural language processing, pages 555   560, 2015a.

yuanfei luo, quan wang, bin wang, and li guo.
context-dependent id13 embedding.
in proc. conference on empirical methods in natural language processing, pages 1656   1661, 2015b.

learning semantic relations from text

83 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

bibliography xvi

tuan luu anh, jung-jae kim, and see kiong ng.
incorporating trustiness and collective synonym/contrastive evidence into taxonomy construction.
in proc. conference on empirical methods in natural language processing, pages 1013   1022, 2015.

mausam, michael schmitz, robert bart, stephen soderland, and oren etzioni.
open language learning for information extraction.
in proc. 2012 conference on empirical methods in natural language processing, jeju island, korea, pages
523   534, 2012.

andrew mccallum and wei li.
early results for id39 with id49, feature induction and
web-enhanced lexicons.
in proc. 7th conference on natural language learning at hlt-naacl 2003     volume 4, conll    03, pages
188   191, 2003.
doi: 10.3115/1119176.1119206.
url http://dx.doi.org/10.3115/1119176.1119206.

john mccarthy.
programs with common sense.
in proc. teddington conference on the mechanization of thought processes, 1958.

ryan mcdonald, fernando pereira, seth kulik, scott winters, yang jin, and pete white.
simple algorithms for complex id36 with applications to biomedical ie.
in proc. 43rd annual meeting of the association for computational linguistics (acl-05), ann arbor, mi, 2005.

learning semantic relations from text

84 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

bibliography xvii

tomas mikolov, ilya sutskever, kai chen, greg s corrado, and jeff dean.
distributed representations of words and phrases and their compositionality.
in c.j.c. burges, l. bottou, m. welling, z. ghahramani, and k.q. weinberger, editors, advances in neural
information processing systems 26, pages 3111   3119, 2013a.

tomas mikolov, wen-tau yih, and geoffrey zweig.
linguistic regularities in continuous space word representations.
in proc. conference of the north american chapter of the association for computational linguistics: human
language technologies, pages 746   751, atlanta, georgia, 2013b.

bonan min, ralph grishman, li wan, chang wang, and david gondek.
distant supervision for id36 with an incomplete knowledge base.
in proceedings of the 2013 conference of the north american chapter of the association for computational
linguistics: human language technologies, pages 777   782, atlanta, georgia, june 2013. association for
computational linguistics.
url http://www.aclweb.org/anthology/n13-1095.

mike mintz, steven bills, rion snow, and dan jurafsky.
distant supervision for id36 without labeled data.
in proc. joint conference of the 47th annual meeting of the acl and the 4th international joint conference
on natural language processing of the afnlp: volume 2, acl    09, pages 1003   1011, 2009.

arindam mitra and chitta baral.
learning to automatically solve logic grid puzzles.
in proc. conference on empirical methods in natural language processing, pages 1023   1033, 2015.

learning semantic relations from text

85 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

bibliography xviii

thahir mohamed, estevam hruschka jr., and tom mitchell.
discovering relations between noun categories.
in proc. 2011 conference on empirical methods in natural language processing, edinburgh, uk, pages
1447   1455, 2011.

dan moldovan, adriana badulescu, marta tatu, daniel antohe, and roxana girju.
models for the semantic classi   cation of noun phrases.
in proc. hlt-naacl workshop on computational lexical semantics, pages 60   67. association for
computational linguistic, 2004.

alessandro moschitti.
ef   cient convolution kernels for dependency and constituent syntactic trees.
proc. 17th european conference on machine learning (ecml-06), 2006.
url http://dit.unitn.it/~moschitt/articles/ecml2006.pdf.

preslav nakov.
improved id151 using monolingual paraphrases.
in proc. 18th european conference on arti   cial intelligence, patras, greece, pages 338   342, 2008.

preslav nakov and marti hearst.
ucb: system description for semeval task #4.
in proc. 4th international workshop on semantic evaluations (semeval-2007), pages 366   369, prague,
czech republic, 2007.

learning semantic relations from text

86 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

bibliography xix

preslav nakov and marti hearst.
solving relational similarity problems using the web as a corpus.
in proc. 6th annual meeting of the association for computational linguistics: human language
technologies, columbus, oh, usa, pages 452   460, 2008.

preslav nakov and zornitsa kozareva.
combining relational and attributional similarity for semantic relation classi   cation.
in proc. international conference on recent advances in natural language processing, hissar, bulgaria,
pages 323   330, 2011.

vivi nastase and stan szpakowicz.
exploring noun-modi   er semantic relations.
in proc. 6th international workshop on computational semantics, tilburg, the netherlands, pages 285   301,
2003.

roberto navigli and simone paolo ponzetto.
babelnet: the automatic construction, evaluation and application of a wide-coverage multilingual semantic
network.
arti   cial intelligence, 193:217   250, 2012.

thien huu nguyen and ralph grishman.
employing word representations and id173 for id20 of id36.
in proceedings of the 52nd annual meeting of the association for computational linguistics (volume 2:
short papers), pages 68   74, baltimore, maryland, june 2014. association for computational linguistics.
url http://www.aclweb.org/anthology/p14-2012.

learning semantic relations from text

87 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

bibliography xx

diarmuid    s  aghdha and ann copestake.
semantic classi   cation with distributional kernels.
in proc. 22nd international conference on computational linguistics, pages 649   656, manchester, uk,
2008.

marius pa  sca.
organizing and searching the world-wide web of facts     step two: harnessing the wisdom of the crowds.
in 16th international world wide web conference, banff, canada, pages 101   110, 2007.

sebastian pad   and mirella lapata.
dependency-based construction of semantic space models.
computational linguistics, 33(2):161   199, 2007.

martha palmer, daniel gildea, and nianwen xue.
id14.
synthesis lectures on human language technologies. morgan & claypool, 2010.

patrick pantel and dekang lin.
discovering word senses from text.
in proc. 8th acm sigkdd conference on knowledge discovery and data mining, edmonton, alberta,
canada, pages 613   619, 2002.

patrick pantel and deepak ravichandran.
automatically labeling semantic classes.
in proc. human language technology conference of the north american chapter of the association for
computational linguistics, boston, ma, usa, pages 321   328, 2004.

learning semantic relations from text

88 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

bibliography xxi

siddharth patwardhan and ellen riloff.
effective information extraction with semantic af   nity patterns and relevant regions.
in proc. 2007 joint conference on empirical methods in natural language processing and computational
language learning, prague, czech republic, pages 717   727, 2007.

charles sanders peirce.
existential graphs (unpublished 1909 manuscript).
in justus buchler, editor, the philosophy of peirce: selected writings. harcourt, brace & co., 1940.

jeffrey pennington, richard socher, and christopher manning.
glove: global vectors for word representation.
in proc. conference on empirical methods in natural language processing (emnlp), pages 1532   1543,
doha, qatar, 2014.

maria pershina, bonan min, wei xu, and ralph grishman.
infusion of labeled data into distant supervision for id36.
in proceedings of the 52nd annual meeting of the association for computational linguistics (volume 2:
short papers), pages 732   738, baltimore, maryland, june 2014. association for computational linguistics.
url http://www.aclweb.org/anthology/p14-2119.

fabio petroni, luciano del corro, and rainer gemulla.
core: context-aware open id36 with factorization machines.
in proc. conference on empirical methods in natural language processing, pages 1763   1773, 2015.

learning semantic relations from text

89 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

bibliography xxii

barbara plank and alessandro moschitti.
embedding semantic similarity in tree kernels for id20 of id36.
in proceedings of acl-13, so   a, bulgaria, 2013.

james pustejovsky, jos   m. casta  o, jason zhang, m. kotecki, and b. cochran.
robust relational parsing over biomedical literature: extracting inhibit relations.
in proc. 7th paci   c symposium on biocomputing (psb-02), lihue, hi, usa, 2002.

likun qiu, yue zhang, and yanan lu.
syntactic dependencies and distributed word representations for chinese analogy detection and mining.
in proc. conference on empirical methods in natural language processing, pages 2441   2450, 2015.

m. ross quillian.
a revised design for an understanding machine.
mechanical translation, 7:17   29, 1962.

randolph quirk, sidney greenbaum, geoffrey leech, and jan svartvik.
a comprehensive grammar of the english language.
longman, 1985.

deepak ravichandran and eduard hovy.
learning surface text patterns for a id53 system.
in proc. 40th annual meeting of the association for computational linguistics, philadelphia, pa< usa, pages
41   47, 2002.

learning semantic relations from text

90 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

bibliography xxiii

sebastian riedel, limin yao, and andrew mccallum.
modeling relations and their mentions without labeled text.
in proc. european conference on machine learning and knowledge discovery in databases (ecml pkdd
   10), volume 6232 of lecture notes in computer science, pages 148   163. springer, 2010.

bryan rink and sanda harabagiu.
utd: classifying semantic relations by combining lexical and semantic resources.
in proc. 5th international workshop on semantic evaluation, pages 256   259, uppsala, sweden, july 2010.
association for computational linguistics.
url http://www.aclweb.org/anthology/s10-1057.

barbara rosario and marti hearst.
classifying the semantic relations in noun compounds via a domain-speci   c lexical hierarchy.
in proc. 2001 conference on empirical methods in natural language processing, pittsburgh, pa< usa,
pages 82   90, 2001.

barbara rosario and marti hearst.
the descent of hierarchy, and selection in relational semantics.
in proc. 40th annual meeting of the association for computational linguistics, philadelphia, pa, usa, pages
247   254, 2002.

minjoon seo, hannaneh hajishirzi, ali farhadi, oren etzioni, and clint malcolm.
solving geometry problems: combining text and diagram interpretation.
in proc. conference on empirical methods in natural language processing, pages 1466   1476, 2015.

learning semantic relations from text

91 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

bibliography xxiv

richard socher, brody huval, christopher d. manning, and andrew y. ng.
semantic compositionality through recursive matrix-vector spaces.
in proc. 2012 conference on empirical methods in natural language processing, jeju, korea, 2012.

vivek srikumar and dan roth.
modeling semantic relations expressed by prepositions.
transactions of the acl, 2013.

jinsong su, deyi xiong, biao zhang, yang liu, junfeng yao, and min zhang.
bilingual correspondence recursive autoencoder for id151.
in proc. conference on empirical methods in natural language processing, pages 1248   1258, 2015.

le sun and xianpei han.
a feature-enriched tree kernel for id36.
in proceedings of the 52nd annual meeting of the association for computational linguistics (volume 2:
short papers), pages 61   67, baltimore, maryland, june 2014. association for computational linguistics.
url http://www.aclweb.org/anthology/p14-2011.

jun suzuki, tsutomu hirao, yutaka sasaki, and eisaku maeda.
hierarchical directed acyclic graph kernel: methods for structured natural language data.
in proce. 41st annual meeting of the association for computational linguistics (acl-03), sapporo, japan,
2003.

don r. swanson.
two medical literatures that are logically but not bibliographically connected.
jasis, 38(4):228   233, 1987.

learning semantic relations from text

92 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

bibliography xxv

diarmuid    s  aghdha.
designing and evaluating a semantic annotation scheme for compound nouns.
in proc. 4th corpus linguistics conference (cl-07), birmingham, uk, 2007.
url www.cl.cam.ac.uk/~do242/papers/dos_cl2007.pdf.

diarmuid    s  aghdha and ann copestake.
co-occurrence contexts for noun compound interpretation.
in proc. acl workshop on a broader perspective on multiword expressions, pages 57   64. association for
computational linguistics, 2007.

lucien tesni  re.
  l  ments de syntaxe structurale.
c. klincksieck, paris, 1959.

kristina toutanova, danqi chen, patrick pantel, hoifung poon, pallavi choudhury, and michael gamon.
representing text for joint embedding of text and knowledge bases.
in proc. conference on empirical methods in natural language processing, pages 1499   1509, 2015.

stephen tratz and eduard hovy.
a taxonomy, dataset, and classi   er for automatic noun compound interpretation.
in proc. 48th annual meeting of the association for computational linguistics, pages 678   687, uppsala,
sweden, 2010.

learning semantic relations from text

93 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

bibliography xxvi

peter turney.
similarity of semantic relations.
computational linguistics, 32(3):379   416, 2006.

peter turney and michael littman.
corpus-based learning of analogies and semantic relations.
machine learning, 60(1-3):251   278, 2005.

lucy vanderwende.
algorithm for the automatic interpretation of noun sequences.
in proc. 15th international conference on computational linguistics, kyoto, japan, pages 782   788, 1994.

beatrice warren.
semantic patterns of noun-noun compounds.
in gothenburg studies in english 41, goteburg, acta universtatis gothoburgensis, 1978.

joseph weizenbaum.
eliza     a computer program for the study of natural language communication between man and machine.
communications of the acm, 9(1):36   45, 1966.

terry winograd.
understanding natural language.
cognitive psychology, 3(1):1   191, 1972.

learning semantic relations from text

94 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

bibliography xxvii

fei wu and daniel s. weld.
autonomously semantifying wikipedia.
in proc. acm 17th conference on information and knowledge management (cikm 2008), napa valley, ca,
usa, pages 41   50, 2007.

fei wu and daniel s. weld.
id10 using wikipedia.
in proc. 48th annual meeting of the association for computational linguistics, uppsala, sweden, pages
118   127, 2010.

kun xu, yansong feng, songfang huang, and dongyan zhao.
semantic relation classi   cation via convolutional neural networks with simple negative sampling.
in proc. conference on empirical methods in natural language processing, pages 536   540, 2015a.

wei xu, raphael hoffmann, le zhao, and ralph grishman.
filling knowledge base gaps for distant supervision of id36.
in proceedings of the 51st annual meeting of the association for computational linguistics (volume 2: short
papers), pages 665   670, so   a, bulgaria, august 2013. association for computational linguistics.
url http://www.aclweb.org/anthology/p13-2117.

yan xu, lili mou, ge li, yunchuan chen, hao peng, and zhi jin.
classifying relations via long short term memory networks along shortest dependency paths.
in proc. conference on empirical methods in natural language processing, pages 1785   1794, 2015b.

learning semantic relations from text

95 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

bibliography xxviii

alexander yates and oren etzioni.
unsupervised resolution of objects and relations on the web.
in proc. human language technologies 2007: the conference of the north american chapter of the
association for computational linguistics, rochester, ny, usa, pages 121   130, 2007.

mo yu, matthew r. gorid113y, and mark dredze.
factor-based compositional embedding models.
in the nips 2014 learning semantics workshop, 2014.

mo yu, matthew r. gorid113y, and mark dredze.
combining id27s and feature embeddings for    ne-grained id36.
in proceedings of the 2015 conference of the north american chapter of the association for computational
linguistics: human language technologies, pages 1374   1379, denver, colorado, may   june 2015.
association for computational linguistics.
url http://www.aclweb.org/anthology/n15-1155.

dmitry zelenko, chinatsu aone, and anthony richardella.
kernel methods for id36.
journal of machine learning research, 3:1083   1106, 2003.

daojian zeng, kang liu, siwei lai, guangyou zhou, and jun zhao.
relation classi   caiton via convolutional deep neural network.
in proceedings of coling-14, dublin, ireland, 2014.

learning semantic relations from text

96 / 97

introduction

semantic relations

features

supervised methods

unsupervised methods

embeddings wrap-up

bibliography xxix

daojian zeng, kang liu, yubo chen, and jun zhao.
distant supervision for id36 via piecewise convolutional neural networks.
in proc. conference on empirical methods in natural language processing, pages 1753   1762, 2015.

guo dong zhou, min zhang, dong hong ji, and qiao ming zhu.
tree kernel-based id36 with context-sensitive structured parse tree information.
in proc. 2007 joint conference on empirical methods in natural language processing and computational
natural language learning (emnlp-conll-07), pages 728   736, prague, czech republic, 2007.

guo dong zhou, long hua qian, and qiao ming zhu.
label propagation via bootstrapped support vectors for semantic id36 between named entities.
computer speech and language, 23(4):464   478, 2009.

karl e. zimmer.
some general observations about nominal compounds.
working papers on language universals, stanford university, 5, 1971.

learning semantic relations from text

97 / 97

