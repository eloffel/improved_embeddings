deep learning

for natural language processing

ronan collobert

jason weston

nec labs america, princeton, usa

google, new york, usa

joint work with leon bottou, david grangier, bing bai, yanjun qi, antoine bordes,
nicolas usunier, koray kavukcuoglu, pavel kuksa, corinna cortes and mehryar mohri.

deep learning

for natural language processing

ronan collobert

jason weston

nec labs america, princeton, usa

google, new york, usa

disclaimer: the characters and events depicted in this movie are    ctitious. any
similarity to any person living or dead is merely coincidental.

a brief history of machine learning

as with the history of the world, machine learning has a history of

exploration
(   nding new things)
a

and

exploitation

(of what you, or someone else, found)

(and sometimes wars because of it!)

3

in the beginning: discovery of the id88

   it   s cool, it   s sexy.    (franky rosenblatt 1957)

   it   s linear. it sucks    (minsky, papert 1969)..

... and people believed minksy, which made them sad ..

4

the quest to model nonlinearities

so they tried to make it nonlinear:

    random projections
nonlinearities,

to induce

    adding nonlinear features to the
inputs, e.g. products of features,

    they even thought of kernels
(aizerman, brav., roz. 1964).

but they were still depressed.... until......

5

they discovered multi-layered id88s

(backprop - rumelhart, hinton & williams, 1986)

...and they got excited..!

6

they were so excited they kept trying

more and more things...

7

and more and more things...

...until people got scared!

8

even though they hadn   t reached the complexity of the

only known intelligent thing in the universe (the brain)

9

..and the universe they were trying to model itself

seemed just as complex,

they decided what they were doing was too complex...

10

so they found something less complex... someone came

up with a new id88 network!

   it   s cool. it   s sexy    (vlad vapnik, 1992)
   isn   t it a linear model?    (yann lecun, 1992)

11

life was convex

... and life was good.
people published papers about it.
but it didn   t do everything they wanted...

12

learning representations

learning the kernel = multi-layer again!
neural nets are an elegant model for learning representations.

13

multi-tasking: sharing features

task1

task2

task3

task4

inputs

non-convex even for linear models! (ando & zhang, 2005)
nevertheless, neural nets are an elegant model for multi-tasking.

14

semi-supervised learning: transductive id166

the loss was non-convex!
semi-supervision for neural nets is no problem, don   t worry.

(& convex relaxations = slow)

15

feature engineering

multi-layer: 1st layer = human brain = nonconvex!!
the    rst layers of a neural net use machine learning not human learning,
which is what we   re supposed to be doing.

16

scalability

id166s are slow, even though books were devoted to making them fast
(bottou, chapelle, descoste, weston 2007). problem: too many svs!

solutions:
    using stochastic id119
like nns (bottou, nips 2008)

    learning which svs
non-convex, like a 2-layer nn.

to use    

    using linear id166s (very popular)    
back to the id88!

17

             do  not existidea! rebrand    neural nets           deep nets   

dfsfdgdfg(and add some semi-supervision to improve their performance)

   it   s cool!    (geo    hinton, this morning after breakfast)

   it   s sexy!    (yann l. and yoshua b., just before lunch)

   haven   t we been here before?    (everyone else, 2009)

...but, still, some were

enough to come to this tutorial!

18

but seriously, putting it all together:

    nns are    exible:

    di   erent module (layers), losses, regularizers, . . .

    multi-tasking

    semi-supervised learning

    learning hidden representations

    nns are scalable

the ideal tool for nlp!

all hail nns!

19

this talk: the big picture

the goal:
    we want to have a conversation with our computer

(not easy)

    convert a piece of english into a computer-friendly data structure

=    nd hidden representations

    use nlp tasks to measure if the computer    understands   

learn nlp from    scratch   

(i.e. minimal feature engineering)

the plan:

part i brainwashing: neural networks are awesome!
part ii labeling: hidden representations for tagging
part iii retrieval: hidden representations for semantic search
part iv situated learning: hidden representations for grounding

20

part ii

nlp labeling

ronan collobert

jason weston

ronan@collobert.com

jaseweston@gmail.com

l  eon bottou, koray kavukcuoglu, pavel kuksa

nec laboratories america, google labs

natural language processing tasks

part-of-speech tagging (pos): syntactic roles (noun, adverb...)

chunking (chunk): syntactic constituents (noun phrase, verb phrase...)

name entity recognition (ner): person/company/location...

id14 (srl): semantic role

[john]arg0 [ate]rel [the apple]arg1 [in the garden]argm   loc

2

nlp benchmarks

datasets:
(cid:63) pos, chunk, srl: wsj (    up to 1m labeled words)
(cid:63) ner: reuters (    200k labeled words)

accuracy
system
97.33%
shen, 2007
toutanova, 2003 97.24%
gimenez, 2004
97.16%

f1

system
95.23%
shen, 2005
sha, 2003
94.29%
kudoh, 2001 93.91%

(a) pos: as in (toutanova, 2003)

(b) chunk: conll 2000

f1

system
ando, 2005 89.31%
florian, 2003 88.76%
kudoh, 2001 88.31%
(c) ner: conll 2003

system
koomen, 2005 77.92%
77.30%
pradhan, 2005
haghighi, 2005
77.04%

f1

(d) srl: conll 2005

we chose as benchmark systems:
(cid:63) well-established systems
(cid:63) systems avoiding external labeled data

notes:
(cid:63) ando, 2005 uses external unlabeled data
(cid:63) koomen, 2005 uses 4 parse trees not provided by the challenge

3

complex systems

two extreme choices to get a complex system

(cid:63) large scale engineering: design a lot of complex features, use a fast

existing linear machine learning algorithm

4

complex systems

two extreme choices to get a complex system

(cid:63) large scale engineering: design a lot of complex features, use a fast

existing linear machine learning algorithm

(cid:63) large scale machine learning: use simple features, design a complex

model which will implicitly learn the right features

5

nlp: large scale engineering

(1/2)

choose some good hand-crafted features

predicate and pos tag of predicate

voice: active or passive (hand-built rules)

phrase type: adverbial phrase, prepositional phrase, . . . governing category: parent node   s phrase type(s)

head word and pos tag of the head word

position: left or right of verb

path: traversal from predicate to constituent

predicted named entity class

word-sense disambiguation of the verb

verb id91

length of the target constituent (number of words)

neg feature: whether the verb chunk has a    not   

partial path: lowest common ancestor in path

head word replacement in prepositional phrases

first and last words and pos in constituents

ordinal position from predicate + constituent type

constituent tree distance

temporal cue words (hand-built rules)

dynamic class context: previous node labels

constituent relative features: phrase type

constituent relative features: head word

constituent relative features: head word pos

constituent relative features: siblings

number of pirates existing in the world. . .

feed them to a shallow classi   er like id166

6

nlp: large scale engineering

(2/2)

cascade features: e.g. extract pos, construct a parse tree

extract hand-made features from the parse tree

feed these features to a shallow classi   er like id166

7

nlp: large scale machine learning

goals

task-speci   c engineering limits nlp scope

can we    nd uni   ed hidden representations?

can we build uni   ed nlp architecture?

means

start from scratch: forget (most of) nlp knowledge

compare against classical nlp benchmarks

our dogma: avoid task-speci   c engineering

8

chapter ii

the networks

9

neural networks

stack several layers together

increasing level of abstraction at each layer

requires simpler features than    shallow    classi   ers

the    weights    wi are trained by id119

how can we feed words?

10

w xmatrix-vectoroperationnon-linearityxinput vector1linear layerhardtanhw matrix-vectoroperation2linear layeryoutput vectorf(   )words into vectors

idea

words are embed in a vector space

mat

the

on

car

jesus

sits

smoke

cat

r50

embeddings are trained

implementation

a word w is an index in a dictionary d     n
use a lookup-table (w     feature size    dictionary size)

ltw (w) = w    w

remarks

applicable to any discrete feature (words, caps, stems...)

see (bengio et al, 2001)

11

words into vectors

idea

words are embed in a vector space

mat

the

embeddings are trained

on

car

jesus

sits

smoke

cat

r50

implementation

a word w is an index in a dictionary d     n
use a lookup-table (w     feature size    dictionary size)

ltw (w) = w    w

remarks

applicable to any discrete feature (words, caps, stems...)

see (bengio et al, 2001)

12

window approach

tags one word at the time

feed a    xed-size window of text
around each word to tag

works    ne for most tasks

how do deal with long-range
dependencies?

e.g.
in srl, the verb of
interest might be outside
the window!

13

inputwindowlookuptablelinearhardtanhlineartextcatsatonthematfeature1w11w12...w1n...featurekwk1wk2...wknltw1...ltwkm1    m2    wordofinterestdconcatn1hun2hu=#tagssentence approach

(1/2)

feed the whole sentence to the network

tag one word at the time: add extra position features

convolutions to handle variable-length inputs

time

w       

see (bottou, 1989)
or (lecun, 1989).

produces local features with higher level of abstraction

max over time to capture most relevant features

max

outputs a    xed-sized feature
vector

14

sentence approach

(2/2)

15

inputsentencelookuptableconvolutionmaxovertimelinearhardtanhlineartextthecatsatonthematfeature1w11w12...w1n...featurekwk1wk2...wknltw1...ltwkmax(  )m2    m3    dpaddingpaddingn1hum1    n1hun2hun3hu=#tagstraining

given a training set t

convert network outputs into probabilities

maximize a log-likelihood

   (cid:55)       (cid:88)

(x, y)   t

log p(y | x,   )

use stochastic gradient ascent (see bottou, 1991)

             +   

    log p(y | x,   )

     

fixed learning rate.    tricks   :
(cid:63) divide learning by    fan-in   
(cid:63) initialization according to    fan-in   

use chain rule (   back-propagation   ) for e   cient gradient computation
network f (  ) has l layers

f = fl                f1

parameters

   = (  l, . . . ,   1)

    log p(y | x,   )

     i

    log p(y | x,   )

   fi   1

=

    log p(y | x,   )

=
    log p(y | x,   )

   fi

   fi

      fi
     i
      fi
   fi   1

how to interpret neural networks outputs as probabilities?

16

word tag likelihood (wtl)

the network has one output f (x, i,   ) per tag i

interpreted as a id203 with a softmax over all tags

p(i| x,   ) =

(cid:80)

ef (x, i,   )
j ef (x, j,   )

de   ne the logadd operation

logadd

i

zi = log(

ezi)

(cid:88)

i

log-likelihood for example (x, y)

log p(y | x,   ) = f (x, y,   )     logadd

f (x, j,   )

j

how to leverage the sentence structure?

17

sentence tag likelihood (stl)

(1/2)

the network score for tag k at the tth word is f ([x]t
akl transition score to jump from tag k to tag l

1 , k, t,   )

sentence score for a tag path [i]t
1

s([x]t

1 , [i]t

1 ,     ) =

(cid:16)

(cid:17)

a[i]t   1[i]t

+ f ([x]t

1 , [i]t, t,   )

t(cid:88)

t=1

conditional likelihood by normalizing w.r.t all possible paths:
1 ,     )

1 ,     )     logadd
   [j]t
how to e   ciently compute the id172?

1 ,     ) = s([x]t

log p([y]t
1

1 , [y]t

1 , [j]t

| [x]t

s([x]t

1

18

thearg0arg1arg2verbcatsatonthemataijf(x , k, t)1tk   sentence tag likelihood (stl)

(1/2)

the network score for tag k at the tth word is f ([x]t
akl transition score to jump from tag k to tag l

1 , k, t,   )

sentence score for a tag path [i]t
1

s([x]t

1 , [i]t

1 ,     ) =

(cid:16)

(cid:17)

a[i]t   1[i]t

+ f ([x]t

1 , [i]t, t,   )

t(cid:88)

t=1

conditional likelihood by normalizing w.r.t all possible paths:
1 ,     )

1 ,     )     logadd
   [j]t
how to e   ciently compute the id172?

1 ,     ) = s([x]t

log p([y]t
1

1 , [y]t

1 , [j]t

| [x]t

s([x]t

1

19

thearg0arg1arg2verbcatsatonthematsentence tag likelihood (stl)

(2/2)

id172 computed with recursive forward algorithm:

(cid:104)

(cid:105)

  t(j) = logaddi
termination:

  t   1(i) + ai,j + f  (j, xt

1 , t)

s([x]t

1 , [j]t

1 ,     ) = logaddi   t (i)

logadd
   [j]t

1

simply backpropagate through this recursion with chain rule

non-linear crfs: graph transformer networks (bottou, 1997)

compared to crfs, we train features (network parameters    and
transitions scores akl)

id136: viterbi algorithm (replace logadd by max)

20

aijf(x , j, t)1t  (i)t-1supervised benchmark results

network architectures:

(cid:63) window (5) approach for pos, chunk & ner (300hu)

(cid:63) convolutional (3) for srl (300+500hu)

(cid:63) word tag likelihood (wtl) and sentence tag likelihood (stl)

network features: lower case words (size 50), capital letters (size 5)

dictionary size 100,000 words

approach

pos chunking ner srl
(f1)
(f1)
(pwa)
benchmark systems 97.24
89.31 77.92
79.53 55.40
96.31
nn+wtl
nn+stl
96.37
81.47 70.99

(f1)
94.29
89.13
90.33

stl helps, but... fair performance.

capacity mainly in words features... are we training it right?

21

supervised id27s

sentences with similar words should be tagged in the same way:
(cid:63) the cat sat on the mat
(cid:63) the feline sat on the mat

blackstock sympathetic

france

454

persuade

faw

giorgi

shaheed
rumelia
planum
goa   uld
collation

bacha

jesus
1973

xbox
6909

reddish
11724

decadent widescreen

thickets
savary

divo
verus
oxide
khwarazm urbina
stationery
epos

jfk

ilias

eglinton
gsnumber edging

antica
shabby

awe
thud

occupant

revised
leavened

operator

w.j.

frg

pandionidae

namsos

shirt

scratched megabits

29869

odd

anchieta

87025

ppa
uddin

emigration biologically

marking

heuer

sambhaji

worshippers

ritsuko
lifeless
mahan

kayak

mclarens
gladwin
centrally
indonesia

moneo
nilgiris

about 1m of words in wsj

15% of most frequent words in the dictionary are seen 90% of the time
cannot expect words to be trained properly!

22

chapter iii

lots of unlabeled data

23

ranking language model

language model:    is a sentence actually english or not?    
implicitly captures:

(cid:63) semantics

(cid:63) syntax

bengio & ducharme (2001) id203 of next word given previous
words. overcomplicated     we do not need probabilities here

id178 criterion largely determined by most frequent phrases

rare legal phrases are no less signi   cant that common phrases

f () a window approach network

ranking margin cost:(cid:88)

(cid:88)

max (0, 1     f (s, w(cid:63)

w   d

s) + f (s, w))
s   s
s: sentence windows d: dictionary

f (s, w): network score for sentence s and middle word w

w(cid:63)

s: true middle word in s

stochastic training:
(cid:63) positive example: random corpus sentence
(cid:63) negative example: replace middle word by random word

24

training language model

two window approach (11) networks (100hu) trained on two corpus:
(cid:63) lm1: wikipedia: 631m of words

(cid:63) lm2: wikipedia+reuters rcv1: 631m+221m=852m of words

massive dataset: cannot a   ord classical training-validation scheme

like in biology: breed a couple of network lines

breeding decisions according to 1m words validation set

lm1
(cid:63) order dictionary words by frequency

(cid:63) increase dictionary size: 5000, 10, 000, 30, 000, 50, 000, 100, 000

(cid:63) 4 weeks of training

lm2
(cid:63) initialized with lm1, dictionary size is 130, 000

(cid:63) 30,000 additional most frequent reuters words

(cid:63) 3 additional weeks of training

25

unsupervised id27s

454

italy

france

playstation

29869
nailed

xbox
6909
amiga

austria
belgium
germany

reddish scratched megabits
11724
greenish

jesus
1973
god
smashed
bluish
sati
punched
pinkish
christ
purplish
popped
satan
brownish crimped
kali
scraped
indra psnumber greyish
screwed megahertz
vishnu
grayish
ananda dreamcast whitish sectioned megapixels
parvati
switzerland grace

87025
octets
mb/s
bit/s
baud
carats
kbit/s

greece
sweden
norway
europe
hungary

geforce
capcom yellowish

slashed
ripped

msx
ipod
sega

amperes

silvery

gbit/s

hd

26

semi-supervised benchmark results

initialize id27s with lm1 or lm2
same training procedure

(f1)

approach

pos chunk ner srl
(pwa)
(f1)
(f1)
94.29 89.31 77.92
benchmark systems 97.24
nn+wtl
96.31
89.13
79.53 55.40
81.47 70.99
90.33
96.37
nn+stl
85.68 58.18
91.91
97.05
nn+wtl+lm1
97.10
93.65
87.58 73.84
nn+stl+lm1
86.96
92.04
97.14
nn+wtl+lm2
nn+stl+lm2
97.20
93.63
88.67 74.15

   

huge boost from language models
training set word coverage:

lm1

lm2

pos 97.86% 98.83%
chk 97.93% 98.91%
ner 95.50% 98.95%
srl 97.98% 98.87%

27

chapter iv

id72

28

id72

joint training

good overview in (caruana, 1997)

29

lookuptablelinearlookuptablelinearhardtanhhardtanhlineartask1lineartask2m2(t1)    m2(t2)    ltw1...ltwkm1    n1hun1hun2hu,(t1)=#tagsn2hu,(t2)=#tagsid72 benchmark results

approach

pos chunk ner
(f1)
(pwa)
(f1)
94.29 89.31
benchmark systems
97.24
nn+stc+lm2
97.20
93.63
88.67
88.62
94.10
nn+stc+lm2+mtl 97.22

30

chapter v

the temptation

31

cascading tasks

increase level of engineering by incorporating common nlp techniques

id30 for western languages bene   ts pos (ratnaparkhi, 1996)

(cid:63) use last two characters as feature (455 di   erent stems)

gazetteers are often used for ner (florian, 2003)

(cid:63) 8, 000 locations, person names, organizations and misc entries

from conll 2003

pos is a good feature for chunk & ner (shen, 2005) (florian, 2003)

(cid:63) we feed our own pos tags as feature

chunk is also a common feature for srl (koomen, 2005)

(cid:63) we feed our own chunk tags as feature

32

cascading tasks benchmark results

approach

benchmark systems
nn+stc+lm2
nn+stc+lm2+su   x2
nn+stc+lm2+gazetteer
nn+stc+lm2+pos
nn+stc+lm2+chunk

pos chunk ner srl
(pwa)
97.24
97.20
97.29

(f1)
94.29 89.31 77.92
88.67 74.15
93.63

(f1)

   

   
   

94.32

   

89.59
88.67

   
   
   

   
   
   

   

74.72

33

variance

train 10 networks

approach

pos chunk ner
(pwa)
(f1)
benchmark systems
97.24% 94.29% 89.31%
nn+stc+lm2+pos worst 97.29% 93.99% 89.35%
nn+stc+lm2+pos mean 97.31% 94.17% 89.65%
nn+stc+lm2+pos best
97.35% 94.32% 89.86%

(f1)

previous experiments:
same seed was used for all networks to reduce variance

34

parsing

parsing is essential to srl (punyakanok, 2005) (pradhan, 2005)
state-of-the-art srl systems use several parse trees (up to 6!!)
we feed our network several levels of charniak parse tree
provided by conll 2005

level 0

level 1

level 2

35

snptheluxuryautomakerb-npi-npi-npe-npnplastyearb-npe-npvpsolds-vpnp1,214carsb-npe-npppins-vpnptheu.s.b-npe-npstheluxuryautomakerlastyearoooooovpsold1,214carsb-vpi-vpe-vpppintheu.s.b-ppi-ppe-ppstheluxuryautomakerlastyearoooooovpsold1,214carsintheu.s.b-vpi-vpi-vpi-vpi-vpe-vpsrl benchmark results with parsing

approach

srl

(test set f1)

benchmark system (six parse trees)
benchmark system (top charniak only)
nn+stc+lm2
nn+stc+lm2+chunk
nn+stc+lm2+charniak (level 0 only)
nn+stc+lm2+charniak (levels 0 & 1)
nn+stc+lm2+charniak (levels 0 to 2)
nn+stc+lm2+charniak (levels 0 to 3)
nn+stc+lm2+charniak (levels 0 to 4)

77.92
74.76   
74.15
74.72
75.62
75.86
76.03
75.90
75.66

   

on the validation set

36

engineering a sweet spot

senna: implements our networks in simple c (    2500 lines)

neural networks mainly perform matrix-vector multiplications: use blas

all networks are fed with lower case words (130,000) and caps features

pos uses pre   xes

chunk uses pos tags

ner uses gazetteer

srl uses level 0 of parse tree

(cid:63) we trained a network to predict level 0 (uses pos tags):

92.25% f1 score against 91.94% for charniak

(cid:63) we trained a network to predict verbs as in srl

(cid:63) optionaly, we can use pos verbs

37

senna speed

system

toutanova, 2003

shen, 2007

senna

ram (mb) time (s)
1065
833
4

1100
2200
32

(a) pos

system

koomen, 2005

senna

ram (mb) time (s)
6253
52

3400
124

(b) srl

38

senna demo

will be available in january at

http://ml.nec-labs.com/software/senna

if interested: email ronan@collobert.com

39

conclusion

achievements

   all purpose    neural network architecture for nlp tagging
limit task-speci   c engineering
rely on very large unlabeled datasets
we do not plan to stop here

critics

why forgetting nlp expertise for neural network training skills?
(cid:63) nlp goals are not limited to existing nlp task
(cid:63) excessive task-speci   c engineering is not desirable

why neural networks?
(cid:63) scale on massive datasets
(cid:63) discover hidden representations
(cid:63) most of neural network technology existed in 1997 (bottou, 1997)

if we had started in 1997 with vintage computers,

training would be near completion today!!

40

deep learning

for nlp: parts 3 & 4

ronan collobert

jason weston

nec labs america, princeton, usa

google, new york, usa

1

part 3

   semantic search   

learning hidden representations for retrieval

collaborators: b. bai, d. grangier, k. sadamasa, y. qi, c. cortes, m. mohri

2

document ranking: our goal

we want to learn to match a query (text) to a target (text).

most supervised ranking methods use hand-coded features.

methods like lsi that learn from words are unsupervised.

in this work we use supervised learning from text only:

learn hidden representations of text for learning to rank from words.

outperforms existing methods (on words) like tfidf, lsi or a

(supervised) margin ranking id88 baseline.

3

basic bag-o   -words

bag-of-words + cosine similarity:
    each doc. {dt}n
    similarity with query q is: f (q, d) = q>d

t=1     rd is a normalized bag-of-words.

doesn   t deal with synonyms: bag vectors can be orthogonal

no machine learning at all

4

id45 (lsi)

learn a linear embedding   (di) = u di via a reconstruction objective.
    rank with: f (q, d) = q>u>u d =   (q)>  (di) 1.

uses    synonyms   : low-dimensional latent    concepts   .

unsupervised machine learning: useful for goal?

1 f (q, d) = q>(u>u +   i)d gives better results.
also, usually normalize this     cosine similarity.

5

(polynomial) supervised semantic indexing (ssi )
    de   ne document-query similarity function: f (q, d) = w>  k([q, d]), where
  k(x1, . . . , xd) considers all possible k-degree terms:

  k(x1, . . . , xd) = hxi1 . . . xik : 1     i1 . . . ik     di.

we consider:

    f 2(q, d) = pd
    f 3(q, d) = pd

i,j=1 wijqidj = q>w d

i,j,k=1 wijkqidjdk + f 2(q, d).

(1)

(2)

supervised machine learning: targeted for goal, uses synonyms

too big/slow?!

6

ssi: why is this a good model?

classical bag-of-words doesnt work when there are few matching terms:

q=(kitten, vet, nyc)

d=(cat, veterinarian, new, york)

our method q>w d learns that e.g. kitten and cat are highly related.

e.g. if i is the index of kitten and j is the index of cat, then wij > 0
after training.

usefulness of degree 3 model::
poly degree 2:
weights for word pairs: e.g.    jagger        q &    stones        d.
poly degree 3:
weights for word triples: e.g.    jagger        q &   stones   ,    gem        d.

7

ssi: why the basic model sucks

even for degree 2, w is big : 3.4gb if d = 30000, 14.5tb if d = 2.5m .

slow: q>w d computation has mn computations qjwijdi, where q and
d have m and n nonzero terms.

or one computes v = q>w once, and then vd for each document.
classical speed where query has d terms, assuming w is dense     still
slow.

8

ssi improved model: low rank w

for degree 2, constrain w :

w = u>v + i.

u and v are n    d matrices     smaller
low dimensional    latent concept    space like lsi (same speed).
di   erences: supervised, asymmetric, learns with i.
    for k = 2, replace w with w = (u>v ) + i:

f 2

lr(q, d) = q>(u>v + i)d, = pn
    for k = 3, approximate wijk with w ijk =p
lr(q, d) =pn

i=1(u q)i(v d)i(y d)i + f 2

f 3

l ulivljylk:

lr(q, d).

i=1(u q)i(v d)i + q>d.

9

neural network models for retrieval

10

3452345234253455   query1xddocument1xdinput: 1xdoutput: 1xninput: 1xdoutput: 1xn    e.g. d=2.5m(dictionary)(embeddingspace)0 0000 1 1 00 0 0 1 1 010 01 010 100100 10 10 0010 1010 100 01 1 1001 010 10 1010  01 10 1 010 0001 1 01 1001 01 01final scoredot productmodule 1module 21xn1xn4523452345435345e.g. n=100doc. embedding for polynomial degree 3

11

document   input: 1xdmap1xd0 0000 1 1 00 0 0 1 1 010 01 010 100100 10 10 0010 1010 11xnoutput: 1xn*component   wise productlinear map 2d x n d x nlinearomodule 2ssi: training

training loss

    ranking loss from preference triplets (q, d+, d   ),    for query q, d+ should
appear above d      :
    l(w ;r) =

max(0, 1     fw (q, d+) + fw (q, d   ))

x

(q,d+,d   )   r

learning algorithm stochastic id119: fast & scalable.

iterate

sample a triplet (q, d+, d   ),
update w     w           

   w max(0, 1     fw (q, d+) + fw (q, d   )).

12

prior work: summary of learning to rank

    id166 [joachims, 2002] and nn ranking methods [burges, 2005] .
use hand-coded features: title, body, url, search rankings,. . . (don   t use words)
(e.g. burges uses 569 features in all).
    in contrast we use only the words and try to    nd their hidden representation.
    several works on optimizing di   erent id168s (map, roc, ndcg): [cao,
2008], [yu, 2007], [qin, 2006],. . . .
    [grangier & bengio,    06] used similar methods to basic ssi for retrieving images.
    [goel, langord & strehl,
placement.
    main di   erence: i) we use low rank & ii) polynomial degree 3 features.

   08] used hash kernels (vowpal wabbit) for advert

we could also add features + new loss to our method ..

13

experimental comparison

    wikipedia

    1,828,645 documents. 24,667,286 links.

    split into 70% train, 30% test.

    pick random doc. as query, then rank other docs.
    docs that are linked to it should be highly ranked.
    two setups:

(i) whole document is used as query;

(ii) 5,10 or 20 words are picked to mimic keyword search.

14

experiments: doc-doc ranking
d = 30000

params rank-loss map

algorithm
tfidf
id183
lsi
  lsi + (1       )tfidf
marg. rank id88
ssi: poly (k = 2)
ssi: poly (k = 3)

0
2
200d
200d+1
d2
400d
600d

0.342  0.01
1.62%
0.330
1.62%
0.161
4.79%
0.346
1.28%
0.477
0.41%
0.30% 0.517
0.14% 0.539

note:best possible p10= 0.31     on average every query has only about 3 links.

p10
0.170  0.007
0.160
0.101
0.170
0.212
0.229
0.236

15

experiments: doc-doc ranking
d = 2.5m

algorithm
tfidf
id183
  lsi + (1       )tfidf
hash kernels +   i
ssi: poly (k = 2)
ssi: poly (k = 3)

rank-loss map

0.432  0.012
0.842%
0.842%
0.432
0.721%
0.433
0.322%
0.492
0.547  0.012
0.158%
0.099% 0.590  0.012

p10
0.193
0.1933
0.193
0.215
0.239  0.008
0.249  0.008

16

experiments: query-document ranking

k-keywords based retrieval (d = 30000):

k = 5

rank map p@10
algorithm
21.6% 0.047 0.023
tfidf
  lsi + (1       )tfidf 200d+1 14.2% 0.049 0.023
400d 4.37% 0.166 0.083
ssi: poly (k = 2)

params

0

algorithm
tfidf
  lsi + (1       )tfidf
ssi: poly (k = 2)

0

rank map p@10
params
14.0% 0.083 0.035
200d+1 9.73% 0.089 0.037
400d 2.91% 0.229 0.100

k = 10

k = 20

algorithm
rank map p@10
tfidf
9.14% 0.128 0.054
  lsi + (1       )tfidf 200d+1 6.36% 0.133 0.059
400d 1.80% 0.302 0.130
ssi: poly (k = 2)

params

0

17

experiments: cross-language retrieval

query: in japanese

target doc: in english     use links from wikipedia as before.

algorithm
tfidfengeng(google translated queries)
  lsiengeng+(1       )tfidfengeng
  cl-lsijapeng+(1       )tfidfengeng
ssiengeng (google translated)
ssijapeng

rank-loss

map

4.78% 0.319  0.009
3.71% 0.300  0.008
3.31% 0.275  0.009
1.72% 0.399  0.009
0.96% 0.438  0.009

p10

0.259  0.008
0.253  0.008
0.212  0.008
0.325  0.009
0.351  0.009

18

what   s inside w ?

we can look at the matrix w we learn and see the synonyms it learns
(large values of w ij):

kitten cat

cats

animals

species

dogs

vet

ibm

nyc

veterinarian

veterinary medicine

animals

animal

computer

company technology software data

york

new

manhattan city

brooklyn

c++ programming windows mac

unix

linux

xbox

console

game

games

microsoft windows

beatles mccartney

lennon

song

britney spears

album

music

band

pop

harrison

her

19

summary

powerful: supervised method for document ranking.

e   cient low-rank models     learn hidden representations.

nonlinearities improve accuracy.

20

part 4

situated learning: hidden representations

for grounding language

the concept labeling task

collaborators: antoine bordes, nicolas usunier

21

connecting nlp with a world: why?
    existing nlp: much (not all) solves syntactic or semantic sub-tasks:

e.g. pos, chunking, parsing, srl, mt, summarization . . .
they don   t use    situated    learning.

we understand language because it has a deep connection to the
world it is used in/for     strong prior knowledge

   john saw bill in the park with his telescope.   

   he passed the exam.   
   john went to the bank.   

world knowledge we might already have:

bill owns a telescope.

fred took an exam last week.

john is in the countryside (not the city).

how can a computer do that?

22

learning speech in a situated environment?

23

the learning signal : text adventure game

planet earth = tricky:

vision, speech, motor control + language understanding.

multi-user game (e.g. on the internet) = easier.

simplest version = text adventure game. good test-bed for ml?

represent atomic actions as concepts (get, move, give, shoot, ...).
represent physical objects as concepts (character1, key1, key2, ...).

(can consider this signal as a pre-processed version of a visual signal.)

24

the concept labeling task

de   nition:
map any natural
concepts y     y, where y is a sequence of concepts.

language sentence x     x to its labeling in terms of

one is given training data triples {xi, yi, ui}i=1,...,m     x    y    u where ui
is the current state the world.

universe = set of concepts and their relations to other concepts,
u = (c,r1, . . . ,rn), where n is the number of types of relation and ri     c2,
   i = 1, . . . , n.

    learning to perform this task is appropriate because:

- possibly very complex rules to learn,
- rule-based systems might scale badly with large problems,
-    exibility from one domain to another.

25

example of concept labeling
de   ne two relations:
    location(c) = c0 with c, c0     c,
    containedby(c) = c0 with c, c0     c.
a training triple (x, y, u)     x    y    u:

26

hecookstherice????<kitchen><garden><john><rice><cook>x:y:u:<gina><mark>location<john><cook><rice><hat><get><move>containedbylocationlocationlocationcontainedbylocationdisambiguation example

27

hecookstherice????<kitchen><garden><john><rice><cook>x:y:u:step 0:<gina><mark>locationdisambiguation example

label    he    requires two rules which are never explicitly given.

28

hecookstherice????x:y:u:step 4:(2)(1)<kitchen><garden><john><rice><cook><gina><mark>ambiguities we will handle

he picked up the hat there.

the milk on the table.
the one on the table.
she left the kitchen.

the adult left the kitchen.

mark drinks the orange.

. . .

(e.g. for sentence (2) there may be several milk cartons that exist. . . )

29

concept labeling is challenging

    solving ambiguities
information and available universe knowledge.

requires

to use rules based on linguistic

    but, these rules are never made explicit in training.
    a concept labeling algorithm has to learn them.

    no engineered features for describing words/concepts are given.
    a concept labeling algorithm has to discover them from raw data.

30

learning algorithm : basic argmax

we could do this:

y = f (x, u) = argmaxy0 g(x, y0, u),

g(  ) should be large if concepts y0 are consistent with both the sentence
x and the current state of the universe u.

however. . . could be slow.

31

simulation : algorithm

model a world + generate training data for our learning task:

1. generate a new event, (v, a) = event(u).

    generates verb+ set of args     a coherent action given the universe.
e.g. actors change location and pick up, exchange & drop objects. . .

2. generate a training triple, i.e. (x,y)=generate(v, a).

    returns a sentence and concept labeling pair given a verb + args.
this sentence should describe the event.

3. update the universe, i.e. u := exec(v)(a, u).

32

labeled data generated by the simulation

simulation of a house with 58 concepts: 15 verbs, 10 actors, 15 small
objects, 6 rooms and 12 pieces of furniture. . .

. . .

he sits on the chair

the father gets some yoghurt from the sideboard
- <father> <get> - <yoghurt> - - <sideboard>

x:
y:
x:
y:
x:
y: <mother> <move> - - <bedroom> - - <kitchen>
x:
y:

she goes from the bedroom to the kitchen

- <brother> <give> - <toy> - <sister>

the brother gives the toy to her

<brother> <sit> - - <chair>

. . .

    generate a dataset of 50,000 training triples and 20,000 testing triples
(   55% ambiguous), without any human annotation.

33

experimental results using an id166

method features
id166struct x
id166struct x + u (loc, contain)

train err test err
42.26% 42.61%
18.68% 23.57%

    no feature engineering: used raw words (and concept relations) as
input.

    using world knowledge leads to better generalization.
    can we learn a hidden representation and do better?

34

neural network scoring function

our score combines two functions gi(  ) and h(  )     rn which are neural
networks.

|x|x

g(x, y, u) =

gi(x, y   i, u)>h(yi, u)

i=1

    gi(x, y   i, u) is a sliding-window on the text and neighboring concepts
centered around ith word     embeds to n dim-space.
    h(yi, u) embeds the ith concept to n dim-space.
    dot-product: con   dence that ith word labeled with concept yi.

35

scoring illustration
step 0: set the sliding-window around the 1st word.

36

hecooksthericepadpadpadlocation?<kitchen><rice><cook>?padpadpadpadpadpadsliding   window on the text and neighboring concepts.scoring illustration
step 1: retrieve words representations from the    lookup table   .

37

hecooksthericepadpadpadlocation?<kitchen><rice><cook>?padpadpadpadpadpadwords represented using a "lookup   table" d = hash   table word   vector.sliding   window on the text and neighboring concepts.scoring illustration
step 2: similarly retrieve concepts representations.

38

hecooksthericepadpadpadlocation?<kitchen><rice><cook>?padpadpadpadpadpadwords represented using a "lookup   table" d = hash   table word   vector.concepts and their relations represented using another "lookup   table" c.sliding   window on the text and neighboring concepts.scoring illustration
step 3: concatenate vectors to obtain window representation.

39

hecooksthericepadpadpadlocation?<kitchen><rice><cook>?padpadpadpadpadpadwords represented using a "lookup   table" d = hash   table word   vector.concepts and their relations represented using another "lookup   table" c.concatenation in a big vector represents the sliding   windowsliding   window on the text and neighboring concepts.scoring illustration
step 4: compute g1(x, y   1, u).

40

hecooksthericepadpadpadlocation?<kitchen><rice><cook>?padpadpadpadpadpadembedding of the sliding   windowin n dim   space.words represented using a "lookup   table" d = hash   table word   vector.concepts and their relations represented using another "lookup   table" c.concatenation in a big vector represents the sliding   windowsliding   window on the text and neighboring concepts.scoring illustration
step 5: get the concept <john> and its relations.

41

hecooksthericepadpadpadlocation?<kitchen><rice><cook>?padpadpadpadpadpad<john><kitchen>locationembedding of the sliding   windowin n dim   space.words represented using a "lookup   table" d = hash   table word   vector.concepts and their relations represented using another "lookup   table" c.concatenation in a big vector represents the sliding   windowsliding   window on the text and neighboring concepts.scoring illustration
step 6: compute h(<john>, u).

42

hecooksthericepadpadpadlocation?<kitchen><rice><cook>?padpadpadpadpadpad<john><kitchen>locationembedding of the sliding   windowin n dim   space.words represented using a "lookup   table" d = hash   table word   vector.concepts and their relations represented using another "lookup   table" c.concatenation in a big vector represents the sliding   windowembedding of each concept and its relations in n dim   space.sliding   window on the text and neighboring concepts.scoring illustration
step 7: finally compute the score: g1(x, y   1, u)>h(<john>, u).

43

hecooksthericepadpadpadlocation?<kitchen><rice><cook>?padpadpadpadpadpad<john><kitchen>locationembedding of the sliding   windowin n dim   space.words represented using a "lookup   table" d = hash   table word   vector.concepts and their relations represented using another "lookup   table" c.concatenation in a big vector represents the sliding   windowembedding of each concept and its relations in n dim   space.scoresliding   window on the text and neighboring concepts.dot product between embeddings: confidence in the labeling.< | >greedy    order-free    id136 using laso

adapted from laso (learning as search optimization) [daum  e & al.,   05].

id136 algorithm:

1. for all the positions not yet labeled, predict the most likely concept.

2. select the pair (position, concept) you are the most con   dent in.

(hopefully the least ambiguous)

3. remove this position from the set of available ones.

4. collect all universe-based features of this concept to help label

remaining ones.

5. loop.

44

experimental results

method features
id166struct x
id166struct x + u (loc, contain)
nnof
nnof
nnof
nnof

x
x + u (contain)
x + u (loc)
x + u (loc, contain)

train err test err
42.26% 42.61%
18.68% 23.57%
32.50% 35.87%
15.15% 17.04%
5.07% 5.22%
0.0% 0.11%

    di   erent amounts of universe knowledge: no knowledge, knowledge
about containedby, location, or both.

    more world knowledge leads to better generalization.
    learning representations leads to better generalization.

45

features learnt by the model

our model learns representations of concepts embedding space.

nearest neighbors in this space:

query concept
gina
mark
mother
brother
cat
football
chocolate
desk
livingroom
get

most similar concepts
francoise , maggie
harry, john
sister, grandma
friend, father
hamster, dog
toy, videogame
salad, milk
bed, table
kitchen, garden
sit, give

e.g. the model learns that female actors are similar, even though we
have not given this information to the model.

46

summary

simple, but general framework for language grounding based on the
task of concept labeling.

scalable,    exible learning algorithm that can learn without hand-
crafted rules or features.

simulation validates our approach and shows that learning to
disambiguate with world knowledge is possible.

ai goal:
language from scratch from interaction alone (communication, actions).

train learner living in a    computer game world    to learn

47

final conclusion

48

(some of the) previous work

    blocks world, krl [winograd,    72],[bobrow & winograd,    76]
    ground language with visual reference, e.g. in blocks world [winston
   96] or more recent works [fleischman & roy
   76],[feldman et al.
   07],[barnard & johnson    05],[yu & ballard    04],[siskind   00].
    map from sentence to meaning in formal language [zettlemoyer &
collins,    05], [wong & mooney,    07], [chen & mooney    08]

example applications:
(i) word-sense disambiguation (from images),
(ii) generate robocup commentaries from actions,
(iii) convert questions to database queries.

49

train the system

    online training i.e. prediction and update for each example.
    at each greedy step, if a prediction   yt is incorrect, several updates are
made to the model to satisfy:

for each correct labeling alternative   yt   1
+yi

, u) > g(x,   yt, u).
    intuitively, we want any incorrect partial prediction to be ranked below
all correct partial labeling.
       order-free    is not directly supervised.

, g(x,   yt   1
+yi

    all updates performed with sgd + id26.

50

the learning signal: weak labeling scenario
even more challenging setting: training data {xi, yi, ui}i=1,...,m as before.
however, y is a set (bag) of concepts - no alignment to sentence.

this is more realistic:
a child sees actions and hears sentences     must learn correlation.

51

extension: weak concept labeling

52

hecookstherice<kitchen><garden><john><rice><cook>x:y:u:<gina><mark>location<cook><john><rice><hat><get><move>containedbylocationlocationlocationcontainedbylocationextension: weak concept labeling

solution: modi   ed laso updates     rank anything in the    bag    higher
than something not in the bag.

results:

method features
id166struct x + u (loc, contain)
nnof
x + u (loc, contain)
nnw eak x + u (loc, contain)

train err test err
18.68% 23.57%
0.0% 0.11%
0.0% 0.17%

53

