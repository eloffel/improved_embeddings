deep learning & neural networks

lecture 1

kevin duh

graduate school of information science
nara institute of science and technology

jan 14, 2014

2/40

3/40

4/40

what is deep learning?

a family of methods that uses deep architectures to learn high-level
feature representations

5/40

what is deep learning?

a family of methods that uses deep architectures to learn high-level
feature representations

5/40

example of trainable features

hierarchical object-parts features in id161 [lee et al., 2009]

6/40

course outline

goal:
to understand the foundations of neural networks and deep learning,
at a level su   cient for reading recent research papers

7/40

course outline

goal:
to understand the foundations of neural networks and deep learning,
at a level su   cient for reading recent research papers
schedule:

(cid:73) lecture 1 (jan 14): machine learning background & neural networks
(cid:73) lecture 2 (jan 16): deep architectures (dbn, sae)
(cid:73) lecture 3 (jan 21): applications in vision, speech, language
(cid:73) lecture 4 (jan 23): advanced topics in optimization

7/40

course outline

goal:
to understand the foundations of neural networks and deep learning,
at a level su   cient for reading recent research papers
schedule:

(cid:73) lecture 1 (jan 14): machine learning background & neural networks
(cid:73) lecture 2 (jan 16): deep architectures (dbn, sae)
(cid:73) lecture 3 (jan 21): applications in vision, speech, language
(cid:73) lecture 4 (jan 23): advanced topics in optimization

prerequisites:

(cid:73) basic calculus, id203, id202

7/40

course material

course website:
http://cl.naist.jp/~kevinduh/a/deep2014/

useful references:

1 yoshua bengio   s [bengio, 2009] short book: learning deep

architectures for ai1

2 yann lecun & marc   aurelio ranzato   s icml2013 tutorial2
3 richard socher et. al.   s naacl2013 tutorial3
4 geo    hinton   s coursera course4
5 theano code samples5
6 chris bishop   s book pattern recognition and machine learning

(prml)6

1http://www.iro.umontreal.ca/~bengioy/papers/ftml.pdf
2http://techtalks.tv/talks/deep-learning/58122/
3http://www.socher.org/index.php/deeplearningtutorial/
4https://www.coursera.org/course/neuralnets
5http://deeplearning.net/tutorial/
6http://research.microsoft.com/en-us/um/people/cmbishop/prml/

8/40

grading

the only criteria for grading:
are you actively participating and asking questions in class?

(cid:73) if you ask (or answer) 3+ questions, grade = a
(cid:73) if you ask (or answer) 2 questions, grade = b
(cid:73) if you ask (or answer) 1 question, grade = c
(cid:73) if you don   t ask (or answer) any questions, you get no credit.

9/40

best advice i got while in grad school

always ask questions!

10/40

best advice i got while in grad school

always ask questions!

if you don   t understand, you must ask questions in order to
understand.

10/40

best advice i got while in grad school

always ask questions!

if you don   t understand, you must ask questions in order to
understand.

if you understand, you will naturally have questions.

10/40

best advice i got while in grad school

always ask questions!

if you don   t understand, you must ask questions in order to
understand.

if you understand, you will naturally have questions.

having no questions is a sign that you are not thinking.

10/40

today   s topics

1 machine learning background

why machine learning is needed?
main concepts: generalization, model expressiveness, over   tting
formal notation

2 neural networks

1-layer nets (id28)
2-layer nets and model expressiveness
training by id26

11/40

today   s topics

1 machine learning background

why machine learning is needed?
main concepts: generalization, model expressiveness, over   tting
formal notation

2 neural networks

1-layer nets (id28)
2-layer nets and model expressiveness
training by id26

12/40

write a program    to recognize the digit 2

this is hard to do manually!
bool recognizedigitas2(int** imagepixels){...}

*example from hinton   s coursera course

13/40

write a program    to recognize the digit 2

this is hard to do manually!
bool recognizedigitas2(int** imagepixels){...}

machine learning solution:

1 assume you have a database (training data) of 2   s and non-2   s.

2 automatically    learn    this function from data

*example from hinton   s coursera course

13/40

a machine learning solution

training data are represented as pixel matrices:
classi   er is parameterized by weight matrix of same dimension.

14/40

a machine learning solution

training data are represented as pixel matrices:
classi   er is parameterized by weight matrix of same dimension.

training procedure:

1 when observe    2   , add 1 to corresponding matrix elements
2 when observe    non-2   , subtract 1 to corresponding matrix elements

0
0
0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
0
0
0

0
1
0
0
0
0
0
0
1
0

0
1
0
0
0
0
0
1
1
0

0
1
0
0
0
0
1
0
1
0

0
1
0
0
0
1
0
0
1
0

0
1
0
0
1
0
0
0
1
0

0
1
1
1
0
0
0
0
1
0

0
0
0
0
0
0
0
0
1
0

0
0
0
0
0
0
0
0
1
0

0
0
0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
0
0
0

0
1
0
0
0
0
0
0
1
0

0
1
0
0
0
0
0
1
1
0

0
1
0
0
0
0
1
0
1
0

0
1
0
0
0
1
0
0
1
0

0
1
0
0
1
0
0
0
1
0

0
0
0
0
-1
-1
-1
-1
-1
0

0
0
0
0
0
0
0
0
1
0

0
0
0
0
0
0
0
0
1
0

0
0
0
0
0
0
0
0
0
0

14/40

a machine learning solution

training data are represented as pixel matrices:
classi   er is parameterized by weight matrix of same dimension.

training procedure:

1 when observe    2   , add 1 to corresponding matrix elements
2 when observe    non-2   , subtract 1 to corresponding matrix elements

0
0
0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
0
0
0

0
1
0
0
0
0
0
0
1
0

0
1
0
0
0
0
0
1
1
0

0
1
0
0
0
0
1
0
1
0

0
1
0
0
0
1
0
0
1
0

0
1
0
0
1
0
0
0
1
0

0
1
1
1
0
0
0
0
1
0

0
0
0
0
0
0
0
0
1
0

0
0
0
0
0
0
0
0
1
0

0
0
0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
0
0
0

0
1
0
0
0
0
0
0
1
0

0
1
0
0
0
0
0
1
1
0

0
1
0
0
0
0
1
0
1
0

0
1
0
0
0
1
0
0
1
0

0
1
0
0
1
0
0
0
1
0

0
0
0
0
-1
-1
-1
-1
-1
0

0
0
0
0
0
0
0
0
1
0

0
0
0
0
0
0
0
0
1
0

0
0
0
0
0
0
0
0
0
0

test procedure: given new image, take sum of element-wise product.
if positive, predict    2   ; else predict    non-2   .

14/40

today   s topics

1 machine learning background

why machine learning is needed?
main concepts: generalization, model expressiveness, over   tting
formal notation

2 neural networks

1-layer nets (id28)
2-layer nets and model expressiveness
training by id26

15/40

generalization (cid:54)= memorization

key issue in machine learning: training data is limited

if the classi   er just memorizes the training data, it may perform
poorly on new data

   generalization    is ability to extend accurate predictions to new data

16/40

generalization (cid:54)= memorization

key issue in machine learning: training data is limited

if the classi   er just memorizes the training data, it may perform
poorly on new data

   generalization    is ability to extend accurate predictions to new data

e.g. consider shifted image:

will this classi   er generalize?

0
0
0
0
0
0
0
0
0
0

0
1
0
0
0
0
0
0
1
0

0
1
0
0
0
0
0
1
1
0

0
1
0
0
0
0
1
0
1
0

0
1
0
0
0
1
0
0
1
0

0
1
0
0
1
0
0
0
1
0

0
0
0
0
-1
-1
-1
-1
-1
0

0
0
0
0
0
0
0
0
1
0

0
0
0
0
0
0
0
0
1
0

0
0
0
0
0
0
0
0
0
0

16/40

generalization (cid:54)= memorization

one potential way to increase generalization ability:

discretize weight matrix with larger grids (fewer weights to train)

e.g. consider shifted image:

now will this classi   er generalize?

0
0
0
0
0
0
0
0
0
0

0
1
0
0
0
0
0
0
1
0

0
1
0
0
0
0
0
1
1
0

0
1
0
0
0
0
1
0
1
0

0
1
0
0
0
1
0
0
1
0

0
1
0
0
1
0
0
0
1
0

0
0
0
0
-1
-1
-1
-1
-1
0

0
0
0
0
0
0
0
0
1
0

0
0
0
0
0
0
0
0
1
0

0
0
0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
0
0
0

0
1
0
0
0
0
0
0
1
0

0
1
0
0
0
0
0
1
1
0

0
1
0
0
0
0
1
0
1
0

0
1
0
0
0
1
0
0
1
0

0
1
0
0
1
0
0
0
1
0

0
0
0
0
-1
-1
-1
-1
-1
0

0
0
0
0
0
0
0
0
1
0

0
0
0
0
0
0
0
0
1
0

0
0
0
0
0
0
0
0
0
0

1
0
0
0
1

1
0
0
1
1

1
0
1
0
1

0
0
-1
-1
0

0
0
0
0
1

17/40

model expressiveness and over   tting

a model with more weight parameters may    t training data better

but since training data is limited, expressive model stand the risk of
over   tting to peculiarities of the data.

less expressive model        more expressive model

(fewer weights)

(more weights)

under   t training data        over   t training data

18/40

model expressiveness and over   tting

fitting the training data (blue points: xn)
with a polynomial model: f (x) = w0 + w1x + w2x 2 + . . . + wm x m
under squared error objective 1
2

(cid:80)
n(f (xn)     tn)2

from prml chapter 1 [bishop, 2006]

19/40

xtm=001   101xtm=101   101xtm=301   101xtm=901   101today   s topics

1 machine learning background

why machine learning is needed?
main concepts: generalization, model expressiveness, over   tting
formal notation

2 neural networks

1-layer nets (id28)
2-layer nets and model expressiveness
training by id26

20/40

basic problem setup in machine learning

training data: a set of (x (m), y (m))m={1,2,..m} pairs, where input
x (m)     r d and output y (m) = {0, 1}
goal: learn function f : x     y to predicts correctly on new inputs x.

(cid:73) e.g. x=vectorized image pixels, y=2 or non-2

21/40

basic problem setup in machine learning

training data: a set of (x (m), y (m))m={1,2,..m} pairs, where input
x (m)     r d and output y (m) = {0, 1}
goal: learn function f : x     y to predicts correctly on new inputs x.

(cid:73) e.g. x=vectorized image pixels, y=2 or non-2

(cid:73) step 1: choose a function model family:

(cid:70) e.g. id28, support vector machines, neural networks

21/40

basic problem setup in machine learning

training data: a set of (x (m), y (m))m={1,2,..m} pairs, where input
x (m)     r d and output y (m) = {0, 1}
goal: learn function f : x     y to predicts correctly on new inputs x.

(cid:73) e.g. x=vectorized image pixels, y=2 or non-2

(cid:73) step 1: choose a function model family:

(cid:70) e.g. id28, support vector machines, neural networks

(cid:73) step 2: optimize parameters w on the training data

(cid:80)m
m=1(fw (x (m))     y (m))2

(cid:70) e.g. minimize id168 minw

21/40

today   s topics

1 machine learning background

why machine learning is needed?
main concepts: generalization, model expressiveness, over   tting
formal notation

2 neural networks

1-layer nets (id28)
2-layer nets and model expressiveness
training by id26

22/40

1-layer nets (id28)

function model: f (x) =   (w t    x + b)

(cid:73) parameters: vector w     r d , b is scalar bias term
(cid:73)    is a non-linearity, e.g. sigmoid:   (z) = 1/(1 + exp(   z))
(cid:73) for simplicity, sometimes write f (x) =   (w t x) where w = [w ; b] and

x = [x; 1]

23/40

1-layer nets (id28)

function model: f (x) =   (w t    x + b)

(cid:73) parameters: vector w     r d , b is scalar bias term
(cid:73)    is a non-linearity, e.g. sigmoid:   (z) = 1/(1 + exp(   z))
(cid:73) for simplicity, sometimes write f (x) =   (w t x) where w = [w ; b] and

x = [x; 1]

non-linearity will be important in expressiveness multi-layer nets.
other non-linearities, e.g., tanh(z) = (ez     e   z )/(ez + e   z )

23/40

training 1-layer nets: gradient

assume squared-error    loss(w ) = 1

2

(cid:80)

m(  (w t x (m))     y (m))2

(cid:80)
*an alternative is cross-id178 loss:
m y (m) log(  (w t x (m))) + (1     y (m)) log(1       (w t x (m)))

24/40

training 1-layer nets: gradient

assume squared-error    loss(w ) = 1

gradient:    w loss =(cid:80)

(cid:2)  (w t x (m))     y (m)(cid:3)  (cid:48)(w t x (m))x (m)

m(  (w t x (m))     y (m))2

2

m

(cid:80)

(cid:80)
*an alternative is cross-id178 loss:
m y (m) log(  (w t x (m))) + (1     y (m)) log(1       (w t x (m)))

24/40

training 1-layer nets: gradient

assume squared-error    loss(w ) = 1

gradient:    w loss =(cid:80)
(cid:73) general form of gradient: (cid:80)

(cid:2)  (w t x (m))     y (m)(cid:3)  (cid:48)(w t x (m))x (m)

m(  (w t x (m))     y (m))2

m error (m)       (cid:48)(in(m))     x (m)

m

2

(cid:80)

(cid:80)
*an alternative is cross-id178 loss:
m y (m) log(  (w t x (m))) + (1     y (m)) log(1       (w t x (m)))

24/40

training 1-layer nets: gradient

assume squared-error    loss(w ) = 1

gradient:    w loss =(cid:80)
(cid:73) general form of gradient: (cid:80)

m

(cid:80)

derivative of sigmoid   (z) = 1/(1 + exp(   z)):

2

m(  (w t x (m))     y (m))2

m error (m)       (cid:48)(in(m))     x (m)

(cid:2)  (w t x (m))     y (m)(cid:3)  (cid:48)(w t x (m))x (m)
(cid:19)
(cid:19)2 d
(cid:19)2
(cid:19)(cid:18) exp(   z)

dz
exp(   z)(   1)

(1 + exp(   z))

(cid:19)

1 + exp(   z)

(cid:48)(z) =

  

(cid:18)
(cid:18)
(cid:18)

d
dz
=    

=    

(cid:18)

1

1 + exp(   z)

1

1

1 + exp(   z)

1 + exp(   z)

1

1 + exp(   z)
=
=   (z)(1       (z))

(cid:80)
*an alternative is cross-id178 loss:
m y (m) log(  (w t x (m))) + (1     y (m)) log(1       (w t x (m)))

24/40

training 1-layer nets: id119 algorithm

general form of gradient: (cid:80)
2 compute    w loss =(cid:80)

id119 algorithm:

initialize w

1

3 w     w       (   w loss)
4 repeat steps 2-3 until some condition satis   ed

m error (m)       (cid:48)(in(m))     x (m)

m error (m)       (cid:48)(in(m))     x (m)

25/40

training 1-layer nets: id119 algorithm

general form of gradient: (cid:80)
2 compute    w loss =(cid:80)

id119 algorithm:

initialize w

1

3 w     w       (   w loss)
4 repeat steps 2-3 until some condition satis   ed

m error (m)       (cid:48)(in(m))     x (m)

m error (m)       (cid:48)(in(m))     x (m)

stochastic id119 (sgd) algorithm:

2

1

initialize w
for each sample (x (m), y (m)) in training set:

w     w       (error (m)       (cid:48)(in(m))     x (m))
3
4 repeat loop 2-3 until some condition satis   ed

25/40

training 1-layer nets: id119 algorithm

general form of gradient: (cid:80)
2 compute    w loss =(cid:80)

id119 algorithm:

initialize w

1

3 w     w       (   w loss)
4 repeat steps 2-3 until some condition satis   ed

m error (m)       (cid:48)(in(m))     x (m)

m error (m)       (cid:48)(in(m))     x (m)

stochastic id119 (sgd) algorithm:

2

1

initialize w
for each sample (x (m), y (m)) in training set:

w     w       (error (m)       (cid:48)(in(m))     x (m))
3
4 repeat loop 2-3 until some condition satis   ed

learning rate    > 0 & stopping condition are important in practice

25/40

intuition of sgd update

for some sample (x (m), y (m)):

w     w       ((  (w t x (m))     y (m))       (cid:48)(w t x (m))     x (m))

  (w t x (m))

y (m) error

new w

new prediction

0
1
0
1

0
1
1
0

0
0
-1
+1

no change
no change

w +     (cid:48)(in(m))x (m)
w         (cid:48)(in(m))x (m)

0
1
    0
    1

26/40

intuition of sgd update

for some sample (x (m), y (m)):

w     w       ((  (w t x (m))     y (m))       (cid:48)(w t x (m))     x (m))

  (w t x (m))

y (m) error

new w

new prediction

0
1
0
1

0
1
1
0

(cid:2)w +     (cid:48)(in(m))x (m)(cid:3)t

0
0
-1
+1

no change
no change

w +     (cid:48)(in(m))x (m)
w         (cid:48)(in(m))x (m)

0
1
    0
    1

x (m) = w t x (m) +     (cid:48)(in(m))||x (m)||2     w t x (m)

26/40

intuition of sgd update

for some sample (x (m), y (m)):

w     w       ((  (w t x (m))     y (m))       (cid:48)(w t x (m))     x (m))

  (w t x (m))

y (m) error

new w

new prediction

0
1
0
1

0
1
1
0

0
0
-1
+1

no change
no change

w +     (cid:48)(in(m))x (m)
w         (cid:48)(in(m))x (m)

0
1
    0
    1

(cid:2)w +     (cid:48)(in(m))x (m)(cid:3)t

x (m) = w t x (m) +     (cid:48)(in(m))||x (m)||2     w t x (m)
  (cid:48)(w t x (m)) is near 0 when con   dent, near 0.25 when uncertain.

26/40

intuition of sgd update

for some sample (x (m), y (m)):

w     w       ((  (w t x (m))     y (m))       (cid:48)(w t x (m))     x (m))

  (w t x (m))

y (m) error

new w

new prediction

0
1
0
1

0
1
1
0

0
0
-1
+1

no change
no change

w +     (cid:48)(in(m))x (m)
w         (cid:48)(in(m))x (m)

0
1
    0
    1

(cid:2)w +     (cid:48)(in(m))x (m)(cid:3)t

x (m) = w t x (m) +     (cid:48)(in(m))||x (m)||2     w t x (m)
  (cid:48)(w t x (m)) is near 0 when con   dent, near 0.25 when uncertain.
large    = more aggressive updates; small    = more conservative

26/40

intuition of sgd update

for some sample (x (m), y (m)):

w     w       ((  (w t x (m))     y (m))       (cid:48)(w t x (m))     x (m))

  (w t x (m))

y (m) error

new w

new prediction

0
1
0
1

0
1
1
0

0
0
-1
+1

no change
no change

w +     (cid:48)(in(m))x (m)
w         (cid:48)(in(m))x (m)

0
1
    0
    1

(cid:2)w +     (cid:48)(in(m))x (m)(cid:3)t

x (m) = w t x (m) +     (cid:48)(in(m))||x (m)||2     w t x (m)
  (cid:48)(w t x (m)) is near 0 when con   dent, near 0.25 when uncertain.
large    = more aggressive updates; small    = more conservative
sgd improves classi   cation for current sample, but no guarantee
about others

26/40

geometric view of sgd update

(cid:80)

loss objective contour plot: 1
2

m(  (w t x (m))     y (m))2 + ||w||

(cid:73) id119 goes in steepest descent direction, but slower to

compute per iteration for large datasets

(cid:73) sgd can be viewed as noisy descent, but faster per iteration
(cid:73) in practice, a good tradeo    is mini-batch sgd

27/40

e   ect of learning rate    on convergence speed

sgd update: w     w       (error (m)       (cid:48)(in(m))     x (m))

(cid:73) ideally,    should be as large as possible without causing divergence.
(cid:73) common heuristic:   (t) =   0

1+  t = o(1/t)

analysis by [schaul et al., 2013] (in plot,          ):

28/40

(cid:80)
generalization issues: id173 and early-stopping
m(  (w t x (m))     y (m))2 on training data

optimizing loss(w ) = 1
2
not necessarily leads to generalization.

figures from chapter 5, [bishop, 2006]

29/40

010203040500.150.20.25010203040500.350.40.45(cid:80)
generalization issues: id173 and early-stopping
m(  (w t x (m))     y (m))2 on training data
(cid:80)
m(  (w t x (m))     y (m))2 + ||w||
reduces sensitivity to training input and decreases risk of over   tting

optimizing loss(w ) = 1
2
not necessarily leads to generalization.
1 adding id173: loss(w ) = 1
2

figures from chapter 5, [bishop, 2006]

29/40

010203040500.150.20.25010203040500.350.40.45(cid:80)
generalization issues: id173 and early-stopping
m(  (w t x (m))     y (m))2 on training data
(cid:80)
m(  (w t x (m))     y (m))2 + ||w||
reduces sensitivity to training input and decreases risk of over   tting

optimizing loss(w ) = 1
2
not necessarily leads to generalization.
1 adding id173: loss(w ) = 1
2

2 early stopping:

(cid:70) prepare separate training and validation (development) data
(cid:70) optimize loss(w ) on training but stop when loss(w ) on validation

stops improving

figures from chapter 5, [bishop, 2006]

29/40

010203040500.150.20.25010203040500.350.40.45summary

1 given training data: (x (m), y (m))m={1,2,..m}
2 optimize a model f (x) =   (w t    x + b) under

(cid:80)
m(  (w t x (m))     y (m))2

loss(w ) = 1
2

30/40

summary

1 given training data: (x (m), y (m))m={1,2,..m}
2 optimize a model f (x) =   (w t    x + b) under

(cid:80)
m(  (w t x (m))     y (m))2

3 general form of gradient: (cid:80)

loss(w ) = 1
2

m error (m)       (cid:48)(in(m))     x (m)

30/40

summary

1 given training data: (x (m), y (m))m={1,2,..m}
2 optimize a model f (x) =   (w t    x + b) under

(cid:80)
m(  (w t x (m))     y (m))2

3 general form of gradient: (cid:80)

loss(w ) = 1
2

m error (m)       (cid:48)(in(m))     x (m)

4 sgd algorithm: for each sample (x (m), y (m)) in training set,

w     w       (error (m)       (cid:48)(in(m))     x (m))

30/40

summary

1 given training data: (x (m), y (m))m={1,2,..m}
2 optimize a model f (x) =   (w t    x + b) under

(cid:80)
m(  (w t x (m))     y (m))2

3 general form of gradient: (cid:80)

loss(w ) = 1
2

m error (m)       (cid:48)(in(m))     x (m)

4 sgd algorithm: for each sample (x (m), y (m)) in training set,

w     w       (error (m)       (cid:48)(in(m))     x (m))

5

important issues:

(cid:73) optimization speed/convergence: batch vs. mini-batch, learning rate   
(cid:73) generalization ability: id173, early-stopping

30/40

today   s topics

1 machine learning background

why machine learning is needed?
main concepts: generalization, model expressiveness, over   tting
formal notation

2 neural networks

1-layer nets (id28)
2-layer nets and model expressiveness
training by id26

31/40

2-layer neural networks

y

w1

w2

w3

h1

h2

h3

w11

w12

wj

hj

wij

x1

xi

f (x) =   ((cid:80)

x3

x2

j wj    hj ) =   ((cid:80)

j wj      ((cid:80)

x4

i wij xi ))

called multilayer id88 (mlp), but more like multilayer id28

32/40

2-layer neural networks

y

w1

w2

w3

h1

h2

h3

w11

w12

wj

hj

wij

x1

xi

f (x) =   ((cid:80)

x3

x2

j wj    hj ) =   ((cid:80)

j wj      ((cid:80)

x4

i wij xi ))

hidden units hj    s can be viewed as new    features    from combining xi    s

called multilayer id88 (mlp), but more like multilayer id28

32/40

modeling complex non-linearities

given same number of units (with non-linear activation), a deeper
architecture is more expressive than a shallow one [bishop, 1995]

(cid:73) 1-layer nets only model linear hyperplanes
(cid:73) 2-layer nets are universal function approximators: given in   nite hidden

nodes, it can express any continuous function

(cid:73) >3-layer nets can do so with fewer nodes/weights

33/40

today   s topics

1 machine learning background

why machine learning is needed?
main concepts: generalization, model expressiveness, over   tting
formal notation

2 neural networks

1-layer nets (id28)
2-layer nets and model expressiveness
training by id26

34/40

training a 2-layer net with id26

y

w1

w2

w3

h1

h2

h3

adjust weights

predict f (x (m))

w11

w12

wj

hj

wij

xi

x1

1. for each sample, compute f (x (m)) =   ((cid:80)

x2

x3

x4

j wj      ((cid:80)

i wij x (m)

i

))

2. if f (x (m)) (cid:54)= y (m), back-propagate error and adjust weights {wij , wj}.

35/40

derivatives of the weights

and loss per sample: loss =(cid:80)

assume two outputs (y1, y2) per input x,

1

2 [  (ink )     yk ]2

k

yk

wjk

hj

wij

xi

y1

y2

h1

h2

h3

x1

x2

x3

x4

36/40

derivatives of the weights

and loss per sample: loss =(cid:80)

assume two outputs (y1, y2) per input x,

1

2 [  (ink )     yk ]2

k

yk

wjk

hj

wij

xi

   loss
   wjk

=    loss
   ink

   ink
   wjk

=   k

y1

y2

h1

h2

h3

x1

   ((cid:80)

j wjk hj )
   wjk

x2

x3

x4

=   k hj

36/40

derivatives of the weights

and loss per sample: loss =(cid:80)

assume two outputs (y1, y2) per input x,

1

2 [  (ink )     yk ]2

k

y1

y2

h1

h2

h3

yk

wjk

hj

wij

   loss
   wjk
   loss
   wij

=    loss
   ink
=    loss
   inj

   ink
   wjk
   inj
   wij

=   k

=   j

x2

x3

x4

=   k hj

xi

x1

   ((cid:80)
   ((cid:80)

j wjk hj )
   wjk
j wij xi )
   wij

=   j xi

36/40

derivatives of the weights

and loss per sample: loss =(cid:80)

assume two outputs (y1, y2) per input x,

1

2 [  (ink )     yk ]2

k

y1

y2

h1

h2

h3

yk

wjk

hj

wij

x1

xi

=   k

   ((cid:80)
   ((cid:80)
2 [  (ink )     yk ]2(cid:17)

j wjk hj )
   wjk
j wij xi )
   wij

=   j

   ink
   wjk
   inj
   wij
1

k

x2

x3

x4

=   k hj

=   j xi
= [  (ink )     yk ]   (cid:48)(ink )

   loss
   wjk
   loss
   wij
  k =    
   ink

=    loss
   ink
=    loss
   inj

(cid:16)(cid:80)

36/40

derivatives of the weights

and loss per sample: loss =(cid:80)

assume two outputs (y1, y2) per input x,

1

2 [  (ink )     yk ]2

k

y1

y2

h1

h2

h3

yk

wjk

hj

wij

x1

xi

=   k

   ((cid:80)
   ((cid:80)
2 [  (ink )     yk ]2(cid:17)
=(cid:80)

j wjk hj )
   wjk
j wij xi )
   wij

k   k       

=   j

   inj

   ink
   wjk
   inj
   wij
1

k

   ink
   inj

x2

x3

x4

=   k hj

=   j xi
= [  (ink )     yk ]   (cid:48)(ink )

(cid:17)

= [(cid:80)

(cid:16)(cid:80)

j wjk   (inj )

=    loss
   ink
=    loss
   inj

(cid:16)(cid:80)

   loss
   wjk
   loss
   wij
  k =    
   ink

  j =(cid:80)

   loss
   ink

k

k   k wjk ]   (cid:48)(inj )

36/40

id26 algorithm

all updates involve some scaled error from output     input feature:

=   k hj where   k = [  (ink )     yk ]   (cid:48)(ink )

=   j xi where   j = [(cid:80)

k   k wjk ]   (cid:48)(inj )

   loss
   wjk
   loss
   wij

after forward pass, compute   k from    nal layer, then   j for previous layer.
for deeper nets, iterate backwards further.

yk

wjk

hj

wij

xi

y1

y2

h1

h2

h3

x1

x2

x3

x4

37/40

summary

1 by extending from 1-layer to 2-layer net, we get dramatic increase in

model expressiveness:

38/40

summary

1 by extending from 1-layer to 2-layer net, we get dramatic increase in

model expressiveness:

2 id26 is an e   cient way to train 2-layer nets:

(cid:73) similar to sgd for 1-layer net, just more chaining in gradient

38/40

summary

1 by extending from 1-layer to 2-layer net, we get dramatic increase in

model expressiveness:

2 id26 is an e   cient way to train 2-layer nets:

(cid:73) similar to sgd for 1-layer net, just more chaining in gradient
(cid:73) general form: update wij by   j xi , and   j is scaled/weighted sum of

errors from outgoing layers

38/40

summary

1 by extending from 1-layer to 2-layer net, we get dramatic increase in

model expressiveness:

2 id26 is an e   cient way to train 2-layer nets:

(cid:73) similar to sgd for 1-layer net, just more chaining in gradient
(cid:73) general form: update wij by   j xi , and   j is scaled/weighted sum of

errors from outgoing layers

3

ideally, we want even deeper architectures

38/40

summary

1 by extending from 1-layer to 2-layer net, we get dramatic increase in

model expressiveness:

2 id26 is an e   cient way to train 2-layer nets:

(cid:73) similar to sgd for 1-layer net, just more chaining in gradient
(cid:73) general form: update wij by   j xi , and   j is scaled/weighted sum of

errors from outgoing layers

3

ideally, we want even deeper architectures

(cid:73) but id26 becomes ine   ective due to vanishing gradients
(cid:73) deep learning comes to the rescue! (next lecture)

38/40

references i

bengio, y. (2009).
learning deep architectures for ai, volume foundations and trends in
machine learning.
now publishers.

bishop, c. (1995).
neural networks for pattern recognition.
oxford university press.

bishop, c. (2006).
pattern recognition and machine learning.
springer.

lee, h., grosse, r., ranganath, r., and ng, a. (2009).
convolutional id50 for scalable unsupervised learning
of hierarchical representations.
in icml.

39/40

references ii

schaul, t., zhang, s., and lecun, y. (2013).
no more pesky learning rates.
in proc. international conference on machine learning (icml   13).

40/40

