deep learning & neural networks

lecture 2

kevin duh

graduate school of information science
nara institute of science and technology

jan 16, 2014

today   s topics

1 general ideas in deep learning

motivation for deep architectures and why is it hard?
main breakthrough in 2006: layer-wise pre-training

2 approach 1: deep belief nets [hinton et al., 2006]

restricted id82s (rbm)
training rbms with contrastive divergence
stacking rbms to form deep belief nets

3 approach 2: stacked auto-encoders [bengio et al., 2006]

auto-encoders
denoising auto-encoders

4 discussions

why it works, when it works, and the bigger picture

2/45

today   s topics

1 general ideas in deep learning

motivation for deep architectures and why is it hard?
main breakthrough in 2006: layer-wise pre-training

2 approach 1: deep belief nets [hinton et al., 2006]

restricted id82s (rbm)
training rbms with contrastive divergence
stacking rbms to form deep belief nets

3 approach 2: stacked auto-encoders [bengio et al., 2006]

auto-encoders
denoising auto-encoders

4 discussions

why it works, when it works, and the bigger picture

3/45

the promise of deep architectures

understanding in ai requires
high-level abstractions, modeled
by highly non-linear functions

4/45

the promise of deep architectures

understanding in ai requires
high-level abstractions, modeled
by highly non-linear functions

these abstractions must
disentangle factors of variation
in data (e.g. 3d pose, lighting)

4/45

the promise of deep architectures

understanding in ai requires
high-level abstractions, modeled
by highly non-linear functions

these abstractions must
disentangle factors of variation
in data (e.g. 3d pose, lighting)

deep architecture is one way to
achieve this: each intermediate
layer is a successively higher
level abstraction

(*example from [bengio, 2009])

4/45

the promise of deep architectures

y

h(cid:48)

2

h2

x2

h(cid:48)

1

h1

x1

h(cid:48)

3

h3

x3

5/45

why are deep architectures hard to train?

vanishing gradient problem in
id26
=    loss
   inj

   loss
   wij

=   j xi

   inj
   wij

(cid:105)

(cid:104)(cid:80)

  j =

j+1   j+1wj(j+1)

  (cid:48)(inj )

  j may vanish after repeated
multiplication

y

h(cid:48)

2

h2

x2

h(cid:48)

1

h1

x1

h(cid:48)

3

wj(j+1)

h3

wij

x3

6/45

empirical results: poor performance of id26
on deep neural nets [erhan et al., 2009]

each layer: initialize wij by uniform[   1/(cid:112)(fanin), 1/(cid:112)(fanin)]

mnist digit classi   cation task; 400 trials (random seed)

although l + 1 layers is more expressive, worse error than l layers

7/45

local optimum issue in neural nets

for 2-layer net and more, the training objective is not convex, so
di   erent local optima may be achieved depending on initial point

for deep architectures, id26 is apparently getting a local
optimum that does not generalize well

*figure from chapter 5, [bishop, 2006]

8/45

w1w2e(w)wawbwc   etoday   s topics

1 general ideas in deep learning

motivation for deep architectures and why is it hard?
main breakthrough in 2006: layer-wise pre-training

2 approach 1: deep belief nets [hinton et al., 2006]

restricted id82s (rbm)
training rbms with contrastive divergence
stacking rbms to form deep belief nets

3 approach 2: stacked auto-encoders [bengio et al., 2006]

auto-encoders
denoising auto-encoders

4 discussions

why it works, when it works, and the bigger picture

9/45

layer-wise pre-training [hinton et al., 2006]

first, train one layer at a time, optimizing data-likelihood objective p(x)

y

h(cid:48)

2

h2

x2

h(cid:48)

1

h1

x1

h(cid:48)

3

h3

x3

train layer1

10/45

layer-wise pre-training [hinton et al., 2006]

first, train one layer at a time, optimizing data-likelihood objective p(x)

y

h(cid:48)

2

h2

x2

h(cid:48)

1

h1

x1

h(cid:48)

3

h3

x3

train layer2

keep layer1    xed

11/45

layer-wise pre-training [hinton et al., 2006]
finally,    ne-tune labeled objective p(y|x) by id26

y

h(cid:48)

2

h2

x2

h(cid:48)

1

h1

x1

h(cid:48)

3

h3

x3

adjust weights

predict f(x)

12/45

layer-wise pre-training [hinton et al., 2006]

key idea:
focus on modeling the input p(x ) better with each successive layer.
worry about optimizing the task p(y|x ) later.

   if you want to do id161,    rst learn computer
graphics.        geo    hinton

y

h(cid:48)

2

h2

x2

h(cid:48)

1

h1

x1

h(cid:48)

3

h3

x3

train layer2

train layer1

13/45

layer-wise pre-training [hinton et al., 2006]

key idea:
focus on modeling the input p(x ) better with each successive layer.
worry about optimizing the task p(y|x ) later.

   if you want to do id161,    rst learn computer
graphics.        geo    hinton

y

h(cid:48)

2

h2

x2

h(cid:48)

1

h1

x1

h(cid:48)

3

h3

x3

train layer2

train layer1

extra advantage:
can exploit large
amounts of unlabeled
data!

13/45

today   s topics

1 general ideas in deep learning

motivation for deep architectures and why is it hard?
main breakthrough in 2006: layer-wise pre-training

2 approach 1: deep belief nets [hinton et al., 2006]

restricted id82s (rbm)
training rbms with contrastive divergence
stacking rbms to form deep belief nets

3 approach 2: stacked auto-encoders [bengio et al., 2006]

auto-encoders
denoising auto-encoders

4 discussions

why it works, when it works, and the bigger picture

14/45

general approach for deep learning

recall the problem setup: learn function f : x     y

15/45

general approach for deep learning

recall the problem setup: learn function f : x     y
but rather doing this directly, we    rst learn hidden features h that
model input x, i.e. x     h     y

15/45

general approach for deep learning

recall the problem setup: learn function f : x     y
but rather doing this directly, we    rst learn hidden features h that
model input x, i.e. x     h     y
how do we discover useful latent features h from data x?

(cid:73) di   erent deep learning methods di   er by this basic component
(cid:73) e.g. deep belief nets use restricted id82s (rbms)

15/45

restricted id82 (rbm)

rbm is a simple energy-based model: p(x, h) = 1
z  

(cid:73) with only h-x interactions: e  (x, h) =    x t wh     bt x     d t h
(cid:73) here, we assume hj and xi are binary variables

(cid:73) normalizer: z   =(cid:80)

(x,h) exp(   e  (x, h)) is called partition function

exp (   e  (x, h))

h1

x1

h2

x2

h3

x3

16/45

restricted id82 (rbm)

rbm is a simple energy-based model: p(x, h) = 1
z  

(cid:73) with only h-x interactions: e  (x, h) =    x t wh     bt x     d t h
(cid:73) here, we assume hj and xi are binary variables

(cid:73) normalizer: z   =(cid:80)

(x,h) exp(   e  (x, h)) is called partition function

exp (   e  (x, h))

h1

x1

h2

x2

h3

x3

example:

(cid:73) let weights (h1, x1), (h1, x3) be positive, others be zero, b = d = 0.

16/45

restricted id82 (rbm)

rbm is a simple energy-based model: p(x, h) = 1
z  

(cid:73) with only h-x interactions: e  (x, h) =    x t wh     bt x     d t h
(cid:73) here, we assume hj and xi are binary variables

(cid:73) normalizer: z   =(cid:80)

(x,h) exp(   e  (x, h)) is called partition function

exp (   e  (x, h))

h1

x1

h2

x2

h3

x3

example:

(cid:73) let weights (h1, x1), (h1, x3) be positive, others be zero, b = d = 0.
(cid:73) then this rbm de   nes a distribution over [x1, x2, x3, h1, h2, h3] where
p(x1 = 1, x2 = 0, x3 = 1, h1 = 1, h2 = 0, h3 = 0) has high id203

16/45

computing posteriors in rbms

computing p(h|x) is easy due to factorization:

p(h|x) =

=

=

=

=

=

p(x, h)
h p(x, h)
exp(x t wh + bt x + d t h)
h exp(x t wh + bt x + d t h)

(cid:80)
1/z   exp(    e(x, h))
h 1/z   exp(    e(x, h))
(cid:81)
h2   {0,1}       (cid:80)

(cid:81)

hj
j exp(x t wj hj + dj hj )
hj   {0,1} exp(x t wj hj + dj hj )

(cid:80)
(cid:80)
(cid:80)
h1   {0,1}(cid:80)
(cid:81)
(cid:80)
(cid:81)
(cid:89)
(cid:80)

j

exp(x t wj hj + dj hj )

hj   {0,1} exp(x t wj hj + dj hj )

j

(cid:89)

j

=

p(hj|x)

j exp(x t wj hj + dj hj )    exp(bt x)

j exp(x t wj hj + dj hj )    exp(bt x)

note p(hj = 1|x) = exp(x t wj + dj )/z =   (x t wj + dj )

17/45

computing posteriors in rbms

computing p(h|x) is easy due to factorization:

p(h|x) =

=

=

=

=

=

p(x, h)
h p(x, h)
exp(x t wh + bt x + d t h)
h exp(x t wh + bt x + d t h)

(cid:80)
1/z   exp(    e(x, h))
h 1/z   exp(    e(x, h))
(cid:81)
h2   {0,1}       (cid:80)

(cid:81)

hj
j exp(x t wj hj + dj hj )
hj   {0,1} exp(x t wj hj + dj hj )

(cid:80)
(cid:80)
(cid:80)
h1   {0,1}(cid:80)
(cid:81)
(cid:80)
(cid:81)
(cid:89)
(cid:80)

j

exp(x t wj hj + dj hj )

hj   {0,1} exp(x t wj hj + dj hj )

j

(cid:89)

j

=

p(hj|x)

note p(hj = 1|x) = exp(x t wj + dj )/z =   (x t wj + dj )

similarly, computing p(x|h) =(cid:81)

i p(xi|h) is easy

j exp(x t wj hj + dj hj )    exp(bt x)

j exp(x t wj hj + dj hj )    exp(bt x)

17/45

today   s topics

1 general ideas in deep learning

motivation for deep architectures and why is it hard?
main breakthrough in 2006: layer-wise pre-training

2 approach 1: deep belief nets [hinton et al., 2006]

restricted id82s (rbm)
training rbms with contrastive divergence
stacking rbms to form deep belief nets

3 approach 2: stacked auto-encoders [bengio et al., 2006]

auto-encoders
denoising auto-encoders

4 discussions

why it works, when it works, and the bigger picture

18/45

training rbms to optimize p(x )

derivative of the log-likelihood:    wij log pw (x = x (m))

(cid:88)
(cid:88)

h

=    wij log

pw (x = x (m), h)

h

1
zw

exp (    ew(x(m), h))

(cid:88)
=    wij log
=        wij log zw +    wij log
(cid:88)
e(    ew(x,h))    wij ew(x, h)   
(cid:88)

(cid:80)
pw (x, h)[   wij ew(x, h)]    (cid:88)

=

h,x

h

=

1
zw

exp (    ew(x(m), h))

1

h e(    ew(x(m),h))

(cid:88)

h

h,x

h

=    ep(x,h)[xi    hj ] + e

p(h|x=x (m))[x (m)

i

   hj ]

pw (x (m), h)[   wij ew(x(m), h)]

(1)

(2)

(3)

e(    ew(x(m),h))    wij ew(x(m), h)

(4)

(5)

19/45

training rbms to optimize p(x )

derivative of the log-likelihood:    wij log pw (x = x (m))

(cid:88)
(cid:88)

h

=    wij log

pw (x = x (m), h)

h

1
zw

exp (    ew(x(m), h))

(cid:88)
=    wij log
=        wij log zw +    wij log
(cid:88)
e(    ew(x,h))    wij ew(x, h)   
(cid:88)

(cid:80)
pw (x, h)[   wij ew(x, h)]    (cid:88)

=

h,x

h

=

1
zw

exp (    ew(x(m), h))

1

h e(    ew(x(m),h))

(cid:88)

h

h,x

h

=    ep(x,h)[xi    hj ] + e

p(h|x=x (m))[x (m)

i

   hj ]

(1)

(2)

(3)

e(    ew(x(m),h))    wij ew(x(m), h)

pw (x (m), h)[   wij ew(x(m), h)]

(4)

(5)

second term (positive phase) increases id203 of x (m); first term
(negative phase) decreases id203 of samples generated by the model

19/45

contrastive divergence algorithm

the negative phase term (ep(x,h)[xi    hj ]) is expensive because it
requires sampling (x,h) from the model

20/45

contrastive divergence algorithm

the negative phase term (ep(x,h)[xi    hj ]) is expensive because it
requires sampling (x,h) from the model

id150 (sample x then h iteratively) works, but waiting for
convergence at each gradient step is slow.

20/45

contrastive divergence algorithm

the negative phase term (ep(x,h)[xi    hj ]) is expensive because it
requires sampling (x,h) from the model

id150 (sample x then h iteratively) works, but waiting for
convergence at each gradient step is slow.
contrastive divergence is a faster but biased method: initialize with
training point and wait only a few (usu. 1) sampling steps

20/45

contrastive divergence algorithm

the negative phase term (ep(x,h)[xi    hj ]) is expensive because it
requires sampling (x,h) from the model

id150 (sample x then h iteratively) works, but waiting for
convergence at each gradient step is slow.
contrastive divergence is a faster but biased method: initialize with
training point and wait only a few (usu. 1) sampling steps

1 let x (m) be training point, w = [wij ] be current model weights
i + dj )    j.

2 sample   hj     {0, 1} from p(hj|x = x (m)) =   ((cid:80)
3 sample   xi     {0, 1} from p(xi|h =   h) =   ((cid:80)
4 sample   hj     {0, 1} from p(hj|x =   x) =   ((cid:80)

j wij   hj + bi )    i.
i wij   xi + dj )    j.

i wij x (m)

5 wij     wij +   (x (m)

     hj       xi      hj )

i

20/45

pictorial view of contrastive divergence

goal: make rbm p(x, h) have high id203 on training samples
to do so, we   ll    steal    id203 mass from nearby samples that
incorrectly preferred by the model
for detailed analysis, see [carreira-perpinan and hinton, 2005]

21/45

today   s topics

1 general ideas in deep learning

motivation for deep architectures and why is it hard?
main breakthrough in 2006: layer-wise pre-training

2 approach 1: deep belief nets [hinton et al., 2006]

restricted id82s (rbm)
training rbms with contrastive divergence
stacking rbms to form deep belief nets

3 approach 2: stacked auto-encoders [bengio et al., 2006]

auto-encoders
denoising auto-encoders

4 discussions

why it works, when it works, and the bigger picture

22/45

deep belief nets (dbn) = stacked rbm

h(cid:48)(cid:48)

1

h(cid:48)

1

h1

x1

h(cid:48)(cid:48)

2

h(cid:48)

2

h2

x2

h(cid:48)(cid:48)

3

layer3 rbm

h(cid:48)

3

layer2 rbm

h3

layer1 rbm

x3

dbn de   nes a probabilistic
generative model p(x) =

(cid:80)
h,h(cid:48),h(cid:48)(cid:48) p(x|h)p(h|h(cid:48))p(h(cid:48), h(cid:48)(cid:48))
(top 2 layers is interpreted as a
rbm; lower layers are directed
sigmoids)

23/45

deep belief nets (dbn) = stacked rbm

h(cid:48)(cid:48)

1

h(cid:48)

1

h1

x1

h(cid:48)(cid:48)

2

h(cid:48)

2

h2

x2

h(cid:48)(cid:48)

3

layer3 rbm

h(cid:48)

3

layer2 rbm

h3

layer1 rbm

x3

dbn de   nes a probabilistic
generative model p(x) =

(cid:80)
h,h(cid:48),h(cid:48)(cid:48) p(x|h)p(h|h(cid:48))p(h(cid:48), h(cid:48)(cid:48))
(top 2 layers is interpreted as a
rbm; lower layers are directed
sigmoids)

stacked rbms can also be used
to initialize a deep neural
network (dnn)

23/45

generating data from a deep generative model

after training on 20k images, the generative model of
[salakhutdinov and hinton, 2009]* can generate random images
(dimension=8976) that are amazingly realistic!

this model is a deep id82 (dbm), di   erent from deep

belief nets (dbn) but also built by stacking rbms.

24/45

summary: things to remember about dbns

1 layer-wise pre-training is the innovation that rekindled interest in

deep architectures.

25/45

summary: things to remember about dbns

1 layer-wise pre-training is the innovation that rekindled interest in

deep architectures.

2 pre-training focuses on optimizing likelihood on the data, not the

target label. first model p(x) to do better p(y|x).

25/45

summary: things to remember about dbns

1 layer-wise pre-training is the innovation that rekindled interest in

deep architectures.

2 pre-training focuses on optimizing likelihood on the data, not the

target label. first model p(x) to do better p(y|x).

3 why rbm? p(h|x) is tractable, so it   s easy to stack.

25/45

summary: things to remember about dbns

1 layer-wise pre-training is the innovation that rekindled interest in

deep architectures.

2 pre-training focuses on optimizing likelihood on the data, not the

target label. first model p(x) to do better p(y|x).

3 why rbm? p(h|x) is tractable, so it   s easy to stack.
4 rbm training can be expensive. solution: contrastive divergence

25/45

summary: things to remember about dbns

1 layer-wise pre-training is the innovation that rekindled interest in

deep architectures.

2 pre-training focuses on optimizing likelihood on the data, not the

target label. first model p(x) to do better p(y|x).

3 why rbm? p(h|x) is tractable, so it   s easy to stack.
4 rbm training can be expensive. solution: contrastive divergence

5 dbn formed by stacking rbms is a probabilistic generative model

25/45

today   s topics

1 general ideas in deep learning

motivation for deep architectures and why is it hard?
main breakthrough in 2006: layer-wise pre-training

2 approach 1: deep belief nets [hinton et al., 2006]

restricted id82s (rbm)
training rbms with contrastive divergence
stacking rbms to form deep belief nets

3 approach 2: stacked auto-encoders [bengio et al., 2006]

auto-encoders
denoising auto-encoders

4 discussions

why it works, when it works, and the bigger picture

26/45

auto-encoders: simpler alternatives to rbms

x(cid:48)

1

x(cid:48)

2

x(cid:48)

3

h1

h2

x1

x2

x3

decoder: x(cid:48) =   (w (cid:48)h + d)

encoder: h =   (wx + b)

27/45

auto-encoders: simpler alternatives to rbms

x(cid:48)

1

x(cid:48)

2

x(cid:48)

3

h1

h2

x1

x2

x3

decoder: x(cid:48) =   (w (cid:48)h + d)

encoder: h =   (wx + b)

e.g. loss =(cid:80)

encourage h to give small reconstruction error:

m ||x (m)     decoder(encoder(x (m)))||2

27/45

auto-encoders: simpler alternatives to rbms

x(cid:48)

1

x(cid:48)

2

x(cid:48)

3

h1

h2

x1

x2

x3

decoder: x(cid:48) =   (w (cid:48)h + d)

encoder: h =   (wx + b)

e.g. loss =(cid:80)

encourage h to give small reconstruction error:

m ||x (m)     decoder(encoder(x (m)))||2

reconstruction: x(cid:48) =   (w (cid:48)  (wx + b) + d)

27/45

auto-encoders: simpler alternatives to rbms

x(cid:48)

1

x(cid:48)

2

x(cid:48)

3

h1

h2

x1

x2

x3

decoder: x(cid:48) =   (w (cid:48)h + d)

encoder: h =   (wx + b)

e.g. loss =(cid:80)

encourage h to give small reconstruction error:

m ||x (m)     decoder(encoder(x (m)))||2

reconstruction: x(cid:48) =   (w (cid:48)  (wx + b) + d)
this can be trained with the same id26 algorithm for
2-layer nets, with x (m) as both input and output

27/45

stacked auto-encoders (sae)

the encoder/decoder gives same form p(h|x), p(x|h) as rbms, so
can be stacked in the same way to form deep architectures

28/45

stacked auto-encoders (sae)

the encoder/decoder gives same form p(h|x), p(x|h) as rbms, so
can be stacked in the same way to form deep architectures

y

h(cid:48)

1

h(cid:48)

2

h1

h2

h3

layer3 encoder

layer2 encoder

layer1 encoder

x1

x2

x3

x4

28/45

stacked auto-encoders (sae)

the encoder/decoder gives same form p(h|x), p(x|h) as rbms, so
can be stacked in the same way to form deep architectures

y

h(cid:48)

1

h(cid:48)

2

h1

h2

h3

layer3 encoder

layer2 encoder

layer1 encoder

x1

x2

x3

x4

unlike rbms, auto-encoders are deterministic.

(cid:73) h =   (wx + b), not p(h = {0, 1}) =   (wx + b)

28/45

stacked auto-encoders (sae)

the encoder/decoder gives same form p(h|x), p(x|h) as rbms, so
can be stacked in the same way to form deep architectures

y

h(cid:48)

1

h(cid:48)

2

h1

h2

h3

layer3 encoder

layer2 encoder

layer1 encoder

x1

x2

x3

x4

unlike rbms, auto-encoders are deterministic.

(cid:73) h =   (wx + b), not p(h = {0, 1}) =   (wx + b)
(cid:73) disadvantage: can   t form deep generative model
(cid:73) advantage: fast to train, and useful still for deep neural nets

28/45

many variants of auto-encoders

enforce compression to get latent factors (lower dimensional h)

29/45

many variants of auto-encoders

enforce compression to get latent factors (lower dimensional h)

linear encoder/decoder with squared reconstruction error learns same
subspace of pca [bourlard and kamp, 1988]

29/45

many variants of auto-encoders

enforce compression to get latent factors (lower dimensional h)

linear encoder/decoder with squared reconstruction error learns same
subspace of pca [bourlard and kamp, 1988]

enforce sparsity and over-complete representations (high dimensional
h) [ranzato et al., 2006]

enforce binary hidden layers to build hash codes
[salakhutdinov and hinton, 2007]

incorporate domain knowledge, e.g. denoising auto-encoders
[vincent et al., 2010]

29/45

today   s topics

1 general ideas in deep learning

motivation for deep architectures and why is it hard?
main breakthrough in 2006: layer-wise pre-training

2 approach 1: deep belief nets [hinton et al., 2006]

restricted id82s (rbm)
training rbms with contrastive divergence
stacking rbms to form deep belief nets

3 approach 2: stacked auto-encoders [bengio et al., 2006]

auto-encoders
denoising auto-encoders

4 discussions

why it works, when it works, and the bigger picture

30/45

denoising auto-encoders

x(cid:48)

1

x(cid:48)

2

x(cid:48)

3

h1

h2

decoder: x(cid:48) =   (w (cid:48)h + d)

encoder: h =   (w   x + b)

  x1

  x2

  x3

  x = x+ noise

1 perturb input data x to   x using invariance from domain knowledge.

2 train weights to reduce reconstruction error with respect to original

input: ||x     x(cid:48)||

31/45

denoising auto-encoders

example: randomly shift, rotate, and scale input image; add
gaussian or salt-and-pepper noise.

a    2    is a    2    no matter how you add noise, so the auto-encoder will
be forced to cancel the variations that are not important.

32/45

summary: things to remember about sae

1 auto-encoders are cheaper alternatives to rbms.

(cid:73) not probabilistic, but fast to train using id26 or sgd

33/45

summary: things to remember about sae

1 auto-encoders are cheaper alternatives to rbms.

(cid:73) not probabilistic, but fast to train using id26 or sgd

2 auto-encoders learn to    compress    and    re-construct    input data.

again, the focus is on modeling p(x)    rst.

33/45

summary: things to remember about sae

1 auto-encoders are cheaper alternatives to rbms.

(cid:73) not probabilistic, but fast to train using id26 or sgd

2 auto-encoders learn to    compress    and    re-construct    input data.

again, the focus is on modeling p(x)    rst.

3 many variants, some provide ways to incorporate domain knowledge.

33/45

today   s topics

1 general ideas in deep learning

motivation for deep architectures and why is it hard?
main breakthrough in 2006: layer-wise pre-training

2 approach 1: deep belief nets [hinton et al., 2006]

restricted id82s (rbm)
training rbms with contrastive divergence
stacking rbms to form deep belief nets

3 approach 2: stacked auto-encoders [bengio et al., 2006]

auto-encoders
denoising auto-encoders

4 discussions

why it works, when it works, and the bigger picture

34/45

why does layer-wise pre-training work?

one hypothesis [bengio, 2009, erhan et al., 2010]:

a deep net can    t the training data in many ways (non-convex):

1 by optimizing upper-layers really hard
2 by optimizing lower-layers really hard

35/45

why does layer-wise pre-training work?

one hypothesis [bengio, 2009, erhan et al., 2010]:

a deep net can    t the training data in many ways (non-convex):

1 by optimizing upper-layers really hard
2 by optimizing lower-layers really hard

top-down vs. bottom-up information

1 even if lower-layers are random weights, upper-layer may still    t well.

but this might not generalize to new data

2 pre-training with objective on p(x) learns more generalizable features

35/45

why does layer-wise pre-training work?

one hypothesis [bengio, 2009, erhan et al., 2010]:

a deep net can    t the training data in many ways (non-convex):

1 by optimizing upper-layers really hard
2 by optimizing lower-layers really hard

top-down vs. bottom-up information

1 even if lower-layers are random weights, upper-layer may still    t well.

but this might not generalize to new data

2 pre-training with objective on p(x) learns more generalizable features

pre-training seems to help put weights at a better local optimum

35/45

is layer-wise pre-training always necessary?

36/45

is layer-wise pre-training always necessary?

answer in 2006: yes!

36/45

is layer-wise pre-training always necessary?

answer in 2006: yes!
answer in 2014: no!

1

if initialization is done well by design (e.g. sparse connections and
convolutional nets), maybe won   t have vanishing gradient problem

36/45

is layer-wise pre-training always necessary?

answer in 2006: yes!
answer in 2014: no!

1

2

if initialization is done well by design (e.g. sparse connections and
convolutional nets), maybe won   t have vanishing gradient problem

if you have an extremely large datasets, maybe won   t over   t. (but
maybe that also means you want an ever deeper net)

36/45

is layer-wise pre-training always necessary?

answer in 2006: yes!
answer in 2014: no!

1

2

if initialization is done well by design (e.g. sparse connections and
convolutional nets), maybe won   t have vanishing gradient problem

if you have an extremely large datasets, maybe won   t over   t. (but
maybe that also means you want an ever deeper net)

3 new architectures are emerging:

(cid:73) stacked id166   s with random projections [vinyals et al., 2012]
(cid:73) sum-product networks [poon and domingos, 2011]

36/45

connections with other machine learning concepts

a rbm is like a product-of-expert model and forms a distributed
representation of the data

(cid:73) compared with id91 (which compresses data but loses

information), distributed representations (multi-id91) are richer
representations

p(x) =(cid:80)

(cid:73) like a mixture model with 2n hidden components
h p(h)p(x|h), but much more compact

37/45

connections with other machine learning concepts

a rbm is like a product-of-expert model and forms a distributed
representation of the data

(cid:73) compared with id91 (which compresses data but loses

information), distributed representations (multi-id91) are richer
representations

p(x) =(cid:80)

(cid:73) like a mixture model with 2n hidden components
h p(h)p(x|h), but much more compact

neural net as kernel for id166 [li et al., 2005] and id166 training for
neural nets [collobert and bengio, 2004]

37/45

connections with other machine learning concepts

a rbm is like a product-of-expert model and forms a distributed
representation of the data

(cid:73) compared with id91 (which compresses data but loses

information), distributed representations (multi-id91) are richer
representations

p(x) =(cid:80)

(cid:73) like a mixture model with 2n hidden components
h p(h)p(x|h), but much more compact

neural net as kernel for id166 [li et al., 2005] and id166 training for
neural nets [collobert and bengio, 2004]

id90 are deep (but no distributed representation). random
forests are both deep and distributed. they do well in practice too!

37/45

connections with other machine learning concepts

a rbm is like a product-of-expert model and forms a distributed
representation of the data

(cid:73) compared with id91 (which compresses data but loses

information), distributed representations (multi-id91) are richer
representations

p(x) =(cid:80)

(cid:73) like a mixture model with 2n hidden components
h p(h)p(x|h), but much more compact

neural net as kernel for id166 [li et al., 2005] and id166 training for
neural nets [collobert and bengio, 2004]

id90 are deep (but no distributed representation). random
forests are both deep and distributed. they do well in practice too!
philosophical connections to:

(cid:73) semi-supervised learning: exploit both labeled and unlabeled data
(cid:73) curriculum learning: start on easy task, gradually level-up
(cid:73) id72: learn and share sub-tasks

37/45

history

early days of ai. invention of arti   cial neuron
[mcculloch and pitts, 1943] & id88 [rosenblatt, 1958]

38/45

history

early days of ai. invention of arti   cial neuron
[mcculloch and pitts, 1943] & id88 [rosenblatt, 1958]

ai winter. [minsky and papert, 1969] showed id88 only learns
linearly separable concepts

38/45

history

early days of ai. invention of arti   cial neuron
[mcculloch and pitts, 1943] & id88 [rosenblatt, 1958]

ai winter. [minsky and papert, 1969] showed id88 only learns
linearly separable concepts

revival in 1980s: multi-layer id88s (mlp) and
back-propagation [rumelhart et al., 1986]

38/45

history

early days of ai. invention of arti   cial neuron
[mcculloch and pitts, 1943] & id88 [rosenblatt, 1958]

ai winter. [minsky and papert, 1969] showed id88 only learns
linearly separable concepts

revival in 1980s: multi-layer id88s (mlp) and
back-propagation [rumelhart et al., 1986]

other directions (1990s - present): id166s, id110s

38/45

history

early days of ai. invention of arti   cial neuron
[mcculloch and pitts, 1943] & id88 [rosenblatt, 1958]

ai winter. [minsky and papert, 1969] showed id88 only learns
linearly separable concepts

revival in 1980s: multi-layer id88s (mlp) and
back-propagation [rumelhart et al., 1986]

other directions (1990s - present): id166s, id110s

revival in 2006: deep learning [hinton et al., 2006]

38/45

history

early days of ai. invention of arti   cial neuron
[mcculloch and pitts, 1943] & id88 [rosenblatt, 1958]

ai winter. [minsky and papert, 1969] showed id88 only learns
linearly separable concepts

revival in 1980s: multi-layer id88s (mlp) and
back-propagation [rumelhart et al., 1986]

other directions (1990s - present): id166s, id110s

revival in 2006: deep learning [hinton et al., 2006]

successes in applications: speech at ibm/toronto
[sainath et al., 2011], microsoft [dahl et al., 2012]. vision at
google/stanford [le et al., 2012]

38/45

references i

bengio, y. (2009).
learning deep architectures for ai, volume foundations and trends in
machine learning.
now publishers.

bengio, y., lamblin, p., popovici, d., and larochelle, h. (2006).
greedy layer-wise training of deep networks.
in nips   06, pages 153   160.

bishop, c. (2006).
pattern recognition and machine learning.
springer.

bourlard, h. and kamp, y. (1988).
auto-association by multilayer id88s and singular value
decomposition.
biological cybernetics, 59:291   294.

39/45

references ii

carreira-perpinan, m. a. and hinton, g. e. (2005).
on contrastive divergence learning.
in aistats.

collobert, r. and bengio, s. (2004).
links between id88s, mlps and id166s.
in icml.

dahl, g., yu, d., deng, l., and acero, a. (2012).
context-dependent pre-trained deep neural networks for large
vocabulary id103.
ieee transactions on audio, speech, and language processing,
special issue on deep learning for speech and langauge processing.

40/45

references iii

erhan, d., bengio, y., courville, a., manzagol, p., vincent, p., and
bengio, s. (2010).
why does unsupervised pre-training help deep learning?
journal of machine learning research, 11:625   660.

erhan, d., manzagol, p., bengio, y., bengio, s., and vincent, p.
(2009).
the di   culty of training deep architectures and the e   ect of
unsupervised pre-training.
in aistats.

hinton, g., osindero, s., and teh, y.-w. (2006).
a fast learning algorithm for deep belief nets.
neural computation, 18:1527   1554.

41/45

references iv

le, q. v., ranzato, m., monga, r., devin, m., chen, k., corrado,
g. s., dean, j., and ng, a. y. (2012).
building high-level features using large scale unsupervised learning.
in icml.

li, x., bilmes, j., and malkin, j. (2005).
maximum margin learning and adaptation of mlp classi   ers.
in interspeech.

mcculloch, w. s. and pitts, w. h. (1943).
a logical calculus of the ideas immanent in nervous activity.
in bulletin of mathematical biophysics, volume 5, pages 115   137.

minsky, m. and papert, s. (1969).
id88s: an introduction to computational geometry.
mit press.

42/45

references v

poon, h. and domingos, p. (2011).
sum-product networks.
in uai.

ranzato, m., boureau, y.-l., and lecun, y. (2006).
sparse id171 for id50.
in nips.

rosenblatt, f. (1958).
the id88: a probabilistic model for information storage and
organization in the brain.
psychological review, 65:386   408.

rumelhart, d. e., hinton, g. e., and williams, r. j. (1986).
learning representations by back-propagating errors.
nature, 323:533   536.

43/45

references vi

sainath, t. n., kingsbury, b., ramabhadran, b., fousek, p., novak,
p., and mohamed, a. (2011).
making id50 e   ective for large vocabulary continuous
id103.
in asru.

salakhutdinov, r. and hinton, g. (2007).
semantic hashing.
in sigir.

salakhutdinov, r. and hinton, g. (2009).
deep id82s.
in proceedings of the international conference on arti   cial
intelligence and statistics, volume 5, pages 448   455.

44/45

references vii

vincent, p., larochelle, h., lajoie, i., bengio, y., and manzagol, p.-a.
(2010).
stacked denoising autoencoders: learning useful representations in a
deep network with a local denoising criterion.
journal of machine learning research, 11:3371   3408.

vinyals, o., jia, y., deng, l., and darrell, t. (2012).
learning with recursive perceptual representations.
in nips.

45/45

