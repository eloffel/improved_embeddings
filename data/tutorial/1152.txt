deep learning & neural networks

lecture 3

kevin duh

graduate school of information science
nara institute of science and technology

jan 21, 2014

applications of deep learning

goal: to give a taste of how deep learning is used in practice, and
how varied it is, e.g.:

1 id103: hybrid dnn-id48 system
2 id161: local receptive    eld / pooling architecture
3 id38: recurrent structure

2/33

today   s topic

1 deep neural networks for acoustic modeling in id103

[hinton et al., 2012]

2 building high-level features using large scale unsupervised learning

[le et al., 2012]

3 recurrent neural network language models [mikolov et al., 2010]

3/33

background: simpli   ed view of id103

task: given input acoustic signal, predict word/phone sequence
arg maxphone sequence p(acoustics|phone)p(phone|previous phones)
(cid:73) p(acoustics|phone) modeled by gaussian mixture model (gmm)
(cid:73) p(phone|previous phones) by transitions in hidden markov model

(id48)

acoustic features:

4/33

dnn-id48 hybrid architecture

1 train deep belief nets on speech features: typically 3-8 layers, 2000

units/layer, 15 frames of input, 6000 output

2 fine-tune with frame-per-frame phone labels obtained from

traditional gaussian models

3 further discriminative training in conjunction with higher-level hidden

markov model

5/33

gaussian-bernoulli rbm for continuous data

h1

x1

h2

x2

h3

x3

hj are binary, xi are continuous variables
p(x, h) = 1
z  

exp (   e  (x, h)) = 1

p(hj = 1|x) =   ((cid:80)

p(xi|h)     gaussian with mean bi +

exp
   

wij xi   
vi

+ dj )

vi

z  

i

(cid:16)(cid:80)
(cid:80)

i

+(cid:80)

   (xi   bi )2

2vi

xi wij hj   
vi

ij

+ d t h

(cid:17)

j wij hj and variance vi

usually, x is normalized to zero mean, unit variance beforehand

6/33

gmm vs. dnn in modeling speech

speech is produced by modulating a small number of parameters in a
dynamical system (e.g vocal tract)

(cid:73) true structure should be in low-dimensional space

7/33

gmm vs. dnn in modeling speech

speech is produced by modulating a small number of parameters in a
dynamical system (e.g vocal tract)

gmm   s: p(x) =(cid:80)

(cid:73) true structure should be in low-dimensional space

j p(hj )p(x|hj ) with p(x|hj ) as gaussian
(cid:73) high model expressiveness: can model any non-linear data
(cid:73) but may require large full-covariance gaussians or many
diagonal-covariance gaussians     statistically ine   cient

7/33

gmm vs. dnn in modeling speech

speech is produced by modulating a small number of parameters in a
dynamical system (e.g vocal tract)

gmm   s: p(x) =(cid:80)

(cid:73) true structure should be in low-dimensional space

j p(hj )p(x|hj ) with p(x|hj ) as gaussian
(cid:73) high model expressiveness: can model any non-linear data
(cid:73) but may require large full-covariance gaussians or many
diagonal-covariance gaussians     statistically ine   cient

rbm & dnn   s distributed factor representation is more e   cient
(cid:73) also: no need to worry about feature correlation     exploit larger

temporal window as input

7/33

results

dnn-id48 outperforms gmm-id48 on various datasets
already commercialized!

word error rate results:

why it works: larger context and less hand-engineered preprocessing

8/33

more details on switchboard result [seide et al., 2011]

basic setup:

input: 39-dim derived from plp, hlda transform
output: 9304 cross-word triphone states (tied)

baseline gmm-id48:

gmm with 40 gaussians.
training: (1) max-likelihood (em), (2) discriminative bmmi

dnn-id48:

7 stacked rbm   s with 2048 units per layer
pre-training on 2 passes over training data (300 hours of speech)
mini-batch size:100-300 (pre-training), 1000 (id26)

9/33

today   s topic

1 deep neural networks for acoustic modeling in id103

[hinton et al., 2012]

2 building high-level features using large scale unsupervised learning

[le et al., 2012]

3 recurrent neural network language models [mikolov et al., 2010]

10/33

motivating question: is it possible to learn high-level
features (e.g. face detectors) using only unlabeled images?

11/33

motivating question: is it possible to learn high-level
features (e.g. face detectors) using only unlabeled images?

answer: yes.

11/33

motivating question: is it possible to learn high-level
features (e.g. face detectors) using only unlabeled images?

answer: yes.

(cid:73) using a deep network of 1 billion parameters
(cid:73) 10 million images (sampled from youtube)
(cid:73) 1000 machines (16,000 cores) x 1 week.

11/33

   grandmother cell    hypothesis

grandmother cell: a neuron that lights up when you see or hear your
grandmother

(cid:73) lots of interesting (controversial) discussions in the neuroscience

literature

for our purposes: is it possible to learn such high-level concepts from
raw pixels?

12/33

previous work: convolutional nets [lecun et al., 1998]

p1

p2

h1

x2

h2

x3

h3

x4

x1

receptive field (rf): each hj only
connects to small input region.
tied weights     convolution
pooling: e.g. p1 = max(h1, h2) or
p1 =

(cid:113)

h2
1 + h2
2

advantages:

x5

1 fewer weights

2 shift invariance

13/33

previous work: convolutional nets [lecun et al., 1998]

p1

p2

h1

x2

h2

x3

h3

x4

x1

receptive field (rf): each hj only
connects to small input region.
tied weights     convolution
pooling: e.g. p1 = max(h1, h2) or
p1 =

(cid:113)

h2
1 + h2
2

advantages:

x5

1 fewer weights

2 shift invariance

(figure from http://deeplearning.net/tutorial/lenet.html)

13/33

architecture

min
wd ,we

(cid:88)
(cid:88)

m

+

(cid:113)

||wd wex (m)     x (m)||

  + pk (wex (m))2

m,k

(1): auto-encoder
(2): pooling

repeated 3 times to form deep architecture
x (m) = image of 200x200 pixels x3 channels

(1)

(2)

14/33

id171 by topographic ica
[hyv  arinen et al., 2001]

learns shift/scale/rotation-invariant features

reconstruction version
[le et al., 2011] can be trained faster

min
wd ,we

||wd wex (m)     x (m)||

(cid:88)
(cid:88)

m

+

(cid:113)

  + pk (wex (m))2

m,k

15/33

training setup

3-layer network, 1 billion parameters (trained jointly)

10 million 200x200 pixel images from 10 million youtube videos

1000 machines (16,000 cores) x 1 week

lots of tricks for data/model parallelization (next lecture)

16/33

face neuron

*graphics from [le et al., 2012]

17/33

face neuron

*graphics from [le et al., 2012]

18/33

cat neuron

*graphics from [le et al., 2012]

19/33

more examples

*graphics from [le et al., 2012]

20/33

more examples

*graphics from [le et al., 2012]

21/33

more examples

*graphics from [le et al., 2012]

22/33

id163 classi   cation results

add id28 on top of    nal layer

supervised learning on id163 dataset

test accuracy (22k categories):

method
random
previous state-of-the-art
[le et al., 2012] without pre-training on youtube data
[le et al., 2012] with pre-training on youtube data

accuracy
0.005%
9.3%
13.6%
15.8%

23/33

today   s topic

1 deep neural networks for acoustic modeling in id103

[hinton et al., 2012]

2 building high-level features using large scale unsupervised learning

[le et al., 2012]

3 recurrent neural network language models [mikolov et al., 2010]

24/33

goal of id38

give probabilities to word sequences (e.g. sentences)

(cid:73) likely sentences in the world (e.g.    let   s recognize speech   )     high
(cid:73) unlikely sentences in the world (e.g.    let   s wreck a nice beach   )     low

id203

id203

useful for various applications involving natural language

25/33

goal of id38

give probabilities to word sequences (e.g. sentences)

(cid:73) likely sentences in the world (e.g.    let   s recognize speech   )     high
(cid:73) unlikely sentences in the world (e.g.    let   s wreck a nice beach   )     low

id203

id203

useful for various applications involving natural language
id165 model decomposes sentence id203, e.g.
p(w (1), w (2), w (3), w (4)) =

(cid:73) p(w (4)|w (3))p(w (3)|w (2))p(w (2)|w (1))p(w (1)) (2-gram)
(cid:73) p(w (4)|w (3), w (2))p(w (3)|w (2), w (1))p(w (2)|w (1))p(w (1)) (3-gram)

25/33

goal of id38

give probabilities to word sequences (e.g. sentences)

(cid:73) likely sentences in the world (e.g.    let   s recognize speech   )     high
(cid:73) unlikely sentences in the world (e.g.    let   s wreck a nice beach   )     low

id203

id203

useful for various applications involving natural language
id165 model decomposes sentence id203, e.g.
p(w (1), w (2), w (3), w (4)) =

(cid:73) p(w (4)|w (3))p(w (3)|w (2))p(w (2)|w (1))p(w (1)) (2-gram)
(cid:73) p(w (4)|w (3), w (2))p(w (3)|w (2), w (1))p(w (2)|w (1))p(w (1)) (3-gram)

estimate from text data:
p(w (2)|w (1)) = count(w (1), w (2))/count(w (1)), plus smoothing to
account for unknown words and word sequences

25/33

recurrent neural net architecture for id38

model p(current word|previous words) with a recurrent hidden layer

current word (assume 3-word vocabulary)

y1

y2

wjk

wij

x1

x2

y3

h1

x3

h2

x4

x5

previous word

previous h

id203 of word k:

(cid:80)

exp(w t
jk h)
k(cid:48) exp(w t

yk =

jk(cid:48) h)
[x1, x2, x3] is binary
vector with 1 at current
vocabulary & 0 otherwise
[x4, x5] is a copy of
[h1, h2] from the
previous time-step
hj =   (w t
   state    of the system

ij xi ) is hidden

26/33

training: id26 through time

unroll the hidden states for certain time-steps.
given error at y , update weights by id26
example: he loves | her
y1
y3

y2

wjk

wij

h1

x3
x1
   loves    [x1, x2, x3] = [1, 0, 0]

x2

h2

h(cid:48)

1

h(cid:48)

2

previous h

wij

x3
x1
   he    [x1, x2, x3] = [0, 1, 0]

x2

h(cid:48)(cid:48)

1

h(cid:48)(cid:48)

2

initial h

27/33

advantages of recurrent nets

hidden nodes h form a distributed representation of partial sentence

(cid:73) h is a succinct conditioning factor for predicting current word
(cid:73) arbitrarily-long history is (theoretically) kept through recurrence

28/33

advantages of recurrent nets

hidden nodes h form a distributed representation of partial sentence

(cid:73) h is a succinct conditioning factor for predicting current word
(cid:73) arbitrarily-long history is (theoretically) kept through recurrence

in practice:

(cid:73) backpropatation through time forms a deep network; may be hard to

(cid:80)

(cid:73) yk =

train. fixed to < 10 previous time-steps/words

exp(w t
jk h)
jk(cid:48) h) requires summation k over vocabulary size, which is
k(cid:48) exp(w t

large. there are shortcuts to reduce computation.

28/33

advantages of recurrent nets

hidden nodes h form a distributed representation of partial sentence

(cid:73) h is a succinct conditioning factor for predicting current word
(cid:73) arbitrarily-long history is (theoretically) kept through recurrence

in practice:

(cid:73) backpropatation through time forms a deep network; may be hard to

(cid:80)

(cid:73) yk =

train. fixed to < 10 previous time-steps/words

exp(w t
jk h)
jk(cid:48) h) requires summation k over vocabulary size, which is
k(cid:48) exp(w t

large. there are shortcuts to reduce computation.

by-product: [wij ]i can be used as    id27s   . useful for
various natural language processing tasks
[zhila et al., 2013, turian et al., 2010]

28/33

results [mikolov et al., 2010]

trained on 6 million words (300k sentences) of new york times data.

evaluation on held-out data:

perplexity = 2id178 = 2

(cid:80)

    1|data|

data log pmodel (data)

model

id165 (n=5)

recurrent net |h| = 60
recurrent net |h| = 90
recurrent net |h| = 250
recurrent net |h| = 400

combining 3 recurrent nets

combining 3 recurrent nets, dynamic update on held-out

perplexity

221
229
202
173
171
151
128

29/33

references i

hinton, g., deng, l., yu, d., dahl, g., a.mohamed, jaitly, n., senior,
a., vanhoucke, v., nguyen, p., sainath, t., and kingsbury, b. (2012).

deep neural networks for acoustic modeling in id103.
ieee signal processing magazine, 29.

hyv  arinen, a., hoyer, p., and inki, m. (2001).
topographic independent component analysis.
neural computation, 13(7):1527   1558.

le, q., karpenko, a., ngiam, j., and ng, a. (2011).
ica with reconstruction cost for e   cient overcomplete feature
learning.
in nips.

30/33

references ii

le, q. v., ranzato, m., monga, r., devin, m., chen, k., corrado,
g. s., dean, j., and ng, a. y. (2012).
building high-level features using large scale unsupervised learning.
in icml.

lecun, y., bottou, l., bengio, y., and ha   ner, p. (1998).
gradient-based learning applied to document recognition.
proc, 86(11):2278   2324.
mikolov, t., kara   at, s., burget, l.,   cernock  y, j., and khudanpur, s.
(2010).
recurrent neural network based language models.
in proceedings of the 11th annual conference of the international
speech communication association (interspeech 2010).

31/33

references iii

seide, f., li, g., and yu, d. (2011).
conversational speech transcription using context-dependent deep
neural networks.
in proc. interspeech 2011, pages 437   440.

turian, j., ratinov, l.-a., and bengio, y. (2010).
word representations: a simple and general method for
semi-supervised learning.
in proceedings of the 48th annual meeting of the association for
computational linguistics, pages 384   394, uppsala, sweden.
association for computational linguistics.

32/33

references iv

zhila, a., yih, w.-t., meek, c., zweig, g., and mikolov, t. (2013).
combining heterogeneous models for measuring relational similarity.
in proceedings of the 2013 conference of the north american chapter
of the association for computational linguistics: human language
technologies, pages 1000   1009, atlanta, georgia. association for
computational linguistics.

33/33

