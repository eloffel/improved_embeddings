deep learning & neural networks

lecture 4

kevin duh

graduate school of information science
nara institute of science and technology

jan 23, 2014

2/20

advanced topics in optimization

today we   ll brie   y survey an assortment of exciting new tricks for
optimizing deep architectures
although there are many exciting new things out of nips/icml every
year, i   ll pick the four that i think is most helpful for practitioners:

(cid:73) sgd alternative
(cid:73) better id173
(cid:73) scaling to large data
(cid:73) hyper-parameter search

3/20

today   s topics

1 hessian-free optimization [martens, 2010]

2 dropout id173 [hinton et al., 2012]

3 large-scale distributed training [dean et al., 2012]

4 hyper-parameter search [bergstra et al., 2011]

4/20

di   culty of optimizing highly non-convex id168s

   pathological curvature    is tough to navigate for sgd
2nd-order (newton) methods may be needed to avoid under   tting

figure from [martens, 2010]

5/20

2nd-order (newton) methods

idea: approximate the id168 locally with a quadratic
l(w + z)     qw (z)     l(w ) +    l(w )t z + 1

2 z t hz

where h is the hessian (curvature matrix) at w

6/20

2nd-order (newton) methods

idea: approximate the id168 locally with a quadratic
l(w + z)     qw (z)     l(w ) +    l(w )t z + 1

2 z t hz

where h is the hessian (curvature matrix) at w
minimizing this gives the search direction: z    =    h   1   l(w )

(cid:73) intuitively, h   1    xes any pathological curvature for    l(w )
(cid:73) in practice, don   t want to store nor invert h

6/20

2nd-order (newton) methods

idea: approximate the id168 locally with a quadratic
l(w + z)     qw (z)     l(w ) +    l(w )t z + 1

2 z t hz

where h is the hessian (curvature matrix) at w
minimizing this gives the search direction: z    =    h   1   l(w )

(cid:73) intuitively, h   1    xes any pathological curvature for    l(w )
(cid:73) in practice, don   t want to store nor invert h

quasi-id77s

(cid:73) l-bfgs: uses low-rank approximation of h   1

6/20

2nd-order (newton) methods

idea: approximate the id168 locally with a quadratic
l(w + z)     qw (z)     l(w ) +    l(w )t z + 1

2 z t hz

where h is the hessian (curvature matrix) at w
minimizing this gives the search direction: z    =    h   1   l(w )

(cid:73) intuitively, h   1    xes any pathological curvature for    l(w )
(cid:73) in practice, don   t want to store nor invert h

quasi-id77s

(cid:73) l-bfgs: uses low-rank approximation of h   1
(cid:73) hessian-free (i.e. truncated newton): (1) minimize qw (z) with

conjugate gradient method; (2) computes hz directly using
   nite-di   erence: hz = lim    0

   l(w + z)      l(w )

 

6/20

hessian-free optimization applied to deep learning

[martens, 2010] describes some important modi   cations/settings to
make hessian-free methods work for deep learning

experiments:

(random initialization + 2nd-order hessian-free optimizer)

gives lower training error than

(pre-training initialization + 1-order optimizer).

nice results in recurrent nets too [martens and sutskever, 2011]

7/20

today   s topics

1 hessian-free optimization [martens, 2010]

2 dropout id173 [hinton et al., 2012]

3 large-scale distributed training [dean et al., 2012]

4 hyper-parameter search [bergstra et al., 2011]

8/20

dropout [hinton et al., 2012]

each time we present x (m),
randomly delete each hidden
node with 0.5 id203
this is like sampling from 2|h|
di   erent architectures

at test time, use all nodes but
halve the weights

e   ect: reduce over   tting by
preventing    co-adaptation   ;
ensemble model averaging

y

h(cid:48)

2

h2

x2

h(cid:48)

1

h1

x1

h(cid:48)

3

h3

x3

9/20

some results: timit phone recognition

10/20

today   s topics

1 hessian-free optimization [martens, 2010]

2 dropout id173 [hinton et al., 2012]

3 large-scale distributed training [dean et al., 2012]

4 hyper-parameter search [bergstra et al., 2011]

11/20

model parallelism

deep net is stored and processed on multiple cores (multi-thread) or
machines (message passing)
performance bene   t depends on connectivity structure vs.
computational demand

figure from [dean et al., 2012]

12/20

data parallelism

left: asynchronous sgd

(cid:73) training data is partitioned; di   erent model per shard
(cid:73) each model pushes and pulls gradient/weight info from parameter
server asynchronously        robust to machine failures

(cid:73) note gradient updates may be out-of-order and weights may be

out-of-date. but it works! (c.f. [langford et al., 2009])

(cid:73) adagrad learning rate is bene   cial in practice

right: distributed l-bfgs
   more precisely, each machine within the model communicates with the

relevant parameter server. (figure from [dean et al., 2012])

13/20

performance analysis

left: exploiting model parallelism on a single data shard, up to 12x
measured for sparse nets (images) but diminishing returns. for dense
nets (speech), max at 8 machines.
right: exploiting data parallelism also, how much time and how
many machines are needed to achieve 16% test accuracy (speech)?

figure from [dean et al., 2012]

14/20

today   s topics

1 hessian-free optimization [martens, 2010]

2 dropout id173 [hinton et al., 2012]

3 large-scale distributed training [dean et al., 2012]

4 hyper-parameter search [bergstra et al., 2011]

15/20

hyper-parameter search is important

lots of hyper-parameters!

1 number of layers
2 number of nodes per layer
3 sgd learning rate
4 id173 constant
5 mini-batch size
6 type of non-linearity
7 type of distribution for random initialization

16/20

hyper-parameter search is important

lots of hyper-parameters!

1 number of layers
2 number of nodes per layer
3 sgd learning rate
4 id173 constant
5 mini-batch size
6 type of non-linearity
7 type of distribution for random initialization

it   s important to invest in    nding good settings for your data

(cid:73) could be the di   erence between a winning system vs. useless system

16/20

approaches to hyper-parameter search

1 grid search

17/20

approaches to hyper-parameter search

1 grid search

2 random search

17/20

approaches to hyper-parameter search

1 grid search

2 random search

3 manual search, a.k.a graduate student descent (gsd)

17/20

approaches to hyper-parameter search

1 grid search

2 random search

3 manual search, a.k.a graduate student descent (gsd)
4 treat search itself as a meta machine learning problem

[bergstra et al., 2011]

(cid:73) input x = space of hyper-parameters
(cid:73) output y = validation error after training with given hyper-parameters

17/20

hyper-parameter search as machine learning problem

input x = space of hyper-parameters

output y = validation error after training with given
hyper-parameters

1 computing y is expensive, so we learn a function f(x) that can predict

it based on past (x, y ) pairs

(cid:73) e.g. id75
(cid:73) e.g. gaussian process, parzen estimator [bergstra et al., 2011]

18/20

hyper-parameter search as machine learning problem

input x = space of hyper-parameters

output y = validation error after training with given
hyper-parameters

1 computing y is expensive, so we learn a function f(x) that can predict

it based on past (x, y ) pairs

(cid:73) e.g. id75
(cid:73) e.g. gaussian process, parzen estimator [bergstra et al., 2011]

2 try the hyper-parameter setting x    = arg minx f (x), or some variant

18/20

hyper-parameter search as machine learning problem

input x = space of hyper-parameters

output y = validation error after training with given
hyper-parameters

1 computing y is expensive, so we learn a function f(x) that can predict

it based on past (x, y ) pairs

(cid:73) e.g. id75
(cid:73) e.g. gaussian process, parzen estimator [bergstra et al., 2011]

2 try the hyper-parameter setting x    = arg minx f (x), or some variant
3 repeat steps 1-2 until you solve ai!

18/20

references i

bergstra, j., bardenet, r., bengio, y., and k  egel, b. (2011).
algorithms for hyper-parameter optimization.
in proc. neural information processing systems 24 (nips2011).

dean, j., corrado, g. s., monga, r., chen, k., devin, m., le, q. v.,
mao, m. z., ranzato, m., senior, a., tucker, p., yang, k., and ng,
a. y. (2012).
large scale distributed deep networks.
in neural information processing systems (nips).

hinton, g. e., srivastava, n., krizhevsky, a., sutskever, i., and
salakhutdinov, r. (2012).
improving neural networks by preventing co-adaptation of feature
detectors.
corr, abs/1207.0580.

19/20

references ii

langford, j., smola, a., and zinkevich, m. (2009).
slow learners are fast.
in nips.

martens, j. (2010).
deep learning via hessian-free optimization.
in proceedings of the 27th international conference on machine
learning (icml).

martens, j. and sutskever, i. (2011).
learning recurrent neural networks with hessian-free optimization.
in proceedings of the 28th international conference on machine
learning (icml).

20/20

