nlp lunch tutorial: smoothing

bill maccartney

21 april 2005

preface

    everything is from this great paper by stanley f. chen and joshua
goodman (1998),    an empirical study of smoothing techniques
for id38   , which i read yesterday.

    everything is presented in the context of id165 language models,
but smoothing is needed in many problem contexts, and most of
the smoothing methods we   ll look at generalize without di   culty.

1

the plan

    motivation

    the problem
    an example

    all the smoothing methods

    formula after formula
    intuitions for each

    so which one is the best?

    (answer: modi   ed kneser-ney)

    excel    demo    for absolute discounting and good-turing?

2

probabilistic modeling

    you have some kind of probabilistic model, which is a distribution

p(e) over an event space e.

    you want to estimate the parameters of your model distribution p

from data.

    in principle, you might to like to use maximum likelihood (ml)

estimates, so that your model is

pm l(x) =

c(x)p

e c(e)

but...

3

problem: data sparsity

    but, you have insu   cient data: there are many events x such that

c(x) = 0, so that the ml estimate is pm l(x) = 0.

    in problem settings where the event space e is unbounded (e.g.

most nlp problems), this is generally undesirable.

    ex: a language model which gives id203 0 to unseen words.
    just because an event has never been observed in training data does

not mean it cannot occur in test data.

    so if c(x) = 0, what should p(x) be?
    if data sparsity isn   t a problem for you, your model is too simple!

4

   whenever data sparsity is an issue, smoothing can help performance,
and data sparsity is almost always an issue in statistical modeling. in the
extreme case where there is so much training data that all parameters
can be accurately trained without smoothing, one can almost always
expand the model, such as by moving to a higher id165 model, to
achieve improved performance. with more parameters data sparsity
becomes an issue again, but with proper smoothing the models are
usually more accurate than the original models. thus, no matter how
much data one has, smoothing can almost always help performace, and
for a relatively small e   ort.   

chen & goodman (1998)

5

example: bigram model

john read moby dick

mary read a different book

she read a book by cher

p(wi|wi   1) =

p(s) =

p
l+1y

c(wi   1wi)
c(wi   1wi)
wi
p(wi|wi   1)

i=1

6

john read moby dick

mary read a different book

she read a book by cher

p(john read a book)
= p(john|   ) p(read|john)
p
= c(    john)
w c(    w)

c(john read)
w c(john w)

p

p(a|read)
p

c(read a)
w c(read w)

p(book|a)
p

c(a book)
w c(a w)

p(   |book)
p
c(book    )
w c(book w)

1
3

=
    0.06

1
1

2
3

1
2

1
2

7

john read moby dick

mary read a different book

she read a book by cher

p(cher read a book)
= p(cher|   ) p(read|cher)
p
= c(    cher)
w c(    w)

c(cher read)
w c(cher w)

p

p(a|read)
p

c(read a)
w c(read w)

p(book|a)
p

c(a book)
w c(a w)

p(   |book)
p
c(book    )
w c(book w)

0
3

=

= 0

0
1

2
3

1
2

1
2

8

add-one smoothing

p(wi|wi   1) =

p

1 + c(wi   1wi)
[1 + c(wi   1wi)]
wi

=

    originally due to laplace.

|v | +p

1 + c(wi   1wi)

c(wi   1wi)

wi

    typically, we assume v = {w : c(w) > 0}     {unk}

    add-one smoothing is generally a horrible choice.

9

john read moby dick

mary read a different book

she read a book by cher

p(john read a book)

1+1
11+1

= 1+1
11+3
    0.0001

1+2
11+3

1+1
11+2

1+1
11+2

p(cher read a book)

1+0
11+1

= 1+0
11+3
    0.00003

1+2
11+3

1+1
11+2

1+1
11+2

10

smoothing methods

    additive smoothing
    good-turing estimate
    jelinek-mercer smoothing (interpolation)
    katz smoothing (backo   )
    witten-bell smoothing
    absolute discounting
    kneser-ney smoothing

11

additive smoothing

padd(wi|wi   1

i   n+1) =

   + c(wi

  |v | +p

wi

i   n+1)
c(wi

i   n+1)

    idea: pretend we   ve seen each id165    times more than we have.
    typically, 0 <        1.
    lidstone and je   reys advocate    = 1.
    gale & church (1994) argue that this method performs poorly.

12

good-turing estimation

    idea: reallocate the id203 mass of id165s that occur r + 1

times in the training data to the id165s that occur r times.

    in particular, reallocate the id203 mass of id165s that were

seen once to the id165s that were never seen.

    for each count r, we compute an adjusted count r   :

r    = (r + 1)

nr+1

nr

where nr is the number of id165s seen exactly r times.

    then we have:

where n =p   

r=0 r   nr =p   

r=1 rnr.

pgt (x : c(x) = r) =

r   
n

13

good-turing problems

    problem: what if nr+1 = 0? this is common for high r: there are

   holes    in the counts of counts.

    problem: even if we   re not just below a hole, for high r, the nr are

quite noisy.

    really, we should think of r    as:

r    = (r + 1)

e[nr+1]

e[nr]

    but how do we estimate that expectation? (the original formula

amounts to using the ml estimate.)

    good-turing thus requires elaboration to be useful. it forms a foun-

dation on which other smoothing methods build.

14

jelinek-mercer smoothing (interpolation)

    observation:

if c(burnish the) = 0 and c(burnish thou) = 0,

then under both additive smoothing and good-turing:

p(the|burnish) = p(thou|burnish)

    this seems wrong: we should have

p(the|burnish) > p(thou|burnish)

because the is much more common than thou

    solution: interpolate between bigram and unigram models.

15

jelinek-mercer smoothing (interpolation)

    unigram ml model:

    bigram interpolated model:

pm l(wi) =

p

c(wi)
wi

c(wi)

pinterp(wi|wi   1) =   pm l(wi|wi   1) + (1       )pm l(wi)

16

jelinek-mercer smoothing (interpolation)

    recursive formulation: nth-order smoothed model is de   ned recur-
sively as a linear interpolation between the nth-order ml model and
the (n     1)th-order smoothed model.

pinterp(wi|wi   1

wi   1
  
i   n+1

i   n+1) =
pm l(wi|wi   1

i   n+1) + (1       

)pinterp(wi|wi   1

i   n+2)

wi   1
i   n+1

    can ground recursion with:

    1st-order model: ml (or otherwise smoothed) unigram model
    0th-order model: uniform model

punif (wi) =

1
|v |

17

jelinek-mercer smoothing (interpolation)

    the   

wi   1
i   n+1

can be estimated using em on held-out data (held-out

interpolation) or in cross-validation fashion (deleted interpolation).

    the optimal   

wi   1
i   n+1
should get high   s.

depend on context: high-frequency contexts

    but, can   t tune all   s separately: need to bucket them.

    bucket by p

wi

c(wi

i   n+1): total count in higher-order model.

18

katz smoothing: bigrams

    as in good-turing, we compute adjusted counts.
    bigrams with nonzero count r are discounted according to discount
r , the discount predicted by good-

ratio dr, which is approximately r   
turing. (details below.)

    count mass subtracted from nonzero counts is redistributed among
the zero-count bigrams according to next lower-order distribution
(i.e. the unigram model).

19

katz smoothing: bigrams

ckatz(wi

    katz adjusted counts:

(
      (wi   1) is chosen so that p
1    p
1    p

  (wi   1) =

i   1) =

wi

if r > 0
drr
  (wi   1)pm l(wi) if r = 0

i   1) =p
c(wi
i   1):
i   1)>0 pkatz(wi|wi   1)

wi

ckatz(wi

wi:c(wi

wi:c(wi

i   1)>0 pm l(wi)

    compute pkatz(wi|wi   1) from corrected count by normalizing:

pkatz(wi|wi   1) =

p

ckatz(wi

i   1)
ckatz(wi

wi

i   1)

20

katz smoothing

    what about dr? large counts are taken to be reliable, so dr = 1 for

r > k, where katz suggests k = 5. for r     k...

    we want discounts to be proportional to good-turing discounts:

1     dr =   (1     r   

)

r

    we want the total count mass saved to equal the count mass which

good-turing assigns to zero counts:

kx

nr(1     dr)r = n1

    the unique solution is:

r=1

dr =

r   
r     (k+1)nk+1
1     (k+1)nk+1

n1

n1

21

katz smoothing

    katz smoothing for higher-order id165s is de   ned analogously.
    like jelinek-mercer, can be given a recursive formulation: the katz
id165 model is de   ned in terms of the katz (n     1)-gram model.

22

witten-bell smoothing

    an instance of jelinek-mercer smoothing:

pw b(wi|wi   1

wi   1
  
i   n+1

i   n+1) =
pm l(wi|wi   1
i   n+1) + (1       
wi   1
i   n+1

    motivation: interpret   

order model.

    we should use higher-order model

)pw b(wi|wi   1

i   n+2)

wi   1
i   n+1

as the id203 of using the higher-

if id165 wi

i   n+1 was seen in

training data, and back o    to lower-order model otherwise.

    so 1       

wi   1
i   n+1

should be the id203 that a word not seen after

wi   1
i   n+1 in training data occurs after that history in test data.

    estimate this by the number of unique words that follow the history

wi   1
i   n+1 in the training data.

23

witten-bell smoothing

    to compute the   s, we   ll need the number of unique words that

follow the history wi   1
n1+(wi   1

    set   s such that

1       

wi   1
i   n+1

i   n+1wi) > 0}|

i   n+1:
i   n+1    ) = |{wi : c(wi   1
n1+(wi   1
i   n+1    ) +p
i   n+1    )

n1+(wi   1

=

wi

c(wi

i   n+1)

24

absolute discounting

    like jelinek-mercer, involves interpolation of higher- and lower-order

models.

    but instead of multiplying the higher-order pm l by a   , we subtract

a    xed discount        [0, 1] from each nonzero count:

i   n+1) =

pabs(wi|wi   1
p
max{c(wi

i   n+1)       , 0}
c(wi
    to make it sum to 1:

i   n+1)

wi

+ (1       

wi   1
i   n+1

)pabs(wi|wi   1

i   n+2)

1       

wi   1
i   n+1

=

  
c(wi
i   n+1)

n1+(wi   1

i   n+1    )

wi

p

    choose    using held-out estimation.

25

kneser-ney smoothing

    an extension of absolute discounting with a clever way of construct-

ing the lower-order (backo   ) model.

    idea: the lower-order model is sign   cant only when count is small
or zero in the higher-order model, and so should be optimized for
that purpose.

    example: suppose    san francisco    is common, but    francisco   

occurs only after    san   .

       francisco    will get a high unigram id203, and so absolute
discounting will give a high id203 to    francisco    appearing
after novel bigram histories.

    better to give    francisco    a low unigram id203, because the
only time it occurs is after    san   , in which case the bigram model
   ts well.

26

kneser-ney smoothing

    let the count assigned to each unigram be the number of di   erent

words that it follows. de   ne:

n1+(    wi) = |{wi   1 : c(wi   1wi) > 0}|

n1+(       ) =x

wi

n1+(    wi)

    let lower-order distribution be:

    put it all together:

pkn (wi) =

n1+(    wi)
n1+(       )

pkn (wi|wi   1
max{c(wi

i   n+1) =

p

wi

i   n+1)       , 0}
c(wi

i   n+1)

p

wi

+

  
c(wi
i   n+1)

n1+(wi   1

i   n+1    )pkn (wi|wi   1

i   n+2)

27

interpolation vs. backo   

    both interpolation (jelinek-mercer) and backo    (katz) involve com-

bining information from higher- and lower-order models.

    key di   erence: in determining the id203 of id165s with nonzero
counts, interpolated models use information from lower-order mod-
els while backo    models do not.

    (in both backo    and interpolated models, lower-order models are
used in determining the id203 of id165s with zero counts.)
    it turns out that it   s not hard to create a backo    version of an
interpolated algorithm, and vice-versa. (kneser-ney was originally
backo   ; chen & goodman made interpolated version.)

28

modi   ed kneser-ney

    chen and goodman introduced modi   ed kneser-ney :

    interpolation is used instead of backo   .
    uses a separate discount for one- and two-counts instead of a
    estimates discounts on held-out data instead of using a formula

single discount for all counts.
based on training counts.

    experiments show all three modi   cations improve performance.
    modi   ed kneser-ney consistently had best performance.

29

conclusions

    the factor with the largest in   uence is the use of a modi   ed backo   

distribution as in kneser-ney smoothing.

    jelinek-mercer performs better on small training sets; katz performs

better on large training sets.

    katz smoothing performs well on id165s with large counts; kneser-

ney is best for small counts.

    absolute discounting is superior to linear discounting.
    interpolated models are superior to backo    models for low (nonzero)

counts.

    adding free parameters to an algorithm and optimizing these pa-

rameters on held-out data can improve performance.

adapted from chen & goodman (1998)

30

end

31

   

32

