id33

tutorial at coling-acl, sydney 2006

joakim nivre1 sandra k  ubler2

1uppsala university and v  axj  o university, sweden

e-mail: nivre@msi.vxu.se

2eberhard-karls universit  at t  ubingen, germany

e-mail: kuebler@sfs.uni-tuebingen.de

id33

1(103)

why?

introduction

    increasing interest in dependency-based approaches to

syntactic parsing in recent years

    new methods emerging
    applied to a wide range of languages
    conll-x shared task (june, 2006)

    dependency-based methods still less accessible for the

majority of researchers and developers than the more widely
known constituency-based methods

id33

2(103)

for whom?

introduction

    researchers and students working on syntactic parsing or

related topics within other traditions

    researchers and application developers interested in using

dependency parsers as components in larger systems

id33

3(103)

what?

introduction

    computational methods for dependency-based parsing

    syntactic representations
    parsing algorithms
    machine learning

    available resources for di   erent languages

    parsers
    treebanks

id33

4(103)

introduction

outline

introduction

motivation and contents
basic concepts of dependency syntax

parsing methods

id145
id124
deterministic parsing
non-projective id33

pros and cons of id33
practical issues

parsers
treebanks
evaluation

outlook

id33

5(103)

outline

introduction

motivation and contents
basic concepts of dependency syntax

parsing methods

id145
id124
deterministic parsing
non-projective id33

pros and cons of id33
practical issues

parsers
treebanks
evaluation

outlook

introduction

break

joakim

sandra

joakim

sandra

id33

5(103)

introduction

dependency syntax

    the basic idea:

    syntactic structure consists of lexical items, linked by binary

asymmetric relations called dependencies.

    in the words of lucien tesni`ere [tesni`ere 1959]:

    la phrase est un ensemble organis  e dont les   el  ements constituants
sont les mots. [1.2] tout mot qui fait partie d   une phrase cesse par
lui-m  eme d     etre isol  e comme dans le dictionnaire. entre lui et ses
voisins, l   esprit aper  coit des connexions, dont l   ensemble forme la
charpente de la phrase. [1.3] les connexions structurales   etablissent
entre les mots des rapports de d  ependance. chaque connexion unit
en principe un terme sup  erieur `a un terme inf  erieur. [2.1] le terme
sup  erieur re  coit le nom de r  egissant. le terme inf  erieur re  coit le
nom de subordonn  e. ainsi dans la phrase alfred parle [. . . ], parle
est le r  egissant et alfred le subordonn  e. [2.2]

id33

6(103)

introduction

dependency syntax

    the basic idea:

    syntactic structure consists of lexical items, linked by binary

asymmetric relations called dependencies.

    in the words of lucien tesni`ere [tesni`ere 1959]:

    the sentence is an organized whole, the constituent elements of

which are words. [1.2] every word that belongs to a sentence ceases
by itself to be isolated as in the dictionary. between the word and
its neighbors, the mind perceives connections, the totality of which
forms the structure of the sentence. [1.3] the structural
connections establish dependency relations between the words. each
connection in principle unites a superior term and an inferior term.
[2.1] the superior term receives the name governor. the inferior
term receives the name subordinate. thus, in the sentence alfred
parle [. . . ], parle is the governor and alfred the subordinate. [2.2]

id33

6(103)

dependency structure

introduction

economic

news

had

little

e   ect

on    nancial markets

.

id33

7(103)

dependency structure

introduction

economic

news

had

little

e   ect

on    nancial markets

.

id33

7(103)

dependency structure

introduction

economic

news

had

little

e   ect

on    nancial markets

.

id33

7(103)

dependency structure

introduction

economic

news

had

little

e   ect

on    nancial markets

.

id33

7(103)

dependency structure

introduction

economic

news

had

little

e   ect

on    nancial markets

.

id33

7(103)

dependency structure

introduction

sbj

economic

news

had

little

e   ect

on    nancial markets

.

id33

7(103)

dependency structure

introduction

nmod

sbj

economic

news

had

little

e   ect

on    nancial markets

.

id33

7(103)

dependency structure

introduction

obj

nmod

sbj

economic

news

had

little

e   ect

on    nancial markets

.

id33

7(103)

dependency structure

introduction

p

obj

pc

nmod

sbj

nmod nmod

nmod

economic

news

had

little

e   ect

on    nancial markets

.

id33

7(103)

terminology

introduction

superior
head
governor
regent
...

inferior
dependent
modi   er
subordinate
...

id33

8(103)

terminology

introduction

superior
head
governor
regent
...

inferior
dependent
modi   er
subordinate
...

id33

8(103)

notational variants

introduction

had

obj

sbj

news

p

e   ect

.

nmod

nmod

nmod

economic

little

on

pc

markets

nmod

   nancial

id33

9(103)

notational variants

introduction

sbj

nn

nmod

jj

vbd

obj

p

nmod

nn

nmod

jj

in

pc

pu

nns

nmod

jj

economic news

had

little e   ect on    nancial markets

.

id33

9(103)

notational variants

introduction

p

obj

pc

nmod

sbj

nmod nmod

nmod

economic

news

had

little

e   ect

on    nancial markets

.

id33

9(103)

notational variants

introduction

p

obj

pc

nmod

sbj

nmod nmod

nmod

economic

news

had

little

e   ect

on    nancial markets

.

id33

9(103)

phrase structure

introduction

 

 

 

 

 

 

 

"

 

 

 

 

 

 
np
hh
  
nn
jj

 

 
"
np
hh
  
nn
jj

 
vbd

s
qq
  

q

"

q

q

q

q
vp
hh
 
np
hh
"
pp
hh
 
np
  
hh
jj

 
in

q

 

"

nns

q

q

q

q
pu

economic

news

had

little

e   ect

on

   nancial

markets

.

id33

10(103)

comparison

introduction

    dependency structures explicitly represent
    head-dependent relations (directed arcs),
    functional categories (arc labels),
    possibly some structural categories (parts-of-speech).

    phrase structures explicitly represent

    phrases (nonterminal nodes),
    structural categories (nonterminal labels),
    possibly some functional categories (grammatical functions).

    hybrid representations may combine all elements.

id33

11(103)

some theoretical frameworks

introduction

    word grammar (wg) [hudson 1984, hudson 1990]

    functional generative description (fgd) [sgall et al. 1986]

    dependency uni   cation grammar (dug)

[hellwig 1986, hellwig 2003]

    meaning-text theory (mtt) [mel     cuk 1988]

    (weighted) constraint dependency grammar ([w]cdg)

[maruyama 1990, harper and helzerman 1995,
menzel and schr  oder 1998, schr  oder 2002]

    functional dependency grammar (fdg)

[tapanainen and j  arvinen 1997, j  arvinen and tapanainen 1998]

    topological/extensible dependency grammar ([t/x]dg)

[duchier and debusmann 2001, debusmann et al. 2004]

id33

12(103)

some theoretical issues

introduction

    dependency structure su   cient as well as necessary?

    mono-stratal or multi-stratal syntactic representations?
    what is the nature of lexical elements (nodes)?

    morphemes?
    word forms?
    multi-word units?

    what is the nature of dependency types (arc labels)?

    grammatical functions?
    semantic roles?

    what are the criteria for identifying heads and dependents?

    what are the formal properties of dependency structures?

id33

13(103)

some theoretical issues

introduction

    dependency structure su   cient as well as necessary?

    mono-stratal or multi-stratal syntactic representations?
    what is the nature of lexical elements (nodes)?

    morphemes?
    word forms?
    multi-word units?

    what is the nature of dependency types (arc labels)?

    grammatical functions?
    semantic roles?

    what are the criteria for identifying heads and dependents?

    what are the formal properties of dependency structures?

id33

13(103)

criteria for heads and dependents

introduction

    criteria for a syntactic relation between a head h and a

dependent d in a construction c [zwicky 1985, hudson 1990]:
1. h determines the syntactic category of c ; h can replace c .
2. h determines the semantic category of c ; d speci   es h.
3. h is obligatory; d may be optional.
4. h selects d and determines whether d is obligatory.
5. the form of d depends on h (agreement or government).
6. the linear position of d is speci   ed with reference to h.

    issues:

    syntactic (and morphological) versus semantic criteria
    exocentric versus endocentric constructions

id33

14(103)

some clear cases

introduction

construction head dependent
subject (sbj)
exocentric
verb
object (obj)
verb
adverbial (vmod)
verb
noun attribute (nmod)

endocentric

sbj

obj

nmod

vmod

nmod

economic

news

suddenly

a   ected    nancial markets

.

id33

15(103)

introduction

some tricky cases

    complex verb groups (auxiliary     main verb)
    subordinate clauses (complementizer     verb)
    coordination (coordinator     conjuncts)
    prepositional phrases (preposition     nominal)

    punctuation

?

i

can

see

that

they

rely

on

this

and

that

.

id33

16(103)

introduction

some tricky cases

    complex verb groups (auxiliary     main verb)
    subordinate clauses (complementizer     verb)
    coordination (coordinator     conjuncts)
    prepositional phrases (preposition     nominal)

    punctuation

sbj

vg

sbj

i

can

see

that

they

rely

on

this

and

that

.

id33

16(103)

introduction

some tricky cases

    complex verb groups (auxiliary     main verb)
    subordinate clauses (complementizer     verb)
    coordination (coordinator     conjuncts)
    prepositional phrases (preposition     nominal)

    punctuation

sbj

vg

?

sbj

i

can

see

that

they

rely

on

this

and

that

.

id33

16(103)

introduction

some tricky cases

    complex verb groups (auxiliary     main verb)
    subordinate clauses (complementizer     verb)
    coordination (coordinator     conjuncts)
    prepositional phrases (preposition     nominal)

    punctuation

sbj

vg

obj

sbar

sbj

i

can

see

that

they

rely

on

this

and

that

.

id33

16(103)

some tricky cases

introduction

    complex verb groups (auxiliary     main verb)
    subordinate clauses (complementizer     verb)
    coordination (coordinator     conjuncts)
    prepositional phrases (preposition     nominal)

    punctuation

sbj

vg

obj

sbar

sbj

?

?

i

can

see

that

they

rely

on

this

and

that

.

id33

16(103)

some tricky cases

introduction

    complex verb groups (auxiliary     main verb)
    subordinate clauses (complementizer     verb)
    coordination (coordinator     conjuncts)
    prepositional phrases (preposition     nominal)

    punctuation

sbj

vg

obj

sbar

sbj

co

cj

i

can

see

that

they

rely

on

this

and

that

.

id33

16(103)

some tricky cases

introduction

    complex verb groups (auxiliary     main verb)
    subordinate clauses (complementizer     verb)
    coordination (coordinator     conjuncts)
    prepositional phrases (preposition     nominal)

    punctuation

sbj

vg

obj

sbar

sbj

?

co

cj

i

can

see

that

they

rely

on

this

and

that

.

id33

16(103)

some tricky cases

introduction

    complex verb groups (auxiliary     main verb)
    subordinate clauses (complementizer     verb)
    coordination (coordinator     conjuncts)
    prepositional phrases (preposition     nominal)

    punctuation

sbar

sbj

vg

obj

sbj

vc

pc

co

cj

i

can

see

that

they

rely

on

this

and

that

.

id33

16(103)

some tricky cases

introduction

    complex verb groups (auxiliary     main verb)
    subordinate clauses (complementizer     verb)
    coordination (coordinator     conjuncts)
    prepositional phrases (preposition     nominal)

    punctuation

?

sbar

sbj

vg

obj

sbj

vc

pc

co

cj

i

can

see

that

they

rely

on

this

and

that

.

id33

16(103)

some tricky cases

introduction

    complex verb groups (auxiliary     main verb)
    subordinate clauses (complementizer     verb)
    coordination (coordinator     conjuncts)
    prepositional phrases (preposition     nominal)

    punctuation

p

sbar

sbj

vg

obj

sbj

vc

pc

co

cj

i

can

see

that

they

rely

on

this

and

that

.

id33

16(103)

dependency graphs

introduction

    a dependency structure can be de   ned as a directed graph g ,

consisting of

    a set v of nodes,
    a set e of arcs (edges),
    a linear precedence order < on v .

    labeled graphs:

    nodes in v are labeled with word forms (and annotation).
    arcs in e are labeled with dependency types.

    notational conventions (i, j     v ):

    i     j     (i, j)     e
    i        j     i = j        k : i     k, k        j

id33

17(103)

formal conditions on dependency graphs

introduction

    g is (weakly) connected:

    for every node i there is a node j such that i     j or j     i.

    g is acyclic:

    if i     j then not j        i.

    g obeys the single-head constraint:

    if i     j, then not k     j, for any k 6= i.

    g is projective:

    if i     j then i        k, for any k such that i < k < j or j < k < i.

id33

18(103)

introduction

connectedness, acyclicity and single-head

    intuitions:

    syntactic structure is complete (connectedness).
    syntactic structure is hierarchical (acyclicity).
    every word has at most one syntactic head (single-head).

    connectedness can be enforced by adding a special root node.

obj

pc

nmod

sbj

nmod nmod

nmod

economic

news

had

little

e   ect

on    nancial markets

.

id33

19(103)

introduction

connectedness, acyclicity and single-head

    intuitions:

    syntactic structure is complete (connectedness).
    syntactic structure is hierarchical (acyclicity).
    every word has at most one syntactic head (single-head).

    connectedness can be enforced by adding a special root node.

pred

p

obj

pc

nmod

sbj

nmod nmod

nmod

root economic

news

had

little

e   ect

on    nancial markets

.

id33

19(103)

introduction

projectivity

    most theoretical frameworks do not assume projectivity.
    non-projective structures are needed to account for

    long-distance dependencies,
    free word order.

pc

p

vg

sbj

nmod

obj

nmod nmod

what

did

economic

news

have

little

e   ect

on

?

id33

20(103)

scope of the tutorial

introduction

    id33:

    input: sentence x = w1, . . . , wn
    output: dependency graph g

    focus of tutorial:

    computational methods for id33
    resources for id33 (parsers, treebanks)

    not included:

    theoretical frameworks of dependency syntax
    constituency parsers that exploit lexical dependencies
    unsupervised learning of dependency structure

id33

21(103)

parsing methods

parsing methods

    three main traditions:

    id145
    id124
    deterministic parsing

    special issue:

    non-projective id33

id33

22(103)

id145

parsing methods

    basic idea: treat dependencies as constituents.

    use, e.g., cyk parser (with minor modi   cations).

    dependencies as constituents:

id33

23(103)

id145

parsing methods

    basic idea: treat dependencies as constituents.

    use, e.g., cyk parser (with minor modi   cations).

    dependencies as constituents:

the

dog

barked

   

barked

dog

barked

the

dog

id33

23(103)

id145

parsing methods

    basic idea: treat dependencies as constituents.

    use, e.g., cyk parser (with minor modi   cations).

    dependencies as constituents:

nmod

sbj

   

barked

sbj
dog

barked

the

dog

barked

nmod

the

dog

id33

23(103)

dependency chart parsing

parsing methods

    grammar is regarded as context-free, in which each node is

lexicalized.

    chart entries are subtrees, i.e., words with all their left and

right dependents.

    problem: di   erent entries for di   erent subtrees spanning a

sequence of words with di   erent heads.

    time requirement: o(n5).

id33

24(103)

id145 approaches

parsing methods

    original version: [hays 1964]

    link grammar: [sleator and temperley 1991]

    earley-style parser with left-corner    ltering:

[lombardo and lesmo 1996]

    bilexical grammar: [eisner 1996a, eisner 1996b, eisner 2000]

    bilexical grammar with discriminative estimation methods:

[mcdonald et al. 2005a, mcdonald et al. 2005b]

id33

25(103)

eisner   s bilexical algorithm

parsing methods

    two novel aspects:

    modi   ed parsing algorithm
    probabilistic id33

    time requirement: o(n3).
    modi   cation: instead of storing subtrees, store spans.

    def. span: substring such that no interior word links to any

word outside the span.

    underlying idea: in a span, only the endwords are active, i.e.

still need a head.

    one or both of the endwords can be active.

id33

26(103)

example

parsing methods

the man in the corner

taught his dog to play golf

root

id33

27(103)

example

parsing methods

the man in the corner

taught his dog to play golf

root

spans:

( man

in

the

corner )

( dog

to

play )

id33

incorrect span:

27(103)

assembly of correct parse

parsing methods

start by combining adjacent words to minimal spans:

( the man )

( man

in )

( in

the )

. . .

id33

28(103)

assembly of correct parse

parsing methods

combine spans which overlap in one word; this word must be
governed by a word in the left or right span.

( in

the ) + ( the

corner )     ( in

the

corner )

id33

28(103)

assembly of correct parse

parsing methods

combine spans which overlap in one word; this word must be
governed by a word in the left or right span.

( man

in ) + ( in

the

corner )     ( man

in

the

corner )

id33

28(103)

assembly of correct parse

parsing methods

combine spans which overlap in one word; this word must be
governed by a word in the left or right span.

invalid span:

( the man

in

the

corner )

id33

28(103)

assembly of correct parse

parsing methods

combine spans which overlap in one word; this word must be
governed by a word in the left or right span.

( dog

to ) + ( to

play )     ( dog

to

play )

id33

28(103)

assembly of correct parse

parsing methods

( the man ) + ( man in the corner taught his dog to play golf root )

    ( the man in the corner taught his dog to play golf root )

id33

28(103)

parsing methods

eisner   s id203 models

    model a: bigram lexical a   nities

    first generates a trigram markov model for id52.
    decides for each word pair whether they have a dependency.
    model is leaky because it does not control for crossing

dependencies, multiple heads, . . .

id33

29(103)

parsing methods

eisner   s id203 models

    model a: bigram lexical a   nities

    first generates a trigram markov model for id52.
    decides for each word pair whether they have a dependency.
    model is leaky because it does not control for crossing

dependencies, multiple heads, . . .

    model b: selectional preferences

    first generates a trigram markov model for id52.
    each word chooses a subcat/supercat frame.
    selects an analysis that satis   es all frames if possible.
    model is also leaky because last step may fail.

id33

29(103)

parsing methods

eisner   s id203 models

    model a: bigram lexical a   nities

    first generates a trigram markov model for id52.
    decides for each word pair whether they have a dependency.
    model is leaky because it does not control for crossing

dependencies, multiple heads, . . .

    model b: selectional preferences

    first generates a trigram markov model for id52.
    each word chooses a subcat/supercat frame.
    selects an analysis that satis   es all frames if possible.
    model is also leaky because last step may fail.

    model c: recursive generation

    each word generates its actual dependents.
    two markov chains:
    left dependents
    right dependents

    model is not leaky.

id33

29(103)

eisner   s model c

parsing methods

pr (words, tags, links) =

y1   i    n yc

pr (tword(depc(i)) | tag (depc   1(i)), tword(i))!

c =    (1 + #left     deps(i)) . . . 1 + #right     deps(i), c 6= 0

or: depc+1(i) if c < 0

id33

30(103)

parsing methods

eisner   s results

    25 000 wall street journal sentences

    baseline: most frequent tag chosen for a word, each word

chooses a head with most common distance

    model x: trigram tagging, no dependencies

    for comparison: state-of-the-art constituent parsing,

charniak: 92.2 f-measure

model
baseline
model x
model a
model b
model c

non-punct tagging
76.1
93.1

41.9
   

too slow
83.8
86.9

92.8
92.0

id33

31(103)

maximum spanning trees

[mcdonald et al. 2005a, mcdonald et al. 2005b]

parsing methods

    score of a dependency tree = sum of scores of dependencies

    scores are independent of other dependencies.

    if scores are available, parsing can be formulated as maximum

spanning tree problem.

    two cases:

    projective: use eisner   s parsing algorithm.
    non-projective: use chu-liu-edmonds algorithm for    nding

the maximum spanning tree in a directed graph
[chu and liu 1965, edmonds 1967].

    use online learning for determining weight vector w:

large-margin multi-class classi   cation (mira)

id33

32(103)

maximum spanning trees (2)

parsing methods

    complexity:

    projective (eisner): o(n3)
    non-projective (cle): o(n2)

score(sent, deps) = x(i ,j)   deps

score(i, j) = x(i ,j)   deps

w    f (i, j)

id33

33(103)

parsing methods

online learning

training data: t = (sentt, depst)t

t=1

1. w = 0; v = 0; i = 0;
2. for n : 1.. n

3.

for t : 1..t

4.

5.

w(i +1) = update w(i ) according to (sentt, depst)
v = v + w(i +1)
6.
i = i + 1
7. w = v/(n    t )

id33

34(103)

parsing methods

mira

mira weight update:

min ||w(i +1)     w(i )|| so that

score(sentt, depst)     score(sentt, deps    )     l(depst, deps    )

   deps         dt(sentt)

    l(deps, deps    ): id168
    dt(sent): possible dependency parses for sentence

id33

35(103)

results by mcdonald et al. (2005a, 2005b)

    unlabeled accuracy per word (w) and per sentence (s)

parsing methods

english
czech
w s w s
parser
90.9
k-best mira eisner
best mira cle
90.2
factored mira cle 90.2

37.5
33.2
32.2

83.3
84.1
84.4

31.3
32.2
32.3

    new development (eacl 2006):

    scores of dependencies are not independent any more
    better results
    more later

id33

36(103)

parsing methods

parsing methods

    three main traditions:

    id145
    id124
    deterministic parsing

    special issue:

    non-projective id33

id33

37(103)

id124

parsing methods

    uses constraint dependency grammar.

    grammar consists of a set of boolean constraints, i.e. logical

formulas that describe well-formed trees.

    a constraint is a logical formula with variables that range over

a set of prede   ned values.

    parsing is de   ned as a id124 problem.

    parsing is an eliminative process rather than a constructive

one such as in id18 parsing.

    id124 removes values that contradict

constraints.

id33

38(103)

examples for constraints

    based on [maruyama 1990]

parsing methods

id33

39(103)

parsing methods

examples for constraints

    based on [maruyama 1990]
    example 1:

    word(pos(x)) = det    

(label(x) = nmod, word(mod(x)) = nn, pos(x) < mod(x))
    a determiner (det) modi   es a noun (nn) on the right with

the label nmod.

id33

39(103)

parsing methods

examples for constraints

    based on [maruyama 1990]
    example 1:

    word(pos(x)) = det    

(label(x) = nmod, word(mod(x)) = nn, pos(x) < mod(x))
    a determiner (det) modi   es a noun (nn) on the right with

the label nmod.

    example 2:

    word(pos(x)) = nn    

(label(x) = sbj, word(mod(x)) = vb, pos(x) < mod(x))

    a noun modi   es a verb (vb) on the right with the label sbj.

id33

39(103)

parsing methods

examples for constraints

    based on [maruyama 1990]
    example 1:

    word(pos(x)) = det    

(label(x) = nmod, word(mod(x)) = nn, pos(x) < mod(x))
    a determiner (det) modi   es a noun (nn) on the right with

the label nmod.

    example 2:

    word(pos(x)) = nn    

(label(x) = sbj, word(mod(x)) = vb, pos(x) < mod(x))

    a noun modi   es a verb (vb) on the right with the label sbj.

    example 3:

    word(pos(x)) = vb    

(label(x) = root, mod(x) = nil)

    a verb modi   es nothing, its label is root.

id33

39(103)

id124 approaches

parsing methods

    constraint grammar: [karlsson 1990, karlsson et al. 1995]

    constraint dependency grammar:

[maruyama 1990, harper and helzerman 1995]

    functional dependency grammar: [j  arvinen and tapanainen 1998]

    topological dependency grammar: [duchier 1999, duchier 2003]

    extensible dependency grammar: [debusmann et al. 2004]

    constraint dependency grammar with defeasible constraints:

[foth et al. 2000, foth et al. 2004, menzel and schr  oder 1998,
schr  oder 2002]

id33

40(103)

id124

parsing methods

    id124 in general is np complete.

    parser design must ensure practical e   ciency.
    di   erent approaches to do id124:

    maruyama applies constraint propagation techniques, which

ensure local consistency (arc consistency).

    weighted cdg uses transformation-based constraint resolution

with anytime properties [foth et al. 2000, foth et al. 2004,
menzel and schr  oder 1998, schr  oder 2002].

    tdg uses constraint programming [duchier 1999, duchier 2003].

id33

41(103)

maruyama   s constraint propagation

parsing methods

three steps:

1. form initial constraint network using a    core    grammar.

2. remove local inconsistencies.

3. if ambiguity remains, add new constraints and repeat step 2.

id33

42(103)

constraint propagation example

parsing methods

    problem: pp attachment

    sentence: put the block on the    oor on the table in the room
    simpli   ed representation: v1 np2 pp3 pp4 pp5

id33

43(103)

constraint propagation example

parsing methods

    problem: pp attachment

    sentence: put the block on the    oor on the table in the room
    simpli   ed representation: v1 np2 pp3 pp4 pp5
    correct analysis:

loc

obj

pmod

pmod

v1

np2

pp3

pp4

pp5

put

the block

on the    oor

on the table

in the room

id33

43(103)

initial constraints

   

    word(pos(x))=pp

    (word(mod(x))     {pp, np, v}, mod(x) < pos(x))

    a pp modi   es a pp, an np, or a v on the left.

parsing methods

id33

44(103)

parsing methods

initial constraints

   

    word(pos(x))=pp

    (word(mod(x))     {pp, np, v}, mod(x) < pos(x))

    a pp modi   es a pp, an np, or a v on the left.
    word(pos(x))=pp, word(mod(x))     {pp, np}

   

    label(x)=pmod

    if a pp modi   es a pp or an np, its label is pmod.

id33

44(103)

parsing methods

initial constraints

   

    word(pos(x))=pp

    (word(mod(x))     {pp, np, v}, mod(x) < pos(x))

    a pp modi   es a pp, an np, or a v on the left.
    word(pos(x))=pp, word(mod(x))     {pp, np}

    label(x)=pmod

    if a pp modi   es a pp or an np, its label is pmod.
    word(pos(x))=pp, word(mod(x))=v     label(x)=loc
    if a pp modi   es a v, its label is loc.

   

   

id33

44(103)

parsing methods

initial constraints

   

    word(pos(x))=pp

   

   

   

    (word(mod(x))     {pp, np, v}, mod(x) < pos(x))

    a pp modi   es a pp, an np, or a v on the left.
    word(pos(x))=pp, word(mod(x))     {pp, np}

    label(x)=pmod

    if a pp modi   es a pp or an np, its label is pmod.
    word(pos(x))=pp, word(mod(x))=v     label(x)=loc
    if a pp modi   es a v, its label is loc.
    word(pos(x))=np

    (word(mod(x))=v, label(x)=obj, mod(x) < pos(x))

    an np modi   es a v on the left with the label obj.

id33

44(103)

parsing methods

initial constraints

   

    word(pos(x))=pp

    (word(mod(x))     {pp, np, v}, mod(x) < pos(x))

    a pp modi   es a pp, an np, or a v on the left.
    word(pos(x))=pp, word(mod(x))     {pp, np}

    label(x)=pmod

    if a pp modi   es a pp or an np, its label is pmod.
    word(pos(x))=pp, word(mod(x))=v     label(x)=loc
    if a pp modi   es a v, its label is loc.
    word(pos(x))=np

    (word(mod(x))=v, label(x)=obj, mod(x) < pos(x))

    an np modi   es a v on the left with the label obj.
    word(pos(x))=v     (mod(x)=nil, label(x)=root)
    a v modi   es nothing with the label root.

   

   

   

   

id33

44(103)

parsing methods

initial constraints

   

    word(pos(x))=pp

    (word(mod(x))     {pp, np, v}, mod(x) < pos(x))

    a pp modi   es a pp, an np, or a v on the left.
    word(pos(x))=pp, word(mod(x))     {pp, np}

    label(x)=pmod

    if a pp modi   es a pp or an np, its label is pmod.
    word(pos(x))=pp, word(mod(x))=v     label(x)=loc
    if a pp modi   es a v, its label is loc.
    word(pos(x))=np

    (word(mod(x))=v, label(x)=obj, mod(x) < pos(x))

    an np modi   es a v on the left with the label obj.
    word(pos(x))=v     (mod(x)=nil, label(x)=root)
    a v modi   es nothing with the label root.
    mod(x) < pos(y) < pos(x)     mod(x)     mod(y)     pos(x)
    modi   cation links do not cross.

   

   

   

   

   

id33

44(103)

initial constraint network

parsing methods

v1

pp5

np2

pp4

pp3

id33

45(103)

parsing methods

initial constraint network

v1

pp5

np2

pp4

pp3

possible values     unary constraints:

<root, nil>

v1:
np2: <obj, 1>
pp3: <loc, 1>, <pmod, 2>
pp4: <loc, 1>, <pmod, 2>, <pmod, 3>
pp5: <loc, 1>, <pmod, 2>, <pmod, 3>, <pmod,4>

id33

45(103)

initial constraint network

parsing methods

1

np2

v1

pp5

pp4

pp3

each arc has a constraint matrix:
for arc 1 :

    v1 \ np2     <obj, 1>
<root, nil>

1

id33

45(103)

parsing methods

initial constraint network

v1

pp5

np2

pp4

pp3

2

each arc has a constraint matrix:
for arc 2 :

    pp3 \ pp4     <loc, 1> <pmod, 2> <pmod, 3>
<loc, 1>
<pmod, 2>

1
1

1
1

0
1

id33

45(103)

adding new constraints

parsing methods

    still 14 possible analyses.

    filtering with binary constraints does not reduce ambiguity.

    introduce more constraints:

id33

46(103)

adding new constraints

parsing methods

    still 14 possible analyses.

    filtering with binary constraints does not reduce ambiguity.

    introduce more constraints:

   

    word(pos(x))=pp, on table     sem(pos(x))

      (   oor     sem(mod(x)))
    a    oor is not on the table.

id33

46(103)

adding new constraints

parsing methods

    still 14 possible analyses.

    filtering with binary constraints does not reduce ambiguity.

    introduce more constraints:

   

    word(pos(x))=pp, on table     sem(pos(x))

      (   oor     sem(mod(x)))
    a    oor is not on the table.
    label(x)=loc, label(y)=loc, mod(x)=mod(y), word(mod(x))=v

   

    x=y

    no verb can take two locatives.

id33

46(103)

adding new constraints

parsing methods

    still 14 possible analyses.

    filtering with binary constraints does not reduce ambiguity.

    introduce more constraints:

   

    word(pos(x))=pp, on table     sem(pos(x))

      (   oor     sem(mod(x)))
    a    oor is not on the table.
    label(x)=loc, label(y)=loc, mod(x)=mod(y), word(mod(x))=v

   

    x=y

    no verb can take two locatives.

    each value in the domains of nodes is tested against the new

constraints.

id33

46(103)

modi   ed tables
old:

parsing methods

    pp3 \ pp4     <loc, 1> <pmod, 2> <pmod, 3>
<loc, 1>
<pmod, 2>

1
1

1
1

0
1

id33

47(103)

modi   ed tables
old:

parsing methods

    pp3 \ pp4     <loc, 1> <pmod, 2> <pmod, 3>
<loc, 1>
<pmod, 2>

1
1

1
1

0
1

violates    rst constraint

id33

47(103)

modi   ed tables
old:

parsing methods

    pp3 \ pp4     <loc, 1> <pmod, 2> <pmod, 3>
<loc, 1>
<pmod, 2>

1
1

1
1

0
1

after applying    rst new constraint:

    pp3 \ pp4     <loc, 1> <pmod, 2>
<loc, 1>
<pmod, 2>

0
1

1
1

id33

47(103)

modi   ed tables
old:

parsing methods

    pp3 \ pp4     <loc, 1> <pmod, 2> <pmod, 3>
<loc, 1>
<pmod, 2>

1
1

1
1

0
1

after applying    rst new constraint:

    pp3 \ pp4     <loc, 1> <pmod, 2>
<loc, 1>
<pmod, 2>

0
1

1
1

violates second constraint

id33

47(103)

modi   ed tables
old:

parsing methods

    pp3 \ pp4     <loc, 1> <pmod, 2> <pmod, 3>
<loc, 1>
<pmod, 2>

1
1

1
1

0
1

after applying    rst new constraint:

    pp3 \ pp4     <loc, 1> <pmod, 2>
<loc, 1>
<pmod, 2>

0
1

0
1

id33

47(103)

modi   ed tables
old:

parsing methods

    pp3 \ pp4     <loc, 1> <pmod, 2> <pmod, 3>
<loc, 1>
<pmod, 2>

1
1

1
1

0
1

after applying    rst new constraint:

    pp3 \ pp4     <loc, 1> <pmod, 2>
<loc, 1>
<pmod, 2>

0
1

0
1

after applying second new constraint:

    pp3 \ pp4     <loc, 1> <pmod, 2>
<pmod, 2>

1

1

id33

47(103)

weighted constraint parsing

parsing methods

    approach by [foth et al. 2004, foth et al. 2000,

menzel and schr  oder 1998, schr  oder 2002]

    robust parser, which uses soft constraints

    each constraint is assigned a weight between 0.0 and 1.0

    weight 0.0: hard constraint, can only be violated when no

other parse is possible

    constraints assigned manually (or estimated from treebank)

    e   ciency: uses a heuristic transformation-based constraint

resolution method

id33

48(103)

transformation-based constraint resolution

parsing methods

    heuristic search

    very e   cient

    idea:    rst construct arbitrary dependency structure, then try

to correct errors

    error correction by transformations

    selection of transformations based on constraints that cause

con   icts

    anytime property: parser maintains a complete analysis at any

time     can be stopped at any time and return a complete
analysis

id33

49(103)

parsing methods

menzel et al.   s results

    evaluation on negra treebank for german

    german more di   cult to parse than english (free word order)

    constituent-based parsing: labeled f measure including
grammatical functions: 53.4 [k  ubler et al. 2006], labeled f
measure: 73.1 [dubey 2005].

    best conll-x results: unlabeled: 90.4, labeled: 87.3

[mcdonald et al. 2006].

data
1000 sentences
< 40 words

unlabeled labeled
87.0
87.7

89.0
89.7

id33

50(103)

parsing methods

parsing methods

    three main traditions:

    id145
    id124
    deterministic parsing

    special issue:

    non-projective id33

id33

51(103)

deterministic parsing

parsing methods

    basic idea:

    derive a single syntactic representation (dependency graph)

through a deterministic sequence of elementary parsing actions

    sometimes combined with backtracking or repair

    motivation:

    psycholinguistic modeling
    e   ciency
    simplicity

id33

52(103)

covington   s incremental algorithm

    deterministic incremental parsing in o(n2) time by trying to

link each new word to each preceding one [covington 2001]:

parsing methods

for i = 1 up to n

parse(x = (w1, . . . , wn))
1
2
3

link(wi , wj )

for j = i     1 down to 1

link(wi , wj ) =       
   

e     e     (i, j)
e     e     (j, i)
e     e

if wj is a dependent of wi
if wi is a dependent of wj
otherwise

    di   erent conditions, such as single-head and projectivity, can

be incorporated into the link operation.

id33

53(103)

shift-reduce type algorithms

parsing methods

    data structures:

    stack [. . . , wi ]s of partially processed tokens
    queue [wj , . . .]q of remaining input tokens

    parsing actions built from atomic actions:

    adding arcs (wi     wj , wi     wj )
    stack and queue operations

    left-to-right parsing in o(n) time

    restricted to projective dependency graphs

id33

54(103)

parsing methods

yamada   s algorithm

    three parsing actions:

shift

left

right

[. . .]s
[. . . , wi ]s

[wi , . . .]q
[. . .]q

[. . . , wi , wj ]s
[. . . , wi ]s

[. . .]q
[. . .]q wi     wj

[. . . , wi , wj ]s
[. . . , wj ]s

[. . .]q
[. . .]q wi     wj

    algorithm variants:

    originally developed for japanese (strictly head-   nal) with only

the shift and right actions [kudo and matsumoto 2002]

    adapted for english (with mixed headedness) by adding the

left action [yamada and matsumoto 2003]

    multiple passes over the input give time complexity o(n2)

id33

55(103)

nivre   s algorithm

    four parsing actions:

parsing methods

shift

reduce

left-arcr

right-arcr

[. . .]s
[. . . , wi ]s

[. . . , wi ]s
[. . .]s

[. . . , wi ]s
[. . .]s

[wi , . . .]q
[. . .]q

[. . .]q    wk : wk     wi
[. . .]q

[wj , . . .]q      wk : wk     wi
[wj , . . .]q wi

r    wj

[. . . , wi ]s
[. . . , wi , wj ]s

[wj , . . .]q      wk : wk     wj
[. . .]q

r    wj

wi

    characteristics:

    integrated labeled id33
    arc-eager processing of right-dependents
    single pass over the input gives time complexity o(n)

id33

56(103)

example

parsing methods

[root]s

[economic news had little e   ect on    nancial markets

.]q

id33

57(103)

example

parsing methods

[root economic]s

[news had little e   ect on    nancial markets

.]q

shift

id33

57(103)

parsing methods

example

nmod

[root]s economic [news had little e   ect on    nancial markets

.]q

left-arcnmod

id33

57(103)

parsing methods

example

nmod

[root economic news]s

[had little e   ect on    nancial markets

.]q

shift

id33

57(103)

example

parsing methods

nmod

sbj

[root]s economic news

[had little e   ect on    nancial markets

.]q

left-arcsbj

id33

57(103)

parsing methods

example

pred

nmod

sbj

[root economic news had]s

[little e   ect on    nancial markets

.]q

right-arcpred

id33

57(103)

parsing methods

example

pred

nmod

sbj

[root economic news had little]s

[e   ect on    nancial markets

.]q

shift

id33

57(103)

example

parsing methods

pred

nmod

sbj

nmod

[root economic news had]s

little [e   ect on    nancial markets

.]q

left-arcnmod

id33

57(103)

example

parsing methods

pred

nmod

sbj

obj

nmod

[root economic news had little e   ect]s

[on    nancial markets

.]q

right-arcobj

id33

57(103)

example

parsing methods

pred

obj

nmod

sbj

nmod nmod

[root economic news had little e   ect on]s

[   nancial markets

.]q

right-arcnmod

id33

57(103)

example

parsing methods

pred

obj

nmod

sbj

nmod nmod

[root economic news had little e   ect on    nancial]s

[markets

.]q

shift

id33

57(103)

example

parsing methods

pred

obj

nmod

sbj

nmod nmod

nmod

[root economic news had little e   ect on]s    nancial

[markets

.]q

left-arcnmod

id33

57(103)

example

parsing methods

pred

obj

pc

nmod

sbj

nmod nmod

nmod

[root economic news had little e   ect on    nancial markets]s

[.]q

right-arcpc

id33

57(103)

example

parsing methods

pred

obj

pc

nmod

sbj

nmod nmod

nmod

[root economic news had little e   ect on]s    nancial markets

[.]q

reduce

id33

57(103)

example

parsing methods

pred

obj

pc

nmod

sbj

nmod nmod

nmod

[root economic news had little e   ect]s on    nancial markets

[.]q

reduce

id33

57(103)

example

parsing methods

pred

obj

pc

nmod

sbj

nmod nmod

nmod

[root economic news had]s

little e   ect on    nancial markets

[.]q

reduce

id33

57(103)

example

parsing methods

pred

obj

pc

nmod

sbj

nmod nmod

nmod

[root]s economic news had little e   ect on    nancial markets

[.]q

reduce

id33

57(103)

example

parsing methods

pred

p

obj

pc

nmod

sbj

nmod nmod

nmod

[root economic news had little e   ect on    nancial markets

.]s

[]q

right-arcp

id33

57(103)

classi   er-based parsing

parsing methods

    data-driven deterministic parsing:

    deterministic parsing requires an oracle.
    an oracle can be approximated by a classi   er.
    a classi   er can be trained using treebank data.

    learning methods:

    support vector machines (id166)

[kudo and matsumoto 2002, yamada and matsumoto 2003,
isozaki et al. 2004, cheng et al. 2004, nivre et al. 2006]

    memory-based learning (mbl)

[nivre et al. 2004, nivre and scholz 2004]
    maximum id178 modeling (maxent)

[cheng et al. 2005]

id33

58(103)

parsing methods

feature models

    learning problem:

    approximate a function from parser states, represented by

feature vectors to parser actions, given a training set of gold
standard derivations.

    typical features:

    tokens:

    target tokens
    linear context (neighbors in s and q)
    structural context (parents, children, siblings in g )

    attributes:

    word form (and lemma)
    part-of-speech (and morpho-syntactic features)
    dependency type (if labeled)
    distance (between target tokens)

id33

59(103)

parsing methods

state of the art     english

    evaluation:

    id32 (wsj) converted to dependency graphs
    unlabeled accuracy per word (w) and per sentence (s)

    deterministic classi   er-based parsers

[yamada and matsumoto 2003, isozaki et al. 2004]

    spanning tree parsers with online training

[mcdonald et al. 2005a, mcdonald and pereira 2006]

    collins and charniak parsers with same conversion

parser
charniak
collins
mcdonald and pereira
isozaki et al.
mcdonald et al.
yamada and matsumoto

w
92.2
91.7
91.5
91.4
91.0
90.4

s

45.2
43.3
42.1
40.7
37.5
38.4

id33

60(103)

comparing algorithms

parsing methods

    parsing algorithm:

    nivre   s algorithm gives higher accuracy than yamada   s

algorithm for parsing the chinese ckip treebank
[cheng et al. 2004].

    learning algorithm:

    id166 gives higher accuracy than maxent for parsing the

chinese ckip treebank [cheng et al. 2004].

    id166 gives higher accuracy than mbl with lexicalized feature

models for three languages [hall et al. 2006]:

    chinese (penn)
    english (penn)
    swedish (talbanken)

id33

61(103)

parsing methods

parsing methods

    three main traditions:

    id145
    id124
    deterministic parsing

    special issue:

    non-projective id33

id33

62(103)

parsing methods

non-projective id33

    many parsing algorithms are restricted to projective

dependency graphs.

    is this a problem?
    statistics from conll-x shared task [buchholz and marsi 2006]

    npd = non-projective dependencies
    nps = non-projective sentences

language %npd %nps
dutch
36.4
27.8
german
23.2
czech
slovene
22.2
18.9
portuguese
danish
15.6

5.4
2.3
1.9
1.9
1.3
1.0

id33

63(103)

two main approaches

parsing methods

    algorithms for non-projective id33:

    id124 methods [tapanainen and j  arvinen 1997,

duchier and debusmann 2001, foth et al. 2004]

    mcdonald   s spanning tree algorithm [mcdonald et al. 2005b]
    covington   s algorithm [nivre 2006]

    post-processing of projective dependency graphs:
    pseudo-projective parsing [nivre and nilsson 2005]
    corrective modeling [hall and nov  ak 2005]
    approximate non-projective parsing [mcdonald and pereira 2006]

id33

64(103)

non-projective parsing algorithms

parsing methods

    complexity considerations:

    projective (proj)
    non-projective (nonp)

problem/algorithm
complete grammar parsing
[gaifman 1965, neuhaus and br  oker 1997]

proj

p

nonp

np hard

deterministic parsing
[nivre 2003, covington 2001]

first order spanning tree
[mcdonald et al. 2005b]

o(n)

o(n2)

o(n3)

o(n2)

nth order spanning tree (n > 1)
[mcdonald and pereira 2006]

p

np hard

id33

65(103)

parsing methods

post-processing

    two-step approach:

1. derive the best projective approximation of the correct

(possibly) non-projective dependency graph.

2. improve the approximation by replacing projective arcs by

(possibly) non-projective arcs.

    rationale:

    most    naturally occurring    dependency graphs are primarily

projective, with only a few non-projective arcs.

    approaches:

    pseudo-projective parsing [nivre and nilsson 2005]
    corrective modeling [hall and nov  ak 2005]
    approximate non-projective parsing [mcdonald and pereira 2006]

id33

66(103)

pseudo-projective parsing

    projectivize training data:

    projective head nearest permissible ancestor of real head
    arc label extended with dependency type of real head

parsing methods

auxk

auxp

pred

atr

auxp

sb

auxz

adv

root

z

nich

je

jen

jedna na

kvalitu .

(out-of) (them) (is) (only) (one) (to) (quality)

id33

67(103)

pseudo-projective parsing

    projectivize training data:

    projective head nearest permissible ancestor of real head
    arc label extended with dependency type of real head

parsing methods

auxk

auxp

pred

auxp   sb

atr

auxp

sb

auxz

adv

root

z

nich

je

jen

jedna na

kvalitu .

(out-of) (them) (is) (only) (one) (to) (quality)

id33

67(103)

parsing methods

pseudo-projective parsing

    deprojectivize parser output:

    top-down, breadth-   rst search for real head
    search constrained by extended arc label

auxk

pred

auxp   sb

atr

auxp

sb

auxz

adv

root

z

nich

je

jen

jedna na

kvalitu .

(out-of) (them) (is) (only) (one) (to) (quality)

id33

67(103)

parsing methods

pseudo-projective parsing

    deprojectivize parser output:

    top-down, breadth-   rst search for real head
    search constrained by extended arc label

auxk

auxp

pred

auxp   sb

atr

auxp

sb

auxz

adv

root

z

nich

je

jen

jedna na

kvalitu .

(out-of) (them) (is) (only) (one) (to) (quality)

id33

67(103)

corrective modeling

parsing methods

    id155 model

p(h   

i |wi , n(hi ))

for correcting the head hi of word wi to h   
local neighboorhood n(hi ) of hi

i , restricted to the

    model trained on parser output and gold standard parses

(maxent estimation)

    post-processing:

    for every word wi , replace hi by argmaxh   

i

p(h   

i |wi , n(hi )).

id33

68(103)

second-order non-projective parsing

parsing methods

    the score of a dependency tree y for input sentence x is

s(i, k, j)

x(i ,k ,j)   y

where k and j are adjacent, same-side children of i in y .

    the highest scoring projective dependency tree can be

computed exactly in o(n3) time using eisner   s algorithm.
    the highest scoring non-projective dependency tree can be

approximated with a greedy post-processing procedure:

    while improving the global score of the dependency tree,
i     wi , greedily selecting the

replace an arc hi     wi by h   
substitution that gives the greatest improvement.

id33

69(103)

parsing methods

state of the art     czech

    evaluation:

    prague dependency treebank (pdt)
    unlabeled accuracy per word (w) and per sentence (s)

    non-projective spanning tree parsing [mcdonald et al. 2005b]
    corrective modeling on top of the charniak parser

[hall and nov  ak 2005]

    approximate non-projective parsing on top of a second-order
projective spanning tree parser [mcdonald and pereira 2006]

    pseudo-projective parsing on top of a deterministic

classi   er-based parser [nilsson et al. 2006]

parser
mcdonald and pereira
hall and nov  ak
nilsson et al.
mcdonald et al.
charniak

s

w
85.2
35.9
85.1    
84.6
37.7
32.3
84.4
84.4
   

id33

70(103)

state of the art     multilingual parsing

parsing methods

    conll-x shared task: 12 (13) languages

    organizers: sabine buchholz, erwin marsi, yuval

krymolowski, amit dubey

    main evaluation metric: labeled accuracy per word

    top scores ranging from 91.65 (japanese) to 65.68 (turkish)
    top systems (over all languages):

    approximate second-order non-projective spanning tree parsing

with online learning (mira) [mcdonald et al. 2006]

    labeled deterministic pseudo-projective parsing with support

vector machines [nivre et al. 2006]

id33

71(103)

pros and cons of id33

pros and cons of id33

    what are the advantages of dependency-based methods?

    what are the disadvantages?
    four types of considerations:

    complexity
    transparency
    word order
    expressivity

id33

72(103)

complexity

pros and cons of id33

    practical complexity:

    given the single-head constraint, parsing a sentence

x = w1, . . . , wn can be reduced to labeling each token wi with:

    a head word hi ,
    a dependency type di .

    theoretical complexity:

    by exploiting the special properties of dependency graphs, it is
sometimes possible to improve worst-case complexity compared
to constituency-based parsing:

    lexicalized parsing in o(n3) time [eisner 1996b]

id33

73(103)

transparency

pros and cons of id33

    direct encoding of predicate-argument structure

s

vp

sbj

obj

np

prp vbz

np

nns

she writes books

she writes

books

id33

74(103)

transparency

pros and cons of id33

    direct encoding of predicate-argument structure

    fragments directly interpretable

sbj

np

prp vbz

np

nns

she writes books

she writes

books

id33

74(103)

transparency

pros and cons of id33

    direct encoding of predicate-argument structure

    fragments directly interpretable

    but only with labeled dependency graphs

sbj

np

prp vbz

np

nns

she writes books

she writes

books

id33

74(103)

pros and cons of id33

word order

    dependency structure independent of word order

    suitable for free word order languages (cf. german results)

s

vp

sbj

vg

obj

np

np

prp vb

vbn

prp

hon

har

sett

honom

hon

har

sett

honom

(she) (has) (seen)

(him)

(she) (has) (seen)

(him)

id33

75(103)

pros and cons of id33

word order

    dependency structure independent of word order

    suitable for free word order languages (cf. german results)

obj

sbj

vg

s

vp

np

np

prp

vb prp vbn

honom har

hon

sett

honom har

hon

sett

(him)

(has) (she) (seen)

(him)

(has) (she) (seen)

id33

75(103)

pros and cons of id33

word order

    dependency structure independent of word order

    suitable for free word order languages (cf. german results)

    but only with non-projective dependency graphs

obj

sbj

vg

s

vp

np

np

prp

vb prp vbn

honom har

hon

sett

honom har

hon

sett

(him)

(has) (she) (seen)

(him)

(has) (she) (seen)

id33

75(103)

pros and cons of id33

expressivity

    limited expressivity:

    every projective dependency grammar has a strongly equivalent

context-free grammar, but not vice versa [gaifman 1965].

    impossible to distinguish between phrase modi   cation and head
modi   cation in unlabeled dependency structure [mel     cuk 1988].

sbj verb obj adverbial v, vp or s modi   cation?

    what about labeled non-projective dependency structures?

id33

76(103)

practical issues

practical issues

    where to get the software?

    dependency parsers
    conversion programs for constituent-based treebanks

    where to get the data?

    dependency treebanks
    treebanks that can be converted into dependency

representation

    how to evaluate id33?

    evaluation scores

    where to get help and information?

    id33 wiki

id33

77(103)

parsers

practical issues

    trainable parsers

    parsers with manually written grammars

id33

78(103)

parsers

practical issues

    trainable parsers

    parsers with manually written grammars

    concentrate on freely available parsers

id33

78(103)

trainable parsers

practical issues

    jason eisner   s probabilistic dependency parser

    based on bilexical grammar
    contact jason eisner: jason@cs.jhu.edu
    written in lisp

    ryan mcdonald   s mstparser

    based on the algorithms of

[mcdonald et al. 2005a, mcdonald et al. 2005b]

    url:

http://www.seas.upenn.edu/~ryantm/software/mstparser/

    written in java

id33

79(103)

trainable parsers (2)

practical issues

    joakim nivre   s maltparser

    inductive dependency parser with memory-based learning and

id166s
    url:

http://w3.msi.vxu.se/~nivre/research/maltparser.html

    executable versions are available for solaris, linux, windows,

and macos (open source version planned for fall 2006)

id33

80(103)

parsers for speci   c languages

practical issues

    dekang lin   s minipar

    principle-based parser
    grammar for english
    url: http://www.cs.ualberta.ca/~lindek/minipar.htm
    executable versions for linux, solaris, and windows

    wolfgang menzel   s cdg parser:

    weighted constraint dependency parser
    grammar for german, (english under construction)
    online demo:

http://nats-www.informatik.uni-hamburg.de/papa/parserdemo

    download:

http://nats-www.informatik.uni-hamburg.de/download

id33

81(103)

parsers for speci   c languages (2)

practical issues

    taku kudo   s cabocha

    based on algorithms of [kudo and matsumoto 2002], uses id166s
    url: http://www.chasen.org/~taku/software/cabocha/
    web page in japanese

    gerold schneider   s pro3gres

    id203-based dependency parser
    grammar for english
    url: http://www.ifi.unizh.ch/cl/gschneid/parser/
    written in prolog

    daniel sleator   s & davy temperley   s link grammar parser

    undirected links between words
    grammar for english
    url: http://www.link.cs.cmu.edu/link/

id33

82(103)

treebanks

practical issues

    genuine dependency treebanks

    treebanks for which conversions to dependencies exist

    see also conll-x shared task

url: http://nextens.uvt.nl/~conll/

    conversion strategy from constituents to dependencies

id33

83(103)

dependency treebanks

practical issues

    arabic: prague arabic dependency treebank

    czech: prague dependency treebank

    danish: danish dependency treebank

    portuguese: bosque: floresta sint  a(c)tica

    slovene: slovene dependency treebank

    turkish: metu-sabanci turkish treebank

id33

84(103)

dependency treebanks (2)

practical issues

    prague arabic dependency treebank

    ca. 100 000 words
    available from ldc, license fee

(conll-x shared task data, catalogue number ldc2006e01)

    url: http://ufal.mff.cuni.cz/padt/

    prague dependency treebank

    1.5 million words
    3 layers of annotation: morphological, syntactical,

tectogrammatical

    available from ldc, license fee

(conll-x shared task data, catalogue number ldc2006e02)

    url: http://ufal.mff.cuni.cz/pdt2.0/

id33

85(103)

dependency treebanks (3)

practical issues

    danish dependency treebank

    ca. 5 500 trees
    annotation based on discontinuous grammar [kromann 2005]
    freely downloadable
    url: http://www.id.cbs.dk/~mtk/treebank/

    bosque, floresta sint  a(c)tica

    ca. 10 000 trees
    freely downloadable
    url:

http://acdc.linguateca.pt/treebank/info_floresta_english.html

id33

86(103)

dependency treebanks (4)

practical issues

    slovene dependency treebank

    ca. 30 000 words
    freely downloadable
    url: http://nl.ijs.si/sdt/

    metu-sabanci turkish treebank

    ca. 7 000 trees
    freely available, license agreement
    url: http://www.ii.metu.edu.tr/~corpus/treebank.html

id33

87(103)

constituent treebanks

practical issues

    english: id32

    bulgarian: bultreebank

    chinese: penn chinese treebank, sinica treebank

    dutch: alpino treebank for dutch
    german: tiger/negra, t  uba-d/z
    japanese: t  uba-j/s
    spanish: cast3lb

    swedish: talbanken05

id33

88(103)

constituent treebanks (2)

practical issues

    id32

    ca. 1 million words
    available from ldc, license fee
    url: http://www.cis.upenn.edu/~treebank/home.html
    dependency conversion rules, available from e.g. [collins 1999]
    for conversion with arc labels: penn2malt:

http://w3.msi.vxu.se/~nivre/research/penn2malt.html

    bultreebank

    ca. 14 000 sentences
    url: http://www.bultreebank.org/
    dependency version available from kiril simov

(kivs@bultreebank.org)

id33

89(103)

constituent treebanks (3)

practical issues

    penn chinese treebank
    ca. 4 000 sentences
    available from ldc, license fee
    url: http://www.cis.upenn.edu/~chinese/ctb.html
    for conversion with arc labels: penn2malt:

http://w3.msi.vxu.se/~nivre/research/penn2malt.html

    sinica treebank

    ca. 61 000 sentences
    available academia sinica, license fee
    url:

http://godel.iis.sinica.edu.tw/ckip/engversion/treebank.htm

    dependency version available from academia sinica

id33

90(103)

practical issues

constituent treebanks (4)

    alpino treebank for dutch

    ca. 150 000 words
    freely downloadable
    url: http://www.let.rug.nl/vannoord/trees/
    dependency version downloadable at

http://nextens.uvt.nl/~conll/free_data.html

    tiger/negra

    ca. 50 000/20 000 sentences
    freely available, license agreement
    tiger url:

http://www.ims.uni-stuttgart.de/projekte/tiger/tigercorpus/
negra url:
http://www.coli.uni-saarland.de/projects/sfb378/negra-corpus/

    dependency version of tiger is included in release

id33

91(103)

constituent treebanks (5)

practical issues

    t  uba-d/z

    ca. 22 000 sentences
    freely available, license agreement
    url: http://www.sfs.uni-tuebingen.de/en_tuebadz.shtml
    dependency version available from sfs t  ubingen

    t  uba-j/s

    dialog data
    ca. 18 000 sentences
    freely available, license agreement
    dependency version available from sfs t  ubingen
    url: http://www.sfs.uni-tuebingen.de/en_tuebajs.shtml

(under construction)

id33

92(103)

constituent treebanks (6)

practical issues

    cast3lb

    ca. 18 000 sentences
    url: http://www.dlsi.ua.es/projectes/3lb/index_en.html
    dependency version available from toni mart     (amarti@ub.edu)

    talbanken05

    ca. 300 000 words
    freely downloadable
    url:

http://w3.msi.vxu.se/~nivre/research/talbanken05.html

    dependency version also available

id33

93(103)

conversion from constituents to
dependencies

practical issues

    conversion from constituents to dependencies is possible

    needs head/non-head information
    if no such information is given     heuristics
    conversion for id32 to dependencies: e.g.,
magerman, collins, lin, yamada and matsumoto . . .

    conversion restricted to structural conversion, no labeling

    concentrate on lin   s conversion: [lin 1995, lin 1998]

id33

94(103)

lin   s conversion

practical issues

    idea: head of a phrase governs all sisters.

    uses tree head table: list of rules where to    nd the head

of a constituent.

    an entry consists of the node, the direction of search, and the

list of possible heads.

id33

95(103)

lin   s conversion

practical issues

    idea: head of a phrase governs all sisters.

    uses tree head table: list of rules where to    nd the head

of a constituent.

    an entry consists of the node, the direction of search, and the

list of possible heads.

    sample entries:

right-to-left
(s
(vp left-to-right
(np right-to-left

(aux vp np ap pp))
(v vp))
(pron n np))

    first line: the head of an s constituent is the    rst aux

daughter from the right; if there is no aux, then the    rst vp,
etc.

id33

95(103)

practical issues

lin   s conversion - example

(s
right-to-left
(vp left-to-right
(np right-to-left

(aux vp np ap pp))
(v vp))
(pron n np))

id33

96(103)

practical issues

root

head

lex. head

lin   s conversion - example

(s
right-to-left
(vp left-to-right
(np right-to-left

(aux vp np ap pp))
(v vp))
(pron n np))

s

np1

vp1

pron

adv

vp2

i

really

v

np2

like

adj

n

black

coffee

id33

96(103)

practical issues

root
s

head
vp1

lex. head
??

lin   s conversion - example

(s
right-to-left
(vp left-to-right
(np right-to-left

(aux vp np ap pp))
(v vp))
(pron n np))

s

np1

vp1

pron

adv

vp2

i

really

v

np2

like

adj

n

black

coffee

id33

96(103)

practical issues

root
head
vp1 vp2

lex. head
??

lin   s conversion - example

(s
right-to-left
(vp left-to-right
(np right-to-left

(aux vp np ap pp))
(v vp))
(pron n np))

s

np1

vp1

pron

adv

vp2

i

really

v

np2

like

adj

n

black

coffee

id33

96(103)

practical issues

head
root
s
vp1
vp1 vp2
vp2 v

lex. head
like
like
like

lin   s conversion - example

right-to-left
(s
(vp left-to-right
(np right-to-left

(aux vp np ap pp))
(v vp))
(pron n np))

s

np1

vp1

pron

adv

vp2

i

really

v

np2

like

adj

n

black

coffee

id33

96(103)

lin   s conversion - example (2)

practical issues

    the head of a phrase dominates all sisters.
    vp1 governs np1     like governs i
    vp2 governs adv     like governs really

id33

97(103)

lin   s conversion - example (2)

practical issues

    the head of a phrase dominates all sisters.
    vp1 governs np1     like governs i
    vp2 governs adv     like governs really

like

i

really

coffee

black

id33

97(103)

from structural to labeled conversion

practical issues

    conversion so far gives only pure dependencies from head to

dependent.

    collins uses combination of constituent labels to label relation

[collins 1999]:

    idea: combination of mother node and two subordinate nodes

gives information about grammatical functions.

    if headword(yh)     headword(yd) is derived from rule

x     y1 . . . yn, the relation is <yd , x , yh>

id33

98(103)

collins    example

practical issues

s

np

vp

nns

vbd

np

pp

workers

dumped

nns

in

np

sacks

into

dt

nn

a

bin

id33

99(103)

collins    example

practical issues

s

np

vp

nns

vbd

np

pp

workers

dumped

nns

in

np

relation

dependency
dumped     workers <np, s, vp>
dumped     root
dumped     sacks
dumped     into
into     bin
bin     a

<s, start, start>
<np, vp, vbd>
<pp, vp, vbd>
<np, pp, in>
<dt, np, nn>

sacks

into

dt

nn

a

bin

id33

99(103)

example with grammatical functions

practical issues

subj
np

hd

nns

s
hd

hd
vbd

workers

dumped

nns

vp

obj

v-mod

np

hd

hd
in

pp

nhd

nhd
np

hd
nn

sacks

into

dt

a

bin

id33

100(103)

example with grammatical functions

practical issues

subj
np

hd

nns

s
hd

hd
vbd

workers

dumped

nns

vp

obj

v-mod

np

hd

hd
in

pp

nhd

sacks

into

dt

nhd
np

hd
nn

a

bin

dependency
dumped     workers
dumped     root
dumped     sacks
dumped     into
into     bin
bin     a

relation
sbj
punct
obj
v-mod
nhd
nhd

id33

100(103)

practical issues

evaluation
evaluation scores:

    exact match (= s)

percentage of correctly parsed sentences

    attachment score (= w)

percentage of words that have the correct head

    for single dependency types (labels):

    precision
    recall
    f   measure

    correct root

percentage of sentences that have the correct root

id33

101(103)

practical issues

evaluation
evaluation scores:

    exact match (= s)

percentage of correctly parsed sentences

    attachment score (= w)

percentage of words that have the correct head

    for single dependency types (labels):

    precision
    recall
    f   measure

    correct root

percentage of sentences that have the correct root

id33

101(103)

practical issues

evaluation
evaluation scores:

    exact match (= s)

percentage of correctly parsed sentences

    attachment score (= w)

percentage of words that have the correct head

    for single dependency types (labels):

    precision
    recall
    f   measure

    correct root

percentage of sentences that have the correct root

id33

101(103)

practical issues

evaluation
evaluation scores:

    exact match (= s)

percentage of correctly parsed sentences

    attachment score (= w)

percentage of words that have the correct head

    for single dependency types (labels):

    precision
    recall
    f   measure

    correct root

percentage of sentences that have the correct root

    all labeled and unlabeled

id33

101(103)

further information

    id33 wiki

http://depparse.uvt.nl

    book by joakim: inductive id33

practical issues

id33

102(103)

outlook

outlook

    future trends (observed or predicted):

    multilingual id33

    conll shared task
    comparative error analysis
    typological diversity and parsing methods

    non-projective id33

    non-projective parsing algorithms
    post-processing of projective approximations
    other approaches

    global constraints

    grammar-driven approaches
    nth-order spanning tree parsing
    hybrid approaches [foth et al. 2004]

    dependency and constituency

    what are the essential di   erences?
    very few theoretical results

id33

103(103)

references

    sabine buchholz and erwin marsi. 2006. conll-x shared task on multilingual
id33. in proceedings of the tenth conference on computational
natural language learning.

    yuchang cheng, masayuki asahara, and yuji matsumoto. 2004. determinstic

dependency structure analyzer for chinese. in proceedings of the first international
joint conference on natural language processing (ijcnlp), pages 500   508.

    yuchang cheng, masayuki asahara, and yuji matsumoto. 2005. machine

learning-based dependency analyzer for chinese. in proceedings of international
conference on chinese computing (iccc).

    y. j. chu and t. j. liu. 1965. on the shortest arborescence of a directed graph.

science sinica, 14:1396   1400.

    michael collins. 1999. head-driven statistical models for natural language

parsing. ph.d. thesis, university of pennsylvania.

    michael a. covington. 2001. a fundamental algorithm for id33. in

proceedings of the 39th annual acm southeast conference, pages 95   102.

    ralph debusmann, denys duchier, and geert-jan m. kruij   . 2004. extensible

dependency grammar: a new methodology. in proceedings of the workshop on
recent advances in dependency grammar, pages 78   85.

id33

103(103)

references

    amit dubey. 2005. what to do when lexicalization fails: parsing german with

su   x analysis and smoothing. in proceedings of the 43rd annual meeting of the
association for computational linguistics, ann arbor, mi.

    denys duchier and ralph debusmann. 2001. topological dependency trees: a

constraint-based account of linear precedence. in proceedings of the 39th annual
meeting of the association for computational linguistics (acl), pages 180   187.

    denys duchier. 1999. axiomatizing id33 using set constraints. in
proceedings of the sixth meeting on mathematics of language, pages 115   126.

    denys duchier. 2003. con   guration of labeled trees under lexicalized constraints

and principles. research on language and computation, 1:307   336.

    j. edmonds. 1967. optimum branchings. journal of research of the national

bureau of standards, 71b:233   240.

    jason m. eisner. 1996a. an empirical comparison of id203 models for

dependency grammar. technical report ircs-96-11, institute for research in
cognitive science, university of pennsylvania.

    jason m. eisner. 1996b. three new probabilistic models for id33:

an exploration. in proceedings of the 16th international conference on
computational linguistics (coling), pages 340   345.

id33

103(103)

references

    jason m. eisner. 2000. bilexical grammars and their cubic-time parsing algorithms.

in harry bunt and anton nijholt, editors, advances in probabilistic and other
parsing technologies, pages 29   62. kluwer.

    kilian foth, ingo schr  oder, and wolfgang menzel. 2000. a transformation-based

parsing technique with anytime properties. in proceedings of the sixth
international workshop on parsing technologies (iwpt), pages 89   100.

    kilian foth, michael daum, and wolfgang menzel. 2004. a broad-coverage parser

for german based on defeasible constraints. in proceedings of konvens 2004,
pages 45   52.

    haim gaifman. 1965. dependency systems and phrase-structure systems.

information and control, 8:304   337.

    keith hall and vaclav nov  ak. 2005. corrective modeling for non-projective

id33. in proceedings of the 9th international workshop on parsing
technologies (iwpt), pages 42   52.

    johan hall, joakim nivre, and jens nilsson. 2006. discriminative classi   ers for

deterministic id33. in proceedings of coling-acl.

    mary p. harper and r. a. helzerman. 1995. extensions to constraint dependency

parsing for spoken language processing. computer speech and language,
9:187   234.

id33

103(103)

references

    david g. hays. 1964. dependency theory: a formalism and some observations.

language, 40:511   525.

    peter hellwig. 1986. dependency uni   cation grammar. in proceedings of the 11th
international conference on computational linguistics (coling), pages 195   198.

    peter hellwig. 2003. dependency uni   cation grammar. in vilmos agel, ludwig m.
eichinger, hans-werner eroms, peter hellwig, hans j  urgen heringer, and hening
lobin, editors, dependency and valency, pages 593   635. walter de gruyter.

    richard a. hudson. 1984. word grammar. blackwell.

    richard a. hudson. 1990. english word grammar. blackwell.

    hideki isozaki, hideto kazawa, and tsutomu hirao. 2004. a deterministic word

dependency analyzer enhanced with preference learning. in proceedings of the 20th
international conference on computational linguistics (coling), pages 275   281.

    timo j  arvinen and pasi tapanainen. 1998. towards an implementable dependency

grammar. in sylvain kahane and alain polgu`ere, editors, proceedings of the
workshop on processing of dependency-based grammars, pages 1   10.

    fred karlsson, atro voutilainen, juha heikkil  a, and arto anttila, editors. 1995.

constraint grammar: a language-independent system for parsing unrestricted text.
mouton de gruyter.

id33

103(103)

references

    fred karlsson. 1990. constraint grammar as a framework for parsing running text.
in hans karlgren, editor, papers presented to the 13th international conference on
computational linguistics (coling), pages 168   173.

    matthias trautner kromann. 2005. discontinuous grammar: a dependency-based

model of human parsing and language learning. doctoral dissertation,
copenhagen business school.

    sandra k  ubler, erhard w. hinrichs, and wolfgang maier. 2006. is it really that
di   cult to parse german? in proceedings of the 2006 conference on empirical
methods in natural language processing, emnlp 2006, sydney, australia.

    taku kudo and yuji matsumoto. 2002. japanese dependency analysis using
cascaded chunking. in proceedings of the sixth workshop on computational
language learning (conll), pages 63   69.

    dekang lin. 1995. a dependency-based method for evaluating broad-coverage

parsers. in proceedings of ijcai-95, pages 1420   1425.

    dekang lin. 1998. a dependency-based method for evaluating broad-coverage

parsers. natural language engineering, 4:97   114.

    vincenzio lombardo and leonardo lesmo. 1996. an earley-type recognizer for
dependency grammar. in proceedings of the 16th international conference on
computational linguistics (coling), pages 723   728.

id33

103(103)

references

    hiroshi maruyama. 1990. structural disambiguation with constraint propagation. in

proceedings of the 28th meeting of the association for computational linguistics
(acl), pages 31   38.

    ryan mcdonald and fernando pereira. 2006. online learning of approximate
id33 algorithms. in proceedings of the 11th conference of the
european chapter of the association for computational linguistics (eacl), pages
81   88.

    ryan mcdonald, koby crammer, and fernando pereira. 2005a. online

large-margin training of dependency parsers. in proceedings of the 43rd annual
meeting of the association for computational linguistics (acl), pages 91   98.

    ryan mcdonald, fernando pereira, kiril ribarov, and jan haji  c. 2005b.

non-projective id33 using spanning tree algorithms. in proceedings
of the human language technology conference and the conference on empirical
methods in natural language processing (hlt/emnlp), pages 523   530.

    ryan mcdonald, kevin lerman, and fernando pereira. 2006. multilingual

dependency analysis with a two-stage discriminative parser. in proceedings of the
tenth conference on computational natural language learning (conll).

    igor mel     cuk. 1988. dependency syntax: theory and practice. state university of

new york press.

id33

103(103)

references

    wolfgang menzel and ingo schr  oder. 1998. decision procedures for dependency
parsing using graded constraints. in sylvain kahane and alain polgu`ere, editors,
proceedings of the workshop on processing of dependency-based grammars,
pages 78   87.

    peter neuhaus and norbert br  oker. 1997. the complexity of recognition of

linguistically adequate dependency grammars. in proceedings of the 35th annual
meeting of the association for computational linguistics (acl) and the 8th
conference of the european chapter of the association for computational
linguistics (eacl), pages 337   343.

    jens nilsson, joakim nivre, and johan hall. 2006. graph transformations in

data-driven id33. in proceedings of coling-acl.

    joakim nivre and jens nilsson. 2005. pseudo-projective id33. in
proceedings of the 43rd annual meeting of the association for computational
linguistics (acl), pages 99   106.

    joakim nivre and mario scholz. 2004. deterministic id33 of english

text. in proceedings of the 20th international conference on computational
linguistics (coling), pages 64   70.

    joakim nivre, johan hall, and jens nilsson. 2004. memory-based dependency

parsing. in hwee tou ng and ellen rilo   , editors, proceedings of the 8th
conference on computational natural language learning (conll), pages 49   56.

id33

103(103)

references

    joakim nivre, johan hall, jens nilsson, g  ulsen eryi  git, and svetoslav marinov.

2006. labeled pseudo-projective id33 with support vector machines.
in proceedings of the tenth conference on computational natural language
learning (conll).

    joakim nivre. 2003. an e   cient algorithm for projective id33. in

gertjan van noord, editor, proceedings of the 8th international workshop on
parsing technologies (iwpt), pages 149   160.

    joakim nivre. 2006. constraints on non-projective dependency graphs. in

proceedings of the 11th conference of the european chapter of the association for
computational linguistics (eacl), pages 73   80.

    ingo schr  oder. 2002. natural language parsing with graded constraints. ph.d.

thesis, hamburg university.

    petr sgall, eva haji  cov  a, and jarmila panevov  a. 1986. the meaning of the

sentence in its pragmatic aspects. reidel.

    daniel sleator and davy temperley. 1991. parsing english with a link grammar.

technical report cmu-cs-91-196, carnegie mellon university, computer science.

    pasi tapanainen and timo j  arvinen. 1997. a non-projective dependency parser. in
proceedings of the 5th conference on applied natural language processing, pages
64   71.

id33

103(103)

    lucien tesni`ere. 1959.   el  ements de syntaxe structurale. editions klincksieck.

    hiroyasu yamada and yuji matsumoto. 2003. statistical dependency analysis with

support vector machines. in gertjan van noord, editor, proceedings of the 8th
international workshop on parsing technologies (iwpt), pages 195   206.

    a. m. zwicky. 1985. heads. journal of linguistics, 21:1   29.

references

id33

103(103)

