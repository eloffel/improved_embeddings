new directions in

vector space models of meaning

edward grefenstette1 karl moritz hermann1

georgiana dinu2

phil blunsom1

1dept of computer science

university of oxford

2centre for mind/brain sciences

university of trento

acl 2014 tutorial

slides at: http://www.clg.ox.ac.uk/resources

what is the meaning of life?

a joke for semanticists
q: what is the meaning of life?

2/113

what is the meaning of life?

a joke for semanticists
q: what is the meaning of life?
a: life(cid:48)

2/113

what is the meaning of life?

a joke for semanticists
q: what is the meaning of life?
a: life(cid:48) / i (life) / [[life]] / etc.

2/113

what is the meaning of life?

a joke for semanticists
q: what is the meaning of life?
a: life(cid:48) / i (life) / [[life]] / etc.
    what semantic value to give life(cid:48)?

    logical atom?
    logical predicate/relation?
    just the token itself?

    what is the relation between life and death?
    how can we infer the meaning of life?

2/113

symbolic success

we like the symbolic/discrete approach because. . .

    discrete models can be cheap and fast
    many success stories. e.g.:
    id165 language models
    id52/parsing

    logical analysis:
    long history
    powerful id136

3/113

your logic is no good here. . .

but. . .

    doesn   t capture

   messiness   
    no similarity
    sparsity
    rules are hard to learn
    limited variety of

id136

4/113

your logic is no good here. . .

but. . .

    doesn   t capture

   messiness   
    no similarity
    sparsity
    rules are hard to learn
    limited variety of

id136

4/113

vector representations for words

    go from discrete to distributed representations
    word meanings are vectors of properties
    well studied mathematical structure
    well motivated, theoretically and practically

background

philosophy
linguistics
engineering + statistics feature vectors

hume, wittgenstein
firth, harris

5/113

vector representations for words

many successful applications in lexical semantics:

    word-sense disambiguation
    thesaurus extraction

also many use cases in nlp pipelines, e.g.:

    automated essay marking
    plagiarism detection

6/113

more than mere words

what   s missing?
word representations alone are not enough to do:

    machine translation
    information extraction
    id53
    etc.

we need sentence/id194s.

7/113

vector representations for phrases

what could we do with sentence/document vectors?

    generation

    classi   cation

    english translation from french sentence
    next sentence in a conversation
    metadata for documents

    topic/sentiment
    stock market predictions ($$$!!)
    recommendations (movies, books, restaurants)

8/113

vector representations for phrases

why can we classify and generate with vectors?

    learn spatial boundaries to separate subspaces
    similarity metrics give predictors for next word
    geometric transforms model contextual in   uence

9/113

tasks for vector models of meaning

today   s tutorial is about two kinds of basic tasks for the
construction of vector models of meaning:

    learning vector representations for words
    learning how to compose them to get vector

representations for phrases/sentences/documents

10/113

today   s menu

1 id65

2 neural distributed representations

3 semantic composition

4 last words

11/113

goals of this tutorial

by the end of this tutorial, you should have:

    a good understanding of distributed word representations

and their usage.

    some background knowledge about neural language

models and (conditional) generation.

    a decent overview of options for integrating
compositionality into vector-based models.

    su   cient knowledge about the terms and mathematics of

neural methods to read deep learning papers in nlp.

    hopefully, some new ideas of your own!

12/113

outline

1 id65

2 neural distributed representations

3 semantic composition

4 last words

13/113

the distributional hypothesis

found a

we
cute
wampimuk c(cid:13)marcobaroni
sleeping in a tree.

little

?

14/113

id65 in a nutshell

he curtains open and the stars shining in on the barely
ars and the cold , close stars " . and neither of the w
rough the night with the stars shining so brightly , it
made in the light of the stars . it all boils down , wr

surely under the bright stars , thrilled by ice-white

sun , the seasons of the stars ? home , alone , jay pla
m is dazzling snow , the stars have risen full and cold
un and the temple of the stars , driving out of the hug

in the dark and now the stars rise , full and amber a
bird on the shape of the stars over the trees in front

but i could n   t see the stars or the moon , only the
they love the sun , the stars and the stars . none of

r the light of the shiny stars . the plash of flowing w
man    s first look at the stars ; various exhibits , aer
rief information on both stars and constellations, inc

15/113

id65 in a nutshell

construct vector representations

shining bright

stars

38

45

trees dark
27

2

look
12

similarity in meaning as vector similarity

    cucumber

    stars

    sun

16/113

in more detail

core components of distributional models of semantics:

    co-occurrence counts extraction
    weighting schemes
    id84
    similarity measures

17/113

extracting co-occurrence counts

a matrix of co-occurrence counts is built, representing the
target linguistic units over context features.

variations in the type of context features

doc1

doc2

doc3

stars

38

45

2

dobj

         see

38

mod         bright

45

mod         shiny

44

the nearest     to earth stories of     and their

10

12

stars

stars

18/113

extracting co-occurrence counts

variations in the de   nition of co-occurrence
co-occurrence with words, window of size 2, scaling by
distance to target:
... two [intensely bright stars in the] night sky ...

intensely bright

stars

0.5

1

in the
0.5
1

for more details, see:

    pado and lapata (2007),
    turney and pantel (2010).
    comparisons: agirre et al (2009), baroni and lenci
(2010), bullinaria and levy (2012), kiela and clark
(2014)

19/113

weighting

re-weight the counts using corpus-level statistics to re   ect
co-occurrence signi   cance.

point-wise mutual information (pmi)

pmi(target, ctxt) = log

p(target, ctxt)
p(target)p(ctxt)

20/113

weighting

adjusting raw collocational counts:

bright
385

in

10788

stars

stars

43.6

5.3

...     counts
...     pmi

other weighting schemes:

    tf-idf
    local mutual information
    dice

see ch4 of j.r. curran   s thesis (2004) for a great survey.

21/113

id84

problem
vectors spaces often range from tens of thousands to millions
of dimensions.

22/113

id84

some of the methods to reduce dimensionality:

    select context features based on various relevance criteria
    random indexing
    having also a smoothing e   ect
    singular value decomposition
    non-negative id105
    probabilistic latent semantic analysis
    id44

23/113

distance and similarity

vector similarity measures (or inverted distance measures) are
used to approximate similarity in meaning.

sun

stars

cosine similarity

cos(x, y) =

x    y

(cid:107)x(cid:107)    (cid:107)y(cid:107)

24/113

distance and similarity

other similarity measures:

    euclidean
    lin
    jaccard
    dice
    kullback-leibler (for distributions)

25/113

summary

distributional tradition: vector representations over intuitive,
linguistically-motivated, context features

    pros: easy to obtain, vectors are interpretable
    cons: involves a large number of design choices (what

weighting scheme? what similarity measure?)

    problems: going from word to sentence representations is

non-trivial, and no clear intuitions exist.

an open question
are there other ways to learn composeable vector
representations of meaning, based on the distributional
hypothesis, without this parametric burden?

26/113

outline

1 id65

2 neural distributed representations

3 semantic composition

4 last words

27/113

features and nlp

twenty years ago id148 freed us from the shackles
of simple multinomial parametrisations, but imposed the
tyranny of feature engineering.

28/113

features and nlp

distributed/neural models allow us to learn shallow features
for our classi   ers, capturing simple correlations between inputs.

29/113

features and nlp

deep learning allows us to learn hierarchical generalisations.
something that is proving rather useful for vision, speech, and
now nlp...

30/113

k-max pooling(k=3) fully connectedlayerfoldingwideconvolution(m=2)dynamick-max pooling (k= f(s) =5) projectedsentence matrix(s=7)wideconvolution(m=3)game's the same, just got more    erceneural language models

a neural probabilistic language model. bengio et al. jmlr 2003.

31/113

bengio,ducharme,vincentandjauvinsoftmaxtanh. . .. . .. . .. . .. . .. . .. . .across wordsmost  computation hereindex forindex forindex forshared parametersmatrixinlook   uptable. . .ccwt 1wt 2c(wt 2)c(wt 1)c(wt n+1)wt n+1i-thoutput=p(wt=i|context)figure1:neuralarchitecture:f(i,wt 1,      ,wt n+1)=g(i,c(wt 1),      ,c(wt n+1))wheregistheneuralnetworkandc(i)isthei-thwordfeaturevector.parametersofthemappingcaresimplythefeaturevectorsthemselves,representedbya|v|   mmatrixcwhoserowiisthefeaturevectorc(i)forwordi.thefunctiongmaybeimplementedbyafeed-forwardorrecurrentneuralnetworkoranotherparametrizedfunction,withparameters  .theoverallparametersetis  =(c,  ).trainingisachievedbylookingfor  thatmaximizesthetrainingcorpuspenalizedlog-likelihood:l=1t   tlogf(wt,wt 1,      ,wt n+1;  )+r(  ),wherer(  )isaid173term.forexample,inourexperiments,risaweightdecaypenaltyappliedonlytotheweightsoftheneuralnetworkandtothecmatrix,nottothebiases.3intheabovemodel,thenumberoffreeparametersonlyscaleslinearlywithv,thenumberofwordsinthevocabulary.italsoonlyscaleslinearlywiththeordern:thescalingfactorcouldbereducedtosub-linearifmoresharingstructurewereintroduced,e.g.usingatime-delayneuralnetworkorarecurrentneuralnetwork(oracombinationofboth).inmostexperimentsbelow,theneuralnetworkhasonehiddenlayerbeyondthewordfeaturesmapping,andoptionally,directconnectionsfromthewordfeaturestotheoutput.thereforetherearereallytwohiddenlayers:thesharedwordfeatureslayerc,whichhasnonon-linearity(itwouldnotaddanythinguseful),andtheordinaryhyperbolictangenthiddenlayer.moreprecisely,theneuralnetworkcomputesthefollowingfunction,withasoftmaxoutputlayer,whichguaranteespositiveprobabilitiessummingto1:  p(wt|wt 1,      wt n+1)=eywt   ieyi.3.thebiasesaretheadditiveparametersoftheneuralnetwork,suchasbanddinequation1below.1142id148 for classi   cation

features:   (x)     rd and weights:   k     rd for k     {1, ..., k}

classes:

(cid:80)k

k   (x))

exp(  t
j exp(  t

j   (x))

(cid:104)

(cid:105)

p(ck|x) =
gradient required for training:
   
     j
1
z(x)
exp

    log p(ck|x)

   
     j

(cid:16)

=

=

  t
k   (x)

   
     j

j   (x)(cid:1)

   
     j

   

  t
k   (x)

log z(x)    

   
     j

exp(cid:0)  t
(cid:17)
(cid:125)

=

j   (x)
  t
z(x)

(cid:123)(cid:122)

= p(cj|x)  (x)

(cid:124)

  t
k   (x)

   
     j

  (x)    
      (j, k)  (x)

(cid:123)(cid:122)

(cid:124)

(cid:125)

expected features

observed features

z(x) =(cid:80)k

  (j, k) is the kronecker delta function which is 1 if j = k and 0 otherwise, and

j exp(  t

j   (x)) is referred to as the partition function.

32/113

a simple log-linear (tri-gram) language model

classify the next word wn given wn   1, wn   2: features:
  (wn   1, wn   2)     rd and weights:   i     rd:1

p(wn|wn   1, wn   2)     exp(cid:0)  t

wn   (wn   1, wn   2) + bwn

(cid:1)

traditionally the feature maps   (  ) are rule based, but can we learn
them from the data?

1

we now explicitly include a per-word bias parameter bwn that is initialised to the empirical log p(wn).

33/113

a simple log-linear (tri-gram) language model

traditionally the feature maps   (  ) are rule based, but can we learn
them from the data?
assume the features factorise across the context words:

(cid:0)     1(wn   1) +      2(wn   2)(cid:1) + bwn

(cid:17)

p(wn|wn   1, wn   2)     exp

  t
wn

(cid:16)

34/113

learning the features: the log-bilinear language model

represent the context words by the columns of a d    |vocab|
matrix q, and output words by the columns of a matrix r;
assume   i is a linear function of these representations
parametrised by a matrix ci :

35/113

learning the features: the log-bilinear language model

  (wn   1, wn   2) = c   2q(wn   2) + c   1q(wn   1)

p(wn|wn   1, wn   2)     exp

r(wn)t   (wn   1, wn   2) + bwn

(cid:16)

(cid:17)

this is referred to as a log-bilinear model.2

2

three new id114 for statistical language modelling. mnih and hinton, icml   07.

35/113

learning the features: the log-bilinear language model

(cid:16)

p(wn|wn   1, wn   2)     exp

(cid:17)

r(wn)t   (wn   1, wn   2) + bwn

   

error objective: e =     log p(wn|wn   1, wn   2)
(cid:17)
log z(wn   1, wn   2)    
   r(j)
p(j|wn   1, wn   2)       (j, wn)

   r(j)

(cid:16)

e =

=

   

   

   r(j)

r(wn)t  

  

35/113

learning the features: the log-bilinear language model

e =

   
     

error objective: e =     log p(wn|wn   1, wn   2)
   
log z(wn   1, wn   2)    
     
(cid:125)
p(j|wn   1, wn   2)r(wj )

(cid:104)(cid:88)
(cid:124)

(cid:123)(cid:122)

   
     

=

j

(cid:105)

model expected next word vector

r(wn)t  

(cid:124) (cid:123)(cid:122) (cid:125)

    r(wn)

data vector

35/113

learning the features: the log-bilinear language model

error objective: e =     log p(wn|wn   1, wn   2)

   

   q(j)
     

e =

     
   q(j)   

   e
     

(cid:104)

=

   

   q(j)

   q(j)

c   2q(wn   2) + c   1q(wn   1)

=   (j, wn   2)c t   2 +   (j, wn   1)c t   1

(cid:105)

35/113

learning the features: the log-bilinear language model

error objective: e =     log p(wn|wn   1, wn   2)

     
   c   2

e =

   

   c   2
     
   c   1

(cid:104)

   e
        
   
   a

(cid:105)

=

c   1q(wn   2) + c   2q(wn   1)

= q(wn   2)t

35/113

adding non-linearities: the neural language model

replacing the simple bi-linear relationship between context and
output words with a more powerful non-linear function f (  )
(logistic sigmoid, tanh, etc.):

(cid:104)

p(wn|wn   1, wn   2)

    exp

(cid:16)

this is a neural language model!

r(wn)tf

c 1q(wn   1) + c 2q(wn   2)

(cid:17)

(cid:105)

+ bwn

36/113

adding non-linearities: the neural language model

replacing the simple bi-linear relationship between context and
output words with a more powerful non-linear function f (  )
(logistic sigmoid, tanh, etc.):

if f = the element wise logistic sigmoid   (  ):
   
     

     (  )
         

e =

   e

     (  )
=   (  )(1       (  ))    

(cid:104)(cid:88)

j

p(j|wn   1, wn   2)r(wj )     r(wn)

(cid:105)

where     is the element wise product.

36/113

in   nite context: a recurrent neural language model

a recurrent lm drops the ngram assumption and directly
approximate p(wn|wn   1, . . . , w0) using a recurrent hidden
layer:

  n = f(cid:0)c f (  n   1) + wq(wn   1)(cid:1)

p(wn|wn   1, . . . , w0)     exp

r(wn)tf (  n) + bwn

(cid:104)

(cid:105)

simple id56s like this are not actually terribly e   ective
models. more compelling results are obtained with complex
hidden units (e.g. long short term memory (lstm),
clockwork id56s, etc.), or by making the recurrent
transformation c conditional on the last output.

37/113

e   ciency

for large d, calculating the context vector-matrix products is
costly. diagonal context transformation matrices (cx ) solve
this and result in little performance loss.

38/113

e   ciency

most of the computational cost of a neural lm is a function of the
size of the vocabulary and is dominated by calculating r t  .

39/113

e   ciency

most of the computational cost of a neural lm is a function of the
size of the vocabulary and is dominated by calculating r t  .

solutions
short-lists: use the neural lm for the most frequent words, and a
vanilla ngram lm for the rest. while this is easy to implement, it
nulli   es the neural lm   s main advantage, i.e. generalisation to rare
events.

39/113

e   ciency

most of the computational cost of a neural lm is a function of the
size of the vocabulary and is dominated by calculating r t  .

solutions
approximate the gradient/change the objective: if we did not
have to sum over the vocabulary to normalise during training it
would be much faster. it is tempting to consider maximising
likelihood by making the log partition function a separate
parameter c, but this leads to an ill de   ned objective.

pmodel(wn|wn   1, wn   2,   )     punnormalised

model

(wn|wn   1, wn   2,   )    exp(c)

39/113

e   ciency

most of the computational cost of a neural lm is a function of the
size of the vocabulary and is dominated by calculating r t  .

solutions
approximate the gradient/change the objective: mnih and
teh use noise contrastive estimation. this amounts to learning a
binary classi   er to distinguish data samples from (k) samples from
a noise distribution (a unigram is a good choice):

p(data = 1|wn, wn   1,   ) =

pmodel(wn|wn   1,   )

pmodel(wn|wn   1,   ) + kpnoise(wn)

now parametrising the log partition function as c does not
degenerate. this is very e   ective for speeding up training, but has
no impact on testing time.a

a

in practice    xing c = 0 is e   ective. it is tempting to believe that this noise contrastive objective justi   es

using unnormalised scores at test time. this is not the case and leads to high variance results.

39/113

e   ciency

most of the computational cost of a neural lm is a function of the
size of the vocabulary and is dominated by calculating r t  .

solutions
factorise the output vocabulary: one level factorisation works
well (brown id91 is a good choice, frequency binning is not):

p(wn|  ) = p(class(wn)|  )    p(wn|class(wn),   ),

balanced classes, this gives a(cid:112)

where the function class(  ) maps each word to one class. assuming

|vocab| speedup.

this renders properly normalised neural lms fast enough to be
directly integrated into an mt decoder.a

acompositional morphology for word representations and language modelling.

botha and blunsom, icml   14

39/113

e   ciency

most of the computational cost of a neural lm is a function of the
size of the vocabulary and is dominated by calculating r t  .

solutions
factorise the output vocabulary: by extending the factorisation
to a binary tree (or code) we can get a log |vocab| speedup,a but
choosing a tree is hard (frequency based hu   man coding is a poor
choice):

(cid:89)

i

p(wn|  ) =

p(di|ri ,   ),

where di is i th digit in the code for word wn, and ri is the feature
vector for the i th node in the path corresponding to that code.

aa scalable hierarchical distributed language model. mnih and hinton, nips   09.

39/113

comparison with vanilla id165 lms

good

    better generalisation on unseen ngrams, poorer on seen

ngrams. solution: direct (linear) ngram features mimicking
original log-linear language model features.

    simple nlms are often an order magnitude smaller in
memory footprint than their vanilla ngram cousins (though
not if you use the linear features suggested above!).

bad

    nlms are not as e   ective for extrinsic tasks such as machine
translation compared to kneser-ney models, even when their
intrinsic perplexity is much lower.

    nlms easily beat kneser-ney models on perplexity for small
training sets (<100m), but the representation size must grow
with the data to be competitive at a larger scale.

40/113

learning better representations for rich morphology

illustration of how a 3-gram morphologically factored neural
lm model treats the czech phrase    pro novou   skolu    (for
[the] new school).2

2compositional morphology for word representations and language modelling.

botha and blunsom, icml   14

41/113

learning better representations for rich morphology

42/113

learning representations directly

collobert and weston, mikolov et al. id97, etc.

if we do not care about language modelling, i.e. p(w), and
just want the word representations, we can condition on future
context and/or use more e   cient margin based objectives.

43/113

conditional generation

44/113

(cid:2)(cid:1)         i 'd like a glass of white wine , please .generation               generalisationconditional generation

  n = c    2q(wn   2) + c    1q(wn   1) + csm(n, s)

p(wn|wn   1, wn   2, s)     exp(cid:0)r(wn)t   (  n) + bwn

(cid:1)

45/113

s(s1)s(s2)s(s3)s(s4)s(s5)s(s6)s(s7)s(s8)cncsm++=q(wn-2)tq(wn-1)t  nxc2xc1conditional generation: a naive additive model

pn = c    2q(wn   2) + c    1q(wn   1) +

|s|(cid:88)
p(wn|wn   1, wn   2, s)     exp(cid:0)r(wn)t   (  n) + bwn

j=1

s(sj )

(cid:1)

46/113

s(s1)s(s2)s(s3)s(s4)s(s5)s(s6)s(s7)s(s8)cn+++++++=++=q(wn-2)tq(wn-1)t  nxc2xc1conditional generation: a naive additive model

47/113

                              (cid:1)?conditional generation: a naive additive model

47/113

                              (cid:1)?conditional generation: a naive additive model

47/113

                              (cid:1)?+++++++=conditional generation: a naive additive model

47/113

                              (cid:1)?may i have a wake-up call at seven tomorrow morning ?+++++++=clmconditional generation: a naive additive model

48/113

(cid:5)(cid:3)(cid:1)(cid:4)(cid:2)         ?where 's the currency exchange office ?++++=clmconditional generation: a naive additive model

48/113

(cid:2)(cid:1)         i 'd like a glass of white wine , please .++++=clm   +         +   +conditional generation: a naive additive model

48/113

               (cid:1)         (cid:2)i 'm going to los angeles this afternoon .++++=clm   +conditional generation: a naive additive model

48/113

                           i 'd like to have a room under thirty dollars a night .+++++++=clm      +      (cid:1)   ++conditional generation: a naive additive model

rough gloss
i would like a night thirty dollars under room.

48/113

                           i 'd like to have a room under thirty dollars a night .+++++++=clm      +      (cid:1)   ++conditional generation: a naive additive model

google translate

i want a late thirties under $   s room.

48/113

                           i 'd like to have a room under thirty dollars a night .+++++++=clm      +      (cid:1)   ++conditional generation: a naive additive model

48/113

      (cid:2)(cid:3)   (cid:4)      (cid:1)(cid:5)+++++++=clmyou have to do something about it .+   +      conditional generation: a naive additive model

48/113

      (cid:2)(cid:3)   (cid:4)      (cid:1)(cid:5)+++++++=clmi can n't urinate .+   +      conditional neural lms and mt and beyond

such conditional neural language models are now being exploited
in mt and other multi-modal generation problems:

recurrent continuous translation models.
kalchbrenner and blunsom, emnlp   13.

joint language and translation modeling with
recurrent neural networks.
auli et al., emnlp   13.

fast and robust neural network joint models for
id151.
devlin et al., acl   14.

multimodal neural language models.
kiros et al., icml   14.

49/113

outline

1 id65

2 neural distributed representations

3 semantic composition

motivation
models
training
application nuggets

4 last words

50/113

a simple task

q: do two words (roughly) mean the same?

   cat           dog    ?

a: use a distributional representation to    nd out.

given a vector representation, we can calculate the similarity
between two things using some distance metric (as discussed
earlier).

51/113

a di   erent task: paraphrase detection

q: do two sentences (roughly) mean the same?

   he enjoys jazz music           he likes listening to jazz    ?

a: use a distributional representation to    nd out?

no
we cannot learn distributional features at the sentence level.

52/113

why can   t we extract distributional features?

linguistic creativity
we formulate and understand language by composing units
(words/phrases), not memorising sentences.

crucially: this is what allows us to understand sentences we   ve
never observed/heard before.

53/113

why can   t we extract distributional features?

the curse of dimensionality
as the dimensionality of a representation increases, learning
becomes less and less viable due to sparsity.

dimensionality for collocation

    one entry per word: size of dictionary (small)
    one entry per sentence: number of possible sentences

(in   nite)

    we need a di   erent method for representing sentences

54/113

why care about compositionality

id141
   he enjoys jazz music           he likes listening to jazz    ?
sentiment
   this    lm was perfectly horrible    (good;bad)

translation
   je ne veux pas travailler           i do not want to work    ?

55/113

id152

semantic composition
learning a hierarchy of features, where higher levels of
abstraction are derived from lower levels.

56/113

a door, a roof, a window: it   s a house

0.2
0.3
0.4

0.4
0.7
0.3

0.5
0.3
0.8

   

0.1
0.5
0.1

57/113

id152

a    generic    composition function

p = f (u, v , r, k )

where u, v are the child representations, r the relational
information and k the background knowledge. most
composition models can be expressed as some such function f .

    we may also want to consider the action of sentence-,
paragraph-, or document-level context on composition.

58/113

composition

algebraic

composition

lexical
function
models

collocational

features

abstract
features

requirements

not commutative
encode its parts?
more than parts?

mary likes john (cid:54)= john likes mary
magic carpet     magic + carpet
memory lane (cid:54)= memory + lane

59/113

algebraic vector composition

we take the full composition function ...

p = f (u, v , r, k )

60/113

algebraic vector composition

... and simplify it as follows.

p = f (u, v )

    simple mechanisms for composing vectors
    works well on some tasks
    large choice in composition functionsa

    addition
    multiplication
    dilation
    ...

acomposition in distributional models of semantics. mitchell and lapata,

cognitive science 2010

60/113

algebraic vector composition

... and simplify it as follows.

p = f (u, v )

but it   s broken
this simpli   cation fails to capture important aspects such as

    grammatical relations
    word order
    ambiguity
    context
    quanti   er scope

60/113

lexical function models

one solution: lexicalise composition.

    di   erent syntactic patterns indicate di   erence in

composition function.

    some words modify others to form compounds

(e.g. adjectives).

    let   s encode this at the lexical level!
example: adjectives as lexical functions

p = f (red, house) = fred (house)

61/113

lexical function model example

baroni and zamparelli (2010)3

    adjectives are parameter matrices (  red ,   furry , etc.).
    nouns are vectors (house, dog, etc.).
    composition is simply red house =   red    house.

learning adjective matrices

1 obtain vector nj for each noun nj in lexicon.
2 collect adjective noun pairs (ai , nj ) from corpus.
3 obtain vector hij of each bigram ai nj .
4 the set of tuples {(nj , hij )}j is a dataset di for adj. ai .
5 learn matrix   i from di using id75.

3nouns are vectors, adjectives are matrices: representing adjective-noun

constructions in semantic space. baroni and zamparelli, emnlp   10

62/113

uses and evaluations for lexical function models

lexical function models are generally applied to short phrases
or particular types of compostion (e.g. noun compounds).

related tasks and evaluations
semantic plausibility judge short phrasesab

   re beam
table show results

   re glow
table express results
morphology learn composition for morphemesc
shamelessness

f(f(shame, less), ness)

decomposition extract words from a composed unitd

fdecomp (reasoning)
fdecomp (f(black, tie))

deductive thinking
cravatta nera

a vector-based models of semantic composition. mitchell and lapata, acl   08
b experimental support [...]. grefenstette and sadrzadeh, emnlp   11
c lazaridou et al, acl   13; botha and blunsom icml   14
d andreas and ghahramani, cvsc   13; dinu and baroni, acl   14

63/113

higher valency functions with tensors

how do we go from predicates (adjectives) to higher-valency
relations (verbs, adverbs)?

    matrices encode linear maps. good for adjectives.
    what encodes multilinear maps? tensors.
    an order-n tensor tr represents a function r of n   1

arguments.

    tensor contraction models function application.

for n-ary functions to order n + 1 tensors

r(a, b, c)     ((tr    a)    b)    c

64/113

i   m getting tensor every day

we like tensors. . .

    encode multilinear maps.
    nice algebraic properties.
    learnable through regressiona
    decomposable/factorisable.
    capture k-way correlations between argument features

and outputs.

a grefenstette et al., iwcs   13

but. . .

    big data structures (d n elements).
    hard to learn (curse of dimensionality).

65/113

from tensors to non-linearities

q: can we learn k-way correlations without tensors?
a: non-linearities + hidden layers!

for example:

    xor not linearly

separable in 2d space.

    order-3 tensors can

model any binary logical
operation (grefenstette
2013).

    non-linearities and
hidden layers o   er
compact alternative.

66/113

pqbiasp xor q  (p xor q)neural models

a lower-dimensional alternative
having established nonlinear layers as a low-dimensinonal
alternative to tensors, we can rede   ne semantic composition
through some function such as

p = f (u, v , r, k ) = g (w u

rk u + w v

rk v + brk ) ,

where g is a nonlinearity, w are composition matrices and b a
bias term.

67/113

neural models

a lower-dimensional alternative
having established nonlinear layers as a low-dimensinonal
alternative to tensors, we can rede   ne semantic composition
through some function such as

p = f (u, v , r, k ) = g (w u

rk u + w v

rk v + brk ) ,

where g is a nonlinearity, w are composition matrices and b a
bias term.

recursion
if w u
functions can be applied recursively.

rk and w v

rk are square, this class of composition

67/113

id56s

composition function:
f (u, v ) = g (w (u(cid:107)v ) + b)
g is a non-linearity

w     rn  2n is a weight matrix
b     rn is a bias
u, v     rn are inputs

this is (almost) all you need
this is the de   nition of a simple id56.a
but key decisions are still open: how to parametrise,
composition tree, training algorithm, which non-linearity etc.

a pollack,    90; goller and k  uchler,    96; socher et al., emnlp   11; scheible and

sch  utze, iclr   13

68/113

choices to make

decisions, decisions
tree structure left/right-branching, greedy based on errora,

based on parseb, ...

non-linearity c tanh, logistic sigmoid, recti   ed linear ...
initialisation d zeros, gaussian noise, identity matrices, ...

a socher et al., emnlp   11
b hermann and blunsom, acl   13
c lecun et al., springer 1998
d saxe et al., iclr   14

69/113

matrix-vector neural networks

alternative: represent everything as both a vector and a
matrix (socher et al. (2012)).

this adds an element similar to the lexical function models
discussed earlier.

70/113

(     ,   )(     ,   )(     ,   )   erce   erce gamegamematrix-vector neural networks

alternative: represent everything by both a vector and a
matrix (socher et al. (2012)).

this adds an element similar to the lexical function models
discussed earlier.

71/113

(     ,   )(     ,   )      g(                    )  (                ,                        )   ercegamematrix-vector neural networks

alternative: represent everything by both a vector and a
matrix (socher et al. (2012)).

formalizing mvid56s

(c , c) = f (((a, a), (b, b)))

c = g (w   

(cid:21)

)

(cid:20) ba
(cid:21)
(cid:20) a

ab

c = wm   
a, b, c     r d ; a, b, c     r d  d ; w , wm     r d  2d

b

this adds an element similar to the lexical function models
discussed earlier.

72/113

convolution neural networks

a step back: how do we learn to recognise pictures?
will a fully connected neural network do the trick?

73/113

8convnets for pictures

problem: lots of variance that shouldn   t matter (position,
rotation, skew, di   erence in font/handwriting).

74/113

888888convnets for pictures

solution: accept that features are local. search for local
features with a window.

75/113

8convnets for pictures

convolutional window acts as a classifer for local features.

   

76/113

8convnets for pictures

di   erent convolutional maps can be trained to recognise
di   erent features (e.g. edges, curves, serifs).

77/113

...convnets for pictures

stacked convolutional layers learn higher-level features.

one or more fully-connected layers learn classi   cation function
over highest level of representation.

78/113

fully connected layerconvolutional layer88raw imagefirst order local featureshigher order featurespredictionconvnets for language

convolutional neural networks    t natural language well.

deep convnets capture:
    positional invariances
    local features
    hierarchical structure

language has:

    some positional

invariance

    local features (e.g. pos)
    hierarchical structure

(phrases, dependencies)

79/113

convnets for language

how do we go from images to sentences? sentence matrices!

80/113

w1w2w3w4w5convnets for language

does a convolutional window make sense for language?

81/113

w1w2w3w4w5convnets for language

a better solution: feature-speci   c windows.

82/113

w1w2w3w4w5convnets for language

to compute the layerwise convolution, let:

application matrix

    m be the width of the convolution window
    d be the input dimensionality
    m     rd  m be a matrix with    lters as rows
    f     r d  dm = [diag (m:,1), . . . , diag (m:,m)] be the    lter
    wi     rd be the embedding of the ith word in the input
    h     rd  l be the    sentence    matrix obtained by applying
    b     rd a bias vector

the convolution to the input layer of l id27s

sentence

83/113

convnets for language

applying the convolution

   i     [1, l] h:,i = g (f

         

wi
...

wi+m   1

          + b)

84/113

++++++h:,ifddmdmd11[wi   : ... : wi+m-1   ]   convnets for language

a full convolutional sentence model
come and see the poster for kalchbrenner et al. (2014),
a convolutional neural network for modelling sentences.

monday, 18:50-21:30pm, grand ballroom, lp17

85/113

training compositional vector space models

several things to consider
training signals autoencoders, classi   ers, unsupervised signals
gradient calculation id26
gradient updates sgd, l-bfgs, adagrad, ...
black magic drop-out, layer-wise training, initialisation, ...

86/113

autoencoders

autoencoders can be used to minimise information loss during
composition:

we minimise an objective function over inputs xi , i     n and
their reconstructions x(cid:48)
i :

n(cid:88)

i

j =

1
2

(cid:107)x(cid:48)
i     xi(cid:107)2

87/113

recursive autoencoders

we still want to learn how to
represent a full sentence (or
house). to do this, we chain
autoencoders to create a
recursive structure.

88/113

recursive autoencoders

objective function
minimizing the reconstruction
error will learn a compression
function over the inputs:

(cid:13)(cid:13)(cid:13)xi     x(cid:48)

i

(cid:13)(cid:13)(cid:13)2

erec(i,   ) =

1
2

88/113

recursive autoencoders

objective function
minimizing the reconstruction
error will learn a compression
function over the inputs:

(cid:13)(cid:13)(cid:13)xi     x(cid:48)

i

(cid:13)(cid:13)(cid:13)2

erec(i,   ) =

1
2

question: composition = compression?

88/113

classi   cation signals

classi   cation error

e (n, l,   ) =

1
2 (cid:107)l     vn(cid:107)2

(cid:88)

n   n

where vn is the output of a
softmax layer on top of the neural
network.

89/113

classi   cation signals

classi   cation error

e (n, l,   ) =

1
2 (cid:107)l     vn(cid:107)2

(cid:88)

n   n

where vn is the output of a
softmax layer on top of the neural
network.

question: sentiment = semantics?

89/113

semantic transfer functions

simple energy function
strongly align representations of semantically
equivalent sentences (a, b)

edist(a, b) = (cid:107)f (a)     g (b)(cid:107)2

    works if cvm and representations in one model are    xed

(semantic transfer).

    will degenerate if representations are being learned jointly

(i.e. in a multilingual setup).

90/113

a noise-contrastive large-margin function

representations in both models can be learned in parallel with
a modi   ed energy function as follows.

a large-margin objective function
enforce a margin between unaligned sentences (a, n)

enoise(a, b, n) = [m + edist(a, b)     edist(a, n)]+

objective function for a parallel corpus ca,b

j(  bi ) =

enoise(a, b, ni )

+

(cid:88)

(cid:32) k(cid:88)

(a,b)   ca,b

i=1

(cid:33)

  
2(cid:107)  bi(cid:107)2

91/113

multilingual models with large-margin training

monolingual composition model

    needs objective function
    supervised or autoencoder?
    compression or sentiment?

92/113

multilingual models with large-margin training

monolingual composition model

    needs objective function
    supervised or autoencoder?
    compression or sentiment?

multilingual model

    task-independent learning
    multilingual representations
    joint-space representations
    composition function
provides large context

92/113

learning

id26
calculating gradients is simple and fast with backprop:

    fast
    uses network structure for e   cient gradient calculation
    simple to adapt for dynamic structures
    fast

gradient-descent based strategies
    stochastic id119
    l-bfgs
    adaptive id119 (adagrad)

93/113

id26 (autoencoder walk-through)

autoencoder
this is a simple autoencoder:
    intermediary layers z, k
    input i
    output/reconstruction o
    hidden layer h
    weight matrices we, wr
    e = 1

2((cid:107)o     i(cid:107))2
we omit bias terms for
simplicity.

94/113

id26 (autoencoder walk-through)

forwardpagate

z = w ei

94/113

id26 (autoencoder walk-through)

forwardpagate

z = w ei

94/113

id26 (autoencoder walk-through)

forwardpagate

h =   (z)

94/113

id26 (autoencoder walk-through)

forwardpagate

k = w r h

94/113

id26 (autoencoder walk-through)

forwardpagate

o =   (k)

94/113

id26 (autoencoder walk-through)

error function
1
2

e =

((cid:107)o     i(cid:107))2

94/113

id26 (autoencoder walk-through)

error function
1
2

e =

((cid:107)o     i(cid:107))2

id26
we begin by calculating the
error with respect to the
output node o.

94/113

id26 (autoencoder walk-through)

error function
1
2

e =

((cid:107)o     i(cid:107))2

id26
we begin by calculating the
error with respect to the
output node o.

   e
   o

= (o     i)

94/113

id26 (autoencoder walk-through)

forwardpagate

o =   (k)

id26

   e
   k
   e
   o
   o
   k

=

   o
   k

   e
   o

= (cid:8)
=   (cid:48)(k) =   (k)(1       (k))

94/113

id26 (autoencoder walk-through)

forwardpagate

k = w r h

id26

=

   e
   k

   k
   wr

= (cid:8)

= h

   e
   wr
   e
   k
   k
   wr

94/113

id26 (autoencoder walk-through)

forwardpagate

k = w r h

id26

=

   k
   h

   e
   k

= (cid:8)

= wr

   e
   h
   e
   k
   k
   h

94/113

id26 (autoencoder walk-through)

forwardpagate

h =   (z)

id26

   e
   z
   e
   h
   h
   z

=

   h
   z

   e
   h

= (cid:8)
=   (cid:48)(z) =   (z)(1       (z))

94/113

id26 (autoencoder walk-through)

forwardpagate

z = w ei

id26

=

   e
   z

   k
   we

= (cid:8)

= i

   e
   we
   e
   k
   k
   we

94/113

id26 (autoencoder walk-through)

forwardpagate

z = w ei

id26

=

   z
   i

   e
   z

= (cid:8)

= we

   e
   i
   e
   z
   z
   i

94/113

id26 (autoencoder walk-through)

forwardpagate + error

z = w ei

e =

1
2

((cid:107)o     i(cid:107))2

id26

   e
   i
   z
   i
   e
   i

=

   z
   i

   e
   z

+

   e
   i

= we

=    (o     i)

94/113

id26 for recursive neural nets

id26 can be modi   ed for tree structures and to
adjust for a distributed error function.

we know that

(cid:88)

y   y

   e
   x

=

   y
   x

   e
   y

y = successors of x

this allows us to e   ciently
calculate all gradients with
respect to e .

95/113

gradient update strategies

once we have gradients, we needs some function

  t+1 = f (gt,   t)

that sets models parameters given previous model parameters
and gradients.

gradient update strategies

    stochastic id119
    l-bfgs
    adaptive id119

96/113

gradient update strategies

adagrad
fine-tune the learning rate for each parameter based on the
historical gradient for that parameter.

first, initialise hi = 0 for each parameter wi and set step-size
hyperparameter   . during training, at each iteration:

1 calculate gradient gi =   e
. update hi = hi + g 2
i .
  wi
2 calculate parameter-speci   c learning rate   i =      
.
hi
3 update parameters as in sgd: wi = wi       i gi .

explanation
parameter-speci   c learning rate   i decays over time, and more
quickly when weights are updated more heavily.

97/113

learning tricks

various things will improve your odds

    pre-train any deep model with layer-wise autoencoders
    regularise all embeddings (with l1/l2 regulariser)
    train in randomised mini-batches rather than full batch
    use patience/early stopping instead of training to

convergence

98/113

application: sentiment labelling with reid98s

we can use a id56 to learn sentiment:
    sentiment signal attached to root (sentence) vector
    trained using softmax function and id26

id31
assume the simplest composition
function to begin:

p = g (w (u(cid:107)v ) + b)

99/113

application: sentiment labelling with reid98s

we can use a id56 to learn sentiment:
    sentiment signal attached to root (sentence) vector
    trained using softmax function and id26

id31
assume the simplest composition
function to begin:

p = g (w (u(cid:107)v ) + b)

this will work ...
... sort of.

99/113

making id31 work better

the basic system will work. however, to produce
state-of-the-art results, a number of improvements and tricks
are necessary.

composition function

other changes

    parametrise the composition

function

    more complex word

representations

    structure the composition

on parse trees

    convolution instead of

binary composition

    instead of the root node,

evaluate on all nodes
    add autoencoders as a
second learning signal

    initialise with pre-trained

representations

    drop-out training and

similar techniques

100/113

corpora for id31

corpora

    movie reviews (pang and lee)

    relatively small, but has been used extensively
    sota    87% accuracy (kalchbrenner et al., 2014)
    http://www.cs.cornell.edu/people/pabo/

movie-review-data/

    sentiment treebank

    sentiment annotation for sentences and sub-trees
    sota    49% accuracy (kalchbrenner et al., 2014)
    http://nlp.stanford.edu/sentiment/

treebank.html

    twitter sentiment140 corpora
    fairly large amount of data
    twitter language is strange!
    sota    87% (kalchbrenner et al., 2014)
    http://help.sentiment140.com/for-students/

101/113

application: cross-lingual document classi   cation

one application for multilingual representations is cross-lingual
annotation transfer. this can be evaluated with cross-lingual
document classi   cation (klementiev et al., 2012):

102/113

application: cross-lingual document classi   cation

one application for multilingual representations is cross-lingual
annotation transfer. this can be evaluated with cross-lingual
document classi   cation (klementiev et al., 2012):

102/113

application: cross-lingual document classi   cation

one application for multilingual representations is cross-lingual
annotation transfer. this can be evaluated with cross-lingual
document classi   cation (klementiev et al., 2012):

102/113

application: cross-lingual document classi   cation

one application for multilingual representations is cross-lingual
annotation transfer. this can be evaluated with cross-lingual
document classi   cation (klementiev et al., 2012):

102/113

cross-lingual document classi   cation

two stage strategy

1 representation learning

using the large-margin objective introduced earlier, it is
easy to train a model on large amounts of parallel data
(here: europarl) using any composition function together
with adagrad and an l2 regularizer.

2 classi   er training

subsequently, sentence or id194s can
be used as input to train a supervised classi   er (here:
averaged id88). assuming the vectors are
semantically similar across languages this classi   er should
be useful independent of its training language.

103/113

cldc results

two composition models in the multilingual setting

fadd(a) =(cid:80)|a|

i=0 ai

fbi (a) =(cid:80)|a|

i=1 tanh (xi   1 + xi )

88.1

86.1

86.2

83.7

79

76.9

79.2

71.1

71.4

68.6

67.4

80

60

e
r
o
c
s
-
1
f

77.6

68.1

65.1

46.8

46.8

maj gloss mt i-matrix add bi add+ bi+

en   de

de   en

104/113

cldc results

two composition models in the multilingual setting

fadd(a) =(cid:80)|a|

i=0 ai

fbi (a) =(cid:80)|a|

i=1 tanh (xi   1 + xi )

more details on these results
come and see the talk for hermann and blunsom (2014),
multilingual models for compositional distributed semantics

monday, 10:10am, grand ballroom vi, session 1b

104/113

outline

1 id65

2 neural distributed representations

3 semantic composition

4 last words

105/113

recap

distributional models:
    well motivated
    empirically successful at the word level
    useable at the phrase level

but. . .

    no easy way from word to sentence
    primarily oriented towards measuring word similarity
    large number of discrete hyperparameters which must be

set manually

106/113

recap

distributed neural models:

    free us from the curse of distributional hyperparameters
    fast
    compact
    generative
    easy to jointly condition representations

107/113

recap

distributed compositional models:

    allow classi   cation over and generation from phrase,

sentence, or id194s

    id56s integrate syntactic structure
    convnets go from local to global context hiearchically
    multimodal embeddings

108/113

conclusions

    neural methods provide us with a powerful set of tools for

embedding language.

    they are easier to use than people think.
    they are true to a generalization of the distributional

hypothesis: meaning is inferred from use.

    they provide better ways of tying language learning to

extra-linguistic contexts (images, knowledge-bases,
cross-lingual data).

    you should use them.

thanks for listening!

109/113

references

id65

for corpus-based semantics.

word co-occurrence statistics: stop lists, id30 and svd.

    baroni, m. and lenci, a. (2010). distributional memory: a general framework
    bullinaria, j. and levy, j. (2012). extracting semantic representations from
    firth, j.r. (1957). a synopsis of linguistic theory 1930-1955.
    grefenstette, g. (1994). explorations in automatic thesaurus discovery.
    harris, z.s. (1968). mathematical structures of language.
    ho   man, t. and puzicha, j. (1998). unsupervised learning from dyadic data.
    landauer, t.k. and dumais, s.t. (1997). a solution to plato   s problem: the

latent semantic analysis theory of acquisition, induction, and representation of
knowledge.

    lin, d. and pantel, p. (2001). dirt     discovery of id136 rules from text.
    pad  o, s. and lapata, m. (2007). dependency-based construction of semantic
    turney, p.d. and pantel, p. (2010). from frequency to meaning: vector space

space models.

models of semantics.

110/113

references

neural language modelling

models.

neural probabilistic language models.

deep architecture for id29.

class-based id165 models of natural language.

    bengio, y., schwenk, h., sen  ecal, j. s., morin, f. and gauvain, j.l. (2006).
    brown, p.f., desouza, p.v., mercer, r.l., pietra, v.j.d. and lai, j.c. (1992).
    grefenstette, e., blunsom, p., de freitas, n. and hermann, k.m. (2014). a
    kalchbrenner, n. and blunsom, p. (2013). recurrent continuous translation
    mikolov, t., kara     at, m., burget, l., cernock  y, j. and khudanpur, s. (2010).
    mnih, a. and hinton, g. (2007). three new id114 for statistical
    mnih, a. and hinton, g. (2008). a scalable hierarchical distributed language
    sutskever, i., martens, j. and hinton, g. (2011). generating text with

recurrent neural network based language model.

language modelling.

recurrent neural networks.

model.

111/113

references

compositionality

logical calculi with tensors.

generation in id65.

categorical compositional distributional model of meaning.

multi-step regression learning for compositional id65.

    dinu, g. and baroni, m. (2014). how to make words with vectors: phrase
    grefenstette, e. (2013). towards a formal id65: simulating
    grefenstette, e., dinu, g., zhang, y.z., sadrzadeh, m. and baroni, m. (2013).
    grefenstette, e. and sadrzadeh, m. (2011). experimental support for a
    hermann, k.m. and blunsom, p. (2013). the role of syntax in vector space
    hermann, k. m. and blunsom, p. (2014). multilingual models for
    kalchbrenner, n. and blunsom, p. (2013). recurrent convolutional neural
    kalchbrenner, n., grefenstette, e. and blunsom, p. (2014). a convolutional
    lazaridou, a., marelli, m., zamparelli, r. and baroni, m. (2013).

compositionally derived representations of morphologically complex words in
id65.

neural network for modelling sentences.

networks for discourse compositionality.

compositional distributed semantics.

models of id152.

112/113

references

compositionality (continued)

and time series.

    lecun, y. and bengio, y. (1995). convolutional networks for images, speech,
    marelli, m., menini, s., baroni, m., bentivogli, l., bernardi, r. and zamparelli,

r. (2014). a sick cure for the evaluation of compositional distributional
semantic models.

composition.

    mitchell, j. and lapata, m. (2008). vector-based models of semantic
    socher, r., pennington, j., huang, e.h., ng, a.y. and manning, c.d. (2011).

semi-supervised recursive autoencoders for predicting sentiment distributions.

113/113

